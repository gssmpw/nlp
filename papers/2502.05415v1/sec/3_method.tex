
\section{Preliminary: Show-o}
\label{sec:pre}
This section provides a brief overview of Show-o, a unified generative model for both images and text.%, mainly about how it works and more importantly how to integrate discrete diffusion modeling and autoregressive (AR) modeling into a unified framework.
% \subsubsection{Discrete Denoising of show-o}
% \zj{This section briefly review xxxx and highlight the issue .....}

% \zj{Some key components should be introduced. Like the notations for images and texts, the image tokenizer, the ar loss and diffusion loss of show-o, the inference procedure of image tokens, etc.}

\noindent \textbf{Image Tokenization.}
Show-o opts to model the distribution of discrete image tokens via discrete diffusion~\cite{austin2021structured}. 
To this end, it exploits MAGVIT-v2~\cite{yu2023language} to convert high-dimensional continuous images into discrete token sequences and employs a unified large vocabulary to represent both text and image tokens. 
This enables using a single transformer to jointly characterize these two modalities.

% \noindent \textbf{Autoregressive and Diffusion Loss.}
\noindent \textbf{Training Objectives.}
% (\RN{1}) 
% \textbf{Next Token Prediction}: For the text component, the autoregressive loss is calculated by comparing the predicted next token distribution with the ground-truth label.
% \textbf{Masked Token Prediction}: For the image component, the loss is calculated by comparing the predicted distribution with the ground-truth labels of the masked regions.
Show-o adopts an autoregressive (AR) modeling for text tokens following the principle of Next Token Prediction (NTP). 
For image modeling, 
% integrates autoregressive (AR) modeling for text tokens and discrete diffusion modeling for image tokens within a unified transformer architecture. 
% Specifically, we use Next Token Prediction (NTP) to model the text generation process;
the discrete diffusion paradigm can be equivalently simplified to a Mask Token Prediction (MTP) objective~\cite{chang2022maskgit}. 
Formally, let $\mathbf{u} := \{u_1, u_2, \cdots, u_m\}$ and $\mathbf{v} := \{v_1, v_2, \cdots, v_n\}$ denote a sequence of $m$ image tokens and a sequence of $n$ text tokens, 
% let 
% $u_*$ denote a mask token,
% $\mathbf{u}_*=\{u_*, u_2, \cdots,u_*, u_m\}$ denote an image sequence that is randomly replaced by mask tokens, 
Show-o maximizes these objectives for training:
% The total object is divided into two components, including Next Token Prediction (NTP) and Mask Token Prediction (MTP) 
\begin{equation}\small\label{eq:ntp}
\mathcal{L}_{\text{NTP}} := \sum_i \log p_\theta(v_i | v_1, \cdots , v_{i-1}, \mathbf{u}), 
\end{equation}
\begin{equation}\small\label{eq:mtp}
\mathcal{L}_{\text{MTP}} :=\sum_j \log p_\theta(u_j | u_*, u_2, \cdots,u_*,u_m, \mathbf{v}),
\end{equation}
% \begin{equation}
%     \mathcal{L}_{\text{NTP}} = \sum_{i=1}^{n} \log \left[ p_\theta(\mathbf{u}, \mathbf{v}_{<i})(v_i) \right]
% \end{equation}
% \begin{equation}
%     \mathcal{L}_{\text{MTP}} = \sum_{j=1}^{m} \log \left[ p_\theta(\mathbf{v},\mathbf{u}_*)(u_j) \right]
% \end{equation}
where $p_\theta$ is the prediction distribution represented by Show-o, $u_*$ refers to the special mask token \texttt{[MASK]}, and 
$i$ and $j$ traverse all text tokens and mask image tokens respectively.
% whose locations are randomly sampled during training. 
% and $p_\theta(\cdot|\cdot)$ denote the conditional probability modeled by Show-o, the objectives can be written as follows:

\noindent \textbf{Inference.}
The text generation of Show-o is based on the naive AR strategy. 
For image generation, Show-o adheres to the methodology outlined in MaskGIT~\cite{chang2022maskgit}. This involves predefining a progressively decreased mask ratio schedule over $K$ steps, initializing a sequence of full mask tokens $\mathbf{u}^0$, and iteratively reducing the number of mask tokens according to the schedule.
% The image sampling starts from a series of fully masked image tokens $\mathbf{u}^K$ and gradually reconstructs the masked input through iterations. 
Specifically, letting $\mathbf{u}^{k}$ denote the sequence containing partial mask tokens at $k$-th iteration, the model yields the prediction distribution for each mask token in $\mathbf{u}^{k}$ and use a sample from the distribution to replace each mask token. 
% ,  performs two main steps: predicting position regions and re-masking low-confidence regions. 
% In the first step, the model predicts the logits of the unknown $u^*$ regions in $\mathbf{u}^{k}$, and samples token IDs for the $u^*$ regions according to the logits, while fixing that of the known regions. 
After that, the model follows the mask schedule to replace the low-confidence predictions back as mask tokens, yielding $\mathbf{u}^{k+1}$. 
% Subsequently, the model retrieves the mask ratio corresponding to the current timestep $k$ based on the mask schedule. 
% It then replaces the low-confidence regions of $u^*$ with the sampled results, thereby generating the input for the next iteration.
The sampling trajectory $\{ \mathbf{u}^{0},\mathbf{u}^{1},\ldots,\mathbf{u}^K\}$ are illustrated in Figure~\ref{fig:trajectory_framework}. %, and the proportion of masks contained in the trajectory gradually decreases.
Besides, it is shown that Classifier-Free Guidance (CFG)~\cite{ho2021classifier} can be incorporated into Show-o to improve the sample quality~\cite{xie2024show}. 
However, CFG introduces an additional evaluation of the model, thereby increasing the sampling cost.
% \noindent \textbf{Classifier-Free Guidance (CFG)}~\cite{ho2021classifier} to enhance sampling quality. 
%
% a technique first introduced in diffusion models to improve image generation quality~\cite{ho2021classifier}.
% It enhances the guidance of the conditioning variables on the model's output by interpolating between conditional and unconditional predictions during sampling. 
% Specifically, given the conditional prediction $p_{\theta}(\cdot | \mathbf{u}^*, \mathbf{v})$ and the unconditional prediction $p_{\theta}(\cdot | \mathbf{u}^*)$, the actual probability distribution used for sampling can be defined as follow:

% \begin{equation}
%     p_{cfg} = (1 + \omega) p_{\theta}(\cdot | \mathbf{u}^*, \mathbf{v}) - \omega p_{\theta}(\cdot | \mathbf{u}^*),
% \end{equation}
% \noindent where $\omega$ represents the guidance scale.

% While CFG allows the model output to better align with the prompt, the unconditional output necessitates an additional evaluation of the model, thereby increasing the sampling cost.

% With the superscript denoting the timestep and $K$ as the total sampling steps, 
% the sampling of an image starts from a sequence of fully masked image tokens $\mathbf{u}^K$ and gradually reconstructs the masked input through iteration.
% During iteration, the model takes a partially masked image token $\mathbf{u}^k$ and performs two main operations: (\romannumeral 1) The model predicts the class probabilities for all masked regions in $\mathbf{u}^k$; 
% (\romannumeral 2) Based on predictions, the model selectively retains only the high-confidence regions by unmasking them, while the regions with lower confidence remain masked.
% During iteration, given input $\mathbf{u}^{k},\mathbf{v}^{k}$ at timestep $k$, the model predicts the class probabilities for all masked regions in the image sequence. 
% For the probability distribution predicted by the model at any position $u_j$, we denote a specific token index value at position $u_j$ by $u$, i.e. $u \sim p_\theta(u_{j}|\mathbf{u}^{k},\mathbf{v}^{k})$, where $j$ is the index for the image token.
% During iteration, given input $\mathbf{u}^{k},\mathbf{v}^{k}$ at timestep $k$, the model predicts the probabilities for all regions in the image sequence. 
% For the probability distribution predicted by the model at any position $u_j$, with $j$ the index for the image token, we denote a specific token index value at position $u_j$ by $u$, i.e. $u \sim p_\theta(u_{j}|\mathbf{u}^{k},\mathbf{v}^{k})$.
% Based on predictions, the model only concerns the mask regions and selectively retains only the high-confidence regions higher than the confidence threshold $\mathbf{\tau}$ by unmasking them, i.e. $u_{j}^{k-1} = u$, while the regions with lower confidence remain masked, i.e. $u_{j}^{k-1} = u_*$.
% \begin{itemize}
%     \item{Class Probability Prediction}: The model predicts the class probabilities for all masked regions in  $x_k$.
%     \item{Selective Masking Update}: Based on these predictions, the model selectively retains only the high-confidence regions by unmasking them, while the regions with lower confidence remain masked.
% \end{itemize}
% $p_\theta$ denote the probability distribution of the model's predictions for all regions, 
% $\mathbf{u}^k=\{u_1^k, u_2^k, \cdots, u_m^k\}$ denote the image tokens at time $k$,
% $p_\theta(\cdot|\mathbf{u}^{k},\mathbf{v}^{k})$ represent the class probability distribution predicted by the model,

% Formally, let 
% $j \in \mathcal{M}^{k}$, the index set for masked tokens of $\mathbf{u}^{k}$,
% % $j$ represent the indices of image tokens,
% and $\mathbf{\tau}$ for the confidence threshold for unmasking tokens,
% the iteration follows:
% \begin{align}
%     u &\sim p_\theta(u_{j}|\mathbf{u}^{k},\mathbf{v}^{k}), \\
%     u_{j}^{k-1} &= 
%     \begin{cases}
%         u_{j}^{k}, & \text{if } j \notin \mathcal{M}^{k}, \\
%         u , & \text{if } p_\theta(u_{j}=u|\mathbf{u}^{k},\mathbf{v}^{k}) > \mathbf{\tau} \text{ and } j \in \mathcal{M}^{k}, \\
%         MASK, & \text{if } p_\theta(u_{j}=u|\mathbf{u}^{k},\mathbf{v}^{k}) \leq \mathbf{\tau} \text{ and } j \in \mathcal{M}^{k}.
%     \end{cases}
% \end{align}


% the updated input $x_{k-1} $ of the next iteration is represented as follows:
% \begin{equation}
%     p_{k} &= p_\theta(x_k), \\
%     x_{k-1} &= \text{M}\left(\xi(p_{k}), L_{C}(p_{k})\right),
% \end{equation}
% the updated rules for $x_i$ follow:

% We observe that this trajectory is conceptually similar to the Jacobian trajectory in Jacobian decoding, as both are formed through model-based iterations.
% Inspired by CLLM, we unify the trajectory concepts of the two and apply consistency distillation to the trajectories generated by discrete diffusion iterations.


\section{Method}
\label{sec:method}
% \zj{revise according to the logics in intro; besides, many notations in this section should be defined in preliminary}
% \zj{have you defined the abbreviation of T2I and MMU?} 
This section introduces Show-o Turbo to reduce the sampling steps of Show-o for inference acceleration. 
% First, we introduce our observation on the unified perspective for images and text. 
% Then we explain in detail how a unified approach is used to accelerate the inference of Show-o, 
% simultaneously train two tasks: text-to-image (T2I) generation and multimodal understanding (MMU). 
% followed by the description of the sampling strategy employed by Show-o Turbo.

% \subsection{The Denoising Perspective of Text Generation}

% In text acceleration, Show-o Turbo generates text trajectories based on Jacobi decoding~\cite{song2021accelerating}, a parallel method that accelerates text generation by iteratively refining token predictions until a stable output (i.e. fixed point) is reached.

% % Jacobi decoding starts with an initial sequence of $L$ tokens, $\{ y_1^{(0)}, \dots, y_n^{(0)} \}$, where each $y_i^{(0)}$ is a preliminary guess for the $i$-th token. This sequence can be random or based on prior knowledge. At each iteration $j$, the method updates each token $y_i^{(j+1)}$ using a greedy strategy to maximize the probability conditioned on the input context $p$ and preceding tokens $y_{<i}^{(j)}$. This is formalized as:

% % \[
% % y_i^{(j+1)} = \arg \max_y p(y | y_{<i}^{(j)}, p), \quad \text{for } i = 1, \dots, L,
% % \]
% % where $p$ can be input text or image. 

% Since the baseline Show-o model is not trained on pure text question-answering instructions, we take mixed instructions of images and text as input and extract the Jacobian trajectory generated by the MMU task. In the MMU task, causal attention is used for the output part of the text, so as with the pure text task, a stable convergent Jacobian trajectory can be obtained.

% % \noindent\textbf{Jacobi decoding can be understood as a denoising process.} 
% % \zj{better refer to a figure to explain this point} 
% Jacobi decoding can be understood as a denoising process. As illustrated in Figure \ref{fig:trajectory_framework}, the Jacobi trajectory starts from a sequence of random noise, and through iterative steps, incorrect tokens are progressively corrected. The final converged outcome is high-quality text without noise. Therefore, both the Jacobi trajectory and the Show-o image denoising trajectory can be interpreted as discrete diffusion denoising processes.

% \textbf{Jacobi decoding can be understood as a denoising process.} As the discrete diffusion theory proposed in the D3PM paper, the inverse process of Jacobi decoding can be interpreted as a random transfer matrix $\mathbf{Q}$
% \begin{equation}
% \mathbf{Q}_t = (1 - \beta_t) \mathbf{I} + \beta_t \mathbf{Q}_{\infty},
% \end{equation}
% \noindent where each category transfers to other categories with equal probability $\beta_t$. In the denoising process, the initialized random sequence is pure noise. As the denoising iteration proceeds, the number of correctly predicted tokens increases, indicating that the noise is removed. Therefore, the Jacobi trajectory and the Show-O image denoising trajectory can be unified into a discrete diffusion denoising trajectory.

% \noindent\textbf{Unified cross-modal denoising trajecoties.} 
% If we understand the image trajectory as a denoising trajectory with a mask, and the Jacobian trajectory as a denoising trajectory starting from a random point, then we can use the following unified symbols to represent the trajectory: x represents the trajectory generated by the image, and y represents the trajectory generated by the text. The trajectory we define is as follows:
% \[
% T_{text} = \{ y_N,\ldots,y_i,\ldots,y_0 \},
% \]
% \[
% T_{image} = \{ x_K,\ldots,x_j,\ldots,x_0 \},
% \]

% \noindent The former represents the iterative denoising trajectory of the text, $y_{N}$ is the random starting point of the trajectory, the smaller i is, the more correct tokens there are in $y_{i}$; the latter represents the denoising trajectory of the image, $x_{K}$ represents the pure mask, the smaller j is, the fewer masks the trajectory points contain.

\subsection{View Text Generation as Denoising}
We first establish a unified perspective for the generation of both images and text in Show-o so that a concise and general acceleration strategy can apply. 
% We aim to unify image and text generation. 
We notice the gap between the generation of images and text is mainly that the (mask) image tokens are decoded in parallel but text tokens emerge in an autoregressive manner. 
This motivates us to resort to fixed-point iteration algorithms that decode multiple text tokens in parallel~\cite{song2021accelerating,santilli2023accelerating,kou2024cllms} to bridge the gap. 
% the potential for parallel inference for both images and text. 
% Fortunately, Jacobi Decoding~\cite{santilli2023accelerating} can be employed to facilitate parallel text generation.

\noindent \textbf{Jacobi Decoding}~\cite{santilli2023accelerating} is a representative fixed-point iteration algorithm for parallel text decoding.
Given that, for text generation, Show-o equals a regular language model, we can directly apply Jacobi decoding to Show-o.
Starting from a sequence of $n$ randomly initialized text tokens, denoted as $\mathbf{v}^{0} := \{ v_1^{0}, \dots, v_n^{0}\}$, Jacobi decoding iteratively refines the token sequence until a stable output (i.e. a fixed point). 
% It starts with an initial sequence of $n$ tokens $\{ v_1^{0}, \dots, v_n^{0} \}$, where each $v_i^{0}$ is a preliminary guess for the $i$-th text token. 
% This sequence can be random or based on prior knowledge. 
At $k$-th iteration, the refinement corresponds to simultaneously solving the following $n$ problems:
\begin{equation}\small
\begin{aligned}
    v_1^{k+1} &= \arg \max_v p_\theta(v | v_1^{k}, \mathbf{u}),\\
    v_2^{k+1} &= \arg \max_v p_\theta(v | v_1^{k}, v_2^{k}, \mathbf{u}),\\
    &...\\
    v_n^{k+1} &= \arg \max_v p_\theta(v | v_1^{k}, \dots, v_{n-1}^{k}, \mathbf{u}).
\end{aligned}
\end{equation}
They can be solved simultaneously with only one forward pass of Show-o using a casual attention mask, which takes roughly identical time as decoding one new token. 
Note that the greedy sampling strategy is used here. 
Abusing $K$ to denote the number of iterations to reach the fixed point $\mathbf{v}^{K}$, it is easy to see $K \leq n + 1$ because there is at least one token being correctly predicted in each iteration.\footnote{By correctness, we mean the generated tokens equal to those generated by regular AR decoding.}


% At each iteration $k$, the method updates each token $v_i^{k+1}$ using a greedy strategy to maximize the probability conditioned on the input context and preceding tokens $v_{<i}^{k}$. 
% In the context of Show-o, an image token sequence $\mathbf{u}$ serves as the input context, and the iteration can be formulated as follows:
% \begin{equation}\small
%     v_i^{k+1} = \arg \max_v p_\theta(v | v_1^{k}, \dots , v_{i-1}^{k}, \mathbf{u}), \quad i = 1, \dots, n.
% \end{equation}

Refer to Figure~\ref{fig:trajectory_framework} for a visualization of the sampling trajectory $\{ \mathbf{v}^{0}, \ldots, \mathbf{v}^{K}\}$, which displays a gradual noise removal pattern. 
Although empirical studies in Table~\ref{tab:speed_comparison} show that applying Jacobi decoding to Show-o cannot witness a considerable inference speedup (because Show-o is originally trained to predict the next token instead of decoding multiple tokens concurrently), Jacobi decoding offers us a unified denoising view of the generation of images and text. 
% Thus, the inherent analogy between the sampling process of image tokens and text tokens arises---the image tokens follow a denoising trajectory starting from a pure mask sequence and the text tokens follow a denoising trajectory starting from a random text sequence. 
% As illustrated in Figure~\ref{fig:trajectory_framework}, the Jacobi trajectory begins from a sequence of random noise and progressively corrects erroneous tags through iterative steps, leading to an observation that the Jacobi trajectory in Jacobi decoding is conceptually analogous to the denoising trajectory in the image inference in Show-o, as both can be interpreted as denoising processes. 
% i.e., both the Jacobian trajectory and the Show-o image denoising trajectory are essentially discrete denoising trajectories.
% In this case, we can attain a unified perspective by interpreting the image trajectory as a denoising trajectory starting from a pure mask sequence, and the Jacobian trajectory as a denoising trajectory starting from a random sequence.
% Hereinafter, we abuse $
%     \mathcal{T} = \{ \mathbf{\tau}^{0},\ldots,\mathbf{\tau}^{k},\ldots,\mathbf{\tau}^K \}
% $ 
% to uniformly denote the denoising trajectory of image and text tokens. 
% For the image denoising trajectory, as 
% $k$ decreases, the mask in trajectory point $\mathbf{\tau}^{k}$ reduces; 
% for the text jacobian trajectory, as $k$ decreases, the erroneous tokens in $\mathbf{\tau}^{k}$ decrease.
% \noindent As $k$ decreases, the masks in image denoising trajectory $\mathbf{\tau}^{k}$ diminishes, while the erroneous tokens in 
% the text Jacobian trajectory also decreases.

\subsection{Show-o Turbo}

% \zj{Do not separate this section into MMU and T2I parts. Instead, use only one unified objective to accommodate the two aspects.
% The regularization terms can be separately described for each part. }
Given the above discussion, the problem of accelerating Show-o amounts to shortening the multimodal denoising trajectories. 
Drawing inspiration from the diffusion acceleration community~\cite{salimans2022progressive,song2023consistency,sauer2025adversarial,liu2023instaflow}, 
we propose to adapt the qualified consistency distillation (CD) strategy~\cite{song2023consistency} to Show-o. 
In particular, we aim at learning a Show-o Turbo model, denoted as $p_\phi$, to consistently map any point on the trajectory of $p_\theta$ to the same endpoint. 
Such an objective can drive Show-o Turbo to generate meaningful content as fast as possible~\cite{song2023consistency,luo2023latent,song2023improved}. 
% including consistency loss, AR loss, next token prediction, and regularization loss.
% In addition, we also introduce trajectory segmentation and curriculum learning strategies for faster convergence. 
% To bridge the gap in sampling steps, we also introduce a sampling strategy distinct from that of Show-o.
In practice, we initialize $\phi$ with the parameter $\theta$ of the teacher Show-o. 
We elaborate on the algorithmic details in the following. 

\noindent \textbf{Consistency Loss.}
% We have defined the denoising trajectories of text and images before.
% Now we divide their trajectories into $S$ segments, $l_{1} = \left\lfloor \frac{N}{S} \right\rfloor $ represents the length of a text trajectory, $l_{2} = \left\lfloor \frac{K}{S} \right\rfloor $ represents the length of an image trajectory. For the sth segment of the trajectory, we can write it as:
% \begin{equation}
% T_{text}^{(s)} = \{ y^{s}_{r+l_{1}-1}, \ldots, y^{s}_{r+1},y^{s}_{r} \}.
% \end{equation}
% \begin{equation}
% T_{image}^{(s)} = \{ x^{s}_{r+l_{2}-1}, \ldots,x^{s}_{r+1}, x^{s}_{r} \}.    
% \end{equation}
% Let $\mathcal{D}_t$ denote the collection of text denoising trajectories obtained by running Jacobi decoding with Show-o on various image 
% Show-o Turbo performs consistency distillation on the unified trajectories defined in Equation~\ref{t1}. 
We use the Jacobi iteration algorithm and the image sampling algorithm detailed in Section~\ref{sec:pre} to collect the trajectories of the original Show-o $p_\theta$ on both text-to-image and image-to-text tasks. 
% the teacher model, i.e. the original Show-o, 
% to generate the Jacobian trajectory dataset and the image denoising dataset denoted as $\mathcal{D}_t$ and $\mathcal{D}_i$ respectively. 
% to construct $\mathcal{D}_t$ by collecting image prompts $\mathbf{u}$ and their iterative text denoising trajectories $\mathcal{T}$, and similarly construct $\mathcal{D}_i$ by collecting text prompts $\mathbf{v}$ and their image denoising trajectories $\mathcal{T}$. 
Then, the consistency loss on image trajectories takes the form of:
% Let $\phi$ denote a set of trainable parameters copied from the $\theta$ of teacher model,
% for the distillation of image denoising trajectories, the consistency loss can be formulated as follows:
% \begin{equation}
% \mathcal{L}_{cMMU} = \ell \left( p_\theta(y_{i}), \text{no\_grad}(p_\theta(y_{0})) \right), \\
%  i \sim \mathcal{U}(1, N)
% \end{equation}
% \begin{equation}
% \mathcal{L}_{cT2I} = \ell\left( p_\theta(x_{j}), \text{no\_grad}(p_\theta(x_{0})) \right), \\
%  j \sim \mathcal{U}(1, K)
% \end{equation}
% \begin{equation}
% \mathcal{L}_{c} = \mathbb{E}_{(x, \mathcal{T}) \sim \mathcal{D}, k \sim \mathcal{U}(0, K)} \ell \left( p_{\theta}(\cdot | \mathcal{T}^k, x), p_{\theta}(\cdot | \mathcal{T}^0, x) \right)
% \end{equation}
% \begin{equation}\small
% \label{lc}
% \mathcal{L}_c^i = \mathbb{E}_{(\mathbf{v}, \mathcal{T}) \sim \mathcal{D}_i, k \sim \mathcal{U}(0,K)} \left[ \sum_j KL(p_{\theta^-}(\cdot|\mathbf{\tau}^0,\mathbf{v})||p_\theta(\cdot|\mathbf{\tau}^k,\mathbf{v})) \right]
% \end{equation}
\begin{equation}\small
\label{lc}
\mathcal{L}^u_{c} = \mathbb{E}_{k \sim \mathcal{U}(0,K)}  d\left(p_{\phi^-}(\cdot|\mathbf{u}^K, \mathbf{v}), p_\phi(\cdot|\mathbf{u}^k, \mathbf{v})\right) ,
\end{equation}
where ${\phi^-}$ denotes the frozen version of ${\phi}$ and $d$ indicates a divergence measure. 
$d$ aggregates the disparity between categorical prediction distributions (e.g., measured by KL divergence) over the mask image tokens as in Equation~\ref{eq:mtp}. % (or the text token as in Equation~\ref{eq:ntp} for $\mathcal{L}^v_{c}$). 
% The above equation corresponds to the consistency loss for images and that 
The consistency loss on text trajectories, denoted as $\mathcal{L}^v_{c}$, can be similarly defined.
% Note that 


Recall that $\mathbf{u}^K$ refers to the endpoint of the trajectory, i.e., the final generation, so $\mathcal{L}^u_{c}$ corresponds to a global consistency loss, which is empirically proven superior over the local one (operating on the adjacent points of the trajectory) for the acceleration of text generation~\cite{kou2024cllms}. 
Conceptually, our objective forms an empirical generalization of the original CD defined on ODE trajectories and a cross-modal extension of~\cite{kou2024cllms}. 
Besides, when collecting the trajectories, we disable the randomness in the sampling process of both modalities by applying a greedy strategy, which makes the discrete sampling trajectories deterministic and is likely to remediate training instability. 
Despite being trained with deterministic trajectories, Show-o Turbo is empirically evidenced to be compatible with the random sampling method for inference (see Table~\ref{tab:topk_comparison}). 
We also clarify that the trajectories corresponding to the image tokens are collected with the involvement of CFG to guarantee the quality of $\mathbf{u}^K$. 

% to general (deterministic) discrete sampling trajectories, and forms a cross-modal extension of consistency LLMs (CLLMs)~\cite{kou2024cllms}.



% \begin{equation}
% \mathcal{L}_{\text{cMMU}} = \ell \left( p_\theta(y^{s}_{i}), \text{no\_grad}(p_\theta(y^{s}_{r})) \right), \\
% \quad i \sim \text{Uniform}(r+1, r+l_{1})
% \end{equation}

% \begin{equation}
% \mathcal{L}_{\text{cT2I}} = \ell\left( p_\theta(x_{j}^{s}), \text{no\_grad}(p_\theta(x_{r}^{s})) \right), \\
% \quad j \sim \text{Uniform}(r+1, r+l_{2})
% \end{equation}
% \noindent where $\theta^- = \text{stop\_grad}(\theta)$, $j$ traverses the unknown region of $\mathbf{\tau}^k$. For distillation of Jacobian denoising trajectories, we simply replace the dataset with $\mathcal{D}_t$ for sampling, and then calculate the consistency loss for all erroneous text token regions in $\mathbf{\tau}$.
% \noindent where ${\phi^-}$ denotes the frozen version of ${\phi}$ and $j$ for the index of mask token of $\mathbf{\tau}$. 
% For distillation of Jacobian denoising trajectories, we simply replace the dataset with $\mathcal{D}_t$ for sampling, and then calculate the consistency loss for all erroneous text token regions in $\mathbf{\tau}$.
% For the text counterpart, we have a consistency loss $\mathcal{L}_{c^t}$ analogous to Equation~\ref{lc} for all erroneous text token regions in $\mathbf{\tau}$, and the final consistency loss $\mathcal{L}_c = \mathcal{L}_{c^i}+\mathcal{L}_{c^t}$.
% \begin{equation}
%     \mathcal{L}_c^t =
% \end{equation}


\noindent \textbf{Regularization.}
% As mentioned in the CLLM paper~\cite{kou2024cllms}, 
% We introduced an auxiliary regularization loss  $AR\_Loss$  to prevent the model from deviating too far from the distribution of the original target language model and generating semantically unavailable information. Specifically, the consistency loss encourages the model to map any intermediate state to the final fixed point (accelerating convergence), while the  $AR\_Loss$ helps ensure that the model does not deviate from the language patterns and quality standards established by the original model.
% While the consistency loss encourages the model to map any intermediate state to the final fixed point, decreasing the number of iteration needed for the convergence, 
Training with only the consistency loss can drive $p_\phi$ to a trivial convergence (e.g., always yielding the same outputs for arbitrary inputs). 
To avoid this, we introduce regularizations for both modalities. 
On the text side, we demand $p_\phi$ to fit the endpoint text tokens $\mathbf{v}^K$ with an objective similar to $\mathcal{L}_{NTP}$, which ensures Show-o Turbo excels in image-to-text modeling. 
On the image side, we record the prediction distributions of the recovered image tokens at each sampling step during the trajectory collection procedure.
The concentration of these distributions contains rich information about the generation process of the teacher Show-o, such as the easy-to-difficult hierarchy, so we advocate using them as another guidance for $p_\phi(\cdot|\mathbf{u}^k, \mathbf{v})$. 


We use $\mathcal{L}_{REG}^v$ and $\mathcal{L}_{REG}^u$ to represent these two regularizations respectively. 
We also include an AR loss $\mathcal{L}_{AR}$ on pure text to maintain the language modeling capacity following the original Show-o.
That said, the total loss is 
% , denoted as $ \mathcal{L}_{NTP}^t $:
\begin{equation}
\mathcal{L} =\mathcal{L}_{c}^u + \alpha \mathcal{L}_{c}^v  +  \beta \mathcal{L}_{REG}^u + \gamma \mathcal{L}_{REG}^v+\delta \mathcal{L}_{AR},
\end{equation}
where $ \alpha $, $ \beta $, $ \gamma$, and $\delta$ are the trade-off coefficnets.

% We concatenate them to serve as a stronger guidance than the prediction distribution of the endpoint image tokens $\mathbf{u}^K$, by introducing a regularization loss with a similar formula as $\mathcal{L}_c$. 
% We observe that relying solely on consistency loss causes the model to deviate into an incorrect state. 
% To prevent such an issue, 
% the model from deviating excessively from the distribution of the original target language model,
% auxiliary regularization loss $L_{AR}$ is introduced. 
% We select the endpoint $\mathbf{\tau}^{0}=\{\mathbf{\tau}^{0}_1,\cdots,\mathbf{\tau}^{0}_i, \cdots,\mathbf{\tau}^{0}_n\}$ of the teacher's trajectory as the label and it follows:
% \begin{equation}
% \mathcal{L}_{AR} =\ell\left( p_\theta(y_0), y_0 \right).
% \end{equation}
% \begin{equation}
% \mathcal{L}_{AR} = \sum_{i=1} \log p_\phi(\mathbf{\tau}^K_i | \mathbf{\tau}^K_1, \cdots , \mathbf{\tau}^K_{i-1}).
% \end{equation}
% so the loss of mmu task can be formulated as follows:
% \begin{equation}
% \mathcal{L}_{\text{MMU}} &= \alpha \cdot \mathcal{L}_{\text{AR}} + \mathcal{L}_{\text{cMMU}}.
% \end{equation}
% Similarly, our image acceleration method also follows the principle of segmentation consistency. We segment the image trajectory generated by the teacher, and then calculate the consistency loss of the model's predicted logits for random points and segmentation points.

% \noindent \textbf{consistency loss} 
% It is worth noting that we calculate the consistency loss of random points and fixed points, rather than the consistency loss of adjacent points (such as MCM \cite{heek2024multistep}). This is because in the discrete diffusion model, the logits output by the model need to be sampled before they can be mapped to the trajectory space, so we cannot directly map the logits to the segmentation points, and can only directly use the logits output at the segmentation points.
% Similar to the distillation of the text part, we divide the image trajectory of length $ K $ into $ S $ segments, $ l_{2} = \left\lfloor \frac{K}{S} \right\rfloor $ represents the length of a segment. The trajectory of the $ s $th segment can be expressed as:
% \[
% T_{image}^{(s)} = \{ x^{s}_{r+l_{2}-1}, \ldots,x^{s}_{r+1}, x^{s}_{r} \}
% .\]
% The regularization loss for image generation can be described as:
% \begin{equation}
% \mathcal{L}_{\text{cT2I}} &= \ell\left( p_\theta(x_{j}^{s}), \, \text{no\_grad}(p_\theta(x_{r}^{s})) \right), \\
% & \quad j \sim \text{Uniform}(r+1, r+l_{2}). \notag 
% \end{equation}
% Similar to text distillation, 
% To relieve deviation from the teacher distribution, the regularization loss is introduced during the training, but directly using the endpoint of the teacher's trajectory with greedy sampling as the regularization causes performance decrease in experiments. 
% as Show-o uses polynomial sampling and the concept of temperature to achieve better results than greedy sampling. 
% Therefore, we design a special logit label. 
% We use $P^{k}$ to record the probability distribution predicted by the teacher model and initialize it to $P^{K}=0$. 
% Denoted $P^{k}$ as the probability distribution 
% Denoted $l$ as the logits sampled from $p_{\theta}$, 
% $\mathbf{M}(\mathbf{\tau}^k)$ as a binary matrix with $1$ representing the masked region in $\mathbf{\tau}^k$, $P^{k}$ as a global logits variable and set $P^{K}=0$, 
% $\phi$ as the parameters of the teacher model,
% the iteration of $P^{k}$ can be formulated as follows:
% When iterating the image denoising trajectory $\mathbf{\tau}^{k}$, we also iterate $P^{k}$ to get $P^{k-1}$. 
% Let $\mathbf{M}(\mathbf{\tau}^k)$ be a binary matrix representing the masked region in $\mathbf{\tau}^k$, $\phi$ represents the parameters of the teacher model.
% The iterative process of $P^{k}$ can be expressed as follows:
% \begin{equation}
% P^{k-1} = P^{k} \odot (1-\mathbf{M}(\mathbf{\tau}^{k})) + l(\mathbf{\tau}^k, \mathbf{v}) \odot \mathbf{M}(\mathbf{\tau}^{k}).
% \end{equation}
% \noindent Through iteration, we can obtain $P^0$ as the label of the regularization constraint. 
% Then the regularization loss can be defined as follows:
% \begin{equation}
% \mathcal{L}_{REG} = \ell\left( p_\theta(x_j), P_{0} \right), j \sim \mathcal{U}(1, K).
% \end{equation}
% \begin{equation}
% \small
% \label{reg}
% \mathcal{L}_{Reg} = \mathbb{E}_{(\mathbf{v}, \mathcal{T}) \sim \mathcal{D}_i, k \sim \mathcal{U}(0,K)} \left[ \sum_j KL(P^0 || p_\theta(\cdot|\mathbf{\tau}^k,\mathbf{v})) \right]
% \end{equation}
% \begin{equation}
% \small
% \label{reg}
% \mathcal{L}_{Reg} = \mathbb{E}_{(\mathbf{v}, \mathcal{T}) \sim \mathcal{D}_i, k \sim \mathcal{U}(0,K)} \left[ \sum_j KL(P^0 || p_\phi(\cdot|\mathbf{\tau}^k,\mathbf{v})) \right]
% \end{equation}
% \noindent so the loss of t2i task can be formulated as follows:
% \begin{equation}
% \mathcal{L}_{\text{T2I}} &= \beta \cdot \mathcal{L}_{\text{reg}} + \mathcal{L}_{\text{cT2I}},
% \end{equation}
% where $ \beta $ is a weighting factor for the regularization loss.

% \noindent\textbf{Total Loss} 

% Show-o Turbo incorporates the same pure text loss as the original Show-o training \cite{xie2024show}. Specifically, in order to maintain its basic language capabilities, we retain an autoregressive loss based on a pure text dataset during training, denoted as $ \mathcal{L}_{\text{NTP}}^{T} $. Therefore, the final loss is as follows:

% \begin{equation}
% \mathcal{L} =\alpha \cdot \mathcal{L}_{\text{cMMU}} +\beta \cdot \mathcal{L}_{\text{cT2I}}+ \gamma \cdot \mathcal{L}_{\text{AR}} + \delta \cdot \mathcal{L}_{\text{REG}} + \mathcal{L}_{\text{NTP}}^{T},
% \end{equation}

% \noindent where $ \alpha $,$ \beta $,$ \gamma $ and $ \delta $ control the ratio of loss.
% % \subsection{Trajectory Segmentation and Curriculum Learning}
% \zj{To realise what, we propose to xxx }

\noindent \textbf{Trajectory Segmentation and Curriculum Learning.}
We empirically ascertain that imposing long-range consistency may introduce unnecessary learning challenges, potentially impeding model convergence and ultimately limiting acceleration capabilities.
Consequently, we suggest splitting the entire training process into multiple stages with decreasing numbers of segments of the sampling trajectory. We enforce consistency solely between the points within a segment and the endpoint of that segment.
The efficacy of such a strategy is also supported by recent advances in consistency distillation~\cite{heek2024multistep,zheng2024trajectory,xie2024mlcm}.
% i.e., only calculate the consistency loss within a segment 
% between any point in the segment and the endpoint of the segment.
% Specifically, for a trajectory, we divide it into $S$ segments with each segment length $L=\left\lfloor \frac{K+1}{S} \right\rfloor$. 
% Let $r=\left\lfloor \frac{k}{L} \right\rfloor \cdot L$  to represent the index of the endpoint of the segment, then Equation~\ref{lc} can be modified as follows:
% The segmented consistency loss of image denoising trajectory defined by Show-o Turbo is as follow:
% \begin{equation}
% \small
% \mathcal{L}_{c^i}^{r} = \mathbb{E}_{k \sim \mathcal{U}(0,K)} \left[ \sum_j KL(p_{\phi^-}(\cdot|\mathbf{\tau}^r,\mathbf{v})||p_\phi(\cdot|\mathbf{\tau}^k,\mathbf{v})) \right]
% \end{equation}
% Following the same logic, we have $\mathcal{L}_c^r = \mathcal{L}_{c^i}^r+\mathcal{L}_{c^t}^r$ and Equation~\ref{reg} is modified as follows:
% \noindent where $j$ traverses the intersection of the unknown regions of $\mathbf{\tau}^k$ and $\mathbf{\tau}^r$. For the distillation of Jacobi denoising trajectory, we only require calculating the consistency loss over regions with different text tokens in $\mathbf{\tau}^k$ and $\mathbf{\tau}^r$.
% Since autoregressive modeling is performed using noise-free text, we do not need to modify $\mathcal{L}_{AR}$ and still use the trajectory endpoint as the label.
% In that case, Equation~\ref{reg} 
% For the regularization loss $\mathcal{L}_{Reg}$, we can use the logits $P^r$ recorded at the segmentation point to constrain the student model:
% \begin{equation}
% \small
% \mathcal{L}_{Reg}^r = \mathbb{E}_{(k \sim \mathcal{U}(0,K)} \left[ \sum_j KL(P^r || p_\phi(\cdot|\mathbf{\tau}^k,\mathbf{v})) \right].
% \end{equation}

% \noindent\textbf{Total Loss.} 
% % Show-o Turbo incorporates the same pure text loss as the original Show-o training~\cite{xie2024show}. 
% To maintain its basic language capabilities, we retain an autoregressive loss based on a pure text dataset during training following Show-o:
% % , denoted as $ \mathcal{L}_{NTP}^t $:
% \begin{equation}
% \mathcal{L}_{NTP}^t = \sum_{i=1}^{n} \log p_\phi(v_i | v_1, \cdots , v_{i-1}). 
% \end{equation}
% Therefore, the final loss is as follows:

% \begin{equation}
% \mathcal{L} =\alpha\mathcal{L}_{c}^r +\beta\mathcal{L}_{AR} + \gamma\mathcal{L}_{Reg}^r + \mathcal{L}_{NTP}^t,
% \end{equation}

% \noindent where $ \alpha $, $ \beta $ and $ \gamma$ control the ratio of loss.

% \noindent \textbf{Curriculum Learning.}
% To achieve more efficient acceleration,
% Show-o Turbo introduces a multi-stage curriculum learning approach, which aims To address the distribution shift between training and sampling. 
As the training proceeds, the trajectory of the student Show-o Turbo may deviate considerably from that of the teacher Show-o.
Thus, persisting in utilizing Show-o's trajectory for distillation purposes could constrain the ultimate acceleration effect. %so continuing to adopt the teacher trajectory for distillation can limit the final acceleration effect. 
To mitigate this, we suggest using the acquired model from the past stage as teacher to construct new trajectories.
Doing so encourages the final Show-o Turbo to learn consistency mapping over long distances.
% \subsection{}
% \noindent \textbf{Top-k in discrete denoising.} 





\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccc|ccccccc|ccc|c}
        \toprule
        \multirow{2}{*}{\textbf{\small Steps}} & \multirow{2}{*}{\textbf{\small Model}} & \multirow{2}{*}{\textbf{\small CFG}} & \multicolumn{7}{c|}{\textbf{\small GenEval \large$\uparrow$}} & \multirow{2}{*}{\textbf{\small HPS \large$\uparrow$}} & \multirow{2}{*}{\textbf{\small IR \large$\uparrow$}} & \multirow{2}{*}{\textbf{\small CS \large$\uparrow$}} & \multirow{2}{*}{\textbf{\small Time (sec) \large$\downarrow$}} \\
        \cmidrule(lr){4-10}
         &  &  & \textbf{\small AVG} & {\small TO} & {\small CT} & {\small P} & {\small CL} & {\small SO} & {\small CA} &  &  &  &  \\
        \midrule
        \multirow{4}{*}{16} & Show-o & 10 & \textbf{0.674} & 0.823 & 0.647 & 0.288 & 0.838 & 0.984 & 0.463 & \textbf{0.277} & \textbf{0.992} & \textbf{0.318} & 1.39 \\
         & Show-o & 5 & 0.672 & 0.778 & 0.666 & 0.293 & 0.835 & 0.991 & 0.468 & 0.270 & 0.885 & \textbf{0.318} & 1.39 \\
         & Show-o Turbo$^*$ & 0 & 0.649 & 0.793 & 0.644 & 0.253 & 0.809 & 0.956 & 0.440 & 0.266 & 0.768 & 0.315 & 0.77 \\
         & Show-o Turbo$\;\,$ & 0 & 0.646 & 0.818 & 0.597 & 0.218 & 0.827 & 0.984 & 0.430 & 0.273 & 0.925 & \textbf{0.318} & 0.77 \\
        \midrule
        \multirow{4}{*}{8} & Show-o & 10 & 0.578 & 0.631 & 0.519 & 0.235 & 0.811 & 0.991 & 0.280 & 0.257 & 0.672 & 0.313 & 0.76 \\
         & Show-o & 5 & 0.580 & 0.647 & 0.584 & 0.225 & 0.766 & 0.984 & 0.275 & 0.255 & 0.632 & 0.313 & 0.76 \\
         & Show-o Turbo$^*$ & 0 & \textbf{0.642} & 0.788 & 0.631 & 0.253 & 0.787 & 0.981 & 0.413 & 0.264 & 0.800 & 0.315 & 0.46 \\
         & Show-o Turbo$\;\,$ & 0 & 0.638 & 0.813 & 0.541 & 0.250 & 0.814 & 0.991 & 0.420 & \textbf{0.273} & \textbf{0.963} & \textbf{0.318} & 0.46 \\
        \midrule
        \multirow{4}{*}{4} & Show-o & 10 & 0.353 & 0.237 & 0.325 & 0.095 & 0.540 & 0.863 & 0.060 & 0.197 & -0.560 & 0.283 & 0.44 \\
         & Show-o & 5 & 0.396 & 0.298 & 0.334 & 0.158 & 0.572 & 0.925 & 0.088 & 0.207 & -0.300 & 0.294 & 0.44 \\
         & Show-o Turbo$^*$ & 0 & 0.596 & 0.692 & 0.553 & 0.218 & 0.758 & 0.978 & 0.375 & 0.249 & 0.633 & 0.312 & 0.30 \\
         & Show-o Turbo$\;\,$ & 0 & \textbf{0.625} & 0.770 & 0.553 & 0.245 & 0.806 & 0.978 & 0.398 & \textbf{0.269} & \textbf{0.934} & \textbf{0.318} & 0.30 \\
         \midrule
         \multirow{4}{*}{2} & Show-o & 10 & 0.181 & 0.025 & 0.131 & 0.008 & 0.327 & 0.588 & 0.008 & 0.140 & -1.756 & 0.246 & 0.29 \\
         & Show-o & 5 & 0.251 & 0.051 & 0.188 & 0.038 & 0.442 & 0.778 & 0.010 & 0.152 & -1.456 & 0.260 & 0.29 \\
         & Show-o Turbo$^*$ & 0 & 0.459 & 0.407 & 0.422 & 0.148 & 0.668 & 0.925 & 0.185 & 0.201 & -0.259 & 0.295 & 0.22 \\
         & Show-o Turbo$\;\,$ & 0 & \textbf{0.557} & 0.614 & 0.478 & 0.180 & 0.793 & 0.972 & 0.305 & \textbf{0.247} & \textbf{0.680} & \textbf{0.312} & 0.22 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Comparison of 512 $\times$ 512 T2I performance on GenEval, HPS, IR, and CS.}
    AVG: average, TO: Two Object, CT: Counting, P: Position, CL: colors, SO: Single Object, CLA: Color Attr. }
    \label{tab:merged_method_comparison}
\end{table*}

\noindent \textbf{Sampling Strategy.}
In the inference phase, the original Show-o generates image tokens with polynomial sampling~\cite{xie2024show}. 
% However, in our refined accelerated model, we found that when generating with fewer steps, the model needs to predict a large number of regions at once, which leads to higher uncertainty in the logits. 
% However, we found that Show-o Turbo needs to predict a large number of regions at once in fewer steps of sampling, leading to higher uncertainty in the logits. 
% However, we found that the gap in sampling steps between Show-o and the  Turbo version leads to higher uncertainty in the logits. 
However, we find that for the learned Show-o Turbo model with few sampling steps, there is significantly higher uncertainty in the prediction distribution of the mask tokens.
% higher uncertainty in the logits occurs when Show-o Turbo performs fewer-step sampling. 
We empirically identify that incorporating the top-k sampling strategy, which is widely used in language models, can alleviate this issue, substantially improving the sampling quality in 2-4 steps (see Table~\ref{tab:topk_comparison}). 
% To address this issue, we introduce top-$k$ sampling in 2-step and 4-step and experimentally achieve better quality.
% , which significantly improves the results. 
% Therefore, in both two-step and four-step sampling, we utilize top-$k$ sampling to ensure better quality of generated images.

% \noindent \textbf{Comparison to CLLMs.} 
% Text acceleration method in our paper adopts a similar strategy to CLLM~\cite{kou2024cllms}, but there is a key difference in how to define the fixed point in the trajectory. 
% CLLM chooses the endpoint of the trajectory as the fixed point, while we introduce the idea of segmentation, dividing the trajectory into several segments and selecting the segmentation points of each segment to calculate the consistency loss, which reduces the difficulty of the learning objective.

% \noindent \textbf{Multimodal Understanding Inference} For the Multimodal Understanding (MMU) task, we use the same Jacobi decoding for sampling as for training.
