\vspace{-2ex}
\section{Introduction}
\label{sec:intro}
Multimodal large models like LLaVA~\cite{liu2024llava} and Stable Diffusion~\cite{esser2024scaling} have shown promise across a variety of multimodal understanding (e.g., image/video to text) and generation (e.g., text to image/video) tasks.
% , with LLaVa~\cite{liu2024llava} and Stable Diffusion 3~\cite{esser2024scalingrectifiedflowtransformers} as the typical works. 
Recently, the attention has been shifted from dedicated multimodal models to a unified, versatile one, with Chameleon~\cite{team2023gemini}, Transfusion~\cite{zhou2024transfusion}, Emu3~\cite{wang2024emu3} and Show-o~\cite{xie2024show} as popular examples. 
Among them, Show-o is distinguished by its outstanding capabilities given the small model size (of only 1.3B parameters) as well as its open-source nature. %, having sparked a series of studies~\cite{}. 
% which can complete the image understanding and generation tasks with only 1.3 B parameters. 
% It beats several models (\eg Gemini-Nano-1~\cite{team2023gemini}, SEED-X~\cite{ge2024seedxmultimodalmodelsunified}) which have the same or larger parameters than Show-o on several benchmarks for text-to-image and image-to-text tasks.
% \zj{also emphasize how its performance is}

% \zj{first describe some details of show-o. for example, you should mention image tokenization, the concept of discrete diffusion on image tokens, ar for texts, and unified modeling, etc. }
In short, Show-o integrates the discrete diffusion modeling of image tokens (yielded by an image tokenizer like MAGVIT-v2~\cite{yu2023language}) and discrete autoregressive (AR) modeling of text tokens into one single transformer. 
% Show-o shares the same parameters to achieve both the discrete diffusion (\eg D3PM~\cite{austin2021structured}) and autoregressive generation. 
% It uses the MAGVIT-v2~\cite{chang2022maskgit} image quantizer to convert images into discrete tokens, and uses a unified vocabulary for both discrete image and text encodings. 
The discrete diffusion can boil down to a masked autoregressive formula in practice. 
As a result, the inference of Show-o involves progressively denoising image tokens and autoregressively decoding the text tokens. %, which, however,  
% It also adopts mask-based discrete diffusion for image denoising and handles text generation in an autoregressive manner.
% However, both of these methods 
This, undesirably, causes tens to hundreds of sampling steps for both image and text generation and hence a high serving cost. 
% require numerous steps to complete their respective tasks, resulting in slow inference speeds.
Although prior studies on accelerating diffusion models~\cite{salimans2022progressive,song2023consistency} or large language models (LLMs)~\cite{pope2023efficiently,leviathan2023fast,li2024eagle} can be separately applied to remediate such issues, the question remains whether a more unified approach exists to enhance the efficiency of Show-o.

%it is still questionable if there is a more unified strategy to improve the generation efficiency of Show-o. % both image understanding and generation tasks. 
% For example, SteamingLLM~\cite{xiao2024efficientstreaminglanguagemodels} controls the KVcache used in casual attention dynamically to accelerate the speed of generation while Show-o applies the full attention to image tokens;
% Adversarial Diffusion Distillation~\cite{sauer2023adversarialdiffusiondistillation} requires to add noise to the image in accelerating diffusion models, which is inconsistent with the text generation process of show-o.
% In this case, there is an urgent need for a new approach to accelerate Show-o.


% \zj{why difficult? should be more clear on this.}
% Unified models, such as Show-o, Emu3, and HART, have shown significant promise by combining language, vision, and even video generation capabilities within a single framework. Leveraging shared representations across tasks, these models reduce training and deployment complexity while enhancing performance, particularly when data is limited. In contrast, earlier approaches used separate models for different tasks---such as the SD series using continuous diffusion for image generation, and open-source models like LLaVa using a combination of CLIP and language models for visual understanding---resulting in modeling redundancy and limited cross-modal learning benefits. These individual models needed separate optimization and training, making the system cumbersome, resource-intensive, and less efficient in exploiting cross-modal dependencies.

% The unified approach simplifies the model pipeline by enabling shared learning across modalities, reducing the need for multiple specialized models. This joint learning minimizes redundancy and enables better information flow between modalities, leading to improved performance in tasks requiring an understanding of relationships between different types of data. For example, Emu3's unified next-token prediction across text, images, and videos demonstrated that a single architecture could produce more coherent outputs compared to separate models. Similarly, HART combined discrete and continuous representations effectively, highlighting the potential of a unified approach for handling diverse content types at high resolutions.

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Comparisons of three types of unified model.}
%    \label{fig:intro_work_compare}
% \end{figure}

% Despite these advances, current unified models still face significant challenges, particularly in terms of generation speed. Most existing models rely on autoregressive generation techniques, which predict each token in sequence. This sequential approach inherently limits generation speed, making these models inefficient for real-time applications requiring fast inference. Emu3 and HART, while effective in multimodal tasks, suffer from slow inference due to their reliance on token-by-token generation.

% Show-o adopts a generation strategy inspired by MaskGIT, which combines parallel decoding with mask prediction to speed up the process compared to fully autoregressive models. However, despite reducing the number of steps needed compared to pure autoregressive methods, Show-o still requires at least 8 steps to generate high-quality 256x256 images. This number of steps remains a bottleneck for tasks requiring fast inference. Projects like Transfusion attempt to address some of these inefficiencies by combining diffusion modeling for images with autoregressive modeling for text. However, they require additional preprocessing steps, such as diffusing images before feeding them into the transformer, which increases computational overhead and limits system efficiency. These limitations highlight the challenge of balancing quality and speed in a unified framework.
%
% \zj{align this paragraph with the contents in abs}
This work introduces Show-o Turbo to reply to the question.
We first advocate exploring parallel decoding algorithms of text tokens~\cite{song2021accelerating,santilli2023accelerating,kou2024cllms} to establish a unified view for the generation of both images and text. 
% Inspired by the Consistency Large Language Models~\cite{kou2024cllmsconsistencylargelanguage}, we identify a unified denoising perspective for the generation of image and text tokens. 
Basically, such algorithms invoke the language model to refine $n$ ($n>1$) text tokens in parallel and iterate until the fixed point. 
The sampling trajectory exhibits a gradual noise removal pattern (see Figure~\ref{fig:trajectory_framework}).
This way, we can unify the generation of both text and images from a denoising view, 
%Show-o Turbo generates the text with  method, which views n-token sequence generation task as a fixed-point iteration and is similar to the denoising process in the image generation. 
and the learning of Show-o Turbo reduces to the shortening of the multimodal denoising trajectories.
% In this sense, both the generation of text tokens and image tokens follow 
% certain denoising trajectories. 
% To shorten the generation trajectories of both tasks, 
Drawing inspiration from diffusion acceleration literature~\cite{salimans2022progressive,song2023consistency,sauer2025adversarial,liu2023instaflow}, we resort to the promising consistency distillation (CD)~\cite{song2023consistency} technique to realize this.
% a well-known approach for shorten the image generation trajectories. 
Concretely, we train Show-o Turbo to consistently map an arbitrary point on the sampling trajectory of Show-o to the same endpoint. 
Such an objective aids in pushing Show-o Turbo to generate meaningful content as fast as possible~\cite{song2023consistency,luo2023latent,song2023improved}. 
Conceptually, our approach forms an empirical generalization of CD, which was originally defined on ODE trajectories, to general (deterministic) discrete sampling trajectories, and stands as a cross-modal extension of consistency LLMs (CLLMs)~\cite{kou2024cllms}.
% and witnesses satisfactory empirical efficacy. 

% Due to the (masked) AR nature of the denoising trajectories of Show-o, we eliminate the need for a careful parameterization of the consistency model as in original CD~\cite{song2023consistency}. 
% Instead, w
We can simply initialize Show-o Turbo as the pre-trained Show-o and perform consistency distillation based on distributional disparity (e.g., KL divergence) due to the discrete nature of the modeling. % and then perform CD. 
Following~\cite{heek2024multistep,zheng2024trajectory,xie2024mlcm}, we further introduce trajectory segmentation and curriculum learning strategies, % to improve the training efficiency and convergence. 
where multiple training stages with decreasing numbers of segments of the entire sampling trajectory are employed and CD is performed within each segment.
This helps improve the model convergence.



% divide the training process into two stages: 
% first, the trajectory is split into $T$ segments and Show-o Turbo learns the mapping within each trajectory segment; then, we use the acquired Show-o Turbo to define trajectories and learn a new Show-o Turbo based on $T/2$ segments for further acceleration.
 %make the student model shorten the trajectories gradually. 
%We draw inspiration from the progress of accelerating denoising processes in diffusion models and use unified consistency distillation for text and image trajectories to shorten these trajectories.~\cite{heek2024multistepconsistencymodels} Specifically, we introduce a student model that consistently maps any two points on the teacher trajectory to the same endpoint. 
%This goal encourages the student model to guess the final result as quickly as possible given a noisy input. 
%Compared with the original consistency distillation defined on ODE trajectories, this study extends the scope of the consistency distillation concept to general discrete sampling trajectories. 
%We introduce multi-segment and curriculum learning strategies to improve training efficiency and convergence.

% To address these challenges, we propose Show-o Turbo, a novel approach that integrates a unified trajectory-consistency distillation method into the Show-o model, aiming to accelerate both understanding and generation tasks simultaneously.Inspired by multistep consistency models~\cite{heek2024multistepconsistencymodels} and consistency large language models~\cite{kou2024cllmsconsistencylargelanguage}, our approach leverages the concept of trajectory-consistency distillation, previously effective in diffusion models and language models, and extends it to a unified multimodal setting. By applying this distillation technique, our model learns from intermediate representations, leading to faster inference. This approach not only provides a consistent acceleration strategy for discrete-labeled image generation but also unifies acceleration for both understanding and generation tasks within a single model.

We perform extensive studies to evaluate the effectiveness of Show-o Turbo. 
% \zj{be consistent and precise. use text-to-image generation} 
For text-to-image generation, we evaluate on various metrics including GenEval~\cite{ghazvininejad2019mask}, Human Preference Score (HPS)~\cite{team2023gemini}, ImageReward (IR)~\cite{li2024eagle}, and CLIP Score (CS)~\cite{hessel2022clipscorereferencefreeevaluationmetric}. % as our metrics to evaluate the the quality of generated images.
We show that the 4-step Show-o Turbo without classifier-free guidance (CFG)~\cite{ho2021classifier} can attain better GenEval, HPS, IR, and CS scores than the 8-step Show-o with CFG. %, while outperforming the latter in terms of HPS, IR, and CS metrics.
% 0.252 HPS, 0.706 IR, 30.88 CS, outperforming the Show-o in 8 steps. 
For image-to-text generation, we evaluate Show-o Turbo on the
% \zj{image captioning benchmark?} 
image description benchmarks Flickr30K~\cite{flickrentitiesijcv,flickr30k} and  NoCaps~\cite{agrawal2019nocaps}, 
% TextCaps~\cite{sidorov2020textcaps}, 
observing a 1.5x inference speedup without a significant performance drop. 
Show-o Turbo also shows reasonable performance on multimodal understanding (MMU) tasks that rely on one-token responses, including POPE~\cite{li2023evaluating}, MME~\cite{fu2023mme}, and MMMU~\cite{yue2024mmmu}. 
% Additionally, we assess the generation speed on COCO 2014 Caption Validation. 
% Experiments demonstrate that Show-o Turbo can infer 1.5 times faster than Show-o with little decrease in the metrics. 
We also conduct ablation experiments for a deep understanding of Show-o Turbo.


% Our contributions can be summarized as follows:
% \begin{enumerate}
%     \item We introduce a unified consistency distillation framework and integrates trajectory-based learning into the Show-o model, accelerating both understanding and generation tasks.
%     \item We use multistep consistency to distill model knowledge in different stages, demonstrating that the distillation technique used in continuous diffusion can also be applied to discrete diffusion and multimodal understanding (MMU) tasks.
%     \item We validate our approach on common benchmark, demonstrating that 4-step sampling without CFG can achieve comparable results to show-o 16-step sampling in the T2I task, while achieving 1.5 times faster text generation in MMU comprehension with little quality loss.
% \end{enumerate}

% Multimodal foundation models like LLaVa~\cite{liu2024visual,liu2024llava}, Emu2~\cite{sun2024generative} and NExT-GPT~\cite{wunext} have shown impressive capabilities in multimodal tasks. However, these models generally assemble expert models from different modalities (\eg CLIP-ViT~\cite{radford2021learning}, Stable Diffusion~\cite{rombach2022high,podellsdxl} and LlaMa~\cite{touvron2023llama,touvron2023llama2}) in a system and treat different modalities separately. This can cause modeling redundancy and reduce information gain brought by cross-modal learning, limiting their ability to generate multimodal documents. Consequently, recent works aim to model multimodality in a unified manner, using a single transformer to handle multimodal understanding and generation.

% A straightforward solution is fully using token-based representations for both image and text modalities and training with next-token prediction autoregressively.~\cite{liu2024worldmodelmillionlengthvideo,team2024chameleon,wu2024janus} Yet, this often leads to information loss due to spatial compression in images. Both Chameleon~\cite{team2024chameleon} and Lumina-Next~\cite{liu2024lumina} have identified that this vector quantization (VQ) bottleneck harms multimodal tasks which require high-frequency details from images, such as OCR-related understanding task and human face generation task. On the other hand, diffusion models~\cite{esser2024scaling,ho2020denoising,rombach2022high,wu2023tune} in image/video generation capacity have exhibited superior visual generation capabilities than autoregressive. This motivates recent works like Show-o~\cite{xie2024show} and Transfusion~\cite{zhou2024transfusion} to combine autoregressive modeling for text with diffusion modeling for images within a single transformer. However, training of these models requires diffuse images before being input into the transformer, which poses challenges for modeling cross-modality interdependence when provided data with noised images before text. For example, Transfusion~\cite{zhou2024transfusion} reported a performance degradation in image captioning (nearly 15\% drop measured by CIDEr) when full-range noise was introduced during training. Furthermore, when facing interleave data (\eg text-image-text-image), this can lead to inconsistency within images and misalignment between image and text.

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Comparisons of three types of unified model.}
%    \label{fig:intro_work_compare}
% \end{figure}

% Given these challenges, we propose \textcolor{blue}{model}, a new paradigm for unified modeling of joint text-image distributions. \textcolor{blue}{model} breaks the quantization bottleneck in image representations and avoids the noise disturbance inherent in directly applying diffusion to the image modality. Specifically, given a series of text tokens and image patches\footnote{Compared with discrete image tokens, we refer to continuous representations without quantization as image patches.}, we model the interdependence of text tokens and image patches (both within and across modalities) by autoregressive, per text-token distribution by a categorical distribution, and per image-patch distribution by diffusion. This decouples the diffusion process from dependencies within image patches, ensuring that the transformer—responsible for capturing cross-modal dependencies—receives clean signals, thereby facilitating more effective cross-modal learning. During inference, \textcolor{blue}{model} performs next-token prediction for the text modality and next-patch distribution for the vision modality.

% \textcolor{blue}{model} can also be seen as a generalization of MAR~\cite{li2024autoregressive} from image generation to cross-modality generation. Unlike DiT~\cite{peebles2023scalable} that uses diffusion to model the joint distribution of all patches, MAR only uses it to model per-patch distribution while capturing interdependency among patches through (masked) autoregression. MAR yields superior image quality due to its quantization-free modeling with diffusion loss. Drawing insights from MAR's success, \textcolor{blue}{model} has the potential to outperform autoregressive-based multimodal models in terms of image generation. Empirical results further validate this assertion, as demonstrated in Table xxx.

% Training a base \textcolor{blue}{model} is both data-efficient and parameter-efficient, and we find that it is unnecessary to train it from scratch. Many autoregressive-based unified models, such as Chameleon~\cite{team2024chameleon}, have been trained on millions of image-text pairs, demonstrating significant capacity for modeling text-image interdependency. Accordingly, we propose an efficient training recipe that embraces pretrained Chameleon and adapts it for \textcolor{blue}{model}. During training, we freeze the transformer to preserve its inherent ability to model interdependency between text tokens and image patches, both within and across modalities, while only tuning the input and output projectors with 10k image-only data. 

% Quantitatively, the instruction-tuned \textcolor{blue}{model} demonstrates superior multimodal understanding and generation capacity compared with Chameleon finetuned with the same high-quality data. Notably, \textcolor{blue}{model} achieves a GenEval score of 0.6, surpassing text-to-image generative models such as such as DALL-E 2~\cite{ramesh2022hierarchical} and SDXL~\cite{podellsdxl}. Furthermore, \textcolor{blue}{model} is comparable and even better than current unified models across various benchmarks—even without heavy pretraining—positioning it as a promising approach for unified multimodal modeling.

% To summarize, our key contributions are as follows:
% \begin{itemize}
%     \item We introduce \textcolor{blue}{model}, a new framework for unified multi-modality modeling, overcoming quantization bottleneck and noise-disturbance overhead present in previous autoregressive and diffusion unified modeling.
%     \item We design an efficient training strategy for \textcolor{blue}{model}, requiring only around 32 A100 GPU hours to build a base \textcolor{blue}{model}.
%     \item \textcolor{blue}{model} demonstrates strong performance across various multimodal understanding and generation benchmarks. It also excels in mix-modality generation in both zero-shot scenarios and when fine-tuned for downstream applications, such as storybook generation.
% \end{itemize}
