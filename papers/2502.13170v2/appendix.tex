\clearpage
\appendix

\section{DSL Grammars}
\label{app:dsl}
RobustFill is a string manipulation task using the DSL. Figure~\ref{fig:robustfill_dsl} illustrates the DSL syntax for RobustFill. Our implementation is based on the works of ExeDec~\citep{shi2024exedec} and RobustFill~\citep{devlin2017robustfill}.

Deepcoder is a list transformation task using the DSL. Figure~\ref{fig:deepcoder_dsl}. This implementation is based on the works of ExeDec~\citep{shi2024exedec} and DeepCoder~\citep{balog2016deepcoder}.
\begin{figure*}[ht]
\small
\begin{alignat*}{2}
\mbox{Program } P\quad &:= &\quad& \T{Concat}(e_1, e_2, \hdots) \\
\mbox{Expression } e\quad &:= && s \logicalOR m \logicalOR o \logicalOR \T{ConstStr}(c) \\
\mbox{Compose } o\quad &:= && m_1(m_2) \logicalOR m(s) \\
\mbox{Substring } s\quad &:= && \T{SubStr}(k_1, k_2) \logicalOR 
\T{GetSpan}(r_1, i_1, b_1, r_2, i_2, b_2) \logicalOR \T{GetToken}(r, i) \\
&&& \logicalOR \T{GetUpto}(r) 
\logicalOR \T{GetFrom}(r)  \\
\mbox{Modification } m\quad &:= && \T{ToCase}(a) \logicalOR \T{Replace}(\delta_1, \delta_2) \logicalOR \T{Trim}() \logicalOR \T{GetFirst}(r, i) \logicalOR \T{GetAll}(r) \\
&&&\logicalOR  \T{Substitute}(r, i, c) \logicalOR  \T{SubstituteAll}(r, c) \logicalOR \T{Remove}(r, i) \logicalOR \T{RemoveAll}(r) \\
\mbox{Regex } r\quad &:= &&  \T{NUMBER} \logicalOR \T{WORD} \logicalOR \T{ALPHANUM} \logicalOR \T{ALL\_CAPS} \logicalOR \T{PROPER\_CASE} \logicalOR \T{LOWER} \logicalOR \T{DIGIT} \logicalOR \T{CHAR} \logicalOR \delta \\
\mbox{Case } a\quad &:= && \T{ALL\_CAPS} \logicalOR \T{PROPER\_CASE} \logicalOR \T{LOWER} \\
\mbox{Position } k\quad &:= && -100 \logicalOR -99 \logicalOR \hdots \logicalOR -1 \logicalOR 0 \logicalOR 1 \logicalOR 2 \logicalOR \hdots \logicalOR 100 \\
\mbox{Index } i\quad &:= && -5 \logicalOR -4 \logicalOR \hdots \logicalOR -1 \logicalOR 1 \logicalOR 2 \logicalOR \hdots \logicalOR 5 \\
\mbox{Boundary } b\quad &:= && \T{START} \logicalOR \T{END} \\
\mbox{Character } c\quad &:= && A\logicalOR\dots\logicalOR Z \logicalOR a\logicalOR\dots\logicalOR z \logicalOR 0\logicalOR\dots\logicalOR 9 \logicalOR \delta \\
\mbox{Delimiter } \delta\quad &:= && \texttt{\&,.?!@()[]\%\string{\string}/:;\$\# "'}
\end{alignat*}
    \caption{The DSL syntax for string manipulation tasks in the RobustFill domain.} 
\label{fig:robustfill_dsl}
\end{figure*}

\begin{figure*}[ht]
\small
\begin{alignat*}{2}
\mbox{Program } P\quad &:= &\quad& i_1; i_2; \hdots; a_1; a_2; \hdots \\
\mbox{Initialization } i\quad &:= && v \leftarrow \T{INPUT} \\
\mbox{Assignment } a\quad &:= && v \leftarrow f \logicalOR v \leftarrow h \\
\mbox{First-Order Operation } f\quad &:= && \T{Head}(l) \logicalOR \T{Last}(l) \logicalOR \T{Access}(n, l) \logicalOR \T{Minimum}(l) \logicalOR \T{Maximum}(l) \logicalOR \T{Sum}(l) \\
&&& \logicalOR \T{Take}(n, l) \logicalOR \T{Drop}(n, l) \logicalOR \T{Reverse}(l) \logicalOR \T{Sort}(l) \\
\mbox{Higher-Order Operation } h\quad &:= && \T{Map}(\lambda, l) \logicalOR \T{Filter}(\beta, l) \logicalOR \T{Count}(\beta, l) \logicalOR \T{ZipWith}(\Sigma, l, l) \logicalOR \T{Scanl1}(\Sigma, l) \\
\mbox{int $\rightarrow$ int Lambda } \lambda\quad &:= && (+1) \logicalOR (-1) \logicalOR (*2) \logicalOR (/2) \logicalOR (*(-1)) \logicalOR (*\!*\!2) \logicalOR (*3) \logicalOR (/3) \logicalOR (*4) \logicalOR (/4) \\
\mbox{int $\rightarrow$ bool Lambda } \beta\quad &:= && (>0) \logicalOR (<0) \logicalOR (\%2==0) \logicalOR (\%2==1) \\
\mbox{(int, int) $\rightarrow$ int Lambda } \Sigma\quad &:= && (+) \logicalOR (-) \logicalOR(*) \logicalOR (\min) \logicalOR (\max) \\
\mbox{Integer Variable } n\quad &:= && v \\
\mbox{List Variable } l\quad &:= && v \\
\mbox{Variable Name } v\quad &:= && x_1  \logicalOR x_2 \logicalOR \hdots
\end{alignat*}
    \caption{The DSL for integer and list manipulation tasks in the DeepCoder domain.} 
\label{fig:deepcoder_dsl}
\end{figure*}

\section{Experimental Results Using More LLMs}
\label{app:more_llms}
\begin{table*}[ht!]
\centering
\caption{Performance comparison of Llama3.1-70B-Instruct, Qwen-max, Claude 3.5 and GPT-4o on the PoT and RHDA methods in inductive code reasoning task. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidates.}
\scalebox{0.72}{
\begin{tabular}{clcccccccc}
\toprule
\multicolumn{1}{l}{}    &          & \multicolumn{4}{c}{Accuracy}  & \multicolumn{4}{c}{Task Accuracy} \\ \cmidrule(lr){3-6} \cmidrule(lr){7-10}
Model                       & Method & MiniARC & List Func & RobustFill & DeepCoder & MiniARC & List Func & RobustFill & DeepCoder \\ \midrule
\multirow{3}{*}{Llama3.1} & PoT    & 3.08    & 35.25     & 14.78      & 22.92     & 1.54    & 26.80     & 8.70       & 11.46     \\
                          & Sub-Hyp  & 3.33 & 26.45 & 13.04 & 18.06 & 3.08   & 20.40  & 4.35   & 6.25   \\
                          & T=2, N=1 & 3.85 & 32.35 & 20.87 & 11.46 & 3.85   & 26.40  & 13.04  & 7.29  \\ \midrule
\multirow{3}{*}{Qwen-max} & PoT      & 6.41 & 41.75 & 36.52 & 25.35 & 3.85   & 30.00  & 21.74  & 14.58  \\
                          & Sub-Hyp  & 5.90 & 46.25 & 26.09 & 17.36 & 3.08   & 36.40  & 8.70   & 5.21   \\
                          & T=2, N=1 & 6.41 & 46.60 & 33.91 & 24.64 & 3.08   & 41.60  & 13.04  & 10.42 \\ \midrule
\multirow{3}{*}{Claude-3.5} & PoT    & 11.79   & 51.30     & 30.43      & 25.69     & 8.46    & 39.20     & 27.14      & 13.54     \\
                        & Sub-Hyp  & 12.56 & 53.55 & 22.61 & 33.33 & 9.23   & 42.40  & 8.70   & 16.67  \\
                        & T=2, N=1 & \textbf{18.21} & \textbf{57.95} & 33.91 & 29.86 & \textbf{13.85}  & \textbf{48.40}  & 17.39  & 20.83  \\ \midrule
\multirow{3}{*}{GPT-4o} & PoT      & 10.90 & 44.90 & 37.39 & 30.90 & 8.46   & 33.60  & 26.09  & 19.79  \\
                        & Sub-Hyp  & 8.46  & 47.10 & 35.65 & 24.65 & 6.92   & 36.40  & 17.39  & 12.50  \\
                        & T=2, N=1 & 12.56 & 51.05 & \textbf{43.48} & \textbf{38.89} & 10.77  & 41.20  & \textbf{40.43}  & \textbf{23.96}  \\ \bottomrule
\end{tabular}
}
\label{tab:in_main_claude}
\end{table*}

\begin{table*}[ht!]
\centering
\caption{Performance comparison of Llama3.1-70B-Instruct, Qwen-max, Claude 3.5 and GPT-4o on the CoT and RHDA methods in deductive and abductive code reasoning tasks. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidates.}
\scalebox{0.9}{
\begin{tabular}{clcccc}
\toprule
\multicolumn{1}{l}{}        &          & \multicolumn{2}{c}{Deductive} & \multicolumn{2}{c}{Abductive} \\ \cmidrule(lr){3-4} \cmidrule(lr){5-6}
Model                       & Method   & CRUXEval    & LiveCodeBench   & CRUXEval    & LiveCodeBench   \\ \midrule
\multirow{3}{*}{Llama3.1} & CoT      & 40.25       & 7.84            & 53.12       & 38.24           \\
                          & Sub-Hyp  & 30.75       & 6.86            & 50.88       & 8.82            \\
                          & T=2, N=1 & 45.62       & 10.78           & 59.62       & 40.20   \\ \midrule
\multirow{3}{*}{Qwen-max} & CoT      & 81.12       & 86.27           & 75.12       & 58.82           \\
                          & Sub-Hyp  & 78.25       & 81.37           & 72.25       & 59.80           \\
                          & T=2, N=1 & 81.62       & \textbf{88.24}           & 79.38       & 66.67   \\ \midrule       
\multirow{3}{*}{Claude-3.5} & CoT      & 82.75       & 77.45           & 73.62       & 61.76           \\
                            & Sub-Hyp  & 77.75       & 65.69           & 74.75       & 53.92           \\
                            & T=2, N=1 & 86.88       & 80.39           & 83.38       & 61.76           \\ \midrule
\multirow{3}{*}{GPT-4o}     & CoT      & 89.12       & 83.14           & 71.00       & 66.67           \\
                            & Sub-Hyp  & 86.62       & 71.29           & 77.12       & 60.78           \\
                            & T=2, N=1 & \textbf{90.62}       & 84.16           & \textbf{83.75}       & \textbf{71.57}           \\ \bottomrule
\end{tabular}
}
\label{tab:de_ab_main_claude}
\end{table*}
We report the performance of Llama3.1-70B-Instruct, Qwen-max (qwen-max-2024-09-19), Claude 3.5 (claude-3-5-sonnet-20240620) using the RHDA method and compare them with GPT-4o (gpt-4o-2024-0806). The results for inductive code reasoning are shown in Table~\ref{tab:in_main_claude}. The experimental results indicate that GPT-4o performs better in solving DSL problems, while Claude 3.5 excels in General Propose Language (GPL) tasks. Compared to closed-source models, the open-source model Llama still exhibits relatively limited reasoning capabilities. However, in list manipulation tasks (List Function and Deepcoder), Llama demonstrates stronger programming abilities.
In Table~\ref{tab:de_ab_main_claude}, we report the performance of the models in deductive and abductive code reasoning together. The experimental results show that GPT-4o outperforms Claude 3.5 in terms of program understanding and execution capabilities. These results suggest that RHDA is a framework-agnostic general process that can achieve optimal performance through a single reflection, applicable to both Llama, Qwen, Claude and GPT series models.

\section{Benchmark Details}
\label{app:benchmark_details}
\begin{wraptable}{r}{0.47\textwidth}
\vspace{-7.7mm}
\footnotesize
\centering
\caption{The number of tasks per dataset, the numbers of seen examples per task, and unseen examples per task.}
\begin{tabular}{lccc}
\toprule
Dataset       & \# Tasks & \# Seen & \# Unseen \\ \midrule
List Function & 250      & 8       & 8         \\
MiniARC       & 130      & 3       & 3         \\
RobustFill    & 22       & 5       & 5         \\
Deepcoder     & 96       & 3       & 3         \\
CRUXEval      & 800      & 1       & 1         \\
LiveCodeBench & 102      & 1       & 1         \\ \bottomrule
\end{tabular}
\vspace{-2mm}
\label{tab:datasets}
\end{wraptable}
\paragraph{List Function.} We use the original dataset (Rule, 2020), which consists of a total of 250 tasks.
Due to the limited context lengths of LMs, we only use the first 16 examples from BIG-Bench (bench authors, 2023): 8 for seen examples and 8 for unseen examples. We manually examined the exemplars and found 8 examples are generally sufficient to describe the pattern.

\paragraph{MiniARC.} We use the data from ~\citep{qiu2024phenomenal}. Such tasks are typically difficult to describe in natural language at an
abstract level. Therefore, we did not consider them for our evaluations. As we only evaluate textonly models, we use textual representations of the original visual grids by mapping each cell to a corresponding integer.

\paragraph{RobustFill.} RobustFill is a string manipulation task where the model is expected to perform a combination of atomic operations, such as extracting a substring from position $k_1$ to $k_2$ using $SubString(k_1, k_1)$, to achieve generalization.
As an example, a program \texttt{ToCase(Lower, SubStr(1,3))}  converts full month names (January, April) to their abbreviations (jan, apr).

\paragraph{DeepCoder.} The DeepCoder task involves using DSL to perform operations on integer lists. In DeepCoder, each line represents a subroutine that performs atomic operations on previous variables and assigns the results to new variables. The result of the final line is the program's output. For example, program \texttt{a $\leftarrow$ [int] | b $\leftarrow$ FILTER(<0) a | c $\leftarrow$ MAP(*4) b | d $\leftarrow$ SORT c | e $\leftarrow$ REVERSE b} (where ``\texttt{|}'' denotes subroutine separator.) transforms the input \texttt{[-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11]} into the output \texttt{[-12, -20, -32, -36, -68]}

\section{RHDA Acting as an Agent in VirtualHome}
\label{app:virtualhome}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.97\textwidth]{fig/VirtualHome2.pdf}
    \caption{We illustrate how the RHDA framework can be extended to the VirtualHome environment to effectively accomplish the task of cleaning the bathroom.}
    \label{fig:virtualhome2}
\end{figure}
We utilized the RHDA framework to drive agent actions in the VirtualHome environment powered by LLMs. Figure~\ref{fig:virtualhome2} illustrates a task of cleaning the bathroom.

\begin{table}[ht!]
\caption{Execution Error Rate on VirtualHome}
\centering
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{1}{l}{native GPT-4o} & w/o Sub-Hyp & \multicolumn{1}{l}{w/o Amend} & \multicolumn{1}{l}{RHDA} \\ \midrule
\# Error Action $\downarrow$  & 92   & 84   & 84   & \textbf{52}   \\
Avg. Err per Step $\downarrow$& 0.84 & 0.35 & 0.20 & \textbf{0.16} \\
Avg. Err per Task $\downarrow$ & 2.09 & 1.83 & 1.75 & \textbf{1.08} \\ \bottomrule
\end{tabular}
\label{tab:VH_quan}
\end{table}

We also provided some quantitative metrics to validate the potential of RHDA as a agent in VirtualHome. Specifically, we selected a total of 52 tasks across two scenarios in VirtualHome and manually tested their execution error rates. The test results are shown in Table~\ref{tab:VH_quan}, which indicate that native GPT-4o struggles to handle simulated real-world scenarios effectively. The primary cause of failure lies in generating scripts that, while semantically similar to correct actions, are not executable within the environment (e.g., `open the tap' is invalid action, whereas `touch the tap' is valid action). By employing the RHDA method, which incorporates step-by-step solutions and effective feedback mechanisms, the error rate was significantly reduced.

\section{Examples Analyses}
\subsection{Effective Case Study}
\label{app:examples}

\input{table/qualitative3}

\input{table/qualitative4}

\input{table/qualitative5}

\input{table/qualitative6}

We validated the effectiveness of the proposed method using examples from various benchmarks. For instance, as shown in Table~\ref{tab:case_study_feed2}, the MiniARC task example with ID 37 highlights how the LLM, after receiving feedback, successfully reflects on its errors and submits a revised solution.

In Table~\ref{tab:case_study_hyp2}, hypothesis decomposition reveals that the output number is determined not only by its position at the middle of the input array but also by being the third character.

In Table~\ref{tab:case_study_hyp3}, compared to models without hypothesis decomposition, those utilizing this approach progressively analyze the function's behavior, ultimately achieving an abstract understanding of the program and making accurate assertions. In Table~\ref{tab:case_study_feed3}, for a complex recursive function, while the LLM accurately grasped the overall functionality of the function through hypothesis decomposition, it encountered difficulties during the detailed analysis of specific instances. Following the submission of a revised solution, the LLM reflected on its errors and successfully resolved the issue, addressing the collapse of the overall logical chain caused by a failure in a single step during multi-step reasoning.

\subsection{Failure Analyze}
\label{app:failure}

\input{table/failure1}

\input{table/failure2}

We analyze RHDA's performance in numerous failure cases and summarize the underlying causes of these failures. Our findings suggest that the primary reason can be attributed to the insufficient intrinsic capability of LLMs in code reasoning tasks. This limitation is specifically reflected in two aspects:  
\begin{itemize}
    \item \textbf{Sub-hypotheses fail to address the problem}: For tasks that are overly complex or abstract (e.g., cases shown in Table~\ref{tab:fail1}), even though hypothesis decomposition attempts to reduce task complexity, LLMs still struggle to handle them effectively.  
    \item \textbf{Amendments fail to correct sub-hypotheses}: While amendments leverage external feedback to help LLMs reflect on their mistakes, the models often remain confined to their existing thought framework, even after recognizing errors (e.g., cases shown in Table~\ref{tab:fail2}). This results in the correction failing to resolve the issue.  
\end{itemize}
These observations indicate that for tasks exceeding the intrinsic capabilities of LLMs, relying solely on reflective hypothesis decomposition and amendment may not be sufficient to improve the model's performance.

\section{Costs}
\label{app:costs}
\begin{table}[!ht]
\centering
\caption{Avg. API calls and Total Cost using GPT-4o.}
\scalebox{0.65}{
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{Avg. API Calls}                            & \multicolumn{4}{c}{Total Cost (cent)}                         \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
 &
  \multicolumn{1}{l}{List Func} &
  \multicolumn{1}{l}{MiniARC} &
  \multicolumn{1}{l}{RobustFill} &
  \multicolumn{1}{l}{Deepcoder} &
  \multicolumn{1}{l}{List Func} &
  \multicolumn{1}{l}{MiniARC} &
  \multicolumn{1}{l}{RobustFill} &
  \multicolumn{1}{l}{Deepcoder} \\ \midrule
IO                               & 8.0           & 4.0           & 5.0            & 3.0          & 10.2           & 4.6          & 2.0            & 3.3          \\
PoT                              & 1.0           & 1.0           & 1.0            & 1.0          & 5.0            & 3.7          & 0.6            & 1.2          \\
CoC                              & 1.0           & 1.0           & 1.0            & 1.0          & 11.0           & 9.0          & 1.1            & 1.4          \\
SC (N=3)                         & 3.0           & 24.0          & 15.0           & 9.0          & 5.3            & 3.7          & 0.6            & 1.2          \\
SR (T=2)                         & 1.4           & 1.9           & 1.5            & 1.6          & 4.6            & 3.3          & 0.5            & 1.1          \\
T=2, N=3                         & 5.4           & 5.9           & 5.5            & 5.6          & 8.6            & 4.0          & 3.1            & 4.7          \\ \midrule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Deductive} & \multicolumn{2}{c}{Abductive} & \multicolumn{2}{c}{Deductive} & \multicolumn{2}{c}{Abductive} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 &
  \multicolumn{1}{l}{CRUXEval} &
  \multicolumn{1}{l}{LiveCodeBench} &
  \multicolumn{1}{l}{CRUXEval} &
  \multicolumn{1}{l}{LiveCodeBench} &
  \multicolumn{1}{l}{CRUXEval} &
  \multicolumn{1}{l}{LiveCodeBench} &
  \multicolumn{1}{l}{CRUXEval} &
  \multicolumn{1}{l}{LiveCodeBench} \\ \midrule
Standard                         & 1.0           & 1.0           & 1.0            & 1.0          & 2.9            & 0.5          & 3.1            & 1.5          \\
CoT                              & 1.0           & 1.0           & 1.0            & 1.0          & 19.4           & 3.7          & 19.5           & 3.3          \\
SC (N=3)                         & 3.0           & 3.0           & 3.0            & 3.0          & 2.9            & 0.5          & 3.3            & 1.5          \\
SR (T=2)                         & 1.6           & 1.7           & 1.4            & 1.5          & 3.8            & 0.6          & 3.4            & 1.6          \\
CoC                              & 1.0           & 1.0           & 1.0            & 1.0          & 18.3           & 4.1          & 19.0           & 3.4          \\
T=2, N=1                         & 1.6           & 1.7           & 1.4            & 1.5          & 19.0           & 4.4          & 18.8           & 4.4          \\ \bottomrule
\end{tabular}}
\label{tab:cost}
\end{table}

In Table~\ref{tab:cost}, we present the average number of API calls and the total cost for each task. We used GPT-4o, with an input cost of \$0.0025/1K tokens and an output cost of \$0.01/1K tokens. The results indicate that our approach still demonstrates high cost-effectiveness for certain tasks.

\section{Trade off between number of iterations and performance gain}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.85\textwidth]{fig/iter_results1.pdf}
    \caption{In the inductive code reasoning tasks, as the number of iterations increased, the performance continued to improve.}
    \label{fig:iter1}
\end{figure}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/iter_results2.pdf}
    \caption{In the deductive code reasoning tasks, the performance slightly decreased as the number of iterations increased. Conversely, in the abductive code reasoning tasks, the performance consistently improved with an increasing number of iterations.}
    \label{fig:iter2}
\end{figure}
In this section, we investigate the impact of iteration count on the performance of three types of reasoning tasks, with experimental results illustrated in Figure~\ref{fig:iter1} and Figure~\ref{fig:iter2}. For inductive and abductive code reasoning tasks, performance consistently improved as the number of iterations increased. However, the rate of improvement diminished, with marginal gains becoming less significant at higher iteration counts. Conversely, for deductive code reasoning tasks, performance followed a rise-and-fall trend, initially improving but declining with excessive iterations. These findings suggest that while increasing the number of iterations can enhance performance for general code reasoning tasks, it is crucial to balance iterative gains against potential performance instability.
\section{Prompts}
\label{app:prompts}
\begin{table*}[ht!]
    \centering
    \caption{Prompts used in our study. \{\} refers to a placeholder.}
    \scalebox{0.9}{
    \begin{tabular}{ll}
        \toprule
        Type & Prompt \\
        \midrule
        \thead{Sub Hypothesis \\ Generation} & \texttt{\thead{Generate a rule that maps the following inputs to their \\ corresponding outputs step by steps. \textbf{\{Task description\}} \\\\ \textbf{\{Examples\}} \\\\ Please format your rule as follows: \\\\  \textbf{\{Rule format\}}}}\\
        \midrule
        \thead{Amendment \\ Submission} & \texttt{\thead{Your rule: \textbf{\{Rule\}} \\\\ This rule does not work for the following examples. \\\\\textbf{\{Feedback\}} \\\\ Please carefully reconsider each of your steps to ensure \\that the rules are correct. Systematically\\ generate new rules, step by step.\\ \textbf{\{Feedback description\}} Please \\ format your rule as follows: \\\\  \textbf{\{Rule format\}}}}\\
        \midrule
        \thead{Hypothesis \\ Translation} & \texttt{\thead{You are an expert Python programmer. Write a Python \\ function `fn` for the following rule. \textbf{\{Translation} \\ \textbf{Example description\}}\\\\ Rule: \textbf{\{Rule\}}}}\\
        \midrule
        \thead{Rule \\ Application} & \texttt{\thead{Generate an output corresponding to the given input based \\ on the rule. \textbf{\{Application Example description\}}\\\\ Rule: \textbf{\{Rule\}} \\\\ Input: \textbf{\{Test input\}} \\Output:}}\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:prompts}
\end{table*}

