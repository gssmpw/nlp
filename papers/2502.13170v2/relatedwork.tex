\section{Related Work}
\paragraph{Reasoning with LLMs.}
LLMs such as GPT~\citep{openai2023gpt4}, LLaMA~\citep{touvron2023llama}, and Claude~\citep{anthropic2024claude}, demonstrate impressive reasoning capabilities across various NLP tasks~\citep{zhang2024llm_reasong_survey}. However, due to the problems of direct reasoning with LLMs such as hallucinations~\citep{ji2023survey_hallucination}, researchers have proposed several methods to enhance the reasoning power of LLMs. For example, 
%Least-to-Most~
\citep{zhouleast2most, xue2025decompose} decompose complex tasks into sequential subproblems, while %AdaPlanner~
\citep{sun2024adaplanner_feedback} refine reasoning through environment feedback. Moreover, intermediate representations, such as graphs~\citep{jiang2024resprompt_graph}, planning domain definition languages (PDDL)~\citep{guan2023leveraging_PDDL}, and triples~\citep{wang2023boosting_CoK}, have been employed to enhance LLM's reasoning.
Most recently, OpenAI o1~\citep{openai2024o1} demonstrates strong reasoning capabilities and broad world knowledge. Upon further contemplation, it is capable of reasoning through complex tasks and addressing challenges that exceed those faced by previous scientific, coding, and mathematical models.

Simultaneously, domain-specific reasoning with LLMs has gained attention. \citep{kim2024language_reason_computer} enhance reasoning outputs in computer tasks through recursive critique. In a case study using Minecraft, \citep{wang2023describe_reason_mc} introduce a Describe, Interpret, Plan, and Select framework for open-world multitasking. In computer vision, \citep{gupta2023visual_reason_cv} employ Python-like modular programs to tackle complex tasks. Nonetheless, reasoning in code remains an area yet to be thoroughly explored.

\paragraph{Improvement with Reflection.} Reflective ability is regarded as a crucial metric for evaluating LLMs as agents. Reflection can be categorized into internal and external based on its feedback source~\citep{pan2024automatically}. Internal reflection relies feedback from the model's own knowledge and parameters~\citep{huang2022large}, while external feedback comes from various sources, including humans~\citep{wang2023shepherd}, other models~\citep{paul2024refiner}, external tools~\citep{gou2024critic, chen2024teaching}, or knowledge bases~\citep{yao2023react, asai2024selfrag}.
\citep{huang2024large} find that LLMs struggle to self-correct their responses without external feedback, and in some cases, their performance may even decline following self-correction. Our work focuses on leveraging external tools, such as compilers, to generate feedback and enhance the performance of LLMs.