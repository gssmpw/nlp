
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{url}
\usepackage{calligra}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{appendix}
\newcommand{\T}[1]{\texttt{#1}}
\newcommand{\logicalOR}{\; | \;}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    commentstyle=\itshape\color{codegreen}, % 注释为斜体绿色
    keywordstyle=\bfseries\color{magenta}, % 关键字为加粗的洋红色
    stringstyle=\color{codepurple}, % 字符串为紫色
    basicstyle=\ttfamily\tiny,
    breakatwhitespace=false, % 不在空白处自动换行
    breaklines=true, % 允许长行自动换行
    keepspaces=true, % 保持代码中的空格
    showspaces=false, % 不显示空格符
    showstringspaces=false, % 字符串中不显示空格符
    showtabs=false, % 不显示制表符
    tabsize=1, % 制表符宽度为 2 个空格
    % linewidth=0.48\textwidth, % 设置代码块的最大宽度为页面宽度的90%
}

\title{Unveiling the Magic of Code Reasoning \\ through Reflective Hypothesis \\ Decomposition and Amendment}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yuze Zhao$^1$, Tianyun Ji$^{1,*}$, Wenjun Feng$^{1,*}$, Zhenya Huang$^{1,2,\dag}$, Qi Liu$^{1,2}$, \\ {\bf Zhiding Liu$^{1}$, Yixiao Ma$^{1}$, Kai Zhang$^{1}$, Enhong Chen$^{1}$} \\
$^1$State Key Laboratory of Cognitive Intelligence,\\
University of Science and Technology of China\\
$^2$Institute of Artificial Intelligence, Hefei Comprehensive National Science Center \\
\texttt{yuzezhao@mail.ustc.edu.cn huangzhy@ustc.edu.cn}}
% \texttt{\{yuzezhao, jitianyun2002, fengwenjun, zhiding, iiishawn\}@mail.ustc.edu.cn}, \\
% \texttt{\{huangzhy, qiliuql, kkzhang08, cheneh\}@ustc.edu.cn}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\def\thefootnote{*}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}

\def\thefootnote{\dag}\footnotetext{Corresponding author}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, \textbf{code reasoning}, to provide a new perspective for the reasoning abilities of LLMs.
We summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways.
Additionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This \textbf{R}eflective \textbf{H}ypothesis \textbf{D}ecomposition and \textbf{A}mendment (\textbf{RHDA}) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\times$. Finally, we expand this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at \url{https://github.com/TnTWoW/code_reasoning}.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs), which are trained on billions of tokens, have demonstrated impressive reasoning abilities in complex tasks~\citep{brown2020language,wei2022chain,kojima2022large,openai2023gpt4}. 
However, it is evident that as potential fuzzy retrieval systems or parameterized knowledge compression systems~\citep{xie2021explanation}, LLMs perform better on System 1 tasks than on System 2 tasks~\citep{kahneman2011thinking, Bengio2019from, yao2023tree, weston20232attention, liu2023guiding}. Specifically, LLMs excel in intuitive memory retrieval tasks, but continue to face significant challenges with tasks requiring rational reasoning~\citep{kambhampati2024can}.

From the perspective of human cognitive psychology, \textbf{reasoning can be viewed as a process of memory retrieval}, in which people retrieve relevant information from memory and use it to make inferences~\citep{Kyllonen1990Reasoning,SU2002Working,hayes2014memory,feeney2014reasoning,Kyle2015Reasoning}. For example, \citet{haidt2001emotional} proposed that when individuals engage in moral reasoning, they typically draw on their prior knowledge from social and cultural contexts.
Similarly, studies involving animal lesions and human neuroimaging have confirmed that the hippocampus, which is primarily associated with memory, also plays a crucial role in reasoning abilities~\citep{zeithamova2012hippocampus}.
Therefore, memory and reasoning are interdependent, with considerable overlap between the two, rendering the distinction between them somewhat arbitrary~\citep{Heit2012Relations, liu2023learning}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/intro.pdf}
    \caption{Code reasoning is a category of tasks that incorporates logical reasoning into code, aiming to solve programming problems through logical reasoning. These tasks require a balance between background knowledge and thinking span, placing greater emphasis on the collaborative functioning of both System 1 and System 2 thinking.}
    \label{fig:intro}
    \vspace{-5mm}
\end{figure}
 
We believe that, similar to humans~\citep{strachan2024testing, liu2024socraticlm, lin2024learning}, LLMs do not exhibit a clear boundary between memory and reasoning~\citep{schaeffer2024emergent, razeghi2022impact}. However, tasks that lie at this intersection are often overlooked in research. Here, we propose a novel task to explore the capability boundaries of LLMs: \textbf{Code Reasoning}. Code reasoning encompasses a category of tasks that demonstrate logical reasoning through code and address problems in a systematic manner.
As illustrated in Figure~\ref{fig:intro}, we position some tasks along an axis that reflects 1) the degree of reliance on prior knowledge (Recall) and 2) the extent to which prior knowledge is applied to the current context (Reasoning). We position the code reasoning task between memory and reasoning.  On one hand, the highly structured nature of code requires the model to learn syntax from pre-training data, enabling it to recall relevant information during solving a problem. 
On the other hand, generating code solutions necessitates the model's understanding of the problem and context, involving reasoning to produce appropriate solutions. 
% Therefore, we describe code reasoning as ``free play within a constrained environment''.

In this paper, we introduce code reasoning, a task that formalizes reasoning steps into a programming language and offloads the computation process to the compiler. To explore different aspects of code reasoning, we summarize three meta-benchmarks based on existing forms of logical reasoning: inductive code reasoning, deductive code reasoning, and abductive code reasoning.

Inductive code reasoning involves deriving broad generalizations from a series of observations, demonstrating the ability to infer rules from examples and generate programs to meet input-output mapping. Deductive code reasoning starts from premises and derives valid conclusions, focusing on the model's capacity to understand a program's intermediate states and reasoning step by step. Abductive code reasoning seeks the simplest and most likely explanation based on a set of observations, highlighting the model's ability to abstractly understand a function's purpose. 

We concretize these three meta-benchmarks into eight specific benchmarks. Based on these eight benchmarks, we evaluate the performance of existing models in code reasoning. Due to data sparsity, we find that current state-of-the-art LLMs still struggle to achieve satisfactory results in solving such problems. To enhance the reasoning process, we implement a \textbf{R}eflective \textbf{H}ypothesis \textbf{D}ecomposition and \textbf{A}mendment (\textbf{RHDA}) pipeline.
This pipeline is iterative, encompassing hypothesis decomposition, execution verification, and amendment submission.
Specifically, we first guide the LLM to formulate initial hypotheses based on complex observations and decompose these into sub-hypotheses. These sub-hypotheses are then compiled into executable functions through a translator, enabling direct application to the observations, followed by validation using external tools. Subsequently, based on the execution results and observations, the LLM submits amendments to reflect on and refine the issues within the sub-hypotheses.

Our experimental results indicate that RHDA methods effectively mitigate reasoning failures caused by data sparsity. With the same or even lower overhead, this method achieved performance improvements of up to three times compared to baseline methods. Finally, we extend this pipeline to complex, simulated real-world household tasks VirtualHome~\citep{puig2018virtualhome, puig2020watchandhelp}, guiding the LLM to complete a series of intricate operations.

\section{Meta-Benchmark}
We describe the general process of code reasoning as the transformation from Input $\mathcal{I}$ and Program $\mathcal{P}$ to Output $\mathcal{O}$, represented as $\mathcal{I}\stackrel{\mathcal{P}}{\longrightarrow}\mathcal{O}$. Inductive code reasoning is concretized as the Programming by Example (PBE) task. In this task, a neural program synthesis model $\mathcal{M}$ searches the execution space to find a program that best satisfies all given input-output specifications. We donate this meta-benchmark as $\mathcal{M}(\mathcal{I}, \mathcal{O})\rightarrow\mathcal{\widetilde{P}}$.
Deductive code reasoning is exemplified in tasks that simulate the program execution process. In this task, a neural simulation compiler model $\mathcal{M}$ tracks the program's execution and records intermediate states, gradually deriving the final valid output. We denote this meta-benchmark as $\mathcal{M}(\mathcal{I}, \mathcal{P})\rightarrow\mathcal{\widetilde{O}}$.
Abductive code reasoning is concretized as input prediction tasks. This task requires the neural understanding model $\mathcal{M}$ to form an abstract-level understanding of function's behavior and perform abductive inference based on the given program and output. We represent this meta-benchmark as $\mathcal{M}(\mathcal{O}, \mathcal{P})\rightarrow\mathcal{\widetilde{I}}$.
The details of the benchmarks are provided in the Appendix~\ref{app:benchmark_details}.

\subsection{Inductive Code Reasoning}
Inductive code reasoning can be represented as $\mathcal{M}(\mathcal{I}, \mathcal{O})\rightarrow\mathcal{\widetilde{P}}$ and is concretized as a PBE task~\citep{qiu2024phenomenal, shi2024exedec}. PBE is a program synthesis task designed to help end-users, particularly non-programmers, create scripts to automate repetitive tasks~\citep{gulwani2016programming}. Based on input-output specifications, PBE systems can synthesize a program in either a general-purpose language (GPL) or a domain-specific language (DSL). 
Inductive code reasoning encompasses four challenging PBE tasks, two of which are GPL tasks: List Function~\citep{rule2020child} and MiniARC~\citep{kim2022playgrounds}, while the other two are DSL tasks: RobustFill~\citep{devlin2017robustfill} and DeepCoder~\citep{balog2016deepcoder}.
GPL tasks are relatively complex, allowing the model to solve problems in a more flexible manner. In contrast, DSL tasks require the model to quickly learn the syntax of DSL through few-shot learning and address relatively simpler problems.

\paragraph{List Function.} The List Function task was originally designed to investigate how humans learn the concept of computable functions that map lists to lists. Given input and output specifications in the form of lists, the model generates GPL rules that conform to these specifications. For example, with an input specification of \texttt{[2, 4, 8, 10]} and an output specification of \texttt{[3, 5, 9, 11]}, we expect the resulting rule to be \texttt{lambda x : x + 1}\footnote{For conciseness while maintaining generality, we will use lambda expressions to represent a program.}.

\paragraph{MiniARC.} MiniARC is a compressed 5x5 version of the Abstraction and Reasoning Corpus~\citep{chollet2019measure, moskvichev2023concept}, designed to assess imaginative and reasoning abilities.
MiniARC balances the length of the input-output pairs with the difficulty of the problems. The specifications are 5x5 2D grids, where the numbers represent blocks of specific colors. The model must find valid problem-solving paths (such as color swapping, row flipping) to achieve the transformation from input to output.

\paragraph{RobustFill.} RobustFill is a string manipulation task where the model is expected to perform a combination of atomic operations, such as extracting a substring from position $k_1$ to $k_2$ using $SubString(k_1, k_1)$, to achieve generalization.
As an example, a program \texttt{ToCase(Lower, SubStr(1,3))} converts full month names (January, April) to their abbreviations (jan, apr).

\paragraph{DeepCoder.} The DeepCoder task involves using DSL to perform operations on integer lists. In DeepCoder, each line represents a subroutine that performs atomic operations on previous variables and assigns the results to new variables. The result of the final line is the program's output. For example, program \texttt{a $\leftarrow$ [int] | b $\leftarrow$ FILTER(<0) a | c $\leftarrow$ MAP(*4) b | d $\leftarrow$ SORT c | e $\leftarrow$ REVERSE b} (where ``\texttt{|}'' denotes subroutine separator.) transforms the input \texttt{[-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11]} into the output \texttt{[-12, -20, -32, -36, -68]}. We provide detailed RobustFill and Deepcoder DSLs in Appendix~\ref{app:dsl}. 
\subsection{Deductive Code Reasoning}
Deductive code reasoning refers to the process of deriving a sound inference $\mathcal{O}$ by reasoning from the given premise $\mathcal{I}$, assuming the validity of the argument $\mathcal{P}$. Deductive code reasoning can be instantiated as an output prediction task~\citep{gu2024cruxeval}. Based on the given premise, the output prediction requires the LLM to simulate a compiler~\citep{kim2024llmcompiler}, executing step by step until it arrives at a valid conclusion.
For example, given a program \texttt{P = lambda text, value: ''.join(list(text) + [value])} and inputs \texttt{text = `bcksrut', b = `q'}, the output prediction from LLM should be \texttt{`bcksrutq'}.

\subsection{Abductive Code Reasoning}
Starting from existing facts $\mathcal{P}$ and $\mathcal{O}$, deriving the most reasonable and optimal explanation $\mathcal{I}$ is referred to as abductive code reasoning. This meta-benchmark can be framed as an input prediction task. Given the provided facts, the input prediction requires the LLM to backtrack through the program's execution process to recover the potential inputs. In cases where multiple possible inputs exist, the model should apply Occam's Razor and return the simplest input. For example, given a program \texttt{P = lambda nums: nums + [nums[i \% 2] for i in range(len(nums))]} and outputs \texttt{[-1, 0, 0, 1, 1, -1, 0, -1, 0, -1]}, the input prediction from LLM should be \texttt{[-1, 0, 0, 1, 1]}.

Deductive code and abductive code reasoning can be regarded as opposite processes; therefore, we selected two identical and representative datasets, CRUXEval~\citep{gu2024cruxeval} and LiveCodeBench~\citep{jain2024livecodebench}, as benchmarks to validate these two capabilities.

\paragraph{CRUXEval.} CRUXEval is a benchmark designed to evaluate code understanding and execution. Many models that achieve high scores on HumanEval~\citep{chen2021evaluating} do not show the same level of improvement on the CRUXEval benchmark. This benchmark includes 800 functions along with their corresponding inputs and outputs.

\paragraph{LiveCodeBench.} LiveCodeBench is a dynamically updated benchmark sourced from competition platforms. Each problem is timestamped, and we selected data from October 2023 (later than GPT-4o training) to March 2024 (the most recent), ensuring there is no data leakage and thereby guaranteeing the model's generalization performance.

\section{Code Reasoning with Hypothesis Decomposition and Amendment}
We aim to generate a reliable reasoning process for problem-solving by establishing a problem-solving pathway $f: \mathcal{X} \rightarrow \mathcal{Y}$. For a given task $\tau$ and the seen specifications/observations $\mathcal{X}^{s}_\tau$, the pathway $f$, should lead to a seen valid solution $\mathcal{Y}^{s}_\tau$ through a chain of reasoning.
We expect this pathway $f$ to have sufficient generalization capabilities to handle unseen specifications/observations $\mathcal{X}^{u}_\tau$.
To this aim, we employ a process involving hypothesis decomposition, execution verification, and amendment submission to iteratively explore and refine the reasoning pathway.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/method.pdf}
    \caption{An overview of pipeline to solve code reasoning task. We decompose the hypothesis and generate executable functions step by step. After comparing the results with the seen observations and receiving feedback, we propose amendments, reflect on potential errors at each step, and generate revised hypotheses. This process is repeated until a valid problem-solving pathway is discovered. For concise expression, we show partial code snippets.}
    \label{fig:method}
    \vspace{-0.2cm}
\end{figure}
We first establish an initial hypothesis $h^0 \in \Sigma^*$ based on observations $x^{s}_\tau \in \mathcal{X}^{s}_\tau$, where $\Sigma^*$ is the closure form of LLM's vocabulary. This initial hypothesis $h^0$ serves as a preliminary solution pathway to the problem. Given the complexity of many problems, we decompose the hypothesis $h^0$ into simpler sub-hypotheses $h^0 \iff \{h^0_{s_0}, h^0_{s_1}, h^0_{s_2},...\}$. A translator function $g:\Sigma^* \rightarrow \Sigma_\mathcal{E}^*$, which maps the hypothesis space $\Sigma^*$ into an executable function space $\Sigma_\mathcal{E}^*$, is then used to `compiled' the sub-hypotheses $h^0$ into an executable function $e^0$. This executable function is directly applicable to the observations $x^{s}_\tau$, allowing for the derivation of conclusions $\widetilde{y}^{s}_\tau$, that is: 
\begin{equation}
    \widetilde{y}^{s}_\tau = g(h^0)({x}^{s}_\tau).
\end{equation}
Feedback $\mathcal{F}(y^{s}_\tau, \widetilde{y}^{s}_\tau)$ is used to evaluate the conclusions drawn from the current hypothesis, guiding the LLM to reflect on its sub-hypotheses. Through this iterative process of reflection, the model generates a new hypothesis $h^1$ for the next iteration. Finally, the problem-solving pathway $f$ is applied to unseen observations $\mathcal{X}^{u}_\tau$, and the model's generalization performance is assessed by measuring its accuracy:
\begin{equation}
    acc_\tau = \frac{1}{|\mathcal{X}^{u}_\tau|}\sum_{x^{u}_\tau \in \mathcal{X}^{u}_\tau}{\1\left[f(x^{u}_\tau)=y^{u}_\tau\right]}.
\end{equation}
The preceding section presents a unified framework for the hypothesis decomposition and amendment method. However, the implementation specifics differ across various tasks. In the following sections, we will introduce these task-specific variations in detail.

\paragraph{Hypothesis Decomposition.} We recognize that complex logical reasoning problems are difficult to encapsulate in a single reasonable hypothesis, which can adversely affect the performance of LLMs. Therefore, we require the LLM to decompose its hypotheses. Specifically, given an observation $x^{s}_\tau$, the LLM gradually presents corresponding hypotheses step by step.
For inductive code reasoning, $h_0$ represents the step-by-step hypothesis of the input-to-output transformation rules. For deductive and abductive code reasoning, $h_0$ refers to the step-by-step hypothesis regarding the functionality of the program.

\paragraph{Execution Verification.} After obtaining the hypothesis, we need to apply it to the observations. However, hypotheses are often not directly usable, so we need to convert the decomposed hypothesis into an executable function $e$ through a translator $g$. For inductive code reasoning, the executable function is a program; for deductive and abductive code reasoning, the executable function is the predicted output and input, respectively. These three types of task are then sent to a compiler to obtain the actual execution results, and the feedback generated by the compiler is provided to the LLM to help it further refine and adjust the sub-hypotheses.

\paragraph{Amendment Submission.} During the amendment submission stage, there are no significant differences in handling the three tasks. The LLM receives validation feedback from the tools and generates amendments based on this feedback, reflecting on possible issues in the previous hypotheses. The reflection process involves revising each sub-hypothesis individually, forming an updated hypothesis $h_1 \iff \{h^1_{s_0}, h^1_{s_1}, h^1_{s_2},...\}$. This process ensures that each sub-hypothesis is adjusted to better align with the observations and validation results, gradually improving the reasoning pathway's coherence and accuracy.

\section{Experiments}
\paragraph{Experimental Setup.} We utilize the latest and most advanced model, gpt-4o-2024-08-06, as the backbone LLM for all our experiments. We report the results using Llama-3.1-70B-Instruct, Qwen-max (qwen-max-2024-09-19)~\citep{bai2023qwen}, Claude 3.5 (claude-3-5-sonnet-20240620) in Appendix~\ref{app:more_llms}. Following the methodology of \citet{qiu2024phenomenal}, we set the temperature to 0.7. We report results using several methods: input-output (IO) prompting, standard prompting, Chain of Thought (CoT) \citep{wei2023chainofthought}, Program of Thought (PoT) \citep{chen2023programthought}, Chain of Code (CoC) \citep{li2024chaincode}, Self-Consistency (SC) \citep{wang2023selfconsistency} and Self-Refine (SR)~\citep{madaan2024self}, all implemented with 2-shot learning.\footnote{Not all methods are suitable for these three meta-benchmarks, thus we selected the most appropriate methods for each benchmark.} For our proposed process, we employ 0-shot prompts, allowing the LLM to explore problem-solving pathways in a more flexible manner. We provide detailed prompt templates in Appendix~\ref{app:prompts}.
\subsection{Inductive Code Reasoning}
\begin{table*}[t!]
\centering
\caption{RHDA method on inductive code reasoning task. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidates.}
\scalebox{0.75}{
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Method}} & \multicolumn{4}{c}{\textbf{Accuracy}} & \multicolumn{4}{c}{\textbf{Task Accuracy}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
\multicolumn{1}{c}{} & List Func & MiniARC & RobustFill & Deepcoder & List Func & MiniARC & RobustFill & Deepcoder \\ \midrule
IO & \textbf{64.85} & \textbf{28.21} & \textbf{61.74} & 23.78 & 38.00 & 13.08 & 21.74 & 10.42 \\
PoT & 44.90 & 10.90 & 37.39 & 30.90 & 33.60 & 8.46 & 21.74 & 19.79 \\
CoC & 42.45 & 10.90 & 31.30 & 26.39 & 34.40 & 4.62 & 13.04 & 13.54 \\
SC \scriptsize{(N=3)} & 52.95 & 12.31 & 46.09 & 37.85 & 41.20 & 9.23 & 26.09 & 26.04 \\ 
SR \scriptsize{(T=2)} & 51.10 & 10.26 & 41.74 & 36.81 & 41.60 & 8.46 & 21.74 & 25.00 \\ \midrule
w/o Sub-Hyp & 42.45 & 7.95 & 40.87 & 18.05 & 33.20 & 4.62 & 21.74 & 9.37 \\ w/o Amend & 47.10 & 8.46 & 35.65 & 30.21 & 36.40 & 6.92 & 17.39 & 19.79 \\ \midrule
T=2, N=1 & 51.05 & 12.56 & 43.48 & 38.89 & 41.20 & 10.77 & 30.43 & 23.96 \\
T=3, N=1 & 53.20 &  14.10 &  47.83 & 38.19  & 44.00 &  11.54 & 30.43  & 26.04  \\
T=2, N=3 & 58.35 & 19.74  & 54.78  & \textbf{43.06}  & \textbf{48.80} & \textbf{13.85}  &  \textbf{34.78} & \textbf{29.17}  \\
\bottomrule
\end{tabular}
}
\label{tab:in_main}
\end{table*}
For inductive code reasoning, we establish four baseline methods. The Input-Output (IO) prompting requires the LLM to predict outputs based on all seen observations and an unseen input. The Program of Thought (PoT) method generates and executes programs to derive outputs. The CoC method prompts the LLM to utilize pseudocode for reasoning in output prediction. The SC method builds upon PoT by sampling multiple programs and selecting the one that demonstrates optimal performance on seen observations.
Furthermore, since each example may contain multiple unseen observations, we adopt the approach from~\citep{qiu2024phenomenal} to define task accuracy externally. An example is deemed passed only when all unseen observations within it pass; thus, the proportion of passed examples reflects the task accuracy. The experimental results are presented in Table~\ref{tab:in_main}. 

The results demonstrate that the RHDA method achieves optimal performance across four benchmarks, with task accuracy exceeding that of the second-best methods by 18.45\%, 5.89\%, 33.31\%, and 12.02\%, respectively. However, we observe that RHDA appears to underperform compared to IO prompting. This is because the IO prompt does not generate a hypothesis that satisfies all observations but instead predicts the output for a single input. A successful prediction for a single instance does not generate a hypothesis that satisfies all observations, resulting in a high prediction accuracy but a relatively low task accuracy.

\paragraph{Ablation Study.}
We introduce two variants to separately validate the effectiveness of hypothesis decomposition and amendment submission. The first variant does not require the LLM to decompose hypotheses, referred to as w/o Sub-Hyp. The second variant, termed w/o Amend, indicates that the model no longer modifies its hypotheses through reflection.
The experimental results presented in Table~\ref{tab:in_main} show that the performance of these two variants declined by 25.39\% to 67.88\% and 19.28\% to 57.14\%, respectively. This finding suggests that the introduction of sub-hypotheses is a critical step, as it simplifies complex problems, reducing the workload for the subsequent translator $g$ while also enabling individual amendments to each sub-hypothesis. Nonetheless, the reflection process is equally important. Our results align with previous research~\citep{zhao2024repair, olausson2024repair, peng2023check} indicating that rational reflection can significantly enhance performance.
\subsection{Deductive Code Reasoning}
\begin{wraptable}{r}{0.47\textwidth}
\centering
\footnotesize
\vspace{-20pt}
\caption{RHDA method on deductive code reasoning task. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidates.}
\begin{tabular}{lcc}
\toprule
  & CRUXEval & LiveCodeBench \\ \midrule
Standard    & 68.75  & 41.18  \\
CoT  & 89.12   & 83.14   \\
SC \scriptsize{(N=3)}   & 71.12    & 36.27   \\
SR \scriptsize{(T=2)}   & 80.38    & 63.73   \\
CoC  & 85.62    & 81.37  \\ \midrule
w/o Amend   & 86.62 & 71.29 \\
T=2, N=1 & \textbf{90.62}  & \textbf{84.16} \\ \bottomrule
\end{tabular}
\label{tab:de_main}
\end{wraptable}
For deductive code reasoning, we select standard prompting, CoT, SC, SR and CoC as benchmark methods. The experimental results are presented in Table~\ref{tab:de_main}. These results indicate that the CoT and CoC methods significantly enhanced the accuracy of reasoning outcomes by guiding the model to think step-by-step about function capabilities. Our proposed method advances this further, achieving optimal performance with a single round of amendments, resulting in an improvement of up to 104.37\% compared with baseline method. A horizontal comparison of the two datasets revealed that, due to the absence of LiveCodeBench data in internet corpora, the performance with standard prompts showed a marked advantage, with the SC method amplifying this gap. Notably, the combination of CoT, CoC, and hypothesis decomposition and amendment enabled the LLM to exhibit a substantial degree of reasoning and generalization ability, nearly solving all presented problems.

\subsection{Abductive Code Reasoning}
\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.5\textwidth]{fig/abductive_results.pdf}
    \caption{RHDA method on abductive code reasoning task. $T$ refers to the maximum number of iterations. $N$ refers to the number of candidates.}
    \vspace{-6pt}
    \label{fig:ab_result}
\end{wrapfigure}
For abductive code reasoning, we employ the same baseline methods as those used for deductive reasoning. The experimental results are presented in Figure~\ref{fig:ab_result}. Compared to deductive reasoning, abductive reasoning involves a reverse thinking process, which presents significant challenges. The LLM cannot derive the program's intermediate states through deduction and must first establish an abstract-level understanding of the function's behavior before proceeding with abduction.
On the CRUXEval dataset, the performance decline for abductive reasoning ranged from 8.20\% to 25.52\%. However, the hypothesis decomposition and amendment approach demonstrate robustness, as the change in reasoning modes resulted in only minimal performance degradation (8.20\%) while still outperforming baseline methods by 10.02\% to 31.89\% on the CRUXEval dataset and 7.35\% to 40.39\% on the LiveCodeBench dataset. A horizontal comparison of the two datasets revealed a trend similar to that observed in deductive reasoning, with an overall performance decline on the LiveCodeBench dataset, suggesting a complex relationship between reasoning and recall.
\subsection{Qualitative Analyze}
We select some cases to conduct an in-depth exploration of the quality of RHDA.
\input{table/qualitative1}
\paragraph{Hypothesis Decomposition Reduces Task Complexity.} In Table~\ref{tab:case_study_hyp1}, we present a qualitative analysis of the MiniARC benchmark, comparing the effects of using hypothesis decomposition versus not using it. As illustrated in the examples above, without hypothesis decomposition, the descriptions require a substantial amount of language, which ultimately leads to severe hallucinations in the program and results in failure. In contrast, the examples below, which utilize hypothesis decomposition, present clear and executable objectives, enabling the translator to easily write executable functions and achieve successful solutions as instructed.
\input{table/qualitative2}
\paragraph{Amendments Guide LLM Towards Correct Pathway.} We present a qualitative analysis of the use of amendments in the List Function benchmark in Table~\ref{tab:case_study_feed1)}. The upper section displays the initialization of the hypothesis, where the LLM generates a potential guess based on the observations and translates it into an executable program. After offloading the execution to the tool (e.g., Python executor) and receiving feedback, amendments are proposed to modify the initial hypothesis. Following this reflection, the LLM re-optimizes the rules, ultimately yielding the correct execution results. More qualitative analyse examples please refer to Appendix~\ref{app:examples}.

\paragraph{Failure Analyse.} We also conduct an in-depth analysis of the reasons behind process failures in RHDA, detailed in Appendix~\ref{app:failure}. Our findings reveal that the primary limitation arises from the restricted intrinsic reasoning capabilities of LLMs, which continue to face challenges in understanding and addressing complex problems. These limitations are primarily reflected in two aspects:
\begin{itemize}
    \item Difficulty in Generating Accurate Sub-Hypotheses: The generation of sub-hypotheses during the reasoning process often proves inaccurate, leading to subsequent breakdowns in reasoning chains.
    \item Sensitivity to Initial Hypotheses: The model exhibits a pronounced dependency on its initial hypotheses. Even when feedback is provided through amendment submissions, the model struggles to break free from its original thought framework, constraining its reasoning capabilities.
\end{itemize}

\subsection{RHDA is a Flexible and Scalable Problem-solving Pathway}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{fig/VirtualHome1.pdf}
    \caption{We demonstrate how RHDA can be extended to the VirtualHome framework to successfully complete the task of storing the pie in fridge.}
    \label{fig:virtualhome1}
\end{figure}

We consider extending the RHDA pipeline to more complex scenarios. To this end, we select VirtualHome~\citep{puig2018virtualhome, puig2020watchandhelp}, a sophisticated multi-agent platform for simulating household activities, as our new exploration subject. VirtualHome comprises a set of predefined atomic actions and objects that can be combined into high-level instructions. For example, `〈char0〉 [walk] 〈salmon〉' describes character 0 walking to the salmon. Given a specific scenario, the LLM is tasked with completing concrete housework using a series of high-level instructions. As depicted in Figure~\ref{fig:virtualhome1}, and guided by the RHDA process, we demonstrate how the LLM successfully accomplishes the task of storing pie in the fridge through the methods of hypothesis decomposition, execution verification (offloading to VirtualHome engine), and reflection. we show another example in Appneidx~\ref{app:virtualhome}.

\section{Limitation and Discussions}
\paragraph{Benchmark Selection.} This paper represents the first systematic exploration of the code reasoning task, focusing on the analysis of three forms of logical reasoning: inductive, deductive, and abductive. Due to time and cognitive constraints, we were unable to collect all benchmarks for testing. Our aim is to stimulate in-depth discussion on this topic and inspire meaningful follow-up research. While several excellent studies utilize code to address logical reasoning tasks~\citep{zelikman2023parsel, hu2023code, srivastava2024functional, liu2024codemind}, we did not include them here due to their differing starting points from this paper.
\paragraph{Hyperparameters.} The goal of this paper is to explore the potential of LLMs in code reasoning, rather than solely improving the performance of a specific code reasoning task. The RHDA framework serves as a preliminary exploration process; therefore, we didn't fully optimized the prompt templates or specific hyperparameters (such as temperature, $T$, and $N$) utilized. In the inductive code reasoning task, we examined a broader range of hyperparameter settings to illustrate that exploring multiple pathways aids in more effectively solving problems.
\paragraph{Task Assessment.} We propose a novel code reasoning task, and experimental results indicate that current state-of-the-art LLMs exhibit limitations in tackling this task. In the future, we aim to further explore this challenging area and investigate the boundaries of human capabilities in similar tasks.
\section{Related Work}
\paragraph{Reasoning with LLMs.}
LLMs such as GPT~\citep{openai2023gpt4}, LLaMA~\citep{touvron2023llama}, and Claude~\citep{anthropic2024claude}, demonstrate impressive reasoning capabilities across various NLP tasks~\citep{zhang2024llm_reasong_survey}. However, due to the problems of direct reasoning with LLMs such as hallucinations~\citep{ji2023survey_hallucination}, researchers have proposed several methods to enhance the reasoning power of LLMs. For example, 
%Least-to-Most~
\citep{zhouleast2most, xue2025decompose} decompose complex tasks into sequential subproblems, while %AdaPlanner~
\citep{sun2024adaplanner_feedback} refine reasoning through environment feedback. Moreover, intermediate representations, such as graphs~\citep{jiang2024resprompt_graph}, planning domain definition languages (PDDL)~\citep{guan2023leveraging_PDDL}, and triples~\citep{wang2023boosting_CoK}, have been employed to enhance LLM's reasoning.
Most recently, OpenAI o1~\citep{openai2024o1} demonstrates strong reasoning capabilities and broad world knowledge. Upon further contemplation, it is capable of reasoning through complex tasks and addressing challenges that exceed those faced by previous scientific, coding, and mathematical models.

Simultaneously, domain-specific reasoning with LLMs has gained attention. \citep{kim2024language_reason_computer} enhance reasoning outputs in computer tasks through recursive critique. In a case study using Minecraft, \citep{wang2023describe_reason_mc} introduce a Describe, Interpret, Plan, and Select framework for open-world multitasking. In computer vision, \citep{gupta2023visual_reason_cv} employ Python-like modular programs to tackle complex tasks. Nonetheless, reasoning in code remains an area yet to be thoroughly explored.

\paragraph{Improvement with Reflection.} Reflective ability is regarded as a crucial metric for evaluating LLMs as agents. Reflection can be categorized into internal and external based on its feedback source~\citep{pan2024automatically}. Internal reflection relies feedback from the model's own knowledge and parameters~\citep{huang2022large}, while external feedback comes from various sources, including humans~\citep{wang2023shepherd}, other models~\citep{paul2024refiner}, external tools~\citep{gou2024critic, chen2024teaching}, or knowledge bases~\citep{yao2023react, asai2024selfrag}.
\citep{huang2024large} find that LLMs struggle to self-correct their responses without external feedback, and in some cases, their performance may even decline following self-correction. Our work focuses on leveraging external tools, such as compilers, to generate feedback and enhance the performance of LLMs.

\section{Conclusion}
In this paper, we emphasized that the reasoning capabilities of LLMs still depend on recalling prior knowledge and highlighted that code reasoning has not been sufficiently explored as a novel perspective for examining the boundaries of LLM capabilities. Based on this consideration, we designed three meta-benchmarks—inductive code reasoning, deductive code reasoning, and abductive code reasoning—drawing on established forms of logical reasoning, and instantiated these benchmarks into eight specific tasks. Experimental results indicated that these benchmarks present significant challenges for current state-of-the-art LLMs.
To initially explore code reasoning tasks, we proposed a method involving \textbf{R}eflective \textbf{H}ypothesis \textbf{D}ecomposition and \textbf{A}mendment (\textbf{RHDA}). This method was iterative: LLMs need to generate decomposed initial hypotheses based on observations and employ a translator to interpret these into executable functions that can be directly applied to the observations. After obtaining the executable functions, we performed execution verification and submit amendments, allowing for reflection and refinement of the sub-hypotheses. Experimental results demonstrated that this approach, which integrated the principles of divide-and-conquer and reflection, can flexibly solve complex code reasoning problems, achieving performance improvements of 2 to 3 times compared to baseline methods. Finally, we extended this process to simulate household tasks in real-world complex scenarios to validate its scalability and transferability.
\section{Acknowledgment}
This research was partially supported by the Key Technologies R\&D Program of Anhui Province (No.202423k09020039), the National Natural Science Foundation of China (Grants No.62477044, 62406303), Anhui Provincial Natural Science Foundation (No. 2308085QF229), the Fundamental Research Funds for the Central Universities (No.WK2150110038, WK2150110034).
\section{Reproducibility Statement}
Our code, datasets and experimental results are available at \url{https://github.com/TnTWoW/code_reasoning}. Additionally, Appendix~\ref{app:prompts} contains details about pipeline and prompts used in method.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\input{appendix}

\end{document}
