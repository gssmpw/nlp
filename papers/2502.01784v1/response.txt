\section{Related Work}
\label{sec:related_work}
\subsection{Imitation Learning}
Due to the simplicity in formulation and demonstrated potential in robotics, imitation learning has garnered significant attention in recent years and has seen development across various domains. For example, the works in **Schaal, "Learning from Demonstration by Conditioning on Demonstrated Tasks"**__**Hester et al., "Deep Q-Learning from Demonstrations"** focus on handling the multimodal distribution in the demonstration data. A recent survey of imitation learning is presented in **Kober et al., "Reinforcement Learning in Robotics: A Review"**.


Imitation learning typically requires substantial action data to achieve high-performance policies. However, videos, rich in information and more economical to collect, offer an alternative. Recent advancements in video generation models, as shown in **Mathieu et al., "Deep Multi-View Stereo"**, confirm the potential of using videos to predict robotic tasks. Our paper emphasizes the integration of video generation models into policies, showing that they can achieve robust performance with less reliance on extensive, high-quality action data.

\subsection{Video Generation Model in Robotics}
Video generation models align well with temporal information, as it requires predicting temporal consistency between frames. For example, the works in **Butepage et al., "Deep Imitation Learning for Complex Tasks in Robotics"**__**Peng et al., "DeepLoco: Dynamic Locomotion Skills for Autonomous Characters" propose different video generation models for robot learning. Traditionally, video generation models have required significant computational time, posing challenges for real-time robotics applications. The works mentioned above use an  ``open-loop" strategy with long replanning intervals or lack policy implementation. In contrast, through extensive experiments, we show that VILP not only produces high-quality videos but also achieves time efficiency, enabling real-time receding horizon planning.