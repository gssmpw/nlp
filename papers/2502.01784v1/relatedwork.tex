\section{Related Work}
\label{sec:related_work}
\subsection{Imitation Learning}
Due to the simplicity in formulation and demonstrated potential in robotics, imitation learning has garnered significant attention in recent years and has seen development across various domains. For example, the works in \cite{florence2022implicit,chi2023diffusionpolicy,lee2024behavior} focus on handling the multimodal distribution in the demonstration data. A recent survey of imitation learning is presented in \cite{urain2024deep}.


Imitation learning typically requires substantial action data to achieve high-performance policies. However, videos, rich in information and more economical to collect, offer an alternative. Recent advancements in video generation models, as shown in \cite{brooks2024video}, confirm the potential of using videos to predict robotic tasks. Our paper emphasizes the integration of video generation models into policies, showing that they can achieve robust performance with less reliance on extensive, high-quality action data.

\subsection{Video Generation Model in Robotics}
Video generation models align well with temporal information, as it requires predicting temporal consistency between frames. For example, the works in \cite{du2023video,du2024learning,Ko2023Learning,yang2023learning,gu2023seer,liang2024dreamitate} propose different video generation models for robot learning. Traditionally, video generation models have required significant computational time, posing challenges for real-time robotics applications. The works mentioned above use an  ``open-loop" strategy with long replanning intervals or lack policy implementation. In contrast, through extensive experiments, we show that VILP not only produces high-quality videos but also achieves time efficiency, enabling real-time receding horizon planning.