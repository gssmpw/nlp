\section{Introduction}

Bayesian modelling is an elegant and flexible method to quantify uncertainties of parametric models. Treating the parameters of the model as random variables allows to incorporate model uncertainty. Bayesian neural networks implement this idea for neural networks (NNs) \cite{Gal2016,blundell2015weight,kendall2017uncertainties,hernandez2015probabilistic,maddox2019simple}. In practice, however, full posterior inference over Bayesian NNs is intractable due to the large number of parameters that define the NNs. Thus, to quantify the uncertainty of a certain model, practitioners have to approximate the exact posterior distribution by a simpler one.
Several methods were developed to make this approximation feasible:
The posterior distribution can be approximated, e.g., by variational inference \cite{blundell2015weight,kingma2015variational,Gal2016,kendall2017uncertainties,Jordan1999,Wainwright2008}. A different idea, that goes in fact back to the 90s, is to use the technique called Laplace approximation (LA) \cite{MacKay1992}, which has found increasing popularity in recent years due to scalable approximations \cite{lecun1989optimal,Ritter2018} and its flexible usability \cite{LaplaceRedux2021}. Moreover, in contrast to variational-inference-based approaches, it can be applied to off-the-shelf networks without any retraining: Given a maximum a posteriori (MAP) solution, that often coincides with the minimum of canonical loss functions, the LA  replaces the exact posterior by a Gaussian distribution with the MAP as the mean and the inverse of the negative Hessian of the log posterior at the MAP as covariance matrix.

However, this approximation is still infeasible for NNs since the Hessian scales quadratically in the number of parameters such that often it cannot be computed or even stored, let alone be inverted. In addition, training NNs is a high dimensional non-convex optimization problem. In practice fully trained NNs are not located in a minimum of the loss function but rather on a saddle point \cite{Dauphin2014}. Hence, the so-computed Hessian is in general not positive semi-definite \cite{Sagun2016, Papyan2018}.
A partial solution to these issues is provided by approximating the Hessian by the generalized Gauss-Newton (GGN) matrix, which is identical to the Fisher Information matrix for common likelihoods \cite{Schraudolph2002,Pascanu2013,Martens14}. The GGN matrix is positive semi-definite and is constructed from objects that are feasible to compute, cf. Section \ref{sec:terminlogy_and_background} for details. 

However, it's sheer size makes the GGN matrix still unstorable, even for medium sized networks. 
Thus, to make the LA feasible for NNs, additional steps are necessary to reduce the size of the Hessian and to allow for an easier computation of its inverse. Common approaches include approximations via a diagonal \cite{lecun1989optimal,Salimans2016,kirkpatrick2017overcoming}, last layer \cite{kristiadi2020} or a Kronecker-factored \cite{Ritter2018} structure.

A recent series of works argues that it might suffice to consider partially stochastic NNs \cite{kristiadi2020,Snoek2015,Izmailov2019,Daxberger2021,Sharma2023} that is NNs where the Bayesian inference is performed in a lower dimensional subspace. NNs are heavily overparametrized and the idea is that a subset or well-selected linear combination of parameters is sufficient to obtain reliable uncertainty estimates. 
We refer to this idea in this work as \emph{subspace inference}. In \cite{Daxberger2021} this idea is applied to make the LA for Bayesian NNs feasible by storing only a submatrix of the full GGN matrix. The submatrix is constructed using a subset of parameters that can be found via a diagonal approximation of the Hessian \cite{Daxberger2021}, via the magnitude of the parameters \cite{Cheng2017} or via an application of SWAG \cite{maddox2019simple}. 

The aim of our work is to give a systematic, generic and statistically sound approach to study the usability of subspace inference for the LA of Bayesian NNs.
Similar as in \cite{Daxberger2021} we use the widespread \cite{Foong2019,Immer2021,Deng2022,Ortega2023}  combination of the LA with a linearization of our NN $f_\theta$ around the MAP value $\hat{\theta}$ of the parameters $\theta$:
\begin{align}
   \label{eq:intro_lin_model}
   f_{\mathrm{Lin},\theta}(X) = f_{\hat{\theta}}(X) + J_X (\theta-\hat{\theta})\,,
\end{align}
where $J_X = \nabla_\theta f_\theta(X)\vert_{\theta=\map{\theta}}$. This method is known as the linearized LA. Our method differs from existing work by making the \emph{predictive covariance} of the linearized Laplace approximation the centerpiece of our analysis.
This viewpoint allows us to give some precise statements of approximation quality and optimality.

The contributions of our article are as follows:

\begin{enumerate}
    \item We specify a criterion that states when a subspace LA is optimal on a given dataset. We allow for general affine relations, similar to \cite{Izmailov2019}, and do not restrict ourselves to a selection of subsets of parameters as in \cite{Daxberger2021}.
    \item We show that there is an optimal subspace satisfying the criterion from 1 and give a formula for the according affine relation. 
    \item We demonstrate how this theoretical formula can be used in practice to give a subspace LA and observe that it performs in many cases superior to the subset selection of \cite{Daxberger2021,LaplaceRedux2021}.
    \item To measure the performance we propose a new easy-to-compute criterion.
\end{enumerate}

This article is organized as follows: In Section \ref{sec:recent_work} we recall recent work on the subject of the article and then evoke some background on the LA for Bayesian NNs in Section \ref{sec:terminlogy_and_background}. In Section \ref{sec:LA_for_subspace_models} we provide the main theoretical contributions of this work. In Section \ref{sec:Experiment} several experiments to empirically verify our theoretical analysis are carried out. Additional information is provided in the Appendix.

\section{Recent Work}
\label{sec:recent_work}

\textbf{Laplace Approximation.} The first application of the LA using the Hessian for NNs was introduced by MacKay \cite{MacKay1992}. \cite{MacKay1992Class} also proposed an approximation similar to the generalized Gauss-Newton (GGN) method. The combination of scalable factorizations or diagonal Hessian approximations with the GGN approximation \cite{Schraudolph2002,Martens14} made the LA applicable for larger networks. In particular, the GGN approximation  gained more attention due to 
the introduction of the Kronecker-factored Approximate Curvature (KFAC) \cite{Ritter2018,Botev2017,Martens2015} which is scalable and outperforms the diagonal Hessian approximation.
Due to underfitting issues of the LA \cite{Foong2019}, the linearized
LA based on \eqref{eq:intro_lin_model} was developed \cite{Immer2021}. We use the same setting in
this work.

\textbf{Partially Stochastic Neural Networks.} 
Studying partially stochastic NNs gained some attention due to their computational efficiency. But even from a statistics viewpoint partially stochastic NNs are attractive because they can capture the uncertainty of the full model by using only a fraction of the parameters. \cite{Sharma2023} showed that a low-dimensional subspace is sufficient to obtain expressive predictive distributions. They developed the concept of Universal Conditional Distribution Approximators and proved that certain partially stochastic NNs can form samplers of any continuous target conditional distribution arbitrary well. \cite{calvoordonez2024} extended this idea to infinitely deep Bayesian NNs.  

\cite{Izmailov2019} developed a low-dimensional affine subspace inference scheme. They selected a linear combination of parameter vectors which span a vector space around the MAP. Since this subspace is low-dimensional different methods can be used to approximately sample from the posterior distribution. However, they observed that their uncertainties are too small such that they had to use a tempered posterior to obtain reasonable uncertainties. 
\cite{Daxberger2021} chose a subset of parameters to construct a subspace model. This subset is selected by the parameters that have the highest posterior variance. However, this work requires quite a large number of parameters to be selected (up to $4\cdot 10^4$).
Our framework is closest to this work. In contrast, we study the predictive instead of the posterior distribution to obtain a feasible parameter subspace. In addition, we show in the following that neither an ad hoc tempering of the posterior distribution nor thousands of parameters are needed to estimate the uncertainty reliable.


\section{Terminology and Background}
\label{sec:terminlogy_and_background}
\textbf{Setup and Notational Remarks.}
We consider the supervised learning framework. We model the relation between the independent observable $x$ and the target $y$ by a parametric distribution $p(y | x, \theta)$ with parameters $\theta \in \RR^p$. Different observations are, as usual, assumed to be independent and identically distributed.
We denote the training set of observations as $\mathcal{D} = \{(x_i,y_i) | 1 \leq i \leq N\}$ where $N$ denotes the number of observations.
We study regression and classification tasks. $C$ represents the number of outputs $f_\theta(x)=(f_\theta^1(x),\ldots, f_\theta^C(x))^\intercal\in \RR ^C$ of the NN $f_\theta$, for both, regression and classification problems. For regression we make a Gaussian model assumption $p(y | x, \theta) = \mathcal{N}(y | f_{\theta}(x), \sigma^2\mathbb{1}_C)$, where only the mean is modelled by the NN. Classification tasks with $C$ classes are modelled by a categorical distribution $p(y|x,\theta)=\mathrm{Cat}\left(y | \phi(f_{\theta}(x))\right)$ with probability vector $\phi(f_{\theta}(x))$, where $\phi$ denotes the softmax function. 

We will often consider not a single input sample to $f_\theta$ but a whole set such as $X=(x_1,\ldots,x_n)$. In this case $f_\theta(X)=(f_\theta(x_1)^\intercal,\ldots,f_\theta(x_n)^\intercal)^\intercal\in \RR^{nC}$ should be read as the concatenation of the outputs. We will frequently use the Jacobian of $f_\theta$ w.r.t. its parameter $\theta \in \RR^p$ evaluated at the MAP $\map{\theta}$ defined in \eqref{eq:MAP} below. Given a set $X$ we concatenate the single input Jacobians along the output dimension and use the symbol
\begin{equation}
    \label{eq:J_X}
    \begin{aligned}
        J_X := ( \nabla_\theta f_\theta(x_1)^\intercal,\ldots,  \nabla_\theta f_\theta(x_n)^\intercal)^\intercal\vert_{\theta=\map{\theta}}\in \RR^{nC\times p}\,.
    \end{aligned}
\end{equation}


\textbf{Bayesian Neural Networks.}
When taking a Bayesian view on NNs the parameter $\theta$ is considered as a random variable equipped with a prior distribution $p(\theta)$. Given the training data $\mathcal{D}=\{(x_i,y_i) | 1 \leq i \leq N\}$, the posterior distribution of $\theta$ is given by $p(\theta | \mathcal{D}) \propto p(\theta) p(D|\theta)= p(\theta)\prod_{i=1}^N p(y_i | x_i, \theta)$ (with $p(y_i|x_i, \theta)$ as above). A point estimate for $\theta$ is then given by the value that is most likely under $p(\theta|\mathcal{D})$, the so-called MAP (\emph{maximum a posteriori}) estimate, that is
\begin{align}
    \label{eq:MAP}
     \map{\theta} = \argmin_{\theta} \mathcal{L}_{\theta}(\mathcal{D})  \,,
\end{align} 
where we used the (unnormalized) negative log-posterior
\begin{align}
     \label{eq:log_posterior_unnormalized}
     \mathcal{L}_\theta(\mathcal{D}) = -\sum_{i=1}^N \ln p(y_i |x_i, \theta) - \ln p(\theta) \,.
\end{align}
In this work we will use the common choice $p(\theta) = \mathcal{N}(\theta|0,\lambda^{-1}\mathbb{1}_p)$ with precision $\lambda>0$ for which \eqref{eq:log_posterior_unnormalized} just boils down to the MSE loss (for regression) or cross-entropy loss (for classification) combined with L2 regularization.

\textbf{Laplace Approximation.} With $\mathcal{L}_\theta(\mathcal{D})$ as in \eqref{eq:log_posterior_unnormalized} the posterior distribution $p(\theta|\mathcal{D})$ reads as
\begin{align}
\label{eq:BayesPosterior}
    p(\theta| \mathcal{D}) = \frac{1}{Z} p(\mathcal{D} | \theta) p(\theta)
    = \frac{1}{Z} e^{-\mathcal{L}_{\theta}(\mathcal{D})}
\end{align}
with the normalization constant $Z=\int d\theta\; p(\mathcal{D} | \theta) p(\theta)$. 
For complex models such as Bayesian NNs the exact posterior is typically infeasible to compute or sample from. Expanding \eqref{eq:log_posterior_unnormalized} to second order around the MAP $\map{\theta}$ from \eqref{eq:MAP}, we obtain 
\[
    \mathcal{L}_{\theta}(\mathcal{D}) \simeq
    \mathcal{L}_{\map{\theta}}(\mathcal{D}) 
    + \frac{1}{2} \left(\theta - \map{\theta}\right)^\intercal
    \left(\nabla_{\theta}^2 \mathcal{L}_{\theta}(\mathcal{D})\vert_{\theta=\hat{\theta}}\right)
    \left(\theta - \map{\theta}\right) \,.
\]
Inserting this expansion in \eqref{eq:BayesPosterior} we arrive at the \emph{Laplace approximation} of the posterior 
\[
p(\theta| \mathcal{D}) \simeq \mathcal{N}(\theta | \map{\theta}, \Psi)
\]

with mean $\map{\theta}$ and covariance $\Psi = \left(\nabla_{\theta}^2 \mathcal{L}_{\theta}(\mathcal{D}) \vert_{\theta=\map{\theta}}\right)^{-1} = \left( N H + \lambda\mathbb{1}_p  \right)^{-1} \in \mathbb{R}^{p\times p}$, where we denote by $H=-\frac{1}{N}\sum_{i=1}^N\nabla_\theta^2 \ln p(y_i|\theta,x_i)\vert_{\theta=\map{\theta}}$ the Hessian of the averaged negative log-likelihood.



\textbf{Generalized Gauss-Newton Matrix.}
The Hessian $H \in \mathbb{R}^{p \times p}$ from above is the second order derivative of the averaged negative-log-likelihood $-\frac{1}{N}\ln p(\mathcal{D}|\theta)$ at the MAP $\hat{\theta}$. On the one hand, to compute $H$ is infeasible, and on the other hand, even if $H$ could be computed, it would be impossible to store the $\frac{p \left(p+1\right)}{2}$ free components, since $p\gg 1$.
In addition, for trained NNs the Hessian does usually not have the nice property of positive semi-definiteness that is found, e.g., in the context of convex problems, because the learned MAP $\map{\theta}$ is, in general, not a local minimum but rather a saddle point. 
These difficulties of computational complexity and missing positive definiteness can be overcome by using the generalized Gauss-Newton (GGN) matrix \cite{Schraudolph2002} instead of $H$:
\begin{align}
    \label{eq:GGN}
    H_{\mathrm{GGN}} = \frac{1}{N} \sum_{i=1}^N J^\intercal_{f_i} H_{\text{-}\ln p(y_i|f_i)} J_{f_i}\,,
    \end{align}
where $J_{f_i} = \nabla_{\theta} f_{\theta}(x_i)\vert_{\theta=\hat{\theta}} \in \mathbb{R}^{C \times p}$ and $H_{\text{-}\ln p(y_i|f_i)} = - \nabla_{f}^2 \ln p(y_i|f_i)\vert_{f_i=f_{\hat{\theta}}(x_i)} \in  \mathbb{R}^{C \times C}$ is the Hessian of the negative log-likelihood w.r.t. model output ${f_i} = f_{\theta}(x_i)$. $H_{\mathrm{GGN}}$ can be interpreted as the Hessian of the linearized model \cite{Martens14, Immer2021} and is positive semi-definite if the $H_{\text{-}\ln p(y|f_i)}$ are positive-semi definite \cite{Schraudolph2002}, which is the case in our work. More detailed  information on $H_{\mathrm{GGN}}$ and the relation between $H_{\mathrm{GGN}}$ and $H$ is provided in Appendix \ref{sec:FI_GGN_and_H}.
Combining \eqref{eq:GGN} with the term arising from the prior $p(\theta)$ we obtain the following precision matrix of the Laplace approximated model
\begin{align}
    \label{eq:IPsiGGN}
    \Psi_{\mathrm{GGN}}^{-1}=  \sum_{i=1}^N J^\intercal_{f_i} H_{\text{-}\ln p(y_i|f_i)} J_{f_i} + \lambda \mathbb{1}_p\,.
\end{align}

\textbf{Approximations.}
While the GGN relation \eqref{eq:GGN} consists of objects, $J_{f_i}$ and $H_{\text{-}\ln p(y_i|f_i)}$, that are scalable in their computation we usually can't compute $H_{\mathrm{GGN}}$ or $\Psi_{\mathrm{GGN}}^{-1}$ as the resulting matrices have still too many dimensions for modern NNs. In particular, we can't invert $\Psi_{\mathrm{GGN}}^{-1}$ to obtain the posterior covariance
$\Psi_{\mathrm{GGN}}$.
As a consequence, various approximations have been developed that modify the structure in such a way that it takes less amount of storage and is easier to invert. An easy solution is to only keep the diagonal of $\Psi_{\mathrm{GGN}}$. In the KFAC approximation the Hessian is reduced to a form where it is the Kronecker product of two smaller matrices. 

\textbf{Equivalence Between GGN and Fisher Information.}
For the computations in our experiments we use the Fisher information matrix $\mathcal{I}$ instead of $H_{\mathrm{GGN}}$ which are identical objects for the cases considered in this work \cite{Heskes2000, Martens14, Dauphin2014}, cf. Appendix \ref{sec:FI_and_GGN}. As follows from the identities in Appendix \ref{sec:FI} we have $\mathcal{I}=VV^\intercal$ with a $V\in \RR^{p\times NC}$ that can be computed via minibatches from $\mathcal{D}$ and expressions that involve first order derivatives of $f_\theta$. This allows us to compute for any matrix $P\in \RR^{p\times s}$ the expression 
\begin{align}
\label{eq:quadratic_Fisher_Form}
P^\intercal H_{\mathrm{GGN}}P =P^\intercal \mathcal{I} P  = (VP)^\intercal VP \in \RR^{s\times s}
\end{align}
in a scalable manner if $s$ is sufficiently small. Thus, while we often can't actually compute $H_{\mathrm{GGN}}$ or $\mathcal{I}$ in practice, we can usually compute quadratic forms such as \eqref{eq:quadratic_Fisher_Form}.


\textbf{Predictive Distribution.}
For the posterior distribution $p(\theta|\mathcal{D})$ and a set of $n$ inputs $X$ the posterior predictive distribution is given by
\begin{align}
    \label{eq:pred_distribution}
    p(Y|X, \mathcal{D}) = \int d\theta\; p(Y| X, \theta) p(\theta | \mathcal{D}).
\end{align}
Under the LA and using the linearized model \eqref{eq:intro_lin_model} for $p(Y|X, \mathcal{D})$ we can give an explicit formula to this distribution for regression problems
\begin{align}
    \label{eq:PredDistrGauss}
    p(Y|X,\mathcal{D}) & \simeq \mathcal{N}(Y|f_{\map{\theta}}(X), \Sigma_X + \sigma^2 \mathbb{1}_{nC} )
    \\
    \label{eq:Sigma_X}
    \text{with} \quad
    \Sigma_X & = J_{X} \Psi J_{X}^\intercal  \in \RR^{nC\times nC}
\end{align}
denoting the model uncertainty part of the predictive covariance.
For classification tasks the predictive distribution can be approximated by the probit approximation \cite{Bishop2006} 
\eqn{
    p(Y|X,\mathcal{D}) \simeq \Cat\left(Y | \phi\left(\frac{f_{ \map{\theta}}(X)}{\sqrt{1 + \frac{\pi}{8} \diag\Sigma_X}}\right)\right)
}{eq:PredDistrCat}
with $\Sigma_X$ as in \eqref{eq:Sigma_X} and the softmax function $\phi$. Note that in both cases, regression and classification, the predictive distribution is essentially fixed by $\Sigma_X$ from \eqref{eq:Sigma_X}, which is why this object will be the linchpin of our analysis below. We will call $\Sigma_X$ the \emph{epistemic predictive covariance}.

\section{The Laplace Approximation for Subspace Models}
\label{sec:LA_for_subspace_models}
\textbf{Subspace Models.}
In this work we study, as in \cite{Izmailov2019}, models that are defined on an affine subspace of the parameter space $\mathbb{R}^p$ chosen to contain the MAP $\hat{\theta}$ from \eqref{eq:MAP}. That is, we consider a re-parametrization 
\begin{align}
    \label{eq:reparametrization}
    \theta = \hat{\theta} + P \mu \,,
\end{align}
where $P\in \RR^{p\times s}$ is a matrix that we call, somewhat loosely, the projection matrix (in general it's not related to a mathematical projection) and $\mu$ is a new parameter that runs through $\RR^s$ where $s\leq p$ is the subspace dimension.
The assumption in considering Bayesian inference of NNs in a subspace is that only a fraction of the parameter space is actually needed to represent the (epistemic) uncertainty faithfully. 

Note that the selection of a subset of parameters on which to perform inference, as it's done in \cite{Daxberger2021,Sharma2023}, is a special case of \eqref{eq:reparametrization}, as can be seen by choosing $P=(e_{i_1}, \ldots, e_{i_s})$ as a concatenation of canonical basis vectors, where the set $\{i_j | 1\leq j \leq s\}\subseteq \{1,\ldots, p\}$ corresponds to the chosen subset.


\textbf{Bayesian Inference for $\mu$.} 
To perform Bayesian inference in the subspace model, we choose the following prior 
\begin{align}
    \label{eq:mu_prior}
    \tilde{p}(\mu) = \mathcal{N}(\mu|0, \left(\lambda P^\intercal P \right)^{-1}) \,,
\end{align}
where $\mu$ is the random variable in this subspace and we recall that $\lambda$ is the precision of $p(\theta)$. The set of maps $P$ that we analyse in this work can always be chosen such that $P^\intercal P = \mathbb{1}_s$. Together with the following likelihood 
\begin{align}
    \label{eq:mu_likelihood}
    \tilde{p}(\mathcal{D}|\mu) = p(\mathcal{D} |\hat{\theta} + P\mu)
\end{align}
that is induced by \eqref{eq:reparametrization}, the following lemma holds: 
\begin{lemma}
\label{lem:Bayes_model}
In the setting above, consider a full rank $P\in \RR^{p\times s}$.  For the posterior $\tilde{p}(\mu|\mathcal{D})\propto \tilde{p}(\mu) \tilde{p}(\mathcal{D}|\mu)$ with prior $\tilde{p}(\mu)$ as in \eqref{eq:mu_prior} we have the LA
\begin{align}
    \label{eq:LA_mu}
    \tilde{p}(\mu|\mathcal{D}) \simeq \mathcal{N}(0,(P^\intercal \Psi^{-1} P)^{-1}) \,.
\end{align}
\end{lemma}

Lemma \ref{lem:Bayes_model} is true because from \eqref{eq:mu_likelihood} and \eqref{eq:mu_prior}, we can deduce $-\nabla_\mu^2 \ln\left(\tilde{p}(\mathcal{D}|\mu) \tilde{p}(\mu)\right) \vert_{\mu=0} = P^\intercal (NH+ \lambda \mathbb{1}_p)P = P^\intercal \Psi^{-1} P$.

We will find in Theorem \ref{thm:OptSubmodel} below that the family of posteriors \eqref{eq:LA_mu} is rich enough to approximate the full LA optimally in a certain sense when a suitable $P$ is chosen.

\textbf{Predictive Distributions of Subspace Models.}
Similar to \eqref{eq:intro_lin_model} we linearize $\tilde{f}_{\mu} =f_{\map{\theta} + P \mu}$ around $\mu=0$ to obtain for a set of $n$ inputs $X$ 
\begin{align}
    \label{eq:lin_model_mu}
    \tilde{f}_{\mathrm{Lin},\mu}(X) = f_{\hat{\theta}}(X) + J_X P (\mu-0) \,,
\end{align}
where we denoted, as in \eqref{eq:intro_lin_model}, by $J_X=\nabla_\theta f_{\theta}(X)\vert_{\theta=\map{\theta}} \in \RR^{nC\times p}$ the Jacobian of the full network at the MAP.

Combining \eqref{eq:LA_mu} with \eqref{eq:lin_model_mu} we obtain as above the predictive distributions
\begin{equation}
    \begin{gathered}
        \mathcal{N}(Y |f_{\map{\theta}}(X), \Sigma_{P,X}+ \sigma^2 \mathbb{1}_{nC} ), 
        \\
        \Cat\left(Y | \phi\left(\frac{f_{ \map{\theta}}(X)}{\sqrt{1 + \frac{\pi}{8} \diag\Sigma_{P,X}}}\right)\right),
    \end{gathered}
    \label{eq:LaplaceSubmodel}
\end{equation}

for the LAs of a subspace model
with the notation 
\begin{align}
    \label{eq:Sigma_p}
    \Sigma_{P,X} = J_X P (P^\intercal \Psi^{-1} P)^{-1} P^\intercal J_X^\intercal \in \mathbb{R}^{nC\times nC}
\end{align}
for its epistemic predictive covariance.

\textbf{Evaluation of $P$.} We would like to find a subspace model \eqref{eq:reparametrization} whose LA closely aligns  with the (typically infeasible) LA of the full model. The subspace model is fixed by the projection matrix $P$. 
As the posterior predictive distribution \eqref{eq:pred_distribution} is the object of genuine interest for prediction via Bayesian NNs it is natural to require that the distributions in \eqref{eq:PredDistrGauss}, \eqref{eq:PredDistrCat} and in \eqref{eq:LaplaceSubmodel} are as similar as possible.
As those distributions arise from each other by replacing the epistemic predictive covariance, i.e. replacing $\Sigma_X$ by $\Sigma_{P,X}$, we can measure the approximation quality by the relative error 
\begin{equation}
    \label{eq:RelativeError}
    \frac{\norm{\Sigma_X - \Sigma_{P,X}}_F}{\norm{\Sigma_X}_F}\,,
\end{equation}
where we use the Frobenius norm $\norm{\ldots}_F$.

\subsection{The Optimal Subspace Model}
Consider a set of $n$ inputs $X=\left(x_1, \ldots, x_n \right)$. This could be the set of inputs in the training set $\mathcal{D}$ or a subset of the latter.  Given this set $X$ and a fixed subspace dimension $s\leq p$ we want to find the optimal $P^*\in \RR^{p\times s}$ that solves the following minimization problem
\begin{align}
    \label{eq:minimization_problem}
    P^* \in \argmin_{P\in \RR^{p\times s}, \,\rank \,P=s} \|\Sigma_{P,X} - \Sigma_X\|_F \,,
\end{align}
where the epistemic predictive covariances are defined as in \eqref{eq:Sigma_X} and \eqref{eq:Sigma_p}.
A solution to this problem will then also minimize the relative error \eqref{eq:RelativeError}. Note that such a solution is never unique. In fact, for any $P^*$ that solves \eqref{eq:minimization_problem} we can also consider $P^*Q$ for an arbitrary invertible $Q\in \RR^{s\times s}$ since we have $\Sigma_{P^*Q,X}=\Sigma_{P^*,X}$, cf. \eqref{eq:Sigma_p}.

For the solution of the problem \eqref{eq:minimization_problem} we will need the eigenvalue decomposition $\Sigma_X = J_X \Psi J_X = U \Lambda U^\intercal$ where $U$ is an orthogonal matrix and $\Lambda \in \mathbb{R}^{nC \times nC}$ is a positive semi-definite diagonal matrix. We choose this eigendecomposition such that the diagonal entries of $\Lambda$ are decreasing. We will use the Eckart-Young-Mirsky-Theorem \cite{Schmidt1907,Eckart1936,Mirsky1960} which states that the following low rank problem has an explicit solution
\begin{align}
    \label{eq:BestLowRankSigma}
    U_s \Lambda_s U_s^\intercal \in \argmin_{A\in \mathbb{R}^{nC \times nC}: \,\rank\,A \leq s} \| A- \Sigma_X \|_F \,,
\end{align}
where $U_s \in \mathbb{R}^{nC \times s}$ contains the first $s$ eigenvectors, called dominant eigenvectors from now on, and $\Lambda_s \in \mathbb{R}^{s\times s}$ is the reduced diagonal matrix obtained by taking the upper $s \times s$ block containing the $s$ leading eigenvalues of $\Sigma_X$.
The following theorem shows that the LA to a subspace model can reach the rank-$s$ minimum from \eqref{eq:BestLowRankSigma} for a suitable class of $P^*$. 

\begin{theorem}[Existence of an optimal subspace model for the Laplace approximation]
\label{thm:OptSubmodel}
Consider the problem \eqref{eq:minimization_problem} with $s\leq s_{\mathrm{max}}=\min(nC,p)$.
Suppose that $J_X\in \RR^{nC\times p}$ has full rank. For any invertible $Q\in \RR^{s\times s}$ the matrix
\begin{align}
    \label{eq:optimal_P}
    P^* = \Psi J_X^\intercal U_s Q
\end{align}
solves \eqref{eq:minimization_problem}.
 For any such $P^*$ we have
\begin{align}
    \label{eq:low_rank_laplace_optimal_Sigma}
    \Sigma_{P^*,X} = U_s \Lambda_s U_s^\intercal.
\end{align}
\end{theorem}

The proof is provided in Appendix \ref{sec:AppendixProofSubmodel}.
The restriction to dimensions below $s_{\mathrm{max}} = \min(nC,p)$ and the assumption on the full rank of $J_X$ is needed to assure that $P^*$ has full rank which is required for $\Sigma_{P^*,X}$ in order to be well-defined. If $J_X$ doesn't have full rank, we restrict the experiments to the rank of the Jacobian. This is done in some regression problems in Section \ref{sec:Experiment}.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/enb/error.png}
        \caption{ENB}
        \label{subfig:rel_error_enb}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/redwine/error.png}
        \caption{Red Wine}
        \label{subfig:rel_error_redwine}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/california/error.png}
        \caption{California}
        \label{subfig:rel_error_california}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/navalpro/error.png}
        \caption{Naval Propulsion}
        \label{subfig:rel_error_navalpro}
    \end{subfigure}
    
    \vspace{0.1cm}
    
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/enb/logtrace.png}
        \caption{ENB}
        \label{fig:enb}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/redwine/logtrace.png}
        \caption{Red Wine}
        \label{fig:redwine}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/california/logtrace.png}
        \caption{California}
        \label{fig:california}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/navalpro/logtrace.png}
        \caption{Naval Propulsion}
        \label{fig:navalpro}
    \end{subfigure}

    \caption{Comparison of low rank approximations and subset methods for different regression datasets. Different choices of $P$ are marked by different colours and line types. The first row displays the relative error \eqref{eq:RelativeError} and the second the logarithm of the trace \eqref{eq:TraceOrderMain} of the epistemic covariance matrix. Missing values in the logarithm of trace plots have a trace of zero at these values of $s$ (e.g. SWAG for the lowest $s$ in Red Wine.)}
    \label{fig:regression_plots}
\end{figure*}

\subsection{Applying Theorem \ref{thm:OptSubmodel} in Practice}
\label{sec:thm_in_practice}

Theorem \ref{thm:OptSubmodel} states that there is an optimal solution to problem \eqref{eq:minimization_problem} and it is, to the best knowledge of the authors, the first systematic solution to a subspace modelling for Bayesian NNs in the context of LA. 
However, applying Theorem \ref{thm:OptSubmodel} in practice will usually not be possible, due to the following reasons:

\textbf{Epistemic Limitation.} Training datasets $\mathcal{D}$ are often so large that computing a eigendecomposition of $\Sigma_X$ and thus of $U_s$ is infeasible. However, even if we can pick $X=\mathcal{D}$ we actually want the subspace model to work for unseen data points, that is data points that are not contained in $\mathcal{D}$.

\textbf{No Access to $\Psi$.} The posterior covariance $\Psi$ from the LA is usually not available. In fact, if it was, this would raise the question of why to use a subspace model at all.

In practice we will therefore use the following workflow:
\begin{enumerate}
    \item Fix an approximation $\Psi_{\mathrm{approx}}$ to $\Psi$ such as the KFAC or diagonal approximation.
    \item Use a subset $X'$ of size $n$ of the inputs in the training set to construct $J_{X'} \Psi_{\mathrm{approx}} J_{X'}^\intercal \in \RR^{nC\times nC}$ and determine its $s$ dominant eigenvectors $U_s\in \RR^{nC \times s}$
    \item Construct $P$ via $P=\Psi_{\mathrm{approx}} J_{X'}^\intercal U_s$ (we will in this work $Q$ fix to be always the identity).
    \item For the $X$ of interest (usually not contained in the training set), compute the predictive covariance $J_X P (P^\intercal \Psi P)^{-1} P^\intercal J_X^\intercal$. Note, that we can really use the GGN $\Psi$ here, since $\Psi_{\mathrm{GGN}}=VV^\intercal$ can be written as an outer product which allows for a batch-wise computation, cf. \eqref{eq:quadratic_Fisher_Form}. For our experiments we used an $X$ of size $n$ that was randomly drawn from the test data.
\end{enumerate}

As our construct deviates due to $X'\neq X$ and $\Psi_{\mathrm{approx}}\neq \Psi$ from the setting in Theorem \ref{thm:OptSubmodel} we do not have any longer a guarantee of choosing an optimal $P$. In Section \ref{sec:Experiment} we study therefore empirically the performance of the above construction on various datasets. 

\textbf{Trace Metric for $P$.} The relative error \eqref{eq:RelativeError} quantifies the deviation of the subspace model to the full LA. As the latter is usually not known we propose a different metric that gives qualitatively the same ranking of the subspace models as we empirically demonstrate in Section \ref{sec:Experiment}. Heuristically, $\Sigma_{P,X}$ approximates better $\Sigma_X$ if it contains the dominant eigenspace, because in the directions of these eigenvectors the covariance has its largest contributions. Hence, we propose as an alternative to \eqref{eq:RelativeError} the \emph{trace criterion}: If 
\begin{equation}
    \label{eq:TraceOrderMain}
    0 \leq \Tr \Sigma_{P_1,X} < \Tr \Sigma_{P_2,X} \leq \Tr \Sigma_{X}
\end{equation}
holds, $P_2$ is a better projector than $P_1$. A larger trace value indicates that the more dominant eigenspace is captured for a given $P$. The proof of $\Tr \Sigma_{P,X} \leq \Tr \Sigma_X$ and an extended explanation are given in Appendix \ref{sec:AppendixTraceCriterion}. 
 

\section{Experiments}
    \label{sec:Experiment}

For our experiments we use various regression datasets from \texttt{OpenML} \cite{OpenML2013} \cite{OpenMLPython2019} as well as common classification tasks as MNIST \cite{LeCun1998mnist}, a corrupted version of MNIST \cite{Mu2019mnistc}, FashionMNIST \cite{Xiao2017FashionMNIST}, CIFAR10 \cite{Krizhevsky2009cifar} and a subset of ImageNet \cite{ImageNet2009}, called ImageNet10, that contains only the ten classes listed in Appendix \ref{sec:AppendixExp}. Details about the used NNs can be found in Appendix \ref{sec:AppendixExp} and the repository.%
    \footnote{{\scriptsize \url{https://github.com/josh3142/LowRankLaplaceApproximation}}}
We compare the following LAs:
\begin{itemize}
    \item $P_{\mathrm{subset-Magnitude}}, P_{\mathrm{subset-Diagonal}}$ and $P_{\mathrm{subset-SWAG}}$ (dashed lines) select a subset of parameters according to the magnitude of parameters, the diagonal GGN approximation or variances produced via SWAG. We use the term \textit{subset methods} for these approximations from \cite{Daxberger2021,LaplaceRedux2021} because they select certain parameters to construct $P$. 
    \item $P_{\mathrm{lowrank-KFAC}}$ and $P_{\mathrm{lowrank-Diagonal}}$ (solid lines) are constructed as in Section \ref{sec:thm_in_practice} and use a KFAC or a diagonal GGN approximation to estimate $\Psi$. Hence, these construction are based on an approximation of the posterior covariance $\Psi$ (cf. Section \ref{sec:thm_in_practice}). We use the term \textit{low rank methods} for these since Theorem \ref{thm:OptSubmodel} bases its argument on a low rank approximation. A subset of the training data was used for the construction of these subspace models, cf. Appendix \ref{subsec:size_of_training_data_subset}.
    \item Moreover, where feasible, we show results for a $P_{\mathrm{lowrankopt-GGN}}$ (dashed-dotted line) that is exactly constructed as in Theorem \ref{thm:OptSubmodel} by using the \emph{test} data and the $\Psi_{\mathrm{GGN}}$ for the construction of the subspace model. 
    This is the optimal subspace model for a given $s$. $P_{\mathrm{None-Full}}$ (dotted line) is the regular LA without any dimensional reduction.
\end{itemize}
All experiments are done with five different seeds and the average of the results is plotted with markers. To enhance the visualization, the markers are linearly interpolated by lines whose type indicates the methods used to approximate the LA. The shaded area around the mean value illustrates the sample standard error. All plots use the same colour and line coding.

To evaluate the different subspace models \eqref{eq:reparametrization}, parametrized by $P\in \mathbb{R}^{p\times s}$, we use the relative error \eqref{eq:RelativeError} because it quantifies the approximation quality of the epistemic predictive covariance $\Sigma_{P,X}$ w.r.t. the full epistemic predictive covariance matrix $\Sigma_X$. In addition, we use the auxiliary trace metric \eqref{eq:TraceOrderMain} to empirically verify that it yields qualitatively the same ordering as the relative error. This enables us to compare subspace models if the relative error isn't computable. In addition, we also studied the widespread NLL metric, which however yielded inconsistent results for the problem studied in this work. The results and an according discussion are provided in Appendix \ref{sec:AppendixNLL}.


\textbf{Regression Datasets.} Figure \ref{fig:regression_plots} shows the relative error \eqref{eq:RelativeError} and the logarithm of the trace criterion \eqref{eq:TraceOrderMain} (log-trace) for different regression datasets and the subspace models listed above for different $s$. For ENB, Red Wine and Naval Propulsion the Jacobian is rank-deficient, so that only $s$ up to the rank of the Jacobian on the training data are considered. California is plotted up to $s=5000$. First, we observe that the ideal subspace model (black dashed-dotted line) needs only a fraction of the number of model parameters that are around 18000 to reach a small relative error. The exact number of parameters is listed in Table \ref{tab:NumParamsModel} in the Appendix \ref{sec:AppendixExp}. Hence, subspace models can be suitable to quantify the uncertainty provided by a LA. However, $P_{\mathrm{lowrankopt-GGN}}$ is usually unknown such that the ideal approximation isn't available. 
Comparing the feasible approximations in Figure \ref{fig:regression_plots} we find that low rank approximations demonstrate superior approximations compared to subset methods in general. In particular, the performance of $P_{\mathrm{subset-Diagonal}}$ is strictly inferior to $P_{\mathrm{lowrank-Diagonal}}$. For ENB the subset methods obtain a better performance. We speculate that the different performance on this dataset is related to the number of `dead parameters' whose gradient is almost zero, which provides a natural subset to be selected. Indeed, ENB has the most number of dead parameters with 93\%. More details on this investigation are given in Appendix \ref{sec:AppendixDeadParam}.

A comparison between the first and the second row of Figure \ref{fig:regression_plots} demonstrates that the log-trace retains the ordering of the relative error. Differences are rare and if they happen they are small and usually contained in the sample standard deviation.

\begin{figure}[!t]
    \centering
     \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mnist/error.png}
        \caption{MNIST}
        \label{subfig:rel_error_mnist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fashionmnist/error.png}
        \caption{FashionMNIST}
        \label{subfig:rel_error_fashionmnist}
    \end{subfigure}

    \vspace{0.25cm}
    
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mnist/logtrace.png}
        \caption{MNIST}
        \label{fig:mnist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fashionmnist/logtrace.png}
        \caption{FashionMNIST}
        \label{fig:fashionmnist}
    \end{subfigure}        
    \caption{Relative error \eqref{eq:RelativeError} and logarithm of trace \eqref{eq:TraceOrderMain} of the epistemic covariance matrix for MNIST and FashionMNIST.}
    \label{fig:MNIST_plots}
\end{figure}


\textbf{Classification Tasks.} MNIST and FashionMNIST are trained with small CNNs such that the relative error is computable. For these datasets the discrepancy between low rank and subset methods is even larger. Figure \ref{fig:MNIST_plots} shows that the subset methods yield a relative error of approximately 1.0 which implies that these methods aren't able to approximate the full covariance matrix. Only for very large $s$ the relative error starts to decrease which demonstrates that these methods fail to approximate the full solution effectively (cf. Appendix \ref{sec:AppendixClassSubsetLarge}). $P_{\mathrm{lowrank-KFAC}}$ shows the best performance. It's relative error decreases below $0.2$ and $0.1$ for $s=10$ for MNIST and FashionMNIST, respectively, and the large trace values indicate that $P_{\mathrm{lowrank-KFAC}}$ parametrizes the eigenspace corresponding to the largest eigenvalues of $\Sigma_X$, well. 

In Figure \ref{fig:mnist_c} the quality of the approximation on out-of-distribution data is evaluated. We use the NNs that were trained on MNIST but apply them on corrupted test data. 15 different corruptions are studied. For each subspace dimension we consider, as above, various choices of $P$ indicated by different colours, where the same coding as in Figures \ref{fig:regression_plots} and \ref{fig:MNIST_plots} applies. The relative error and the trace of different subspace models for $s \in \{100,500, 1000\}$ are plotted in Figure \ref{fig:mnist_c}. While the optimal low rank approximation yields good results, the performance of all the other methods decreases. E.g. for certain corruptions like brightness and fog all non-optimal subspace models perform bad. These results indicate that the performance on the subspace models depends on the nature of the out-of distribution data. Interestingly, we can observe that the jump from $s=100$ to $s=1000$ has far less impact on the relative error of subset methods than the transition to a $P$ as constructed in Section \ref{sec:thm_in_practice}. Hence, the approximation method is more important than the size $s$ of the subspace.

\begin{figure}[t!] 
    \centering
    \begin{subfigure}{0.23\textwidth}  
        \centering
        \includegraphics[width=\textwidth]{figures/mnist_c/rel_error.png}
        \caption{Relative error}
        \label{fig:mnist_c_rel_error}
    \end{subfigure} \hfill 
    \begin{subfigure}{0.23\textwidth}  
        \centering
        \includegraphics[width=\textwidth]{figures/mnist_c/trace.png}
        \caption{Trace criterion}
        \label{fig:mnist_c_trace}
    \end{subfigure}
    \caption{Relative error \eqref{eq:RelativeError} (left) and trace criterion \eqref{eq:TraceOrderMain} (right) for corrupted MNIST datasets \cite{Mu2019mnistc}  and three different dimensions $s=100,\, 500,\, 1000$ (shown by markers in increasing size). Different choices for $P$ are indicated by different colours and marker shapes: Square markers $\blacksquare$ indicate subset based methods, whereas discs $\bullet$ indicate low-rank based methods (proposed in this work). The colour coding is chosen as in Figure \ref{fig:regression_plots}. Note there are two $P$s constructed from a diagonal approximation to the Hessian that either use a subset (pink squares) or a low rank based (pink circles) approach. Results were obtained by averaging over five seeds. Standard errors are depicted by bars, where the latter are larger than the marker size.}
    \label{fig:mnist_c}
\end{figure}

In Figure \ref{fig:trace_high_dimensional} we consider a ResNet9 for CIFAR10 and a ResNet18 for ImageNet10. For computational reasons we restricted our analysis for ImageNet10 to $s\leq 30$. For both networks the number of parameters is so large that $\Sigma_X$ and thus the relative error is computationally infeasible. 
While the relative error is not available we can, however, still evaluate different methods with the trace criterion \eqref{eq:TraceOrderMain}. Figure \ref{fig:trace_high_dimensional} confirms our observations from lower dimensional problems. $P_{\mathrm{lowrank-KFAC}}$ is superior to all other methods and the low dimensional eigenspace of $\Sigma_X$ spanned by the selected eigenvectors in parameter space is orders of magnitude higher for $P_{\mathrm{lowrank-KFAC}}$ in CIFAR10 compared to all other methods. In ImageNet10 both low rank approximations perform well, but the subspace methods fail. 

In all of our classification experiments the performance of the subset methods is quite unsatisfying. The only acceptable approximation is obtained by $P_{\mathrm{lowrank-KFAC}}$. This trend is also reflected in the traces of the epistemic covariance matrices. Hence, none of the methods but $P_{\mathrm{lowrank-KFAC}}$ is able to select the eigenvector corresponding to the largest eigenvalues in parameter space and so to approximate $\Sigma_{P,X}$ well.


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/cifar10/logtrace.png}
        \caption{CIFAR10}
        \label{fig:cifar10_resnet9}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figures/imagenet10/logtrace.png}
        \caption{ImageNet10}
        \label{fig:imagenet10_resnet18}
    \end{subfigure}
    \caption{Evaluation with the trace criterion \eqref{eq:TraceOrderMain} for CIFAR10 and ImageNet10 and different choices of $P$. Missing values in Figure \ref{fig:imagenet10_resnet18} are due to vanishing trace values.
    }
    \label{fig:trace_high_dimensional}
\end{figure}

\section{Conclusion}

In this work we propose to look at subspace Laplace approximations of Bayesian neural networks through the lens of their predictive covariances. This approach allows us to derive the existence of an optimal subspace model via low rank techniques and yields a natural metric, the relative error, to judge the approximation quality. To make these theoretical insights practically usable we propose a subspace model that is conceptually based on the optimal solution and provide a metric that we observe empirically to correlate well with the relative error. The proposed subspace model outperforms existing methods on the studied datasets.
In fact, we observe that a well chosen method for subspace construction can often have more impact than an increase in the subspace dimension $s$.
In practice our proposed subspace model has to rely on approximations of the posterior covariance. Our experiments demonstrate that the quality of our method depends strongly on these as the different performance of $P_{\mathrm{lowrank-KFAC}}$ and $P_{\mathrm{lowrank-Diagonal}}$ illustrates.  A further restriction of our low rank based approach is its computational dependency on the number of model parameters $p$ because the projection $P \in \mathbb{R}^{p \times s}$ has to be explicitly stored. 

Even though the optimality of the projector $P_{\mathrm{lowrankopt-GGN}}$ is proven, it isn't clear that this solution is unique. If there was another optimal solution that is computationally more feasible the practicability could be improved.

\section*{Acknowledgements}

The authors would like to thank Clemens Elster for helpful discussions and suggestions. This project is part of the programme ``Metrology for Artificial Intelligence in Medicine'' (M4AIM) that is funded by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) in the frame of the QI-Digital initiative.
