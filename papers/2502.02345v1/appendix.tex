\appendix
\section{Experiments}
    \label{sec:AppendixExp}

\subsection{Architectures and Training}

The code of all experiments was developed in \texttt{PyTorch} \cite{PyTorch2019}.

The regression datasets are obtained by \texttt{OpenML} \cite{OpenML2013, OpenMLPython2019} and the expected mean $E_{y\sim p(y|x,\theta)}[y]$ is estimated by multi-layer perceptrons (MLPs) with ReLU activation functions and two hidden layers that include 128 units in each layer. The bias term is used as well. A full batch training is performed in each epoch, i.e. the batch size equals the size of the training set. The input data is normalized with respect to its mean value and its standard deviation. Further details of the architecture and training procedure are given in Table \ref{tab:MLPTrainingArchitecture}.

\begin{table}[tb!]
\centering
 \begin{tabular}{c c c c c c c c} 
 \toprule
 dataset &  $\alpha_{\text{init}}$ & $n_{\mathrm{epoch}}$ & warm up/ decay \\ 
 \midrule
  Red Wine &  $0.0004$ & 300 &  (0.3/0.3) \\
  ENB & $0.004$ & 1500 &  (0.1/0.5)  \\
  California & $0.0004$ & 100 &  (0.3/0.5)  \\
  Naval Propulsion & $0.0004$ & 100 & (0.3/0.5)  \\
 \bottomrule
 \end{tabular}
 \caption{The architecture of all networks trained on regression datasets is an MLP with two hidden layers with 128 neurons each. After each hidden layer a ReLU is used. The models are trained on $n_{\mathrm{epoch}}$ epochs with learning rate $\alpha = \alpha_{\text{init}} \frac{b}{256}$ ($b$ is the batch size which here equals the number of training data points). For the fraction of epochs `warm up' the learning rate is linearly increased to $\alpha$ and starting from the fraction of epochs `decay' the learning rate is linearly decreased.}
 \label{tab:MLPTrainingArchitecture}
\end{table}

The architecture of MNIST \cite{LeCun1998mnist} and FashionMNIST \cite{Xiao2017FashionMNIST} is a small hand-designed convolutional network (CNN) with 2d-convolutions, max-pooling, batch normalization and ReLU activation function. Before the softmax function a linear layer is applied. The exact architecture can be found in the linked code. To train the CNNs, the input data is mapped to the interval $\left[ 0,1\right]$ and then normalized with ``mean'' and ``standard deviation'' $0.5$. Additional details are given in Table \ref{tab:CNNTrainingArchitecture}.   
\begin{table}[tb!]
\centering
 \begin{tabular}{c c c c c c c c} 
 \toprule
 dataset & $\alpha$ & $n_{\mathrm{epoch}}$ & warm up/ decay \\
 \midrule
 MNIST &  $ 0.004 $ & 20 & (0.1/0.3) \\
 FashionMNIST & $0.002$ & 40 & (0.1/0.5)  \\
 \bottomrule
 \end{tabular}
 \caption{The models are trained on $n_{\mathrm{epoch}}$ epochs with learning rate $\alpha$ and batch size $b=256$. For the fraction of epochs `warm up' the learning rate is linearely increased to $\alpha$ and starting from the fraction of epochs `decay' the learning rate is linearly decreased.}
 \label{tab:CNNTrainingArchitecture}
\end{table}

CIFAR10 \cite{Krizhevsky2009cifar} and ImageNet10 \cite{ImageNet2009} are classified by ResNet architectures \cite{He2015resnet}. CIFAR10 is trained from scratch with ResNet9, but for ImageNet10 the pretrained ResNet18 from \texttt{Pytorch} with weights \texttt{IMAGENET1K\_V1} is chosen, where the last layer is replaced by a linear layer with 10 classes. During training the images are normalized with respect to their channelwise pixel mean and pixel standard deviation. In addition random flips are applied on both datasets. For CIFAR10 greyscale and random crops are used, too. More information is provided in Table \ref{tab:ResNetTrainingArchitecture}.
\begin{table}[tb!]
\centering
 \begin{tabular}{c c c c c c c c} 
  \toprule
 dataset & $\alpha$ & $n_{\mathrm{epoch}}$ &  warm up/ decay \\ 
 \midrule
 CIFAR10 &  $\ 0.004$ & 100 & (0.1/0.7) \\
 ImageNet10 & 0.0004 & 10 & (0.5/0.5)  \\
 \bottomrule
 \end{tabular}
 \caption{The models are trained on $n_{\mathrm{epoch}}$ epochs with learning rate $\alpha$ and batch size $b=256$. For the fraction of epochs `warm up' the learning rate is linearly increased to $\alpha$ and starting from the fraction of epochs `decay' the learning rate is linearly decreased.}
 \label{tab:ResNetTrainingArchitecture}
\end{table}

To evaluate the quality of the dimensional reduction, the size of the different models that are used for predictions are required. Table \ref{tab:NumParamsModel} lists the number of model parameters. The number of model parameters of the MLP and CNN has been chosen large enough such that the prediction performance is satisfying, but is also limited to be able to compute $P_{\mathrm{lowrankopt-GGN}}$.

\begin{table}[tb!]
\centering
 \begin{tabular}{c c c} 
 \toprule
 dataset & model & $p$  \\
 \midrule
  California & MLP  & 17,793 \\
  ENB & MLP  & 17,922 \\ 
  Naval & MLP  & 18,690 \\ 
  Red Wine & MLP  & 18,177 \\ 
  MNIST & CNN & 12,458 \\ 
  FashionMNIST & CNN & 12,458 \\ 
  CIFAR10 & ResNet9 & 668,234 \\ 
  ImageNet10 & ResNet18  & 11,181,642 \\ 
 \bottomrule
 \end{tabular}
 \caption{Number of trainable parameters $p$ for each model that is trained on the corresponding dataset.}
 \label{tab:NumParamsModel}
\end{table}


\subsection{ImageNet10 Classes}

ImageNet10 is a proper subset of ImageNet \cite{ImageNet2009}. The selection of classes used for ImageNet10 is given in Table \ref{tab:ImagNet10}.

\begin{table}[tb!]
\centering
 \begin{tabular}{c c c c c c c c} 
 \toprule
 label & motifs \\ [0.5ex] 
 \midrule
 \multirow{2}{*}{n01968897} & pearly nautilus, nautilus, \\
           & chambered nautilus \\
 \multirow{2}{*}{n01770081} & harvestman, daddy longlegs, \\
           & Phalangium opilio \\
 \multirow{2}{*}{n01496331} & crampfish, numbfish,\\
           &  torpedo, electric ray \\
 \multirow{2}{*}{n01537544} & indigo bunting, indigo finch, \\
           & indigo bird, Passerina cyanea \\ 
 n01818515 & macaw \\
 n02011460 & bittern \\
 n01847000 & drake \\ 
 n01687978 & agama \\
 n01740131 & night snake, Hypsiglena torquata \\ 
 n01491361 & tiger shark, Galeocerdo cuvieri \\
 \bottomrule
 \end{tabular}
 \caption{These ten labels are selected from ImageNet to construct ImageNet10.}
 \label{tab:ImagNet10}
\end{table}

\subsection{Size of Training Data Subset for Low Rank methods}
\label{subsec:size_of_training_data_subset}
For the low rank methods we construct $P$ as described in Section \ref{sec:thm_in_practice} as
\begin{align}
    \label{eq:P_from_training_data}
    P = \Psi_{\mathrm{approx}} J_{X'}^T U_s\,.
\end{align}
All three objects in \eqref{eq:P_from_training_data}, $\Psi_{\mathrm{approx}}$, $J_{X'}$ and $U_s$, are constructed from the training data. While we can take the full training data for the construction of $\Psi_{\mathrm{approx}}$, both, $J_{X'}$ and $U_s$, are constructed from a subset $X'$ of size $n$ of the training data. Ideally, we would of course like to take $X'$ to be full training data. However, doing so presents us with two difficulties:
\begin{enumerate}
    \item The object $U_s$ needs to be computable. 
    \item The computation of the product $J_{X'}^T U_s$ needs to be feasible.
\end{enumerate}
Obstacle 2 is rather straightforward to circumvent as we can compute the matrix product via mini-batches from the training data. It turns out that Obstacle 1 sets the actual limit on the subset of training data as we compute $U_s$ via an SVD of the object $J_{X'} \Psi_{\mathrm{approx}} J_{X'}^T\in \RR^{nC\times nC}$. For Red Wine and Naval we picked  $n=1000$. For ENB, the training set has only $514$ data points which is why the entire training dataset was considered. For California we could analyze the subspace models until $s=5000$ as the Jacobian of the model has full rank. To allow for this analysis we chose $n=5000$. For the classification problems, i.e. MNIST, FashionMNIST, CIFAR10 and ImageNet10, we picked $n=100$ so that we have $nC=1000$ for these datasets. This choice allowed for a substantially faster computation of $P$.
Our methods demand the explicit storage of $P$, which limits the the maximum value of $s$. Hence, we compute for ImageNet10 the submodels to a maximal dimension of $s=30$.

\subsection{Prior distribution}

For all problems the prior distribution of the full parameter $\theta\in \RR^p$ was chosen to be a centred Gaussian prior $p(\theta)=\mathcal{N}(\theta|0,\lambda^{-1})$ with prior precision $\lambda$ equal to 1.0.


\section{Existence of an Optimal Subspace Model for the
Laplace Approximation}
    \label{sec:AppendixProofSubmodel}

\begin{theorem*}[Existence of an optimal subspace model for the Laplace approximation]
Consider the problem \eqref{eq:minimization_problem} with $s\leq s_{\mathrm{max}}=\min(nC,p)$.
Suppose that $J_X\in \RR^{nC\times p}$ has full rank. For any invertible $Q\in \RR^{s\times s}$ the matrix
\begin{align*}
    P^* = \Psi J_X^\intercal U_s Q
\end{align*}
solves \eqref{eq:minimization_problem}.
 For any such $P^*$ we have
\begin{align}
    \label{eq:Appendixlow_rank_laplace_optimal_Sigma}
    \Sigma_{P^*,X} = U_s \Lambda_s U_s^\intercal.
\end{align}
\end{theorem*}

\begin{proof}
    Note that any $P^* = \Psi J_X^\intercal U_s Q$ yields
    \begin{align*}
        (P^*)^\intercal \Psi^{-1} P^* &= ( Q^\intercal U_s^\intercal J_X \Psi) \Psi^{-1} (\Psi J_X^\intercal U_s Q) \\
        &= Q^\intercal U_s^\intercal J_X \Psi J_X^\intercal U_s Q \\
        &= Q^\intercal U_s^\intercal \Sigma U_s Q
        = Q^\intercal \Lambda_s Q \,,
    \end{align*}
     where we used $\Sigma U_s =U_s \Lambda_s$ and $U_s^\intercal U_s = \mathbb{1}_s$. Putting this into \eqref{eq:Sigma_p} we obtain indeed \eqref{eq:Appendixlow_rank_laplace_optimal_Sigma}:
    \begin{align*}
        \Sigma_{P^*,X} &= J_X P^* (P^{*\intercal} \Psi^{-1} P^*)^{-1} P^{*\intercal} J_X^\intercal
        \\
        & = U_s\Lambda_s Q \left(Q^\intercal \Lambda_s Q\right)^{-1} Q^\intercal \Lambda_s U_s^\intercal 
        = U_s \Lambda_s U_s^\intercal.
    \end{align*}
  But this already shows that $P^*$ solves \eqref{eq:minimization_problem}, since any $\Sigma_{P,X}$ is of rank at most $s$, so that
  $\|\Sigma_{P,X} - \Sigma_X\|\geq \|U_s \Lambda_s U_s^\intercal - \Sigma_X\|$ due to the Eckart-Young-Mirsky theorem.
\end{proof}

Note that all we used in the proof of this Theorem were the identities \eqref{eq:Sigma_X} and \eqref{eq:Sigma_p} so that the statement of the theorem does not really require $\Psi$ to be derived via a Laplace approximation.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/mnist_long_run/error.png}
    \caption{Relative error \eqref{eq:RelativeError} of the epistemic covariance matrix of the studied subset methods for $s$ up to the number of parameters $p$ for MNIST.}
    \label{fig:MNIST_longrun_plots}
\end{figure}

\section{Trace Criterion}
    \label{sec:AppendixTraceCriterion}
Ideally we would like to choose a map $P$ such that the predictive distribution of the full \eqref{eq:PredDistrGauss}, \eqref{eq:PredDistrCat} and subspace model \eqref{eq:LaplaceSubmodel} are as close as possible. Both distributions differ only in their epistemic predictive covariance. Therefore the relative error \eqref{eq:RelativeError} is a good measure to validate the quality of $P$. However, in practice the relative error cannot be computed because the full covariance matrix is unknown.

As an alternative criterion we propose for our purposes to use the trace $\Tr \Sigma_{P,X}$ instead. This criterion is feasible to compute, aligns well with the relative error as we show empirically in Section \ref{sec:Experiment} and can be motivated by the following lemma: 

\begin{lemma}
    \label{lem:TraceOrder}
For any $P \in \mathbb{R}^{p \times s}$ we have
\begin{equation}
    \label{eq:SigmaLoewner}
    \Sigma_{P,X} \preccurlyeq \Sigma_X
\end{equation}
in the Loewner ordering, i.e. $\Sigma_X - \Sigma_{P,X}$ is positive semi-definite. In particular we have 
\begin{equation}
    \label{eq:TraceOrder}
    \Tr \Sigma_{P,X} \leq \Tr \Sigma_X \,.
\end{equation}
\end{lemma}
\begin{proof}
Due to the identities \eqref{eq:Sigma_X} and \eqref{eq:Sigma_p} it suffices to show that $ P \left(P^\intercal \Psi^{-1} P\right)^{-1}P^\intercal \preccurlyeq \Psi$, i.e. that the matrix 
\[
\Psi - P(P^\intercal\Psi^{-1} P)^{-1} P^\intercal= \Psi^{1/2} (\mathbb{1}-B) \Psi^{1/2}
\]
is positive definite, where we introduced $B=
W \left(W^\intercal W\right)^{-1} W^\intercal$ with $W=\Psi^{-1/2}P$. It's easy to check that $B$ is a projection ($B^2=B$ and $B^\intercal=B$) which thus has only eigenvalues contained in $\{0,1\}$. From this it follows that $\mathbb{1}-B$ and $\Psi^{1/2} (\mathbb{1}-B) \Psi^{1/2}$ is positive semi-definite and thus \eqref{eq:SigmaLoewner}. 

From \eqref{eq:SigmaLoewner} we obtain $\Tr(\Sigma_X - \Sigma_{P,X})=\Tr \Sigma_X - \Tr \Sigma_{P,X} \geq 0$ from which \eqref{eq:TraceOrder} follows.
\end{proof}

The relation \eqref{eq:TraceOrder} shows that $\Tr (\Sigma_X  - \Sigma_{P,X})\geq 0$ is a non-negative quantity that quantifies the closeness between $\Sigma_X$ and $\Sigma_{P,X}$.
Since $\Sigma_X$ does not depend on $P$ we can judge whether for two $P_1,P_2$ we have $\Tr(\Sigma_X - \Sigma_{P_1,X}) \geq \Tr(\Sigma_X - \Sigma_{P_2,X})$ by simply comparing whether $\Tr\Sigma_{P_1,X} \geq \Tr \Sigma_{P_2,X}$. In other words, we can take $\Tr {\Sigma_{P,X}}$ to rank the quality of different $P$. Relation \eqref{eq:TraceOrder} ensures that there is an upper bound for this quantity.
 We observe in Section \ref{sec:Experiment} empirically that a greater value of the trace implies a lower relative error, which motivates the usage of $\Tr \Sigma_{P,X}$ further. Recall that the trace is the sum of all eigenvalues of a matrix. If the trace of one approximation is greater than another one, it means that this affine subspace covers an eigenspace of greater eigenvalues. 

 \begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/enb/sorted_J_X.png}
        \caption{ENB}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/california/sorted_J_X.png}
        \caption{California}
    \end{subfigure}
    \caption{The top displays a heatmap which highlights the activity of the gradients corresponding to the parameter $\theta_i$. The parameters are sorted according to their sensitivity. A dark bluish colour implies that the gradient w.r.t. the data point $x_i$ is negligible. The lower plot summarizes the magnitude of the sensitivity over all data points.}
    \label{fig:dead_parameters}
\end{figure*}


\section{MNIST for Large $s$}
    \label{sec:AppendixClassSubsetLarge}

In Figure \ref{fig:MNIST_plots} it appears that the subset projection matrices $P_{\mathrm{subset-Magnitude}}, P_{\mathrm{subset-Diagonal}}$ and $P_{\mathrm{subset-SWAG}}$ fail to approximate the epistemic covariance matrix $\Sigma_X$ that is obtained from the Laplace approximation. However, this is misleading. In contrast, Figure \ref{fig:MNIST_longrun_plots} reveals that if a sufficient amount of parameters is selected, the subset methods approximate $\Sigma_X$ arbitrary well. This has to be expected because in the limiting case that all parameters are selected the projector for these methods is the identity map. But Figure \ref{fig:MNIST_longrun_plots} shows that the subset methods cannot provide a reliable approximation of $\Sigma_X$ for small $s$. All subset methods require more than one thousand parameters to lead to a slight improvement in the relative error and to achieve a significant reduction more than 9000 out of 12458 parameters are needed (cf. Table \ref{tab:NumParamsModel}). Hence, for a selection of few parameters all subset methods fail.


\section{Dead Parameters}
    \label{sec:AppendixDeadParam}

\begin{table}[h!]
\centering
 \begin{tabular}{c c c c c c c c} 
 \toprule
 dataset & ENB & Wine & California & Naval \\
 \midrule
  dead $p$  & $92 \pm 1\% $ & $89 \pm 2\%$ & $60\pm 2\%$ & $34\pm 4\%$ \\
 \bottomrule
 \end{tabular}
 \caption{Relative number of parameters $p$ that are insensitive to the input data with standard deviation over five seeds.}
 \label{tab:DeadParam}
\end{table}
Even though there is no guarantee that the approximated low rank methods provide better solutions as the subset methods, we would still expect that, in general, they do, because they allow for linear combinations of the parameters instead of a simple selection. In particular, all subset solutions could be found by the low rank approximations, however, the opposite isn't possible. One reason why subset methods could outperform low rank methods is that most of the parameters are irrelevant for a certain problem, i.e. have a gradient of zero w.r.t. the input. Indeed, Table \ref{tab:DeadParam} confirms this hypotheses, because the number of insensitive parameters positively correlates with an improved performance of the subset methods compared to the low rank methods. ENB is the only experiment in which the selection subspace models are superior to the low rank subspace models, but it also the model with most insensitive parameters. Further, for California or Naval Propulsion low rank approximations clearly outperform subset approximations (cf. Figure \ref{fig:regression_plots}).

This effect is visualized in Figure \ref{fig:dead_parameters}. The top displays a heatmap that highlights the sensitivity of parameters (the gradient w.r.t. the input) for a certain data point. Light colours denote high sensitivity and dark colours low sensitivity. Below the average gradient of all data points w.r.t. a certain parameter is shown. Both plots indicate that only a few parameters are responsive for most data points. According to Table \ref{tab:DeadParam}, for ENB the used neural network has the least amount of sensitive parameters. If a subset method can capture these parameters, it shall perform well. In contrast, for California the sensitivity is more spread and hence, a linear combination could be more appropriate.  


\section{NLL}
    \label{sec:AppendixNLL}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/enb/nll.png}
        \caption{ENB}
        \label{subfig:nll_enb}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/redwine/nll.png}
        \caption{Red Wine}
        \label{subfig:nll_redwine}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mnist/nll.png}
        \caption{MNIST}
        \label{subfig:nll_mnist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cifar10/nll.png}
        \caption{CIFAR10}
        \label{fig:nll_cifar10}
    \end{subfigure}
    
    \vspace{0.1cm} 
    
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/california/nll.png}
        \caption{California}
        \label{fig:nll_california}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/navalpro/nll.png}
        \caption{Naval Propulsion}
        \label{fig:nll_navalpro}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/fashionmnist/nll.png}
        \caption{FashionMNIST}
        \label{subfig:nll_fashionmnist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/imagenet10/nll.png}
        \caption{ImageNet10}
        \label{fig:nll_imagenet10}
    \end{subfigure}
    
    \caption{The NLL metric \eqref{eq:nll} for the datasets and subspace models considered in this work. The colour and linestyle coding is identical to the one in Figure \ref{fig:regression_plots}.}
    \label{fig:nll_plots}
\end{figure*}
    
The NLL (negative log-likelihood) is a common metric used in the literature \cite{hernandez2015probabilistic,lakshminarayanan2017simple,kendall2017uncertainties,maddox2019simple,Izmailov2019,Daxberger2021} to evaluate uncertainties associated with the predictions of (Bayesian) neural networks.
The NLL metric is actually the averaged negative logarithm of the posterior predictive distribution \eqref{eq:pred_distribution} on the test data, that is
\begin{align}
    \label{eq:nll}
    \mathrm{NLL} = - \frac{1}{N_{\mathrm{test}}}\ln p(Y_{\mathrm{test}}| X_{\mathrm{test}}, \mathcal{D}) \,,
\end{align}
where $X_{\mathrm{test}}$ and $Y_{\mathrm{test}}$ denote the inputs and labels for the $N_{\mathrm{test}}$ test data points. While the NLL is easy to compute for most uncertainty evaluations, there is some criticism that it is not really measuring the real objective but rather something different \cite{Yao2019,Deshpande2024loglikelihood}. Our observations fall in line with these arguments. 

Figure \ref{fig:nll_plots} shows the results of the NLL for all datasets considered in this work. Recall that a lower NLL is supposed to indicate a superior model. Following this logic most plots in Figure \ref{fig:nll_plots} would indicate a \textit{reverse} ranking of the subspace models compared to the one observed with the relative error and trace criterion in Figure \ref{fig:regression_plots} and Figure \ref{fig:MNIST_plots}. One might argue, that this could demonstrate that the relative error and trace criterion are unsuitable for evaluating our models. 
However, it seems unlikely that a criterion such as the relative error that uses information of the full model yields an inferior evaluation of the considered models as a criterion such as the NLL that does not. Moreover, there are two observation in Figure \ref{fig:nll_plots} that raises considerable doubt on the NLL ranking:

\begin{enumerate}
    \item First, note that for most models the NLL rises with increasing $s$. In other words, the NLL evaluates subspace models that use less parameters as better. 
    \item Second, the full model has the highest NLL value. In other words the NLL ranks it as the worst performing model, whereas the models that approximate it perform better under this metric.
\end{enumerate}
It seems rather implausible that an approximated object yields preciser estimates than the object which it approximates. We feel therefore save to conclude that the ranking obtained via the NLL is unsuitable for our purposes.

Observation 1 was not made in \cite{Daxberger2021}, which is the only reference we could find with a comparable plot of the NLL over a range of $s$. We found that their scaling of the prior precision $\lambda_s = \frac{s}{p} \lambda$ can lead to a decrease of the NLL but this seems to root in the effect that this scaling tends to decrease the full posterior variance with increasing $s$ (as can be observed, e.g., via the trace of the posterior predictive variance).

To better understand why a misleading behaviour of the NLL as in Observation 1 can occur, let us look at a 1D regression problem (1D input and 1D output) with homoscedastic noise.
To simplify the theoretical discussion let us further assume an input independent  epistemic predictive covariance $\Sigma \geq 0$ (which is a scalar for the considered problems). The NLL is then given by
\begin{align*}
    \mathrm{NLL}(\Sigma)  =  & \frac{1}{2N_{\mathrm{test}}(\sigma^2+\Sigma)} \sum_{i=1}^{N_{\mathrm{test}}} (y_i-f_{\hat{\theta}}(x_i))^2  \\
    & + \frac{1}{2} \ln (2\pi (\Sigma+\sigma^2)).
\end{align*} 

It is easy to check that this is a concave function with a global minimum that is dependent on the MSE on the test data:
\begin{align}
    \label{eq:minimum_nll_regression}
    \argmin_{\Sigma} \mathrm{NLL}(\Sigma) = \mathrm{MSE}_{\mathrm{test}} - \sigma^2 \,.
\end{align}
In standard problems the label noise $\sigma^2$ is unknown and needs to be estimated. The standard way of doing this \cite{Gal2016,Daxberger2021,Izmailov2019} is to learn $\sigma$ as an extra parameter while training. But this leads to an estimate $\hat{\sigma}^2 = \mathrm{MSE}_{\mathrm{train}} \simeq \mathrm{MSE}_{\mathrm{test}}$ (provided there is no substantial overfitting). As a consequence the NLL \eqref{eq:minimum_nll_regression}, computed with $\hat{\sigma}$ instead of $\sigma$, will due to \eqref{eq:minimum_nll_regression} obtain its minimum around $\Sigma=0$. In other words, independent of the problem, fit quality and actual model error, the NLL will rank smaller model uncertainties better. 

To exemplify this, Figure \ref{fig:synthetic_example} shows the results for a synthetic regression dataset with regression function $g(x)=\sin(x/4)\cdot \cos(x/2)$ (Fig. \ref{subfig:synthetic_dataset})  and noise $\sigma=0.1$. In Figure \ref{subfig:nll_synthetic_estimated_sigma} the NLL metric is plotted for the methods used in this work when $\sigma$ is estimated ($\hat{\sigma}\simeq 0.23$). We recognize the familiar rise of the NLL with increasing $s$ already observed in Figure \ref{fig:nll_plots}. When the true $\sigma$ is used instead, the behaviour gets more complicated as can be seen in Figure \ref{subfig:nll_synthetic_true_sigma}. Figure \ref{subfig:nll_synthetic_true_sigma_long_range} shows the behaviour of the subset methods over a longer range of $s$. In Figure \ref{subfig:nll_synthetic_true_sigma} and \ref{subfig:nll_synthetic_true_sigma_long_range} we see the concave behaviour of the NLL postulated above. The NLL reaches a minimum before it rises to the NLL of the full model. The studied low rank and subset methods achieve a similar minimal value for the NLL, but at different $s_0$ that depend on the chosen method. Observation 2 still holds and the full model is outperformed by its approximations. This indicates that even when $\sigma$ is known the usage of the NLL for the assessment of subspace models as studied in this work is questionable.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Synthetic/pred1.png}
        \caption{Synthetic dataset}
        \label{subfig:synthetic_dataset}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Synthetic/nll_stdestimated.png}
        \caption{NLL with estimated $\hat{\sigma}$}
        \label{subfig:nll_synthetic_estimated_sigma}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Synthetic/nll_st01_60.png}
        \caption{NLL with true $\sigma$}
        \label{subfig:nll_synthetic_true_sigma}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.22\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Synthetic/nll_std01_subset.png}
        \caption{NLL with true $\sigma$}
        \label{subfig:nll_synthetic_true_sigma_long_range}
    \end{subfigure}
    \caption{Figure \ref{subfig:synthetic_dataset} visualizes the prediction quality of the parametric function $f_{\theta}$ in comparison to the true curve $g$. The following Figures \ref{subfig:nll_synthetic_estimated_sigma} - \ref{subfig:nll_synthetic_true_sigma} study the NLL for different data variances. In Figure \ref{subfig:nll_synthetic_estimated_sigma} the NLL for the estimated data variance $\hat{\sigma} > \sigma = 0.1$ is studied. All NLL curves increase with $s$. Adding the epistemic covariance increases the total variance, however, this leads to an increase to the NLL. In contrast, if the true $\sigma$ is taken the NLL is concave. Colour and line encoding are the same as in Figure \ref{fig:regression_plots}.}
    \label{fig:synthetic_example}
\end{figure*}

\section{Fisher Information, Generalized Gauss Newton and Hessian}
\label{sec:FI_GGN_and_H}

The Fisher information matrix, generalized Gauss-Newton matrix and Hessian are closely related and in certain situations they are even equivalent. We summarize some of these relations, but for a more detailed analyses we refer to the excellent survey \cite{Martens14}.

In supervised machine learning the data is usually distributed by a joint distribution $q(x, y) = q(y| x) q(x)$ which is often unknown. Only the empirical data distribution $\hat{q}(x, x) = \hat{q}(y|x) \hat{q}(x)$ is given in form of samples. The task of supervised machine learning is to learn a parametric distribution $p(x, y | \theta) = p(y |x, \theta) q(x)$ that approximates $q(x, y)$. Since only the conditional distribution $p(y |x, \theta)$ is learned, $q(x, y)$ and $p(x, y | \theta)$ have the same marginal distribution in $x$. 

\subsection{Fisher Information}
\label{sec:FI}

\subsubsection{Multivariate Regression}

\[
    p(y_i | x_i, \theta) = \frac{1}{\sqrt{(2 \pi)^{C} \det(\Sigma)}}
    e^{- \frac{1}{2} \norm{y_i - f(x_i, \theta)}^2_{\Sigma^{-1}}}
\]
is a common choice to model multivariate regression problems. For simplicity we assume that the covariance matrix $\Sigma \in \mathbb{R}^{C\times C}$ is independent of the parameter $\theta$. The explicit form of the information matrix for a single input $x_i$ is 
\eqn{
    \mathcal{I}_{kl}(x_i) & = \frac{1}{2}
    \E{y \sim p(y | x_i, \theta)}{\partial_{\theta_k}\partial_{\theta_l} \norm{y_i - f_i}^2_{\Sigma^{-1}}} \\
    & = \sum_{c_1,c_2=1}^C \partial_{\theta_k} f_i^{c_1} \left(\Sigma^{-1}\right)^{c_1 c_2} \partial_{\theta_l} f^{c_2}_i \\
    & = \left(\left(\nabla_{\theta} f_i\right)^{\intercal} \Sigma^{-1} \nabla_{\theta} f_i \right)_{kl}\,. 
}{eq:IMultiRegr}
The abbreviation $f_i=f_{\theta}(x_i)$ is used for readability. 
For $\Sigma = \sigma^2 \mathbb{1}$ (as in this work), we obtain 
\eqnn{
    \mathcal{I}_{kl}(x_i) = \sigma^{-2}\left(\left(\nabla_{\theta} f_i\right)^{\intercal} \nabla_{\theta} f_i \right)_{kl} \,.
}
For the Fisher information matrix of the joint distribution we arrive at
\eqnn{
    \mathcal{I}_{kl} & = \frac{1}{2}
    \E{(x, y) \sim p(y,  x | \theta)}{\partial_{\theta_k}\partial_{\theta_l} \norm{y_i - f_i}^2_{\sigma^{-2}\mathbb{1}}} \\
    & \simeq \frac{\sigma^{-2}}{N} \sum_{i=1}^N \left(\left(\nabla_{\theta} f_i\right)^{\intercal} \nabla_{\theta} f_i \right)_{kl}\,. 
}
where in the last line $q(x)$ is approximated by $\hat{q}(x)$.

\subsubsection{Softmax Classifier}

For classification we consider the categorical distribution $y| x, \theta \sim \mathrm{Cat}(y | \phi(f_{\theta}(x))$ with probability vector
\[
    \phi^c_i = \phi^c(f_{\theta}(x_i)) 
    = \frac{e^{f_\theta^c(x_i)}}{\sum_{\tilde{c}= 1}^C e^{f^{\tilde{c}}_\theta(x_i)}}
    = \frac{e^{f^c_i}}{\sum_{\tilde{c}= 1}^C e^{f^{\tilde{c}}_i}}.
\]
The general form of the Fisher information matrix is given by 
\eqnn{ 
    \mathcal{I}_{kl} 
    & = \E{(x, y) \sim p(x, y | \theta)}{\partial_{\theta_k} \ln p(x, y | \theta) \partial_{\theta_l} \ln p(x, y | \theta)}  \\
    & = \E{y \sim p(y | x, \theta), x \sim q(x)}{\partial_{\theta_k} \ln p(y|x, \theta) \partial_{\theta_l} \ln p(y|x, \theta)}  \\
    & \simeq \frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C \phi^c_i \partial_{\theta_k} \ln \phi^c_i \partial_{\theta_l} \ln \phi^c_i  \\
    & = \frac{4}{N} \sum_{i=1}^N \sum_{c=1}^C  \partial_{\theta_k} \sqrt{\phi^c_i} \partial_{\theta_l} \sqrt{\phi^c_i} \,,
}
where in the third line the empirical distribution $\hat{q}(x)$ is used to compute the expected value of the random variable $x$.

\subsection{Relation Between Hessian and Fisher Information Matrix}
\label{sec:FI_and_H}
Given the averaged log-likelihood $\frac{1}{N} \sum_{i} \ln p(y_i | f_{\theta}(x_i))$ of the data its Hessian w.r.t $\theta$ can be written as
\eqn{
    H & = - \frac{1}{N} \sum_{i=1}^N \nabla^2_{\theta}\ln p(y_i | f_{\theta}(x_i)) \\
    & = \E{(x,y) \sim \hat{q}(x, y)}{H_{\text{-}\ln p(y | x, \theta)}} \\
    & = \frac{1}{N} \sum_{i=1}^N \E{y \sim \hat{q}(y| x_i)}{H_{\text{-}\ln p(y | x_i, \theta)}} \,,
}{eq:HessianSimilarFisher}
where we wrote $H_{\text{-}\ln p(y | x, \theta)} = - \nabla^2_{\theta}\ln p(y | f_{\theta}(x))$. 

The Fisher information matrix $\mathcal{I}$ of $p(x, y | \theta)$ w.r.t the parameter $\theta$ is
\eqnn{
    \mathcal{I} & = \E{(x,y) \sim p(x, y | \theta)}{\nabla_{\theta} \ln p(x, y | \theta)^\intercal \nabla_{\theta} \ln p(x, y | \theta)} \\
    & = \E{y \sim p(y | x, \theta), x \sim q(x)}{\nabla_{\theta} \ln p(y | x, \theta)^\intercal \nabla_{\theta} \ln p(y | x, \theta)} \\
    & = - \E{y \sim p(y | x, \theta), x \sim q(x)}{\nabla_{\theta}^2 \ln p(y | x, \theta)} \\
    & = \E{y \sim p(y | x, \theta), x \sim q(x)}{H_{\text{-}\ln p(y | x, \theta)}} \,.
}
Since $q(x)$ is not analytically known, we shall use the empirical distribution $\hat{q}(x)$ instead.
\eqn{
    \mathcal{I} = \frac{1}{N} \sum_{i=1}^N \E{y \sim p(y | x=x_i, \theta)}{H_{\text{-}\ln p(y | x, \theta)}} \,.
}{eq:FisherSimilarHessian}
The equations \eqref{eq:FisherSimilarHessian} and \eqref{eq:HessianSimilarFisher} are quite similar. The difference is the distribution under which the expectation is computed. However, note that \eqref{eq:HessianSimilarFisher} and \eqref{eq:FisherSimilarHessian} are different from the empirical Fisher information matrix
\eqnn{
    \mathcal{I_{\mathrm{empirical}}} & = \E{y \sim \hat{q}(x, y | \theta)}{\nabla_{\theta} \ln p(x, y | \theta)^\intercal \nabla_{\theta} \ln p(x, y | \theta)} \\
    & = \frac{1}{N} \sum_{i=1}^N \nabla_{\theta} \ln p(y_i | x_i,\theta)^\intercal \nabla_{\theta} \ln p(y_i | x_i, \theta).
}

\subsection{Relation Between Hessian and Generalized Gauss-Newton Matrix}
\label{sec:GGN_and_H}

The generalized Gauss-Newton matrix is often used as a substitute of the Hessian because it is positive semi-definite and easier to compute \cite{Martens14}. For generalized linear models both quantities coincide. Let us write the Jacobian w.r.t. the log-likelihood as $\nabla_{\theta} \ln p(y_i | f_{\theta}(x_i)) =  \nabla_{f_i} \ln p(y_i | f_i) \nabla_{\theta} f_{\theta}(x_i)=  \nabla_{f_i} \ln p(y_i | f_i)J_{f_i}$ and $H_{f_i^c} = \nabla_{\theta}^2 f_{\theta}(x_i)^c$ for $1\leq c\leq C$. Then the Hessian can be decomposed into
\eqn{
    H  = &\frac{-1}{N} \sum_{i=1}^N \Big(J_{f_i}^\intercal \nabla_{f_i}^2 \ln p(y_i | f_i) J_{f_i}   \\
     & + \sum_{c=1}^C H_{f_i^c} \partial_{f_i^c} \ln p(y_i | f_i) \Big)\\
     = &H_{\mathrm{GGN}} - \frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C H_{f_i^c} \partial_{f_i^c} \ln p(y_i | f_i)
}{eq:HessinaSimilarGGN}
with the generalized Gauss-Newton matrix 
\eqn{
    H_{\mathrm{GGN}} & = - \frac{1}{N} \sum_{i=1}^N J_{f_i}^\intercal \nabla_{f_i}^2 \ln p(y_i | f_i) J_{f_i} \\
    & = \frac{1}{N} \sum_{i=1}^N J_{f_i}^\intercal H_{\text{-}\ln p(y_i | {f_i})} J_{f_i}\,.
}{eq:AppendixGGN}
A sufficient condition that the generalized Gauss-Newton matrix and the Hessian coincide is that the model is linear, because for linear models $H_{f_i^c} = 0$ for $1\leq c \leq C$. In the definition of the generalized Gauss-Newton matrix a choice about where the cut between the loss and the network function has to be made. This is to some degree arbitrary, however, \cite{Schraudolph2002} recommends to perform as much as possible of the computation in the loss such that $\ln p(y_i | f)$ is still convex to ensure positive semi-definiteness of $H_{\mathrm{GGN}}$.

\subsection{Relation Between Fisher Information Matrix and Generalized Gauss-Newton Matrix}
\label{sec:FI_and_GGN}

Rewriting $\nabla_{\theta} \ln p(y_i | f_{\theta}(x_i)) = \nabla_{f_i} \ln p(y_i | f_i)  J_{f_i}$ the Fisher information matrix is of the form 
\eqn{
    \mathcal{I} & = \E{y \sim p(y | x, \theta), x \sim q(x)}{\nabla_{\theta} \ln p(y | x, \theta)^\intercal \nabla_{\theta} \ln p(y | x, \theta)} \\
    & = \E{x}{J_f^\intercal \E{y}{\nabla_f \ln p(y | f)^\intercal \left( \nabla_f \ln p(y | f)\right)} J_f} \\
    & := \E{x}{J_f^\intercal \mathcal{I}_{\ln p(y|f)} J_f} \,,
}{eq:FisherSimilarGGN}
where we write shorthand $f=f_\theta(x)$ and
\eqnn{
    \mathcal{I}_{\ln p(y|f)} & = \E{y}{\nabla_f \ln p(y | f)^\intercal \nabla_f \ln p(y | f)} \\
    & = - \E{y}{\nabla_f^2 \ln p(y | f)} \\
    & = \E{y}{H_{\text{-}\ln p(y | f)}}
}
is the ``Fisher information matrix of the predictive distribution''. 

From these two identities it easily follows that if we substitute $q(x)$ by its empirical distribution $\hat{q}(x)$, the generalized Gauss-Newton matrix \eqref{eq:AppendixGGN} is identical to the Fisher information matrix \eqref{eq:FisherSimilarGGN} if $H_{\text{-}\ln p(y|f)}$ is constant in $y$. This is the case for squared error loss and cross-entropy loss \cite{Heskes2000,Pascanu2013,Martens14}. Indeed, for squared error loss we have
\[
     H_{\text{-}\ln p(y|f)} = \nabla_f^2 \frac{1}{2} \norm{f - y}^2_{\Sigma^{-1}} = \Sigma^{-1}
\]
and for cross-entropy loss we obtain
\eqnn{
     H_{\ln p(y|f);c^\prime c^{\prime\prime}} & = \partial_{f^{c^\prime}} \partial_{f^{c^{\prime\prime}}} \sum_{c=1}^C y^c \ln \phi^c \\
    &  = \partial_{f^{c^\prime}} \partial_{f^{c^{\prime\prime}}} \sum_{c=1}^C y^c \ln \frac{e^{f^c}}{\sum_{{\tilde{c}}=1}^C e^{f^{\tilde{c}}}} \\
     & = \partial_{f^{c^\prime}} \partial_{f^{c^{\prime\prime}}} \left(\sum_{c=1}^C y^c f^c - \sum_c y^c \ln \sum_{{\tilde{c}}=1}^C e^{f^{\tilde{c}}}  \right) \\
    & = \partial_{f^{c^\prime}} \partial_{f^{c^{\prime\prime}}} \left(\sum_{c=1}^C y^c f^c - \ln \sum_{{\tilde{c}}=1}^C e^{f^{\tilde{c}}}  \right) \\
    & = - \partial_{f^{c^\prime}} \partial_{f^{c^{\prime\prime}}} \ln \sum_{{\tilde{c}}=1}^C e^{f^{\tilde{c}}}
    = - \partial_{f^{c^{\prime\prime}}} \phi^{c^\prime} \\
    & = - \delta_{c^{\prime}c^{\prime\prime}} \phi^{c^{\prime}} + \phi^{c^{\prime}} \phi^{c^{\prime\prime}} \,,
}
which are both constant in $y$.


