\section{Related work}
\label{sec:related}
\noindent\textbf{Visual Question Answering (VQA)} involves answering natural language questions by interpreting visual content~\cite{antol2015vqa,Naik_2023_ICCV,Li_2024_CVPR}. Recent approaches leverage vision-language models (VLMs) built on transformer architectures~\citep{fields2023vision, shi2024non, xiao2022video, lei2021less, cheng2023vindlu} alongside pre-trained language models~\citep{yang2022zero, yu2023self}. For instance, \citet{cho2021unifying} proposed a generative transformer for VQA that analyses visual elements in conjunction with textual questions. Many methods aim to improve generalisation capabilities by enhancing compositionality, which is essential for cognitive reasoning~\citep{keysers2019measuring, lake2017building}. For instance, \citet{johnson2017clevr} explored the composition of visual attributes by creating a dataset designed for compositional reasoning, while \citet{whitehead2021separating} used contrastive learning to enhance compositionality, disentangling reasoning skills from visual concepts. Although these approaches make strides toward compositionality, the implicit decomposition may impact generalisation, and constructing effective contrastive samples remains complex.
 
\noindent \textbf{Continual Learning (CL)} aims to develop frameworks capable of incrementally learning from sequentially arriving datasets. This is a fundamental challenge for many deep learning methods due to catastrophic forgetting (CF)~\citep{mcclelland1995there}. CL methods can be divided into different categories. While \emph{knowledge distillation}-based methods constrain mappings between incremental models to prevent forgetting \citep{li2017learning, icarl, 8953661, 9880426, douillard2020podnetpooledoutputsdistillation,Kang2022afc}, \emph{optimisation}-based techniques modify the gradient updates during training to minimise interference with previously learnt tasks to mitigate forgetting~\cite{lopez2017gradient,chaudhry2018efficient, saha2021gradient, Yang_2023_ICCV}, and \emph{representation}-based methods focus on learning robust and adaptable feature representations~\cite{gao2023unified,foret2020sharpness,ermis2022memory,douillard2022dytox}. Recently we have also witnessed the uprisal of \emph{prompt}-based methods like~\citep{wang2022learning, wang2022dualprompt, wang2022s, smith2023coda}, which utilise visual prompts~\citep{liu2023pre} with pre-trained transformers for CL scenarios. Most of these methods target Class-Incremental Learning (CIL) in unimodal settings; as such, they cannot simultaneously tackle multimodal continuous data and ignore the essential composition generalisation issue for VQA~\citep{zhang2023vqacl, nikandrou2022task}. Our work falls within the distillation-based category, as \qstmethodshort{} enables distillation with only past questions to reduce CF for continual VQA.

\noindent \textbf{Continual VQA.} Recent research has increasingly focused on multimodal CL in the context of VQA~\citep{del2020ratt, greco2019psycholinguistics, nikandrou2022task, srinivasan2022climb}. While studies like~\citet{greco2019psycholinguistics} have examined forgetting dynamics in VQA, they often focus on a limited set of tasks and overlook the potential of pre-trained models to mitigate forgetting~\citep{mehta2021empirical}. The VQACL benchmark~\citep{zhang2023vqacl} evaluated conventional continual learning approaches such as ~\citep{chaudhry2019tinyepisodicmemoriescontinual, Chaudhry2019er, Wan_2022_CVPR, zhang2023vqacl}. Some works have explored specific aspects of continual VQA, such as question types~\citep{greco2019psycholinguistics}, domain adaptation~\citep{zhang-etal-2022-continual}, and compositionality~\citep{zhang2023vqacl}. Our work introduces a distillation method for continual VQA while remaining memory-efficient, privacy-conscious, and without any stored prototypes as in the VQACL method\footnote{VQACL represents both the setting and the approach for continual learning in Visual Question Answering, as described in ~\citep{zhang2023vqacl}.}~\citep{zhang2023vqacl}.