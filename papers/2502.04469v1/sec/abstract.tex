\begin{abstract}
Continual Learning in Visual Question Answering (VQACL) requires models to learn new visual-linguistic tasks (plasticity) while retaining knowledge from previous tasks (stability). The multimodal nature of VQACL presents unique challenges, requiring models to balance stability across visual and textual domains while maintaining plasticity to adapt to novel objects and reasoning tasks. Existing methods, predominantly designed for unimodal tasks, often struggle to balance these demands effectively. In this work, we introduce \textit{QUestion-only replay with Attention Distillation} (\qstmethodshort{}), a novel approach for VQACL that leverages only past task questions for regularization, eliminating the need to store visual data and addressing both memory and privacy concerns. \qstmethodshort{} achieves stability by introducing a \textit{Question-only Replay} mechanism that selectively uses questions from previous tasks to prevent overfitting to the current taskâ€™s answer space, thereby mitigating the \textit{out-of-answer-set problem}. Complementing this, we propose \textit{Attention Consistency Distillation}, which uniquely enforces both intra-modal and inter-modal attention consistency across tasks, preserving essential visual-linguistic associations. Extensive experiments on VQAv2 and NExT-QA demonstrate that \qstmethodshort{} significantly outperforms state-of-the-art methods, achieving robust performance in continual VQA\footnote{Work done during an internship at Computer Vision Center (CVC), Universitat Autonoma de Barcelona.}. Code is available at: \url{https://github.com/IemProg/QUAD}
\end{abstract}