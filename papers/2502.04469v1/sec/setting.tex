\subsection{Setting Overview}
\label{sec:setting}
%\noindent\textbf{Setting Overview. }
The VQACL setting~\citep{zhang2023vqacl} is designed to assess the capability of a model to adapt to a sequence of tasks, each involving both visual and linguistic inputs, in a continual learning environment. It approaches VQA as a generative task, where the objective is to generate textual answers given an image and a corresponding question~\citep{ghosh2024exploringfrontiervisionlanguagemodels, zhang2023vqacl}. The model encounters a non-stationary stream of data, requiring it to learn and adapt incrementally over time without revisiting prior data. We consider a sequence of $T$ tasks, denoted as $ \mathcal{T}^1, \mathcal{T}^2, \ldots, \mathcal{T}^T$. Each task $\mathcal{T}^t$ is characterized by a set of image-question-answer triplets $(x^t, q^t, y^t)$, where $x^t \in \mathcal{X}^t$ denotes the image, $q^t \in \mathcal{Q}^t$ represents the question, and $y^t \in \mathcal{Y}^t$ corresponds to the answer.\footnote{The sample index is omitted for clarity.} The challenge is to train a model $\phi$ that can effectively learn the current task $\mathcal{T}^t$ while retaining the knowledge from all previous tasks $\{\mathcal{T}^1, \mathcal{T}^2, \ldots, \mathcal{T}^{t-1}\}$. 

In VQACL, the sequence of tasks is organized as a \emph{series of \( L \) macro-tasks}, each comprising \( K \) sub-tasks, resulting in a total of \( T = L \times K \) tasks. Each macro-task is designed to develop specific reasoning skills such as counting, color identification, or object recognition (\textit{i.e} linguistic task). For example, in a counting task, the model primarily engages with questions like ``\textit{How many objects are there?}'' or ``\textit{What number is shown?}''.

Each linguistic macro-task is further divided into visually-driven sub-tasks. Formally, each macro-task \( \mathcal{T}^t \) is split into \( K \) visually-driven sub-tasks, \( \{\mathcal{S}^t_1, \mathcal{S}^t_2, \ldots, \mathcal{S}^t_K\} \), which are learned sequentially. These sub-tasks \( \mathcal{S}^t_k \) are constructed by grouping the \( C \) distinct visual object categories, \( \{c_i\}_{i=1}^C \), into \( K \) sets. This hierarchical structure mirrors the continuous nature of the visual and linguistic data streams that the model processes. 
The VQACL setting introduces two unique challenges for continual learning models. \textit{1) Knowledge Retention}: as the model progresses through the sequence of tasks, it must retain knowledge from earlier tasks to perform well on future tasks, where both visual and linguistic modalities must be preserved. \textit{2) Generalization to Novel Compositions}: this setting also evaluates the model's ability to generalize to novel combinations of visual concepts and reasoning skills that it has not encountered during training. This aspect is crucial for real-world applications where new object-skill combinations are frequently encountered. Details of task sequences, object groupings, and novel composition testing is in the Appendix.
