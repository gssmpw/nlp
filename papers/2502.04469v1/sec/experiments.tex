\section{Experiments}
\label{sec:experiments}

\subsection{Experimental setup}
\noindent\textbf{Implementation Details. }To ensure a fair comparison, we adopt the protocol of \citep{zhang2023vqacl} for both feature extraction and training across datasets. For the visual embeddings, we use a Faster R-CNN~\citep{faster_rcnn} trained on the Visual Genome dataset~\citep{Krishna2016VisualGC} to obtain 36 region-based features per image in the VQAv2 dataset. For videos in the NExT-QA dataset, we extract clip-level motion features using an inflated 3D ResNeXt-101~\citep{hara3dcnns}, setting $n = 16$ regions per clip. These visual features are adapted through a two-layer MLP with GELU activation, preparing them as inputs to the transformer model. Our transformer backbone, based on T5~\citep{2020t5}, consists of 12 blocks for both encoder and decoder modules, each containing 12 attention heads. The embedding dimension $d$ is tailored to the task-specific requirements. Training is conducted for 3 epochs per task, with a batch size of 80. We utilize the Adam optimizer~\citep{KingBa15} with an initial learning rate of $10^{-4}$. $\lambda$ is set to $0.5$ in all experiments. All implementations are based on PyTorch~\citep{pytorch}. Unlike VQACL~\citep{zhang2023vqacl}, \qstmethodshort{} does not create or store any prototypes for questions or visual objects.


\noindent\textbf{Evaluation Metrics. }We utilise two established metrics for continual learning~\citep{zhang2023vqacl, Chaudhry_2018, lopezpaz2022gradientepisodicmemorycontinual}: final average performance ($AP$), and average forgetting ($Forget$). The $AP$ metric reflects the model's overall performance across all learnt tasks, highlighting its ability to consistently acquire new tasks. Let $a_{i,j}$ represent the performance of the model on task $\mathcal{T}^i$ after it has completed learning this task $\mathcal{T}^j$. Then, $AP$ is calculated as: $AP\!=\!\frac{1}{T} \sum_{t=1}^{T} a_{t,T}.$
Additionally, the $Forget$ metric quantifies the performance loss in previous tasks as new tasks are learnt and is computed as:
$Forget\!=\!\frac{1}{T-1} \sum_{t=1}^{T-1} \max_{z \in \{t, \ldots, T-1\}} (a_{t,z} - a_{t,T}).$ To ensure a fair comparison, we calculate $a_{i,j}$ in the NExT-QA dataset as per the approach in \citep{zhang2023vqacl, xiao2021next}, utilising Wu-Palmer Similarity (WUPS) \citep{wups} to assess the quality of the generated answers. For the VQAv2 dataset, following the method outlined in \citep{zhang2023vqacl}, we use the percentage of correctly answered questions as the value for $a_{i,j}$.

\noindent\textbf{Baselines. }To rigorously evaluate the performance of our method, we conduct a comprehensive analysis encompassing five established and state-of-the-art continual learning methods. These methods include two regularisation-based approaches: Elastic Weight Consolidation (EWC)~\citep{kirkpatrick2017overcoming} and Memory Aware Synapses (MAS)~\citep{mas2018}, as well as three rehearsal-based approaches: Experience Replay (ER)~\citep{chaudhry2019tinyepisodicmemoriescontinual}, Dark Experience Replay (DER)~\citep{Chaudhry2019er}, Virtual Sample (VS)~\citep{Wan_2022_CVPR}, and VQACL~\citep{zhang2023vqacl} (see details in Appendix). To contextualise the performance of these methods, we establish two performance boundaries: a lower bound (Vanilla) that performs simple gradient updates without any forgetting mitigation strategies, and an upper bound (Joint) that trains on all tasks simultaneously, providing an upper bound performance metric. Following the evaluation protocol established in VQACL~\citep{zhang2023vqacl}, we employ two key evaluation strategies: standard testing, which evaluates the models' performance on previously encountered task types, assessing their ability to retain learnt knowledge; and novel composition testing, which challenges the models with previously unseen combinations of visual and linguistic elements, probing their capacity for compositional  generalization. This dual-pronged evaluation strategy, adopted from the original VQACL work, provides a nuanced understanding of each method's efficacy in balancing knowledge retention and adaptive learning in the VQACL.

\input{sec/tables/novel_tab}

\subsection{Main results}
\label{sec:mainresults}
\noindent\textbf{Performance Analysis On Standard Setting. }Tab.~\ref{tab:model_performance} shows a detailed comparison of continual learning approaches in the VQACL setting, with our proposed \qstmethodshort{} achieving superior performance across both standard and novel composition tests. \qstmethodshort{} consistently outperforms other methods in AP and forgetting demonstrating robust knowledge retention. On VQAv2, \qstmethodshort{} attains an AP of 39.25\% in standard testing, outperforming the best rehearsal-based approach (VQACL) by 1.79\%. Similarly, in NExT-QA, \qstmethodshort{} achieves an AP of 31.70\%, exceeding competing methods by 0.84\% to 4.73\%. The forgetting rates for \qstmethodshort{} are also the lowest, with 4.91\% and 2.91\% for VQAv2 and NExT-QA, respectively, underscoring its capacity for stable knowledge retention.

In novel composition testing, \qstmethodshort{} demonstrates strong generalization, achieving top AP scores of 40.00\% on VQAv2 and 33.85\% on NExT-QA. Notably, despite not storing images, our approach maintains high performance in novel settings, with a minimal performance gap between standard and novel composition tests (0.75\% for VQAv2 and 0.64\% for NExT-QA). This narrow gap indicates that distilling knowledge using only seen questions effectively reinforces visual-linguistic associations, enabling the model to generalise well even in unfamiliar contexts. This performance underscores \qstmethodshort{}'s capacity to construct robust, adaptable representations, showing that question-only distillation can successfully support compositional reasoning without the need for visual memory.


%Our question-only variant, \qstmethodshort{}, also achieves competitive results, especially on VQAv2, where it yields AP scores of 39.25\% and 40.00\% in standard and novel composition tests, surpassing methods that utilize a full memory buffer (ER, DER, VS). This shows that question-only distillation can perform effectively in VQACL with reduced memory. 

%However, \qstmethodshort{} shows lower performance on the more challenging NExT-QA, particularly in novel composition (AP of 31.70\% compared to VQACL's 33.85\%), likely due to the datasetâ€™s complexity, involving temporal and causal reasoning. For such tasks, storing images may be necessary to maintain comprehensive task understanding.

\noindent\textbf{Performance Analysis of Novel Composition Testing. }Tab.~\ref{tab:vqa_performance} provides a detailed comparison of model performance on novel and seen skill-concept compositions across VQAv2 and NExT-QA datasets. \qstmethodshort{} consistently surpasses previous approaches. On VQAv2, \qstmethodshort{} shows significant gains over VQACL, with an average improvement of 4.60\% on novel compositions and 3.81\% on seen compositions. This result indicates enhanced compositional generalisation, as evidenced by the smaller gap between novel and seen performance compared to other methods. For NExT-QA, \qstmethodshort{} maintains competitive performance, with slight gains over VQACL on seen groups (average +0.27\%) in a dataset that presents unique challenges, such as temporal and causal reasoning. For such tasks, storing images may be necessary to maintain a comprehensive task understanding. These consistent gains highlight \qstmethodshort{}'s robustness in compositional reasoning, validating its effectiveness for continual VQA. 

\section{Ablation Study and Analysis}
\label{sec:ablation}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{Figures/matrix_plots_vqa.pdf}
    \vspace{-8mm}
    \caption{\textbf{Plasticity/Stability analysis on VQAv2. }Each matrix shows the performance of a model trained on tasks (rows) and evaluated on tasks \textbf{sequentially} (columns). The diagonal (highlighted in {\color{orange}{orange}}) represents in-domain performance, while off-diagonal elements indicate cross-domain generalization. Higher values (darker colors) suggest better retention and plasticity. The progression from `Sequential finetuning', to $\mathcal{L}_{PL}$ and then to our full method, QUAD, highlights the improvement in retaining knowledge across sequential tasks.}
    \vspace{-3mm}
    \label{fig:cross_task_generalization}
\end{figure*}

\noindent \circled{1} \noindent\textbf{Distillation Components. }Tab.~\ref{tab:comparison_distillation} shows the impact of PL distillation and attention distillation in \qstmethodshort{}. We evaluate three setups: (1) \textit{PL distillation alone}, (2) \textit{attention distillation alone}, and (3) \textit{the combination of both}.

Using PL distillation alone achieves moderate performance, with AP  of 30.72\% on VQAv2 and 29.04\% on NExT-QA, indicating effective feature alignment across tasks. In contrast, attention distillation alone yields lower AP scores (13.34\% on VQAv2 and 13.24\% on NExT-QA) with high forgetting scores (32.08\% on VQAv2 and 24.56\% on NExT-QA), suggesting limited task adaptation. Combining PL and attention distillation achieves the best results, with AP scores of 39.25\% on VQAv2 and 31.70\% on NExT-QA, and the lowest forgetting rates.

\noindent \circled{2} \textbf{Attention Distillation. }Tab.~\ref{tab:attn_distillation} demonstrates the effectiveness of our proposed {$\mathcal{L}_{\text{Att}}$} within \qstmethodshort{}. Unlike prior methods such as Attn-dist (L1) and Asym-Attn~\citep{pelosin2022towards}, which impose L1 or ReLU+L1 losses on raw attention scores, our approach applies cross-entropy over normalized attention maps. \qstmethodshort{} outperforms previous methods, achieving an AP of 39.25\% and Forgetting of 4.91\% on VQAv2, and an AP of 31.70\% with Forgetting of 2.91\% on NExT-QA. These results highlight the importance of preserving distributional consistency in attention maps and avoiding unstructured alignment.
\begin{table}[t]
    \centering
    \caption{Ablation study of \qstmethodshort{} components on VQAv2 and NExT-QA.}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{8pt}
    \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{c|c|c|cc|cc}
    \toprule
    \multirow{2}{*}{$\mathcal{L}_{\text{PL}}$} & \multirow{2}{*}{$\mathcal{L}_{\text{Att}}$} & \multirow{2}{*}{\textbf{Memory}} & \multicolumn{2}{c|}{\textbf{VQAv2}} & \multicolumn{2}{c}{\textbf{NExT-QA}} \\ 
    \cline{4-7} 
    & & \textbf{Type} & \textbf{AP (â†‘)} & \textbf{Forget (â†“)} & \textbf{AP (â†‘)} & \textbf{Forget (â†“)} \\
    \midrule
    $\checkmark$ &  & \faQuestionCircle & 30.72 & 13.74 & 29.04 & 4.58 \\
    & $\checkmark$ & \faQuestionCircle & 13.34 & 32.08 & 13.24 & 24.56 \\
    $\checkmark$ & $\checkmark$ & \faQuestionCircle & \textbf{39.25} & \textbf{4.91} & \textbf{31.70} & \textbf{2.91} \\ 
    \bottomrule
    \end{tabular}
    }
    \vspace{-3mm}
    \label{tab:comparison_distillation}
\end{table}
\begin{table}[t]
    \centering
    \caption{Comparison of attention distillation methods.}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{8pt}
    \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{c|cc|cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{VQAv2}} & \multicolumn{2}{c}{\textbf{NExT-QA}} \\
    & \textbf{AP (â†‘)} & \textbf{Forget (â†“)} & \textbf{AP (â†‘)} & \textbf{Forget (â†“)} \\
    \midrule
    $\mathcal{L}_{\text{PL}}$ + Attn-dist (L1) & 34.56 & 7.91 & 30.14 & 5.78 \\
    $\mathcal{L}_{\text{PL}}$ + Asym-Attn \citep{pelosin2022towards} & 38.15 & 5.57 & 31.18 & 4.13\\
    \cmidrule{1-5}
    \textbf{\qstmethodshort{} (Ours)} & \textbf{39.25} & \textbf{4.91} & \textbf{31.70} & \textbf{2.91} \\ 
    \bottomrule
    \end{tabular}
    }
    \label{tab:attn_distillation}
\end{table}

\noindent \circled{3} \textbf{Analysing Plasticity/Stability. }Fig.~\ref{fig:cross_task_generalization} illustrates the impact of different strategies on plasticity and forgetting in the VQAv2 dataset. The sequential fine-tuning matrix (left) reveals the impact of learning without rehearsal, showing uniformly low off-diagonal valuesâ€”indicating the model's failure to answer questions from previous tasks. This phenomenon, which we term the \textit{out-of-answer-set problem}, occurs when the model overfits to the current taskâ€™s answer space, preventing it from correctly responding to questions from earlier tasks. The $\mathcal{L}_{\text{PL}}$ matrix (center) mitigates forgetting through pseudo-label distillation, significantly improving cross-domain retention in tasks such as ``commonsense'' and ``count.'' However, it remains less effective in complex reasoning tasks like ``causal'' and ``subcategory.'' Our full method, \qstmethodshort{} (right), achieves the best overall performance by integrating pseudo-label and attention consistency distillation, striking a balance between stability and plasticity. \qstmethodshort{} consistently maintains high values both on and off the diagonal, demonstrating superior retention of prior knowledge and adaptability to new tasks. Attention consistency can be assessed by comparing the right and middle subplots. For instance, after training with QUAD, the model retains more knowledge on the `judge' task (62.6\%) compared to pseudo-labeling alone (35.0\%).

\noindent \circled{4} \textbf{Sensitivity to Memory Size. }Fig.~\ref{fig:memory_size_comparison} shows a sensitivity analysis of CL methods with respect to memory size. \qstmethodshort{} consistently outperforms baseline methods (ER, DER, VS, VQACL) across all memory sizes, demonstrating the effectiveness of our distillation strategy. On VQA-v2, \qstmethodshort{} exhibits strong scalability, with AP increasing steadily from 1000 to 5000 samples. In constrained storage scenarios, \qstmethodshort{} maintains competitive performance by leveraging question-only distillation. On NExT-QA, \qstmethodshort{} outperforms other methods, though with smaller gains than on VQAv2 due to the increased task complexity and reliance on visual information.
\begin{figure}[t]
\centering
        \includegraphics[width=\linewidth]{Figures/memory_size_effect.pdf}
        \caption{\textbf{Sensitivity analysis to memory size}. Our method, \qstmethodshort{}, consistently achieves higher AP than baselines, demonstrating strong scalability on VQAv2 and stable performance on NExT-QA, especially as memory size increases.}
        \label{fig:memory_size_comparison}
\end{figure}

\noindent \circled{5} \textbf{Effectiveness of Object-Matched Question Selection. }Fig.~\ref{fig:random_vs_obj} demonstrates the benefits of object-matched question selection over random selection. We analyze its impact across memory sizes on VQAv2 and NExT-QA, where it consistently improves AP as memory grows. Aligning selected questions with object categories enhances knowledge transfer, leading to better adaptation to new tasks.

\definecolor{mediumpurple}{HTML}{9370DB}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/random_performance_comparison.pdf}
    \caption{\textbf{Question selection strategy in \qstmethodshort{}}. Figure shows AP for random (\textcolor{blue}{blue}) and object-matched (\textcolor{mediumpurple}{purple}) question selection across memory sizes on VQAv2, and NExT-QA. Object-matched selection consistently outperforms random selection.}
    \label{fig:random_vs_obj}
\end{figure}