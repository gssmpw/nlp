\clearpage
\setcounter{page}{1}
\maketitlesupplementary

In this supplementary material, we provide additional details on the experimental results presented in the main paper, along with further empirical evaluations and discussions. 

Section~\ref{sec.ethics} presents an ethics statement regarding our work. Section~\ref{sec.limitations} discusses the limitations of our proposed method, \qstmethodshort{}. Section~\ref{sec.computational} reports the computational and memory footprint analysis of \qstmethodshort{}. Section~\ref{sec:details_setting} and Section~\ref{sec:datasets_order} describe the datasets, task sequences, and evaluation structure. Section~\ref{extended_analysis} provides analysis on plasticity-stability for NExT-QA dataset. Lastly, Section~\ref{explain_methods} explains the baselines used for comparison.

\section{Ethics Statement}
\label{sec.ethics}
Our method, \qstmethodshort{}, is designed to improve continual learning in Visual Question Answering (VQACL) while maintaining generalization and privacy through distilaltion using questions-only. We do not foresee any negative societal impact from this work, as it does not involve the generation of harmful or biased data. However, like any machine learning system, there remains a potential risk if it is applied unethically or without proper oversight. \qstmethodshort{}'s design includes mechanisms to enhance privacy, reducing the storage of sensitive visual data. Despite this, its applicability beyond the specific datasets and tasks used in our experiments remains to be thoroughly tested, and we caution against the unconsidered deployment of the method in sensitive applications without further validation.

% \section{Reproducibility Statement}
% \label{sec.reproduce}
% Code and trained models for our experiments will be made available upon acceptance. We provide detailed documentation on our dataset usage, including the specific task categories and data splits for both VQA-v2 and NExT-QA, ensuring the reproducibility of our work. Further experimental details, such as hyperparameters, model architectures, and computational setups, are provided in the main text.

\section{Limitations of QUAD} 
\label{sec.limitations}
While \qstmethodshort{} effectively reduces storage requirements and enhances privacy by eliminating the need to store images, it may be suboptimal for tasks that heavily rely on detailed visual or spatial reasoning. Certain VQA tasks, such as object classification, fine-grained attribute recognition, or spatial relationships, inherently require access to visual information to retain critical knowledge from previous tasks. For instance, as shown in Fig. \ref{fig:cross_task_generalization}, \qstmethodshort{} struggles to maintain performance on the `type' task in VQAv2, which depends on visual cues, whereas it performs well on conceptually driven tasks like `commonsense' reasoning.  

Our findings suggest that question-only replay is particularly well-suited for constrained scenarios where privacy and storage efficiency are primary concerns. However, in settings where high fidelity in visual reasoning is essential, storing a subset of representative images may be necessary to preserve task-specific knowledge and improve overall performance. Future work could explore hybrid approaches that selectively retain visual information while leveraging question-based replay, striking a balance between efficiency and task-specific retention.

Furthermore, \qstmethodshort{} prevents storing original sensitive visual information, aligning with GDPR constraints, which permit data storage only when strictly necessary for the task. However, our approach specifically addresses storage-related privacy concerns and does not guarantee protection against attacks such as inversion attacks~\cite{dibbo2023sok, zhang2020privacy}.


\section{Computational Analysis}
\label{sec.computational}
Efficient memory and storage management is crucial for continual VQA, where scalability is a key challenge. This section analyzes storage requirements, computational complexity, and GPU memory usage of our text-only replay approach compared to image-based methods. By storing only past task questions, we significantly reduce storage complexity from $\mathcal{O}(N\cdot (I + L_q+L_a))$ to $\mathcal{O}(N\cdot L_q)$, where $N$ is the number of stored samples, $I$ is the image size, and $L_q$ and $L_a$ represent the question and answer lengths in bits. %This reduction eliminates the need for high-dimensional visual feature storage.  

In terms of \textit{GPU memory usage}, question-only replay has a minimal impact since the number of processed input pairs remains the same. The primary reduction stems from loading fewer images, but this accounts for less than 5\% of the total memory footprint, which is dominated by gradients, weights, and activations. This makes our approach particularly appealing in scenarios where storage is constrained but GPU memory availability remains a concern.  

From a \textit{computational complexity} perspective, our method does not introduce any additional overhead. The computational cost remains unchanged when processing images from past or current tasks. The forward and backward passes are identical, ensuring that our approach maintains the same efficiency while significantly improving storage scalability.  

This analysis validates our design choices, demonstrating that question-only replay can achieve competitive performance while substantially reducing storage requirements. This efficiency makes it highly scalable and practical for real-world deployment.

\section{Detailed Description of the VQACL Setting}
\label{sec:details_setting}

This section provides a detailed overview of the Visual Question Answering Continual Learning (VQACL) setting, as introduced by \cite{zhang2023vqacl}. The VQACL setting is designed to test a model's ability to generalise and retain knowledge across a sequence of tasks involving both visual and linguistic modalities, with a particular focus on compositional generalisation and knowledge retention.

The VQACL setting is organised into a two-level hierarchy of tasks that challenge both the visual and linguistic capabilities of the model.

% Table 1
\begin{table*}[ht]
\centering
\caption{Linguistic-driven task statistics of VQA v2 in the VQACL setting. Stan. Test denotes the standard test set.}
\label{tab:vqav2_stats}
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|r|r|r|l}
\toprule
\textbf{Task}         & \textbf{Train} & \textbf{Val} & \textbf{Stan. Test} & \textbf{Examples} \\ \midrule
\textbf{Recognition}  & 131,478        & 5,579        & 5,628               & What is on the floor? What does the sign say? \\
\textbf{Location}     & 12,580         & 611          & 611                 & Where is the giraffe? Where are the people standing? \\
\textbf{Judge}        & 160,179        & 7,126        & 7,194               & Is the baby playing ball? Are the windows big? \\
\textbf{Commonsense}  & 25,211         & 1,114        & 1,100               & Do the elephants have tusks? Do the dogs know how to swim? \\
\textbf{Count}        & 62,156         & 2,651        & 2,658               & How many beds? How many seats are there? \\
\textbf{Action}       & 33,633         & 1,498        & 1,373               & Are they drinking wine? Is the person flying? \\
\textbf{Color}        & 50,872         & 2,322        & 2,192               & What color is the bedspread? What color are the gym shoes? \\
\textbf{Type}         & 23,932         & 1,119        & 1,089               & What type of building is this? What type of animal is shown? \\
\textbf{Subcategory}  & 31,594         & 1,477        & 1,416               & What brand is the umbrella? What brand are his shoes? \\
\textbf{Causal}       & 5,868          & 231          & 200                 & Why does he have glasses on? Why is the dog jumping? \\ \bottomrule
\end{tabular}
}
\end{table*}

% Table 2
\begin{table*}[ht]
\centering
\caption{Linguistic-driven task statistics of NExT-QA in the VQACL setting. Stan. Test denotes the standard test set. CW: CausalWhy; TN: TemporalNext; TC: TemporalCurrent; DL: DescriptiveLocation; DB: DescriptiveBinary; DC: DescriptiveCount; DO: DescriptiveOther; CH: CausalHow.}
\label{tab:nextqa_stats}
\renewcommand{\arraystretch}{1.2}  % Adjust row height for better readability
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|r|r|r|l}
\toprule
\textbf{Task}     & \textbf{Train} & \textbf{Val} & \textbf{Stan. Test} & \textbf{Examples} \\ \midrule
\textbf{CW}       & 13,552         & 1,928        & 3,333               & Why is the lady sitting down? Why is the baby's hair wet? \\
\textbf{TN}       & 5,685          & 895          & 1,399               & What does the baby do after picking up the toy? What did the lady do after adjusting the shirt? \\
\textbf{TC}       & 4,797          & 663          & 1,165               & What event is happening? What sport is the man doing? \\
\textbf{DL}       & 1,942          & 295          & 482                 & Where are the two people dancing? Where is this video taken? \\
\textbf{DB}       & 2,928          & 277          & 495                 & Is the baby able to walk? Does the girl cry? \\
\textbf{DC}       & 1,378          & 192          & 365                 & How many babies are there? How many dogs are there? \\
\textbf{DO}       & 2,549          & 356          & 672                 & What season is this? What does the man use to stir the food in the pan? \\
\textbf{CH}       & 4,400          & 683          & 1,174               & How did the singer project her voice? How did the boy in the box move forward? \\ \bottomrule
\end{tabular}
}
\end{table*}

% Table 3: VQA v2 object groups
\begin{table*}[ht]
\centering
\caption{Detailed information about the five object groups in VQA v2.}
\label{tab:vqa_groups}
\renewcommand{\arraystretch}{1.2}  % Adjust row height for better readability
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l}
\toprule
\textbf{Task} & \textbf{Objects} \\ \midrule
\textbf{Group 1} & hot dog, fork, orange, snowboard, potted plant, person, toilet, laptop, surfboard, bench, bus, dog, knife, pizza, handbag, bicycle \\ \hline
\textbf{Group 2} & horse, cell phone, elephant, boat, zebra, apple, stop sign, microwave, spoon, cup, skateboard, tie, umbrella, sandwich, bear \\ \hline
\textbf{Group 3} & donut, truck, frisbee, giraffe, dining table, motorcycle, parking meter, car, oven, airplane, bed, sheep, baseball bat \\ \hline
\textbf{Group 4} & skis, baseball glove, tennis racket, tv, traffic light, kite, cake, keyboard, bottle, remote, bird, carrot \\ \hline
\textbf{Group 5} & suitcase, couch, broccoli, cow, fire hydrant, chair, mouse, cat, banana, wine glass, backpack, bowl, sports ball, train \\ \bottomrule
\end{tabular}%
}
\end{table*}

% Table 4: NExT-QA object groups
\begin{table*}[ht]
\centering
\caption{Detailed information about the five object groups in NExT-QA.}
\label{tab:nextqa_groups}
\renewcommand{\arraystretch}{1.2}  % Adjust row height for better readability
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l}
\toprule
\textbf{Task} & \textbf{Objects} \\ \midrule
\textbf{Group 1} & bicycle, camel, bat, microwave, snake, sofa, traffic light, hamster/rat, chicken, oven, stop sign, vegetables, skateboard, bird, toilet, racket \\ \hline
\textbf{Group 2} & crab, camera, lion, ball/sports ball, crocodile, screen/monitor, baby walker, cat, squirrel, frisbee, cattle/cow, sheep/goat, adult, scooter, electric fan, stool \\ \hline
\textbf{Group 3} & piano, watercraft, kangaroo, train, fruits, pig, suitcase, bear, tiger, bench, elephant, motorcycle, horse, snowboard, surfboard, handbag \\ \hline
\textbf{Group 4} & ski, stingray, antelope, toy, child, duck, guitar, dish, fish, cake, turtle, leopard, laptop, panda, table, cup \\ \hline
\textbf{Group 5} & penguin, faucet, car, bottle, bus/truck, aircraft, baby, bread, baby seat, cellphone, sink, rabbit, backpack, chair, dog, refrigerator \\ \bottomrule
\end{tabular}%
}
\end{table*}


\begin{itemize}
    \item \textbf{Linguistically-Driven Tasks. }At the higher level, the VQACL setting comprises a series of linguistically-driven tasks, denoted as ${\mathcal{T}^1, \dots, \mathcal{T}^T}$, where $T$ represents the total number of tasks. Each task focuses on a specific reasoning skill, such as counting or color identification, and is characterized by a particular type of question. For example, a task focused on counting might involve questions beginning with "\textit{How many}" or "\textit{What number}". In our experiments, the VQAv2 dataset consists of $T\!=\!10$ such tasks, while the NExT-QA dataset includes $T\!=\!8$ tasks.

    \item \textbf{Visually-Driven Subtasks. }Nested within each linguistically-driven task are a series of visually-driven subtasks ${\mathcal{S}_1^t, \dots, \mathcal{S}_K^t}$. Each visually-driven subtask is associated with a specific object group $G_k$, formed by partitioning the total set of object classes ${\{c_i}\}_{i=1}^{C}$ into $K$ groups. These groups are then randomly assigned to different subtasks within each linguistic-driven task. In our implementation, both the VQAv2 and NExT-QA datasets are divided into $K\!=\!5$ visual subtasks, covering a total of $C\!=\!80$ object classes, following the categorization used in the COCO dataset \citep{cocodataset}.

    \item \textbf{Novel Composition Testing. }The VQACL setting also includes a novel composition testing process, designed to evaluate the model's compositional generalization abilities—its capacity to apply learned concepts to new combinations of objects and questions.
\end{itemize}

\noindent\textbf{Training and Testing Procedure.} During training, the model is exposed to a subset of the visual-driven subtasks within each linguistically-driven task. Specifically, one visual-driven subtask $\mathcal{S}_k^v$ is randomly excluded from the training phase for each linguistic-driven task. This excluded subtask is reserved for testing and serves as a novel composition, where the model must answer questions about unseen combinations of objects and reasoning skills.

\noindent\textbf{Cross-Validation and Fair Testing. }To ensure a fair evaluation of the model's generalization capabilities, the VQACL setting employs a $K$-fold object-independent cross-validation process. This involves repeating the training and testing procedure $K$ times, each time excluding a different visual-driven subtask. This ensures that the model encounters all object classes across different folds, thereby providing a comprehensive assessment of its ability to generalize to new combinations of objects and tasks.

\noindent\textbf{Continual Learning Challenges.} The VQACL setting presents a significant challenge for continual learning models, requiring them to balance the retention of knowledge from previously learnt tasks (stability) with the ability to adapt to new, continually arriving tasks (plasticity). By structuring tasks to involve both new and previously encountered concepts, the VQACL setting effectively tests the model's ability to minimize catastrophic forgetting while enabling knowledge transfer across tasks.

\section{Details of Evaluation Datasets}
\label{sec:datasets_order}
In this section, we provide a detailed overview of the two datasets used in our evaluation: \textbf{VQA v2} and \textbf{NExT-QA}. Each dataset has been carefully structured into different tasks, which are used to evaluate the performance of our continual learning models.

We summarize the statistics of each dataset, focusing on both linguistic and object-related tasks. Tables \ref{tab:vqav2_stats} and \ref{tab:nextqa_stats} (previously described) present the linguistic-driven task breakdown, including categories such as \textit{Recognition}, \textit{Commonsense}, \textit{Count}, and others.

Additionally, we grouped the objects in each dataset into five distinct object groups to facilitate better understanding and comparison of the models' object recognition capabilities. Tables \ref{tab:vqa_groups} and \ref{tab:nextqa_groups} offer a detailed breakdown of the objects associated with each group in VQA v2 and NExT-QA, respectively. This categorization will aid in analyzing how the models perform across different object categories.

These two datasets, each structured uniquely in terms of linguistic tasks and object types, allow us to rigorously assess the models in varied real-world scenarios. Together, these benchmarks enable a comprehensive evaluation of the continual learning approaches proposed in this work.

\section{Extended Analysis of Plasticity/Stability Trade-Off}
\label{extended_analysis}
Fig.\ref{fig:cross_task_generalization_nextqa} compares the impact of three continual learning strategies on performance across tasks in the NExT-QA dataset. The sequential finetuning baseline (left) demonstrates severe forgetting, with consistently low off-diagonal values. Specifically, tasks like temporal reasoning (TN and TC) exhibit the worst performance, as these tasks require advanced reasoning over time sequences, which is inherently challenging for the model.

Introducing pseudo-label distillation through $\mathcal{L}_{\text{PL}}$ (center) mitigates the issue of forgetting by enforcing output consistency with the previous model. This results in improved cross-domain retention, particularly in easier tasks like `DB' and `DL'. However, its performance on complex tasks such as "DO" (Descriptive Others) and `CH' (Causal How) remains suboptimal, as these tasks require the model to maintain intricate visual-linguistic relationships, which $\mathcal{L}_{\text{PL}}$ alone struggles to address.

Our method, \qstmethodshort{} (right), achieves the highest overall performance by combining pseudo-labeling with attention consistency distillation. This dual mechanism effectively balances stability and plasticity, as evidenced by the consistently high diagonal values and substantial improvements in off-diagonal cross-domain generalization. Notably, \qstmethodshort{} performs significantly better on retraining prior knowledge (row 6, 7). The results underscore the strength of \qstmethodshort{} in preserving visual-linguistic associations and mitigating the \textit{out-of-answer-set problem} across tasks in NExT-QA.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures/matrix_plots_nextqa.pdf}
    \caption{Comparison of feature distillation methods on \textbf{NExT-QA}. Each matrix shows the performance of a model trained on tasks (rows) and evaluated on tasks (columns). The diagonal (highlighted in {\color{orange}{orange}}) represents in-domain performance, while off-diagonal elements show cross-domain generalization. Higher values (darker colors) indicate better performance.}
    \label{fig:cross_task_generalization_nextqa}
\end{figure*}

\section{Continual Learning Methods}
\label{explain_methods}
We assess and benchmark five prominent continual learning methods, encompassing two regularization techniques (EWC \citep{kirkpatrick2017overcoming}, MAS \citep{mas2018}) and three rehearsal-based methods (ER \citep{chaudhry2019tinyepisodicmemoriescontinual}, DER \citep{Chaudhry2019er}, VS \citep{Wan_2022_CVPR}, and VQACL\citep{zhang2023vqacl}). To ensure a consistent evaluation, all methods are implemented using their official codebases and integrated into the same transformer backbone as described in Section 5.1. 

\textbf{EWC} \citep{kirkpatrick2017overcoming} is a regularization method designed to preserve knowledge of prior tasks by selectively reducing updates on critical parameters. This is achieved by leveraging the Fisher Information Matrix, which quantifies the importance of parameters and incorporates an auxiliary L2 loss between significant parameters from old and new tasks.

\textbf{MAS} \citep{mas2018} similarly applies regularization, aiming to prevent significant changes to parameters vital for previous tasks by introducing an L2 loss. In contrast to EWC, MAS measures the sensitivity of the output with respect to parameter perturbations to estimate parameter importance.

\textbf{ER} \citep{chaudhry2019tinyepisodicmemoriescontinual} is a rehearsal method that utilizes a fixed-size memory buffer, where visited examples are stored and randomly sampled for retraining. In line with our approach, the memory size for ER is fixed at 5,000 for VQA v2 and 500 for NExT-QA. Given its simplicity and effectiveness, ER serves as the baseline for our proposed method.

\textbf{DER} \citep{Chaudhry2019er} is another rehearsal technique that employs reservoir sampling to manage memory, ensuring every visited sample has an equal chance of being stored. DER also incorporates a dark knowledge distillation strategy, which aims to align the network’s outputs with logits recorded during training, thus encouraging consistency in responses to prior examples. In our experiments, DER also utilizes memory sizes of 5,000 for VQA v2 and 500 for NExT-QA.

\textbf{VS} \citep{Wan_2022_CVPR} is a rehearsal-based method that emphasizes feature consistency between current and past data. To address forgetting, VS introduces two losses: a neighbor-session model coherence loss and an inter-session data coherence loss. For more details, we refer readers to \citet{Wan_2022_CVPR}. The memory size for VS is similarly set to 5,000 for VQA v2 and 500 for NExT-QA.

\textbf{VQACL} \citep{zhang2023vqacl} represents a rehearsal-based approach, incorporating a prototype module to learn both task-specific and invariant features, facilitating robust and generalizable representations for VQA tasks.
