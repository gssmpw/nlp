\section{Question-only replay with Attention Distillation (\qstmethodshort{})}
\input{sec/setting}
\label{sec:method}


\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/method_quid.pdf}
    \vspace{-3mm}
    \caption{\textbf{Overview of \qstmethodshort{}}. 
    Our method consists of three key components designed to maintain stability and plasticity in the VQACL setting. 
    \textbf{(1) Questions-Only Memory (\(\mathcal{M}\))} stores questions from past tasks, without visual data. \textbf{(2) Pseudo-label Distillation} (\( \mathcal{L}_{\text{PL}} \)) leverages answers generated by the previous model \( \theta^{t-1} \) for new image-question pairs, guiding the current model \( \theta^{t} \) to retain past knowledge, and mitigate the out-of-answer-set problem. \textbf{(3) Attention Consistency Distillation} (\( \mathcal{L}_{\text{Att}} \)) aligns the self-attention maps between \( \theta^{t-1} \) and \( \theta^{t} \) to maintain focus on relevant visual-linguistic relationships. 
    The task-specific loss (\( \mathcal{L}_{\text{CE}} \)) is applied solely to current task samples, promoting adaptation to new data.}
    \vspace{-3mm}
    \label{fig:main_fig}
\end{figure*} 


\subsection{Overview}

We propose \textbf{\qstmethodshort{}}, a novel approach for \setting{} that eliminates the need to store images from previous tasks by using only past task questions (see Fig.~\ref{fig:main_fig}). Following several works in continual learning \citep{li2017lwf, dhar2019learning}, we adopt a regularisation framework where the total learning objective $\mathcal{L}_{\text{VQACL}}$ consists of two main components:
\begin{equation}
    \label{eq.trad_feat}
    \mathcal{L}_{\text{VQACL}} =  \mathcal{L}_{\text{Plasticity}}+ \lambda\mathcal{L}_{\text{Stability}}.
\end{equation}
The first, the plasticity term \(\mathcal{L}_{\text{Plasticity}}\), guides the model’s adaptation to the current task $\mathcal{T}^t$, and $\lambda>0$ is a weighting coefficient adjusting the balance between plasticity and stability. Following common practice in VQA~\cite{zhang2023vqacl, Ravi_2023_WACV, Antol_2015_ICCV}, we implement this loss using cross-entropy to compare the network’s prediction for an input image-question pair \((x^t, q^t)\) with the corresponding annotated answer \(y^t\):
\begin{equation}
    \label{eq.trad_feat}
    \mathcal{L}_{\text{Plasticity}} = \mathbb{E}_{(x^t, q^t, y^t) \sim \mathcal{T}^t} \mathcal{L}_{\text{CE}}\left[\phi(x^t, q^t), y^t\right] .
\end{equation}
The second loss component, the stability term \(\mathcal{L}_{\text{Stability}}\), helps prevent forgetting. In the standard VQACL setting, where images from past tasks can be stored in the memory \(\mathcal{M}\), this term can be implemented similarly to the plasticity term, using cross-entropy and averaging the loss over triplets \(~{(x^m, q^m, y^m) \in \mathcal{M}}\).
In the \setting~setting, storage of images \(x^m\) from past tasks in the memory \(\mathcal{M}\) is prohibited. To address this limitation, we propose a novel \(\mathcal{L}_{\text{Stability}}\) loss tailored for \setting. This loss combines two complementary strategies—pseudo-labeling  $\mathcal{L}_{\text{PL}}$ and attention distillation $\mathcal{L}_{\text{Att}}$— to effectively compensate for the absence of past task images, ensuring robust knowledge retention while adhering to the constraints of the \setting~framework. In our case, the stability term can be written as:
\begin{equation}
    \label{eq.trad_feat_stability}
    \mathcal{L}_{\text{Stability}} = \mathcal{L}_{\text{PL}}+\mathcal{L}_{\text{Att}},
\end{equation}
%where $\mathcal{L}_{\text{PL}}$ and $\mathcal{L}_{\text{Att}}$ denote the losses corresponding to our pseudo-labeling and attention distillation, respectively. 
In the following, we explain how we implement these two losses without using any images from past tasks.

\subsection{Pseudo-Labeling for Question-only Replay}
To address knowledge retention in continual VQA, we propose a pseudo-labeling approach that leverages image-question pairs formed by combining current task images with questions from past tasks stored in memory. Specifically, for each image from the current task $x^{t}$, we pair it with a question $q^{m}$ sampled from the memory. By asking past questions on new images, we prompt the model to recall and retain prior knowledge. Inspired by prior distillation-based techniques~\citep{li2017lwf, dhar2019learning}, we employ the model from the previous task $\phi^{t-1}$ to generate answers for each new image-question pair $(x^t, q^m)$. These answers serve as soft pseudo-labels for the current model $\phi^{t}$, enforcing consistency with past knowledge. The pseudo-labeling loss is defined as:
\begin{equation}
    \label{eq.trad_feat}
    \mathcal{L}_{\text{PL}} = \mathbb{E}_{x^t\sim \mathcal{T}^t}\mathbb{E}_{q^m\sim \mathcal{M}}\mathcal{L}_{\text{CE}}\left[\phi^{t}(x^t,q^m),\phi^{t-1}(x^t,q^m)\right].
\end{equation}

We use the network's output as soft pseudo-labels, avoiding the application of the argmax operator. Applying argmax would compel the network to align its predictions solely with the most likely class, while soft pseudo-labeling enables a more nuanced alignment with the output distribution of the model $\phi^{t-1}$~\citep{hinton2015distilling, zhang2021refining, learning-soft-labels}. 

Our new image-question pairs $(x^t,q^m)$ exposes the model to a broad set of visual-question combinations, enhancing its stability across tasks. Importantly, it allows the model to retain a broad ability to answer different types of questions, even if it cannot produce precise, correct answers. Crucially, without this pseudo-labelling regularisation, the model becomes susceptible to the \textit{out-of-answer-set problem}, where overfitting to the current task's answer space leads to incorrect responses for questions from previous tasks. For instance, after training on a color recognition task, the model might incorrectly respond with a color name when asked a counting question from a prior task.

\noindent\textbf{Question Selection for \qstmethodshort{}. }  
Randomly pairing images from the current task \(x^{t}\) with past questions \(q^{m}\) from memory can help mitigate forgetting, but it is inherently suboptimal as it often results in mismatched pairs. For instance, if the current macro-task \(\mathcal{T}^t\) focuses on counting, random pairing could lead to questions like ``\textit{How many cows are in this image?}'' being associated with an image of cars, which undermines the relevance and effectiveness of the training process. To address this, we introduce a question selection strategy that prioritises questions directly related to the object categories \(c_i\) being learnt in the current visually-driven subtask \(\mathcal{S}^l\). This targeted pairing ensures that the training process leverages meaningful and contextually relevant image-question combinations. By aligning questions with the visual context, \qstmethodshort{} enhances adaptation to new linguistically-driven tasks and visually-driven subtasks, leading to effective knowledge transfer.

\noindent \textit{Example:} Suppose the current visually-driven subtask 
$\mathcal{S}^t$ involves learning to count cars. Thus, the question selection strategy prioritises questions from memory relevant to cars, such as ``\textit{What’s the color of the car?}'', ensuring alignment between the visual content and the linguistic query.

\subsection{Attention Consistency Distillation}
Continual VQA demands a careful balance between stability and plasticity, particularly as models must adapt to new tasks without forgetting. While pseudo-labeling offers a straightforward way to preserve prior knowledge, it provides limited stability because it only influences the network's outputs, leaving its internal representations vulnerable to drift. Feature distillation across multiple layers~\cite{Kang2022afc, xu2023multi,dhar2019learning, pelosin2022towards} provides stronger regularisation by guiding internal representations but often compromises plasticity. These methods impose rigid constraints across all layers, which is particularly problematic in our setting, where new image-question pairs $(x^t, q^m)$ introduced during learning the current task were not seen in previous tasks. Although these pairs are meaningful and aligned, the model has no prior experience processing them, making strict layer-wise regularisation overly restrictive. Our VQA model~\cite{zhang2023vqacl, nikandrou2022task} processes image features and language tokens as a unified sequence in a transformer, where self-attention naturally captures both intra-modal (language-to-language and image-to-image) and inter-modal (language-to-image) interactions. This implicit cross-attention capability enables the model to compute similarity scores between language and image tokens without requiring cross-attention layers. However, as the model is sequentially fine-tuned on new tasks, it runs the risk of suffering from \textit{self-attention drift}—a gradual shift in focus away from relevant visual regions associated with previous tasks~\cite{voita2019analyzing, godey2024anisotropy}. This drift occurs because pseudo-labeling alone cannot prevent changes in the model's internal representations. 

To address this, we introduce an intermediate approach: \textit{attention consistency distillation}. Our approach aligns the self-attention weights of the current model $\phi^t$ with those of the previous task model $\phi^{t-1}$, preserving both intra-modal and inter-modal relationships. This alignment acts as a lightweight but effective regularisation that maintains stability within the self-attention patterns. By distilling only the attention maps, we constrain the model to maintain its focus on relevant visual regions for answering questions without imposing strong regularisation on all layers. 

Let us sample an image-question pair $(x^t, q^m)$ with \(~{x^t \sim \mathcal{T}^t},~{q^m \sim \mathcal{M}}\): we denote %and for an attention head \(k\) in a given self-attention layer $l$ of \(\phi\). 
\( A_k^t \) and \( A_k^{t-1} \) as the corresponding self-attention maps in an attention head $k$ for the current and previous models, respectively, computed over the unified token sequence of both language and image features~\cite{zhang2023vqacl, nikandrou2022task}. These maps are generated by applying softmax to the scaled query-key product, yielding normalised distributions that represent the focus between intra-modal and inter-modal interactions. To align these distributions, we define the self-attention distillation loss \( \mathcal{L}_{\text{Att}} \) using cross-entropy, encouraging the current model’s attention maps \( A_k^{t} \) to resemble those from the previous task \( A_k^{t-1} \):
\begin{equation}
\begin{split}
  \mathcal{L}_{\text{Att}} = \mathbb{E}_{x^t\sim \mathcal{T}^t}\mathbb{E}_{q^m\sim \mathcal{M}}&\mathbb{E}_{k\sim \mathcal{K}_\phi}\\
  &\mathcal{L}_{\text{CE}}\left[A_{k}^{t}(x^t,q^m),A_{k}^{t-1}(x^t,q^m)\right],
\end{split}
\end{equation}
where $\mathcal{K}_\phi$ denotes the set of all the attention heads across all the layers of $\phi$. Our approach employs cross-entropy loss over normalized attention maps, in contrast to prior works~\citep{dhar2019learning, pelosin2022towards} that rely on L1 or ReLU+L1 losses. Cross-entropy enforces alignment between attention distributions, preserving both individual token importance and structured inter-modal relationships essential for VQACL. In contrast, L1-based methods treat attention scores as independent values, imposing rigid constraints that limit adaptability to new tasks. A key advantage of cross-entropy is its consistency across all loss terms in \qstmethodshort{} (\(\mathcal{L}_{\text{CE}}, \mathcal{L}_{\text{PL}}, \mathcal{L}_{\text{Att}}\)), ensuring coherent optimization. Empirically, this design mitigates catastrophic forgetting.
\input{sec/tables/main_tab}
