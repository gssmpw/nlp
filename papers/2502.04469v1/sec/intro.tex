\section{Introduction}
\label{sec:introduction}
Continual learning (CL) seeks to enable models to incorporate new information while retaining previously learnt knowledge, thereby addressing the problem of catastrophic forgetting (CF)~\citep{mcclelland1995there, mccloskey1989catastrophic}. This capability is essential in dynamic real-world settings, where models must continuously adapt to evolving data while preserving previously acquired knowledge~\citep{zhang2023vqacl}. While significant advances have been made in CL, research has predominantly focused on unimodal tasks, particularly image classification~\citep{wang2022dualprompt, wang2022continual, kirkpatrick2017overcoming, NEURIPS2023_15294ba2, marouf2024weightedensemblemodelsstrong, zhou2024expandable}. However, real-world applications often demand more sophisticated multimodal learning approaches, especially for tasks that integrate visual and textual information. Visual Question Answering (VQA) stands as a representative multimodal task, requiring models to simultaneously process and reason over both images and natural language. For instance, answering questions like ``\textit{What color is the car on the left?}'' or ``\textit{How many trucks are visible?}'' demands both object recognition from the image and an understanding of the linguistic query structure~\citep{zhang2023vqacl, schwenk2022okvqa, yang2022zero, cheng2023vindlu, Wang_2023_ICCV}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/teaser2.pdf}
    \vspace{-7mm}
    \caption{Comparison of continual learning methods for Visual Question Answering (VQA) in terms of storage, forgetting, and privacy. \textbf{Memory-free} methods ensure privacy but suffer from high forgetting. \textbf{Memory-rehearsal} methods reduce forgetting but raise privacy concerns by storing sensitive images (\textit{e.g.} people's identities, car plates). Our approach \textbf{\qstmethodshort{}}, only stores questions and avoids image storage, preserving privacy while achieving low forgetting by leveraging question-based regularisation to effectively solve the out-of-answer set problem.}
    \vspace{-3mm}
    \label{fig.teaser}
\end{figure}

The emerging field of Visual Question Answering Continual Learning (VQACL)~\citep{zhang2023vqacl} presents unique challenges at the intersection of continual learning and multimodal reasoning. VQACL requires maintaining a balance between \textit{stability}—the preservation of past knowledge—and \textit{plasticity}—the ability to learn new information—across both visual and linguistic modalities~\citep{zhang2023vqacl, antol2015vqa}. This dual-modality requirement, combined with the need for generalisation ability, introduces additional complexity in learning dynamics. Models must not only retain their understanding of previously encountered visual elements and language patterns but also generalise to unseen objects and novel question formats. For example, a model that masters counting vehicles should be able to transfer this counting capability to novel object categories, such as bicycles~\citep{greco-etal-2019-psycholinguistics}. 

Memory-based methods in CL~\citep{chaudhry2019tinyepisodicmemoriescontinual, Chaudhry2019er, Wan_2022_CVPR, zhang2023vqacl} effectively  mitigate catastrophic forgetting through storing and replaying exemplar data. In the VQACL setting, these approaches store complete image-question pairs, necessitating significant memory resources—often thousands per task (\eg, 5000 samples in VQACL~\citep{zhang2023vqacl}). This extensive storage of visual data raises two critical challenges. \emph{First}, it imposes substantial computational and storage burdens, which can be infeasible in resource-constrained real-world applications. \emph{Second}, and more importantly, it raises profound privacy concerns. Visual data often contains sensitive and personally identifiable information, especially in domains like healthcare, finance, and surveillance~\citep{tian2024privacy, liu2022privacy}, where stringent regulations such as GDPR~\citep{gdpr2016general, 10.1145/3389685} govern data usage and storage. In contrast, textual data (the questions) is typically non-identifiable and generic, inherently posing less privacy risk. Memory-free methods~\citep{kruengkrai-yamagishi-2022-mitigating, mas2018, Zhang_2024_CVPR, kirkpatrick2017overcoming}, though eliminating storage and privacy concerns by avoiding data retention entirely, exhibit suboptimal performance in multimodal settings like VQACL. This dichotomy between memory-rehearsal and memory-free approaches raises an intriguing questions: \textit{Is it truly necessary to store visual data? Could storing only past questions suffice to mitigate forgetting?} To explore these questions, we propose a novel intermediate setting: VQACL with Question-only Rehearsal (\setting) (Fig.~\ref{fig.teaser}). In this framework, only questions from past tasks are stored, eliminating the need for visual data storage. It provides a practical, privacy-preserving solution for continual VQA setting.

To address \setting~, we propose \textsc{\textbf{QU}estion-only replay with \textbf{A}ttention \textbf{D}istillation} (\textbf{\qstmethodshort{}}), a novel distillation method to balance stability and plasticity using only past questions. Our approach introduces two key contributions, each targeting specific challenges in continual VQA. \textit{First}, we introduce a \textbf{Question-only Replay} mechanism, which utilises only questions from past tasks to regularise the current model with a strategic question selection process that directly tackles the \textit{out-of-answer-set problem} -- model tend to overfit to the current task's answer space, leading to incorrect responses for previous tasks' questions (see Fig.~\ref{fig.teaser}). \textit{Second}, we propose \textbf{attention consistency distillation}, a novel attention distillation strategy that preserves the model's attention patterns across tasks. Our distillation strategy enforces both \textit{intra-modal} consistency (language-to-language, visual-to-visual) and \textit{inter-modal} consistency (language-to-visual), ensuring that the model consistently focuses on relevant regions while adapting to new information.

Through extensive experiments on standard VQACL benchmarks (VQAv2, and NExT-QA datasets), we demonstrate that \qstmethodshort{} achieves state-of-the-art performance, surpassing both memory-free approaches and traditional rehearsal methods that store complete image-question pairs. Our findings suggest that \textit{storing only questions is sufficient to mitigate forgetting}, validating the practicality of the Question-only Rehearsal (VQACL-QR) setting.