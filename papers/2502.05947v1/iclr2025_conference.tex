\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}



\title{Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention}

\author{Zhendong Zhang \\
\texttt{zhd.zhang.ai@gmail.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle

\begin{abstract}
   Multiple heads decoding accelerates the inference of Large Language Models (LLMs) by predicting next several tokens simultaneously.
   It generates and verifies multiple candidate sequences in parallel via tree attention with a fixed structure. 
   In this paper, we replace the fixed tree attention with dynamic tree attention on multiple head decoding, specifically in the context of MEDUSA. 
   We propose a simple and low complexity strategy to generate candidates and construct the dynamic tree structure. 
   Preliminary experiments show that the proposed method improves the decoding efficiency of multiple head decoding for LLMs 
   while maintaining the generation quality. This result demonstrates the potential for improvement of multiple head decoding in candidate generation.
\end{abstract}

\section{Introduction}
The scale of Large Language Models (LLMs) has been growing rapidly in recent years 
\citep{Radford2019LanguageMA,Brown2020LanguageMA,Achiam2023GPT4TR}. However,
this growth leads to an increase in inference latency. From a system perspective, 
the main latency bottleneck of LLM inference is memory bandwidth rather than
arithmetic computations \citep{Shazeer2019FastTD}. This bottleneck is inherent to
the sequential nature of auto-regressive decoding, which generates only a single token at a time, 
underutilizes the arithmetic computation potential of modern accelerators \citep{Cai2024MedusaSL}.

Researchers have explored generating multiple tokens simultaneously which follows a guess-verify approach. 
Depending on the methods used for initial token guessing and subsequent verification, recent techniques can be classified into three categories: 
speculative decoding, Jacobi decoding and multiple heads decoding. 
Speculative decoding uses a smaller draft model to generate a token sequence, 
which is subsequently verified by the original model \citep{Leviathan2022FastIF,Chen2023AcceleratingLL}.
Jacobi decoding typically initiates a new sequence with $[PAD]$ tokens, then iteratively verifies and updates the sequence by solving Jacobi equations 
until a fixed point is reached \citep{Song2020AcceleratingFC,Santilli2023AcceleratingTI}. 
Multiple heads decoding predicts multiple next tokens by extra output heads. 
It then constructs multiple candidate sequences by combining these heads, and verifies them in parallel by the original model 
\citep{Stern2018BlockwisePD,Cai2024MedusaSL}.

This paper focus on multiple heads decoding, particularly MEDUSA proposed in \citep{Cai2024MedusaSL}.
Given the original model's last hidden states $\mathbf{h}_t$ at last input position $t$, MEDUSA introduces $K$ additional decoding heads
to $h_t$. The $k$-th head is designed to predict the token in the $(t + k + 1)$-th position, 
while the original head predicts the $(t + 1)$-th position. Denote $\mathbf{p}^{(k)}$ as the predicted vocabulary distribution of $k$-th head.
The top predictions from $\mathbf{p}^{(k)}$ are used to generate candidate sequences, 
with each candidate being a combination of the top predictions from different heads.
MEDUSA uses a fixed set of combining patterns. By merging their common parts, these patterns are represented as a tree.
This tree structure is constructed by estimation of the accuracy via a calibration dataset. After generating candidates using the fixed tree
structure, MEDUSA verifies them in parallel through tree attention \citep{Miao2023SpecInferAG,Cai2024MedusaSL},
which involves incorporating the tree structure into the attention mask.

Although the fixed tree structure captures certain inherent biases of MEDUSA heads, it may not fully account for context-dependent variations.
We believe that a dynamic tree structure can handle context dependency better and improve the decoding efficiency of LLMs. 
In this paper, we propose a simple and efficient strategy to dynamically construct the tree structure: 
selecting top-$n$ candidates from all possible combinations (we will show how to efficiently do this). 
Experiments demonstrate that dynamic tree improves the decoding efficiency in terms of tokens per inference.
Our code is available at \url{https://github.com/zzd1992/MEDUSA-Plus}.

\section{Methodology}

\begin{algorithm}[t]
   \caption{Candidate Generation}
   \label{alg}
   \begin{algorithmic}
   
   \Require Top-$m$ probability of $K$ MEDUSA heads $\mathbf{P} \in \mathbb{R}^{K \times m}$ and number of candidates $n$
   \Ensure candidate set $S$ 
   
   \State Initialize candidate set $S \in \emptyset$
   \For{$i = 1$ to $m$}
      \State Add $(\mathbf{P}[1, i], i, 1)$ to $S$
   \EndFor
   
   \For{$k = 2$ to $K$}
      \State Initialize a priority queue $Q$ with maximum size $n$
      \For{$(p, idx, depth) \in S$}
         \State Push $(p, idx, depth)$ to $Q$
         \If{$depth=k-1$}
         \For{$i = 1$ to $m$}
            \State Push $(p \cdot \mathbf{P}[k, i], idx \times m + i, k)$ to $Q$
         \EndFor
         \EndIf
      \EndFor
      \State $S \leftarrow set($Q$)$
   \EndFor
   
   \State Return $S$
   
\end{algorithmic}
\end{algorithm}

The proposed method first dynamically generates candidates, then prepares the buffers of dynamic tree attention based on those candidates.
Ideally, candidates should be sampled according to their joint distribution. However, it is not directly accessible. 
As an alternative, we approximate the joint distribution using the Cartesian product of marginal distributions, which is provided by MEDUSA heads. Let $p_{i}^{(k)}$ 
be the $i$-th top prediction of $k$-th MEDUSA head. Then the probability of sequence $(i_1, i_2, \dots i_k)$ is
\begin{equation}
   P(i_1, i_2, \dots i_k) = \prod_{j=1}^{k} p_{i_j}^{(j)}
\end{equation}
By emulating the Cartesian Product of marginal distributions, we generate all possible candidates. 
We only consider the top-$m$ predictions of each marginal distribution, i.e. there are $\sum_{k=1}^K m^k$ possible candidates. 
Then we select top-$n$ candidates with the highest probability. This can be done efficiently by a priority queue, 
as shown in algorithm \ref{alg}. The computational complexity is $O(K nm \log n)$. 
Following \citep{Cai2024MedusaSL}, we set $K=4, n=64, m=32$. 
Thus, the actual complexity for candidates generating is quite small. 
The selected candidates form the structure of a tree due to the Cartesian Product of marginal distributions. 
\begin{equation}
   P(i_1, i_2, \dots i_k) = P(i_1, i_2, \dots i_{k-1}) P(i_k) \le P(i_1, i_2, \dots i_{k-1})
\end{equation}
If candidate $(i_1, i_2, \dots i_k)$ is selected, then it's parent $(i_1, i_2, \dots i_{k-1})$ is also selected. 
Once the candidates are generated, we prepare the buffers of dynamic tree attention, specifically the position embedding and attention mask. 
This is achieved by emulating the generated candidates with a computational complexity of $O(K n)$.
Thus, the overall computational complexity is quite small.

\section{Experiments}
We evaluate the proposed method in terms of tokens per inference (i.e. speed up) and generation quality. 
The evaluation is carried out using MT-Bench \citep{NEURIPS2023_91f18a12}, a multi-turn, conversational-format benchmark.
We use the same model of MEDUSA-1 and MEDUSA-2 which are trained with fixed backbone and trainable backbone respectively.
Currently, our evaluation is limited to Vicuna-7B model \citep{vicuna2023}. 
Generation quality is measured using the single judgment method on MT-Bench. The results are presented in Table \ref{tab}. 
The proposed method improves the decoding efficiency of MEDUSA-1 and MEDUSA-2 while maintaining the generation quality.
We provide a visualization of tree attention mask in Figure \ref{mask}. 
The dynamic tree structure shares common parts with the fixed tree structure, but can adapt to context dependencies. 
In terms of tokens per second, our method is approximately 10\% slower than MEDUSA. 
However, our implementation is not yet optimized. 
We are confident that the overhead associated with dynamic tree construction can be substantially reduced through further engineering efforts.


\begin{table}[t]
   \caption{MT-Bench results with Vicuna-7B model}
   \label{tab}
   \centering
   \begin{tabular}{ccc}
      \hline
      \hline
      Method & Speed up & Generation Quality \\
      \hline
      MEDUSA-1                & 2.50 & 5.21 \\
      MEDUSA-1 Dynamic        & 2.66 & 5.17 \\
      \hline
      MEDUSA-2                & 3.32 & 5.24 \\
      MEDUSA-2 Dynamic        & 3.51 & 5.21 \\
      \hline
   \end{tabular}
\end{table}

\begin{tabular}[t]{cccc}
   \label{mask}
   \thead{Prompt} & 
   &  
   \thead{Compose an engaging travel blog \\ post about a recent trip to Hawaii, \\ highlighting cultural experiences \\ and must-see attractions.} & 
   \thead{Describe a vivid and unique character, \\ using strong imagery and creative \\ language. Please answer in \\ fewer than two paragraphs.} \\ 
   \thead{MEDUSA-1} & 
   \thead{\includegraphics[width=0.27\columnwidth]{figs/1/mask_fixed.png}} &
   \thead{\includegraphics[width=0.27\columnwidth]{figs/1/mask_dynamic.png}} &
   \thead{\includegraphics[width=0.27\columnwidth]{figs/2/mask_dynamic.png}} \\
   \thead{MEDUSA-2} & 
   \thead{\includegraphics[width=0.27\columnwidth]{figs/1/mask_fixed_v2.png}} &
   \thead{\includegraphics[width=0.27\columnwidth]{figs/1/mask_dynamic_v2.png}} &
   \thead{\includegraphics[width=0.27\columnwidth]{figs/2/mask_dynamic_v2.png}} \\
   & Fixed & Dynamic & Dynamic \\
\end{tabular}

\section{Discussion}
In this paper, we replace the fixed tree attention with dynamic tree attention on multiple head decoding, specifically in the context of MEDUSA. 
We propose a simple and low complexity strategy to generate candidates and construct the dynamic tree structure.
Preliminary experiments show that the proposed method improves the decoding efficiency of multiple head decoding for LLMs
while maintaining the generation quality.
This result demonstrates the potential for improvement of multiple head decoding in candidate generation.
For future work, we plan to improve the proposed method in the following ways:
\begin{itemize}
   \item \textbf{Optimize the overhead}: optimize candidate generation process to make our method more competitive in terms of wall time.
   \item \textbf{Improve joint distribution approximation}: currently, the joint distribution is approximated by the Cartesian product of marginal distributions. 
   We will explore better strategies to approximate it.
\end{itemize}

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\end{document}
