%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% ARXIV %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article} % For LaTeX2e
\usepackage[table]{xcolor}
\usepackage{times}
\usepackage{eso-pic} % used by \AddToShipoutPicture
\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\renewcommand\Authfont{\large}
\renewcommand\Affilfont{\small\color{black}}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{placeins}         % colors


%%%%%%%%%%%%%%%%%%%%%%%%%% more packages
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage[export]{adjustbox}% http://ctan.org/pkg/adjustbox

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\usepackage[textsize=tiny]{todonotes}
\usepackage[export]{adjustbox}
% ################################################################
\usepackage{amsmath, amssymb} % Ensure these packages are included
\newcommand{\I}[1]{\mathbb{I} \left\{ #1 \right\}}
\renewcommand{\P}[1]{\mathbb{P} \left[ #1 \right]}
\newcommand{\EV}[1]{\mathbb{E} \left[ #1 \right]}

\usepackage{subcaption}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\p}{\mathbb{P}}
\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator{\H0}{\mathcal{H}_0}

\definecolor{Green}{RGB}{119,221,119}
\usepackage{pdflscape}
\usepackage{placeins}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{enumitem}
\allowdisplaybreaks
% ################################################################
\usepackage{xr}
\title{Robust Conformal Outlier Detection under\\Contaminated Reference Data}
\author[1]{Meshi Bashari}
\author[2,3]{Matteo Sesia}
\author[1,4]{Yaniv Romano}
\affil[1]{Department of Electrical and Computer Engineering, Technion IIT, Haifa, Israel}
\affil[2]{Department of Data Sciences and Operations, University of Southern California, Los Angeles, California, USA}
\affil[3]{Department of Computer Science, University of Southern California, Los Angeles, California, USA}
\affil[4]{Department of Computer Science, Technion IIT, Haifa, Israel}



\begin{document}

\date{}
\maketitle


\begin{abstract}
Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity.

\end{abstract}

\noindent \textbf{Keywords:}  Conformal Prediction, Hypothesis Testing, Out-of-Distribution Detection, Contaminated Data
\section{Introduction}
\label{sec:intro}
\subsection{Background and Motivation}
\label{sec:background}

This paper studies the problem of outlier detection: given a reference dataset (e.g., a collection of legitimate financial transactions) and an unlabeled test point (a new transaction), our goal is to determine whether the test point is an outlier (a fraudulent transaction) by assessing its deviation from the reference data distribution. Naturally, we aim to maximize the detection of outliers by harnessing the capabilities of complex machine learning (ML) models. However, these models typically lack type-I error rate control, potentially resulting in unreliable detections. In our running example, the type-I error is the probability of falsely flagging a legitimate transaction as fraudulent. As such, uncontrolled error rates can lead to costly unnecessary investigations of legitimate transactions and negatively impact customer experience. 

The broad need for reliable ML systems has sparked a surge of interest in conformal prediction---a versatile framework that can provide statistical guarantees for any ``black-box'' predictive model \cite{vovk2005algorithmic}. This framework formulates the outlier detection task as a statistical test, where the null hypothesis is that the new data point is not an outlier \cite{laxhammar2015inductive,conformal-p-values}. To derive a decision rule guaranteeing type-I error control, conformal inference relies on a reference (calibration) set of inlier data points. These points are assumed to be sampled i.i.d.~from an unknown distribution, independent of the data used to train the outlier detection model.

In practice, however, it is often difficult to obtain a perfectly clean reference dataset that contains no outliers \cite{park2021wrong,zhao2019robust,chalapathy2019deep,jiang2022softpatch}. 
In our example, a more realistic scenario would assume instead access to a slightly \emph{contaminated} reference set, mostly legitimate transactions with a few unnoticed outliers \cite{zhao2019robust}.
But this setting poses new challenges for conformal prediction methods, potentially invalidating the error control guarantees or, as we shall see, often reducing the power to detect true outliers at test time.

\subsection{Outline and Contributions}

While type-I error control in conformal inference theoretically requires perfectly clean reference data, in practice contaminated data often makes these methods overly conservative, reducing the power to detect true outliers rather than inflating the type-I error rate. This empirical observation motivates the first question explored in this paper:

\textbf{Q1:} \emph{When does conformal outlier detection with contaminated reference data yield valid type-I error control?}

In Section \ref{sec:conservativeness}, we present the first contribution of this paper: a novel theoretical analysis that identifies common conditions under which this conservative behavior arises. Unfortunately, this conservativeness often comes at the cost of reduced detection power, particularly when targeting low type-I error rates. To address this issue, we investigate data-driven cleaning strategies aimed at mitigating the contamination in the reference dataset.

A straightforward approach to cleaning the contaminated set is to remove all data points flagged as likely outliers by the detection model. However, this method is unsatisfactory, as it risks inadvertently removing inliers along with outliers, resulting in an "overly clean" reference set. This, in turn, distorts the inlier distribution and inflates the type-I error rate above the desired nominal level.

This challenge motivates our second and main contribution. In Section \ref{sec:label-trim}, we introduce an approach to clean the contaminated reference set by leveraging a limited labeling budget (e.g., 50 annotations). The outlier detection model is first used to identify suspected outliers within the contaminated reference set. The limited budget is then strategically allocated to annotate these points, thereby avoiding the unintended removal of inliers. While this is a practical and intuitive approach, it naturally prompts a critical question:

\textbf{Q2:} \emph{How does the selective annotation and partial removal of outliers from a contaminated reference set affect the validity of conformal inferences?}

We analyze the validity of this active labeling approach for trimming outliers in the contaminated set. Our theoretical results identify the conditions required to achieve approximate type-I error control, even when the data are selectively annotated and not all outliers are removed. This analysis also highlights key factors that can inflate the error rate, offering practitioners guiding principles to enhance the power of conformal methods in the presence of contaminated data.

Finally, in Section \ref{sec:experiments}, we empirically validate our theory and proposed data-cleaning approach through comprehensive experiments on real-world datasets. The experiments confirm that conformal inference with contaminated data tends to be conservative. Furthermore, they demonstrate that our method significantly boosts power, particularly when the target type-I error rate is low and the number of outliers in the contaminated set is small.
Software for reproducing the experiments is available at \href{https://github.com/Meshiba/robust-conformal-od}{https://github.com/Meshiba/robust-conformal-od}.

\subsection{Related Work}

Recently, there has been growing interest in studying the statistical properties of conformal inference methods under more realistic scenarios, moving beyond the idealized assumption of perfectly clean and exchangeable observations to account for various types of {\em distribution shift} \citep{tibshirani2019conformal,einbinder2022conformal,sesia2023adaptive,barber2023conformal,gibbs2021adaptive,zaffran2022adaptive,feldman2022achieving,gibbs2024conformal,podkopaev2021distribution,si2023pac,prinsterconformal}. This paper draws inspiration from several prior works in this area.

\citet{tibshirani2019conformal} introduced a weighted conformal prediction approach to address covariate shift between calibration and test data, later extended by \citet{podkopaev2021distribution} to accommodate label shift. Both settings, however, involve a different form of distribution shift from the one we study here. \citet{barber2023conformal} extend this line of work by analyzing the effects of general distribution shifts on the validity of conformal methods, focusing however on worst-case scenarios; see also \citet{farinhas2024nonexchangeable}.

In contrast, our work moves away from this worst-case perspective. We aim to explain why conformal outlier detection with contaminated reference data often results in a conservative type-I error rate, rather than investigating type-I error inflation, which, while theoretically possible in adversarial settings, appears less common in practice. Furthermore, we focus on developing methods to address this over-conservativeness, boosting detection power.

A more closely related line of work investigates the robustness of conformal prediction to label noise \cite{einbinder2022conformal, sesia2023adaptive, clarkson2024splitconformalpredictiondata,penso2025estimating} or other forms of data contamination \cite{pmlr-v202-zaffran23a, zaffran2024predictive, feldman2024robust}. Specifically, \citet{einbinder2022conformal} and \citet{sesia2023adaptive} show that, under certain assumptions, conformal prediction for classification with noisy labels often results in conservative type-I error rates. Furthermore, \citet{sesia2023adaptive} propose a method to address this conservativeness by leveraging an explicit ``label noise model'' that captures the relationship between the true and contaminated labels in the calibration dataset.

In contrast, this paper avoids relying on an explicit model for the contaminated data, as such models can be difficult to estimate in practice within our context. Instead, we utilize a pre-trained black-box outlier detection model and a limited annotation budget to selectively and reliably trim outliers from the contaminated set. Furthermore, it is important to emphasize that the method proposed by \citet{sesia2023adaptive} is primarily designed for classification tasks with relatively balanced data, whereas outlier detection naturally involves extreme class imbalance. This distinction underscores the need for solutions specifically tailored to outlier detection.

\section{Setup and Preliminary Results}

\subsection{Inference with Clean Calibration Data} \label{sec:cp}

Conformal inference for outlier detection requires a reference (or \emph{calibration}) set, $\D_{\mathrm{cal}} = [n] := \{1,\ldots,n\}$, containing $n$ data points. The reference set is typically assumed to be \emph{clean}, consisting solely of \emph{inliers}, which are i.i.d.~samples from an unknown distribution $\p_0$ (exchangeability may sometimes suffice, but this work assumes i.i.d.~inliers). Under this assumption, $\D_{\mathrm{cal}}$ may be referred to as $\D_{\mathrm{inlier}}$.

The goal is to determine whether a new observation, $X_{n+1}$, is an inlier—independently sampled from $\p_0$---or an \emph{outlier}, sampled from a different distribution $\p_1 \neq \p_0$. This can be formulated as a hypothesis testing problem, where the null hypothesis $\mathcal{H}_0$ claims that $X_{n+1}$ is an inlier:
\begin{align} \label{eq:setup-clean}
\begin{split}
  & X_i \overset{\text{i.i.d.}}{\sim} \p_0, \; \forall i \in \D_{\mathrm{inlier}}, \quad
  \D_{\mathrm{inlier}} = \D_{\mathrm{cal}} = [n], \\
  & \mathcal{H}_0 : X_{n+1} \overset{\text{ind.}}{\sim} \p_0.
\end{split}
\end{align}

The split-conformal method, a simple and computationally efficient approach, uses a pre-trained outlier detection model—potentially any machine learning model—to compute \emph{nonconformity scores} that quantify how different a data point is from the reference distribution. The model, represented by a score function $s$, is trained on a separate dataset $\D_{\mathrm{train}}$, which is similar to but independent of $\D_{\mathrm{cal}}$. Typically, a larger dataset of inliers, assumed to be i.i.d.~samples from $\p_0$, is randomly split into $\D_{\mathrm{train}}$ and $\D_{\mathrm{cal}}$.

The model tries to learn a score function $s$ such that larger values of $s(X_{n+1})$ indicate stronger evidence that the test point may be an outlier. Conformal inference rigorously quantifies this evidence, providing a principled decision rule for rejecting $\mathcal{H}_0$ when the evidence is strong enough, while controlling the type-I error rate—the probability of incorrectly rejecting $\mathcal{H}_0$ when $X_{n+1}$ is actually an inlier.

This statistical evidence is quantified by computing a \emph{conformal p-value}, defined as: 
\begin{align} \label{eq:conformal-p-value}
  \hat{p}_{n+1} = \frac{1 + \sum_{i=1}^{n} \mathbb{I}[s(X_i) \geq s(X_{n+1})]}{1 + n}. 
\end{align} 
Thus, larger values of $s(X_{n+1})$ correspond to smaller values of $\hat{p}_{n+1}$, and the test point $X_{n+1}$ can be confidently classified as an outlier (rejecting $\mathcal{H}_0$) when $\hat{p}_{n+1}$ is smaller than a given significance level $\alpha \in (0,1)$.

\begin{proposition}[from \citet{vovk2005algorithmic}] \label{prop:standard-conformal}
Under~\eqref{eq:setup-clean}, if the null hypothesis $\mathcal{H}_0$ is true, then for any $\alpha \in (0,1)$:
$\p \left( \hat{p}_{n+1} \leq \alpha \right) \leq \alpha$.
Further, if $s(X)$ has a continuous distribution under $\p_0$, then
$\p \left( \hat{p}_{n+1} \leq \alpha \right) \geq \alpha - 1/(n+1)$.
\end{proposition}

Proposition~\ref{prop:standard-conformal} intuitively states that the conformal p-value defined in~\eqref{eq:conformal-p-value} provides a well-calibrated rule for flagging a new data point as a likely outlier. Rejecting $\mathcal{H}_0$ when $\hat{p}_{n+1} \leq \alpha$ ensures type-I error control at level $\alpha$ while avoiding excessive conservatism. Specifically, the type-I error rate closely matches $\alpha$ when the sample size $n$ is large and the nonconformity scores have a continuous distribution with no ties—a mild condition that can be achieved in practice by adding small random noise to the scores.

What remains unclear, and serves as the starting point of this paper, is how conformal p-values behave when the calibration dataset is contaminated, containing not only inliers but also a fraction of misplaced outliers.


\subsection{Inference with Contaminated Calibration Data} \label{sec:notations}

In this paper, we consider a more general setting where the calibration dataset, indexed by $\D_{\mathrm{cal}} = [n]$, may contain both inliers ($\D_{\mathrm{inlier}}$), sampled i.i.d.~from a distribution $\p_0$, and outliers ($\D_{\mathrm{outlier}}$), sampled i.i.d.~from a different distribution $\p_1 \neq \p_0$. Thus, $\D_{\mathrm{cal}} = \D_{\mathrm{inlier}} \cup \D_{\mathrm{outlier}}$.
The numbers of inliers and outliers, respectively $n_0 = |\D_{\mathrm{inlier}}|$ and $n_1 = |\D_{\mathrm{outlier}}|$, are treated as fixed, with $n = n_0 + n_1$. 
The goal remains to test the null hypothesis $\mathcal{H}_0$ that a new data point $X_{n+1}$ is an inlier, independently sampled from $\p_0$. Formally, this setup can be written as:
\begin{align} \label{eq:setup-contaminated}
\begin{split}
  & X_i \overset{\text{i.i.d.}}{\sim} \p_0, \; \forall i \in \D_{\mathrm{inlier}}, \quad X_i \overset{\text{i.i.d.}}{\sim} \p_1, \; \forall i \in \D_{\mathrm{outlier}}, \\
  & \D_{\mathrm{inlier}} \cup \D_{\mathrm{outlier}} = \D_{\mathrm{cal}} = [n], \\
  & \mathcal{H}_0 : X_{n+1} \overset{\text{ind.}}{\sim} \p_0.
\end{split}
\end{align}

In the following, we first analyze the behavior of standard conformal p-values, computed as in~\eqref{eq:conformal-p-value}, when applied to contaminated data scenarios described by~\eqref{eq:setup-contaminated}. Subsequently, we will propose a novel method for computing more refined conformal p-values by approximately cleaning the calibration set to remove undesired outliers.

\subsection{Explaining the Conservativeness}\label{sec:conservativeness}

Empirical results suggest that contamination by outliers in the calibration data often makes standard conformal p-values overly conservative, resulting in a type-I error rate significantly lower than the desired nominal level $\alpha$.

We begin by examining~\Cref{fig:scores-shuttle}, which provides some insight into this behavior based on the analysis of the ``shuttle'' dataset \cite{shuttle}, as detailed in Section~\ref{sec:experiments}. In this example, the nonconformity scores of outlier data points in the contaminated calibration set are typically larger than those of the inliers.
This pattern reflects the goal of a well-designed outlier detection model: to differentiate outliers from inliers by assigning higher scores to the former. Consequently, conformal p-values computed using~\eqref{eq:conformal-p-value} will be inflated relative to the ideal scenario in which all calibration points are inliers, reducing our power to detect true outliers at test time.

\begin{figure}[!htb]
\centering
    \includegraphics[width=0.5\textwidth]{figures/exp/real_data/shuttle/hist_shuttle_n_2500_p_0.05_p_train_0.05_a_0.02.pdf}
    \caption{Histogram of nonconformity scores for inliers and outliers in a contaminated calibration subset of the ``shuttle" data, with a contamination rate of 5\%. The vertical lines indicate the $(1-\alpha)$ empirical quantile of all calibration scores (black), as well as separately for inliers (blue) and outliers (red), with $\alpha = 0.02$.}
    \label{fig:scores-shuttle}
\end{figure}

This phenomenon is further corroborated by extensive numerical experiments presented in Section~\ref{sec:experiments} and \Cref{app-sec:real-data-exp,app-sec:images-data-exp}, which consistently demonstrate this conservative behavior across nine different datasets.

While the conservativeness of conformal prediction methods in the presence of contaminated calibration data has already been observed and studied theoretically in different contexts \citep{einbinder2022conformal,sesia2023adaptive,clarkson2024splitconformalpredictiondata}, prior works did not focus on outlier detection.
Therefore, it is helpful to introduce a new theoretical result that precisely quantifies the inflation of standard conformal p-values that we often observe in practice. 
This will serve as a foundation for the novel method of computing adaptive conformal p-values presented in the next section.

Let $\hat{F}_{1}$ denote the empirical cumulative distribution function (CDF) of the scores in $\D_{\mathrm{outlier}}$ and $\hat{Q}_{1-\alpha}^{\mathrm{cal}}$ represent the $\lceil (1-\alpha)(1+n)\rceil$-th smallest score in the calibration set.

\begin{lemma}
\label{lem:conservativeness}
Under the setup defined in~\eqref{eq:setup-contaminated}, if $\mathcal{H}_0$ is true, then for any  $\alpha\in(0,1)$,
    \begin{align*}
    \p &\left( \hat{p}_{n+1}\leq \alpha \right) \leq \alpha -\frac{n_1}{n_0 + 1} \left( 1-\alpha -   \E\left[ \hat{F}_{1} \left( \hat{Q}^{\mathrm{cal}}_{1-\alpha} \right) \right]\right).
    \end{align*}
\end{lemma}

This result is related to Theorem 1 in \citet{sesia2023adaptive}, which studies the behavior of conformal prediction sets for multi-class classification \citep{lei2013distribution,romano2020classification} calibrated with contaminated data. The key distinction is in our treatment of the calibration set: we assume that $n_0$ and $n_1$ are fixed, whereas \citet{sesia2023adaptive} consider a mixture model where the observed proportions of data points from different classes in the calibration set are random.
While treating $n_0$ and $n_1$ as fixed is convenient for this paper, we also include an additional result (\Cref{cor:conservativeness-p-values}) in Appendix~\ref{app-sec:proofs}, which reaches qualitatively similar conclusions by adopting an approach more closely aligned with Theorem 1 from \citet{sesia2023adaptive}.

A direct corollary of \Cref{lem:conservativeness} is that standard conformal p-values are conservative when outlier scores are typically larger than inlier scores. Formally, this condition is:
\begin{assumption}\label{asm:model-scores} 
$\E [ \hat{F}_{1} ( \hat{Q}_{1-\alpha}^{\mathrm{cal}} ) ] < 1-\alpha$. 
\end{assumption}
If \Cref{asm:model-scores} fails—for instance, when outlier scores are smaller than inlier scores—data contamination may invalidate standard conformal p-values, inflating the type-I error rate.
However, it is more common in practice that \Cref{asm:model-scores} holds, in which case contamination tends to reduce calibration power, and more so if $n_1$ is large.
In particular, \Cref{asm:model-scores} holds if:
(i) the outlier detection model is relatively accurate, and
(ii) the outlier distribution $\p_1$ is not adversarial.
Our experiments will show this power loss can be substantial, motivating the need for new methods that can approximately ``clean up'' the calibration data.


\section{Methods}\label{sec:trim}

\subsection{Key Idea: Boosting Power by Cleaning the Data}

Ideally, we would like to remove all $n_1$ outliers from $\D_{\mathrm{cal}}$, restoring the ideal behavior of conformal p-values calibrated on a clean dataset, as described in \Cref{prop:standard-conformal}, and likely boosting power. However, manually labeling the entire contaminated calibration set $\D_{\mathrm{cal}}$ is often impractical, especially when $n = |\D_{\mathrm{cal}}|$ is large. At the same time, utilizing only a small calibration set is not always desirable.

A large calibration set is often needed because the smallest conformal p-value obtainable through~\eqref{eq:conformal-p-value} scales as $1/n$. Thus, a large $n$ is critical for achieving high confidence in identifying outliers, especially in ``needle-in-a-haystack'' scenarios \citep{conformal-p-values}, where a few outliers must be detected in a large test set dominated by inliers. In such cases, the ability to obtain very small p-values is essential to achieve non-trivial power while controlling the false discovery rate \citep{BH}.

\subsection{A Simple but Unsatisfactory Approach: Naive-Trim} \label{sec:naive-trim}

The above challenge underscores the need for a method to mitigate the impact of outliers in the calibration dataset without requiring exhaustive annotation. An intuitive approach is to forgo annotations and simply remove all ``suspicious'' data points with large nonconformity scores. For instance, one could remove the top $m$ scores from $\D_{\mathrm{cal}}$, where $m$ is a fixed guess of the true number of outliers $n_1$ in the calibration set. We refer to this approach as \texttt{Naive-Trim}.

While \texttt{Naive-Trim} can reduce conservativeness by removing some outliers, it is not a satisfactory solution as it risks ``over-compensating''. By potentially removing also true inliers with large nonconformity scores, it can significantly skew the inlier score distribution to the left. This side effect is problematic, as it tends to invalidate conformal p-values and inflate the type-I error rate, over-correcting the conservativeness of standard conformal p-values.

This issue is particularly pronounced when $m > n_1$ or in noisy settings where the outlier detection model cannot perfectly distinguish between inliers and outliers. For example, as shown in Section~\ref{sec:experiments}, applying \texttt{Naive-Trim} to the dataset illustrated in \Cref{fig:scores-shuttle} results in uncontrolled inflation of the type-I error rate.

To address this challenge, we will now present a more sophisticated method, which we refer to as  \texttt{Label-Trim}. These approach utilize a limited labeling budget to remove outliers from $\D_{\mathrm{cal}}$ in a more reliable manner, mitigating the risk of over-correcting the conformal p-value.

\subsection{The \texttt{Label-Trim} Method}\label{sec:label-trim}

Consider having a limited budget to label $m < n$ calibration samples, where $m$ is much smaller than $n$. We aim to utilize this budget to remove as many outliers as possible from the calibration set without altering the inlier score distribution.

A practical approach is to annotate the $m$ largest scores in $\D_{\mathrm{cal}}$, as these are most likely outliers based on the model. Denote these annotated samples as $\D_{\mathrm{labeled}} \subseteq \D_{\mathrm{cal}}$, and let $\D_{\mathrm{labeled}}^{\mathrm{outlier}}$ denote the subset of annotated data points that are true outliers. Removing these outliers from $\D_{\mathrm{cal}}$ yields a smaller, cleaner calibration set, which we call $\D_{\mathrm{cal}}^{\mathrm{LT}} \subseteq \D_{\mathrm{cal}}$.

The \texttt{Label-Trim} method then calculates a refined conformal p-value, now denoted as $\hat{p}^{\mathrm{LT}}_{n+1}$, following the same procedure as in~\eqref{eq:conformal-p-value} with $\D_{\mathrm{cal}}$ replaced by the (partially) cleaned calibration set $\D_{\mathrm{cal}}^{\mathrm{LT}}$:
\begin{align} \label{eq:LT-p-value}
  \hat{p}^{\mathrm{LT}}_{n+1} = \frac{1 + \sum_{i \in \D_{\mathrm{cal}}^{\mathrm{LT}}} \mathbb{I}[s(X_i) \geq s(X_{n+1})]}{1 + |\D_{\mathrm{cal}}^{\mathrm{LT}}|}. 
\end{align} 
Algorithms~\ref{algo:label-trim-construction} and~\ref{algo:label-trim-testing} summarize this procedure, which intuitively offers advantages over both the standard method for computing $\hat{p}_{n+1}$ in~\eqref{eq:conformal-p-value}, by potentially increasing power, and the \texttt{Naive-Trim} approach, by mitigating the risk of over-correcting $\hat{p}_{n+1}$.

\begin{algorithm}[!htb]
\caption{Label-trim calibration  (construction phase)}
\label{algo:label-trim-construction}
\begin{algorithmic}[1]
\STATE \textbf{Input:} labeling budget $m$; contaminate calibration-set $\D_{\mathrm{cal}} = \left\{ X_i \right\}_{i=1}^{n}$; score function $s(\cdot)$, obtained by a pre-trained outlier detection model;

\STATE Compute the calibration scores $S_i = s(X_i)$, $\forall i\in \D_{\mathrm{cal}}$.
\STATE Sort the calibration scores, such that $S_{\pi(1)} \leq \dots \leq S_{\pi(n)}$ where $\pi : [n] \rightarrow [n]$ is the corresponding permutation of the indices.
\STATE Annotate the $m$ largest scores  $\D_{\mathrm{labeled}}$:= $\{ (S_{\pi(i)}, Y_{\pi(i)}) : i > n-m\}$, with $Y_{\pi(i)}=0$ if $X_{\pi(i)}$ is an inlier and $Y_{\pi(i)}=1$ otherwise. 
\STATE Construct the trimmed calibration set $\D_{\mathrm{cal}}^{\mathrm{LT}} = \left\{\pi (i) : i \leq n - m\right\}\cup \left\{j:  j\in \D_{\mathrm{labeled}}\text{ and }Y_j=0\right\}$.
% \STATE Construct the trimmed calibration set $\D_{\mathrm{cal}}^{\mathrm{LT}} = \left\{S_{\pi(i)}\right\}_{i=1}^{n-m} \cup \left\{S_{j}:  j\in \D_{\mathrm{labeled}}\text{ and }Y_j=0\right\}$

\STATE \textbf{Output:} trimmed calibration set $\D_{\mathrm{cal}}^{\mathrm{LT}}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!htb]
\caption{Label-trim calibration (testing phase)}
\label{algo:label-trim-testing}
\begin{algorithmic}[1]
\STATE \textbf{Input:} test point $X_{n+1}$; score function $s$; trimmed calibration set $\D_{\mathrm{cal}}^{\mathrm{LT}}$; type-I error level $\alpha$;
% \STATE Compute the test score $S_{n+1}=s(X_{n+1})$
\STATE Compute the conformal p-value $\hat{p}^{\mathrm{LT}}_{n+1}$ according to \eqref{eq:LT-p-value}.
\STATE \textbf{Output:} reject the null hypothesis $\mathcal{H}_0$ if $\hat{p}_{n+1}^{\mathrm{LT}} \leq \alpha$, classifying $X_{n+1}$ as an outlier.
\end{algorithmic}
\end{algorithm}

The following theorem provides justification for \texttt{Label-Trim}, demonstrating that $\hat{p}^{\mathrm{LT}}_{n+1}$ is an approximately valid p-value under relatively mild conditions. While our method is intuitive, this result is nontrivial for two reasons.
First, \texttt{Label-Trim} cannot guarantee the removal of all outliers from the calibration set, as it may be that $m < n_1$ or some outliers are not among the $m$ largest scores. Second, it involves annotating the $m$ largest scores, revealing the true labels of some calibration points but not others, which could disrupt the exchangeability typically assumed among inlier data points in conformal inference.
Therefore, this justification requires novel proof techniques and does not follow directly from existing results.

Following a notation similar to that of Lemma~\ref{lem:conservativeness}, let $\hat{F}^{\mathrm{LT}}_{1}$ denote the empirical CDF of the scores in $\D^{\mathrm{LT}}_{\mathrm{outlier}}$. Define also $\hat{Q}^{\mathrm{LT}}_{1-\alpha}$ as the $\hat{i}_{\mathrm{LT}}$-th smallest element in $\{S_i\}_{i\in \D_{\mathrm{cal}}^{\mathrm{LT}}}\cup\{\infty\}$, with $\hat{i}_{\mathrm{LT}} :=\lceil (1-\alpha)(n^{\mathrm{LT}}+1)\rceil$ and $n^{\mathrm{LT}}:=\left|\D_{\mathrm{cal}}^{\mathrm{LT}}\right|$.

\begin{theorem}
    \label{thm:labeled-trim}
    Consider the setup in~\eqref{eq:setup-contaminated}, with $\mathcal{H}_0$ being true.
    For any fixed $\alpha \in (0,1)$, assume that $m \leq \alpha (n+1)$.
    Then,
    \begin{align*}
     \p \left( \hat{p}_{n+1}^{\mathrm{LT}} \leq \alpha \right) \leq \alpha + \frac{1}{n_0+1}
         - \E\left[ \frac{\hat{n}^{\mathrm{LT}}_{1}}{n_0+1} \left( (1-\alpha) - \hat{F}_{1}^{\mathrm{LT}} \left( \hat{Q}_{1-\alpha}^{\mathrm{LT}} \right) \right)\right].
    \end{align*}
\end{theorem}

The upper bound on the type-I error rate provided by Theorem~\ref{thm:labeled-trim} resembles that of Lemma~\ref{lem:conservativeness} and can be interpreted as follows: \texttt{Label-Trim} produces approximately valid conformal p-values if: (i) the labeling budget is small relative to the calibration set size, i.e., $m \leq \alpha(n+1)$; and (ii) the calibration set contains a large number of inliers, $n_0$.

However, the upper bound in Theorem~\ref{thm:labeled-trim} also suggests that \texttt{Label-Trim} may remain overly conservative, similar to standard conformal p-values, if (i) not all outliers are removed from the calibration set ($\hat{n}^{\mathrm{LT}}_{1} > 0$ with high probability), and (ii) the remaining outlier scores are generally larger than the remaining inlier scores, consistent with $\E [ \hat{F}^{\mathrm{LT}}_{1} ( \hat{Q}^{\mathrm{LT}}_{1-\alpha} ) ] < 1-\alpha$, akin to Assumption~\ref{asm:model-scores}. 

This potential conservative behavior arises naturally from the use of a limited labeling budget, especially when the model guiding the construction of the set $\D_{\mathrm{labeled}}$ fails to effectively detect true outliers. Nevertheless, as we will see in the next section, \texttt{Label-Trim} often enhances power.

\section{Experiments} \label{sec:experiments}

We turn to evaluate the performance of conformal outlier detection methods under contaminated data. The experiments presented in this section are conducted on nine benchmark datasets: three tabular datasets, listed in Section~\ref{sec:real-data-exp}, and six visual datasets, listed in \Cref{sec:img-exp}.

\paragraph{Methods} We compare the following methods:
\begin{itemize}[noitemsep, topsep=0pt]
    \item \texttt{Standard}: The basic conformal method that uses the contaminated reference set $\mathcal{D}_\text{cal}=\mathcal{D}_\text{inlier}\cup\mathcal{D}_\text{outlier}$.
    \item \texttt{Oracle}: An infeasible benchmark method where the reference set contains only inliers, i.e., $\mathcal{D}_\text{cal}=\mathcal{D}_\text{inlier}$.
    \item \texttt{Naive-Trim}: The baseline method from Section~\ref{sec:naive-trim}, which removes the top $r\%$ non-conformity scores from $\mathcal{D}_\text{cal}$, where $r = n_1 / (n_0 + n_1)$.
    \item \texttt{Label-Trim}: Our proposed reliable data-cleaning method from Section~\ref{sec:label-trim}, applied with a labeling budget of $m=50$ annotations to label the $m$ data points with the largest non-conformity scores from $\mathcal{D}_\text{cal}$.
    \item \texttt{Small-Clean}: A baseline method that uses the labeling budget to construct a small, clean reference set by (i) randomly selecting $m$ data points from $\mathcal{D}_\text{cal}$ and (ii) extracting the true inliers from this subset.
\end{itemize}

\paragraph{Setup and performance metrics} In all experiments, we randomly split a given dataset into disjoint training $\mathcal{D}_\text{train}$, calibration $\mathcal{D}_\text{cal}$, and test sets of inliers $\mathcal{D}_\text{test}^\text{inlier}$ and outliers $\mathcal{D}_\text{test}^\text{outlier}$. To simulate a realistic setting, we construct the training and contaminated calibration sets with the same contamination rate of $r\%$. The inlier $\mathcal{D}_\text{test}^\text{inlier}$ and outlier $\mathcal{D}_\text{test}^\text{outlier}$ test tests are used to compute the type-I error and power of the outlier detection model, respectively. To ensure fair comparisons, all conformal methods use the same outlier detection model, trained on $\mathcal{D}_\text{train}$. Performance metrics are evaluated across 100 random splits of the data. The size of each dataset, along with the details of how  $\mathcal{D}_\text{train}$, $\mathcal{D}_\text{cal}$, and $\mathcal{D}_\text{test}$ are constructed are provided in~\Cref{app-sec:data}. 

\subsection{Tabular Data} \label{sec:real-data-exp}

We now compare the performance of the different methods on three benchmark tabular datasets for outlier detection, previously used in the conformal literature \citep{conformal-p-values}. 
Since conclusions are similar across datasets, we focus here on results for the {\em shuttle} dataset \citep{shuttle}. Results for the {\em credit card} \citep{creditcard} and {\em KDDCup99} \citep{KDDCup99} datasets are presented in \Cref{app-sec:real-data-exp}. For all conformal methods, we use Isolation Forest~\citep{liu2008isolation} as the base outlier detection model, implemented using \texttt{scikit-learn} with default hyperparameters~\citep{sklearn_api}.

\begin{figure*}[!h]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend.pdf}
    \caption{Comparison of conformal outlier detection methods on a tabular dataset (``shuttle'') as a function of the contamination rate  $r$ . The target type-I error rate is  $\alpha = 0.02$. Left: Empirical type-I error. Middle: Average detection rate (power), where higher values indicate better performance. Right: Number of outliers trimmed by the \texttt{Label-Trim} method. Results are averaged across 100 random splits of the data. 
}
    \label{fig:shuttle-outlier-prop}
\end{figure*}

\Cref{fig:shuttle-outlier-prop} presents the performance metrics of each method as a function of the contamination rate $r$. 
Following the left panel in that figure, we can see that the \texttt{Standard} conformal method results in conservative type-I error control, with a decrease in the error rate as the outlier proportion increases---a behavior that is aligned with~\Cref{lem:conservativeness}. Notably, the type-I error of the \texttt{Oracle} method is tightly centered around $\alpha$, as guaranteed by~\Cref{prop:standard-conformal}. The \texttt{Naive-Trim} method does not control the type-I error rate, emphasizing the need for reliable data-cleaning procedures. In striking contrast, our \texttt{Label-Trim} method, achieves a valid type-I error rate. At lower outlier proportions, the empirical type-I error is close to  $\alpha$, but the method becomes more conservative as the outlier proportion increases. This observation aligns with the upper bound on the error rate derived in~\Cref{thm:labeled-trim}. Notably, as the contamination rate in the training data increases, the outlier detection model’s ability to distinguish between inliers and outliers weakens. This, in turn, adversely affects the effectiveness of forming a subset of data points for annotation, as demonstrated in the right panel of \Cref{fig:shuttle-outlier-prop}. The \texttt{Small-Clean} method also controls the type-I error but is more conservative than \texttt{Label-Trim} due to its much smaller reference set, which becomes even smaller as the contamination rate increases. Observe how the power of the \texttt{Small-Clean} method is lower than that of the \texttt{Standard} approach, despite the latter using a contaminated reference set. By contrast, our proposed \texttt{Label-Trim} method significantly improves the power of the \texttt{Standard} method and even achieves near-oracle performance when the outlier proportion is low.

Next, we study the effect of the labeling budget on the performance of our \texttt{Label-Trim} method. As shown in \Cref{fig:shuttle-labeled-exp}, increasing the labeling budget brings the \texttt{Label-Trim} method closer to the \texttt{Oracle} in terms of both type-I error and power. Notably, even with a modest budget of $40$–$50$ annotations, the power of \texttt{Label-Trim} is nearly indistinguishable from that of the \texttt{Oracle}. This is attributed to the method’s effective trimming of outliers, as shown in the right panel. Notably, for labeling budgets $m > 50$, the condition in~\Cref{thm:labeled-trim} no longer holds, and yet the \texttt{Label-Trim} method still achieves valid type-I error control at level  $\alpha$ in practice. This highlights the robustness of the proposed method to the choice of $m$ beyond the restrictions specified in \Cref{thm:labeled-trim}, where we attribute this robustness to the non-adversarial nature of the outlier distribution and the underlying detection model. 

\Cref{fig:shuttle-labeled-exp} also illustrates that the \texttt{Small-Clean} method lags behind \texttt{Label-Trim} both in terms of power and conservativeness. For small labeling budgets of $m < 45$, the coarse granularity of conformal p-values \eqref{eq:conformal-p-value} renders the method powerless; the smallest achievable p-value in this case is $1/(m+1) > 0.02 = \alpha$. Even for slightly larger labeling budgets, the conservative nature of the conformal p-value---specifically, the `plus 1’ term in \eqref{eq:conformal-p-value}---continues to have a significant impact. This effect is rigorously quantified by the lower bound on type-I error provided in \Cref{prop:standard-conformal}. For instance, with $m = 80$ and $\alpha = 0.02$, the lower bound is approximately $\alpha - 1/(m+1) \approx 0.0076$, which aligns closely with the empirical error rate shown in the left panel of \Cref{fig:shuttle-labeled-exp}. Overall, these results highlight the benefits of selectively cleaning a relatively large contaminated set compared to relying on a small clean reference set, offering both improved stability and higher power.

\begin{figure*}[htb]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_n.pdf}
    \caption{Comparison of conformal outlier detection methods on a tabular dataset (``shuttle'') as a function of the labeling budget $m$. The contamination rate is fixed to $r=0.03$. Other details are as in \Cref{fig:shuttle-outlier-prop}.}
    \label{fig:shuttle-labeled-exp}
\end{figure*}


Next, we examine how the target error level $\alpha$ affects the performance of different methods. \Cref{fig:shuttle-levels} shows that our \texttt{Label-Trim} method performs particularly well at low type-I error rates, especially when $\alpha$ is smaller than the contamination rate ($r=3\%$). This behavior can be explained as follows. For a relatively accurate model, the outliers primarily distort the tail of the empirical distribution of nonconformity scores---see \Cref{fig:scores-shuttle}. Consequently, the influence of these outliers on the rejection rule $\hat{p}_{n+1} \leq \alpha$ from \eqref{eq:conformal-p-value}, or $\hat{p}^{\text{LT}}_{n+1} \leq \alpha$ from \eqref{eq:LT-p-value}, diminishes as $\alpha$ increases.

\begin{figure*}[htb]
    \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/levels/IF_e_100_s_auto_train_5000_exp_levels_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/shuttle/levels/IF_e_100_s_auto_train_5000_exp_levels_shuttle_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_trm.pdf}
    \caption{Comparison of conformal outlier detection methods on a tabular dataset (``shuttle'') as a function of the target type-I error rate $\alpha$. The contamination rate $r$ is fixed to 3\%. Other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{fig:shuttle-levels}
\end{figure*}

\subsection{Visual Data}\label{sec:img-exp}

In what follows, we compare all methods using benchmark visual datasets for outlier detection. Similar to \citet{zhang2023openood}, we construct six datasets, where the inlier samples are always images from CIFAR10~\citep{cifar-10, cifar-data} and the outlier samples vary across datasets. Specifically, the outliers are drawn from (1) MNIST~\citep{deng2012mnist}, (2) SVHN~\citep{svhn}, (3) Texture~\citep{texture}, (4) Places365~\citep{texture}, (5) TinyImageNet~\citep{tinyimages}, and (6) CIFAR100~\citep{cifar-data}. For all datasets, we use the outlier detection model proposed by \citet{react}, which operates on feature representations extracted by a pre-trained ResNet18 model. More details are in~\Cref{app-sec:data}.

\Cref{tab:avg-images} summarizes the results for all six datasets. Overall, we can see a trend similar to the one of the tabular data: the \texttt{Standard} and \texttt{Small-Clean} methods are valid but conservative, the \texttt{Naive-Trim} fails to control the type-I error, and our \texttt{Label-Trim} achieves a significant boost in power while practically controlling the type-I error. Notably, our \texttt{Label-Trim} method attains near-oracle performance for low contamination rates. Detailed results for each dataset are provided in \Cref{app-sec:images-data-exp}.

\begin{table*}[!htb]
\caption{Comparison of conformal outlier detection methods on six visual datasets for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical type-I error values are averaged across all datasets. The empirical power is presented relative to the \texttt{Standard} method (higher is better), and averaged across all datasets. Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.
}
\label{tab:avg-images}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0317) & \cellcolor{white} 0.008 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0354) & \cellcolor{white} 0.005 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0408) & \cellcolor{white} 0.004 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.166 ($\pm$ 0.0336) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.549 ($\pm$ 0.0425) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.961 ($\pm$ 0.0531) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.659 ($\pm$ 0.0342) & \cellcolor{red!20} 0.017 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.79 ($\pm$ 0.045) & \cellcolor{red!20} 0.027 ($\pm$ 0.0006)  & \cellcolor{red!20} 4.16 ($\pm$ 0.0596) & \cellcolor{red!20} 0.036 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.166 ($\pm$ 0.0336) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.517 ($\pm$ 0.042) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.786 ($\pm$ 0.0498) & \cellcolor{white} 0.008 ($\pm$ 0.0003) \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0174) & \cellcolor{white} 0.027 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0189) & \cellcolor{white} 0.019 ($\pm$ 0.0005)  & \cellcolor{white} 1.0 ($\pm$ 0.0212) & \cellcolor{white} 0.015 ($\pm$ 0.0005) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.062 ($\pm$ 0.0176) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.235 ($\pm$ 0.0192) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.448 ($\pm$ 0.023) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.146 ($\pm$ 0.0175) & \cellcolor{red!20} 0.035 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.487 ($\pm$ 0.0186) & \cellcolor{red!20} 0.043 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.882 ($\pm$ 0.0224) & \cellcolor{red!20} 0.052 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.714 ($\pm$ 0.0448) & \cellcolor{white} 0.02 ($\pm$ 0.0021)  & \cellcolor{white} 0.869 ($\pm$ 0.0501) & \cellcolor{white} 0.02 ($\pm$ 0.002)  & \bfseries \cellcolor{Green!30} 1.033 ($\pm$ 0.0613) & \cellcolor{white} 0.021 ($\pm$ 0.0023) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.041 ($\pm$ 0.0177) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.139 ($\pm$ 0.019) & \cellcolor{white} 0.025 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.215 ($\pm$ 0.0226) & \cellcolor{white} 0.021 ($\pm$ 0.0006) \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}
\end{table*}
% \FloatBarrier
\section{Discussion}

In this work, we studied the robustness of conformal prediction under contaminated reference data. Motivated by empirical evidence, we characterized the conditions under which conformal outlier detection methods become too conservative. To improve power, we proposed the \texttt{Label-Trim} method, which leverages an outlier detection model and a limited labeling budget to remove outliers from the contaminated reference set. We also provided a theoretical justification for this approach, employing novel proof techniques. Numerical experiments with real data confirmed that standard conformal outlier detection methods are conservative under contaminated data and demonstrated that our \texttt{Label-Trim} method can significantly enhance power.

However, the experiments also reveal a limitation of our \texttt{Label-Trim} method: while it improves power compared to standard conformal inference, it often remains too conservative, particularly when the labeling budget is very limited, leaving room for further improvement. A promising direction for future research is to enhance \texttt{Label-Trim} with {\em active learning} strategies \cite{makili2012active,fannjiang2022conformal,prinsterconformal}, enabling the removal of more outliers without increasing the labeling budget.

A second limitation of the \texttt{Label-Trim} approach is its reliance on actively collecting new annotations. In scenarios where a flexible labeling budget is unavailable but access to a small, clean reference set is feasible, this dependency becomes restrictive. As our experiments demonstrate, the limited sample size imposes a fundamental constraint on the power of conformal outlier detection methods. This raises an intriguing question for future research: given a small clean reference set and a larger, contaminated reference set, how can we effectively and safely clean the contaminated data to enhance detection power at test time?

One potential solution could involve using the small clean reference set to calibrate a base outlier detection model. This calibrated model could then be employed to clean the larger contaminated set by removing detected outliers, while carefully accounting for inliers mistakenly classified as outliers. Exploring such a semi-supervised data-cleaning approach represents a promising direction for future work, though we anticipate that establishing the theoretical validity of such a method may not be straightforward.

\section*{Acknowledgments}
M.~S.~was partly supported by NSF grant DMS 2210637 and by a Capital One CREDIF Research Award.
Y.~R. and M.~B. were funded by the European Union (ERC, SafetyBounds, 101163414). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.


% \bibliography{bib}
\input{main.bbl}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix


\section{Mathematical Proofs}
\label{app-sec:proofs}

\subsection{Auxiliary Technical Results}

In this section, we begin by introducing two useful propositions, \Cref{app-prop:p-value-to-quantile} and \Cref{app-prop:quantiles}, which will be used later in the proofs presented here. 


\begin{proposition}
\label{app-prop:p-value-to-quantile}
    Let $\D$ be a dataset containing $n$ scores, and define the threshold $\hat{Q}_{1-\alpha}$ as
    \begin{align*}
\hat{Q}_{1-\alpha} &:= \hat{i} \text{-th smallest element in } \D\cup \{\infty\},
\end{align*}
where 
\begin{align*}
    \hat{i} :=\lceil (1-\alpha)(n+1)\rceil.
\end{align*}
For any test point $X_{n+1}$, the following holds:
    \begin{align*}
        s(X_{n+1}) > \hat{Q}_{1-\alpha} \quad\text{ if and only if }\quad \hat{p}_{n+1} \leq \alpha,
    \end{align*}
    where $\hat{p}_{n+1}$ is the conformal p-value \eqref{eq:conformal-p-value}.
\end{proposition}

\begin{proof}[Proof of \Cref{app-prop:p-value-to-quantile}]
The proof follows the definition of conformal p-value from \eqref{eq:conformal-p-value}, and its relation to the empirical quantile function:
\begin{alignat*}{2}
    \hat{p}_{n+1} = \frac{1 + \sum_{i=1}^{n} \mathbb{I}[s(X_i) \geq s(X_{n+1})]}{n+1}
    &\leq \alpha &&\overset{(i)}{\Longleftrightarrow} \\
    \hat{p}_{n+1} = \frac{1 + \sum_{i=1}^{n} \mathbb{I}[s(X_i) \geq s(X_{n+1})]}{n+1}
    &\leq \frac{\lfloor \alpha(n+1) \rfloor }{n+1} &&\Longleftrightarrow \\
    1 + \sum_{i=1}^{n} \mathbb{I}[s(X_i) \geq s(X_{n+1})] &\leq \lfloor \alpha (n+1) \rfloor &&\Longleftrightarrow \\
    1 + n - \sum_{i=1}^{n} \mathbb{I}[s(X_i) < s(X_{n+1})] &\leq \lfloor \alpha (n+1) \rfloor &&\Longleftrightarrow \\
     \sum_{i=1}^{n} \mathbb{I}[s(X_i) < s(X_{n+1})] &\geq n + 1 - \lfloor \alpha (n+1) \rfloor &&\overset{(ii)}{\Longleftrightarrow} \\
    \sum_{i=1}^{n} \mathbb{I}[s(X_i) < s(X_{n+1})] &\geq  \lceil (1-\alpha) (n+1) \rceil && \numberthis \label{app-eq:prop-inequality}
\end{alignat*}
The labeled steps above can be explained as follows.
\begin{itemize}
    \item (i) The values of $\hat{p}_{n+1}$ are discrete, taking values from $\{\frac{1}{n+1}, \frac{2}{n+1}, \dots, 1\}$. Therefore, $\hat{p}_{n+1} = \frac{k}{n+1}$ for some $k\in[n+1]$. We explicitly prove that $\hat{p}_{n+1}\leq \alpha$ iff $\hat{p}_{n+1}\leq \frac{\lfloor \alpha(n+1)\rfloor}{n+1}$ as follows:
    \begin{itemize}
        \item[$\Leftarrow$] Assume $\hat{p}_{n+1} \leq \frac{\lfloor \alpha(n+1)\rfloor}{n+1}$. Therefore, $\hat{p}_{n+1} \leq \frac{\lfloor \alpha(n+1)\rfloor}{n+1} \leq \frac{\alpha(n+1)}{n+1} = \alpha$.
        \item[$\Rightarrow$] Assume $\hat{p}_{n+1} \leq \alpha$, then $\frac{k}{n+1}\leq \alpha$. This implies that 
        $k \leq \alpha (n+1)$. Since $k$ is an integer, it follows that $k \leq \lfloor \alpha (n+1) \rfloor$. Therefore, $\hat{p}_{n+1} = \frac{k}{n+1} \leq \frac{\lfloor \alpha (n+1) \rfloor}{n+1}$.
    \end{itemize}
    \item (ii) This step follows directly from the equality $n + 1 = \lceil (1-\alpha)(n+1)\rceil + \lfloor \alpha(n+1)\rfloor$. We explicitly prove this equality as follows:
    \begin{itemize}
        \item The term $\lceil (1-\alpha) (n+1)\rceil$ represents the smallest integer greater than or equal to $(1-\alpha)(n+1)$. Hence, we can write:
        \begin{align*}
            \lceil (1-\alpha)(n+1)\rceil = (1-\alpha)(n+1) + \delta_1, 
        \end{align*}
        where $0\leq\delta_1<1$.
        \item Similarly, the term $\lfloor \alpha (n+1)\rfloor$ represents the largest integer less than or equal to $\alpha(n+1)$. Thus:
        \begin{align*}
            \lfloor \alpha(n+1)\rfloor = \alpha(n+1) - \delta_2, 
        \end{align*}
        where $0\leq\delta_2<1$.
        \item Adding these two terms gives:
    \begin{align*}
        \lfloor \alpha(n+1)\rfloor + \lceil (1-\alpha)(n+1)\rceil = \alpha(n+1) - \delta_2 + (1-\alpha)(n+1) + \delta_1 = n + 1 + (\delta_1 - \delta_2).
    \end{align*}
    Since $\lfloor \alpha(n+1)\rfloor + \lceil (1-\alpha)(n+1)\rceil$ must be an integer and $\delta_1,\delta_2\in [0,1)$, it follows that $\delta_1 - \delta_2 = 0$.
    \end{itemize}
    
    Therefore, $\lfloor \alpha(n+1)\rfloor + \lceil (1-\alpha)(n+1)\rceil = n + 1$.
\end{itemize}

To complete the proof, we now show that \eqref{app-eq:prop-inequality} holds if and only if $s(X_{n+1}) > \hat{Q}_{1-\alpha}$. 
\begin{itemize}
    \item[$\Leftarrow$] Assume $s(X_{n+1}) > \hat{Q}_{1-\alpha}$. 
    By definition, $\sum_{i=1}^{n} \mathbb{I}[s(X_i) \leq \hat{Q}_{1-\alpha}] = \lceil (1-\alpha)(n+1)\rceil$.
    Then, \eqref{app-eq:prop-inequality} holds since
    \begin{align*}
        \sum_{i=1}^{n} \mathbb{I}[s(X_i) < s(X_{n+1})] \geq \sum_{i=1}^{n} \mathbb{I}[s(X_i) \leq \hat{Q}_{1-\alpha}] =  \lceil (1-\alpha)(n+1)\rceil.
    \end{align*}
    \item[$\Rightarrow$] We prove this direction by contradiction, assuming that \eqref{app-eq:prop-inequality} holds. Now, suppose that $s(X_{n+1})\leq \hat{Q}_{1-\alpha}$ also holds, implying that
    \begin{align*}
        \sum_{i=1}^{n} \mathbb{I}[s(X_i) < s(X_{n+1})] \leq \sum_{i=1}^{n} \mathbb{I}[s(X_i) < \hat{Q}_{1-\alpha}] \overset{(i)}{<} \lceil (1-\alpha) (n+1) \rceil,
    \end{align*}
    which contradicts the assumption \eqref{app-eq:prop-inequality}. Therefore, we conclude that $s(X_{n+1}) > \hat{Q}_{1-\alpha}$. The last step above can be explained as follows.
    \begin{itemize}
        \item (i) Recall that by definition, $\hat{Q}_{1-\alpha}$ is a specific value in $\{s(X_i)\}_{i=1}^{n}$ and $\sum_{i=1}^{n} \mathbb{I}[s(X_i) \leq \hat{Q}_{1-\alpha}] = \lceil (1-\alpha)(n+1)\rceil$.
This implies that
\begin{align*}
%\label{eq:quantile_and_p}
    \sum_{i=1}^{n} \mathbb{I}[s(X_i) < \hat{Q}_{1-\alpha}] 
 = \lceil (1-\alpha)(n+1)\rceil - \sum_{i=1}^{n} \mathbb{I}[s(X_i) = \hat{Q}_{1-\alpha}] < \lceil (1-\alpha)(n+1)\rceil.
\end{align*}
    \end{itemize}
\end{itemize}
In sum, $\hat{p}_{n+1}\leq \alpha$ holds if and only if \eqref{app-eq:prop-inequality} holds, and the latter holds if and only if $s(X_{n+1}) > \hat{Q}_{1-\alpha}$. This completes the proof.
\end{proof}

\begin{proposition}\label{app-prop:quantiles}
    Let $\D$ be a dataset containing $n$ scores, and let $S_{n+1}$ be a test score. Define the following thresholds:
        \begin{align*}
\hat{Q}_{1-\alpha} &:= \hat{i} \text{-th smallest element in } \D\cup \{\infty\},
\end{align*}
where 
\begin{align*}
    \hat{i} :=\lceil (1-\alpha)(n+1)\rceil.
\end{align*}
Similarly, let
    \begin{align*}
\hat{Q}_{1-\alpha}^{n+1} &:= \hat{i} \text{-th smallest element in } \D\cup \{S_{n+1}\}.
\end{align*}
It follows that
\begin{align*}
    \hat{Q}_{1-\alpha} \geq \hat{Q}_{1-\alpha}^{n+1} \text{ almost surely}.
\end{align*}
\end{proposition}

\begin{proof}[Proof of \Cref{app-prop:quantiles}]
    Since the largest possible score is $\infty$, the set $\D \cup \{\infty \}$ almost surely contains scores that are greater or equal to those in $\D \cup \{S_{n+1}\}$. Consequently, $\hat{Q}_{1-\alpha} := \hat{i}$-th smallest element in $\D \cup \{ \infty \}$ is almost surely greater than or equal to $\hat{Q}_{1-\alpha}^{n+1} := \hat{i}$-th smallest element in $\D \cup \{S_{n+1}\}$.
\end{proof}

\subsection{Explaining the Conservativeness of Standard Conformal p-Values}

\subsubsection{Proof of~\Cref{lem:conservativeness}}

\begin{proof}[Proof of~\Cref{lem:conservativeness}]
\label{prf:lem}
To simplify the notation define the random score $S_i := s(X_i)$ for all $i\in \D_{\mathrm{cal}} \cup \{n+1\}$. Throughout the proof, we refer to the calibration set as the set of nonconformity scores corresponding to the calibration points.
Without loss of generality, assume that the inliers in $\D_{\mathrm{cal}}$ are located at the first $n_0$ indices. Let $\D_{\mathrm{inlier}}=[n_0]$ denote the set of indices corresponding to the inlier scores. Consequently, define $\D_{\mathrm{outlier}} = \{n_0+1, n_0+2, \ldots, n\}$ as the set of indices corresponding to the outlier scores in $\D_{\mathrm{cal}}$.
We assume the scores have no ties (which can always be achieved by adding a negligible random noise to the scores output by any model).

Given a fixed realization of the score vector $(s_1,\ldots,s_n,s_{n+1}) \in \mathbb{R}^{n+1}$, define the following two events: 
    \begin{itemize}
        \item $E_{\mathrm{in}}$: the unordered set of inlier scores, including the test score, is $\{S_1,\dots,S_{n_0},S_{n+1}\} = \{s_1,\dots,s_{n_0},s_{n+1}\}$;
        \item $E_{\mathrm{out}}$: the unordered set of outlier scores is $\{S_{n_0+1}, \dots, S_n\} = \{s_{n_0+1}, \dots, s_n\}$.
    \end{itemize}

Under the setup defined in~\eqref{eq:setup-contaminated}, when $\mathcal{H}_0$ is true, the test score $S_{n+1}$ and the inlier scores in the calibration set are i.i.d.~from $\p_0$. 
    Therefore, by exchangeability, the following holds for each inlier index $i\in \D_{\mathrm{inlier}}\cup\{n+1\}$:
    \begin{align}\label{eq:exch-inlier}
        \p \left( S_{n+1} = s_i \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right) =         \frac{1}{n_0 +1}.
    \end{align}
    Since the calibration scores are almost-surely distinct, the probability of a null test point obtaining any outlier score is zero. Therefore, for each outlier index $j\in \D_{\mathrm{outlier}}$:
    \begin{align}\label{eq:exch-outlier}
        \p \left( S_{n+1} = s_j \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right) = 0.
    \end{align}

To obtain an upper bound on the type-I error rate, $\p \left( \hat{p}_{n+1} \leq \alpha \right)$, we use the equivalence established in~\Cref{app-prop:p-value-to-quantile}. According to this result, the following holds:
\begin{align*}
    \p \left( \hat{p}_{n+1} \leq \alpha \right) = \p \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{cal}}\right),
\end{align*}
where $\hat{Q}^{\mathrm{cal}}_{1-\alpha}$ is the $\hat{i}_{\mathrm{cal}}$-th smallest element in $\{S_i\}_{i=1}^{n}\cup\{\infty\}$ and $\hat{i}_{\mathrm{cal}} :=\lceil (1-\alpha)(n+1)\rceil$.

Moreover, define $\hat{Q}_{1-\alpha}^{n+1}$ as the $\hat{i}_{\mathrm{cal}}$-th smallest score in $\{S_i\}_{i=1}^{n+1}$.
By~\Cref{app-prop:quantiles}, $\hat{Q}^{\mathrm{cal}}_{1-\alpha} \geq \hat{Q}_{1-\alpha}^{n+1}$ almost surely.\\


Now, we obtain an upper bound for $\p \left( S_{n+1} > \hat{Q}^{\mathrm{cal}}_{1-\alpha} \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right)$, where the probability is taken over random permutations of the scores conditional on $E_{\mathrm{in}}, E_{\mathrm{out}}$.
    \begin{align*} 
        \p \left( S_{n+1} > \hat{Q}^{\mathrm{cal}}_{1-\alpha} \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right) &\leq \p \left( S_{n+1} > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right)\\
        &= \E \left[ \mathbb{I} \left[ S_{n+1} > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right]\mid E_{\mathrm{in}}, E_{\mathrm{out}} \right] \\
     &= \sum\limits_{i\in\D_{\mathrm{inlier}}\cup\{n+1\}} \E \left[ \mathbb{I} \left[S_{n+1}=s_i \right] \mathbb{I}\left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right] \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right]\\
          &\overset{(i)}{=} \sum\limits_{i\in\D_{\mathrm{inlier}}\cup\{n+1\}} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right] \p \left(  S_{n+1}=s_i   \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right)\\
        &= \frac{1}{n_0+1} \sum\limits_{i\in  \D_{\mathrm{inlier}} \cup \{ n+1\}} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right] \\
        &\overset{(ii)}{\leq}  \frac{1}{n_0+1} \left(\alpha(n+1) - \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right]\right)\\
        &= \alpha + \frac{1}{n_0+1} \left( \alpha n_1 - \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right]\right)\\
        &= \alpha - \frac{n_1}{n_0+1} \left( 1-\alpha - \frac{1}{n_1} \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I} \left[ s_i \leq \hat{Q}^{\mathrm{n+1}}_{1-\alpha}\right]\right) \\
        &\overset{(iii)}{\leq} \alpha - \frac{n_1}{n_0+1} \left( 1-\alpha - \frac{1}{n_1} \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I} \left[ s_i \leq \hat{Q}^{\mathrm{cal}}_{1-\alpha}\right]\right) \\
        &= \alpha - \frac{n_1}{n_0 + 1} \left( 1-\alpha -\hat{F}_{1} \left( \hat{Q}^{\mathrm{cal}}_{1-\alpha} \right) \right),
    \end{align*}
where $\hat{F}_{1}$ is the empirical CDF of the outlier scores.
The labeled steps above can be explained as follows.
\begin{itemize}
\item (i) $\mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right]$ is measurable with respect to the $\sigma$-algebra generated by $E_{\mathrm{in}}, E_{\mathrm{out}}$. This follows because $\hat{Q}_{1-\alpha}^{\mathrm{n+1}}$ is the $\hat{i}_{\mathrm{cal}}$-th smallest element of $\{s_1,\dots,s_{n+1}\}$, which is fully determined by these variables. Thus, we can pull it out of the expectation.\\
\item (ii) $\hat{Q}^{\mathrm{n+1}}_{1-\alpha}$ is the $\hat{i}_{\mathrm{cal}}$-th smallest score in $\{s_1,\dots,s_n,s_{n+1}\}$ 
  and $[n+1] = \D_{\mathrm{outlier}} \cup \D_{\mathrm{inlier}} \cup \{n+1\}$. By definition,\\ 
  \begin{align*}
    &\sum\limits_{i\in  \D_{\mathrm{inlier}} \cup \{ n+1\}} \mathbb{I}\left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right] + \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right] = 
      \sum\limits_{i=1}^{n+1} \mathbb{I} \left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right] = \lfloor\alpha(n+1) \rfloor
      \leq \alpha(n+1)\\
    \intertext{and therefore,}\\
    &\sum\limits_{i\in  \D_{\mathrm{inlier}} \cup \{ n+1\}} \mathbb{I}\left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right]  \leq \alpha(n+1) - \sum\limits_{i\in  \D_{\mathrm{outlier}}} \mathbb{I}\left[ s_i > \hat{Q}^{\mathrm{n+1}}_{1-\alpha} \right]
  \end{align*}
\item (iii) Since $\hat{Q}^{\mathrm{cal}}_{1-\alpha} \geq \hat{Q}_{1-\alpha}^{n+1}$ almost surely, increasing the threshold (i.e., using $\hat{Q}^{\mathrm{cal}}_{1-\alpha}$) results in an equal or larger value of the sum.
\end{itemize}


Now, we can derive an upper bound for $\p \left(\hat{p}_{n+1} \leq \alpha\right)$ as follows:
\begin{align*}
\p \left(\hat{p}_{n+1} \leq \alpha\right) &= 
\p \left( S_{n+1} > \hat{Q}^{\mathrm{cal}}_{1-\alpha} \right)\\
&= \E\left[ \p \left( S_{n+1} > \hat{Q}^{\mathrm{cal}}_{1-\alpha} \mid E_{\mathrm{in}}, E_{\mathrm{out}}\right) \right]\\
    &\leq \E\left[\alpha - \frac{n_1}{n_0 + 1} \left( 1-\alpha - \hat{F}_{1} \left( \hat{Q}^{\mathrm{cal}}_{1-\alpha} \right)\ \right)\right]\\
    &= \alpha -\frac{n_1}{n_0 + 1} \left( 1-\alpha -   \E\left[ \hat{F}_{1} \left( \hat{Q}^{\mathrm{cal}}_{1-\alpha} \right) \right]\right),
\end{align*}
with this expectation being taken over different realizations of the inlier and outlier nonconformity scores.
\end{proof}

\subsubsection{An Alternative View Based on Mixture Distributions}

Next, we provide an additional theoretical result concerning the conservativeness of conformal outlier detection methods, to supplement the result presented in Section~\ref{sec:conservativeness} from a point of view closer to that of \citet{sesia2023adaptive}. 

Specifically, we consider a contaminated calibration set, $\D_{\mathrm{cal}}$, which may include both inliers (samples i.i.d. from $\p_0$) and outliers (samples i.i.d. from $\p_1 \neq \p_0$). The goal remains to test the null hypothesis $\mathcal{H}_0$ that a new data point $X_{n+1}$ is an inlier, independently sampled from $\p_0$.

This setup differs from \eqref{eq:setup-contaminated} in that the calibration set is drawn from a mixed distribution, where the proportion of outliers in the population is denoted by $\delta\in [0,1)$. Hence, the numbers of inliers and outliers in the calibration set are random, rather than fixed. Formally, this setup is expressed as:

\begin{align} \label{eq:setup-contaminated-random} 
\begin{split}
& X_i \overset{\text{i.i.d.}}{\sim} \p_{\mathrm{mixed}} = (1-\delta)\cdot \p_0 + \delta \cdot P_1, \quad \forall i \in \D_{\mathrm{cal}},\\ 
& \mathcal{H}_0 : X_{n+1} \overset{\text{ind.}}{\sim} \p_0.  
\end{split}\end{align}

Let $F_0$ and $F_1$ denote the CDFs of $\p_0$ and $\p_1$, respectively, and $\hat{Q}_{1-\alpha}^{\mathrm{cal}}$ represent the $\lceil (1-\alpha)(n+1)\rceil$-th smallest score in the calibration set.

\begin{corollary}[Conservativeness]
\label{cor:conservativeness-p-values}
    Under the setup defined in~\eqref{eq:setup-contaminated-random}, if $\mathcal{H}_0$ is true, then, for any $\alpha\in(0,1)$,
    \begin{align*}
    \p &\left( \hat{p}_{n+1}\leq \alpha \right) \leq \alpha - \delta \E \left[ F_0(\hat{Q}_{1-\alpha}^{\mathrm{cal}}) - F_1(\hat{Q}_{1-\alpha}^{\mathrm{cal}} )\right].
    \end{align*}
\end{corollary}

\Cref{cor:conservativeness-p-values} reformulates Theorem 1 in \citet{sesia2023adaptive} under the setup in \eqref{eq:setup-contaminated-random}. This result quantifies the behavior of conformal outlier detection methods in the presence of contaminated data and establishes guarantees on the type-I error rate. 
This result complements our analysis of the conservativeness of these methods.

\begin{proof}[Proof of~\Cref{cor:conservativeness-p-values}]
The proof adapts the argument of Theorem 1 in \citet{sesia2023adaptive} to the outlier detection setting considered here. Specifically, we follow the structure of the original proof, making adjustments to account for the presence of inliers and outliers in the calibration set.

By~\Cref{app-prop:p-value-to-quantile}, we have
\begin{align*}
    \hat{p}_{n+1} \leq \alpha \Longleftrightarrow S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{cal}}.
\end{align*}

Under the null, we upper bound $\p \left( \hat{p}_{n+1}\leq \alpha\right)$ as follows:
    \begin{align*}
        \p_0 \left( \hat{p}_{n+1} \leq \alpha \right) &= \p_0 \left( \hat{p}_{n+1} \leq \alpha\right) + \p_{\mathrm{mixed}} \left( \hat{p}_{n+1} \leq \alpha\right) - \p_{\mathrm{mixed}} \left( \hat{p}_{n+1} \leq \alpha\right) \\
        &= \p_{\mathrm{mixed}} \left( \hat{p}_{n+1} \leq \alpha\right) - \left[ \p_{\mathrm{mixed}} \left( \hat{p}_{n+1} \leq \alpha\right) - \p_0 \left( \hat{p}_{n+1} \leq \alpha\right)\right]\\
        &\leq \alpha - \left[ \p_{\mathrm{mixed}} \left( \hat{p}_{n+1} \leq \alpha\right) - \p_0 \left( \hat{p}_{n+1} \leq \alpha\right)\right]\\
        &= \alpha - \left[ \left( (1-\delta)\p_0 \left( \hat{p}_{n+1} \leq \alpha\right) + \delta\p_1 \left( \hat{p}_{n+1} \leq \alpha\right)\right) - \p_0 \left( \hat{p}_{n+1} \leq \alpha\right) \right]\\
        &= \alpha - \delta\left[ \p_1 \left( \hat{p}_{n+1} \leq \alpha\right) - \p_0 \left( \hat{p}_{n+1} \leq \alpha\right)\right]\\
        &= \alpha - \delta\left[ \p_1 \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{cal}}\right) - \p_0 \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{cal}}\right) \right]\\
        &= \alpha - \delta\left[ \p_0 \left( S_{n+1} \leq \hat{Q}_{1-\alpha}^{\mathrm{cal}}\right) - \p_1 \left( S_{n+1} \leq \hat{Q}_{1-\alpha}^{\mathrm{cal}}\right) \right]\\
        &= \alpha - \delta\E \left[ \p_0 \left( S_{n+1} \leq \hat{Q}_{1-\alpha}^{\mathrm{cal}} \mid \D_{\mathrm{cal}}\right) - \p_1 \left( S_{n+1} \leq \hat{Q}_{1-\alpha}^{\mathrm{cal}} \mid \D_{\mathrm{cal}}\right) \right]\\
        &= \alpha - \delta\E \left[ F_0 \left( \hat{Q}_{1-\alpha}^{\mathrm{cal}} \right) - F_1 \left( \hat{Q}_{1-\alpha}^{\mathrm{cal}} \right) \right].
    \end{align*}
\end{proof}


\subsection{Validity of the \texttt{Label-Trim} Method}

\subsubsection{Proof of~\Cref{thm:labeled-trim} | Main Steps}

\begin{proof}[Proof of~\Cref{thm:labeled-trim}]
\label{prf:labeled-trim}
As in the proof of~\Cref{lem:conservativeness}, define the random score $S_i := s(X_i)$ for all $i\in \D_{\mathrm{cal}} \cup \{n+1\}$. 
By~\Cref{app-prop:p-value-to-quantile}, for any fixed $\alpha \in (0,1)$, the probability of a type-I error, $\p \left( \hat{p}_{n+1}^{\mathrm{LT}} \leq \alpha \right)$, can be expressed as
\begin{align*}
    \p \left( \hat{p}_{n+1}^{\mathrm{LT}} \leq \alpha \right) = \p \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{LT}}\right).
\end{align*}

Consider the augmented set $\{S_i\}_{i\in \D_{\mathrm{cal}}^{\mathrm{LT}}}\cup \{ S_{n+1}\}$, which includes the test score $S_{n+1}$. Define $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ as follows:
\begin{align*}
\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}} &:= \hat{i}_{\mathrm{LT}} \text{-th smallest element in } \{S_i\}_{i\in \D_{\mathrm{cal}}^{\mathrm{LT}}}\cup \{S_{n+1}\}.
\end{align*}
By~\Cref{app-prop:quantiles}, it holds that $\hat{Q}_{1-\alpha}^{\mathrm{LT}} \geq \hat{Q}_{1-\alpha}^{\mathrm{LT, n+1}}$ almost surely. 

Now, consider an imaginary ``mirror" version of this method that applies the label-trim algorithm with two key differences:
\begin{itemize}
\item it uses a larger labeling budget, $\tilde{m}=m+1$;
\item it treats $\{S_i\}_{i=1}^{n+1}$ as the calibration set instead of $\{S_i\}_{i=1}^{n}$---that is, it includes the test point in the annotation process, preserving the exchangeability with the calibration inliers.
\end{itemize}
Let $\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}\cup \{ n+1\}$ denote the indices of the trimmed augmented calibration set produced by the mirror procedure, let $\tilde{\D}_{\mathrm{labeled}}$ denote the indices of the corresponding labeled data points, and define $\tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{outlier}}$ as the corresponding subsets of inliers and outliers, respectively.
Under this mirror procedure, the empirical quantile $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ corresponds to
\begin{align*}
\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} &:= \tilde{i}_{\mathrm{LT}} \text{-th smallest element in } \{S_i\}_{i \in \tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}} \cup \{S_{n+1}\},
\end{align*}
where $\tilde{i}_{\mathrm{LT}} := \lceil (1- \alpha ) (\tilde{n}^{\mathrm{LT}}+1) \rceil$ and $\tilde{n}_{\mathrm{LT}} := |\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}|$.

By construction of this mirror procedure, $\tilde{i}_{\mathrm{LT}} \leq \hat{i}_{\mathrm{LT}}$ almost surely, because $\tilde{n}^{\mathrm{LT}} \leq n^{\mathrm{LT}}$ almost surely and thus
\begin{align*}
    \tilde{i}_{\mathrm{LT}} 
    & = \left \lceil ( 1-\alpha ) (\tilde{n}^{\mathrm{LT}}+1) \right \rceil
     \leq \lceil (1- \alpha )  (n^{\mathrm{LT}}+1) \rceil
     = \hat{i}_{\mathrm{LT}}.
\end{align*}

Using the fact that $\tilde{i}_{\mathrm{LT}}  \leq \hat{i}_{\mathrm{LT}}$ almost surely, we prove in Appendix~\ref{app:proof-label-trim-a1} that, almost surely,
\begin{align}\label{app-eq:tilde-hat-relation}
\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \leq \hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}.
\end{align}
Since we already knew that $\hat{Q}_{1-\alpha}^{\mathrm{LT}} \geq \hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$, this implies:
\begin{align} \label{app-eq:tilde-hat-relation-b}
    \hat{Q}_{1-\alpha}^{\mathrm{LT}} \geq \hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \geq \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}.
\end{align}
Therefore, the type-I error rate of the Label-Trim approach can be bounded from above by the type-I error rate of the mirror procedure, which can be studied with an approach similar to that of the proof of~\Cref{lem:conservativeness}.


Let $\tilde{\D}_{\mathrm{outlier}}^{\mathrm{LT}}$ denote the outlier indices remaining in $\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}$, with $\tilde{n}_{1}^{\mathrm{LT}}=|\tilde{\D}_{\mathrm{outlier}}^{\mathrm{LT}}|$.
 As in the proof of~\Cref{lem:conservativeness}, define $E_{\mathrm{in}}$ and $E_{\mathrm{out}}$ as two unordered realizations of the inlier and outlier scores in $\{S_i\}_{i=1}^{n+1}$, respectively. Appendix~\ref{app:proof-label-trim-a0} proves that
    \begin{align} \label{app-eq:type-I-error-cond}
    \p \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{LT}} \mid E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right)
        \leq \alpha + \frac{1}{n_0+1} - \frac{\hat{n}_1^{\mathrm{LT}}}{n_0+1} \left( (1-\alpha) - \hat{F}_{1}^{\mathrm{LT}}(\hat{Q}_{1-\alpha}^{\mathrm{LT}}) \right),
    \end{align}
from which it follows immediately, by marginalizing over $E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}},$ and $\tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}$, that
 \begin{align*}
    \p \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{LT}} \right)
   & \leq \E\left[ \alpha + \frac{1}{n_0+1} - \frac{\hat{n}^{\mathrm{LT}}_{1}}{n_0+1} \left( (1-\alpha) - \hat{F}_{1}^{\mathrm{LT}} \left( \hat{Q}_{1-\alpha}^{\mathrm{LT}} \right) \right)\right].
    \end{align*}
\end{proof}


\subsubsection{Proof of~\Cref{thm:labeled-trim} | Proof of Equation~\eqref{app-eq:tilde-hat-relation}} \label{app:proof-label-trim-a1}

\begin{proof}
We prove~\eqref{app-eq:tilde-hat-relation} by analyzing two distinct cases, depending on whether $S_{n+1}$ is among the $m + 1$ largest scores or not.

Recall that $m \leq \alpha (n+1)$, by assumption, and $n^{\mathrm{LT}} \leq n$.
It is easy to see that this implies that, almost surely,
    \begin{align} \label{eq:app-C1}
        \hat{i}_{\mathrm{LT}} = \lceil (1-\alpha)(n^{\mathrm{LT}}+1)\rceil \leq n+1 - (m+1).
    \end{align}

The mirror procedure labels and potentially removes the largest $m+1$ scores out of the $n+1$ total scores. Thus, $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ is smaller than all the outliers removed during trimming, i.e., $\hat{Q}^{\mathrm{LT,n+1}}_{1-\alpha} < S_j$ for all $S_j \in \tilde{\D}_{\mathrm{labeled}}^{\mathrm{outlier}}$.


\begin{itemize}
\item Suppose $S_{n+1}$ is among the $m + 1$ largest scores in $\{S_i\}_{i=1}^{n+1}$. This case is illustrated below:\\
\begin{minipage}{.4\textwidth}
In this scenario, the mirror trimming approach additionally labels the test score $S_{n+1}$, which, under the null hypothesis, is an inlier and is thus not removed. As a result, both trimming procedures yield the same set; i.e.,
$\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}} = \D_{\mathrm{cal}}^{\mathrm{LT}}$. Consequently, $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}} = \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$.
\end{minipage}
\begin{minipage}{.05\textwidth}
\hspace{1pt}
\end{minipage}
\begin{minipage}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/theorem3.1_proof/idx-thm3.1-case-1.pdf}
\end{minipage}

\item Suppose $S_{n+1}$ is not among the $m + 1$ largest scores in $\{S_i\}_{i=1}^{n+1}$. Within this case, there are two sub-cases to consider.
\begin{itemize}
    \item  The $(m+1)$-th largest score in an inlier:\\
    \begin{minipage}[t]{.4\textwidth}
In this case, the mirror trimming approach additionally labels an inlier, which is not removed. As a result, both trimming procedures yield the same set; i.e.,
$\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}} = \D_{\mathrm{cal}}^{\mathrm{LT}}$. Consequently, $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}} = \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$.
\end{minipage}
\begin{minipage}[t]{0.05\textwidth}
    \hspace{1pt}
\end{minipage}
\begin{minipage}[t]{.5\textwidth}
\vspace{-40pt}
\centering
\includegraphics[width=\linewidth, valign=t]{figures/theorem3.1_proof/idx-thm3.1-case-2-1.pdf}
\end{minipage}
    \item The $(m+1)$-th largest score in an outlier:\\
    \begin{minipage}[t]{.4\textwidth}
This is the interesting case where $\tilde{n}^{\mathrm{LT}} = n^{\mathrm{LT}}-1$ and the set $\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}$ contains one fewer outlier score than $\D_{\mathrm{cal}}^{\mathrm{LT}}$. 
It follows from~\eqref{eq:app-C1} that the threshold $\hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ is smaller than the $m+1$ largest scores. Then, in this region,  $\D_{\mathrm{cal}}^{\mathrm{LT}}$ and $\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}$ contain the same scores. Therefore, the $\hat{i}_{\mathrm{LT}}$-th smallest score corresponds to the same score in both $\D_{\mathrm{cal}}^{\mathrm{LT}}\cup\{n+1\}$ and $\tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}} \cup \{n+1\}$.
Since $\tilde{i}_{\mathrm{LT}} \leq \hat{i}_{\mathrm{LT}}$, it follows that $\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \leq \hat{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$.
\end{minipage}
\begin{minipage}[t]{0.05\textwidth}
    \hspace{1pt}
\end{minipage}
\begin{minipage}[t]{.5\textwidth}
\centering
\includegraphics[width=\linewidth, valign=t]{figures/theorem3.1_proof/idx-thm3.1-case-2-2.pdf}
\end{minipage}
\end{itemize}
\end{itemize}

\end{proof}


\subsubsection{Proof of~\Cref{thm:labeled-trim} | Proof of Equation~\eqref{app-eq:type-I-error-cond}} \label{app:proof-label-trim-a0}

\begin{proof}
    \begin{align*}
    & \p \left( S_{n+1} > \hat{Q}_{1-\alpha}^{\mathrm{LT}} \mid E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right) \\
        & \qquad \leq \p \left( S_{n+1} > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \mid E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right) \\
        & \qquad = \sum\limits_{i\in \D_{\mathrm{inlier}} \cup \{ n+1\} } \E \left[ \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \right] \cdot \mathbb{I} \left[ S_{n+1} = s_i \right]\mid E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right] \\
        & \qquad \overset{(i)}{=} \sum\limits_{i\in \D_{\mathrm{inlier}} \cup \{ n+1\} }         \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}} \right] \cdot \p \left( S_{n+1} = s_i \mid E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}}, \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right) \\
        & \qquad \overset{(ii)}{=} \sum\limits_{i\in \D_{\mathrm{inlier}} \cup \{ n+1\} }  \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}\right] \cdot \p \left( S_{n+1} = s_i \mid E_{\mathrm{in}}, E_{\mathrm{out}} \right) 
        \\
        & \qquad = \frac{1}{n_0+1}  \sum\limits_{i\in  \D_{\mathrm{inlier}} \cup \{ n+1\}} \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}\right] \\
        & \qquad \overset{(iii)}{\leq}  \frac{1}{n_0+1} \left( \alpha(\tilde{n}^{\mathrm{LT}}+1) - \sum\limits_{i\in  \tilde{\D}_{\mathrm{outlier}}^{\mathrm{LT}}} \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}\right] \right)\\
        & \qquad \overset{(iv)}{\leq}  \frac{1}{n_0+1} \left( \alpha(n^{\mathrm{LT}}+1) - \sum\limits_{i\in \D_{\mathrm{outlier}}^{\mathrm{LT}}} \mathbb{I} \left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}\right] + 1\right)\\
        & \qquad \overset{(v)}{\leq}  \frac{1}{n_0+1} \left( \alpha(n^{\mathrm{LT}}+1) - \sum\limits_{i\in \D_{\mathrm{outlier}}^{\mathrm{LT}}} \mathbb{I} \left[ s_i > \hat{Q}_{1-\alpha}^{\mathrm{LT}}\right] + 1\right)\\
        & \qquad = \frac{1}{n_0+1} \left( \alpha(n^{\mathrm{LT}}+1) - \hat{n}_1^{\mathrm{LT}} + \sum\limits_{i\in \D_{\mathrm{outlier}}^{\mathrm{LT}}} \mathbb{I} \left[ s_i \leq \hat{Q}_{1-\alpha}^{\mathrm{LT}}\right] + 1\right)\\
        & \qquad = \alpha + \frac{1}{n_0+1} - \frac{\hat{n}_1^{\mathrm{LT}}}{n_0+1} \left( (1-\alpha) - \hat{F}_{1}^{\mathrm{LT}}(\hat{Q}_{1-\alpha}^{\mathrm{LT}}) \right).
    \end{align*}


The labeled steps above can be explained as follows.
\begin{itemize}
\item     (i) $\mathbb{I}\left[ s_i > \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}\right]$ is measurable with respect to the $\sigma$-algebra generated by $E_{\mathrm{in}}, E_{\mathrm{out}}, \tilde{\D}_{\mathrm{labeled}},$ and $\tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}$ since $\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ is the $\tilde{i}_{\mathrm{LT}}$-th smallest element of $\{s_1,\dots,s_{n+1}\} \setminus \left(\tilde{\D}_{\mathrm{labeled}} \setminus \tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}\right)$.
\item    (ii) The mirror procedure is applied on $\{S_i\}_{i=1}^{n+1}$, preserving the exchangeability of the test score $S_{n+1}$ with the calibration inliers. Hence, the resulting labeled sets $\tilde{\D}_{\mathrm{labeled}}$ and $\tilde{\D}_{\mathrm{labeled}}^{\mathrm{inlier}}$ contain no additional information about $S_{n+1}$ beyond $E_{\mathrm{in}}, E_{\mathrm{out}}$.
\item    (iii) By definition, $\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ is the $\tilde{i}_{\mathrm{LT}}$-th smallest element of $\{S_i\}_{i\in \tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}} \cup \{S_{n+1}\}$, where $\tilde{i}_{\mathrm{LT}}=\lceil(1-\alpha)(\tilde{n}^{\mathrm{LT}}+1)\rceil$. Consequently, $\lfloor\alpha(\tilde{n}^{\mathrm{LT}}+1)\rfloor$ scores in $\{S_i\}_{i\in \tilde{\D}_{\mathrm{cal}}^{\mathrm{LT}}} \cup \{S_{n+1}\}$ are larger than $\tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$.
\item (iv) The set $\tilde{\D}_{\mathrm{outlier}}^{\mathrm{LT}}$ is either equal to $\D_{\mathrm{outlier}}^{\mathrm{LT}}$ or contains one fewer outlier, and $\tilde{n}^{\mathrm{LT}} \in \{n^{\mathrm{LT}}, n^{\mathrm{LT}}-1\}$ almost surely.
\item  (v) Recall from~\eqref{app-eq:tilde-hat-relation-b} that $\hat{Q}_{1-\alpha}^{\mathrm{LT}} \geq \tilde{Q}_{1-\alpha}^{\mathrm{LT,n+1}}$ almost surely.
\end{itemize}

\end{proof}



\section{Supplementary Experiments and Implementation Details}
\subsection{Datasets}\label{app-sec:data}
\Cref{app-tab:tabular-info} summarizes details of the three tabular benchmark datasets. For all tabular datasets, we perform random subsampling to construct contaminated train, calibration, and test sets. Specifically, for the shuttle and KDDCup99 datasets, the train set contains 5,000 samples, and the calibration set contains 2,500 samples, both with a contamination rate of $r=3\%$, unless stated otherwise. The inlier and outlier test sets consist of 950 and 50 samples, respectively. For the credit-card dataset, the train set contains 2,000 samples, while the calibration and test sets follow the same setup as the Shuttle and KDDCup99 datasets.
\begin{table}[ht]
\centering
\caption{Summary of tabular datasets}
\label{app-tab:tabular-info}
\begin{tabular}{lccc}
\toprule
Dataset              & Shuttle \citep{shuttle} & Credit-card \citep{creditcard} & KDDCup99 \citep{KDDCup99} \\ 
\midrule
Total Samples        & 58,000           & 284,807              & 494,020           \\ 
Number of Outliers   & 12,414           & 492                  & 396,743           \\ 
Number of Features   & 9                & 29                   & 41                \\ 
\bottomrule
\end{tabular}
\end{table}

For visual datasets, we use the OpenOOD benchmark \citep{zhang2023openood, yang2022openood}. Specifically, for each dataset and contamination rate, we perform a one-time training of the ReAct outlier detection model~\citep{react}, which operates on feature representations extracted from a pre-trained ResNet-18~\citep{zhang2023openood,he2016deep}. The model applies a percentile-based threshold (set to 90\%) to truncate activations, where the threshold is computed on the contaminated train set. These truncated activations then pass through the fully connected layer of the pre-trained ResNet-18.
The outlier score is computed using an energy-based log-sum-exp function applied to these truncated activations. 
After training, we save the outlier scores for the remaining outlier samples and the CIFAR-10 test set. We randomly subsample this pool of scores to construct the calibration and test sets, ensuring that all sets are disjoint.

The sizes of the train and calibration sets are 2,000 and 3,000, respectively, with the same contamination rate.
The inlier and outlier test sets consist of 950 and 50 samples, respectively.
\FloatBarrier

\subsection{Tabular Datasets}
\label{app-sec:real-data-exp}
In this section, we provide additional real-data experiments conducted on the {\em credit card} \citep{creditcard} and {\em KDDCup99} datasets,
complementing the analysis provided for the shuttle dataset in the main manuscript. Each figure in this section corresponds to and extends the figures presented in the main text.

\paragraph{Results as a function of the contamination rate} In~\Cref{fig:shuttle-outlier-prop} of the main manuscript, we analyze the performance of conformal outlier methods as a function of the contamination rate $r$. Here, we repeat the same experiment on the credit-card and KDDCup99 datasets (Figures \ref{app-fig:creditcard-outlier-prop} and \ref{app-fig:KDDCup99-outlier-prop}). The performance trends are similar to the one presented in the main manuscript: both the \texttt{Standard} and \texttt{Small-Clean} methods achieve valid type-I error rate but exhibit conservative behavior. In contrast, the \texttt{Naive-Trim} method fails to control the type-I error rate. The \texttt{Label-Trim} method attains improved power while practically controlling the type-I error at level $\alpha$.

\begin{figure*}[htb]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/outlier_prop/IF_e_100_s_auto_train_2000_exp_outliers_calib_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/outlier_prop/IF_e_100_s_auto_train_2000_exp_outliers_calib_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/outlier_prop/IF_e_100_s_auto_train_2000_exp_outliers_calib_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend.pdf}
    \caption{Comparison of conformal outlier detection methods on real dataset ``credit-card'' as a function of the contamination rate  $r$. Other details are as in \Cref{fig:shuttle-outlier-prop}. 
}
    \label{app-fig:creditcard-outlier-prop}
\end{figure*}

\begin{figure*}[htb]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/outlier_prop/IF_e_100_s_auto_train_5000_exp_outliers_calib_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.05_test_1000_p_0.05_q_0.02/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend.pdf}
    \caption{Comparison of conformal outlier detection methods on real dataset ``KDDCup99'' as a function of the contamination rate  $r$. Other details are as in \Cref{fig:shuttle-outlier-prop}. 
}
    \label{app-fig:KDDCup99-outlier-prop}
\end{figure*}

\paragraph{Results as a function of the labeling budget} In~\Cref{fig:shuttle-labeled-exp} of the main manuscript, we evaluate the performance of the \texttt{Label-Trim}, \texttt{Small-Clean} and, \texttt{Oracle} methods as a function of the labeling budget $m$. \Cref{app-fig:creditcard-labeled-exp,app-fig:KDDCup99-labeled-exp} extend this analysis to the credit-card and KDDCup99 datasets, respectively. Consistent with the trends observed in~\Cref{fig:shuttle-labeled-exp}, increasing the labeling budget improves the performance of the \texttt{Label-Trim} method in terms of both type-I error and power. Notably, although the condition in \Cref{thm:labeled-trim} is no longer satisfied for $m > 50$, the \texttt{Label-Trim} method still maintains valid type-I error control at the desired level $\alpha$. 

For the KDDCup99 dataset, however, we observe that the \texttt{Small-Clean} method shows higher power than the \texttt{Oracle} across several labeling budgets ($m \geq 55$). This is due to the high variability in the dataset and the small sample size used by this method, resulting in significant variance in type-I error. To illustrate this, \Cref{app-fig:KDDCup99-labeled-exp-55-105} presents a box plot showing the variability and instability of the \texttt{Small-Clean} method in this regime. While this leads to a higher average power, this variability is undesirable in practice.

\begin{figure*}[!htb]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/labeled_size/IF_e_100_s_auto_train_2000_exp_clean_calib_size_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/labeled_size/IF_e_100_s_auto_train_2000_exp_clean_calib_size_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/labeled_size/IF_e_100_s_auto_train_2000_exp_clean_calib_size_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_n.pdf}
    \caption{Performance on real dataset ``credit-card'' as a function of the labeling budget $m$. The contamination rate is $r=0.03$. Other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{app-fig:creditcard-labeled-exp}
\end{figure*}

\begin{figure*}[!htb]
    % \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02_n_seeds_400/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02_n_seeds_400/Power_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02_n_seeds_400/Trimmed_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_n.pdf}
    \caption{Performance on real dataset ``KDDCup99'' as a function of the labeling budget $m$. The contamination rate is $r=0.03$. Results are averages across 400 random splits of the data. Other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{app-fig:KDDCup99-labeled-exp}
\end{figure*}

\begin{figure*}[!htb]
    \centering 
    \includegraphics[width=0.33\textwidth, valign=t]{figures/exp/real_data/KDDCup99/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02_n_seeds_400/55-105/Type-1-Error_no_legend.pdf}
    \includegraphics[width=0.33\textwidth, valign=t]{figures/exp/real_data/KDDCup99/labeled_size/IF_e_100_s_auto_train_5000_exp_clean_calib_size_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02_n_seeds_400/55-105/Power_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_n_box.pdf}
    \caption{Performance on real dataset ``KDDCup99'' as a function of the labeling budget $m$. The contamination rate is $r=0.03$. Results are averages across 400 random splits of the data. Other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{app-fig:KDDCup99-labeled-exp-55-105}
\end{figure*}

\paragraph{Results as a function of the target type-I error level} In~\Cref{fig:shuttle-levels} we examine the performance of conformal outlier detection methods as a function of the target type-I error level $\alpha$. Here, we replicate the experiments on the credit-card and KDDCup99 datasets (\Cref{app-fig:creditcard-levels,app-fig:KDDCup99-levels}). In line with the trends observed in~\Cref{fig:shuttle-levels}, the \texttt{Label-Trim} method performs well at low type-I error rates. Notably, for $\alpha=0.01$, the \texttt{Label-Trim} method outperforms the baselines, while practically controlling the type-I error at level $\alpha$, even though the condition of~\Cref{thm:labeled-trim} is not satisfied in this case. This highlights the robustness of our approach.

\begin{figure*}[!htb]
    \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/levels/IF_e_100_s_auto_train_2000_exp_levels_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/creditcard/levels/IF_e_100_s_auto_train_2000_exp_levels_creditcard_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_trm.pdf}
    \caption{Comparison of conformal outlier detection methods on real dataset ``credit-card'' as a function of the target type-I error rate $\alpha$. The contamination rate $r$ is fixed to 3\%; other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{app-fig:creditcard-levels}
\end{figure*}

\begin{figure*}[!htb]
    \centering 
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/levels/IF_e_100_s_auto_train_5000_exp_levels_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Type-1-Error_point_no_legend.pdf}
    \includegraphics[height=3.3cm, valign=t]{figures/exp/real_data/KDDCup99/levels/IF_e_100_s_auto_train_5000_exp_levels_KDDCup99_1_fdr_model_0.5_initial_50_cal_2500_p_0.03_test_1000_p_0.05_q_0.02/Power_point_no_legend.pdf}
    \includegraphics[width=3.3cm, valign=t]{figures/exp/legend_wo_trm.pdf}
    \caption{Comparison of conformal outlier detection methods on real dataset ``KDDCup99'' as a function of the target type-I error rate $\alpha$. The contamination rate $r$ is fixed to 3\%; other details are as in \Cref{fig:shuttle-outlier-prop}.
}
    \label{app-fig:KDDCup99-levels}
\end{figure*}

% \clearpage
\FloatBarrier

\subsection{Visual Datasets}\label{app-sec:images-data-exp}
This section provides detailed results for each dataset, covering various contamination rates and target type-I error levels.

\Cref{app-tab:all} summarizes the results across all six datasets for a target type-I error rate of $\alpha = 0.02$, showing trends consistent with those observed in \Cref{tab:avg-images} of the main manuscript.

\Cref{app-tab:texture,app-tab:svhn,app-tab:places365,app-tab:mnist,app-tab:cifar100,app-tab:tin} present the type-I error rates and the power of each method, with the power reported relative to the \texttt{Standard} method (normalized to 1). The actual power of the \texttt{Standard} method is included in the last row of each table.

\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on six visual datasets for varying contamination rate $r$ and target type-I error level $\alpha = 0.02$. The empirical type-I error values are averaged across all datasets. The empirical power is presented relative to the \texttt{Standard} method (higher is better), and averaged across all datasets. Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\centering
\label{app-tab:all}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0204) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0238) & \cellcolor{white} 0.012 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0272) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.096 ($\pm$ 0.0214) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.33 ($\pm$ 0.025) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.674 ($\pm$ 0.0311) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.249 ($\pm$ 0.0219) & \cellcolor{red!20} 0.026 ($\pm$ 0.0005)  & \cellcolor{red!20} 1.777 ($\pm$ 0.0254) & \cellcolor{red!20} 0.035 ($\pm$ 0.0006)  & \cellcolor{red!20} 2.452 ($\pm$ 0.0324) & \cellcolor{red!20} 0.044 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.819 ($\pm$ 0.0592) & \cellcolor{white} 0.018 ($\pm$ 0.002)  & \cellcolor{white} 0.613 ($\pm$ 0.0753) & \cellcolor{white} 0.011 ($\pm$ 0.0018)  & \cellcolor{white} 0.406 ($\pm$ 0.0769) & \cellcolor{white} 0.006 ($\pm$ 0.0013) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.079 ($\pm$ 0.0212) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.23 ($\pm$ 0.0247) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.381 ($\pm$ 0.0298) & \cellcolor{white} 0.014 ($\pm$ 0.0005) \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}
\end{table}


\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on Texture dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:texture}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0295) & \cellcolor{white} 0.008 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0369) & \cellcolor{white} 0.006 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0405) & \cellcolor{white} 0.004 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.173 ($\pm$ 0.0295) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.455 ($\pm$ 0.0432) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.824 ($\pm$ 0.051) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.722 ($\pm$ 0.035) & \cellcolor{red!20} 0.017 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.668 ($\pm$ 0.0428) & \cellcolor{red!20} 0.028 ($\pm$ 0.0006)  & \cellcolor{red!20} 4.008 ($\pm$ 0.0599) & \cellcolor{red!20} 0.037 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.173 ($\pm$ 0.0295) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.448 ($\pm$ 0.0428) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.73 ($\pm$ 0.0477) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\
\hline
Standard Power & \cellcolor{white} 0.194 ($\pm$ 0.0057) &  & \cellcolor{white} 0.159 ($\pm$ 0.0059) &  & \cellcolor{white} 0.123 ($\pm$ 0.005) & \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0202) & \cellcolor{white} 0.017 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0239) & \cellcolor{white} 0.012 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0264) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.109 ($\pm$ 0.0199) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.315 ($\pm$ 0.0259) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.623 ($\pm$ 0.0346) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.24 ($\pm$ 0.0193) & \cellcolor{red!20} 0.026 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.781 ($\pm$ 0.026) & \cellcolor{red!20} 0.035 ($\pm$ 0.0007)  & \cellcolor{red!20} 2.431 ($\pm$ 0.0334) & \cellcolor{red!20} 0.045 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.819 ($\pm$ 0.0599) & \cellcolor{white} 0.018 ($\pm$ 0.002)  & \cellcolor{white} 0.702 ($\pm$ 0.0824) & \cellcolor{white} 0.014 ($\pm$ 0.0025)  & \cellcolor{white} 0.478 ($\pm$ 0.0842) & \cellcolor{white} 0.007 ($\pm$ 0.0016) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.089 ($\pm$ 0.0199) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.219 ($\pm$ 0.0253) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.353 ($\pm$ 0.0303) & \cellcolor{white} 0.014 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.337 ($\pm$ 0.0068) &  & \cellcolor{white} 0.272 ($\pm$ 0.0065) &   & \cellcolor{white} 0.224 ($\pm$ 0.0059) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0155) & \cellcolor{white} 0.027 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0197) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \cellcolor{white} 1.0 ($\pm$ 0.0215) & \cellcolor{white} 0.015 ($\pm$ 0.0005) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.068 ($\pm$ 0.0152) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.224 ($\pm$ 0.0191) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.427 ($\pm$ 0.0249) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.15 ($\pm$ 0.0152) & \cellcolor{red!20} 0.036 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.514 ($\pm$ 0.0178) & \cellcolor{red!20} 0.043 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.919 ($\pm$ 0.0229) & \cellcolor{red!20} 0.052 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.743 ($\pm$ 0.0441) & \cellcolor{white} 0.02 ($\pm$ 0.002)  & \cellcolor{white} 0.883 ($\pm$ 0.0549) & \cellcolor{white} 0.021 ($\pm$ 0.0025)  & \bfseries \cellcolor{Green!30} 1.062 ($\pm$ 0.0665) & \cellcolor{white} 0.024 ($\pm$ 0.0027) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.046 ($\pm$ 0.0153) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.13 ($\pm$ 0.019) & \cellcolor{white} 0.025 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.215 ($\pm$ 0.0245) & \cellcolor{white} 0.021 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.421 ($\pm$ 0.0065) &   & \cellcolor{white} 0.357 ($\pm$ 0.007) &   & \cellcolor{white} 0.309 ($\pm$ 0.0067) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}


\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on SVHN dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:svhn}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0255) & \cellcolor{white} 0.008 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0316) & \cellcolor{white} 0.004 ($\pm$ 0.0002)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0388) & \cellcolor{white} 0.002 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.192 ($\pm$ 0.0262) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.654 ($\pm$ 0.0368) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 2.208 ($\pm$ 0.052) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.573 ($\pm$ 0.0271) & \cellcolor{red!20} 0.016 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.689 ($\pm$ 0.0353) & \cellcolor{red!20} 0.025 ($\pm$ 0.0006)  & \cellcolor{red!20} 4.073 ($\pm$ 0.0533) & \cellcolor{red!20} 0.033 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.192 ($\pm$ 0.0262) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.601 ($\pm$ 0.0367) & \cellcolor{white} 0.009 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.895 ($\pm$ 0.0448) & \cellcolor{white} 0.007 ($\pm$ 0.0003) \\
\hline
Standard Power & \cellcolor{white} 0.271 ($\pm$ 0.0069) &  & \cellcolor{white} 0.191 ($\pm$ 0.006) &  & \cellcolor{white} 0.137 ($\pm$ 0.0053) & \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0168) & \cellcolor{white} 0.016 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0218) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0245) & \cellcolor{white} 0.007 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.096 ($\pm$ 0.0173) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.404 ($\pm$ 0.0211) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.828 ($\pm$ 0.0304) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.2 ($\pm$ 0.0175) & \cellcolor{red!20} 0.026 ($\pm$ 0.0005)  & \cellcolor{red!20} 1.721 ($\pm$ 0.0216) & \cellcolor{red!20} 0.032 ($\pm$ 0.0006)  & \cellcolor{red!20} 2.42 ($\pm$ 0.0281) & \cellcolor{red!20} 0.041 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.833 ($\pm$ 0.053) & \cellcolor{white} 0.02 ($\pm$ 0.0026)  & \cellcolor{white} 0.598 ($\pm$ 0.0754) & \cellcolor{white} 0.01 ($\pm$ 0.0016)  & \cellcolor{white} 0.509 ($\pm$ 0.0929) & \cellcolor{white} 0.008 ($\pm$ 0.0019) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.08 ($\pm$ 0.0176) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.277 ($\pm$ 0.0228) & \cellcolor{white} 0.016 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.469 ($\pm$ 0.0288) & \cellcolor{white} 0.013 ($\pm$ 0.0004) \\
\hline
Standard Power & \cellcolor{white} 0.429 ($\pm$ 0.0072) &  & \cellcolor{white} 0.332 ($\pm$ 0.0072) &  & \cellcolor{white} 0.25 ($\pm$ 0.0061) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0145) & \cellcolor{white} 0.026 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0169) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \cellcolor{white} 1.0 ($\pm$ 0.0204) & \cellcolor{white} 0.012 ($\pm$ 0.0004) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.063 ($\pm$ 0.014) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.246 ($\pm$ 0.0158) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.513 ($\pm$ 0.0212) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.115 ($\pm$ 0.0137) & \cellcolor{red!20} 0.035 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.395 ($\pm$ 0.0148) & \cellcolor{red!20} 0.041 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.825 ($\pm$ 0.0199) & \cellcolor{red!20} 0.049 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.73 ($\pm$ 0.0419) & \cellcolor{white} 0.021 ($\pm$ 0.0027)  & \cellcolor{white} 0.948 ($\pm$ 0.0447) & \cellcolor{white} 0.022 ($\pm$ 0.0019)  & \bfseries \cellcolor{Green!30} 1.092 ($\pm$ 0.0625) & \cellcolor{white} 0.022 ($\pm$ 0.0023) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.042 ($\pm$ 0.0143) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.144 ($\pm$ 0.0151) & \cellcolor{white} 0.024 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.266 ($\pm$ 0.0209) & \cellcolor{white} 0.019 ($\pm$ 0.0006) \\
\hline
Standard Power & \cellcolor{white} 0.517 ($\pm$ 0.0075) &  & \cellcolor{white} 0.44 ($\pm$ 0.0074) &  & \cellcolor{white} 0.355 ($\pm$ 0.0072) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}

\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on Places365 dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:places365}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.033) & \cellcolor{white} 0.009 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0364) & \cellcolor{white} 0.006 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0431) & \cellcolor{white} 0.004 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.139 ($\pm$ 0.0339) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.47 ($\pm$ 0.0423) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.839 ($\pm$ 0.0558) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.624 ($\pm$ 0.0339) & \cellcolor{red!20} 0.017 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.73 ($\pm$ 0.0487) & \cellcolor{red!20} 0.028 ($\pm$ 0.0006)  & \cellcolor{red!20} 4.12 ($\pm$ 0.069) & \cellcolor{red!20} 0.039 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.139 ($\pm$ 0.0339) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.466 ($\pm$ 0.0421) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.707 ($\pm$ 0.0531) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\
\hline
Standard Power & \cellcolor{white} 0.193 ($\pm$ 0.0064) &  & \cellcolor{white} 0.148 ($\pm$ 0.0054) &  & \cellcolor{white} 0.115 ($\pm$ 0.005) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0209) & \cellcolor{white} 0.017 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0241) & \cellcolor{white} 0.013 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0298) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.091 ($\pm$ 0.0224) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.271 ($\pm$ 0.0258) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.599 ($\pm$ 0.0308) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.246 ($\pm$ 0.0225) & \cellcolor{red!20} 0.027 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.753 ($\pm$ 0.0277) & \cellcolor{red!20} 0.036 ($\pm$ 0.0007)  & \cellcolor{red!20} 2.462 ($\pm$ 0.0376) & \cellcolor{red!20} 0.046 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.799 ($\pm$ 0.0632) & \cellcolor{white} 0.018 ($\pm$ 0.0021)  & \cellcolor{white} 0.549 ($\pm$ 0.0716) & \cellcolor{white} 0.01 ($\pm$ 0.0016)  & \cellcolor{white} 0.255 ($\pm$ 0.0595) & \cellcolor{white} 0.003 ($\pm$ 0.0009) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.071 ($\pm$ 0.0219) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.187 ($\pm$ 0.0247) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.361 ($\pm$ 0.0306) & \cellcolor{white} 0.015 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.316 ($\pm$ 0.0066) &  & \cellcolor{white} 0.261 ($\pm$ 0.0063) &  & \cellcolor{white} 0.213 ($\pm$ 0.0064) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.018) & \cellcolor{white} 0.027 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0204) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \cellcolor{white} 1.0 ($\pm$ 0.021) & \cellcolor{white} 0.015 ($\pm$ 0.0005) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.055 ($\pm$ 0.0189) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.231 ($\pm$ 0.0213) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.403 ($\pm$ 0.025) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.145 ($\pm$ 0.0199) & \cellcolor{red!20} 0.036 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.496 ($\pm$ 0.0217) & \cellcolor{red!20} 0.044 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.883 ($\pm$ 0.0261) & \cellcolor{red!20} 0.054 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.746 ($\pm$ 0.0469) & \cellcolor{white} 0.021 ($\pm$ 0.002)  & \cellcolor{white} 0.833 ($\pm$ 0.0504) & \cellcolor{white} 0.018 ($\pm$ 0.0018)  & \bfseries \cellcolor{Green!30} 1.029 ($\pm$ 0.0593) & \cellcolor{white} 0.022 ($\pm$ 0.0022) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.038 ($\pm$ 0.0188) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.149 ($\pm$ 0.0211) & \cellcolor{white} 0.025 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.186 ($\pm$ 0.0228) & \cellcolor{white} 0.022 ($\pm$ 0.0006) \\
\hline
Standard Power & \cellcolor{white} 0.395 ($\pm$ 0.0071) &  & \cellcolor{white} 0.335 ($\pm$ 0.0068) &  & \cellcolor{white} 0.302 ($\pm$ 0.0063) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}
\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on MNIST dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:mnist}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0248) & \cellcolor{white} 0.008 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0283) & \cellcolor{white} 0.004 ($\pm$ 0.0002)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0363) & \cellcolor{white} 0.003 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.241 ($\pm$ 0.0291) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.758 ($\pm$ 0.0352) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 2.554 ($\pm$ 0.0601) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.629 ($\pm$ 0.0292) & \cellcolor{red!20} 0.016 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.722 ($\pm$ 0.0363) & \cellcolor{red!20} 0.023 ($\pm$ 0.0005)  & \cellcolor{red!20} 4.718 ($\pm$ 0.0552) & \cellcolor{red!20} 0.03 ($\pm$ 0.0006) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.241 ($\pm$ 0.0291) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.636 ($\pm$ 0.0331) & \cellcolor{white} 0.009 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 2.113 ($\pm$ 0.0555) & \cellcolor{white} 0.007 ($\pm$ 0.0003) \\
\hline
Standard Power & \cellcolor{white} 0.282 ($\pm$ 0.007) &  & \cellcolor{white} 0.208 ($\pm$ 0.0059) &  & \cellcolor{white} 0.131 ($\pm$ 0.0048) & \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0173) & \cellcolor{white} 0.016 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0205) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.027) & \cellcolor{white} 0.007 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.117 ($\pm$ 0.0179) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.452 ($\pm$ 0.0209) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.971 ($\pm$ 0.0297) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.231 ($\pm$ 0.0179) & \cellcolor{red!20} 0.025 ($\pm$ 0.0005)  & \cellcolor{red!20} 1.759 ($\pm$ 0.0207) & \cellcolor{red!20} 0.031 ($\pm$ 0.0006)  & \cellcolor{red!20} 2.531 ($\pm$ 0.0273) & \cellcolor{red!20} 0.038 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.844 ($\pm$ 0.0514) & \cellcolor{white} 0.019 ($\pm$ 0.0019)  & \cellcolor{white} 0.617 ($\pm$ 0.0738) & \cellcolor{white} 0.01 ($\pm$ 0.0017)  & \cellcolor{white} 0.438 ($\pm$ 0.0817) & \cellcolor{white} 0.005 ($\pm$ 0.0011) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.096 ($\pm$ 0.0181) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.308 ($\pm$ 0.0206) & \cellcolor{white} 0.015 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.493 ($\pm$ 0.0295) & \cellcolor{white} 0.012 ($\pm$ 0.0004) \\
\hline
Standard Power & \cellcolor{white} 0.465 ($\pm$ 0.008) &  & \cellcolor{white} 0.36 ($\pm$ 0.0074) & & \cellcolor{white} 0.264 ($\pm$ 0.0071) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0147) & \cellcolor{white} 0.025 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0151) & \cellcolor{white} 0.016 ($\pm$ 0.0004)  & \cellcolor{white} 1.0 ($\pm$ 0.0201) & \cellcolor{white} 0.011 ($\pm$ 0.0004) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.072 ($\pm$ 0.0149) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.295 ($\pm$ 0.0155) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.617 ($\pm$ 0.0186) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.116 ($\pm$ 0.0144) & \cellcolor{red!20} 0.034 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.424 ($\pm$ 0.0142) & \cellcolor{red!20} 0.039 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.851 ($\pm$ 0.0178) & \cellcolor{red!20} 0.046 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.699 ($\pm$ 0.0394) & \cellcolor{white} 0.019 ($\pm$ 0.0019)  & \cellcolor{white} 0.884 ($\pm$ 0.0464) & \cellcolor{white} 0.019 ($\pm$ 0.0018)  & \bfseries \cellcolor{Green!30} 1.164 ($\pm$ 0.0576) & \cellcolor{white} 0.02 ($\pm$ 0.002) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.049 ($\pm$ 0.0148) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.158 ($\pm$ 0.016) & \cellcolor{white} 0.023 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.273 ($\pm$ 0.0218) & \cellcolor{white} 0.017 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.576 ($\pm$ 0.0085) &  & \cellcolor{white} 0.483 ($\pm$ 0.0073) &  & \cellcolor{white} 0.382 ($\pm$ 0.0077) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}

\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on CIFAR-100 dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:cifar100}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0393) & \cellcolor{white} 0.009 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0391) & \cellcolor{white} 0.007 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.042) & \cellcolor{white} 0.005 ($\pm$ 0.0003) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.116 ($\pm$ 0.0417) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.463 ($\pm$ 0.05) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.588 ($\pm$ 0.0477) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.733 ($\pm$ 0.0397) & \cellcolor{red!20} 0.018 ($\pm$ 0.0005)  & \cellcolor{red!20} 3.06 ($\pm$ 0.0585) & \cellcolor{red!20} 0.03 ($\pm$ 0.0006)  & \cellcolor{red!20} 4.054 ($\pm$ 0.0631) & \cellcolor{red!20} 0.041 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.116 ($\pm$ 0.0417) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.463 ($\pm$ 0.05) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.587 ($\pm$ 0.0468) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\
\hline
Standard Power & \cellcolor{white} 0.145 ($\pm$ 0.0057) &  & \cellcolor{white} 0.118 ($\pm$ 0.0046) &  & \cellcolor{white} 0.104 ($\pm$ 0.0044) & \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.023) & \cellcolor{white} 0.018 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0265) & \cellcolor{white} 0.014 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0264) & \cellcolor{white} 0.011 ($\pm$ 0.0004) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.082 ($\pm$ 0.0256) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.265 ($\pm$ 0.0299) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.476 ($\pm$ 0.0317) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.294 ($\pm$ 0.0266) & \cellcolor{red!20} 0.027 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.831 ($\pm$ 0.0288) & \cellcolor{red!20} 0.038 ($\pm$ 0.0006)  & \cellcolor{red!20} 2.464 ($\pm$ 0.0372) & \cellcolor{red!20} 0.049 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.844 ($\pm$ 0.0709) & \cellcolor{white} 0.019 ($\pm$ 0.0022)  & \cellcolor{white} 0.686 ($\pm$ 0.0797) & \cellcolor{white} 0.012 ($\pm$ 0.0017)  & \cellcolor{white} 0.392 ($\pm$ 0.078) & \cellcolor{white} 0.006 ($\pm$ 0.0014) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.064 ($\pm$ 0.0247) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.193 ($\pm$ 0.0293) & \cellcolor{white} 0.018 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.288 ($\pm$ 0.0296) & \cellcolor{white} 0.016 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.255 ($\pm$ 0.0059) &   & \cellcolor{white} 0.225 ($\pm$ 0.006) &  & \cellcolor{white} 0.19 ($\pm$ 0.005) & \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0204) & \cellcolor{white} 0.027 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0214) & \cellcolor{white} 0.022 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0228) & \cellcolor{white} 0.018 ($\pm$ 0.0005) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.054 ($\pm$ 0.0212) & 0.031 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.184 ($\pm$ 0.0228) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.33 ($\pm$ 0.0246) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.184 ($\pm$ 0.0213) & \cellcolor{red!20} 0.036 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.523 ($\pm$ 0.0222) & \cellcolor{red!20} 0.046 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.94 ($\pm$ 0.026) & \cellcolor{red!20} 0.056 ($\pm$ 0.0009) \\

Small-Clean & \cellcolor{white} 0.709 ($\pm$ 0.0527) & \cellcolor{white} 0.021 ($\pm$ 0.0023)  & \cellcolor{white} 0.842 ($\pm$ 0.0545) & \cellcolor{white} 0.021 ($\pm$ 0.0019)  & \cellcolor{white} 0.947 ($\pm$ 0.0628) & \cellcolor{white} 0.02 ($\pm$ 0.002) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.029 ($\pm$ 0.0212) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.114 ($\pm$ 0.0222) & \cellcolor{white} 0.026 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.16 ($\pm$ 0.0229) & \cellcolor{white} 0.023 ($\pm$ 0.0006) \\
\hline
Standard Power & \cellcolor{white} 0.332 ($\pm$ 0.0068) &  & \cellcolor{white} 0.302 ($\pm$ 0.0064) &  & \cellcolor{white} 0.263 ($\pm$ 0.006) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}


\begin{table}[!htb]
\caption{Comparison of conformal outlier detection methods on TinyImageNet dataset (outliers) and CIFAR-10 dataset (inliers) for varying contamination rate $r$ and target type-I error level $\alpha$. The empirical power is presented relative to the \texttt{Standard} method (higher is better). Results are averaged across 100 random splits of the data, with standard errors presented in parentheses.}
\label{app-tab:tin}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0381) & \cellcolor{white} 0.009 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0398) & \cellcolor{white} 0.006 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0439) & \cellcolor{white} 0.004 ($\pm$ 0.0002) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.137 ($\pm$ 0.0409) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.492 ($\pm$ 0.0474) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.754 ($\pm$ 0.0518) & \cellcolor{white} 0.009 ($\pm$ 0.0004) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.675 ($\pm$ 0.0401) & \cellcolor{red!20} 0.018 ($\pm$ 0.0004)  & \cellcolor{red!20} 2.872 ($\pm$ 0.0485) & \cellcolor{red!20} 0.028 ($\pm$ 0.0006)  & \cellcolor{red!20} 3.986 ($\pm$ 0.0568) & \cellcolor{red!20} 0.039 ($\pm$ 0.0007) \\

Small-Clean & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0)  & \cellcolor{white} 0.0 ($\pm$ 0.0) & \cellcolor{white} 0.0 ($\pm$ 0.0) \\

Label-Trim & \bfseries \cellcolor{Green!100} 1.137 ($\pm$ 0.0409) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!100} 1.488 ($\pm$ 0.0472) & \cellcolor{white} 0.01 ($\pm$ 0.0003)  & \bfseries \cellcolor{Green!60} 1.681 ($\pm$ 0.0511) & \cellcolor{white} 0.009 ($\pm$ 0.0003) \\
\hline
Standard Power & \cellcolor{white} 0.168 ($\pm$ 0.0064) &   & \cellcolor{white} 0.133 ($\pm$ 0.0053) &  & \cellcolor{white} 0.115 ($\pm$ 0.005) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.01$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0239) & \cellcolor{white} 0.018 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0262) & \cellcolor{white} 0.013 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0293) & \cellcolor{white} 0.01 ($\pm$ 0.0004) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.084 ($\pm$ 0.0253) & 0.021 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.275 ($\pm$ 0.0263) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!100} 1.544 ($\pm$ 0.0293) & \cellcolor{white} 0.02 ($\pm$ 0.0006) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.284 ($\pm$ 0.0274) & \cellcolor{red!20} 0.027 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.819 ($\pm$ 0.0277) & \cellcolor{red!20} 0.036 ($\pm$ 0.0006)  & \cellcolor{red!20} 2.406 ($\pm$ 0.0308) & \cellcolor{red!20} 0.046 ($\pm$ 0.0008) \\

Small-Clean & \cellcolor{white} 0.777 ($\pm$ 0.0566) & \cellcolor{white} 0.015 ($\pm$ 0.0015)  & \cellcolor{white} 0.525 ($\pm$ 0.0687) & \cellcolor{white} 0.009 ($\pm$ 0.0015)  & \cellcolor{white} 0.366 ($\pm$ 0.0653) & \cellcolor{white} 0.005 ($\pm$ 0.0011) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.074 ($\pm$ 0.0249) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!60} 1.193 ($\pm$ 0.0254) & \cellcolor{white} 0.017 ($\pm$ 0.0004)  & \bfseries \cellcolor{Green!60} 1.322 ($\pm$ 0.03) & \cellcolor{white} 0.015 ($\pm$ 0.0005) \\
\hline
Standard Power & \cellcolor{white} 0.285 ($\pm$ 0.0068) &  & \cellcolor{white} 0.243 ($\pm$ 0.0064) & & \cellcolor{white} 0.209 ($\pm$ 0.0061) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.02$}

\resizebox{\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline
& \multicolumn{6}{c}{Contamination rate} \\
\hline
 & \multicolumn{2}{c|}{1\%} & \multicolumn{2}{c|}{3\%} & \multicolumn{2}{c}{5\%} \\ \hline
 Method      & Power & Type-I Error & Power & Type-I Error & Power & Type-I Error \\ \hline
Standard & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0213) & \cellcolor{white} 0.027 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0202) & \cellcolor{white} 0.02 ($\pm$ 0.0005)  & \bfseries \cellcolor{Green!30} 1.0 ($\pm$ 0.0216) & \cellcolor{white} 0.016 ($\pm$ 0.0005) \\

Oracle (infeasible) & \bfseries \cellcolor{Green!100} 1.062 ($\pm$ 0.0216) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.23 ($\pm$ 0.0205) & \cellcolor{white} 0.03 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!100} 1.397 ($\pm$ 0.0236) & \cellcolor{white} 0.03 ($\pm$ 0.0007) \\

Naive-Trim (invalid) & \cellcolor{red!20} 1.168 ($\pm$ 0.0203) & \cellcolor{red!20} 0.036 ($\pm$ 0.0006)  & \cellcolor{red!20} 1.569 ($\pm$ 0.0207) & \cellcolor{red!20} 0.045 ($\pm$ 0.0007)  & \cellcolor{red!20} 1.877 ($\pm$ 0.0217) & \cellcolor{red!20} 0.054 ($\pm$ 0.0009) \\

Small-Clean & \cellcolor{white} 0.656 ($\pm$ 0.0438) & \cellcolor{white} 0.017 ($\pm$ 0.0017)  & \cellcolor{white} 0.825 ($\pm$ 0.0496) & \cellcolor{white} 0.019 ($\pm$ 0.0019)  & \cellcolor{white} 0.904 ($\pm$ 0.0589) & \cellcolor{white} 0.02 ($\pm$ 0.0024) \\

Label-Trim & \bfseries \cellcolor{Green!60} 1.04 ($\pm$ 0.0219) & \cellcolor{white} 0.029 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.139 ($\pm$ 0.0208) & \cellcolor{white} 0.026 ($\pm$ 0.0006)  & \bfseries \cellcolor{Green!60} 1.19 ($\pm$ 0.0225) & \cellcolor{white} 0.022 ($\pm$ 0.0006) \\
\hline
Standard Power & \cellcolor{white} 0.368 ($\pm$ 0.0078) &  & \cellcolor{white} 0.318 ($\pm$ 0.0064) &  & \cellcolor{white} 0.29 ($\pm$ 0.0063) &  \\
\end{tabular}
}
\subcaption{Target type-I error rate $\alpha=0.03$}

\end{table}


\FloatBarrier

\end{document}
