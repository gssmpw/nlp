\section{Related Work}
Recently, there has been growing interest in studying the statistical properties of conformal inference methods under more realistic scenarios, moving beyond the idealized assumption of perfectly clean and exchangeable observations to account for various types of {\em distribution shift} \citep{tibshirani2019conformal,einbinder2022conformal,sesia2023adaptive,barber2023conformal,gibbs2021adaptive,zaffran2022adaptive,feldman2022achieving,gibbs2024conformal,podkopaev2021distribution,si2023pac,prinsterconformal}. This paper draws inspiration from several prior works in this area.

\citet{tibshirani2019conformal} introduced a weighted conformal prediction approach to address covariate shift between calibration and test data, later extended by \citet{podkopaev2021distribution} to accommodate label shift. Both settings, however, involve a different form of distribution shift from the one we study here. \citet{barber2023conformal} extend this line of work by analyzing the effects of general distribution shifts on the validity of conformal methods, focusing however on worst-case scenarios; see also \citet{farinhas2024nonexchangeable}.

In contrast, our work moves away from this worst-case perspective. We aim to explain why conformal outlier detection with contaminated reference data often results in a conservative type-I error rate, rather than investigating type-I error inflation, which, while theoretically possible in adversarial settings, appears less common in practice. Furthermore, we focus on developing methods to address this over-conservativeness, boosting detection power.

A more closely related line of work investigates the robustness of conformal prediction to label noise \cite{einbinder2022conformal, sesia2023adaptive, clarkson2024splitconformalpredictiondata,penso2025estimating} or other forms of data contamination \cite{pmlr-v202-zaffran23a, zaffran2024predictive, feldman2024robust}. Specifically, \citet{einbinder2022conformal} and \citet{sesia2023adaptive} show that, under certain assumptions, conformal prediction for classification with noisy labels often results in conservative type-I error rates. Furthermore, \citet{sesia2023adaptive} propose a method to address this conservativeness by leveraging an explicit ``label noise model'' that captures the relationship between the true and contaminated labels in the calibration dataset.

In contrast, this paper avoids relying on an explicit model for the contaminated data, as such models can be difficult to estimate in practice within our context. Instead, we utilize a pre-trained black-box outlier detection model and a limited annotation budget to selectively and reliably trim outliers from the contaminated set. Furthermore, it is important to emphasize that the method proposed by \citet{sesia2023adaptive} is primarily designed for classification tasks with relatively balanced data, whereas outlier detection naturally involves extreme class imbalance. This distinction underscores the need for solutions specifically tailored to outlier detection.