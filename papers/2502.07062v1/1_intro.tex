\section{Introduction}
\textbf{Submodular Optimization.} Submodular optimization is a powerful framework for solving combinatorial optimization problems that exhibit diminishing returns~\citep{Nemhauser1978,feige1995approximating,cornuejols1977exceptional}. In a monotone setting, adding more elements to a solution always increases its utility, but at a decreasing rate. On the other hand, non-monotone objectives may have elements that, when added, can reduce the utility of a solution. This versatility makes submodular optimization widely applicable across various domains. For instance, in data summarization~\citep{DBLP:conf/aaai/MirzasoleimanJ018,DBLP:conf/nips/TschiatschekIWB14} and feature selection~\citep{DBLP:journals/corr/abs-2202-00132}, it helps identify the most informative subsets efficiently. In social network analysis~\citep{DBLP:conf/kdd/KempeKT03}, it aids in influence maximization by selecting a subset of individuals to maximize information spread. Additionally, in machine learning~\citep{DBLP:conf/acl/BairiIRB15,DBLP:conf/nips/ElenbergDFK17,DBLP:conf/iclr/PrajapatMZ024}, submodular functions are used for diverse tasks like active learning, sensor placement, and diverse set selection in recommendation systems. These applications demonstrate the flexibility and effectiveness of submodular optimization in addressing real-world problems with both monotone and non-monotone objectives. % From ChatGPT

\textbf{Problem Definition and Greedy Algorithms.} In this work, we consider the size-constrained maximization of a submodular function: given a submodular function $f$ on ground set of size $n$, and given an integer $k$, find $\argmax_{S \subseteq \uni, |S| \le k} f(S)$. If additionally $f$ is assumed to be monotone, we refer to this problem as \mon{}; otherwise, we call the problem \nmon{}. 
For \mon{}, a standard greedy algorithm gives the optimal\footnote{Optimal in polynomially many queries to $f$ in the value query model \citep{Nemhauser1978a}.} approximation ratio
of $1 - 1/e \approx 0.63$ \citep{Nemhauser1978} in
at most $kn$ queries\footnote{Typically, queries to $f$ dominate other parts of the computation, so in this field time complexity of an algorithm is usually given as oracle complexity to $f$.} to $f$.
In contrast, standard greedy can't achieve any constant
approximation factor in the non-monotone case; however,
an elegant, randomized variant of greedy, the \randomgreedy{}
of \citet{Buchbinder2014a} obtains the same greedy
ratio (in expectation) of $1-1/e$ for monotone objectives, and $1/e \approx 0.367$ for the general,
non-monotone case, also in at most $kn$ queries. 
However, as modern instance sizes in applications have become very large,
$kn$ queries is too many. In the worst case, $k = \Omega(n)$, and
the time complexity of these greedy algorithms becomes quadratic.
To improve efficiency, \citet{DBLP:journals/mor/BuchbinderFS17} 
leverage sampling technique to get \frg,
reducing the query complexity to $\oh{n}$ when
$k \ge 8\epsi^{-2}\log(2\epsi^{-1})$.

\begin{table*}[ht]
  \centering \scriptsize
  \caption{Theoretical comparison of greedy algorithms and parallel algorithms with sublinear adaptivity.}
    \begin{tabular}{clll}
    \toprule
    Algorithm  & Ratio& Queries & Adaptivity \\%& Randomized?\\
    \midrule
    \randomgreedy{} \citep{Buchbinder2014a} & \boldmath{$1/e$} & $kn$ & $k$ \\
    \frg~\citep{DBLP:journals/mor/BuchbinderFS17} & \boldmath{$1/e-\epsi$} & $\oh{n}$ & $k$ \\
    \ig~\citep{DBLP:conf/nips/Kuhnle19} & $0.25\| 0.25-\epsi$ & $\oh{kn} \|$\boldmath{$\oh{n\log{k}}$} & $k \| \oh{n}$\\
    \itg~\citep{DBLP:conf/kdd/ChenK23} & \boldmath{$1/e-\epsi$}* & $\oh{kn} \|$\boldmath{$\oh{n\log{k}}$} & $k \| \oh{n}$\\
    \midrule
    \textsc{Blits}~\citep{Balkanskia} & $1/(2e) - \epsi$ & $  \oh{OPT^2 n \log^2(n) \log(k)} $ & $ \oh{\log^2(n)} $\\
    {\citet{Chekuri2019a}} & $3 - 2 \sqrt{2} - \epsi$ & $ \oh{k^4n \log^2(n)}$ & $\oh{\log^2(n)}$ \\
    \textsc{ANM}~\citep{fahrbach2018non}& $0.039 - \epsi$
     & \boldmath{$\oh{{n \log (k)}}$} & \boldmath{$\oh{\log (n)}$}\\
     \citet{Ene2020a} & \boldmath{$1/e - \epsi$} & $\oh{k^2n\log^2(n)}\ddagger$ & \boldmath{$\oh{\log (n)}$}\\
    {\textsc{ParKnapsack}\citep{amanatidis2021submodular}}& $0.172-\epsi$ 
    & $\oh{kn\log(n)\log(k)} \| \oh{n\log(n)\log^2(k)}$
    & {\boldmath{$\oh{\log(n)}$}} $\|\oh{\log(n)\log(k)}$ \\
    AST \citep{Chen2024} & $1/6 - \epsi $ & \boldmath{$\oh{n \log (k)}$} & \boldmath{$\oh{\log(n)}$} \\
    ATG \citep{Chen2024} & $0.193 - \epsi$ & \boldmath{$\oh{n \log (k)}$} & $\oh{\log(n) \log(k)}$ \\
    \parskp \citep{Cui2023}& $0.125-\epsi$ & $\oh{n\log^2(n)\log (k) }\| \oh{kn \log^2(n)}$ & $\oh{ \log(n) \log (k) } \| \oh{\log(n)} $ \\
    \parssp \citep{Cui2023}& $0.25-\epsi$ & $\oh{n\log^2(n)\log (k) }\|\oh{kn\log^2(n)}$ & $\oh{ \log^2(n) \log (k) }\| \oh{\log^2(n)} $ \\
    \midrule
    \ptgone (Alg.~\ref{alg:ptgone}) & $0.25-\epsi \dagger$  & $\oh{n\log(n)\log(k)}$& $\oh{\log(n)\log(k)}$\\
    \ptgtwo (Alg.~\ref{alg:ptgtwo}) & \boldmath{$1/e - \epsi $} & $\oh{n\log(n)\log(k)}$ & $\oh{\log(n)\log(k)}$ \\
    \bottomrule
    \end{tabular}
    \parbox{\linewidth}{\small * Randomized \itg achieve $1/e-\epsi$ approximation ratio with probability $(\ell+1)^{-\ell}$, where $\ell = \frac{2e}{\epsi}+1$}
    % \parbox{\linewidth}{\small $\dagger$ \ptgone is the only deterministic parallel approximation algorithm.}
    \parbox{\linewidth}{\small $\ddagger$ The parallel algorithm in \citet{Ene2020a} queries to the continuous oracle.}
    \parbox{\linewidth}{\small $\dagger$ The approximation ratio is achieved with high probability.}
  \label{table:cmp}
  \vspace*{-1.5em}
\end{table*}  

\textbf{Parallelizable Algorithms.}
In addition to reducing the number of queries,
recently, much work has focused on developing parallelizable algorithms for submodular optimization.
Parallelizability is measured by the \textit{adaptive complexity} of an algorithm. That is,
the queries to $f$ are divided into adaptive rounds, where within each round the set queried may only depend
on the results of queries in previous rounds,
and thus the queries in each round may be arbitrarily parallelized. Thus, the lower the adaptive complexity, the more
parallelizable an algorithm is. 
Although the initial algorithms with sublinear adaptivity were
impractical, for the monotone case, these works culminated in two practical algorithms:
\fast{} \citep{Breuer2020} and \lspgb{} \citep{Chen2021}, both of which achieve nearly
the optimal ratio in nearly linear time and nearly optimal adaptive rounds. 
For the nonmonotone case, 
the best approximation ratio achieved in sublinear adaptive rounds is $1/e$~\citep{Ene2020a}.
However, this algorithm queries to a continuous oracle,
which needs to be estimated through a substantial number of queries to the original set function oracle.
Although practical, sublinearly adaptive algorithms have also been
developed, the best ratio achieved in nearly linear time is nearly $1/4$ \citep{Cui2023},
significantly worse than the state-of-the-art\footnote{The best known ratio in polynomial time was very recently improved from close to $1/e$ to $0.401$ \citep{Buchbinder2023}},
this $1/4$ ratio also stands as the best even for superlinear time parallel algorithms.
Further references to parallel algorithms and their theoretical guarantees are provided in Table~\ref{table:cmp}.
 
\textbf{Greedy Variants for Parallelization.}
To enhance the approximation ratios for combinatorial sublinear adaptive algorithms, 
it is crucial to develop parallelizable algorithms that serve as universal frameworks for such parallel approaches. 
Among existing methods, \ig~\citep{DBLP:conf/nips/Kuhnle19}, 
with an approximation ratio of $1/4$, and \itg\citep{DBLP:conf/kdd/ChenK23},
with an expected approximation ratio of $1/e$, 
have emerged as promising candidates 
due to their unique and deterministic interlacing greedy procedures. 
We provide their pseudocodes and theoretical guarantees in Appendix~\ref{apx:pseudocode}. 
\ig alternates between two sets, adding elements greedily to each set in turn. 
Building on this, \itg generalizes the approach by maintaining $\ell$ sets, 
each containing $k/\ell$ elements at each iteration.
However, in their original formulations, both algorithms require an initial step to guess whether the first element added to each solution belongs to the optimal set $O$.
This results in repeated for loop with different initial values in \ig
and a success probability of 
$(\ell+1)^{-\ell}$
for the randomized version of \itg. 
% In this paper, we present an improved analysis for a revised version of \ig and \itg, 
% which eliminates the need for this guessing step, providing a more effective and theoretically sound approach.


% \textbf{The threshold sampling subroutines.}

\subsection{Contributions}
% \textbf{Simplified \ig~\citep{DBLP:conf/nips/Kuhnle19} 
% and \itg~\citep{DBLP:conf/kdd/ChenK23} with New Insights.}
\textbf{Technical Contributions: Blended Marginal Gains Strategy.}
Our first contribution is a technical advancement: 
a novel blended marginal gains strategy for analyzing
interlaced greedy-based algorithms,
such as \ig~\citep{DBLP:conf/nips/Kuhnle19} and \itg\citep{DBLP:conf/kdd/ChenK23}.
This strategy introduces multiple upper bounds on the marginal gain 
of the same element regarding the solution, 
offering greater flexibility in the analysis.
As a result, we present Alg.~\ref{alg:gdone} and Alg.~\ref{alg:gdtwo} 
with $\oh{kn}$ query complexity in Section~\ref{sec:gd},
achieving approximation ratios of $1/4$ and $1/e-\epsi$, respectively.
A key advantage of our approach is the elimination of the guessing step 
required in prior methods, which improves the success probability of \itg
from $(1+\ell)^{-\ell}$ to $1$.

Furthermore, we extend this analysis to fast variants of interlaced greedy algorithms, 
where the traditional greedy procedure is replaced with 
the descending threshold greedy method~\citep{DBLP:conf/soda/BadanidiyuruV14}.
This reduces the query complexity to $\oh{n\log(k)}$,
while maintaining strong theoretical guarantees.
In summary, our techniques not only simplify the analysis but also 
enhance the practicality and performance of these algorithms.

Although these algorithms have high adaptive complexity,
they serve as building blocks for our parallel algorithms.


% \textbf{Interlace Descending Threshold Greedy Algorithms.}
% We replace the traditional greedy procedure with the descending threshold greedy procedure~\citep{DBLP:conf/soda/BadanidiyuruV14},
% resulting in Algs.~\ref{alg:tgone} and~\ref{alg:tgtwo},
% which serve as a foundation for our parallelized algorithms.
% These algorithms interlace the threshold
% greedy procedure, where sets alternately adopt elements
% similar to Algs.~\ref{alg:gdone} and~\ref{alg:gdtwo}.
% We provide pseudocode and theoretical analysis in Section~\ref{sec:tg},
% showing that this approach achieves the same approximation ratio 
% while requiring $\oh{n\log(k)}$ queries to the oracle.

\textbf{Parallel Algorithms with Logarithmic Adaptivity and Nearly-linear Query Complexity
Using a Unified Subroutine.}
In Section~\ref{sec:ptg},
we present two sublinear adaptive algorithms, 
\ptgone (\ptgoneshort) and \ptgtwo (\ptgtwoshort).
These algorithms are unified by a single subroutine, \ptgoneshort,
which incorporates a novel subset selection technique 
tailored for interlaced threshold sampling procedures.
% It not only ensures that the elements are added in an alternating manner, 
% preserving the core property of interlaced greedy-based algorithms,
% but also guaranteeing robust average marginal gains for the selected subsets-a 
% critical property of threshold sampling procedures.
It ensures that elements are added alternately, preserving the core property of interlaced greedy-based algorithms. Additionally, it guarantees robust average marginal gains for the selected subsets—a critical property of threshold sampling procedures.

Leveraging this unified framework, a single call to \ptgtwoshort achieves an approximation ratio of $(1/4-\epsi)$ with high probability.
Repeated calls to \ptgtwoshort yields \ptgtwoshort,
further enhance the approximation ratio to an expected $(1/e-\epsi)$.
Both algorithms achieve $\oh{\log(n) \log(k)}$ adaptivity
and $\oh{n\log(n)\log(k)}$ query complexity,
making them efficient for large-scale applications.

\textbf{Empirical Evaluation.}
Finally, we evaluate the performance of our parallel algorithms
in Section~\ref{sec:exp} and Appendix~\ref{apx:exp}
across two applications and four datasets.
The results demonstrate that our algorithms achieve competitive objective values.
Notably, \ptgone outperforms other algorithms in terms of query efficiency,
highlighting its practical advantages.








%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main.tex"
%%% End: 
