\section{Preliminary}
\textbf{Notation.}
We denote the marginal gain of adding $A$ to $B$ by $\marge{A}{B} = \ff{A\cup B} - \ff{B}$.
For every set $S\subseteq U$ and an element $x\in \uni$,
we denote $S\cup \{x\}$ by $S+x$ and $S\setminus \{x\}$ by $S-x$.

\textbf{Submodularity.}
A set function $f:2^\uni\to \reals$ is submodular, 
if $\marge{x}{S}\ge \marge{x}{T}$ for all $S\subseteq T\subseteq \uni$
and $x\in \uni\setminus T$,
or equivalently, for all $A, B\subseteq \uni$,
it holds that $\ff{A} + \ff{B}\ge \ff{A\cup B} + \ff{A\cap B}$.
With a size constraint $k$,
let $O =\argmax_{S\subseteq \uni, |S| \le k} \ff{S}$.

In Appendix~\ref{apx:prop}, we provide several key propositions
derived from submodularity that streamline the analysis.

\textbf{Organization.}
In Section~\ref{sec:gd}, the blending technique is introduced
and applied to \ig and \itg,
with detailed analysis provided in Appendix~\ref{apx:greedy-1/4}
and~\ref{apx:greedy-1/e}.
Section 4 discusses their fast versions with
pseudocodes and comprehensive analysis in
Appendix~\ref{apx:tg}.
Subsequently, Section~\ref{sec:ptg} delves into our sublinear adaptive algorithms, 
with further technical details and proofs available in Appendix~\ref{apx:ptg}.
Finally, we provide the empirical evaluation in Section~\ref{sec:exp},
with its detailed setups and additional results
in Appendix~\ref{apx:exp}.


\section{The Blending Technique for Greedy}\label{sec:gd}
% \section{A Fresh Take on Interlaced Greedy Based Algorithms}
% In this section, we present a novel analysis of 
% \ig~\citep{DBLP:conf/nips/Kuhnle19} and \itg~\citep{DBLP:conf/kdd/ChenK23}.
% This analysis eliminates the need for the guessing step in both algorithms,
% simplifying their implementation while preserving their theoretical guarantees.
In this section, we present two practical greedy variants
that simplify \ig~\citep{DBLP:conf/nips/Kuhnle19} and \itg~\citep{DBLP:conf/kdd/ChenK23}.
These variants are developed using a novel analysis technique 
called the blended marginal gains strategy. 
Notably, the proposed algorithms are not only simpler than \ig and \itg 
but also retain their theoretical guarantees, 
making them both efficient and theoretically sound. 
This section provides a detailed exposition of these algorithms and their underlying principles.

\subsection{Deterministic Greedy Variant with $1/4$ Approximation Ratio}
\label{sec:greedy-1/4}
\textbf{Analysis of \ig.}
\ig (Alg.~\ref{alg:ig} in Appendix~\ref{apx:pseudocode}) operates as follows.
% Initially, it guesses whether the max singleton $a_0$ is in the optimal solution $O$.
First, two empty solution sets, $A$ and $B$, are maintained
and constructed by interlacing two greedy procedures.
Second, two additional solution sets, $D$ and $E$, are initialized with the maximum singleton $a_0$,
and are constructed using the same interlaced greedy procedure.
% and they are constructed using an interlaced greedy for loop.
% (same as Lines~\ref{line:gdone-for-begin}-\ref{line:gdone-for-end} of Alg.~\ref{alg:gdone}).
% Conversely, if $a_0\in O$, $A$ and $B$ are initialized with $\{a_0\}$,
% and the interlaced greedy step is repeated.
Finally, the best solution among these sets are returned.

The core idea of the algorithm is to greedily construct two disjoint solutions,
$A$ and $B$, in an alternating manner,
ensuring the following inequalities,
\begin{align}
    &\ff{O\cup A} + \ff{O\cup B} \ge \ff{O}, \label{inq:gdone-opt}\\
    &2\ff{A} \ge \ff{O\cup A},\label{inq:gdone-A}\\
    &2\ff{B} \ge \ff{O\cup B},\label{inq:gdone-B}
\end{align}
Note that, the element $a_0$ is treated separately.
% 1) if $a_0\notin O$, then above inequalities holds for $\{S, T\} = \{A, B\}$;
% 2) otherwise, $\{S, T\} = \{D, E\}$.
The second round of the interlaced greedy procedure is specifically designed to
handle the case where $a_0\in O$.
The above inequalities are then guaranteed to hold with solution sets $\{D, E\}$.
However, intuitively, adding an element from $O$ to the solution should not negatively impact it.
This leads to a natural question: 
\textit{Can the second interlaced greedy step be eliminated?}    


% To bound $\marge{O}{A}$, consider the elements in $O\setminus A$.
% Since the interlaced greedy for loop begins by adding elements to $A$
% in both cases,
% $O\setminus A$ can be ordered as $\{o_1, o_2, \ldots\}$,
% where each $o_i$ is a candidate for greedy selection
% when the $i$-th element is added to $A$.
% By the greedy selection rule,
% $\marge{o_i}{A} \le \ff{A_i} -\ff{A_{i-1}}$ for each $o_i\in O\setminus A$,
% where $A_i$ represents the first $i$ elements added to $A$.
% Then, by the first property in Proposition~\ref{prop:sum-marge}, it follows that

% \vspace*{-1em}
% {\small\begin{equation*}
% \marge{O}{A} \le \ff{A} \Rightarrow \ff{O\cup A} \le 2\ff{A}.
% \end{equation*}}

% Bounding $\marge{O}{B}$ requires a slightly different approach.
% In general, if $a_0 \notin O\setminus B$,
% the above analysis can be directly applied to bound $\marge{O}{B}$.
% Next, consider the two cases of $a_0$ discussed earlier.
% In the first case, where $a_0 \notin O$,
% it naturally follows that $a_0 \notin O\setminus B$.
% In the second case, where $a_0 \in O$,
% since $a_0$ is also included in $B$,
% it again follows that $a_0 \notin O\setminus B$.
% Therefore, a similar conclusion holds,

% \vspace*{-1em}
% {\small\begin{equation*}
% \marge{O}{B} \le \ff{B} \Rightarrow \ff{O\cup B} \le 2\ff{B}.
% \end{equation*}}

% However, this result does not directly extend to $B$,
% as the first element added to $A$, say $a_0$, might be in $O$.
% This prevents us from bounding 
% $\marge{a_0}{B}$ by $\ff{B_1}- \ff{\emptyset}$.
% To address this issue, $a_0$ is also added to $B$
% and solutions are then built with the same interlaced greedy step.
% Then, we can reorder $O\setminus B$ as $\{o_1, o_2, \ldots\}$,
% satisfying $\marge{o_i}{B} \le \ff{B_i} - \ff{B_{i-1}}$.
% Furthermore, the shared element $a_0$ is in $O$ which does not break Inequality~\eqref{inq:gdone-opt}.

% In summary, the second interlaced greedy step is introduced
% to address the case where $a_0\in O$.
% However, intuitively, adding an element from $O$ to the solution should not compromise it.
% This raises the question: 

% \textit{Can we eliminate the second interlaced greedy step?}

\textbf{A New Insight on Analyzing \ig: Blended Marginal Gains Strategy.}
The answer to the above question is YES.
In the original analysis of \ig, the upper bound of $\ff{O\cup A}$
only relies on $\ff{A}$ (Inequality~\eqref{inq:gdone-A}). 
Same as $\ff{O\cup B}$ (Inequality~\eqref{inq:gdone-B}).
In what follows,
we introduce a blended approach to analyze \ig with only
a single interlaced greedy step (Alg.~\ref{alg:gdone}).
Specifically, we utilize a mixture of $\ff{A}$ and $\ff{B}$,
or more precisely, a combination of the marginal gains when adding elements to each,
to establish tighter bounds for $\ff{O\cup A}$ or $\ff{O\cup B}$.
\begin{algorithm}[ht]
    \KwIn{evaluation oracle $f:2^{\uni} 
    \to \reals$, constraint $k$}
    \Init{$A\gets B\gets \emptyset$, add $2k$ dummy elements to the ground set}
    \For{$i\gets 1$ to $k$ \label{line:gdone-for-begin}}{
        $a\gets \argmax_{x\in \uni\setminus \left(A\cup B\right)} \marge{x}{A}$\; \label{line:gdone-greedy-A}
        $A\gets A+a$\;
        % \tcc*[r]{If $\marge{a_i}{A_{i-1}} < 0$, a dummy element is added instead.}
        $b\gets \argmax_{x\in \uni\setminus \left(A\cup B\right)} \marge{x}{B}$\; \label{line:gdone-greedy-B}
        $B\gets B+b$\;\label{line:gdone-for-end}
        % \tcc*[r]{If $\marge{b_i}{B_{i-1}} < 0$, a dummy element is added instead.}
    }
    \Return{$S\gets \argmax\{\ff{A}, \ff{B}\}$}
    \caption{A deterministic $1/4$-approximation algorithm with $\oh{ nk }$ queries.}
    \label{alg:gdone}
\end{algorithm}

We summarize our blending technique as follows.
Rather than relying solely on the greedy selection rule to 
bound $\marge{o}{A}$ or $\marge{o}{B}$ for each $o\in O$,
we split $O$ into two parts,
relaxing the marginal gain of one part solely through submodularity.
% To bound $\marge{O}{A}$, 
% split $O$ into two parts. 
% One is the part overlapping with the prefixes of $B$,
% and we use portion of $\ff{B}$ to bound it by submodularity.
% For the remaining element, we follow the original analysis
% of \ig and bound it with portion of $\ff{A}$.
Below, we provide a general proposition that captures the key
insight achieved by the blending technique.
\begin{proposition}\label{prop:blend}
For any submodular function $f:2^{\uni}\to \reals$ and $A, B, O \in \uni$,
Let $A_i$ be a prefix of $A$ with size $i$
such that $A_i \subseteq O$.
Similarly, define $B_j$.
It satisfies that,
\begin{align}
    \marge{O}{B} &\le \ff{A_i} + \marge{O\setminus A_i}{B},\label{inq:gdone-B2}\\
    \marge{O}{A} &\le \ff{B_j} + \marge{O\setminus B_j}{A}, \label{inq:gdone-A2}
\end{align}
\end{proposition}


By summing up Inequalities~\eqref{inq:gdone-B2} and~\eqref{inq:gdone-A2} and carefully
selecting the values of $i$ and $j$ under different cases illustrated in Fig.~\ref{fig:gdone},
the $1/4$ approximation ratio holds for Alg.~\ref{alg:gdone}.
\begin{figure}[ht]
\centering
\subfigure[$i^* \le j^*$]{\label{fig:gdone-1}\includegraphics[width=0.2\textwidth]{fig/ig-1.pdf}}
\subfigure[$i^* > j^*$]{\label{fig:gdone-2}\includegraphics[width=0.2\textwidth]{fig/ig-2.pdf}}
    \caption{This figure depicts the components of solution sets $A$ and $B$ in Alg.~\ref{alg:gdone}.
    The black rectangle highlights a sequence of consecutive elements from $O$
    that were added to the solution at the initial.
    Red circles with a cross mark signifies the first element in $A$ or $B$ that is outside $O$.
    }
\label{fig:gdone}
\end{figure}

% Let $a_i$ be the $i$-th element added to $A$,
% and $A_i$ be the set containing the first $i$ elements of $A$.
% Similarly, define $b_i$ and $B_i$ for the solution $B$.

% Following the original analysis of \ig, 
% by the greedy selection rule, the following inequalities hold,

% \vspace*{-1em}
% {\small\begin{align}
%     &\marge{o}{A_{i-1}} \le \marge{a_i}{A_{i-1}}, 
%     \forall o\in O\setminus (A_{i-1} \cup B_{i-1}) \label{inq:gdone-blend-1}\\
%     &\marge{o}{B_{i-1}} \le \marge{b_i}{B_{i-1}}, 
%     \forall o\in O\setminus (A_{i} \cup B_{i-1})\label{inq:gdone-blend-2}
% \end{align}}
% To derive a new bound, we track the longest prefix of $A$ and $B$ 
% that lies within the optimal solution $O$.
% Define $i^* = \argmax\{i \in [k]: A_i \subseteq O\}$
% and $j^* = \argmax\{i \in [k]: B_i \subseteq O\}$.
% Refer to Fig.~\ref{fig:gdone} as an illustration.

% For any $i \le i^*$, let $o_i = a_i$ and $O_i = A_i$.
% Then, by submodularity,

% \vspace*{-1em}
% {\small\begin{align}\label{inq:gdone-blend-3}
%     \marge{o_i}{B\cup O_{i-1}} \le \marge{a_i}{A_{i-1}}, \forall i \le i^*.
% \end{align}}
% Thus, by ordering $O\setminus B$,
% we can bound $\marge{O}{B}$ by blending Inequalities~\eqref{inq:gdone-blend-2}
% and~\eqref{inq:gdone-blend-3}.

% Similarly, for any $i \le j^*$, let $o_i = b_i$ and $O_i = B_i$.
% By submodularity,

% \vspace*{-1em}
% {\small\begin{align}\label{inq:gdone-blend-4}
%     \marge{o_i}{A\cup O_{i-1}} \le \marge{b_i}{B_{i-1}}, \forall i \le j^*.
% \end{align}}
% By ordering $O\setminus A$,
% we can bound $\marge{O}{A}$ by blending Inequalities~\eqref{inq:gdone-blend-1}
% and~\eqref{inq:gdone-blend-4}.

% By choosing the best blending strategy under different cases
% (Fig.~\ref{fig:gdone-1} and~\ref{fig:gdone-2}).
% It holds that

% \vspace*{-1em}
% {\small\begin{align*}
%     \marge{O}{A} + \marge{O}{B} \le \ff{A} + \ff{B}.
% \end{align*}}

% Next, order $O\setminus A$ as $\{o_1, o_2, \ldots\}$ such that
% $o_i = b_i$ if $o_i\in B$.
% Let $O_i$ be the first $i$ elements with this order.
% Then, if $i \le j^*$, we know that $o_i = b_i$ and $O_{i-1} = B_{i-1}$.
% By submodularity, it holds that,
% \begin{equation}\label{inq:gdone-blend-1}
% \marge{o_i}{A\cup O_{i-1}} \le \marge{b_i}{B_{i-1}}, \forall i \le j^*.
% \end{equation}
% Moreover, since $o_i \notin B_{i-1}$ for each $o_i \in O\setminus A$,
% $o_i$ is a candidate when $a_i$ is added to $A$.
% Thus, the following inequality holds by submodularity and the greedy selection rule,
% \begin{equation}\label{inq:gdone-blend-2}
% \marge{o_i}{A\cup O_{i-1}} \le \marge{a_i}{A_{i-1}}, \forall i \ge 1.
% \end{equation}

% Similarly, we can order $O\setminus B$ as $\{o_1, o_2, \ldots\}$ such that
% $o_i = a_i$ if $o_i\in A$,
% and let $O_i$ be the first $i$ elements with this order.
% Then, if $i \le i^*$, we know that $o_i = a_i$ and $O_{i-1} = A_{i-1}$.
% By submodularity, it holds that,
% \begin{equation}\label{inq:gdone-blend-3}
% \marge{o_i}{B\cup O_{i-1}} \le \marge{a_i}{A_{i-1}}, \forall i \le i^*.
% \end{equation}
% Also, since $o_{i} \notin A_{i-1}$,
% $o_i$ is a candidate when $b_{i-1}$ is added to $B$ for each $i \ge 2$.
% Thus,
% \begin{equation}\label{inq:gdone-blend-4}
% \marge{o_i}{B\cup O_{i-1}} \le \marge{b_{i-1}}{B_{i-2}}, \forall i \ge 2.
% \end{equation}
We provide the theoretical guarantee of Alg.~\ref{alg:gdone} below.
The detailed analysis can be found in Appendix~\ref{apx:greedy-1/4}.
\begin{restatable}{theorem}{thmgdone}\label{thm:gdone}
With input instance $(f, k)$, Alg.~\ref{alg:gdone} returns a set $S$ with $\oh{kn}$ queries
such that $\ff{S} \ge 1/4 \ff{O}$.
\end{restatable}


\begin{algorithm}[ht]
	\KwIn{evaluation oracle $f:2^{\uni} \to \reals$, constraint $k$, size of solution $\ell$, error $\epsi$}
    \Init{$G\gets \emptyset$, $V\gets \uni$, $m \gets \left\lfloor\frac{k}{\ell}\right\rfloor$, add $2k$ dummy elements to the ground set.}
    \For{$i\gets 1$ to $\ell$}{
    	$A_{l}\gets G, \forall l \in [\ell]$\;
    	\For{$j\gets 1$ to $m$}{\label{line:gdtwo-for-2-start}
    		\For{$l\gets 1$ to $\ell$}{
    			$a \gets \argmax_{x\in V}\marge{x}{A_{l}}$\;
                % $\delta_{l, j} \gets \marge{a_{l, j}}{A_l}$\;
    			$A_{l}\gets A_{l}+a$, $V\gets V-a$ \hspace*{-0.7em}\;
                % \tcc*[r]{If $\marge{a_{l, j}}{A_{l}} < 0$, a dummy element is added instead.}
    		}
    	}\label{line:gdtwo-for-2-end}
        % $A_l'\gets \left\{a_{l, j}: j\in \argmax\limits_{I\subseteq [m], |I| = m-1} \sum\limits_{j\in I}\delta_{l, j}\right\}$\;
    	$G\gets$ a random set in $\{A_l\}_{l\in [\ell]}$\;
    }
    \Return{$G$}
    \caption{A randomized $1/e$-approximation algorithm with $\oh{ nk\ell }$ queries. }
    \label{alg:gdtwo}
\end{algorithm}
\subsection{Randomized Greedy Variant with $1/e$ Approximation Ratio}
\label{sec:greedy-1/e}
In this section, we extend the blending technique introduced in the previous section 
to \itg~\citep{DBLP:conf/kdd/ChenK23} 
and propose a simplified algorithm (Alg.~\ref{alg:gdtwo}).
Alg.~\ref{alg:gdtwo} avoids the guessing step in \itg
improving the success probability from $(\ell+1)^{-\ell}$ to $1$.
The intuition behind the algorithm is outlined in the following.
\begin{restatable}{theorem}{thmgdtwo}\label{thm:gdtwo}
With input instance $(f, k, \ell, \epsi)$
such that $\ell =\oh{\epsi^{-1}} \ge \frac{2}{e\epsi}$ and $k \ge \frac{2(e\ell-2)}{e\epsi-\frac{2}{\ell}}$,
Alg.~\ref{alg:gdtwo} returns a set $G$ with $\oh{kn/\epsi}$ queries
such that $\ex{\ff{G}} \ge \left(1/e-\epsi\right) \ff{O}$.
\end{restatable}
In the following discussion, we focus on the case where $k\, \text{mod}\,\ell = 0$.
For the scenario where $k \, \text{mod}\,\ell > 0$, please refer to Appendix~\ref{apx:greedy-1/e}.

% \itg can be seen as an interpolation between 
% the standard greedy algorithm~\citep{DBLP:journals/mp/NemhauserWF78}
% and the \rg~\cite{DBLP:conf/soda/BuchbinderFNS14}.
% The algorithm runs a for loop, where in each iteration,
% built upon the solutions returned from the previous iteration,
% it creates $\ell+1$ pools, each containing $\ell$ candidate solutions,
% by guessing the element in the optimal solution that provides the largest marginal gain.
% To find the right pool, the algorithm must search through all pools.
% In the following, we offer a different perspective on analyzing the algorithm
% and show how the guessing step can be eliminated.

\textbf{From Interlacing $2$ Greedy to $\ell$ Greedy.}
% Let $G$ be the intermediate solution at the beginning of an iteration in \itg,
% and $\{A_l: l \in [\ell]\}$ be the solution sets at the end of this iteration.
% In iteration $m$ of the outer for loop in \itg (Alg.~\ref{alg:itg} in Appendix~\ref{apx:pseudocode}),
% $G_{m-1}$ represents the intermediate solution at the beginning of this iteration,
% and $\{a_1, \ldots, a_\ell\}$ are the top $\ell$ elements in $\uni\setminus G_{m-1}$
% with the largest marginal gains on $G_{m-1}$.
% Similar to \ig, \itg makes $\ell+1$ guesses to locate the element
% $o_{\max} = \argmax_{x \in O\setminus G_{m-1}} \marge{x}{G_{m-1}}$.
% Based on these guesses, $\ell+1$ sets of solutions are initialized,
% each constructed using the interlaced greedy procedure.
% At the end of iteration $m$, the algorithm finalizes the sets
% $\{A_{u,i}: 1\le i\le \ell, 0\le u\le \ell\}$.
% With a probability of $(\ell+1)^{-1}$, $G_m$ is chosen from
% the set of solutions $\{A_{u,i}: 1\le i\le \ell\}$ corresponding to the correct guess.
% Under this selection, the following inequality holds,
%
% \vspace*{-1em}
% {\small\[\marge{O}{A_{u, i}} \le \ell\marge{A_{u, i}}{G}, \forall i \in [\ell].\]}
%
% Recall the improvement we made on \ig, introduced in Section~\ref{sec:greedy-1/4}.
% This raises a direct question:
% \textit{Can we apply the blending technique to bound $\sum_{l=1}^\ell\marge{O}{A_l}$ without relying on the guessing step?}
By interlacing $\ell$ greedy procedures, \itg 
(Alg.~\ref{alg:itg} in Appendix~\ref{apx:pseudocode}) constructs 
$\ell+1$ pools of candidates, each containing $\ell$ nearly pairwise disjoint sets.
Among these pools, only $1$ is the \textit{right} pool that ensures the following guarantee.
\begin{align}
    \marge{O}{A_{u, i}} \le \ell\marge{A_{u, i}}{G}, \forall i \in [\ell],\label{inq:gdtwo-itg}
\end{align}
where $G$ is the intermediate solution at the start of this iteration,
and $A_{u,i}$ is the $i$-th solution in the $u$-th pool.
At each iteration, \itg randomly selects a set from all candidate pools,
resulting in a success probability of $(\ell+1)^{-\ell}$,
where the algorithm consistently identifies the correct pool.
In the following, we introduce how to incorporate the blending technique
into the analysis to eliminate the guessing step in \itg.

\textbf{Blended Marginal Gains for Each Pair of Solutions.}
% The only difference between the interlaced greedy procedures
% in Alg.~\ref{alg:gdone} and~\ref{alg:gdtwo}
% is the number of solutions built.
Unlike Alg.~\ref{alg:gdone}, which constructs solutions each of size $k$,
Alg.~\ref{alg:gdtwo} builds solutions with a total size of $k$.
% Different from the interlaced greedy procedure in Alg.~\ref{alg:gdone},
% the procedure in Alg.~\ref{alg:gdtwo} constructs
% $\ell$ solutions, each containing $k/\ell$ elements, rather than $k$.
To align with this structure, we partition $O$ into $\ell$ subsets,
as outlined in Claim~\ref{claim:par-A},
and pair each subset with one of the solutions.
\begin{restatable}{claim}{claimParA}\label{claim:par-A}
At an iteration $i$ of the outer for loop in Alg.~\ref{alg:gdtwo},
let $G_{i-1}$ be $G$ at the start of this iteration,
and $A_{l}$ be the set at the end of this iteration,
for each $l\in [\ell]$.
% Add dummy elements to $O\setminus G_{i-1}$ until its size equals $k$.
The set $O\setminus G_{i-1}$ can then be split into $\ell$ pairwise disjoint sets $\{O_1, \ldots, O_\ell\}$
such that $|O_l| \le\frac{k}{\ell}$ and $\left(O\setminus G_{i-1}\right) \cap \left(A_{l}\setminus G_{i-1}\right) \subseteq O_l$, for all $l \in [\ell]$.
\end{restatable}
Moreover, based on this claim,
we partition the marginal gain of adding $O$ to each $A_l$ as follows.
\begin{align*}
&\sum_{l\in [\ell]}\marge{O}{A_{l}} \le \sum_{l\in [\ell]} \sum_{i\in [\ell]} \marge{O_i}{A_{l}} \tag{Proposition~\ref{prop:sum-marge}}\\
&= \sum_{1\le l_1 < l_2 \le \ell} \left(\marge{O_{l_1}}{A_{l_2}}+\marge{O_{l_2}}{A_{l_1}}\right)+\sum_{l\in [\ell]} \marge{O_l}{A_{l}}. \numberthis \label{inq:gdtwo-par}
\end{align*}
According to Claim~\ref{claim:par-A}, any element in $O_l\setminus A_l$ 
is not sufficiently beneficial to be added to any solution set.
This ensures that $\marge{O_l}{A_{l}}\le \marge{A_l}{G_{i-1}}$.
As for the term $\marge{O_{l_1}}{A_{l_2}}+\marge{O_{l_2}}{A_{l_1}}$,
since elements are added to the solutions in an alternating manner,
Proposition~\ref{prop:blend} for the blending technique can be applied
to bound this term with the greedy selection rule.
These insights are formalized in the following lemma.
\begin{restatable}{lemma}{lemmaparA}\label{lemma:par-A}
Fix on $G_{i-1}$ for an iteration $i$ of the outer for loop in Alg.~\ref{alg:gdtwo}.
Following the definition in Claim~\ref{claim:par-A}, it holds that
\begin{align*}
\text{1) }&\marge{A_{l}}{G_{i-1}} \ge \marge{O_{l}}{A_{l}}, \forall 1\le l \le \ell,\\
\text{2) }&\left(1+\frac{1}{m}\right)\left(\marge{A_{l_1}}{G_{i-1}} + \marge{A_{l_2}}{G_{i-1}}\right)\ge \marge{O_{l_2}}{A_{l_1}} + \marge{O_{l_1}}{A_{l_2}}, \forall 1\le l_1< l_2 \le \ell.
\end{align*}
\end{restatable}
By applying Inequality~\eqref{inq:gdtwo-par} and Lemma~\ref{lemma:par-A}, 
we derive a result analogous to Inequality~\eqref{inq:gdtwo-itg} achieved by \itg,
\begin{align}
    \sum_{l\in [\ell]}\marge{O}{A_{u, i}} \le \ell\left(1+\frac{1}{m}\right)\sum_{l\in [\ell]}\marge{A_{u, i}}{G}.
\end{align} 
This forms the key property necessary to establish the $1/e-\epsi$ approximation ratio.
The detailed analysis of the approximation ratio is provided in Appendix~\ref{apx:greedy-1/e}.

\section{Preliminary Warm-Up of Parallel Approaches: Nearly-Linear Time Algorithms}\label{sec:tg}
In this section, we introduce the fast versions of Alg.~\ref{alg:gdone}
and~\ref{alg:gdtwo},
which substitute the standard greedy procedures
with descending threshold greedy procedure~\citep{DBLP:conf/soda/BadanidiyuruV14}
to achieve a query complexity of $\oh{n\log(k)}$.
Pseudocodes are provided in Appendix~\ref{apx:tg}
as Alg.~\ref{alg:tgone} and~\ref{alg:tgtwo}.
These fast algorithms serve as building blocks for the parallel algorithms introduced later in this work.
Below, we discuss the intuition behind Alg.~\ref{alg:tgone},
while Alg.~\ref{alg:tgtwo} operates in a similar fashion.

Like the greedy variants discussed earlier,
Alg.~\ref{alg:tgone} constructs the solution sets also in
an alternating manner.
Following the blending analysis technique introduced 
in Section~\ref{sec:greedy-1/e}-particularly Proposition~\ref{prop:blend}-the
primary challenge lies in bounding $\marge{O\setminus A_i}{B}$
and $\marge{O\setminus B_i}{A}$
under the threshold greedy framework.

By the alternating addition property,
the threshold value $\tau_i$ decreases to $\tau_i/(1-\epsi)$
if and only if any element outside $A\cup B$ has marginal gain
less than $\tau_i$.
% Furthermore, if the threshold value for solution $A$
% decreases from $\tau_1$ to $\tau_1/(1-\epsi)$ at some point,
% we know that any element outside $A\cup B$ has margianl gain less than $\tau_1$.
% Same for solution $B$.
Define $a_i$ as the $i$-th element added to $A$,
$A_i$ as the first $i$ elements added to $A$,
and $\tau_1^{a_i}$ as the threshold value when adding $a_i$ to $A$.
Similarly, define $b_i$, $B_i$, and $\tau_2^{b_i}$.
The following inequalities hold with upper bounds increased by a factor of
$1/(1-\epsi)$ compared to the standard greedy variants:
% Then, different from the properties 
% (Inequalities~\eqref{inq:gdone-blend-1} and~\eqref{inq:gdone-blend-2}) guaranteed by the standard greedy procedure,
% the descending threshold greedy procedure ensures
% the following inequalities,
% % for each $a_i$ with the corresponding threshold value $\tau_{a_i}$
% % when $a_i$ is added to $A$,
% % any element outside $A_{i-1}\cup B_{i-1}$ has marginal gain
% % less than $\tau_{a_i} /(1-\epsi)$.
% % Similarly, any element outside $A_{i}\cup B_{i-1}$ has marginal gain
% % less than $\tau_{b_i} /(1-\epsi)$.
% % The following inequalities are guaranteed by Alg.~\ref{alg:tgone}.
\begin{align*}
    &\marge{o}{A_{i-1}} \le \tau_1^{a_i} /(1-\epsi) \le \marge{a_i}{A_{i-1}}/(1-\epsi), \forall o\in O\setminus (A_{i-1} \cup B_{i-1}) \\
    &\marge{o}{B_{i-1}} \le \tau_2^{b_i} /(1-\epsi) \le \marge{b_i}{B_{i-1}}/(1-\epsi),\forall o\in O\setminus (A_{i} \cup B_{i-1})
\end{align*}
% However, unlike Alg.~\ref{alg:gdone},
% Alg.~\ref{alg:tgone} might return a set with size less than $k$.
% If final solution $A$ has size less than $k$,
% then, for any $o\in O\setminus (A\cup B)$,
% it holds that 
%
% \vspace*{-1em}
% {\small\begin{equation}\label{inq:tgone-blend-3}
%     \marge{o}{A} < \frac{\epsi M}{(1-\epsi)k} \le \frac{\epsi}{(1-\epsi)k}\ff{O}, \text{ if } |A| < k.
% \end{equation}}
% We can get a similar result for $B$ as follows,
%
% \vspace*{-1em}
% {\small\begin{equation}\label{inq:tgone-blend-4}
%     \marge{o}{B} < \frac{\epsi M}{(1-\epsi)k} \le \frac{\epsi}{(1-\epsi)k}\ff{O}, \text{ if } |B| < k.
% \end{equation}}
%
% Moreover, Inequalities~\eqref{inq:gdone-blend-3} and~\eqref{inq:gdone-blend-4} are also ensured in this case.
%
% Therefore, by blending those Inequalities, we can prove that
% Alg.~\ref{alg:tgone} achieve $1/4-\epsi$ approximation ratio.
Overall, Alg.~\ref{alg:tgone} sacrifices a constant $\epsi$ in approximation ratio
compared to Alg.~\ref{alg:gdone},
but achieves a significantly improved query complexity of $\oh{n\log (k)}$.

In the following, we provide the theoretical guarantees for
Alg.~\ref{alg:tgone} and~\ref{alg:tgtwo}
and left their detailed analysis in Appendix~\ref{apx:tg}.
\begin{restatable}{theorem}{thmtgone}\label{thm:tgone}
With input instance $(f, k, \epsi)$, Alg.~\ref{alg:tgone} returns a set $S$ with $\oh{n\log (k)/\epsi}$ queries
such that $\ff{S} \ge \left(\frac{1}{4}-\epsi\right) \ff{O}$.
\end{restatable}

\begin{restatable}{theorem}{thmtgtwo}\label{thm:tgtwo}
With input instance $(f, k, \epsi)$
such that $\ell = \oh{\epsi^{-1}}\ge \frac{4}{e\epsi}$
and $k \ge \frac{2(2-\epsi)\ell^2}{e\epsi\ell-4}$,
Alg.~\ref{alg:gdtwo} (Alg.~\ref{alg:tgtwo}) returns a set $G_\ell$ with $\oh{n\log(k)/\epsi^2}$ queries
such that $\ff{G_\ell} \ge \left(1/e-\epsi\right) \ff{O}$.
\end{restatable}

\begin{algorithm*}[ht]
\Fn{\ptgone($f, m, \ell, \tau_{\min}, \epsi$)}{
    \KwIn{evaluation oracle $f:2^{\uni} \to \reals$, 
    constraint $m$, constant $\ell$,
    minimum threshold value $\tau_{\min}$, error $\epsi$}
    \Init{$M\gets \max_{x\in \uni} \marge{\{x\}}{\emptyset}$, $I = [\ell]$, $m_0 \gets m$, $A_j\gets A_j'\gets \emptyset$, 
    $\tau_j\gets M$, $V_j \gets \uni, \forall j \in [\ell]$ }
    % \tcp*[h]{$V_j$ contains all good elements outside solutions}
    \While{$I \neq \emptyset$ and $m_0 > 0$}{
        \For(\textcolor{blue}{\tcc*[h]{Update candidate sets with high-quality elements}}){$j\in I$ in parallel\label{line:tgone-update-for-begin}}{
            $\{V_j, \tau_j\} \gets \update(f_{A_j}\restriction_{\uni\setminus\left(\bigcup_{l\in [\ell] A_l}\right)}, V_j, \tau_j, \epsi)$\label{line:tgone-update}\;
            \lIf{$\tau_j < \tau_{\min}$}{ $I\gets I-j$}\label{line:tgone-update-for-end}
        }
        \If(\textcolor{blue}{\tcc*[h]{Add 1 element to each solution alternately}}){$\exists i\in I$ \st $|V_i|< 2\ell$}{
            \For{$j\in I$ in sequence\label{line:tgone-for-begin}\label{line:pig-if-start}}{
                \lIf{$|V_j| = 0$}{
                    $\{V_j, \tau_j\} \gets \update(f_{A_j}\restriction_{\uni\setminus\left(\bigcup_{l\in [\ell] A_l}\right)}, V_j, \tau_j, \epsi)$\label{line:tgone-update-2}
                }
                \lIf{$\tau_j < \tau_{\min}$}{ $I\gets I-j$}
                \Else{
                    $x_j\gets $ randomly select one element from $V_j$ \label{line:tgone-select}\;
                    $A_j\gets A_j+x_j, A_j'\gets A_j'+x_j$\label{line:tgone-update-A}\;
                    $V_l\gets V_l-x_j, \forall l\in [\ell]$\;
                }
            }\label{line:tgone-for-end}
            $m_0\gets m_0-1$\;\label{line:pig-if-end}
        }
        \Else(\textcolor{blue}{\tcc*[h]{Add an equal number of elements to each solution}}){
            $\{\mathcal V_l: l\in I\} \gets \dist(\{V_l: l\in I\})$\label{line:tgone-dist}\label{line:pig-else-start} \tcp*[h]{Create pairwise disjoint candidate sets}\;
            $s \gets \min \{m_0, \min\{|\mathcal V_l|: l\in I\}\}$\;
            \lFor{$j\in I$ in parallel}{
                $i^*_j, B_j \gets \prefix(f_{A_j}, \mathcal V_j, s, \tau_j, \epsi)$\label{line:tgone-prefix}
            }
            $i^*\gets \min\{i^*_1, \ldots, i^*_\ell\}$ \label{line:tgone-index}\;
            \For(\textcolor{blue}{\tcc*[h]{Add $i^*$ high-quality elements to each set}}){$j\gets 1$ to $\ell$ in parallel \label{line:tgone-add-begin}}{
                $S_j\gets$ select $i^*$ elements from $\mathcal V_j[1:i^*_l]$ in three passes, prioritizing $B_j[i] = \textbf{true}$, then $B_j[i] = \textbf{none}$, and finally $B_j[i] = \textbf{false}$
                \label{line:tgone-subset}\;
                $S_j'\gets S_j\cap \left\{v_i \in \mathcal V_j: B_j[i]\neq \textbf{false}\right\}$\label{line:tgone-subset-2}\;
                $A_j\gets A_j\cup S_j, A_j'\gets A_j'\cup S_j'$\label{line:tgone-update-A-2}\;
            }
            $m_0\gets m_0-i*$ \label{line:tgone-update-size}\;\label{line:pig-else-end}
        }
    }
    \Return{$\{A_l': l\in [\ell]\}$}
}
\caption{A highly parallelized algorithm with $\oh{\ell^2\epsi^{-2} \log(n)\log\left(\frac{M}{\tau_{\min}}\right)}$ adaptivity
and $\oh{\ell^3\epsi^{-2} n \log(n)\log\left(\frac{M}{\tau_{\min}}\right)}$ query complexity. Subroutines \update, \dist and \prefix are provided in Appendix~\ref{apx:subroutine}.}
\label{alg:ptgone}
\end{algorithm*}

\section{Sublinear Adaptive Algorithms}\label{sec:ptg}
In this section, we present the main subroutine for our parallel algorithms, 
\ptgone (\ptgoneshort, Alg.~\ref{alg:ptgone}).
A single execution of \ptgoneshort achieves an 
approximation ratio of $1/4-\epsi$ with high probability,
while repeatedly running \ptgoneshort, as in \ptgtwo (\ptgtwoshort, Alg.~\ref{alg:ptgtwo} in 
Appendix~\ref{apx:ptgtwo}), 
guarantees a randomized approximation ratio of $1/e-\epsi$.
Below, we outline the theoretical guarantees,
with the detailed analysis provided in Appendix~\ref{apx:ptg}.
The remainder of this section is dedicated to 
explaining the intuition behind the algorithm.
\begin{restatable}{theorem}{thmptgone}\label{thm:ptgone}
With input $(f, k, 2, \frac{\epsi M}{k}, \epsi)$,
where $M = \max_{x\in \uni} \ff{x}$,
\ptgoneshort (Alg.~\ref{alg:ptgone}) returns $\{A_1', A_2'\}$
with $\oh{\epsi^{-4}\log(n)\log(k)}$ adaptive rounds and $\oh{\epsi^{-5}n\log(n)\log(k)}$ queries with a probability of $1-1/n$.
It satisfies that $\max\{\ff{A_1'}, \ff{A_2'}\}\ge (1/4-\epsi)\ff{O}$.
\end{restatable}

\begin{restatable}{theorem}{thmptgtwo}\label{thm:ptgtwo}
With input $(f, k, \epsi)$ such that
$\ell = \oh{\epsi^{-1}}\ge \frac{4}{e\epsi}$
and $k \ge \frac{(2-\epsi)^2\ell}{e\epsi\ell-4}$,
\ptgtwoshort (Alg.~\ref{alg:ptgtwo}) returns $G$
such that $\ex{\ff{G}} \ge (1/e-\epsi)\ff{O}$
with $\oh{\epsi^{-5}\log(n)\log(k)}$ adaptive rounds and $\oh{\epsi^{-6}n\log(n)\log(k)}$ queries with a probability of $1-\oh{1/(\epsi n)}$.
\end{restatable}
\ptgoneshort begins by initializing $\ell$ empty solutions $\{A_j: j\in [\ell]\}$,
with corresponding threshold values set to 
$M = \max_{x\in \uni}\marge{x}{\emptyset}$. 
Candidate sets $\{V_j: j\in [\ell]\}$ are maintained
and updated using \update
(Alg.~\ref{alg:update} in Appendix~\ref{apx:subroutine})
to filter out elements with marginal gain
less than $\tau_j$ for each solution $A_j$ 
at the start of every iteration 
in Lines~\ref{line:tgone-update-for-begin}-\ref{line:tgone-update-for-end}.
Two cases are then considered.
If any candidate set satisfies $|V_j| < 2\ell$,
$1$ element is added to each set alternately.
Otherwise, if all solutions have sufficient candidates,
pairwise disjoint candidate sets are then constructed in Line~\ref{line:tgone-dist} using \dist (Alg.~\ref{alg:dist} in Appendix~\ref{apx:subroutine}).
By executing a threshold sampling procedure \prefix (Alg.~\ref{alg:prefix} in Appendix~\ref{apx:subroutine}) on Line~\ref{line:tgone-prefix},
a block of elements of equal size is carefully selected and added to each solution.


\ptgoneshort is built upon the descending threshold greedy-based algorithms described in Section~\ref{sec:tg} to ensure nearly linear query complexity.
Additionally, it incorporates the threshold sampling algorithm, \ts~\citep{Chen2024},
to achieve sublinear adaptivity.
% It works as follows.
% For $j$-th solution, a pair of subsets $\{A_j, A_j'\}$ are maintained.
% The first set $A_j$ is responsible for filtering \textit{bad} elements that
% has marginal gain less than the current threshold value $\tau_j$,
% and the second set $A_j'$ is the actual solution such that
% $A_j'\subseteq A_j$ and $\ff{A_j'} \ge \ff{A_j}$.
% Furthermore, a candidate set $V_j$ is also maintained for each solution,
% where it contains all \textit{good} elements with marginal gain larger than $\tau_j$.
%
% To parallelize those algorithms, the following key properties must be maintained.
% \begin{enumerate}
%     \item Elements are added in an alternating manner.
%     \item Multiple elements are added to the solutions within constant adaptive round to achieve sublinear adaptivity.
%     \item Most of the elements added should contribute enough to the solutions.
% \end{enumerate}
% Next, we will introduce how \ptgoneshort achieve those goals.
To accomplish these goals,
several critical properties must be preserved throughout the process.

\subsection{Maintaining Alternating Additions during Parallel Algorithms}
\label{sec:ptg-alter}
This property is crucial to interlaced greedy variants 
introduced in prior sections. 
Below, we demonstrate that \ptgoneshort preserves this property.

During an iteration of the while loop in Alg.~\ref{alg:ptgone},
after updating the candidate sets in Lines~\ref{line:tgone-update-for-begin}-\ref{line:tgone-update-for-end},
two scenarios arise. 
In the first scenario, there exists a candidate set satisfies $|V_j| < 2\ell$,
Lines~\ref{line:pig-if-start}-\ref{line:pig-if-end} are executed,
and elements are appended to solutions one at a time in turn.
In this case, the alternating property is maintained immediately.

In the second scenario, Lines~\ref{line:pig-else-start}-\ref{line:pig-else-end} are executed.
Here, a block of elements with average marginal gain approximately exceeding $\tau_j$
is added to each solution $A_j$.
These blocks $S_j$ are of the same size $i^*$ (Line~\ref{line:tgone-subset})
selected from $\mathcal V_j$,
and guaranteed to be pairwise disjoint by 
Lemma~\ref{lemma:dist} (for \dist, Alg.~\ref{alg:dist} in Appendix~\ref{apx:subroutine}).
Crucially, threshold values $\tau_j$ 
remain unchanged during this step.
While a small fraction of elements in the blocks may have marginal gains below $\tau_j$,
the process retains the alternating property at a structural level: 
the uniform block sizes, and disjoint selection mimic the alternating addition of elements, even when processing multiple elements in parallel.

\subsection{Ensuring Sublinear Adaptivity Through Threshold Sampling}
The core mechanism for achieving sublinear adaptivity 
lies in iteratively reducing the pool of high-quality candidate elements 
(those with marginal gains above the threshold) 
by a constant factor within a constant number of adaptive rounds. 
This progressive reduction ensures efficient convergence.

At every iteration of the while loop in Alg.~\ref{alg:ptgone}, 
after updating the candidate sets,
if there exists $V_j$ such that $|V_j| < 2\ell$,
the following occurs after the for loop
(Lines~\ref{line:tgone-for-begin}-\ref{line:tgone-for-end}):
If the threshold $\tau_j$ remains unchanged,
one element from $V_j$ is added to the solution.
If $\tau_j$ is reduced,
$V_j$ is repopulated with high-quality elements.
This implies that a $1/(2\ell)$-fraction of $V_j$ is filtered out
after per iteration,
or even further, it becomes empty and
the threshold value is updated.

In the second case, where Lines~\ref{line:pig-else-start}-\ref{line:pig-else-end} are executed,
the algorithm employs \prefix (Alg.~\ref{alg:prefix} in Appendix~\ref{apx:subroutine}) in Line~\ref{line:tgone-prefix},
inspired by \ts~\citep{Chen2024}.
% At each iteration of \ts, a \textit{good prefix} is selected
% from the candidate set,
% consisting of elements outside the current solution
% that have marginal gains greater than
% the threshold value.
% After each addition to the solution, a constant fraction of elements in the candidate set
% is filtered out with constant probability, 
% based on the given threshold value,
% thus ensuring sublinear adaptivity.
% 
% In Alg.~\ref{alg:ptgone},
% we apply the same prefix selection step from \ts as
% \prefix (Alg.~\ref{alg:prefix} in Appendix~\ref{apx:subroutine})
% in Line~\ref{line:tgone-prefix}.
% If the prefix sizes returned for each solution are different,
% their minimum value $i^*$ are chosen and
% subsets of size $i^*$ are added to each solution 
% (Line~\ref{line:tgone-index}-\ref{line:tgone-update-A-2}).
Then, the smallest prefix size $i^*$ is selected in Line~\ref{line:tgone-index}.
For the solution where its corresponding call to \prefix returns $i^*$,
the entire prefix with size $i^*$ is added to it.
This ensures that a constant fraction of elements in $\mathcal V_j$
can be filtered out by Lemma~\ref{lemma:prefix-prob} in Appendix~\ref{apx:subroutine} with probability at least $1/2$.
Moreover, Lemma~\ref{lemma:dist} in Appendix~\ref{apx:subroutine}
guarantees that $|\mathcal V_j| \ge \frac{1}{\ell}|V_j|$
for each candidate set.
As a result, with constant probability,
at least one candidate set will filter out a constant fraction
of the elements.

\subsection{Ensuring Most Added Elements Significantly Contribute to the Solutions}
In \ts, the selection of a \textit{good prefix} inherently ensures this property immediately.
However, when interlacing $\ell$ threshold sampling processes,
prefix sizes selected in Line~\ref{line:tgone-prefix} by each solution may vary.
To preserve the alternating addition property introduced in Section~\ref{sec:ptg-alter},
subsets of equal size are selected instead of variable-length good prefixes.
This raises the question: \textit{How can a good subset be derived from a good prefix?}
The solution lies in Line~\ref{line:tgone-subset} of Alg.~\ref{alg:ptgone}.

For any $j \in I$, if $i_j^* = i^*$, $S_j$ is directly the good prefix $\mathcal V_j[1:i_j^*]$.
Otherwise, if $i_j^* \ge i^*$,
$i^*$ elements are selected from $\mathcal V_j[1:i_j^*]$ in three sequential passes until the size limit is reached:

\textbf{First pass}: Iterate through the prefix,
selecting those with marginal gains strictly greater than
$\tau_j$ (marked as \text{true} in $B_j$).

\textbf{Second pass}: 
From the remaining elements in the prefix, select those 
with marginal gains between $0$ and $\tau_j$
(marked as \text{none} in $B_j$).

\textbf{Third pass}: Fill any remaining slots with remaining elements from the prefix (marked as \text{false} in $B_j$).

This approach, combined with submodularity, 
ensures that any element marked as \textbf{true} in the selected subset has a marginal gain greater than $\tau_j$.
By prioritizing the addition of these \textbf{true} elements,
the selected subset remain high-quality while adhering to the alternating
addition framework.









