\section{Empirical Evaluation}\label{sec:exp}
\begin{figure*}[ht]
    \centering
    \subfigure[er, solution value]{\label{fig:er-val}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/er-val.pdf}}
    \subfigure[er, query]{\label{fig:er-query}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/er-query.pdf}}
    \subfigure[er, round]{\label{fig:er-round}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/er-round.pdf}}
    \subfigure[twitch-gamers, solution value]{\label{fig:twitch-val}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/twitch-val.pdf}}
    \subfigure[twitch-gamers, query]{\label{fig:twitch-query}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/twitch-query.pdf}}
    \subfigure[twitch-gamers, round]{\label{fig:twitch-round}
    \includegraphics[width=0.31\linewidth]{fig/epsi1/twitch-round.pdf}}
    \caption{Results for \maxcut on er with $n=99,997$,
    and \revmax on twitch-gamers with $n=168,114$.}
    \label{fig:er}
\end{figure*}
To evaluate the effectiveness of our algorithms,
we conducted experiments on $2$ applications of \nmon,
comparing its performance to $4$ baseline algorithms.
We measured the objective value (normalized by ATG~\citep{Chen2024}) achieved by each algorithm,
the number of queries made, and the number of adaptive rounds required.
The results showed that our algorithm achieved competitive objective value,
number of queries and adaptive rounds 
compared to nearly linear time algorithms.

\textbf{Applications and Datasets.}
The algorithms were evaluated on $2$ applications: 
Maximum Cut (\maxcut) and Revenue Maximization (\revmax),
with er ($n=99,997$), a synthetic random graph,
web-Google ($n=875,713$),
musae-github ($n=37,700$) and twitch-gamers ($n=168,114$) datasets, the rest of which are
real-world social network datasets from Stanford Large Network Dataset Collection~\citep{snapnets}.
% and Image Summarization (\imgsum) with CIFAR-10~\citep{krizhevsky2009learning} dataset. 
See Appendix~\ref{apx:app} and~\ref{apx:data} for more details.
We provide the result of er in the main paper,
while the other results can be found in Appendix~\ref{apx:nmon}.

\textbf{Baselines and their Setups.}
We compare our algorithms with \frg~\citep{DBLP:journals/mor/BuchbinderFS17}, 
\anm~\citep{fahrbach2018non},
\textsc{AST}~\citep{Chen2024}, and \textsc{ATG}~\citep{Chen2024}.
For those algorithms who required an \unc algorithm,
a random subset was employed instead.
For all algorithms, the accuracy parameter $\epsi$ and
the failure probability parameter $\delta$ were both set to $0.1$.
Whenever smaller $\epsi$ and $\delta$ values were specified by the algorithm, 
we substituted them with the input values.
\frg samples each element in the ground set $\uni$ with
a probability $p = 8k^{-1}\epsi^{-2}\log(2\epsi^{-1})$.
If $p > 1$, \randomgreedy in \citet{Buchbinder2014a} was implemented instead.
As for \textsc{Threshold-Sampling} in \anm, $100$ samples were used 
to estimate an indicator.
In the implementation of \ptgtwoshort, $\ell$ was set to $5$.
All randomized algorithms were repeated with $5$ runs,
and the mean is reported. 
The standard deviation is represented by a shaded region in the plots.

\textbf{Overview of Results.}
On the er dataset (Fig.~\ref{fig:er-val}), \ptgoneshort and \ptgtwoshort achieve the highest objective values, 
followed by \textsc{ATG} and \frg.
For the twitch-gamers dataset (Fig.~\ref{fig:twitch-val}), 
\ptgoneshort and \ptgtwoshort 
demonstrate greater robustness, particularly for larger values of $k$.

In terms of query complexity, our algorithms exhibit the highest query complexity ($\oh{n\log(n) \log(k)}$) compared to the other algorithms ($\oh{n\log(k)}$).
Nevertheless, \ptgoneshort outperforms all algorithms on twitch-gamers (Fig.~\ref{fig:twitch-query}),
and all but \textsc{ATG} on er (Fig.~\ref{fig:er-query}).
\ptgtwoshort is slightly better than \anm on er,
and both are more efficient than \frg on the two datasets.
\frg's high query count arises from its requirement for a large number of samples (specifically, $8nk^{-1}\epsi^{-1}\log(2\epsi^{-1})$) at every iteration.
When $8k^{-1}\epsi^{-1}\log(2\epsi^{-1}) \ge 1$, it defaults to executing \rg instead,
which incurs $\oh{nk}$ queries.

Regarding adaptive rounds (Fig.~\ref{fig:er-round} and~\ref{fig:twitch-query}),
the results align with theoretical guarantees.
\anm and \textsc{AST} operate with
$\oh{\log(n)}$ adaptivity
and achieve the best performance.
They are followed by \textsc{ATG}, \ptgoneshort, and \ptgtwoshort
which all achieve $\oh{\log(n)\log(k)}$ adaptivity.