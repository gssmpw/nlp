\section{Limitations of Existing Inference Attacks on RAG Systems}
\label{sec:existing_rag_inference}

\input{tables/detection}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/RAG-MIA.pdf}
    \caption{Overview of the problem setting and our Interrogation attack. Given black-box access to a RAG system $\mathcal{S}$, the adversary wants to infer membership of a given target document in the RAG's private database. Our method uses auxiliary LLMs to generate benign queries in the form of natural questions, and uses the correctness of the generated responses as a signal for membership inference test.}
    \label{fig:system_diagram}
\end{figure*}

A well-established issue in deploying LLM-based systems is \textit{jailbreaking}, where adversarial prompts are used to bypass a model's guardrails and induce it to perform unintended actions. To counteract such vulnerabilities, many LLM deployments incorporate countermeasures like detection tools to selective reject such queries.

Several prior works on membership inference and data extraction for RAG systems rely on prompting the model to either regurgitate its context directly or answer questions indirectly tied to the content. For instance, \citet{zeng2024good} explore targeted and untargeted information extraction by designing queries that trigger the retrieval of specific documents, paired with a command suffix intended to induce the generative model to repeat its context and, consequently, the retrieved documents. Similarly, \citet{anderson2024my} propose directly querying the RAG system to determine whether a target document is included in the model's context. On the other hand, some related works \citep{qi2024follow, cohen2024unleashing} employ adversarial prompts to coax the generator into regurgitating data from its context.

However, these adversarial (or even unnatural) queries heavily rely on \emph{prompt injection} techniques. Prompt injection \citep{perez2022ignore} is a broader concept that refers to an LLM vulnerability where attackers craft inputs to manipulate the LLM into executing their instructions unknowingly. In the specific case of these prompt injection attacks, known as \emph{context probing} attacks, the adversary attempts to extract information from the hidden context provided to the LLM. Therefore, it is crucial to analyze the effectiveness of existing inference attacks that rely on prompt injection to determine how successful their queries are in bypassing current detection filtersâ€”an area currently underexplored in the literature.

To evaluate the ability of current attacks to bypass detection methods, we adopt two different approaches. First we utilize LakeraGuard, a commercial off-the-shelf guard model designed for detecting prompt-injection and jailbreak attempts \citep{li2024injecguard}, to evaluate queries from different attacks. While this tool can detect queries from some existing attacks, it tends to fall short in identifying queries from attacks whose prompts appear more natural. These tools are designed to detect a wide range of prompt injection queries, so it is unsurprising that they may not perform perfectly in specialized settings like context probing attacks.
To develop a more tailored detection tool, we leverage the capabilities of GPT-4o as a classifier with few-shot prompting to classify input queries as either "natural" or "context probing."\footnote{Instruction template for this classification task is presented in the Appendix (\Cref{fig:classifier_prompt})} GPT-4o has recently shown great performance in prompt injection detection, further supporting its use for this task \citep{li2024injecguard}. Both approaches have shown good performance in prompt injection detection \citep{liu2024formalizing}.

\shortsection{Setup}
We consider attack prompts from three document extraction attacks and four MIAs, including ours. Apart from the MBA attack \citep{liu2024mask}, all prior inference attacks use a fixed, specific template for their attack queries. The templates for these queries are presented in \Cref{tab:prompt_guard_evals}. To evaluate baseline behavior of these detection methods on natural user queries, we include baselines on SQuAD and AI Medical Chatbot question-answer datasets. For more details, see \Cref{app:detection_setup}.

\shortsection{Easily Detectable Attacks}
Employing an off-the-shelf detection method can completely filter out the attack queries for two out of seven attacks, including the RAG document extraction attack \citep{cohen2024unleashing} and MBA \citep{liu2024mask} (\Cref{tab:prompt_guard_evals}), and can filter approximately 58\% of the attack queries for the prompt-injected data extraction attack \citep{qi2024follow}. When using GPT-4o as a detection technique specifically aligned with context probing attacks, the majority of attack queries are successfully filtered out. Importantly, neither of these two techniques significantly impacts natural queries from users, ensuring that legitimate queries remain mostly unaffected. %

These results highlight the necessity for attackers to craft stealthy queries that avoid explicit instructions aimed at recovering documents from the model's context. While adversarially crafted texts designed to bypass detection are feasible, an ideal attack strategy would involve generating clean-text queries that are immune to such defensive countermeasures. 
Thus for an inference attack to be successful in the context of a practical RAG system, it \textbf{must bypass any query-filtering systems that can detect obvious inference attempts}.

