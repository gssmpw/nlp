\section{Our Method: \ourattackfull}
\label{sec:method}


Given black-box access to a RAG system $\mathcal{S}$, the adversary can only interact with it by submitting queries and observing generated responses. Approaches that aggressively probe the system with suspicious or contrived queries deviate from typical usage patterns, thus making them easily detectable. 

\input{figs/main_example}

We aim to craft \emph{natural} queries---those resembling ordinary user inputs---yet \emph{highly specific} to a target document. The premise here is that such a document contains information that is uniquely specific, often the rationale for employing RAG in the first place. To leverage this specificity, we design questions likely to be answerable only in the document's presence. Increasing the number of queries would help cover multiple descriptive aspects of the document, enhancing coverage and specificity for membership inference. These queries should be natural, relevant, and easy to validate, ensuring effectiveness and plausibility. When aggregated, they yield reliable membership signals without arousing suspicion.

Our attack (\ourattack) has three main stages: generating queries (\Cref{sec:query-generation})
, generating ground-truth answers for these queries (\Cref{sec:ground_truth}), and finally aggregating model responses for membership inference (\Cref{sec:aggregation}).


\subsection{Query Generation}
\label{sec:query-generation}

We begin by creating a set of queries that are highly specific to the target document $d^*$. The overarching goal is to produce questions that are \emph{natural} in form—thus undetectable—and \emph{highly relevant} to $d^*$, making them effective probes for membership. Concretely, each query must simultaneously: (i) ensure retrieval of the target document $d^*$ (if present in the RAG) by incorporating keywords or contextual clues, and (ii) probe with questions that can only be accurately answered with the target document $d^*$ as relevant context. We achieve this by designing a \emph{two-part} query format consisting of a \textbf{Retrieval Summary} and a \textbf{Probe Question}, as described below.

\shortsection{Retrieval Summary}
We first craft a dedicated prompt, denoted $\mathbf{P}_{\mathrm{sum}}$, to guide an LLM in producing a short, natural-sounding description $s^*$. This summary, generated only \emph{once per target document}, includes key terms from $d^*$ and mimics realistic user queries (e.g., “I have a question about \ldots”). Including these keywords increases the likelihood of retrieving $d^*$, assuming it resides in the RAG system’s knowledge base.
The exact prompts used to generate $s^*$ are detailed in the Appendix (\Cref{fig:topic_description_task}).

\shortsection{Probe Question}
Next, we generate a set of questions that are highly aligned with the content of $d^*$. Drawing inspiration from doc2query tasks in the IR literature, we adopt a few-shot prompting strategy \citep{dai2023promptagator} that instructs an LLM to create natural, information-seeking queries based on $d^*$.  By default, these questions follow a yes/no structure, which simplifies validation and aggregation in later stages. This process yields a set of candidate \emph{Probe Questions}:
\[
\mathcal{P} = \{p_1, p_2, \ldots, p_n\}.
\]
The exact prompt used, along with further examples, is detailed in the Appendix (\Cref{fig:corpus_question_generation}).

\shortsection{Combining Summaries and Questions}
Finally, we concatenate each probe question $p_i$ with the single Retrieval Summary $s^*$ to form the final query set $Q = \{q_1, \ldots, q_n\}$, with
\begin{align}
    q_i = s^*\| p_i,
\end{align}
This two-part structure fulfills both retrieval and membership inference objectives simultaneously. An example of our generated queries is shown in Figure~\ref{fig:main_example}.






\subsection{Ground Truth Answer Generation}
\label{sec:ground_truth}

After obtaining our queries, $Q=\{q_1, \ldots, q_n\}$, we generate their corresponding ground truth answers using a \emph{shadow LLM}. Concretely, we provide the text of the target document $d^*$ as a reference, prompting this LLM to produce accurate answers for each query $q_i$. Since the questions are framed in a way that elicits binary responses, extracting answers from LLM outputs is straightforward.
Let $G=\{g_1, g_2, \ldots, g_n\}$ denote the resulting ground truth answers. These answers serve as baselines for evaluating the correctness of the RAG system’s responses and, ultimately, for deriving membership signals.

\subsection{Membership Inference}
\label{sec:aggregation}

We submit the queries $Q$ to the RAG system by issuing standard inference requests through its interface. Note that the RAG system may rewrite these queries, which the adversary has no control over. Let $R=\{r_1, r_2, \ldots, r_n\}$ represent the set of responses returned by the RAG system. If the target document $d^*$ is part of the knowledge base, a good retriever would fetch it for these highly specific and relevant queries, resulting in more accurate answers.

To infer membership, we compare the RAG system's responses $R=\{r_1, \ldots, r_n\}$ with the corresponding ground truth answers $G=\{g_1, \ldots, g_n\}$ derived from the shadow LLM. A final membership score is then calculated by aggregating the correctness of the responses. Specifically, as described in \Cref{sec:query-generation}, each query is a yes/no question, and correctness is assessed by comparing the RAG system’s response to the ground truth.

In our initial explorations, we notice that RAG systems often resort to responding with  "I don't know" or similarly vague expressions to some questions, especially under the absence of $d^*$. This is arguably a stronger signal for the lack of membership than simply giving incorrect answers, as the model is unlikely to contain the target document or any other relevant documents in its context when it is unable to answer a given query. Thus, while aggregating scores across model responses, we add $+1$ each correct response and subtract $\lambda$ every time the model is unable to respond and generate the final compute the membership score as
\begin{align}
S = \frac{1}{n} \sum_{i=1}^{n} \big(\mathbb{I}[r_i = g_i] - \lambda \mathbb{I}[r_i = \text{UNK}]\big),
\end{align}
where $\mathbb{I}[\cdot]$ is the indicator function that evaluates to $1$ if the equality condition holds and $0$ otherwise, and $\lambda$ is a hyper-parameter that penalizes the inability to answer a question.
A higher score $S$ indicates that the RAG system consistently retrieves correct information, suggesting that $d^*$ is included in the knowledge base.

