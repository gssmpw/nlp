\section{Experiments}
\label{sec:experiments}

We evaluate our attack across multiple retrievers, generators, and datasets (\Cref{sec:exp_setting}). As we observed before, none of the existing attacks would make it past a simple detection stage (\Cref{sec:existing_rag_inference}) in a practical RAG system. \textbf{Regardless, we find that even the absence of such guardrails, our attack outperforms existing baselines in most cases and is fairly robust across all these configurations} (\Cref{sec:main_results}).

\subsection{Evaluation Setup}
\label{sec:exp_setting}
\shortsection{Dataset} For our evaluations, we consider three distinct datasets representing scientific and medical documents. Specifically, we select NFCorpus, TREC-COVID, and SCIDOCS from the BEIR benchmark \citep{thakur2021beir}: collections of scientific and medical documents, containing approximately 3.5K, 116K, and 23K samples respectively.
For each dataset, after de-duplicating the samples, we randomly select 1000 members and 1000 non-members. Additionally, we use the TF-IDF technique to identify near-duplicate samples to the non-members (with a similarity threshold of 0.95) and remove them from the entire dataset. This ensures that the non-members do not overlap with or exist in the final dataset, maintaining the integrity of the evaluation, an issue observed in membership-inference evaluations for LLMs \citep{duan2024membership, maini2024llm, das2024blind, meeus2024sok}.

\shortsection{Generator and Retriever} We utilize two retrievers in our evaluations: GTE \citep{li2023towards} and BGE \citep{li2023towards}. For generators, we evaluate four different models: Llama 3.1 Instruct-8B \citep{dubey2024llama}, Command-R-7B\footnote{\url{https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024}}, Microsoft Phi-4 \citep{abdin2024phi}, and Gemma-2-2B \citep{team2024gemma}.

\shortsection{Shadow LLM} As described, the shadow LLM is employed to generate ground-truth answers for the questions created based on the target documents. In all experiments, we use GPT-4o-mini as the shadow model because it is fast and cost-efficient, and it belongs to a different family of LLMs compared to the RAG's generator. This ensures adherence to the black-box setting scenario, where the adversary has no knowledge of the RAG's generator.


\shortsection{Query Generation Setting} For \ourattack, we employ few-shot prompting with GPT-4o to generate 30 queries based on the target document. We also use GPT-4o to generate a short description of the target document, summarizing its main idea and keywords. For details of different prompting strategies and the corresponding prompts for each stage, see \Cref{app:query_generation_setting} and \Cref{app:prompts}.


\shortsection{RAG Setting} As described in \Cref{sec:RAG_description}, we evaluate our attack in a more realistic setting compared to previous works, where the RAG system employs query-rewriting on the user's query. We implement query-rewriting using a simple query-paraphrasing prompt via GPT-4o. We set $k=3$ for retrieval and investigate the impact of this hyperparameter across all attacks in \Cref{sec:ablation_k}. These retrieved documents are then provided as context to the generator via a system prompt. Details on both the query-paraphrasing and system prompts are presented in Appendix \ref{app:prompts}. To demonstrate the impact of query-rewriting on inference, we also evaluate attacks in a vanilla RAG setup where query-rewriting is disabled (\Cref{raw_rag}). 

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/roc_curves_appendix/commandr/roc_curve_command_r_gte_trec-covid.pdf}
        \label{fig:commanr_treccovid}
    \end{subfigure}
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/roc_curves_appendix/commandr/roc_curve_command_r_gte_scidocs.pdf}
        \label{fig:commandr_scidocs}
    \end{subfigure}
    \begin{subfigure}[t]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/roc_curves_appendix/commandr/roc_curve_command_r_gte_nfcorpus.pdf}
        \label{fig:commandr_nfcorpus}
    \end{subfigure}
    \caption{ROC curves for Command-R (7B) as generator, GTE as retriever, across various datasets. Our attack (\ourattack) achieves near-perfect inference across multiple datasets. ROC curves for other RAG configurations, can be found in  \Cref{app:roc_curves}.}
    \label{fig:rocs_commandr}
\end{figure*}
 
\shortsection{Baselines} We compare our attack with three prior black-box MIAs against RAG systems: RAG-MIA \citep{anderson2024my}, $S^2$MIA \citep{li2024generating}, and MBA \citep{liu2024mask}. RAG-MIA takes a simpler approach by directly probing the RAG system to ask if the target document appears in the context. $S^2$MIA uses the first half of the target document as a query and calculates the semantic similarity score (\ie BLEU) between the RAG system's responses and the original document as the membership score. They hypothesize that if a document is present in the database, the RAG system's responses will exhibit high semantic similarity to the original content. MBA uses a proxy-LM to selectively mask words in the target document, followed by querying the RAG to predict those masked words. The number of successfully predicted masked words is used as the membership score. In our experiments, we use Qwen-2.5-1.5B \citep{yang2024qwen2} as the proxy LM.  

In line with our black-box assumptions, we configure each attack so that the adversary has access only to the \emph{final generated answers}, without any logit-level data. Concretely, for RAG-MIA and $S^2$MIA, we focus on their black-box versions, which rely solely on the outputs rather than logits/perplexity. We describe the exact prompting strategies for RAG-MIA and  $S^2$MIA, along with an example of the format used for MBA, in \Cref{tab:prompt_guard_evals}.

\shortsection{Metrics} Following previous works, we evaluate our attack using the AUC-ROC score and True Positive Rates (TPRs) at low False Positive Rates (FPRs), which provide valuable insights into the success of our attack in inferring membership. Since RAG-MIA only produces a binary membership label for each target document, we report accuracy for that attack and compute accuracy for other attacks by using a threshold corresponding to FPR$=0.1$.


\subsection{Results}
\label{sec:main_results}

As shown in \Cref{tab:main_res_all}, our attack outperforms all baselines in both AUC and accuracy across various settings, including all datasets and RAG generator types. In particular, for the TREC-COVID dataset with Gemma2-2B as the generator, there is a noticeable performance gap in AUC between our attack and the baselines, demonstrating the the robustness of our method. In terms of TPR@low FPR, our attack generally achieves higher performance in most settings (\Cref{fig:rocs_commandr}). 
However, the MBA baseline shows better TPR in some cases, specifically when using LlamA 3.1 as the RAG generator\footnote{The drop primarily stems from the model's ability to answer questions correctly without any context. See \Cref{app:llama} for details.}. On the other hand, our attack is robust to changes in the generator.

\input{tables/main_res_all}

Lakera and GPT4-based detection methods are highly effective at spotting queries corresponding to MBA, with detection rates of 0.974 and 0.928, respectively, and high confidence levels (average confidence of 0.964). \textbf{This means attacks like MBA would typically fail to bypass these detection models in a RAG system}. For comparison, we hypothetically assume in our evaluations that MBA and other attacks could evade detection---though they do not---while our attack (\ourattack) successfully bypasses detection. Even if MBA evades detection, its performance is inconsistent across different LLM generators in the RAG system. In contrast, our attack maintains strong performance while slipping past detection filters.

Among the baselines, S$^{2}$MIA consistently performs the worst, highlighting its limitations in this evaluation. Additionally, the TREC-COVID dataset poses more challenges for our attack, with lower performance metrics (AUC, accuracy, and TPR@low FPR) compared to NFCorpus and SCIDOCS. This suggests that the dataset's complexity or the diversity of its queries and documents may introduce extra difficulties for inference attacks.

While \ourattack shows slightly lower TPRs, this trade-off is intentional, prioritizing undetectability. In contrast, MBA and similar attacks prioritize performance over stealth, making them more suitable for illustrative purposes than practical use.


\shortsection{Retrieval Recall}\label{sec:retrieval_recall}
In addition to directly measuring inference success, we consider retrieval recall as another metric. A good attack query is expected to retrieve the target document if it is a member. In \Cref{tab:retriever_recall}, we present the retrieval recall for all attacks across three datasets using both BGE and GTE as retrievers, before and after query rewriting. All attacks demonstrate high recall ($\geq 0.9$) in all settings, indicating their effectiveness in retrieving the target document. It is not surprising that some baselines achieve a perfect recall of 1.000, often outperforming our attack. This is because these baselines typically integrate the entire target document or significant portions of it directly into the query. In contrast, our queries are general yes/no questions derived from the target document, making them less explicit. 

As expected, retrieval recall after paraphrasing is generally similar to or slightly lower than without paraphrasing, but it remains high overall. It is important to note that the retrieval recall for our attack reflects the average proportion of queries that successfully retrieve the target document. For example, a retrieval recall of 0.930 in the paraphrased setting on the TREC-COVID dataset using GTE indicates that, on average, 93\% of the 30 questions for each target document successfully retrieve it. This is sufficient to distinguish members from non-members effectively.

\shortsection{Impact of Retriever}
Apart from GTE \citep{li2023towards}, we also experiment with BGE \citep{li2023towards} as a retriever. Table~\ref{tab:retriever_recall} compares the retrieval rates for both retrievers across various attacks, with or without query rewriting. Although GTE and BGE differ slightly in terms of recall, all attacks maintain consistently high retrieval rates overall. We also evaluate the end-to-end RAG after replacing GTE with BGE, under the same settings as \Cref{tab:main_res_all}, with Llama3.1 as the generator. We observe similar performance trends (\Cref{bge_results}) for this setup, confirming our primary conclusion: 
despite operating more stealthily, our attack achieves performance on par with (often surpassing) baselines.

Regarding query rewriting, \Cref{tab:retriever_recall} shows that each attackâ€™s recall rate--including \ourattack--does not significantly degrade after rewriting. However, MBA exhibit a noticeable performance drop under rewriting (see \cref{tab:main_res_all} and \cref{tab:llama8b_norewrite}), while \ourattack is minimally affected. This observation suggests that with query rewriting, performance decline for MBA is not driven by lower retrieval rates. Instead, even when the target document is successfully retrieved, MBA often relies on verbatim queries rather than knowledge-focused probing, rendering it more vulnerable to modifications in query phrasing.

\input{tables/retrieval_recall}
