\section{Introduction}
\label{sec:introduction}



Large Language Models (LLMs) have surged in popularity, yet they remain plagued by a critical challenge of hallucination \citep{ji2023survey}, generating plausible-sounding but factually incorrect information. \citet{lewis2020retrieval} proposed Retrieval Augmented Generation (RAG) as a plausible remedy to ground model outputs. RAG involves retrieving relevant text from a knowledge base for a given query using a retrieval model. These retrieved documents are then incorporated into the model's prompt as context, augmenting its knowledge. RAG offers a promising approach to grounding model outputs while enabling flexible, domain-specific knowledge customization without the need for expensive model retraining.
However, this advantage of parameter-free customization introduces a significant vulnerability: exposure to adversaries aiming to extract sensitive information from the underlying set of documents. Apart from adversaries that can inject their own documents via poisoning \citep{chaudhari2024phantom}, prompt-stealing adversaries \citep{hui2024pleak} may be able to infer the presence of retrieved documents present in the model's context via membership inference \citep{shokri2017membership}, or extract them directly via data-extraction \citep{carlini2021extracting}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/roc_curves_appendix/gemma2/roc_curve_gemma2_2b_gte_nfcorpus.pdf}
    \caption{ROC for Gemma-2 (2B) as generator, GTE as retriever, for NFCorpus dataset. Our attack (\ourattack) consistently achieves near-perfect membership inference.}
    \label{fig:roc_maintext_example}
\end{figure}

Membership inference attacks (MIAs) in machine learning attempt to discern if a given record was part of a given model's training data. MIAs thus have great utility for privacy auditing, copyright violations \citep{maini2024llm}, and test-set contamination \citep{oren2024proving}. While MIAs generally relate to the information contained in the model's parameters (with the model having seen some data during training), inferring the presence of particular documents in a RAG's data-store is different as the knowledge is not directly contained in model parameters. Inferring the membership of such documents can directly damage one of the primary benefits of RAG for privacy. With private knowledge being non-parametric, existing membership-inference attacks that rely on the memorization of training data may not work.

Although several studies have demonstrated membership inference on RAG-based systems, these methods generally rely on unnatural queries (\eg high-perplexity documents generated during optimization \citep{ebrahimi2018hotflip,qin2022cold}) or exploit "jailbreaking" \citep{shin-etal-2020-autoprompt, wei2024jailbroken} to coerce the generative models into undesired behaviors. 
Such attacks can be detected using off-the-shelf detection tools such as Lakera~\footnote{\url{https://platform.lakera.ai}}, allowing RAG systems to thwart these attacks or even simply refuse to respond. To the best of our knowledge, \textbf{there are currently no privacy-leakage attacks on RAG systems that cannot be easily thwarted through straightforward detection mechanisms}. 
A desirable MIA for a RAG system should thus be undetectable while retaining its effectiveness.

Towards this, we systematically evaluate existing MIAs \citep{liu2024mask, anderson2024my, li2024generating} across various detection mechanisms and show that prior attacks completely break down against these detection strategies (\Cref{sec:existing_rag_inference}).
We then introduce \ourattackfull (\ourattack), a MIA which is:
\begin{itemize}
    \item \textbf{Effective}: Achieves high precision and recall.
    \item \textbf{Black-box}: Does not rely on  access or knowledge of the underlying retriever/generator models.
    \item \textbf{Stealthy:} Comprises only of natural-text queries that are not flagged by detection systems.
    \item \textbf{Efficient}: Requires as few as 30 queries to the RAG system.
\end{itemize}
\ourattack leverages the intuition that \emph{natural} queries, when crafted to be highly specific to a target document, can serve as stealthy membership probes for RAG systems (\Cref{sec:method}).

Inspired by the doc2query task in Information Retrieval (IR) literature \cite{nogueira2019document, gospodinov2023doc2query}, we employ established few-shot prompting \cite{dai2023promptagator} techniques to guide a LLM in creating queries that are both topically aligned with and uniquely answerable by the target document.
These queries capture fine-grained and nuanced information specific to the target document, enabling us to subtly exploit the behavior of the RAG system in an undetectable manner.
We then issue these queries to the RAG system. Since they are highly relevant to the target document ($d^*$), a well-performing RAG will retrieve and incorporate $d^*$ (if available) to generate accurate answers. We can thus verify the correctness of these answers to probe membership. Aggregating signals from multiple queries enables strong membership inference. Crucially, each query remains benign, avoiding direct requests for verbatim content or displaying suspicious “jailbreaking” patterns, ensuring the attack remains undetectable by any detection systems.

We conduct extensive experiments across multiple datasets and RAG configurations by varying retrieval and generation models (\Cref{sec:experiments}).
While existing attacks are either detected easily or lack potency, we achieve successful inference while remaining virtually indistinguishable from natural queries, with detection rates as low as 5\%, compared to upwards of 90\% for most inference attacks against RAG.
Finally, we analyze our attack's failure cases (\Cref{sec:discussion}) and find that RAG may often be unnecessary: in many instances, the underlying LLM can answer questions about a given document without direct access to it, thereby questioning the necessity of a RAG-based system for such scenarios.
