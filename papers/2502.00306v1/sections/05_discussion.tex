\section{Discussion}
\label{sec:discussion}
In this section, we begin by outlining the assumptions regarding the nature of the RAG documents in our setup (\cref{sec:assumptions_on_docs}). Following that, we analyze the failure cases observed during our attack in \cref{failed_cases_potential_reasons}. We then examine the financial costs involved in launching the attack (\cref{sec:financial_cost}), and finally, in \cref{sec:countermeasures}, we explore potential countermeasures against the attack, along with their limitations.

\subsection{Assumptions on RAG Documents}
\label{sec:assumptions_on_docs}

While we observe impressive inference performance with our attack, even under the presence of detection schemes, we now discuss the list of assumptions made related to the nature of documents in a RAG setup.

\shortsection{Length Dependency} The documents targeted by our attack must be sufficiently long to provide enough information for generating meaningful questions. Applications involving short documents (\eg 2-3 lines) may lack the necessary content to generate 30 distinct and effective questions. This limitation is less critical in domain-specific RAG applications, where documents are typically longer and rich enough in content to justify the use of a RAG system.

\shortsection{Generic Documents} In addition to length, the targeted documents must be sufficiently informative. The attack may perform poorly on highly generic documents, as they do not contain enough specific details to craft unique and distinguishable questions. However, it is worth noting that in such cases, the utility of RAG might be limited, as generic documents provide less value for retrieval-based systems and the RAG owner might benefit in efficiency from discarding such documents from their datastore.


\subsection{Analyzing Failure Cases} \label{failed_cases_potential_reasons}

Although our attack achieves a higher AUC in all settings compared to the baselines, its TPR@low FPR  leaves room for improvement in some cases. Examining the failed examples can shed light on why this happens. We begin by visualizing the distribution of MIA scores for member and non-members documents with our attack.  In \Cref{fig:distribution}, we observe the distribution of the member and  non-member scores to be mostly separable but do note some overlap between them. This overlap between distributions can be attributed to two reasons: (1) members with low MIA scores, and (2) non-members with high MIA scores. 

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/histogram_nfcorpus_percentage.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/histogram_scidocs_percentage.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/histogram_trec_covid_percentage.pdf}
    \end{subfigure}
    
    \caption{Distribution of MIA scores for member and non-member documents when the RAG's generator is LLaMA 3.1. While the distributions are largely separable, there is some overlap between member and non-member documents.} 
    \label{fig:distribution}
\end{figure*}

\shortsection{False Negatives}
The fact that we observe high retrieval recall for our attack rules out the possibility of the target document being absent from the context provided to the RAG generator. The RAG's inability to answer the question properly can thus have two potential reasons. On rare occasions, GPT-4o fails to paraphrase the user's query accurately (see Appendix \ref{app:failed_examples} for an example), which reflects a shortcoming in the RAG system---not being able to paraphrase a normal, benign query.
For other cases, the RAG generator may struggle to answer the question even when the appropriate document is present in the provided context. Similarly, this can be attributed to the RAG's generator lacking capabilities---especially given the fact that the question, by design, can be answered by GPT-4o-mini under the presence of the target document.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nonmember_analysis/mia_score_vs_ngram_overlap_gemma2.pdf}
        \label{fig:plot1}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/nonmember_analysis/mia_score_vs_embedding_similarity_gemma2.pdf}
        \label{fig:plot2}
    \end{subfigure}
    \caption{Distribution of MIA scores for non-member documents for TREC-COVID, plotted alongside some similarity metric computed between each non-member document and a similar but non-identical document retrieved by the RAG. Above certain thresholds of which capture meaningful similarity, we observe a positive correlation between MIA score and similarity. Gemma2-2B is the RAG generator; Visualizations with LLaMA 3.1 as the generator can be found in \Cref{fig:nonmember_score_similarity_llama}.
}
    \label{fig:nonmember_score_similarity}
\end{figure*}

\shortsection{False Positives}
The RAG answering our queries correctly implies that the target document (corresponding to the query) is not required specifically as context to respond correctly. This can happen if similar documents with the necessary context are fetched by the retriever, or if the generator already possess sufficient knowledge to answer the question without relying on any context (see Appendix \ref{app:failed_examples} for examples).
To better understand this failure case, we compute the similarity between a non-member document $d$ and the document actually retrieved as context for a query corresponding to that non-member $d$, across multiple non-member documents and their corresponding queries generated for our attack. In \cref{fig:nonmember_score_similarity}, we look at $n$-gram overlap and cosine similarity between retriever embeddings, and visualize them with respect to MIA scores for our attack. We observe that above some certain meaningful threshold ($0.2$ for 4-gram overlap, $0.9$ for embedding cosine similarity), there is a positive correlation between how "similar" the non-member documents are to documents already present in the RAG, and the MIA Score (and by extension, questions answered correctly by the RAG). In summary, the failure cases are primarily due to limitations of the RAG system itself, such as occasional paraphrasing failures and the generator's inability to answer questions effectively, rather than drawbacks of our attack.


\subsection{Financial Cost Analysis}
\label{sec:financial_cost}
Since our attack requires the adversary to deploy paid APIs to access models, such as GPT-4o, it is essential to analyze the financial cost of this process. These models are utilized in three stages: generating yes/no questions, creating a general description of the target document, and obtaining ground-truth answers. Below, we provide an estimate of the cost for each stage. OpenAI pricing\footnote{All costs are according to the pricing information on OpenAI's website as of 01/2025.} accounts for both input and output tokens, so both are considered in our calculations. For all calculations, we calculate the compute the cost to be able to cover $99\%$ of all samples. For all estimations, we use the NFCorpus dataset, which contains the longest texts, as the worst-case scenario. 

\shortersection{Yes/No Question Generation} For this stage, we use GPT-4o to generate yes/no questions. Based on our analysis, the input to GPT-4o for this task is $902 \pm 108$ tokens on average, and the output is $513 \pm 64$ tokens on average. Based on these numbers, the cost for this stage is \$0.01 per document.

\shortersection{Description Generation} Similarly, for generating the description of each document, we use GPT-4o. Based on our analysis, the average number of input tokens is $648 \pm 108$, and the average number of output tokens is $21 \pm 5$. The cost for generating ground-truth answers is \$0.003 per document.

\shortersection{Ground-Truth Answer Generation} To generate the ground-truth answers, we use GPT-4o-mini. The average number of input tokens for this task is $13,244 \pm 3,317$, and the average number of output tokens is $48 \pm 5$. The cost for generating ground-truth answers is \$0.004 per document.

Based on these estimates, \textbf{the total cost for processing each document is \$0.017}. 




\subsection{Potential Countermeasures}
\label{sec:countermeasures}
Our attack relies on natural queries and the capability of RAG systems to answer user questions accurately based on private database knowledge. This makes devising countermeasures without negatively impacting performance challenging. \Cref{fig:distribution} provides valuable insights for considering defensive strategies against our attack. The core reason our attack is effective lies in the distinguishable distributions of MIA scores for members and non-members. Any effective countermeasure must focus on making these two distributions less distinguishable, either by moving members' scores closer to non-members' or vice versa.

\textbf{Moving members towards non-members} implies that the RAG system would deliberately answer questions related to documents in the database incorrectly. However, this approach would degrade the overall performance and utility of the RAG system, undermining its primary purpose.

\textbf{Moving non-members towards members} would require the RAG system to answer questions accurately even when the related document is not in the database. While this could be a promising  defense against membership inference, but then it also undermines the necessity of the RAG system if the generator is consistently able to answer questions without relying on the retrieved context. We already observe something similar with Llama, where the generator can answer several queries successfully without any provided context, but refuses to answer under the presence of irrelevant queries (\Cref{app:llama}).

Both approaches present significant trade-offs, highlighting the difficulty of defending against our attack without compromising either the system's performance or its utility.
