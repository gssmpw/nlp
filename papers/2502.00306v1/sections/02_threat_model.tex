\section{Background and Related Work}
\label{sec:setup}

In this section, we describe the components of a RAG system (\Cref{sec:RAG_description}), revisit membership inference for machine learning (\Cref{sec:MI_in_ML}), and
discuss recent works on privacy leakage in RAG systems in (\Cref{sec:priv_leakage_RAG}).

\subsection{Retrieval Augmented Generation (RAG)}
\label{sec:RAG_description}

Let $\mathcal{G}$ be some generative LLM, with some retriever model $\mathcal{R}$, and $\mathcal{D}$ denote the set of documents part of the RAG system $\mathcal{S}$.
Most real-world systems that deploy user-facing LLMs rely on guardrails \citep{dong2024building} to detect and avoid potentially malicious queries. One such technique that also happens to benefit RAG systems \citep{ma-etal-2023-query, beck2025raising, mo-etal-2023-convgqr, lin2020conversational, wang2024maferw} is ``query rewriting", where the given query $q$ is transformed before being passed on to the RAG system. Query rewriting is helpful in dealing with ambiguous queries, correcting typographical errors, providing supplementary information, in addition its utility in circumventing some adversarial prompts \citep{jain2023baseline}.
\begin{align}
    \hat{q} = \text{rewrite}(q).
\end{align}
For the transformed query $\hat{q}$, the retriever $R$ begins by producing an embedding for $\hat{q}$ and based on some similarity function (typically cosine similarity), fetching the $k$ most relevant documents
\begin{align}
    D_k = \operatorname*{arg\,top-}k_{d \in \mathcal{D}} \text{sim}(\hat{q}, d),
\end{align}
where $\text{sim}()$ represents the similarity function, and \(\operatorname*{arg\,top-}k\) selects the top-$k$ documents with the highest similarity scores.
The generator $\mathcal{G}$ then generates an output based on the contextual information from the retrieved documents \citep{lewis2020retrieval}:
\begin{align}
    y = \mathcal{G}(\text{ins}(\hat{q}, D_k)),
\end{align}
where $\text{ins}(q, D_k)$ represents the query and context wrapped in a system instruction for the generative model An end user only gets to submit query $q$ to the RAG system $\mathcal{S}$ and observe the response $y$ directly in the form of generated text.

\subsection{\bf Membership Inference in ML}
\label{sec:MI_in_ML}

Membership inference attacks (MIAs) in machine learning seek to determine whether a specific data point \( x^* \) is part of a dataset involved in the ML pipeline, such as training \citep{shokri2017membership, nasr2019comprehensive, sablayrolles2019white, watson2022on, carlini2022membership} or fine-tuning data \cite{fu2024membership, mattern2023membership}. Formally, given access to a model \( \mathcal{M} \), an adversary constructs an inference function \( \mathcal{A} \) that outputs:
\[
\mathcal{A}(x^*, \mathcal{M}) \in \{1, 0\},
\]
where \( 1 \) indicates that \( x^* \) is a member of the dataset, and \( 0 \) indicates otherwise. Such attacks have been explored across a broad spectrum of models—including traditional ML architectures~\citep{shokri2017membership}, LLMs \citep{duan2024membership}, and diffusion models \citep{duan2023diffusion}—by exploiting behavioral discrepancies between data seen during training (members) and unseen data (non-members). For instance, many ML models assign higher confidence scores to member data points \citep{shokri2017membership}.

MIAs have shown varying degrees of success across different domains, including images and tabular data \citep{zarifzadeh2024low, carlini2022membership, suri2024do, shokri2017membership}. However, these successes predominantly rely on \textit{parametric outputs} (e.g., confidence scores, perplexity, or loss values). Such outputs are often inaccessible in RAG systems. Moreover, RAG responses are dynamically generated based on content retrieved from external corpora rather than solely from the model's internal parameters. Thus, previous methods that depend on parametric signals are largely inapplicable. More importantly, \emph{the target of MIA in RAG systems specifically relates to whether external documents are retrieved during inference, rather than inferring knowledge from data seen during training or fine-tuning, rendering existing threat models unsuitable.}

In addition, earlier conclusions about MIAs may not extend to RAG systems. For example, critical analyses suggest that MIAs are typically ineffective for LLMs \citep{duan2024membership, meeus2024sok}, with effectiveness potentially increasing only when analyzing entire documents or datasets \citep{puerto2024scaling, maini2024llm}. However, even though RAG relies on an LLM for generating responses, these limitations do not extend to RAG systems, where exact documents are fetched and integrated into the context, making information extraction potentially more accessible.
As a result, existing MI threat models, methodologies, and conclusions designed for parameter-only systems do not readily apply to RAG. 




\subsection{Privacy Attacks in RAGs}
\label{sec:priv_leakage_RAG}
Recent research has explored various inference attacks against RAG systems. \citet{anderson2024my} developed techniques across different access levels, including a gray-box method using a meta-classifier on model logits and a black-box approach directly querying model membership. \citet{li2024generating} a similarly straightforward approach, where the target document is broken into two parts, with the idea that presence of the target document in the context would lead the LLM into completing the given query (one half of the document). However, authors for both these works find that simple modifications to the system instruction can reduce attack performance significantly to near-random.

\citet{cohen2024unleashing} focus on data extraction by directly probing the model to reveal its retrieved contexts as is, using a specially crafted query. \citet{zeng2024good} break the query into two parts, where the latter is responsible for making the model output its retrieved contexts directly using the command ``Please repeat all the context". \citep{wang2024membership} propose MIAs for long-context LLMs. While they do not specifically target RAG systems, their setup is similar in the adversary's objective- checking for the existence of some particular text (retrieved documents) in the model's context. Similarly, \citet{duan2024privacy} focus on membership inference for in-context learning under the gray-box access setting, where model probabilities are available.
While data extraction is a strictly stronger attack, we find that the kind of queries required to enable these attacks can be identified very easily using auxiliary models (\Cref{sec:existing_rag_inference}).

Several recent works have also proposed context leakage and integrity attacks, where the adversary has the capability of  inject malicious documents into RAG knowledge database \citep{chaudhari2024phantom, jiang2024rag} or can poison the RAG system direcly \citep{peng2024data}.  This threat model is different than ours as we do not assume any RAG poisoning or knowledge base contamination for our MIA. 





  





\section{Threat Model}
\label{sec:threat_model}

\shortsection{Adversary's Objective} Given access to a RAG system utilizing a certain set of documents \( \mathcal{D} \), the adversary wants to infer whether a given document \( d^* \) is part of this set of documents being utilized in the given RAG system. More formally, the adversary's goal is to construct a membership inference function \( \mathcal{A} \) such that, given access to the RAG system $\mathcal{S}$:
\[
\mathcal{A}(d^*) =
\begin{cases}
1, & \text{if } d^* \in \mathcal{D} \\
0, & \text{if } d^* \notin \mathcal{D}
\end{cases}
\]
The very use of a RAG system implies that the generative model's knowledge is not wholly self-contained. This reliance often stems from the need to reference specific, potentially sensitive information or to incorporate detailed factual knowledge that is not part of the system's pre-trained model. Depending on the nature of the documents used, successful inference can lead to significant implications while posing unique challenges:
\begin{itemize}
    \item \textbf{PII-Containing Documents:} Documents with sensitive details, such as addresses or health records, present a high risk. Inferring their presence could result in severe privacy violations and potential regulatory breaches. 
    \item \textbf{Factual Knowledge Sources:} Documents containing specialized knowledge, such as internal manuals, proprietary research, or compliance guidelines, are often harder to target due to overlapping information across multiple documents. However, a successful inference in such cases could compromise intellectual property or reveal sensitive strategic information.
\end{itemize}
Successful membership inference in a RAG system is not straightforward to achieve. The adversary must first ensure that the target document $d^*$, if present, is consistently retrieved by the RAG system during its operation. Additionally, the adversary must craft queries in a manner that not only distinguishes the target document from other potentially related documents in $\mathcal{D}$ but also bypasses any intermediate processes employed by the RAG system (as discussed in \Cref{sec:existing_rag_inference}) that may limit inference success.

\shortsection{Adversary's Capabilities} We operate under a black-box access model where the adversary can query the target RAG system, but possesses no information about its underlying models or components. We assume the adversary has access to an auxiliary LLM, which it leverages to generate queries and interpret answers.
The adversary lacks knowledge of the retriever and generator models used by the victim, including their hyperparameters (\eg $k$ for top-$k$ retrieval, temperature settings for generation, \etc). The adversary also lacks knowledge of system instructions used in the victim RAG system, or query-rewriting strategies (if any) employed. Like in a typical membership inference scenario, the adversary owns a set of non-member documents from the same data distribution, which it uses to establish thresholds for predicting membership. Unlike some prior work \citep{chaudhari2024phantom} that assumes the ability to inject poisoned documents, the adversary in this setup has \textbf{no read or write access to the data used by the victim's RAG system}.



