\section{Failed Cases Examples}
\label{app:failed_examples}

As described in \Cref{failed_cases_potential_reasons}, one potential reason a member receives a low MIA score is when GPT-4o fails to paraphrase the question accurately. While this is a rare occurrence, it can impact overall performance. In \Cref{fig:failed_example_1}, we provide an example of this type of failure.

For non-members misclassified as members due to high MIA scores, we identify two main potential reasons. The first occurs when, although the non-member document is not in the RAG database, there exists at least one similar document in the database that the LLM uses to answer the questions. An example of this case, taken from the SCIDOCS dataset, is shown in \Cref{fig:failed_example_2}. For all 30 questions, the same similar document is consistently retrieved from the database.

The second potential reason arises when the RAG generator has sufficient prior knowledge to answer most of the questions correctly without relying on retrieved documents. For instance, with an example from the NFCorpus dataset, LLaMA 3.1 (used as the RAG generator) can answer 23 out of 30 questions accurately without accessing any retrieved documents. This demonstrates that, even though the document is not a member of the database, the LLM can answer most of the questions correctly based on its inherent knowledge.
