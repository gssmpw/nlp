\section{Query Generation Setting}
\label{app:query_generation_setting}

\input{tables/query_gen}

As mentioned, we utilize GPT-4o to generate queries for each target document. There are several approaches to achieve this by prompting GPT-4o, and we consider three distinct strategies:

\begin{enumerate}
    \item \textbf{Instruction Only}: Provide a detailed instruction to GPT-4o to generate the queries.
    \item \textbf{Few-Shot Prompting}: In addition to the detailed instruction, include an example of a text along with multiple example queries based on the text.
    \item \textbf{Iterative Generation}: Use the same instruction and examples but execute the query generation in three stages. In each stage, we generate five queries, and in subsequent stages, we add the previously generated queries to the prompt and instruct the model to generate new, non-redundant queries. This ensures the final set of queries is diverse and avoids duplication.
\end{enumerate}
To compare these strategies, we consider three metrics. A good set of queries for each document should be diverse, achieve a high retrieval score (\ie the target document is successfully retrieved from the database), and lead to better attack performance. Thus, the metrics we use are: 

\begin{itemize}
    \item \textbf{Attack Success Rate (ASR)}: The effectiveness of the attack using the generated queries.
    \item \textbf{Retrieval Recall}: Described in \Cref{sec:retrieval_recall}, measuring whether the target document is retrieved.
    \item \textbf{Semantic Diversity}: Calculated as the average cosine distance, representing the diversity of the queries for each document based on their semantic embeddings.
\end{itemize}
We conducted a small experiment with 250 members and 250 non-members from the TREC-COVID dataset, with Llama 3.1 Instruct-8B as both the shadow model and generator to evaluate the ASR, with ColBERT as the retriever model. For semantic similarity, we used the all-MiniLM-L6-v2 \footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}} model to compute embeddings.

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_non_members_gemma.pdf}
        \caption{Gemma2-2B}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_non_members_llama.pdf}
        \caption{Llama3.1-8B}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_non_members_phi4.pdf}
        \caption{Phi4-14B}
    \end{subfigure}
    \caption{Distribution for MIA scores for non-member documents for TREC-COVID, using the RAG's generator directly without any context (LLM), and when using the RAG normally (RAG). We observe peculiar behavior for the Llama model, where the model's ability to answer questions deteriorates significantly in the presence of unrelated documents.}
    \label{fig:llm_vs_rag_nonmember}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_members_gemma.pdf}
        \caption{Gemma2-2B}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_members_llama.pdf}
        \caption{Llama3.1-8B}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/llm_vs_rag/mia_score_histogram_members_phi4.pdf}
        \caption{Phi4-14B}
    \end{subfigure}
    \caption{Distribution for MIA scores for member documents for TREC-COVID, using the RAG's generator directly without any context (LLM), and when using the RAG normally (RAG). The Llama model can answer most questions correctly even when the relevant document is absent from context, suggesting that it has seen similar documents in its training.}
    \label{fig:llm_vs_rag_member}
\end{figure*}

As shown in \Cref{tab:query_gen}, few-shot prompting achieves higher ASR and retrieval recall compared to the other two methods. The third generation strategy performed the worst across all three metrics. Consequently, we adopt the second method (few-shot prompting) for all experiments to prompt GPT-4o. 
