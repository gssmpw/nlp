\section{Understanding Llama Behavior}
\label{app:llama}

To better understand the performance drop in our attack for the Llama model, we examine the MIA score under two scenarios: using the RAG setup (RAG) and querying the underlying LLM without providing \textit{any} context (LLM). Ideally, the model's ability to answer questions related to a target document should improve when that document is available, as this justifies the use of retrieval-augmented generation.

For non-member documents, an interesting trend emerges (Figure~\ref{fig:llm_vs_rag_nonmember}). The Gemma2 and Phi4 models exhibit similar MIA scores regardless of context presence, as expected, since the provided documents are unrelated. However, the Llama model behaves peculiarly: not only does it successfully answer most questions generated as part of our attack (as indicated by most MIA scores being $>0$), but its performance \textit{drops} when unrelated documents are provided as context. This suggests that Llama possesses the necessary knowledge to answer these questions but is easily confused by irrelevant context.

A comparable pattern appears in the distribution of scores for member documents (Figure~\ref{fig:llm_vs_rag_member}). The Llama model can answer most questions without context, but when the relevant document is included via RAG, its accuracy improves. This implies that Llama has likely encountered the TREC-COVID dataset (or similar data) during training. However, without precise knowledge of its training corpus, we can only speculate. More importantly, our findings highlight that users of RAG systems should benchmark whether the underlying model truly benefits from additional context. While our attack is designed as a MIA, it can be adapted for analyses like ours to assess whether incorporating external documents meaningfully enhances model performance.
