\section{Details for Detection Setup}
\label{app:detection_setup}

\paragraph{Baselines} A robust detection method should also perform well against natural user queries. To evaluate this, we include two QA datasets: SQuAD and AI Medical Chatbot. These datasets allow us to assess how each detection method behaves when faced with standard, benign queries.

\paragraph{Datasets} We consider three datasets: three from the BEIR benchmark, including NFCorpus, TREC-COVID, and SCIDOCS, as well as the HealthCareMagic dataset. From each dataset, we select 125 samples and integrate them into the attack prompt templates, resulting in a total of 500 samples for each attack. For the RAG-MIA attack, which includes multiple templates, we distribute the selected samples evenly across the different templates.

\paragraph{Metrics} We evaluate the detection methods against these attacks using the detection rate, which measures the proportion of samples identified as "context probing" by the GPT-4o-based classifier or as "prompt injection" by the Lakera detection method. 
