%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Machine Learning and Neural NLG}
\label{neuralnlg} % Always give a unique label


Instead of building an NLG system using rules from domain experts, we can use machine learning (ML) techniques to create an NLG \emph{model}.  The model is trained on data (eg, inputs and outputs for NLG), and can apply the behaviour it has learnt from the data to generate new NLG outputs from novel NLG inputs.  At the time of writing. most machine-learning in NLG uses \emph{neural} models, that is models that are loosely inspired by how neurons in the human brain work.

There are many possibilities within the broad space of using ML and neural techniques in NLG.  Perhaps the most fundamental distinction is in how models are trained (Figure~\ref{fig:modelTypes}):
\begin{itemize}
\item \textbf{Trained models}\index{models!trained} are trained from scratch to perform the target NLG task; typically they need large amounts of task-specific training data.
\item \textbf{Fine-tuned models}\index{models!fine-tuned} take a \scare{pre-trained} large language model\index{large language models} (sometimes called a \emph{foundation model}\index{models!foundation}), which is usually trained on large amounts of Internet content, and adapt it for the target NLG task using a limited amount of task-specific training data.
\item \textbf{Prompted models}\index{models!prompted} directly use a pre-trained language model to perform a task, by giving the model a request (\emph{prompt}); task-specific examples can be included, but this is not necessary.
\end{itemize}

\begin{figure}
\lineacross\\
\includegraphics[scale  = 0.5]{mlmodels.pdf}
\caption{Trained, fine-tuned, and prompted models.}
\label{fig:modelTypes}       
\lineacross
\end{figure}

Other important distinctions include whether the model does the complete NLG process or only part of it (for example generating texts from insights, with the insights produced by a separate component); and whether the model is used autonomously or whether a person manually checks its outputs (\emph{human-in-the-loop}\index{human-in-loop}; this is discussed in Section~\ref{sec:humanchecking}).

Machine learning and neural technology for NLG has been changing very quickly over the past 10 years.  N-gram models (Section~\ref{sec:ngram}) dominated until 2016 or so, early neural models such as LSTM (Section~\ref{sec:earlyneural}) then became the focus of attention until 2019 or so, transformers (Section~\ref{sec:transformers}) then became the most popular approach until 2022 or so, and instruction-tuned models are pre-eminent at the time of writing.  I would not be at all surprised if a new approach became prominent in 2025.

This book presents neural and ML technology at a very high level, focusing on fundamental concepts and issues which I believe will still be important in 2030.  It deliberately do not attempt to give a detailed description of ML/neural technologies at the time of writing, since anything written along these lines in 2024 would probably be obsolete by 2025, and of only historical interest by 2030.

\section{Examples}

\subsection{Very simple trained model\index{models!trained}: \lingform{a} vs \lingform{an}}\label{sec:aan}

One of the simplest ML models in NLG chooses whether (in English) to use \lingform{a} or \lingform{an}.  This difficult to do using rules, but easy to do using machine learning; this model also illustrates some of the fundamental concepts of ML in NLG.

In English, we use \lingform{a} before words that start with a consonant sound, but \lingform{an} before words that start with a vowel sound.  Since this choice is based on how a word is pronounced, we cannot simply look at the first letter of the following word to make this choice; for example correct usage is \lingform{an umbrella} but \lingform{a university}.

Of course, there are many special cases, including the following:
\begin{itemize}
\item \emph{Acronyms:}\index{acronyms} Pronunciation of acronyms depends on whether it is spelled out letter-by-letter; for example \lingform{an SAT prep course} if we spell out the letters \lingform{S-A-T}, but \lingform{a SAT prep course} if we pronounce \lingform{SAT} like the word \lingform{sat}.
\item \emph{Currency:} When we pronounce a currency, we say the number first even if the unit comes first in the written form.  Thus we would write \lingform{an \$80 fine} because \lingform{\$80} is pronounced as \lingform{eighty dollars}, not \lingform{dollar eighty}, even though \lingform{\$} comes before \lingform{80} in the written form \lingform{\$80} .
\end{itemize}

Writing rules to choose between \lingform{a} or \lingform{an} is complex and difficult.  A better approach is to analyse a large collection (\emph{corpus})\index{corpus} of well written English texts, such as Wikipedia\index{Wikipedia} to see whether \lingform{a} or \lingform{an} are preferred in front of specific words.   For example, \lingform{an umbrella} occurs 9500 times in Wikipedia, while \lingform{a umbrella} only occurs 45 times; this tell us that \lingform{an umbrella} is the preferred usage.  We can repeat this process for all words in Wikipedia, and create a database which tells us when to use \lingform{an} and when to use \lingform{a}; this is  accurate and does not require any rules!

This is technically called a \emph{bigram} model\index{models!bigram}, which means it is based on the frequency and probability of words pairs such as \lingform{an umbrella},. Bigram models are a type of \emph{n-gram} model\index{models!ngram} (Section~\ref{sec:ngram}).  This is also a trained model in the sense of Figure~\ref{fig:modelTypes}.

We can also \emph{generalise}\index{generalisation} our findings.  We know that the first few letters of a word are the most important in choosing between \lingform{a} and \lingform{an}, so we can look for cases where all words that start with the same letters have the same  \lingform{a} vs \lingform{an} behaviour.  For example, if all words starting with \lingform{d} use \lingform{a}, then we can replace the D-section of our database by a simple rule that any word starting with \lingform{d} takes \lingform{a} (this analysis can be automated).  Not only does this reduce the size of our model, it also lets us handle new words which were not in the corpus.  For example, \lingform{dzild} is not in Wikipedia, but the generalisation process tells us to use \lingform{a dzild} instead of \lingform{an dzild}.

Of course, the above process only works if the corpus or \lingform{training data}\index{training data} is well-written English.  Wikipedia satisfies this criteria, but Twitter does not.  When machine learning models fail, it is often because of quality issues in the training data, and  commercial ML models builders often spend more time on data issues (Section~\ref{sec:trainingdata}) than on algorithms.

Lastly, language changes\index{language change}, it is not static.  For example. Figure~\ref{fig:ahistoric} show the usage of \lingform{a historic} vs \lingform{an historic} over time, in British English. This shows that while \lingform{a historic} is more common now, \lingform{an historic} was more common before 1985.  This means that models should be trained on recent data; they may make mistakes (such as predict \lingform{an historic}) if it is trained on old data.  More generally, we need to keep in mind that models trained on a corpus become less useful as the corpus ages; this is related to \lingform{domain shift} in machine learning (Section~\ref{sec:domainshift}).


\begin{figure}
\lineacross\\
\includegraphics[scale  = 1]{aan}
\caption{Usage of \lingform{a historic} vs \lingform{an historic} in British English, over time.  From Google Books NGram\index{Google Books} Viewer, \url{https://books.google.com/ngrams}}
\label{fig:ahistoric}       
\lineacross
\end{figure}

\subsection{Fine-tuned neural model: Facebook weather dialogues}\label{sec:Facebookweather}
An example of a more complex ML model is the NLG system developed by Facebook (Meta) to respond to weather inquiries\index{weather forecasts} \cite{arun-etal-2020-best}, which was deployed and operationally used.  This system relies on another component to select the content to be communicated in the text; it automates the task of generating a text from a meaning (content) representation.  It uses neural technology, and in particular relies on fine-tuning\index{fine-tuning} (Figure~\ref{fig:modelTypes})  a large language model (BART\index{BART} \cite{lewis-etal-2020-bart}) to the task of generating weather information.

Figure~\ref{fig:facebookweather} shows an example from this system.  The Query is entered by the user.  Other parts of the system do signal analysis\index{signal analysis}, data interpretation\index{data interpretation}, and some document planning\index{document planning}, and produce a \emph{meaning representation}\index{meaning representation} which specifies the content that should be communicated to the user.  The NLG system then generates an actual text (similar to the reference text) which communicates the meaning to the user.

The meaning representation specifies the information to be communicated, for example that the low temperature will be 20 (\semantic{temp\_low[20]}, and that it will rain on Sunday (\semantic{condition[ rain ] date\_time[weekday[ Sunday ]]}).  It also specifies connectives which relate insights (what I called \emph{linkage}\index{data interpretation!linkage} in Section~\ref{sec:dp-principles}), for example that the rain on Sunday contrasts with the sunshine on Saturday.

\begin{figure}
\lineacross\\
\emph{Query}: How is the weather over the next weekend?\\

\emph{Response content (meaning representation produced by other components)}:\\
\begin{verbatim}
INFORM 1[temp_low[20] temp_high[45] date_time[colloquial[ next weekend ]]]
CONTRAST 1[
              INFORM 2[condition[ sun ] date_time[weekday[ Saturday ]]]
              INFORM 3[condition[ rain ] date_time[weekday[ Sunday ]]]
               ]
\end{verbatim}

\emph{Reference (human-written)}: \lingform{Next weekend expect a low of 20 and a high of 45. It will be sunny on Saturday but it’ll rain on Sunday.}
\caption{Example input and reference (human-written) output for weather system, from \cite{arun-etal-2020-best}}
\label{fig:facebookweather}
\lineacross
\end{figure}

In order to fine-tine BART, the Facebook team needed high-quality examples of NLG inputs and outputs, that is of meaning representations and corresponding generated texts; this was the training data\index{training data}.  The Facebook team used to following process to create these examples:
\begin{itemize}
\item asked engineers to create a varied set of queries and scenarios.
\item automatically generated meaning representations for these \cite{balakrishnan-etal-2019-constrained}.
\item asked human annotators to manually produce high-quality responses from the meaning representation, following guidelines written by computational linguists.
\item had linguists\index{linguists} check and verify the responses.
\end{itemize}
This process was used to create 25,000 query-response pairs for training models, and a further 6,000 for testing and validation.

Note that the training data for this system was explicitly created for the project, it was not scraped off the internet.  No cost figures are given in the paper, but creating 31,000 query-response pairs using the above process must have been a major task.  The team subsequently developed techniques to reduce the amount of data needed \cite{heidari-etal-2021-getting}.

At any rate, once the team had this dataset, they then used it to fine-tune a BART model \cite{lewis-etal-2020-bart}.  In other words, rather than creating a new model from scratch, they took an existing large language model and tuned it to work on the target NLG task (Section~\ref{sec:transformers}).   The team also used knowledge distillation\index{knowledge distillation} techniques (essentially training a smaller model on the fine-tuned BART model) to speed up run-time processing and reduce run-time memory requirements, which is very important in a production context.

Finally, because of the dangers of the neural model making mistakes, the team built a tool which identified generated texts which were possibly incorrect.  They also implemented a backup rule-based NLG\index{rule-based NLG} system which was invoked if the checking tool found potential problems in a text.

A key lesson from this example is that building a production NLG system based on a fine-tuned neural model required creating high-quality data sets for training \cite{dusek-etal-2019-semantic} (Section~\ref{sec:trainingdata}), ensuring that models are acceptable from a time/memory/cost perspective, and implementing techniques to deal with model errors.  The actual model training and tuning process (which is what most students and researchers focus on) is important, but  its only part of the bigger picture if we are building real-world NLG systems.

\subsection{Prompted model\index{models!prompted}: Using ChatGPT\index{ChatGPT} to generate weather forecasts}\label{sec:exampleprompted}
If we want to build a system to generate weather forecasts using a trained or fine-tuned model, we need to collect a data set of thousands of high-quality input-output pairs. as was done for the Facebook system described above.  An alternative approach is to take a large language model such as ChatGPT, which is trained on a huge amount of Internet material, and give the model a prompt\index{prompt} (request) such as \lingform{Generate a short weather forecast for the below weather data}, followed by the numeric weather data (Figure~\ref{fig:chatgpt-example}).

\begin{figure}
\lineacross\\
\emph{Input to prompted model (ChatGPT):}\\
\begin{verbatim}
Generate a short weather forecast from the below data.
Weather data:
[
 {
   "time": 0,
   "wind": 10,
   "precipitation": 0,
   "temperature": 6
 },
 {
   "time": 3,
   "wind": 11,
   "precipitation": 0,
   "temperature": 6
 },
...
]
\end{verbatim}

\emph{Output of prompted model:}\\
\lingform{Here's a short weather forecast based on the provided data:}\\

\lingform{Expect a day with changing wind speeds and temperatures. The wind will range from 9 km/h to 12 km/h throughout the day, occasionally gusting higher. The temperature will start at 6°C and gradually increase to 15°C in the afternoon before dropping to 10°C in the evening.}\\

\lingform{There is no precipitation expected, so it will be a dry day.}\\

\lingform{In summary, anticipate a mix of wind speeds, with temperatures ranging from 6°C to 15°C, and no rainfall.}\\

\caption{Example of using a prompted model (ChatGPT) to generate a weather forecast.  The model is generic and has \emph{not} been trained or fine-tuned to produce weather forecasts}
\label{fig:chatgpt-example}
\lineacross
\end{figure}
If we want to control the wording and content of the weather forecast, we can explicitly request this in the prompt, or we can include a small number of  examples (5 or 10, not 25,000)  in the prompt (Section~\ref{sec:fewshot}).

Prompted language models have many advantages over trained or fine-tuned models, including:
\begin{itemize}
\item No need to create a large set of domain-specific input-output examples.
\item Since the model is general-purpose, its easy to add additional capabilities, such as generating texts in different languages.
\item From a developer perspective,  creating prompts does not require specialist expertise in machine learning, NLP, or programming (although these can help); fine-tuning or training neural models, in contrast, does require considerable expertise.
\end{itemize}
Many prompted models can be fine-tuned to improve their performance.

\section{Machine Learning Models for NLG}

Machine learning is a rapidly evolving field, below I summarise a few of the different machine learning models and technologies which have been used to build NLG systems.  This description is high level; Further Reading (Section~\ref{sec:mlreading}) suggests additional sources for technologies.

\subsection{Classifiers\index{classifiers}}\label{sec:classifiers}
A \emph{classifier} puts an input dataset into one of a finite number of \emph{classes}.  For example, a sentiment analysis classifier classifies an input text into categories such as Positive, Neutral, or Negative.  Classifiers are one of the oldest areas of machine learning, and indeed many classifier algorithms draw heavily on statistical research as well as AI.  The \texttt{scikit-learn}\index{scikit-learn} Python library\footnote{\url{https://scikit-learn.org/}} includes a number of algorithms for building classifiers, including decision trees, Naive Bayes, K nearest neighbour, support vector, and many others.

To take a simple example,the SumTime\index{SumTime} weather forecast\index{weather forecasts} generator \cite{REITER2005137} needed to decide which verb to use when describing changes in the weather; this is an example of lexical choice\index{lexical choice} (Sect~\ref{sec:lexchoice}; Section~\ref{sec:lexprinciples} discusses SumTime).   In particular, in statements describing the wind, there is a choice between three types of verbs
\begin{itemize}
\item Verb emphasising direction change, such as \lingform{W 10-14 veering N 12-16}
\item Verb emphasising speed change, such as \lingform{W 10-14 increasing WNW 18-22}
\item Conjoined verb emphasising both speed and direction change, such as \lingform{W 10-14 veering and increasing N 18-22}
\end{itemize}
SumTime used a classifier to decide on verb type (Figure~\ref{fig:classifierExample}).  Essentially, developers extracted all wind change phrases from a corpus\index{corpus} of human-written forecasts and corresponding numerical weather data sets, and used this to train a classifier which predicted the above choice from \semantic{<StartWindDirection, StartWindLowSpeed, EndWindDirection, EndWindLowSpeed>} tuples.  SumTime used a decision tree, but other classifier techniques could have worked just as well \cite{chen-yao-2019-closer}.

Classifiers cannot generate texts on their own (i.e., they do not do \emph{end-to-end} generation), but they are useful in making specific NLG choices, including content (insights) as well as language choices.

\begin{figure}
\lineacross\\
\includegraphics[scale  = 0.5]{classifier.pdf}
\caption{Classifier decides to use Speed Verb in a context where there is a large change in speed and a small change in direction.}
\label{fig:classifierExample}      
\lineacross 
\end{figure}


\subsection{N-gram language models\index{models!ngram}}\label{sec:ngram}
An n-gram language model is a model which gives information about the likelihood of a sequence of words.  The \lingform{a}-vs-\lingform{an} algorithm (Section~\ref{sec:aan}) is a simple example of this.  Essentially given a word XXX, the model decides whether to use \lingform{a XXX} or \lingform{an XXX} by counting how often the \emph{bigrams}\index{models!bigram}  \lingform{a XXX} and \lingform{an XXX} occur in a corpus (collection of texts) such as Wikipedia\index{Wikipedia}.   A bigram is a two-word sequence (2-gram); n-gram models support word sequences of other lengths including \emph{unigrams} (1 word) and \emph{trigrams} (3 words).  Usually n-grams are structured to produce probabilities instead of raw frequencies, as can be seen in Figure~\ref{fig:ahistoric}.

We can get good data on the frequency of bigrams and other small n-grams from corpora, and use these to estimate probabilities.  However, it is much harder to do this with large word sequences.  For example, suppose a speech-to-text system is trying to interpret an input sentence, and from an acoustic perspective is unsure whether the sentence was \lingform{I eat too much strawberry ice cream} or \lingform{I eat too much strawberry I scream} (since \lingform{ice cream} and \lingform{I scream} are very similar from an acoustic perspective).  In theory it could decide between these based on frequency in a corpus.   Unfortunately, the Internet (according to a Google search) contains \emph{no} examples of either \lingform{I eat too much strawberry ice cream} or \lingform{I eat too much strawberry I scream}, so raw frequencies tell us nothing.  However, we can use  techniques such as Markov models\index{models!Markov} and smoothing to estimate the probability of these word sequences from smaller n-grams such as \lingform{strawberry ice cream} and \lingform{strawberry I scream} \cite{jurafskyspeech}; this tells us that \lingform{I eat too much strawberry ice cream} is a much more likely sentence than \lingform{I eat too much strawberry I scream}.

From a software perspective, \emph{nltk.lm}\index{nltk} (part of the Natural Language Technologies toolkit\footnote{\url{https://www.nltk.org/}}) provides good support for creating and using n-gram models.

N-gram models were popular for many years, but at the time of writing have largely been replace by neural models.

\subsection{Early neural models}\label{sec:earlyneural}
In recent years there has been huge interest in using \emph{neural} (especially \emph{deep learning}\index{deep learning} \cite{Goodfellow-et-al-2016}) models to generate texts and indeed do other natural language processing tasks.  These models are loosely inspired by neurons in the human brain.  An enormous number of neural models and architectures have been discussed in the literature.  

Neural NLG is usually considered to be a \emph{sequence-to-sequence}\index{sequence-to-sequence} task, where an input sequence (input texts for summarisation, time series for data-to-text) is converted into an output sequence of words (ie, a text).  Early neural models for NLG \cite{goldberg2022neural} used \emph{recurrent} neural networks\index{recurrent neural networks} (RNNs), which are a type of neural network which iterates through arbitrary-length sequences of tokens (words).   \emph{Long short-term memory} (\emph{LSTM}\index{LSTM}) architectures modify the core RNN to give the network \scare{memory} which makes it easier for the network to consider already-output words when it decides on the next word to output.  \emph{Encoder-decoder}\index{encoder-decoder} architectures use two neural networks to generate texts, an \emph{encoder} which maps the system's input into an internal state, and a \emph{decoder} which translates the internal state into generated texts.

These models were usually trained from data; that is, initially the model's neural network are in a default state, and training algorithms such as \emph{back propagation}\index{back-propagation} are used to set the \emph{parameters}\index{parameters} (internal weights) of the neural network.  Of course there are many variants of the above, and indeed many different possible neural architectures (number of layers, number of nodes in layers, etc).

\subsection{Transformers\index{transformers} and foundation models\index{models!foundation}}\label{sec:transformers}\label{sec:foundationmodels}
In 2017, Vaswani et al \cite{Vaswani:transformer} introduced the \emph{Transformer} architecture.  In very crude terms, while earlier neural models sequentially generated an output token from an input token (while using a memory mechanism to keep key information from earlier tokens), transformers could process larger chunks of information (such as complete sentences).  They did this using an \scare{attention}\index{attention} mechanism which highlighted key connections between inputs to the model.  For example, when translating \lingform{I ate a grape} into (French) \lingform{j'ai mangé un raisin}, the attention mechanism tells the model that the word \lingform{grape} has an impact on how the word \lingform{a} is translated.  Since \lingform{raisin} is a masculine noun in French, \lingform{a} is translated as \lingform{un}.  In contrast, \lingform{une} would be used with a feminine noun such as \lingform{banane}, eg \lingform{j'ai mangé une banane}.

The transformer architecture worked very well in the context of fine-tuning large pre-trained language models, sometimes called \emph{foundation} models.  In other words, a large transformer model could be trained on a large amount of generic Internet content.  This model could then be efficiently \emph{fine-tuned}\index{fine-tuning} (adapted) to a specific domain and task with a much smaller domain/task corpus; this was done by updating the model's parameters\index{parameters} (links between nodes in the neural network) to better fit the domain/task corpus.

This has a huge impact, because it meant that developers could build powerful task-specific models with much less training data\index{training data}.  This approach was used in the Facebook weather system described in Section~\ref{sec:Facebookweather} and also for the Note Generator\index{Note Generator} system described in the Introduction chapter (Section~\ref{IntroConsultationSummary}).  In both cases, it would not have been possible to acquire sufficient training data to allow high-performance models to be trained from scratch, but  it was possible to get enough data to create high-performance models by fine-tuning a pre-trained model.

The transformer architecture also proved well suited to efficient execution on AI hardware, and could be scaled up to very large language models\index{large language models} (\emph{LLMs}) with billions of parameters (links between nodes in the neural network) which were trained on tens or hundreds of billions of words.  At the time of writing it is the dominant architecture for neural NLP and NLG systems, and used in models such as BART\index{BART} \cite{lewis-etal-2020-bart},  T5\index{T5} \cite{raffel2020exploring},  BLOOM\index{BLOOM} \cite{scao2022bloom}, GPT\index{GPT} \cite{brown2020language}, and PaLM\index{PaLM} \cite{chowdhery2022palm}.  Huggingface\footnote{\url{https://huggingface.co/}}\index{Huggingface} provides a comprehensive library and collection of open-source transformers \cite{tunstall2022natural} (and other neural models as well), supported by high-quality documentation and training material.

\subsection{Instruction Tuning\index{instruction tuning} and RLHF\index{RLHF}}\label{sec:promptedmodels}
Researchers discovered that large language models such as GPT3 could perform many tasks without being adapted or fine-tuned, simply be giving them a \emph{prompt}\index{prompt} which made a request, presented the input data, and (optionally) included a few examples (Figure~\ref{fig:chatgpt-example}) \cite{brown2020language}.  This was very exciting, since constructing data sets even for fine-tuning models could be a lot of work.

However, models such as GPT3 and BLOOM were very sensitive to the exact wording of the prompt.
Using these models required a very good understanding of \emph{prompt engineering}\index{prompt!engineering}, ie of creating prompts which framed the desired task and use case in a way which resonated with the model and the data it was trained on.

Fig~\ref{fig:bloom-instruction} shows an example using an early version of BLOOM\index{BLOOM}.  When we craft the prompt correctly, BLOOM gives us the translation (\lingform{rouge}) which we requested.  However, if we use a different prompt which is intuitive but does not match corpus usage, then we get an explanation that it is impossible to have a single unique translation which works in all contexts.  Which is true, but for most people the first response (\lingform{rouge}) is probably more useful.

One technique to make the models more robust and accept a wider variety of prompts is \emph{instruction tuning} \cite{weifinetuned}.  Essentially this process fine-tunes\index{fine-tuning} the model using a data set of common and intuitive instructions along with expected responses.   The fine-tuned model learns how to respond appropriately to intuitive prompts; for example to response to a request for a translation with the best possible translation, instead of explaining the difficulties of providing a translation.  

Another technique for enabling models to respond appropriately to prompts is \emph{Reinforcement Learning from Human Feedback} (RLHF).  The core idea of RLHF is to collect a set of representative prompts, get the model to generate responses to these prompts, show the prompt-response pairs to humans, and ask the humans to assess whether the response was appropriate for the prompt.  This feedback data is then used to update the model using reinforcement learning techniques \cite{ouyang2022training}.

Instruction tuning and RLHF are essentially forms of \emph{alignment}\index{alignment}, that is techniques which helps models respond to human prompts in useful and appropriate manners which aligns with user expectations.

For instance, ChatGPT\index{ChatGPT} (which uses both instruction-tuned and RLHF) responds to the second prompt in Fig~\ref{fig:bloom-instruction} with \lingform{The translation of the English word "red" to French is "rouge."}; this is probably what most users are expecting.

Indeed, instruction tuning and RLHF in ChatGPT and other large language models worked so well that ordinary members of the public, who did not have a background in machine learning or natural language processing (or prompt engineering), were able to create prompts and use these models to generate texts and documents, answer questions, etc.  Such models still gave better results with carefully engineered prompts (and indeed if they are fine-tuned for a specific task), but instruction tuning and RLHF meant that models could provide acceptable and useful responses \scare{out-of-the-box} in many cases from straightforward prompts.
This had a huge impact, and when OpenAI launched a free version of ChatGPT, it reached 100 million active users two months after it was launched; nothing like this had been seen in AI before.



\begin{figure}
\lineacross\\
Prompt:
\begin{verbatim}
Translate English to French.
English: red
French:
\end{verbatim}
Response from BLOOM:
\begin{quotation}
\lingform{rouge}
\end{quotation}
Prompt:
\begin{verbatim}
Translate English word "red" to French.
\end{verbatim}
Response from BLOOM:
\begin{quotation}
\lingform{We, thus, conclude that there is no unique translation from English to French
which preserves the various usages of the concept red, both in the sense of
the noun color and in the sense of the adjective or adverb meaning "with red color".}
\end{quotation}
\caption{Effect of changing the prompt in an early version of BLOOM, which does not use instruction-tuning or RHLF}
\label{fig:bloom-instruction}
\lineacross
\end{figure}

\subsection{End-to-end vs. modular architectures}\label{sec:modularML}
A final point is that neural and ML technology can be used in many different ways in NLG:
\begin{itemize}
\item \emph{Component:}  If an NLG system is organised as a set of components or modules (perhaps following the pipeline architecture presented in Section~\ref{sec:pipeline}), then ML models can be used within specific components in order to perform specific tasks.
One example is choosing \lingform{a} or \lingform{an} (Section~\ref{sec:aan}); this is part of surface realisation (Section~\ref{sec:realisation}).  There are many other examples, include detecting patterns in data, identifying the most important insights, and deciding whether to use pronouns\index{pronouns}.  Castro Ferreira et al \cite{castro-ferreira-etal-2019-neural} show how an entire NLG pipeline\index{pipeline} can be constructed from neural components.
\item \emph{Linguistic processing:} The Facebook weather system (Section~\ref{sec:Facebookweather}) uses a neural model to do linguistic processing (convert insights into words), but uses other techniques to do data-side processing (choose insights to communicate).  I have seen a number of systems that use this strategy, perhaps because \emph{language} models\index{language models} by their nature are best suited to doing language processing.
\item \emph{Retrieval-augmented generation\index{retrieval-augmented generation} (RAG) \cite{lewisrag}:}  Another approach is to first use web search to find relevant information which may be useful to the user, and then give the search results, along with input data or prompt, to a neural language model.  This reduces the amount of content determination which the model needs to do, and also gives it access to up-to-date information from the web.
\item \emph{End-to-end} An end-to-end system uses an ML model to do the complete generation process; ie it takes in the system input data and produces output texts.   This approach is popular with text summarisation\index{summarisation} systems, and also with simpler data-to-text tasks such as using ChatGPT to produce weather forecasts\index{weather forecasts} (Section~\ref{fig:chatgpt-example}).
\end{itemize}


\section{Training data}\label{sec:trainingdata}\index{training data}

Building machine learning models requires training data, preferably large amounts of high-quality data, so acquiring good data sets for training is very important. Indeed, in many ML projects, more effort goes into data acquisition than anything else.  Especially since building models\index{models} (once we have acquired training data) is usually straightforward because of libraries such as scikit-learn\index{scikit-learn}, Huggingface\index{Huggingface} Transformers, Keras\index{Keras}, etc.

This section looks at some issues around training data. The focus is on training smaller models, or fine-tuning large language models.   Creating large models (such as GPT or PaLM) requires much more training data (billions or even trillions of words) for the core model, plus additional data for instruction tuning\index{instruction tuning} and RLHF\index{RLHF}.  However, building such models is a specialist and expensive exercise which is only done by a handful of companies and organisations.

\personal{As an academic, I have seen many student projects which use ML for NLP.  Unfortunately I have often seen students grab an impressive-sounding data set from Kaggle\index{Kaggle} or a shared task repository, spend a lot of time building ML models, and then realise that the data is flawed, so their model is garbage (\scare{garbage in, garbage out}).  I've also seen a lot of published academic papers which suffer from the same problem. Don't make this mistake - carefully investigate the data \emph{before} you spend a lot of time on model building.}

\subsection{Data sources}
Sometimes developers building an NLG system can use an existing data set, but real-world applications often require new data sets.  These can be acquired in several ways, including
\begin{itemize}
\item \emph{Real-world data and tests:}  The ideal data source is a pre-existing dataset which contains input data and high-quality human-written output texts.  For example, the Note Generator\index{Note Generator} (Section~\ref{IntroConsultationSummary}) system, which summarised doctor-patient consultations, was built by fine-tuning the BART model on 10,000 consultations which had been manually summarised by doctors\index{doctors} and entered into the patient's electronic health record.   Similarly the word-choice models for weather forecasts\index{weather forecasts} described in Section~\ref{sec:classifiers} were trained on a corpus\index{corpus} of 1,045 forecasts written by human forecasters.
%Incidentally, I built this weather corpus and it required several person-months of effort to assemble the data, clean it up so it was suitable for ML, and document it so other researchers could use it; this is not unusual for such projects.
\item \emph{Crowdworkers:}\index{crowdworkers} If developers cannot find real-world data and/or texts, a common practice is to get workers on \emph{crowdsourcing} sites such as Prolific\index{Prolific}\footnote{\url{https://www.prolific.com/}} and Mechanical Turk\index{Mechanical Turk}\footnote{\url{https://www.mturk.com/}} to write texts from real or synthetic data.  For example, fifty thousand restaurant descriptions were created by crowdworkers for the E2E corpus \cite{DUSEK2020123}.  If crowdworkers are used, developers must design the process carefully in order to maximise quality; see discussion of using crowdworkers for evaluation in Section~\ref{sec:humansubjects}, where similar concerns arise.
\item \emph{Domain experts:}\index{domain!experts}  It may not be possible to use crowdworkers (who essentially are members of the general public) in specialised domains.  In such cases, domain experts can be asked to write texts from data. For example, the PriMock57 dataset \cite{korfiatis2022primock57} contains summaries written by clinicians of 57 mocked doctor-patient consultations.  Of course asking domain experts to write texts requires a lot more time and money than asking crowdworkers to do this; this is one reason why PriMock57 is so much smaller than the above-mentioned datasets.
\end{itemize}
Each of these techniques has its own challenges.  For example, real-world human-written texts are sometimes not as high-quality as we would hope; Thomson et al \cite{thomson-csl23} found many factual errors in journalist-written stories about basketball games\index{journalism}.   Crowdworkers are paid per task, so they make more money if they do tasks as quickly as possible, which means they may not be very careful when they write texts (Section~\ref{sec:humansubjects}).  And domain experts are usually very knowledgeable about content, but sometimes do not write very well.



\subsection{Dataset criteria}\label{sec:datasetcriteria}
 In an ideal world. the training data set is (A) large, (B) high-quality, and (C) representative.  In NLG, we also usually want data sets to include system inputs as well as system outputs.  Unfortunately, in the real world most training data sets do not meet this criteria.  Poor quality training data will result in poor-quality NLG systems, regardless of the sophistication of the ML technology used (again, \scare{Garbage In, Garbage Out}). This is one reason why most commercial system builders spend more time worrying about data than about algorithms or model types.

\textbf{Size:} Training a good neural model from scratch requires a lot of data.  The amount depends on the complexity of the NLG task and the size of the generated texts, but even a simple task like the E2E\index{E2E} challenge \cite{DUSEK2020123} (generating 10-word restaurant descriptions from 8 features of the restaurant) required a training data set of 50K (feature-set, description) pairs.
However, less training data is needed to fine-tune\index{models!fine-tuned} a pre-trained model.  For example, about 25K of (meaning, output) pairs were required to fine-tune BART for the Facebook weather system (Section~\ref{sec:Facebookweather}); these texts were substantially more complex than E2E, but data requirements were reduced because a pre-trained model was used.  Indeed a fine-tuned model for this task  could have been created with a few hundred elements \cite{heidari-etal-2021-getting} although output quality would have been lower.

\textbf{Quality:} Training data also need to be high quality.  If developers simply scrape data off the Internet or ask random crowdworkers\index{crowdworkers} to generate data, then it is likely that the data sets will have quality issues (readers should also be very careful with data sets on data science repositories such as Kaggle\index{Kaggle}, some are fine but many have quality problems).  One approach is to create a bespoke workflow for creating high-quality data, such as the one described in Section~\ref{sec:Facebookweather}. Another approach is to try to automatically clean datasets (ie, identify problems and then either fix them or delete suspect data) \cite{dusek-etal-2019-semantic}.  Regardless of the approach taken, it is important to assess and evaluate data quality issues, and understand where there are problems.

\textbf{Representative:}\index{representativeness}  Last but not least, training data needs to be representative of  real usage.  Sometimes researchers use easy-to-obtain data which is not representative; for example training dialogue systems on datasets which contain dialogues from TV shows, which are easy to get but completely unrepresentative of real-world dialogue.  In medical\index{health} NLP, a lot of research has used the MIMIC data set \cite{johnson2016mimic}, which is a great resource but comes from a single hospital in Boston, and is not representative of hospitals in general.   Building representative data sets is hard, but it makes a difference in real-world performance.  At minimum developers need to understand and evaluate how representative their data is, and identify contexts where the data set does not have good coverage of real usage.

\subsection{Impact of training data on prompted models}
Prompted models\index{models!prompted} do not need application-specific training data (except perhaps for a handful of examples\index{examples}, Section~\ref{sec:fewshot}), but they are impacted by data quality issues in the data which was used to train the underlying language model\index{large language models}.  Generally speaking models used for prompting are primarily trained on Internet data (such as that gathered by Common Crawl\footnote{\url{https://commoncrawl.org/}}\index{Common Crawl}), since this is the only data source which is large enough to provide sufficient training data.  However, Internet data has many problems, which possibly may impact the quality of text produced by the prompted model.  These include:

\begin{itemize}
\item \emph{Inappropriate content:} The Internet contains material which we do not want to see included in system outputs, such as pornography, racist and sexist rants, stereotypes, encouragement to commit crimes, etc.  This is an aspect of \emph{safety}\index{safety}, which is discussed in Section~\ref{sec:safety}. One approach to this problem is to try to exclude such material either from the training data or from prompt responses; however, this is not straightforward \cite{welbl-etal-2021-challenges-detoxifying}.  
\item \emph{Spam\index{spam} and malicious content:}  As prompted models such as ChatGPT\index{ChatGPT} become widely used in society, it seems likely that spammers and other \scare{adversarial} agents will try to create Internet content which, when used to train the model\index{models}, will guide the model towards producing a response that advances the spammer's agenda.  Spam of course is already a huge problem with Internet search, but the black-box nature of neural networks may make it harder to detect spam in neural models.
\item \emph{Biased content:}\index{bias} Most Internet material comes from well-off individuals in rich countries, which introduces biases.  Very little, for example, comes from homeless people, which means that if a model is generating texts about homeless people, it will primarily be driven by Internet content created by well-off people commenting on homelessness, not content created by homeless people themselves.
\item \emph{Low-quality content:} A related issue is that a lot of material on the Internet is not of high-quality.  For example, Balloccu et al \cite{balloccu2024ask} looked at using ChatGPT to give health\index{health} advice, and discovered that a lot of the advice was inappropriate, in part because it seemed to largely be based on comments on discussion forums (such as Reddit\index{Reddit}) instead of high-quality advice from health professionals \cite{balloccuphd} (see also \cite{abercrombie-rieser-2022-risk}).  This is probably because there is  more data in discussion forums than in high-quality curated health websites.  
\end{itemize}
Most of the above problems are worse when the model is asked to answer questions (Section~\ref{sec:llm-content}), which is how many people use ChatGPT.  However, even in use cases where the task is summarising or explaining input data (which is what this book focuses on), there is still potential for the above problems to lead to toxic content\index{toxic content}, spam, bias, etc. (Section~\ref{sec:inappropriatelanguage}).

Data issues also effect testing and evaluation data, as well as training data; this is discussed in Section~\ref{sec:trainingtestdata}.

\subsection{Synthetic data\index{synthetic data} and data augmentation\index{data augmentation}}\label{sec:syntheticdata}
If a sufficiently large data set cannot be constructed from real data, developers may consider \emph{augmenting} the data set by adding \emph{synthetic data}  to it \cite{feng-etal-2021-survey}.  To take a simple example, assume a developer is training a model to generate weather forecasts and has 500 genuine human-written forecasts\index{weather forecasts} but needs 1000 forecasts in order to train or fine-tune an NLG forecaster.   The developer could use a machine translation tool to translate each forecast into German, and then \emph{back-translate}\index{back-translate} the German version of each story into a new English version.  The back-translation process gives a \emph{paraphrases}\index{paraphrase} of the original story, which can added to the training data set (Figure~\ref{fig:backtranslate}).  Of course the process can be repeated using other languages (French, Chinese, Finnish, etc.) if we want even more versions.   There are many other techniques for data augmentation \cite{feng-etal-2021-survey}.

\begin{figure}
\lineacross\\
\emph{Original English text (from Figure~\ref{fig:facebookweather}):}\\
Next weekend expect a low of 20 and a high of 45. It will be sunny on Saturday but it’ll rain on Sunday.\\

\emph{Google Translate's German translation of above English text:}\\
Am kommenden Wochenende wird ein Tiefstwert von 20 und ein Höchstwert von 45 erwartet. Am Samstag wird es sonnig sein, aber am Sonntag wird es regnen.\\

\emph{Google Translate's English translation of above German text (this is the back-translation):}\\
A low of 20 and a high of 45 are expected next weekend. On Saturday it will be sunny, but on Sunday it will rain.

\caption{Example of using back-translation to create a paraphrased version of a human text.  This can be added to a data set if more data elements are needed and it is acceptable to use synthetic data.}\label{fig:backtranslate}
\lineacross
\end{figure}

Data augmentation can be very useful in many cases and indeed is widely used.  To take one random example, the BLEURT metric for assessing the quality of generated texts (described in Section~\ref{sec:bleurt}) uses synthetic data (in part obtained by back-translation, as described above) to tune the model used to assess text quality; this enhances the robustness and accuracy of BLEURT.

Data augmentation is not always helpful, however.  Anyone using data augmentation should carefully evaluate the result of using the augmented data set to see if adding synthetic data actually helps.

Also, evaluation should not purely be based on synthetic data.  Sometimes it is acceptable to evaluate using a mix of real and synthetic data, but do not evaluate purely using synthetic data.  If the goal is to evaluate real-world effectiveness, the evaluation must (at least in part) be based on real-world data.

\subsection{Including examples\index{examples} in prompts\index{prompt}}\label{sec:fewshot}
In all of the above examples of prompts, the prompts have basically made a request and provided input data.  It is also possible to include examples in the prompt; this is called \emph{few-shot}\index{few-shot} (prompts without examples are called \emph{zero-shot}\index{zero-shot}).   The examples are not used to train the model in a classic machine learning sense, but they can tell the model what the user is looking for and guide it to producing an appropriate output.

For example, the first half of Figure~\ref{fig:BLOOMfewshot} shows the result of a request to BLOOM\index{BLOOM} to summarise sports information, without an example.  The result is fine in abstract, but suppose that a shorter and more direct sentence is more appropriate in the target application.  The second half of Figure~\ref{fig:BLOOMfewshot}  shows how we an example can be used to guide BLOOM into generating a short and direct summary sentence.

BLOOM does not use instruction-tuning\index{instruction tuning} or RLHF\index{RLHF}.  Examples can be less effective with models that use instruction tuning and RLHF, such as ChatGPT, but they still are useful in many cases.

\begin{figure}
\lineacross\\
Prompt (no example):
\begin{verbatim}
Summarise the following text
Text: Aberdeen ended their long wait for a Scottish Premiership
away win by beating Dundee United 3-1 at Tannadice.
Summary:
\end{verbatim}
Response:
\begin{verbatim}
Dundee United was defeated 3 -1 by Aberdeen at their home ground, Tannadice.

\end{verbatim}
Prompt (with example):
\begin{verbatim}
Summarise the following text

Text: Celtic kept their hopes of winning a domestic treble alive afte
beating rivals Rangers 1-0 in the Scottish Cup semifinal on Sunday
thanks to a goal from Jota.
Summary: Celtic narrowly beat Rangers 1-0.

Text: Aberdeen ended their long wait for a Scottish Premiership
away win by beating Dundee United 3-1 at Tannadice.
Summary:
\end{verbatim}
Response:
\begin{verbatim}
Aberdeen beat Dundee United 3-1 at Tannadice.
\end{verbatim}
\caption{Result of prompt without and with example to BLOOM}\label{fig:BLOOMfewshot}
\lineacross
\end{figure}

\section{Issues}
Many issues arise when using machine learning and neural techniques.  Some of the most important apply to all text-generation systems (not just neural ones), so I discuss them at length in Part 2 of this book.  In particular, concerns about safety, bias, and fairness are discussed in Section~\ref{sec:safety}, and concerns about maintenance are discussed in Section~\ref{sec:maintenance}.

In this section, I more briefly discuss several issues which primarily affect systems built with machine learning and neural techniques.


\subsection{Domain shift}\label{sec:domainshift}\index{domain!shift}

A fundamental problem with machine learning models is that they become out-of-date; this is called \emph{domain shift} and is related to software maintenance\index{maintenance} (Section~\ref{sec:maintenance}).   A model trained in 2020 will reflect the world in 2020, and hence may not give appropriate answers in 2024.  Strickland \cite{strickland2019ibm} points out that the IBM Watson\index{IBM Watson} question-answering system suffered because it could not give up-to-date information about medical\index{health} interventions which included the latest findings, and we saw in Section~\ref{sec:aan} that English (and other human languages) is dynamic and changes over time.  Braun and Matthes \cite{braun2024agbdecorpusautomatedlegal} show that GPT\index{GPT} 3.5 can give incorrect legal\index{legal} assessments because it ignores changes in laws.
 In data-to-text\index{data-to-text}, a related issue is that new data sources become available (eg, new types of scanners in hospitals) and users expect such data to be used by the NLG system; this will not happen if the NLG system was trained on earlier data which did not include the new scanner.

Domain shift issues were especially prominent during the Covid-19 pandemic\index{Covid pandemic}, when activities that had previously been fine and indeed encouraged (like going for a hike in the countryside, or meeting friends in pubs) became inappropriate and indeed sometimes illegal.  In a data-to-text context, systems which reported on business data became less useful because the business world changed as some income streams collapsed but new ones (such as emergency support from government) emerged.

At the time of writing, considerable effort is going into updating commercial prompted models to try to keep them up-to-date.  However most of this work is not published because it is commercially sensitive.  There is also a lot of interest in techniques such as retrieval-augmented generation\index{retrieval-augmented generation} \cite{lewisrag} which augment (out-of-date) models with (up-to-date) web search results.


\subsection{Question answering}\label{sec:llm-content}
As mentioned above, the focus of this book is on NLG which generates output texts based on some kind of input data.  However, many (probably most) users of prompted models\index{models!prompted} also use them as general tools to answer questions, provide information, and create documents.   In such use cases, the models are creating content based on Internet data they were trained on, they are not just describing and summarising run-time input data.

I regard this as a risky thing to do, because of the data issues mentioned above, especially as most models do not tell you where the data comes from.
Incorrect and dated content are also concerns (which relates to domain shift\index{domain!shift}, Section~\ref{sec:domainshift})

\personal{Several years ago a few websites described me as an \lingform{Israeli computer scientist}, which is not true (I am not Israeli).  I eventually got these websites to update their material on me.  However, in 2023 ChatGPT\index{ChatGPT} still described me as an \lingform{Israeli computer scientist}, presumably because it was trained on old versions of websites.}

Models can also generate content which is accurate but still not appropriate (Section~\ref{sec:upsetting}).  For example, MedPaLM\index{MedPaLM} \cite{singhal2023large} generates the following text in the context of giving heath advice
\begin{quotation}
Ear pain can be a sign of several underlying conditions, including: Middle ear infection (otitis media), which is commonly caused by a viral infection and can cause pain, redness, and swelling of the ear. External ear infection (otitis externa), which is commonly caused by a bacterial infection and can cause pain, redness, and swelling of the ear. Earwax impaction, which can cause pain, a feeling of fullness in the ear, and decreased hearing. Ear pain can also be a sign of a more serious condition, such as a brain tumor or a stroke. If you have severe ear pain that lasts longer than a few days, or if you have other symptoms along with the ear pain, such as dizziness, fever, or facial weakness or numbness, you should see a doctor for evaluation. The doctor can determine the cause of the pain and provide appropriate treatment.
\end{quotation}
When I showed the above text to a doctor, she commented that she would never say this to a patient, because it states that ear pain can be a sign of brain tumour or stroke.  This is true, but rare, and telling this to a patient is likely to cause a considerable amount of anxiety and stress.  Hence it is better to drop this statement, while keeping the statement that the patient should see a doctor if severe ear pain persists..

\subsection{Auditability\index{auditable} and Controllability\index{controllability}}
In many contexts clients want to control the behaviour of an NLG system, for example to ensure that it uses language compatible with a corporate brand, that  it says nothing which raises legal\index{legal} concerns (e.g., racist language), and more generally that the system's output is appropriate and not misleading.  There are also contexts where developers need to prove to a third party (such as a regulator\index{regulator}) that an NLG system never engages in unacceptable behaviour.

Doing this with a neural  NLG system is challenging.  Developers can control the output by adjusting training data, fine-tuning data, or changing the prompt, but this does not always work.  Even worse, proving that a neural network with hundreds of billions of parameters never engages in a behaviour is not currently possible.  Its probably more realistic to add a checking tool as a post-processor, which identifies and deletes unsafe or inappropriate content (Section~\ref{sec:autosafetydetection}); but again this does not always work, and also high-quality checking can get complex and expensive to develop.

\subsection{Legal\index{legal} and Regulatory\index{regulator} Issues}\label{sec:legal}
 As mentioned above, it seems likely that neural NLG systems which are used in healthcare and other regulated safety\index{safety}-critical areas will need regulatory approval, and this may be challenging, in part because of difficulties in rigorously testing these systems (Section~\ref{sec:testingnlg}).  Provenance\index{provenance} is also an issue for models (especially prompted models\index{models!prompted}) trained on large chunks of the Internet, since much Internet material is incorrect or otherwise inappropriate \cite{gilbert2023large}.

Companies which use language models can be legally liable for mistakes made by the models.  For example, the airline Air Canda deployed a customer service chatbot which gave incorrect (hallucinated) information about some of the airline's fares.  A court rules that the airline had to honour what the chatbot told customers even if it was not in line with the airline's actual fares\footnote{\url{https://www.forbes.com/sites/marisagarcia/2024/02/19/what-air-canada-lost-in-remarkable-lying-ai-chatbot-case/}}.

Another important legal issue is copyright\index{copyright} and intellectual property\index{intellectual property}. Early language models were trained on Internet material, such as Wikipedia\index{Wikipedia}, which is available under a \emph{Creative Commons}\index{Creative Commons}  license which allows the material to be used in all sorts of ways, including training language models.  However, larger prompted models such as PaLM and GPT4 are primarily trained on material which does \emph{not} have such a license; it is not possible to build up a sufficiently large training dataset purely from material which has a Creative Commons (or similar) license.   This has led to lawsuits\index{lawsuits}, where content authors claim that material they authored and put on the Internet has been used for commercial purposes (building a language model) which they did not authorise, and for which they receive no compensation.  The situation becomes especially difficult if content authors believe that they have lost contracts (or even jobs) to systems which were trained on material produced by these authors.

At the time of writing, the legal situation is unclear, especially since some countries are considering changing the laws which govern usage of Internet content.  One possibility is that some countries will allow models which are trained on random Internet material and others will only allow models which are built on appropriately licensed (e.g., Creative Commons) material; this international disagreement would not be ideal, to put it mildly.  It would be much better to have a worldwide convention on usage of Internet material, similar to the Berne convention\index{Berne convention} which governs copyright law globally.  However, creating international conventions is a slow process; for example the USA only joined the Berne convention in 1989, which is \emph{102 years} after the convention was accepted in 1887 by European countries including France, Germany, and the UK.



\section{Further reading and resources}\label{sec:mlreading}
At the time of writing, Jurafsky and Martin are preparing a third edition of their classic textbook, \emph{Speech and Language Processing}, which will focus on machine learning algorithms and techniques used in natural language processing.  This will be  a superb resource when it is formally published\footnote{Before publication, draft chapters are available at \url{https://web.stanford.edu/~jurafsky/slp3/}}.

Many large technology companies publish high-quality white papers and research papers about neural approaches to NLG.   I have been particularly impressed by Google's blogs and white papers (\url{https://blog.google/technology/ai/}) and research publications (\url{https://research.google/}).  Readers should remember that companies do not say bad things about their products, so there may be biases in analyses and evaluations in Google papers about Google products.  But the material from Google and other technology companies  can be a great resource for learning about how the latest technologies work.

There are numerous companies which offer language models and neural NLG systems, with new ones seemingly announced every month (sometimes every week).  I cannot recommend specific ones, since anything I say at the time of writing may no longer be true when this book is published.

Huggingface\index{Huggingface} (\url{https://huggingface.co/}) is an outstanding source for downloadable models, software, and tookits for neural NLG.   NLTK\index{nltk} (\url{https://www.nltk.org/}) is an excellent resource for earlier statistical  and ML models used in NLP. Python's \emph{scipy}\index{scipy} library contains classifiers and other generic ML tools.

There is a large literature on data issues in NLP.  Rogers \cite{rogers-2021-changing} is a good overall position paper on the topic, and Bender and Friedman \cite{benderDataStatements} argue for comprehensive data statements.  Feng et al \cite{feng-etal-2021-survey} survey data augmentation\index{data augmentation} techniques, and Lu et al \cite{lu2024machine} surveys synthetic data\index{synthetic data} in AI generally.  There are many companies and consultants who will assist with data acquisition, I will not recommend any specific companies here.

Hu et al \cite{hu2020challenges} describes the real-world challenges of dealing with domain shift\index{domain!shift} in AI models during the Covid-19 pandemic\index{Covid pandemic}; the paper is not about NLP but it does graphically illustrate the problem in a real-world setting.  Ramponi and Plack \cite{ramponi-plank-2020-neural} describe the problem from an NLP perspective, and some potential approaches to dealing with it.

I am not aware of many research papers on auditing AI or NLP models.  The UK National Audit\index{auditable} Office has a nice summary at \url{https://www.nao.org.uk/insights/how-to-audit-artificial-intelligence-models/}.  

AI law\index{legal}, governance and regulation\index{regulator} (including auditing rules) are evolving extremely quickly at the time of writing.  The EU Artificial Intelligence Act\index{EU Artificial Intelligence Act} (\url{https://artificialintelligenceact.eu/}) will establish a legal framework for AI in Europe, and other countries (including the US and UK) are also moving ahead with efforts to regulate AI.  Legal case law is also rapidly evolving, including use of Internet material for training language models.  Readers should check the up-to-date legal situation in their home country; anything said in this book about AI laws, governance, and regulation is likely to be out-of-date.












