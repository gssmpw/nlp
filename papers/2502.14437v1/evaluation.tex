%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Evaluation}
\label{evaluation} % Always give a unique label

A key issue in NLG is \emph{evaluation}, in other words assessing how well an NLG system works and meets its requirements\index{requirements}.  Does it produce texts which are acceptable in its use case\index{use cases}, based on the relevant quality criteria\index{quality criteria} (Section~\ref{req-criteria})? 
Evaluations can also assess whether a system is suitable for its expected workflow\index{workflow} (Section~\ref{sec:workflow}) and if it is acceptable to different stakeholders\index{stakeholders} (Section~\ref{sec:reqstakeholders}).

In addition to assessing the effectiveness of an NLG solution, evaluation can also highlight where systems are weak and need to be improved.  For example, an evaluation of an NLG system may show that its texts have good readability\index{readability} but poor accuracy\index{accuracy}; this is very useful information for developers who are trying to improve the system.

In this chapter I first give an example of an evaluation (Section~\ref{sec:stopsmoking}), then look at fundamental evaluation issues and concepts (Section~\ref{Evaluation:Fundamentals}).  I then discuss evaluation with human subjects (Section~\ref{sec:humanevaluation}), evaluation based on automatic metrics or algorithms (Section~\ref{sec:automaticevaluation}), and evaluation based on real-world impact (Section~\ref{sec:impactevaluation}).  I also look at some topics which are important in many commercial evaluations, but less important in many academic evaluations, such as system cost (Section~\ref{sec:commercialevaluation}).  I conclude with some general advice on doing good evaluations (Section~\ref{sec:ten-tips-eval}) and suggestions for further reading. Throughout the chapter I often refer to research on evaluating machine translation\index{machine translation} systems, which is well developed and provides many insights which are relevant to evaluation of NLG.

Evaluation is related to \emph{software testing}\index{testing} of NLG systems (Section~\ref{sec:testingnlg}).  However, the focus of evaluation is on trying to quantify the performance of an NLG system (usually on a set of relevant quality criteria), while the focus of software testing is on identifying specific cases and scenarios where an NLG system behaves unacceptably.

\section{Example: Smoking Cessation (Impact Evaluation)}\label{sec:stopsmoking}\index{impact!evaluation} 
I will start with an example evaluation of an NLG system which generated leaflets which were intended to help people stop smoking \cite{REITER200341}.  This was an \emph{impact} evaluation, because it evaluated the system's real-world effectiveness in encouraging smoking cessation.  The evaluation was simple conceptually (which is why I use it as an example), although it required a lot of effort to carry out.

The STOP\index{STOP} system took as input a questionnaire about smoking habits; an extract is shown in Figure~\ref{fig:stopquestionnaire}.  From this information, STOP generated a 4-page leaflet about stopping smoking (which included graphics as well as text); an extract is shown in Figure~\ref{fig:stopleaflet}.  Because this was done in the late 1990s and the researchers wanted to reach a wide variety of smokers, including those who did not have internet access, the experiment was paper-based.  Subjects filled out the questionnaires on paper and posted them; researchers scanned the questionnaires, ran the software, printed the leaflets, and posted them back to the subjects.

\begin{figure}
\lineacross\\
%\includegraphics[scale  = 0.75]{images/stopquestshort}
\includegraphics[scale  = 0.6,trim={0 10cm 0 0},clip]{quest1.pdf}
\caption{Part of the STOP questionnaire \cite{reiterjair03}}
\label{fig:stopquestionnaire}       
\lineacross
\end{figure}

\begin{figure}
\lineacross\\
\textbf{Dear Ms Cameron}
\blankline\\
Thank you for taking the trouble to return the smoking questionnaire that we sent you.  It appears from your answers that although you're not planning to stop smoking in the near future, you would like to stop if it was easy.  You think it would be difficult to stop because \emph{smoking helps you cope with stress}, \emph{it is something to do when you are bored}, and \emph{smoking stops you putting on weight}.  However, you have reasons to be confident of success if you did try to stop, and there are ways of coping with the difficulties.\\
%\includegraphics[scale  = 0.8,trim={0 9cm 14cm 0},clip]{images/tailor2.pdf}
\caption{First paragraph of an example STOP leaflet \cite{reiterjair03}}
\label{fig:stopleaflet}    
\lineacross   
\end{figure}


To evaluate STOP \cite{Lennox1396}, the researchers recruited 2553 subjects in the Aberdeen area who smoked, asked them to fill out the questionnaire, and then sent each of them either (A) a STOP leaflet, (B) a default leaflet which was not personalised based on their questionnaire, or (C) a simple \scare{thank you for being in our study} letter.  The reason for the different responses was to check if people who got STOP leaflets were more likely to stop smoking than people who got something else; these were \emph{control groups}\index{control groups} or \emph{baselines}\index{baselines}.  Of course some people are going to stop smoking regardless of the NLG system, so its not very useful to just measure how many STOP recipients managed to stop smoking.  Its much better to be able to compare smoking-cessation rates in STOP recipients against smoking cessation rates in people who did not get STOP leaflets.

Anyways, 6 months after sending out the leaflets and thank-you letters, smokers were contacted and asked if they had stopped smoking.  If they said they had stopped, they were asked to provide a saliva sample, which was tested for nicotine residues (this is necessary because sometimes people are not truthful about smoking cessation).  Subjects who did not respond or who did not provide saliva samples were assumed to still be smoking.

This gave data on how many people in each of the groups (STOP leaflet, default leaflet, and thank-you letter) had managed to stop smoking:
\begin{itemize}
\item 3.5\% of people who got STOP leaflets managed to stop smoking.
\item 4.4\% of people who got default leaflets managed to stop smoking.
\item 2.6\% of people who just got thank-you letters managed to stop smoking.
\end{itemize}
In other words, the STOP leaflets seemed to be \emph{less} effective than the non-personalised default leaflets!  This difference was not statistically significant\index{statistical!significance} (Section~\ref{sec:statisticalsig}), but still the evaluation provided no evidence that the STOP leaflets were worthwhile; it was a \emph{negative result}.  Negative results\index{negative results} are disappointing, but it is important that they are published, not least because they warn other researchers that some research directions may not be productive.

\personal{We published STOP's negative findings in both a medical journal \cite{Lennox1396} and an AI journal \cite{REITER200341}.  Medical journals routinely published negative results, but they were almost unheard of in the  AI literature at the time (early 2000s); indeed I had several discussions with the editor about how to best present a negative result to the AI community.  Thankfully it is more common (although still unusual) for negative AI results to be published in 2024.}

\section{Fundamentals}
\label{Evaluation:Fundamentals}

\subsection{Stakeholder perspective}\label{sec:stakeholders}
Developers must keep in mind that different stakeholders\index{stakeholders} care about different things (Section~\ref{sec:reqstakeholders}).  This means that an evaluation which is acceptable to one stakeholder may not be useful to a different stakeholder.

For example, the Babytalk\index{Babytalk} project (Section~\ref{sec:babytalk}) was a collaboration between computer scientists, doctors, psychologists, and a software company (not Arria)\footnote{Arria was founded several years after the Babytalk project started.} to build NLG systems which summarised data from a baby's electronic patient record.  Different collaborators cared about different things from an evaluation perspective \cite{reiter-belz-2009-investigation}:
\begin{itemize}
\item \emph{Doctors\index{doctors} and medical researchers} wanted to know if Babytalk was medically effective.  They wanted to evaluate whether using the system led to better patient outcomes (or reduced clinical workload).
\item \emph{Psychologists\index{psychologists}} were interested in the effectiveness of textual vs graphical\index{graphics} presentation of information (Section~\ref{sec:textgraphics}).  They wanted to evaluate how presentation medium impacted decision quality\index{decision support}.
\item \emph{Software house} wanted to know if a profitable product could be created using Babytalk technology.  They wanted to assess costs, (commercial) benefits, and risks (Section~\ref{sec:commercialevaluation}).
\item \emph{Computer scientists} wanted to understand effectiveness of algorithms and models.  Whereas other stakeholders just cared about the system as a whole, CS researchers wanted to understand how well components worked.
\item \emph{Parents} (who use the Babytalk system for families) wanted reports which were accurate and easy to read, and which told them how well their babies were doing.
\end{itemize}
It was impossible to use a single evaluation to assess all of the above stakeholder concerns.  The Babytalk team therefore did a number of evaluations (not all of which were published), including \cite{PORTET2009789,HUNTER2012157,mahamood-reiter-2011-generating,moncurhci14}, in order to assess different aspects of the Babytalk systems.

While it remains the norm in academic research to do one evaluation of a system, multiple evaluations may make sense if the system has different types of stakeholders.

Understanding what different stakeholders care about is part of requirements analysis (Chapter~\ref{requirements}), and as such this issue should be explored \emph{before} an evaluation is designed, not after it is carried out.

\subsection{Hypothesis Testing}\index{hypothesis testing}
Evaluation, at least in the academic world, is usually a type of \emph{scientific hypothesis testing}.  Hypothesis testing is an integral part of the scientific method.  Essentially scientists propose a hypothesis, such as \scare{People who smoke are more likely to get lung cancer} (medicine), \scare{Nothing can travel faster than the speed of light} (physics), or \scare{the moon is made of green cheese} (space science).  Scientists then create experiments to test hypotheses; for example conduct a spectrographic analysis of the moon (or indeed ask astronauts to collect samples), or monitor a large cohort of people and compute statistically whether smokers are more likely to get lung cancer.   In doing so, scientists discover that some hypotheses seem to be true (e.g., smokers are more likely to get lung cancer), while others are clearly false (e.g., the moon is not made of green cheese).

The type of hypothesis being tested of course depends on the field (e.g., physics vs medicine), as does the way hypotheses are tested (e.g., collecting samples vs statistical cohort analysis).  However, the notion of experimentally testing hypotheses is fundamental to science, and indeed is what has made the modern scientific method so successful.

From this perspective, the STOP\index{STOP} evaluation discussed in Section~\ref{sec:stopsmoking} tested the hypothesis that \scare{smokers who receive STOP leaflets are more likely to stop smoking than smokers who receive a default letter}.  The evaluation showed that this hypothesis was probably not true.

In NLG, most hypotheses are based on comparing the output quality between texts produced by different systems. A typical hypothesis states that texts produced by system A are better than text produced by system B, according to quality criteria\index{quality criteria} X, Y, and Z.  For example, if accuracy is an important quality criteria in the target NLG use case, researchers can hypothesise that texts produced by system A contain fewer incorrect statements (hallucinations) than texts produced by system B.  In the STOP example, the main hypothesis was that STOP texts were better according to the \emph{utility} quality criteria (Section~\ref{req-utility}).

There are other types of hypotheses which do not involve quality criteria, such as hypotheses based on cognitive plausibility\index{cognitive science} (eg, hypothesising that an NLG system will make similar mistakes to a human).  These are important for some types of research, but I will not discuss these here.

\subsection{Statistical hypothesis testing}\label{sec:statisticalsig}\index{statistical!hypothesis testing}
Most NLG hypotheses are \emph{statistical}.  For instance, using an example from machine translation\index{machine translation}, lets say we hypothesise that Google Translate produces better English translations of Chinese news articles than Bing Translate, as judged by expert translators on the basis of accuracy and fluency.  We do not expect that Google's translation will be better than Bing's on every possible Chinese news article!  Instead, we usually hypothesise that when the translators compare Google's translation of an article to Bing's translation, in most cases they will prefer Google's translation.

But small difference may just be due to random chance.  For example,  suppose we translate 1000 random Chinese news articles into English using Google and Bing, and our judges say that the Google translation is better in 510 cases and the Bing translation is better in 490 cases.   Is this difference meaningful, or could it just be random chance?

In science, we answer such questions using \emph{statistical hypothesis testing}
\cite{lakensstats}.  In order to do this, we first frame a \emph{null hypothesis}\index{null hypothesis}, for example that on average there is no difference between the quality of Google and Bing translations.  Then we use statistical tests\index{statistical!tests} to measure how likely the observed outcome (Google texts are better in 510 out of 1000 cases) is if the null hypothesis is true (there is no difference on average between Google and Bing).  In this case, a \emph{binomial test}\index{binomial test} tells us that if Google and Bing texts have the same quality on average, then:
\begin{itemize}
\item There is a 2\% chance that Google will be better on \emph{exactly} 510 cases
\item There is a 27\% chance that Google will be better on \emph{at least} 510 cases. This is called the \emph{one-tailed p value}\index{p value!one-tailed}.
\item There is a 54\% chance that either Google will be better than Bing on at least 510 cases, or Bing will be better than Google on at least 510 cases. This is called the \emph{two-tailed p value}\index{p value!two-tailed}.
\end{itemize}
In NLP, we normally consider that experimental results support a hypothesis if the two-tailed p-value is less than 5\%.  This is not true in the above example, so we could not conclude that Google was better than Bing if its translation was better in 510 cases (two-tailed p-value is 54\%).

However, if our experiment showed that the Google translation was better in 540 cases (and Bing's translation was better in 460 cases), then the two-tailed p-value for the binomial test would be 1.2\%.  Since this is less than 5\%, we could conclude from this experimental result that Google was probably better than Bing at translating news articles from Chinese to English.

In the STOP\index{STOP} example (Section~\ref{sec:stopsmoking}), a \emph{chi-square test}\index{chi-square test} was used to assess the difference between the smoking cessation rate in the STOP group (30 smokers quit out of 857 total; 3.5\% quit rate) and in the default-leaflet group (37 smokers quit out of 846 total; 4.4\% quit rate).  This gave a two-tailed p-value of 35\%.  Since this is much higher than 5\%, this means that the experiment is inconclusive.  Its possible that the default leaflets are more effective than STOP, but its also possible that they are equally effective. Indeed, its even possible that the STOP leaflets are in fact better, but this was obscured by bad luck; perhaps smokers who happened to get STOP letters were committed smokers who were less likely to quit than smokers who got the default leaflets.

Dror et al \cite{dror-etal-2018-hitchhikers} discuss common statistical tests in NLP research.  Note that the 5\% threshold is just a convention, and different areas of science use different thresholds.  Psychology and medicine also generally use a 5\% threshold, but particle physics uses a threshold of 0.00003\%.

\subsection{Experimental design, execution, reporting, and follow-up}
When we do an experiment (including evaluating an NLG system), we need to ensure that it is well designed, executed, reported, and followed up.

\subsubsection{Experimental design}\label{eval-gen-exp-design}\index{experimental design}
Experiments must be well designed.  For example, if we are comparing Google Translate and Bing Translate as described above, then the experimental design would include:
\begin{itemize}
\item \emph{Research questions:}\index{research questions} What research questions and hypotheses is the experiment intended to address; for example, as mentioned above, we could hypothesise that expert translators prefer Google over Bing when translating Chinese news articles to English.
\item \emph{Study type:}\index{study type}  What type of study will we do; for example the above experiment is a human evaluation based on rankings (Section~\ref{sec:humanevalrankings}).
\item \emph{Subjects:}\index{subjects} If our experiment uses people, how are they chosen, and what criteria must they satisfy; for example, we may decide to use expert translators recruited from a freelancing platform such as Upwork.
\item \emph{Material:}\index{material} What scenarios are evaluated; in other words, what are the 1000 texts being translated, and how are they chosen?  For example, we may decide to take 1000 random articles from the Xinhua news agency.
\item \emph{Procedure:}\index{procedure} What do the subjects do?   For example, what user interface do they use to express their preference?
\item \emph{Analysis:}\index{analysis} How are results analysed?  For example, we can use a binomial test (as described above) to compute statistical significance\index{statistical!significance}.
\end{itemize}
Of course a detailed experimental design would include much more information than above!  We will discuss experimental designs for specific types of evaluations in more detail in Sections~\ref{sec:humanevaldesign} and \ref{sec:metricevaldesign}.

A final point is that it is good practice in many fields of science to \emph{pre-register}\index{pre-registration} experiments, that is to enter experimental design details in a pre-registration website before carrying out the experiment.  This is common practice in medicine and psychology, but is less common in AI and NLG \cite{van-miltenburg-etal-2021-preregistering}.

\subsubsection{Experimental execution}\label{eval:experimental-execution}\index{experimental execution}
Experiments must also be carefully executed.  Unfortunately, many NLG experiments are not well executed, because of problems such as the following \cite{thomson-cl24}:
\begin{itemize}
\item \emph{Code bugs:}\index{bugs!code} Experiments can be invalid because of bugs in the code used to run the experiment.  For example, a subject is shown an input text X and \scare{translations} from Google and Bing, but because of a code bug the Bing translation is of a different input text Y.
\item \emph{Reporting errors:}\index{errors!reporting} Sometimes the data reported in the paper is different from the actual experimental results.
 For example, the paper states that the Google text was preferred in 540 cases, but the experimental data shows that it was preferred in 510 cases.
\item \emph{User interface issues:}\index{user interface} Experiments with human subjects can produce misleading results if a confusing user-interface is used.  For example, if subjects are asked to use radio buttons to show whether they preferred the Google text or the Bing text, but the buttons are not clearly labelled so it is not clear which button indicates a preference for the Google text.
\item \emph{Analysis bugs:}\index{bugs!analysis} Statistical analyses can also be misleading.  For example, if 1500 subjects did the experiment but the analysis only looked at the results from 1000 subjects and ignored the remaining 500.
\end{itemize}
Thomson et al  \cite{thomson-cl24} give concrete examples of the above issues in published NLP research papers.

\subsubsection{Experimental follow-up}\index{followup}
Even after an experiment has been written up and published, researchers must respond to questions and otherwise support their research.  This includes
\begin{itemize}
\item \emph{Respond to questions:}  If readers are interested in an experimental evaluation, they are likely to have questions and want further details; the experimenters should respond to such requests.  Unfortunately, many researchers do not do this. The ReproHum\index{ReproHum} project \cite{belz-etal-2023-missing} contacted 116 authors of academic NLP papers (all published in good venues), and asked the authors for additional information about their project.  Only 45 (39\%) of authors responded in any fashion, and only 15 (13\%) provided the requested information.
\item \emph{Correct problems:} Sometimes problems in experiments are discovered after results are published or otherwise released.  If this happens, the researchers should publicly acknowledge the problems, and fix their papers.  Unfortunately, most NLP researchers do not do this.  For example, in the ten-year period 2013-2022, the TACL (\emph{Transactions of the ACL})\index{Transactions of the ACL} journal only had \emph{one} formal correction\index{correction} to experimental results and findings \cite{thomson-cl24}. I personally know of more TACL papers in this period which had mistakes and should have been corrected.
\end{itemize}

\subsection{Research questions}\label{sec:researchquestions}\index{research questions}
Evaluations are intended to test research questions and hypotheses, which in NLG often take the form that system X is better than system Y under quality criteria\index{quality criteria} Z; for example Google Translate is better than Bing Translate under the criteria of accuracy of their generated translations.   The research question must be important scientifically (for basic research) or to stakeholders\index{stakeholders} (for applications); there is no point in testing a research question which is of no interest to anyone.

From this perspective, it is essential to evaluate quality criteria that people care about; in applied NLG this means criteria that are important to stakeholders (Section~\ref{sec:stakeholders}).  Sometimes the things that stakeholders are about are difficult to measure.  For example in medicine doctors usually want to measure impact\index{impact} on clinical outcomes (does a system help patients); but measuring this experimentally can be a lot of work (Section~\ref{sec:stopsmoking}).  But regardless, if this is what people care about, it should be evaluated.  There is a temptation for researchers to evaluate quality criteria which are relatively easy to measure, such as fluency (Section~\ref{sec:fluency}), but this is of limited value if it is not what stakeholders care about.

Another point is that if the goal is to demonstrate that a new NLG system is better than existing NLG systems (sometimes called \emph{baselines}\index{baselines}), it is essential to compare the new system to the best (state-of-the-art\index{state-of-art}) existing NLG system(s); comparing the new system to obsolete systems is not valuable or interesting. For example, if the goal is to show that a new language model can generate better texts in some contexts than existing models, then the new model should be compared to the best existing models, not to obsolete models such as GPT2.

\personal{The above points may seem obvious, but I see many research papers which evaluate unimportant but easy-to-measure quality criteria, and/or compare a new system against obsolete baselines.  Such experiments are easy to run and produce impressive results, but they are not useful either scientifically or practically.}

\subsection{Replication}\label{sec:replication}\index{replication}
Scientific experiments need to be \emph{replicable}.  In other words, other researchers should be able to repeat an experiment and get similar results.  This is fundamental to the scientific method; experiments which cannot be replicated have limited scientific utility and validity.

Much has been written about replicability in different fields of science \cite{WikipediaReplication}.  In NLG, replication requires very detailed information about the system and experiment, including how data is preprocessed, the exact libraries and options used for automatic evaluation, the exact user interface\index{user interface} used for human evaluation, etc. Seemingly small differences can lead to big differences in outcomes \cite{belz-etal-2021-systematic}.

Various data sheets\index{data sheet} have been proposed to help gather this information \cite{gebru2021datasheets,shimorina-belz-2022-human}.  Unfortunately, at least in my experience it is very rare for all the necessary details to be provided in papers and associated material such as data sheets.  Usually researchers who want to repeat an experiment need to contact the original authors in order to get all the details.

It is important to keep in mind that \emph{exact} replication of experimental results is unlikely, because of experimental noise.  Certainly replicated experiment with human subjects rarely give exactly the same results, even if they use the same subjects\index{subjects}, because people do not respond 100\% consistently.  For example, if a subject is hungry and tired, she may give lower ratings to texts than if she is well fed and rested.  Another source of noise is random seeds\index{random seeds}; any system whose output is influenced by a random seed (which includes most neural NLG systems) may generate different outputs on different runs.

So we expect to see some difference in the exact numbers if an experiment is replicated.  However, replications should not lead to different statistically-significant\index{statistical!significance} outcomes at the hypothesis level.  For example, if the original experiment claimed that system A produced more accurate texts than system B, with the difference being statistically significant (Section~\ref{sec:statisticalsig}), then it would be very worrying if a replication found that system A produced \emph{less} accurate texts than system B, with the difference being statistically significant.  Such a finding would strongly suggest that that there are serious flaws in how the experiment was designed, executed, or reported.

Unfortunately, some published NLG evaluations, even in good venues, do not seem to be replicable.  For example, as part of a shared task on replication \cite{repronlp2023}, several labs tried to reproduce published human evaluations.  The worst result was for a paper which asked crowdworkers to count the number of content errors in a text \cite{puduppully-lapata-2021-data}; this paper re-used an evaluation methodology from an earlier paper \cite{wiseman-etal-2017-challenges}.  Table~\ref{tab:badreplication} shows one result (number of errors in a 4-sentence extract from a corpus text) from the original paper, the reproduction studies, and a study which measured the same thing (number of errors) using a different and better methodology.  The large variation in results (the highest number is 25 times larger than the lowest) raises serious concerns about the validity of the experiment, and suggests that we should not evaluate systems by asking crowdworkers to count content errors.

\begin{table}
\begin{tabular}{|l|l|r|}
\hline
\textbf{Paper} & \textbf{Note}  & \textbf{Mean number errors} \\
Original paper \cite{puduppully-lapata-2021-data} &	Original & 0.07 \\
Gonzalez-Corbelle et al \cite{gonzalez2023}	& Replication	 & 0.66\\
Watson and Gkatzia \cite{watson2023} & 	Replication &1.52\\
Watson and Gkatzia \cite{watson2023} & 	Replication with different subjects	& 0.06\\
\hline
Thomson et al \cite{thomson-csl23} & More rigorous measurement & 0.50 \\
\hline
\end{tabular}
\caption{Mean number of content errors in four-sentence extracts from corpus (human-written) texts, as measured by asking crowdworkers to count errors.   Result from original paper and two replications (one of which repeated the study using in-house subjects instead of crowdworkers). Thomson et al \cite{thomson-csl23} (Figure~\ref{fig:ThomsonAnnotation}) measured the errors using a more rigorous annotation methodology; they reported 1.58 errors on average in a human corpus text, which is approximately 0.5 errors per four-sentence extract.}
\label{tab:badreplication}
\end{table}

\personal{One of the frustrations I have with the paper being replicated \cite{puduppully-lapata-2021-data} is its usage of an old evaluation methodology \cite{wiseman-etal-2017-challenges}, when newer and better evaluation techniques were available \cite{thomson-reiter-2020-gold}.  I have seen many other cases where papers use the latest technology to \emph{build} an NLG system, but outdated techniques to \emph{evaluate} the system.  Sometimes this happens because authors want to enter their system into public \scare{leaderboards} which show the best-performing system at a specific task as assessed by a specific evaluation technique.  Leaderboards which are based on obsolete evaluation techniques are not helpful to scientific progress.}


\subsection{Ecological validity: Artificial vs real-world context}\label{sec:ecologicalvalidity}
We can do an evaluation in either artificial or real contexts; this is sometimes called \emph{ecological validity}\index{ecological validity}.  For example, when comparing translated texts, we can either get translators to assess them in a generic way, or we can ask people who genuinely need the information to use the translations for real and then assess their effectiveness.

For instance, as mentioned in Section~\ref{sec:stakeholders}, different stakeholders\index{stakeholders} in Babytalk\index{Babytalk} had different perspectives on evaluation.  The psychologists\index{psychologists} wanted to do evaluations in controlled artificial settings, because this reduced the number of confounding factors when testing the impact of different media on decision making; they believed this was more important than evaluating the system in real-world usage.  However, the doctors\index{doctors} believed that evaluating systems in real clinical usage was essential, because this gave a much better understanding of utility\index{utility} and potential impact\index{impact}; they did not trust evaluations in artificial settings. The project ended up doing both types of evaluations \cite{PORTET2009789,HUNTER2012157}.

Ecological validity is especially important if the goal is to assess real-world impact (Section~\ref{sec:impactevaluation}).  The real-world is a messy place, and we cannot assess real-world impact if we ignore this messiness.

\personal{Evaluations in real-world contexts are still rare in NLG at the time of writing, which is a shame; I hope they become more popular.}


\subsection{Test data\index{test data}: representative, different from training data\index{training data}}\label{sec:trainingtestdata}
When evaluating an NLG system, researchers need to decide which test data (scenarios) the system will be tested on; from an experimental design perspective (Secttion~\ref{eval-gen-exp-design}), this is part of choosing material\index{material}.   Test data should be real data or in some cases a mix of real and synthetic data; it should not just be synthetic data\index{synthetic data} (Section~\ref{sec:syntheticdata}).  It should also be representative of real usage.

For example, a system which summarises doctor-patient consultations should be tested in a wide variety of such consultations; it should  not be tested on pharmacist-patient consultations.  Obtaining representative test data\index{representativeness} can be challenging, especially in use cases (such as medicine) with strong data protection\index{data protection} constraints, but it is essential for meaningful evaluation.

For NLG systems built using machine learning, it is also essential that they not be tested on their training data; this is a fundamental principle of machine learning (systems which are tested on training data can get perfect scores on evaluations simply by memorising their training data \cite{schaeffer2023pretraining}).  For example, if we are evaluating texts produced by Google and Bing Translate, we should ensure that these texts are not part of the training data used to build these systems.  This is called \emph{data contamination}\index{data contamination} \cite{sainz-etal-2023-nlp}.

Unfortunately, the emergence of large language models\index{large language models} which are trained on the Internet (Section~\ref{sec:promptedmodels}) has made it much harder to guarantee that test data was not present in the system's training data.  Prompted language models can also in some case memorise or learn from test data that is provided in prompts\index{prompt} as examples (Section~\ref{sec:fewshot}) or when models are used to evaluate texts (Section~\ref{sec:gemba}) \cite{balloccu-etal-2024-leak}.

I have seen many evaluations of LLMs where it seemed very likely that the test data was part of the model's training data or prompts, and also many evaluations where this issue was unclear. One problem is that commercial LLMs generally do not reveal their training data, and also are constantly being updated.  Open-source LLMs are better from this perspective, and usually clearly state what data they were trained on.  For this reason, it is usually easier to evaluate open-source LLMs than proprietary commercial ones.


\section{Human Evaluation}\label{sec:humanevaluation}
In general, NLG systems can be evaluated by people (human evaluation), by algorithms (metrics), or by assessing real-world impact.
If an  impact evaluation\index{impact!evaluation}  is not possible, then a careful and well-designed human evaluation is usually the best way to meaningfully evaluate an NLG system. However a good metric evaluation is better than a poorly designed or executed human evaluation.

In this section we look at different types of human evaluation; discuss how such evaluations should be designed, executed, and evaluated; and give some examples.

\subsection{Types of Human Evaluation}\label{eval:humanevaltypes}
There are many types of human evaluations, with more being introduced every year.  But in rough terms, we can distinguish between evaluations based on (A) subjective ratings or rankings; (B) error annotation; and (C) task performance (such as decision making).

\subsubsection{Human evaluation based on ratings or rankings}\label{sec:humanevalrankings}
The most common form of human evaluation in NLP  asks human subjects to rate\index{rating} texts and/or rank\index{ranking} a set of texts, based on one or more quality criteria (Section~\ref{req-criteria}).  This evaluation is based on the subject's subjective opinion.

The conceptually simplest approach is to give subjects two or more texts, and ask them to rank them according to the chosen quality criteria.  Figure~\ref{fig:SumTimeRanking} shows an example of this, which was used in the evaluation of the SumTime\index{SumTime} weather forecast generator \cite{REITER2005137}.  SumTime generated marine weather forecasts for workers in the offshore oil industry, and this evaluation asked users to rank two texts that described the predicted wind speed (at 10 meters altitude).   The numeric wind prediction (produced by a supercomputer running atmosphere simulations) was given in a table; this is in the input to the NLG system.  In this example, text (a) was written by a human forecaster, indeed it was extracted from an actual weather forecast for the offshore oil industry.  Text (b) was produced by the SumTime NLG system.  The user was asked to say which text was preferred based on three different quality criteria (Section~\ref{req-criteria}): easiest to read, most accurate, and most appropriate; they could also add comments about the texts.
\begin{figure}
\lineacross//
\includegraphics[scale  = 0.75, trim={2cm, 10cm, 2cm, 3cm},clip]{sumtime-exp-short}
\caption{Example of ranking experiment in SumTime}
\label{fig:SumTimeRanking}    
\lineacross   
\end{figure}

Many evaluations of this type have been reported in the literature.  Most follow the above structure:
\begin{itemize}
\item The user is given the input to the NLG system (if appropriate).
\item The user is given two or more texts produced from the input (sometimes including a human-written text, as was done in SumTime); the user is not told the source of the texts
\item The user is asked to rank texts on one or more quality criteria.  Sometimes a \scare{same} or \scare{no difference} option is given, sometimes the user is forced to make a choice.
\item Sometimes the user can optionally write free-text comments about the texts.
\end{itemize}
It is good practice to randomise the order of the texts.  For example, if the user is shown one human text and one NLG text, then  choice (a) should sometimes be the human text and sometimes be the NLG text.

Of course different user interfaces can be used; for example users can be asked to drag texts into a ranked order, instead of using checkboxes.  The SumTime experiment, incidentally, was done on paper, because (at the time it was run) subjects on oil rigs and supply boats did not always have easy access to computers or the Internet.

Human subjects find it difficult to rank large numbers of texts.  Therefore if the experiment requires comparing more than 5 texts, the usual practice is to ask different subjects to rank different subsets of the entire collection of texts, and combine these using an algorithm such as TrueSkill\index{TrueSkill} \cite{herbrich2006trueskill,sakaguchi-etal-2014-efficient}.

Another popular approach is to ask subjects to \emph{rate} texts.  An example is shown in Figure~\ref{fig:SumTimeRating}, from \cite{reiter-belz-2009-investigation}.  This is in the same domain as Figure~\ref{fig:SumTimeRanking}, generation of marine weather forecasts for the offshore oil industry, but this experiment (which was online) asked users to rate the quality of the text on two quality criteria (clarity/readability, accuracy/appropriateness), it does not ask users to explicitly compare texts.  Texts produced by different systems are rated separately.

\begin{figure}
\lineacross//
\includegraphics[scale  = 0.65,trim={0 0 2cm 2.75cm},clip]{belzsumtime}
\caption{Example of rating experiment for SumTime, used in \cite{reiter-belz-2009-investigation}}
\label{fig:SumTimeRating}
\lineacross
\end{figure}

Likert scales\footnote{\url{https://en.wikipedia.org/wiki/Likert\_scale}}\index{Likert scale} (usually 5 or 7 points) are often used for rating texts.  Another option is \emph{magnitude estimation}\index{magnitude estimation}, where users are asked to position a slider on a best-worst scale, instead of choosing a discrete option.

\subsubsection{Evaluations based on annotations\index{annotation}}\label{sec:humanevalannotations}
Human evaluations can also be done by asking subjects to \emph{annotate} errors and other problems in a text.   Annotators often have some domain expertise\index{domain!experts}, and may be asked to include error type and/or severity\index{errors!severity} in their annotations.   Once a text has been annotated, an overall score can be computed based on the number of errors, possibly weighted by type and severity.  The error distribution across types and severity, and indeed the individual error annotations, can provide useful guidance for developers who wish to improve a system.

\begin{figure}
\newcommand{\err}[1]{{\color{red} \ul{#1}}}
\lineacross{}
The Memphis Grizzlies (5-\err{2}) defeated the Phoenix Suns (3 - 2) \err{Monday} 102-91 at the \err{Talking Stick Resort Arena} in Phoenix. The Grizzlies had a \err{strong} first half where they \err{out-scored} the Suns \err{59}-\err{42}. Marc Gasol scored 18 points, \err{leading} the Grizzlies.  \err{Isaiah Thomas added} 15 points, he is \err{averaging 19 points on the season so far}.  The Suns' next game will be \err{on the road} against the \err{Boston Celtics} on Friday.

\vspace{5mm}
List of errors:
\begin{itemize}
    \item \err{2}: incorrect number, should be 0.
    \item \err{Monday}: incorrect named entity, should be Wednesday.
    \item \err{Talking Stick Resort Arena}: incorrect named entity, should be US Airways Center.
    \item \err{strong}: incorrect word, the Grizzlies did not do well in the first half.
    \item \err{out-scored}: incorrect word, the Suns had a higher score in first half.
    \item \err{59}: incorrect number, should be 46.
    \item \err{42}: incorrect number, should be 52 .
    \item \err{leading}: incorrect word,  Marc Gasol did not lead the Grizzlies, Mike Conley did with 24 points.
    \item \err{Isaiah Thomas added}: context error, Thomas played for the Suns, but context here implies he played for the Grizzlies and added to their score.
    % \item \err{led}: incorrect word.  Thomas did not lead the Grizzles since he played for the Suns.
    % \item \err{Isaiah Thomas}: Context error.  Thomas played for the Suns, but context here implies he played for the Grizzlies.
    \item \err{averaging 19 points in the season so far}: Not checkable.  Data sources report performance per season and per game, not performance at a particular point in a season.
    \item \err{on the road}: incorrect word, The Suns will play at home.
    \item \err{Boston Celtics}: incorrect named entity, the Suns will play the Sacramento Kings
    
\end{itemize}
\caption{Example text with error annotations from \cite{thomson-reiter-2020-gold}.  Each annotation includes an error type and a correction.  Annotators can add explanations where useful. Box score data for this game is available at
\url{https://www.basketball-reference.com/boxscores/201411050PHO.html} .}\label{fig:ThomsonAnnotation}   
\lineacross{}
\end{figure}


An example is shown in Figure~\ref{fig:ThomsonAnnotation}.  This was part of the training material given to subjects who were asked to evaluate the factual accuracy of sports stories generated by neural NLG systems \cite{thomson-csl23}.  Annotators were asked to find inaccurate statements, classify them (incorrect number, incorrect named entity, incorrect word, context error, not checkable, other), and also (if possible) correct the statement.

At the time of writing, the most popular annotation scheme is MQM\index{MQM} \cite{FreitagMQM}, which is used to annotate machine translation outputs.  It is a more complex annotation scheme than the one in Figure~\ref{fig:ThomsonAnnotation}, which annotates linguistic as well as content errors, and also assigns a severity to errors.

Annotation schemes can be domain-dependent.  For example Morarmarco et al \cite{moramarco-etal-2022-human} use an annotation scheme as one of several evaluation techniques for Note Generator\index{Note Generator} (Section~\ref{IntroConsultationSummary}), which generates summaries of doctor-patient consultations. The annotation scheme includes domain-specific categories such as \scare{Use of not universally recognised [medical] acronyms}.  Similarly Magesh et al \cite{magesh2024hallucinationfree} describe an annotation scheme for the output for AI-driven legal research tools, which includes groundedness (claims are supported by citations to relevant legal documents) as well as correctness.

Some researchers use annotations to evaluate human-written texts as well as computer-generated texts \cite{FreitagMQM,moramarco-etal-2022-human,thomson-csl23}.  This gives  information about the types of mistakes made by human authors\index{human authors} in a domain; this number is often higher than initially expected.

At the time of writing, this is a relatively new form of human evaluation in NLG, so specific annotation schemes are evolving, as are user interfaces for annotation.  Some schemes allow document-level annotations, especially for missing information, in other words important information which should have been included in the text but was not present.

\personal{I believe that evaluation-by-annotation usually gives more meaningful results than evaluation by rating or ranking, and hope to see more such evaluations in the future.}

\subsubsection{Evaluations based on task performance}\label{sec:humantaskeval}\index{task-based evaluation}
Another type of of human evaluation is to ask people to use an NLG system and assess its effect on how well they perform a task.   A simple example was discussed in Section~\ref{sec:gkatziaweather} (Figure~\ref{fig:GkatziaWeather}), where the driver of an ice cream truck decided when to work based on a weather forecast.

A more complex example is the evaluation of the Babytalk BT45\index{Babytalk!BT45} system (Section~\ref{sec:babytalk}) which generated texts that summarised patient record data, and were intended to help doctors make good clinical decisions.  This system was evaluated by running an experiment where clinicians were asked to decide on the best intervention (action), after seeing either a Babytalk NLG text summary of relevant patient data, a human-written summary of the data, or a visualisation \cite{van2010graph,PORTET2009789}.

Note that this is not an impact evaluation\index{impact!evaluation} in the sense of Section~\ref{sec:impactevaluation}, since the experiment is done in artificial context. In the BT45 experiment doctors\index{doctors} were asked to make decisions based purely on patient record data; this is not a real-world task, since when clinicians make decisions about patients in a real hospital, they have access to many other information sources (e.g., observing the patient, talking to nurses, previous interactions they have had with the patient) in addition to patient record data.
But a task-based evaluation in an artificial context nonetheless can give good insights as to how well a system works and where it needs to be improved

It is difficult to generalise about task-based evaluations, they are very different.  One fairly common type of task-based evaluation in NLG is \emph{post-edit-time}\index{post-editing!time} evaluation, where researchers ask human domain experts \index{domain!experts} to check an NLG text and fix problems so that it can be released to real users, and measure how long this checking and editing takes.  Human post-editing is fairly common (Section~\ref{sec:humanchecking}), and the amount of time required to post-edit a text is an important real-world measure of the utility of the NLG system.  However, different people have different post-editing behaviour; some just fix mistakes whereas others rewrite texts into a different style \cite{sripada-etal-2005-evaluation}.  Also post-edit time is very dependent on the user interface\index{user interface} and workflow\index{workflow}, as well as the texts being edited.  So it is a somewhat \scare{noisy} measure of text quality.  If post-edit time is used to compare two NLG systems, it is essential that the two systems be similar from a UI, workflow, and user perspective.

From a practical perspective, task-based evaluations are often more expensive and time-consuming than evaluations based on ratings ot annotations.  However, they can give very valuable insights on how texts influence and help users.


\subsection{Experimental Design}\label{sec:humanevaldesign}\index{experimental design}
Good experimental design is critical for human evaluations, as for other evaluations.  The key steps are the ones described in Section~\ref{eval-gen-exp-design}:
\begin{enumerate}
\item Choose your research hypotheses and questions.
\item Choose the overall study type.
\item Choose subjects.
\item Choose material.
\item Decide on experimental procedure.
\item Decide on analysis.
\end{enumerate}
These are described in more detail below, for human evaluations.  I will use an example of evaluating a system BasketballNLG\index{BasketballNLG} which generates summaries of basketball games. Figure~\ref{fig:basketballDesign} shows a high-level experimental design for this evaluation; the following sections describe each step in more detail.

\begin{figure}
\lineacross\\
\textbf{Research Hypothesis}
\begin{itemize}
\item Texts produced by BasketballNLG are better than texts produced by \emph{StateofArtSystem} under the quality criteria of accuracy and interestingness.
\end{itemize}

\textbf{Study Type:}
\begin{itemize}
\item Annotation-based evaluation of accuracy.
\item Ranking-based evaluation of interestingness.
\end{itemize}

\textbf{Subjects:}
\begin{itemize}
\item 50 subjects evaluate interestingness; ten of these (after additional training and screening) annotate accuracy errors.
\end{itemize}

\textbf{Material:}
\begin{itemize}
\item 20 games (scenarios), chosen at random, for interestingness evaluation.
\item 10 games, chosen at random (but different from above), for accuracy evaluation.
summaries are produced using both BasketballNLG and \emph{StateofArtSystem}.
\end{itemize}

\textbf{Procedure:}
\begin{itemize}
\item Experiment is run on the web.
\item All subjects are given detailed instructions; subjects doing error annotation must complete a training session and get a score of 80\% on a screening test.
\item UI is a simple form for interestingness ranking; subjects see summaries from both BasketballNLG and \emph{StateofArtSystem} and are asked to rank them.  An online version of Microsoft Word is used for annotation; subjects annotate just one summary for each game (ie, either the BasketballNLG summary or the \emph{StateofArtSystem} summary,  not both).
\item A Latin Square design is used to decide which summary (BasketballNLG or \emph{StateofArtSystem}) each subject sees for error annotation. For interestingness, scenarios are randomly ordered, as is the order of the texts being ranked in each scenario (ie, whether the first text is BasketballNLG or \emph{StateofArtSystem})
\item Simple attention check questions are included halfway through the experiment.
\item At the end of the experiment, subjects can (optionally) provide free-text comments on the texts and/or experiment.
\item If circumstances abort an experiment,  data collected so far is kept and subjects are asked to complete the experiment at a later date.
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
\item Outliers are removed using Interquartile Range (IQR) algorithm.
\item Accuracy: Report average error counts for the systems being compared; use a t-test to calculate statistical significance.
\item Interestingness: Report preference split from the interestingness rankings; use a binomial test to calculate statistical significance.
\item Do qualitative error analysis on 5 texts with poor interestingness and 5 texts with poor accuracy.
\item Analyse free-text comments using thematic analysis and report key insights.
\end{itemize}

\caption{Experimental design for human evaluation of BasketballNLG\index{BasketballNLG}}\label{fig:basketballDesign}
\lineacross
\end{figure}

\subsubsection{Step 1: Choose research hypotheses and questions}\label{sec:humanhypotheses}\index{research questions} 
A good experiment cannot be designed without knowing the core hypotheses being tested, such as \scare{people who get STOP\index{STOP} letters are more likely to stop smoking than people who get non-tailored letters}.  In applied NLG, research questions and hypotheses are usually based on user requirements\index{requirements} (Chapter~\ref{requirements}).  They often are of the form that \scare{Texts produced by A are better than texts produced by B under quality criteria\index{quality criteria} C}, for example \scare{Texts produced by SumTim\index{SumTime}e are better than texts produced by human forecasters under the quality criterion of \emph{approriateness}}.   Hypotheses can aso be framed around task-based outcome measures, for example \scare{Doctors who see Babytalk\index{Babytalk} summaries make better clinical decisions than doctors who see visualisations}, or indeed \scare{people who get STOP letters are more likely to stop smoking than people who get non-tailored letters}.

For example, for BasketballNLG\index{BasketballNLG}, one possible set of concrete research hypotheses is that \scare{texts produced by BasketballNLG are better than texts produced by \emph{StateofArtSystem}\index{state-of-art} under the quality criteria of accuracy and interestingness}; this is two hypotheses because it includes two quality criteria.  \emph{StateofArtSystem}   should be replaced by the current best system for doing this task.

\subsubsection{Step 2: Choose the overall study type}\label{sec:humanstudytype}\index{study type}
The most common study types are the ones described above, but other types are also possible, such as measuring reading time.  The choice is determined by the quality criteria and also by pragmatic issues such as time and cost.  In rough terms:
\begin{itemize}
\item Ranking\index{ranking} and rating\index{rating} experiments are cheapest and quickest, and can be used for most quality criteria.  Opinions differ as to which is better.  I personally lean towards ranking in most cases, but other researchers prefer ratings, and indeed the WMT\index{WMT} shared task for evaluating machine translation explicitly shifted from ranking to rating in 2017 \cite{bojar-etal-2017-findings}, after experimenting with both approaches in previous years.
\item Annotation\index{evaluation} experiments are better for assessing accuracy and more generally at finding errors, but they are significantly more expensive and time-consuming than experiments based on ratings and rankings.  For example, the 2022 WMT shared task \cite{kocmi-etal-2022-findings} used the MQM\index{MQM} annotation technique in a limited way to supplement ranking-based evaluation; MQM gave
more meaningful assessments, but it was not feasible from a financial perspective to do all evaluation using MQM.
\item Task-based evaluations\index{task-based evaluation} are even more expensive and time-consuming than annotation evaluations,  but they are the best way to measure the effect of NLG systems on users, and are better at assessing utility and related quality criteria than rankings, ratings, or annotation experiments.  
\end{itemize}

For example, Figure~\ref{fig:basketballDesign} describes a strategy for evaluating BasketBallNLG which uses a ranking-based evaluation for interestingness, and an annotation evaluation for accuracy.  The expense of task-based evaluation is not justified for evaluating sports stories (which are essentially entertainment).  Annotation is better than ranking/rating for accuracy, but is not normally used for interestngness.

\subsubsection{Step 3: Choose subjects}\label{sec:humansubjects}\index{subjects}
Human evaluations require human subjects!  Some experiments can be done by any fluent speaker of the language being generated.  Researchers can recruit subjects for such experiments by asking friends, family members, colleagues, or students; they can also use crowdsourcing\index{crowdworkers} platforms such as Mechanical Turk\index{Mechanical Turk}\footnote{\url{https://www.mturk.com/}} or Prolific\index{Prolific}\footnote{\url{https://www.prolific.com/}}.  Other experiments require subjects with specific domain knowledge\index{domain!knowledge} (e.g., doctors) or attributes (e.g., they smoke); such subjects are usually explicitly recruited, sometimes from collaborating organisations. For example, the clinicians who evaluated Babytalk\index{Babytalk} (as described in Section~\ref{sec:humantaskeval}) were recruited from a specific hospital (Edinburgh Royal Infirmary) which was a collaborator in the Babytalk project.

If subjects are recruited via crowdsourcing, it is important to remember that their incentive is to do tasks as quickly as possible, since this will maximise their income. There is also a danger that crowdworkers will use chatbots to do the tasks, which makes their contribution useless for a human evaluation.  Perhaps in part for these reasons, some crowdworker-based evaluations are difficult to replicate; one example was discussed in Section~\ref{sec:replication}.

\personal{I have unfortunately seen a number of cases where experiments with crowdsourced subjects were useless.  For example the  subjects simply ticked boxes at random without actually doing the experimental tasks, because this was the fastest way to do the task and get paid.}

Sometimes the quality of crowdworker (and other) evaluations can be improved by using techniques such as the following:
\begin{itemize}
\item Adding special \emph{attention check}\index{attention check} questions, which subjects can only answer if they are doing the experiment properly.  If subjects do not answer these correctly, their data is discarded.
\item Checking for \scare{outliers}\index{outlier}, such as subjects doing an experiment incredibly quickly.  If this is detected, again data can be discarded.
\item Recruiting a pool of trusted subjects who will be repeatedly used.  For example, for the annotation experiments reported in \cite{thomson-csl23}, Thomson et al recruited 7 annotators on Mechanical Turk, and asked these annotators to do multiple tasks over a two-year period.  Since annotators liked the work (which was interesting and also well-paid by Mechanical Turk standards), they were motivated to do a good job, in order to keep on getting more such work in the future.
\end{itemize}

A general principle which seems obvious but is sometimes ignored is that researchers should treat subjects well; respect them, answer their questions, pay them (if appropriate), etc \cite{silberman18}.  Besides being ethically\index{ethics} correct, a well-treated subject is more likely to provide high-quality experimental data.

Another general point is that in some cases it is important that subjects are representative\index{representativeness} of a user group.  For example, an evaluation of the utility of computer-generated weather forecasts\index{weather forecasts} should be done with a representative sample of forecast users, and not just with undergraduate students (whose use of forecasts is probably not representative).  In all cases, researchers should report the demographic\index{demographics} mix (age, gender, etc) of their subjects when they write up their experiments.  These can have an impact on what subjects do \cite{gkatzia-etal-2016-natural}, and indeed it may make sense to test whether males and females (for example) evaluate texts differently.

A key question is how many subjects should be recruited.  In theory, this can be answered using a \emph{statistical power calculation}, which computes the number of subjects based on expected effect size (expected difference in measured quality criteria between systems), the statistical test used in analysis, and other such parameters.  However, it is rare to see power calculations used when designing NLG experiments, partially because the necessary information (e.g., expected effect size) is often not available.

In very crude terms, ranking/rating\index{rating}\index{ranking} and task-based evaluations\index{task-based evaluation} usually have at least 20 subjects, especially if subjects do not need specialist expertise. Annotation\index{annotation}-based evaluations, and evaluations with domain experts\index{domain!experts}, can be done with fewer subjects, sometimes as few as 3 or 4.
These are \emph{minimum} numbers; it is much better to have 50 or 100 subjects in a ranking/rating evaluation.

For example, Figure~\ref{fig:basketballDesign} describes a strategy for evaluating BasketBallNLG\index{BasketballNLG} which
uses 50 subjects to rank basketball texts on the basis of interestingness.  From this group, 10 people are asked to annotate texts for accuracy; researchers look for subjects who have good knowledge of basketball, and ask them to pass a screening test.  The test could ask subjects to annotate a summary with known errors, and only accept people whose annotation is 80\% correct (precision and recall compared to the correct annotation).

\subsubsection{Step 4: Choose material}\label{sec:humanmaterial}\index{material}
In most human evaluations of NLG, subjects are shown texts generated in specific \emph{scenarios}\index{scenarios}.  For example, in the Babytalk\index{Babytalk} evaluation doctors evaluated texts generated from patient record data about a specific baby over a specific time period; a scenario here might be \emph{baby JOHNSMITH23 at 1600-1645 on 3 September 2004}.  In the SumTime\index{SumTime} evaluation weather forecast users evaluated forecasts for specific locations on specific days; an example scenario here would be \emph{Brent (offshore oil field) on 1 August 2000}.
Hence an important design issue in human evaluations is choosing the specific scenarios used in the experiment.

One approach is to choose scenarios randomly.  For example, in the SumTime context, if there is forecast data for 10 locations over a 100-day period period, we can randomly select location 5 on day 59, location 1 on day 80, etc (I generated these using a random number generator).

In other cases it is important that scenarios are diverse, and cover important categories and/or edge cases\index{edge cases}.  One possibility in such cases is to randomly select within a category.   For example, in the Babytalk BT45 evaluation mentioned in Section~\ref{sec:humantaskeval}, it was essential that the selected scenarios
covered 8 different recommended interventions (scenarios where the recommended clinical action was to increase oxygen levels,  scenarios where the recommendation was that the doctors do nothing, etc.).   So the researchers created sets of potential scenarios which met each of these criteria (e.g., one set was all potential scenarios where the recommended action was to increase oxygen levels), and then randomly selected experimental scenarios from each of these sets.

As with subject choice, another decision is how many scenarios to use. Sometimes this can  be computed from the number of subjects, the number of judgements (ratings, rankings, or annotations) made by each subject, the number of texts in each scenario (often one per system), and the number of times each text is judged, as follows:
\begin{verbatim}
NumberOfJudgements = NumberOfSubjects * NumberJudgementPerSubject
JudgementsPerScenario = TextsPerScenario * JudgementsPerText
NumberScenarios = NumberOfJudgements / JudgementsPerScenario
\end{verbatim}

For example, for the annotation portion of the BasketballNLG\index{BasketballNLG} evaluation, scenarios can be chosen at random.  In terms of numbers, assume:
\begin{itemize}
\item 5 annotators (subjects).
\item 8 texts annotated by each annotator.
\item 2 texts per scenario (one produced by BasketballNLG, one produced by state-of-art baseline).
\item each text annotated by two people (to reduce the chance of missing something).
\end{itemize}
The above formula suggests 10 scenarios.
\begin{verbatim}
NumberOfJudgements = 5 * 8 = 40
JudgementsPerScenario = 2 * 2 = 4
NumberScenarios = 40 / 4 = 10
\end{verbatim}


\subsubsection{Step 5: Decide on experimental procedure}\label{sec:humanevalproc}\index{procedure}
Experimental procedure includes many different things, such as:
\begin{itemize}
\item \emph{Delivery:} How is the experiment run?  Is it done over the web, online but in an experimental room, or on paper?  Web-based experiments\index{web-based experiments} are usually the easiest to run, but experimenters have more control and can detect problems better if they are in the same room as the subjects; they can also observe subjects, which is sometimes useful.  Some quality criteria\index{quality criteria}, such as reading time, are difficult to accurately measure in web-based experiments.
\item \emph{User Interface\index{user interface} and Training\index{training}:} What user interface (UI) do subjects\index{subjects} use, and what training (if any) are they given to explain the task and UI?  Its important to get this right, since it is essential that subjects understand what they are supposed to do\footnote{\url{https://ehudreiter.com/2024/05/28/human-eval-subjects-must-understand-the-task/}}.  Piloting the experiment\index{pilot experiments} (Section~\ref{sec:pilotexperiments}) can often detect UI problems.
\item \emph{Text selection:} Which texts do subjects see, and in what order?  It is often good practice to use a \emph{Latin Square design}\index{Latin square design} \cite{latinsquare} to ensure that each subject sees a balanced number of texts from each system (since some subjects are more  generous in their ratings or more skilled at the task).  If the experiment involves ranking texts produced by different systems, then the order in which the texts are shown should be randomised (since some subjects may have a preference for preferring the first text that they see).
\item \emph{Attention checks:}\index{attention check} Does the experiment include attention-check questions? These are questions with clear answers whose purpose is to ensure that subjects are taking the experiment seriously and not just clicking at random.  Attention checks are common in web-based experiments using crowdworkers\index{crowdworkers}, where there is a fear that crowdworkers will try to do the experiment as quickly as possible in order to maximise their income (Section~\ref{sec:humansubjects}).  Data from subjects who fail attention checks should be discarded.
\item \emph{Dealing with disruptions:} What should happen if something goes wrong?  For example, a network outage or fire alarm disrupts an experiment, or a subject has to drop out of an experiment because she is not feeling well?  It is not possible to plan for every potential event, but it is worth thinking at least in general terms about how to handle such disruptions.
\end{itemize}

\noindent{}Figure~\ref{fig:basketballDesign} includes a high-level experimental design for evaluating BasketBallNLG\index{BasketballNLG}.
Of course many details need to be fleshed out, such as the exact design of UI forms.

\subsubsection{Step 6: Decide on Analysis}\label{sec:humananalysis}\label{sec:outlier}\index{analysis}
Another key experiment design issue is how experimental data will be analysed;
 this should be decided \emph{before} the experiment is run and data is collected.  Analyses should focus on testing hypotheses\index{hypothesis testing} (the ones chosen in Step 1).  From an insight\index{insights} perspective, it is important to qualitatively as well as quantitatively analyse the experimental data.

Most NLG hypotheses are stated in terms of system A having a higher score than system B.  Score should of course be computed and reported, but it is also important to identify and remove outliers and to perform statistical significance testing.

An \emph{outlier}\index{outlier} is a data point that is very different from the other data points.  Sometimes outliers are genuine; if so, examining them can lead to major insights about hypotheses.  However, outliers often signify experimental flaws, for example a subject\index{subjects} who did not take the experiment seriously (and just clicked things at random), or who misunderstood the task.  Such outliers are not genuine data about the hypothesis, and should be discarded.   Sometimes it makes sense to identify subjects (as well as individual data points) who are outliers, and discard all data from this subject.

 It is dangerous for experimenters to identify and discard outliers on an ad-hoc basis, because the experimenter may then (perhaps unconsciously) be tempted to discard \scare{inconvenient} data points that go against the experimental hypothesis.  For this reason, researchers should specify an outlier identification policy \emph{before} running the experiment, such as the widely used \emph{IQR}\index{IQR} method\footnote{\url{https://en.wikipedia.org/wiki/Interquartile\_range}}.

Once outliers have been removed, researchers can compute the mean (or median) score for each system.   Statistical significance\index{statistical!significance} (Section~\ref{sec:statisticalsig}) of the differences in scores should also be computed.

Different statistical tests\index{statistical!tests} are used in different experiments \cite{dror-etal-2018-hitchhikers}.   Some simple advice which works for many (not all) experiments is:
\begin{itemize}
\item When comparing numerical scores, such as the number of errors in a text, use a \emph{t-test}\index{t-test} if comparing two systems, and an \emph{ANOVA}\index{ANOVA} if comparing more than two systems.
\item When comparing Likert-scale\index{Likert scale} ratings\index{rating} of texts in a ratings experiment, use a \emph{Mann Whitney test}\index{Mann-Whitney} test if comparing two systems, and a \emph{Kruskall-Wallis}\index{Kruskall-Wallis test} test if comparing more than two systems.
\item When comparing preferences in a ranking\index{ranking} experiment, use a \emph{binomial} test\index{binomial test} if just two systems are being compared (ignore \scare{no difference} choices if these are allowed).  If more than two systems are being compared, use  a \emph{Friedman}\index{Friedman test} test.
\item When comparing categorical outcomes (eg, how many people stopped smoking in different groups), use a \emph{chi-square} test\index{chi-square test} (regardless of the number of systems being compared).
\end{itemize}
If in doubt about which statistical test to use, you should consult with a statistician. 

Once the main hypotheses have been tested, researchers can also look for other interesting patterns in the data.  This is often done using Exploratory Data Analysis\index{Exploratory Data Analysis} (EDA) techniques \cite{tukey1977exploratory}, where a researcher explores the data and looks for patterns using a general-purpose visualisation and analysis tool such as R (people with limited programming expertise often use Microsoft Excel).   When reporting results, it is important to distinguish between formal resuts from hypothesis testing, and additional insights from EDA.

In addition to numerically analysing data, it is also important to \emph{qualitatively} analyse data.  One technique which I recommend is \emph{qualitative error analysis}\index{qualitative error analysis}; this involves taking a few example texts (often including texts with poor scores), and qualitatively analysing them.  For example, if the experiment is based on annotating\index{annotation} errors (Section~\ref{sec:humanevalannotations}), then the quantitative analysis will be based on counting errors (perhaps weighted by severity), while the qualitative analysis explores specific individual errors which shed light on what can go wrong.
If the experiment is based on Likert ratings of readability (Section~\ref{sec:humanevalannotations}), the quantitative analysis is usually based on mean score, while the qualitative analysis could discuss specific individual texts which got low Likert ratings from subjects.
Qualitative analysis does not prove or disprove hypotheses, but it does give insights on what the problems are in generated texts, which can help developers improve the relevant systems.

In many experiments subjects are asked to provide free-text comments as well as specific ratings, rankings, annotations, etc; I definitely recommend doing this when possible.  These free-text comments are another good source of insights into problems and areas for improvement.

If subjects provide free-text comments, these can be analysed using a thematic analysis\index{thematic analysis} \cite{braun2012thematic}.  Another approach is simply to read the comments and identify ones which provide interesting insights about the texts and hypotheses; these can be summarised in the experimental report, including quotations from especially interesting comments.  When doing this, it is important to ensure that quotations are anonymous and do not reveal the authors identity.

Figure~\ref{fig:basketballDesign} includes a possible analysis plan for BasketballNLG\index{BasketballNLG}.

\subsection{Issues in human evaluation}

Regardless of the experimental design and type of evaluation, there are a number of generic issues which are important in human evaluation.

\subsubsection{Interannotator agreement}\label{sec:IAA}\index{inter-annotator agreement}
It is good practice in human evaluation to ask multiple subjects to assess the same texts, and then measure how well they agree.  Ie, if subjects A, B, and C evaluate a text T under the relevant quality criteria, how likely are they to agree, for example to all say that the text is excellent?  This is called \emph{inter-annotator agreement}.

Interannotator agreement is most commonly measured using a \emph{kappa}\index{kappa} score, typically Cohen's kappa\index{kappa!Cohen's} for agreement between 2 annotators and Fleiss kappa\index{kappa!Fleiss} for agreement between more than two annotators; Krippendorff's alpha\index{Krippendorff's alpha} can also be used.  McHugh \cite{mchugh2012interrater} gives a nice summary which is accessible to non-statisticians of kappa and related measures, how they are used to measure annnotator agreement, and some issues and concerns.  Most statistical software packages include functions for calculating kappa and alpha, and it is good practice in human evaluations to report a kappa or alpha score.

If the experimental design specifies that most texts are only evaluated by a single subject, then it may make sense to get a subset of texts annotated by multiple annotators, so that interannotator agreement can be reported.

The kappa statistic\index{kappa!statistic} is a number between 0 and 1. McHugh \cite{mchugh2012interrater} suggests interpreting kappa as follows:
\begin{itemize}
\item above 0.90: \emph{Almost perfect}
\item 0.80-0.90:  \emph{Strong}
\item 0.60-0.79:  \emph{Moderate}
\item 0.40-0.59:  \emph{Weak}
\item 0.21-0.39:  \emph{Minimal}
\item 0-0.20:  \emph{None}
\end{itemize}
Many NLP researchers use a more lenient interpretation, where for example a kappa statistic of 0.4 is considered to be Moderate rather than Weak.  Of course the interpretation of kappa depends on the circumstances and type of experiment, but in general I recommend that researchers use McHugh's more conservative interpretation.

If interannotator agreement is weak (less than 0.60), this means that different annotators are assessing texts quite differently, which is not good; replication\index{replication} (Section~\ref{sec:replication}) may be less successful if agreement is low.  Of course human beings are different, which means that no two individuals will assess a text in exactly the same way, but nonetheless we have more confidence in the result of a human evaluation if there is good agreement between annotators

Agreement can often be increased by changing the experimental design\index{experimental design}.   For example, adding attention checks to filter out subjects\index{subjects} who are not taking the experiment seriously, and/or providing training\index{training} and clear guidance\index{guidelines} to subjects.   Agreement is also often higher for simple experimental tasks than more complex tasks.

\personal{In my experience, inter-annotator agreement is usually higher for annotation-based evaluations (Section~\ref{sec:humanevalannotations}) than for rating/ranking evaluations (Section~\ref{sec:humanevalrankings}), which is one reason I prefer not to use rating/ranking evaluations if I have a choice.}

\subsubsection{Piloting experiments}\label{sec:pilotexperiments}\index{pilot experiments}
Experiments need to be carefully designed and also executed\index{experimental execution}, as described in Section~\ref{eval:experimental-execution}.  Since human experiments are expensive and time-consuming, it usually makes sense to first conduct a small-scale \emph{pilot} experiment, and use this to investigate problems such as code bugs\index{bugs!code}, randomisation errors, and confusing user interfaces\index{user interface}.  For example, researchers can use a pilot to check:
\begin{itemize}
\item Are the correct texts shown to users \cite{thomson-cl24}? 
\item Does the experimental user interface (UI) make sense to subjects\index{subjects}, and do they use it correctly?
\item Is all of the expected experimental data recorded? 
\item Does the analysis\index{analysis} (including outlier\index{outlier} detection) make sense?
\end{itemize}
\personal{We once ran a fairly expensive (and impossible to replicate) experiment where we discovered after completion that some key data had not been recorded \cite{williams_reiter_2008}.  If we had done a pilot first, we would have realised this and fixed our software before running the experiment!}

Please remember that the point of a pilot is not to gather data, but rather to debug the experiment!  Sometimes pilot data can be interesting, but the primary objective is to ensure that the experimental design\index{experimental design} works and that there are no execution errors.  If you detect problems in a pilot and fix them, you may want to consider doing another pilot, since it is possible that fixing one set of bugs has inadvertently introduced new bugs.

I usually do pilots with 5-10 subjects, but it is possible to do useful pilots with just 2 or 3 subjects.  However experimenters should not do pilots using themselves as subjects, it is better to get other people to be subjects.   This is because experimenters know what is supposed to happen in an experiment, and thus (for example) may not detect confusing instructions or user interfaces.




\subsubsection{Research ethics}\index{ethics}\label{sec:researchethics}
Experiments with human subjects must be \emph{ethical}.  Amongst other things:
\begin{itemize}
\item Experiments should not harm subjects, third parties, or researchers. For example, experiments should not normally give subjects misleading medical advice, or show them pornographic or otherwise offensive material (which could happen if subjects are shown random material from the Internet). 
\item Experiments should not lead to sensitive personal data being published or otherwise released.
\item Human subjects must agree to participate in experiments, they should be not be forced or coerced (for example, managers cannot order subordinates to take part in an experiment).
\end{itemize}
In many contexts, experiments must be formally approved by a \emph{research ethics} committee\index{research ethics committee} (called \emph{institutional review board}\index{institutional review board} (IRB) in USA).   Details depend on the type of experiment, the location,  whether the experimenter works for a university or a company, and who is funding the research. Researchers therefore should consult locally about research ethics procedures and rules.

Even when formal ethical approval is not legally required, it is good practice to consider if an experiment could inadvertently harm people or otherwise be unethical.  Indeed, I know of companies which are not legally required to review ethical issues in experiments, but have still implemented very strict review procedures, partially to minimise legal\index{legal} (Section~\ref{sec:legal}) and reputational risks if an experiment harms subjects or third parties.

Experiments may be modified in order to reduce ethical concerns; this is part of the experimental design\index{experimental design} process.  For example, material which experimenters would like to show to subjects can be checked beforehand for offensive content, so that only non-offensive material is actually shown to subjects.

In Aberdeen University in 2024, all experiments which involve human subjects must be approved by a research ethics committee.  Some types of medical-related experiments must be approved by an NHS\index{National Health Service} (UK National Health Service) ethics committee; other experiments are approved by an ethics committee within the university.   Approval typically takes a few weeks for straightforward experiments, which includes most rating, ranking, and annotation experiments.  Approval can take longer for task-based experiments\index{task-based evaluation} which impact a user's actions or behaviour, experiments which involve children or other vulnerable populations, experiments which require gathering sensitive personal information, and experiments where subjects are deceived.

\personal{I recently discussed with a colleague ethical review procedures for an experiment I planned to do with collaborators in China.  My colleague had previously done experiments in China, and explained that he had not needed to do an ethical review.  However, ethical review requirements in China are changing and reviews will be required for this type of experiment in the future; fortunately we realised this in time.}

\section{Automatic evaluation}\label{sec:automaticevaluation}
It is also possible to evaluate texts automatically (without human involvement), by using \emph{metrics}\index{metrics}, that is algorithms that assess the quality of a generated text.  These can either directly assess quality criteria\index{quality criteria}, or they can measure how similar  generated texts are to \emph{reference texts}\index{reference texts}, which are usually high-quality human-written texts in a corpus\index{corpus}.

Automatic evaluation is much cheaper and quicker than  human evaluation, it is also usually easier to replicate\index{replication}.  The problem is that the results of automatic evaluation are not always meaningful predictors of real-world text quality and utility; metrics should be \emph{validated}\index{metrics!validation} to assess how well they agree with trusted high-quality human or impact evaluations (Section~\ref{sec:validationmetrics}).

Sometimes researchers carry out both an automatic evaluation on a large test set and a human evaluation on a subset of the test data.  The human-evaluated subset can be randomly selected; readers will have more confidence in the automatic evaluation if it broadly agrees with the human evaluation.  Another strategy is to use an automatic evaluation to find cases where the NLG system seems to do poorly, and get these evaluated by humans in order to get a deeper understanding of what the problems are.

A large number of evaluation metrics have been proposed in the literature, with new ones coming out every month, so the discussion in this section will focus on principles instead of specific metrics.



\subsection{Types of automatic evaluation}

\subsubsection{Reference-based metrics}\label{sec:refmetrics}\index{metrics!reference-based}
The most common type of automatic evaluation (at the time of writing) compares generated texts against human-written reference texts\index{reference texts}.   The simplest such metric is \emph{edit distance}\index{edit distance} which simply counts how many edits (changes) need to be made to a generated text in order to match the reference text; this can be computed at either word or character level.  An example is shown in Figure~\ref{fig:SumTimeMetric}, using the same SumTime weather domain as Figures \ref{fig:SumTimeRanking} and \ref{fig:SumTimeRating}. The BLEU\index{BLEU}  (Section~\ref{sec:bleu})  and ROUGE\index{ROUGE} \cite{lin-2004-rouge} metrics, which were introduced in the early 2000s but are still popular, essentially apply a more sophisticated scoring formula (based on n-grams) to word-level differences between the generated and target texts.  The chrF\index{chrF} metric \cite{popovic-2015-chrf} takes a similar approach but looks at character-level differences instead of word-level differences; it may be the best of the simple string-similarity metrics \cite{kocmi-etal-2021-ship}.

\begin{figure}
\lineacross\\
\emph{Reference text:}\\
SSW 16-20 GRADUALLY BACKING SSE THEN BECOMING VARIABLE 10 OR LESS BY MIDNIGHT\\
\\
\emph{Generated text:}\\
SSWLY 16-20 GRADUALLY BACKING SSELY THEN DECREASING VARIABLE 4-8 BY LATE EVENING\\
\\
\emph{Differences:}\\
SSW\st{LY} 16-20 GRADUALLY BACKING SSE\st{LY} THEN \st{DECREASING} \emph{BECOMING} VARIABLE \st{4-8} \emph{10 OR LESS} BY \st{LATE EVENING} \emph{MIDNIGHT}.\\
\\
\emph{Edit count:}
\begin{itemize}
\item Two deletions of \st{LY} (one token deleted, twice)
\item \st{DECREASING} changed to \emph{BECOMING} (one token changed)
\item \st{4-8} changed to \emph{10 OR LESS} (three tokens changed)
\item \st{LATE EVENING} changed to \emph{MIDNIGHT} (two tokens changed)
\item No tokens added
\end{itemize}
Token-level edit distance is \emph{8} tokens deleted, changed, or modified\\
Character-level (Levenshtein\index{Levenshtein distance} \cite{Levenshtein}) edit distance is 27\\

\caption{Example of edit-distance evaluation in weather domain}
\label{fig:SumTimeMetric}       
\lineacross
\end{figure}

Recent reference-based metrics such as BLEURT\index{BLEURT} (Section~\ref{sec:bleurt}) and BERTScore\index{BERTScore} \cite{zhangbertscore} are usually trained.  In other words, instead of identifying differences and applying a scoring algorithm,  they use a machine learning model which is trained on human evaluation results to predict the quality of a generated text from a reference text.
Amongst other things, this means that these metrics can consider semantic similarity as well as string similarity.  

As explained by Kocmi et al \cite{kocmi-etal-2021-ship}, trained metrics give judgements which are more similar to human evaluations than string-based metrics.  On the other hand, the results of string-similarity metrics are easier to interpret, which may be useful for developers who are trying to improve an NLG system.

Some metrics, including BLEU, can use multiple reference texts, which allows for cases (common in NLG) when there are many acceptable output texts.

Reference-based metrics of course require good-quality reference texts.  Unfortunately, in some cases insufficient attention is paid to the quality of reference texts.  More fundamentally, if the NLG system is capable of generating human-quality (or better than human texts), then it may not make sense to evaluate a  generated text by comparing it to a human-written reference text.

\subsubsection{Referenceless metrics}\index{metrics!referenceless}
Generated texts can also be evaluated using metrics or algorithms which do not require reference texts.  A simple example is the Flesch-Kincaid\index{Flesch-Kincaid} reading grade level \cite{kincaid1975derivation}, which assesses the readability of a text by giving a US \scare{grade level}; for example if a text gets a grade level of 9, then it should be readable by the average 15 year old.  The score is calculated using a linear regression on the mean number of words per sentence, and the mean number of syllables per word; these coefficients were originally calculated by fitting the regression model to a dataset of how well children of different ages could read different texts (as determined by educational tests).

Recently much more sophisticated models have been built, using modern machine learning techniques, which estimate different quality criteria\index{quality criteria} of generated texts.   Kocmi et al \cite{kocmi-etal-2021-ship} say that the best of these models are almost as good as the best reference-based metrics, at least for evaluating the quality of machine translation\index{machine translation} texts.  At the time of writing, there is a lot of interest and excitement in using large language models such as GPT\index{GPT} to evaluate texts (Section~\ref{sec:gemba}).

\subsubsection{Compute speed, resources and cost}\index{compute!speed}\index{compute!resources}\index{compute!cost}
Most NLG evaluations focus on text quality, but in many contexts computational speed, resources and cost are also important; this is usually measured either as the time taken to generate a text with a given computational resource (eg, GPU), or the cost required to generate a text using cloud-based computational resources.

From an evaluation perspective, speed evaluations can be done using timing functions which are built into Python and other programming languages.  It is good practice when doing a speed evaluation to shut down other processes and even (if possible) disconnect from the Internet; this reduces the amount of \scare{noise} in the evaluation.  From a replicability\index{replication} perspective, when describing the experiment it is essential to give not just full hardware specs, but also details about operating system, libraries used, compilers, and so forth.  It is also good practice to generate each text several times, and report the mean (average) time across runs.

Cost-based evaluations of NLG are relatively unusual at the time of writing, but they can be conducted in the same way, using the metering functionality built into most cloud computing services.   Of course it is not possible to disconnect from the Internet if the NLG system uses cloud computing services.

\subsection{Experimental Design}\label{sec:metricevaldesign}\index{experimental design}

Experimental design for automatic evaluation must address the key steps described in Section~\ref{eval-gen-exp-design}, except that there is no need to choose subjects (since human subjects are not needed for metric evaluations).
\begin{enumerate}
\item Choose research hypotheses and questions.
\item Choose the overall study type.
\item \st{Choose subjects.}
\item Choose material, ie the test set that metrics are calculated on.
\item Decide on experimental procedure.
\item Decide on analysis.
\end{enumerate}

An example is shown in Figure~\ref{fig:basketballMetricDesign}; this is for BasketballNLG,  the same system which was used in the human evaluation example shown in Figure~\ref{fig:basketballDesign}.

\begin{figure}
\lineacross\\
\textbf{Research Hypothesis}
\begin{itemize}
\item Texts produced by BasketballNLG are better than texts produced by \emph{StateofArtSystem} under the quality criteria of accuracy and fluency.
\end{itemize}

\textbf{Study Type:}
\begin{itemize}
\item Reference-based metric
\item Metric is not trained or fine-tuned
\end{itemize}

\textbf{Material:}
\begin{itemize}
\item 100 recent games which were played no earlier than one month before the experiment.
\end{itemize}

\textbf{Procedure:}
\begin{itemize}
\item Use BLEURT metric with standard parameters and no fine-tuning, to evaluate texts produced by BasketballNLG and \emph{StateofArtSystem} for the 100 games.
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
\item Compute system-level score by taking mean of text-level BLEURT scores for texts produced by each system (BasketballNLG and \emph{StateofArtSystem}).
\item Use a paired t-test on text-level scores to compute statistical significance of differences between BasketballNLG and \emph{StateofArtSystem}.
\item Do qualitative error analysis on the 5 BasketballNLG texts that had the lowest BLEURT scores.
\end{itemize}

\caption{Experimental design for metric evaluation of BasketballNLG}\label{fig:basketballMetricDesign}
\lineacross
\end{figure}

\subsubsection{Research hypotheses}\index{research questions}
Similar to human evaluations (Section~\ref{sec:humanhypotheses}), most automatic evaluations test hypotheses of the form \scare{Texts produced by A are better than texts produced by B under quality criterion C}.  Some quality criteria\index{quality criteria}, such as utility, are difficult to measure with metrics\index{metrics}, but others (such as fluency) can often be assessed with metrics.

Most automatic evaluations compare a new NLG system against an existing system.  If the underlying claim is that the NLG system is better than existing systems, then it is essential that the comparison system be a high-quality state-of-the-art\index{state-of-art} system.   

\personal{I have seen many papers which claimed impressive results by comparing a new NLG system against an obsolete and out-of-date comparison system; this is not good science.}

Automatic evaluations can also be used to guide design decisions during the development process.  For example, if developers want to use a prompted model but are unsure which prompt\index{prompt} is best, they can use automatic evaluations to assess how well the system works with different prompts.  In this case the research hypothesis is that a specific NLG system produces better quality texts if it uses prompt XXX instead of YYY.

\subsubsection{Study Type}\index{study type}
There is less variety in study types with metric-based evaluations compared to human evaluations (Section~\ref{sec:humanstudytype}), but some decisions still need to be made:
\begin{itemize}
\item Will reference-based\index{metrics!reference-based} or referenceless\index{metrics!referenceless} metrics be used (or both)?  If reference-based metrics are used, what is the source of the reference texts\index{reference texts}?
\item Are metrics fine-tuned so that they work better in the target domain?  If so, what data will be used to fine-tune\index{fine-tuning} the metrics?
\end{itemize}

\subsubsection{Material}\index{material}
As with human evaluations (Section~\ref{sec:humanmaterial}), scenarios\index{scenarios} must be chosen to evaluate the systems; in machine learning terminology, a \emph{test set}\index{test set} must be created.  But while most human evaluations use a relatively small number of scenarios (typically less than 100), metric-based evaluations can use thousands or even millions of scenarios.  This is one of the strengths of metric-based evaluation, it can assess performance on a much wider range of scenarios than is possible with human evaluation.

It is common practice in machine learning to create a test set by taking a large data set and extracting part of it as test data; the rest is used to train and tune the model. However this is not always a good approach when evaluating systems built on large language models because of data contamination\index{data contamination} concerns (Section~\ref{sec:trainingtestdata}).
A better approach in such contexts is to create test scenarios from new data; for example a basketball-story generator can be tested on data from recent matches.  This is the approach suggested in the experimental design shown in Figure~\ref{fig:basketballMetricDesign}.

If researchers have access to confidential data which was never published on the Internet because of privacy or commercial secrecy, it may be possible to use this to create test data. However it will be difficult for other researchers to replicate the experiment if the test data cannot be published.

\personal{Producing good test data for testing large language models is difficult.  Unfortunately, I have seen many academic papers which ignore this issue and simply test models on data which they were trained on, without even acknowledging that this is a problem.  This is not good science.}

\subsubsection{Experimental procedure}\index{procedure}
A key decision is which specific metrics\index{metrics} will be used.  I cannot give specific advice here because anything I say will be out of date by the time this book is published.   Researchers must also decide on metric parameters and pre-processing techniques such as tokenization; these can have a major impact on metric scores \cite{post-2018-call}.

As mentioned in Section~\ref{sec:replication}, one dilemma is that sometimes researchers want to use similar metrics to previously published work in order to make their results more comparable to previous research, but the metrics used in this older work are less meaningful than newer metrics.  For example, the BLEU\index{BLEU} metric continued to be used long after much better alternatives were available, in part because it made comparison with earlier work easier.   In such cases, researchers can show results both from the best-available metrics and from metrics used in earlier papers.

In some cases, when asked to compare two texts, metrics will be biased and rate the first text more highly because it is first \cite{wang2023large}.  For this reason, it is useful to vary the order in which texts are presented, which is also good practice for human evaluations (Section~\ref{sec:replication}).


\subsubsection{Analysis}\index{analysis}
The best analysis procedure for metrics depends on the metric chosen.  Researchers should check the literature and best-practice guidelines for their chosen metrics when deciding on issues such as the following:
\begin{itemize}
\item If a metric produces scores for individual texts, then these scores are usually aggregated into a system level score.  The obvious approach is to use the mean of the text-level scores as the system score, but there are alternatives, some of which involve weighting some texts (eg, longer texts) more highly than others.  Worst-case\index{worst-case} performance should be reported if requirements say this is important (Section~\ref{sec:req-worstcase}).

\item As with human evaluations, it is essential to perform statistical significance\index{statistical!significance} tests as well as presenting raw numbers.  The best statistical test depends on the metrics, and sometimes specialised tests are needed \cite{koehn-2004-statistical}.

\item Small difference in metric scores, even if statistically significant, may not be meaningful; this is discussed in Section~\ref{sec:validationmetrics}.
\end{itemize}
There are many software packages which automatically run metrics on a test set and analyse the results.  It is fine to use such packages, but researchers should understand how the chosen package analyses the data.

When reporting the experiment, researchers should specify which metrics were used and why they were chosen.  Details should be given about the software used to compute the metric, including parameter settings (if appropriate).  If the metric software came from a library, library details (including version number) should be given.

If a human evaluation was also performed, the correlation between the human and metric evaluations should be reported.

A general point is that is very easy to compute multiple metrics on a data set; some papers report scores for more than ten different metrics when comparing systems.  If this is done, then a multiple-hypothesis correction should be considered.  For example, if ten different metric scores are reported, then a \emph{Bonferroni correction}\footnote{\url{https://en.wikipedia.org/wiki/Bonferroni\_correction}} can be applied by dividing the usual statistical significance threshold of 0.05 by the number of metrics used (10).  A result is only considered to be statistically significant if the p-value is less than the modified threshold, which is 0.005 in this example.

Last but not least, as with human evaluation (Section~\ref{sec:humananalysis}), it can be very useful to do a (manual) qualitative error analysis\index{qualitative error analysis} on individual texts which had poor metric scores.

\subsection{Examples of metrics}
It is impossible to provide an up-to-date list of metrics\index{metrics}, because new metrics are being proposed every month, and any list provided in this book would be obsolete by the time the book is published.  Nonetheless, I will describe a few of of the more popular metrics (at the time of writing), to give readers a better understanding of how they work.

\subsubsection{BLEU}\label{sec:bleu}\index{BLEU}
One of the oldest and best-known reference-based metrics\index{metrics!reference-based} is BLEU \cite{papineni-etal-2002-bleu}.  The core idea of BLEU is to compute n-gram precision.  That is, it counts the number of \emph{n-grams}\index{models!ngram} (Section~\ref{sec:ngram}) in the generated text which also occur in the reference text\index{reference texts}, and divides by the number of n-grams in the generated text.  N-gram refers to word combinations: 1-gram is just words, 2-grams are pairs of words, 3-grams are word triples, etc.

For example, consider the following sentences:
\begin{itemize}
\item \emph{Generated:} It will be very windy on Sunday.
\item \emph{Reference:} It will be windy on the weekend.
\end{itemize}
There are seven words, and hence \emph{unigrams} (1-grams) in the generated sentence: \emph{It, will, be, very, windy, on, Sunday}.  Five of these (\emph{It, will, be, windy, on}) appear in the reference sentence.  So the 1-gram precision of the generated sentence is 5/7 = 0.71 

We can also look at \emph{bigrams} (2-grams) in the generated sentence: there are six of these (\emph{It will, will be, be very, very windy, windy on, on Sunday}).  Three of these are in the reference text (\emph{It will, will be, windy on}), so the 2-gram precision of the generated sentence is 3/6 = 0.50 .

The real BLEU algorithm incorporates a number of adjustments to the core n-gram precision formula, including allowing multiple reference texts to be used and specifying how a system-level score can be calculated from scores for individual texts (like the example above). There are many implementations of BLEU available in NLP toolkits and repositories, and almost all researchers use one of these instead of coding BLEU from scratch.  By far the most common n-gram size is 4, and published BLEU scores are based on 4-grams unless otherwise indicated.

A number of other n-gram-based metrics have been proposed in the literature, including ROUGE\index{ROUGE} and chrF\index{chrF}.  Kocmi et al \cite{kocmi-etal-2021-ship} recommend chrF \cite{popovic-2015-chrf} as the best such metric.

\subsubsection{BLEURT}\label{sec:bleurt}\index{BLEURT}
BLEURT \cite{sellam-etal-2020-bleurt} is a more recent reference-based metric\index{metrics!reference-based} which is trained on data.  In other words, while BLEU essentially uses an explicit algorithm to compute its score, BLEURT uses machine-learning techniques to train a model\index{models} which produces scores.

The details are again complex, but the core idea is to take the BERT\index{BERT} language model \cite{devlin-etal-2019-bert}, and fine-tune\index{fine-tuning} it for the score generation task.  Fine-tuning requires a data set which contains texts and corresponding human evaluation scores. 

Since there is a limited amount of data which provides high-quality human evaluations of generated texts, BLEURT first fine-tunes the model on synthetic data\index{fine-tuning} (Section~\ref{sec:syntheticdata}), which it produces by perturbing Wikipedia sentences (for instance by backtranslation, see example in Figure~\ref{fig:backtranslate}).  It then uses BLEU and other older metrics to estimate the quality of the perturbed sentences.  The quality of this synthetic data set is not great, but it is very large, and tuning on it makes BLEURT more robust.

After the model has been fine-tuned on synthetic data, it is then further fine-tuned on actual (text, human ratings) pairs from one of the WMT tasks.

BLEURT can be used \scare{off the shelf}, or it can be further fine-tuned for a specific NLG task, by giving it appropriate (text, human rating) training data.

\subsubsection{Evaluation using large language models: GEMBA-MQM}\label{sec:gemba}

At the time of writing, there is a lot of interest and excitement in using prompted large language models (such as GPT\index{GPT}) to evaluate the quality of generated texts, by essentially just asking the model to do this in its prompt\index{prompt} (and possibly including some examples).  In some contexts this can give more meaningful results than simple human evaluations \cite{zhang-etal-2023-needle}.
This space is evolving very quickly, so I will just give a representative example, which is GEMBA-MQM\index{GEMBA-MQM} \cite{kocmi-federmann-2023-gemba}.

GEMBA-MQM asks the GPT4 language model to annotate\index{annotation} texts (output of machine translation systems) using the MQM\index{MQM} annotation scheme \cite{FreitagMQM}.  MQM annotation by human translation is (at the time of writing) probably the most rigorous and meaningful evaluation technique in machine translation, but it is expensive and time-consuming.

GEMBA-MQM asks GPT4 to perform an MQM annotation, using a prompt which describes the MQM protocol in a fairly straightforward manner (an extract is given in Figure~\ref{gemba-prompt}) and also gives three examples of MQM-annotated texts.


\begin{figure}
\lineacross\\
(System) You are an annotator for the quality of machine translation. Your task is to identify errors and assess the quality of the translation.\\
\begin{verbatim}
(user) {source_language} source:\n
```{source_segment}```\n
{target_language} translation:\n
```{target_segment}```\n
\n
\end{verbatim}
Based on the source segment and machine translation surrounded with triple backticks, identify
error types in the translation and classify them. The categories of errors are: accuracy
(addition, mistranslation, omission, untranslated text), fluency (character encoding, grammar,
inconsistency, punctuation, register, spelling),   [\emph{etc}]
\caption{Part of the GPT4 prompt used by GEMBA-MQM, from \cite{kocmi-federmann-2023-gemba}. \texttt{\{source\_language\}} is replaced by the language of the text being translated, \texttt{\{source\_segment\}} is replaced by the text being translated, etc. The prompt also includes three examples of MQM annotated texts.}\label{gemba-prompt}
\lineacross
\end{figure}

According to \cite{kocmi-federmann-2023-gemba}, GEMBA-MQM agrees very well with human MQM ratings when comparing systems.  In other words, if human translators judge that system A outputs are better than system B outputs based on MQM annotations, then GEMBA-MQM is very likely to make the same assessment (see paper for details). 

\personal{Several of my students have tried to use GPT to evaluate texts, and all of them have found that GPT is biased\index{bias} towards texts that it generated.  In other words, if GPT4 and Gemini\index{Gemini} generate texts of equivalent quality as judged by human evaluation, GPT4 (when used as an evaluator) will rate the GPT4 text more highly than the Gemini text.  Again this is a rapidly-developing area, and we may see new variants of language-model-based evaluation which reduce such biases.}


\subsection{Validation of metrics}\label{sec:validationmetrics}\index{metrics!validation}
In most cases (compute speed/cost evaluations are an exception), it is important to \emph{validate} metrics, that is show that they correlate (agree) with high-quality human or impact evaluations\index{impact!evaluation} \cite{reiter-belz-2009-investigation} (we can similarly validate cheap/quick human evaluations by seeing how well they agree with a high-quality \scare{gold-standard} human evaluation \cite{garneau-lamontagne-2021-shared}).  Validation does \emph{not} need to be done every time a metric is used; it is generally done when a metric is first proposed, and then repeated on a regular basis for metrics which are heavily used.

Of course, how well a metric correlates with a human evaluation depends on many factors, including the NLG task, the human evaluation,  the quality criteria, and evaluation granularity (are texts or systems being evaluated?).  In rough terms, metrics often seem to have higher correlations with human evaluations in the following contexts:
\begin{itemize}
\item \emph{NLG task} is relatively simple.
\item \emph{Quality criteria}\index{quality criteria} focus on readability or related criteria such as fluency.
\item \emph{Granularity}\index{evaluation!granularity} is system-level evaluation based on average scores (as opposed to text-level evaluations or system-level evaluations based on worst-case performance (Section~\ref{sec:req-worstcase})).
\item \emph{Human evaluation} used for correlation has good inter-annotator agreement\index{inter-annotator agreement} (Section~\ref{sec:IAA}).
\end{itemize}
These are very rough observations, and it is important to explicitly measure correlation between metrics and human evaluations.

The best known and longest-running validation studies in NLG and related fields are the annual metrics evaluation shared task in the WMT\index{WMT} conference.  WMT is an annual conference which includes a high-quality human evaluation of a number of machine translation systems.  Alongside this, WMT runs a shared task where various automatic metrics are used to evaluate the same systems, and correlations are computed between the metric scores and the WMT human evaluations.  The exact design of the validation exercise varies, I encourage interested readers to read about specific metrics evaluation shared tasks such as the one at WMT22 \cite{freitag-etal-2022-results}.  Regardless of the details, the shared task involves computing correlations separately for different language pairs, and also at both the system and text (called \emph{segment} in this context) levels; sometimes an overall aggregate score is also computed for the metrics.

Most of the readers of this book will not need to run metric validation studies, this is a specialist endeavour.  However, readers who use metrics should understand how the metric was validated, and think twice about using metrics which have poor validation (ie, do not correlate well with human evaluations) and/or have not been well validated.  Specific warning signs of poor validation include:
\begin{itemize}
\item \emph{Poor validation results:}  Numerous studies have shown that the BLEU\index{BLEU} metric (Section~\ref{sec:bleu}) in particular does not correlate as well with human evaluations as competing metrics \cite{kocmi-etal-2021-ship,freitag-etal-2022-results}.  It is frustrating that so many researchers continue to use BLEU despite the existence of much better alternatives!
\item \emph{No validation:} Occasionally I run across metrics which have not been validated against human judgements; their developers claim that the metrics \scare{make sense} and \scare{produce plausible results} but do not back this up with validation data.   Readers should avoid such metrics!
\item \emph{Poor human experiment for validation:} Another common problem is that the human evaluations used for validation may be poorly done and not very reliable; validation should only be done against very high-quality human experiments.  Unfortunately, carefully checking the rigour of a validation experiment is not easy.  One rule of thumb is to look at whether the human evaluation used for validation is described in detail in relevant papers; if it is not, then the validation study may not be meaningful.
\item \emph{Old validation:} Widely used metrics should be re-validated every few years.  NLG systems are evolving rapidly, and a metric which correlates well with human evaluation on moderate-quality texts produced by older systems may have a poorer correlation on higher-quality texts produced by newer systems.  If the validation data is more than 3 years old, it may not be meaningful.
\end{itemize}

It is also worth noting that validation quality (ie, how closely metric results correlate with high-quality human evaluation) impacts the interpretation of metric results.  For example, suppose two NLG systems, X and Y, are compared using a metric that has a moderate (instead of strong) correlation with high-quality human or impact evaluations.  If X has a much higher metric score than Y, then there is a good chance that X's texts are genuinely better than Y's.  However, if X's score is just slightly better than Y's, then this may not translate into a genuine difference in text quality \cite{mathur-etal-2020-tangled}.

A final note is that some metrics have biases\index{bias}. For example, the BLEU metric is biased against rule-based systems \cite{freitag-etal-2020-bleu}.  In other words, if a rule-based NLG\index{rule-based NLG} system R and a neural NLG system N are judged to produce equal quality texts in a high-quality human evaluation, BLEU will probably give higher scores to the neural system N.  A good validation study should help uncover such biases

\subsubsection{Example: Survey of Validity of BLEU}
In 2018 I reviewed validations\index{metrics!validation} of the BLEU\index{BLEU} metric (Section~\ref{sec:bleu}) which were published in the ACL Anthology before July 2017 and which met certain quality criteria \cite{reiter-2018-bleu}.  This exercise showed the wide range of results of validation studies, in terms of both what  they looked at and what they found.

This was done as a structured survey\index{structured survey} \cite{moher339group}, where the goal was to systemically find relevant studies in the literature (as opposed to analysing studies which I was already aware of).  In the survey, I found 34 papers which reported validation data for BLEU, which included 284 correlations between BLEU scores and human evaluations.  Many papers reported multiple correlations, for example looking at correlations with different human-judged quality criteria\index{quality criteria} (e.g., both clarity and accuracy), different subjects\index{subjects} (e.g., both domain experts and non-experts), and/or different languages for the generated texts (eg, English, German, Chinese).

The correlations I found are shown as box plots in Figure~\ref{mt-results} (for machine translation\index{machine translation}), and in Figure~\ref{nlg-results} (for NLG).  \emph{System} granularity\index{evaluation!granularity} means correlations reported for NLG/MT systems; \emph{Text} granularity  shows correlations reported for individual generated texts.  Following guidelines from medical research \cite{prasad2015strength}, I categorised correlations as
\begin{itemize}
\item \emph{High:} correlation with human evaluation is 0.85 or higher;
\item \emph{Medium:} correlation with human evaluation is between 0.7 and 0.85;
\item \emph{Low} correlation with human evaluation is between 0 and 0.7; or
\item  \emph{Negative} correlation with human evaluation is less than 0 (ie, systems with higher BLEU scores generally got lower human ratings).
\end{itemize}
Overall correlations seemed best  when BLEU is used for system-level evaluation in Machine Translation (which is what BLEU was designed for). Mean correlation for this type of evaluation (the \scare{system} box in Figure~\ref{mt-results}) was at the Medium level, for all other categories it was Low.   However, there was a lot of variation, and even for evaluations of MT systems a few studies reported negative correlations between BLEU scores and human evaluations.

\begin{figure}
\includegraphics[scale=0.2, clip, trim = 80 40 0 40]{boxplotmt}
\caption{Box plot of reported BLEU-human correlations for MT, at system and text granularities \cite{reiter-2018-bleu}.}\label{mt-results}
\vspace{1 mm}
\includegraphics[scale=0.2, clip, trim = 80 40 0 40]{boxplotnlg}
\caption{Box plot of reported BLEU-human correlations for NLG, at system and text granularities \cite{reiter-2018-bleu}.}\label{nlg-results}
\end{figure}


\section{Impact evaluation}\label{sec:impactevaluation}\index{impact!evaluation}

An impact evaluation measures the impact \index{impact} of an NLG system in real-world usage.  Such evaluations are rare at the time of writing, but I hope that they will become more common (which is why I am making them prominent in this chapter), since they are the best way to measure the real-world utility\index{utility} of NLG systems.

Impact evaluations are often task-based human evaluations\index{task-based evaluation} (Section~\ref{sec:humantaskeval}), ie they measure how well users perform tasks with and without the NLG system being evaluated.  By definition they have high ecological validity\index{ecological validity} (Section~\ref{sec:ecologicalvalidity}), since they measure real-world usage.

\subsection{Comparison between users: Randomised controlled trials and A/B Testing}

One way to measure the utility of an NLG system is to conduct a formal \emph{randomised controlled trial (RCT)}\index{randomised controlled trial}; this is the same technique that is used in medical research to assess the real-world effectiveness of new medications and other novel interventions.

An example of using an RCT trial to evaluate an NLG system was presented in Section~\ref{sec:stopsmoking}, where the STOP\index{STOP} smoking-cessation system was evaluated by recruiting a large number of smokers, randomly assigning smokers to a group which got a STOP letter or a group which got a control letter, measuring smoking-cessation rates in the groups, and comparing cessations rates in the STOP and control groups.  Cessation rates were in fact higher in one of the control groups, which showed that STOP was not effective.

A variant of the RCT concept which is popular in the IT world is \emph{A/B Testing}\index{A/B testing}, where different groups of users use different systems and the outcome is compared \cite{gupta2019top}.  For example, an e-commerce website may use A/B Testing to evaluate whether a new website design increases sales, by giving some users access to the new site and others access to the old site, and comparing sales in the two groups.  I am not aware of any published work on A/B testing of NLG systems, which is a shame, since it seems like the technique could be used for many NLG applications (including evaluating NLG summaries of basketball games).

\subsection{Historical comparison}\label{sec:historicalcomparisons}\index{historical comparison}

Another way to do real-world impact evaluation is to measure user productivity before and after an NLG system is  introduced.
For example, the Note Generator\index{Note Generator} system (Section~\ref{IntroConsultationSummary}) is intended to help doctors create summaries of doctor-patient consultations, and in this context two important criteria are the amount of time doctors take to write or post-edit\index{post-editing!time} the summaries, and the accuracy\index{accuracy} of the summaries.

Researchers measured the amount of time that 20 doctors\index{doctors} spent post-editing Note Generator summaries in real doctor-patient consultations, and compared this to the amount of time that the same doctors spent manually writing summaries, before Note Generator was introduced  \cite{moramarcophd}.   This showed that the time required to post-edit a summary was 9\% less than the time required to manually write a summary.  The time saved was different for different types of consultation\index{consultations}, with larger gains seen on simple and straightforward consultations.  The data also showed that from a quality perspective, post-edited summaries seemed to have slightly higher quality (fewer mistakes, more coherent) than manually written summaries.

An advantage of the historical comparison approach is that there is no need to run an explicit evaluation experiment, the analysis is based on data accumulated while the system is being used (although researchers may wish to add additional monitoring functions to the software to capture relevant data).  Of course permission must be obtained to gather, analyse, and (if appropriate) publish the data!  

Historical comparisons can also be used to compute \emph{return on investment (ROI)}\index{return on investment}; this is discussed in Section~\ref{sec:ROI}.

\subsection{Challenges}

Real-world impact evaluations can be difficult and complex to organise, and also often raise ethical\index{ethics} concerns.  If people are using an NLG system for real, then researchers should show that the system cannot have an adverse impact either on the users or on third parties\index{third parties} (for example, a system used by doctors cannot have an adverse impact on patients).  This is less of an issue if a historical comparison approach is used, since the evaluation exercise has no impact on production usage of the deployed system, but data privacy\index{privacy} issues must still be addressed.

Another issue with real-world impact evaluations is that the contexts where the NLG system is used may be different from the contexts where the control system is used. 
For example, different people received the NLG and control letters in the STOP\index{STOP} evaluation, and the consultations summarised using Note Generator were different from the summaries which were manually summarised before Note Generator\index{Note Generator} was introduced.  Because of this, it is useful to analyse the NLG and control scenarios to see if they are different in a way which could influence results; for example such an analysis showed that the NLG group in the STOP evaluation had more heavy smokers (who are less likely to stop smoking) than were present in the control group \cite{REITER200341}.

A final point is that evaluations of real-world impact are of complete systems and workflows\index{workflow}, not just of NLG modules, and as such they are affected by factors such as user-interface\index{user interface} quality and attitude of subjects\index{subjects} to the system.  For example, doctors\index{doctors} who are concerned that managers\index{managers} want to replace them with NLG systems may try to make the systems look bad, for example by doing more post-editing than is actually needed.  Also sometimes companies are reluctant to publish impact evaluations of their systems if the results are not positive.


\section{Commercial Evaluation}\label{sec:commercialevaluation}\index{commercial evaluation}
This chapter focuses on scientific evaluation of research questions\index{research questions} and hypotheses, but of course evaluation is also important in a commercial context.  A key difference is that while academic evaluations usually focus on whether the texts produced by an NLG system are useful and effective, commercial evaluations also look at whether an NLG system will be profitable or otherwise commercially successful.  This in turn requires looking at costs, benefits, risks, and return on investment.  I will briefly discuss these below, using Babytalk (Section~\ref{sec:babytalk}) as an example (Figure~\ref{fig:BabytalkCommercial}).

\begin{figure}
\lineacross\\
\textbf{Costs:}\index{cost}
\begin{itemize}
\item 50 person-years to develop commercial version of Babytalk
\item 10 person-years (annually) to maintain this system
\end{itemize}
\textbf{Benefits:}\index{benefits}
\begin{itemize}
\item Limited because only 5-10 UK hospitals (at the time) had sufficiently powerful sensors and IT systems to use Babytalk
\end{itemize}

\textbf{Risks:}\index{risk}
\begin{itemize}
\item Unclear who is liable if doctors make a mistake because of bad advice from Babytalk
\end{itemize}

\textbf{Decision:} Do not commercialise Babytalk

\caption{Commercial evaluation of Babytalk}\label{fig:BabytalkCommercial}
\lineacross
\end{figure}

\subsection{Costs}\index{cost}
A key commercial question is how much money and resource (developer time, computer hardware, etc.) is required to build and maintain a robust NLG system.  The biggest life-cycle costs\index{cost!life-cycle} of a successful software product is usually maintenance\index{maintenance}, not initial coding \cite{davis200997}.  Building a robust\index{robustness} commercial product is an order of magnitude more expensive than building a system for private or academic use, because a commercial system needs to be robust, configurable\index{configurability}, and integrate\index{integration} with data sources and presentation tools.  Hence the costs of a commercial NLG product are usually \emph{much} higher than the cost of an academic research system in the same area.

To give a concrete example, at one point we entered into commercial discussions about creating a commercial product based on  Babytalk\index{Babytalk}.   Babytalk had taken around 6 person-years to develop, but this was for a system which ran in one hospital (Edinburgh) and was difficult to update; this also was just development costs and excluded maintenance costs.  Making a robust system which could be deployed in many hospitals and which could be easily updated (new medical kit or drugs, new sensors, new IT systems, etc) probably would have required on the order of 50 person-years (including testing and quality assurance), and if the system was successful it might have needed a 10-person maintenance team.  This ratio of commercial vs. research development effort is not unusual for academic-to-commercial transitions.

\subsection{Benefits}\label{sec:nlgbenefits}\index{benefits}
Just as important is the question of the commercial benefits that an NLG system will provide.  How many copies of the system can be sold, and how much can be charged for each copy?  There may be other benefits as well; for example, having a flashy AI product can be a \emph{loss-leader}\index{loss-leader} which is not profitable in itself but encourages clients to buy other products from a company, or investors\index{investors} to buy shares in the company.

Of course, the price of the software is largely determined by how useful it is to customers.  Many things contribute to utility\index{utility}, including productivity increases, cost savings, greater consistency, and empowering junior/new staff.  However, customers only benefit from software if they (or their employees) use it, and unfortunately there is a history in AI of systems not being used because staff saw them as threats to their jobs or otherwise undesirable (note that an AI tool can be desirable to a company but not to that company's employees, for example if it leads to large-scale job losses\index{job losses}).  Introducing an AI system often requires paying careful attention to \emph{change management}\index{change management} issues \cite{lauer2010change}.

In the Babytalk\index{Babytalk} case, one thing that became clear was that only a few hospitals would be interested in buying the system, because (at the time) only a few hospitals had sufficiently powerful sensors and patient-record systems; Babytalk required a lot of data, and most hospitals could not provide this data.  This meant that sales and hence commercial benefits of the system were limited.

\subsection{Risks}\label{sec:risk}\index{risk}
Risk is very important in commercial evaluations; is there a chance that something horrendous could happen once in a while?  For example, one reason the adoption of self-driving cars\index{self-driving cars} has been slow is the fear that even if such cars are safe 99.99\% of the time, they could go wrong once in a while and kill people.  This is not acceptable, even if the death rate from self-driving cars is less than from human-driven cars.  AI systems are held to higher standards than humans, especially if they use neural models which are difficult to explain. In crude money terms, such events can lead to large financial losses from lawsuits\index{lawsuits} and also from loss of credibility and brand loyalty\index{brand!loyalty}.

Risks essentially require understanding the worst-case\index{worst-case} behaviour of an AI system (Section~\ref{sec:req-worstcase}); especially if this raises safety\index{safety} concerns (Section~\ref{sec:safety}).  Worst-case behaviour can be hard to predict because of unexpected edge cases\index{edge cases}, obscure software bugs\index{bugs}, and in some cases the presence of malicious agents such as hackers\index{hackers}.

In the case of Babytalk\index{Babytalk}, for example, potential commercial partners were concerned that the system could made a mistake and offer poor and indeed dangerous advice, because of unexpected edge cases or obscure bugs.  Babytalk was an advisory system, with the doctor or nurse ultimately being control, but there were still concerns that if Babytalk offered bad advice and clinicians acted on it, then the company which sold Babytalk could be hit with a massive lawsuit.

\subsection{Return on investment (ROI)}\label{sec:ROI}\index{return on investment}
Sometimes an organisation invests money to build or commission an NLG system which provides benefits over many years.  In such cases an important figure is \emph{return on investment}\footnote{\url{https://en.wikipedia.org/wiki/Return\_on\_investment}} (ROI), that is how yearly benefits compare to the one-off investment in building the system.

For example, suppose a company spends 1,000,000 to build an NLG system, and it expects to sell 100 of these systems each year, for 100 each.  Ignoring maintenance, sales, and support costs, this means the company will earn 10,000 per year, which is 1\% of the cost of building the system; hence the system has an ROI of 1\%.  This is not very attractive, the company could earn more from its 1,000,000 by buying government bonds.

On the other hand, suppose the company expected to sell 5,000 NLG systems each year, at 100 each.  This is an income of 500,000, which means the ROI is 50\% (500,000/1,000,000).  This is very attractive, and much more than could be earned from government bonds!

Real ROI calculations are much more complex, and also take into consideration (amongst other things) risks, non-monetary costs and benefits, product lifespan, and yearly costs such as maintenance, sales, and support.    Most NLG developers do not need to understand the details of ROI, but it is important to realise that it is not sufficient for an NLG system to work and be useful, it also needs to provide an acceptable ROI which is higher than the ROI of other potential projects and investments.

We did not calculate ROI for Babytalk, but it would have been unacceptably low because of the above-mentioned high costs and limited benefits.

\section{Ten Tips on Evaluating NLG}\label{sec:ten-tips-eval}
The choice between evaluation type depends on context; for example impact evaluations are great in principle, but unfortunately often not possible in practice.  Annotations by domain experts are also very effective, but can take considerable amounts of money and effort, which is not realistic in many contexts.  Also, the balance between different types of evaluation is changing as technology progresses.  For example a few years ago, a ratings/ranking study with crowdworkers gave more meaningful results than metric-based evaluation, but in 2024 in many contexts the latest LLM-based metrics may give more accurate assessments than crowdworker ratings/rankings.

Anyways, regardless of the technique used, it is essential to design and execute a good experiment.
I will therefore conclude this chapter with ten \scare{tips} on doing good evaluations; this is based on common mistakes which I personally have seen in NLG evaluations\footnote{\url{https://ehudreiter.com/2024/04/08/ten-tips-on-doing-a-good-evaluation/}}.
\begin{enumerate}
\item \emph{Evaluate what is important:}  As discussed in Section~\ref{sec:researchquestions}, evaluate the quality criteria\index{quality criteria} which are most important scientifically and/or to your users and stakeholders\index{stakeholders}.  In medical\index{health} use cases, for example, accuracy\index{accuracy} and safety\index{safety} (Section~\ref{sec:medical-safety}) are usually very important, and hence should be evaluated.
\item \emph{Do not use obsolete evaluation techniques:} Which evaluation techniques is most appropriate depends on the circumstances, but there are very few circumstance in which an obsolete technique such as BLEU\index{BLEU} (Section~\ref{sec:bleu}) should be used.
\item \emph{Use good test data\index{test data}:} As discussed in Section~\ref{sec:trainingtestdata}, test data should be real data which is representative of real-world usage.  Do not, for example, test a system which generates marine weather forecasts\index{weather forecasts} (Section~\ref{sec:humanevalrankings}) on weather data from land sites.
\item \emph{Use strong baselines\index{baselines}:}As discussed in Section~\ref{sec:researchquestions},  if the evaluation compares a new NLG system to an existing one, the existing one should be state-of-the-art\index{state-of-art}.  Do not, for example, compare a 2024 weather forecast generator to Arria's 2014 weather forecast generator (Section~\ref{sec:Arriaweather}).
\item \emph{Avoid data contamination\index{data contamination} and testing on training data}.  As discussed in Section~\ref{sec:trainingtestdata}, do not test an NLG system on data it was trained on.
\item \emph{Compute statistical significance\index{statistical!significance}:} As discussed in Section~\ref{sec:statisticalsig}, it is essential to statistically compute the likelihood that a result is spurious and just due to experimental noise.
\item \emph{Make your experiment replicable\index{replication}:} As discussed in Section~\ref{sec:replication}, researchers should make it easy for other researchers to replicate their experiments.
\item \emph{Carefully execute\index{experimental execution} and report your experiment:} As discussed in Section~\ref{eval:experimental-execution}, experiments need to be carefully executed.  A sloppy experiment is meaningless.
\item \emph{Submit your work to peer review\index{peer review}:} Academic NLP researchers should submit their work to a peer-reviewed conference or journal, so that reviewers can check that it is of high quality.
\item \emph{Respond to questions from other researchers:} It is essential that researchers respond to questions and concerns about their work from other researchers, and correct\index{correction} or retract\index{retraction} their work if necessary.
\end{enumerate}
I hope that these tips make sense to readers of this book; unfortunately I see \emph{many} published NLG evaluations which do not follow them.

\section{Further reading}
There are numerous textbooks about experimental design\index{experimental design} and statistics\index{statistical}, such as Field and Hole \cite{fieldhold}.  Laken's book \cite{lakensstats} is available online at no cost (at time of writing).    University students, researchers, and faculty members should check if their university offers courses on these topics; check psychology and medical departments if there are no suitable courses in the computer science department.

De Leeuw et al \cite{leeuw2012international} is an excellent resource for survey design\index{survey design}, which is important in many human evaluations. 

Replication\index{replication} is an important topic across science, and there is great in concern in medicine (amongst other areas) about research findings which cannot be replicated and indeed are likely to be wrong; I highly recommend Ioannidis's classic paper on this \cite{ioannidis2005most}.  Within NLP, a good early paper on reproducibility is Wieling et al \cite{WielingCL}.   Belz et al \cite{belz-etal-2021-systematic} review work on reproducibility in NLP. The ReproHum\index{ReproHum} project has identified many challenges to reproducibility, including the reluctance of authors to cooperate with reproduction attempts \cite{belz-etal-2023-missing} and flawed execution of the experiments\index{experimental execution} being replicated \cite{thomson-cl24}.

Gehrmann et al \cite{GehrmannEvaluation} is an excellent survey of evaluation in NLG, which includes best practice recommendations.  Van der Lee et al \cite{VANDERLEE2021101151} gives best practice recommendations for human evaluations of NLG, focusing on rating\index{rating} and ranking\index{ranking} evaluations (Section~\ref{sec:humanevalrankings}).  Thomson et al \cite{thomson-cl24} describe common flaws in experiments, van Miltenburg et al \cite{van-miltenburg-etal-2021-underreporting} give recommendations for error reporting in experiments, and Dror et al \cite{dror-etal-2018-hitchhikers} discuss statistical analyses\index{statistical!analyses} in NLP experiments.  Sparck-Jones and Gallier \cite{jones1995evaluating} is an older book on NLP evaluation with many valuable insights.

The annual WMT\index{WMT} conference usually contains high-quality evaluations and validations of metrics\index{metrics!validation}, as well as well-designed human evaluations of machine translation\index{machine translation} systems. WMT conference proceedings are available at \url{https://aclanthology.org/sigs/sigmt/}.

There are numerous shared tasks in NLG, where participants submit systems which are centrally evaluated by the task organisers; participating in such tasks can be a good way to get a better understanding of evaluation.  The annual GEM\index{GEM} workshop is a good source for high-quality shared tasks in NLG (\url{https://gem-benchmark.com}).

GEM (\url{https://gem-benchmark.com}) is also an excellent source for evaluation resources.   Huggingface\index{Huggingface} provides software for many types of automatic evaluation (\url{https://huggingface.co/docs/evaluate/index}).

Finally, I have written many blogs about NLG evaluation in my blogsite (\url{ehudreiter.com}), which cover a wide range of evaluation topics including techniques, experimental design, problems, statistical analysis, reproducibility, research ethics (and other topics as well).   These blogs have not been peer reviewed, but many people have nonetheless found them to be useful.

