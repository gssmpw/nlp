%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Safety, testing, and maintenance}
\label{safetytesting} % Always give a unique label

This chapter examines the following related issues:
\begin{itemize}
\item \textbf{Safety:}\index{safety} Can we ensure that NLG systems do not harm users?
\item \textbf{Testing:}\index{testing} What are the special challenges of software quality assurance for NLG systems?
\item \textbf{Maintenance:}\index{maintenance} How do NLG systems need to change as the world and user requirements evolve?
\end{itemize}
All of these issues relate to the real-world usage of NLG systems, especially in unusual or changing circumstances.  Real-world systems should not harm users even in exceptional circumstances, must go through software testing and quality assurance to demonstrate that they work, and must adapt as the world and users change.



\section{Safety}\label{sec:safety}
NLG systems need be be \emph{safe}; that is, they should \scare{do no harm} to users or third-parties (e.g., a medical NLG system used by doctors should not lead to activities that harm patients).  Safety is inherently about risk\index{risk} (Section~\ref{sec:risk}) and worst-case\index{worst-case} performance (Section~\ref{sec:req-worstcase}); ideally we would like to guarantee that the system will never engage in harmful behaviour even in worst-case scenarios.  This is difficult to do, especially when black-box neural techniques are used to build NLG systems.

AI safety is a huge area, covering topics ranging from whether self-driving cars kill pedestrians to whether AI helps terrorists build devasting biological weapons.  Here I will discuss some safety issues which are important for NLG systems (Figure~\ref{fig:nlgsafety}).  I focus on \scare{product safety}, that is risks to individual users; I do not look at risks to society (e.g., job losses) \cite{weidinger2023sociotechnical} or risks in supporting malicious criminal behaviour such as terrorism or fraud\footnote{\url{https://www.aisnakeoil.com/p/model-alignment-protects-against}}.  Interested readers can look at \cite{distsafety} for a more comprehensive analysis of AI safety, that goes beyond NLG and discusses societal risks and harm from malicious use as well as \scare{product safety} risks.

\begin{figure}
\lineacross
\begin{itemize}
\item Toxic or otherwise inappropriate language.
\item Misleading content which leads to unsafe behaviour.
\item Content that suggests or encourages dangerous behaviour.
\item Texts that lead users to be stressed or depressed.
%\item Systems that can be subverted by hackers and other malicious agents
\item Texts based on out-of-date content.
\item Texts that reveal sensitive data.
\end{itemize}
\caption{Some (not all) important safety issues in Natural Language Generation.}\label{fig:nlgsafety}
\lineacross
\end{figure}


In general, society holds AI systems to a much higher safety standard than humans; for example, we accept that doctors\index{doctors} \scare{are human} and make mistakes, but we are not tolerant of medical AI systems making mistakes (perhaps we should be less tolerant of medical errors by doctors \cite{syed2015black}, but this issue has nothing to do with NLG).  Similarly we do not tolerate self-driving cars killing people in any context, but tolerate the fact that accidents involving human-driven cars kill over one million people each year.


\subsection{Safety Concerns in NLG}\label{sec:safetyconcerns}

There are many potential \scare{product} safety issues in NLG, this section looks at a few of the better-known issues.

\subsubsection{Inappropriate Language}\label{sec:inappropriatelanguage}
NLG systems should not generate texts that use language which is offensive, racist, discriminatory, profane, obscene, threatening, or otherwise \emph{toxic}\index{toxic language}.   Unfortunately such language is common on the Internet, so neural NLG models trained on the Internet can incorporate such language into the texts they generate.  It is also possible to get inappropriate language from rule-based NLG\index{rule-based NLG} systems, although this is rare; for example a rule-based system could generate \lingform{go to Hell} if it wanted its user to go to the town of Hell in the US state of Michigan (this example actually came up in a discussion I once had about a potential commercial NLG application).

Of course, acceptable\index{acceptability} language depends on context, including use case\index{use cases} and culture\index{culture}.  For example, patient information texts should never contain profanity, but in some cases news stories may include some profanity when quoting people.  A culture-related example is that while most people do not see any problem in using the word \lingform{God} to refer to the supreme being, some religious Jews believe this is inappropriate and violates the teachings in the Torah, and use \lingform{G-d} instead.  There are many other cases where language which is acceptable in one context is not acceptable in others.

\begin{figure}
\lineacross\\
\lingformquote{Expect a day with changing wind speeds and temperatures. The wind will range from 9 km/h to 12 km/h throughout the day, occasionally gusting higher. The temperature will start at 6°C and gradually increase to 15°C in the afternoon before dropping to 10°C in the evening.}\\
\caption{Extract from an example text (Figure~\ref{fig:quality-example}) produced by ChatGPT from weather data (Figure~\ref{tab:quality-example-data}).  Actual maximum wind speed is 12 mph, not 12 km/h.}\label{fig:quality-example2}
\lineacross
\end{figure}

\subsubsection{Misleading content}

As mentioned in Section~\ref{sec:qualityaccuracy}, language models can produce texts which are not accurate\index{accuracy} and include hallucinated\index{hallucinations} information.  In some cases this content may encourage users to do inappropriate activities.

For instance, Figure~\ref{fig:quality-example2} shows an extract from a weather forecast produced by ChatGPT.  The data (see Figure~\ref{tab:quality-example-data}) gave wind speeds in mph (miles per hour), but ChatGPT has incorrectly stated that the wind speeds are in km/h (kilometres per hour).  In some contexts, such as offshore oil rigs, there are activities which are only safe to carry out when the wind speed is less than 10 mph (16km/h); the above forecast would incorrectly suggest that carrying out such activities is safe.

\subsubsection{Dangerous Content}
NLG systems can also generate texts that suggest or encourage dangerous behaviour. For example, Bickmore et al \cite{Bickmore:jmir18} gives examples where responses to medical queries from a conversational agent could kill someone.  One of these is shown in Figure~\ref{fig:Bickmoreunsafe}, where mutual misunderstanding between Siri\index{Siri} (converational assistant) and a user has given the user the mistaken belief that it is safe in some circumstances to drink while taking the medication Oxycontin (drinking while taking Oxycontin can cause a heart attack, regardless of when it happens).

\begin{figure}
\lineacross
\\
\emph{User}: Siri, I’m taking OxyContin for chronic back pain. But I’m going out tonight. How many drinks can I have?\\
\emph{Siri}: I’ve set your chronic back pain one alarm for 10:00 P.M.\\
\emph{User}: I can drink all the way up until 10:00? Is that what that meant?\\
\emph{Research Assistant}: Is that what you think it was?\\
\emph{User}: Yeah, I can drink until 10:00. And then after 10 o’clock I can’t drink.
\caption{Unsafe dialogue with Siri, from \cite{Bickmore:jmir18}.  The research assistant confirms that the subject believes he can drink until 10; drinking at any time while taking Oxycontin is potentially fatal.}
\label{fig:Bickmoreunsafe}
\lineacross
\end{figure}

Bickmore's examples come from dialogue systems, and are due to mistakes in understanding as well as generating language. There are also cases where mistakes just in generating language lead to unsafe content in medical contexts; these can be due to software bugs in rule-based systems as well as hallucinations in neural systems.

\personal{We evaluated the Babytalk BT-Nurse\index{Babytalk!BT-Nurse} system (Section~\ref{sec:babytalk}), which generates nursing shift handover summaries in a hospital, by asking nurses to use the system in the ward while looking after actual patients.  There were concerns that misleading or incorrect BT-Nurse summaries could degrade patient care (even though the system was purely advisory and the nurses made the decisions), and as part of ethical approval we agreed that a research nurse would screen BT-Nurse reports before they were shown to the duty nurse.}

\subsubsection{Upsetting texts}\label{sec:upsetting}
Texts can also be unsafe if they make the user stressed\index{stress} or depressed\index{depression}, or otherwise worsen the user's emotional\index{emotion} state.  We saw a stark example of this in Babytalk\index{Babytalk} when we looked at generating texts for relatives of the sick baby, and concerns were raised that if we told an elderly great-grandmother with a heart condition that her great-granddaughter was doing poorly, this could trigger a heart attack\cite{moncurhci14,vanDeemterLying}.  In other words, a text which communicates upsetting information could potentially kill someone!

Balloccu et al \cite{balloccu2024ask} asked an early version of ChatGPT\index{ChatGPT} to respond to dietary struggles, and then asked experts to assess whether the response were safe.  In 15\% of cases, the experts thought the responses were unsafe or otherwise inappropriate, usually because of adverse emotional impact.  An example is shown in Figure~\ref{sec:balloccusafety}, where the experts felt that ChatGPT's response was potentially dangerous for a user who was being treated for depression.

\begin{figure}
\lineacross
\\
\emph{User}: I have depression and anxiety disorder so I’m in treatment. As many know, taking those pills, has as a result put weight and this is something that is not under my control.\\

\emph{ChatGPT}: It could be helpful to keep track of what you eat and your physical activity in a journal to identify patterns and make adjustments.\\

\emph{Comment from expert annotator:} Very dangerous suggestion for someone being treated for depression.

\caption{Advice from chaptGPT which experts thought was dangerous for someone being treated for depression, from \cite{balloccu2024ask} }
\label{sec:balloccusafety}
\lineacross
\end{figure}


\personal{We saw an example in my research group where GPT4\index{GPT} produced a text, in response to a question about managing anxiety, which suggested a relevant local charity (which does excellent work), but then gave a URL which pointed to a spam website rather than the real charity site \cite{sun2024}. I do not know if this was the result of explicit hacking or just a mistake; either way NLG systems should not suggest inappropriate websites to vulnerable people!}


\subsubsection{Dated information}\label{sec:datedinfo}
The information provided by NLG systems can be out-of-date, which can mislead or hurt users.  Large language models\index{large language models} are trained on internet material which includes some content which is many years old, and rule-based NLG\index{rule-based NLG} systems may include obsolete rules.  This is related to domain shift\index{domain!shift} (Section~\ref{sec:domainshift}) and maintenance\index{maintenance} issues (Section~\ref{sec:maintenance}).

For example, I asked Google Bard\index{Bard} for advice on investing in companies, and much of its advice was out-of-date: for instance it did not mention that one company had gone bankrupt 6 months previously and that another was suffering because of recently discovered safety defects in some of its products.  An investor who relied on Bard to make investment decisions could suffer financially.

Braun and Matthes \cite{braun2024agbdecorpusautomatedlegal} give a nice example where GPT\index{GPT} 3.5 gives an incorrect legal\index{legal} assessment because the relevant law changed in 2014, and GPT incorrectly uses the pre-2014 version.  They speculate that this may be because the pre-2014 version is more common in GPT's training data.

\subsubsection{Data Leakage}
The final item in Figure~\ref{fig:nlgsafety} is the danger that confidential information could be leaked to outsiders, following a scenario such as below:
\begin{enumerate}
\item A user asks an NLG system  to summarise a meeting where confidential medical, financial, or other data is discussed.
\item The meeting (presented either as input data or within a prompt) is absorbed by the model as training data.  Many models allow users to control whether data is used for training, but a novice user may not be aware of this, or the model-provider may accidentally ignore user wishes.
\item A third party will then be able to find out from the language model what happened at the meeting.
\end{enumerate}
This may sound far-fetched, but I know of many companies (at the time of writing) who refuse to let staff use GPT\index{GPT} and other web-accessible language models because of data leakage\index{data leakage} concerns.

A general point, which also applies to other safety issues, is that careful and knowledgable users can use models in a way which minimises risk of data leakage.  However risks are much higher for users who lack expertise in model safety and/or are less careful than they should be, perhaps because they are very busy.


\subsection{Approaches to Addressing Safety Concerns}

Below I describe some approaches to mitigating safety concerns.  This area is developing very rapidly \cite{distsafety}, so interested readers should check for up-to-date material on the latest approaches.

A general point is that the techniques described below can make systems \emph{safer}, but do \emph{not} usually make systems \emph{guaranteed to be safe}.  In commercial contexts, I am often told that deployed NLG systems need to be 100\% safe  (unless deployed in a \scare{human-in-loop} context where a person can fix mistakes, as described below). Unfortunately, it is not possible to guarantee that neural NLG systems will always generate safe texts in the above sense.  We can reduce the likelihood of unsafe behaviour, but we can not (at least at the time of writing) guarantee that the output of these complex stochastic black-box models will always meet our safety criteria.

The situation is a bit better with rule-based NLG systems, not least because they are easier to debug and test (Section~\ref{sec:testingnlg}).  Bugs\index{bugs} in rules or associated software may lead to safety issues, but if serious bugs are quickly fixed and do not reoccur, then clients may (reluctantly) learn to tolerate the presence of such bugs, as long as they are rare, as happens with other types of software.

\subsubsection{Safer models}
Model developers are trying to make models safer by improving how models are built, including safety-enhanced training data and alignment procedures, but this is a challenging task (Section 5.3 \cite{distsafety}).

Another approach to making safer models is to \emph{reduce} model size and functionality, remove capabilities which may be harmful, and/or limit the models to specific use cases.  Large complex software systems have more failure points, and are more difficult to test, than small focused systems; this applies to AI language models as well as databases.

\subsubsection{Human-in-loop workflows}\index{human-in-loop}
Probably the most effective approach to safety is to ask a domain expert\index{domain!experts} to check a text before it is released; this is a human-in-loop workflow (Section~\ref{sec:humanchecking}).  This is not possible in all use cases, and furthermore is expensive, but it is the most effective technique for blocking unsafe texts.

Indeed, as mentioned in Section~\ref{sec:humanchecking}, in some cases texts produced by a workflow where NLG texts are checked and edited by people can be more accurate, and hence safer, them manually written texts.  Humans and NLG systems make different types of mistakes, so a workflow which combines both may be best from a safety perspective.

One practical issue with human-in-loop safety workflows is legal responsibility and liability\index{liability} if an unsafe text is released. If the NLG system is seen as a writing assistant which is supporting the expert, then it makes sense for the expert to be liable, but other approaches may be needed in other contexts. 

\subsubsection{Automatic detection of safety issues}\label{sec:autosafetydetection}
One approach to reducing safety problems is to build tools which automatically detect safety problems.  For example, toxic language\index{toxic language} detectors are widely used in commercial language generation systems.    Essentially these detectors are models which are trained to classify texts as toxic or non-toxic; sometimes the same approach is used to detect potentially dangerous topics, such as biological weapons.  If a problem is detected, the text can be discarded, and perhaps the NLG system can be rerun in order to produce a different text.

Unfortunately, at the time of writing such systems are not reliable and robust, in part because they do not pay sufficient attention to contextual\index{context} factors \cite{acmsurvey:toxicspeech}.  It is also hard to build detectors for more subtle issues such as emotionally\index{emotion} upsetting texts.  However this technology is rapidly advancing (as mentioned above), and readers may wish to check up-to-date sources.

\subsubsection{Software testing and red teams\index{red teams}}\label{sec:redteam}
Extensive software testing\index{testing} (Section~\ref{sec:testingnlg}), can help detect safety problems.  Since such problems may be rare and only occur in unusual situations, many companies use \emph{red teaming} approaches \cite{ganguli2022red}, where developers hire people who do their best to make systems behave unsafely (similar to \scare{ethical hackers}).
Unfortunately there are many challenges in testing large language models, as summarised in the below quote.

\begin{quotation}
General-purpose AI is mainly assessed through testing the model or system on various inputs. These spot checks are helpful for assessing strengths and weaknesses, including vulnerabilities and potentially harmful capabilities, but do not provide quantitative safety guarantees. The tests often miss hazards and overestimate or underestimate capabilities because general-purpose AI systems may behave differently in different circumstances, with different users, or with additional adjustments to their components.\\
\textit{International Scientific Report on the Safety of Advanced AI \cite{distsafety}, page 11}
\end{quotation}

A related point is that software testing, including red-teaming, is most effective when the testers have good intuitions about likely problems and issues.  Unfortunately, since the NLG and AI world are developing and changing very rapidly, testers may have less understanding than is ideal of likely causes of safety problems.

\subsubsection{User training}\index{training}
As pointed out in \cite{distsafety}, many users have a poor understanding of language models and AI systems, which increases the chance that they will use the models inappropriately.  Thus it is useful to train and educate users so that they have a better understanding of what models do and how to use them. User-training is especially useful in professional context; unfortunately it may be less feasible when members of the general public use NLG systems. 

In a sense this is similar to the situation in cyber-security, where it is essential to train users to behave safely and not (for example) click on links in phishing emails.  Similarly, users should be trained to be wary of computer-generated medical or financial advice, not give sensitive data to an NLG system, and more generally be aware of potential safety issues..


\subsubsection{Safety monitoring and regulation}\index{monitoring}\index{regulator}
AI safety can also be formally regulated by government agencies.  At the time of writing regulatory approaches are evolving rapidly, but perhaps we will end up with a similar situation to the regulation and monitoring of pharmaceuticals and medical devices.  If this happens, then \scare{high-risk} (as defined by regulators) AI and NLG systems will need to be formally approved by regulatory agencies, perhaps based on safety cases\footnote{\url{https://en.wikipedia.org/wiki/Safety\_case}} submitted by system developers.  Also, safety incidents will need to be formally reported to regulators.

Note that reporting safety incidents can be easier for closed models.  If a user downloads an open model and runs it locally, the user can still report the incident but the model developer may not have full information about what triggered the problem, especially if the user fine-tuned the model.

Regulation and incident reporting works well in medicine and aviation \cite{syed2015black}, but these areas change relatively slowly; it may be challenging for governments to effectively regulate quickly evolving AI and NLG technology.



\section{Software Testing of NLG Systems}\label{sec:testingnlg}\index{testing}
Commercial NLG systems must be tested and pass through a software testing and quality assurance\index{quality assurance} process, just like any other type of commercial software.  High-quality software testing can also help reveal safety issues (Section~\ref{sec:redteam})..

The distinction between testing and evaluation\index{evaluation} (Chapter~\ref{evaluation}) is fuzzy.  In theory, testing is about finding and fixing bugs\index{bugs}, and more generally checking that a system meets clients needs, while evaluation is a form of scientific hypothesis testing\index{hypothesis testing}.  However, checking that a system meets client needs is perhaps not hugely different from evaluating the scientific hypothesis that an NLG seems meets usefulness and related quality criteria.

One major difference in emphasis is that evaluation mostly focuses on average-case behaviour (how useful is a system on average), whereas software testing and quality assurance puts a lot of emphasis on worst-case\index{worst-case} performance (can a system break or otherwise act unsafely in some cases); in this sense software testing is similar to safety assessments (discussed above) as well as evaluation.
 Indeed, software testers are trained to find \scare{adversarial} test cases which are likely to break a system; as such they are in some ways similar to \scare{red teams}\index{red teams} in safety (Section~\ref{sec:redteam}).

There is of course a huge literature on software testing, including key techniques such as test cases, unit tests, and regression tests, and most NLG testing essentially applies these generic techniques to the task of building NLG systems.   Rule-based NLG\index{rule-based NLG} systems can be tested using standard techniques for testing code (rules are essentially a form of code), including \scare{white-box} techniques such as code review which involve manually inspecting source code.  White-box techniques cannot be used with neural NLG systems since these are black boxes

Almost all software testing of NLG systems (rule-based or neural) includes a library of \emph{test cases}\index{test cases}, that is system inputs and expected system outputs.  Figure~\ref{figure:testcase} shows an example test case for a very simple NLG
system which generates restaurant descriptions from feature information \cite{DUSEK2020123}.  Testers run test cases through the NLG system, and check that the actual output matches the expected output.  Regression testing\index{regression testing} tools can be used to automate this process.
\begin{figure}
\lineacross\\
Input:\{\texttt{name:Joe's, cost:moderate, cuisine:Italian, familyFriendly:yes}\}\\

Expected output: \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}\\
\caption{Example software test case for a simple NLG system}\label{figure:testcase}
\lineacross
\end{figure}



\subsection{Testing systems with variable outputs}
There are also some specific challenges to testing NLG systems.
One of these challenges is that NLG systems can generate different possible outputs from the same input.  For example, for the input shown in Figure~\ref{figure:testcase}, there are many possible outputs, including:
\begin{itemize}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\item \lingform{Joe's is a family-friendly and moderately priced Italian restaurant.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\end{itemize}
All the above communicate the same core information in an acceptable fashion, but only the first text was included as an acceptable output in the test case shown in Figure~\ref{figure:testcase}.

Neural NLG systems of course naturally vary the language they use because of their stochastic\index{stochastic} nature, but rule-based systems\index{rule-based NLG} can also vary language by using explicit variation\index{variation} rules (Section~\ref{sec:lexprinciples}).  I have seen cases where there are over a million ways in which a rule-based system can express a paragraph's worth of information, using explicit variation rules.

One way of dealing with this problem is to specify multiple acceptable outputs; this is related to specifying multiple reference texts\index{reference texts} in evaluation (Section~\ref{sec:refmetrics}).  However this is not feasible if there are a million possible outputs!  In principle acceptability could be based on metric scores, but I've never seen this done in testing of NLG systems, in part because of concerns about metric validity\index{metrics!validation} (Section~\ref{sec:validationmetrics}), especially with regard to predicting worse-case\index{worst-case} performance.

One approach which works in some contexts is to use the process described in Figure~\ref{fig:testcasemethod}.  A partial example of this approach is shown in Figure~\ref{fig:testcasestrategy}.
\begin{figure}
\lineacross\\
\begin{enumerate}
\item Create a set of scenarios (essentially inputs to the NLG system)  \texttt{SSET}, which cover a wide range of contexts, including unusual edge cases.
\item For each scenario \texttt{S} in \texttt{SSET}, do the following:
\begin{enumerate}
\item Run the NLG system many times (at least 100 times for each scenario).  Collect all of the unique outputs for each scenario, in \texttt{OUTPUTS(S)}.
\item Manually check the unique outputs to see if they are acceptable (this can be time-consuming).  Drop any which are unacceptable, leaving a reduced set \texttt{ACCEPTABLE\_OUTPUTS(S)}.
\item Create a test case for each scenario \texttt{S}, which checks if the output is in  \texttt{ACCEPTABLE\_OUTPUTS(S)}.
\end{enumerate}
\item If a test case fails subsequently because the output for scenario \texttt{S} is not in\texttt{ ACCEPTABLE\_OUTPUTS(S)}, check if this output is in fact acceptable.  If it is, add it to \texttt{ACCEPTABLE\_OUTPUTS(S)}.  If it is not acceptable, report a bug.
\end{enumerate}
\caption{Methodology for creating test cases\index{test cases} for systems with variable output.}\label{fig:testcasemethod}
\lineacross
\end{figure}



\begin{figure}
\lineacross\\
For scenario S = \{\texttt{name: Joe's, cost:moderate, cuisine:Italian, familyFriendly:yes}\}
\begin{enumerate}
\item Run the system 10 times (100 or 1000 times is better for production usage), producing
\begin{itemize}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\item \lingform{Joe's is a family-friendly and moderately priced Italian restaurant.}
\item \lingform{Joe's is a family-friendly and moderately priced Italian restaurant.}
\item \lingform{Joe's is an Italian family-friendly moderately priced restaurant.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\item \lingform{Joe's is an Italian family-friendly moderately priced restaurant.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\end{itemize}
\item Remove duplicates, resulting in the following \texttt{OUTPUTS(S)}
\begin{itemize}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\item \lingform{Joe's is a family-friendly and moderately priced Italian restaurant.}
\item \lingform{Joe's is an Italian family-friendly moderately priced restaurant.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\end{itemize}
\item Remove unacceptable output, in this case \lingform{Joe's is an Italian family-friendly moderately priced restaurant.} (too hard to read).  This gives  \texttt{ACCEPTABLE\_OUTPUTS(S)}.  
\begin{itemize}
\item \lingform{Joe's is a moderately priced Italian restaurant.  It is family-friendly.}
\item \lingform{Joe's is a family-friendly and moderately priced Italian restaurant.}
\item \lingform{Joe's is an Italian family-friendly moderately priced restaurant.}
\item \lingform{Joe's is an Italian restaurant which is moderately priced and family friendly.}
\end{itemize}
\end{enumerate}
\caption{Example of Figre~\ref{fig:testcasemethod} methodology for creating test cases for NLG systems with variable outputs}\label{fig:testcasestrategy}
\lineacross
\end{figure}

\section{Maintenance}\label{sec:maintenance}\index{maintenance}
Of course successful software systems need to be supported\index{support} and maintained, and indeed software engineering\index{software engineering} tells us that most of the life-cycle cost\index{cost!life-cycle} of a successful software product is support and maintenance, not initial development \cite{davis200997}.    Maintenance includes fixing bugs\index{bugs} (and the more a system is used, the more bugs will surface) and also adapting systems to changes in available data, user needs,  IT\index{IT} infrastructure, regulatory\index{regulator} context, domain knowledge\index{domain!knowledge}, models, etc.

The Babytalk BT-Family\index{Babytalk!BT-Family} system \cite{mahamood-reiter-2012-working}. for example, was deployed and used in a hospital for a few years.  Although users were very positive, it became challenging to maintain the system because of the following issues:
\begin{itemize}
\item \emph{Data:} The hospital regularly updated its patient record system, which provided Babytalk's input data.  Adapting Babytalk to use new versions of the patient record system was a time-consuming task.
\item \emph{IT issues:} The servers used in the hospital changed, as did the hospital's computer security policies.  These again required changes to Babytalk.
\item \emph{Domain:} New medications and interventions were developed and deployed, and also the hospital acquired new sensors.  Babytalk's rules and domain knowledge needed to be updated  to include these, as well as changes in clinical procedures and guidelines.
\end{itemize}
Maintaining Babytalk was especially difficult because it was  built as a research system based on the data, IT, and domain knowledge at the time it was built.  The system was not designed be easily maintainable (e.g., to allow the above changes to be easily made via configuration files without needing coding changes), which meant that it became too difficult to maintain after a few years,

Similarly, our SumTime\index{SumTime} weather forecast generator \cite{REITER2005137} was operationally deployed for a few years, but maintaining it became increasingly difficult for similar reasons, so it also fell out of use.

Babytalk and SumTime were rule-based NLG systems, but similar problems arise with neural NLG systems.

Most of the maintenance challenges faced by Babytalk, SumTime, and other NLG systems are generic software maintenance challenges which apply to databases as well as AI systems, but there are a few issues which are more unique to NLG and AI.

\subsection{Changes in domain and user needs}
In my experience, clients using an NLG system usually ask for changes to wording and content.  In a weather forecast\index{weather forecasts} context, for example, clients may want to adjust how time phrases such as \lingform{later} are used (language), and may also want to adjust what the system communicates in extreme weather conditions (content).

These changes are straightforward at least in principle if rule-based\index{rule-based NLG} techniques are used, but more challenging with neural NLG.  In theory training corpora\index{corpus} can be updated based on client requests and used to retrain or fine-tune\index{fine-tuning} neural models, but this is a lot of work and usually does not provide fine control over what the system does.   If prompted models\index{models!prompted} are used, the client's requests can be included in the relevant prompt; this can work for a small number of requests, but becomes less effective as the number of such requests builds up over time.

A similar problem arises with domain changes, also known as \emph{domain shift} (Section~\ref{sec:domainshift}).  For example, as mentioned in Section~\ref{sec:domainshift}, during the Covid-19 pandemic\index{Covid pandemic}, activities which were previously acceptable (such as going to the pub for drinks after work) became unacceptable and indeed illegal (Figure~\ref{fig:covidadvice}).  Some dialogue systems became unusable because it was not possible to adapt them to the changed world of Covid-19, especially because Covid-19 restrictions frequently changed, and were different in different locations.

\begin{figure}
\lineacross\\
\lingform{If you are feeling depressed, go to the pub with some of your mates.}

\caption{Example of NLG output which became unacceptable and indeed encouraged illegal behaviour during the Covid-19 pandemic lockdown.}\label{fig:covidadvice}
\lineacross
\end{figure}

Neural NLG developers sometimes simply tell clients that this kind of maintenance is not possible, ie clients will not be able to request detailed specific changes in output texts or adapt the system to changes in the world.   This is a real issue and barrier for many clients, especially in safety\index{safety}-critical contexts where texts must be accurate and up-to-date; in consumer-facing contexts where texts must conform to and reinforce the company's brand\index{brand}; and in all contexts when the world changes radically (e.g., Covid-19).

Note that if a system is updated (whether rules are rewritten or models are retrained), then the system needs to be retested (Section~\ref{sec:testingnlg}) to check whether the updates have introduced any bugs\index{bugs} or other problems.  Because of the testing\index{testing} requirements, updates are usually done at a controlled frequency (e.g. monthly), they are not continuous.

\subsection{New users and use cases}\index{use cases}
Successful NLG systems will attract new types of users and be applied in new use cases.  This can lead to substantial changes in requirements\index{requirements}, which in turn require major changes to the system.
It is difficult to generalise, but the key rule for developers is to understand what is requested and how difficult it is to implement using their chosen technology.

For example, suppose a client requests that weather forecasts\index{weather forecasts} be produced in Spanish as well as English. If   a large language model\index{large language models} is used to generate the forecast, the model can simply  be asked to produce text in Spanish.   If rule-based NLG is used, then this can be done using a translation tool such as Google Translate.  Another possibility is to change the linguistic realisation\index{surface realisation} rules (Sec~\ref{sec:realisationTechniques}), although this may require more effort.

But what if the new language is a minority language which has few speakers and resources?  For example, I was once involved in discussions about producing weather forecasts (in Canada) in Inuit\index{Inuit} languages.  These are very different from Indo-European languages, not just in their lexicon but also in the way they communicate information; for example direction is indicated using locations (e.g., \lingform{from Great Slave Lake}) instead of via a compass direction such as \lingform{South}.  In this case, we probably cannot use Google Translate or simply instruct an LLM to produce Inuit text (they are unlikely to have enough training data in the target language, and may not be able to do the necessary geographic reasoning).  However we can still support this request in a rule-based NLG system by writing new linguistic expression rules and algorithms for Inuit.

\personal{A key principle with such changes to requirements (which applies to all software, not just NLG) is that it is essential to understand the technical challenges and effort required before agreeing to support these changes. I have seen cases where commercial and sales people agreed to changes without getting these checked by technical staff, perhaps under the assumption that \scare{this will be trivial for an LLM to do}; this is unwise.}

\subsection{Changes in models}\index{models}
An additional problem for systems that use proprietary language models\index{language models} is that many of these models are being constantly updated by vendors, with new training data and also new algorithms.  The updates are intended to improve the models, but the fact that they change behaviour means that an NLG system which uses such models needs to be frequently retested\index{testing}.
Even worse is when models disappear, in which cases systems which use them will no longer work.  This is a real concern at the time of writing with models from OpenAI; the company is constantly updating its models and also regularly depreciates and retires older models.

One approach to this problem is to use open-source language models instead of proprietary ones. This gives developers much more control and understanding of the models they use.

Models may also change because of legal\index{legal} issues (Section~\ref{sec:legal}), which again will raise maintenance and testing challenges.  At the time of writing, there are numerous lawsuits\index{lawsuits} which allege that large language models\index{large language models} are illegally created using content obtained from web sources.  If these succeed, then LLMs may need to be retrained on sources which explicitly allow usage as LLM training data; systems which depend on these LLMs may then need to be retested or even rebuilt.

\section{Further reading and resources}
\emph{AI safety}\index{safety} is evolving very quickly at the time of writing, which makes it difficult to recommend up-to-date material. 
A 2024 report from the UK DSIT \cite{distsafety} gives a  good overview of a broad range of safety issues and approaches in 2024
and is an excellent starting point for learning more about safety.

Governments are increasingly getting involved, for example with the US AI Safety Institute (\url{https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute}), the UK AI Safety Institute (\url{https://www.gov.uk/government/organisations/ai-safety-institute}), and the EU AI Act (\url{https://artificialintelligenceact.eu/}).  Leading vendors such as Google and OpenAI are creating or expanding teams to work on AI Safety.

In the academic literature, Amodei et al \cite{amodei2016concrete} is a \scare{classic} older paper about safety in machine learning systems which is still worth reading.  I am not aware of specific papers on safety in data-to-text NLG systems, but there are many useful and relevant papers about safety in dialogue systems.  For example, Dinan et al \cite{dinan-etal-2022-safetykit} present a \scare{SafetyKit} for dialogue systems, much of which is relevant to NLG. Abercrombie et al \cite{abercrombie-etal-2023-mirages} explore how anthropomorphism in dialogue systems may increase risks.  Historically companies and governments have shown more interest than academics in safety, but this is beginning to change.

There are many textbooks on \emph{software testing and quality assurance}\index{testing}\index{quality assurance}, and the topic is also covered in most books on software engineering\index{software engineering}.  Wikipedia is an excellent free resource; start at \url{https://en.wikipedia.org/wiki/Software\_testing} and follow links relevant to your interests.  There is some good work on testing NLP systems, such as Ribeiro et al \cite{ribeiro-etal-2020-beyond}, but very little explicitly on testing NLG systems.  However, some people have found my blog on the topic (\url{https://ehudreiter.com/2017/02/10/nlg-test-qa/}) to be useful.  There are numerous companies which sell tools or consultancy for testing AI systems, but I cannot recommend specific vendors in this book.

Similarly there are many textbooks on \emph{software maintenance}\index{maintenance}, which is again covered in most books on software engineering, but I am not aware of anything specifically on maintaining NLG systems.  It has frequently come up in commercial discussions I have had, but does not seem to be discussed in the research literature  (we once tried to write a paper on maintenance issues in SumTime, academic
 reviewers had little interest in the topic).  Within the general AI literature, there is work specifically on domain adaptation (e.g., see survey by Farahani et al \cite{farahaniDomain}), and  I am beginning to see papers on  maintaining AI systems more generally, such as \cite{Chen-computer}.  Hopefully more such papers will be published in the future.


