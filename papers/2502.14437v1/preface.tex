%%%%%%%%%%%%%%%%%%%%%%preface.tex%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% sample preface
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%

\preface

%% Please write your preface here
I have been working on natural language generation (NLG), that is using artificial intelligence techniques to produce texts in English and other human languages, since I got my PhD in this area in 1990.  In late 2022, NLG became much more prominent because of the impressive capabilities of large language models such as ChatGPT, which was exciting.  However, discussions about NLG in both academic and commercial circles have become focused on the latest developments in language models, with little attention was paid to what had been learned about NLG before ChatGPT.

My \textbf{goal} in this book is to present a broad overview of NLG which talks about language models, but also looks at alternative approaches to NLG, requirements (what users want NLG systems to do), evaluation, safety and testing, and sensible applications of NLG.  I hope that this broad perspective, which builds on decades of work on NLG, will be helpful to both researchers and developers who work in this area.

I co-authored a book about NLG in 2000, and I saw that while the technology content of my 2000 book quickly became out of date, people kept on using it in 2010 and even 2020 because the high-level conceptual and methodological material was still useful.  With this experience in mind, I have focused this book on high-level concepts and methodologies; my hope is that this material will still be useful in 2030 and perhaps even 2040.  I do not attempt to describe the latest technologies, because this information quickly becomes dated (indeed, anything I write in June 2024 will probably be out-of-date by the time the book is published); readers interested in the latest developments in language models should look elsewhere.

In many places I show outputs from ChatGPT and other large language models.  Most of these were all produced in 2023 (I deliberately do not include version numbers or specific dates), and readers should bear in mind that models in 2025, let alone 2030, may produce different outputs.  However, the high-level points I am making should still be valid.

I also focus in this book on my own experiences.  Where possible I use examples from systems which I have worked on or otherwise been involved with, even if they are not the best known system in their area; for instance for this reason I talk about the (somewhat obscure) BLOOM language model as well as better-known ones such as GPT.
More generally the book focuses on data-to-text NLG (systems which use NLG to summarise and explain non-linguistic data), because this is my personal interest.  I also include personal notes throughout the book.   I hope this personal focus makes the book more interesting to readers.



Specifically, the book has the following \textbf{chapters}:
\begin{itemize}
\item \emph{Introduction:} I present some example NLG systems, summarise the content of the rest of the book, and also give a short history of NLG.
\item \emph{Rule-based NLG:} I describe how AI systems can generate texts using algorithms and rules which explicitly make decisions about the content and language of generated texts.  Rule-based NLG has been overshadowed by neural NLG in recent years, but it is still the best way to build some NLG applications.  Rule-based NLG also shows the types of decisions which need to be made in text generation, and I think a good understanding of this helps anyone working in NLG, even if they use other approaches.
\item \emph{Machine learning and neural NLG:} I give an overview of machine learning and neural approaches to NLG, including language models.  This area is changing very rapidly, and models which were exciting state-of-art a few years ago are now obsolete and forgotten.  Because of this, I just give a high-level overview of basic concepts behind models, and then discuss data and other issues which are important regardless of the model used.
\item \emph{Requirements:} As with any type of software, knowing what users and stakeholders are looking for is essential in building a successful NLG application.  I look at some of the different quality criteria that people may care about, workflows for using NLG (including \scare{human-in-loop}), textual vs graphical presentation of information, and methodologies for understanding (acquiring) requirements.
\item \emph{Evaluation:} This is the longest chapter of this book, which reflects my interest in the topic as well as its importance.  From both a scientific and practical perspective, it is essential to evaluate how well NLG algorithms, models, and techniques work, using experiments which are rigorous and replicable.  I discuss basic evaluation concepts, and then describe techniques for human and automatic (metric) evaluation.  I also look at evaluating real-world impact of NLG systems, as well as commercial evaluation.
\item \emph{Safety, testing, and maintenance:}  Society expects that AI systems used in the real-world will be safe (not harm users or third parties); systems which are not safe will not be allowed by governments and regulators.  I examine safety concerns and techniques in NLG, and also look at software testing, which is used to identify bugs and other problems which could lead to unacceptable behaviour.  I conclude with a section on maintaining NLG systems, which is very important (most of the lifecycle costs of software systems are in maintenance) but poorly understood.
\item \emph{Applications:} NLG is not just an academic discipline, it is also a technology which can be used to build useful applications which help people.  In this chapter I discuss some fundamental issues (such as scalability), and then look in more detail at four areas which NLG has been used commercially for many years: journalism, business intelligence, summarisation, and medicine.  Lessons from these long-standing NLG use cases can be applied to newer applications of NLG.
\end{itemize}

It is impossible for me to \textbf{acknowledge} all of the people who have helped me in my NLG career. I would like to give special thanks to my faculty colleagues in the Aberdeen NLG group over the years, including  Kees van Deemter, Jim Hunter,  Ruizhe Li, Chengua Lin, Judith Masthoff, Chris Mellish, Graeme Ritchie, Advaith Siddharthan,  Arabella Sinclair, Yaji Sripada, and  Wei Zhou.  I'd also like to thanks the many PhD students and post-docs I have worked with at Aberdeen, including Jawwad Baig, Simone Balloccu, Daniel Braun, Martin Dempster, Albert Gatt, Rodrigo Gomes de Oliviero, Francois Portet, Stephanie Inglis, Kitt Kuptavanich, Jing Lin, Saad Mahamood, Meg Mitchell, Wendy Moncur, Francesco Moramarco, Joe Reddington, Roma Robertson, Jaime Sevilla, Adarsa Sivaprasad, Mengxun Sun, Barkavi Sundararajun, Iniakpokeikiye Thompson, Craig Thomson, Nava Tintarev, Ross Turner, Sandra Williams, and Jin Yu.  The Aberdeen University NLG group is by no means the largest NLG group in the world, but it has provided a great environment for me to pursue my interests.

Of course funding is very useful for university research, and I am very grateful to Australian Research Council, Cancer Research UK, the EU Horizon and Marie-Curie programmes, the Royal Society of Edinburgh, the Scottish Chief Scientist Office, and the UK Economic and Social Research Council for their support.  Special thanks to the  the UK Engineering and Physical Sciences Research Council,  which has funded my projects over a 30-year period.  I am also grateful to IBM and the US National Science Foundation for supporting my PhD studies at Harvard.

In 2009 I co-founded a company, Data2Text, which was acquired by Arria NLG in 2013.  I worked at Data2text and Arria for many years, and my experiences there shaped much of the content of this book.  At Arria, I am especially grateful to John Alexander, Neil Burnett, William Bradshaw, Robert Dale, Sharon Daniels, Ian Davy, Jay Dewalt, Cathy Herbert, Alasdair Logan, Lyndsee Manna, Daniel Paiva, John Perry, Kapila Ponnamperuma, Jette Viethen, and Keith Wisemann for their help in understanding the commercial perspective on NLG, as well as many colleagues from Aberdeen University (mentioned above) who spent time at Data2text or Arria.

Numerous people have helped me with this book.  I'd especially like to thank the people who reviewed and gave comments on draft material: Kees van Deemter, Nick Diakopoulos, Albert Gatt, Emiel Krahmer, Emiel van Miltenburg, Verena Reiser, and Ross Turner.  Extra-special thanks to Saad Mahamood, who reviewed and commented on a draft of the complete book. Of course I alone am responsible for any mistakes in this book!

Finally, I am grateful to my wife Ann and my children Miriam, Moshe, and Naomi for putting up with me while I worked on this book!


\vspace{\baselineskip}
\begin{flushright}\noindent
Aberdeen, Scotland\\
%Aberdeen, Scotland,\hfill {\it Ehud Reiter}\\
June 2024\hfill {\it Ehud Reiter}\\
\end{flushright}


