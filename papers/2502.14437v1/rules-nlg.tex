%%%%%%%%%%%%%%%%%%%%% chapter.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample chapter
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%%%%%%%%%% Springer-Verlag %%%%%%%%%%%%%%%%%%%%%%%%%%
%\motto{Use the template \emph{chapter.tex} to style the various elements of your chapter content.}
\chapter{Rule-based NLG}
\label{rules} % Always give a unique label


\abstract*{This chapter describes how AI systems can generate texts using algorithms and rules which explicitly make decisions about the content and language of generated texts.  Rule-based NLG has been overshadowed by neural NLG in recent years, but it is still the best way to build some NLG applications.  Rule-based NLG also shows the types of decisions which need to be made in text generation.  A good understanding of this helps anyone working in NLG, even if they use other approaches.}


Natural Language Generation systems can be built using algorithms and rules which explicitly extract insights from texts, structure information into narratives\index{narrative}, and create good linguistic expressions of information; this is called \emph{rule-based NLG}\index{rule-based NLG}.  Rule-based NLG is especially common in data-to-text\index{data-to-text} NLG; it is rare in text-to-text NLG.

The advantage of rule-based approaches to NLG is that developers have complete control\index{controllability} over what the system does, and the system is also testable\index{testing} and auditable\index{auditable}.  In other words, rule-based NLG is a \scare{precision} approach which lets developers build NLG systems with exactly the functionality which clients want, and which (assuming no software bugs) will never go \scare{off the rails} and do crazy or inappropriate things.

The disadvantage of rule-based NLG is that writing (and debugging) the algorithms and rules can require a considerable amount of work, especially for NLG systems which generate complex texts.  Of course good modularisation, structure, tools, and libraries will help (as with all kinds of software development), but writing rules for a large NLG system still can be a daunting task.

Even developers who do not use rule-based NLG will still benefit from understanding it, because it gives a good understanding of the sorts of processes and decisions which NLG systems must do.  For example, suppose an NLG developer uses neural language model technology (Chapter~\ref{neuralnlg})  to build an NLG system which summarises sensor data.  The rule-based perspective shows that most such systems must detect and remove noise (Section~\ref{sec:signalanalysis}).  Unfortunately, many neural language models are not very good at removing noise from sensor data, so they will need help with this task (a separate preprocessor for noise filtering?).

This kind of analysis is only possible if developers understand what an NLG system needs to do at a conceptual level. Such understanding also makes it easier to discuss requirements\index{requirements} with users (Chapter~\ref{requirements}) and to design appropriate evaluation schemes (Chapter~\ref{evaluation}),


\section{NLG Pipeline\index{pipeline}}\label{sec:pipeline}
The simplest rule-based systems use \emph{templates}\index{templates} to generate output texts from input data (Section~\ref{sec:templatenlg}); such functionality is provided by many packages, include Python's Jinja\index{Jinja} library\footnote{\url{https://jinja.palletsprojects.com/}}.  More sophisticated rule-based NLG systems structure the generation process into separate modules, which are often connected together into a \emph{pipeline} (the modules run in sequence, for example the second module does not start until the first module has finished).  In data-to-text\index{data-to-text}, the most common modularisation is the data-to-text pipeline architecture \cite{reiter-2007-architecture}, which divides the process into \emph{Signal analysis}\index{signal analysis}, \emph{Data interpretation}\index{data interpretation}, \emph{Document planning}\index{document planning}, \emph{Microplanning}\index{microplanning}, and \emph{Surface realisation}\index{surface realisation} (Figure~\ref{fig:pipeline}).

\personal{I proposed this data-to-text NLG architecture in a 2007 paper \cite{reiter-2007-architecture}, which was awarded a \scare{Test of Time} award in 2022 as one of the most influential NLG papers published in an INLG conference before 2022.}

\begin{figure}
\lineacross{}\\
\includegraphics[scale  = 0.75]{nlgpipeline}
\caption{The data-to-text NLG pipeline.}
\label{fig:pipeline}       
\lineacross{}
\end{figure}


Not all systems include all of the above steps.  For example we do not need to extract insights from data if they are directly present in the system's input, and we do not need to design texts at a document level if the use case only requires a single sentence to be produced.

 A variety of techniques can be used within the modules, including:
\begin{itemize}
\item \emph{Algorithms}:   We can use algorithms for many tasks, such as pattern detection within signal analysis
\item \emph{Rules}: We can ask domain experts\index{domain!experts} how they do a task, and encode their response into the NLG system using rules.   For example, if we want to choose an appropriate verbal phrase (part of microplanning) to describe stock market changes (eg \lingform{inching up} or \lingform{skyrocketing}), we could ask experts (journalists\index{journalism} and financial experts) how they choose verbs in this context, and write rules based on this information.
\item \emph{Neural or other machine learning}: We can also use neural or ML approaches within modules (Section~\ref{sec:modularML}).  For example, for the verb choice task, we can build an ML model by analysing a corpus of historical financial reports \cite{chen-yao-2019-closer}.
\end{itemize}
The distinction between rules and algorithms is fuzzy.  In theory algorithms define computational processes which can be reused in multiple NLG systems; they are usually implemented in programming languages such as C++, Python, or Java.  Rules encodes domain/genre/usecase specific information which can only be used in this domain\index{domain!knowledge}, genre\index{genre}, and use case\index{use cases}; they can be defined in special rule languages, database or spreadsheet rows, or scripting languages.   In practice it is often difficult to clearly separate rules and algorithms, especially since algorithms can be parametrised in a rule-like fashion for specific domains, genres, and use cases.  Also rules can invoke algorithmic-processing to make decisions.

In this book I will use \scare{rule-based NLG} to cover both rules and algorithms (which is what most academic researchers do), and not explicitly differentiate between these.  The key issue is that both rules and algorithms require writing code to do NLG tasks, and as such are very different from the data-based approaches described in Chapter~\ref{neuralnlg}.

\section{Examples}
In this section I introduce two example rule-based NLG systems, which I will use throughout this chapter to illustrate rule-based NLG.

\subsection{DrivingFeedback}\label{sec:saferdriver}
The first example is a highly simplified version of the SaferDriver\index{SaferDriver} (Figure~\ref{fig:saferdrivermultimodal}) system \cite{braun_reiter_siddharthan_2018}.   The \emph{DrivingFeedback}\index{DrivingFeedback} system takes as input GPS-type data showing the speed of a vehicle at regular timestamps, together with street name and speed limit information (which is extracted from the GPS data using reverse geocoding).  The system produces regular reports on unsafe driving from this data.  Table~\ref{tab:drivingfeedbackdata} shows example input data, and Figure~\ref{fig:drivingfeedbackpipeline} shows how this data is processed by the different stages of the data-to-text pipeline, in order to produce the sentence   \lingform{You sped twice on King Street} (a full report is several paragraphs long).

At a high level, DrivingFeedback works as follows
\begin{itemize}
\item \emph{Signal analysis:}\index{signal analysis} identifies unsafe driving incidents, such as speeding.
\item \emph{Data interpretation:}\index{data interpretation} clusters related unsafe driving incidents, such as multiple speeding incidents on the same road.
\item \emph{Document planning:}\index{document planning} selects most important clusters from a safety perspective.  Also organises content so that the report starts with positive feedback (ie, congratulations on improvements from previous report) if possible, since this enourages people to read the report and take it seriously.
\item \emph{Microplanning:}\index{microplanning} vary wording so that reports read differently from previous reports even if they have similar content.
\item \emph{Surface realisation:}\index{surface realisation} create grammatically correct texts.
\end{itemize}

\begin{table}
\lineacross{}\\
\begin{tabular}{|r|r|l|} \hline
\emph{time} & \emph{speed} & \emph{street} \\ \hline
9:00:00 & 30 & King Street \\
9:00:01 & 32 & King Street \\
9:00:02 & 35 & King Street \\
9:00:03 & 30 & King Street \\
9:00:04 & 33 & King Street \\
9:00:05 & 32 & King Street \\
9:00:06 & 27 & King Street \\
9:00:07 & 25 & King Street \\
9:00:08 & 30 & King Street \\
9:00:09 & 33 & King Street \\
9:00:10 & 33 & King Street \\
9:00:11 & 30 & King Street \\
9:00:12 & 25 & King Street \\
9:00:13 & 20 & King Street \\
9:00:14 & 15 & King Street \\
9:00:15 & 15 & St Machar Drive \\
9:00:16 & 25 & St Machar Drive \\
9:00:17 & 30 & St Machar Drive \\
9:00:18 & 32 & St Machar Drive \\
9:00:19 & 32 & St Machar Drive \\
9:00:20 & 30 & St Machar Drive \\ \hline
\end{tabular}
\caption{Example DrivingFeedback data. Speed limit is 30 on both King Steet and St Machar Drive.}
\label{tab:drivingfeedbackdata}
\end{table}

\begin{figure}
\textbf{Output of signal analysis} (find speeding segments)
\begin{itemize}
\item Speeding(9:00:01-9:00:05, King Street, maxSpeed=35)
\item Speeding(9:00:09-9:00:10, King Street, maxSpeed=33)
\item Speeding(9:00:18-9:00:19, St Machar Drive, maxSpeed=32)
\end{itemize}

\textbf{Output of data interpretation} (cluster related speeding segments)
\begin{itemize}
\item SpeedingCluster(9:00:01-9:00:10, King Street, maxSpeed=35, incidents=2)
\item Speeding(9:00:18-9:00:19, St Machar Drive, 32)
\end{itemize}

\textbf{Output of document planning} (choose which insights to include)
\begin{itemize}
\item Sentence: SpeedingCluster(9:00:01-9:00:10, King Street, maxSpeed=35, incidents=2)
\end{itemize}

\textbf{Output of microplanning} (design sentences to express insights)
\begin{itemize}
\item Sentence: Subject=\lingform{you}, verb=\lingform{speed} (past tense), modifier=\lingform{twice}, location=\lingform{on King Street}
\end{itemize}

\textbf{Output of surface realisation} (create actual text)
\begin{itemize}
\item Sentence: \lingform{You sped twice on King Street.}
\end{itemize}

\caption{Processing DrivingFeedback data through the data-to-text pipeline.}
\label{fig:drivingfeedbackpipeline}
\lineacross
\end{figure}

\subsection{ Babytalk}\label{sec:babytalk}
A more complex rule-based data-to-text system is \emph{Babytalk}\index{Babytalk}.  Babytalk is actually a family of systems which generate texts about babies in a neonatal intensive care unit\index{health} (examples are shown in Figure~\ref{fig:BabytalkOutputs}):
\begin{itemize}
\item \emph{BT45}\index{Babytalk!BT45} generates summaries of recent activity, which are intended to help doctors\index{doctors} and nurses\index{nurses} make decisions about interventions \cite{PORTET2009789}.
\item \emph{BT-Nurse}\index{Babytalk!BT-Nurse} generates shift-handover reports for nurses who are starting a 12-hour shift; the report summarises what happened in the previous shift, and also in earlier shifts (in case the nurse has not dealt with the baby before) \cite{HUNTER2012157}.
\item \emph{BT-Family}\index{Babytalk!BT-Family} generates daily reports for parents, so that they know the status of their baby \cite{mahamood-reiter-2011-generating}.
\end{itemize}
All of these systems extract information from the hospital's electronic patient record.  At least at a conceptual level, they have a similar architecture but use different data-interpretation\index{data interpretation} and document planning\index{document planning} rules and algorithms in order to select appropriate insights for the different use cases. BT-Family also uses a different set of linguistic expressions (microplanning\index{microplanning}) rules than BT45 or NT-Nurse, since it produces texts for non-specialists (parents) instead of domain experts (doctors and nurses).
%The other modules (signal analysis and surface realisation) are similar for all Babytalk systems.

\begin{figure}
\lineacross\\
\emph{Example BT45 output (extract)}:\\
By 11:00 the baby had been hand-bagged a number of times causing 2 successive bradycardias. She was successfully re-intubated after 2 attempts. The baby was sucked out twice. At 11:02 FIO2 was raised to 79\%.\\

\emph{Example BTNurse output (extract)}:\\
\textbf{Respiratory Support}\\
\textbf{Current Status}\\
Currently, the baby is on CMV in 27 \% O2. Vent RR is 55 breaths per minute. Pressures are 20/4 cms H2O. Tidal volume is 1.5.\\

SaO2 is variable within the acceptable range and there have been some desaturations.\\

\emph{Example BTFamily output (extract)}:\\
John was in intensive care. He was stable during the day and night. Since last week, his weight increased from 860 grams (1 lb 14 oz) to 1113 grams (2 lb 7 oz). He was nursed in an incubator.

\caption{Example outputs from Babytalk systems}
\label{fig:BabytalkOutputs}
\lineacross
\end{figure}



At a high level, the Babytalk architecture works as follows
\begin{itemize}
\item \emph{Signal analysis}\index{signal analysis} is done using standard tools, such as pattern detection and noise identification algorithms.
\item \emph{Data interpretation} is quite complex, in part because the system tries to detect and recover from some types of input errors (eg, incorrect time stamps in the patient record).  Much of this is done using \emph{production rules}.
\item \emph{Document planning} is done using a combination of fixed \emph{schemas} and dynamic narrative-creation algorithms.
\item \emph{Microplanning} is done using algorithms for reference, aggregation, lexical choice, and other microplanning tasks.
\item \emph{Surface realisation}\index{surface realisation}  is done using the \emph{Simplenlg}\index{simplenlg} library \cite{gatt-reiter-2009-simplenlg}.
\end{itemize}
Babytalk's processing is much more complex than DrivingFeedback's processing; at the time of writing, it is one of the most sophisticated rule-based NLG systems ever built.

\section{Signal analysis}\label{sec:signalanalysis}
The first stage of the data-to-text\index{data-to-text} pipeline\index{pipeline} is \emph{signal analysis}\index{signal analysis}, that is finding patterns in the input data using signal processing techniques.  

\subsection{Noise detection: Principles}\label{sec:noise}

Part of signal analysis is \emph{noise detection\index{noise detection} and removal}; real-world sensor data is noisy and hence sensor data may not be an accurate measurement of real-world events.  For example, the DrivingFeedback\index{DrivingFeedback} driving data in Figure~\ref{tab:drivingfeedbackdata} shows a period of speeding from 9:00:01 to 9:00:05, except for a single data point at 9:00:03 where the speed drops to 30.  Since GPS data is noisy (position measurements are not exact), DrivingFeedback treats this as noise, and reports a single speeding segment from 9:00:01 to 9:00:05 (Figure~\ref{fig:drivingfeedbackpipeline}).

Babytalk\index{Babytalk} systems got data from sensors attached to babies in a neonatal intensive care unit.  When the baby kicked or was picked up by a nurse, the sensors would often show dramatic spikes or other changes which were purely due to the sensor momentarily losing contact with the baby's skin.  An example is shown in Figure~\ref{fig:btdata}; heart rate drops to zero at several points, and this is noise (the baby's heart did not stop beating!).
At any rate, these artefacts needed to be identified and removed so that Babytalk could focus on actual changes instead of sensor artefacts.

\personal{In the DrivingFeedback and Babytalk contexts,  it at least is possible for noisy sensors to be replaced.  I have worked on projects for oil\index{oil industry} companies where we used sensor data from sensors deep inside an oil well; it is not possible to replace such sensors if they start misbehaving.}

\begin{figure}
\includegraphics[scale  = 0.75]{babytalkHRdata}
\caption{Noisy data (HR, heart rate) from Babytalk.  The data shows HR falling to 0, but the baby's heart did not actually stop beating!}
\label{fig:btdata}       
\end{figure}



\subsection{Pattern detection: Principles}\index{pattern detection}

Once noise processing is done, the main signal analysis task starts, which is detecting patterns in the data.   Which patterns are worth detecting depends on the domain.  Statistics such as mean, range, and standard deviation are often useful.  With time-series\index{time series} data, in many use cases users want to know about spikes, trends, oscillations, and periods where the sensor data is outside of an acceptable range (such as driving speed being above the speed limit).  But there are also specialised patterns which are important in some some domains but not others.

Pattern detectors in NLG systems must find patterns that human readers recognise.  Many spike detectors, for example, use mathematical algorithms which trigger on things which do not look like spikes to human users; this is fine for most analytics\index{analytics}, but not for NLG.  In general, we want patterns that make sense to readers; finding such patterns is called \emph{articulate analytics}\index{analytics!articulate analytics}.

One example of articulate analytics is when NLG systems use linear approximation to describe time series data.  For example, suppose we wanted to describe driving behaviour on St Machar Drive for the data in Figure~\ref{tab:drivingfeedbackdata}, for the period 9:00:15 to 9:00:18, where the driver is increasing speed.  We could use:
\begin{itemize}
\item a simple \emph{linear interpolation}\index{linear interpolation}, and state the speed at the beginning and end of this period.  An example is \emph{After turning on to St Machar Drive, you speeded up from 15 to 32 mph}. 
\item a \emph{linear regression}\index{linear regression} (trend line) to create a \scare{best-fit} segment to describe this driving behaviour, as shown in Figure~\ref{fig:regression}.  An example is \emph{After turning on to St Machar Drive, you speeded up from 17 to 33 mph}.
\end{itemize}
The linear regression is a better fit to the actual data, and hence used by many analytics algorithms.  However, in an NLG context, most readers find the regression text confusing and prefer the interpolation text \cite{sripada:kdd03}.  Hence linear interpolation should be used by data-to-text systems in such contexts, even though regression is more popular in many other analytical contexts

\begin{figure}
\includegraphics[scale  = 1]{regression}
\caption{Regression trend line for speed data.  Initial value is 15 mph and final value is 32 mph.  Regression line is 17 mph at the start of the period, and 33 mph at the end. Readers prefer to see this data described as \lingform{you speeded up from 15 to 32 mph} (with actual values at beginning and end of period) instead of \lingform{you speeded up from 17 to 33 mph} (regression line)}.
\label{fig:regression}       
\end{figure}

\subsection{Techniques for signal analysis}
Signal analysis is usally done with standard noise suppression and pattern detection algorithms; the NLG developer generally selects what he or she thinks is the most appropriate algorithm from a standard pattern analysis library such as Python's scipy\index{scipy}, and perhaps with some domain tuning.

For example, Babytalk\index{Babytalk} used fairly standard auroregressive modelling for noise detection, which was tuned on a clinical dataset where artefacts had been manually annotated \cite{HUNTER2012157}.   Babytalk used a variety of algorithms for pattern detection, including bottom-up segmentation for detecting trends \cite{Keoghsegmentation}.


\section{Data interpretation}
The next stage of the data-to-text\index{data-to-text} pipeline\index{pipeline} (after signal analysis) is \emph{data interpretation}\index{data interpretation}.  The goal of data interpretation is to extract useful \emph{insights}\index{insights} (sometimes called \emph{messages}\index{messages}) from the patterns detected by signal analysis\index{signal analysis}, and indeed from the raw data in some cases.  This step is of critical importance in data-to-text; an NLG system which presents useless or (even worse) incorrect insights is not going to be useful regardless of the quality of the language it generates. From a practical perspective, it is not at all unusual for NLG projects to devote more developer time to data interpretation than anything else.

\subsection{Principles}\label{sec:dp-principles}

The insights detected by data interpretation are quite varied, but often can be characterised as one of the following
\begin{itemize}
\item \emph{Abstraction:}\index{data interpretation!abstraction} combining patterns from signal analysis into a higher-level insight.  For example, clustering together individual \semantic{Speeding} patterns into a \semantic{SpeedingCluster} insight, as shown in Figure~\ref{fig:drivingfeedbackpipeline}.
\item  \emph{Interpretation:}\index{data interpretation!interpretation}  Interpreting patterns.   For example, DrivingFeedback\index{DrivingFeedback} could interpret a \semantic{Speeding} pattern or \semantic{SpeedingCluster} insight as being \lingform{very dangerous} if the maximum speed exceeded the speed limit by 20 mph or more.
\item \emph{Linkage:}\index{data interpretation!linkage}  Relating events; this is very important for generating cohesive narrative instead of a bullet list of insights.  For example, if a driver did some speeding at the beginning of a trip and afterwards did no speeding, the system could say \lingform{You speeded on King St, but afterwards drove safely}.  Here, \lingform{but} links the two insights \lingform{You speeded on King St} and  \lingform{afterwards drove safely}.
\item \emph{Importance:}\index{data interpretation!importance}  How important and relevant are insights to the user?  In the example shown in Figure~\ref{fig:drivingfeedbackpipeline}, for example, we might give more importance to \semantic{SpeedingCluster(9:00:01-9:00:10, King Street, maxspeed=35, incidents=2)} than to \semantic{Speeding(9:00:18-9:00:19, St Machar Drive, 32)}, since the cluster is over a longer time frame and has a higher maximum speed.
\end{itemize}

One especially important type of Linkage is \emph{casual reasoning}, for example \lingform{the baby's heart rate increased because of a nappy change}.  Causal reasoning is difficult, but if it can be done, adding causal links increases the quality and usefulness of generated narratives.

\subsection{Techniques}

Data interpretation is domain-dependent and is based on what insights users want to know.  It is usually is done by writing  rules or algorithms in consultation with users and domain experts\index{domain!experts}; these rules are based on domain knowledge\index{domain!knowledge} and often use domain-specific data analytics and mining techniques.

Once an NLG system is running, users (and domain experts) often want to modify and tweak data interpretation logic; in DrivingFeedback, for example, a user might tell the system to ignore minor speeding incidents (less than 5 mph over the speed limit).  Some commercial NLG systems have user interfaces which allow users or  domain experts to inspect and modify data interpretation logic.

It should be possible in many cases to learn data interpretation models using machine learning, and hopefully we will see more work on this in the future.  Because domain experts want to understand interpretation rules (as mentioned above), it often makes sense in this context to use ML techniques which produce interpretable models\index{models!interpretable} (such as linear regression and decision trees) which can be inspected, checked, and updated by domain experts; non-interpretable models (such as neural models) are often less desirable, even if they perform well.

Data interpretation can help in dealing with noise\index{noise detection}, in contexts where noise can best be detected by rules and consistency checks instead of by pattern analysis algorithms.
In Babytalk's\index{Babytalk} data, for example, timestamps of actions were often incorrect.  When clinicians performed surgery or other interventions, a nurse entered this information into the patient record, along with the time that the procedure was done.  However this was usually done after the surgery finished, and sometimes nurses did not remember the exact timing of the intervention, especially if the nurse had to look after the baby immediately afterwards, and did not get around to updating the patient record until much later.  Pilot studies  suggested that human readers could be very confused by incorrect intervention times, especially if this meant that intervention times did not link up with the sensor data (e.g., blood oxygen seemingly rose before the oxygen level in the baby's incubator was raised, instead of afterwards).  For this reason, Babytalk's data interpretation module attempted to identify and fix incorrect timestamps, by looking for the expected sensor signature of the intervention in the sensor data.


\section{Document Planning}
The third stage of the pipeline\index{pipeline} is \emph{Document Planning\index{document planning}}.  Its goal is to determine the content\index{content} and structure of the generated text.   Although in theory separable, in practice decisions about content and structure tend to be connected, which is why they are usually grouped together into this stage.

\subsection{Principles}

Document Planning is the interface between the analytics\index{analytics} part of the pipeline (signal analysis and data interpretation) and the linguistics\index{linguistics} part of the pipeline (microplanning and surface realisation).  Decisions about which content to include are related to calculations of importance (data interpretation)\index{data interpretation!importance}, while decisions about document structure\index{document structure} influence linguistic decisions such as where to place sentence and paragraph breaks (Section~\ref{sec:aggregation})

In the DrivingFeedback\index{DrivingFeedback} example we are using, the Document Planning module decides that the text should include the \emph{SpeedingCluster} (King Street) insight but not the \emph{Speeding} (St Machar Drive) pattern.  This decision is primarily driven by importance (discussed above).

In some cases, it is useful to add unimportant insights\index{insights} because they improve narrative\index{narrative} coherence\index{coherence}.  For example, the Babytalk\index{Babytalk} system gave higher importance to sudden changes in sensor data than to slower changes, since sudden changes are more significant clinically.  The first version of the Babytalk document planner only selected high-importance insights, which led to texts such as

\begin{quote}
\lingform{TcPO2 suddenly decreased to 8.1. SaO2 suddenly increased to 92. TcPO2 suddenly decreased to 9.3}
\end{quote}
\noindent{}Users complained that it made no sense for TcPO2 to decrease to 9.3 when it had previously decreased to 8.1.  In this case, TcPO2 had risen between the events, but slowly, so the rise was not considered clinically important; hence it was not mentioned.  But narrative coherence is better if we nevertheless mention the rise \cite{reiter-etal-2008-importance}, for example by adding the phrase \lingform{After increasing to 19}:
\begin{quote}
 \lingform{TcPO2 suddenly decreased to 8.1. SaO2 suddenly increased to 92. After increasing to 19, TcPO2 suddenly decreased to 9.3}.
 \end{quote}

\subsection{Techniques}

From an algorithmic perspective, most NLG systems use simple scripts\index{scripts} or schemas\index{schemas} for document planning.  In other words, a script or piece of code defines the overall document structure, and also specifies which insights should be included in the document at different points; of course the script can include conditionals, since the structure may depend on the insights produced by data interpretation.  A simple DrivingFeedback example is shown in Figure~\ref{sec:docplanscript}.


\begin{figure}
\lineacross\\
\begin{verbatim}
procedure DocumentPlan(Insights, importance function, importance threshold)
  
  SelectedInsights = all Insights with importance above a threshold
  
  # we need at least one insight
  if SelectedInsights is empty, add the highest importance Insight
  
  # insights should be presented in time order in the text
  OrderedSelectedInsigts = SelectedInsights ordered by start time of Insight
  
  return OrderedSelectedInsights
\end{verbatim}
\caption{Pseudocode for document-planning script for DrivingFeedback}\label{sec:docplanscript}
\lineacross
\end{figure}
  

More complex approaches are possible.  For example, the Babytalk system uses scripts (in the above sense) for part of the documents it generates, but for other parts it uses a \emph{key event} algorithm which selects a small number of key insights about events, and then creates a paragraph around each of these events by finding linked\index{data interpretation!linkage} insights, such as causes and consequences of the key event \cite{PORTET2009789}.

Attempts have been made to treat document planning as a reasoning task, where the system explicitly reasons about what information should be in the generated text \cite{APPELT19851}; however it is difficult to get this approach to work robustly in real-world contexts.  Attempts have also been made to use psycholinguistic\index{psycholinguistics} principles to inform document structure decisions \cite{thomson-etal-2018-comprehension}, but it is not easy to translate psycholinguistic theories into workable NLG code.

Researchers are looking at using machine learning techniques to determine document structure \cite{puduppully-etal-2022-data}, but document planning seems quite challenging for machine learning.  In general neural NLG  systems seem to be better at expressing content than at selecting content.

One reason for the continuing strength of the script/code approach is that it makes it relatively easy to modify document content and structure if users request this; changes are much harder with the other approaches mentioned above.

The output of the document planning process is a \emph{document plan}\index{document plan}.  There is no agreed standard for representing document plans.  Some developers use trees inspired by Rhetorical Structure Theory\index{Rhetorical Structure Theory} \cite{mann1987rhetorical}, but ad-hoc data structures are more common.

\section{Microplanning}
The fourth stage of the NLG pipeline\index{pipeline} is \emph{Microplanning}\index{microplanning}.  Its goal is to decide how to linguistically express the selected insights\index{insights} and other messages\index{messages}; which words to use, what syntactic\index{syntax} structures to use, etc.   We can think of microplanning as tackling a number of conceptually discrete tasks, including \emph{lexical choice}\index{lexical choice}, \emph{reference}\index{referring expressions}, and \emph{aggregation}\index{aggregation}; see example in Table~\ref{tab:mptasks}.

\begin{table}
\begin{tabular}{|l|l|l|} \hline
Task & Version 1 & Version 2 \\ \hline
Lexical choice & John \emph{bought} a book for £10. & John \emph{purchased} a book for £10. \\ \hline
Reference & \emph{John} bought a book for £10. & \emph{He} bought a book for £10. \\ \hline
Aggregation & John bought a book for £10. & John bought a book. It cost £10.\\ \hline
\end{tabular}
\caption{Different Microplanning choices for expressing \semantic{buy(John,book,10)}.}
\label{tab:mptasks}
\end{table}

\subsection{Lexical choice}\label{sec:lexchoice}
\emph{Lexical choice}\index{lexical choice} is the task of choosing words to express concepts and insights\index{insights}.   In the DrivingFeedback\index{DrivingFeedback} example shown in Table~\ref{fig:drivingfeedbackpipeline}, for instance, the microplanner decides to express \semantic{incidents=2} using the word \lingform{twice}; it could also have chosen \lingform{on two occasions}, \lingform{on 2 occasions}, \lingform{several times}, etc.

\subsubsection{Lexical choice: Principles}\label{sec:lexprinciples}
The core lexical choice task is to map semantic\index{semantics} content onto words.  This is related to \emph{lexical semantics}\index{semantics!lexical semantics} in linguistics\footnote{\url{https://en.wikipedia.org/wiki/Lexical\_semantics}}\index{linguistics}.

Sometimes lexical choice is straightforward, but in other cases challenges arise.
One is that in many contexts it is important to vary which words are used, for example to express \semantic{incidents=2}  using \lingform{twice} in some cases but \lingform{on two occasions} in others.  \emph{Lexical variation}\index{lexical variation} makes texts more interesting and less repetitive, especially in contexts where users receive regular reports (such as weekly driving feedback reports).  We can also vary other linguistic choices, such as passive vs active voice, if this makes texts less repetitive to readers.

Another key issue is \emph{individual variability}\index{individual variability}, i.e. different people use words in different ways; for example my daughter describes as \lingform{purple} a shirt that I would call \lingform{pink}.  In the SumTime\index{SumTime} project on generating weather forecasts\index{weather forecasts} \cite{REITER2005137}, we empirically explored how forecasts readers and writers used and interpreted time phrases such as \lingform{by evening}.
We discovered that \emph{by evening} was used by some forecasters to mean 6PM and by others to mean midnight (Table~\ref{tab:sumtimebyevening}). We also worked with forecast readers, and discovered that a few readers thought the meaning of \emph{by evening} depended on sunset time and hence season, and/or on when people normally had their last meal of the day, and hence culture.

Other researchers have also found differences in how individuals user and understand words.  Berry et al \cite{berry2011} looked at how people interpreted phrases such as \emph{very common} which are used to describe risk\index{risk}, and discovered considerable variation, and also little agreement with usage recommended by official terminologies\index{terminology}.  Ramos Soto et al \cite{ramos-soto-etal-2018-meteorologists} found differences in how people used and interpreted geographic terms.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|}  \hline
time & F1 & F2 & F3 & F4 & F5 \\ \hline
1800 & \textbf{30} & 5 & 2 & \textbf{27} & 13 \\
2100 & 13 & 6 & 8 & 2 & 11 \\
0000 & 2 & \textbf{9} & \textbf{80} & \textbf{5} & \textbf{14} \\
other & 2 & 2 & 1 & 0 & 4\\ \hline
\end{tabular}
\caption{Number of times five forecasters (F1, F2, F3, F4, F5) used \lingform{by evening} to refer to different times in the SumTime weather forecast corpus. Most common usage by a forecaster is in \textbf{bold} font. F1 and F4 usually used \lingform{by evening} to refer to 1800, F3 usually used \lingform{by evening} to refer to 0000, and F2 and F5 were more varied in their usage \cite{reiter-cl02}.}
\label{tab:sumtimebyevening}
\end{table}

\subsubsection{Lexical choice: Techniques}

From an algorithmic perspective, in many cases developers can just write rules on how information is expressed.  For instance, write a rule that the information \semantic{incidents=2} is expressed by the word \lingform{twice} (or varies between \emph{twice} and \emph{on two occasions}).    Rules may depend on users.  For example a medical\index{health} system such as Babytalk\index{Babytalk} may express a medical concept such as \semantic{Desaturation} as \lingform{desatuation} to a clinician, but \lingform{temporary drop in blood oxygen saturation} to a patient who is not familiar with medical terminology (although sometimes patients object to \scare{dumbed-down} language and prefer medical terminology\index{terminology}).

Developers can also use use machine learning to build models for word choice and selection.  A number of researchers have investigated building models for verb choice in financial texts, such as \lingform{rose} or \lingform{soared}.  Chen and Yao \cite{chen-yao-2019-closer} analysed this task and essentially concluded that simple statistical models might be preferable to complex ML models.

ML and rules can be combined.  For example, in SumTime\index{SumTime} we essentially used ML to build an interpretable model\index{models!interpretable} to select time phrases; the data included an \semantic{author} feature.  We inspected the model to identify words and phrases whose meaning was author-dependent (such as \lingform{by evening}, see Table~\ref{tab:sumtimebyevening})  and then adapted the model (in consultation with a domain expert\index{domain!experts}) so that it never chose such words.
The result was a model which chose common words and phrases whose usage was stable across users (eg, phrases such as \lingform{by midnight}, which everyone agreed meant 0000).
User evaluations showed that this strategy worked well in selecting time phrases \cite{REITER2005137}.


\subsection{Generating Referring Expressions}
A \emph{referring expression}\index{referring expressions} identifies an entity to the reader.  For example, I can be referred to as \lingform{Ehud Reiter}, \lingform{Professor Reiter}, and \lingform{him}, amongst many other forms.  If a generated text refers to me, which of these forms should it use?    In a sense, choosing referring expressions is a specialised form of lexical choice\index{lexical choice}, but usually it has been treated as a distinct problem in the research literature.

\subsubsection{Referring expressions: Principles}

Reference often depends on \emph{context}\index{context}, which linguists call \emph{pragmatics}\index{pragmatics}.  Different referring expressions are appropriate in different contexts.  For instance, in our DrivingFeedback\index{DrivingFeedback} example, \lingform{King Street} is a reference to a particular street.  The referring expression can also provide information about the city, ie \lingform{King Street, Aberdeen}; this differentiates the street from \lingform{King Street, Dundee}.   If the reader knows from context (previous sentences, associated map, etc.) that the text is referring to driving in Aberdeen, then it is fine to just say \lingform{King Street}.  However if context does not give the city, then the text should say \lingform{King Street, Aberdeen} (or indeed \lingform{King Street, Aberdeen, UK} if context does not specify the country).

In general, referring expressions should uniquely identify their \emph{target}\index{reference target} in the current context.  For example, if I want to refer to the student Miriam Smith in a class where no other student is called Miriam, then I can just refer to her as \emph{Miriam}.  If however the class also contains student called Miriam Black, then \emph{Miriam} on its own is ambiguous, and I should instead use the referring expression \lingform{Miriam Smith}.

Reference is complex, in part because there are so many different forms; not just reference to people, but also to objects, times, geographic locations, abstract concepts, etc.   Many of these specific types of reference use specialised words and linguistic\index{linguistics} constructs which do not work for other types of reference, such as \lingform{yesterday} for temporal reference.
Also, in many cases the \emph{initial reference} to an entity in a text uses a different form from \emph{subsequent references}; for example \lingform{Professor Ehud Reiter} could be used as an initial reference to me, while \lingform{Reiter} is used subsequently.

\subsubsection{Referring expressions: Techniques}

A variety of algorithms have been proposed in the research literature, especially for the referential task of choosing a \emph{definite descriptions}\index{definite descriptions} such as \lingform{a big black dog} to refer to a visually or otherwise salient entity \cite{krahmerdeemter}.   Algorithms have also been proposed for pronoun\index{pronouns} usage \cite{kibble-power-2004-optimizing}, and for special types of reference such as referring to components of a complex machine \cite{reiter-2017-commercial}.  The above-mentioned research uses algorithms and rules, but there is also work on using neural techniques for reference \cite{CHEN2023101466}.    van Deemter \cite{vanDeemterReference} presents the reference problem from a cognitive science\index{cognitive science} perspective.

One problem with reference algorithms is that different referring expressions are used in different genres and contexts.  For example, pronouns are more common in informal writing than in legal documents and other types of very formal writing; this makes it difficult to propose universal pronoun-selection algorithms.  For this reason, commercial work on reference has put more emphasis on configurability\index{configurability} and flexibility, so that developers can easily configure the types of referring expressions used in their documents \cite{reiter-2017-commercial}.

Regardless of the specific algorithm used, the microplanner will need access to a model of relevant contextual information in order to choose referring expressions.

\subsection{Aggregation}\label{sec:aggregation}
\emph{Aggregation}\index{aggregation} is the task of packaging information into sentences.  For example, supose the DrivingFeedback\index{DrivingFeedback} document planner\index{document planning} had decided to communicate insights about speeding on St Machar Drive as well as speeding on King St (that is, both of the Data Interpretation insights shown in Figure~\ref{fig:drivingfeedbackpipeline}).  In this case, the information could be communicated in one sentence or in two sentences.
\begin{itemize}
\item You sped twice on King Street and once on St Machar Drive.  \emph{(one sentence)}
\item You sped twice on King Street.  You also sped on St Machar Drive.  \emph{(two sentences)}
\end{itemize}

\subsubsection{Aggregation: Principles}

In general, there are many ways of distributing information across sentences.   Since complex sentences may be difficult for below-average readers to understand (including many non-native speakers\index{non-native speakers}), while simple sentences are understood both by below-average and above-average readers, most NLG systems opt to use a large number of simple sentences. 

If related insights\index{insights} or messages\index{messages} are aggregated into the same sentence, sometimes \emph{ellision}\index{elision} can be used to reduce the length of the aggregated sentence.  For example, \lingform{Sales roles in Germany} and \emph{Sales fell in France} can be aggregated into \emph{Sales rose in Germany and fell in France} (instead of \emph{Sales rose in Germany and \underline{sales} fell in France}).  Elision can make a useful contribution to shortening texts and indeed making them more readable.

One issue with aggregation is that it can lead readers to make inferences\index{inferences} about how insights or messages are releated to each other.    For example, in the Babytalk\index{Babytalk} context, if the phrases \lingform{The nurse gave the baby morphine} and \lingform{The baby vomited} are aggregated into \lingform{The nurse gave the baby morphine and the baby vomited}, readers may infer a causal link, i.e. that the baby vomited \emph{because} of the morphine.  Which is misleading if something else (such as illness) could have caused the vomiting.
Because of such concerns, Babytalk used a very cautious and conservative aggregation strategy.

\subsubsection{Aggregation: Techniques}

From an algorithmic perspective, Harbusch and Kempen \cite{harbusch-kempen-2009-generating} proposed a set of linguistically-motivated\index{linguistics} algorithms for doing some types of aggregation and ellipsis; some of these are implemented within the \emph{Simplenlg}\index{simplenlg} package.    Different kinds of aggregation are appropriate in different domains, as well as for different users, which suggests that aggregation techniques should be adapted to domains and users; adaptation can be done based on user feedback \cite{walker2007individual} and/or by learning from appropriate corpora \cite{white-howcroft-2015-inducing}.

From a commercial perspective, most systems which I am aware of restrict themselves to relatively simple aggregations within the microplanner, done by straightforward rules or scripts; this is partially
because of the risk of unwanted inferences (such as morphine causing vomiting).  There tends to be more emphasis on \emph{conceptual aggregation}\index{conceptual aggregation}, usually done as a type of abstraction\index{data interpretation!abstraction} within data interpretation. For example generating an insight that sales rose in every quarter of 2020 by abstracting over individual insights that sales rose in Q1 2020, Q2 2020, Q3 2020, and Q4 2020.



\section{Surface Realisation}\label{sec:realisation}\index{surface realisation}
A \emph{surface realiser} generates actual texts in English or other languages, based on the linguistic\index{linguistics} decisions made in the microplanner\index{microplanning}.

\subsection{Principles}
Conceptually, the realiser takes care of grammatical\index{grammar} details so that the rest of the NLG system does not need to worry about this.  This includes
\begin{itemize}
\item \emph{Syntax}:\index{syntax} For example, forming the negated version of a sentence.  In English, this requires adding \lingform{do} in some cases but not others; for example \lingform{I do not like you} (includes \lingform{do})  but \lingform{I have not met you} (no \lingform{do}).  The realiser takes care of \lingform{do}-insertion so that other parts of the NLG system do not need to worry about this.
\item \emph{Morphology}:\index{morphology} For example, forming the plural form of a noun.  In English, this is usually done by adding \lingform{s} to the end of a word, but there are many exceptions; for instance the plural of \lingform{child} is \lingform{children}, not \lingform{childs}.  Again the realiser takes care of forming plurals so the rest of the system does not need to worry about this.
\item \emph{Orthography}:\index{orthography} For example, in English \lingform{.} is usually added to the end of a sentence, but there some exceptions, for instance we do not add a \lingform{.} if the sentence ends in an abbreviation which ends in \lingform{.}  (\lingform{I went to Washington D. C.} , not \lingform{I went to Washington D. C..} ) \cite{nunberg1990linguistics}.
\end{itemize}
Additional types of grammatical processing are needed in some languages.  For example an English surface realiser must also decide between \lingform{a} and \lingform{an} (Section~\ref{sec:aan}); this is a \emph{morphophonology}\index{morphophonology} task

The above examples are in English\index{English}, but all naturally-evolved languages have grammatical details which can be dealt with by a realiser\footnote{Artifically designed and constructed languages such as Esperanto\index{Esperanto} have more logical and consistent grammars.}.  For instance in French\index{French} we need to replace \lingform{de le} by \lingform{du}; in Mandarin\index{Mandarin} we need to add classifiers to noun phrases in some cases \cite{chen-etal-2018-simplenlg}; in German\index{German} we need to deal with separable verbs \cite{braun-etal-2019-simplenlg}; etc.

\subsection{Techniques}\label{sec:realisationTechniques}
A number of open-source software libraries\index{libraries} have been created to do the above tasks in a fairly straightforward way, of which the best known (at the time of writing) is probably Simplenlg\index{simplenlg} \cite{gatt-reiter-2009-simplenlg}\footnote{\url{https://github.com/simplenlg/simplenlg}}.  Simplenlg started off as a Java-based English realiser, but has subsequently been ported to Python and other programming languages, and also adapted to work in many other human languages, including German \cite{braun-etal-2019-simplenlg},  Mandarin \cite{chen-etal-2018-simplenlg}, and Galician\index{Galician} \cite{cascallar-fuentes-etal-2018-adapting}.  A simple example of Simplenlg is shown in Figure~\ref{fig:simplenlg}.  There are other packages with similar functionalities, such as pyrealb\index{pyrealb} \cite{lapalme2023datatotext}\footnote{\url{https://pypi.org/project/pyrealb/}}; a French example of pyrealb is shown in Figure~\ref{fig:pyrealb}.

\begin{figure}
\lineacross\\
    SPhraseSpec p = nlgFactory.createClause();\\
    p.setSubject("Mary");\\
    p.setVerb("chase");\\
    p.setFeature(Feature.TENSE, Tense.PAST);\\
    p.setObject("the monkey");\\
    String output2 = realiser.realiseSentence(p); \\
\caption{Simplenlg example; output is \lingform{Mary chased the monkey.}}
\label{fig:simplenlg}

\lineacross\\

loadFr()\\
print(S(\\
\hspace*{1cm}NP(D("le"),N("chat"),A("petit"),)\\
\hspace*{1cm}VP(V("sauter").t("ps")) \\
\hspace*{0.5cm}).realize())\\
\caption{pyrealb example; output is \lingform{ Le petit chat sauta.}}
\label{fig:pyrealb}
\lineacross
\end{figure}


There are also open-source packages which just do morphology, such as the Python \emph{inflect}\index{inflect (Python package)} package for English\footnote{\url{https://pypi.org/project/inflect/}} and Abed's Arabic\index{Arabic} language functions \cite{abed-reiter-2020-arabic}.  These are essentially \emph{language functions}\index{language functions} as defined in Section~\ref{sec:languagefunctions}.

% statistical realisers
We can also use statistical and machine-learning models in surface realisation.  One approach is \emph{over-generate\index{over-generation} and select}, where the realiser generates a number of different possible sentences (surface forms), using rule-based techniques, and then a statistical or ML model is used to choose the best of these \cite{langkilde-knight-1998-generation-exploits}.  The OpenCCG\index{OpenCCG} realiser library \cite{white2007towards} includes this capability.

Large neural language models\index{large language models} (Section~\ref{sec:foundationmodels}) are very good at realisation and getting grammatical details correct.  Such models can be used for surface realisation as defined in this section \cite{mille-etal-2020-third}, but its more common for them to be used for linguistic expression (ie, including microplanning\index{microplanning} as well as realisation) or indeed for the entire generation task (Section~\ref{sec:modularML}).

From a commercial perspective, many fielded systems use relatively simple realisation processing, perhaps just morphology and simple syntactic processing \cite{weissgraeber-madsack-2017-working}.   An open-source toolkit designed to support such an approach is the Rosaenlg\index{Rosaenlg} library\footnote{\url{https://rosaenlg.org/}}.



\section{Template NLG}\label{sec:templatenlg}
Instead of using a modularised pipeline, developers can use \emph{templates}\index{templates} to generate texts in a single step from the input data.  The template concept is a vague one, and ranges from simple mail-merge\index{mail merge} templates to complex scripting\index{scripts} languages such as Jinja2\index{Jinja} which include conditional statements, arithmetic computations, and other programming constructs.  I use it here to cover all approaches where rules or algorithms are used to generates texts in a single step without any decomposition into tasks such as data interpretation or microplanning.

\subsection{Principles}

\begin{figure}
\lineacross\\
Simple template for sentence such as \lingform{The Washington Wizards defeated the Los Angeles Lakers, 111-95.}\\
\begin{verbatim}
if (teamA.score > teamB.score)
    ''The [teamA.name] defeated the [teamB.name] [teamA.score] - [teamB.score].''
else if (teamB.score > teamA.score)
   ''The [teamB.name] defeated the [teamA.name] [teamB.score] - [teamA.score].''
else
   ''The [teamA.name] tied the [teamB.name] [teamA.score] - [teamB-score].''
\end{verbatim}
\caption{Simple template to produce a sentence describing the winner of a sporting match.}
\label{fig:simpletemplate}
\lineacross
\end{figure}

\begin{figure}
\lineacross\\
Slightly more complex template for sentence such as \lingform{The Washington Wizards defeated the Los Angeles Lakers, 111-95.}\\
\begin{verbatim}
if (teamA.score >= teamB.score+20)
    ''The [teamA.name] destroyed the [teamB.name] [teamA.score] - [teamB.score].''
else if (teamA.score >= teamB.score+5)
    ''The [teamA.name] defeated the [teamB.name] [teamA.score] - [teamB.score].''
else if (teamA.score >= teamB.score)
      ''The [teamA.name] edged the [teamB.name] [teamA.score] - [teamB.score].''
else if (teamB.score >= teamA.score+20)
    ''The [teamB.name] destroyed the [teamA.name] [teamB.score] - [teamA.score].''
else if (teamB.score >= teamB.score+5)
    ''The [teamB.name] defeated the [teamB.name] [teamB.score] - [teamA.score].''
else if (teamB.score >= teamB.score)
     ''The [teamB.name] edged the [teamA.name] [teamB.score] - [teamA.score].''
else
    ''The [teamA.name] tied the [teamB.name] [teamA.score] - [teamB-score].''
\end{verbatim}
\caption{Slightly more complex template to produce a sentence describing the winner of a sporting match.}
\label{fig:morecomplextemplate}
\lineacross
\end{figure}

A simple example of a template is shown in Figure~\ref{fig:simpletemplate}.  The input to the system is data about a sports match, including names and scores of teams; the output is a simple sentence saying who won the game.  Writing this template is much simpler than building a complete modularised NLG pipeline\index{pipeline}!

Of course, real sports-writing\index{sports-writing} applications probably need something more sophisticated.  For example, the system may want to vary the verb depending on the size of the victory, using \lingform{destroyed} for a victory of 20 points or more, \lingform{edged} for a victory of less than 5 points, and \lingform{defeated} otherwise.  This could be done using the template shown in Figure~\ref{fig:morecomplextemplate}.

There are many other desirable improvements, including:
\begin{itemize}
\item Varying words (Section~\ref{sec:lexprinciples})\index{lexical variation}, for example in some cases using \lingform{beat} instead of \lingform{defeated}.
\item Referring\index{referring expressions} to teams just by city name (eg, \lingform{Washington}) where contextually appropriate.
\item Adding interesting insights\index{insights}, such as winning steaks (\lingform{Washington has won its fifth match in a row}).
\item etc.
\end{itemize}
In principle we can implement all of the above using a simple template structure such as the one in Figure~\ref{fig:morecomplextemplate}, but the code rapidly becomes very long and complex.  At some point it will become easier to introduce modules or functions to choose the verb (\lingform{destroyed}, \lingform{defeated}, \lingform{beat}, \lingform{edged}), decide how to refer to a team, decide whether to mention a winning streak, etc.  This can be done on an ad-hoc basis, especially for student projects. In commercial contexts, though, structuring the NLG process into distinct modules (e.g., treating verb choice as a lexical choice\index{lexical choice} process, as described in Section~\ref{sec:lexchoice}) is better because it makes it easier to reuse and maintain\index{maintenance} code.  Dealing with edge cases\index{edge cases} and exceptional conditions is also easier with a structured modularised approach.

In short, templates make sense for simple projects with straightforward input data, and indeed for demos that only need to work on a few examples.  However, a structured approach (with modules and representations) works better for complex projects, especially in real-world commercial contexts.  One exception to this is that domain experts\index{domain!experts} with limited background in programming and software development often find it easier to do even somewhat complex projects as templates.   However such projects can be very difficult to support and maintain\index{maintenance} in real-world usage.

\personal{I have seen a number of commercial projects where demos and prototypes were built using templates, because this was the fastest way to build these, but the production system was built using an NLG pipeline\index{pipeline}, because this was easier to maintain.}

\subsection{Techniques}\label{sec:languagefunctions}
Very simple templates (such as the one shown in Figure~\ref{fig:simpletemplate}) can be implemented using mail-merge in Microsoft Word\index{Microsoft Word} and other word processing systems.  In my experience, many domain experts like to use Microsoft Excel\index{Microsoft Excel} for templates, even though Excel is not intended or designed for this.

More complex templates are usually created with a scripting\index{scripts} language, which may be embedded into a general purpose programming language; a good example is Jinja2\index{Jinja}, which is embedded in Python.  This approach makes it easy to treat template development as a programming task, and in particular use functions within the template.   In such cases the distinction between template systems and a modularised pipelined NLG systems can become hazy.  In other words, the same processing is going on \scare{under the hood}, the issue is whether tasks such as lexical choice\index{lexical choice} are handled by explicit lexical-choice modules, or by random functions which choose verbs and do other word choice tasks.

Many sophisticated template systems include \emph{language functions}\index{language functions} which perform some limited NLG processing.   For example, we could use the following template with language functions to produce the DrivingFeedback text \lingform{You sped twice on King Street}:
\begin{verbatim}
``You sped [instancePhrase(NumInstance)] on [streetRef(Street)].''
\end{verbatim}
In this template, \code{instancePhrase(NumInstance)} is a language function that returns \lingform{once}, \lingform{twice}, \lingform{three times}, etc based on the value of \code{NumInstance}.   Another language function is \code{StreetRef(Street)}, which returns a contextually appropriate referring expression\index{referring expressions} for \code{Street}, such as \lingform{King Street} or \lingform{King Street, Aberdeen}.

Common types of language functions include:
\begin{itemize}
\item \emph{Orthography}:\index{orthography} For example, \code{EnglishList(cow,sheep,pig)} could return \lingform{cow, sheep and pig}, taking care of edge cases such as using \lingform{;} instead of \lingform{,} when necessary.
\item \emph{Morphology}:\index{morphology} For example, \code{plural(child)} could return \lingform{children}.
\item \emph{Syntax}:\index{syntax} For example, \code{countNoun(2, child)} could return \lingform{2 children}.
\item \emph{Lexical choice:}\index{lexical choice} For example, \code{timePhrase(0000)} could return \lingform{midnight}
\item \emph{Referring expressions:}\index{referring expressions} For example, \code{personReferenece(EhudReiter)} could return \lingform{Professor Ehud Reiter}.
\end{itemize}
These functions can be integrated into the template system, or accessed via generic Java or Python libraries, such as the Python \code{inflect}\index{inflect (Python package)} package\footnote{\url{https://pypi.org/project/inflect/}}.

\section{Further reading and resources}
Gatt and Krahmer (2018) \cite{gatt2018survey} survey (amongst other things) rule-based NLG as it stood in 2018.  Since the NLG community has focused on ML and neural techniques in recent years, the Gatt and Krahmer survey is still a good source for work on rules-based NLG.  My 2000 book \cite{reiterdale2000} is dated, but some people still find that it is useful for understanding the basic concepts of rule-based NLG.
 
Signal analysis\index{signal analysis} is essentially pattern detection and noise suppression\index{noise detection}, and there are numerous data science\index{data science} resources which can be used for it, such as the Python \code{numpy}\index{numpy} and \code{scipy}\index{scipy} libraries.   Data interpretation\index{data interpretation} is likewise related to data science and can draw on data science resources, although data interpretation for NLG can often be somewhat different from conventional data science.

Document planning\index{document planning} is fundamental to rule-based NLG, but I am not aware of good surveys specifically about document planning.  However the topic is covered in  Gatt and Krahmer's survey \cite{gatt2018survey} of NLG.

More resources are available for microplanning\index{microplanning}.  Krahmer and van Deemter \cite{krahmerdeemter} survey work on generating referring expressions\index{referring expressions}; this is a bit dated, but can still be a useful source for fundamentals.  Deemter has written two books related to microplanning which are aimed at non-specialist audiences.  Deemter (2010) \cite{van2010not} discusses vagueness\index{vagueness}, including how this influences lexical choice\index{lexical choice} in microplanning, and Deemter (2016) \cite{vanDeemterReference} looks at  reference.

Many open-surface realisers\index{surface realisers} are available, see Section~\ref{sec:realisationTechniques}.  The Simplenlg\index{simplenlg} package has a tutorial which explains its basics,  this can be a useful way to understand what realisers do and how to use them.  There also many template\index{templates} engines available, see Section~\ref{sec:languagefunctions}.    Unfortunately I cannot recommend any open-source document planning and microplanning libraries.  Some realisation packages (including Simplenlg) do limited amounts of microplanning, and document planning is often done using scripting languages.


There are companies which specialise in building rule-based data-to-text NLG systems, or systems which combine rules and machine learning; their websites can be useful sources to understand what is being used commercially.  At the time of writing, some of the best known are Arria\index{Arria} NLG (my company)\footnote{\url{https://www.arria.com/}} and Ax Semantics\footnote{\url{https://en.ax-semantics.com/}}\index{Ax Semantics}.


