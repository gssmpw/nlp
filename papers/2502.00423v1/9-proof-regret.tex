
%!TEX root = 0-main.tex

\section{Proof for the Regret Results}
\label{sec:proof-regret}

\subsection{Proof for the Excess Misclassification Rate}\label{sec:proof-misclustering}

\begin{proof}{Proof of Theorem \ref{thm:miss-class-rate}}
%For ease of presentation, we denote $n = N_{\tau-1}$. 
The misclassification error for $\btheta^*$ can be expressed by
\begin{align*}
	R(\btheta^*) & = \EE\brackets{\EE\big[\bbone(g_i^* \neq G_{\btheta^*}(\bz_i)) \cond \bz_i\big]} \\
	&=\EE\brackets{\EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*\leq0) \cond \bz_i\big]+\EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*>0) \cond \bz_i\big]} \\
	&=\EE\brackets{\bbone(\bz_i^{\top}\btheta^* \leq 0)p(\bz_i^{\top}\btheta^*)+\bbone(\bz_i^{\top}\btheta^* > 0)(1-p(\bz_i^{\top}\btheta^*))} \\
	& = \EE \Big[\min\braces{(1 - p(\bz_i^\top\btheta^*), p(\bz_i^\top\btheta^*)}\Big],
\end{align*}
and 
\begin{align*}
	R\big(\hat\btheta\big) & = \EE\brackets{\EE\Big[\bbone(g_i^* \neq G_{\hat\btheta}(\bz_i)) \cond \bz_i\Big]}\\
	&=\EE\Big[\EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*\leq0, \bz_i^{\top}\hat{\btheta} \leq 0) \cond \bz_i\big]+\EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*>0, \bz_i^{\top}\hat{\btheta} \leq 0) \cond \bz_i\big]\\
	&\quad\quad +\EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*\leq0, \bz_i^{\top}\hat{\btheta} >0) \cond \bz_i\big]+\EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*>0, \bz_i^{\top}\hat{\btheta} > 0) \cond \bz_i\big]\Big] . %=\int_{\calZ}\sum_{k=1,2}\bbone(G_{\hat\btheta}(\bz_i) \ne k,  g_i^*=k)\PP(g^*_i=k\cond\bz_i) \mu(d\bz_i),
\end{align*}
Note that
\[\EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*\leq0, \bz_i^{\top}\hat{\btheta} \leq 0) \cond \bz_i\big] \leq \EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*\leq0) \cond \bz_i\big] \leq R(\btheta^*),\]
\begin{align*}
	\EE\big[\bbone(g_i^* =1, \bz_i^{\top}\btheta^*>0, \bz_i^{\top}\hat{\btheta} \leq 0) \cond \bz_i\big] & \leq \EE\big[\bbone\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}\geq \bz_i^{\top}\btheta^*>0\right)p(\bz_i^{\top}\btheta^*) \cond \bz_i\big] \\
	&\leq R(\btheta^*) + \EE\brackets{\bbone\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}\geq \bz_i^{\top}\btheta^*>0\right)(2p(\bz_i^{\top}\btheta^*)-1)}\\
	&\leq R(\btheta^*) + \EE\brackets{2p\paren{\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}}-1},
\end{align*}
\begin{align*}
	\EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*\leq0, \bz_i^{\top}\hat{\btheta} > 0) \cond \bz_i\big] &\leq \EE\big[\bbone\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)} \leq\bz_i^{\top}\btheta^*\leq0\right)(1-p(\bz_i^{\top}\btheta^*)) \cond \bz_i\big]\\
	&\leq R(\btheta^*) + \EE\brackets{\bbone\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}\leq \bz_i^{\top}\btheta^*<0\right)(1-2p(\bz_i^{\top}\btheta^*))}\\
	&\leq R(\btheta^*) + \EE\brackets{1-2p\paren{\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}}},
\end{align*}
\[\EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*>0, \bz_i^{\top}\hat{\btheta} > 0) \cond \bz_i\big] \leq \EE\big[\bbone(g_i^* =2, \bz_i^{\top}\btheta^*>0) \cond \bz_i\big] \leq R(\btheta^*),\]
and only one of the above four indicator functions equals one. Therefore, we obtain that
\[R\big(\hat\btheta\big)-R(\btheta^*) \leq \EE\brackets{\abs{2p\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}\right)-1}} \leq 2\EE\brackets{p\left(\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}\right)-p(0)} \leq \frac{1}{2}\EE\brackets{\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}}.\]
Furthermore, since $\lambda_{\max}(\EE[\bz_i\bz_i^{\top}]) \leq M$, we finally obtain
\[R\big(\hat\btheta\big)-R(\btheta^*) \lesssim \EE\brackets{\abs{\bz_i^{\top}(\hat\btheta-\btheta^*)}} \leq \sqrt{\EE\brackets{(\hat\btheta-\btheta^*)^{\top}\bz_i\bz_i^{\top}(\hat\btheta-\btheta^*)}}.\]
Define a ``good'' event $\cE_i$ as
\begin{equation*}
	\cE_i := \left\{ \norm{\hat{\bbeta}_{1}^{(\tau)} - \bbeta_{1}^*}_{2} + \norm{\hat{\bbeta}_{2}^{(\tau)} - \bbeta_{2}^*}_{2}  + \norm{\hat{\btheta}^{(\tau)} - \btheta^*}_{2}
	\leq C	\sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}}\right\},
\end{equation*}
where $\hat{\bbeta}_{1}^{(\tau)}$, $\hat{\bbeta}_{2}^{(\tau)}$, $\hat{\btheta}^{(\tau)}$ are the estimators obtained using the samples in the $(\tau-1)$-th phase.  By Theorem \ref{thm:1-detailed}, it holds that $\Pr(\cE_i^c) \leq c \frac{\log^3 n_0}{\max\{N_{\tau-1}, d\}^2}  \lesssim  \frac{1}{N_{\tau-1}}$ for some constants $c$ and $C$, which yields
\begin{equation*}
	R\left(\hat\btheta^{(\tau)}\right)-R(\btheta^*) \lesssim 1/N_{\tau-1} + \sqrt{\EE\brackets{\big(\hat\btheta^{(\tau)}-\btheta^*\big)^{\top}\bz_i\bz_i^{\top}\big(\hat\btheta^{(\tau)}-\btheta^*\big) \cond \cE_i}} \lesssim \sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}}.
\end{equation*} \hfill\qed
%Note that for any $p,\hat p\in(0,1)$, we have $|\min\{p,1-p\}-\min\{\hat p,1-\hat p\}|\le |p-\hat p|$. In fact, to prove this claim, it suffices to show this inequality holds if $\hat p<1/2<p$. When $\hat p<1/2<p$, $|\min\{p,1-p\}-\min\{\hat p,1-\hat p\}|=|1-p-\hat p|\le |p-\hat p|$. Therefore,
%\begin{align*}
%	R(\btheta^*)-  R\big(\hat\btheta\big)\le& \int_{\calZ}
%	\abs{p(\bz_i^\top\hat\btheta)-p(\bz_i^\top\btheta^*)} \mu(d\bz_i)\\
%	\le& \frac{1}{4} \int_{\calZ}
%	\abs{\bz_i^\top\hat\btheta-\bz_i^\top\btheta^*}   \mu(d\bz_i)\\
%	\le& \sqrt{\norm{\Cov(\bz_i)}_2}\cdot \norm{\hat\btheta-\btheta^*}_2 \\
%	\le& \sqrt{M \frac{s\log d \log N_{\tau-1}}{N_{\tau-1}}}.
%\end{align*}
\end{proof}

\subsection{Proof for the Regret Upper Bound}
\label{sec:proof-upper}
\begin{proof}{Proof of Theorem \ref{thm:regret}}
Let $\calI_{\tau}$ be the set of indices in the $\tau$-th episode and $N_{\tau}=n_02^{\tau}$ be the cardinality of $\calI_{\tau}$. 
The expected cumulative regret over a length of horizon $T$ can be expressed as
\begin{equation*}
\Reg(T) = \sum_{\tau = 0}^{\tau_{\max}} \sum_{i\in\calN_{\tau}} \EE\brackets{\reg_i},
\end{equation*}
where $\tau_{\max}=[\log_2(T/n_0+1)]-1$.

We will show the following results for the instant regret:
Suppose the conditions in Theorem \ref{thm:regret} hold. For an observation $i$ in the $\tau$-th episode ($\tau \geq 2$), we have
\begin{equation} \label{eqn:E[reg_i]-strong}
\EE[\reg_i^{*}] \lesssim   \frac{\overline{x}^2s^2\log d \log n_0}{N_{\tau-1}} + \overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1\cdot R(\btheta^*), 
\end{equation}
and
\begin{equation} \label{eqn:E[reg_i]-weak}
\EE[\tilde{\reg}_i] \lesssim \overline{x} \norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}}. 
\end{equation}
	
We first deal with the instant strong regret. Let $(\hat{\btheta}, \hat{\bbeta}_{1}, \hat{\bbeta}_{2})$ be the estimator in the $\tau$-th episode, which is obtained using the data collected in the $(\tau-1)$-th episode. By the proof of Theorem \ref{thm:coeff-bound-hd}, we have that 
	\[
	\norm{\hat{\bbeta}_{1} - \bbeta_{1}^*}_1 + \norm{\hat{\bbeta}_{2} - \bbeta_{2}^*}_1 + \norm{\hat{\btheta} - \btheta^*}_1
	\leq C
	\sqrt{\frac{s^2\log d \log n_0}{N_{\tau-1}}},
	\]
	with probability at least $1-c \frac{\log^3 n_0}{\max\{N_{\tau-1}, d\}^2}$ for some constant $c, C$.
Define ``good'' events $\cE_i$ and $\cG_i$ as
	\begin{align*}
		\cE_i& := \left\{ \norm{\hat{\bbeta}_{1} - \bbeta_{1}^*}_{1} + \norm{\hat{\bbeta}_{2} - \bbeta_{2}^*}_{1} + \norm{\hat{\btheta} - \btheta^*}_{1}
		\leq C
		\sqrt{\frac{s^2\log d \log n_0}{N_{\tau-1}}}\right\}\\
		%&	\underset{a\in [K]}{\max}\; \abs{\angles{\bx_{i,a}, \bbeta_g^* - \hat \bbeta_g}} \leq  C\sqrt{\log K+\log(d \vee N_{\tau-1})}\cdot \norm{\hat \bbeta_g - \bbeta_g^*}_2 \text{ for } g= 1, 2,\\
			\cG_i&:=\left\{\angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} > \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}} + 2C\overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}, \text{ for } g= 1, 2 \right\},
	\end{align*}
	where $\hat{\bbeta}_{1}$, $\hat{\bbeta}_{2}$, $\hat{\btheta}$ are the estimators obtained using the samples in the $(\tau-1)$-th phase, and $\tilde{a}_{i, g}:=\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_g}$.  Then it holds that $\Pr(\cE_i^c) \leq c \frac{\log^3 n_0}{\max\{N_{\tau-1}, d\}^2}  \lesssim  \frac{1}{N_{\tau-1}}$, which yields
	\begin{equation}\label{eq:Eec-strong}
		\EE[\reg^*_i \cond \cE_i^c] \lesssim \overline{R}/N_{\tau-1}.
	\end{equation}%By the sub-Gaussianity of $\bx_{i,a}$ and the maximal sub-Gaussian inequality, 
	%\[
%	\underset{a\in [K]}{\max}\; \abs{\angles{\bx_{i,a}, \bbeta_g^* - \hat \bbeta_g} }\lesssim \sqrt{\log K+\log(d \vee N_{\tau-1})}\cdot \norm{\hat \bbeta_g - \bbeta_g^*}_2,
%	\]
%	with probability  at least $1-c_0 \frac{1}{\max\{N_{\tau-1}, d\}}$.
 By Assumption \ref{B2}, we have that
	\[\angles{\bx_{\tilde{a}_{i, g}, i}, \bbeta_g^*} \ge \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{a, i}, \bbeta^*_{g}} + 2C\overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}, \text{ for } g= 1, 2,\]
	hold with probability at least $1-2C_1C\overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}$. Therefore,
	\[
	\Pr(\cG_i^c) \lesssim \overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}.
	\]

	
	Since of $\norm{\bx_{i,a}}_{\infty} \leq \overline{x}$, under $\cE_i$, we have for $g=1,2$,
	\[\max_{a \in [K]}\angles{\bx_{i,a}, (\bbeta_g^* - \hat \bbeta_g)} \leq \overline{x}\norm{\bbeta_g^* - \hat \bbeta_g}_1 \leq C\overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}.\]
	If $\cE_i\cap\cG_i$ holds, then
	\[\max\limits_{a\in [K]}\abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}} \leq \frac{1}{2}\left( \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} - \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\right),\]
	which implies that, for any $a \neq \tilde{a}_{i, g}= \underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g}}$,
	\begin{equation}
		\begin{aligned}
			\angles{\bx_{i, a}, \hat\bbeta_{g}} & \leq \abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}}  + \angles{\bx_{i, a}, \bbeta^*_{g}}	\\
			&\leq \max\limits_{a\in [K]}\abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}}  +\max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\\
			& \leq \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*}-\frac{1}{2}\left( \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} - \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\right)\\
			& \leq  \angles{\bx_{i, \tilde{a}_{i, g}}, \hat{\bbeta}_g}.
		\end{aligned}
	\end{equation}
	Therefore, we have $\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \hat\bbeta_{g}}=\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g}}$ for $g=1,2$.

Now we consider two different cases of $g_i$ and $\hat g_i$. When $\hat g_i = g_i$, $\reg^{*}_i = \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_{g_i}^*} - \angles{\bx_{i,\hat a_i}, \bbeta_{g_i}^*}$. Under $\cE_i\cap\cG_i$, since $\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \hat\bbeta_{g}}=\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g}}$ for $g=g_i$, we have $\reg^{*}_i=0$. Otherwise
\begin{align*} 
	\EE(\reg^{*}_i \cond \hat g_i = g_i, \cE_i \cap \cG_i^c)  &= \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_1^*} - \angles{\bx_{i,\hat a_i}, \bbeta_1^*} \cond \cE_i \cap \cG_i^c}  \Pr( \cE_i \cap \cG_i^c) \\
& = \EE\brackets{ \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_1^*} - \underset{a\in [K]}{\max}\;  \angles{\bx_{i,a}, \hat \bbeta_1} 
+ \angles{\bx_{i,\hat a_i}, \hat\bbeta_1} - \angles{\bx_{i, \hat a_i}, \bbeta_1^*} }\Pr( \cE_i \cap \cG_i^c)
\\
& \le 2\; \EE\abs{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, (\bbeta_1^* - \hat \bbeta_1)}} \Pr( \cE_i \cap \cG_i^c)\\
& \lesssim \frac{\overline{x}^2s^2\log d  \log n_0}{N_{\tau-1}}.
%\le 2 \underset{a\in [K]}{\max}\;\norm{\bx_{i,a}}_2 \cdot \norm{\hat \bbeta_1 - \bbeta_1^*}_2. 
\end{align*}
% and sub-Gaussian maximal inequality, we have that
%\begin{equation}\label{eqn:x-beta-diff}
%\EE \abs{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, (\bbeta_1^* - \hat \bbeta_1)}} \lesssim  \sqrt{\log K}\cdot \norm{\hat \bbeta_1 - \bbeta_1^*}_2. 
%\end{equation}

As a result, we obtain that 
\begin{equation*}
 \EE(\reg^{*}_i \cond \hat g_i =  g_i, \cE_i) \lesssim \frac{\overline{x}^2s^2\log d  \log n_0}{N_{\tau-1}}.
\end{equation*}
%By a similar argument, it can be shown that 
%\begin{equation*}
%\EE(\reg^{*}_i \cond \hat g_i = 2, g_i = 2, \cE_i) \lesssim \sqrt{\frac{s\log(d \vee N_{\tau-1}) \log N_{\tau-1} \log K}{N_{\tau-1}}}.
%\end{equation*}

If $g_i = 1$, but the algorithm mistakenly clusters it to $\hat g_i = 2$, the greedy policy prescribes
$\hat a_i = \underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \hat \bbeta_2}$. 
The instant strong regret is
\begin{equation}\label{eq:51}
	\begin{aligned}
		&\quad \EE(\reg^{*}_i \cond \hat g_i = 2, g_i = 1, \cE_i)  =  \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_1^*} - \angles{\bx_{i,\hat a_i}, \bbeta_1^*}}  \\
		& = \EE\Big[ \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_1^*} - \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_2^*}
		+ \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_2^*} \\
		%- \underset{a\in [K]}{\max}\;  \angles{\bx_{i,a}, \hat \bbeta_2}\\
		%& \quad + \angles{\bx_{i,\hat a_i}, \hat\bbeta_2}
		 &\quad \quad -\angles{\bx_{i,\hat a_i}, \bbeta^*_2} +\angles{\bx_{i,\hat a_i},\bbeta^*_2}-\angles{\bx_{i,\hat a_i}, \bbeta^*_1}\Big]
		\\
		& \le \EE\brackets{2\abs{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_2^* - \bbeta_1^*}} 
			+  \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_2^*}-\angles{\bx_{i,\hat a_i}, \bbeta^*_2}} \\
		& \lesssim \overline{x}\norm{\bbeta_2^*-\bbeta_1^*}_1
		+  \frac{\overline{x}^2s^2\log d \log n_0}{N_{\tau-1}}.
	\end{aligned}
\end{equation}

By a similar argument, we have 
\begin{equation}\label{eq:52}
\EE(\reg^{*}_i \cond \hat g_i = 1, g_i = 2,\cE_i) \lesssim \overline{x}\norm{\bbeta_2^*-\bbeta_1^*}_1
+  \frac{\overline{x}^2s^2\log d \log n_0}{N_{\tau-1}}.
\end{equation}
In summary, we have
\begin{align*}
&\quad	\EE[\reg_i^{*}\cond \cE_{i}] \\
& = \EE[\reg^{*}_i \cond \hat g_i = g_i, \cE_i] \cdot \left(1-R\big(\hat\btheta\big)\right) + \EE[\reg^{*}_i \cond \hat g_i \neq g_i, \cE_i] R\left(\hat\btheta\right)\\
& \lesssim \frac{\overline{x}^2s^2\log d  \log n_0}{N_{\tau-1}} + \paren{\overline{x}\norm{\bbeta_2^* - \bbeta_1^*}_1 +    \frac{\overline{x}^2s^2\log d \log n_0}{N_{\tau-1}} } \cdot \paren{R(\btheta^*) + \sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}}} \\
& \lesssim   \frac{\overline{x}^2s^2\log d  \log n_0}{N_{\tau-1}} + \overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1\cdot R(\btheta^*),
\end{align*}  
where we apply Theorem \ref{thm:miss-class-rate}. Combing \eqref{eq:Eec-strong} with the above inequality leads to \eqref{eqn:E[reg_i]-strong}.

Now we deal with the instant regular regret. Similar to \eqref{eq:Eec-strong}, we have 
	\begin{equation}\label{eq:Eec-regular}
	\EE[\tilde{\reg}_i \cond \cE_i^c] \lesssim \overline{R}/N_{\tau-1}, \quad \EE[\tilde{\reg}_i \cond \cG_i^c] \lesssim \overline{x}\sqrt{\frac{s^2\log d  \log n_0}{N_{\tau-1}}}.
\end{equation}
%By the proof of Theorem \ref{thm:coeff-bound-hd}, we have that 
%\[
%\norm{\hat{\bbeta}_{1} - \bbeta_{1}^*}_1 + \norm{\hat{\bbeta}_{2} - \bbeta_{2}^*}_2 + \norm{\hat{\btheta} - \btheta^*}_2
%\lesssim 
%\sqrt{\frac{s\cdot\log(d \vee N_{\tau-1}) \log N_{\tau-1}}{N_{\tau-1}}},
%\]
%with probability at least $1-c_0 \frac{\log^2 N_{\tau-1}}{\max\{N_{\tau-1}, d\}}$. By the sub-Gaussianity of $\bx_{i,a}$ and the maximal sub-Gaussian inequality, 
%\[
%\underset{a\in [K]}{\max}\; \abs{\angles{\bx_{i,a}, \bbeta_g^* - \hat \bbeta_g} }\lesssim \sqrt{\log K+\log(d \vee N_{\tau-1})}\cdot \norm{\hat \bbeta_g - \bbeta_g^*}_2,
%\]
%with probability  at least $1-c_0 \frac{1}{\max\{N_{\tau-1}, d\}}$. By Assumption \ref{assump:reward}, we have that the third inequality in $\cE_i$,
%\[\angles{\bx_{\tilde{a}_{i, g}, i}, \bbeta_g^*} \le \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{a, i}, \bbeta^*_{g}} + 2C^2\sqrt{\frac{s\cdot\log(d \vee N_{\tau-1}) \log N_{\tau-1}(\log K+\log(d \vee N_{\tau-1}))}{N_{\tau-1}}}, \text{ for } g= 1, 2,\]
%hold with probability $1-2C^2\sqrt{\frac{s\cdot\log(d \vee N_{\tau-1}) \log N_{\tau-1}(\log K+\log(d \vee N_{\tau-1}))}{N_{\tau-1}}}$. Therefore,
%\[
%\Pr(\cE_i^c) \lesssim  \sqrt{\frac{s\cdot\log(d \vee N_{\tau-1}) \log N_{\tau-1}(\log K+\log(d \vee N_{\tau-1}))}{N_{\tau-1}}},
%\]
%and thus
%\begin{equation}\label{eq:Eec}
%	\EE[\tilde{\reg}_i \cond \cE_i^c] \lesssim 2\overline{R} \sqrt{\frac{s\cdot\log(d \vee N_{\tau-1}) \log N_{\tau-1}(\log K+\log(d \vee N_{\tau-1}))}{N_{\tau-1}}}.
%\end{equation}
%
%In the sequel, we consider the conditional  expectation on $\cE_i$. 

Note that
\begin{equation}\label{eq:Ee}
	\begin{aligned}
		&\quad \EE[\tilde{\reg}_i\cond \cE_{i} \cap \cG_{i}]\\
		&=\EE\brackets{\angles{\bx_{i,\tilde{a}_i}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond \cE_{i} \cap \cG_{i} }\\
		& =\EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond \cE_{i} \cap \cG_{i} } - \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i, \tilde{a}_i}, \bbeta^*_{g_i}}\cond \cE_{i} \cap \cG_{i} }\\
		& = \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond g_i=\widehat{g}_i, \cE_{i} \cap \cG_{i}} \left(1-R\big(\hat{\btheta}\big)\right) \\
		&\quad + \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_{g_i}^*} - \angles{\bx_{i,\hat a_i}, \bbeta_{g_i}^*} \cond g_i\neq\widehat{g}_i,  \cE_{i} \cap \cG_{i}} R\big(\hat{\btheta}\big)\\
		&  \quad - \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\tilde a_i}, \bbeta^*_{g_i}} \cond g_i=\tilde{g}_i,  \cE_{i} \cap \cG_{i}} \left(1-R(\btheta^*)\right)\\
		&\quad - \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta_{g_i}^*} - \angles{\bx_{i,\tilde a_i}, \bbeta_{g_i}^*} \cond g_i\neq\tilde{g}_i,  \cE_{i} \cap \cG_{i}} R(\btheta^*)
	\end{aligned}
\end{equation}


% Under $\cE_i$, 
%it holds that 
%\[\max\limits_{a\in [K]}\abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}} \leq \frac{1}{2}\left( \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} - \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\right),\]
%which implies that, for any $a \neq \tilde{a}_{i, g}= \underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g}}$,
%\begin{equation}
%	\begin{aligned}
%		\angles{\bx_{i, a}, \hat\bbeta_{g}} & \leq \abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}}  + \angles{\bx_{i, a}, \bbeta^*_{g}}	\\
%		&\leq \max\limits_{a\in [K]}\abs{\angles{\bx_{i,a}, \hat \bbeta_{g}} - \angles{\bx_{i,a}, \bbeta^*_{g}}}  +\max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\\
%		& \leq \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*}-\frac{1}{2}\left( \angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} - \max_{a\ne \tilde{a}_{i, g}}\angles{\bx_{i, a}, \bbeta^*_{g}}\right)\\
%		& \leq  \angles{\bx_{i, \tilde{a}_{i, g}}, \hat{\bbeta}_g}.
%	\end{aligned}
%\end{equation}
Since $\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \hat\bbeta_{g}}=\underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g}}$ for $g=1,2$ under $\cE_i \cap \cG_i$, we have $\hat{a}_i = \underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \hat \bbeta_{g_i}}= \underset{a\in [K]}{\arg \max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} =\tilde{a}_i$ when $g_i=\hat{g}_i=\tilde{g}_i$. 
Therefore, 
\begin{align*}
	&\quad \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond g_i=\widehat{g}_i,  \cE_{i} \cap \calG_{i}} \left(1-R\big(\hat{\btheta}\big)\right) \\
	&\quad - \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\tilde a_i}, \bbeta^*_{g_i}} \cond g_i=\tilde{g}_i, \cE_{i} \cap \calG_{i}} \left(1-R(\btheta^*)\right)\\
	%& = \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond g_i=\widehat{g}_i,  \cE_{i}\cap \calG_{i}} \left(R(\btheta^*)-R\big(\hat{\btheta}\big)\right)\\
	& = 0.
\end{align*}
And 
\begin{align*}
	&\quad \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond g_i \neq \widehat{g}_i,  \cE_{i} \cap \calG_{i}} R\big(\hat{\btheta}\big) \\
	&\quad - \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\tilde a_i}, \bbeta^*_{g_i}} \cond g_i\neq \tilde{g}_i, \cE_{i} \cap \cG_i} R(\btheta^*)\\
	& = \EE\brackets{\underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}} \cond g_i \neq\widehat{g}_i,  \cE_{i} \cap \calG_{i}} \left(R\big(\hat{\btheta}\big)-R(\btheta^*)\right)\\
	& \lesssim \left(\overline{x}\norm{\bbeta_2^*-\bbeta_1^*}_1
	+  \overline{x}\sqrt{\frac{s^2\log d \log n_0 }{N_{\tau-1}}}\right) \cdot \sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}},
\end{align*}
where the last inequality follows from \eqref{eq:51}, \eqref{eq:52}, and Theorem \ref{thm:miss-class-rate}. Combining \eqref{eq:Eec-regular} and the above two inequalities, we obtain that 
\[ \EE[\tilde{\reg}_i] \lesssim \overline{x}\norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{\frac{s\log d \log n_0}{N_{\tau-1}}}.\]

Now we return to the cumulative regrets. The regret accumulated in the $\tau$-th phase can be bounded in two different cases. 
\begin{enumerate}[label=(\roman*)]
	\item When $\tau\leq1$, we have $N_{\tau} \leq 2n_0$, then the boundedness of rewards in Assumption \ref{B1} implies that
	\begin{align*}
		& \sum_{i\in\calN_{\tau}} \EE\brackets{\reg^{*}_i} \leq 2\overline{R} N_{\tau} \lesssim \overline{R}n_0, \quad  \sum_{i\in\calN_{\tau}} \EE\brackets{\tilde{\reg}_i} \leq 2\overline{R} N_{\tau} \lesssim \overline{R}n_0. 
	\end{align*}
	%Compared with the simple linear setting, the additional $\log N_{\tau-1}$ is incurred by the iterative algorithm and the sample splitting in theoretical analysis. 
	\item When $\tau \geq 2$, by \eqref{eqn:E[reg_i]-strong}, the expected strong regret in the $\tau$-th phase satisfies
	\begin{align*}
		\sum_{i\in \calN_{\tau}}\EE[\reg^{*}_i] \lesssim
		\overline{x}^2s^2\log d \log n_0 + \overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1\cdot R(\btheta^*)N_{\tau-1}.
	\end{align*} 
	By \eqref{eqn:E[reg_i]-weak}, the expected regular regret in the $\tau$-th phase satisfies
	\begin{align*}
		\sum_{i\in \calN_{\tau}}\EE[\tilde{\reg}_i] \lesssim \overline{x} \norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{s\log d \log n_0 \cdot N_{\tau-1}}.
	\end{align*} 
\end{enumerate}

Hence, the total expected {\em strong} regret is 
\begin{align*}
	\Reg^{*}(T) & = \sum_{\tau = 0}^{\tau_{\max}} \sum_{i\in\calN_{\tau}} \EE\brackets{\reg^{*}_i} \\
	& \lesssim  \overline{R}n_0 +\overline{x}^2s^2\log d \log n_0 \cdot \log T+ \overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1\cdot R(\btheta^*) \cdot T.
\end{align*}
The total expected regular regret is
\begin{align*}
	\tilde{\Reg}(T) & = \sum_{\tau = 1}^{\tau_{\max}} \sum_{i\in\calN_{\tau}} \EE\brackets{\tilde{\reg}_i}  \\
	& \lesssim \overline{R}n_0 + \sum_{\tau = 2}^{\tau_{\max}}  \overline{x} \norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{sn_02^{\tau-1}\log d \log n_0 }\\
	& \lesssim \overline{R}n_0+ \overline{x} \norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{s\log d \log n_0 } \sqrt{T}.
\end{align*} \hfill\qed
\end{proof}


\subsection{Proof for the Regret Lower Bound}\label{sec:proof-lower}
\begin{proof}{Proof of Theorem \ref{thm:lower-bound}}
	We first show the lower bound for the instant strong regret. Given constants $\overline{L}>0$ and $\overline{x}>0$, let $\bbeta_{1}^{*}=(\overline{L}, 0, \dots, 0)$, $\bbeta_2^*=(-\overline{L}, 0, \dots, 0)$, and the $j$-th entry of $\bx_{i, a_i}$ be $x_{ij} + \frac{\overline{x}}{2}(3- 2a_i)$, where $x_{ij} \stackrel{i.i.d.}{\sim} \calU[-\overline{x}/2, \overline{x}/2]$ for $j=1,2,\dots, d$, and $a_i \in \left\{1, 2\right\}$. For simplicity, we denote $(\bx_{i, 1}, \bx_{i, 2})$ by $\bx_i$ for any $i$. The parameter $\btheta^* \in \RR^d$ and the distribution of $\bz_i$ can be chosen arbitrarily as long as they satisfy $\norm{\btheta^*}_0 \leq s$, \ref{A1}, \ref{A4}, and $\bz_i$ is independent of $x_{i1}$. Then it is straightforward to verify that this choice of $\mu(\bx, y, \bz;\bgamma^*)$ belongs to $ \calP_{d,s,\overline{x},\overline{L}}$. 
	
	We have that 
	\[\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} =\overline{L} \left(x_{i1}+\frac{\overline{x}}{2}(3- 2a_i)\right)(3-2g_i)=\overline{L}x_{i1}(3-2g_i)+\overline{L}\overline{x}(3 - 2a_i)(3-2g_i)/2,\] and hence 
	\begin{equation}\label{eq:max-regret-lower}
		\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} =\overline{L}x_{i1}(3-2g_i)+\overline{L}\overline{x}/2.
	\end{equation}
	
	On the other hand, let $\hat\pi(a_i \mid \bx_i, \bz_i, \calH_{i-1})$ denote a policy for choosing $a_i$, i.e., a conditional distribution of $a_i$ given the present features $(\bx_i, \bz_i)$ and the past history $\calH_{i-1}:=(\bx_{i-1}, \bz_{i-1}, y_{i-1}, \dots, \bx_{1}, \bz_{1}, y_{1})$. Let $\hat\pi_1:=\hat\pi(a_i=1 \mid \bx_i, \bz_i, \calH_{i-1})$ and $p_1:=\PP(g_i=1 \mid \bx_i, \bz_i, \calH_{i-1})=p(\bz_i^{\top}\btheta^*)$. Note that given $\bz_i$, the group $g_i$ is independent of the action $\widehat{a}_i$ under $\pi$. Then the conditional expected reward with $\widehat{a}_i \sim \hat\pi(a_i \mid \bx_i, \bz_i, \calH_{i-1})$ can be written as
	\begin{equation}\label{eq:pi-hat-regret}
	\begin{aligned}
			&\quad \EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bx_i, \bz_i, \calH_{i-1} \right]\\
			&=\overline{L}\Big\{p_1\left[(x_{i1}+\overline{x}/2)\hat\pi_1+(x_{i1}-\overline{x}/2)(1-\hat\pi_1)\right]+(1-p_1)\left[(-x_{i1}-\overline{x}/2)\hat\pi_1+(\overline{x}/2-x_{i1})(1-\hat\pi_1)\right]\Big\},
	\end{aligned}
	\end{equation}
	and hence, for all $\hat\pi(a_i \mid \bx_i, \bz_i, \calH_{i-1})$,
	\begin{align*}
	 \EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bx_i, \bz_i, \calH_{i-1} \right] &\leq \overline{L}(2p_1-1) \left(x_{i1}+\overline{x}\sgn[2p_1-1]/2\right)\\
		&=\overline{L}(2p(\bz_i^{\top}\btheta^*)-1)x_{i1}+\frac{\overline{x}\overline{L}}{2}\abs{2p(\bz_i^{\top}\btheta^*)-1}.
	\end{align*}
	Note that \eqref{eq:max-regret-lower} implies
	\begin{equation}\label{eq:expected-max-regret-lower}
		\begin{aligned}
				\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid \bx_i, \bz_i, \calH_{i-1}\right] 
				&= p_1(\overline{L}x_{i1}+\overline{L}\overline{x}/2)+(1-p_1)(-\overline{L}x_{i1}+\overline{L}\overline{x}/2)\\
				&=\overline{L}(2p(\bz_i^{\top}\btheta^*)-1)x_{i1}+\overline{L}\overline{x}/2.
		\end{aligned}
	\end{equation}
	As  $R(\btheta^*)=\EE \Big[\min\braces{(1 - p(\bz_i^\top\btheta^*), p(\bz_i^\top\btheta^*)}\Big]=\frac{1}{2}\EE\left[1-\abs{2p(\bz_i^{\top}\btheta^*)-1}\right]$, we obtain, for any policy $\hat\pi$, 
	\[\sup_{\mu \in \calP_{s,d,\overline{x},\overline{L}}}\EE_{\hat\pi}[\reg^*_{i}] \geq \frac{\overline{x}\overline{L}}{2}\EE\left[1-\abs{2p(\bz_i^{\top}\btheta^*)-1}\right] \gtrsim \overline{x}\overline{L}R(\btheta^*).\]
	Hence, the cumulative regret 
		\[\inf_{\hat\pi}\sup_{\mu \in \calP_{s,d,\overline{x},\overline{L}}}\sum_{i=1}^{T}\EE_{\hat\pi}[\reg^*_{i}]\gtrsim \overline{x}\overline{L}R(\btheta^*)T.\]
		
	We then show the lower bound for the instant regular regret, where we first introduce the following lemma on the lower bound of the excess misclassification rate for the sparse logistic model:
	\begin{lemma}[\cite{abramovich2018high}, Section \uppercase\expandafter{\romannumeral 6\relax}]\label{lem:risk-lower-bound}
		Define a sparse logistic model $(y, \bz) \sim \calL_{\btheta^*}$ as $y  \sim \mathrm{Bernoulli}(p)$ with $p=\frac{\exp(\bz^{\top}\btheta^*)}{1+\exp(\bz^{\top}\btheta^*)}$, where $\btheta^*\in \RR^d$ and $\norm{\btheta^*}_0 \leq s$. Then we have
		\[\inf_{\hat\eta}\sup_{\norm{\btheta^*}_0 \leq s} \big[\EE_{\{(y_i, \bz_i)\}_{i=1}^n \sim \calL_{\btheta^*}}[R_{\btheta^*}(\hat\eta)]-R_{\btheta^*}(\eta^*)\big] \gtrsim \sqrt{\frac{s\log(d/s)}{n}},\]
		where $R_{\btheta^*}(\eta):=\EE_{(y, \bz) \in \calL_{\btheta^*}}[\one(\eta(\bz) \neq y)]$, $\eta^*(\bz)=\one(\bz^{\top}\btheta^*>0)$, and the infimum is taken over all classifiers $\hat\eta: \RR^d \to \{0, 1\}$ learned from random samples $\{(y_i, \bz_i)\}_{i=1}^n$.
	\end{lemma}

Let $\calH_{i-1, \bz}:=\{\bz_{1}, \bz_{2}, \dots, \bz_{i-1}\}$. 
By \eqref{eq:pi-hat-regret} and \eqref{eq:expected-max-regret-lower}, we have that 
\begin{align*}
	&\quad \EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid \bz_i,  \calH_{i-1, \bz}\right] - \EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bz_i,  \calH_{i-1, \bz}\right] \\
		&=\EE_{\bx}\left[\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid \bx_i, \bz_i, \calH_{i-1}\right]\right] - \EE_{\bx}\left[\EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bx_i, \bz_i, \calH_{i-1} \right]\right] \\
		&= \overline{x}\overline{L}\Big[p_1(1-\EE_{\bx}[\hat\pi_1])+(1-p_1)\EE_{\bx}[\hat\pi_1]\Big],
\end{align*}
%\[
%	\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid \bx_i, \bz_i, \calH_{i-1}\right]- \EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bx_i, \bz_i, \calH_{i-1} \right]
%	= \overline{x}\overline{L}\Big[p_1(1-\hat\pi_1)+(1-p_1)\hat\pi_1\Big].
%\]
where $\EE_{\bx}$ is taken over $\bx_1,\dots, \bx_{i}$. In particular,
\[	\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid   \bz_i, \calH_{i-1,\bz}\right] - \EE\left[\angles{\bx_{i,\widetilde{a}_i}, \bbeta^*_{g_i}} \mid \bz_i, \calH_{i-1,\bz} \right] = \overline{x}\overline{L}[p_1(1-\widetilde\pi_1)+(1-p_1)\widetilde\pi_1],\]
where $\widetilde\pi_1=\one(\bz_i^{\top}\btheta^* \geq 0)$. Note that $\EE_{\bx}[\hat\pi_1]=\EE_{\bx}[\hat\pi(a_i=1 \mid \bx_i, \bz_i, \calH_{i-1})]$ can be viewed a function of $\bz_i$ that is learned based on $\calH_{i-1, \bz}$, and thus we can correspondingly define an estimated classifier $\hat\eta_{\hat\pi}$ such that $\hat\eta_{\hat\pi}=1$ with probability $\EE_{\bx}[ \hat\pi_1]$ and $\hat\eta_{\hat\pi}=2$ with probability $1-\EE_{\bx}[\hat\pi_1]$. Then, using $\EE_{\bz}$ to denote the expectation taken over $\bz_1,\dots, \bz_{i}$, we have $\EE_{\bz}\big[p_1(1-\EE_{\bx}[\hat\pi_1])+(1-p_1)\EE_{\bx}[\hat\pi_1]\big]=\EE_{\bz}[R_{\btheta^*}(\hat\eta_{\hat\pi})]$ and $\EE_{\bz}[p_1(1-\widetilde\pi_1)+(1-p_1)\widetilde\pi_1]=R_{\btheta^*}(\eta^*)$, where $\eta^*(\bz)$ is the classifier such that $g_i=1$ if $\bz^{\top}\btheta^*\geq 0$ and $g_i=2$ otherwise. Therefore, for any policy $\hat\pi$, 
\begin{align*}
	&\quad \EE \left[\angles{\bx_{i, \widetilde{a}_i}, \bbeta^*_{g_i}}\right] - \EE_{\hat\pi} \left[\angles{\bx_{i, \widehat{a}_i}, \bbeta^*_{g_i}}\right]\\
	&=\EE_{\bz}\bigg[\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid \bz_i,  \calH_{i-1, \bz}\right] - \EE_{\hat\pi}\left[\angles{\bx_{i,\widehat{a}_i}, \bbeta^*_{g_i}} \mid  \bz_i,  \calH_{i-1, \bz}\right]\bigg]\\
	&\quad  - \EE_{\bz}\bigg[\EE\left[\max_{a_i \in [2]}\angles{\bx_{i,a_i}, \bbeta^*_{g_i}} \mid   \bz_i, \calH_{i-1,\bz}\right] - \EE\left[\angles{\bx_{i,\widetilde{a}_i}, \bbeta^*_{g_i}} \mid \bz_i, \calH_{i-1,\bz} \right] \bigg]\\
	&=\overline{x}\overline{L}[\EE_{\bz}[R_{\btheta^*}(\hat\eta_{\hat\pi})]-R_{\btheta^*}(\eta^*)].
\end{align*}
By Lemma \ref{lem:risk-lower-bound}, we obtain that
\[ \inf_{\hat\pi}\sup_{\mu \in \calP_{d,s,\overline{x},\overline{L}}}\EE \left[\angles{\bx_{i, \widetilde{a}_i}, \bbeta^*_{g_i}}\right] - \EE_{\hat\pi} \left[\angles{\bx_{i, \widehat{a}_i}, \bbeta^*_{g_i}}\right] \gtrsim \overline{x}\overline{L}\sqrt{\frac{s\log d}{i-1}}.\]
Hence, for $n_0\gtrsim s\log d$, the cumulative regular regret 
\[ \inf_{\hat\pi}\sup_{\mu \in \calP_{d,s,\overline{x},\overline{L}}}\sum_{i=n_0}^{T}\bigg[\EE \left[\angles{\bx_{i, \widetilde{a}_i}, \bbeta^*_{g_i}}\right] - \EE_{\hat\pi} \left[\angles{\bx_{i, \widehat{a}_i}, \bbeta^*_{g_i}}\right]\bigg] \gtrsim \overline{x}\norm{\bbeta_1^*-\bbeta_2^*}_1\sqrt{\frac{s\log d}{i-1}}\gtrsim \overline{x}\overline{L}\sqrt{sT\log d},\]
where we use the fact that $\sqrt{n}\leq \sum_{i=1}^{n}\frac{1}{\sqrt{i}} \leq 2\sqrt{n}$. \hfill\qed
\end{proof}

