
%!TEX root = 0-main.tex

\section{Introduction}  \label{sec:intro}

Recent studies have highlighted the critical importance of latent heterogeneity across various domains, including economics \citep{blundell2007labor,bonhomme2022discretizing}, business \citep{cherry1999unobserved,lewis2024latent}, and healthcare \citep{zhou2018using,chen2024reinforcement}. The prevalence of latent heterogeneity stems from the inherent complexity of human behavior in societal applications, where individual responses to the same policy or intervention can vary significantly based on unobserved factors. For instance, consumer preferences and purchasing decisions often extend beyond observable demographic characteristics, influenced by psychological factors, personal experiences, and social interactions that are difficult to quantify. While existing approaches in data-driven decision-making have focused on leveraging observable contextual features \citep{keskin2017chasing,keskin2021dynamic,chen2019welfare,chen2021nonparametric,simchi2024simple,chen2022transfer,chen2024reinforcement,chai2025deep}, these methods fall short in addressing latent heterogeneity where the sources of variation are unobserved or unknown a priori. This limitation poses significant challenges for businesses, potentially leading to suboptimal resource allocation, reduced marketing effectiveness, and missed opportunities for personalization in customer engagement.

To address this critical challenge in business decision-making, we propose a novel decision framework that explicitly accounts for \emph{latent heterogeneity} by building upon the classical stochastic bandits model --- a prototype for modeling single-stage decision-making with learning in numerous business applications \citep{keskin2022data,ji2022risk,aramayo2023multiarmed,dong2024value,li2024dynamic,simchi2024simple,chen2025express}. We use promotion targeting as our running example throughout the paper, though the proposed model with latent heterogeneity extends to other settings of sequential personalized targeting, advertising, and recommendations.

Consider an e-commerce platform that observes one customer and needs to decide on a coupon to issue to that customer at a sequence of time points $[T]:= \{ 1, \dots, T \}$. The platform observes contextual information stored in a vector $\bz_{i}$, which includes consumer characteristics (such as demographics and historical purchase amount), product characteristics (such as product category and brand tier) from the live stream video watched by the consumer, and environmental characteristics (such as video characteristics of each livestream channel). At each decision point $i \in [T]$, there are $K$ available coupons to be offered to consumers, with each coupon specified by a discount amount and a minimum purchase requirement (e.g., ``\$15 off \$99''). We use $\bx_{i,k}\in\RR^d$, $i\in[T]$, $k\in[K]$ to represent the contextual vector concatenating the customer and coupon features for the $i$-th customer. The stochastic reward $y_{i,k}$ associated with customer $i$ receiving coupon $k$ typically denotes the customer's payment to the platform after receiving the coupon.

Extensive research has documented substantial heterogeneity in customer preferences and purchasing behaviors \citep{cherry1999unobserved,bonhomme2015grouped,lewis2024latent,hess2024latent}. In particular, customer responses to recommendations or advertisements often depend on latent factors such as lifestyle preferences or brand perceptions that are not directly measurable. To model this \emph{latent heterogeneity}, we consider two possible latent statuses for customer $i$, denoted by $g_i = 1$ or $2$, without loss of generality. The stochastic reward $y_{i,k}$ exhibits {\em different associations} for customers with different latent status $g_i$:
\begin{align} \label{eqn:hetero-f}
(y_{i,k} \cond g_i = 1) = f_1(\bx_{i,k}) + \epsilon_i , \quad\text{and}\quad
(y_{i,k} \cond g_i = 2) = f_2(\bx_{i,k}) + \epsilon_i, 
\end{align}
where $f_1(\cdot)\ne f_2(\cdot)$ and $\epsilon_i$ is a mean-zero random noise. The \emph{latent} status of customer $i$ is modeled as a random draw from a binomial distribution:
\begin{align} \label{eqn:hetero-status}
\Pr(g_i = 1 \cond \bz_i) = p(\bz_i^\top \btheta^*), \quad\text{and}\quad
\Pr(g_i = 2 \cond \bz_i) = 1-p(\bz_i^\top \btheta^*),
\end{align}
where $p(x) = 1/\paren{1 + \exp(-x)}$. 
%In our theoretical analysis, we focus on the linear reward case where the coefficients $\bbeta_{g_i}^*$ of the linear functions $f_{g_i}$ differ, specifically, $f_1(\bx_{i,k}) = \angles{\bx_{i,k}, \bbeta_1^*}$, $f_2(\bx_{i,k}) = \angles{\bx_{i,k}, \bbeta_2^*}$, and $\bbeta_1^* \ne \bbeta_2^*$.

Our proposed latent heterogeneous bandit model, characterized by \eqref{eqn:hetero-f} and \eqref{eqn:hetero-status}, differs fundamentally from the classic stochastic bandit model in two interrelated aspects: the reward functions $f_1$ and $f_2$ vary across latent groups, and these group memberships $g_i$ are unobservable. This joint presence of unobservable group membership and group-specific reward functions poses unique challenges that existing methods cannot readily address. Naive approaches such as classification followed by group-specific bandit algorithms are infeasible since the latent groups are not directly observable. Moreover, standard clustering techniques combined with group-specific bandit algorithms are inadequate because cluster membership may not align with the underlying reward associations --- the key source of heterogeneity in decision outcomes.

To address these challenges, we make three main contributions. First, we propose a novel modeling framework that explicitly captures latent heterogeneity in sequential decision-making scenarios. This framework provides a rigorous foundation for analyzing the complexities inherent in business decisions where customer segments respond differently to the same actions. Second, focusing on the linear reward setting, we develop an innovative algorithm that simultaneously learns latent group memberships and group-specific reward functions. Third, we establish comprehensive theoretical guarantees for our approach, including high-probability bounds for parameter estimation, convergence rates for group classification, regret upper bound for the optimal policy, and the minimax lower bound.

Our theoretical analysis reveals several important insights about decision-making under latent heterogeneity. We show that existing algorithms, which ignore latent heterogeneity, inevitably incur constant average regret due to their inability to account for group-specific associations. We further distinguish between two types of regret based on different oracle comparisons. The ``strong regret,'' measured against an oracle with perfect knowledge of customer memberships under the binomial distribution, remains non-sub-linear due to inherent uncertainty in group classification. This is unavoidable due to the minimax lower bound on misclassification rates, which stems from the inherent randomness in group classification. In contrast, the ``regular regret,'' compared against an oracle aware only of deterministic components, achieves a sub-linear rate that is minimax optimal in terms of both time horizon $T$ and feature dimension $d$.

\subsection{Related Works}\label{sec:related-works}

Our work intersects online stochastic contextual bandits, statistical learning, and decision-making under latent heterogeneity. We review the most relevant literature and distinguish our contributions.

\smallskip
\noindent
\textbf{Online Stochastic Contextual Bandits.}
The bandit problem has been extensively studied across computer science, operations, and statistics \citep{li2019dimension, russo2020simple,chen2021multi, russo2022satisficing, si2023distributionally,simchi2023regret,ren2024dynamic,tang2024stochastic,chen2025express}; see \cite{lattimore2020bandit} for a comprehensive review. Contextual bandits incorporate additional information to predict action quality \citep{auer2002using,dani2008stochastic,li2010contextual,chu2011contextual,ji2022risk}. While adversarial settings achieve $\bigO{d\sqrt{T}}$ regret \citep{abbasi2011improved}, stochastic settings --- suitable for applications like news recommendation and clinical trials --- can improve bounds dependent on $T$ from $\sqrt{T}$ to $\log(T)$ for homogeneous bandits.

In low dimensions, \cite{goldenshluger2013linear} achieved $\bigO{d^3\log(T)}$ regret, though this becomes unfavorable as the dimension increases. For high-dimensional settings with sparsity $s$ ($s \ll d$), several approaches have emerged: \cite{bastani2020online}'s LASSO bandit achieved $\bigO{K s^2 \log^2(dT)}$; %with a $\bigO{s^2\log(d)\log(T)}$ exploration stage; 
\cite{wang2024online}'s MCP-based method improved this to $\bigO{Ks^2(s+\log d)\log T}$; and \cite{kim2019doubly}'s doubly-robust approach reached $\bigO{s\log(dT)\sqrt{T}}$; %with $\bigO{\sqrt{T\log(dT)\log(T)}}$ exploration. 
%These methods require known sparsity $s$. 
In contrast, \cite{oh2020sparsity}'s sparse-agnostic approach achieves %$\bigO{s^2\log(d) + s\sqrt{T\log(dT)}}$, improvable to 
$\bigO{\sqrt{sT\log(dT)}}$ under restricted eigenvalue conditions. 
%\cite{ariu2020thresholded}'s Threshold LASSO achieves $\bigO{\log(d) + \sqrt{T\log(T)}}$, reducible to $\bigO{\log(d) + \log(T)}$ under margin conditions.

All of the aforementioned papers work within the classic model of stochastic bandits, which does not address the widely encountered setting of latent heterogeneity in business and economics. Under latent heterogeneity, these algorithms will obtain biased estimation due to the misspecified single linear expectation, incurring non-negligible regret at each decision point and thus linear expected regret over time.
In contrast, our proposed latent heterogeneous bandit model explicitly incorporates an unobservable subgroup structure. Our specially designed algorithm learns both the latent groups and the true parameters for each group. 
%During the decision-making stage, group membership is predicted and the optimal policy for the respective group is applied. Our analysis differentiates between two types of regrets based on different oracle comparisons. The "strong regret," measured against an oracle with perfect knowledge of customer memberships under the binomial distribution, remains non-sub-linear due to inherent uncertainty in group classification. In contrast, the "regular regret," compared against an oracle aware only of deterministic components, achieves $\bigO{\sqrt{Ts\log(d)}}$. We establish the minimax lower bounds for both regret types, demonstrating the optimality of our proposed heterogeneous algorithm.

Recent work has applied the bandits to various management problems, including newsvendor problems \citep{keskin2023nonstationary}, dynamic pricing \citep{keskin2017chasing,chen2019dynamic,keskin2021dynamic,chen2019welfare,chen2021nonparametric,den2022dynamic,bastani2022meta,keskin2022data,chen2024dynamic,li2024dynamic}, joint advertising and assortment \citep{gao2021assortment,gurkan2023dynamic}, resource allocation \citep{dong2024value}, inventory control \citep{che2024stochastic,qin2023sailing}, sub-exponential rewards \citep{yuan2021marrying}, sourcing \citep{tang2024online}, network revenue management \citep{chen2023network}, and smart order routing \citep{ji2022risk}. These works operate within the classic bandit framework and do not consider latent heterogeneity. Our proposed latent heterogeneous bandits framework can be incorporated into these decision scenarios. It is of great interest to explore the effect of latent heterogeneity of the customer population on various decision scenarios and to develop efficient methods or strategies to overcome the challenge of latent heterogeneity. We leave those for future work.

\smallskip
\noindent
\textbf{Statistical learning with Latent heterogeneity.}
In linear regression settings, researchers have explored two major approaches to latent heterogeneity. The non-parametric approach employs grouping penalization on pairwise differences \citep{shen2010grouping,ke2013homogeneity,ma2017concave}, avoiding distributional assumptions but limiting predictive capabilities for new samples. For prediction-oriented tasks, mixture-model-based approaches have shown greater promise. Key developments include: rigorous EM algorithm guarantees for symmetric mixed linear regressions (MLR) where the mixing proportion is known to be $1/2$ \citep{balakrishnan2017statistical}, efficient fixed-parameter algorithms \citep{li2018learning}, computational-statistical tradeoffs \citep{fan2018curse}, improved EM convergence analysis \citep{mclachlan2019finite,klusowski2019estimating}, robust estimation under corruptions \citep{shen2019iterative}, high-dimensional MLR with unknown but constant mixing proportions \citep{zhang2020estimation}, and convergence analysis for federated learning \citep{su2024global, niu2024collaborative}. Our model \eqref{eqn:hetero-f} and \eqref{eqn:hetero-status} extends this literature by addressing variable mixing proportions in high-dimensional MLR, a previously unexplored challenge.

\smallskip
\noindent
\textbf{Decision with Heterogeneity.}
Research addressing learning and decision-making with heterogeneity remains relatively sparse. Notable work spans several domains: personalized dynamic pricing with high-dimensional features and heterogeneous elasticity \citep{ban2021personalized}; regime-switching bandits with temporal heterogeneity \citep{cao2019nearly,zhou2021regime}; and convergence analysis of Langevin diffusion under mixture distributions \citep{dong2022spectral}, where multiple density components can significantly impact sampling efficiency. In sequential decision settings, recent work has explored policy evaluation and optimization with latent heterogeneity in reinforcement learning \citep{chen2024reinforcement,bian2024off}. However, these approaches focus primarily on multi-stage aspects of reinforcement learning, leaving unexplored the fundamental challenge that latent heterogeneity poses to the exploration-exploitation trade-off. Our work addresses this gap through the lens of the bandit problem—essentially a one-step reinforcement learning.


\subsection{Notations and Organization}\label{sec:notations}

For a positive integer $n$, let $[n] := {1,\dots,n}$. For any vector $\bv$, $\norm{\bv}_0$, $\norm{\bv}_1$, and $\norm{\bv}_2$ denote the $\ell_0$ (number of non-zero elements), $\ell_1$, and $\ell_2$ norms respectively. For a matrix $\bA$, $\lambda_{\min}(\bA)$ and $\lambda_{\max}(A)$ denote its minimum and maximum eigenvalues. For positive sequences ${a_n}$ and ${b_n}$, we write $a_n \lesssim b_n$, $a_n=\bigO{b_n}$, or $b_n=\Omega(a_n)$ if there exists $C > 0$ such that $a_n \leq Cb_n$ for all $n$. We write $a_n \asymp b_n$ if $a_n \lesssim b_n$ and $b_n \lesssim a_n$.

The remainder of this paper is organized as follows. Section \ref{sec:model} formulates our latent heterogeneous bandit model and defines two types of regret measures. Section \ref{sec:method} presents our proposed methodology. Section \ref{sec:theory} establishes theoretical guarantees, including estimation error bounds, misclassification rates, and minimax optimal regret bounds. Sections \ref{sec:numerical} validate our approach through simulation studies and an empirical application using cash bonus data from a mobile commerce platform. Section \ref{sec:conclusion} concludes with discussions.

\section{Problem Formulation} \label{sec:model}

In this section, we formulate the linear bandits problem under latent heterogeneity. Section \ref{sec:problem} introduces the latent heterogeneous linear bandit model, which extends the classical stochastic linear bandit setting by incorporating unobserved group structures. Section \ref{sec:two-type-regret} defines two types of regret—strong regret and regular regret—that evaluate the performance of a policy in the presence of latent heterogeneity.

\subsection{Latent Heterogeneous Linear Bandits}\label{sec:problem}

The latent heterogeneous bandit model \eqref{eqn:hetero-f} and \eqref{eqn:hetero-status} introduced at the beginning of this paper is a general framework that allows for arbitrary functional forms of the mean rewards $f_1(\bx)$ and $f_2(\bx)$. We focus on linear functional form in this work and leave non-linear and non-parametric function approximation for future research.
Without loss of generality, we consider the setting for two latent subgroups, as the extension to any known finite number of latent subgroups follows naturally. 
Each customer $i\in[T]$ is characterized by a customer feature $\bz_i\in\RR^{d'}$. 
For any customer $i$, there are $K$ possible arms (coupons) to offer. 
The combined features of customer $i$ and an arm $k$ are denoted as $\bx_{i,k}\in\RR^d$. 
The latent heterogeneous linear bandits are characterized by
\begin{equation} \label{eqn:lhlb}
\begin{aligned}
&\text{(Subgroup model):} & \Pr(g_i = 1 \cond \bz_i) = p(\bz_i^\top \btheta^*), \quad
\Pr(g_i = 2 \cond \bz_i) = 1-p(\bz_i^\top \btheta^*), \\
&\text{(Reward model):} & (y_{i,k} \cond g_i = 1) = \angles{\bx_{i,k}, \bbeta_1^*} + \epsilon_i , \quad
(y_{i,k} \cond g_i = 2) = \angles{\bx_{i,k}, \bbeta_2^*} + \epsilon_i, 
\end{aligned}
\end{equation}
where $\bbeta_1^*\ne\bbeta_2^*$, $p(x) = 1/\paren{1 + \exp(-x)}$, and $\epsilon_i \sim \calN(0, \sigma^2)$. We refer to the two equations as the ``subgroup model'' and the ``reward model.''

For each customer $i$, while the contextual features $\big\{ \{\bx_{i,k}\}_{k\in[K]}, \; \bz_i \big\}$ are observable, the true group membership $g_i$ remains unknown. 
The objective of the platform is to select one coupon $k\in[K]$ for each customer $i$ to maximize the aggregated rewards across all $T$ customers. 
Our goal is to design a sequential decision rule (policy) $\pi$ that maximizes the expected cumulative reward over the time horizon while simultaneously estimating the model parameters and predicting the latent group $g_i$. 
%Specifically, given $\big\{ \{\bx_{i,k}\}_{k\in[K]}, \; \bz_i \big\}$, the platform uses {\red algorithm} to obtain estimators ($\hat{\btheta}, \hat{\bbeta}_1, \hat{\bbeta}_2$), predict the subgroup membership (or environmental status) $\hat{g}_i$, and prescribe an action $\hat{a}_i\in[K]$. %according to 
%\begin{equation}
%\hat{a}_i = \underset{k \in [K]}{\arg \max} \; \angles{\bx_{i,k}, \hat\bbeta_{\hat g_i}}. 
%\end{equation}

\subsection{Two Types of Regrets under Latent Heterogeneity}\label{sec:two-type-regret}

To evaluate the performance of any policy $\pi$, we must account for an important source of randomness: the subgroup model in \eqref{eqn:lhlb}  only specifies probabilities of group assignments rather than deterministic membership. 
As a result, when a customer $i$ arrives, there exist two different types of oracle: (1) the ``ex-post'' oracle who is able to precisely predict the true realized group membership $g_i$, and (2) the ``ex-ante'' oracle who knows the true parameter $\btheta^*$ of the subgroup model and thus knows the group probability $p(\bz_i^{\top}\btheta^*)$. 

This distinction gives rise to two types of regret measures when comparing against optimal policies derived from the two types of oracles. 
Let $\pi^{*}$ denote the {\it strong oracle rule}, which ``knows'' not only the true parameters $\bbeta_1^*$, $\bbeta_2^*$, and $\btheta^*$, but also the realized group $g_i$ beyond the probabilistic structure of the subgroup model. 
For each customer $i$, the strong oracle rule prescribes
\begin{equation}
a_i^{*} = \underset{k \in [K]}{\arg \max} \; \angles{\bx_{i,k}, \bbeta^*_{g_i}}.
\end{equation}
Alternatively, we let $\tilde{\pi} $ denote the  {\it regular oracle rule}, which ``knows'' the true parameters $\bbeta_1^*$, $\bbeta_2^*$, and $\btheta^*$, but {\em not} the realized group $g_i$. 
For each customer $i$, the regular oracle rule prescribes
\begin{equation}
\tilde{a}_i = \underset{k \in [K]}{\arg \max} \; \angles{\bx_{i,k}, \bbeta^*_{\tilde g_i}},
\end{equation}
where $\tilde g_i$ is estimated group using the oracle parameter $\btheta^*$  according to the decision rule in \eqref{eqn:lhlb}, i.e., $\tilde g_i = 1$ if $p(\bz_i^{\top}\btheta^*) \geq 1/2$ and $\tilde g_i=2$ otherwise.

To evaluate any allocation policy $\hat{\pi}$, we measure its performance relative to the two oracle rules.
Let $\hat{a}_i$ denote the action chosen by policy $\hat{\pi}$ for customer $i$. We define the {\em instant strong regret} comparing against the strong oracle,
\begin{equation}
\mathrm{reg}_i^{*} = %\EE\brackets{ 
	\underset{k\in [K]}{\max}\; \angles{\bx_{i,k}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}, 
\end{equation}
 and the  {\em instant regular regret} comparing against the regular oracle,
\begin{equation}  
\tilde{\mathrm{reg}}_i = %\EE\brackets{
	 \angles{\bx_{i,\tilde a_i}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}.
\end{equation}
The expected cumulative strong regret and regular regret at time $T$ are respectively defined as
\begin{equation}
{\mathrm{Reg}}^*(T) = \EE\brackets{\sum_{i=1}^{T} \mathrm{reg}^*_i},  \quad\text{and}\quad
\tilde{\mathrm{Reg}} (T) = \EE\brackets{\sum_{i=1}^{T} \tilde{\mathrm{reg}}_i},
\end{equation}
where the expectation is taken over the randomness in the feature vectors $\{\bx_{i,k}\}_{1\le k \le K}$ and $\bz_i$, group membership $g_i$, and the stochastic noise $\epsilon_i$. 
Our objective is to develop a policy that minimizes both types of expected cumulative regrets. 

\section{Learning and Decision under Latent Heterogeneity} \label{sec:method}

In this section, we present our methodological framework for addressing latent heterogeneity in the linear bandits problem. We first propose our phased learning algorithm that accounts for the latent group structures in Section \ref{sec:phased}, followed by a tailored expectation-maximization (EM) algorithm for parameter estimation in Section \ref{sec:EM}.

\subsection{Phased Learning and Greedy Decisions}\label{sec:phased}

\begin{algorithm}[ht!]
	\caption{Phased Learning and Greedy Decision under Latent Heterogeneity}
	\label{alg:em-bandit}
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{Features $\{\bx_{i, k}, k \in [K]\}$ and $\bz_i$ for sequentially arriving customers $i$, and the minimal episode length $n_0$.}
	
%	\Output{Estimated optimal $a_i$ for $i\ge 1$.}
	
	%Initialize $\tau \leftarrow 1$, $\ba_1 \leftarrow \cU([K])$, $\hat{\btheta}$, $\hat{\bbeta}_1 \leftarrow 0$, $\hat{\bbeta}_1 \leftarrow 0$.
	
	\For{each episode $\tau=0, 1, 2, \dots$}{
		Set the length of the $\tau$-th episode as $N_{\tau} = 2^{\tau}n_0$ and define an index set $\calI_{\tau}$ with cardinality $N_{\tau}$;
		
		\If{$\tau=0$}{
			\For{$i\in\cI_{\tau}$}{
				Receive features $\{\bx_{i,k}, \bz_{i}\}_{k \in [K]}$ ;\\
				Select $a_i \sim {\rm Uniform}([K])$;\\
				Receive the reward $y_i$;
			}
		}
		\Else{
			
			%\tcc{Estimate parameter estimators using data from the previous episode.}
			
			Call Algorithm \ref{alg:em} ``Learning under Latent Heterogeneity'' to obtain $\hat{\btheta}^{(\tau)}$, $\hat{\bbeta}_1^{(\tau)}$, and $\hat{\bbeta}_2^{(\tau)}$ using data collected in the $(\tau - 1)$ -th episode, i.e.,  $\calD_{\tau-1}$;
			
			%\tcc{At each time point of the $\tau$-th episode, act greedily.}
			\For{$i\in\cI_{\tau}$}{
				
				Receive features $\{\bx_{i,k}, \bz_{i}\}_{k \in [K]}$;\\
				Predict the group membership $\hat{g}_i = 1$ if $\bz_i^{\top}\hat{\btheta}^{(\tau)}\ge 0$ and $\hat{g}_i = 2$ otherwise;
				
				Prescribe the optimal action based on estimation and prediction, that is, 
				\[
				a_i=\argmax_{k \in [K]}\; \angles{\bx_{i,k}, \hat{\bbeta}_{\hat{g}_i}^{(\tau)}};
				\]
				Receive the reward $y_i$;
			}
		}
		Collect the dataset $\calD_{\tau}=\{y_i, \bx_{i, a_i}, \bz_{i}\}_{i \in \cI_{\tau}}$;
	}
\end{algorithm}

Our proposed method exploits a key structural property of model \eqref{eqn:lhlb}: customers within the same latent group share common reward parameter $\bbeta^*_1$ or $\bbeta^*_2$ for different actions. The reward observed from taking an action provides information about the rewards of other potential actions due to the shared parametric structure. We leverage this property to develop an exploration-free algorithm that achieves minimax optimal regret.

The proposed Algorithm \ref{alg:em-bandit} implements a phased learning approach that divides the time horizon into non-overlapping episodes, indexed by $\tau =0,1,2,\dots$, and let $i \ge 1$ index sequentially arriving customers.
%The length of the $\tau$-th episode is denoted by $N_{\tau}$. 
In the initial episode 0, the actions are uniformly selected from the $K$ arms, which generates the necessary samples for the learning procedure in episode 1.
%When $\tau\ge 1$, the actions are chosen greedily according to model \eqref{eqn:lhlb}, the predicted subgroup membership $\hat{g}_i$, and the estimated model parameters $\hat{\btheta}$, $\hat{\bbeta}_1$, and $\hat{\bbeta}_2$.

For subsequent episodes ($\tau \geq 1$), model parameters $\hat{\btheta}$, $\hat{\bbeta}_1$, and $\hat{\bbeta}_2$ are updated at the start of each episode using Algorithm \ref{alg:em}, which employs expectation-maximization (EM) iterations and is detailed in Section \ref{sec:EM}. The updates utilize only the samples $\calD_{\tau-1}$ collected from the previous episode.%, which ensures that the actions chosen in each episode are independent of the model noises in the same episode. %which enables us to decouple the action selection from the noise terms, yielding sharper concentration bounds in our theoretical analysis.

With the updated parameter estimates, actions are chosen greedily by first predicting the customer's group membership $\hat{g}_i$ using current $\btheta^*$ estimates, then selecting the action that maximizes expected reward under the predicted group and current $\hbbeta_{\hat{g}_i}$ estimates.
The length of each episode, denoted by $N_{\tau}$,  increases geometrically as $N_{\tau} = n_0 2^{\tau}$, allowing for a more accurate estimate as the episodes progress.
While the algorithm terminates at the end of the horizon (time $T$), it does not require prior knowledge of $T$.



\subsection{Learning under Latent Heterogeneity and High-dimensionality} \label{sec:EM}

We now present the details of the learning procedure under latent heterogeneity in Algorithm \ref{alg:em}. 
Given the samples $\calD_{\tau-1}=\braces{ y_i, \bx_i := \bx_{i, a_i}, \bz_i}_{i \in \calI_{\tau-1}}$ collected in episode $\tau-1$, our goal is to estimate the unknown parameters in model \eqref{eqn:lhlb} via maximum likelihood estimator (MLE).
For notational clarity, we denote the index set of the samples input to Algorithm \ref{alg:em} as $\cI_{\tau-1}$ with size $N_{\tau-1}=\abs{\cI_{\tau-1}}$. 
The MLE aims to maximize the log-likelihood of the observed data $\calD_{\tau-1}$: 
\begin{equation}\label{eqn:llklh}
\ell_{N_{\tau-1}}(y_i,\bx_i,\bz_i; \bgamma) = \frac{1}{N_{\tau-1}}\sum_{i \in \cI_{\tau-1}}\log\brackets{p\paren{\bz_i^\top\btheta} \cdot
\phi\paren{ \frac{y_i - \bx_i^\top\bbeta_1}{\sigma} }
+
\paren{1-p\paren{\bz_i^\top\btheta}} \cdot
\phi\paren{ \frac{y_i - \bx_i^\top\bbeta_2}{\sigma}}},
\end{equation}
where we denote the unknown parameter by $\bgamma=(\btheta,\bbeta_1,\bbeta_2)$ and the standard normal density function by $\phi(\cdot)$. 
Directly searching for the maximizer of $\ell_{N_{\tau-1}}(y_i,\bx_i,\bz_i; \bgamma)$ is computationally intractable due to its non-convexity. Moreover, in the early episodes ($\tau$ is small), the dimension $d$ can substantially exceed the available sample size, making the estimation problem statistically infeasible.

\begin{algorithm}[t!]
\caption{Learning under Latent Heterogeneity in Episode $\tau$ ($\tau \geq 1$)}
\label{alg:em}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{Batch data $\calD_{\tau-1}=\braces{y_i, \bx_i := \bx_{i, a_i}, \bz_i}_{i \in \cI_{\tau-1}}$, batch size $N_{\tau-1} = \abs{\cI_{\tau-1}}$, initial estimators $\bgamma^{(\tau, 0)}=\big(\btheta^{(\tau, 0)}, \bbeta_1^{(\tau, 0)},\bbeta_2^{(\tau, 0)}\big)$, maximum number of iterations $t_{\tau, \max}$, regularization parameters $\big\{\lambda_{n_{\tau}}^{(t)}\big\}_{t \in [t_{\tau, \max}]}$. \\
%constants $\kappa\in(0,1)$ and $C_{\lambda}>0$. 
}

\Output{estimates $\hat{\bgamma}^{(\tau)}=\big(\hat{\btheta}^{(\tau)},\hat{\bbeta}_1^{(\tau)},\hat{\bbeta}_2^{(\tau)}\big)$.}

Split $\calI_{\tau-1}$ into $t_{\tau, \max}$ subsets $\big\{\cI_{\tau-1}^{(t)}\big\}_{t \in [t_{\tau, \max}]}$, each of size $n_{\tau}=N_{\tau-1}/t_{\tau, \max}$; %\tcp{for theoretical consideration}

\For{$t=1, \dots, t_{\tau, \max}$}{
	
%\tcc{Update pseudo-soft-labels:}
	
For each $i\in\calI_{\tau-1}^{(t)}$, calculate $\omega_{i}^{(\tau, t)} = \omega\paren{y_i, \bx_i,  \bz_i; \bgamma^{(\tau, t-1)}}$, where $\omega$ is defined by
\begin{equation} \label{eqn:omega-main}
	\omega(y, \bx, \bz ; \bgamma) 
	= \frac{p(\bz^\top\btheta) \cdot
		\phi\paren{\frac{y - \bx^\top\bbeta_1}{\sigma}}}{p(\bz^\top\btheta) \cdot \phi\paren{\frac{y - \bx^\top\bbeta_1}{\sigma}} 
		+ \paren{1-p(\bz^\top\btheta)} \cdot \phi\paren{\frac{y - \bx^\top\bbeta_2}{\sigma}}},
\end{equation}
with $\bgamma=(\btheta,\bbeta_1,\bbeta_2)$,  $p(x) = 1/\paren{1 + \exp(-x)}$, and $\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$. 
	
Update each elements of $\bgamma^{(\tau, t)}$ by
\begin{equation} \label{eqn:coeff-updates}
\begin{aligned}
\bbeta_{1}^{(\tau, t)} & := \underset{\bbeta_1}{\arg\!\min} \;  Q_{n_{\tau}1}\paren{\bbeta_1 \cond \bgamma^{(\tau, t-1)}}  
+ \lambda_{n_{\tau}}^{(t)} \norm{\bbeta_1}_1,  \\
\bbeta_2^{(\tau, t)} & := \underset{\bbeta_2}{\arg\!\min} \; Q_{n_{\tau}2}\paren{\bbeta_2 \cond \bgamma^{(\tau, t-1)}}  
+ \lambda_{n_{\tau}}^{(t)} \norm{\bbeta_2}_1,   \\
\btheta^{(\tau, t)} & := \underset{\btheta}{\arg\!\min} \; Q_{n_{\tau}3}\paren{\btheta \cond \bgamma^{(\tau, t-1)}} 
+ \lambda_{n_{\tau}}^{(t)} \norm{\btheta}_1,
\end{aligned}
\end{equation}
 where $Q_{1n_{\tau}}$, $Q_{2n_{\tau}}$, and $Q_{3n_{\tau}}$ are defined in \eqref{eqn:Qn123} with the $t$-th subset $\cI_{\tau-1}^{(t)}$.

%\tcc{Update pseudo-soft-labels:}
%Update  $\omega_{i}^{(t+1)} = \omega\paren{\bx_i, y_i, \bz_i; \bgamma^{(t+1)}}$. 
}
Assign $\hat{\bgamma}^{(\tau)} = \bgamma^{(\tau, t_{\tau, \max})}$. 
\end{algorithm}

Algorithm \ref{alg:em} addresses these challenges through an EM algorithm, handling both the non-convexity of $\ell_{N_{\tau-1}}(\bgamma; y_i,\bx_i,\bz_i)$ and the high-dimensionality of the parameter space.
The EM algorithm is essentially an alternating maximization method, iterating between identifying the latent group membership $\{g_i\}$ and estimating the unknown parameter $\bgamma=(\btheta,\bbeta_1,\bbeta_2)$.
To ensure independence between samples across iterations, we first partition the entire sample set into disjoint subsets. 
In the E-step of the $t$-th iteration during episode $\tau$, given the parameters $\bgamma^{(\tau, t-1)}=(\btheta^{(\tau, t-1)},\bbeta_1^{(\tau, t-1)},\bbeta_2^{(\tau, t-1)})$ estimated from the previous iteration, the conditional probability of the $i$-th sample belonging to group 1 given the observed data is 
\begin{equation}
\omega_i^{(\tau, t)} = \omega_i(\bgamma^{(\tau, t-1)})
= \PP(g_i=1\cond y_i, \bx_i, \bz_i;\bgamma^{(\tau, t-1)})  =  \omega(y_i, \bx_i,  \bz_i; \bgamma^{(\tau, t-1)}),    
\end{equation}
where $\omega(y, \bx, \bz; \bgamma)$ is defined in \eqref{eqn:omega-main}. 
Let $\ell\paren{y_i, \bx_i,  \bz_i, g_i; \bgamma}$ be the log-likelihood of complete data where $g_i$ is also observable. 
Thus, conditional on the current estimate $\bgamma^{(\tau, t-1)}$, the conditional log-likelihood function can be calculated as
\begin{equation}
\begin{aligned}
Q_{n_{\tau}}(\bgamma \cond \bgamma^{(\tau, t-1)}) & := \; 
\sum_{i \in \cI_{\tau}^{(t)}}\EE\brackets{ \ell\paren{y_i, \bx_i, \bz_i, g_i; \bgamma} \cond y_i, \bx_i, \bz_i; \bgamma^{(\tau, t-1)}} \\
& = \; Q_{n_{\tau}1}(\bbeta_1 \cond \bgamma^{(\tau, t-1)}) 
+ Q_{n_{\tau}2}(\bbeta_2 \cond \bgamma^{(\tau, t-1)})
+ Q_{n_{\tau}3}(\btheta \cond \bgamma^{(\tau, t-1)}),
\end{aligned}
\end{equation}
where
\begin{equation} \label{eqn:Qn123}
\begin{aligned}
    Q_{n_{\tau}1}(\bbeta_1 \cond \bgamma^{(\tau, t-1)}) & :=  \frac{1}{2n_{\tau}} \sum_{i \in \cI_{\tau}^{(t)}} \omega_i^{(\tau, t)}\cdot  \frac{(y_i - \bx_i^\top\bbeta_1)^2 }{\sigma^2}, \\
    Q_{n_{\tau}2}(\bbeta_2 \cond \bgamma^{(\tau, t-1)}) & :=  \frac{1}{2n_{\tau}} \sum_{i \in \cI_{\tau}^{(t)}} (1- \omega_i^{(\tau, t)}) \cdot \frac{(y_i - \bx_i^\top\bbeta_2)^2 }{\sigma^2}, \quad\text{and}\quad \\
    Q_{n_{\tau}3}(\btheta \cond \bgamma^{(\tau, t-1)}) & := - \frac{1}{n_{\tau}} \sum_{i \in \cI_{\tau}^{(t)}} \paren{ \omega_i^{(\tau, t)} \cdot \log p(\bz_i^\top\btheta) + (1- \omega_i^{(\tau, t)}) \cdot \log(1-p(\bz_i^\top\btheta)) }.
\end{aligned}
\end{equation}
The M-step proceeds by maximizing $Q_{n_{\tau}}(\bgamma \cond \bgamma^{(\tau, t-1)})$, which is equivalent to maximizing $Q_{n_{\tau}1}(\bbeta_1 \cond \bgamma^{(\tau,t-1)})$, $Q_{n_{\tau}2}(\bbeta_2 \cond \bgamma^{(\tau,t-1)})$, and $Q_{n_{\tau}3}(\btheta \cond \bgamma^{(\tau,t-1)})$ simultaneously. 
However, in the high-dimensional setting, direct maximization of these objectives tends to overfit the data. To address this challenge, our algorithm incorporates regularization terms $\norm{\bbeta_1}_1$, $\norm{\bbeta_2}_1$, and $\norm{\btheta}_1$ to induce sparsity in the parameter estimates.
The specific updates of the model parameters are presented in \eqref{eqn:coeff-updates}. 
%We also update $\omega_{i}^{(t+1)} = \omega\paren{\bx_i, y_i, \bz_i; \bgamma^{(t+1)}}$. 
The algorithm then proceeds iteratively, alternating between the E-step and M-step until convergence.


\section{Theoretical Analysis}\label{sec:theory}

%The classical low-dimensional setting is considered in \cite{zhu2004hypothesis}.
%We consider a high-dimensional setting. 
 
In this section, we establish theoretical guarantees for the estimation error of  $\big(\hbtheta^{(\tau)}, \hbbeta_1^{(\tau)}, \hbbeta_2^{(\tau)}\big)$ learned in the $\tau$-th episode using $N_{\tau-1}=n_02^{\tau-1}$ samples collected from the previous episode.
%Let $\calI_{\tau-1}$ be the set of time indices collected from the $(\tau-1)$-th episode. 
%We first establish the properties of estimators $\big(\hat\btheta^{(\tau)}, \hat\bbeta_1^{(\tau)}, \hat\bbeta_2^{(\tau)}\big)$ obtained  at the $\tau$-th episode. 
We begin by introducing the parameter space that characterizes the sparsity of the true parameters $( \btheta^*, \bbeta_1^*, \bbeta_2^*)$:
\begin{equation}
\btheta^*, \bbeta_1^*, \bbeta_2^* \in \Theta(d, s) = \braces{ \btheta, \bbeta_1, \bbeta_2 \in \RR^d:\norm{\btheta}_0 \le s,
\norm{\bbeta_1}_0 \le s, 
\norm{\bbeta_2}_0 \le s
},
\end{equation}
where $\btheta^*$ and $\bbeta^*$ are assumed to share the same dimension $d$ and sparsity level $s$ without loss of generality. 
Our analysis relies on the following regularity conditions:
\begin{enumerate}[label=(A\arabic*)]
\item Assume that there exists $0< \xi < 1/2$ such that $\xi < p(\bz_i^\top\btheta^*) < 1 - \xi$ or equivalently, there exists $C_{\xi} > 0$ such that $\abs{\bz_i^{\top}\btheta^*} < C_{\xi}$ for all $i$. \label{A1}
\item Assume that the initial estimators in the first episode satisfy
$\big\|\bbeta_1^{(1, 0)}-\bbeta_1^*\big\|_2 + \big\|\bbeta_2^{(1, 0)}-\bbeta_2^*\big\|_2+ \norm{\btheta^{(1, 0)}-\btheta^*}_2 \le \delta_{1, 0}$ %\min\braces{\xi, 1-\xi, \norm{\bbeta_1^*-\bbeta_2^*}_2}$ 
for a sufficiently small constant $\delta_{1, 0}$. \label{A2}
\item\label{A3} Define the signal strength $\Delta^* \defeq \norm{\bbeta_1^*-\bbeta_2^*}_2$. Assume that the signal-to-noise ratio (SNR), defined as $\Delta^*/ \sigma$, satisfies $\Delta^*/ \sigma \ge C_{\mathrm{SNR}}$ for a sufficiently large constant $C_{\mathrm{SNR}}$, where $\sigma$ is the standard deviation of the noise $\epsilon_i$. 
\item For each $k\in [K]$, assume that the i.i.d. covariates $(\bx_{i, k}, \bz_i)$ are sub-Gaussian. Moreover, let $\bSigma_{x, k}\defeq\EE[\bx_{i,k}\bx_{i, k}^{\top}]$ and $\bSigma_z\defeq\EE[\bz_{i}\bz_i^{\top}]$, and assume that there exists a costant $M>1$ such that  $1/ M < \lambda_{\min}(\bSigma_{x, k}) \le \lambda_{\max}(\bSigma_{x, k}) < M$ for all $k \in [K]$ and $1/ M < \lambda_{\min}(\bSigma_z) \le \lambda_{\max}(\bSigma_z) < M$. \label{A4}
\end{enumerate}

Assumption \ref{A1} requires non-degenerate group assignment probabilities, preventing the variance of the group indicator $g_i$ from vanishing, as $\Var(g_i) = p_i(1-p_i)$ is bounded away from zero. This condition is standard in high-dimensional logistic regression literature \citep{van2014asymptotically,abramovich2018high,athey2018approximate}.
Assumption \ref{A2} assumes the initial estimators to be within a neighborhood of the true parameters, which can be achieved through any consistent initialization procedure with sufficient samples in episode 0. 
%Due to the non-concavity of the log-likelihood, such a condition is common in the literature of mixed linear regression, {\red \citep{???}}.  
We provide a detailed initialization algorithm in Remark \ref{rmk: Initialization}.
Assumption \ref{A3} characterizes the minimal signal strength required for effective group separation, which is necessary for distinguishing different groups in mixture models \citep{loffler2021optimality, ndaoud2022sharp}. For clear presentation, we defer the explicit specification of constants $\delta_{1, 0}$ and $C_{\mathrm{SNR}}$  to Section  \ref{sec:proof-coeff-bound}  of the supplementary material.
Assumption \ref{A4}  imposes standard regularity conditions that ensure the non-singularity and upper-boundedness of the population covariance matrices.

\begin{remark}[Initialization]\label{rmk: Initialization}
	The initial estimators $\left(\btheta^{(1, 0)}, \bbeta^{(1, 0)}_{1}, \bbeta^{(1, 0)}_{2}\right)$ in the first episode can be obtained through any algorithm that generates consistent estimators for $(\btheta^*, \bbeta^*_{1}, \bbeta^*_{2})$ that satisfy Assumption \ref{A2}.  We recommend the following approach, which proceeds in three stages. First, using all the samples $\{\by_i, \bx_{i}\}_{i \in \calI^{(0)}}$ collected in episode 0, we fit a single LASSO model to select the features in $\bx$ with non-zero coefficients.  Second, we apply Gaussian Mixture clustering to $\{y_i, \bx_{i}'\}_{i\in \calI^{(0)}}$, where $\bx_{i}'$ denotes the selected features observation $i$, to cluster the initial data into two groups. Third, we use these group labels to fit a logistic regression model, obtaining the estimator $\btheta^{(1, 0)}$, and separately fit two LASSO models within the two groups to obtain  $\bbeta^{(1, 0)}_{1}$ and $\bbeta^{(1, 0)}_{2}$. Prior research has demonstrated that Gaussian Mixture clustering can achieve high accuracy of the two groups when the signal-to-noise ratio is sufficiently large  \citep{loffler2021optimality, ndaoud2022sharp}, which, combined with \ref{A3} and sufficiently large $n_0$, ensures that our initial estimators satisfy \ref{A2}. 
\end{remark}

 \subsection{Learning Performance} \label{sec:theory-learning}
 
 
Building upon the above assumptions, we present our theoretical results, which characterize the estimation error of the learned parameters.


\begin{theorem}%[Coefficients bound under the high-dimensional setting.]
	 \label{thm:coeff-bound-hd}
Suppose Assumptions \ref{A1}--\ref{A4} hold and $s^2\log d\log n_0 \lesssim n_0$. Let the initial estimators $\bgamma^{(\tau, 0)}=\hat\bgamma^{(\tau-1)}$ for $\tau \geq 2$. Furthermore, select $t_{\tau, \max}\asymp 
\log n_0$  for $\tau=1$ and $t_{\tau, \max}\asymp 
1$ for $\tau\geq 2$. By choosing appropriate regularization parameters $\big\{\lambda_{n_{\tau}}^{(t)}\big\}$, we have
\begin{equation}\label{eq:est-bound-l2}
\norm{\hat{\bbeta}_{1}^{(\tau)} - \bbeta_{1}^*}_2 + \norm{\hat{\bbeta}_{2}^{(\tau)} - \bbeta_{2}^*}_2 + \norm{\hat{\btheta}^{(\tau)} - \btheta^*}_2
\lesssim 
\sqrt{\frac{s\log d\log n_0}{N_{\tau-1}}},
\end{equation}
and
\begin{equation}\label{eq:est-bound-l1}
	\norm{\hat{\bbeta}_{1}^{(\tau)} - \bbeta_{1}^*}_1 + \norm{\hat{\bbeta}_{2}^{(\tau)} - \bbeta_{2}^*}_1 + \norm{\hat{\btheta}^{(\tau)} - \btheta^*}_1
	\lesssim 
	\sqrt{\frac{s^2\log d\log n_0}{N_{\tau-1}}},
\end{equation}
with probability at least $1-d^{-1}$, where $\big(\hat{\btheta}^{(\tau)},\hat{\bbeta}_{1}^{(\tau)},\hat{\bbeta}_{2}^{(\tau)}\big)$ are obtained from Algorithm \ref{alg:em} in the $\tau$-th episode.
\end{theorem}

Theorem \ref{thm:coeff-bound-hd} establishes the statistical convergence rates for both $\ell_2$ and $\ell_1$ estimation errors. The $\ell_2$ error bound scales as  $\bigO{\sqrt{s\log d \log n_0 / N_{\tau-1}}}$, while the $\ell_1$ estimation error scales as $\bigO{\sqrt{s^2\log d \log n_0 / N_{\tau-1}}}$, matching the minimax optimal rates for high-dimensional sparse estimation up to a logarithm factor in $n_0$. We note that Theorem \ref{thm:coeff-bound-hd} serves as a simplified version of our theoretical results, a complete version of which is provided as Theorem \ref{thm:1-detailed} in Section \ref{sec:proof-coeff-bound} of the supplemental materials, where we explicitly specify the choice of the regularization parameters $\big\{\lambda_{n_{\tau}}^{(t)}\big\}$, the concrete requirements for the constants $\delta_{1, 0}$ and $C_{\mathrm{SNR}}$, and more accurate probability bounds.

To better understand the convergence behavior, we provide a more detailed analysis in Remark \ref{rmk:logn0}, which elucidates the role of $t_{\tau,\max}$ and explains the origin of the additional $\sqrt{\log n_0}$ factor in the error bounds. This analysis reveals how the algorithm's phased learning structure and the initial estimation error influence the final convergence rates.

\begin{remark}\label{rmk:logn0}
	Our theoretical analysis in Section \ref{sec:proof-coeff-bound} of the supplemental material establishes a finer error bound for the estimators $\big(\hbtheta^{(\tau)}, \hbbeta_1^{(\tau)},  \hbbeta_2^{(\tau)}\big)$:
	\begin{equation}\label{eq:est-bound-t}
		\norm{\hbbeta_1^{(\tau)} - \bbeta_1^*}_2 + \norm{\hbbeta_2^{(\tau)} - \bbeta_2^*}_2 + \norm{\hbtheta^{(\tau)} - \btheta^*}_2 
		\lesssim
		\rho^{t_{\tau, \max}}\delta_{\tau, 0}
		+ \sqrt{\frac{st_{\tau, \max}\log d}{N_{\tau-1}}},
	\end{equation}
	where $\rho<1$ is a contraction factor, and $\delta_{\tau, 0}:=
	\big\|\bbeta_1^{(\tau, 0)} - \bbeta_1^*\big\|_2 + \big\|\bbeta_2^{(\tau, 0)} - \bbeta_2^*\big\|_2 + \norm{\btheta^{(\tau, 0)} - \btheta^*}_2$. The first term in \eqref{eq:est-bound-t} quantifies the contraction of initial estimation error, while the second term represents the statistical error rate. For $\tau=1$, since Assumption \ref{A2} assumes the initial estimation error $\delta_{1, 0}$ to be a constant, we require $t_{1, \max} \asymp \log n_0$ iterations to ensure the first term $\rho^{t_{\tau, \max}}\delta_{1, 0}$ is dominated by the second term, yielding a rate of  $\sqrt{\frac{s\log n_0 \log d}{N_{\tau-1}}}$. For $\tau \geq 2$, since the initial estimation error $\delta_{\tau, 0} \asymp  \sqrt{\frac{s\log n_0 \log d}{N_{\tau-2}}}$, a constant number of iterations suffices. If we strengthen \ref{A2} to require that $\delta_{1, 0}=\bigO{a_{n_0}}$ for some $a_{n_0}=o(1)$, then the extra $\sqrt{\log n_0}$ factor in \eqref{eq:est-bound-l2} and \eqref{eq:est-bound-l1} can be reduced to $\sqrt{\log (n_0a_{n_0}^2)}$.
	\end{remark}

%\section{Theoretical Analysis of Decision Performance}


\subsection{Classification Accuracy}\label{sec:theory-classification}

In this section, we provide a theoretical guarantee for the statistical accuracy of the latent group identification procedure Algorithm \ref{alg:em-bandit}. 
%For ease of presentation, we denote the estimated coefficients for decision in the $\tau$-th episode as $\hat\bgamma = \hat\bgamma^{(T)}:= (\hat\bbeta_1^{(T)}, \hat\bbeta_2^{(T)}, \hat\btheta^{(T)})$. 
In the $\tau$-th episode, after obtaining $\big(\hat\btheta^{(\tau)}, \hat\bbeta_1^{(\tau)}, \hat\bbeta_2^{(\tau)}\big)$, Algorithm \ref{alg:em-bandit} employs a Bayes classifier $G\big(\bz_i; \hat\btheta^{(\tau)}\big)$, defined as  
\begin{align*} \label{eqn:bayes-classifier}
G(\bz_i; \btheta) = 
\begin{cases}
1, & p(\bz_i^\top\btheta)\ge 1/2, \\
2, & p(\bz_i^\top\btheta) < 1/2.
\end{cases}
\end{align*}

To characterize the classification performance, we define the optimal misclassification error achievable with the true parameter,
$
R(\btheta^*) := \EE\brackets{\bbone(G(\bz_i;\btheta^*) \ne g_i)},
$
and the misclassification error of the estimated classifier,
$
R\big(\hat\btheta\big) := \EE\brackets{\bbone(G(\bz_i;\hat\btheta) \ne g_i) \mid \hbtheta}.
$

\begin{theorem} \label{thm:miss-class-rate}
Let $\hat{\btheta}^{(\tau)}$ be the estimator obtained in the $\tau$-th episode for $\tau \geq 1$. Under the assumptions of Theorem \ref{thm:coeff-bound-hd}, we have that the excess misclassification error  satisfies
\begin{align*}
R\big(\hat\btheta^{(\tau)}\big)-R(\btheta^*) \lesssim \sqrt{\frac{s\log d\log n_0}{N_{\tau-1}}},
\end{align*}
with probability at least $1-d^{-1}$.
\end{theorem}

Theorem \ref{thm:miss-class-rate} reveals that the excess misclassification error converges at the same rate as the parameter estimation error bound established in Theorem \ref{thm:coeff-bound-hd}, which is useful for our subsequent regret analysis in Section \ref{sec:theory-regret}. %The proof of Theorem \ref{thm:miss-class-rate} is provided in Section \ref{sec:proof-misclustering} of the supplemental material.

\subsection{Regret Analysis}
\label{sec:theory-regret}

Recall that, for any policy $\hat{\pi}$, we define two types of regrets in Section \ref{sec:two-type-regret}: (1) the strong regret, $\reg^{*}_i = \underset{a\in [K]}{\max}\; \angles{\bx_{i,a}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}$, where $\hat{a}_i$ is the action chosen by policy $\hat{\pi}$ for customer $i$, and (2) the regular regret, $\tilde{\reg}_i= \angles{\bx_{i,\tilde{a}_i}, \bbeta^*_{g_i}} - \angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}$, where $\tilde a_i = \underset{a\in [K]}{\arg \max} \angles{\bx_{i, a}, \bbeta_{\tilde g_i}}$ and $\tilde g_i = G(\bz_i; \btheta^*)$. These two regret formulations arise from comparing $\hat{\pi}$ against two types of oracles: the strong oracle, which has access to the value of $(\bbeta_1^*, \bbeta_2^*)$ and the latent group labels $g_i$, and the regular oracle, which knows $(\btheta^*, \bbeta_1^*, \bbeta_2^*)$.  Notably, the regular oracle is weaker than the strong oracle since even with the known $\btheta^*$, there remains an inherent positive misclassification error $R(\btheta^*)$ due to probabilistic nature of the subgroup model in \eqref{eqn:lhlb}. Consequently, the strong regret necessarily exceeds the regular regret.

The cumulative strong and  regular regrets over time horizon$T$ are defined, respectively, as 
\begin{equation*}
\Reg^{*} (T)= \sum_{\tau=0}^{\tau_{\max}}\sum_{i\in\calN_{\tau}}\EE[\reg^{*}_i], \quad\text{and}\quad 
\tilde{\Reg} (T)= \sum_{\tau=0}^{\tau_{\max}}\sum_{i\in\calN_{\tau}}\EE\left[\tilde{\reg}_i\right],
\end{equation*}
where $\tau_{\max}:=[\log_2(T/n_0+1)]-1$ is the maximum number of episodes within horizon length $T$. To establish theoretical bounds for $\Reg^{*} (T)$ and $\tilde{\Reg} (T)$, we require the following additional conditions:
\begin{enumerate}[label=(B\arabic*)]
%\item Sparsity assumption: There exist positive constants $c$ and $s$ such that $\norm{\bbeta_k^*}_1 \le c$ and $\norm{\bbeta_k^*}_0 \le s$ for all $k=1,2$. 
%\item Boundedness: There exists a positive constant $B$ such that $\norm{\bx_i}_{\infty} \le B$ for all $i\in[T]$. 
\item  Assume that $\norm{\bx_{i, k}}_{\infty} \leq \overline{x}$ for some constant $\overline{x}>0$ and the coefficient $\big\|\bbeta_g^*\big\|_1 \leq \overline{L}$ for some constant $\overline{L}>0$ for $g=1,2$. Consequently, the reward function is bounded: $\abs{\angles{\bx_{i, k}, \bbeta_g^*}} \leq \overline{R}=\overline{x}\overline{L}$ for all $i$ and $g=1, 2$. \label{B1}
\item  Assume that there exist positive constants $C$ and $\overline{h}$ such that, for all $h\in\big[0, \overline{h}\big]$, we have $\PP\paren{\angles{\bx_{i, \tilde{a}_{i, g}}, \bbeta_g^*} \le \max\limits_{k\ne \tilde{a}_{i,g}}\angles{\bx_{i, k}, \bbeta^*_{g}} + h} \le Ch $ for $g=1,2$, where $\tilde{a}_{i, g}:=\underset{k\in [K]}{\arg \max} \angles{\bx_{i, k}, \bbeta_g^*}$. \label{B2}
\end{enumerate}

Assumptions \ref{B1} and \ref{B2} both are standard conditions that have been widely assumed in the linear bandit literature \citep{goldenshluger2013linear, bastani2020online, bastani2020mostly, li2021regret, wang2024online}. Specifically, Assumption \ref{B1} ensures that the maximum regret at each time is upper bounded.  Assumption \ref{B2} plays an important role in controlling the distribution of the covariates in the neighborhood of decision boundaries, where small perturbations in parameter estimates can lead to changes in the selected actions. This assumption is satisfied under relatively mild conditions, for example, the density of $\angles{x_{i, k}, \bbeta_g^*}$ is uniformly bounded for all $k \in [K]$. Remark \ref{rmk:margin} further explains the necessity of Assumption \ref{B2} in the context of latent heterogeneity.

We then establish theoretical upper bounds for both the strong and regular regrets of Algorithm \ref{alg:em-bandit}.

\begin{theorem} \label{thm:regret}
Assume the assumptions in Theorem \ref{thm:coeff-bound-hd} and Assumptions \ref{B1} and \ref{B2} hold. We have the cumulative strong regret 
\begin{equation}\label{eq:upper-strong}
\Reg^*(T) \lesssim  \overline{R}n_0 +\overline{x}^2s^2\log d \log n_0\log T+ \overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1R(\btheta^*) T.
\end{equation}
and the cumulative regular regret 
\begin{equation}\label{eq:upper-regular}
\tilde{\Reg}(T) \lesssim \overline{R}n_0+ \overline{x}^2 \norm{\bbeta_2^*-\bbeta_1^*}_1\sqrt{s^2\log d \log n_0 } \sqrt{T}.
\end{equation}
\end{theorem}

%The proof of Theorem \ref{thm:regret} is provided in Section \ref{sec:proof-upper} in the supplemental materials.
The upper bound for strong regret comprises three terms. The first term captures the cost of initial exploration in episode 0, and the second term stems from the small failure probabilities established in Theorems \ref{thm:coeff-bound-hd} and \ref{thm:miss-class-rate}. The third term, which dominates the other two terms for large $T$, reflects the impact of misclassification. As defined in Section \ref{sec:theory-classification}, the classifier $G\big(\bz_i; \btheta^{(\tau)}\big)$ misclassifies a customer $i$ in episode $\tau$ with probability $R\big(\btheta^{(\tau)}\big)$, which converges to a constant $R(\btheta^*)$  as characterized by Theorem \ref{thm:miss-class-rate}. Under such misclassification, the instant strong regret $\reg^{*}_i$ attains a constant level $\overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1$, leading to the linear term $\overline{x} \norm{\bbeta_2^* - \bbeta_1^*}_1R(\btheta^*) T$.

However, this linear term vanishes in the regular regret bound. This occurs because the regular regret compares our algorithm against oracle actions selected using the true value of $\btheta^*$, which itself incurs a constant $R(\btheta^*)$ misclassification rate. The two constants cancel each other, leaving only an $O(\sqrt{T\log d})$ term that emerges from the failure of the event defined in Assumption \ref{B2}.


\begin{remark}\label{rmk:margin}
	Assumption \ref{B2} is essential for establishing bound \eqref{eq:upper-regular} for the regular regret under latent heterogeneity. Violation of this assumption makes it fundamentally difficult to distinguish the optimal arm from the sub-optimal arms, resulting in sub-optimal selection, i.e.,  $\hat{a}_{i}=\argmax_k \angles{\bx_{i, k}, \hbbeta_{g}} \neq \tilde{a}_{i, g}$. This issue remains manageable for regret analysis within a single group since $\angles{\bx_{i, \tilde{a}_{i,g}}, \bbeta^*_{g}}-\angles{\bx_{i, \hat{a}_{i}}, \bbeta^*_{g}}=\max_k \angles{\bx_{i, k}, \bbeta^*_{g}}-\max_k \angles{\bx_{i, k}, \hbbeta_{g}}+\angles{\bx_{i, \hat{a}_i}, \hbbeta_{g}}-\angles{\bx_{i, \hat{a}_{i}}, \bbeta^*_{g}} \leq 2\sup \abs{\angles{\bx_{i, k},\hbbeta_{g}- \bbeta^*_{g}}}$ is still bounded by the $\ell_1$ estimation error $\big\|\hbbeta_{g}-\bbeta^*_{g}\big\|_1$. However, with latent heterogeneity, the consequence of sub-optimal selection becomes more severe. Consider a customer $i$ with $p(\bz_i^{\top}\btheta^*)<1/2$, who can be assigned to group $g_i=1$ with a positive probability. In such case, if the estimated action $\hat{a}_{i}=\argmax_k \angles{\bx_{i, k}, \hbbeta_{2}} \neq \tilde{a}_{i, 2}$, the regular regret $\angles{\bx_{i, \tilde{a}_{i,2}}, \bbeta^*_{1}}-\angles{\bx_{i, \hat{a}_{i}}, \bbeta^*_{1}}$ can attain a constant level, regardless of estimation accuracy. 
\end{remark}

We further develop minimax lower bounds for the strong and regular regrets.  For any policy $\hat\pi$, define \[\EE_{\hat\pi}[\reg^*_{i}]:=\EE\left[\underset{k\in [K]}{\max}\; \angles{\bx_{i,k}, \bbeta^*_{g_i}}\right] -\EE_{\hat\pi}\left[\angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}\right], \quad \EE_{\hat\pi}[\widetilde{\reg}_{i}]:=\EE\left[\angles{\bx_{i,\widetilde{a}_i}, \bbeta^*_{g_i}}\right] -\EE_{\hat\pi}\left[\angles{\bx_{i,\hat a_i}, \bbeta^*_{g_i}}\right], \]
where $\EE_{\hat\pi}$ represents that the actions $\hat{a}_i$ are chosen according to the policy $\hat\pi$. We then establish the following lower-bound result.
\begin{theorem}\label{thm:lower-bound} Let $\mu(y, \bx, \bz; \bgamma^*)$ denote a distribution of $(y_i, \bx_i, \bz_i)$ that satisfies model \eqref{eqn:lhlb}  with $\bgamma^*=(\btheta^*, \bbeta_1^*, \bbeta_2^*)$, and define $\calP_{d, s,\overline{x},\overline{L}}$ as the collection of all the distributions $\mu(y, \bx, \bz; \bgamma^*)$ such that $\bgamma^* \in \Theta(d, s)$ and Assumptions \ref{A1}, \ref{A4}, \ref{B1}, and \ref{B2} hold with constants $\overline{x}$ and $\overline{L}$. Then we have
	\begin{equation}\label{eq:lower-strong}
			\inf_{\hat\pi}\sup_{\mu \in \calP_{d, s, \overline{x}, \overline{L}}}\sum_{i=1}^{T}\EE_{\hat\pi}[\reg^*_{i}]\gtrsim \overline{x}\overline{L}R(\btheta^*)T,  \quad	\inf_{\hat\pi}\sup_{\mu \in \calP_{d,s,\overline{x},\overline{L}}} \sum_{i=1}^{T}\EE_{\hat\pi}[\widetilde{\reg}_{i}]\ \gtrsim \overline{x}\overline{L}\sqrt{s\log d}\sqrt{T},
	\end{equation}
	where the infimum is taken over all the possible policies $\hat{\pi}$.
\end{theorem}

Compared to the upper bounds established in Theorem \ref{thm:regret}, the lower bounds replace $\norm{\bbeta_1^{*}-\bbeta_2^{*}}_1$ by $\overline{L}$, which are equivalent since $\sup\norm{\bbeta_1^{*}-\bbeta_2^{*}}_1 = 2\overline{L}$ under Assumption \ref{B1}. The lower bound $\Omega(R(\btheta^*)T)$ for the strong regret precisely matches the dominant linear term in the upper bound \eqref{eq:upper-strong}, while the lower bound $\Omega(\sqrt{Ts\log d})$ aligns with the upper bound \eqref{eq:upper-regular} in terms of $T$ and $d$, demonstrating the minimax optimality of our proposed method under latent heterogeneity. The gap between the upper and lower bounds for the regular regret amounts to a factor of $\sqrt{s\log n_0}$, which, as discussed in Remark \ref{rmk:logn0}, stems from the sample splitting procedure and can be reduced by assuming stronger conditions on the initialization.
%The proof of Theorem \ref{thm:lower-bound} is provided in Section \ref{sec:proof-lower} of the supplementary material.


\section{Numerical Study} \label{sec:numerical}

In this section, we evaluate the performance of our proposed heterogeneous algorithm through numerical studies. Section \ref{sec:simul} presents simulation studies to show the effectiveness of our algorithm and validate our theoretical findings under various settings. Section \ref{sec:real} further illustrates the practical utility of our method through an application to a cash bonus dataset from a mobile commerce platform.

\subsection{Simulations} \label{sec:simul}


In this section, we conduct numerical simulations to demonstrate the performance of our proposed algorithm under varying conditions. Our simulation is implemented based on the model: $y_{i, k}= \angles{\bx_{i, k}, \bbeta_{g_i}^*}+\epsilon_i$ for $\bx_{i, k} \in \RR^d$, $k \in [K]$ and $g_i \in \{1, 2\}$, with $\epsilon_i \sim \calN(0, \sigma^2)$.  We let the number of available actions $K=2$ and generate the covariates $\bx_{i, k} \sim \calN(\bmu_{k}, \bSigma_k)$ ($k = 1,2$). Each entry of $\bmu_{1}$ and $\bmu_{2}$ is independently generated from $\calN(1, (0.5)^2)$ and $\calN(-1, (0.5)^2)$, respectively, and the covariance matrix $\bSigma_1=\bSigma_2$ is set to be an AR(1) matrix with correlation $0.5$, that is, $(\bSigma_1)_{j_1, j_2}=(\bSigma_2)_{j_1, j_2}=0.5^{\abs{j_1-j_2}}$. For the parameters $\bbeta_g^*$, we set $\bbeta_{1, j}^* = \overline{L}\bbone(1\leq j \leq s) / s$ and $\bbeta_{2, j}^* = -\overline{L}\bbone(d/2\leq j \leq d/2+s) / s $ for $j=1,\dots,d$, which ensures that $\norm{\bbeta_1^*}_1=\norm{\bbeta_2^*}_1=\overline{L}$. Specifically, we fix the noise level $\sigma=1$ and the sparsity $s=20$ and vary $\overline{L} \in \{2.5, 5\}$ and $d \in \{500, 1000\}$. The group assignment probability is determined by $\PP(g_i = 1 | \bz_i) = 1 / (1 + \exp(-z^{\top}_i \btheta^*))$, where $z_i \in \RR^{50}$ is generated from $N(0, I_{50})$, and $\btheta^*$ has nonzero entries in its first 10 dimensions, drawn uniformly from $[-1, 1]$. 

To assess the performance of our proposed algorithm (Algorithm \ref{alg:em-bandit}) and verify our theoretical results, we compare the average strong and regular regrets, i.e., $\frac{1}{T}\Reg^*(T)$ and $\frac{1}{T}\widetilde{\Reg}(T)$, against a benchmark---a single LASSO method applied without considering the latent group structure. In both algorithms, the regularization parameters $\lambda$ are chosen through cross-validation, and the maximum number of iterations $t_{\max}$ in Algorithm \ref{alg:em} is set as 1 for all episodes. Figure \ref{fig:regret} presents the average strong and regular regrets of our proposed algorithm (``hetero'') and the single LASSO (``single'') for $s=20$, $d\in\{500, 1000\}$, and $\overline{L} \in \{2.5, 5\}$. All results are averaged over 100 independent runs. %Additional simulation results, including those for $s=5$, are relegated to the supplementary material. 

\begin{figure}[!t]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/regret_p=500_rho=2.5_s=20.png}
		\caption{$d=500, \overline{L}=2.5$}
		\label{fig:regret_500_2.5}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/regret_p=500_rho=5_s=20.png}
		\caption{$d=500, \overline{L}=5$}
		\label{fig:regret_500_5}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{ figs/regret_p=1000_rho=2.5_s=20.png}
		\caption{$d=1000, \overline{L}=2.5$}
		\label{fig:regret_1000_2.5}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/regret_p=1000_rho=5_s=20.png}
		\caption{$d=1000, \overline{L}=5$}
		\label{fig:regret_1000_5}
	\end{subfigure}
	\caption{Average strong and regular regrets with $s=20$, $d=\{500, 1000\}$ and $ \overline{L}\in \{2.5, 5\}$. The horizontal axis ``time'' represents the sample size $T$. }
	\label{fig:regret}
\end{figure}



As shown in Figure \ref{fig:regret}, our proposed heterogeneous algorithm significantly outperforms the ``single'' algorithm, demonstrating the critical importance of identifying latent group heterogeneity. Under all scenarios, the average regular regret of our algorithm (``regular regret - hetero'') approaches zero as $T$ increases, while the average strong regret (``strong regret - hetero'') stabilizes at a constant level. This observation aligns closely with Theorem \ref{thm:regret}, where we theoretically establish that the cumulative strong regret grows linearly in $T$, and the cumulative regular regret exhibits sublinear growth. 


Furthermore, when the parameter magnitude $\overline{L}$ increases from $2.5$ to $5$, the constant level of average strong regret approximately doubles. This behavior is consistent with the theoretical bound derived in equations \eqref{eq:upper-strong} since $\norm{\bbeta^*_1-\bbeta_2^*}_1 \propto \overline{L}$ in our experimental setup. Moreover, the dimensionality of the problem $d$ demonstrates minimal impact on the regret performance when varying from $500$ to $1000$, illustrating our algorithm's robustness and effectiveness in handling high-dimensional parameter spaces.

\begin{figure}[!t]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/error_p=500_rho=2.5_s=20.png}
		\caption{$d=500$}
		\label{fig:error_500_2.5}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/error_p=1000_rho=2.5_s=20.png}
		\caption{$d=1000$}
		\label{fig:error_1000_2.5}
	\end{subfigure}
	\caption{Estimation errors of the parameters $( \btheta^*,\bbeta_1^*, \bbeta^*_2)$ with $s=20$, $\overline{L}=2.5$, and $d\in\{500, 1000\}$. The horizontal axis ``time'' represents the sample size $T$.}
	\label{fig:error}
\end{figure}

Moreover, we present the $\ell_2$ estimation error for the parameters $(\btheta^*, \bbeta_1^*, \bbeta_2^*)$  in Figure \ref{fig:error}. Due to the potential for the algorithm to interchange the two groups, we compute the $\ell_2$ estimation errors of $\bbeta_g^*$ and $\btheta^*$ as the minimum error between the estimated group and the alternative group, that is, $\min\left\{\norm{\hat\bbeta_g-\bbeta_g^*}_2, \norm{\hat\bbeta_{\{1, 2\} \setminus g}-\bbeta_g^*}_2\right\}$ and $ \min\left\{\norm{\hat\btheta-\btheta^*}_2, \norm{\hat\btheta+\btheta^*}_2\right\}$,
respectively. As illustrated in Figure \ref{fig:error}, the estimation errors consistently decrease with increasing sample size $T$, indicating that our algorithm effectively estimates the model parameters in high-dimensional settings, which leads to the reduction in regret. The estimation errors for $d = 1000$ are only slightly larger than those for $d = 500$, which aligns with our theoretical result that the estimation error scales with $\sqrt{\log d}$. 


\subsection{Real Data Analysis} \label{sec:real}

%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.5\textwidth]{ figs/regret_real.png}
%	\caption{The average regret of different methods on the cash bonus dataset}
%	\label{fig:regret_real}
%\end{figure}


In this section, we illustrate the usefulness of our proposed method with application to the cash bonus dataset, originally presented by \cite{chen2022bcrlsp}. The dataset was collected from a mobile app, Taobao Special Offer Edition, which provided its users with a daily cash bonus that could be subtracted from the final payment at the time of purchase within 24 hours. The aim is to determine the optimal amount of cash bonus allocated to each user that leads to the highest payment.

Each observation in the dataset consists of customer features (user demographics and behavior information), a consecutive action variable $a_i$ (the amount of cash bonus) ranging from 0.25 to 1.95 with increment 0.01, and a continuous reward $y_i$ (the actual payment when the user redeems the cash bonus). To improve the computational efficiency, we compute the top 100 principal components of the customer features as the feature variable $\bz_{i}$. The contextual features $\bx_{i, a_{i}}$ are defined as $\bx_{i, a_{i}}=[a_i, a_i^2, \bz_i, a_i \bz_i, a_i^2\bz_i]$, incorporating quadratic terms of $a_i$ and their interaction terms with $\bz_i$. Furthermore, we divide the dataset into two groups based on the user's consumption level and remove this variable from the dataset to assume that it is unknown in practice. %where the high-consumption and low-consumption groups contain and 290,494 observations, respectively. 
Following model \eqref{eqn:lhlb}, we use the entire dataset to fit an $\ell_1$-regularized logistic regression model for group classification and two LASSO models for the two groups separately. The fitted parameters are viewed as the ground truth $(\btheta^*, \bbeta_1^*, \bbeta_2^*)$, which are further used to generate the unobserved rewards $y_{i, k}$ in model \eqref{eqn:lhlb} for all of the actions $k$. 

Similar to the simulation studies, we evaluate the performance of our proposed heterogeneous method by comparing its average strong and regular regrets with a ``single'' method that applies a single LASSO in each episode without considering the heterogeneous grouping. Additionally, we also implement a ``separate'' method that applies LASSO algorithms separately for the two groups, assuming the group assignment is known. The ``separate'' method indicates the optimal performance one can expect in the heterogeneous setting with unknown groups. 

\begin{figure}[!t]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/regret_real_g1.png}
		\caption{high-consumption group}
		\label{fig:real_g1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{ figs/regret_real_g2.png}
		\caption{low-consumption group}
		\label{fig:real_g2}
	\end{subfigure}
	\caption{Average strong and regular regrets of different methods on the cash bonus dataset}
	\label{fig:real}
\end{figure}


Figure \ref{fig:real} presents a comparative analysis of the average regrets across different methods for both groups, where we set the initial sample size $n_0$ to be 256. Our proposed heterogeneous method demonstrates superior performance compared to the single LASSO approach in both groups, though with differences in the magnitude and pattern of improvement. For the high-consumption group,  the ``single'' method shows consistently higher regret levels, while our heterogeneous method achieves significantly lower regret values. For the low-consumption group, while the improvement is less clear in early episodes, it becomes particularly evident as the sample size increases. 

The performance difference of the ``single'' method between the two groups reveals the heterogeneity in purchasing behaviors and transaction values. The higher regret level in the high-consumption group reflects larger deviations from optimal bonus allocation, likely due to their higher baseline spending and larger transaction amounts. %Conversely, the lower regret levels observed in the low-consumption group reflect smaller deviations from optimal bonus allocation, consistent with their lower transaction values. 
Since the ``single'' method fits only one LASSO model per episode, it effectively tracks only the behavior patterns of low-consumption users, resulting in more accurate predictions for this group but substantial errors for high-consumption users.

In contrast, our proposed heterogeneous method successfully captures the behavior patterns of both groups, verified by its comparable regret to the ``separate'' method that represents the performance of LASSO under known group assignments. This superior performance indicates that platforms can achieve more efficient bonus allocation strategies by incorporating latent group structures, potentially leading to improved user engagement and platform profitability across different user segments.

\section{Conclusion} \label{sec:conclusion}
This paper advances the field of online decision-making by addressing the critical challenge of latent heterogeneity in stochastic linear bandits. We introduce a novel framework that explicitly models unobserved customer characteristics affecting their responses to business actions, filling a significant gap in existing approaches primarily focusing on observable heterogeneity. Our methodology introduces an innovative algorithm that simultaneously learns both latent group memberships and group-specific reward functions, effectively handling the challenge of not having direct observations of group labels.

Our theoretical analysis reveals important insights about decision-making under latent heterogeneity. We establish that while the ``strong regret'' against an oracle with perfect group knowledge remains non-sub-linear due to inherent classification uncertainty, the ``regular regret'' against an oracle aware of only deterministic components achieves a minimax optimal rate in terms of $T$ and $d$. Importantly, we demonstrate that traditional bandit algorithms that ignore latent heterogeneity incur linear regret, highlighting the necessity of our approach.

Through empirical validation using data from a mobile commerce platform, we demonstrate the practical value of our framework. The results show that our approach effectively handles real-world scenarios where customer heterogeneity plays a crucial role, such as in personalized pricing, resource allocation, and inventory management. This work provides practitioners with theoretically grounded tools for making sequential decisions under latent heterogeneity, bridging the gap between theoretical understanding and practical implementation in business applications.

Several important directions remain for future research. First, extending our framework to nonlinear structured bandit models would enable applications in more complex decision scenarios. Developing online tests for latent heterogeneity and studying heterogeneous treatment effects in bandits would further enhance the practical utility of our approach. Moreover, our finding that the ``strong regret'' remains non-sub-linear due to inherent classification uncertainty suggests a fundamental limitation that cannot be addressed through algorithmic improvements alone. This points to the necessity of mechanism design approaches that could incentivize customers to reveal their latent group memberships, potentially opening a new avenue of research at the intersection of bandit algorithms and mechanism design.
