%%
%%  Spinning Up
%%

% \lstinputlisting[language=Octave]{BitXorMatrix.m}

\section{AWS Setup}

To set up AWS, follow the \href{https://course.fast.ai/start_aws.html}{tutorial} by FastAI. Once an instance is created and started, use the following codes to start virtual environment and launch Jupyter notebook. 

\begin{lstlisting}[language=bash,caption={Connect to AWS from Local Machine}]
ssh -L localhost:8888:localhost:8888 ubuntu@<your instance IP>
source activate conda_tensorflow_p36
jupyter notebook
\end{lstlisting}

\href{https://github.com/awslabs/aws-shell}{The interactive productivity booster for the AWS CLI} provides an useful tool for using AWS. 

\section{Google CoLab}

\href{https://colab.research.google.com}{Google Colaboratory} provides that 12GB GPU support with continuous 12 hours runtime. For RL it requires to render the environment visuals. \href{https://medium.com/@kaleajit27/reinforcement-learning-on-google-colab-9cb2e1ef51e}{Here} is sort of a tutorial to get over that issue \& continue free coding. This is good for play around with free computing resources. However, you only have 12 hours for each session and no storage on the server. You need to install necessary packages every time you start. Use the following in .ipynb and then play around with gym. 

\begin{lstlisting}[language=bash,caption={Google CoLab}] 
# install necessary packages
!pip install gym
!git clone https://github.com/openai/spinningup.git
!pip install -e spinningup  
!pip install tensorflow==1.12.0
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1

# import packages 
from spinup import ppo
import tensorflow as tf
import gym
\end{lstlisting}

\section{Environment Setup}

This environment setup follows the instruction \href{https://spinningup.openai.com/en/latest/user/installation.html#installing-python}{here}. 

\subsection{Local}

Installation of system packages on Mac requires \href{https://brew.sh/}{Homebrew}.% The following \ref{lst:install_spinningup} need only run once at the every beginning. Afterwards, you can start your work after activating your environment -- ``spinningup'' in this example -- by ``source activate spinningup''. 

\begin{lstlisting}[language=bash,caption={Setup SpinningUp Environment}] 
conda create -n spinningup python=3.6

# you need to activate the environment before your start
source activate spinningup   

# install openMPI, need first install Homebrew. 
brew install openmpi  

# installing spinningup
git clone https://github.com/openai/spinningup.git
cd spinningup
pip install -e .

# check install
# try running PPO in the LunarLander-v2 environment with
python -m spinup.run ppo --hid "[32,32]" --env LunarLander-v2 --exp_name installtest --gamma 0.999

# After it finishes training, watch a video of the trained policy with
python -m spinup.run test_policy data/installtest/installtest_s0

# And plot the results with
python -m spinup.run plot data/installtest/installtest_s0

# The last step somehow went wrong with error, will check back later. 

\end{lstlisting}

\subsection{Cloud (AWS)}

Basically the same procedure once you log on to the VM through terminal. The only difference is the operating system is Ubuntu. To install ``OpenMPI'', you will need to use 

\begin{lstlisting}[language=bash,caption={Installing OpenMPI on Ubuntu}] 
sudo apt-get update && sudo apt-get install libopenmpi-dev
\end{lstlisting}

\subsection{OpenAI Gym Intro}

This part is greatly based on the \href{https://gym.openai.com/docs/}{Open AI Gym Tutorial} and \href{https://github.com/openai/gym#environment-specific-installation}{Github Repo}.

\subsubsection{Gym Environment Installation}
To run the code in \href{https://colab.research.google.com}{Google Colab}, you will need to install gym every time you start because your environment is not saved on the serve (no free lunch). Put the following codes at the start of you .ipynb to tell Colab to prepare for the environment. 

\begin{lstlisting}[language=python,caption={Google CoLab Preamble}] 
# Simply install gym using pip
!pip install gym  

# Clone the gym Git repository directly
!git clone https://github.com/openai/gym
!cd gym
!pip install -e .
\end{lstlisting}

Cloning the gym Git repository directly is particularly useful when you’re working on modifying Gym itself or adding environments. You can later run pip install -e .[all] to perform a full installation containing all environments. This requires installing several more involved dependencies, including cmake and a recent pip version.

\textbf{Rendering on a server.} If you're trying to render video on a server, you'll need to connect a fake display. The easiest way to do this is by running under xvfb-run (on Ubuntu, install the xvfb package):

\begin{lstlisting}[language=bash,caption={Installing OpenMPI on Ubuntu}] 
xvfb-run -s "-screen 0 1400x900x24" bash
\end{lstlisting}

\subsubsection{Environment}

After installation, you'll be able to run a few environments right away:
\begin{itemize}
	\item algorithmic
	\item toy\_text
	\item classic\_control (you'll need pyglet to render though)
\end{itemize}

It's better laying with those environments at first, and then later installing the dependencies for the remaining environments. The code for each environment group is housed in its own \href{https://github.com/openai/gym/blob/master/gym/envs}{subdirectory gym/envs}. The specification of each task is in \href{https://github.com/openai/gym/blob/master/gym/envs/__init__.py}{gym/envs/\_\_init\_\_.py}. It's worth browsing through both.

Here’s a bare minimum example of getting something running. This will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step. 


\begin{lstlisting}[language=python,caption={CartPole Env}]  
import gym

#Try replacing ``CartPole-v0'' with ``MountainCar-v0'', ``MsPacman-v0'', or ``Hopper-v1''.
env = gym.make('CartPole-v0') 

env.reset()
for _ in range(1000):
env.render()
env.step(env.action_space.sample()) # take a random action
\end{lstlisting}  \label{lst:cartpole_env}

Some other environments in Gym include ``MountainCar-v0'', ``MsPacman-v0'' and ``Hopper-v1''. Remind that ``MsPacman-v0'' requires the Atari dependency and ``Hopper-v1'' requires the MuJoCo dependencies which necessitate  a \href{https://www.roboti.us/license.html}{MuJoCo license}. Environments all descend from the Env base class.

\textbf{Installing dependencies for specific environments}
If you'd like to install the dependencies for only specific environments, see \href{https://github.com/openai/gym/blob/master/setup.py}{setup.py}. OpenAI maintains the lists of dependencies on a per-environment group basis.

\textbf{Environment groups} include (See this \href{https://colab.research.google.com/drive/1mPXv9Z3rIP3KbUzPzBRXiGtk-j3-IsmU}{Gym\_Environments\_1.ipynb} for details.)
\begin{itemize}
	\item Algorithmic. It contains a variety of algorithmic tasks, such as learning to copy a sequence.
	\item Toy text. Toy environments which are text-based.
	\item Classic control. It comprises of a variety of classic control tasks, which would appear in a typical reinforcement learning textbook.
	\item Atari. The Atari environments are a variety of Atari video games. 
	\item Box2d. It is a 2D physics engine.
	\item MuJoCo. It is a physics engine which can do very detailed efficient simulations with contacts.
	\item Robotics. MuJoCo is a physics engine which can do very detailed efficient simulations with contacts and we use it for all robotics environments. 
\end{itemize}
 
 Run the following code to generate a list of all environments.
 
 \begin{lstlisting}[language=python,caption={Python Code to generate a list of all enironments.}]
 #!/usr/bin/env python
 from gym import envs
 envids = [spec.id for spec in envs.registry.all()]
 for envid in sorted(envids):
 print(envid)
 \end{lstlisting}
 
 Run \href{https://github.com/openai/gym/blob/master/examples/agents/random_agent.py}{examples/agents/random\_agent.py} to run an simple random agent.
 Run \href{https://github.com/openai/gym/blob/master/examples/agents/cem.py}{examples/agents/cem.py} to run an actual learning agent (using the cross-entropy method).

\section{Implementation Plans for Next Week}

\href{https://github.com/dennybritz/reinforcement-learning}{}
\href{https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/}{}
\href{https://towardsdatascience.com/reinforcement-learning-with-python-8ef0242a2fa2}{}

\subsection{Mar 4th -- Mar 8th.}  

Next week, I am going to delve deeper into the implementation of deep RL. This include the following algorithms.
\begin{itemize}
	\item Deep Q-Network (DQN)
	\item Deep Deterministic Policy Gradients (DDPG)
\end{itemize}

To serve as a guideline, The following algorithms are implemented in the Spinning Up package. 

\begin{itemize}
	\item \href{https://spinningup.openai.com/en/latest/algorithms/vpg.html}{Vanilla Policy Gradient (VPG)}
	\item \href{https://spinningup.openai.com/en/latest/algorithms/trpo.html}{Trust Region Policy Optimization (TRPO)}
	\item \href{https://spinningup.openai.com/en/latest/algorithms/ppo.html}{Proximal Policy Optimization (PPO)}
	\item \href{https://spinningup.openai.com/en/latest/algorithms/ddpg.html}{Deep Deterministic Policy Gradient (DDPG)}
	\item \href{https://spinningup.openai.com/en/latest/algorithms/td3.html}{Twin Delayed} DDPG (TD3)
	\item \href{https://spinningup.openai.com/en/latest/algorithms/sac.html}{Soft Actor-Critic (SAC)}
\end{itemize}


\section{Useful Resources}

\href{https://github.com/openai/curriculum}{OpenAI Curriculum}

\href{https://spinningup.openai.com/en/latest/}{OpenAI Spinning Up in Deep RL}

\href{https://github.com/openai/spinningup}{SpinningUp GitHub Repo}

\href{https://spinningup.openai.com/en/latest/}{OpenAI Spinning Up}


