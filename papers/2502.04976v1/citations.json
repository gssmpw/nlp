[
  {
    "index": 0,
    "papers": [
      {
        "key": "raamkumar2022empathetic",
        "author": "Raamkumar, Aravind Sesagiri and Yang, Yinping",
        "title": "Empathetic conversational systems: A review of current advances, gaps, and opportunities"
      },
      {
        "key": "qian2023harnessing",
        "author": "Qian, Yushan and Zhang, Wei-Nan and Liu, Ting",
        "title": "Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2019moel",
        "author": "Lin, Zhaojiang and Madotto, Andrea and Shin, Jamin and Xu, Peng and Fung, Pascale",
        "title": "Moel: Mixture of empathetic listeners"
      },
      {
        "key": "zhou2022case",
        "author": "Zhou, Jinfeng and Zheng, Chujie and Wang, Bo and Zhang, Zheng and Huang, Minlie",
        "title": "Case: Aligning coarse-to-fine cognition and affection for empathetic response generation"
      },
      {
        "key": "fei2024empathyear",
        "author": "Fei, Hao and Zhang, Han and Wang, Bin and Liao, Lizi and Liu, Qian and Cambria, Erik",
        "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gao2021improving",
        "author": "Gao, Jun and Liu, Yuhan and Deng, Haolin and Wang, Wei and Cao, Yu and Du, Jiachen and Xu, Ruifeng",
        "title": "Improving empathetic response generation by recognizing emotion cause in conversations"
      },
      {
        "key": "yang2024enhancing",
        "author": "Yang, Zhou and Ren, Zhaochun and Yufeng, Wang and Peng, Shizhong and Sun, Haizhou and Zhu, Xiaofei and Liao, Xiangwen",
        "title": "Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models"
      },
      {
        "key": "sabour2022cem",
        "author": "Sabour, Sahand and Zheng, Chujie and Huang, Minlie",
        "title": "Cem: Commonsense-aware empathetic response generation"
      },
      {
        "key": "chen2024empathetic",
        "author": "Chen, Changyu and Li, Yanran and Wei, Chen and Cui, Jianwei and Wang, Bin and Yan, Rui",
        "title": "Empathetic Response Generation with Relation-aware Commonsense Knowledge"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yan-etal-2024-talk",
        "author": "Yan, Haoqiu  and\nZhu, Yongxin  and\nZheng, Kai  and\nLiu, Bing  and\nCao, Haoyu  and\nJiang, Deqiang  and\nXu, Linli",
        "title": "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction"
      },
      {
        "key": "zhang-etal-2024-stickerconv",
        "author": "Zhang, Yiqun  and\nKong, Fanheng  and\nWang, Peidong  and\nSun, Shuang  and\nSWangLing, SWangLing  and\nFeng, Shi  and\nWang, Daling  and\nZhang, Yifei  and\nSong, Kaisong",
        "title": "{STICKERCONV}: Generating Multimodal Empathetic Responses from Scratch"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "su2023pandagpt",
        "author": "Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng",
        "title": "Pandagpt: One model to instruction-follow them all"
      },
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others",
        "title": "Qwen technical report"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wu24next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "{NE}x{T}-{GPT}: Any-to-Any Multimodal {LLM}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lu2024unified",
        "author": "Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action"
      }
    ]
  }
]