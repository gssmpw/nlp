@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{chen2024empathetic,
  title={Empathetic Response Generation with Relation-aware Commonsense Knowledge},
  author={Chen, Changyu and Li, Yanran and Wei, Chen and Cui, Jianwei and Wang, Bin and Yan, Rui},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={87--95},
  year={2024}
}

@article{fei2024empathyear,
  title={EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot},
  author={Fei, Hao and Zhang, Han and Wang, Bin and Liao, Lizi and Liu, Qian and Cambria, Erik},
  journal={arXiv preprint arXiv:2406.15177},
  year={2024}
}

@inproceedings{gao2021improving,
  title={Improving empathetic response generation by recognizing emotion cause in conversations},
  author={Gao, Jun and Liu, Yuhan and Deng, Haolin and Wang, Wei and Cao, Yu and Du, Jiachen and Xu, Ruifeng},
  booktitle={Findings of the association for computational linguistics: EMNLP 2021},
  pages={807--819},
  year={2021}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023}
}

@article{lin2019moel,
  title={Moel: Mixture of empathetic listeners},
  author={Lin, Zhaojiang and Madotto, Andrea and Shin, Jamin and Xu, Peng and Fung, Pascale},
  journal={arXiv preprint arXiv:1908.07687},
  year={2019}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{lu2024unified,
  title={Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action},
  author={Lu, Jiasen and Clark, Christopher and Lee, Sangho and Zhang, Zichen and Khosla, Savya and Marten, Ryan and Hoiem, Derek and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26439--26455},
  year={2024}
}

@article{qian2023harnessing,
  title={Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements},
  author={Qian, Yushan and Zhang, Wei-Nan and Liu, Ting},
  journal={arXiv preprint arXiv:2310.05140},
  year={2023}
}

@article{raamkumar2022empathetic,
  title={Empathetic conversational systems: A review of current advances, gaps, and opportunities},
  author={Raamkumar, Aravind Sesagiri and Yang, Yinping},
  journal={IEEE Transactions on Affective Computing},
  pages={2722--2739},
  year={2022}
}

@inproceedings{sabour2022cem,
  title={Cem: Commonsense-aware empathetic response generation},
  author={Sabour, Sahand and Zheng, Chujie and Huang, Minlie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={11229--11237},
  year={2022}
}

@article{su2023pandagpt,
  title={Pandagpt: One model to instruction-follow them all},
  author={Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
  journal={arXiv preprint arXiv:2305.16355},
  year={2023}
}

@inproceedings{wu24next,
  title={{NE}x{T}-{GPT}: Any-to-Any Multimodal {LLM}},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages = {53366--53397},
  year={2024}
}

@inproceedings{yan-etal-2024-talk,
    title = "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction",
    author = "Yan, Haoqiu  and
      Zhu, Yongxin  and
      Zheng, Kai  and
      Liu, Bing  and
      Cao, Haoyu  and
      Jiang, Deqiang  and
      Xu, Linli",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "15009--15022",
}

@article{yang2024enhancing,
  title={Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models},
  author={Yang, Zhou and Ren, Zhaochun and Yufeng, Wang and Peng, Shizhong and Sun, Haizhou and Zhu, Xiaofei and Liao, Xiangwen},
  journal={arXiv preprint arXiv:2402.11801},
  year={2024}
}

@inproceedings{zhang-etal-2024-stickerconv,
    title = "{STICKERCONV}: Generating Multimodal Empathetic Responses from Scratch",
    author = "Zhang, Yiqun  and
      Kong, Fanheng  and
      Wang, Peidong  and
      Sun, Shuang  and
      SWangLing, SWangLing  and
      Feng, Shi  and
      Wang, Daling  and
      Zhang, Yifei  and
      Song, Kaisong",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
    pages = "7707--7733",
}

@article{zhou2022case,
  title={Case: Aligning coarse-to-fine cognition and affection for empathetic response generation},
  author={Zhou, Jinfeng and Zheng, Chujie and Wang, Bo and Zhang, Zheng and Huang, Minlie},
  journal={arXiv preprint arXiv:2208.08845},
  year={2022}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

