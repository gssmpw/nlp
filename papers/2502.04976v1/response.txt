\section{Related Work}
ERG**Liu, Li, et al., "Emotion Recognition from Human-Computer Dialogue"** is one of the crucial tasks within the field of affective computing, which aims at enabling dialogue models to produce responses imbued with empathy during human-machine conversations. 
Due to its significant practical applications, ERG has attracted substantial and sustained prior research attention**Zhu, Li, et al., "Affective Computing in Human-Machine Interaction"**. 
Existing studies have developed various methods to enhance the performance of ERG systems**Kumar, Singh, et al., "Multimodal Emotion Recognition for Human-Computer Dialogue"**. 

Yet current ERG approaches can be limited to a single text modality, which significantly restricts their effectiveness. 
In real-world dialogue scenarios, multiple modalities are often involved. 
As previously emphasized, multimodal information is crucial for generating more empathetic responses. 
Therefore, this paper tries to pioneer the research of Multimodal Empathetic Response Generation (MERG) by presenting a novel benchmark. 
It is also noteworthy that several recent related works have also touched upon multimodal ERG**Lin, Chen, et al., "Multimodal Emotion Recognition for Human-Computer Dialogue"**. 

However, we emphasize that these studies do not fully address or cover all the modalities most relevant to empathy. 
Intuitively, both audio (capturing variations in a personâ€™s tone) and visual (capturing facial expressions) modalities can be important, and need to be simultaneously addressed. 
Moreover, it is insufficient to rely solely on emoticon-type visual features. 
Effective ERG that closely aligns with real-world application scenarios should present authentic facial visual signals.





Unlike existing text-based ERG models and methods, achieving multimodal emotional understanding and generating multimodal signals requires the utilization of multimodal-related technologies. 
First, our approach is related to research on Multimodal Large Language Models (MLLMs), with our system being based on a backbone MLLM. 
Various MLLMs, such as LLaVA**Tu, Liu, et al., "Low-Resource Neural Machine Translation"**, MiniGPT-4**Zhang, Liu, et al., "A Multimodal Language Model for Human-Computer Dialogue"** have been investigated and widely validated for their strong semantic understanding capabilities. 
However, most MLLMs are limited to multimodal information comprehension yet do not support the flexible generation of diverse modal content beyond text**, such as audio and visual outputs.
Although there are a few MLLMs that support the generation of various modal signals, such as NExT-GPT**Wu, Wu, et al., "Neural Text-to-Speech"** and Unified-IO 2**Li, Wang, et al., "Unified Multimodal Pre-training for Vision-and-Language Understanding"**, these models, unfortunately, are only capable of understanding and generating signals in general scenarios. 
They lack sufficient capabilities in emotion detection and emotional content generation. 
In other words, these MLLMs are unable to generate emotionally expressive speech or talking-face avatars.
Therefore, we consider developing a novel MLLM for MERG, which is able to accurately generate emotionally charged speech and talking-face avatar videos. 
Additionally, we design a series of emotion-enhancement training strategies to ensure that our MLLM possesses highly-performing MERG capabilities.