We now report on the results of our experimental evaluation. 
We use the GQA dataset~\cite{hudson2019gqa}, allowing us to build on the results of \cite{eiter2022neuro}, which uses the CLEVR~\cite{johnson2017clevr} synthetic data.  
Note that we use ground truth ASP representations of the images and queries.  
We examine our practical heuristic in four different ways.  
First, we examine the accuracy improvements when employing FAST-DAP. 
Second, we examine its data efficiency (e.g., how many examples in \textbf{EX} are required to provide useful results).  
Third, we examine the sensitivity of the support threshold for elements of $\Pi_D$. 
Finally, we examine running time.  
We created our implementation in Python~3.11.7 and use the Clingo solver for the ASP engine.
Experiments were run on an Apple~M2 machine with a~10-core CPU, and 32GB of RAM. 
All computations were carried out using only the CPU (the systemâ€™s GPU was not used).
We now present the results of each experiment.




\medskip
\noindent\textbf{Accuracy.}  We assess our approach's accuracy against the baseline (no $\Pi^D$), evaluating improvements with and without fallback rules (FBR and No FBR), both utilizing FAST-DAP. For the baseline (no FAST-DAP), the ASP solver either provides an answer or returns ``empty'' if it cannot deduce one. 
On our test set (disjoint from the examples), the baseline accuracy across all question types was~$59.98\%$ without domain information. Incorporating domain information learned from the training set significantly boosted accuracy to~$80.62\%$ without fallback rules, and $81.01\%$ with them.
To gain deeper insights, we analyze specific question types, a subset of which is presented in Table~\ref{tab:acc-indv}.
Some types, such as verification questions, show minimal dependence on domain categorization, while others rely more heavily on it. Additionally, certain questions require translating specific concepts into general terms (FAST-DAP, lines~\ref{a:gtsbeg}-\ref{a:gtsend}), like generalizing ``banana'' to ``fruit'' or ``juice'' to ``drink.'' In Table~\ref{tab:acc-indv}, all non-choice queries require such generalization.







\begin{table}[thb!]
    \setlength{\tabcolsep}{12pt}
    \centering
    \begin{tabular}{ m{3cm} m{2.5cm} m{2.5cm} m{2.5cm} }
        \hline
        \textbf{Question Type} & \textbf{Baseline} & \textbf{FBR (Ours)} & \textbf{No FBR (Ours)} \\
        \hline
        choose\_activity   & 69.02 & 95.11 & 94.84 \\
        choose\_color   & 89.80 & 93.48 & 93.21 \\
        choose\_older   & 0 & 97.24 & 97.24 \\
        choose\_rel   & 73.88 & 85.48 & 81.72 \\
        choose\_vposition   & 96.27 & 94.98 & 94.93 \\
        \hline
        and  & 94.25 & 91.93 & 91.83 \\
        verify\_age   & 86.89 & 97.54 & 97.54 \\
        verify\_color   & 95.71 & 96.58 & 96.44 \\
        verify\_location   & 49.28 & 94.5 & 94.5 \\
        query   & 36.07 & 72.83 & 72.20 \\
        \hline
    \end{tabular}
    \caption{Evaluation of answering questions. The ``Baseline'' column shows accuracy (in percentage) without learned domains, ``FBR'' shows accuracy with learned domains and fallback rules, and ``No FBR'' shows accuracy with domain atoms but without using fallback rules.}
    \label{tab:acc-indv}
\end{table}











\medskip
\noindent\textbf{Data Efficiency.} In this second experiment, we aimed to find the optimal sample size for learning domains. We randomly divided the data as follows: 20\% for training, 10\% for validation, and the remaining 70\% for testing. Instead of using the entire training set at once, we divided it into~11 progressively larger subsets as follows:
the first subset served as a baseline model with no samples, the second subset contained 10\% of the training data, the third subset included the first 10\% plus an additional 10\%, making up 20\% of the training data, and this pattern continued until the 11th subset, which encompassed all the training data. 
Each training subset was used independently to learn the domains, and these learned domains were then used to predict the answers in the test set. Figure~\ref{fig:trainsubsets} illustrates the results, showing accuracy across the training data for two scenarios: the black line represents the learned domain without fallback rules, while the red line includes fallback rules. 
As depicted in Figure~\ref{fig:trainsubsets}, using just 10\% of the training set (equivalent to 2\% of the entire dataset) achieves a respectable accuracy of 78.93\%. 
With 20\% of the training data (4\% of the entire dataset), accuracy exceeds 80\%. 
This suggests that a small amount of data can effectively learn domains, with only slight accuracy gains from adding more data.

 


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/trainsubsetAccuracy.png}
        \caption{Accuracy on the test set leveraging learned domains from different training subsets. 
        }
        \label{fig:trainsubsets}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/time.png}
        \caption{Execution time of our algorithm for different sample sizes, run in parallel with identical settings.
        }
        \label{fig:time}
    \end{subfigure}
    \caption{Accuracy and running time on different training subsets.}
    \label{fig:acctime}
\end{figure}




\medskip
\noindent\textbf{Threshold Sensitivity.}  FAST-DAP refines the learned domain set by removing domains whose support falls below a specified threshold. This approach helps regularize the outcome since the domains were derived from the application of possibly noisy data and rules. The threshold is a hyper-parameter determined from the validation set. 
We used the $10^{th}$ to $70^{th}$ percentile support values as potential thresholds. For each, we removed domains with lower support, assessed validation accuracy, and selected the threshold with the highest accuracy. Domains below this final threshold were then removed.
Table \ref{tab:hs} illustrates the accuracy achieved at different thresholds. Based on this data, we selected a threshold of $59.5$, and domains with support below this value were excluded to form the final set of domains.






\begin{table}[t]
    \begin{subtable}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{ m{1.5cm} m{1.5cm} m{1.5cm} }
            \hline
            \textbf{Percentile} & \textbf{Threshold} & \textbf{Accuracy} \\
            \hline
            10   & 12.3 & 79.44 \\
            20   & 20.6 & 79.79\\
            30   & 30.9 & 79.89\\
            40   & 46.4 & 80.10\\
            \hline
        \end{tabular}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
        \centering
        \begin{tabular}{ m{1.5cm} m{1.5cm} m{1.5cm} }
            \hline
            \textbf{Percentile} & \textbf{Threshold} & \textbf{Accuracy} \\
            \hline
            50   & \textbf{59.5} & \textbf{80.54}\\
            60   & 90.8 & 80.02\\
            70   & 121.2 & 79.75\\
                 &      &      \\
            \hline
        \end{tabular}
    \end{subtable}
    \caption{Accuracy results on the validation set after removing domains with support below a threshold.
    }
    \label{tab:hs}
\end{table}



\medskip
\noindent\textbf{Running Time.}  
The running time of our algorithm is primarily influenced by the performance of the ASP solver Clingo, and is directly proportional to the number of atoms it processes. 
Figure~\ref{fig:time} illustrates that the running time grows consistently from the base case with no training samples to the scenario where all training samples are used. 
Incorporating more training samples to learn domains substantially boosts the number of learned new domain atoms, thereby requiring Clingo to process more atoms during deduction. This necessity is the main factor driving the increase in running time. However, note that this increase is bounded by a constant factor related to the domain's size.




