%Entries
@inproceedings{disentangling_hate_in_online_memes,
author = {Lee, Roy Ka-Wei and Cao, Rui and Fan, Ziqing and Jiang, Jing and Chong, Wen-Haw},
title = {Disentangling Hate in Online Memes},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475625},
doi = {10.1145/3474085.3475625},
abstract = {Hateful and offensive content detection has been extensively explored in a single modality such as text. However, such toxic information could also be communicated via multimodal content such as online memes. Therefore, detecting multimodal hateful content has recently garnered much attention in academic and industry research communities. This paper aims to contribute to this emerging research topic by proposing DisMultiHate, which is a novel framework that performed the classification of multimodal hateful content. Specifically, DisMultiHate is designed to disentangle target entities in multimodal memes to improve the hateful content classification and explainability. We conduct extensive experiments on two publicly available hateful and offensive memes datasets. Our experiment results show that DisMultiHate is able to outperform state-of-the-art unimodal and multimodal baselines in the hateful meme classification task. Empirical case studies were also conducted to demonstrate DisMultiHate's ability to disentangle target entities in memes and ultimately showcase DisMultiHate's explainability of the multimodal hateful content classification task.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5138–5147},
numpages = {10},
keywords = {social media mining, multimodal, hateful memes, hate speech},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{multimodal_zeroshot_hateful_memes_detection,
author = {Zhu, Jiawen and Lee, Roy Ka-Wei and Chong, Wen Haw},
title = {Multimodal Zero-Shot Hateful Meme Detection},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3531557},
doi = {10.1145/3501247.3531557},
abstract = {Facebook has recently launched the hateful meme detection challenge, which garnered much attention in academic and industry research communities. Researchers have proposed multimodal deep learning classification methods to perform hateful meme detection. While the proposed methods have yielded promising results, these classification methods are mostly supervised and heavily rely on labeled data that are not always available in the real-world setting. Therefore, this paper explores and aims to perform hateful meme detection in a zero-shot setting. Working towards this goal, we propose Target-Aware Multimodal Enhancement (TAME), which is a novel deep generative framework that can improve existing hateful meme classification models’ performance in detecting unseen types of hateful memes. We conduct extensive experiments on the Facebook hateful meme dataset, and the results show that TAME can significantly improve the state-of-the-art hateful meme classification methods’ performance in seen and unseen settings.},
booktitle = {Proceedings of the 14th ACM Web Science Conference 2022},
pages = {382–389},
numpages = {8},
keywords = {hateful memes, multimodal, social media mining},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@misc{aisg-challenge,
    title = "Online Safety Prize Challenge - Low-Resource Detection of Harmful Memes with Social Bias",
    author = "AI Singapore",
    year = "2024",
    url = "https://ospc.aisingapore.org/"
}

@article{AUROC,
title = {The use of the area under the ROC curve in the evaluation of machine learning algorithms},
journal = {Pattern Recognition},
volume = {30},
number = {7},
pages = {1145-1159},
year = {1997},
issn = {0031-3203},
doi = {https://doi.org/10.1016/S0031-3203(96)00142-2},
url = {https://www.sciencedirect.com/science/article/pii/S0031320396001422},
author = {Andrew P. Bradley},
keywords = {The ROC curve, The area under the ROC curve (AUC), Accuracy measures, Cross-validation, Wilcoxon statistic, Standard error},
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.}
}

@inproceedings{fersini-etal-2022-semeval,
    title = "{S}em{E}val-2022 Task 5: Multimedia Automatic Misogyny Identification",
    author = "Fersini, Elisabetta  and
      Gasparini, Francesca  and
      Rizzi, Giulia  and
      Saibene, Aurora  and
      Chulvi, Berta  and
      Rosso, Paolo  and
      Lees, Alyssa  and
      Sorensen, Jeffrey",
    editor = "Emerson, Guy  and
      Schluter, Natalie  and
      Stanovsky, Gabriel  and
      Kumar, Ritesh  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      Singh, Siddharth  and
      Ratan, Shyam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.74",
    doi = "10.18653/v1/2022.semeval-1.74",
    pages = "533--549",
    abstract = "The paper describes the SemEval-2022 Task 5: Multimedia Automatic Misogyny Identification (MAMI),which explores the detection of misogynous memes on the web by taking advantage of available texts and images. The task has been organised in two related sub-tasks: the first one is focused on recognising whether a meme is misogynous or not (Sub-task A), while the second one is devoted to recognising types of misogyny (Sub-task B). MAMI has been one of the most popular tasks at SemEval-2022 with more than 400 participants, 65 teams involved in Sub-task A and 41 in Sub-task B from 13 countries. The MAMI challenge received 4214 submitted runs (of which 166 uploaded on the leader-board), denoting an enthusiastic participation for the proposed problem. The collection and annotation is described for the task dataset. The paper provides an overview of the systems proposed for the challenge, reports the results achieved in both sub-tasks and outlines a description of the main errors for a comprehension of the systems capabilities and for detailing future research perspectives.",
}

@article{MIND,
   title={Benchmark dataset of memes with text transcriptions for automatic detection of multi-modal misogynistic content},
   volume={44},
   ISSN={2352-3409},
   url={http://dx.doi.org/10.1016/j.dib.2022.108526},
   DOI={10.1016/j.dib.2022.108526},
   journal={Data in Brief},
   publisher={Elsevier BV},
   author={Gasparini, Francesca and Rizzi, Giulia and Saibene, Aurora and Fersini, Elisabetta},
   year={2022},
   month=oct, pages={108526} }

@inproceedings{MOMENTA,
    title = "{MOMENTA}: A Multimodal Framework for Detecting Harmful Memes and Their Targets",
    author = "Pramanick, Shraman  and
      Sharma, Shivam  and
      Dimitrov, Dimitar  and
      Akhtar, Md. Shad  and
      Nakov, Preslav  and
      Chakraborty, Tanmoy",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.379",
    doi = "10.18653/v1/2021.findings-emnlp.379",
    pages = "4439--4455",
    abstract = "Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be highly satirical and cryptic. Moreover, while previous work has focused on specific aspects of memes such as hate speech and propaganda, there has been little work on harm in general. Here, we aim to bridge this gap. In particular, we focus on two tasks: (i)detecting harmful memes, and (ii) identifying the social entities they target. We further extend the recently released HarMeme dataset, which covered COVID-19, with additional memes and a new topic: US politics. To solve these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a novel multimodal deep neural network that uses global and local perspectives to detect harmful memes. MOMENTA systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. MOMENTA is interpretable and generalizable, and our experiments show that it outperforms several strong rivaling approaches.",
}

@inproceedings{MET-Meme,
author = {Xu, Bo and Li, Tingting and Zheng, Junzhe and Naseriparsa, Mehdi and Zhao, Zhehuan and Lin, Hongfei and Xia, Feng},
title = {MET-Meme: A Multimodal Meme Dataset Rich in Metaphors},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532019},
doi = {10.1145/3477495.3532019},
abstract = {Memes have become the popular means of communication for Internet users worldwide. Understanding the Internet meme is one of the most tricky challenges in natural language processing (NLP) tasks due to its convenient non-standard writing and network vocabulary. Recently, many linguists suggested that memes contain rich metaphorical information. However, the existing researches ignore this key feature. Therefore, to incorporate informative metaphors into the meme analysis, we introduce a novel multimodal meme dataset called MET-Meme, which is rich in metaphorical features. It contains 10045 text-image pairs, with manual annotations of the metaphor occurrence, sentiment categories, intentions, and offensiveness degree. Moreover, we propose a range of strong baselines to demonstrate the importance of combining metaphorical features for meme sentiment analysis and semantic understanding tasks, respectively. MET-Meme, and its code are released publicly for research in urlhttps://github.com/liaolianfoka/MET-Meme-A-Multi-modal-Meme-Dataset-Rich-in-Metaphors.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2887–2899},
numpages = {13},
keywords = {sentiment analysis, multimodal learning, metaphor, meme dataset},
series = {SIGIR '22}
}

@inproceedings{tamil_troll,
    title = "A Dataset for Troll Classification of {T}amil{M}emes",
    author = "Suryawanshi, Shardul  and
      Chakravarthi, Bharathi Raja  and
      Verma, Pranav  and
      Arcan, Mihael  and
      McCrae, John Philip  and
      Buitelaar, Paul",
    editor = "Jha, Girish Nath  and
      Bali, Kalika  and
      L., Sobha  and
      Agrawal, S. S.  and
      Ojha, Atul Kr.",
    booktitle = "Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/2020.wildre-1.2",
    pages = "7--13",
    abstract = "Social media are interactive platforms that facilitate the creation or sharing of information, ideas or other forms of expression among people. This exchange is not free from offensive, trolling or malicious contents targeting users or communities. One way of trolling is by making memes, which in most cases combines an image with a concept or catchphrase. The challenge of dealing with memes is that they are region-specific and their meaning is often obscured in humour or sarcasm. To facilitate the computational modelling of trolling in the memes for Indian languages, we created a meme dataset for Tamil (TamilMemes). We annotated and released the dataset containing suspected trolls and not-troll memes. In this paper, we use the a image classification to address the difficulties involved in the classification of troll memes with the existing methods. We found that the identification of a troll meme with such an image classifier is not feasible which has been corroborated with precision, recall and F1-score.",
    language = "English",
    ISBN = "979-10-95546-67-2",
}

@misc{reddit_memes_dataset,
    title={Reddit Memes Dataset},
    url={https://www.kaggle.com/datasets/sayangoswami/reddit-memes-dataset},
    author={Sayan Goswami},
    year={2018}
}

@misc{indian_memes,
    title={Indian Memes},
    url={https://www.kaggle.com/datasets/nehaprabhavalkar/indian-memes},
    author={Neha Prabhavalkar},
    year={2021}
}

@inproceedings{MultiOFF,
    title = "Multimodal Meme Dataset ({M}ulti{OFF}) for Identifying Offensive Content in Image and Text",
    author = "Suryawanshi, Shardul  and
      Chakravarthi, Bharathi Raja  and
      Arcan, Mihael  and
      Buitelaar, Paul",
    editor = "Kumar, Ritesh  and
      Ojha, Atul Kr.  and
      Lahiri, Bornini  and
      Zampieri, Marcos  and
      Malmasi, Shervin  and
      Murdock, Vanessa  and
      Kadar, Daniel",
    booktitle = "Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/2020.trac-1.6/",
    pages = "32--41",
    language = "eng",
    ISBN = "979-10-95546-56-6",
    abstract = "A meme is a form of media that spreads an idea or emotion across the internet. As posting meme has become a new form of communication of the web, due to the multimodal nature of memes, postings of hateful memes or related events like trolling, cyberbullying are increasing day by day. Hate speech, offensive content and aggression content detection have been extensively explored in a single modality such as text or image. However, combining two modalities to detect offensive content is still a developing area. Memes make it even more challenging since they express humour and sarcasm in an implicit way, because of which the meme may not be offensive if we only consider the text or the image. Therefore, it is necessary to combine both modalities to identify whether a given meme is offensive or not. Since there was no publicly available dataset for multimodal offensive meme content detection, we leveraged the memes related to the 2016 U.S. presidential election and created the MultiOFF multimodal meme dataset for offensive content detection dataset. We subsequently developed a classifier for this task using the MultiOFF dataset. We use an early fusion technique to combine the image and text modality and compare it with a text- and an image-only baseline to investigate its effectiveness. Our results show improvements in terms of Precision, Recall, and F-Score. The code and dataset for this paper is published in \textit{ \url{https://github.com/bharathichezhiyan/Multimodal-Meme-Classification-Identifying-Offensive-Content-in-Image-and-Text} }"
}

@misc{memes_classified_and_labelled,
    title={memes classified and labelled},
    url={https://www.kaggle.com/datasets/gmorinan/memes-classified-and-labelled},
    author={gmor},
    year={2020}
}

@misc{6992_memes,
    title={6992 Meme Images Dataset with Labels},
    url={https://www.kaggle.com/datasets/hammadjavaid/6992-labeled-meme-images-dataset},
    author={Hammad Javaid},
    year={2023}
}

@misc{r/memes_dataset,
    title={r/memes dataset},
    url={https://www.kaggle.com/datasets/nikitricky/memes},
    author={NikiTricky},
    year={2023}
}

@inproceedings{memecap,
    title = "{M}eme{C}ap: A Dataset for Captioning and Interpreting Memes",
    author = "Hwang, EunJeong and Shwartz, Vered",
    editor = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.89/",
    doi = "10.18653/v1/2023.emnlp-main.89",
    pages = "1433--1445",
    abstract = "Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We present the task of meme captioning and release a new dataset, MemeCap. Our dataset contains 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors. Despite the recent success of vision and language (VL) models on tasks such as image captioning and visual question answering, our extensive experiments using state-of-the-art VL models show that they still struggle with visual metaphors, and perform substantially worse than humans."
    }


@misc{bawankar_reddit,
    title={Reddit Memes and Comments(Image and Text)},
    url={https://www.kaggle.com/datasets/lucifyme/reddit-memes-comments},
    author={Vipul Bawankar},
    year={2023}
}

@misc{socialstudies.textbook,
    title={@socialstudies.textbook},
    author={@socialstudies.textbook},
    url={https://www.instagram.com/socialstudies.textbook/},
}

@misc{socialstudies_workbook,
    title={@socialstudies\_workbook},
    author={@socialstudies\_workbook},
    url={https://www.instagram.com/socialstudies\_workbook/},
}

@misc{bukittimahpoly,
    title={@bukittimahpoly},
    author={@bukittimahpoly},
    url={https://www.instagram.com/bukittimahpoly/},
}

@misc{doverpoly,
    title={@dover\_poly},
    author={@dover\_poly},
    url={https://www.instagram.com/dover\_poly/},
}

@misc{childrenholdingguns,
    title={@childrenholdingguns},
    author={@childrenholdingguns},
    url={https://www.instagram.com/childrenholdingguns/},
}

@misc{diaozuihotline,
    title={@diaozuihotline},
    author={@diaozuihotline},
    url={https://www.instagram.com/diaozuihotline/},
}

@misc{memedefsg,
    title={@memedefsg},
    author={@memedefsg},
    url={https://www.instagram.com/memedefsg/},
}

@misc{rafflesplacemrt,
    title={@rafflesplacemrt},
    author={@rafflesplacemrt},
    url={https://www.instagram.com/rafflesplacemrt/},
}

@misc{sgagsg,
    title={@sgagsg},
    author={@sgagsg},
    url={https://www.instagram.com/sgagsg/},
}

@misc{tkk.jc,
    title={@tkk.jc},
    author={@tkk.jc},
    url={https://www.instagram.com/tkk.jc/},
}

@misc{yourgirlfriendiswhosia,
    title={@yourgirlfriendiswhosia},
    author={@yourgirlfriendiswhosia},
    url={https://www.instagram.com/yourgirlfriendiswhosia/},
}

@misc{SUTDmemes,
    title={A Better World By Memes (SUTDMemes)},
    url={https://www.facebook.com/SUTDmemes},
    author={SUTDmemes}
}

@misc{filip_tronicek_reddit_memes,
    title={Reddit memes},
    url={https://www.kaggle.com/datasets/filiptronicek/reddit-memes},
    author={Filip Tronicek},
    year={2021}
}

@misc{thakkinapalli_memes_classification,
    title={Memes Classification Dataset},
    url={https://www.kaggle.com/datasets/vineethakkinapalli/memes-classification-dataset},
    author={Vineeth Thakkinapalli},
    year={2022}
}

@misc{shinde_memes_images,
    title={Memes Images: OCR data},
    url={https://www.kaggle.com/datasets/yogesh239/text-data-ocr},
    author={Yogesh Shinde},
    year={2024}
}

@misc{harsh_singh_reddit_memes,
    title={RedditMemes},
    url={https://www.kaggle.com/datasets/tooharsh/redditmemes},
    author={Harsh Singh},
    year={2021}
}

@misc{jafer_covid_reddit_memes,
    title={Coronavirus Memes : Reddit},
    url={https://www.kaggle.com/datasets/syedjaferk/coronavirus-memes-reddit},
    author={Syed Jafer},
    year={2022}
}

@inproceedings{kiela2021hateful,
 author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2611--2624},
 publisher = {Curran Associates, Inc.},
 title = {The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{hatefulmemes_finegrained,
      title={WOAH Shared Task Fine Grained Hateful Memes Classification}, 
      author={Shaoliang Nie and Aida Davani and Lambert Mathias and Douwe Kiela and Zeerak Waseem and Bertie Vidgen and Vinodkumar Prabhakaran},
      url={https://github.com/facebookresearch/fine_grained_hateful_memes},
      year={2021},
}

@inproceedings{HatReD,
  title     = {Decoding the Underlying Meaning of Multimodal Hateful Memes},
  author    = {Hee, Ming Shan and Chong, Wen-Haw and Lee, Roy Ka-Wei},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {5995--6003},
  year      = {2023},
  month     = {8},
  note      = {AI for Good},
  doi       = {10.24963/ijcai.2023/665},
  url       = {https://doi.org/10.24963/ijcai.2023/665},
}


@misc{sgabbreviations,
    author = "{Wikipedia contributors}",
    title = "List of Singapore abbreviations --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2024",
    url = "https://en.wikipedia.org/w/index.php?title=List_of_Singapore_abbreviations&oldid=1219708681",
    note = "[Online; accessed 9-Mar-2024]"
}

@misc{instaloader,
    title={instaloader},
    url={https://github.com/instaloader/instaloader},
    author={Alexander Graf},
    year={2024},
}

@misc{yamlvsjson,
    title={google-gemini-benchmarks},
    url={https://github.com/lizozom/google-gemini-benchmarks},
    author={Liza Katz},
    year={2024},
}

@misc{sea_lion_2024,
  title={SEA-LION (Southeast Asian Languages In One Network): A Family of Large Language Models for Southeast Asia},
  author={AISingapore},
  year={2024},
  howpublished={\url{https://github.com/aisingapore/sealion}}
}

@misc{lionguard,
      title={LionGuard: Building a Contextualized Moderation Classifier to Tackle Localized Unsafe Content}, 
      author={Jessica Foo and Shaun Khoo},
      year={2024},
      eprint={2407.10995},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10995}, 
}

@misc{clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744v1},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.03744v1}, 
}

@inproceedings{liu2023llava,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@misc{qwen2vl,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971},
}


@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825},
}

@misc{qwen2lm,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685},
}

@misc{lora_plus,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}

@misc{dora,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@misc{rslora,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}

@misc{pissa,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02948}, 
}

@misc{ppocrv3,
      title={PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight OCR System}, 
      author={Chenxia Li and Weiwei Liu and Ruoyu Guo and Xiaoting Yin and Kaitao Jiang and Yongkun Du and Yuning Du and Lingfeng Zhu and Baohua Lai and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
      year={2022},
      eprint={2206.03001},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.03001}, 
}

@misc{pplcnet,
      title={PP-LCNet: A Lightweight CPU Convolutional Neural Network}, 
      author={Cheng Cui and Tingquan Gao and Shengyu Wei and Yuning Du and Ruoyu Guo and Shuilong Dong and Bin Lu and Ying Zhou and Xueying Lv and Qiwen Liu and Xiaoguang Hu and Dianhai Yu and Yanjun Ma},
      year={2021},
      eprint={2109.15099},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2109.15099}, 
}

@misc{seamlessm4tv2,
      title={Seamless: Multilingual Expressive and Streaming Speech Translation}, 
      author={Seamless Communication and Loïc Barrault and Yu-An Chung and Mariano Coria Meglioli and David Dale and Ning Dong and Mark Duppenthaler and Paul-Ambroise Duquenne and Brian Ellis and Hady Elsahar and Justin Haaheim and John Hoffman and Min-Jae Hwang and Hirofumi Inaguma and Christopher Klaiber and Ilia Kulikov and Pengwei Li and Daniel Licht and Jean Maillard and Ruslan Mavlyutov and Alice Rakotoarison and Kaushik Ram Sadagopan and Abinesh Ramakrishnan and Tuan Tran and Guillaume Wenzek and Yilin Yang and Ethan Ye and Ivan Evtimov and Pierre Fernandez and Cynthia Gao and Prangthip Hansanti and Elahe Kalbassi and Amanda Kallet and Artyom Kozhevnikov and Gabriel Mejia Gonzalez and Robin San Roman and Christophe Touret and Corinne Wong and Carleigh Wood and Bokai Yu and Pierre Andrews and Can Balioglu and Peng-Jen Chen and Marta R. Costa-jussà and Maha Elbayad and Hongyu Gong and Francisco Guzmán and Kevin Heffernan and Somya Jain and Justine Kao and Ann Lee and Xutai Ma and Alex Mourachko and Benjamin Peloquin and Juan Pino and Sravya Popuri and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Anna Sun and Paden Tomasello and Changhan Wang and Jeff Wang and Skyler Wang and Mary Williamson},
      year={2023},
      eprint={2312.05187},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.05187}, 
}