\section{Related work}
Vision-Language Models (VLMs) have emerged as powerful tools for understanding and generating connections between visual and textual information. Pioneering works like CLIP____ demonstrated the effectiveness of contrastive learning in creating aligned visual-semantic embeddings, while more recent models such as LLaVA____ have expanded these capabilities to include sophisticated visual reasoning and open-ended dialogue. Modern VLMs can perform a wide range of tasks, including image captioning, visual question answering, and cross-modal retrieval without task-specific fine-tuning. The success of these models stems from their ability to learn generalizable visual-semantic representations through self-supervised learning on vast amounts of image-text data, typically utilizing web-scale datasets containing billions of image-text pairs. This approach has proven particularly effective at capturing nuanced relationships between visual content and natural language descriptions, enabling zero-shot transfer to novel tasks and domains.

\subsection{Have existing VLMs seen Remote Sensing imagery?} \label{sec:seen_rs}
While VLMs excel at general visual and textual understanding, their training on massive, web-scraped datasets like LAION-5B raises concerns about their proficiency in specialized domains such as RS____. Recent studies employing rigorous filtering techniques on these large-scale datasets have provided compelling evidence that VLMs have indeed encountered RS images, but this exposure is incidental, unsystematic, and ultimately superficial. This limited and unfocused engagement with RS data highlights a crucial gap in the capabilities of current VLMs.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{images/LAION_EO_samples.png}
\caption{Representative samples from the LAION-EO____ dataset illustrate a fundamental limitation inherent in web-scraped image-text paired datasets for remote sensing: despite images exhibit sufficient visual fidelity, the accompanying textual descriptions are characterized by high noise levels and lack domain-specific details, diminishing their utility for Earth Observation tasks.}
\label{fig:laion_samples}
\end{figure*}

Several research efforts have focused on extracting RS subsets from web-crawled large-scale general-purpose datasets, extensively used within the computer vision domain. As an example, Czerkawski and Francis developed LAION-EO____, leveraging CloudSEN12____ as an "anchor dataset" of Sentinel-2 images to retrieve similar images from LAION-5B. Their multi-stage filtering process, involving nearest neighbor search and refined CLIP-based similarity thresholds, yielded 24k samples in the prototype version and an expanded set of 113k in version 1. Similarly, Zhang et al.____ identified LAION-RS, a subset of 726k RS images, i.e. a mere 0.03\% of LAION-2B, using a binary classifier. Zhang et. al.____ introduced RS5M, a dataset of 5 million RS image-text pairs. A core component of RS5M, the PUB11 subset containing over 3 million pairs, was created by filtering 11 large-scale public datasets using keyword-based filtering, de-duplication, and a VLM-based filter. The 11 filtered large-scale public datasets, include LAION2B-en____, LAION400M____, LAIONCOCO____, COYO700M____, CC3M____, CC12M____, YFCC15M____, WIT____, Redcaps____, SBU____, and Visual Genome____. These filtering efforts offer critical insights into the current state of VLMs. First, the relatively small size of the extracted subsets, such as the minuscule 0.03\% of LAION-2B forming LAION-RS, demonstrates the limited and incidental exposure of VLMs to RS data. Second, as shown in Fig.~\ref{fig:laion_samples}, web-scraped datasets like LAION-EO present a significant challenge: while the extracted imagery is generally of sufficient quality and resolution, the associated captions are inadequate for RS image analysis. They are dominated by generic attributes and lack the detailed, domain-specific descriptions, thus fail to describe land cover types, specific geographical features, or other essential elements that are crucial in RS downstream tasks.

These findings demonstrate that the current generation of VLMs, trained on vast yet generic datasets, possesses an inadequate representation of the RS domain, resulting in shallow and insufficient understanding of RS imagery. Their incidental and unsystematic exposure has resulted in a limited ability to capture the rich semantic details and domain-specific knowledge required for effective RS image analysis. Therefore, the development of dedicated high-quality RS image-text datasets, coupled with specialized model architectures, plays a pivotal role in advancing the field, paving the way for more sophisticated and impactful applications.

\subsection{Remote Sensing Image-Text Paired Datasets}
The increasing availability of RS images, coupled with the remarkable advancements in deep learning (DL), have generated unprecedented opportunities for automated satellite scene understanding, object detection, and change analysis at a global scale. However, the success of these data-intensive DL models hinges critically on the existence of large-scale, high-quality training datasets. While the computer vision community has witnessed the creation of massive image-text paired datasets like LAION-5B, the RS domain lags behind, primarily due to the inherent complexities and unique challenges associated with annotating RS imagery. Creating such datasets for RS applications (see Table~\ref{tab:datasets}) is a formidable task, requiring domain expertise, meticulous manual effort, and substantial resources to accurately describe the complex spatial relationships, diverse land cover types, and varying scales present in aerial and satellite imagery.

Early efforts in associating RS imagery with textual descriptions were pioneered by____ introducing two novel datasets towards semantic understanding of high-resolution RS images: the UCM-Captions dataset and the Sydney-Captions dataset. The UCM-Captions dataset, derived from the UC Merced Land Use dataset____, contains 2,100 aerial images with spatial resolution of 0.3m across 21 land-use classes, sourced from the USGS National Map Urban Area Imagery. The Sydney-Captions dataset features 613 images with spatial resolution of 0.5m across 7 land-use classes, sourced from a Google Earth. Both datasets contain five manually crafted captions for each image, describing the scene's semantic content, including objects, attributes, and their relationships. However, while the captions for each image vary in phrasing, descriptions for images within the same class show limited diversity, often focusing on similar aspects and sentence structures, revealing a limitation in capturing the diverse perspectives of human observers.

Building upon the aforementioned foundational datasets, Lu et al.____ introduced the RSICD dataset, significantly expanding the scale and scope of RS image-text paired datasets available at the time. RSICD contains 10,921 aerial images featuring varying spatial resolutions, collected from multiple sources including Google Earth, Baidu Map, MapABC, and Tianditu, with the objective of achieving high intra-class variance and low inter-class similarity. The images were initially annotated with 24,333 descriptive sentences, with captions per image varying from 1 to 5 and subsequently extended them to 54,605 sentences by duplicating randomly the existing sentences when there are not five different sentences to describe the same image. While RSICD represented a significant advancement in terms of size and diversity compared to UCM-Captions and Sydney-Captions, it still relied on manual annotation, which limited its scalability and required substantial human effort.

To address the need for fine-grained understanding of RS imagery, Yuan et al.____ developed the RSITMD (Remote Sensing Image-Text Match) dataset specifically designed for cross-modal RS image retrieval. RSITMD comprises of 4,743 images across 32 categories, sourced from the RSICD dataset and Google Earth. Uniquely, RSITMD emphasizes fine-grained relationships between image attributes. Multiple annotators provided five descriptive captions per image, following guidelines to document these fine-grained relationships, alongside one to five supplementary object-level keywords. Authors demonstrated that RSITMD's captions are more diverse than previous datasets, with a diversity score of 4.60, significantly higher than Sydney (1.83), UCM (0.97), and RSICD (1.67), capturing nuanced relationships between objects within images. 

Further expanding the scale and diversity of RS image captioning datasets, the NWPU-Captions dataset, introduced by Lu et al.____, became the largest dataset for this task at the time. It contains 31,500 images with spatial resolution ranging from 30 to 0.2 meters, covering 45 classes based on the NWPU-RESISC45 dataset____. Each image was manually annotated with five descriptive captions by experienced RS experts, summing up to 157,500 image-caption pairs.

The TextRS dataset, introduced by Abdullah et al.____, focused on text-to-image matching tasks within the RS domain. TextRS contains approximately 2,144 images, each associated with five descriptive sentences summing up to 10,720 image-caption pairs. The images were randomly selected from four well-established scene classification datasets: AID____, UC Merced____, PatternNet____, and NWPU____, covering in total 134 overlapping categories. The annotation process involved five individuals generating descriptive sentences for each image, following specific guidelines to ensure diversity, accuracy, and relevance. While TextRS addressed limitations of previous datasets by incorporating diverse imagery, its reliance on manual annotation highlighted the ongoing need for automated or semi-automated approaches.

The RSVGD dataset, introduced by Li et al.____, attempted to address the annotation bottleneck by proposing an automatic image-query generation method, although it still required manual verification to ensure correctness. The dataset was sampled from the DIOR____ dataset, originally designed for object detection, featuring 20 object categories. Authors annotation pipeline comprised of four steps: (1) box sampling, (2) attribute extraction, (3) expression generation, and 4) manual verification. The constructed dataset contains 17,402 RS images with spatial resolution ranging from 0.5m to 30m and 38,320 image-caption pairs, where each object instance in the RS image corresponds to a unique caption. The average caption length is 7.47 words and the vocabulary size is 100 words.

Bountos et al.____ introduced Hephaestus, a dataset designed for InSAR image analysis, offering a rich collection of manually annotated Sentinel-1 interferograms specifically designed for diverse computer vision tasks, including image captioning. The dataset has been manually annotated by a team of InSAR experts and contains 19,919 individual interferograms over 44 volcanoes across the globe, with each interferogram paired with a single, yet detailed caption. These captions provide comprehensive textual descriptions of each interferogram, including observed ground deformation, atmospheric contributions, and overall image quality. The InSAR images were acquired from the Comet-LiCS____ portal, an automated system that processes Sentinel-1 data to generate interferograms.

Marking a significant leap towards fully automated image-caption dataset creation, the RSTeller dataset____, provided approximately 1.2 million image patches, each paired with multiple captions, resulting in more than 2.5 million RS image-text pairs, with a ground sampling distance of 0.6 meters. The imagery was sourced exclusively from the National Agriculture Imagery Program (NAIP) via the Google Earth Engine platform. The temporal coverage spans from August 1, 2021, to November 26, 2022, with all imagery captured via aircraft over the United States. RSTeller employed an automated workflow consisting of four main processes: (1) raw data acquisition from Google Earth Engine and OpenStreetMap (OSM), (2) raw caption generation using the Mixtral-7B large language model, (3) caption augmentation for linguistic diversity, and (4) quality control. The caption lengths varied from 5 to 192 tokens, with a median length of 48 tokens and an average of 54.2 tokens per caption. While RSTeller demonstrated the feasibility of large-scale automated dataset creation, its focus on NAIP imagery limited its diversity in terms of geographic coverage and image sources.

The RS5M____ dataset, represented a major advancement in scale, providing a staggering 5 million image-text pairs. Its creation employed a two-stage methodology. First, a comprehensive filtering process was applied to 11 publicly available image-text datasets from the broader computer vision domain. This filtering stage, resulting in the PUB11 subset, involved text-based keyword filtering, de-duplication, and a dual-phase filtering process leveraging a pre-trained VLM to exclude images unrelated to RS. Second, the RS3 subset was constructed from existing class-labeled RS datasets, namely BigEarthNet-v1____, FMoW____, and MillionAID____. Descriptive captions were generated for these datasets using a pre-trained BLIP2____ model, and their quality was enhanced using a ranking system based on CLIP models and a novel rotational-invariance criterion. Finally, relevant meta-information, such as class labels, UTM coordinates, and UTC timestamps, were integrated into the captions, where available. The resulting RS5M dataset boasted a diverse range of image types, including both satellite and aerial perspectives, paired with descriptive English language captions often further enhanced with geographic, temporal, and scene-specific meta-information.

SkyScript____, addressed the need for semantic richness by comprising 2.6 million image-text pairs with an impressive 29 thousand unique tags derived from OpenStreetMap (OSM). These tags detailed object categories, subcategories, and fine-grained attributes, providing a level of detail not found in previous datasets. SkyScript's imagery was sourced from Google Earth Engine (GEE) collections, with spatial resolutions spanning 0.1m to 30m per pixel. The dataset's global coverage favored regions with abundant high-resolution imagery and comprehensive OSM annotations, such as the U.S. and Europe. An automated annotation pipeline connected images to OSM data, employing a two-stage object selection for diverse representation, image selection based on object-specific criteria, and a two-stage tag classification using CLIP embeddings to ensure visual relevance and appropriate ground sampling distance. Rule-based caption generation then synthesized OSM tags into descriptive captions, while a final filtering step, based on CLIP-derived image-text similarity, refined the dataset's quality. While SkyScript represented a significant advancement in terms of semantic richness and automated annotation, its coverage bias towards regions with rich OSM data presented a limitation.

LuojiaHOG____, focused on RS image-text retrieval, offering a dataset of 94,856 images sourced from Google Earth, covering diverse regions globally. A key feature of LuojiaHOG was its hierarchical classification system aligned with Open Geospatial Consortium (OGC) standards, featuring 7 first-level, 21 second-level, and 131 third-level categories, allowing for extensibility and compatibility with various data under different requirements. The dataset employed a hybrid annotation approach, combining manual and automatic processes. Manual annotation involved expert correction of OpenStreetMap (OSM) labels and the creation of detailed image descriptions following specific guidelines. Automatic annotation utilized Minigpt4____ with prompt engineering to generate a large volume of image descriptions, with quality control measures implemented. Statistical analysis revealed an average caption length of 123.56 words and 6.95 sentences per caption. While LuojiaHOG's hierarchical structure and detailed annotations are valuable, the hybrid annotation approach still involved significant manual effort and limited its scalability.

Hu et al.____ introduced RSI-CAP and RSIEval, two interconnected datasets designed for training and evaluating vision-language models in RS. RSICap consists of 2,585 high-quality, human-annotated captions. The dataset included imagery derived from the DOTA____ object detection dataset, featuring different satellite and aerial sensors such as GF-2, JL-1 and Google Earth, encompassing both color and panchromatic images with a wide range of spatial resolutions. The annotation process followed specific guidelines, including describing image attributes, object attributes, and overall scene, as well as including reasonable speculations based on visual content. RSICap captions have an average length of 60 words. RSIEval, on the other hand, was designed for benchmarking VLMs on RS image captioning (RSIC) and visual question answering (RSVQA) tasks. It contains 100 high-quality image-caption pairs (following RSICap annotation guidelines) and 936 diverse image-question-answer triplets, averaging nine questions per image. The questions covered four categories: object-related, image-related, scene-related, and reasoning-related. While RSI-CAP and RSIEval provided valuable resources for training and evaluation, their reliance on manual annotation limited their scalability.

ChatEarthNet____, utilized ChatGPT-3.5 and ChatGPT-4V to generate 163,488 and 10,000 image-text pairs from Sentinel-2 imagery, respectively. The imagery featured global coverage and diverse temporal distribution across all continents except Antarctica, including nine spectral bands. Semantic information was derived from the ESA WorldCover project, which provides land cover maps at 10-meter resolution with 11 classes. ChatEarthNet features detailed natural language descriptions for each image, guided by prompts that incorporated land cover proportions and spatial distributions, offering a mean of 90 words per caption for ChatGPT-4V and 155 for ChatGPT-3.5. While ChatEarthNet demonstrated the potential of large language models for automated caption generation, its reliance on specific data sources (Sentinel-2 and ESA WorldCover) still presented limitations in terms of diversity.

Muhtar et al.____ introduced two new datasets: LHRS-Align and LHRS-Instruct. LHRS-Align is a large-scale dataset of 1.15 million RS image-caption pairs. Images, sourced from Google Earth at 1m resolution, were paired with geographic features from OpenStreetMap (OSM). The dataset's annotation pipeline involved geo-aligning images with OSM features, pruning irrelevant attributes, and generating captions using Vicuna-v1.5-13B. LHRS-Instruct is a multi-modal instruction-following dataset for RS image understanding, including diverse tasks and complex visual reasoning data generated by GPT-4. The dataset was created by forming instruction datasets from public RS caption datasets (RSITMD____ and NWPU____) through data cleaning and conversation generation with Vicuna-v1.5-13B, and selecting 15,000 LHRS-Align images, calculating bounding boxes from OSM data, and using GPT-4 to create complex instruction data. While LHRS datasets offer a high degree of automation and addressed diverse tasks, the reliance on specific language models and the manual construction of the benchmark dataset highlighted ongoing challenges.

Finally, Liu et al.____ introduced Git-10M, a large-scale RS image-text dataset containing 10 million pairs, sourced by 30\% from public datasets (Million-AID____, GeoPile____, SSL4EO-S12____, SkySript____) and 70\% from Google Earth imagery. The dataset spans diverse geographical regions globally and includes various spatial resolutions ranging from 0.5m to 128m/pixel and geospatial metadata. They authors employ an automated annotation pipeline using GPT-4o, with prompts incorporating included geospatial metadata to generate semantically accurate descriptions.

In conclusion, the evolution of image-text paired datasets in RS has been marked by a progression towards larger scales, increased diversity, and greater automation. Early datasets like UCM-Captions and Sydney-Captions laid the groundwork for RS image captioning but suffered from limited diversity and relied heavily on manual annotation. Subsequent datasets like RSICD, RSITMD, and NWPU-Captions expanded the scale and introduced more fine-grained descriptions, but manual annotation remained a significant bottleneck. The emergence of datasets like RSVGD, RSTeller, RS5M, SkyScript, LuojiaHOG and Git-10M represented a shift towards semi-automated and fully automated annotation approaches, leveraging techniques like language models, rule-based systems, and filtering based on visual-language models. However, challenges persist in terms of scalability, the labor and expertise required for annotation (even in semi-automated approaches), and the need for datasets that are both diverse and specific to the unique characteristics of RS imagery. Addressing these challenges will require continued research into novel annotation methodologies, the development of more sophisticated vision-language models tailored for RS, and the creation of comprehensive benchmarks that capture the full spectrum of RS tasks.


\begin{table*}[ht]
\centering
\caption{Summary of Remote Sensing Image-Text Paired Datasets}
\label{tab:datasets}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{l|c|c|c|>{\centering\arraybackslash}p{1.5cm}|p{3.6cm}|c}
\toprule
\textbf{Dataset} & \textbf{Year} & \textbf{Images} & \textbf{Image-Text Pairs} & \textbf{Resolution (m/pixel)} & \textbf{Source(s)} & \textbf{Annotation Method} \\
\midrule
UCM-Captions____ & 2016 & 2,100 & 10,500 & 0.3 & USGS National Map Urban Area Imagery & Manual \\
\midrule
Sydney-Captions____ & 2016 & 613 & 3,065 & 0.5 & Google Earth & Manual \\
\midrule
RSICD____ & 2017 & 10,921 & 54,605 & Variable & Google Earth, Baidu Map,\newline MapABC, Tianditu & Manual \\
\midrule
TextRS____ & 2020 & 2,144 & 10,720 & Variable & AID, UC Merced, PatternNet, NWPU & Manual \\
\midrule
RSITMD____ & 2022 & 4,743 & 23,715 & Variable & RSICD, Google Earth & Manual \\
\midrule
NWPU-Captions____ & 2022 & 31,500 & 157,500 & 30 -- 0.2 & NWPU-RESISC45 & Manual \\
\midrule
RSVGD____ & 2023 & 17,402 & 38,320 & 0.5 -- 30 & DIOR & Hybrid \\
\midrule
Hephaestus____ & 2023 & 19,919 & 19,919 & 100 & Comet-LiCS & Manual \\
\midrule
RSI-CAP____ & 2023 & 2,585 & 2,585 & Variable & DOTA, GF-2, JL-1, Google Earth & Manual \\
\midrule
RSTeller____ & 2024 & 1.2M & 2.5M & 0.6 & NAIP via Google Earth Engine & Automatic \\
\midrule
RS5M____ & 2024 & 5M & 5M & Variable & \textbf{PUB11 filtered datasets}:\newline LAION-400M, YFCC100M\newline CC3M, CC12M\newline RedCaps, WIT\newline TextCaps, VizWiz\newline COCO, SBU\newline Conceptual Captions\newline\newline \textbf{RS3 datasets}:\newline BigEarthNet-v1\newline FMoW\newline MillionAID & Automatic \\
\midrule
SkyScript____ & 2024 & 2.6M & 2.6M & 0.1 -- 30 & Google Earth Engine & Automatic \\
\midrule
LuojiaHOG____ & 2024 & 94,856 & 94,856 & Variable & Google Earth & Hybrid \\
\midrule
ChatEarthNet____ & 2024 & 173,488 & 173,488 & 10 & Sentinel-2, ESA WorldCover & Automatic \\
\midrule
LHRS-Align____ & 2024 & 1.15M & 1.15M & 1 & Google Earth, OSM & Automatic \\
\midrule
Git-10M____ & 2025 & 10M & 10M & 0.5 -- 128 & 30\% public datasets (Million-AID, GeoPile, SSL4EO-S12, SkyScript), 70\% Google Earth & Automatic \\
\midrule
\midrule
GAIA (ours) & 2025 & 41,030 & 205,150 & Variable & Several Remote Sensing-related websites & Automatic \\
\bottomrule
\end{tabular}
}
\end{table*}


\subsection{The imperative need for better annotations}
The performance and efficacy of VLMs are critically dependent on two key factors: the sophistication of their architectural design and the quality, diversity, and scale of the training data. OpenAIâ€™s CLIP____, for instance, revolutionized the field with its unprecedented ability to bridge vision and language modalities, setting a new benchmark for multi-modal learning. However, while the model weights were made publicly available, the specific dataset used for training remains undisclosed, leaving a gap in the understanding of the data-driven foundations of its success. LAION-AI with OpenCLIP____, the open-source implementation of OpenAI's CLIP____, has demonstrated impressive results. They managed to replicate OpenAI's proprietary pre-training dataset____ and subsequently trained and published several models____, using various architectures, on a variety of data sources and compute budgets, ranging from small to large-scale experiments. Recent advancements in the context of CLIP pre-training have showcased remarkable achievements along the axes of pre-train data refinement____, model architecture____ and computational efficiency____, leading to substantial improvements and eventually establishing new standards within the era of pre-trained CLIP models.

Despite the appeal of web-scraped image-text data as a cost-effective resource for encoding general knowledge, their inherent noise and the finite nature of the internet archive itself pose significant limitations. Recognizing that data quality often supersedes sheer quantity____, recent works have been focusing on refining these datasets through advanced filtering and annotation improvement techniques. This pursuit has been significantly accelerated by the advent of foundation models, which have enabled a new wave of innovative approaches to filter and enhance existing datasets.
 
Early endeavors concentrated on the expansion of datasets through the incorporation of synthetic captions. A pioneering effort in this direction was the creation of LAION-COCO____, a dataset encompassing 600 million synthetic captions derived from the LAION2B-en dataset. This work underscored the viability of employing synthetic data to augment the breadth and heterogeneity of image-text datasets.

Subsequent investigations have explored a variety of techniques aimed at refining and augmenting existing datasets. One prominent approach involves improving the caliber of captions through the utilization of sophisticated language models. For instance, Fan et al.____ proposed a method to improve the quality of captions by leveraging language rewrites. They used a large language model, specifically GPT-3, to rewrite existing captions in the Conceptual Captions and SBU Captions datasets, focusing on improving the descriptiveness and accuracy of the text. This method was shown to improve the performance of CLIP models on downstream tasks, demonstrating the importance of high-quality captions for contrastive language-image pre-training.

Concurrently, Nguyen et al.____ focused on refining existing captions in multimodal datasets to improve the training of multimodal models. They utilized BLIP-2, a state-of-the-art vision-language model, to generate synthetic captions, and found that incorporating these captions during training led to significant performance gains in downstream tasks, further highlighting the importance of caption quality.

Another notable contribution in this area is the ShareGPT4V system____, which aimed to enhance the performance of large multi-modal models through high-quality image captions. This approach involved fine-tuning a large language model on a carefully curated dataset of image-caption pairs, demonstrating the benefits of meticulously curated datasets for improving the performance of large-scale models. The study empirically showed that replacing just 3.5\% of standard supervised fine-tuning data with their high-quality captions led to significant performance improvements in existing models like LLaVA-1.5 and Qwen-VL-Chat.

In a paradigm shift, Yu et al.____ introduced CapsFusion, a novel approach that rethinks image-text data at scale. CapsFusion leverages an ensemble of captioning models, including BLIP, BLIP-2, and GIT, to generate a diverse set of captions for each image. These captions are then fused using a large language model, specifically Vicuna-7B, to create a more comprehensive description. This method addresses the limitations of single-model captioning and demonstrates the advantages of leveraging multiple models to enhance caption quality.

Xu et al.____ investigated the potential of alt-text as a valuable source of image descriptions. Their work, altogether, introduced a method for re-aligning alt-text with images, thereby leveraging this often underutilized information source. They employed a multi-stage training process involving human annotation and a specifically designed captioning model to enhance the alignment between alt-text and image content. This method was shown to improve image captioning performance, highlighting the value of alt-text in providing rich visual descriptions.

The application of synthetic data was further explored in specialized domains, such as medical vision-language pre-training. Liu et al.____ investigated the feasibility of training effective medical vision-language models using purely synthetic data. They created a synthetic dataset of chest X-ray images and reports using generative models and demonstrated that models trained on this synthetic data could achieve comparable performance to those trained on real data, thus addressing data scarcity and privacy concerns inherent in medical datasets.

Rotstein et al.____ presented FuseCap, a system that leverages large language models to generate enriched, fused image captions. FuseCap employs frozen vision encoders, including an object detector, an attribute recognizer, and an OCR model, to extract detailed visual information from images. These outputs are then fused with original captions using a fine-tuned Flan-T5 model, resulting in comprehensive image descriptions. They curated a dataset of 12 million image-enriched caption pairs and demonstrated that training a BLIP-based captioning model on this data led to improved performance on downstream tasks.

Advancing the use of synthetic data, Sharifzadeh et al.____ introduced a method that boosts visual-language models by utilizing synthetic captions and image embeddings. This approach employs a text-to-image model (MUSE) to generate synthetic image embeddings from captions generated by a large language model (LLaMA-3). They demonstrated that fine-tuning a VLM on this synthetic data achieves comparable performance to models trained on real data, highlighting the efficacy of combining different types of synthetic data.

Finally, Li et al.____ undertook a large-scale experiment to investigate the capabilities of state-of-the-art language models in generating high-quality captions. They employed a LLaMA-3-powered LLaVA model to recaption 1.3 billion web images from the DataComp-1B dataset. Their results demonstrated significant improvements in the quality of the generated captions compared to the original web-crawled captions, highlighting the potential of advanced language models in large-scale data augmentation.

Despite these advancements, specialized fields such as medical imaging and remote sensing continue to face challenges due to the complex, domain-specific knowledge required for data curation, and the significantly lower tolerance for errors. In these high-stake domains, inaccuracies can have profound consequences. To this end, the medical domain has addressed this challenge by leveraging biomedical image-caption pairs, meticulously collected and filtered from various sources including open-access research publications ____, medical reports ____, and even social media platforms ____, as a trustworthy source of paired image-text data. In contrast, the RS domain has been lately limited to publicly available annotation like land cover maps (e.g. ESA's WorldCover) in conjunction with open-access RS imagery and large language models (LLMs) to generate grounded image-text pairs.

The research trajectory aimed at refining annotated datasets has dramatically progressed, yet challenges persist within specialized domains, particularly given their strict requirements for accuracy. Given the powerful capabilities of current foundation models, it is imperative for the RS community to explore and harness more effective priors. Drawing inspiration from the medical domain's approach, the focus should shift towards leveraging these priors, and with the aid of the current ever-evolving foundation models, restructuring this information to develop more robust and adaptable in-domain vision-language models.