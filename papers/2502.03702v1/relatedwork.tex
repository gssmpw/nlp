\section{Related Works}
Since the publication of an innovative paper~\cite{knowledge}, the knowledge compilation map has been extended when new knowledge representation languages appear~\cite{kcmap_extension1,darwiche_2014}.

The knowledge compilation map has contributed to many varied applications and has also been extended beyond Boolean functions~\cite{FargierMN13,choi2020probabilistic}. However, tensor decomposition-based methods have not been evaluated with these maps.
Knowledge compilation maps, which play an important role in understanding knowledge representation languages, have fueled the emergence of several important applications of knowledge compilation techniques~\cite{CHAVIRA2008772,NEURIPS2018_dc5d637e,pmlr-v80-xu18h}.
An approach similar to tensor decomposition is expected to contribute to numerous applications.

Tensor decomposition~\cite{kolda09} is a method to represent a large-scale high-dimensional tensor as the sum of the products of smaller or low-dimensional tensors.
High-dimensional tensors are frequently used in research areas such as machine learning, numerical calculations, and quantum physics. Because an $m$-dimensional tensor has an exponential number of elements with respect to $m$, tensor decomposition methods are indispensable for addressing high-dimensional tensors.
Canonical Polyadic (CP) decomposition~\cite{carroll70} and Tucker decomposition~\cite{tucker66} are the initial examples; other variants have been proposed, including hierarchical Tucker decomposition~\cite{hackbusck09}, which is more succinct than CP decomposition~\cite{khrulkov2018expressive}.
However, these decompositions do not place much emphasis on operations.
As a tensor decomposition that supports several operations, a tensor train~\cite{oseledets2011tensor} was proposed, which decomposes an $m$-dimensional tensor into the product of $m$ 3-dimensional tensors.
Tensor trains were originally invented in the quantum physics community as matrix product states~\cite{fannes92}. Several tensor operations for tensor trains were developed in a previous study whose runtimes are polynomial with the decomposition size\cite{oseledets2011tensor}.

Two other studies bridge Boolean function representations and tensor decompositions: Tensor Decision Diagrams \cite{hong2022tensor} and {Quantum Multiple-valued Decision Diagrams} (QMDD) \cite{niemann2015qmdds}.
Both represent tensors as decision graphs, similar to OBDDs.
However, these studies addressed the representation of real-valued tensors instead of Boolean functions.
Although a method for model counting using tensor decomposition~\cite{dudek2020parallel} exists, this method does not introduce tensor decomposition into the knowledge compilation or analyze its succinctness or tractability.

Our study focuses on the use of tensor decomposition methods as representations of Boolean functions and analyzes their succinctness and tractable Boolean function operations.
To the best of our knowledge, no similar studies have been previously conducted.