@article{CHAVIRA2008772,
title = {On probabilistic inference by weighted model counting},
journal = {Artificial Intelligence},
volume = {172},
number = {6-7},
pages = {772-799},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001889},
author = {Mark Chavira and Adnan Darwiche},
keywords = {Bayesian networks, Exact inference, Weighted model counting, Compilation},
abstract = {A recent and effective approach to probabilistic inference calls for reducing the problem to one of weighted model counting (WMC) on a propositional knowledge base. Specifically, the approach calls for encoding the probabilistic model, typically a Bayesian network, as a propositional knowledge base in conjunctive normal form (CNF) with weights associated to each model according to the network parameters. Given this CNF, computing the probability of some evidence becomes a matter of summing the weights of all CNF models consistent with the evidence. A number of variations on this approach have appeared in the literature recently, that vary across three orthogonal dimensions. The first dimension concerns the specific encoding used to convert a Bayesian network into a CNF. The second dimensions relates to whether weighted model counting is performed using a search algorithm on the CNF, or by compiling the CNF into a structure that renders WMC a polytime operation in the size of the compiled structure. The third dimension deals with the specific properties of network parameters (local structure) which are captured in the CNF encoding. In this paper, we discuss recent work in this area across the above three dimensions, and demonstrate empirically its practical importance in significantly expanding the reach of exact probabilistic inference. We restrict our discussion to exact inference and model counting, even though other proposals have been extended for approximate inference and approximate model counting.}
}

@inproceedings{FargierMN13,
author = {H{\'{e}}l{\`{e}}ne Fargier and
Pierre Marquis and
Alexandre Niveau},
title= {Towards a knowledge compilation map for heterogeneous representation languages},
booktitle= {In Proceedings of the 23rd International Joint Conference on Artificial Intelligence},
pages= {877--883},
year = {2013},
url= {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6969},
timestamp= {Tue, 08 Mar 2022 17:42:00 +0100},
biburl = {https://dblp.org/rec/conf/ijcai/FargierMN13.bib},
bibsource= {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2018_dc5d637e,
author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
booktitle = {Proceedings of Advances in Neural Information Processing Systems 31 (NeurIPS 2018)},
pages = {},
title = {{DeepProbLog}: neural probabilistic logic programming},
url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/dc5d637ed5e62c36ecb73b654b05ba2a-Paper.pdf},
year = {2018}
}

@article{carroll70,
author={J. D. Carroll and J.-J. Chang},
title={Analysis of individual differences in multidimensional scaling via an n-way generalization of ``{Eckart}-{Young}'' decomposition},
journal={Psychometrika},
volume={35},
pages={283--319},
year={1970}}

@misc{choi2020probabilistic,
  author    = {Choi, YooJung and Vergari, Antonio and Van den Broeck, Guy},
  title     = {Probabilistic circuits: a unifying framework for tractable probabilistic models},
  month     = {oct},
  year      = {2020},
  url       = "http://starai.cs.ucla.edu/papers/ProbCirc20.pdf",
  note  = "Available at: http://starai.cs.ucla.edu/papers/ProbCirc20.pdf"
}

@inbook{darwiche_2014, place={Cambridge}, title={Tractable Knowledge Representation Formalisms}, DOI={10.1017/CBO9781139177801.006}, booktitle={Tractability: Practical Approaches to Hard Problems}, publisher={Cambridge University Press}, author={Darwiche, Adnan}, 
year={2014}, pages={141â€“172}}

@misc{dudek2020parallel,
  title={Parallel weighted model counting with tensor networks},
  author={Dudek, Jeffrey M and Vardi, Moshe Y},
  eprint={2006.15512},
  archivePrefix={arXiv},
  year={2020}
}

@article{fannes92,
author={M. Fannes and B. Nachtergaele and R. F. Werner},
title={Finitely correlated states on quantum spin chains},
journal={Communications in Mathematical Physics},
volume={144},
pages={443--490},
year={1992}}

@article{hackbusck09,
author={W. Hackbusch and S. K\"{u}hn},
title={A new scheme for the tensor representation},
journal={Journal of Fourier Analysis and Applications},
volume={15},
pages={706--722},
year={2009}}

@article{hong2022tensor,
  title={A tensor network based decision diagram for representation of quantum circuits},
  author={Hong, X. and Zhou, X. and Li, S. and Feng, Y. and Ying, M.},
  journal={ACM Transactions on Design Automation of Electronic Systems (TODAES)},
  volume={27},
  number={6},
  pages={1--30},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{kcmap_extension1,
author = {Fargier, H\'{e}l\`{e}ne and Marquis, Pierre},
title = {Extending the knowledge compilation map: Krom, Horn, affine and beyond},
year = {2008},
abstract = {We extend the knowledge compilation map introduced by Darwiche and Marquis with three influential propositional fragments, the Krom CNF one (also known as the bijunctive fragment), the Hom CNF fragment and the affine fragment (also known as the biconditional fragment) as well as seven additional languages based on them, and composed respectively of Krom or Hom CNF formulas, renamable Hom CNF formulas, disjunctions of Krom CNF formulas, disjunctions of Hom CNF formulas, disjunctions of Krom or Hom CNF formulas, disjunctions of renamable Hom CNF formulas, and disjunction of fine formulas. Each fragment is evaluated w.r.t. several criteria, including the complexity of basic quaries and transformation, and its spatial efficiency is also analysed.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence},
pages = {442--447},
volume = {1},
location = {Chicago, Illinois}
}

@inproceedings{khrulkov2018expressive,
  title={Expressive power of recurrent neural networks},
  author={Khrulkov, V and Novikov, A and Oseledets, I},
  booktitle={In Proceedings of the 6th International Conference on Learning Representations},
  year={2018}
}

@article{knowledge,
author = {Darwiche, A. and Marquis, P.},
title = {A knowledge compilation map},
year = {2002},
issue_date = {July 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {17},
number = {1},
issn = {1076-9757},
abstract = {We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages.},
journal = {Journal of Artificial Intelligence Research},
month = {sep},
pages = {229--264},
numpages = {36}
}

@article{kolda09,
author={T. G. Kolda and B. W. Bader},
title={Tensor decompositions and applications},
journal={SIAM Review},
volume={51},
number={3},
pages={455--500},
year={2009}}

@article{niemann2015qmdds,
  title={{QMDD}s: efficient quantum function representation and manipulation},
  author={Niemann, Philipp and Wille, Robert and Miller, David Michael and Thornton, Mitchell A and Drechsler, Rolf},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={35},
  number={1},
  pages={86--99},
  year={2016},
  publisher={IEEE}
}

@article{oseledets2011tensor,
author = {Oseledets, I. V.},
title = {Tensor-train decomposition},
journal = {SIAM Journal on Scientific Computing},
volume = {33},
number = {5},
pages = {2295--2317},
year = {2011},
doi = {10.1137/090752286},
URL = { 
        https://doi.org/10.1137/090752286
},
eprint = { 
        https://doi.org/10.1137/090752286
}
,
    abstract = { A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator. }
}

@InProceedings{pmlr-v80-xu18h,
title = 	 {A semantic loss function for deep learning with symbolic knowledge},
author = {Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Van den Broeck, Guy},
booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
pages = 	 {5502--5511},
year = 	 {2018},
volume = 	 {80},
series = 	 {PMLR},
month = 	 {10--15 Jul},
pdf = 	 {http://proceedings.mlr.press/v80/xu18h/xu18h.pdf},
url = 	 {https://proceedings.mlr.press/v80/xu18h.html},
abstract = 	 {This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.}
}

@article{tucker66,
author={L. R. Tucker},
title={Some mathematical notes on three-mode factor analysis},
journal={Psychometrika},
volume={31},
pages={279--311},
year={1966}}

