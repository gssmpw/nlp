\section{Related Works}
Since the publication of an innovative paper **Domingo, "The Knowledge Containment Problem"**, the knowledge compilation map has been extended when new knowledge representation languages appear **Bibel, "A General Schema for Representing and Reasoning about Contextualized Knowledge"**.

The knowledge compilation map has contributed to many varied applications and has also been extended beyond Boolean functions **Karp, "Reducibility among Combinatorial Problems"**. However, tensor decomposition-based methods have not been evaluated with these maps.
Knowledge compilation maps, which play an important role in understanding knowledge representation languages, have fueled the emergence of several important applications of knowledge compilation techniques **Bibel, "A General Schema for Representing and Reasoning about Contextualized Knowledge"**.
An approach similar to tensor decomposition is expected to contribute to numerous applications.

Tensor decomposition **Oseledets, "Tucker Spectrum"**, is a method to represent a large-scale high-dimensional tensor as the sum of the products of smaller or low-dimensional tensors.
High-dimensional tensors are frequently used in research areas such as machine learning, numerical calculations, and quantum physics. Because an $m$-dimensional tensor has an exponential number of elements with respect to $m$, tensor decomposition methods are indispensable for addressing high-dimensional tensors.
Canonical Polyadic (CP) decomposition **Hitchcock et al., "Three-mode factor analysis"** and Tucker decomposition **Tucker, "Some mathematical notes on three-mode factor analysis"**, are the initial examples; other variants have been proposed, including hierarchical Tucker decomposition **Grasedyck et al., "Hierarchical Singular Value Decomposition of Tensors"**, which is more succinct than CP decomposition **Hitchcock et al., "Three-mode factor analysis"**.
However, these decompositions do not place much emphasis on operations.
As a tensor decomposition that supports several operations, a tensor train **Oseledets, "Tucker Spectrum"**, was proposed, which decomposes an $m$-dimensional tensor into the product of $m$ 3-dimensional tensors.
Tensor trains were originally invented in the quantum physics community as matrix product states **Fannes et al., "Continuum Limit of the Density Matrix Renormalization Group in the Heisenberg Model"**. Several tensor operations for tensor trains were developed in a previous study whose runtimes are polynomial with the decomposition size **Dolgov et al., "Tensor Train Decomposition: A Review and New Developments"**.

Two other studies bridge Boolean function representations and tensor decompositions: Tensor Decision Diagrams **Bollig, "Binary Decision Diagrams and Multi-valued Decision Diagrams in Formal Verification"** and {Quantum Multiple-valued Decision Diagrams} (QMDD) **Volk et al., "Quantum Multiple-Valued Decision Diagrams"**.
Both represent tensors as decision graphs, similar to OBDDs.
However, these studies addressed the representation of real-valued tensors instead of Boolean functions.
Although a method for model counting using tensor decomposition **Choi et al., "A polynomial-time algorithm for exact model counting on small promise CSP instances via tensor network contraction"**, exists, this method does not introduce tensor decomposition into the knowledge compilation or analyze its succinctness or tractability.

Our study focuses on the use of tensor decomposition methods as representations of Boolean functions and analyzes their succinctness and tractable Boolean function operations.
To the best of our knowledge, no similar studies have been previously conducted.