% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{colortbl}
\usepackage{tikz,pgfplots}
\usepackage{tcolorbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{silence}
\usepackage{fontawesome5}
\usepackage{MnSymbol}


\WarningFilter{latex}{`!h' float specifier changed to `!ht'}
\newcommand{\eunki}[1]{\textcolor{cyan}{#1}}
\pgfplotsset{compat=1.18}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Can LVLMs and Automatic Metrics Capture Underlying Preferences of Blind and Low-Vision Individuals for Navigational Aid?}


\author{Na Min An\thanks{\space{ } Equal contribution}~~ Eunki Kim\footnotemark[1]~~ Wan Ju Kang~~ Sangryul Kim~~\\ \textbf{Hyunjung Shim}\thanks{\space{ } Corresponding author}~~ \textbf{James Thorne}\footnotemark[2]
\\
 KAIST AI
\\
 \{naminan, eunkikim, soarhigh, sangryul, kateshim, thorne\}@kaist.ac.kr
}


\begin{document}
\maketitle

\begin{abstract}
Vision is a primary means of how humans perceive the environment, but Blind and Low-Vision (BLV) people need assistance understanding their surroundings, especially in unfamiliar environments. The emergence of semantic-based systems as assistance tools for BLV users has motivated many researchers to explore responses from Large Vision-Language Models (LVLMs). However, it has yet been studied preferences of BLV users on diverse types/styles of responses from LVLMs, specifically for navigational aid. To fill this gap, we first construct \textsc{Eye4B} dataset, consisting of human-validated 1.1k curated outdoor/indoor scenes with 5-10 relevant requests per scene. Then, we conduct an in-depth user study with eight BLV users to evaluate their preferences on six LVLMs from five perspectives: Afraidness, Nonactionability, Sufficiency, and Conciseness. Finally, we introduce \textsc{Eye4B} benchmark for evaluating alignment between widely used model-based image-text metrics and our collected BLV preferences. Our work can be set as a guideline for developing BLV-aware LVLMs towards a Barrier-Free AI system.  
\end{abstract}


\section{Introduction}

While \textit{vision} is a primary sensory modality for humans to perceive their environment for mobility or navigation \cite{gallace2009cognitive,hutmacher2019there}, \textit{language} is an essential medium for approximately 200 million Blind and Low-Vision (BLV) people worldwide \cite{brady2013visual,real2019navigation,yuan2024walkvlm,prajapati2024survey}. Although BLV users resort to other mediums, such as canes and guide dogs, there is an increasing demand for assistive AI technologies that can provide language-based descriptions \cite{bandukda2019understanding, kazemi2023recognizing, kuriakose2023exploring, chidiac2024accessibility}.

Whereas traditional detection-based AI systems help BLV users to detect and avoid specific obstacles/objects \cite{liu2023open,wang2024visiongpt,zheng2024materobot}, they often fall short in providing the \textit{deeper context} needed to navigate environments effectively. For instance, the user might want to not only be aware of surrounding obstacles, but they might also want to be provided with \textit{context-aware natural language-based description} that conveys spatial information of objects or landmarks and other directional cues for navigation. To address this need, systems such as Be My Eyes \cite{be_my_eyes}, Aira \cite{aira_about_us}, SeeingAI \cite{seeing_ai}, and Sullivan A \cite{sullivan_pro_app} utilize human support or generative models to describe the scenes \cite{yuan2024walkvlm, huh2023genassist, zhao2024vialm, yang2024viassist, de2024vqask, wang2024visiongpt, waisberg2024meta}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{Figures/Figure0.pdf}
    \vspace*{-4mm}
    \caption{Our \textsc{Eye4B} benchmark evaluates (1) the preference of BLV users on various LVLM responses based on the image-request pair and (2) how much model-based metrics align with BLV preferences.}
    \label{fig:sample_data}
\end{figure}


Despite improvements in Large Visual Language Models (LVLMs), BLV navigation using LVLM-based systems remains challenging to be directly applied for practice use \cite{yuan2024walkvlm}, integrated with external devices (\textit{e.g.}, GPS, touch sensor) \cite{shadi2019outdoor, prajapati2024survey}. To build reliable LVLMs for BLV individuals, the intuitive step is collecting extensive training data aligned with BLV preferences. However, due to the cost and fatigue of human experiments, it is challenging to collect large-scale data verified by BLV subjects. An alternative way is having an automatic BLV-preference-aligned metric \cite{kapur-kreiss-2024-reference} that can be used as a criterion. Yet, there is a limited number of works exploring the preferences of unconventional groups of users, such as BLV, on different LVLM responses and whether existing widely used automatic metrics are sufficient in capturing the BLV preferences. Hence, our motivations are:


\textbf{\textit{Motivation 1: Necessity of unveiling BLV preferences on context-aware descriptions}}. Unlike past works that test a single LVLM response for the navigation or object finding task \cite{liu2024objectfinder}, we focus on exploring \textit{what types/styles of responses from different LVLMs that BLV users prefer}. Since each LVLM produces uniquely styled responses, we employ five LVLMs shown to be capable of in-context learning \cite{zong2025vlicl} and GPT-4o mini \cite{gpt-4omini} for response generation. To systematically analyze BLV preferences, we evaluate multiple in-context learnable LVLMs in five aspects spanning Afraidness, Nonactionability, Sufficiency, Conciseness, and Overall. Our analysis reveals the implicit preferences of BLV users on LVLMs, providing insight for designing real-time dynamic interactive systems \cite{krishna2017visual, bandukda2019understanding}.


\textbf{\textit{Motivation 2: Comprehensive benchmarking automatic metrics on BLV-aware datasets}}. Many metric-related works generally investigate how much CLIP/BLIP-based metrics align with human preferences on image-text pairs, assessing which image or text aligns better with the given reference text or image \cite{hessel2021clipscore, sarto2024positive, wada2024polos, an2024i0t, xu2024imagereward}. While Context-Aware CLIP \cite{kreiss2022context} and IIT-DAS \cite{zur2024updating} are metrics built in the perspectives of the BLV individuals, no prior work has been conducted on the model-based metric assessment of BLV preferences due to the lack of fine-grained BLV preference datasets. Our \textsc{Eye4B} benchmark fills this gap by introducing a novel evaluation dimension to a standard human correlation benchmark designed to test model-based metrics based on BLV user preferences.


Our study advances BLV accessibility research by presenting the \textsc{Eye4B} benchmark from model perspective (Section~\ref{method:metric_analysis}) using our \textsc{Eye4B} dataset containing image-request-response (Sections~\ref{method:eye4b_dataset}) and BLV preferences (Section~\ref{method:eye4b_benchmark}).



\section{Related Works}

\subsection{BLV-Aware Datasets}

Prior datasets related to BLV individuals, such as VizWiz \cite{gurari2018vizwiz, gurari2019vizwiz, tseng2022vizwiz, bafghi2023new} and BIV-Priv-Seg \cite{tseng2024biv} collect images taken from BLV users. While this approach provides valuable insights, these images are often low quality \cite{bigham2010vizwiz}, degrading test performance in models \cite{chiu2020assessing, olson2021towards}. We build upon high-quality existing outdoor sidewalk \cite{park2020sideguide, aihub_dataset_189}, and indoor scene datasets \cite{aihub_dataset_189}. Although \citealp{xia2023dataset, tang2023dataset} are relevant, we mainly focus on collecting datasets with images taken in South Korea due to the BLV recruitment challenges.

Compared to detection-based AI systems, focusing on vision-centric tasks like object detection \cite{park2020sideguide, xia2023dataset, tang2023dataset}, semantic segmentation \cite{park2020sideguide}, depth estimation \cite{park2020sideguide}, or surface masking \cite{aihub_dataset_189}, there are limited number of semantic-based systems \cite{yuan2024walkvlm}. Our \textsc{Eye4B} dataset contributes to the collection of semantic-based datasets by extending the previous datasets with additional metadata of possible BLV user requests. However, while semantic-based datasets for BLV individuals \cite{yuan2024walkvlm, gurari2019vizwiz, yang2024viassist} deal with visual question-answering tasks, where responses are either `correct' or `incorrect,' our dataset differs by collecting fine-grained preferences of BLV users.

\subsection{LVLMs as BLV Assistance}

Large language models (LLMs) have expanded their capabilities beyond natural language to multiple modalities, bringing significant advancements in LVLMs \cite{dong2024internlmxcomposer2masteringfreeformtextimage, awadalla2023openflamingoopensourceframeworktraining, wang2024qwen2vlenhancingvisionlanguagemodels, llama3_2, gpt-4omini}. LVLMs, enhanced with in-context learning with prompting methods \cite{zong2025vlicl,jin2022goodpromptworthmillions,wu2022generativevisualpromptunifying, dong2024internlmxcomposer2masteringfreeformtextimage}, demonstrate applicability to be integrated with applications for BLV users \cite{liu2024objectfinderopenvocabularyassistiveinteractive}. Be My Eyes \cite{be_my_eyes} is the first BLV-aware application in collaboration with OpenAI \cite{achiam2023gpt}, and \citealp{zhang2024design} introduces an interface for BLV users to access object information using LVLMs. The most related work, WalkVLM, is designed to support the BLV user navigation in cities such as Beijing \cite{yuan2024walkvlm}.




\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Figure1.pdf}
    \caption{The overall framework of \textsc{Eye4B} dataset construction (Section \ref{method:eye4b_dataset}) and benchmarking LVLMs with BLV user preferences (Section \ref{method:eye4b_benchmark}). We first collect and verify requests for the images of outdoor and indoor visual scenes. Second, we generate responses of LVLMs on the validated image-request pairs. Thirdly, we benchmark these responses with the assessment from the BLV users. Finally, we re-generate LVLM responses by incorporating feedback from the BLV users.}
    \label{fig:dataset_gen_framework}
\end{figure*}






\section{\textsc{Eye4B} Dataset Construction}\label{method:eye4b_dataset}

Before extracting BLV preferences from LVLM responses and exploring how much model-based metrics align with BLV preferences, we first curate description-based BLV-aware datasets. We first collect a number of visual scenes (corresponding to BLV viewpoints) and possible requests. This stage is illustrated as step 1 in Figure~\ref{fig:dataset_gen_framework}, and more details can be found in Appendix~\ref{app:data}.

\subsection{Visual Scene Collection and Filtering} 

We curate a diverse collection of indoor and outdoor visual scenes from the existing SideGuide \cite{park2020sideguide} and SideWalk \cite{aihub_dataset_189} datasets. Since SideWalk is a video-based dataset, we randomly sample one picture from each video. We repeat the process of randomly sampling and manually filtering until we reach more than 250 images from 350k and 100k images with bounding box annotations and polygon masks from SideGuide, and 1.2k outdoor and 296 indoor scenes from SideWalk. Specifically, we only leave images that (1) are not excessively blurry and too dark that sighted humans cannot perceive, (2) contain at least more than five detectable objects, and (3) are taken from perspectives of humans on the sidewalk, not in car vehicles, discarding many pictures showing highways from SideWalk. The two authors iterate through four rounds of this filtering process until both agree on using the images, resulting in approximately 1,150 visual scenes that include 300 images of bounding boxes and polygon masks independently, and 271 outdoor and 281 indoor images.

\subsection{Request Generation and Preprocessing}

We generate a list of possible requests for filtered visual scenes using GPT-4o mini \cite{gpt-4omini}, and validate the correctness of generated requests with sighted human participants, similar to \citealp{merchant2024generating}. Specifically, we prompt GPT with 3-shot examples, requesting the model to generate 5 to 10 requests or requests that are related to the navigation of BLV users (more details in Appendix~\ref{app:human}). All the generated requests are then reviewed by 24 sighted human annotators. The reason why we have used sighted annotators is to check the \textit{relevancy} of the request on the visual scene images to ensure their validity in terms of two perspectives: (1) quality itself and (2) relevance with the corresponding visual scene images. For each image-request pair, each annotator decides whether each request (out of 5-10 generated requests) is valid. For example, the annotator has to exclude requests that are not aligned with navigational goals, such as "Explore the bushes" and "Check the shelf." If they answer more than three no's, the annotator proposes their requests. All the image-request pairs are evaluated with two annotators, reaching the agreement level of 62.33\% and 68.97\% ratio of yes/no per annotator. This process results in 4,979 requests with an average of 4.32 (STD: 1.42) requests per image.

\subsection{LVLM Response Generation}

The final step of \textsc{Eye4B} data construction is to generate responses for the 4,979 image-request pairs. We use five open-source LVLMs, including LLaVA-1.6 \cite{liu2023visualinstructiontuning}, Qwen-VL/Qwen-VL-chat \cite{wang2024qwen2vlenhancingvisionlanguagemodels}, InterNLM-x2 \cite{dong2024internlmxcomposer2masteringfreeformtextimage}, OpenFlamingo \cite{awadalla2023openflamingoopensourceframeworktraining} that are known to exhibit in-context learning ability \cite{zong2025vlicl} and one close-source model, GPT-4o mini \cite{gpt-4omini}. We first generate responses with a 3-shot prompting method on open-source 7B models. Then, the responses generated by 7B models are refined using GPT-4o mini based on the assumption that large models could improve the accuracy of responses, but later, we find that even the refined responses sometimes still include hallucinations, such as non-actionable actions (\textit{e.g.}, look up) or inaccurate directions. Since our goal is to collect the preferences of BLV users over diverse styles/structures of responses generated by different models, we consider all these six models in preparation for the BLV user study.


\section{BLV Preferences on LVLM Responses}\label{method:eye4b_benchmark}

In this section, we investigate the underlying preferences on our constructed \textsc{Eye4B} dataset. We conduct two rounds of experiments to explore the preferences of BLV users on LVLM responses with different styles. The second round is performed based on feedback from the first round. The details regarding the human experiment participants and protocols are explained in Appendix~\ref{app:human}.

\subsection{Study Objective and Design}

To evaluate various model responses with BLV users, we ask six BLV participants (Table~\ref{app:personal_blv}) to assess two types of generated responses - one from 7B models \cite{zong2025vlicl} and the other from GPT-4o mini \cite{gpt-4omini}. To ensure the interview quality, we manually discard any low-quality responses and provide only responses aligned with paired images and requests. This is because we observe that even responses of GPT-4o mini often contain hallucinated objects and inaccurate spatial cues (e.g., direction and depth) despite detailed instructions. Hence, our experiment focuses on finding the BLV preferences over different LVLM-generated responses. Based on previous works related to mobility tasks for BLV individuals \cite{liu2024objectfinderopenvocabularyassistiveinteractive, kreiss2022context}, we collect evaluation scores in terms of five perspectives: 1) Afraidness, 2) Nonactionability, 3) Sufficiency, 4) Conciseness, and 5) Overall for each response as illustrated in step c of Figure~\ref{fig:dataset_gen_framework}. It should be noted that the lower scores indicate better ratings for the first two categories and vice versa for the last three categories.

To ensure the safe navigation of BLV users, we set Afraidness and Nonactionability as core indicators of the benchmark. Given that the responses serve as actionable guidelines for navigation, Sufficiency, and Conciseness are chosen to capture the balance of informativeness and brevity. Finally, the Overall category reflects the user’s general satisfaction. The collected BLV preferences extend our \textsc{Eye4B} dataset (from Section~\ref{method:eye4b_dataset}) to be further used for \textsc{Eye4B} benchmark in Section~\ref{method:metric_analysis}.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Figure2.pdf}
    \caption{Analysis of the BLV user evaluation scores benchmarking generated responses from six LVLMs. (\textbf{a}) Likert-scale score distribution ($\smallstar$: average) from BLV users across five criteria/categories. (\textbf{b}) Distribution of agreement status. (\textbf{c}) Correlation between all combinations of criteria. (\textbf{d}) Preference score across six LVLMs. (\textbf{e}) Preference percentage of 7Bs \textit{vs}. GPT-4o mini for each participant.}
    \label{fig:blv_analysis}
\end{figure*}

\subsection{The First Round of BLV Experiment}

To ensure that each BLV user justifies their scoring instead of randomly scoring and to allow them to become accustomed to the experiments, our user study consists of two stages, inspired by the sectional analysis framework developed by \citealp{Fereday2006DemonstratingRU}: (1) an interview procedure where each user needs to justify their assessments on every sub-question and (2) an annotation process where each user only needs to assess, without commenting justifications. We evaluate 10 image-request pairs (one from 7B models and the other from GPT-4o mini) for the first and 10 to 13 for the second stages for each annotator. 

\paragraph{Current LVLMs lack in providing fully satisfactory responses to BLV users.}

Figure~\ref{fig:blv_analysis}\textbf{a} illustrates the score of the five criteria, where we observe low scores for Afraidness and Nonactionability and high scores for Sufficiency, Conciseness, and Overall. Compared to the first two categories, the variances of scores in the Sufficiency and Conciseness are high, indicating that each BLV user has different perspectives on whether the response is \textit{sufficient} or \textit{concise}. Additionally, Figure~\ref{fig:blv_analysis}\textbf{c} shows Pearson's correlations between all combinations of categories. Notably, Afraidness shows a relatively high correlation with Nonactionability, suggesting that the exclusiveness of non-actionable contexts can lessen the Afraidness level. Sufficiency and Nonactionability are the first and second crucial factors in determining the overall scores, showing correlation coefficients of 0.33 and 0.31. While these quantitative analyses give clues to the preferences of BLV users, our interviews with BLV participants offer deeper, qualitative insights. We summarize the results in four points.


\paragraph{(1) Structured Response for Clarity} Several participants emphasized the importance of structured responses such as template formatting. P1 highlighted that \textit{"responses with the format of initial general overview followed by detailed guidance have been particularly helpful for understanding complex requests; however, it would be more helpful if the response has clear, structured templates."}, and P5 similarly argued \textit{"We need consistent formats for the response."} Most BLV users denoted that the inconsistency of the response formats across samples made it difficult to follow some responses, which motivated the authors to adopt a unified template across samples with diverse contexts in the following round of the BLV experiment.

\paragraph{(2) Concrete Directional and Distance Cues} Since our task focuses on actionable and navigational tasks, directional and distance cues are the essential elements. P2 emphasized that \textit{"incorporating concrete directional cues, such as moving to the 1 o'clock position and indicating specific distance metrics, such as steps or meters, are very helpful."} However, while participants largely agreed on the importance of including concrete directional and distance cues for better guidance, preferences varied regarding the terminology used for measurement, such as step-wise \textit{vs}. meter-based instructions. While P2 preferred the more intuitive "steps," P5 preferred the more objective "meters." This feedback underscores the need to integrate the different needs of BLV users since they do not always agree on the preferred response (Figure ~\ref{fig:blv_analysis}\textbf{b}), encouraging us to use either measurement appropriately. Later in our final generation process, we adopt meters for distance measurements in scene descriptions and step-wise instructions for the step-by-step action guidance. We discuss more improvements in providing accurate directional cues for LVLMs in Appendix~\ref{app:dir_acc}. 

\begin{table}[t!]
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|cc|c}
\toprule
\textbf{BLV} & \textbf{Set Type} & \textbf{Preference (B)} & \textbf{Preference (A)} & \textbf{Agreement}                        \\
\midrule 
P1       & \multirow{2}{*}{1}        & 70.83          & 29.17          & \multirow{2}{*}{0.38 (SD:0.21)} \\
P7       &         & 70.83          & 29.17          &                                  \\
\midrule
P2       & \multirow{2}{*}{2}         & 8.33           & 91.67          & \multirow{2}{*}{0.00 (SD: 0.11)} \\
P8       &         & 45.83          & 50.00          &  \\             
\bottomrule
\end{tabular}
}
\end{center}
\caption{Preference percentage (\textbf{B}: before, \textbf{A}: after) for four BLV users (P1 \& P2: old, P7 \& P8: new) distributed with two sets. The agreement value indicates Cohen's Kappa (0.38: fair, 0.00: slight agreement).}
\label{tab:blv_second}
\end{table}


\paragraph{(3) Balance of Conciseness and Sufficiency}
Interestingly, all six participants emphasized the importance of conciseness as a crucial attribute of effective responses. P3 noted that \textit{"long-winded instructions made it harder to follow directions and more concise explanations would be better."} Similarly, P4 stressed the drawbacks of excessive detail, noting that \textit{"overly detailed and redundant instructions are not helpful."} Despite this shared preference for brevity, participants also underscored the importance of sufficiency. P5 synthesized these priorities, stating that \textit{"It is good to keep the sentences concise, but it would be better if they still contain specific detailed content."} As a result, achieving the balance between conciseness and sufficiency remains challenging yet critical for generating responses for BLV users.

\paragraph{(4) Reflecting BLV-Specific Perspectives} Although the seemingly promising quantitative results are shown in Figure~\ref{fig:blv_analysis}\textbf{a}, most participants (5/6) noted that many responses failed to fully reflect the unique needs and experiences of BLV users. Instructions, such as “look at the object” or descriptions involving colors, were deemed irrelevant and, at times, frustrating. P1 argued that \textit{"Responses included many instructions that required visual clues; hence, my fear did not lessen."}

Several BLV users also expressed discomfort with risk assessments that are either too general, such as \textit{"Watch out for cars or pedestrians"}, or failed to address critical safety concerns specific to BLV navigation. This might be the reason why most BLV users prefer smaller models (Figure~\ref{fig:blv_analysis}\textbf{d}\footnote{We allocate either 0 or 1 for the preference score depending on the preference for difference} and~\ref{fig:blv_analysis}\textbf{e}) over GPT-4o mini since GPT tends to contain common safety-related cautions \cite{achiam2023gpt}, which turns out to be unnecessary and redundant information for most BLV individuals. Hence, our attempt to "enhance" responses of smaller models using GPT does not necessarily mean the responses are chosen by the BLV users. 

\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clccccccccc|cc@{}}
\toprule
& & \textbf{PASCAL} & \textbf{FOILR1} & \textbf{FOILR4} & \textbf{FlickrExp} & \textbf{FlickrCF} & \textbf{Polaris} & \textbf{Polaris*} & \textbf{OID} & \textbf{ImgREW} & \cellcolor[HTML]{f5f2ee}\textbf{Eye4B-O} & \cellcolor[HTML]{f5f2ee}\textbf{Eye4B-A} \\ 
\multicolumn{2}{l}{\textit{Automatic Metrics}}& Acc & Acc & Acc & $\tau_{c}$ & $\tau_{b}$ & $\tau_{c}$ & P-Acc & P-Acc & P-Acc & \cellcolor[HTML]{f5f2ee}$\tau_{c}$ & \cellcolor[HTML]{f5f2ee}$\tau_{c}$ \\ 
\midrule 
 \multirow{8}{*}{\rotatebox[origin=c]{90}{CLIP-based}} & CLIP-S & 80.7 & 87.2 & 87.2 & 51.2 & 34.4 & 52.3 & 79.7 & 56.5 & 56.7  & \cellcolor[HTML]{f5f2ee}-2.30 & \cellcolor[HTML]{f5f2ee}3.32\\
 & LongCLIP-S & 82.8 & 91.6 & 91.6 & 54.1 & 35.4 & 54.0 & 77.5 & 58.1 & 56.5 & \cellcolor[HTML]{f5f2ee}2.73 & \cellcolor[HTML]{f5f2ee}1.97 \\
 & PAC-S & 82.4 & 93.7 & 94.9 & \underline{55.9} & \underline{37.6} & 52.5 & 77.0 & 57.7 & 57.2 & \cellcolor[HTML]{f5f2ee}-0.75 & \cellcolor[HTML]{f5f2ee}4.57 \\
 & Ref-free Polos & 81.0 & 88.7 & 88.7 & 51.4 & 34.4 & 52.3 & 60.0 & \textbf{66.2} & 56.6 & \cellcolor[HTML]{f5f2ee} -3.18 & \cellcolor[HTML]{f5f2ee} -4.84 \\
 & CA-CLIPS-S & -  & -  & -  & -  & -  & - & - & - & - & \cellcolor[HTML]{f5f2ee} -5.08 & \cellcolor[HTML]{f5f2ee} -9.69 \\
 & RefCLIP-S & 83.1 & 91.0 & 92.6 & 53.0 & 36.4 & 52.3 & - & -  & - & \cellcolor[HTML]{f5f2ee} -  & \cellcolor[HTML]{f5f2ee} - \\
 & RefPAC-S & \underline{84.7} & 88.7 & \underline{94.9} & \underline{55.9} & \underline{37.6} & \underline{56.0} & - & -  & -  & \cellcolor[HTML]{f5f2ee} - & \cellcolor[HTML]{f5f2ee} - \\
 & Polos & \textbf{86.5} & 93.3 & 95.4 & 56.4 & \textbf{37.8} & \textbf{57.8} & - & -  & - & \cellcolor[HTML]{f5f2ee} - & \cellcolor[HTML]{f5f2ee} - \\
\midrule 
 \multirow{5}{*}{\rotatebox[origin=c]{90}{BLIP-based}} & BLIP-S & 82.5 & \textbf{95.1} & \textbf{95.1} & \textbf{57.1} & \textbf{37.8} & 54.0 & 79.5 & \underline{59.3} & 57.8 & \cellcolor[HTML]{f5f2ee} \underline{10.08} & \cellcolor[HTML]{f5f2ee} \underline{15.48} \\
& \cellcolor[HTML]{f5f2ee} TxtBLIP-S &  \cellcolor[HTML]{f5f2ee} 82.4 &  \cellcolor[HTML]{f5f2ee} \textbf{95.1} &  \cellcolor[HTML]{f5f2ee} \textbf{95.1}  & \cellcolor[HTML]{f5f2ee} 52.3  & \cellcolor[HTML]{f5f2ee}  35.9   &  \cellcolor[HTML]{f5f2ee} 54.0 &  \cellcolor[HTML]{f5f2ee} \underline{84.8} & \cellcolor[HTML]{f5f2ee} 58.5 &  \cellcolor[HTML]{f5f2ee} 57.8 & \cellcolor[HTML]{f5f2ee} 9.40 & \cellcolor[HTML]{f5f2ee} \textbf{18.07}  \\
& ImgREW-S & 81.5 & 93.8 & 93.8 & 49.8 & 36.2 & 52.3 & 73.3 & 58.5 & \textbf{65.2} & \cellcolor[HTML]{f5f2ee} -3.06 & \cellcolor[HTML]{f5f2ee} 2.54 \\
& \cellcolor[HTML]{f5f2ee} ImgTxtREW-S &  \cellcolor[HTML]{f5f2ee} 80.9  &  \cellcolor[HTML]{f5f2ee} 90.3 &  \cellcolor[HTML]{f5f2ee} 90.3  & \cellcolor[HTML]{f5f2ee} 49.8  & \cellcolor[HTML]{f5f2ee}  35.7   &  \cellcolor[HTML]{f5f2ee} 55.0 &  \cellcolor[HTML]{f5f2ee} \textbf{93.0}   & \cellcolor[HTML]{f5f2ee} 57.7  &  \cellcolor[HTML]{f5f2ee} \underline{64.6}  & \cellcolor[HTML]{f5f2ee} -2.96  & \cellcolor[HTML]{f5f2ee} 1.93  \\
& \cellcolor[HTML]{f5f2ee} Eye4B-S  &  \cellcolor[HTML]{f5f2ee} 82.3 &  \cellcolor[HTML]{f5f2ee} \underline{95.0} &  \cellcolor[HTML]{f5f2ee} \underline{95.0} & \cellcolor[HTML]{f5f2ee} 51.7  & \cellcolor[HTML]{f5f2ee} 35.8  &  \cellcolor[HTML]{f5f2ee} 53.9  &  \cellcolor[HTML]{f5f2ee} 79.4 & \cellcolor[HTML]{f5f2ee} \underline{59.3}  &  \cellcolor[HTML]{f5f2ee} 57.8  & \cellcolor[HTML]{f5f2ee} \textbf{10.28} & \cellcolor[HTML]{f5f2ee} 15.25 \\
\bottomrule
\end{tabular}
}
\caption{Performances of widely-used automatic model-based evaluation metrics on nine existing datasets and our \textsc{Eye4B} dataset (Section \ref{method:metric_analysis}). While these metrics show relatively high human correlations with human judgments on general image-text pairs, they lack correlations with BLV user judgments in our navigation task.}
\label{tab:metric_results}
\end{table*}


\subsection{The Second Round of BLV Experiment}\label{second_round}


To incorporate the main feedback from the first experiment conducted with BLV users, which is to make the response with \textit{consistent} format and \textit{concise}, we perform another round of the BLV user experiment with a new set of responses generated using different prompts and few-shot samples. We perform the second BLV experiment involving two annotators (P1 \& P2) from the first round and two new annotators (P7 \& P8) to avoid result overfitting to the BLV annotators from the first round. We distribute 48 image-request pairs (one from the preferred responses - \textit{before} and the other from newly generated responses - \textit{after}, all from the 7B models). Different from the first stage, we ask the users to mark only their preferences (either \textit{before} or \textit{after}) and add a brief explanation of the reason for their choice.

\paragraph{Even newly generated responses do not steer BLV users toward unanimous agreement.}
Unlike our expectation, the responses created based on feedback from the BLV users in the first experiment are not always preferred in the subsequent round, as summarized in Table~\ref{tab:blv_second}. Although the new responses are preferred by P2 with 91.56\% out of the 24 questions, the remaining three annotators either show a slightly higher preference for the new contexts than the old contexts (P8) or prefer the old responses (P1 \& P7). We delve into the qualitative reasons from three perspectives:

\paragraph{(1) Trade-off between Detailed and Formatted Responses}
One of the main reasons why participants prefer the response \textbf{B} (before) over \textbf{A} (after) is that the former is explained in more detail (P2 \& P7) using "easy language" (P7 \& P1). However, the formatted responses generated with the new prompt based on feedback from the first round resulted in a lack of fine-grained details. P1 also argues that \textit{"Response \textbf{B} is more detailed than \textbf{A}, but it is difficult to trust because it contains conflicting statements within the description."} In addition, P8 states that \textit{"There is too much unnecessary information in response \textbf{A}."}, which is not illustrated when describing response \textbf{A}. Hence, we conclude that although the response \textbf{B} might be preferred over \textbf{A} for its descriptive property, it can also negatively affect the users.

\paragraph{(2) Preferred Response Format Dependent on Circumstances and BLV Users}
The format of the responses is dependent on how complex the visual scenes and requests are and BLV users. Response \textbf{A} can be characterized by an unnaturally fragmented sequence of step-by-step actions, despite describing a relatively simple environment, such as the scene of a narrow valley with not many obstacles (P8). However, for more challenging requests in intricate scenes (\textit{e.g.}, Enter the building in distance with obstacles in the way), \textbf{A} is preferred over \textbf{B} due to its format advantage, including overall scene description, obstacles, and step-by-step actions (P2, P1, and P3). Although the keyword mentioned in the reasons people select \textbf{A} is "conciseness," aligning the original purpose of testing a new type of response using a different prompt, the level of concisness is dependent on each scenario and user. This underscores the challenges of building BLV-aware LVLMs.











\section{\textsc{Eye4B} Benchmark on Alignment of Metrics and BLV Preferences}\label{method:metric_analysis}

After evaluating LVLMs regarding BLV preferences/judgments on our \textsc{Eye4B} dataset, we present \textsc{Eye4B} benchmark. We investigate \textit{how much various automatic metrics can capture BLV judgments}. Specifically, we evaluate to what extent various pre-trained automatic image-text evaluation metrics that show high correlations with general human judgments on general image captions can correlate with BLV judgments.


\paragraph{Human Preference Datasets} Most automatic metrics show relatively strong correlation performances across general datasets such as PASCAL \cite{xu2019structuredmodelingjointdeep}, FOIL \cite{shekhar-etal-2017-foil}, Flickr-Exp/CF \cite{plummer2015flickr30k}, and Polaris \cite{wada2024polos} that include fine-grained human judgments. To construct an image-to-text dataset\footnote{The ImageREWARD dataset is a text-to-image dataset since there are human ratings for each generated image corresponding to the text pair.} with characteristics more similar to our \textsc{Eye4B} dataset but in a general domain, we construct Polaris* (\textit{i.e.}, Preference-based Polaris) and OID \cite{narins2024validated} by extracting positive or negative texts per image based on the annotated alignment scores within a 0-1 range (scores below 0.5 are categorized as negative, while those 0.5 or higher are labeled as positive). 

Our \textsc{Eye4B} benchmark extends the general benchmark of human judgment by including BLV preference judgments on LVLM responses given the visual scene image and request. Based on the feedback from BLV users, which reveals that selecting a clear preference between responses is often challenging, and due to the limits in the score-annotated test data size ($n=98$), we select Kendall $\tau_{c}$ for the evaluation metric.

\paragraph{Existing metrics are not tuned to align with BLV judgments.} Table~\ref{tab:metric_results} shows that while general-purpose metrics (Appendix~\ref{app:related_works}) demonstrate strong performance on existing datasets consisting of human judgments, they struggle to predict the BLV user preferences, captured by the Kendall $\tau_{c}$s for the \textsc{Eye4B} dataset. For example, while ImgREW-S \cite{xu2024imagereward} performs superior on the first nine datasets, achieving 93.8\% accuracy on FOIL-R1 and 65.2\% preference accuracy on the ImgREW dataset, it does not show higher correlation performances than CLIP-S \cite{hessel2021clipscore} when applied to Eye4B. This is evident in achieving even lower Kendall $\tau_c$s for both overall and average preference scores\footnote{The average preference score is calculated by averaging the 1-5 score for (5-Afraidness Score), (5 - Nonactionability Score), Sufficiency Score, and Conciseness Score.} than the correlations of CLIP-S (-2.30 \& 3.32).

However, the promising metric among these conventional metrics is BLIP-S \cite{li2022blip}, aligning with previous finding \citealp{xu2024imagereward}. To assess whether metrics trained on general alignment datasets can enhance performance on \textsc{Eye4B}, we present TxtBLIP-S and ImgTextREW-S, which are fine-tuned on BLIP-S and ImgREW-S using Polaris \cite{wada2024polos}, a collection of image-text description pairs annotated with human alignment scores (details in Appendix~\ref{app:blip_finetuning}). Since TxtBLIP-S trained with a new preference dataset (constructed using Polaris) shows a lower sensitivity across all datasets, we fine-tune our newly collected Eye4B training dataset on BLIP-S to build Eye4B-S (further discussed in Appendix ~\ref{app:blip_finetuning}). Eye4B-S shows an increase in Kendall $\tau_{c}$ by 0.2 compared to BLIP-S on the Eye4B-O dataset. A slight decrease in the Eye4B-A dataset is due to the characteristics of our Eye4B training dataset, where we only consider the overall score. These results suggest that there is room for developing enhanced metrics that can better align with BLV preferences.






\section{Discussion}

\paragraph{BLV Perspectives on AI Technology}
AI technologies have the potential to broaden the accessibility for BLV individuals. However, current AI-based assistance tools primarily function as simple QA systems, lacking the capability to generate structured guidance essential for BLV navigation. It is also important that AI-based assistance tools adapt to user needs \cite{lee2022opportunities,khan2022mechanism,valipoor2024analysis}. Unlike conventional navigation datasets, often dominated by road-centric images \cite{chen2020touchdownnaturallanguagenavigation, schumann-riezler-2021-generating} or simple QA captions \cite{nishimura-etal-2024-text360nav} irrelevant for pedestrian navigation, our dataset is specifically designed to capture the BLV preferences on natural language-based descriptions for assistance in mobility scenarios. By fully integrating BLV perspectives into dataset construction, we introduce a new paradigm for AI-driven accessibility research, setting a foundation for the LVLMs that generate actionable, contextually relevant guidance rather than listing isolated fact-based descriptions.

\paragraph{Reliability of LVLMs}
Despite the remarkable generative capabilities of LVLMs, their reliability in producing accurate and logically structured navigational instructions remains a critical limitation \cite{li2023evaluatingobjecthallucinationlarge, huang2023factualinconsistencyproblemabstractive}. Based on our interviews with BLV individuals, most participants stated that they could not solely rely on AI technology due to its lack of ability to provide consistently accurate descriptions. Furthermore, these models fail to reflect the BLV preferences, leading to responses that lack real-world applicability. To address these shortcomings, our \textsc{Eye4B} dataset consists of verified requests from human annotators for constructing relevant scenarios on paired images and evaluation of multiple LVLM responses for capturing the implicit preferences of BLV users.


\paragraph{BLV-Aware Metrics}
One of the reasons why current  CLIP/BLIP-based metrics might not be sufficient to incorporate BLV user judgments is that these models are not tuned to capture the instructions. Since each text sample (either request or response) in our dataset usually contains more than 248 tokens, these metrics cannot properly encode the long contexts and catch the subtle differences between BLV-preferred and BLV-non-preferred responses \cite{zhang2025long} unlike the datasets consisting of sighted-human judgments on image-text pairs. Our \textsc{Eye4B} benchmark raises critical questions regarding the adequacy of model-based metrics and underscores the need for developing context-aware automatic evaluation methods tailored to both general and BLV audiences. We leave future work to develop metrics that can capture the instruction-driven nature of BLV-aware datasets.




\section{Conclusion}
This paper addresses the critical need for reliable visual context generation tailored for BLV individuals due to the limitations of current LVLMs in this domain. Our \textsc{Eye4B} benchmark evaluates LVLM performances, addressing a significant gap between automatic evaluation metrics and BLV preferences. This suggests the importance of incorporating BLV users to build barrier-free LVLMs. As a future study, we plan to develop automatic metrics that can better predict both sighted and BLV human preferences that can be used for training barrier-free LVLMs. We believe our work can advance the field toward enhancing AI usability to tailor the needs of BLV individuals.

\section{Limitation}
While our current study focuses mainly on image-based visual scenes, we could extend the work to generating LVLM responses on video-based visual scenes. The limited number of evaluation data sizes and mobility scenes from a single country is due to the difficulty in recruiting a number of international BLV users. Exploring how we can provide LVLM responses, either with one-way or conversational auditory cues, is another challenging part that we leave as future work to deal with.

\section{Ethical Statement}
The proposed dataset contains responses from GPT-4o mini, which could have unintentional potential risks in the initial stage of the data construction process. However, human users have confirmed and validated all the released data. 

\bibliography{custom}



\appendix
\onecolumn
\section{Appendix}
\label{sec:appendix}



\subsection{Dataset Construction Details}\label{app:data}

We clarify that all the datasets we use are open-source and for training/evaluation model purposes, and the corresponding citations are included in the Reference section.

\paragraph{Requests}
We outline the details of the construction of our final 4,979 request datasets. First, we prompt GPT-4o mini (\$0.15/1M input tokens) \cite{gpt-4omini} using the prompt stated in Table~\ref{app:system_prompts_scengen}. We also provide the model with 3 few-shot examples provided in Table~\ref{app:fewshot_prompts_scengen_outdoors} for outdoor and Table~\ref{app:fewshot_prompts_scengen_indoors} for indoor environmental visual scenes. We set the temperature to 0.0, the maximum tokens to 300, and the response format to 'list' for the hyperparameters. This stage results in a total of 8,149 requests for 1,150 images, resulting in 7.09 requests per image. Next, we filter these raw requests using 24 human annotators, further illustrated in Appendix~\ref{app:human}, resulting in 4,979 image-request pairs.

\paragraph{LVLM Responses}
The few-shot samples we use for prompting the five-open source LVLMs \cite{liu2023visualinstructiontuning, wang2024qwen2vlenhancingvisionlanguagemodels, awadalla2023openflamingoopensourceframeworktraining} are in Table~\ref{app:fewshot_prompts_7b_before_outdoors} for outdoor and Table~\ref{app:fewshot_prompts_7b_before_indoors} for indoor visual scenes. After the generation using these 7B models, we prompt GPT-4o mini \cite{gpt-4omini} to enhance the response using the system prompt (before) in Table~\ref{app:system_prompts_7b} and few-shot examples in Table~\ref{app:fewshot_prompts_gpt_outdoors} for outdoor and Table~\ref{app:fewshot_prompts_gpt_indoors} for indoor scenes. Then, a randomly sampled response from one of 7B models and GPT-4o mini construct a pair for use in the BLV user evaluation (Appendix~\ref{app:human}). All the system prompts, few-shot examples, and generated responses for our second prompts described in Section~\ref{second_round} are in Tables~\ref{app:system_prompts_7b}, \ref{app:fewshot_prompts_7b_before_outdoors} and \ref{app:fewshot_prompts_7b_before_indoors}, and \ref{app:qual_deepcontext_samples}.


\subsection{Human Experiment Details}\label{app:human}

\paragraph{Sighted User Experiment for Request Generation}
To filter the requests generated by GPT-4o mini and verify their relevancy with the corresponding images, we recruited 24 sighted human annotators from the school community after the study design had been approved by the Institutional Review Board (IRB). We distributed the instructions (Table~\ref{app:guideline}) to all the annotators who consented to participate in our task and agreed on the 50k KRW compensation ($\sim$ 34 USD in January 2025). Depending on the participant, the human experiment took $<$ 1 to 2 hours (leading time in Figure~\ref{app:hum_dist}).

The number of newly added captions and the proportion of 'yes' selected over all the samples depend on each participant (the second and third plots in Figure~\ref{app:hum_dist}). We make sure that the shorter leading time does not indicate a lower number of new captions but is correlated with the 'yes' ratio (Figure~\ref{app:hum_corr}), showing that most human annotators followed our guideline. The survey is distributed using Label Studio Interface \cite{Label} (sample screenshot in Figure~\ref{app:screenshot_scen_gen}). The resulting valid request proportions per set (a total of 4,265) are illustrated in Figure~\ref{app:valid_requests}.

For the postprocessing, the authors go through two more rounds of the verification process: (1) filtering requests that include only one 'yes' annotation (Figure~\ref{app:screenshot_filter_oneyes}) and (2) filtering captions added by the human annotators (Figure~\ref{app:screenshot_filter_newcap}), where all these stages need to be verified with a consensus of both two authors. 74 (out of 137; 54\%) and 578 (out of 935; \%62) requests are selected, resulting in 652 (out of 1072; 60\%) newly added verified requests. In addition to these 652 requests, the authors additionally go through one more process with 12 images with no requests, resulting in the addition of 62 requests. Thus, the total number of verified requests that form our \textsc{Eye4B} Benchmark is 4,979 ($=$ 4,265 + 652 + 62).

\paragraph{BLV User Experiment for LVLM Response Evaluation}
The first round of the BLV user experiment lasts 1 to 1 hour and 30 minutes for each participant (sample screenshot in Figure~Figure~\ref{app:screenshot_blv}). All eight participants (demographic information listed in Table~\ref{app:personal_blv} are compensated with 50k KRW ($\sim$ 34 USD in January 2025). The lasting hours and the compensation for the second round of the BLV user experiment are the same as the first. The difference between the two rounds is whether the experiment is offline, conducted with an interview, or online using a screen reader. Since we evaluate using the subquestions for each image-request pair in the first round, the averaged score refers to the evaluation score averaged across four categories-Afraidness, Nonactionability, Sufficiency, and Conciseness, except for Overall (Figure~\ref{app:blv_dist}). The overall scores for each participant are illustrated in Figure~\ref{app:blv_overall}.


\subsection{Additional Related Works}\label{app:related_works}

\paragraph{Automatic Image-Text Evaluation Metrics}

Automatic evaluation metrics for image-text pairs can be divided into reference-based and reference-free approaches. The reference-based approaches \cite{sarto2024positive, wada2024polos} require ground-truth texts (\textit{i.e.}, references) corresponding to the images, unlike reference-free metrics \cite{hessel2021clipscore, zhang2025long, sarto2024positive, wada2024polos, an2024i0t, li2022blip, xu2024imagereward}. Reference-based approaches often show better correlations with human judgments than reference-free methods, with the cost of additional annotation of text (or caption) on images. For example, RefCLIP-S \cite{hessel2021clipscore}, RefPAC-S \cite{sarto2023positive}, and Polos \cite{wada2024polos} not only calculate the similarity between image and generated text, but they also utilize the ground-truth text (\textit{i.e.}, reference). 

If references are unavailable or do not exist, reference-free metrics become the only viable options (\textit{e.g.}, CLIP-S \cite{hessel2021clipscore}, PAC-S \cite{sarto2023positive}, and Reference-free version of Polos (without RoBERTa) \cite{wada2024polos}). Since BLIP-based metrics often correlate better with human judgments \cite{xu2024imagereward}, BLIP-based metrics \cite{li2022blip, xu2024imagereward} could also be opted. \citealp{xu2024imagereward} is trained to rank the quality of the generated images as similar to the human ratings. In the case of our study, we postulate that there is no single ground-truth text for BLV users corresponding to the image. 

There are also a few image-text metrics that are tuned to BLV-related tasks. Context-Aware CLIP \cite{kreiss2022context} captures how much the description is related to the context (webpage in this work) and image without a context (\textit{i.e.}, image - context). IIT-DAS \cite{zur2024updating} aims to assign higher scores to image-description than image-caption pairs by fine-tuning CLIP on the Concadia dataset \cite{kreiss2022context}. Similarly to our work, \citealp{zur2024updating} claims that \textit{description} (or response in our case) helps the imaginability of the BLV users more than the caption with the role of complementing the image.


\subsection{Additional Discussion Point}\label{app:dir_acc}

\paragraph{Direction Accuracy of Generated Responses} Although clearly instructed as in Table~\ref{app:system_prompts_7b}, we notice LVLMs tend to understand the clockwise direction in the perspective of the image itself, not the viewpoint. For example, although we instruct them to choose one of the options of 9 to 3 o'clock for the direction, LVLMs sometimes provide "8 o'clock", which corresponds to "10 o'clock" from an image viewpoint perspective. This suggests that LVLMs are primarily trained on image-caption datasets that describe positional relationships within the image itself, rather than recognizing depth and directional cues from a specific viewpoint. This limitation underscores the critical need for datasets designed to prompt models to interpret and generate descriptions from a defined user perspective.


\subsection{Training and Implementation Details of New Metrics}\label{app:blip_finetuning}

\paragraph{Training Procedure}
When building ImgTxtREW-S in Table~\ref{tab:metric_results}, we use the filtered training, validation, and test sets of the Polaris datasets, which consist of a human score ranging from 0 to 1 (0.00, 0.25, 0.50, 0.75, 1.00) for an image and candidate text, along with five reference texts \cite{wada2024polos}. The filtering process involves selecting candidates that score less than 0.5 for the assigned human score, resulting in data sizes of 22,803, 30,461, and 38,076 for training, validation, and test datasets. Due to the extensive computational cost of training from scratch, we fine-tune ImgREW-S \cite{xu2024imagereward}, setting the hyperparameters as follows: 1 epoch, batch size of 64, accumulation steps of 4, learning rate of 1e-05, learning rate decay style as cosine, and model parameter fix rate of 0.7 (hyperparameter tuning in Table~\ref{app:hyper}). The model checkpoint with the lowest validation loss is saved as the best.  The major difference between ImgREW-S and ImgTxtREW-S is how we encode the pair: While ImgREW-S uses positive and negative image candidates per text, ImgTxtREW-S uses positive and negative text candidates per image. Thus, our final loss objective is as follows:

\begin{equation}
\label{eq:1}
\mathcal{L}(\theta) = -\mathbb{E}_{(I, t_i, t_j) \sim \mathcal{D}} \left[ \log \left( \sigma \left( f_{\theta}(I, t_i) - f_{\theta}(I, t_j) \right) \right) \right]
\end{equation}

In the above equation, $I$, $t$, $\mathcal{D}$, $f_{\theta}$ indicate image, positive/negative text, batch, and preference model. This loss is optimized to learn a reward, a difference between the preference model encoded (image, positive text) and (image, negative text).

TxtBLIP-S is built similarly to ImgTxtREW-S except for how we encode the image-text pairs. Whereas ImgTxtREW-S uses a preference model that inputs image embeddings as the encoder hidden states, TxtBLIP-S is trained using image and text projection layers. In other words, the function $f$ in Equation~\ref{eq:1} equals the cosine similarity between image and text features extracted from image and text projection layers. Also, we observe high overfitting when developing TxtBLIP-S; hence, we lower the learning rate to 5e-7. In addition, unlike ImgTxtREW-S, TxtBLIP-S is fine-tuned on pre-trained BLIP-S, not ImgREW-S. We show the effect of the learning rate on two datasets when building TxtBLIP-S in Table~\ref{app:hyper}. Although TxtBLIP-S trained with the learning rate of 1e-5 shows higher performances than BLIP-S in several datasets, such as Polaris* and ImgREW, it shows unstable performances among other datasets, including PASCAL, FOIL, FlickrExp, and Polaris, which is the reason why we show the results of TxtBLIP-S with the learning rate of 5e-7 in Table~\ref{tab:metric_results}. 

We train Eye4B-S using our training dataset generated by the filtered image-request pairs. We construct the training dataset using positive texts from LLaVA-1.6 \cite{liu2023visualinstructiontuning}, Qwen-VL \cite{wang2024qwen2vlenhancingvisionlanguagemodels}, and InterNLM-x2 \cite{dong2024internlmxcomposer2masteringfreeformtextimage} responses, and negative text from GPT-4o mini \cite{gpt-4omini} responses, based on the BLV preference results obtained from the first round of the BLV experiment (Appendix~\ref{app:human}). The resulting training, validation, and test data sizes are 11.2k, 1.4k, and 1.4k. We emphasize that there is no overlapping between the test data we used for evaluation during training and the Eye4B shown in Table~\ref{tab:metric_results}. The training configurations and procedures are the same as building TxtBLIP-S. All the training and evaluation
experiments are conducted using a single NVIDIA RTX A6000 and A4000, respectively.

\paragraph{Evaluation Stage}
While non-reward-based models, such as CLIP-S \cite{hessel2021clipscore} and BLIP-S \cite{li2022blip} output the similarity value as the cosine similarity between normalized image and text embeddings, the final similarity score for the reward model based metrics - ImgREW-S \cite{xu2024imagereward}, ImgTxtREW-S, and Eye4B-S is calculated using the scalar value of reward. This reward is an output value of the text features encoded with the final multilayer perception. The final score is the normalized reward value using the fixed mean and standard deviations (std).

\section{Visualization Credits}
All the figures are made using the Seaborn package in Python. The icons used in the figures are visualized using resources from Powerpoint, \url{https://emojipedia.org/}, and \url{https://www.flaticon.com/}.


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= System Prompt for Request Generation, fontupper=\small]
You are a request writer. Given an image, your task is to generate 5 to 10 requests related to actions that blind or low-vision (BLV) people can perform. Each request must describe specific, actionable tasks in a detailed and structured manner. The focus should be on mobility, particularly actions related to safe movement, object manipulation, or accessing information that BLV individuals can perform within the context of the scene. The requests should not overlap with each other but be diverse, detailed, and read recognized texts. Please do not mention an object or person not detected in the image, and refrain from using unclear or useless verbs such as organize, explore, navigate, locate, feel, check, and gather information. Do not include color and auditory information. The output should be properly formatted as a list containing 5 to 10 requests.
\end{tcolorbox}
\caption{System prompts we use to prompt GPT-4o mini for request generation}
\label{app:system_prompts_scengen}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompt Sample for Request Generation (Outdoor Visual Scenes), fontupper=\small]
Here is an example. This is a sample image. Based on the given image, you can give requests as follows:\\
1. Reach the other side of the street.\\
2. Enter the market named xxx.\\
3. Enter the parking lot marked by xxx sign named xxx.\\
4. Go in front of a signboard with parking information (30xxx).\\
5. Enter the karaoke place named xxx in the opposite side building identified by red signs.\\
6. Enter the place called xxx.\\
7. Go towards the tall tree.\\
You should not suggest requests as follows:\\
1. Enter the store (unspecific information; If there are multiple stores, you should point out more specific stores with names, if possible).\\
2. Enjoy the scenery of the street view (uninformative information; avoid the usage of xxx).\\
3. Pick up the trash and throw it in the trash can that is located 3 steps away in the 2 o'2019 clock direction (useless request for BLV users; avoid generating request not related with mobility).\\
4. Avoid pedestrian(s) walking toward me (fake request; no pedestrian in image).\\
5. Approach to the signpost marked with 'P' (ambigious request; If there are same multiple objects with different direction, you should specify more detailed, if possible).\\
6. Feel the texture of the brick wall on your right (useless request; avoid generating request without mobility concept).\\
7. Take a picture of sign post on your left for reading (unspecific information; If there are multiple sign posts, you should specify more detailed, if possible).\\
8. Stand near the bushes in the middle of the road (useless request; This is not mobility request).
\end{tcolorbox}
\caption{A single few-shot example for outdoor visual scene sample to  GPT-4o mini for request generation. Note that xxx refers to a detected store name in Korean.}
\label{app:fewshot_prompts_scengen_outdoors}
\end{table*}

\clearpage

\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompt Sample for Request Generation (Indoor Visual Scenes), fontupper=\small]
Here is an example. This is a sample image. Based on the given image, you can give requests as follows:\\
1. Pick up the red fruits.\\
2. Enter the room.\\
3. Go outside the apartment.\\
4. Clean the pots on the stove.\\
5. Pick up the plant pot. \\
6. Water the plants right of refrigerator. \\
7. Sit on the chair in the kitchen. \\
8. Check the gas valve is securely locked. \\
You should not suggest requests as follows:\\
1. Look at yourself in a mirror in the 12 o'2019clock direction (useless information; BLV users cannot see themselves).\\
2. Pick up a phone on the table on the right (misidentified object; There is no phone on the table).\\
3. Go to the kitchen (useless and undetailed information; The user is already located in the kitchen).\\
4. Open the door (unspecific information; There are multiple doors).\\
5. Organize the items on the table (unspecific information; There are multiple items on the table).\\
6. Clear any debris on the floor (unspecific information; You should specify the location and object more detailed).\\
7. Open the door (unspecific information; There are multiple doors).\\
8. Feel the texture of the fruits on the table (useless information; avoid generating request without mobility concept).
\end{tcolorbox}
\caption{A single few-shot example for indoor visual scene sample to GPT-4o mini for request generation.}
\label{app:fewshot_prompts_scengen_indoors}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= System Prompts for LVLM Response Generation (\textit{before} and \textit{after}), fontupper=\small, fontlower=\small]
\textbf{7B LVLMs}:
You are an expert at evaluating the quality of the model responses for a given task. The task for the model was to assist Blind and Low-Vision (BLV) users by providing them with details for their text-based simple requests given an image of a visual scene. You will be given an image of a visual scene, the text-based request, and the text-based model response. Enhance the response in terms of three criteria: a) Accuracy: Your response should include correct information of direction (e.g., x o'clock) and depth (e.g., x steps). b) Length: Your response should include all the correct detailed information; however, it should not include useless information for BLV users, such as color or non-existent assumptions. c) Actionability: Your response should only include possible safe actions that the BLV user can perform and include useful objects such as braille blocks and cautious objects such as motorcycles that might come toward the BLV user.\\
\\
\textbf{GPT-4o mini}:
You are an expert at providing a Blind or Low Vision (BLV) an accurate, helpful description, given an environment scene (outdoor or indoor) and corresponding to their text-based request. Remember that BLV users cannot see as much as normally-sighted humans, so you must provide detailed but precise information from the image. Enhance the model response by including precise clock directions (options: 9, 10, 11, 12, 1, 2, 3 o'clock), depth levels (in meters or steps), and objects BLV users should avoid or utilize. 
\tcblower
\textbf{7B LVLMs}:
You are an expert at providing responses for blind or low-vision (BLV) users. Given an indoor or outdoor visual scene photo taken by BLV user and their text-based requests, your task is to respond to user requests with accurate, structured, and actionable responses. Please ensure your response includes the following:\\
1) Scene Description: Provide a single concise sentence describing the scene or environment relevant to the request, including precise directions (0 to 90 degrees left or right) and depths in meters. Do not include any color information.\\
2) Main Obstacles: Highlight only one or two potential challenges/obstacles in a single sentence. Avoid generic, obvious warnings, and do not include non-detected obstacles.\\
3) Step-by-step Actions: Outline the key actions required to fulfill the request in 1-3 clear and concise sentences. Include precise directions (9 to 3 o-clock directions) and depths in steps (e.g., 3-5 steps) for each stage.
\end{tcolorbox}
\caption{System prompts for generating responses before (\textbf{B}) and after (\textbf{A}) in top and bottom rows.}
\label{app:system_prompts_7b}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompts for LVLM Response Generation (Outdoor Visual Scenes), fontupper=\small]
\textbf{Request}: Enter the store named xxx. \\
\textbf{Response} (\textit{before}): 1. The store named xxx is located in the 10 o’clock direction, approximately 2 to 3 meters away. Be aware that a signpost is in the 9 to 10 o'clock direction, about two to three steps away. 2. Turn left in the 10 o'clock direction and walk straight for three to five steps. Walk with caution as you are on a roadway. There is a parked car about ten steps ahead of you, and the cars may be exiting a building on your right approximately 4 to 5 meters away. 3. Next, turn left in the 11 o'clock direction. Be mindful of a very low curb and a step near the store entrance. 4. The door appears to be closed, so you will need to open it to enter the store.
\\
\textbf{Response} (\textit{after}): 1) Scene Description: You are on a roadway near a store named xxxx,  located in the 10 o’clock direction, about 2–3 meters away. 2) Main Obstacles: There is a signpost located on the front-left side of the store, and there is a low curb and a step near the store entrance. 3) Step-by-Step Actions: - Step 1: Turn left in the 10 o’clock direction and walk 3–5 steps, avoiding the signpost on your left after the turn. - Step 2: Turn left again in the 11 o’clock direction and walk 2-3 steps to enter the store. \\
\\
\textbf{Request}: Enter the market with red sign (named xxx). \\
\textbf{Response} (\textit{before}):  1. The market with a red sign (named xxx) is located in the 11 o’clock direction, approximately 12 meters away. Be cautious of multiple obstacles, including stacked boxes on your left and parked cars on your right as you proceed to xxx due to ongoing construction. 2. To start, walk about five to seven steps in the 1 o’clock direction. As you continue, be aware of a parked SUV on the left. 3. Next, turn slightly left in the 11 to 12 o'clock direction and walk straight for five steps, being cautious of parked cars and pedestrians. There is also the roadway on your right. 4. Finally, walk straight for ten steps, then turn left. Walk another five to eight steps to reach the store entrance. 5. There are several parked cars in front of the store, so proceed carefully. \\
\textbf{Response} (\textit{after}): 1) Scene Description: You are near a market with a red sign named "xxx”, located in the 11 o’clock direction, approximately 12 meters away. 2) Main Obstacles: Be cautious of stacked boxes, a parked SUV on the left side of the street, and pedestrians and parked trucks on the right side of the street.3). Step-by-Step Actions - Step 1: Walk 5–7 steps in the 1 o’clock direction until you reach a parked SUV on the left side of you- Step 2: Turn slightly left in the 11–12 o’clock direction and walk 10 steps straight.- Step 3: Continue 5–8 steps to the store entrance, avoiding several parked cars near the entrance. \\
\\
\textbf{Request}: Enter the market named GS25. \\
\textbf{Response} (\textit{before}): 1. The market is located across the street in the 12 o'clock direction, approximately 9 to 10 meters away. To reach it, you will need to cross the road, so proceed with caution. 2. First, turn left in the 9 to 10 o'clock direction and cross the street until you reach the curb. 3. Turn right in the 3 o'clock direction and walk straight for 15 to 20 steps. There are multiple stores, including a church and real estate building on your left, so please watch out for pedestrians exiting or entering these stores. 4. Once you feel a hard object with your cane or foot, slightly turn left in the 11 o'clock direction and walk three to five steps to enter the store. \\
\textbf{Response} (\textit{after}): 1) Scene Description: You are near a market named “GS25” located across the street at the 12 o’clock direction, about 9–10 meters away. 2). Main Obstacles: Watch out for pedestrians near stores near GS25.  3) Step-by-Step Actions - Step 1: Turn left toward the 9–10 o’clock direction until you reach the curb. - Step 2: Turn right in the 3 o'clock direction and walk 15–20 steps straight along the sidewalk. - Step 3: Turn left in the 11 o’clock direction and walk 3–5 steps to reach the GS25 entrance.
\end{tcolorbox}
\caption{Few-shot examples for response generation using 7B models (outdoor). Note that xxx refers to a detected store name in Korean.}
\label{app:fewshot_prompts_7b_before_outdoors}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompts for LVLM Response Generation (Indoor Visual Scenes), fontupper=\small]
\textbf{Request}: Pick up the red fruits on the table.\\
\textbf{Response} (\textit{before}): 1. The red fruits are on the table directly in front of you, slightly to the right, at the 2 o'clock position. 2. To locate them, place your hand at about a 30-degree angle from straight ahead and feel the edge of the table. 3. Slowly trace the edge of the table to your right, and be mindful of a chair in front of the table. 4. The red fruits are in a dish next to a tissue box. Be cautious, as there is a stove with pots near the table. \\
\textbf{Response} (\textit{after}): 1) Scene Description: You are in a kitchen with a table located directly in front of you at 2 o'clock direction.  2) Main Obstacles: There is a chair without a backrest and a stove with pots located not directly in front of you but on the opposite side of the table. 3) Step-by-Step Actions: - Step 1: Place your right hand in the 2 o'clock direction to locate the edge of the table. - Step 2: Slowly trace the edge of the table until you take two right turns. - Step 3: You will find red fruits directly after your last turn in front of a tissue box. \\
\\ 
\textbf{Request}: Sit on the sofa. \\
\textbf{Response} (\textit{before}): 1. The sofa is located in the 10 to 11 o'clock direction. Be mindful of a low wooden table with a plant pot and a globe between you and the sofa, requiring you to navigate around it. 2. To do so, turn slightly to the right, toward the 12 to 1 o'clock direction, and walk three to five steps. There is a carpet so be careful not to slip. 3. Then, turn 90-degree left and walk two to three steps to reach the left end of the sofa. You can choose to sit on the left end or move to the right side of the sofa. If you move to the right, be cautious of the low wooden table in front of the sofa. 4. Additionally, there is a massage chair to your right. Watch out for objects such as dumbbells and plant pots placed on the low wooden table. \\
\textbf{Response} (\textit{after}): 1) Scene Description: You are in a living room with a sofa located about 3 to 4 meters away in the direction of 10–11 o’clock. 2) Main Obstacles: There is a low wooden table directly in front of the sofa with the width same as that of the sofa. 3) Step-by-Step Actions: - Step 1: Turn right in the 12–1 o’clock direction, and walk 3–5 steps until you reach the end of the low-wooden table. - Step 2: Turn 90 degrees to your left and walk 2–3 steps to reach the left end of the sofa. \\
\\
\textbf{Request}: Pick up the book on 4th floor of the 5-story wooden bookshelf. \\
\textbf{Response} (\textit{before}): 1. The 5-story wooden bookshelf is located in the 10 o'clock direction, about 6 meters away. However, there are obstacles between you and the bookshelf, so you’ll need to navigate carefully. 2. First, turn left toward the 9 to 10 o'clock direction and walk straight for five to eight steps. As you proceed, be cautious of a 1-person sofa and a wooden table and chair on your right. 3. After you reach the wooden table and chair, turn 90-degree right and walk three to four steps to reach the wooden bookshelf. 4. The shelf holds multiple books, but be cautious of objects like a plant pot placed on the very top of the bookshelf. \\
\textbf{Response} (\textit{after}): 1) Scene Description: You are in a room with a 5-story wooden bookshelf located at the 10 o’clock direction, about 6 meters away. 2) Main Obstacles: There is a low-circled table with a 1-person sofa and a chair in the 2 o'clock direction about 2 to 3 steps away, and there is also another set of table and chairs directly in front-right of the 5-story wooden bookshelf. 3) Step-by-Step Actions - Step 1: Turn left toward the 9–10 o’clock direction and walk straight for 5–8 steps. - Step 2: When you reach the set of wooden table and chairs on your right and a low leather sofa on your left, turn 90 degrees to your right. - Step 3: Walk 3–4 steps to reach the 5-story wooden bookshelf.
\end{tcolorbox}
\caption{Few-shot examples for response generation using 7B models (indoor).}
\label{app:fewshot_prompts_7b_before_indoors}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompts for LVLM Response Generation (Outdoor Visual Scenes), fontupper=\small]
\textbf{Request}: Enter the cafe named xxx located on the right side of the street. \\
\textbf{Response}: 1. The cafe named xxx is located in the 3 o'clock direction. 2. First, turn right in the direction of 2 o'clock and walk six to eight steps. 3. Then, turn left in the 9 o'clock direction and walk two to three steps to reach the cafe's entrance. 4. However, there are some obstacles between you and the cafe, so please be careful. \\
\textbf{Enhanced response}: 1. The entrance of the cafe named xxx is located at the 2 o'clock direction, approximately 10 meters away. You are currently on the road in an alley. Be cautious of approaching black car in the 12 o’clock direction approximately 15 meters away. You could turn right and walk more than 5 steps to avoid the car. 2. Turn left again to face the xxx in the 12 to 1 o’clock direction. 3. Walk 10 to 15 steps straight; however, watch out for the billboard on your left as you walk in the earlier steps. Also, slightly turn left in the 11 to 12 o’clock direction since there is a low curb on your right in the later steps. 3. Walk straight for three to five steps, and turn right in the 3 o’clock direction. 4. The cafe is 2 meters away; however there is a parked car in front of the entrance so proceed carefully. \\
\\ 
\textbf{Request}: Approach the subway station exit with the number 4. \\
\textbf{Response}: 1. The subway station exit with the number 4 is on your right. There might be obstacles between you and the exit, so please be careful. 2. First, turn right in the 2 o'clock and walk five to seven steps. 3. Then, turn right slightly and proceed ten to twelve steps. There are stairs in front of the exit, so be careful. \\
\textbf{Enhanced response}: 1. The subway station exit with the number 4 is located to your right in the 2 o’clock direction, approximately 15 meters away, on the opposite side of the street. 2. First, turn right in the 2 o'clock and walk two to four steps, where you will reach the crosswalk. 3. There seems to be a no alarm button on the signal pole so you should cross the street with extra caution. There is a pedestrian across the street so you can shout for the assistance. 4. Walk four to five steps to cross the street. Walk 2 to 3 steps further and turn left to locate the subway exit. You have to walk down the stairs to enter the subway station. \\
\\
\textbf{Request}: Enter the store named LG U+. \\
\textbf{Response}: 1. The store named LG U+ is located in the 4 o'clock direction, approximately 10 meters away. 2. First, turn right at a 20-degree angle. 3. Then, walk 20 to 30 steps to reach the store entrance. 4. Bollards and cars are in front of you, so please be careful. \\
\textbf{Enhanced response}: 1. The store named LG U+ is located in the 2 o'clock direction, approximately 20 meters away. To approach the store, you must cross the street using the crosswalk located in the direction of 12 o’clock, approximately 5 meters away. 2. First, go straight for eight to ten steps and stop. Please stop and wait for the traffic signal if you encounter bollards and tactile paving in front of the crosswalk. 3. Look for the signal pole on the left side of the crosswalk and check for an alarm button. If there is no alarm button, please be aware that the car is coming from the right side of the road. Once you hear the ‘go’ sound from the alarm or no sound of car engines on the right side, carefully cross the crosswalk for 10 to 15 steps. 4. Then, turn right in the 3 o’clock direction and walk 15 to 20 steps. There are multiple stores on your left as you proceed. 5. The LG U+ store is located on the next block so be cautious while crossing the road for 5 to 10 steps. The store will be on your left in the direction of 11 o’clock. 
\end{tcolorbox}
\caption{Few-shot examples for response generation using GPT-4o mini (outdoor).}
\label{app:fewshot_prompts_gpt_outdoors}
\end{table*}

\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Few-Shot Prompts for LVLM Response Generation (Indoor Visual Scenes), fontupper=\small]
\textbf{Request}: Open the refrigerator door in the kitchen. \\
\textbf{Response}: 1. The refrigerator door is located in the 8 o'clock direction. 2. Turn right from where you are and walk two to three steps. 3. Then, turn left in the 2 o'clock direction and continue walking until you reach the fridge. 4. You can access the items inside by opening the fridge door. Watch out for any obstacles in your way. \\
\textbf{Enhanced response}: 1. The kitchen is directly in front of you, and the refrigerator door is located in the 10 o'clock direction, approximately 2 meters away. 2. Walk straight for two to three steps and then turn left in the 9 o'clock direction. Don’t be confused with the cabinet to the left of the refrigerator. 3. Reach out your arm to reach the door handle on the left side of the fridge. 4. The kitchen space appears to be not too large, so be careful when opening the refrigerator door. \\
\\ 
\textbf{Request}: Pick up the bottle of water on the box. \\
\textbf{Response}: 1. The bottle of water is on the box right in front of you, between the 11 to 12 o’clock direction, approximately 10 meters away. There is a machine at knee height between you and the box, so be cautious. 2. Walk one to two steps and stretch your hand in the 11 o’clock direction. 3. You should be able to grab the bottle of water beside the styrofoam box. There are many obstacles around you, so proceed carefully. \\
\textbf{Enhanced response}: 1. The bottle of water is on the box is on top of the shelf and box, next to the styrofoam. It is located between the 11 o’clock direction, approximately 3 meters away. Be aware that there are multiple obstacles around you, and the space appears tight, so proceed cautiously. 2. Walk one to two steps and stretch your hand in the 11 o’clock direction. Be careful since there is a machine at knee height between you and the space where the water bottle is located. 3. You should be able to grab the bottle of water on the right side the styrofoam box. Additionally, note there is a. chair on top of the table to your right and a pile of clothes on your left, so navigate carefully. \\
\\
\textbf{Request}: Enter the bathroom. \\
\textbf{Response}: 1. The bathroom is in the 12 o’clock direction, approximately 3 to 4 meters away. There are toys and other obstacles around you, so be cautious. 2. First, turn slightly right in the 2 o’clock direction and walk 2 to 3 steps. 3. Then, turn left in the 1 o’clock direction and walk 2 to 3 steps. Be mindful of the thin iron shelf on the right of the bathroom door. The bathroom is clean so you don’t have to clean. \\
\textbf{Enhanced response} 1. The bathroom is located in the 12 o’clock direction, approximately 3 meters away. Please proceed carefully since there are toys on your left in the 11 to 12 o’clock direction at a less than 1-meter distance and a bookshelf on your right in the 2 o’clock direction at 1 meter distance. 2. First, turn right in the 2 o’clock direction and walk 3 to 4 steps. The bookshelf should be directly in front of you at this point. 3. Then, turn in the 3 o’clock direction and walk 4 to 5 steps. There is a shelf on the right side of the bathroom door below the bathroom switch button. 
\end{tcolorbox}
\caption{Few-shot examples for response generation using GPT-4o mini (indoor).}
\label{app:fewshot_prompts_gpt_indoors}
\end{table*}



\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Generated Deep Context Samples (\textit{before} and \textit{after}), fontupper=\small, fontlower=\small]
\textbf{Request}: Enter the first building to the left. \\
\textbf{Response} (\textit{before}): The parked car is located at the end of the alley, in the 12 o’clock direction. There are multiple cars and a building on your left side.Walk approximately five to seven steps straight towards the end of the alley.Turn right slightly in the 9 o'clock direction and continue walking for about ten steps, being cautious of pedestrians and other obstacles like parked vehicles.Once you reach the car, carefully navigate around it to proceed further down the alley. 
\tcblower
\textbf{Response} (\textit{after}): 1) Scene Description: You are walking near a building to the left, which is at 11 o'clock and about 15-20 meters away. 2) Main Obstacles: There is a parked car on the left side of your path, so be cautious as you walk past it. 3) Step-by-Step Actions: - Step 1: Turn right in the 10 o'clock direction. - Step 2: Walk straight for 5-7 steps, avoiding the parked car on your left. - Step 3: Continue walking for another 3 steps until you reach the entrance of the first building to the left.
\end{tcolorbox}
\caption{Generated response samples given the request verified by sighted humans.}
\label{app:qual_deepcontext_samples}
\end{table*}


\begin{table*}[t!]
\centering
\begin{tcolorbox}[colback=white, colframe=black, boxsep=3pt, left=3pt, right=3pt, top=3pt, bottom=3pt, width=\textwidth, sharp corners, title= Labeling Guidelines for
Requests of Blind or Low-Vision (BLV) Mobility, fontupper=\small]
\textbf{Introduction}: The goal of this labeling task is to create a dataset that provides detailed and actionable descriptions of mobility requests related to blind or low-vision
(BLV) users. The requests gathered will ultimately support BLV users in safely
navigating and interacting with their surroundings, both indoors and outdoors.
As an annotator, your task is to review 5 to 10 requests associated with a given
image and decide if each request accurately reflects the actions a BLV person
could take in that environment. You will respond with ‘Yes’ or ‘No’ for each
request. \\
\\
\textbf{Image Types}: Each image depicts an indoor or outdoor setting, such as a street, living room, or public space. Your role is to assess mobility-related actions a BLV user could
realistically perform within that context. \\
\\
\textbf{Request Concept}: \\
A valid request should describe specific and actionable tasks that a BLV user might perform within the scene. For example, requests should focus on mobility actions, such as safe movement, spatial exploration, or interaction with objects. requests must be directly related to observable objects in the image and provide clear, specific details. (\textit{e.g.}, "Approach the door on the right," "Pick up a book from the desk in front of you," "Organize the books on the table."). \\
\\
Invalid requests include: (1) requests that involve objects not visible in the image. (2) requests requiring actions based on color or auditory cues, which are difficult for a BLV person to perceive. (3) requests that are vague or ambiguous, such as when identical objects appear in different locations, but the request doesn’t specify which object is referenced (\textit{e.g.}, if there are two tables with water bottles on the left and right and the request says, "Pick up the water bottle from the table," this is unclear and should be marked "No"). Conversely, if the request says, "Pick up the water bottle from the table on the user's right," it is clear and should be marked "Yes." (4) requests with unclear descriptions. (\textit{e.g.}, "Feel the texture of the right wall," "Explore the bushes on the left.") (5) If an object in the request is not clearly identified, select "No" (\textit{e.g.}, if there are many items on a shelf and the request says, "Organize the items on the shelf," it should be marked "No"). \\
\\
\textbf{Annotation Task}: For each image, read 5 to 10 provided requests and determine if they align with the request concept outlined above. If a request is appropriate, select "Yes." If not, select "No."  You may also suggest new requests, provided they do not overlap with the given ones. When reviewing requests, keep the following criteria in mind: \\
- \textit{Specificity}: Does the request offer specific details? For example, if a request suggests "Enter a store," but multiple stores are visible, it should clearly indicate which one. \\
- \textit{Relevance}: Is the request mobility-focused and actionable? Avoid
sensory-focused requests (e.g., "Feel the texture") or vague references to
general actions. \\
- \textit{Accuracy}: Ensure the request doesn’t reference objects, people, or
actions not visible in the image (e.g., mentioning cars or items that aren’t
present). \\
- \textit{Clarity}: Avoid ambiguous verbs like "explore," "organize," or "inspect." requests should describe clear, executable actions. \\ \\
If you select more than 3 “No”, you should propose more than one request related to BLV mobility if you believe they are relevant. (1) Be specific. Use precise details such as "3 o'clock direction," "12 steps ahead," or "door with an exit sign." (2) Focus on mobility: prioritize actions related to movement, positioning, or object interaction. (3) Only mention objects and people visible in the image. (4) Ensure that the request describes an action that a BLV user can realistically perform. (5) Avoid vague or unrelated verbs like "explore," "examine," or "feel." (6) Do not reference objects not present in the image (e.g., unseen cars,
people, or signs). (7) Avoid suggesting actions that are out of context, such as "enjoy nature" or "wait for the bus" if no bus stop is visible. \\
\\
\textbf{Request Examples}: [refer to few-shot samples] \\
\\
\textbf{Final Note}: The ultimate goal of this task is to collect realistic and helpful requests for BLV users. Your annotations will help ensure the requests reflect actions that can be performed in real-life situations by BLV users.
\end{tcolorbox}
\caption{Human experiment guideline for request generation}
\label{app:guideline}
\end{table*}

\clearpage


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/FigureS2.pdf}
    \caption{Distribution of the leading time per sample, the number of newly added captions, and percentage of yes over ratio for each annotator.}
    \label{app:hum_dist}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/FigureS3.pdf}
    \caption{Correlation of the number of newly added captions \textit{vs}. leading time per annotator and the 'yes' over 'yes' and 'no' ratio \textit{vs}. newly added captions.}
    \label{app:hum_corr}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/FigureS1.pdf}
    \caption{Proportion of the number of valid requests per set after the sighted human experiment.}
    \label{app:valid_requests}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/ScenarioGenScreenshot.pdf}
    \caption{Sample screenshot of request generation experiment interface.}
    \label{app:screenshot_scen_gen}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/ConfirmOneYesScreenshot.pdf}
    \caption{Sample screenshot of one 'yes' filtering experiment interface.}
    \label{app:screenshot_filter_oneyes}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/FilterNewCapScreenshot.pdf}
    \caption{Sample screenshot of new caption filtering experiment interface.}
    \label{app:screenshot_filter_newcap}
\end{figure*}




\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/BLVUserStudy.pdf}
    \caption{Sample screenshot of BLV user experiment interface.}
    \label{app:screenshot_blv}
\end{figure*}

\clearpage
\newpage

\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\toprule
\textbf{ID} & \textbf{Sex} & \textbf{Age} & \textbf{Level} & \textbf{Onset Age} & \textbf{AI Experience} & \textbf{Frequency of AI Usage} & \textbf{Usual Navigation Aids} & \textbf{\begin{tabular}[c]{@{}c@{}}{Frequency of}\\{Usual Navigation Aids}\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}{Opinions on Future}\\{Direction of  Assistance Tools}\end{tabular}} \\
\midrule 
P1 & M & 51 & Severe & 7 & Seeing AI & If needed & White cane & Every time & Head phone assistance \\ \midrule
P2 & M & 51 & Blindness & 14 & Seeing AI & 2 times a week & White cane & Every time & Smartphone assistance \\
\midrule
P3 & M & 48 & Severe & 9 & Chat GPT \& BARD & Sometimes & Audible Pedestrian Signal & If needed & Smartphone assistance \\
\midrule
P4 & M & 52 & Blindness & 13 & ChatGPT & For teaching purposes & \begin{tabular}[c]{@{}c@{}}{White cane (past) \&}\\{Assistance from others (current)}\end{tabular} & If needed & Smartphone assistance \\
\midrule
P5 & F & 55 & Moderate & 20 & SeeingAI \& Sullivan A & 2-3 times a week & \begin{tabular}[c]{@{}c@{}}{White cane \&}\\{Assistance from others}\end{tabular} & Every time & Smart glasses \\ \midrule
P6 & M & 54 & Blindness & 16 & \begin{tabular}[c]{@{}c@{}}{Seeing AI, Sullivan A,}\\{ChatGPT \& Gemini}\end{tabular} & 1-2 times a week & White cane & Every time & AI assistance  \\
\midrule
P7 & M & 51 & Severe & 0 & SeeingAI \& Sullivan A & 3 hours a week & White cane & Every time & AI assistance  \\
\midrule
P8 & M & 21 & Severe & 0 & \begin{tabular}[c]{@{}c@{}}{Seeing AI}\\{ChatGPT \& Gemini}\end{tabular} & 3 times a week & \begin{tabular}[c]{@{}c@{}}{White cane}\\{GoogleMap \& Blindsquare}\end{tabular} & If needed & AI assistance  \\
\bottomrule
\end{tabular}
}
\caption{The demographic distribution of the BLV users tested in the first (P1-P6) and second (P1-P2 \& P7-P8) round of human experiments.}
\label{app:personal_blv}
\end{table*}


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/FigureS4.pdf}
    \caption{Distribution of overall and average preference scores evaluated with BLV users and the corresponding correlation plot.}
    \label{app:blv_dist}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/FigureS5.pdf}
    \caption{Average overall score distribution over six BLV participants in the first round. The error bars indicate the standard deviations over all test samples.}
    \label{app:blv_overall}
\end{figure}


\begin{table}[t!]
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{c|ccc|cc}
\toprule
\textit{Model} & \textit{Batch size} & \textit{Learning rate} & \textit{Fix rate}   & \textbf{Polaris*} & \textbf{ImgREW} \\
\midrule
\multirow{4}{*}{ImgTxtREW-S} & 32     & 1e-5          & 0.7      & \textbf{97.8}             & 62.1            \\
 & 32     & 5e-6          & 0.7      & 92.3                  & \underline{64.8}             \\
 & 64    & 1e-5          & 0.7      & \underline{93.0}              & 64.6            \\
 & 64    & 1e-6          & 0.8      &  75.9                & \textbf{65.0}           \\
 \midrule
 \multirow{2}{*}{TxtBLIP-S} & 64    & 1e-5          & 0.7      &   92.9           &     59.0       \\
 & 64     & 5e-7          & 0.7      &      84.8             &    57.8          \\
\bottomrule
\end{tabular}
}
\caption{Hyperparameter tuning for building ImgTxtREW-S and TxtBLIP-S.}
\label{app:hyper}
\end{table}


\end{document}
