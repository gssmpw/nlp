\section{Related Works}
\subsection{BLV-Aware Datasets}

Prior datasets related to BLV individuals, such as VizWiz \cite{gurari2018vizwiz, gurari2019vizwiz, tseng2022vizwiz, bafghi2023new} and BIV-Priv-Seg \cite{tseng2024biv} collect images taken from BLV users. While this approach provides valuable insights, these images are often low quality \cite{bigham2010vizwiz}, degrading test performance in models \cite{chiu2020assessing, olson2021towards}. We build upon high-quality existing outdoor sidewalk \cite{park2020sideguide, aihub_dataset_189}, and indoor scene datasets \cite{aihub_dataset_189}. Although \citealp{xia2023dataset, tang2023dataset} are relevant, we mainly focus on collecting datasets with images taken in South Korea due to the BLV recruitment challenges.

Compared to detection-based AI systems, focusing on vision-centric tasks like object detection \cite{park2020sideguide, xia2023dataset, tang2023dataset}, semantic segmentation \cite{park2020sideguide}, depth estimation \cite{park2020sideguide}, or surface masking \cite{aihub_dataset_189}, there are limited number of semantic-based systems \cite{yuan2024walkvlm}. Our \textsc{Eye4B} dataset contributes to the collection of semantic-based datasets by extending the previous datasets with additional metadata of possible BLV user requests. However, while semantic-based datasets for BLV individuals \cite{yuan2024walkvlm, gurari2019vizwiz, yang2024viassist} deal with visual question-answering tasks, where responses are either `correct' or `incorrect,' our dataset differs by collecting fine-grained preferences of BLV users.

\subsection{LVLMs as BLV Assistance}

Large language models (LLMs) have expanded their capabilities beyond natural language to multiple modalities, bringing significant advancements in LVLMs \cite{dong2024internlmxcomposer2masteringfreeformtextimage, awadalla2023openflamingoopensourceframeworktraining, wang2024qwen2vlenhancingvisionlanguagemodels, llama3_2, gpt-4omini}. LVLMs, enhanced with in-context learning with prompting methods \cite{zong2025vlicl,jin2022goodpromptworthmillions,wu2022generativevisualpromptunifying, dong2024internlmxcomposer2masteringfreeformtextimage}, demonstrate applicability to be integrated with applications for BLV users \cite{liu2024objectfinderopenvocabularyassistiveinteractive}. Be My Eyes \cite{be_my_eyes} is the first BLV-aware application in collaboration with OpenAI \cite{achiam2023gpt}, and \citealp{zhang2024design} introduces an interface for BLV users to access object information using LVLMs. The most related work, WalkVLM, is designed to support the BLV user navigation in cities such as Beijing \cite{yuan2024walkvlm}.




\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Figure1.pdf}
    \caption{The overall framework of \textsc{Eye4B} dataset construction (Section \ref{method:eye4b_dataset}) and benchmarking LVLMs with BLV user preferences (Section \ref{method:eye4b_benchmark}). We first collect and verify requests for the images of outdoor and indoor visual scenes. Second, we generate responses of LVLMs on the validated image-request pairs. Thirdly, we benchmark these responses with the assessment from the BLV users. Finally, we re-generate LVLM responses by incorporating feedback from the BLV users.}
    \label{fig:dataset_gen_framework}
\end{figure*}