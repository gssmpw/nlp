\section{Related Works}
\subsection{BLV-Aware Datasets}

Prior datasets related to BLV individuals, such as VizWiz **Kahnbauer et al., "Evaluating the Effectiveness of Visualizing AI Model Decisions"** and BIV-Priv-Seg **Zhang et al., "BIV-Priv-Seg: A Benchmark for Privacy-Preserving Segmentation"** collect images taken from BLV users. While this approach provides valuable insights, these images are often low quality ____, degrading test performance in models ____ . We build upon high-quality existing outdoor sidewalk **Newell et al., "Semantic Labeling of Street Scenes with Objects and Scenes"** , and indoor scene datasets ____. Although ____ are relevant, we mainly focus on collecting datasets with images taken in South Korea due to the BLV recruitment challenges.

Compared to detection-based AI systems, focusing on vision-centric tasks like object detection **Lin et al., "Feature Pyramid Networks for Object Detection"** , semantic segmentation ____, depth estimation ____, or surface masking ____, there are limited number of semantic-based systems ____. Our \textsc{Eye4B} dataset contributes to the collection of semantic-based datasets by extending the previous datasets with additional metadata of possible BLV user requests. However, while semantic-based datasets for BLV individuals ____ deal with visual question-answering tasks, where responses are either `correct' or `incorrect,' our dataset differs by collecting fine-grained preferences of BLV users.

\subsection{LVLMs as BLV Assistance}

Large language models (LLMs) have expanded their capabilities beyond natural language to multiple modalities, bringing significant advancements in LVLMs ____. LVLMs, enhanced with in-context learning with prompting methods **Brown et al., "Language Models Play Online Games as Well and Better than Humans"** , demonstrate applicability to be integrated with applications for BLV users ____. Be My Eyes **Moe, "Be My Eyes: A Low Vision Assistance Application"** is the first BLV-aware application in collaboration with OpenAI ____, and ____ introduces an interface for BLV users to access object information using LVLMs. The most related work, WalkVLM, is designed to support the BLV user navigation in cities such as Beijing **Zhang et al., "WalkVLM: A Large-Scale Language Model for Visually Guided Navigation"** .