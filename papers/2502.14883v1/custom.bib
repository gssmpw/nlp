@inproceedings{bigham2010vizwiz,
  title={Vizwiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}

@inproceedings{bandukda2021opportunities,
  title={Opportunities for supporting self-efficacy through orientation \& mobility training technologies for blind and partially sighted people},
  author={Bandukda, Maryam and Holloway, Catherine and Singh, Aneesha and Barbareschi, Giulia and Berthouze, Nadia},
  booktitle={Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--13},
  year={2021}
}

@inproceedings{olson2021towards,
  title={Towards Using Live Photos to Mitigate Image Quality Issues In VQA Photography},
  author={Olson, Lauren and Kambhamettu, Chandra and McCoy, Kathleen},
  booktitle={Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--3},
  year={2021}
}

@inproceedings{chiu2020assessing,
  title={Assessing image quality issues for real-world problems. In 2020 IEEE},
  author={Chiu, Tai-Yin and Zhao, Yinan and Gurari, Danna},
  booktitle={CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3643--3653},
  year={2020}
}

@article{tseng2024biv,
  title={BIV-Priv-Seg: Locating Private Content in Images Taken by People With Visual Impairments},
  author={Tseng, Yu-Yun and Sharma, Tanusree and Zhang, Lotus and Stangl, Abigale and Findlater, Leah and Wang, Yang and Gurari, Danna},
  journal={arXiv preprint arXiv:2407.18243},
  year={2024}
}

@inproceedings{gurari2019vizwiz,
  title={Vizwiz-priv: A dataset for recognizing the presence and purpose of private visual information in images taken by blind people},
  author={Gurari, Danna and Li, Qing and Lin, Chi and Zhao, Yinan and Guo, Anhong and Stangl, Abigale and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={939--948},
  year={2019}
}

@inproceedings{bafghi2023new,
  title={A new dataset based on images taken by blind people for testing the robustness of image classification models trained for imagenet categories},
  author={Bafghi, Reza Akbarian and Gurari, Danna},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16261--16270},
  year={2023}
}

@inproceedings{tseng2022vizwiz,
  title={Vizwiz-fewshot: Locating objects in images taken by people with visual impairments},
  author={Tseng, Yu-Yun and Bell, Alexander and Gurari, Danna},
  booktitle={European Conference on Computer Vision},
  pages={575--591},
  year={2022},
  organization={Springer}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{an-etal-2024-capturing,
    title = "Capturing the Relationship Between Sentence Triplets for {LLM} and Human-Generated Texts to Enhance Sentence Embeddings",
    author = "An, Na Min  and
      Waheed, Sania  and
      Thorne, James",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.43/",
    pages = "624--638",
    abstract = "Deriving meaningful sentence embeddings is crucial in capturing the semantic relationship between texts. Recent advances in building sentence embedding models have centered on replacing traditional human-generated text datasets with those generated by LLMs. However, the properties of these widely used LLM-generated texts remain largely unexplored. Here, we evaluate the quality of the LLM-generated texts from four perspectives (Positive Text Repetition, Length Difference Penalty, Positive Score Compactness, and Negative Text Implausibility) and find that there exists an inherent difference between human and LLM-generated datasets. To further enhance sentence embeddings using both human and LLM-generated datasets, we propose a novel loss function that incorporates Positive-Negative sample Augmentation (PNA) within the contrastive learning objective. Our results demonstrate that PNA effectively mitigates the sentence anisotropy problem in Wikipedia corpus (-7{\%} compared to CLHAIF) and simultaneously improves the Spearman`s correlation in standard Semantic Textual Similarity (STS) tasks (+1.47{\%} compared to CLHAIF)."
}

@article{an2024i0t,
  title={I0T: Embedding Standardization Method Towards Zero Modality Gap},
  author={An, Na Min and Kim, Eunki and Thorne, James and Shim, Hyunjung},
  journal={arXiv preprint arXiv:2412.14384},
  year={2024}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{zhang2025long,
  title={Long-clip: Unlocking the long-text capability of clip},
  author={Zhang, Beichen and Zhang, Pan and Dong, Xiaoyi and Zang, Yuhang and Wang, Jiaqi},
  booktitle={European Conference on Computer Vision},
  pages={310--325},
  year={2025},
  organization={Springer}
}

@inproceedings{kreiss2022context,
  title={Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics},
  author={Kreiss, Elisa and Bennett, Cynthia and Hooshmand, Shayan and Zelikman, Eric and Morris, Meredith Ringel and Potts, Christopher},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4685--4697},
  year={2022}
}

@article{kreiss2023contextref,
  title={ContextRef: Evaluating Referenceless Metrics For Image Description Generation},
  author={Kreiss, Elisa and Zelikman, Eric and Potts, Christopher and Haber, Nick},
  journal={arXiv preprint arXiv:2309.11710},
  year={2023}
}

@inproceedings{wada2024polos,
  title={Polos: Multimodal Metric Learning from Human Feedback for Image Captioning},
  author={Wada, Yuiga and Kaneda, Kanta and Saito, Daichi and Sugiura, Komei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13559--13568},
  year={2024}
}

@article{sarto2024positive,
  title={Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training},
  author={Sarto, Sara and Moratelli, Nicholas and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  journal={arXiv preprint arXiv:2410.07336},
  year={2024}
}

@article{xu2024imagereward,
  title={Imagereward: Learning and evaluating human preferences for text-to-image generation},
  author={Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{hessel2021clipscore,
  title={CLIPScore: A Reference-free Evaluation Metric for Image Captioning},
  author={Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7514--7528},
  year={2021}
}

@inproceedings{huh2023genassist,
  title={GenAssist: Making image generation accessible},
  author={Huh, Mina and Peng, Yi-Hao and Pavel, Amy},
  booktitle={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--17},
  year={2023}
}

@article{waisberg2024meta,
  title={Meta smart glasses—large language models and the future for assistive glasses for individuals with vision impairments},
  author={Waisberg, Ethan and Ong, Joshua and Masalkhi, Mouayad and Zaman, Nasif and Sarker, Prithul and Lee, Andrew G and Tavakkoli, Alireza},
  journal={Eye},
  volume={38},
  number={6},
  pages={1036--1038},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{wang2024visiongpt,
  title={VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation},
  author={Wang, Hao and Qin, Jiayou and Bastola, Ashish and Chen, Xiwen and Suchanek, John and Gong, Zihao and Razi, Abolfazl},
  journal={arXiv preprint arXiv:2403.12415},
  year={2024}
}

@inproceedings{de2024vqask,
  title={VQAsk: a multimodal Android GPT-based application to help blind users visualize pictures},
  author={De Marsico, Maria and Giacanelli, Chiara and Manganaro, Clizia Giorgia and Palma, Alessio and Santoro, Davide},
  booktitle={Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  pages={1--5},
  year={2024}
}

@article{yang2024viassist,
  title={VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments},
  author={Yang, Bufang and He, Lixing and Liu, Kaiwei and Yan, Zhenyu},
  journal={arXiv preprint arXiv:2404.02508},
  year={2024}
}

@article{zhao2024vialm,
  title={Vialm: A survey and benchmark of visually impaired assistance with large models},
  author={Zhao, Yi and Zhang, Yilin and Xiang, Rong and Li, Jing and Li, Hillming},
  journal={arXiv preprint arXiv:2402.01735},
  year={2024}
}

@article{liu2024objectfinder,
  title={ObjectFinder: Open-Vocabulary Assistive System for Interactive Object Search by Blind People},
  author={Liu, Ruiping and Zhang, Jiaming and Sch{\"o}n, Angela and M{\"u}ller, Karin and Zheng, Junwei and Yang, Kailun and Gerling, Kathrin and Stiefelhagen, Rainer},
  journal={arXiv preprint arXiv:2412.03118},
  year={2024}
}

@article{tang2023dataset,
  title={A dataset for the recognition of obstacles on blind sidewalk},
  author={Tang, Wu and Liu, De-er and Zhao, Xiaoli and Chen, Zenghui and Zhao, Chen},
  journal={Universal Access in the Information Society},
  volume={22},
  number={1},
  pages={69--82},
  year={2023},
  publisher={Springer}
}

@article{xia2023dataset,
  title={A dataset for the visually impaired walk on the road},
  author={Xia, Haiying and Yao, Cong and Tan, Yumei and Song, Shuxiang},
  journal={Displays},
  volume={79},
  pages={102486},
  year={2023},
  publisher={Elsevier}
}


@inproceedings{
zong2025vlicl,
title={{VL}-{ICL} Bench: The Devil in the Details of Multimodal In-Context Learning},
author={Yongshuo Zong and Ondrej Bohdal and Timothy Hospedales},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=cpGPPLLYYx}
}

@article{bao2024faithbench,
  title={FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMs},
  author={Bao, Forrest Sheng and Li, Miaoran and Qu, Renyi and Luo, Ge and Wan, Erana and Tang, Yujia and Fan, Weisi and Tamber, Manveer Singh and Kazi, Suleman and Sourabh, Vivek and others},
  journal={arXiv preprint arXiv:2410.13210},
  year={2024}
}

@article{yan2024evaluating,
  title={Evaluating the quality of hallucination benchmarks for large vision-language models},
  author={Yan, Bei and Zhang, Jie and Yuan, Zheng and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:2406.17115},
  year={2024}
}

@article{zhang2024toolbehonest,
  title={Toolbehonest: A multi-level hallucination diagnostic benchmark for tool-augmented large language models},
  author={Zhang, Yuxiang and Chen, Jing and Wang, Junjie and Liu, Yaxin and Yang, Cheng and Shi, Chufan and Zhu, Xinyu and Lin, Zihao and Wan, Hanwen and Yang, Yujiu and others},
  journal={arXiv preprint arXiv:2406.20015},
  year={2024}
}

@misc{seeing_ai,
  author       = {Microsoft},
  title        = {Seeing AI - Talking Camera App for the Blind},
  year         = {2017},
  url          = {https://www.seeingai.com/}
}

@misc{be_my_eyes,
  author       = {Wiberg, Hans Jørgen},
  title        = {Be My Eyes - See the World Together},
  year         = {2015},
  url          = {https://www.bemyeyes.com/}
}

@misc{aira_about_us,
  author       = {Aira},
  title        = {About Us and Our Values},
  year         = {2015},
  url          = {https://aira.io/aira-about-us/},
}

@misc{aihub_dataset_189,
  author       = {AIHub},
  title        = {AI Hub: Sidewalk Dataset},
  year         = {2019},
  url          = {https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=189}
}

@misc{sullivan_pro_app,
  author       = {TUAT},
  title        = {Sullivan A},
  year         = {2024},
  url          = {https://play.google.com/store/apps/details?id=tuat.kr.sullivan.pro&hl=ko}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{ma2024spreadsheetbench,
  title={SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation},
  author={Ma, Zeyao and Zhang, Bohan and Zhang, Jing and Yu, Jifan and Zhang, Xiaokang and Zhang, Xiaohan and Luo, Sijia and Wang, Xi and Tang, Jie},
  journal={CoRR},
  year={2024}
}

@inproceedings{liu2024custom,
  title={Custom GPTs enhancing performance and evidence compared with GPT-3.5, GPT-4, and GPT-4o? A study on the emergency medicine specialist examination},
  author={Liu, Chiu-Liang and Ho, Chien-Ta and Wu, Tzu-Chi},
  booktitle={Healthcare},
  volume={12},
  pages={1726},
  year={2024},
  organization={MDPI}
}
@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{gallace2009cognitive,
  title={The cognitive and neural correlates of tactile memory.},
  author={Gallace, Alberto and Spence, Charles},
  journal={Psychological bulletin},
  volume={135},
  number={3},
  pages={380},
  year={2009},
  publisher={American Psychological Association}
}

@article{hutmacher2019there,
  title={Why is there so much more research on vision than on any other sensory modality?},
  author={Hutmacher, Fabian},
  journal={Frontiers in psychology},
  volume={10},
  pages={481030},
  year={2019},
  publisher={Frontiers}
}

@inproceedings{kuriakose2023exploring,
  title={Exploring the User Experience of an AI-based Smartphone Navigation Assistant for People with Visual Impairments},
  author={Kuriakose, Bineeth and Shrestha, Raju and Sandnes, Frode Eika},
  booktitle={Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
  pages={1--8},
  year={2023}
}

@article{chidiac2024accessibility,
  title={Accessibility of the Built Environment for People with Sensory Disabilities—Review Quality and Representation of Evidence},
  author={Chidiac, SE and Reda, MA and Marjaba, GE},
  journal={Buildings},
  volume={14},
  number={3},
  pages={707},
  year={2024},
  publisher={MDPI}
}

@inproceedings{bandukda2019understanding,
  title={Understanding experiences of blind individuals in outdoor nature},
  author={Bandukda, Maryam and Singh, Aneesha and Berthouze, Nadia and Holloway, Catherine},
  booktitle={Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--6},
  year={2019}
}

@article{kazemi2023recognizing,
  title={Recognizing the Viewpoint and Experience of Blind People in Navigation and Daily Traffic},
  author={Kazemi, Homa and Kamali, Mohammad and Salehi, Reza and Mobaraki, Hossein},
  journal={Function and Disability Journal},
  volume={6},
  number={1},
  pages={0--0},
  year={2023},
  publisher={Function and Disability Journal}
}

@inproceedings{avila2016remote,
  title={Remote assistance for blind users in daily life: A survey about be my eyes},
  author={Avila, Mauro and Wolf, Katrin and Brock, Anke and Henze, Niels},
  booktitle={Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
  pages={1--2},
  year={2016}
}

@inproceedings{lasecki2013answering,
  title={Answering visual questions with conversational crowd assistants},
  author={Lasecki, Walter S and Thiha, Phyo and Zhong, Yu and Brady, Erin and Bigham, Jeffrey P},
  booktitle={Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--8},
  year={2013}
}

@inproceedings{burton2012crowdsourcing,
  title={Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities},
  author={Burton, Michele A and Brady, Erin and Brewer, Robin and Neylan, Callie and Bigham, Jeffrey P and Hurst, Amy},
  booktitle={Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility},
  pages={135--142},
  year={2012}
}
@inproceedings{park2020sideguide,
  title={Sideguide: a large-scale sidewalk dataset for guiding impaired people},
  author={Park, Kibaek and Oh, Youngtaek and Ham, Soomin and Joo, Kyungdon and Kim, Hyokyoung and Kum, Hyoyoung and Kweon, In So},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={10022--10029},
  year={2020},
  organization={IEEE}
}

@article{alex2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Alex, Krizhevsky},
  journal={https://www. cs. toronto. edu/kriz/learning-features-2009-TR. pdf},
  year={2009}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2641--2649},
  year={2015}
}
@inproceedings{agrawal2019nocaps,
  title={Nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8948--8957},
  year={2019}
}
@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}
@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}
@article{gao2015you,
  title={Are you talking to a machine? dataset and methods for multilingual image question},
  author={Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}
@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}
@inproceedings{zhu2016visual7w,
  title={Visual7w: Grounded question answering in images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4995--5004},
  year={2016}
}
@article{malinowski2014multi,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@article{wang2017fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2017},
  publisher={IEEE}
}

@misc{liu2023visualinstructiontuning,
      title={Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
      year={2023},
      eprint={2304.08485},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.08485}, 
}
@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
      title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
      author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.12191}, 
}
@misc{dong2024internlmxcomposer2masteringfreeformtextimage,
      title={InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model}, 
      author={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Xilin Wei and Songyang Zhang and Haodong Duan and Maosong Cao and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Xinyue Zhang and Wei Li and Jingwen Li and Kai Chen and Conghui He and Xingcheng Zhang and Yu Qiao and Dahua Lin and Jiaqi Wang},
      year={2024},
      eprint={2401.16420},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.16420}, 
}
@misc{awadalla2023openflamingoopensourceframeworktraining,
      title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models}, 
      author={Anas Awadalla and Irena Gao and Josh Gardner and Jack Hessel and Yusuf Hanafy and Wanrong Zhu and Kalyani Marathe and Yonatan Bitton and Samir Gadre and Shiori Sagawa and Jenia Jitsev and Simon Kornblith and Pang Wei Koh and Gabriel Ilharco and Mitchell Wortsman and Ludwig Schmidt},
      year={2023},
      eprint={2308.01390},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.01390}, 
}

@inproceedings{gubbi2024context,
  title={Context-Aware Image Descriptions for Web Accessibility},
  author={Gubbi Mohanbabu, Ananya and Pavel, Amy},
  booktitle={Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={1--17},
  year={2024}
}

@article{zur2024updating,
  title={Updating CLIP to Prefer Descriptions Over Captions},
  author={Zur, Amir and Kreiss, Elisa and D'Oosterlinck, Karel and Potts, Christopher and Geiger, Atticus},
  journal={arXiv preprint arXiv:2406.09458},
  year={2024}
}

@misc{gpt-4omini,
title={GPT-4o mini},
author={OpenAI},
url={https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
year={2024b},
}

@misc{liu2024objectfinderopenvocabularyassistiveinteractive,
      title={ObjectFinder: Open-Vocabulary Assistive System for Interactive Object Search by Blind People}, 
      author={Ruiping Liu and Jiaming Zhang and Angela Schön and Karin Müller and Junwei Zheng and Kailun Yang and Kathrin Gerling and Rainer Stiefelhagen},
      year={2024},
      eprint={2412.03118},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2412.03118}, 
}
@article{Fereday2006DemonstratingRU,
  title={Demonstrating Rigor Using Thematic Analysis: A Hybrid Approach of Inductive and Deductive Coding and Theme Development},
  author={Jennifer Fereday and Eimear Muir‐Cochrane},
  journal={International Journal of Qualitative Methods},
  year={2006},
  volume={5},
  pages={80 - 92},
  url={https://api.semanticscholar.org/CorpusID:16954911}
}
@inproceedings{lee2022opportunities,
  title={Opportunities for human-AI collaboration in remote sighted assistance},
  author={Lee, Sooyeon and Yu, Rui and Xie, Jingyi and Billah, Syed Masum and Carroll, John M},
  booktitle={Proceedings of the 27th International Conference on Intelligent User Interfaces},
  pages={63--78},
  year={2022}
}
@inproceedings{prajapati2024survey,
  title={A Survey on Navigation Assistance System for Visually Impaired and Blind People},
  author={Prajapati, Devanshi and Bordoloi, Prapti and Sheth, Rushil and Sharma, Ankit K},
  booktitle={International Conference on Information and Communication Technology for Intelligent Systems},
  pages={569--577},
  year={2024},
  organization={Springer}
}
@inproceedings{shadi2019outdoor,
  title={Outdoor navigation for visually impaired based on deep learning},
  author={Shadi, Saleh and Hadi, Saleh and Nazari, Mohammad Amin and Hardt, Wolfram},
  booktitle={Proc. CEUR Workshop Proc},
  volume={2514},
  pages={97--406},
  year={2019}
}
@inproceedings{gunjal2024detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={16},
  pages={18135--18143},
  year={2024}
}
@article{liu2024survey,
  title={A survey on hallucination in large vision-language models},
  author={Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
  journal={arXiv preprint arXiv:2402.00253},
  year={2024}
}
@article{valipoor2024analysis,
  title={Analysis and design framework for the development of indoor scene understanding assistive solutions for the person with visual impairment/blindness},
  author={Valipoor, Moeen and de Antonio, Ang{\'e}lica and Cabrera, Juli{\'a}n},
  journal={Multimedia Systems},
  volume={30},
  number={3},
  pages={1--28},
  year={2024},
  publisher={Springer}
}
@article{khan2022mechanism,
  title={A mechanism for blind-friendly user interface adaptation of mobile apps: A case study for improving the user experience of the blind people},
  author={Khan, Akif and Khusro, Shah},
  journal={Journal of Ambient Intelligence and Humanized Computing},
  volume={13},
  number={5},
  pages={2841--2871},
  year={2022},
  publisher={Springer}
}
@inproceedings{zhang2024design,
  title={A Design of Interface for Visual-Impaired People to Access Visual Information from Images Featuring Large Language Models and Visual Language Models},
  author={Zhang, Zhe-Xin and Ochiai, Yoichi},
  booktitle={Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  pages={1--4},
  year={2024}
}
@article{narins2024validated,
  title={Validated image caption rating dataset},
  author={Narins, Lothar D and Scott, Andrew and Gautam, Aakash and Kulkarni, Anagha and Castanon, Mar and Kao, Benjamin and Ihorn, Shasta and Siu, Yue-Ting and Mason, James M and Blum, Alexander and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{sarto2023positive,
  title={Positive-augmented contrastive learning for image and video captioning evaluation},
  author={Sarto, Sara and Barraco, Manuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6914--6924},
  year={2023}
}
@article{yuan2024walkvlm,
  title={WalkVLM: Aid Visually Impaired People Walking by Vision Language Model},
  author={Yuan, Zhiqiang and Zhang, Ting and Zhang, Jiapei and Zhou, Jie and Zhang, Jinchao},
  journal={arXiv preprint arXiv:2412.20903},
  year={2024}
}
@misc{llama3_2,
    title = {Llama 3.2},
    author = {Meta},
    url = {https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
    year={2024}
}
@misc{Label,
  title={{Label Studio}: Data labeling software},
  url={https://github.com/heartexlabs/label-studio},
  note={Open source software available from https://github.com/heartexlabs/label-studio},
  author={
    Maxim Tkachenko and
    Mikhail Malyuk and
    Andrey Holmanyuk and
    Nikolai Liubimov},
  year={2020-2022},
}
@article{Zhou_2022,
   title={Learning to Prompt for Vision-Language Models},
   volume={130},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-022-01653-1},
   DOI={10.1007/s11263-022-01653-1},
   number={9},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
   year={2022},
   month=jul, pages={2337–2348} }
@misc{wu2022generativevisualpromptunifying,
      title={Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models}, 
      author={Chen Henry Wu and Saman Motamed and Shaunak Srivastava and Fernando De la Torre},
      year={2022},
      eprint={2209.06970},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.06970}, 
}
@misc{jin2022goodpromptworthmillions,
      title={A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models}, 
      author={Woojeong Jin and Yu Cheng and Yelong Shen and Weizhu Chen and Xiang Ren},
      year={2022},
      eprint={2110.08484},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2110.08484}, 
}

@misc{xu2019structuredmodelingjointdeep,
      title={Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection}, 
      author={Yingyue Xu and Dan Xu and Xiaopeng Hong and Wanli Ouyang and Rongrong Ji and Min Xu and Guoying Zhao},
      year={2019},
      eprint={1909.04366},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1909.04366}, 
}
@inproceedings{shekhar-etal-2017-foil,
    title = "{FOIL} it! Find One mismatch between Image and Language caption",
    author = "Shekhar, Ravi  and
      Pezzelle, Sandro  and
      Klimovich, Yauhen  and
      Herbelot, Aur{\'e}lie  and
      Nabi, Moin  and
      Sangineto, Enver  and
      Bernardi, Raffaella",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1024/",
    doi = "10.18653/v1/P17-1024",
    pages = "255--265",
}

@inproceedings{nishimura-etal-2024-text360nav,
    title = "{T}ext360{N}av: 360-Degree Image Captioning Dataset for Urban Pedestrians Navigation",
    author = "Nishimura, Chieko  and
      Kurita, Shuhei  and
      Seki, Yohei",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1371/",
    pages = "15783--15788",
}
@misc{chen2020touchdownnaturallanguagenavigation,
      title={Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments}, 
      author={Howard Chen and Alane Suhr and Dipendra Misra and Noah Snavely and Yoav Artzi},
      year={2020},
      eprint={1811.12354},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1811.12354}, 
}
@inproceedings{zheng2024materobot,
  title={Materobot: Material recognition in wearable robotics for people with visual impairments},
  author={Zheng, Junwei and Zhang, Jiaming and Yang, Kailun and Peng, Kunyu and Stiefelhagen, Rainer},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2303--2309},
  year={2024},
  organization={IEEE}
}
@inproceedings{liu2023open,
  title={Open scene understanding: Grounded situation recognition meets segment anything for helping people with visual impairments},
  author={Liu, Ruiping and Zhang, Jiaming and Peng, Kunyu and Zheng, Junwei and Cao, Ke and Chen, Yufan and Yang, Kailun and Stiefelhagen, Rainer},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1857--1867},
  year={2023}
}
@inproceedings{brady2013visual,
  title={Visual challenges in the everyday lives of blind people},
  author={Brady, Erin and Morris, Meredith Ringel and Zhong, Yu and White, Samuel and Bigham, Jeffrey P},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={2117--2126},
  year={2013}
}
@article{real2019navigation,
  title={Navigation systems for the blind and visually impaired: Past work, challenges, and open problems},
  author={Real, Santiago and Araujo, Alvaro},
  journal={Sensors},
  volume={19},
  number={15},
  pages={3404},
  year={2019},
  publisher={MDPI}
}
@inproceedings{schumann-riezler-2021-generating,
    title = "Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem",
    author = "Schumann, Raphael  and
      Riezler, Stefan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.41/",
    doi = "10.18653/v1/2021.acl-long.41",
    pages = "489--502",
}
@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}
@article{merchant2024generating,
  title={Generating Contextually-Relevant Navigation Instructions for Blind and Low Vision People},
  author={Merchant, Zain and Anwar, Abrar and Wang, Emily and Chattopadhyay, Souti and Thomason, Jesse},
  journal={arXiv preprint arXiv:2407.08219},
  year={2024}
}
@misc{li2023evaluatingobjecthallucinationlarge,
      title={Evaluating Object Hallucination in Large Vision-Language Models}, 
      author={Yifan Li and Yifan Du and Kun Zhou and Jinpeng Wang and Wayne Xin Zhao and Ji-Rong Wen},
      year={2023},
      eprint={2305.10355},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.10355}, 
}
@misc{huang2023factualinconsistencyproblemabstractive,
      title={The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey}, 
      author={Yichong Huang and Xiachong Feng and Xiaocheng Feng and Bing Qin},
      year={2023},
      eprint={2104.14839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.14839}, 
}
@inproceedings{kapur-kreiss-2024-reference,
    title = "Reference-Based Metrics Are Biased Against Blind and Low-Vision Users' Image Description Preferences",
    author = "Kapur, Rhea  and
      Kreiss, Elisa",
    editor = "Dementieva, Daryna  and
      Ignat, Oana  and
      Jin, Zhijing  and
      Mihalcea, Rada  and
      Piatti, Giorgio  and
      Tetreault, Joel  and
      Wilson, Steven  and
      Zhao, Jieyu",
    booktitle = "Proceedings of the Third Workshop on NLP for Positive Impact",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.nlp4pi-1.26/",
    doi = "10.18653/v1/2024.nlp4pi-1.26",
    pages = "308--314"
}