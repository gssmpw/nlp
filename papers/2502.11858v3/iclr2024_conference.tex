
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{multirow}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{wrapfig}
\usepackage{cleveref}
\usepackage{lipsum}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{graphicx} 		%图片

\iclrfinalcopy

\usepackage{subfigure}		%子图
\title{Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.



\author{\textsuperscript{$\star$}Zeliang Zhang$^{1}$, \thanks{Equal contribution. Listed order is random.}~~Susan Liang$^{1}$,  Daiki Shimada$^{1,2}$, Chenliang Xu$^{1}$\\
  $^1$University of Rochester \quad \quad $^2$Sony Group Corporation\\
  \texttt{\{zeliang.zhang, susan.liang, chenliang.xu\}@rochester.edu} \\
  \texttt{Daiki.Shimada@sony.com} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
  While audio-visual learning equips models with a richer understanding of the real world by leveraging multiple sensory modalities, this integration also introduces new vulnerabilities to adversarial attacks.
 In this paper, we present a comprehensive study of the adversarial robustness of audio-visual models, considering both temporal and modality-specific vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal invariance attack that exploits the inherent temporal redundancy across consecutive time segments and 2) a modality misalignment attack that introduces incongruence between the audio and visual modalities. These attacks are designed to thoroughly assess the robustness of audio-visual models against diverse threats. Furthermore, to defend against such attacks, we introduce a novel audio-visual adversarial training framework. This framework addresses key challenges in vanilla adversarial training by incorporating efficient adversarial perturbation crafting tailored to multi-modal data and an adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds dataset demonstrate that our proposed temporal and modality-based attacks in degrading model performance can achieve state-of-the-art performance, while our adversarial training defense largely improves the adversarial robustness as well as the adversarial training efficiency.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Audio-visual models, capable of integrating both auditory and visual information, have gained significant traction in recent years due to their ability to create a comprehensive understanding of the surrounding world~\citep{li2022learning, song2023emotional,song2024texttoon,song2024tri}. These models have demonstrated remarkable success in a wide range of applications, including multimedia analysis~\citep{DBLP:journals/tmm/Dimoulas16}, human-computer interaction~\citep{zhen2023human}, and autonomous systems~\citep{guo2023audio}.
However, a critical challenge lies in their susceptibility to adversarial attacks~\citep{zhang2024bag,zhang2024random,yao2024understanding}. These attacks can craft imperceptible perturbations to the input data, causing audio-visual models to make erroneous predictions~\citep{wang2022triangle} or interpretations~\citep{han2023interpreting}. Such errors can have disastrous consequences, especially in safety-critical domains like auto-driving~\citep{kloukiniotis2022countering} and identity verification~\citep{zhang2021attack}.
% However, their vulnerability to adversarial attacks raises serious concerns about their reliability and security, especially in safety-critical domains like auto-driving~\citep{kloukiniotis2022countering} and identity verification~\citep{zhang2021attack}. Adversarial attacks can craft imperceptible perturbations to the input data, causing audio-visual models to produce incorrect predictions~\citep{wang2022triangle} or interpretations~\citep{han2023interpreting}, with potentially catastrophic consequences.

While prior work has investigated the adversarial robustness of audio-visual models~\citep{DBLP:conf/cvpr/TianX21,DBLP:conf/cvpr/YangLBCK21, DBLP:conf/icassp/LiQLHM22}, they primarily rely on general adversarial attack methods, such as FGSM~\citep{DBLP:journals/corr/GoodfellowSS14} and I-FGSM~\citep{kurakin2017adversarial}, originally designed for the single-modality data. These methods are simply adapted to the audio-visual domain without fully capitalizing on its unique characteristics. A key limitation of such approaches lies in their inability to consider the inherent properties of audio-visual data. Unlike single modality, audio-visual data possesses two crucial aspects: temporal consistency and intermodal correlation. For instance, in a video of a dog barking, we \textit{see} and \textit{hear} the barking event \textit{unfold over time}, not just in a single frame. These properties play a vital role in human perception of the real world~\citep{sun2022human, yang2023target}. However, current attack methods fail to exploit them, potentially limiting their effectiveness. Conversely, by leveraging these characteristics, we can craft more potent attacks and develop improved robust learning strategies specifically tailored for audio-visual models.

% Crucially, audio-visual data possesses inherent properties, namely temporal consistency and intermodal correlation that are essential for human perception~\citep{sun2022human, yang2023target}. For example, a dog barking video can be seen and heard and the barking event happens not just a snippet but consecutive in time. Exploiting these properties can lead to more effective adversarial attacks and improved robust learning strategies tailored to the audio-visual domain.

% these works overlook the unique characteristics of audio-visual data, namely temporal consistency and intermodal correlation, which are essential in enhancing our understanding of the world~\citep{sun2022human, yang2023target}. Exploiting these properties can lead to more effective adversarial attacks and improved robust learning strategies tailored to the audio-visual domain.

% In this work, we rethink the audio-visual adversarial attack and defense from both temporal and modality perspectives. We first conduct an empirical analysis of the robustness of audio-visual models. There are several takeaways derived from our experiments of case study, including the existence of adversarial transferability in audio-visual learning, and the fact that temporal consistency and modality correlations impact the model robustness.  Leveraging these insights from audio-visual learning, we propose two powerful adversarial attacks: 1) the temporal invariance-based attack, which seeks to attack robust temporal invariant audio-visual features, and 2) the modality misalignment attack, which introduces information incongruencies between the audio and visual modalities.  We also introduce a novel audio-visual adversarial training framework to defend against such strong attacks. Our framework addresses key challenges by incorporating efficient adversarial perturbation crafting tailored to multi-modal temporal data and an adversarial curriculum training strategy. The proposed defense aims to significantly improve the robustness of audio-visual models against adversarial attacks with minimum impact on training efficiency.

In this work, we rethink the adversarial vulnerability of audio-visual models through the lenses of temporal and modality perspectives. We begin with an empirical analysis to assess the vulnerability of existing models. Our case study experiments reveal several key findings, including the presence of adversarial transferability within the audio-visual domain, and the significant impact of temporal consistency and modality correlations on model robustness. Leveraging these insights, we propose two novel adversarial attacks tailored to the unique properties of multi-modal data: 1) the temporal invariance attack, which targets robust and temporally consistent audio-visual features by introducing inconsistencies across consecutive frames, and 2) the modality misalignment attack, which crafts adversarial examples by inducing incongruencies between the audio and visual streams.

To mitigate the vulnerabilities exposed by these dedicated attacks, we propose a novel audio-visual adversarial training framework that serves as a robust defense mechanism. Our framework addresses critical challenges in robust multi-modal learning by incorporating efficient adversarial perturbation crafting techniques along with an adversarial curriculum training strategy. The proposed defense aims to significantly improve the robustness of audio-visual models against adversarial attacks with minimal impact on training efficiency.

Our contributions can be summarized as follows:
\begin{enumerate}
    \item We first identify the existence of adversarial transferability in audio-visual learning, and introduce two powerful adversarial attacks, namely the Temporal Invariance-based Attack (TIA) and the Modality Misalignment-based Attack (MMA), to evaluate the adversarial robustness of audio-visual models comprehensively.  
    \item We propose efficient adversarial perturbation crafting and adversarial curriculum training aimed at enhancing both the robustness and efficiency of audio-visual models. 
    \item We validate the effectiveness of both our proposed attacks and defense mechanisms through extensive experiments conducted on the widely-used Kinetics-Sounds dataset.
\end{enumerate}

\section{Related Work}

\subsection{Audio-visual Learning}
The field of audio-visual learning encompasses a wide range of tasks, including audio-visual event recognition~\citep{brousmiche2019audio,xia2022cross,brousmiche2022multimodal}, separation~\citep{wu2019time, majumder2021move2hear,majumder2022active,huang2023davis}, localization~\citep{wu2019dual, wu2021binaural,huang2023egocentric}, correspondence learning~\citep{min2020multimodal,zhu2021learning,morgado2021robust}, representation learning~\citep{zhou2019talking,cheng2020look,rahman2021tribert}, and cross-modal generation~\citep{chen2017deep,hao2018cmcgan, sung2023sound,liang2023av,liang2024language,huang2024scaling,liang2023neural}. Among these, audio-visual event recognition stands out as a fundamental task~\citep{gao2024audio} that has attracted significant research attention, particularly regarding robustness and security issues~\citep{yang2023quantifying}.

Deep learning models employed in audio-visual event recognition typically comprise three main components: visual encoder, audio encoder, and fusion layer. Although prior research has extensively focused on optimizing these components to enhance task performance, there has been limited consideration of their implications for security and robustness. 

In this work, we delve into the individual components of these models and examine their respective impacts on robustness, shedding light on crucial but often overlooked aspects.





\subsection{Adversarial Attack \& Defense}




Research efforts in adversarial robustness for audio-visual models have been relatively limited. Tian \textit{et al.}~\citep{DBLP:conf/cvpr/TianX21} were among the first to explore the potential of audio-visual integration in enhancing robustness against multi-modal attacks. Yang \textit{et al.}\citep{DBLP:conf/cvpr/YangLBCK21} proposed an adversarially robust audio-visual fusion layer to defend against single-source adversarial attacks. Li \textit{et al.} \citep{DBLP:conf/icassp/LiQLHM22} introduced a novel mix-up strategy in the audio-visual fusion layer to improve the robustness of audio-visual models. Yang \textit{et al.} \citep{yang2023quantifying} proposed a certified robust training method to boost the multi-modal robustness. 
However, they primarily focused on adapting single-modality adversarial attacks to audio-visual scenes. There remains a critical need for powerful audio-visual adversarial attacks that can serve as benchmarks for evaluating the adversarial robustness of audio-visual methods and the effectiveness of robust training techniques.

In this work, we address the gap by designing an effective audio-visual adversarial attack method that facilitates a more comprehensive assessment of model robustness. We further propose an efficient defense technique to enhance the robustness of audio-visual models against adversarial attacks.

\section{Empirical Robustness Analysis of Audio-Visual Models}


%\subsection{Preliminaries}
\noindent \textbf{Notations.} Given an audio clip $x_a$ and video frames $x_v$, we use an audio network $f_a(x_a;\theta_a)$ to extract audio features, a visual network $f_v(x_v;\theta_v)$ to extract visual features and a fusion network $f_u$ for modality integration. We denote the complete audio-visual network with $F(x_v,x_a;\theta) \coloneqq f_u(f_v(x_v;\theta_v),f_a(x_a;\theta_a);\theta_u)$, where $\theta=(\theta_v,\theta_a,\theta_u)$ are the overall network parameters.





% We begin our motivation in Sec.~\ref{sec:motivation}. In Sec.~\ref{sec:attack}, we introduce two novel adversarial attack methods. In Sec.~\ref{sec:training}, we present our efficient adversarial training approach.

%A generalized adversarial example not only fools the white-box model, which is used to craft the adversarial perturbation, $\textit{i.e.}$, the surrogate model, but also deceives other black-box models, \textit{i.e.}, the victim models. A robust model should effectively defend against adversarial examples in both white-box and black-box settings.  





% Temporal consistency and modality correlation build two basic characteristics of audio-visual learning. On the one hand, the presence of these characteristics strengthens the robustness and generalization ability of audio-visual models. On the other hand, the redundancy in temporal information and the cross-attention between different modalities provide the potential to boost the adversarial transferability of crafted adversarial examples.


\label{sec:motivation}
Temporal consistency and modality correlation are two fundamental characteristics of audio-visual learning.  On the one hand, \textit{these characteristics provide the robustness and generalization capabilities to audio-visual models}.  The temporal consistency in audio-visual data reinforces learning across consecutive frames, while the cross-modal correlations between auditory and visual signals offer mutually complementary information. These properties enable audio-visual models to learn robust and reliable representations.  However, \textit{the temporal and cross-modal dependency also paradoxically create new vulnerabilities}. 
Unlike conventional attack methods, audio-visual adversarial attacks can exploit these relationships to cause inconsistencies within the model, leading to errors. 
This duality underscores the critical need to understand and address adversarial robustness in audio-visual models from both temporal and modality perspectives.



\noindent \textbf{Corruption Robustness}.  Here, we provide experiments to support our arguments. We train an audio-visual model on the Kinetics-Sounds dataset~\citep{arandjelovic2017look}, which takes the VGG as the vision encoder, the AlexNet as the audio encoder, and the sum operation as the fusion operation followed by the decision layer.  We randomly mask a ratio of $\rho$ of the audio-visual data along the temporal dimension, where $0\%<\rho<30\%$, and evaluate the model performance on the masked audio-visual data. For a comprehensive understanding of how different modalities affect the audio-visual model's decision, we set up three groups, namely perturbing the video only (\text{V}$\times$), perturbing the audio only (\text{A}$\times$), and perturbing both the audio and visual synchronously (\text{A}$\times\oplus$\text{V}$\times$) and asynchronously (\text{A}$\times\otimes$\text{V}$\times$). 





\begin{figure*}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figs/draw_mask.pdf}
\caption{Classification accuracy of the model when masking the audio and visual data with a ratio of $\rho$.} \label{fig:clean_perturb}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figs/draw_mask_transfer.pdf }
\caption{Average black-box attack success rate ($\%$) against $7$ black-box models.}\label{fig:adv_perturb}
\end{minipage}
\end{figure*}





We have three observations from the results shown in \cref{fig:clean_perturb}. \underline{First}, {the audio-visual data is relatively redundant in temporal information}.  By masking the audio-visual data with certain ranges, \textit{e.g.}, $\rho<20\%$, the model remains at least $50\%$  classification success rate, indicating robustness against the temporal perturbation.   \underline{Second}, the model heavily relies on the visual modality to make the decision, leaving the audio modality less attention. With the same ratio $\rho$ to be masked, the degradation of model performance caused by perturbing the visual data is significantly greater than perturbing the audio data only with a clear margin of $12.3\%$. \underline{Third},  the correlations between different modalities increase the performance of audio-visual learning. Compared to asynchronous perturbation of audio and visual data,  synchronous perturbation causes relatively larger performance degradation, indicating the complementary of two modalities in helping the model make decisions.  


\label{sec:adv_motivation}

\noindent\textbf{Adversarial Robustness}. For a given audio-visual input $(x_a,x_v)$ with ground-truth $y$, the goal of the adversarial attack is crafting perturbation $\delta_a$ (audio) and $\delta_v$ (visual) to deceive $F$ into making a wrong prediction. This process is formulated as follows, 
\begin{equation}
    \begin{aligned}
    \max ~&\mathcal{L}(F(x_v+\delta_v,x_a+\delta_a;\theta),y)\\
    \textit{s.t.} & ~\Vert \delta_v \Vert_p \leq \epsilon_v, ~ ~\Vert \delta_a \Vert_p \leq \epsilon_a, 
    \end{aligned}
    \label{eq:mattack_general}
\end{equation}
where $\mathcal{L}$ is an arbitrary loss function, $\Vert \Vert_p$ is the $p$-norm, and $\epsilon_v$ and $\epsilon_a$ are the adversarial perturbation budgets for visual and audio modalities, respectively. 



%The adversarial training, inversely, is training models with previously %generated adversarial examples to enhance models' robustness:
%\begin{equation}
%    \begin{aligned}
%    \min_\theta \max_{\delta_v,\delta_a} \mathcal{L}%(F(x_v+\delta_v,x_a+\delta_a;\theta),y).
%    \end{aligned}
%    \label{vanilla_adv}
%\end{equation}



For a further study of the impact of temporal consistency and modality correlation on the adversarial robustness, we additionally train various audio-visual models on the Kinetics-Sounds dataset to evaluate the adversarial attack performance. We, respectively, use the VGG and ResNet as the vision backbone, the AlexNet and ResNet as the audio backbone, and the sum and concat as the fusion layer, in a total of $8$ models. We set the model with VGG as the vision backbone, AlexNet as the audio backbone, and concat as the fusion layer, as the surrogate model to generate adversarial examples by FGSM~\citep{DBLP:journals/corr/GoodfellowSS14} under the white-box setting, which is up to $78.3\%$ attack success rate. Then, we set the other $7$ models as black-box models to further evaluate adversarial transferability. 



As shown in \cref{fig:adv_perturb}, we can find that adversarial transferability between different models also exists in the audio-visual data. Without any precomputed masking operation, the generated audio-visual adversarial examples have an average attack success rate of $31.7\%$ against the selected black-box models. By masking different modalities along the temporal dimension adequately with ratio $\rho$ to generate multiple copies for gradient calculation, the adversarial transferability can be boosted. Also, a high setting of $\rho$ causes a loss of information, hindering the quality of generated adversarial examples and degrading the adversarial transferability.  Specifically, it can bring up to an improvement of $3.8\%$ by only masking the audio, $9.3\%$ by only masking the video, and $13.8\%$ by masking both the audio and visual modality. These results uncover that temporal redundancy in audio and visual data can be naturally used to boost adversarial transferability.  Besides, the compensation function from the modality correlation mitigates the impact of adversarial transferability with up to $1.5\%$,  comparing the synchronous and asynchronous perturbation. 




\begin{tcolorbox}[colframe=brown,colback=white, title = {Takeaways of our empirical study}]
(1) The adversarial transferability is also exhibited in audio-visual learning, posing security problems in applications.
 
(2) Temporal consistency and redundancy bring robustness against corruption but also remain potentially leveraged to improve adversarial transferability. 

(3) The inter-modal correlation compensates against temporal corruptions and alleviates the influence of adversarial perturbation.
\end{tcolorbox}


% While the models rely more on the visual modality than the audio modality, 
% We also use the surrogate model to transform these masked audio-visual data to adversarial examples under the white-box setting and evaluate the adversarial transferability by attacking the remaining models under the black-box setting. We use the classification accuracy to evaluate the model performance against these perturbed data. 



% \noindent \textbf{Modality correlation}. 





\section{Audio-Visual Adversarial Attack}
\label{sec:attack}


Motivated by previous empirical robustness analysis on temporal consistency and modality correlation, we propose two powerful audio-visual adversarial attacks, namely the temporal invariance-based attack and the modality misalignment-based attack.
  

% As shown in Fig. \textbf{\textit{FIG}}, we augment the data with temporal alignment for the audio and visual modalities to keep the consistency without semantic change, thus making the crafting of adversarial examples stabilized on the temporal invariant features.

\subsection{Temporal Invariance-based Attack}
Audio-visual data comprises temporally invariant features and time-varying information within each frame. Our goal is to craft adversarial perturbations that target these invariant features, thereby fostering the transferability of adversarial instances. We achieve this by introducing a temporal regularization term that steers perturbations toward the most significant video features. With this novel regularization, we ensure that the adversarial perturbations are consistent and coherent across different frames.

Specifically, we calculate the variation of extracted features along the temporal dimension as a structure-unrelated statistic of feature consistency. By minimizing this variation for both audio and visual features, we encourage perturbations to focus on temporally invariant characteristics.

Additionally, leveraging the inherent temporal dependency, we can diversify video inputs to encourage the model to learn robust, invariant features. In practice, we apply different input transformations on temporal inputs, including scaling, masking, blurring, and mix-up on audio or visual modalities, independently or in parallel, synchronously or asynchronously.


This temporal regularization can be expressed as
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{R} &= \text{Var}\left[\left\{\mathbb{E}\left(f_a\left(\mathcal{T}_a\left(x_a+\delta_a\right);\theta_a\right)\left(t\right)\right)\right\}_{t=1}^{T}\right]\\ &+ \text{Var}\left[\left\{\mathbb{E}\left(f_v\left(\mathcal{T}_v\left(x_v+\delta_v\right);\theta_v\right)\left(t\right)\right)\right\}_{t=1}^{T}\right],
    \end{aligned}
    \label{eq:tir}
\end{equation}
where we denote $f_a(x_a+\delta_a;\theta_a)(t)$ and $f_v(x_v+\delta_v;\theta_v)(t)$ as the audio and visual features extracted at the $t$-th frame by the audio and visual networks, $\mathcal{T}_{a}$ and $\mathcal{T}_{v}$ as input transformation methods for audio and visual modalities, respectively. We consider the mean value of the feature at each time step and compute the variance along the temporal axis. 



\subsection{Modality Misalignment-based Attack}

%The modality correlation reflects the semantic consistency between audio and visual modalities. Although the audio-visual perturbation $\delta_a$ and $\delta_v$ are optimized jointly using \cref{eq:mattack_general}, there is no guarantee of the semantic consistency between $x_a+\delta_a$ and $x_v+\delta_v$. Due to the parallel architecture of audio and visual encoding backbones, the prediction of the audio network could have a semantic gap between that of the visual network after adversarial perturbations. This lack of semantic correlation has the potential to diminish the effectiveness of crafted audio-visual adversarial examples.


From the empirical study about the modality correlation in \cref{sec:adv_motivation}, especially the comparison between the synchronous and asynchronous perturbation, we notice that aligning semantic changes across modalities can mitigate the influence of adversarial perturbations and diminish the adversarial transferability. Inspired by this, we propose a novel attack strategy that disrupts the strong semantic correlation between audio and visual modalities. We hypothesize that \textbf{lower semantic correlation leads to higher adversarial audio-visual transferability}.


We conduct experiments to support this hypothesis.
% , \textit{i.e.}, \textbf{lower semantic correlation indicates higher adversarial audio-visual transferability}. 
% We use the feature vectors of two modalities to represent the semantic information and employ the cosine similarity to assess the alignment of these two modalities.  
To quantify semantic information, we use feature vectors from both modalities and assess their alignment with cosine similarity.  
Following the experimental setting in \cref{sec:adv_motivation} (without masking operation), we compute cosine similarities between feature vectors of adversarial examples generated by FGSM~\citep{DBLP:journals/corr/GoodfellowSS14}, I-FGSM~\citep{kurakin2017adversarial},  MI-FGSM~\citep{DBLP:conf/cvpr/DongLPS0HL18}, and NI-FGSM~\citep{lin2019nesterov}, which exhibit progressively stronger attack performance under the white-box setting.

\iffalse
\begin{table}[t]
    \centering
    \caption{The average black-box attack success rate (A.S.R.) and cosine similarity (C.S.) of the audio-visual adversarial examples generated by different attack methods.}
    \label{tab:asr_cs}
    \begin{tabular}{c|ccccc}
    \toprule
         Method & FGSM & I-FGSM & MI-FGSM & NI-FGSM \\
         \hline
         A.S.R. (\%)   & $31.7$ & $30.2$  & $53.1$  & $54.8$ \\
         C.S. (\%)     & $47.6$ & $49.3$  &  $35.2$ & $34.9$  \\
         \hline
    \end{tabular}
\end{table}
\fi


\input{figs/motivation_fig}From the results shown in \cref{tab:asr_cs}, we can see that as the attack success rate against black-box models (\textit{i.e.}, adversarial transferability) increases, the cosine similarity between audio and visual feature vectors decreases. This correlation reinforces our claim that disrupting semantic alignment between modalities enhances transferability.

Thus, to enhance the transferability of adversarial attacks, we propose an approach that aims to misalign the semantic correspondence between audio and visual features. Specifically, during each iteration of the adversarial attack, we minimize the feature similarity between the modalities, ensuring that the perturbations disrupt the semantic alignment at the modality level,
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{M} = \frac{f_a(x_a+\delta_a;\theta_a) \cdot f_v(x_v+\delta_v;\theta_v)}{\Vert f_a(x_a+\delta_a;\theta_a)\cdot f_v(x_v+\delta_v;\theta_v)\Vert_2},
    \end{aligned}
    \label{eq:maa}
\end{equation}
where $f_a(x_a+\delta_a;\theta_a)$ and $f_v(x_v+\delta_v;\theta_v)$ are the feature vectors encoded by the audio and visual backbones, respectively. 


\subsection{Attack Integration}
The temporal invariance-based and modality misalignment-based attacks can be integrated together with the classification loss to achieve strong adversarial audiovisual attacks. At each iteration, we optimize the following loss function to conduct the  attack,
\begin{equation}
    \begin{aligned}
        \max ~&\mathcal{L}_{cls}(F(\mathcal{T}(x_v+\delta_v,x_a+\delta_a);\theta),y)\\
        &-\lambda_1 \mathcal{L}_R(F(\mathcal{T}(x_v+\delta_v,x_a+\delta_a);\theta))\\
        &- \lambda_2\mathcal{L}_M(F(\mathcal{T}(x_v+\delta_v,x_a+\delta_a);\theta))\\
    \textit{s.t.} & ~\Vert \delta_v \Vert_p \leq \epsilon_v, ~ ~\Vert \delta_a \Vert_p \leq \epsilon_a, 
    \end{aligned}
    \label{eq:attack_loss}
\end{equation}
where $\mathcal{L}_{cls}$ is the classification loss function, $\mathcal{T}$ is the input transformation on audio-visual data, and $\lambda_1$ and $\lambda_2$ are two coefficients to balance losses. 



% \input{figs/attack_performance}

% \input{figs/defense_performance}

\input{figs/overview}


\section{ Audio-Visual Adversarial Training}
Having established the powerful audio-visual adversarial attacks, a question naturally arises: \textit{ How can we defend these attacks efficiently?} In this section, we start with the preliminary adversarial training method, followed by an in-depth analysis of adversarial perturbations in audio-visual data, and propose several strategies to fortify audio-visual models against such attacks. 


\label{sec:training}
\subsection{Preliminary Method}
Adversarial training is one of the most powerful robust training paradigms to defend adversarial examples,  which trains the model on adversarial examples and can be formalized  in the audio-visual context as  follows,
\begin{equation}
    \begin{aligned}
    \min_\theta \max_{\delta_v,\delta_a} \mathcal{L}_{cls}(F(x_v+\delta_v,x_a+\delta_a),\theta),
    \end{aligned}
    \label{vanilla_adv}
\end{equation}
which optimizes the model parameters to minimize the upper bound of the loss function. 

It is challenging to directly optimize the min-max problem in \cref{vanilla_adv}. In practice, it is solved by training models on adversarial examples, in which adversarial examples are used to compute the values of the inner maximum loss function~\citep{madry2018towards}. 





\subsection{Discussion on Audio-Visual Perturbation}
% Different from the study on uni-modal adversarial training, audio-visual adversarial training needs to generate the adversarial perturbation across different time samples and modalities. The huge time consumption brought by the computational overhead hinders the application of strong adversarial attacks to generate adversarial examples in the adversarial training process, as well as incorporating advanced techniques to mitigate the posed challenges. 
Audio-visual adversarial training presents a unique challenge compared to its uni-modal counterpart (\textit{e.g.,} perturbating a single image). Here, generating adversarial perturbations across multiple time steps in both audio and visual data leads to significant computational overhead. This hinders the use of strong attacks and advanced mitigation techniques during training.
Thus, the key to solving the challenge is to reduce the computational cost for audio-visual adversarial example crafting. 


As shown in \cref{sec:motivation}, random masking of audio-visual data slightly affects model performance yet boosts adversarial transferability. Additionally, recent studies~\citep{kim2023breaking} demonstrate that the universal adversarial perturbation of a single image model can be used to fool the video model. These findings indicate that an effective strategy should reduce the computational overhead of adversarial training by generating temporal universal adversarial perturbations instead of frame-specific perturbations.  
However, strong attacks are crucial for effective adversarial training. To address this, we propose generating universal adversarial perturbations for continuous local regions in the video and audio data, rather than a single perturbation for all frames.
We also employ a bag of tricks to amplify the impact of these generated perturbations on the model.
% At the same time, due to the strength of adversarial attacks that also plays a crucial role in the robustness improvement by adversarial training, instead of generating universal perturbation for all frames, we propose to generate the temporal universal perturbation for continuous video and audio local frames and design a bag of tricks to boost the harm of generated adversarial examples. 

\subsection{Our Approach}



\noindent \textbf{Efficient adversarial perturbation crafting}. 
% Due to the significant temporal correlation among different frames, adversarial perturbations generated from a subset of frames can still propagate confusion to the rest of the frames.
Since audio-visual data exhibits strong temporal correlation, adversarial perturbations crafted from a subset of time samples can be propagated to the remaining samples as well.

To exploit this property and achieve efficient perturbation crafting, we propose the following approach:
We first divide the entire audio-visual data into smaller segments. Within each segment, we randomly sample a portion of audio and visual frames with a selection ratio of $\rho$. We then generate universal adversarial perturbations upon the selected frames and propagate them to the remaining. The optimization of perturbations $\delta_v$ and $\delta_a$ can be expressed as follows,
\begin{equation}
    \begin{aligned}
        \delta_v,\delta_a  = \arg\max_{\hat{\delta}_v,\hat{\delta}_a} \mathop{\mathbb{E}}\limits_{(\hat{x}_v,\hat{x}_a) \sim (x_v, x_a)}[\mathcal{L}(\hat{x}_v+\hat{\delta}_v, \hat{x}_a+\hat{\delta}_a, y)],
    \end{aligned}
    \label{eq:universal_perturbation}
\end{equation}
where the $(\hat{x}_v,\hat{x}_a)$ is sampled from  $(x_v, x_a)$, $\mathcal{L}$ is the loss we proposed in \cref{eq:attack_loss} \textcolor{black}{which encounters both the temporal invariance-based and modality misalignment-based attacks}. The adversarial perturbations generated $\delta_v$ and $\delta_a$ are shared with neighboring frames within the same segment. This design allows us to generate adversarial examples efficiently. 

% thus alleviating the \textit{Challenge 1} and \textit{Challenge 2}. 

% \noindent \textbf{Temporal information augmentation}. Diversity has been shown to be effective in both model training~\citep{rebuffi2021data} and adversarial perturbation crafting~\citep{lin2019nesterov}. We use this property to address Challenge \#2 by augmenting the data at the $t$-th frame, $(x_a, x_v)[t]$, with a set of operations $\mathcal{T}=\{$Gaussian Blur, Dropout, Mix-up, Shifting, Padding$\}$.


\noindent \textbf{Adversarial curriculum  training}. Previous studies~\citep{yu2022understanding,kim2021understanding,rice2020overfitting} have identified that adversarial training easily gets overfitted to certain attack methods and settings, leaving the model vulnerable to others. To address this issue and improve the generalization ability,  we propose a randomized adversarial curriculum learning to optimize \cref{vanilla_adv}. Concretely, our proposed randomized adversarial curriculum learning approach incorporates two strategies: 
\begin{itemize}
    \item \textbf{Data-level strategy}. Temporal redundancy can be leveraged to control the impact of adversarial examples crafted in adversarial training, leaving the potential to alleviate overfitting. We propose to randomly \textcolor{black}{sample} $\rho_x$ of the audio-visual input along the temporal dimension, where $0<\rho_x<1$.  We cyclically vary the value of $\rho_x$ to generate adversarial examples for curriculum learning.
    \item \textbf{Model-level strategy}. The over-parameterization of models can be exploited in adversarial training to boost the models' generalization. We randomly drop out the neurons of the fusion layer with a ratio of $\rho_f$, where $0<\rho_f<1$ and the dropout ratio $\rho_f$ is synchronized with $\rho_x$. 
\end{itemize}

The curriculum learning approach hinges on a cyclical variation in the data-level masking ratio $\rho_x$ and the model-level dropout ratio $\rho_f$. This cyclic design offers an advantage: it gradually increases the training difficulty. During the initial stages of the cycle, both \textcolor{black}{sampling} and dropout ratios are lower and the number of steps to generate attacks is reduced. This translates to a simpler training task with fewer adversarial perturbations applied to the data. As both ratios and the number of steps gradually increase, complex adversarial examples are created, resulting in a more difficult learning task.
With this cyclic design, the models are trained on diverse perturbations, thus fully exploring the loss landscape for the searching minimum and improving the robustness against  attacks.

% With a lower $\rho_x$ and $\rho_f$ in the cycle, we also reduce the number of steps for generating audio-visual attacks, which combines to lead to an easy training task; Opposedly, while $\rho_x$ and $\rho_f$ increase, we increase the number of steps as well, leading to a difficult learning task.  By this cyclic design, the models are adversarially trained on multiple types of perturbation, thus fully exploring the loss landscape for the searching minimum and improving the robustness against different attacks. 


\iffalse
\noindent \textbf{Implementation details.} We present \cref{fig:overview} to illustrate the adversarial example generation process in our adversarial training method. For a given audio-visual data $(x_a,x_v)$, we randomly segment it into different parts and sample frames from each of the segments as $(\hat{x}_a,\hat{x}_v)$ in a ratio $\alpha$. Then, we generate a universal temporal adversarial perturbation for the selected audio-visual frame using \cref{eq:universal_perturbation}. Last, we map the generated adversarial perturbation of the sampled frames to their corresponding segments, creating audio-visual adversarial examples for adversarial training. In this process, the cyclic randomized ratio $\rho_x$ and $\rho_f$ changes to achieve adversarial training in the curriculum. 


\fi


\section{Performance Evaluation}
% In experiments, we first present the experiment settings, then conduct experiments to verify the effectiveness of our proposed attack methods and defense methods, lastly do the ablation study on the key parameters of our methods. 


\input{figs/av_performance}


\subsection{Experimental Setup}
\noindent \textbf{Datasets.} We use the Kinetics-Sounds \citep{arandjelovic2017look} for evaluation, which contains $1,551,610$ second video clips in $27$ human action categories. For model training, we split the dataset into $7:2:1$ for training, validation, and testing. We also conduct experiments on MIT-MUSIC \citep{DBLP:conf/eccv/ZhaoGRVMT18} for further verification, which is provided in the appendix. 






\noindent\textbf{Models.} The audio-visual model comprises three modules: the visual backbone, the audio backbone, and the audio-visual fusion network. For the audio backbone, we select VGG and ResNet as candidates. For the visual backbone, we select AlexNet and ResNet as candidates. We can study the impact of model capacity on the adversarial transferability by this design. Following the previous work on audio-visual adversarial robustness~\citep{DBLP:conf/cvpr/TianX21}, we selected the sum and \textcolor{black}{concat} operation as candidates for the fusion layer. There are $2\times 2 \times 2 = 8$ models in total. For simplicity, \textcolor{black}{we use the format of ``\{ visual backbone \}-\{ fusion layer \}-\{ audio backbone \}'' to represent the audio-visual models, where the initials indicate each backbone and layer.}  We denote AlexNet as A,  VGG as V, ResNet as R, sum operation as s, and \textcolor{black}{concat} operation as c. For example,  ResNet as the visual backbone, the audio-visual model with AlexNet as the audio backbone, and the sum operation as the fusion layer can be represented by ``RsA''.  





% To study the influence of different components on the adversarial robustness,  we select the  ResNet-18 ($R^{A}_{18}$) and  ResNet-34 ($R^{A}_{34}$) as the audio backbone candidates, the ResNet-18 ($R^{V}_{18}$) and ResNet-34 ($R^{V}_{34}$) as the visual backbone candidates, and sum ($s$) and concatenate ($c$) as the fusion methods. The fusion layer for classification follows the fully connected layer. We use all models for attack performance evaluations and $R^{A}_{18}-s-R^{A}_{18}$ for defense performance evaluations. 


\noindent\textbf{Baselines.} For attack methods, we use FGSM~\citep{DBLP:journals/corr/GoodfellowSS14}, I-FGSM~\citep{kurakin2017adversarial},  MI-FGSM~\citep{DBLP:conf/cvpr/DongLPS0HL18}, MIG~\citep{ma2023transferable}, and PAM~\citep{zhang2023improving} as baselines. We compare the baseline methods with our proposed approaches, which encompass the temporal invariance-based attack (TIA), the modality misalignment attack (MMA), and the integration of TIA and MAA attacks, namely the temporal and modality-based attack (TMA). For defense methods, we use the vanilla adversarial training (AT)~\citep{madry2018towards}, discriminative and compact feature learning  (DCFL)~\citep{DBLP:conf/cvpr/TianX21}, single-source defensive fusion (SSDF)~\citep{DBLP:conf/cvpr/YangLBCK21}, the mix-up strategy for adversarial defense (Mixup)~\citep{DBLP:conf/icassp/LiQLHM22}, and the certified robust multi-modal training method (CRMT-AT)~\citep{yang2023quantifying}. 


\iffalse
\noindent \textbf{Evaluation metric.} For attack, we use the attack success rate under the black-box setting to evaluate the performance of different attack methods. We utilize each model as a surrogate model to generate adversarial examples and deploy these examples to attack all models. We then calculate the attack success rates across all models to assess the effectiveness of adversarial examples generated from the surrogate model. For defense, we use the classification success rate to measure the robustness of audio-visual models. We first train models with robust training methods and then test trained models with various white-box attacks. We calculate the classification success rate of trained models on the adversarial examples to measure the models' performance. 
\fi

\subsection{Single Model Attack}
To validate the effectiveness of our proposed methods, we first compare ours with five popular attacks selected by previous audio-visual robustness research, namely FGSM, I-FGSM, MI-FGSM, MIG, and PAM. Among these methods, FGSM and I-FGSM are initially designed for white-box attacks, while others are designed for transferable adversarial attacks in the image domain.  We generate adversarial examples on a single model and test them on the other models. The attack success rates, \textit{i.e.}, and the misclassification rates of the victim model in the adversarial examples crafted are summarized in \cref{fig:av_performance}.


The figure shows that audio-visual adversarial examples crafted on white-box models can partially deceive black-box models, confirming adversarial transferability in audio-visual learning. However, unlike in the image domain, momentum doesn't always improve performance. For instance, when using VGG as the audio backbone and AlexNet with a sum fusion layer, FGSM achieves a $31.5\%$ success rate, while MI-FGSM decreases performance by $2.3\%$, indicating momentum's negative impact on audio-visual transferability. This issue is common with models using sum fusion layers. In contrast, models with concat fusion layers show better black-box performance (+$3.6\%$). The results also suggest that improving input diversity with PAM helps momentum perform better over sum fusion layers. Our proposed methods (TIA, MMA, and TMA) consistently outperform others, with TIA exceeding PAM by $15.7\%$ and further improving by $5.2\%$ on average. Combining TIA and MMA achieves a $95.2\%$ success rate across all eight models.

% It should be noted that our proposed methods come from the rethinking of temporal consistency and modality alignment. These results not only verify the effectiveness of our proposed methods in boosting the audio-visual adversarial transferability but also provide another empirical proof for our arguments in audio-visual adversarial robustness discussed in \cref{sec:motivation}. 

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pdf/ensemble_attack_performance.pdf}
    \caption{Attack success rates (\%) of each of $8$ deep models under the ensemble setting. \textcolor{black}{TIA, MMA, and TMA are our proposed attack methods.} }
    \label{fig:ens_attack}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pdf/defense_attack_performance.pdf}
    \caption{Attack success rates (\%) of each of $8$ deep models on the adversarial examples crafted under the white-box setting with our proposed TMA method. }\label{fig:defense}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pdf/copies_abalation.pdf}
    \caption{Ablation study of the number of attack iterations on the attack performance. \textcolor{black}{TIA, MMA, and TMA are our proposed attack methods.}  }
    \label{fig:iteration_aba}
    \end{minipage}
\end{figure}


\subsection{Ensemble Model Attack}
% \input{figs/draw_ens_attack_performance}
% \input{figs/defense_performance}
There have been many works studying using the ensemble uni-modal model to craft the adversarial example to improve the adversarial transferability~\citep{DBLP:conf/cvpr/DongLPS0HL18}. Here, we conduct experiments to utilize the ensemble of multi-modal, \textit{i.e.}, the audio-visual models, to boost the performance. We iteratively select each one of the $8$ models as the victim black-box model and use the remaining $7$ models as surrogate models to craft the adversarial examples. We use the adversarial attack success rate against the victim model to evaluate the performance. 

As shown in \cref{fig:ens_attack}, we can notice that the performances of all selected methods are significantly boosted by the ensemble strategy,  even surpassing the performance under the white-box attacks. It indicates that \textit{different audio-visual models also share similar areas of interest.} By considering as many surrogate models as possible in attacks, we can fool the target victim model with a high success rate. The previous result shows that the audio-visual model with concat operation as the fusion layer is relatively robust against different attacks. Using the ensemble attacks, the FGSM has an average attack success rate of up to $79.9\%$ against these models, indicating the dimming performance of the concat fusion layer on defense.  It should be noted that all our proposed methods achieve an attack success rate of $100\%$ on all victim models, sufficiently demonstrating the effectiveness.  





\subsection{Attacking Defense Models}


Our proposed attack methods, including TIA, MMA, and TMA, have achieved the best attack performance on eight normally trained audio-visual models with different backbones and fusion layers.  Recent studies on audio-visual learning proposed to mitigate the threat of audio-visual adversarial examples. In our work, we also propose a bag of novel tricks to enhance adversarial training for defense.  Here, to validate the effectiveness of these defenses, as well as show the power of our proposed attack method against the defense mechanism, we conduct white-box adversarial attacks on these eight audio-visual models. For defense methods, we select adversarial training (AT)~\citep{madry2018towards}, DCFL~\citep{DBLP:conf/cvpr/TianX21}, Mixup~\citep{DBLP:conf/icassp/LiQLHM22}, CRMT-AT~\citep{yang2023quantifying}, and the combination of our tricks (Ours). We use our strongest attack TMA to evaluate the robustness. To align the attack setting, we use $10$-step PGD adversarial training as the baseline.

The results are shown in \cref{fig:defense}. Among the different defense methods, training the models on the adversarial examples is more efficient. We can see the adversarial training-based methods, including AT, CRMT-AT, and ours, perform much better than non-adversarial training methods with a clear gap of $10.9\%$. By integrating our proposed techniques, including the efficient adversarial perturbation crafting and curriculum adversarial training, our adversarial training can further boost the robustness by $2.28\%$ on average versus the runner-up method CRMT-AT. 


\iffalse
\noindent \textbf{Time consumption}. With the help of our proposed training trick, especially the temporal invariant universal perturbation, our method enjoys a speed increase of $3.70 \times$ compared to the vanilla adversarial training method on average ($17.8$ hours \textit{v.s.}~$65.8$ hours). Our method is also faster than CRMT-AT ($67.9$ hours).
\fi

\iffalse
On the basis of previous results, there are some interesting insights derived from our experiments on the evaluation of attack performance. We summarize them in the following box.  

\begin{tcolorbox}[colframe=brown,colback=white, title = {Insights  about the adversarial transferability}]
(1) The adversarial perturbation is easier to transfer between models with the same audio and visual backbones but with different fusion layers.   

(2) The generated adversarial example is more transferable by attacking the robust audio-visual, $\textit{e.g.}$, audio-visual models with the concat fusion layer. 
 

\end{tcolorbox}

\fi







\subsection{Ablation Study}

\noindent \textbf{On the number of iterations for the attack}. In experiments, we find that the multistep FGSM, \textit{i.e.}, I-FGSM, cannot always beat FGSM under both white- and black-box settings.  This raises the question of whether the number of iterations impacts the adversarial transferability in audio-visual learning. This motivates us to do the ablation study of the number of iterations in attacks.\footnote{With the number of iterations as $1$, the MI-FGSM and I-FGSM degrade to the FGSM.}  As shown in \cref{fig:iteration_aba}, by generating the adversarial examples on the audio-visual model (AcV), we can see that the number of iterations greatly impacts the attack performance. With increasing the number of iterations, the adversarial transferability of I-FGSM is degraded. While the use of momentum (MI-FGSM) can alleviate the overfitting to the surrogate model, a sufficiently large number of iterations still leads to getting stuck in a local optimum, degrading the performance. The use of input transformations (PAM) can improve the input diversity for better capturing the robust feature, thus boosting the adversarial transferability, but still limited.  Our proposed temporal invariant and modality misalignment attack methods sufficiently help the optimization jump over the local optima, thus significantly boosting the performance.  







\input{figs/exp_fig} \noindent \textbf{On the sampling ratio for the adversarial training}. In our approaches, we propose to sample a ratio of frames to generate the temporal-universal adversarial perturbation for efficient adversarial training. There is a balance between the time consumption of adversarial training and the adversarial robustness by selecting different sample ratios. Here, we conduct an ablation study on the sampling ratio. As shown in \cref{tab:sam_aba},  with increasing the sampling ratio in adversarial training, the defense performance is improved, but it also leads to more time consumption, supporting our argument on the balance.  At the same time, a larger sampling ratio does not improve the adversarial robustness much but introduces more time consumption, and a smaller sampling ratio harms defense performance. 

\iffalse
\begin{table}[t]
    \centering
     \caption{Ablation study of the sampling ratio on the performance of adversarial training. \textcolor{black}{We use our proposed TMA to generate the adversarial example for adversarial training.} }
    \label{tab:sam_aba}
    \begin{tabular}{c|cccccccc}
    \toprule
         Sampling ratio & 5\% & 10\% & 15\% &20\% & 25\% & \textcolor{black}{30\%} & \textcolor{black}{35\%}  &\textcolor{black}{40\%} \\\hline 
         Successful defense rate (\%) &72.1  & 75.4 & 81.5 & 82.0 &  82.6 & \textcolor{black}{82.9}& \textcolor{black}{83.0}& \textcolor{black}{83.0}\\
         Training time (Hours) & 6.2 & 9.3 & 17.8 & 18.9 & 21.3 &  \textcolor{black}{26.4} & \textcolor{black}{31.8} & \textcolor{black}{34.2}   \\
    \bottomrule
    \end{tabular}
\end{table}
\fi










\section{Conclusion}
In this work, we developed two efficient audio-visual adversarial attack methods: the temporal invariance-based attack and the modality misalignment-based attack. We also introduced an adversarial training framework with strategies for efficient perturbation crafting and curriculum training to reduce time and improve robustness.

Results on the Kinetics-Sounds dataset show that our attacks effectively benchmark audio-visual model robustness and that our training framework improves both robustness and efficiency. Our experiments provided new insights, including the importance of temporal consistency, modality alignment, and the concat fusion layer's robustness. We hope this research will serve as a benchmark for audio-visual robustness and inspire further exploration of AI security in multi-modal data.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}


\appendix
\newpage
\section{Results on MIT-MUSIC dataset}
\noindent \textbf{Setup}. We use the MIT-MUSIC~\citep{zhao2018sound} for further verification, which contains $685$ videos of musical solos and duets. It consists of  $11$ categories. We split the dataset into $7:1:2$ as the train, validation and test set. Following the same setting in our paper, we train $8$ audio-visual models on the MIT-MUSIC dataset and respectively use one surrogate model to attack others under the black-box setting. We use FGSM, I-FGSM, MI-FGSM, MIG, and PAM as our baselines, and compare the attack performance with our proposed methods, including TIA, MMA, and TMA. For defense methods, we select AT, DCFL, Mixup, CRMT-AT, and the baselines. 


\noindent \textbf{Results}. We first use various attack methods to generate adversarial examples by attacking AcR under the white-box setting, and then attack other models under the black-box setting. As shown in \cref{fig:music_attack}, our proposed TIA, MMA, and TMA achieve state-of-the-art attack performance among the selected  methods. Specifically, TIA achieves an average attack success rate of $95.2\%$, MMA achieves an average attack success rate of $93.7\%$, and the combination method TMA achieves an average attack success rate of $97.1\%$, while that of the runner-up method is $85.1\%$. We can also see a consistent phenomenon that the adversarial example is easier to transfer between similar architecture, \textit{i.e.}, from AcR to AsR.  

\textcolor{black}{We also evaluate the defense performance of our method. The results are depicted in \cref{fig:music_defense}.  Our proposed method surpasses the baseline methods with a clear margin of $2.2\%$ on defending the adversarial attack versus the runner-up method CRMT-AT.} 







\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pdf/music_attack_performance.pdf}
    \caption{Attack success rates (\%) of eight deep models, where the adversarial examples are generated on the white-box surrogate model and attack all models (one white-box model and seven black-box models).}
        \label{fig:music_attack}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/pdf/music_defense_attack_performance.pdf}
   \caption{Attack success rates (\%) of each of 8 deep models on the adversarial examples crafted under the white-box setting with our proposed TMA method.}
        \label{fig:music_defense}
    \end{minipage}
\end{figure}



\color{black}{

\section{Ablation study on the adversarial curriculum training}
In our paper, we propose adversarial curriculum training to enhance the performance of adversarial training. This approach includes both data-level and model-level strategies. To study the influence of the scheduler used to adjust the masking ratio (i.e., $\rho_x$ and $\rho_f$), we conduct additional experiments to evaluate the impact of different schedulers on audio-visual robustness. These schedulers are applied to boost the audio-visual adversarial robustness of RsV. Details are as follows:

\begin{itemize}
    \item \textbf{None}: This represents standard adversarial training without employing the adversarial curriculum training strategy.
    \item \textbf{D*M: 5/20/30\%}: These represent constant ratios used in the data- and model-centric strategies. For example:
    \begin{itemize}
        \item \textbf{D*M: 5\%} indicates that 5\% of the audio-visual frames and 5\% of the model parameters are randomly masked to generate adversarial examples. This setup provides a relatively simple defense scenario for the model.
        \item \textbf{D*M: 20\%} means that 20\% of the frames and 20\% of the parameters are used to generate adversarial examples. This setup creates a stronger adversarial attack and increases the difficulty of adversarial training.
        \item \textbf{D*M: 30\%} follows a similar logic with a higher masking ratio for stronger adversarial perturbations.
    \end{itemize}
    \item \textbf{D: 20\%}: This indicates that only 20\% of the frames are randomly sampled for adversarial example generation during adversarial training, without masking model parameters.
    \item \textbf{M: 20\%}: This indicates that 20\% of the model parameters are randomly masked in each iteration for adversarial example generation, without randomly dropping frames.
    \item \textbf{M: 40\%}: This indicates that 40\% of the model parameters are randomly masked in each iteration for adversarial example generation, without randomly dropping frames.
    \item \textbf{Linear}: This applies a linear scheduler from 5\% to 20\% for both data sampling and model masking ratios.
    \item \textbf{Cosine}: This applies a cosine scheduler from 5\% to 20\% for both data sampling and model masking ratios.
\end{itemize}\input{figs/sched}


The results are shown in \cref{fig:shed_adv_training}.  Compared with vanilla adversarial training, applying the proposed schedulers in adversarial curriculum training enhances adversarial robustness. For instance, when we synchronize masking at 20\% on both data and model parameters (D*M: 20\%), the success defense rate improves by 3.8\%. Furthermore, integrating linear and cosine schedulers to dynamically adjust the difficulty of adversarial examples during training leads to additional improvements of 0.8\% and 1.4\%, respectively.

It is worth noting that a lower sampling ratio for frames combined with a higher parameter masking ratio can negatively impact the quality of the generated adversarial examples. This, in turn, degrades adversarial robustness, as demonstrated by the results for D*M: 5\% and M: 40\%.



\section{The difference between Vision-language and Audio-visual attacks/defense}
There are certain similarities between audio-visual attack/defense and vision-language attack/defense methods~\citep{zhang2022towards,lu2023set}, as both require consideration of alignment and consistency between the two modalities. However, there are also notable differences between them. (1) Task difference: vision-language attacks aim at attacking content retrieval-related problems while audio-visual attacks focus on classification problems. (2) Operation difference: vision-language attacks perturb input data by optimizing latent embeddings while our method perturbs input by adjusting output logits. (3) Modality difference: vision-language attacks focus on static images while our approach considers the temporal redundancy of dynamic videos. This redundancy motivates our design of curriculum training to exploit sparsity, enhancing adversarial robustness while improving training efficiency.

Considering these reasons, we do not apply our proposed methods to the vision-language domain. However, we believe these works in audio-visual learning are inspiring for other multi-modal areas and we think exploring audio-visual attacks from the perspectives of model pretraining, modality alignment, and content retrieval is a valuable future direction.

%There are notable similarities and differences between vision-language and audio-visual attack/defense methods. Both domains require careful consideration of the alignment and consistency between the two modalities, as these factors are crucial for ensuring effective adversarial robustness. However, the nature of the modalities introduces key differences. In vision-language models, the two modalities provide rich and complementary information, which can be leveraged to enhance the robustness of the model. In contrast, audio-visual learning often involves temporal redundancy along the time axis, where many frames contain repetitive or non-informative data. This temporal sparsity motivates our adoption of a curriculum training strategy to exploit this characteristic, thereby improving adversarial robustness and training efficiency.

%Additionally, while prior studies~\citep{zhang2022towards,lu2023set} primarily explore adversarial attacks on pre-trained vision-language models, our defense method is constrained by limited access to pre-training data, which restricts the implementation of adversarial training in such models. Future research could focus on addressing these limitations by developing strategies to effectively incorporate adversarial training into pre-trained foundation models under data constraints, while also mitigating the challenges of catastrophic forgetting. 



\section{Demo Application on Attacking VideoLLaMA}
\input{figs/attackLLaMA} To showcase the scalability of our method in real-world applications, we employ our proposed attack method to generate audio-visual adversarial examples under an ensemble setting. These examples are then used to deceive VideoLLaMA2~\citep{cheng2024videollama}. Using the prompt, "Please point out the main object generating the sound based on the input video," we evaluate VideoLLaMA's responses. Three results are illustrated in \cref{fig:demo}, where all inputs are misidentified by VideoLLaMA. For instance, in the case of an airplane, VideoLLaMA incorrectly recognizes it as car racing. This demonstrates the vulnerability of current MLLMs to adversarial attacks, even when the adversarial examples are generated using conventional models.


For a quantitative evaluation, we generated a total of 100 audio-visual adversarial examples using our proposed TMA method and the best baseline method, PAM, respectively, and tested them on VideoLLaMA. While PAM successfully deceived VideoLLaMA in 41 cases, our proposed TMA achieved a higher success rate, with 74 examples successfully attacked. 




}

\end{document}
