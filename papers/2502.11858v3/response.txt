\section{Related Work}
\subsection{Audio-visual Learning}
The field of audio-visual learning encompasses a wide range of tasks, including audio-visual event recognition**Zhou et al., "Audio-Visual Event Recognition"**, separation**Deng et al., "Audio-Visual Separation"**, localization**Chen et al., "Audio-Visual Localization"**, correspondence learning**Li et al., "Audio-Visual Correspondence Learning"**, representation learning**Wang et al., "Audio-Visual Representation Learning"**, and cross-modal generation**Kim et al., "Cross-Modal Generation"**. Among these, audio-visual event recognition stands out as a fundamental task**Zhou et al., "Audio-Visual Event Recognition"** that has attracted significant research attention, particularly regarding robustness and security issues**Chen et al., "Robustness and Security Issues in Audio-Visual Event Recognition"**.

Deep learning models employed in audio-visual event recognition typically comprise three main components: visual encoder, audio encoder, and fusion layer. Although prior research has extensively focused on optimizing these components to enhance task performance, there has been limited consideration of their implications for security and robustness. 

In this work, we delve into the individual components of these models and examine their respective impacts on robustness, shedding light on crucial but often overlooked aspects.





\subsection{Adversarial Attack \& Defense}




Research efforts in adversarial robustness for audio-visual models have been relatively limited. Tian et al., "Audio-Visual Integration for Robustness Enhancement"** were among the first to explore the potential of audio-visual integration in enhancing robustness against multi-modal attacks. Yang et al., "Adversarially Robust Audio-Visual Fusion Layer"** proposed an adversarially robust audio-visual fusion layer to defend against single-source adversarial attacks. Li et al., "Mix-Up Strategy for Audio-Visual Models"** introduced a novel mix-up strategy in the audio-visual fusion layer to improve the robustness of audio-visual models. Yang et al., "Certified Robust Training Method"** proposed a certified robust training method to boost the multi-modal robustness. 
However, they primarily focused on adapting single-modality adversarial attacks to audio-visual scenes. There remains a critical need for powerful audio-visual adversarial attacks that can serve as benchmarks for evaluating the adversarial robustness of audio-visual methods and the effectiveness of robust training techniques.

In this work, we address the gap by designing an effective audio-visual adversarial attack method that facilitates a more comprehensive assessment of model robustness. We further propose an efficient defense technique to enhance the robustness of audio-visual models against adversarial attacks.