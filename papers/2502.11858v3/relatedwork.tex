\section{Related Work}
\subsection{Audio-visual Learning}
The field of audio-visual learning encompasses a wide range of tasks, including audio-visual event recognition~\citep{brousmiche2019audio,xia2022cross,brousmiche2022multimodal}, separation~\citep{wu2019time, majumder2021move2hear,majumder2022active,huang2023davis}, localization~\citep{wu2019dual, wu2021binaural,huang2023egocentric}, correspondence learning~\citep{min2020multimodal,zhu2021learning,morgado2021robust}, representation learning~\citep{zhou2019talking,cheng2020look,rahman2021tribert}, and cross-modal generation~\citep{chen2017deep,hao2018cmcgan, sung2023sound,liang2023av,liang2024language,huang2024scaling,liang2023neural}. Among these, audio-visual event recognition stands out as a fundamental task~\citep{gao2024audio} that has attracted significant research attention, particularly regarding robustness and security issues~\citep{yang2023quantifying}.

Deep learning models employed in audio-visual event recognition typically comprise three main components: visual encoder, audio encoder, and fusion layer. Although prior research has extensively focused on optimizing these components to enhance task performance, there has been limited consideration of their implications for security and robustness. 

In this work, we delve into the individual components of these models and examine their respective impacts on robustness, shedding light on crucial but often overlooked aspects.





\subsection{Adversarial Attack \& Defense}




Research efforts in adversarial robustness for audio-visual models have been relatively limited. Tian \textit{et al.}~\citep{DBLP:conf/cvpr/TianX21} were among the first to explore the potential of audio-visual integration in enhancing robustness against multi-modal attacks. Yang \textit{et al.}\citep{DBLP:conf/cvpr/YangLBCK21} proposed an adversarially robust audio-visual fusion layer to defend against single-source adversarial attacks. Li \textit{et al.} \citep{DBLP:conf/icassp/LiQLHM22} introduced a novel mix-up strategy in the audio-visual fusion layer to improve the robustness of audio-visual models. Yang \textit{et al.} \citep{yang2023quantifying} proposed a certified robust training method to boost the multi-modal robustness. 
However, they primarily focused on adapting single-modality adversarial attacks to audio-visual scenes. There remains a critical need for powerful audio-visual adversarial attacks that can serve as benchmarks for evaluating the adversarial robustness of audio-visual methods and the effectiveness of robust training techniques.

In this work, we address the gap by designing an effective audio-visual adversarial attack method that facilitates a more comprehensive assessment of model robustness. We further propose an efficient defense technique to enhance the robustness of audio-visual models against adversarial attacks.