%% 
%% Copyright 2007-2025 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 272 2025-01-09 17:36:26Z rishi $
%%
%% \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
% \documentclass[times,review,10pt]{elsarticle}
\documentclass[review,3p,12pt]{elsarticle} % arxiv

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
% \usepackage{lineno}
% \linenumbers

\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue,
            filecolor=magenta,
            urlcolor=blue]{hyperref}
\usepackage{url}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage[font=normalsize,skip=0pt]{caption}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\DeclareMathOperator*{\dprime}{{\prime\prime}}

% \journal{Pattern Recognition}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Structure-preserving contrastive learning for spatial time series}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[label1,label3]{Yiru Jiao} %% Author name
\author[label2,label3]{Sander van Cranenburgh}
\author[label1,label3]{Simeon Calvert}
\author[label1,label3]{Hans van Lint}

%% Author affiliation
\affiliation[label1]{organization={Department of Transport\&Planning, Delft University of Technology},%Department and Organization
            % addressline={Stevinweg 1}, 
            city={Delft},
            % postcode={2628 CN}, 
            % state={Zuid-Holland},
            country={the Netherlands}
            }
\affiliation[label2]{organization={Department of Engineering Systems and Services, Delft University of Technology},%Department and Organization
            % addressline={Jaffalaan 5}, 
            city={Delft},
            % postcode={2628 BX}, 
            % state={Zuid-Holland},
            country={the Netherlands}
            }
\affiliation[label3]{organization={CityAI lab, Delft University of Technology},%Department and Organization
            % addressline={Van Mourik Broekmanweg 6}, 
            city={Delft},
            % postcode={2628 XE}, 
            % state={Zuid-Holland},
            country={the Netherlands}}

%% Abstract
\begin{abstract}
%% Text of abstract
Informative representations enhance model performance and generalisability in downstream tasks. However, learning self-supervised representations for spatially characterised time series, like traffic interactions, poses challenges as it requires maintaining fine-grained similarity relations in the latent space. In this study, we incorporate two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance contrastive learning and structure preservation, we propose a dynamic mechanism that adaptively weighs the trade-off and stabilises training. We conduct experiments on multivariate time series classification, as well as macroscopic and microscopic traffic prediction. For all three tasks, our approach preserves the structures of similarity relations more effectively and improves state-of-the-art task performances. The proposed approach can be applied to an arbitrary encoder and is particularly beneficial for time series with spatial or geographical features. Furthermore, this study suggests that higher similarity structure preservation indicates more informative and useful representations. This may help to understand the contribution of representation learning in pattern recognition with neural networks.
Our code is made openly accessible with all resulting data at \url{https://github.com/yiru-jiao/spclt}.

\vspace{10pt}

\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Contrastive learning \sep similarity preservation \sep time series \sep spatio-temporal data \sep traffic interaction
%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

\newpage
%///////////////////////
\section{Introduction}\label{sec: introduction}
%///////////////////////
Self-supervised representation learning (SSRL) theoretically can learn latent embeddings that facilitate downstream tasks \cite{saunshi2019theoretical,haochen2021,ge2024pretraining,Duan2025}. Also, it is practically shown to improve model generalisability \cite{tendle2021study,zhou2022domain}. The latter is in particular valuable for real-world applications, where both measurements and labels are often uncertain and unreliable. In fact, SSRL has been widely applied across fields such as computer vision, natural language processing, and recommendation systems. There are many literature reviews, to name a few, \cite{schiappa2023self,liu2023self,yu2023self}. 

In SSRL of time series, contrastive learning is becoming the mainstay technique, contributing to applications in, e.g., anomaly detection \cite{Kim2024,Darban2025}, robotics \cite{Zheng2023,Liu2025sgclr}, and healthcare \cite{Liu2025semi}. This widespread adoption is supported by empirical investigation. In 2022, Lafabregue et al. \cite{lafabregue2022end} conducted an extensive experimental comparison over 300 combinations of network architectures and loss functions to evaluate the performance of time series representation learning. One of their key findings is that the reconstruction loss used by traditional autoencoders does not sufficiently fit temporal patterns. Instead, contrastive learning has emerged as a more effective approach, which embeds similar samples closer together while dissimilar samples farther apart in the latent space \cite{wu2023cl,yang2024negative}. 

Unique challenges arise when learning contrastive representations for \textit{spatial} time series. First, data with both temporal and spatial characteristics demand more fine-grained similarity comparisons, which underpins contrastive learning. Financial time series may be considered similar even if some variables show significant divergence, while movement traces with very different spatial features can be anything but similar. Second, effective representation of spatial time series needs to capture spatio-temporal patterns at the certain scale required by a practical task. For example, traffic interactions involve two different spatial scales: at the macroscopic scale, traffic flow measures collective road usage evolving over the road network; at the microscopic scale, trajectories describe the motion dynamics of individual road users such as car drivers, cyclists, and pedestrians, in local road space.

To address the challenges, we incorporate two regularisers \textit{at different scales} to \textit{preserve the original similarity structure} in the latent space for time series contrastive learning. One is a topology-preserving regulariser for the global scale, and the other is a graph-geometry-preserving regulariser for the local scale. This incorporation can be simplified as a weighted loss $\gL=\eta_\text{CLT}\cdot\ell_\text{CLT}+\eta_\text{SP}\cdot\ell_\text{SP}+r_{\boldsymbol{\eta}}$, where we propose a mechanism to dynamically balance the weights $\eta_\text{CLT}$ of contrastive learning for time series (CLT) and $\eta_\text{SP}$ of structure preservation (SP). Within this mechanism, the adaptive trade-off between contrastive learning and structure preservation is based on the uncertainties of their corresponding terms $\ell_\text{CLT}$ and $\ell_\text{SP}$; meanwhile, the term $r_{\boldsymbol{\eta}}$ adds regularisation against overfitting of the dynamic weights.

The proposed approach is applicable to spatial time series in general, and we consider traffic interaction as a specific case in this paper. To validate the approach, we conduct experiments on tasks of 1) multivariate time series classification, where we benchmark against the current state-of-the-art (SOTA) models, i.e., \cite{Yue2022ts2vec} and \cite{lee2024softclt}; and 2) traffic prediction, where we use \cite{Li2024macro} for macroscopic benchmark and \cite{Li2024micro} for microscopic. Along these experiments, the efficiency of this approach is evaluated with various model architectures. In addition to performance improvement, we also investigate and discuss the importance of preserving similarity structure. Below is a summary of the key contributions in this study.
%
\begin{itemize}
    \item We introduce an approach that incorporates structure-preserving regularisation in contrastive learning of multivariate time series, to maintain finer-grained similarity relations in the latent space of sample representations. We propose a dynamic weighing mechanism to adaptively balance contrastive learning and structure preservation during training. This approach can be applied to an arbitrary encoder for more effective self-supervised representation learning. 
    \item Preserving similarity structure can enhance SOTA performance on various downstream tasks. The \textit{relative improvement} on spatial datasets in the UEA archive is 2.96\% in average classification accuracy; on macroscopic traffic prediction task is 0.57\% in flow speed MAE and 0.55\% in the standard deviation of prediction errors; on microscopic trajectory prediction task is 1.87\% and 3.40\% in missing rates under radii of 0.5 m and 1 m, respectively.
    \item Considering neural network pattern recognition as learning the conditional probability distribution of outputs over inputs, the similarity structure hidden in the input data implies the distribution of conditions. Our approach is therefore important to preserve the original distribution in the latent space for more effective representational learning. This is particularly beneficial for time-series data with spatial or geographical features as they require finer-grained information, such as in robotics, meteorology, remote sensing, urban planning, etc.
\end{itemize}

The rest of this paper is organised as follows. In Section \ref{sec: related work}, we briefly review related works in the literature. We use Section \ref{sec: methods} to systematically introduce the methods. Then in Section \ref{sec: exps and results}, we present the demonstration experiments and according results. With Section \ref{sec: discussion}, we discuss the importance of preserving similarity structure for representation learning. Finally, Section \ref{sec: conclusion} concludes this study.


%///////////////////////
\section{Related work}\label{sec: related work}
%///////////////////////
%-----------------------
\subsection{Time series contrastive learning}
%-----------------------
Contrastive learning for time series data is a relatively young niche and is rapidly developing. The development has been dominantly focused on defining positive and negative samples. Early approaches construct positive and negative samples with subseries within time series \cite{franceschi2019tloss} and temporal neighbourhoods \cite{tonekaboni2021unsupervised}; and later methods create augmentations by transforming original series \cite{eldele2021tstcc,eldele2023}. More recently, \cite{Yue2022ts2vec} generates random masks to enable both instance-wise and time-wise contextual representations at flexible hierarchical levels, which exceeds previous state-of-the-art performances (SOTAs). Given that not all negatives may be useful \cite{cai2020negative,Jeon2021}, \cite{liu2024timesurl} makes hard negatives to boost performance, while \cite{lee2024softclt} utilises soft contrastive learning to weigh sample pairs of varying similarities, both of which reach new SOTAs. 

The preceding paragraph outlines a brief summary, and we refer the readers to Section 2 in \cite{lee2024softclt} and Section 5.3 in \cite{trirat2024universal} for a detailed overview of the methods proposed in the past 6 years. These advances have led to increasingly sophisticated methods that mine the contextual information embedded in time series by contrasting similarities. However, the structural details of similarity relations between samples remain to be explored. 

%-----------------------
\subsection{Structure-preserving SSRL}
%-----------------------
Preserving the original structure of data when mapping into a latent space has been widely and actively researched in manifold learning (for a literature review, see \cite{meila2024}) and graph representation learning \cite{ju2024,khoshraftar2024}. In manifold learning, which is also known as nonlinear dimension reduction, the focus is on revealing the geometric shape of data point clouds for visualisation, denoising, and interpretation. In graph representation learning, the focus is on maintaining the connectivity of nodes in the graph while compressing the data space required for large-scale graphs \cite{Yao2024}. Structure-preserving has not yet attracted much dedication to time series data. \cite{ashraf2023} provides a literature review on time series data dimensionality reduction, where none of the methods are specifically tailored for time series.

Zooming in within structure-preserving SSRL, there are two major branches respectively focusing on topology and geometry. Topology-preserving SSRL aims to maintain global properties such as clusters, loops, and voids in the latent space; representative models include \cite{moor2020topoae} and \cite{trofimov2023rtdae} using autoencoders, as well as \cite{madhu2023toposrl} and \cite{Chen2024topogcl} with contrastive learning. The other branch is geometry-preserving and focuses more on local shapes such as relative distances, angles, and areas. Geometry-preserving autoencoders include \cite{Nazari2023geomae} and \cite{lim2024ggae}, while \cite{li2022geomgcl} and \cite{Koishekenov2023} use contrastive learning. The aforementioned topology and geometry preserving autoencoders are all developed for dimensionality reduction; whereas the combination of contrastive learning and structure-preserving has been explored majorly with graphs.

%-----------------------
\subsection{Traffic interaction SSRL}
%-----------------------
In line with the conclusions in previous sub-sections, existing exploration in the context of traffic interaction data and tasks also predominantly relies on autoencoders and graphs. For instance, using a transformer-based multivariate time series autoencoder \cite{zerveas2021transformer},  \cite{lu2022learning} clusters traffic scenarios with trajectories of pairwise vehicles. Then a series of studies investigate masking strategies with autoencoders for individual trajectories and road networks, including \cite{Cheng2023mae,Chen2023trajmae,lan2024sept}. 

Leveraging data augmentation, \cite{Mao2022jointlycontrastive} utilises graphs and contrastive learning to jointly learn representations for vehicle trajectories and road networks. The authors design road segment positive samples as neighbours in the graph, and trajectory positive samples by replacing a random part with another path having the same origin and destination. Similarly, \cite{Zipfl2023trafficsimilarity} uses a graph-based contrastive learning approach to learn traffic scene similarity. The authors randomly modify the position and velocity of individual traffic participants in a scene to construct positive samples, with negative samples drawn uniformly from the rest of a training batch. Also using augmentation, \cite{Zheng2024longterm} focuses on capturing seasonal and holiday information for traffic prediction.

%///////////////////////
\section{Methods}\label{sec: methods}
%///////////////////////
This section begins by defining the problem of structure-preserving contrastive learning for spatial time series. Following that, we explain the overall loss function to be optimised, where we propose a dynamic weighing mechanism to balance contrastive learning and structure preservation during training. Then we present the contrastive learning loss for time series, unifying both hard and soft versions in a consistent format. Lastly, we introduce two structure-preserving regularisers, which are respectively adapted to maintain the global and local structure of similarity relations. 

%-----------------------
\subsection{Problem definition}
%-----------------------
We define the problem for general spatial time series, with traffic interaction as a specific case. Learning the representations of a set of samples $\{\vx_1, \vx_2,\cdots,\vx_N\}$ aims to obtain a nonlinear function $f_{\boldsymbol{\theta}}: \vx \rightarrow \vz$ that encodes each $\vx$ into $\vz$ in a latent space. Let $T$ denote the sequence length of a time series and $D$ the feature dimension at each timestamp $t$. The original space of $\vx$ can have the form $\sR^{T\times D}$, where spatial features are among the $D$ dimensions; or $\sR^{T\times S\times D}$, where $S$ represents spatially distributed objects (e.g., sensors or road users). The latent space of $\vz$ can also be structured in different forms, such as $\sR^P$, $\sR^{T\times P}$, or $\sR^{T\times S\times P}$, where $P$ is the dimension of encoded features. 

By contrastive learning, (dis)similar samples in the original space should remain close (far) in the latent space. Meanwhile, by structure preservation, the distance/similarity relations between samples should maintain certain features after mapping into the latent space. We use $d(\vx_i,\vx_j)$ to denote the distance between two samples $i$ and $j$, and this also applies to their encoded representations $\vz_i$ and $\vz_j$. Various distance measures can be used to define $d$, such as cosine distance (COS), Euclidean distance (EUC), and dynamic time warping (DTW). The smaller the distance between two samples, the more similar they are. Considering the limitation of storage efficiency, similarity comparison is performed in each mini-batch, where $B$ samples are randomly selected.

%-----------------------
\subsection{Trade-off between contrastive learning and structure preservation}
%-----------------------
We define the complete loss function for optimising $f_{\boldsymbol{\theta}}$ as shown in Equation (\ref{eq: combined_loss}). Referring to the simplified loss in Section \ref{sec: introduction}, i.e., $\gL=\eta_\text{CLT}\cdot\ell_\text{CLT}+\eta_\text{SP}\cdot\ell_\text{SP}+r_{\boldsymbol{\eta}}$, the contrastive learning loss for time series ($\gL_\text{CLT}$) and structure-preserving loss ($\gL_\text{SP}$) are modified using the function $x(1-\exp(-x))$ and correspond to $\ell_\text{CLT}$ and $\ell_\text{SP}$; $\eta_\text{CLT}$, $\eta_\text{SP}$, and $r_{\boldsymbol{\eta}}$ depend on two deviation terms $\sigma_\text{CLT}$ and $\sigma_\text{SP}$, which dynamically change during training.
%
\begin{equation}\label{eq: combined_loss}
    \gL=\frac{1}{2\sigma_\text{CLT}^2}\gL_\text{CLT}\left(1-\exp(-\gL_\text{CLT})\right)+\frac{1}{2\sigma_\text{SP}^2}\gL_\text{SP}\left(1-\exp(-\gL_\text{SP})\right)+\log\sigma_\text{CLT}\sigma_\text{SP}
\end{equation}

The modification by $x(1-\exp(-x))$ serves two purposes: it penalises negative values of $\gL_\text{SP}$, and stabilises training when either $\gL_\text{CLT}$ or $\gL_\text{SP}$ approaches its optimal value. While in computation the value of $\gL_\text{SP}$ sometimes is below zero, the losses used as $\gL_\text{CLT}$ and $\gL_\text{SP}$ in this study all have their theoretical optimal values of zero\footnote{We offer a more detailed analysis in \ref{sec: apdx_optimal_loss}.}. As $\gL_\text{CLT}$ decreases and approaches its optimum zero, the modified term $\ell_\text{CLT}$ has a slower decreasing rate when $\gL_\text{CLT}<1$. More specifically, the derivative of $x(1-\exp(-x))$ is $x^\prime(1-\exp(-x)(1-x))$, where $x^\prime$ denotes the derivative of $x$ and the multiplier in parentheses decreases from 1 to 0 while $x$ decreases from 1 to 0. This thus stabilises the training when $\gL_\text{CLT}$ approaches zero, and works the same for $\gL_\text{SP}$.

Inspired by \cite{Kendall2018multitask}, we then weigh the two modified losses by considering their uncertainties. The magnitudes of $\gL_\text{CLT}$ and $\gL_\text{SP}$ may vary with different datasets and hyperparameter settings. This variation precludes fixed weights for contrastive learning and structure preservation. We consider the loss values (denoted by $\ell$) as deviations from their optimal values, and learn adaptive weights according to the deviations. Given the optimal value of 0, we assume a Gaussian distribution of $\ell$ with standard deviation $\sigma$, i.e., $p(\ell)=\gN(0, \sigma^2)$. Then we can maximise the log likelihood $\sum\log p(\ell)=\sum(-\log 2\pi-\log \sigma^2-\ell^2/\sigma^2)/2$ to learn $\sigma$. This is equivalent to minimising $\sum\left(\ell^2/2/\sigma^2+\log\sigma\right)$. When balancing between two losses $\ell_\text{CLT}$ and $\ell_\text{SP}$ that have deviations $\sigma_\text{CLT}$ and $\sigma_\text{SP}$,  respectively, we need to use Equation (\ref{eq: balanced_loss}).  
%
\begin{equation}\label{eq: balanced_loss}
    \argmax -\sum\log p(\ell_\text{CLT})p(\ell_\text{SP})\Leftrightarrow \argmin \sum\left(\frac{1}{2\sigma_\text{CLT}^2}\ell_\text{CLT}+\frac{1}{2\sigma_\text{SP}^2}\ell_\text{SP}+\log\sigma_\text{CLT}\sigma_\text{SP}\right)
\end{equation}

Replacing $\ell_\text{CLT}$ in Equation (\ref{eq: balanced_loss}) with $\gL_\text{CLT}\left(1-\exp(-\gL_\text{CLT})\right)$ and $\ell_\text{SP}$ with $\gL_\text{SP}\left(1-\exp(-\gL_\text{SP})\right)$, Equation (\ref{eq: combined_loss}) is then derived to be the complete loss. The training process trades-off between $\gL_\text{CLT}$ and $\gL_\text{SP}$, as well as between the weight regulariser $r_{\boldsymbol{\eta}}=\log\sigma_\text{CLT}\sigma_\text{SP}$ and the rest of Equation (\ref{eq: combined_loss}). When $\gL_\text{CLT}$ is small and $\gL_\text{SP}$ is large, $\sigma_\text{CLT}$ becomes small and $\sigma_\text{SP}$ becomes large, which then increases the weight for $\gL_\text{CLT}$ while reduces the weight for $\gL_\text{SP}$. The reverse occurs when $\gL_\text{CLT}$ is large and $\gL_\text{SP}$ is small. As the weighted sum of $\gL_\text{CLT}$ and $\gL_\text{SP}$ increases by larger weights, $\log\sigma_\text{CLT}\sigma_\text{SP}$ decreases and discourages the increase from being too much. Similarly, if the weighted sum decreases by smaller weights, $\log\sigma_\text{CLT}\sigma_\text{SP}$ also regularises the decrease.

%-----------------------
\subsection{Contrastive learning loss}
%-----------------------
In this study, we use the time series contrastive learning loss introduced in TS2Vec \cite{Yue2022ts2vec} and its succeeder SoftCLT \cite{lee2024softclt} that utilises soft weights for similarity comparison\footnote{We write the loss function equations in this sub-section following the code open-sourced with the original papers; as such, they are slightly adjusted from those in the original papers.}. For each sample $\vx_i$, two augmentations are created by timestamp masking and random cropping, and then encoded as two representations $\vz_i^\prime$ and $\vz_i^{\dprime}$. TS2Vec and SoftCLT losses consider the same sum of similarities for a sample $i$ at a timestamp $t$, as shown in Equations (\ref{eq: sum_inst}) and (\ref{eq: sum_temp}). Equation (\ref{eq: sum_inst}) is used for instance-wise contrasting, which we denote by the subscript $_{\text{inst}}$; Equation (\ref{eq: sum_temp}) is used for time-wise contrasting, denoted by the subscript $_{\text{temp}}$.
%
\begin{equation}\label{eq: sum_inst}
    S_\text{inst}(i,t) = \sum_{j=1}^B\left(\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{j,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^\prime_{j,t})\right)+\sum_{\substack{j=1\\j\neq i}}^B\left(\exp(\vz^\prime_{i,t}\cdot \vz^\prime_{j,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\dprime}_{j,t})\right)
\end{equation}

\begin{equation}\label{eq: sum_temp}
    S_\text{temp}(i,t) = \sum_{s=1}^T\left(\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{i,s})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^\prime_{i,s})\right)+\sum_{\substack{s=1\\s\neq t}}^T\left(\exp(\vz^\prime_{i,t}\cdot \vz^\prime_{i,s})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\dprime}_{i,s})\right)
\end{equation}

Equation (\ref{eq: ts2vec_loss}) then shows the TS2Vec loss. We refer the readers to \cite{Yue2022ts2vec} for more details about the hierarchical contrasting method.
%
\begin{equation}\label{eq: ts2vec_loss}
\begin{aligned}
    &\gL_\text{TS2Vec} = \frac{1}{NT}\sum_i\sum_t\left(\ell_{\substack{\text{inst}\\\text{TS2Vec}}}^{(i,t)}+\ell_{\substack{\text{temp}\\\text{TS2Vec}}}^{(i,t)}\right),\\
    \text{where } &
    \begin{cases}
        \displaystyle \ell_{\substack{\text{inst}\\\text{TS2Vec}}}^{(i,t)}=-\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{i,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\prime}_{i,t})}{S_\text{inst}(i,t)}\\
        \displaystyle \ell_{\substack{\text{temp}\\\text{TS2Vec}}}^{(i,t)}=-\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{i,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\prime}_{i,t})}{S_\text{temp}(i,t)}
    \end{cases}
\end{aligned}
\end{equation}

Similarity comparison in TS2Vec is between two different augmentations for the same sample. This is expanded by SoftCLT to also involve other samples in the same mini-batch. Varying instance-wise and time-wise weights are assigned to different comparison pairs as soft assignments, with Equations (\ref{eq: weight_inst}) and (\ref{eq: weight_temp}). This introduces four hyperparameters, i.e., $\tau_\text{inst}$, $\tau_\text{temp}$, $\alpha$, and $m$. We use DTW to compute $d(\vx_i,\vx_j)$ and set $\alpha=0.5$, as recommended in the original paper; the other parameters need to be tuned for different datasets. Specifically, $m$ controls the sharpness of time hierarchical contrasting. TS2Vec uses $m=1$ (constant) and SoftCLT uses $m(k)=2^k$ (exponential), where $k$ is the depth of pooling layers when computing temporal loss. In this study, we add one more option $m(k)=k+1$ (linear), and will tune the best way for different datasets.
%
\begin{equation}\label{eq: weight_inst}
    w_\text{inst}(i,j) = \frac{2\alpha}{1+\exp(\tau_\text{inst}\cdot d(\vx_i,\vx_j)))}+
    \begin{cases}
        1-\alpha, &\text{if } i=j \\
        0, &\text{if } i\neq j
    \end{cases}
\end{equation}

\begin{equation}\label{eq: weight_temp}
    w_\text{temp}(t,s) = \frac{2}{1+\exp(\tau_\text{temp}\cdot m\cdot|t-s|)}
\end{equation}

Then Equation (\ref{eq: softclt_loss}) shows the SoftCLT loss, where we let $\lambda$ be 0.5 as recommended in the original paper. For a more detailed explanation and analysis, we refer the readers to \cite{lee2024softclt}.
%
\begin{equation}\label{eq: softclt_loss}
\begin{aligned}
    &\gL_\text{SoftCLT} = \frac{1}{NT}\sum_i\sum_t\left(\lambda\ell_{\substack{\text{inst}\\\text{SoftCLT}}}^{(i,t)}+(1-\lambda)\ell_{\substack{\text{temp}\\\text{SoftCLT}}}^{(i,t)}\right), \\
    \text{where } &
    \begin{cases}
        \begin{aligned}
            \ell_{\substack{\text{inst}\\\text{SoftCLT}}}^{(i,t)} &= -\sum_{j=1}^B w_\text{inst}(i,j)\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{j,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\prime}_{j,t})}{S_\text{inst}(i,t)} \\
            &\quad - \sum_{\substack{j=1\\j\neq i}}^B w_\text{inst}(i,j)\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\prime}_{j,t})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\dprime}_{j,t})}{S_\text{inst}(i,t)}
        \end{aligned} \\
        \begin{aligned}
            \ell_{\substack{\text{temp}\\\text{SoftCLT}}}^{(i,t)} &= -\sum_{s=1}^T w_\text{temp}(t,s)\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\dprime}_{i,s})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\prime}_{i,s})}{S_\text{temp}(i,t)} \\
            &\quad - \sum_{\substack{s=1\\s\neq t}}^T w_\text{temp}(t,s)\log\frac{\exp(\vz^\prime_{i,t}\cdot \vz^{\prime}_{i,s})+\exp(\vz^{\dprime}_{i,t}\cdot \vz^{\dprime}_{i,s})}{S_\text{temp}(i,t)}
        \end{aligned}        
    \end{cases}
\end{aligned}
\end{equation}

%-----------------------
\subsection{Structure-preserving regularisers}
%-----------------------
We use the topology-preserving loss proposed in \cite{moor2020topoae} and the graph-geometry-preserving loss proposed in \cite{lim2024ggae} as two structure-preserving regularisers, respectively focusing on the global and local structure of similarity relations. The global structure is preserved for instance-wise comparison, and the local structure is preserved for comparison across temporal or spatial features. In the following, we briefly describe the two losses, and the readers are referred to the original papers for more details.

Equation (\ref{eq: topo_loss}) presents the topology-preserving loss computed in each mini-batch. Here $\mA$ is a $B\times B$ EUC distance matrix between the samples in a batch, and is used to construct the Vietoris-Rips complex; $\boldsymbol{\pi}$ represents the persistence pairing indices of simplices that are considered topologically significant. The superscripts $X$ and $Z$ indicate original data space and latent space, respectively. 
%
\begin{equation}\label{eq: topo_loss}
    \gL_\text{Topo}=\frac{1}{2}\left\|\mA^X\left[\boldsymbol{\pi}^X\right]-\mA^Z\left[\boldsymbol{\pi}^X\right]\right\|^2+\frac{1}{2}\left\|\mA^Z\left[\boldsymbol{\pi}^Z\right]-\mA^X\left[\boldsymbol{\pi}^Z\right]\right\|^2
\end{equation}

The graph-geometry-preserving loss is also computed per mini-batch, as is shown in Equation (\ref{eq: ggeo_loss}). This loss measures geometry distortion, i.e., how much $f_{\boldsymbol{\theta}}$ deviates from being an isometry that preserves distances and angles. The geometry to be preserved of the original data manifold is implied by a similarity graph. To represent temporal and spatial characteristics, instead of using an instance as a node in the graph, we consider the nodes as timestamps or in a spatial dimension such as sensors or road users. Then the edges in the graph are defined by pairwise geodesic distances between nodes. 
\begin{equation}\label{eq: ggeo_loss}
    \gL_\text{GGeo}=\frac{1}{B}\sum_{i=1}^B \text{Tr}\left[\tilde{H}_i\left(L,\tilde{f}_\theta(\vx_i)\right)^2-2\tilde{H}_i\left(L,\tilde{f}_\theta(\vx_i)\right)\right],
\end{equation}
%
where $\tilde{H}_i$ represents an approximation of the Jacobian matrix of $f_{\boldsymbol{\theta}}$. Note that $\tilde{f}_\theta(\vx_i)$ as the latent representation of $\vx_i$ needs to maintain the node dimension. For example, if the nodes are considered as timestamps, $\tilde{f}_\theta(\vx_i)\in\sR^{T\times P}$; if the nodes are spatial objects, $\tilde{f}_\theta(\vx_i)\in\sR^{S\times P}$. With a similarity graph defined, then $L$ is the graph Laplacian that is approximated using a kernel matrix with width hyperparameter $h$, which requires tuning for different datasets.

%///////////////////////
\section{Experiments and results}\label{sec: exps and results}
%///////////////////////
%-----------------------
\subsection{Experiment setup}
%-----------------------
We compare 6 losses for self-supervised representation learning (SSRL) of time series: TS2Vec, SoftCLT, Topo-TS2Vec, GGeo-TS2Vec, Topo-SoftCLT, and GGeo-SoftCLT. Among the losses, TS2Vec \cite{Yue2022ts2vec} and SoftCLT \cite{lee2024softclt} are baselines, and the others extend these two with a topology-preserving or a graph-geometry-preserving regulariser. The comparison is then evaluated by downstream task performances using these differently encoded representations. Consequently, the comparison and evaluation serve as an extensive ablation study focusing on the effects of structure-preserving regularisers. Our experiments are conducted with an NVIDIA A100 GPU with 80GB RAM and 5 Intel Xeon CPUs. For fair comparisons, we control the following conditions during experiments: random seed, the space and strategy for hyperparameter search, maximum training epochs, early stopping criteria, and samples used for evaluating local structure preservation.
% in \cite{DHPC2024}

%-----------------------
\subsubsection{Baselines and datasets}
%-----------------------
The evaluation of performance improvement is on 3 downstream tasks: multivariate time series classification, macroscopic traffic prediction, and microscopic traffic prediction. For every downstream task, we split training/(validation)/test sets following the baseline study and make sure the same data are used across models. Each experiment for a task has two stages, of which the first is SSRL and the second uses the encoded representations to perform classification/prediction. Only the split training set is used in the first stage, with 25\% separated as an internal validation set to schedule the learning rate for SSRL.

The classification task is on 28 datasets\footnote{The UEA archive collects 30 datasets in total. We omitted the two largest, InsectWingbeat and PenDigits, due to limited computation resources.} retrieved from the UEA archive \cite{bagnall2018uea}. For each dataset, we set the representation dimension to 320 as used in the TS2Vec and SoftCLT studies, train 6 encoders with the 6 losses, and then classify the encoded representations with an RBF-kernel SVM. For traffic prediction, we use the dataset and model in \cite{Li2024macro} for the macroscopic baseline, and those in \cite{Li2024micro} for the microscopic baseline. The macroscopic traffic prediction uses 40 minutes (2-minute intervals) of historical data in 193 consecutive road segments to predict for all segments in the next 30 minutes. The microscopic traffic prediction forecasts the trajectory of an ego vehicle in 3 seconds, based on the history of up to 26 surrounding road users in the past 1 second (0.1-second intervals). Both traffic prediction baselines use encoder-decoder structures. We first pretrain the encoder with the 6 different losses for SSRL, and then fine-tune the complete model for prediction. The baseline trained from scratch is also compared.

To facilitate clearer analyses when presenting results, we divide the datasets included in the UEA archive into those with spatial features and those without. According to data descriptions in \cite{bagnall2018uea}, the UEA datasets are grouped into 6 categories: human activity recognition, motion classification, ECG classification, EEG/MEG classification, audio spectra classification, and other problems. The human activity and motion categories, along with the PEMS-SF and LSST datasets that are categorised as other problems, contain spatial features. We thus consider these as spatial, and the remaining datasets as non-spatial. As a result, each division includes 14 datasets.

%-----------------------
\subsubsection{Hyperparameters}
%-----------------------
For each dataset, we perform a grid search to find the parameters that minimise $\gL_\text{CLT}$ after a certain number of iterations, where we set a constant learning rate of 0.001. Table \ref{tab: para_search_space} shows the search spaces of various hyperparameters, where bs is abbreviated for batch size and lr$_\eta$ is a separate learning rate for dynamic weights. When searching for best-suited parameters, we first set them as default values, and then follow the search strategy presented in Table \ref{tab: para_search_strategy}.
%
\begin{table}[htbp]
\begin{minipage}{0.5\textwidth}
\renewcommand*\footnoterule{}
\begin{center}
\caption{Hyperparameter search space.}
\label{tab: para_search_space}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
 & Default & Search space \\ \midrule
bs & 8 & [8, 16, 32, 64] \footnote{Maximum bs does not exceed train size.} \\
lr$_\eta$ & 0.05 & [0.01, 0.05] \\
$h$ & 1 & [0.25, 1, 9, 25, 49] \\
$\tau_\text{temp}$ & 0 & [0.5, 1, 1.5, 2, 2.5] \\
$m$ & constant & [constant, linear, exponential] \\
$\tau_\text{inst}$ & 0 & [1, 3, 5, 10, 20] \\ \bottomrule
\multicolumn{3}{l}{bs: batch size; lr$_\eta$: learning rate for dynamic weights.}
\end{tabular}}
\end{center}
\end{minipage}%
\hfill
\begin{minipage}{.48\textwidth}
\begin{center}
\caption{Hyperparameter search strategy.}
\label{tab: para_search_strategy}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccc@{}}
\toprule
Stage & bs & lr$_\eta$ & $h$ & $\tau_\text{temp}$ & $m$ & $\tau_\text{inst}$ \\ \midrule
TS2Vec & $\triangle$ &  &  &  &  &  \\
Topo-TS2Vec & $\square$ & $\triangle$ &  &  &  &  \\
GGeo-TS2Vec & $\square$ & $\triangle$ & $\triangle$ &  &  &  \\
SoftCLT Phase 1 & $\bigcirc$ &  &  & $\triangle$ & $\triangle$ & $\bigcirc$ \\
SofrCLT Phase 2 & $\triangle$ &  &  & $\square$ & $\square$ & $\triangle$ \\
Topo-SoftCLT & $\square$ & $\triangle$ &  & $\square$ & $\square$ & $\square$ \\
GGeo-SoftCLT & $\square$ & $\triangle$ & $\triangle$ & $\square$ & $\square$ & $\square$ \\ 
\bottomrule
\multicolumn{7}{l}{$\bigcirc$: default; $\square$: inherited; $\triangle$: tuned.}
\end{tabular}}
\end{center}
\end{minipage}
\end{table}

The search spaces and strategy can result in up to 63 runs for one dataset. To save searching time, we adjust the number of iterations to be adequate to reflect the progress of loss reduction but limited to prevent overfitting, as our goal is to identify suitable parameters rather than fully train the models. The number of iterations is scaled according to the number of training samples, with larger datasets receiving more iterations. 

%-----------------------
\subsubsection{Evaluation metrics}
%-----------------------
Our performance evaluation uses both task-specific metrics and structure-preserving metrics. The former serves to validate performance improvements, while the latter serves to verify the effectiveness of preserving similarity structures. These metrics differ in whether a higher or lower value signifies better performance. To consistently indicate the best method, in the tables presented in the following sub-sections, the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.

For evaluating the classification task, we use accuracy (Acc.) and the area under the precision-recall curve (AUPRC). To evaluate macroscopic traffic prediction, we use mean absolute error (MAE), root mean squared error (RMSE), the standard deviation of prediction errors (SDEP), and the explained variance by prediction (EVar). Dealing with microscopic traffic, we predict vehicle trajectories and assess the minimum final displacement error (min. FDE) as well as missing rates under radius thresholds of 0.5 m, 1 m, and 2 m (MR$_{0.5}$, MR$_{1}$, and MR$_{2}$). 

As for metrics to evaluate structure preservation, we adopt a combination of those used in \cite{moor2020topoae} and \cite{lim2024ggae}. More specifically, we consider 1) kNN, the proportion of shared k-nearest neighbours according to distance matrices in the latent space and in the original space; 2) continuity (Cont.), one minus the proportion of neighbours in the original space that are no longer neighbours in the latent space; 3) trustworthiness (Trust.), the counterpart of continuity, measuring the proportion of neighbours in the latent space but not in the original space; 4) MRRE, the averaged error in the relative ranks of sample distances between in the latent and original space; and 5) distance matrix RMSE (dRMSE), the root mean squared difference between sample distance matrices in the latent and original space. We calculate these metrics at two scales to evaluate global and local structure preservation. For global evaluation, our calculation is based on the EUC distances between samples; for local evaluation, it is based on the EUC distances between timestamps in a sample for at most 500 samples in the test set. 

%-----------------------
\subsection{Multivariate time series classification}
%-----------------------
Table \ref{tab: uea_task} displays the classification performance on spatial and non-spatial UEA datasets. Next to the averaged accuracy, we also include the loss values on test sets to offer more information. More detailed results can be found in Tables \ref{tab: uea_spatial_details} and \ref{tab: uea_nonspatial_details} in \ref{sec: apdx_UEA_details}, where we present the classification accuracy with different representation learning losses for each dataset. Then we use Table \ref{tab: uea_improvement} to more specifically compare the relative improvements induced by adding a topology or graph-geometry preserving regulariser. The improvement is measured by the percentage of accuracy difference from the corresponding baseline performance.
%
\begin{table}[htbp]
\begin{minipage}{0.51\textwidth}
\begin{center}
\caption{UEA classification evaluation.}
\label{tab: uea_task}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clcccc@{}}
\toprule
Datasets & \multicolumn{1}{c}{Method} & Acc. & AUPRC & $\gL_\text{CLT}$ & $\gL_\text{SP}$ \\ \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}With\\ spatial\\ features\\ (14)\end{tabular}} & TS2Vec & 0.848 & 0.872 & 2.943 &  \\
 & Topo-TS2Vec & 0.851 & 0.876 & 2.264 & 0.085 \\
 & GGeo-TS2Vec & 0.856 & 0.881 & 2.200 & 186.9 \\
 & SoftCLT & 0.852 & 0.876 & 7.943 &  \\
 & Topo-SoftCLT & \textbf{0.862} & \textbf{0.882} & 4.900 & 0.087 \\
 & GGeo-SoftCLT & \textbf{\underline{0.864}} & \textbf{\underline{0.883}} & 2.316 & 221.1 \\ \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Without\\ spatial\\ features\\ (14)\end{tabular}} & TS2Vec & 0.523 & 0.555 & 8.417 &  \\
 & Topo-TS2Vec & \textbf{\underline{0.553}} & \textbf{0.561} & 11.12 & 0.122 \\
 & GGeo-TS2Vec & 0.536 & \textbf{\underline{0.564}} & 15.58 & 957.0 \\
 & SoftCLT & 0.508 & 0.532 & 4.714 &  \\
 & Topo-SoftCLT & 0.496 & 0.534 & 7.328 & 0.124 \\
 & GGeo-SoftCLT & \textbf{0.537} & 0.549 & 10.09 & 144.7 \\ 
\bottomrule
\multicolumn{6}{l}{\multirow{2}{*}{\begin{tabular}[l]{@{}r@{}}Note: the \textbf{\underline{best}} values are both bold and underlined;\\ the \textbf{second-best} values are bold.\end{tabular}}}
\end{tabular}}
\end{center}
\end{minipage}%
\hfill
\begin{minipage}{.45\textwidth}
\begin{center}
\caption{Classification accuracy improved by Topo/GGeo regulariser. The comparisons are made with corresponding baseline performances.}
\label{tab: uea_improvement}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Improvement\\ by method\end{tabular}}} & \multicolumn{3}{c}{Persentage in Acc. (\%)} \\ \cmidrule(l){3-5} 
 & \multicolumn{1}{c}{} & min. & mean & max. \\ \midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}With\\ spatial\\ features\\ (14)\end{tabular}} & Topo-TS2Vec & -4.403 & 0.800 & 16.54 \\
 & GGeo-TS2Vec & -3.783 & 1.143 & 10.44 \\
 & Topo-SoftCLT & -4.375 & 2.121 & 25.94 \\
 & GGeo-SoftCLT & -5.674 & 2.959 & 28.55 \\ \midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Without\\ spatial\\ features\\ (14)\end{tabular}} & Topo-TS2Vec & -5.263 & 8.852 & 50.00 \\
 & GGeo-TS2Vec & -33.33 & 2.083 & 44.44 \\
 & Topo-SoftCLT & -33.33 & -0.815 & 50.00 \\
 & GGeo-SoftCLT & -20.83 & 18.49 & 166.7 \\ \bottomrule
\end{tabular}}
\end{center}
\end{minipage}%
\end{table}

Tables \ref{tab: uea_task} and \ref{tab: uea_improvement} clearly show that structure preservation improves classification accuracy, not only when time series data involve spatial features, but also when they do not. The relative improvements in Table \ref{tab: uea_improvement} are higher for non-spatial datasets than for spatial datasets, which is because the datasets without spatial features are more difficult to learn in the UEA archive. As is shown in Table \ref{tab: uea_task}, the loss of contrastive learning \textit{decreases} when a structure-preserving regulariser is added for spatial datasets, while \textit{increases} for non-spatial datasets. This implies that preserving similarity structure is well aligned with contrastive learning for spatial datasts, and can even enhance contrastive learning.

\begin{table}[htb]
\caption{Structure preservation evaluation over datasets with and without spatial features in the UEA archive.}
\label{tab: uea_dist_dens}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}clcccccccccc@{}}
\toprule
\multirow{2}{*}{Datasets} & \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Local mean between timestamps} & \multicolumn{5}{c}{Global mean between all samples} \\ \cmidrule(l){3-7}\cmidrule(l){8-12} 
 & \multicolumn{1}{c}{} & kNN & Trust. & Cont. & MRRE & dRMSE & kNN & Trust. & Cont. & MRRE & dRMSE \\ \midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}With\\ spatial\\ features\\ (14)\end{tabular}} & TS2Vec & 0.563 & 0.868 & 0.875 & 0.117 & 0.346 & 0.419 & 0.784 & 0.765 & 0.189 & \textbf{0.150} \\
 & Topo-TS2Vec & \textbf{0.569} & \textbf{0.873} & 0.878 & \textbf{0.114} & 0.344 & 0.418 & 0.783 & 0.764 & 0.190 & 0.154 \\
 & GGeo-TS2Vec & \textbf{0.569} & \textbf{0.873} & \textbf{0.881} & \textbf{0.114} & \textbf{0.341} & 0.418 & 0.781 & 0.762 & 0.190 & 0.157 \\
 & SoftCLT & 0.562 & 0.866 & 0.875 & 0.117 & 0.348 & 0.420 & \textbf{0.788} & 0.765 & \textbf{0.187} & 0.171 \\
 & Topo-SoftCLT & 0.564 & 0.869 & 0.877 & 0.115 & 0.344 & \textbf{0.421} & 0.784 & \textbf{0.767} & 0.188 & 0.153 \\
 & GGeo-SoftCLT & \textbf{\underline{0.571}} & \textbf{\underline{0.875}} & \textbf{\underline{0.883}} & \textbf{\underline{0.111}} & \textbf{\underline{0.337}} & \textbf{\underline{0.425}} & \textbf{\underline{0.790}} & \textbf{\underline{0.768}} & \textbf{\underline{0.185}} & \textbf{\underline{0.149}} \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Without\\ spatial\\ features\\ (14)\end{tabular}} & TS2Vec & 0.423 & \textbf{0.820} & \textbf{\underline{0.835}} & 0.150 & \textbf{\underline{0.304}} & \textbf{0.362} & 0.767 & 0.767 & \textbf{\underline{0.252}} & 0.197 \\
 & Topo-TS2Vec & 0.424 & \textbf{0.820} & 0.831 & 0.151 & \textbf{0.308} & 0.356 & 0.763 & 0.767 & 0.254 & \textbf{0.191} \\
 & GGeo-TS2Vec & 0.420 & \textbf{0.820} & 0.832 & 0.151 & 0.310 & \textbf{\underline{0.365}} & \textbf{\underline{0.769}} & \textbf{\underline{0.771}} & 0.253 & \textbf{\underline{0.189}} \\
 & SoftCLT & \textbf{\underline{0.432}} & \textbf{0.820} & \textbf{\underline{0.835}} & \textbf{0.148} & 0.312 & 0.354 & 0.763 & 0.764 & \textbf{\underline{0.252}} & 0.197 \\
 & Topo-SoftCLT & 0.426 & 0.818 & 0.834 & \textbf{0.148} & 0.312 & 0.361 & \textbf{0.768} & \textbf{0.768} & 0.254 & 0.205 \\
 & GGeo-SoftCLT & \textbf{0.430} & \textbf{\underline{0.822}} & \textbf{\underline{0.835}} & \textbf{\underline{0.147}} & 0.315 & 0.355 & 0.761 & 0.762 & 0.257 & 0.203 \\
\bottomrule
\multicolumn{12}{l}{Note: the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.}
\end{tabular}}
\end{center}
\end{table}

The assessment of similarity preservation is presented in Table \ref{tab: uea_dist_dens} at both local and global scales. Consistent with the task-specific evaluation, Table \ref{tab: uea_dist_dens} shows that structure-preserving regularisation preserves more complete information on similarity relations. The improvements are generally more significant on datasets with spatial features, which makes it more evident that our proposed preservation suits spatial time series data better. Although the comparisons in Tables \ref{tab: uea_task}$\sim $\ref{tab: uea_dist_dens} indicate more notable improvements by preserving graph geometry than preserving topology, we have to note that this does not demonstrate the universal superiority of one over the others. Different datasets have different characteristics that benefit from preserving global or local structure, and domain knowledge is necessary to determine which could be more effective. We will discuss this more in Section \ref{sec: discussion}.

%-----------------------
\subsection{Macroscopic and microscopic traffic prediction}
%-----------------------
In Table \ref{tab: traffic_task}, we present the performance evaluation for both macroscopic and microscopic traffic prediction. This table shows consistent improvements by pretraining encoders with our methods. Notably, single contrastive learning (i.e., TS2Vec and SoftCLT) does not necessarily improve downstream prediction, whereas it does when used together with preserving certain similarity structures. Given that our comparisons are conducted through controlling random conditions, this result effectively shows the necessity of preserving structure when learning traffic interaction representations.
%
\begin{table}[htbp]
\caption{Macroscopic and microscopic traffic prediction performance evaluation.}
\label{tab: traffic_task}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{Method}} & \multicolumn{4}{c}{Macroscopic Traffic} & \multicolumn{4}{c}{Microscopic Traffic} \\ \cmidrule(l){2-5}\cmidrule(l){6-9} 
\multicolumn{1}{c}{} & \begin{tabular}[c]{@{}c@{}}MAE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}RMSE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SDEP\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}EVar\\ (\%)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}min. FDE\\ (m)\end{tabular} & \begin{tabular}[c]{@{}c@{}}MR$_{0.5}$\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}MR$_1$\\ (\%)\end{tabular} & \begin{tabular}[c]{@{}c@{}}MR$_2$\\ (\%)\end{tabular} \\ \midrule
No pretraining & \textbf{2.850} & 5.912 & 5.910 & 84.788 & 0.640 & 59.253 & 12.161 & 0.744 \\
TS2Vec & 2.878 & 5.982 & 5.982 & 84.415 & \textbf{0.636} & 59.453 & 11.899 & \textbf{\underline{0.558}} \\
Topo-TS2Vec & 2.862 & 5.915 & 5.908 & 84.798 & \textbf{\underline{0.634}} & \textbf{\underline{58.144}} & \textbf{11.761} & 0.737 \\
GGeo-TS2Vec & 2.887 & 5.981 & 5.978 & 84.436 & \textbf{0.636} & \textbf{58.289} & \textbf{\underline{11.747}} & 0.737 \\
SoftCLT & 2.856 & 5.938 & 5.932 & 84.675 & 0.641 & 59.501 & 11.940 & 0.785 \\
Topo-SoftCLT & \textbf{2.850} & \textbf{5.882} & \textbf{5.881} & \textbf{84.937} & 0.640 & 58.626 & 12.044 & 0.820 \\
GGeo-SoftCLT & \textbf{\underline{2.834}} & \textbf{\underline{5.879}} & \textbf{\underline{5.878}} & \textbf{\underline{84.954}} & 0.652 & 60.638 & 13.249 & \textbf{0.723} \\ \midrule
Best improvement (\%) & 0.568 & 0.558 & 0.547 & 0.196 & 0.929 & 1.872 & 3.399 & 25.000 \\
\bottomrule
\multicolumn{9}{l}{Note: the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.}
\end{tabular}}
\end{center}
\end{table}

Table \ref{tab: traffic_dist_dens} then displays the corresponding evaluation on similarity structure preservation, which is obtained by assessing the encoders after fine-tuning for traffic prediction. The results show that the better performing methods in macro-traffic prediction (i.e., GGeo-SoftCLT, Topo-SoftCLT) and micro-traffic prediction (i.e., Topo-TS2Vec, GGeo-TS2Vec) preserve more similarity structures at both global and local scales. In general, the metric values are close for the same task across methods; however, when a method has a significant advantage over the others, it indicates superior performance. Examples for macroscopic traffic prediction are Topo-SoftCLT in local Cont. and GGeo-SoftCLT in global dRMSE; for microscopic traffic prediction are Topo-TS2Vec in local Trust. and GGeo-TS2Vec in global Cont.
%
\begin{table}[htbp]
\caption{Structure preservation evaluation of encoders after the fine-tuning in traffic prediction tasks.}
\label{tab: traffic_dist_dens}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{Macroscopic Traffic} & \multicolumn{5}{c}{Microscopic Traffic} \\ \cmidrule(l){2-6}\cmidrule(l){7-11} 
\multicolumn{1}{c}{} & kNN & Cont. & Trust. & MRRE & dRMSE & kNN & Cont. & Trust. & MRRE & dRMSE \\ \midrule
\multicolumn{11}{c}{Local mean between timestamps for at most 500 samples} \\ \midrule
No pretraining & 0.125 & 0.524 & \textbf{\underline{0.526}} & 0.496 & \textbf{\underline{0.224}} & 0.373 & 0.742 & 0.552 & 0.426 & \textbf{\underline{0.478}} \\
TS2Vec & 0.125 & 0.523 & 0.522 & 0.501 & 0.251 & \textbf{\underline{0.398}} & \textbf{\underline{0.761}} & \textbf{\underline{0.592}} & \textbf{\underline{0.393}} & 0.496 \\
Topo-TS2Vec & \textbf{0.128} & \textbf{0.533} & 0.522 & \textbf{\underline{0.491}} & \textbf{0.242} & 0.397 & 0.754 & \textbf{0.590} & 0.399 & 0.506 \\
GGeo-TS2Vec & 0.126 & 0.529 & \textbf{0.524} & 0.496 & 0.249 & 0.397 & \textbf{0.756} & 0.589 & \textbf{0.396} & 0.508 \\
SoftCLT & 0.127 & 0.526 & 0.523 & 0.500 & 0.246 & 0.378 & 0.746 & 0.552 & 0.427 & \textbf{\underline{0.478}} \\
Topo-SoftCLT & \textbf{\underline{0.129}} & \textbf{\underline{0.536}} & \textbf{0.524} & \textbf{0.492} & 0.250 & \textbf{\underline{0.398}} & 0.755 & 0.588 & 0.405 & 0.480 \\
GGeo-SoftCLT & 0.127 & 0.527 & 0.523 & 0.498 & 0.261 & 0.397 & 0.751 & 0.589 & 0.405 & 0.485 \\ \midrule
\multicolumn{11}{c}{Global mean between all samples} \\ \midrule
No pretraining & \textbf{\underline{0.316}} & \textbf{\underline{0.949}} & \textbf{\underline{0.969}} & \textbf{\underline{0.031}} & \textbf{0.364} & 0.218 & 0.937 & 0.920 & 0.049 & 0.141 \\
TS2Vec & 0.264 & 0.940 & 0.957 & 0.039 & 0.377 & 0.232 & 0.953 & 0.920 & \textbf{0.044} & \textbf{0.139} \\
Topo-TS2Vec & 0.276 & 0.942 & 0.963 & 0.036 & 0.379 & \textbf{0.233} & \textbf{0.958} & 0.917 & 0.045 & \textbf{\underline{0.138}} \\
GGeo-TS2Vec & 0.263 & 0.940 & 0.959 & 0.039 & 0.400 & 0.231 & \textbf{\underline{0.959}} & \textbf{0.923} & \textbf{\underline{0.041}} & 0.140 \\
SoftCLT & \textbf{0.299} & \textbf{0.943} & \textbf{0.966} & \textbf{0.035} & 0.391 & 0.224 & 0.924 & 0.916 & 0.055 & 0.148 \\
Topo-SoftCLT & 0.288 & 0.940 & 0.965 & 0.036 & 0.371 & 0.215 & 0.909 & 0.901 & 0.065 & 0.150 \\
GGeo-SoftCLT & 0.287 & 0.939 & 0.964 & 0.037 & \textbf{\underline{0.359}} & \textbf{\underline{0.240}} & 0.935 & \textbf{\underline{0.926}} & 0.046 & 0.146 \\ \bottomrule
\multicolumn{11}{l}{Note: the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.}
\end{tabular}}
\end{center}
% \vspace{-15pt}
\end{table}

Notably, in macroscopic traffic prediction, fine-tuning from scratch maintains the greatest global similarities. This implies that the specific model architecture might allow for learning similarity structure without pretraining. This is not crystal clear with the final evaluation only. In the next sub-section, we will add different model architectures for the macro-traffic prediction task, and visualise the fine-tuning progress to further understand the contribution of structure preservation to downstream task performance.

%-----------------------
\subsection{Training efficiency}
%-----------------------
Incorporating structure-preserving regularisation increases computational complexity, and consequently, training time. The magnitude of this increase depends on the data and model that are applied on. With Table \ref{tab: training_time}, we quantify the additional time required for structure preservation and evaluate its impact across diverse model architectures. In prior experiments, we used Convolutional Neural Network (CNN) encoders for the classification task on UEA datasets, Dynamic Graph Convolution Network (DGCN, \cite{Li2021dgcn}) encoder for macroscopic traffic prediction, and VectorNet \cite{Gu2021} encoder for microscopic traffic prediction. To obtain a more comprehensive evaluation, we include two more Recurrent Neural Network (RNN) models for macroscopic traffic prediction: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) encoders, paired with simple linear decoders.
%
\begin{table}[htbp]
\centering
\caption{Training time per epoch in the stage of self-supervised representation learning.}
\label{tab: training_time}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccccccccc@{}}
\toprule
Task/data & Encoder & Base (sec/epoch) & TS2Vec & Topo-TS2Vec & GGeo-TS2Vec & SoftCLT & Topo-SoftCLT & GGeo-SoftCLT \\ \midrule
\begin{tabular}[c]{@{}c@{}}Avg. UEA$^a$\end{tabular} & CNN & 11.94 & 1.00$\times$ & 1.46$\times$ & 2.35$\times$ & 1.00$\times$ & 1.46$\times$ & 2.36$\times$ \\
\begin{tabular}[c]{@{}c@{}}MicroTraffic\end{tabular} & VectorNet & 122.93 & 1.00$\times$ & 1.45$\times$ & 1.16$\times$ & 1.23$\times$ & 1.69$\times$ & 1.42$\times$ \\
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}MacroTraffic\end{tabular}} & DGCN & 74.78 & 1.00$\times$ & 1.29$\times$ & 1.09$\times$ & 1.02$\times$ & 1.40$\times$ & 1.37$\times$ \\
 & LSTM & 18.04 & 1.00$\times$ & 1.49$\times$ & 1.12$\times$ & 1.09$\times$ & 1.57$\times$ & 1.22$\times$ \\
 & GRU & 16.49 & 1.00$\times$ & 1.54$\times$ & 1.14$\times$ & 1.10$\times$ & 1.61$\times$ & 1.24$\times$ \\ \bottomrule
 \multicolumn{9}{l}{$^a$ Detailed results are referred to Tables \ref{tab: uea_spatial_details_time} and \ref{tab: uea_nonspatial_details_time} in \ref{sec: apdx_UEA_details}.}
\end{tabular}}
\end{center}
\vspace{-15pt}
\end{table}

Table \ref{tab: training_time} shows that preserving structure increases training time by less than 50\% in most cases, and suits DGCN particularly well with the least additional time. However, when time sequences are very long, the computation of graph-geometry preserving loss becomes intense. For example, the time series length is 1,197 in the Cricket dataset and results in a pretraining time of 2.86 times the base; likewise, the EthanolConcentration dataset has a length of 1,751 and a pretraining time of 4.09 times the base, and the EigenWorms dataset uses 7.59 times of the base pretraining time with a time series length of 17,984.

In more detail, we evaluate the fine-tuning efficiency in macroscopic traffic prediction to further investigate the contribution of structure preservation. Figure \ref{fig: macro_traffic_progress} shows the convergence process of different models with and without pretraining, where RMSE is used to evaluate prediction performance and the other metrics indicate the preservation of global similarity relations.
%
\begin{figure}[ht]
\begin{center}
\includegraphics[width=\textwidth]{macro_traffic_progress.pdf}
\end{center}
\caption{Fine-tuning progress of models trained from scratch and pretrained with different losses in macroscopic traffic prediction. Values of the final performance are referred to Table \ref{tab: traffic_task} for DGCN, to Tables \ref{tab: lstm_gru_prediction} and \ref{tab: lstm_gru_structure} in \ref{sec: apdx_lstm_gru_details} for LSTM and GRU.}
\label{fig: macro_traffic_progress}
\end{figure}

For all models of DGCN, LSTM, and GRU, structure preservation consistently enhances prediction performance compared to training from scratch (No pretraining). The enhancement is significant when using LSTM and GRU, achieving 6.68\% and 10.14\% improvement in RMSE, respectively. Meanwhile, the progress of structure preservation is stable when the encoder is pretrained, and maintains the advantage over no pretraining throughout the fine-tuning process. In contrast, for DGCN which is a more sophisticated model tailored for the task, training from scratch is already very effective and pretraining brings relatively minor improvement. This implies that certain model architectures are better suited to a specific task than others, and the preservation of similarity structure in the latent representations may be a good indicator for model selection.


%///////////////////////
\section{Discussion}\label{sec: discussion}
%///////////////////////
The theoretical foundation and experimental results presented in this paper not only demonstrate evident improvements in downstream task performance but also reveal a critical bridge between contrastive learning and similarity structure preservation. In this section, we discuss these findings to guide method selection and interpret the observed performance improvements.

A key consideration when applying our approach lies in selecting an appropriate loss function. This involves two layers of choices: TS2Vec versus SoftCLT, and topology-preserving (Topo-) versus graph-geometry-preserving (GGeo-) regularisation. For the first choice, TS2Vec is generally more suitable for classification tasks with fewer classes as TS2Vec only compares the similarity between two different augmentations of the same sample. In contrast, SoftCLT incorporates all samples in a mini-batch by assigning soft labels based on similarity. This performs a more detailed similarity comparison and thus is advantageous for tasks with a larger number of classes or for regression. As shown in Figure \ref{fig: uea_num_classes}, in the UEA archive, datasets for which (Topo/GGeo-)TS2Vec achieves the best accuracy tend to have fewer classes than those with the best performance achieved by (Topo/GGeo-)SoftCLT. In essence, SoftCLT implicitly embeds the similarity structure through soft labels.
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=4.in]{uea_num_classes.pdf}
    \caption{Box plots of the number of classes in the UEA datasets for which the best classification accuracy is achieved after contrastive learning based on TS2Vec or SoftCLT.}
    \label{fig: uea_num_classes}
\end{figure}

Then the second layer of choice hinges on the scale of structural relevance to the downstream task. The topology-preserving regulariser is designed to maintain the global similarity structure between data samples, and the graph-geometry-preserving regulariser focuses on locally preserving temporal or spatial similarity structures within samples. Therefore, Topo-regularisation is especially beneficial when the downstream task relies on inter-sample relations; whereas GGeo-regularisation is particularly useful in tasks where subtle intra-sample variations are critical, such as traffic prediction.

Our hypothesis for the performance improvements observed across tasks is that explicit structure-preserving regularisation enforces the encoded latent space to reflect the original distribution of data. This aligns with the theoretical view of neural networks as models that approximate the conditional distributions of outputs on inputs. Contrastive learning typically relies on predefined positive and negative augmentations, which may disrupt the original patterns underlying data and introduce biases. By anchoring the latent space to the original data manifold, structure preservation can effectively mitigate the potential biases by reducing the dependency on augmentations. 


%///////////////////////
\section{Conclusion}\label{sec: conclusion}
%///////////////////////
This paper presents an approach to structure-preserving contrastive learning for spatial time series, where we propose a dynamic mechanism to adaptively balance contrastive learning and structure preservation. Extensive experiments demonstrate that our methods can improve the SOTA performance, including for multivariate time series classification in various contexts and for traffic prediction at both macroscopic and microscopic scales. In particular, preserving similarity structure is shown to be well aligned with and beneficial to contrastive learning for spatio-temporal data. This provides a solid yes to the proposed question: preserving fine-grained structure of similarity relations in contrastive learning can enhance downstream task performance for spatial time series.

Our findings emphasise that preserving the original similarity structure in the latent space is crucial for effective representational learning, as it impacts a neural networks ability to model the underlying conditional distribution. In general, adding structure-preserving regularisation has a limited impact on the efficiency of self-supervised representation learning. It can be computationally intensive when the time sequence is long; however, evident performance improvement makes it an acceptable price to pay for utilising the information embedded in time series data. In addition, our experiments suggest that higher similarity structure preservation may be a good indicator of more informative representations, highlighting that the structural information of similarities in spatio-temporal data remains yet to be exploited. Given that many real-world practices involve spatial time series, this study and future research based on it can be applied not only to traffic interactions, but also to any that can benefit from preserving specific structures in similarity relations. 



\section*{Acknowledgments}
This work is supported by the TU Delft AI Labs programme. We extend our sincere gratitude to the researchers and organisations who collected, created, cleaned, and curated the high-quality datasets for research use, among which the UEA Multivariate Time Series Classification Archive is openly accessible at \url{https://www.timeseriesclassification.com/dataset.php}. We also would like to thank the anonymous reviewers for their valuable comments and advice.

The authors acknowledge the use of computational resources of the DelftBlue supercomputer, provided by Delft High Performance Computing Centre (https://www.tudelft.nl/dhpc).



\section*{Declaration of generative AI and AI-assisted technologies in the writing process}
During the preparation of this work the authors used ChatGPT and DeepSeek in order to obtain suggestions for readability improvement. No sentence was entirely generated by the generative tools. After using the tools, the authors have reviewed and edited the content as needed and take full responsibility for the content of the publication.


%% For citations use: 
%%       \cite{<label>} ==> [1]

%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num} 
\bibliography{references}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% \begin{thebibliography}{00}

% %% For numbered reference style
% %% \bibitem{label}
% %% Text of bibliographic item

% \bibitem{lamport94}
%   Leslie Lamport,
%   \textit{\LaTeX: a document preparation system},
%   Addison Wesley, Massachusetts,
%   2nd edition,
%   1994.

% \end{thebibliography}


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\newpage
\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand{\theequation}{\thesection.\arabic{equation}}

%///////////////////////
\section{Theoretical optimal values of the losses}\label{sec: apdx_optimal_loss}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{equation}{0}
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
%///////////////////////
For $\gL_\text{TS2Vec}$, a value of 0 is reached when $\vz_{i,t}^\prime$ and $\vz_{i,t}^{\dprime}$ are identical. Similarly, the optimal case of $\gL_\text{SoftCLT}$ is when the samples with soft assignments close to 1 are identical, while dissimilar samples have soft assignments close to 0. The topology-preserving loss $\gL_\text{Topo}$ is 0 when the topologically relevant distances remain the same in the latent space as in the original space, i.e., $\mA^X\left[\boldsymbol{\pi}^X\right]=\mA^Z\left[\boldsymbol{\pi}^X\right]$ and $\mA^X\left[\boldsymbol{\pi}^Z\right]=\mA^Z\left[\boldsymbol{\pi}^Z\right]$. Finally, $\gL_\text{GGeo}$ approximates the distortion measure of isometry and is ideally 0, but can be negative when $\text{Tr}(\tilde{H}_i)<2$, as the approximation of $\tilde{H}_i$ is kernel-based depending on a hyperparameter $h$. 

%///////////////////////
\section{Detailed results on UEA datasets}\label{sec: apdx_UEA_details}
\setcounter{figure}{0}
\renewcommand{\thefigure}{B\arabic{figure}}
\setcounter{equation}{0}
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{table}{0}
\renewcommand{\thetable}{B\arabic{table}}
%///////////////////////
This section provides detailed comparisons of evaluation results for the used 28 datasets in the UEA archive. Tables \ref{tab: uea_spatial_details} and \ref{tab: uea_nonspatial_details} present the results of classification accuracy. Tables \ref{tab: uea_spatial_details_time} and \ref{tab: uea_nonspatial_details_time} present the training time for self-supervised representation learning. 
%
\begin{table}[htbp]
\caption{Detailed evaluation of classification accuracy on spatial datasets in the UEA archive.}
\label{tab: uea_spatial_details}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Dataset & TS2Vec & Topo-TS2Vec & GGeo-TS2Vec & SoftCLT & Topo-SoftCLT & GGeo-SoftCLT \\ \midrule
ArticularyWordRecognition & 0.980 & \textbf{\underline{0.987}} & 0.983 & \textbf{\underline{0.987}} & 0.977 & \textbf{\underline{0.987}} \\
BasicMotions & \textbf{\underline{1.000}} & \textbf{\underline{1.000}} & \textbf{\underline{1.000}} & \textbf{\underline{1.000}} & \textbf{\underline{1.000}} & \textbf{\underline{1.000}} \\
CharacterTrajectories & 0.971 & 0.985 & 0.972 & 0.980 & 0.977 & \textbf{\underline{0.986}} \\
Cricket & 0.944 & 0.944 & 0.972 & 0.972 & 0.972 & \textbf{\underline{0.986}} \\
ERing & 0.867 & 0.874 & 0.881 & \textbf{\underline{0.893}} & 0.878 & 0.863 \\
EigenWorms & 0.809 & 0.817 & 0.863 & 0.817 & \textbf{\underline{0.901}} & 0.840 \\
Epilepsy & 0.957 & 0.957 & 0.949 & \textbf{\underline{0.964}} & 0.957 & 0.949 \\
Handwriting & 0.498 & 0.499 & 0.479 & 0.487 & 0.478 & \textbf{\underline{0.580}} \\
LSST & 0.485 & 0.566 & 0.536 & 0.452 & 0.569 & \textbf{\underline{0.581}} \\
Libras & 0.883 & 0.844 & 0.850 & \textbf{\underline{0.889}} & 0.850 & 0.867 \\
NATOPS & 0.917 & 0.917 & 0.933 & 0.922 & 0.917 & \textbf{\underline{0.944}} \\
PEMS-SF & 0.792 & 0.775 & \textbf{\underline{0.815}} & 0.751 & 0.803 & 0.740 \\
RacketSports & 0.908 & 0.914 & 0.914 & \textbf{\underline{0.928}} & 0.908 & 0.875 \\
UWaveGestureLibrary & 0.862 & 0.831 & 0.834 & 0.888 & 0.881 & \textbf{\underline{0.897}} \\ \midrule
Avg. over spatial datasets & 0.848 & 0.851 & 0.856 & 0.852 & 0.862 & \textbf{\underline{0.864}} \\ 
 \bottomrule
\end{tabular}}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Detailed evaluation of classification accuracy on non-spatial datasets in the UEA archive.}
\label{tab: uea_nonspatial_details}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Dataset & TS2Vec & Topo-TS2Vec & GGeo-TS2Vec & SoftCLT & Topo-SoftCLT & GGeo-SoftCLT \\ \midrule
AtrialFibrillation & 0.200 & \textbf{\underline{0.267}} & 0.133 & 0.133 & 0.200 & \textbf{\underline{0.267}} \\
DuckDuckGeese & 0.360 & \textbf{\underline{0.540}} & 0.520 & 0.400 & 0.420 & 0.400 \\
EthanolConcentration & 0.289 & 0.274 & 0.297 & 0.243 & \textbf{\underline{0.308}} & \textbf{\underline{0.308}} \\
FaceDetection & 0.510 & 0.508 & 0.505 & \textbf{\underline{0.516}} & 0.497 & 0.505 \\
FingerMovements & 0.480 & 0.480 & 0.480 & 0.530 & 0.470 & \textbf{\underline{0.540}} \\
HandMovementDirection & 0.324 & \textbf{\underline{0.405}} & 0.257 & 0.324 & 0.230 & 0.257 \\
Heartbeat & 0.751 & \textbf{\underline{0.761}} & 0.717 & 0.756 & 0.737 & 0.732 \\
JapaneseVowels & 0.978 & \textbf{\underline{0.986}} & 0.978 & 0.970 & 0.978 & 0.978 \\
MotorImagery & 0.480 & 0.500 & 0.500 & \textbf{\underline{0.520}} & 0.500 & 0.500 \\
PhonemeSpectra & 0.263 & 0.258 & \textbf{\underline{0.269}} & \textbf{\underline{0.269}} & 0.260 & 0.257 \\
SelfRegulationSCP1 & 0.778 & 0.768 & \textbf{\underline{0.788}} & 0.761 & 0.730 & 0.771 \\
SelfRegulationSCP2 & 0.467 & 0.550 & \textbf{\underline{0.561}} & 0.528 & 0.511 & 0.511 \\
SpokenArabicDigits & 0.973 & \textbf{\underline{0.976}} & 0.966 & 0.964 & 0.968 & 0.957 \\
StandWalkJump & 0.467 & 0.467 & \textbf{\underline{0.533}} & 0.200 & 0.133 & \textbf{\underline{0.533}} \\ \midrule
Avg. over non-spatial datasets & 0.523 & \textbf{\underline{0.553}} & 0.536 & 0.508 & 0.496 & 0.537 \\ 
 \bottomrule
\end{tabular}}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Detailed representation training time per epoch (unit: s) on spatial datasets in the UEA archive.}
\label{tab: uea_spatial_details_time}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Dataset & TS2Vec & Topo-TS2Vec & GGeo-TS2Vec & SoftCLT & Topo-SoftCLT & GGeo-SoftCLT \\ \midrule
ArticularyWordRecognition & 3.799 (1.00$\times$) & 5.61 (1.48$\times$) & 5.863 (1.54$\times$) & 3.772 (0.99$\times$) & 5.77 (1.52$\times$) & 5.983 (1.57$\times$) \\
BasicMotions & 0.475 (1.00$\times$) & 0.685 (1.44$\times$) & 0.709 (1.49$\times$) & 0.457 (0.96$\times$) & 0.687 (1.45$\times$) & 0.711 (1.50$\times$) \\
CharacterTrajectories & 20.640 (1.00$\times$) & 30.863 (1.50$\times$) & 33.32 (1.61$\times$) & 20.652 (1.00$\times$) & 30.948 (1.50$\times$) & 33.18 (1.61$\times$) \\
Cricket & 1.903 (1.00$\times$) & 2.653 (1.39$\times$) & 5.437 (2.86$\times$) & 1.904 (1.00$\times$) & 2.655 (1.40$\times$) & 5.436 (2.86$\times$) \\
ERing & 0.319 (1.00$\times$) & 0.482 (1.51$\times$) & 0.487 (1.53$\times$) & 0.316 (0.99$\times$) & 0.483 (1.51$\times$) & 0.49 (1.54$\times$) \\
EigenWorms & 19.862 (1.00$\times$) & 23.823 (1.20$\times$) & 149.05 (7.50$\times$) & 20.224 (1.02$\times$) & 24.856 (1.25$\times$) & 150.7 (7.59$\times$) \\
Epilepsy & 1.737 (1.00$\times$) & 2.49 (1.43$\times$) & 2.753 (1.58$\times$) & 1.686 (0.97$\times$) & 2.506 (1.44$\times$) & 2.755 (1.59$\times$) \\
Handwriting & 1.875 (1.00$\times$) & 2.771 (1.48$\times$) & 2.959 (1.58$\times$) & 1.88 (1.00$\times$) & 2.775 (1.48$\times$) & 2.987 (1.59$\times$) \\
LSST & 29.786 (1.00$\times$) & 45.273 (1.52$\times$) & 45.162 (1.52$\times$) & 29.859 (1.00$\times$) & 45.216 (1.52$\times$) & 45.154 (1.52$\times$) \\
Libras & 2.081 (1.00$\times$) & 3.142 (1.51$\times$) & 3.142 (1.51$\times$) & 2.085 (1.00$\times$) & 3.135 (1.51$\times$) & 3.141 (1.51$\times$) \\
NATOPS & 1.953 (1.00$\times$) & 2.989 (1.53$\times$) & 2.949 (1.51$\times$) & 2.085 (1.07$\times$) & 3.147 (1.61$\times$) & 3.159 (1.62$\times$) \\
PEMS-SF & 3.413 (1.00$\times$) & 5.069 (1.49$\times$) & 5.38 (1.58$\times$) & 3.415 (1.00$\times$) & 5.064 (1.48$\times$) & 5.399 (1.58$\times$) \\
RacketSports & 1.781 (1.00$\times$) & 2.685 (1.51$\times$) & 2.664 (1.50$\times$) & 1.771 (0.99$\times$) & 2.711 (1.52$\times$) & 2.665 (1.50$\times$) \\
UWaveGestureLibrary & 1.699 (1.00$\times$) & 2.395 (1.41$\times$) & 2.788 (1.64$\times$) & 1.776 (1.05$\times$) & 2.595 (1.53$\times$) & 2.99 (1.76$\times$) \\ \midrule
Avg. over spatial datasets & 6.523 sec/epoch & 1.46$\times$ & 2.12$\times$ & 1.00$\times$ & 1.48$\times$ & 2.15$\times$ \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Detailed representation training time per epoch (unit: s) on non-spatial datasets in the UEA archive.}
\label{tab: uea_nonspatial_details_time}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
Dataset & TS2Vec & Topo-TS2Vec & GGeo-TS2Vec & SoftCLT & Topo-SoftCLT & GGeo-SoftCLT \\ \midrule
AtrialFibrillation & 0.182 (1.00$\times$) & 0.258 (1.42$\times$) & 0.369 (2.03$\times$) & 0.177 (0.97$\times$) & 0.259 (1.42$\times$) & 0.366 (2.01$\times$) \\
DuckDuckGeese & 0.617 (1.00$\times$) & 0.973 (1.58$\times$) & 1.059 (1.72$\times$) & 0.621 (1.01$\times$) & 0.968 (1.57$\times$) & 1.104 (1.79$\times$) \\
EthanolConcentration & 4.939 (1.00$\times$) & 6.655 (1.35$\times$) & 20.128 (4.08$\times$) & 4.89 (0.99$\times$) & 6.664 (1.35$\times$) & 20.182 (4.09$\times$) \\
FaceDetection & 70.709 (1.00$\times$) & 109.6 (1.55$\times$) & 108.83 (1.54$\times$) & 71.104 (1.01$\times$) & 107.523 (1.52$\times$) & 107.092 (1.51$\times$) \\
FingerMovements & 3.826 (1.00$\times$) & 5.67 (1.48$\times$) & 5.706 (1.49$\times$) & 3.779 (0.99$\times$) & 5.671 (1.48$\times$) & 5.716 (1.49$\times$) \\
HandMovementDirection & 2.221 (1.00$\times$) & 3.353 (1.51$\times$) & 4.151 (1.87$\times$) & 2.226 (1.00$\times$) & 3.334 (1.50$\times$) & 4.142 (1.86$\times$) \\
Heartbeat & 2.811 (1.00$\times$) & 4.218 (1.50$\times$) & 5.22 (1.86$\times$) & 2.818 (1.00$\times$) & 4.216 (1.50$\times$) & 5.221 (1.86$\times$) \\
JapaneseVowels & 3.211 (1.00$\times$) & 4.871 (1.52$\times$) & 4.821 (1.50$\times$) & 3.199 (1.00$\times$) & 4.846 (1.51$\times$) & 4.83 (1.50$\times$) \\
MotorImagery & 7.450 (1.00$\times$) & 9.637 (1.29$\times$) & 51.0 (6.85$\times$) & 7.475 (1.00$\times$) & 9.659 (1.30$\times$) & 50.881 (6.83$\times$) \\
PhonemeSpectra & 42.956 (1.00$\times$) & 63.801 (1.49$\times$) & 70.578 (1.64$\times$) & 43.015 (1.00$\times$) & 63.807 (1.49$\times$) & 70.806 (1.65$\times$) \\
SelfRegulationSCP1 & 4.178 (1.00$\times$) & 6.042 (1.45$\times$) & 10.446 (2.50$\times$) & 4.237 (1.01$\times$) & 6.094 (1.46$\times$) & 10.415 (2.49$\times$) \\
SelfRegulationSCP2 & 3.295 (1.00$\times$) & 4.67 (1.42$\times$) & 9.391 (2.85$\times$) & 3.269 (0.99$\times$) & 4.629 (1.40$\times$) & 9.376 (2.85$\times$) \\
SpokenArabicDigits & 96.299 (1.00$\times$) & 143.411 (1.49$\times$) & 131.577 (1.37$\times$) & 86.495 (0.90$\times$) & 125.916 (1.31$\times$) & 129.073 (1.34$\times$) \\
StandWalkJump & 0.304 (1.00$\times$) & 0.404 (1.33$\times$) & 1.68 (5.53$\times$) & 0.31 (1.02$\times$) & 0.4 (1.32$\times$) & 1.7 (5.59$\times$) \\ \midrule
Avg. over non-spatial datasets & 17.357 sec/epoch & 1.46$\times$ & 2.57$\times$ & 0.99$\times$ & 1.44$\times$ & 2.57$\times$ \\
\bottomrule
\end{tabular}}
\end{center}
\end{table}

In addition, to visually show the effect of differently regularised contrastive learning losses on representation, we apply t-SNE to compress the encoded representations into 3 dimensions, as plotted in Figure \ref{fig: latent_epilepsy} for the dataset Epilepsy, and Figure \ref{fig: latent_racketsports} for RacketSports. The classes are indicated by colours. We use these two datasets because they are visualisation-friendly, with 4 classes and around 150 test samples. 
%
\begin{figure}[htbp]
\centering
\begin{center}
    \includegraphics[width=\textwidth]{Epilepsy_latents.pdf}
\end{center}
\caption{Encoded representations after training with different losses on the test set of Epilepsy.}
\label{fig: latent_epilepsy}
% \vspace{5pt}
\begin{center}
    \includegraphics[width=\textwidth]{RacketSports_latents.pdf}
\end{center}
\caption{Encoded representations after training with different losses on the test set of RacketSports.}
\label{fig: latent_racketsports}
\end{figure}


%///////////////////////
\section{Detailed results of macroscopic prediction with LSTM and GRU}\label{sec: apdx_lstm_gru_details}
\setcounter{figure}{0}
\renewcommand{\thefigure}{C\arabic{figure}}
\setcounter{equation}{0}
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{table}{0}
\renewcommand{\thetable}{C\arabic{table}}
%///////////////////////
This section provides additional tables presenting the evaluation of the final results using LSTM and GRU in macroscopic traffic prediction. Table \ref{tab: lstm_gru_prediction} shows the task-specific metrics and Table \ref{tab: lstm_gru_structure} shows the metrics for global structure preservation.

\begin{table}[htbp]
\caption{Macroscopic traffic prediction evaluation with LSTM and GRU encoders.}
\label{tab: lstm_gru_prediction}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{4}{c}{LSTM} & \multicolumn{4}{c}{GRU} \\ \cmidrule(l){2-9} 
\multicolumn{1}{c}{} & \begin{tabular}[c]{@{}c@{}}MAE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}RMSE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SDEP\\ (km/h)\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}EVar\\ (\%)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}MAE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}RMSE\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SDEP\\ (km/h)\end{tabular} & \begin{tabular}[c]{@{}c@{}}EVar\\ (\%)\end{tabular} \\ \midrule
No pretraining & 3.244 & 6.402 & 6.400 & 82.161 & 3.552 & 7.228 & 7.228 & 77.248 \\
TS2Vec & 3.158 & 6.187 & 6.187 & 83.331 & 3.601 & 7.297 & 7.297 & 76.813 \\
Topo-TS2Vec & 3.139 & 6.155 & 6.154 & 83.505 & 3.491 & 7.006 & 7.006 & 78.626 \\
GGeo-TS2Vec & \textbf{\underline{3.101}} & \textbf{\underline{5.974}} & \textbf{\underline{5.973}} & \textbf{\underline{84.461}} & 3.466 & 6.948 & 6.947 & 78.984 \\
SoftCLT & 3.191 & 6.319 & 6.319 & 82.612 & \textbf{3.349} & \textbf{6.649} & \textbf{6.649} & \textbf{80.749} \\
Topo-SoftCLT & 3.192 & 6.271 & 6.270 & 82.877 & 3.367 & 6.661 & 6.660 & 80.682 \\
GGeo-SoftCLT & \textbf{3.128} & \textbf{6.121} & \textbf{6.121} & \textbf{83.683} & \textbf{\underline{3.302}} & \textbf{\underline{6.495}} & \textbf{\underline{6.495}} & \textbf{\underline{81.630}} \\ \midrule
Best improvement (\%) & 4.415 & 6.676 & 6.668 & 2.799 & 7.026 & 10.141 & 10.145 & 5.673 \\
\bottomrule
\multicolumn{9}{l}{Note: the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.}
\end{tabular}}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Global structure preservation of LSTM and GRU encoders in macroscopic traffic prediction task.}
\label{tab: lstm_gru_structure}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{5}{c}{LSTM} & \multicolumn{5}{c}{GRU} \\ \cmidrule(l){2-11} 
\multicolumn{1}{c}{} & kNN & Cont. & Trust. & MRRE & \multicolumn{1}{c}{dRMSE} & kNN & Cont. & Trust. & MRRE & dRMSE \\ \midrule
No pretraining & \textbf{\underline{0.161}} & 0.858 & 0.899 & 0.097 & \textbf{\underline{0.427}} & 0.151 & 0.870 & 0.902 & 0.091 & \textbf{0.424} \\
TS2Vec & 0.120 & 0.907 & 0.891 & 0.086 & 0.437 & 0.132 & 0.921 & 0.906 & 0.072 & \textbf{\underline{0.399}} \\
Topo-TS2Vec & 0.122 & 0.899 & 0.888 & 0.089 & 0.452 & 0.134 & 0.914 & 0.904 & 0.076 & 0.429 \\
GGeo-TS2Vec & 0.134 & 0.912 & 0.905 & 0.078 & \textbf{\underline{0.427}} & 0.138 & 0.916 & 0.913 & 0.071 & 0.426 \\
SoftCLT & 0.141 & \textbf{0.938} & 0.912 & 0.063 & 0.453 & 0.166 & 0.942 & 0.931 & 0.052 & 0.435 \\
Topo-SoftCLT & 0.145 & 0.932 & \textbf{0.917} & \textbf{0.061} & 0.444 & \textbf{0.175} & \textbf{\underline{0.951}} & \textbf{0.934} & \textbf{\underline{0.046}} & 0.471 \\
GGeo-SoftCLT & \textbf{0.147} & \textbf{\underline{0.940}} & \textbf{\underline{0.918}} & \textbf{\underline{0.059}} & 0.477 & \textbf{\underline{0.177}} & \textbf{0.948} & \textbf{\underline{0.935}} & \textbf{\underline{0.046}} & 0.456 \\
\bottomrule
\multicolumn{11}{l}{Note: the \textbf{\underline{best}} values are both bold and underlined; the \textbf{second-best} values are bold.}
\end{tabular}}
\end{center}
\end{table}


\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
