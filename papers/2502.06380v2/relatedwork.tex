\section{Related work}
\label{sec: related work}
%///////////////////////
%-----------------------
\subsection{Time series contrastive learning}
%-----------------------
Contrastive learning for time series data is a relatively young niche and is rapidly developing. The development has been dominantly focused on defining positive and negative samples. Early approaches construct positive and negative samples with subseries within time series \cite{franceschi2019tloss} and temporal neighbourhoods \cite{tonekaboni2021unsupervised}; and later methods create augmentations by transforming original series \cite{eldele2021tstcc,eldele2023}. More recently, \cite{Yue2022ts2vec} generates random masks to enable both instance-wise and time-wise contextual representations at flexible hierarchical levels, which exceeds previous state-of-the-art performances (SOTAs). Given that not all negatives may be useful \cite{cai2020negative,Jeon2021}, \cite{liu2024timesurl} makes hard negatives to boost performance, while \cite{lee2024softclt} utilises soft contrastive learning to weigh sample pairs of varying similarities, both of which reach new SOTAs. 

The preceding paragraph outlines a brief summary, and we refer the readers to Section 2 in \cite{lee2024softclt} and Section 5.3 in \cite{trirat2024universal} for a detailed overview of the methods proposed in the past 6 years. These advances have led to increasingly sophisticated methods that mine the contextual information embedded in time series by contrasting similarities. However, the structural details of similarity relations between samples remain to be explored. 

%-----------------------
\subsection{Structure-preserving SSRL}
%-----------------------
Preserving the original structure of data when mapping into a latent space has been widely and actively researched in manifold learning (for a literature review, see \cite{meila2024}) and graph representation learning \cite{ju2024,khoshraftar2024}. In manifold learning, which is also known as nonlinear dimension reduction, the focus is on revealing the geometric shape of data point clouds for visualisation, denoising, and interpretation. In graph representation learning, the focus is on maintaining the connectivity of nodes in the graph while compressing the data space required for large-scale graphs \cite{Yao2024}. Structure-preserving has not yet attracted much dedication to time series data. \cite{ashraf2023} provides a literature review on time series data dimensionality reduction, where none of the methods are specifically tailored for time series.

Zooming in within structure-preserving SSRL, there are two major branches respectively focusing on topology and geometry. Topology-preserving SSRL aims to maintain global properties such as clusters, loops, and voids in the latent space; representative models include \cite{moor2020topoae} and \cite{trofimov2023rtdae} using autoencoders, as well as \cite{madhu2023toposrl} and \cite{Chen2024topogcl} with contrastive learning. The other branch is geometry-preserving and focuses more on local shapes such as relative distances, angles, and areas. Geometry-preserving autoencoders include \cite{Nazari2023geomae} and \cite{lim2024ggae}, while \cite{li2022geomgcl} and \cite{Koishekenov2023} use contrastive learning. The aforementioned topology and geometry preserving autoencoders are all developed for dimensionality reduction; whereas the combination of contrastive learning and structure-preserving has been explored majorly with graphs.

%-----------------------
\subsection{Traffic interaction SSRL}
%-----------------------
In line with the conclusions in previous sub-sections, existing exploration in the context of traffic interaction data and tasks also predominantly relies on autoencoders and graphs. For instance, using a transformer-based multivariate time series autoencoder \cite{zerveas2021transformer},  \cite{lu2022learning} clusters traffic scenarios with trajectories of pairwise vehicles. Then a series of studies investigate masking strategies with autoencoders for individual trajectories and road networks, including \cite{Cheng2023mae,Chen2023trajmae,lan2024sept}. 

Leveraging data augmentation, \cite{Mao2022jointlycontrastive} utilises graphs and contrastive learning to jointly learn representations for vehicle trajectories and road networks. The authors design road segment positive samples as neighbours in the graph, and trajectory positive samples by replacing a random part with another path having the same origin and destination. Similarly, \cite{Zipfl2023trafficsimilarity} uses a graph-based contrastive learning approach to learn traffic scene similarity. The authors randomly modify the position and velocity of individual traffic participants in a scene to construct positive samples, with negative samples drawn uniformly from the rest of a training batch. Also using augmentation, \cite{Zheng2024longterm} focuses on capturing seasonal and holiday information for traffic prediction.

%///////////////////////