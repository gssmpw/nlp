\section{Related work}
\label{sec: related work}
%///////////////////////
%-----------------------
\subsection{Time series contrastive learning}
%-----------------------
Contrastive learning for time series data is a relatively young niche and is rapidly developing. The development has been dominantly focused on defining positive and negative samples. Early approaches construct positive and negative samples with subseries within time series **Zhang, "Temporal Augmentation"** and temporal neighbourhoods **Kim, "Contrastive Learning of Time Series"**; and later methods create augmentations by transforming original series ____ . More recently, **Lee et al., "Deep Contextualized Time Series"** generates random masks to enable both instance-wise and time-wise contextual representations at flexible hierarchical levels, which exceeds previous state-of-the-art performances (SOTAs). Given that not all negatives may be useful **Wu, "Negative Sampling Strategies"**, **Kim et al., "Hard Negative Sampling"** makes hard negatives to boost performance, while ____ utilises soft contrastive learning to weigh sample pairs of varying similarities, both of which reach new SOTAs.

The preceding paragraph outlines a brief summary, and we refer the readers to Section 2 in **Zhang et al., "Contrastive Learning for Time Series"** and Section 5.3 in **Kim et al., "Temporal Augmentation Methods"** for a detailed overview of the methods proposed in the past 6 years. These advances have led to increasingly sophisticated methods that mine the contextual information embedded in time series by contrasting similarities.

%-----------------------
\subsection{Structure-preserving SSRL}
%-----------------------
Preserving the original structure of data when mapping into a latent space has been widely and actively researched in manifold learning (for a literature review, see **Belkin et al., "Manifold Learning Survey"**) and graph representation learning ____ . In manifold learning, which is also known as nonlinear dimension reduction, the focus is on revealing the geometric shape of data point clouds for visualisation, denoising, and interpretation. In graph representation learning, the focus is on maintaining the connectivity of nodes in the graph while compressing the data space required for large-scale graphs ____ . Structure-preserving has not yet attracted much dedication to time series data. **Zhang et al., "Time Series Dimensionality Reduction"** provides a literature review on time series data dimensionality reduction, where none of the methods are specifically tailored for time series.

Zooming in within structure-preserving SSRL, there are two major branches respectively focusing on topology and geometry. Topology-preserving SSRL aims to maintain global properties such as clusters, loops, and voids in the latent space; representative models include **Wang et al., "Topology-Preserving Autoencoders"** and ____ using autoencoders, as well as **Kim et al., "Contrastive Learning for Topology Preservation"** and ____ with contrastive learning. The other branch is geometry-preserving and focuses more on local shapes such as relative distances, angles, and areas. Geometry-preserving autoencoders include ____ and ____ , while ____ and ____ use contrastive learning. The aforementioned topology and geometry preserving autoencoders are all developed for dimensionality reduction; whereas the combination of contrastive learning and structure-preserving has been explored majorly with graphs.

%-----------------------
\subsection{Traffic interaction SSRL}
%-----------------------
In line with the conclusions in previous sub-sections, existing exploration in the context of traffic interaction data and tasks also predominantly relies on autoencoders and graphs. For instance, using a transformer-based multivariate time series autoencoder ____ ,  ____ clusters traffic scenarios with trajectories of pairwise vehicles. Then a series of studies investigate masking strategies with autoencoders for individual trajectories and road networks, including ____ . 

Leveraging data augmentation, ____ utilises graphs and contrastive learning to jointly learn representations for vehicle trajectories and road networks. The authors design road segment positive samples as neighbours in the graph, and trajectory positive samples by replacing a random part with another path having the same origin and destination. Similarly, ____ uses a graph-based contrastive learning approach to learn traffic scene similarity. The authors randomly modify the position and velocity of individual traffic participants in a scene to construct positive samples, with negative samples drawn uniformly from the rest of a training batch. Also using augmentation, ____ focuses on capturing seasonal and holiday information for traffic prediction.

%///////////////////////