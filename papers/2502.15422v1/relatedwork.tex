\section{Related Work}
\label{sec:related_work}

\paragraph{Text Benchmarks.}

MMLU~\citep{hendrycks2021measuring} assesses general language proficiency, while GSM8K~\citep{cobbe2021gsm8k}, CS-Bench~\citep{song2024cs}, and SciBench~\citep{wang2024scibench} focus on math, computer science, and science skills. These offer a focused evaluation of AI capabilities within educational contexts.

\paragraph{Multimodal Benchmarks.}

SEEDBench~\citep{li2023seed} and MMStar~\citep{chen2024we} provide general multimodal evaluations. Notably, there are educationally focused benchmarks such as ScienceQA~\citep{lu2022learn} and MathVista~\citep{lu2024mathvista}, which assess AIâ€™s ability with scientific and mathematical content. Further, MMMU~\citep{yue2023mmmu} provides diverse subject evaluations, including Art and Medicine, while AI2D~\citep{Kembhavi2016ADI} examines diagram interpretation in grade school science.

\paragraph{Korean Benchmarks.}
Korean benchmarks are limited, but efforts like K-MMLU~\citep{son2024kmmlumeasuringmassivemultitask} and Ko-H5~\citep{park-etal-2024-open} have emerged. In multimodal contexts, KVQA~\citep{Kim_Lim2019} and CVQA~\citep{romero2024cvqa} focus on VQA and cultural understanding.
Despite the advances, there is a notable absence of Korean educational benchmarks, particularly in the multimodal domain. No existing frameworks comprehensively evaluate AI's educational performance across various school subjects within a Korean context.