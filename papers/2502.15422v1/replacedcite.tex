\section{Related Work}
\label{sec:related_work}

\paragraph{Text Benchmarks.}

MMLU____ assesses general language proficiency, while GSM8K____, CS-Bench____, and SciBench____ focus on math, computer science, and science skills. These offer a focused evaluation of AI capabilities within educational contexts.

\paragraph{Multimodal Benchmarks.}

SEEDBench____ and MMStar____ provide general multimodal evaluations. Notably, there are educationally focused benchmarks such as ScienceQA____ and MathVista____, which assess AIâ€™s ability with scientific and mathematical content. Further, MMMU____ provides diverse subject evaluations, including Art and Medicine, while AI2D____ examines diagram interpretation in grade school science.

\paragraph{Korean Benchmarks.}
Korean benchmarks are limited, but efforts like K-MMLU____ and Ko-H5____ have emerged. In multimodal contexts, KVQA____ and CVQA____ focus on VQA and cultural understanding.
Despite the advances, there is a notable absence of Korean educational benchmarks, particularly in the multimodal domain. No existing frameworks comprehensively evaluate AI's educational performance across various school subjects within a Korean context.