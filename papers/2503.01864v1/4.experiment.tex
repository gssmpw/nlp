\section{Experiments}
In this section, we present the experimental results utilizing our \methodname{} metric within the context of the eva self-play data generation framework.
Following eva, we use the UltraFeedback dataset \citep{cui2024ultrafeedback} as our source of initial prompts and adopt the ArmoRM \citep{ArmoRM} reward model for preference annotation.
Similar to the setup in \S\ref{sec:metric_exp}, we employ SFT models from the Llama-3-8b and Gemma-2-9b model families, apply SimPO and DPO loss for alignment training, and utilize AlpacaEval 2 and Arena Hard benchmarks for model evaluations.

\subsection{Main Results}\label{sec:main_exp}

We first compare models trained with different datasets under the single-iteration setting.
Utilizing the procedure outlined in Algorithm~\ref{algo:eva}, we begin with 10k prompts from the UltraFeedback dataset to generate the initial uf-10k dataset $\mathcal D$.
Then we evolve new prompts $\mathcal X'$ and construct the additional preference dataset $\mathcal D'$.
Based on this new dataset, we select the top-10k subset using eva and our \methodname{} metric, resulting in the $\mathcal D'_\text{eva-10k}$ and $\mathcal D'_\text{ours-10k}$ datasets.
To evaluate the effectiveness of evolve-selected data against existing datasets, we create an additional dataset, $\mathcal D_\text{uf-10k}$, using supplementary 10k prompts from UltraFeedback.

Table~\ref{table:main_exp_evolve_first} illustrates the alignment performance across models trained on various datasets. 
Here, ``uf-10k'' refers to the initial dataset $\mathcal D$, while ``+uf-10k', ``+eva-10k'' and ``+ours-10k'' denotes models trained on the union of $\mathcal D$ with $\mathcal D_\text{uf-10k}$, $\mathcal D'_\text{eva-10k}$, and $\mathcal D'_\text{ours-10k}$ respectively.
As shown in the table, while all three additional datasets contribute to improved alignment performance, datasets selected via our metric consistently achieve superior results across different base models and training objectives, surpassing both existing datasets and the current SOTA method, eva.
These consistent improvements underscore the effectiveness and robustness of our \methodname{} metric in discerning high-quality data for alignment training.

\setlength{\textfloatsep}{\origtextfloatsep}
%%%%%%%%%%%%%%%% main table %%%%%%%%%%%%%%%%
\input{tables/main_table_evolve_first}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Furthermore, we conduct additional experiments employing eva's original \textit{select-then-evolve} pipeline, which initially selects a high-quality subset $\mathcal D_k$ for data evolving into the additional dataset $\mathcal D_k'$ (more details can be found in Appendix~\ref{appendix:compare_eva}).
While the resultant $\mathcal D_k'$ dataset's quality may not match that of $\mathcal D_k$ due to the intricate data-evolving process, our experiments, as shown in Table~\ref{table:main_exp_select_first}, reveal that evolved datasets based on our metric's selected subsets still achieve the best overall performance. 
This suggests that high-quality initial subsets might positively influence the quality of the evolved data, thus enhancing the model alignment.
Specifically, we illustrate this correlation between model performance and the \methodname{} score of their corresponding datasets in Figure~\ref{fig:perf_potential_ablation} (\cf Appendix~\ref{appendix:compare_eva}). The selected subset $\mathcal D_k$ using our metric yields a higher $M_{AP}$ score in the resultant $\mathcal D_k'$, and the model performance consistently improves with this score, affirming enhanced alignment through high-quality data generation.

\subsection{Scaling with Dataset Size}
Since the main experiments involve only 20k data, we extend our investigation to larger datasets to assess the scalability of the proposed metric.
Within the \textit{evolve-then-select} framework, we leverage LLMs to generate additional preference data based on 20k seed prompts from UltraFeedback and select a top-$k$ subset from the combined dataset with our \methodname{} metric for SimPO training.
For comparison, a full-size dataset built upon the entire UltraFeedback prompts, comprising approximately 60k data, is randomly sampled with varied sizes for training. 

Figure~\ref{fig:data_scaling} illustrates the model performance relative to dataset size. 
In contrast to the gradual performance increase observed with the full UltraFeedback dataset, datasets selected using our metric demonstrate significantly faster improvements. 
Remarkably, training on a 30k subset containing only half the data leads to superior performance compared to the full UltraFeedback dataset, improving 2.3 points in Alpaca LC and 3.4 points in Arena Hard. 
These substantial gains highlight the efficacy of our metric for selecting high-quality data, suggesting a potent avenue for LLM alignment with superior models even using smaller datasets.


%%%%%%%%%%%%%%%% data-scaling figure %%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/perf_per_data_size.pdf}
    \vspace{-10pt}
    \caption{Performance of Gemma models as the dataset size increases.}
    \label{fig:data_scaling}
    \vspace{-5pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Scaling with Training Iteration}
Our metric is also applied to multi-iteration settings.
Like the main experiment, we utilize a fixed 10k subset $\mathcal X$ from UltraFeedback to generate the initial dataset $\mathcal D$ for each iteration, and augment it with 10k additional evolve-selected data $\mathcal D_\text{ours-10k}'$, or supplementary data from UltraFeedback $\mathcal D_\text{uf-10k}$, for SimPO training.
We report results for generating $\mathcal D_\text{uf-10k}$ by both fixed and additional prompts from UltraFeedback.
Figure~\ref{fig:iter_scaling} illustrates the continuously increasing performance with more iterations when using our metric, which consistently outperforms the UltraFeedback data, even when extra data are sampled. 
% This result demonstrates the metric's utility in iterative learning frameworks.

%%%%%%%%%%%%%%%% iteration-scaling figure %%%%%%%%%%%%%%%%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/perf_per_iter.pdf}
    \vspace{-10pt}
    \caption{Performance of Llama models trained under iterative preference learning setting.}
    \label{fig:iter_scaling}
    \vspace{-5pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

