\vspace{-2em}
\section{Introduction}\label{sec:intro}

Learning from human feedback is essential for aligning large language models (LLMs) \citep{gpt4report,llama2report} with human preference, ensuring they are helpful, honest, and harmless \citep{3H-askell2021general}. 
A standard method to achieve such alignment is reinforcement learning from human feedback (RLHF) \citep{rlhf-christiano2017deep,rlhf-ouyang2022training,rlhf-stiennon2020learning}, which involves iterative LLM fine-tuning and reward model training. 
To address the complexity inherent in this multi-stage training process, several offline methods --- such as DPO \citep{DPO} and SimPO \citep{SimPO} --- have been proposed. 
% \czq{
% To address the complexity inherent in this multi-stage training process, several offline methods have been proposed, such as DPO \citep{DPO} and SimPO \citep{SimPO}.
% }
%
They directly align LLMs using a static offline preference dataset $\{(x,y_w,y_l)\}$, where $y_w$ and $y_l$ denote the preferred (winning) and less preferred (losing) %(loss) 
responses to the input prompt $x$, respectively.
% to directly align LLMs with a static offline preference dataset $\{(x,y_w,y_l)\}$.
% \czq{
% Nevertheless, due to the offline nature, these methods are heavily reliant on the quality of the preference dataset and substantially impact the alignment process 
% }
Nevertheless, due to their offline nature, these methods rely heavily on %are heavily reliant on
the quality of the preference dataset, which can substantially impact the alignment process \citep{iclr24statistical, icml24onpolicydata, icml24bridging, a_recipe}.

% However, given their offline nature, these methods become highly dependent on the quality of the preference dataset, which can significantly influence the alignment process \citep{iclr24statistical, icml24onpolicydata, icml24bridging}.  % 这里点出数据重要，引出下面通过挑选高质量数据的一些方法

Recent research has focused on improving LLM alignment by enhancing the quality of preference datasets, employing strategies such as selecting high-quality data for training \citep{rs_dpo, active_pref_learn, filter_dpo} and re-weighting the loss based on data quality \citep{cringe, reward_diff_dpo, wu2024betadpo}.
% Recently, researchers have been exploring different methods to improve LLM alignment by enhancing data quality in preference datasets, such as selecting high-quality data for training \citep{rs_dpo, active_pref_learn, filter_dpo} and re-weighting loss by data quality \citep{cringe, reward_diff_dpo}.
% 分为2种
These approaches primarily rely on two %distinct 
metrics to assess %for determining 
the quality of preference pair $(x,y_w,y_l)$ in the offline dataset:
% the \textit{reward difference} metric \citep{reward_diff_dpo}:
the \textit{explicit reward margin} metric \citep{reward_diff_dpo}:
$$M_r(x,y_w,y_l) = |r(x,y_w) - r(x,y_l)|,$$
based on the explicit reward $r(x,\cdot)$ provided by a reward model;
and the \textit{implicit reward margin} metric \citep{a_recipe, active_pref_learn}: 
$$M_\pi(x,y_w,y_l) = |\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|,$$ 
based on the implicit reward \citep{DPO} $\hat r_\theta(x,\cdot) = \beta\log\frac{\pi_\theta(\cdot|x)}{\pi_\mathrm{ref}(\cdot|x)}$ derived by the LLM policy $\pi_\theta(\cdot|x)$ and a reference policy $\pi_\mathrm{ref}(\cdot|x)$.
% the LLM policy $\pi_\theta(\cdot|x)$.
% While it has been demonstrated that selecting data with \textit{larger explicit reward margins} \citep{reward_diff_dpo, eva} or \textit{smaller implicit reward margins} \citep{a_recipe} for training can improve alignment performance, these two metrics often provide contradictory guidance. 
% \czq{
% Though the alignment performance can be boosted with the data selection guided by the principle of \textit{larger explicit reward margins} \citep{reward_diff_dpo, eva} or \textit{smaller implicit reward margins} \citep{a_recipe}, these two metrics often offer contradictory guidance when choosing a training data.
% }
While it has been demonstrated that selecting data with \textit{larger explicit reward margins} \citep{reward_diff_dpo, eva} or \textit{smaller implicit reward margins} \citep{a_recipe} for training can improve alignment performance, these two metrics could provide conflicting guidance for the same data.
% To illustrate this conflict, we present two examples from the preference dataset used by SimPO \citep{SimPO} in Figure \ref{fig:teaser_a}, where these two metrics deliver opposing data quality evaluations.
% This inconsistency naturally raises a critical question: 
To illustrate this conflict, we present two examples from the preference dataset used in SimPO (Figure~\ref{fig:teaser_a}), where the same preference pair is deemed high quality by one metric but low quality by the other.
Such inconsistency in data quality evaluations naturally raises a critical question: 
% To illustrate this conflict, we present two examples from the preference dataset used in SimPO (Figure~\ref{fig:teaser_a}), where the same preference pair $(x,y_w,y_l)$ is deemed high quality with large $M_r$, but low quality with large $M_\pi$, and vice versa. These two metrics deliver opposing data quality evaluations.
% Such inconsistency naturally raises a critical question: 

%%%%%%%%%%%%%%%%%%%%% Teaser fig %%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/teaser.pdf}
        \caption{Two examples illustrating the conflict between explicit and implicit reward margins}
        \label{fig:teaser_a}
    \end{subfigure}%
    % \hfill
    \begin{subfigure}{0.28\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/perf_per_scores_teaser_all.pdf}
        \caption{Data selection performance}
        \label{fig:teaser_b}
    \end{subfigure}
    \vspace{-6pt}
    \caption{(\ref{fig:teaser_a}) \textbf{Contradiction of existing metrics.} 
    The left example, with large explicit and implicit reward margins, is rated as ``high quality'' by explicit reward margin but ``low quality'' by implicit reward margin. The right example, where both margins are small, is rated as ``low quality'' by the explicit margin but ``high quality'' by the implicit margin. 
    In both cases, the implicit margins are already aligned with the explicit ones, making both data rated as ``low quality'' by our metric, \ie no need for further training.
    % The left example, where both explicit reward margin and implicit reward margin are large, will be graded as ``high quality'' data by explicit reward margin but ``low quality'' data by implicit reward margin. Conversely, the right example, where both metrics are small, is considered ``low quality'' by explicit reward margin but ``high quality'' by implicit reward margin. 
    (\ref{fig:teaser_b}) \textbf{Enhanced performance by data selection.} Llama-3-8b-instruct and Gemma-2-9b-it's performance on Top-40\% data subset selected by different data metrics (uniform refers to uniformly sampling 40\% data from the dataset), with our proposed metric achieving the highest results.}
    \label{fig:teaser}
    \vspace{-8pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-2pt}
{\it 
Is there a more universal and theoretically grounded metric for evaluating preference data quality in alignment?
}
\vspace{-2pt}


In response to this question, we revisit %investigate 
the alignment objective in RLHF, where the %whose 
optimum is characterized by \citet{DPO}: $\hat r_\theta^*(x,y) = r(x,y) + c(x)$, with $c(x)$ being a partition term independent of $y$.
% This suggests that the implicit reward margin and explicit reward margin should be equivalent at optimality: $|\hat r_{\theta}^*(x,y_w) - \hat r_{\theta}^*(x,y_l)| \equiv |r(x,y_w)-r(x,y_l)|.$ 
This suggests that the implicit reward margin at optimality should be equivalent to explicit reward margin: $|\hat r_{\theta}^*(x,y_w) - \hat r_{\theta}^*(x,y_l)| \equiv |r(x,y_w)-r(x,y_l)|.$ 
Building upon this insight, we propose the \textit{\methodname{}} metric, $M_{AP}$,
which quantifies the gap from the current model's implicit reward margin 
to the target explicit reward margin:
% \czq{The proposed metric estimates }
% This metric estimates the model's ``potential'' to align on the preference data $(x,y_w,y_l)$:
%\footnote{Following \citet{SimPO}, we also denote the length-normalized log probability $\beta\log\pi_\theta(y|x)/|y|$ as an implicit reward, resulting in an alternative implementation of implicit margin.} 
% designed to assess the gap from the current model's implicit margin\footnote{Following \citet{SimPO}, we also denote the length-normalized log probability $\beta\log\pi_\theta(y|x)/|y|$ as an implicit reward, resulting in an alternative implementation of implicit margin.} to the final reward difference target, thus estimating the model's ``potential'' to align on the preference data $(x,y_w,y_l)$:
% 这里按照SimPO的格式写implicit margin, 后面正文再介绍实际需要通过normalize + alpha 加权
% \begin{align}\label{eqn:metric_def}
% &M_{AP}(x,y_w,y_l) = \\
% &\underbrace{|r(x,y_w) - r(x,y_l)|}_\text{reward difference target} 
% - \alpha\underbrace{\left|\frac{\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\log\pi_\theta(y_l|x)}{|y_l|}\right|}_\text{current implicit margin (length normalized)}.\notag
% \end{align}
\begin{equation}\label{eqn:metric_def}
\begin{aligned}
&M_{AP}(x,y_w,y_l)\\
= &\underbrace{|r(x,y_w) - r(x,y_l)|}_\text{target explicit reward margin} 
- \underbrace{\left|\hat r_{\theta}(x,y_w) - \hat r_{\theta}(x,y_l)\right|}_\text{current implicit reward margin}.
\end{aligned}
\end{equation}
% \begin{align}\label{eqn:metric_def}
% &M_{AP}(x,y_w,y_l) = \\
% &\underbrace{|r(x,y_w) - r(x,y_l)|}_\text{reward difference target} 
% - \underbrace{\left|\frac{\beta\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\beta\log\pi_\theta(y_l|x)}{|y_l|}\right|}_\text{current implicit margin (length normalized)}.\notag
% \end{align}
This metric estimates the model's ``potential'' to align on the preference data by measuring how much the model can improve its preference discrimination:
as shown in Equation \eqref{eqn:metric_def}, a preference pair $(x,y_w,y_l)$ with a \textit{higher \methodname{}} score indicates a significant reward improvement of the preferred response
%%% suggests a notable reward improvement of chosen response 
$y_w$ over $y_l$ while the model cannot sufficiently differentiate them (\ie similar implicit rewards), reflecting the large potential for %space for further 
alignment enhancement. %an opportunity for alignment enhancement.
% As shown in Equation \eqref{eqn:metric_def}, a preference pair $(x,y_w,y_l)$ with a \textit{higher \methodname{}} score indicates a significant reward improvement of the preferred response
% %%% suggests a notable reward improvement of chosen response 
% $y_w$ over $y_l$ while the model yields similar implicit rewards, reflecting the space for further alignment enhancement. %an opportunity for alignment enhancement.
To validate this, we conduct a preliminary experiment by benchmarking models trained on the top-rated data subsets evaluated by various metrics within SimPO's preference dataset.
% top-40\% preference pairs, rated and selected using various metrics within SimPO's preference dataset.
% within the UltraFeedback dataset \citep{cui2024ultrafeedback}. 
As depicted in Figure \ref{fig:teaser_b},  our proposed $M_{AP}$ metric %our \methodname{} metric 
outperforms other metrics, verifying its effectiveness in identifying high-quality data for alignment.
% As depicted in Figure \ref{fig:teaser_b}, our metric entails the highest performance compared with other metrics, affirming its effectiveness in selecting high-quality data for alignment.

%%%%%% 之前写的这一段过于复杂了
% \begin{align}\label{eqn:metric_def}
% &M_{AP}(x,y_w,y_l) = \\
% &\underbrace{|r(x,y_w) - r(x,y_l)|}_\text{reward difference} 
% - \alpha\underbrace{\left|\frac{\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\log\pi_\theta(y_l|x)}{|y_l|}\right|}_\text{(length normalized) implicit margin}.\notag
% \end{align}
% As shown in Equation \eqref{eqn:metric_def}, our metric integrates two components: the first term is the reward difference, measuring the explicit improvement in reward by the chosen response $y_w$ over the less preferred response $y_l$; the second term, corresponding to a length-normalized implicit margin\footnote{Following \citet{SimPO}, we also denote the length-normalized log probability $\log\pi_\theta(y|x)/|y|$ as an alternative version of implicit reward $\hat r_\theta(x,y)$.}, captures the model's ability to recognize preferences between $y_w$ and $y_l$.
% % Combining both terms, our metric measures the divergence between the model's current judgment of the pair's difference and the actual reward difference, thereby revealing the potential for alignment improvement.
% Jointly considering both aspects, our \textit{\methodname{}} metric identifies preference pairs with notable reward improvements that the model has yet to align on, reflecting an opportunity for alignment enhancement.
% Empirically, we conduct a preliminary experiment to validate this metric. We benchmark the performance of models trained with the top-40\% data subset rated by different metrics within the UltraFeedback dataset \citep{cui2024ultrafeedback}. 
% As depicted in Figure \ref{fig:teaser_b}, our metric enables the highest performance for both the Llama-3 \citep{llama3report} and Gemma-2 \citep{gemma2} models, affirming its effectiveness in selecting high-quality data for alignment.

% 数据质量指标的作用不仅局限在选择数据，更重要是其可以作为guidance to generate data in self play 
% 解释 selfplay
% 在 selfplay 中怎么用
% 通过self-play framework, 我们可以让模型使用少量seed数据，生成更多符合我们标准的偏好数据，
% 既能摆脱数据量的限制，还能进一步增强偏好数据的质量，实现更快更好的对齐


% Furthermore, the proposed metric can extend beyond merely selecting high-quality preference pairs in \textit{existing} datasets and generalize to %; it can be generalized to 
% scenarios involving \textit{additional} data, 
Furthermore, $M_{AP}$ not only supports selecting high-quality preference pairs in \textit{existing} datasets, but also generalizes well to scenarios involving \textit{additional} data, 
such as self-play alignment frameworks. 
In these frameworks, LLMs actively generate new training data through response sampling \citep{NLHF, SPIN} or prompt creation \citep{eva}.
% By leveraging the LLM %the trained LLM itself
% to generate new responses $y$ \citep{NLHF, SPIN} or prompts $x$ \citep{eva}, self-play alignment strategies empower LLMs to continuously learn with self-generated data, emerging as a novel and efficient approach for alignment. 
Our metric seamlessly integrates into such self-play frameworks, facilitating the identification of high-quality data within the intricate self-generated content.
Consequently, it enables efficient expansion of high-quality preference data from a minimal seed dataset,
% it enables the generation of high-quality preference data from a minimal seed dataset, 
thereby circumventing the constraints of static datasets and enhancing both alignment efficacy and efficiency.

We evaluate our proposed $M_{AP}$ metric through extensive experiments within such a %this 
self-play framework.
Using $M_{AP}$, %our metric, 
we first select top-rated subsets from the self-generated dataset for continued policy model training, achieving significant improvements over the current state-of-the-art %(SOTA) 
results across various base models (\eg Llama \citep{llama3report}, Gemma \citep{gemma2}) and learning algorithms (\eg DPO \citep{DPO}, SimPO \citep{SimPO}).
Scaling up training by enlarging the generate-then-selected dataset further yields remarkable %rapid 
performance gains, even surpassing the model trained on twice the size of the UltraFeedback dataset.
Additional experiments demonstrate that increasing training iterations under our metric's guidance leads to consistent performance gains over the models trained on the default dataset.
% First, we apply our metric to the self-generated dataset, extract the top-rated subsets, and use them for continued training of the policy model, achieving significant improvements over current state-of-the-art (SOTA) results across various base models (\eg Llama \citep{llama3report}, Gemma \citep{gemma2}) and learning algorithms (\eg DPO \citep{DPO}, SimPO \citep{SimPO}).
% % resulting in significant performance improvements over current state-of-the-art (SOTA) results across various base models and learning algorithms.
% We then scale up training %the training procedure 
% by increasing the quantity of generated-and-selected data. 
% As the selected dataset size expands, the trained model exhibits rapid performance improvements, 
% surpassing even models trained on twice the size of the UltraFeedback dataset.
% %achieving even better results compared to models trained on twice the size of the UltraFeedback dataset.
% Additionally, we explore the impact of increased training iterations --- guided by our metric, the performance of the trained model consistently improves with more iterations, leading to significant gains over models trained on default datasets. 
These empirical results %robustly 
validate the practical utility of our proposed $M_{AP}$ %\textit{\methodname{}} 
metric across diverse training conditions, providing a robust %comprehensive 
solution for enhancing LLM alignment through optimized preference data selection and utilization.

% Although our data filtering experiment supports the efficacy of the proposed metric,
% its benefits may be limited by the fixed nature of offline datasets. 
% %the benefits from such data quality metrics may be constrained by the fixed nature of offline datasets. 
% To maximize the utility of our metric, we integrate it into the data generation process of self-play alignment frameworks \citep{eva, SPIN}. 
% Following the strategy in \citet{eva}, we generate additional data consistent with our data quality metric, by prompting the LLM to rewrite existing prompts and evolve new data \citep{evolinstruct}.
% This self-play data generation process allows for the more comprehensive utilization and evaluation of different data quality metrics.

% % 最后一段讲实验效果，因此后面可能还会调整里面的结果。
% We subsequently evaluate our method through extensive experiments within this data generation framework.
% First, we apply our metric to the evolved dataset, extract the top-rated subsets, and use them for continued training of the policy model, resulting in significant performance improvements over current state-of-the-art (SOTA) results across various base models and learning algorithms.
% validate our metric by selecting the top-rated subsets within the evolved dataset for training and present strong performance improvement over the current state-of-the-art (SOTA) results across various base models and learning algorithms.
% % We first utilize our metric to select the top-rated subsets within the evolved dataset and find that the trained models surpass the current state-of-the-art (SOTA) results across various base models (e.g., Llama, Gemma) and learning algorithms (e.g., DPO, SimPO), thus certifying our metric's ability to identify high-quality preference data in the evolving dataset setting.
% We then scale up the training procedure by increasing the quantity of evolved-then-selected data. 
% As the selected dataset size expands, the trained model exhibits rapid performance improvements, 
% surpassing even models trained on twice the size of the UltraFeedback dataset.
% %achieving even better results compared to models trained on twice the size of the UltraFeedback dataset.
% Additionally, we explore the impact of increased training iterations --- guided by our metric, the performance of the trained model consistently improves with more iterations, leading to significant gains over models trained on default datasets. 
% These experimental results robustly validate the practical applicability of our proposed \textit{\methodname{}} metric in diverse training conditions, providing a comprehensive solution for enhancing LLM alignment through optimized preference data selection and utilization.


%%% 之前版本的方法 %%%
% In response to this question, we propose the \textit{\methodname{}} metric:
% \begin{align*}
% &M_\mathrm{AP}(x,y_w,y_l) = \\
% &|r(x,y_w) - r(x,y_l)| - \alpha\left|\frac{\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\log\pi_\theta(y_l|x)}{|y_l|}\right|,
% \end{align*}
% where the first term corresponds to the reward difference, measuring the explicit improvement in reward by the chosen response $y_w$ over the alternative response $y_l$; the second term represents the negative implicit reward margin\footnote{Following \citet{SimPO}, we also denote the length-normalized log probability $\log\pi_\theta(\cdot|x)/|\cdot|$as an implicit reward.}, which captures the model's difficulty in distinguishing preferences between the paired responses $y_w$ and $y_l$.
% Rather than emphasizing a single perspective (explicit or implicit reward) of the preference data, our metric evaluates the divergence between the model's current judgment on the pair's difference and the true reward difference, thereby indicating the potential for alignment improvement.
% % 指标的效果 (要不要加一句对图中例子的判别结果？)
% As shown in Figure \ref{fig:teaser_b}, the model trained with the top-rated data subset selected by our metric renders the highest performance for both the Llama-3-8b \cite{llama3report} and Gemma-2-9b models \cite{gemma2}.


%%% 之前版本的背景 %%%
% The reward difference metric \citep{reward_diff_dpo} $$M_r = |r(x,y_w) - r(x,y_l)|,$$ based on the explicit reward $r(x,\cdot)$ provided by a reward model, measures the improvement in reward of the chosen response $y_w$ over the less preferred one $y_l$.
% The implicit margin metric \citep{a_recipe, active_pref_learn} $$M_\pi = |\hat r(x,y_w) - \hat r(x,y_l)|,$$ based on the implicit reward \citep{DPO} $\hat r(x,\cdot) = \beta\log\frac{\pi_\theta(\cdot|x)}{\pi_\mathrm{ref}(\cdot|x)}$ derived by the LLM $\pi_\theta(\cdot|x)$, measures the model's current ability to distinguish the paired responses.
% % 说明存在冲突
% While it has been demonstrated that prioritizing data with \textit{higher reward differences} \citep{reward_diff_dpo, eva} or \textit{lower implicit margins} \citep{a_recipe} during training can improve alignment performance, these two metrics can be contradictory: As shown in Figure \ref{fig:teaser_a}, when the model's implicit reward is already aligned with the reward value, these two metrics become contradictory. 
% This naturally raises the following question: 

% {\it 
% Is there a more universal and theoretically grounded metric for evaluating preference data quality in Alignment?
% }

% Motivated by this question, we propose the following \textit{preference mismatch} metric:
% $$|r(x,y_w) - r(x,y_l)| - \alpha\left|\frac{\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\log\pi_\theta(y_l|x)}{|y_l|}\right|,$$
% where the first part corresponds to the reward difference, measuring the explicit reward gain of responding $y_w$ rather than $r_l$, and the second part corresponds to the implicit reward margin\footnote{Following \citet{SimPO}, we also denote the length-normalized log probability $\log\pi_\theta(\cdot|x)/|\cdot|$as an implicit reward.}


