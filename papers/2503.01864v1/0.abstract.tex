%%%%%%%%%%%%%%%%%% 摘要 %%%%%%%%%%%%%%%%%%
% Preference Learning 对于LLM很重要，而其中的偏好数据质量是影响性能的重要因素
% 已有数据指标从 explicit/implicit reward margin 出发， 但存在分歧和冲突
% 为了解决此问题，我们提出 alignment potential 指标，度量当前reward margin到最优解的reward margin的距离，从而体现了potential for alignment enhancement
% 通过筛选数据实验，我们的指标在不同模型和benchamrk上都超过了现有指标
% 进一步扩展到造数据的场景下，通过从intricate generated context中筛选高质量内容，只用small seed data 便可构造高质量数据集
% 借助我们的指标，能超过SOTA的结果（eva）并且在多轮、更多数据下都有持续提升(一半数据、更强性能)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Preference learning is critical for aligning large language models (LLMs) with human values, with the quality of preference datasets playing a crucial role in this process. 
While existing metrics primarily assess data quality based on either \textit{explicit} or \textit{implicit} reward margins, they often provide contradictory evaluations for the same data.
To address this issue, we propose a new metric of \textit{\methodname{}}, $M_{AP}$, 
% introduce the \methodname{} metric, 
which quantifies the gap from the model's \textit{current implicit} reward margin to the \textit{target explicit} reward margin, thereby estimating the model's potential to align on the preference data.
Empirical results demonstrate that training on the data selected by $M_{AP}$ consistently enhances alignment performance, surpassing existing metrics across different base models and optimization objectives.
Furthermore, our method can be extended to self-play data generation frameworks, where we use this metric to identify high-quality data within the self-generated content by LLMs. 
Under this data generation scenario, our method surpasses current state-of-the-art %(SOTA) results
methods across various training settings and demonstrates continuous improvements
with increasing dataset size and training iterations.
% in alignment performance as dataset size and training iterations increase.
\end{abstract}