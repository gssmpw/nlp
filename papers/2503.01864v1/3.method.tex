\section{The \MethodName{} Metric}
This section introduces our \methodname{} metric, devised to assess the quality of preference data $(x,y_w,y_l)$ for alignment. \S\ref{sec:derive_AP_metric} elaborates on the formulation of \methodname{} metric. \S\ref{sec:metric_exp} empirically evaluates the performance of various metrics by using them to select preference data for training. \S\ref{sec:theoretic_analysis} provides a theoretical analysis justifying the proposed metric's ability to identify high-quality data for model alignment.

% \subsection{Introduce the \MethodName{} Metric}\label{sec:derive_AP_metric}
\subsection{Metric Formulation}\label{sec:derive_AP_metric}
%%%%% 现在的写法是：
% 1. 通过对比最优解的差距 r_w - r_l 和当前的差距 \hat r_w - \hat r_l
% 可以度量模型的被改进空间：| (r_w - r_l) - (\hat r_w - \hat r_l) |
% 2. 然后我们首先把外层的绝对值去掉，因为一般的对齐loss是单向优化的 （weakness: 如果考虑IPO这种回归loss呢？）
% 3. 再进一步由于reward存在噪声，引入内层的绝对值 |r_w - r_l| - |\hat r_w - \hat r_l|
%%%%%

Preference optimization methods (\eg DPO, SimPO) leverage preference datasets to transform unaligned LMs into models that align with human values.
A natural principle for evaluating the quality of preference data $(x,y_w,y_l)$ is to measure how much the model can improve its alignment with human preferences by learning from such data.
% With the guidance of preference datasets, preference learning methods can transform unaligned LMs into language models that respond according to human values.
% Therefore, a straightforward principle for determining the quality of preference data $(x,y_w,y_l)$, is to evaluate how much better the model can align with human preferences upon learning from such data.
This can be essentially quantified by measuring the discrepancy between the current model and an aligned optimal state on that data.
For the aligned optimal policy $\pi_\theta^*$, Equation~\eqref{eqn:opt_policy} describes an optimal condition:
$$r^*(x,y) = \hat r_\theta^*(x,y) + \log Z(x),$$
which leads to the following equivalence for any preference data $(x,y_w,y_l)$:
\begin{equation}\label{eqn:opt_equiv}
    r^*(x,y_w) - r^*(x,y_l) \equiv \hat r_\theta^*(x,y_w) - \hat r_\theta^*(x,y_l).
\end{equation}
Based on this, the discrepancy between the current model and the aligned optimal on any given preference data can be quantified by the $\ell_1$-distance, resulting in the $M_1$ metric:
\begin{align}\label{eqn:M_ideal_def}
    &M_1(x,y_w,y_l;\theta,r^*)\\ 
    \mathrm{:=}&|(\hat r_\theta^*(x,y_w) - \hat r_\theta^*(x,y_l))- (\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l))|\notag\\
    =&|\underbrace{(r^*(x,y_w) - r^*(x,y_l))}_\text{given by aligned optimum $\pi_\theta^*$} - 
    \underbrace{(\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l))}_\text{given by current model $\pi_\theta$}|.\notag
\end{align}
Here the term $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)$ represents the reward margin (without absolute value) between $y_w$ and $y_l$ as evaluated by the model $\pi_\theta$, indicating the model's preference judgment between $y_w$ and $y_l$ \citep{DPO,active_pref_learn}.
Consequently, $M_1$ measures the gap between preferences given by the current model $\pi_\theta$ and the aligned optimum $\pi_\theta^*$ for the data $(x,y_w,y_l)$.
With this metric, a larger gap indicates greater potential for improvement (\ie high-quality data), while a smaller gap suggests less need for enhancement (\ie low-quality data).
% Here the term $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)$ represents model $\pi_\theta$'s evaluation on the preference between $y_w$ and $y_l$ \citep{DPO,active_pref_learn}.
% Consequently, $M_1(x,y_w,y_l)$ precisely measures the gap between preferences given by the current model $\pi_\theta$ and the aligned optimum $\pi_\theta^*$ on data $(x,y_w,y_l)$, with a larger gap indicating a greater potential for improvement (\ie high quality) while smaller gap implying less need for enhancement (\ie low quality).

Nevertheless, direct computing $M_1$ requires access to the latent, inaccessible human preference reward $r^*$.
% direct computation of this function mandates access to human preference's intrinsic, inaccessible reward $r^*$. 
% To render this metric practically applicable, we will employ a reward model $r(\cdot)$ for reward 
To make this metric practically applicable, we propose using a reward model $r(\cdot)$ for guidance and introduce two key adaptations for practical offline preference learning: \textit{unidirectional calibration} and \textit{reward noise regularization}.
%, for enhanced applicability in practical offline preference learning contexts.

\textbf{Unidirectional calibration.}
In typical offline preference learning scenarios (\eg DPO), preferences $y_w\succ y_l|x$ in the dataset are annotated by an external reward model such that $r(x,y_w) > r(x,y_l)$, rather than being sampled from real human preference probabilities as in Equation~\eqref{eqn:BT_model}.
Given this deterministic nature, objectives like DPO and SimPO adopt a \textit{unidirectional} update strategy to consistently increase the margin $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)$ (\cf  Equation~\eqref{eqn:DPO_loss}), as opposed to regressing the target aligned optimum $\hat r^*(x,y_w) - \hat r^*(x,y_l)$  \citep{DNO}.
In contrast, the absolute value in $M_1(x,y_w,y_l)$ function measures a \textit{bidirectional} gap relative to the centering optimum $r^*(x,y_w) - r^*(x,y_l)$.
To reconcile this, we apply a unidirectional calibration by omitting the absolute value from $M_1(x,y_w,y_l)$:
\begin{align}\label{eqn:M_+_def}
    &M_+(x,y_w,y_l;\theta,r^*) \\
    \mathrm{:=} & (r^*(x,y_w) - r^*(x,y_l)) - (\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)).\notag
\end{align}
Large $M_+(x,y_w,y_l)$ values thus highlight preference data where $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)$ is small, implying a need for increases consistent with offline preference methods.

\textbf{Reward noise regularization.}
Moreover, substituting $r^*$ with $r$ can introduce reward model noise \citep{rlhf_openproblems},
which might lead to erroneous preference annotations \citep{rlhf-stiennon2020learning, reward_overopt}.
To illustrate this phenomenon, we present a preference data example from SimPO's dataset in Figure~\ref{fig:noise_example} (further details are provided in Table~\ref{example:rm_wrong} in Appendix~\ref{appendix:examples}).
In this example, the reward model erroneously assigns a higher reward to the incorrect answer.
% To illustrate this phenomenon, we present a preference data example from SimPO's dataset in Table~\ref{example:rm_wrong}, where the reward model erroneously assigns a higher reward to the incorrect answer.
In contrast, the implicit reward given by the LLM $\pi_\theta$ in this example correctly identifies the preferable answer, reflecting LLM's ability to discern certain preferences, as supported by recent studies \citep{boostrapping_implicit_r, CDR_annotation}.
% spread_annotate}. \kxnote{(todo: update iclr ref)}

%%%%%%%%%%%%%%% Example Fig %%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/example.pdf}
    \vspace{-5pt}
    \caption{Reward noise example from SimPO's dataset, where the less preferred response $y_l$ by RM is the correct answer. $M_+$ will mislabel this data as high-quality, which the $M_{AP}$ metric avoids.}
    \label{fig:noise_example}
    \vspace{-5pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{example}
%     A preference data from SimPO's dataset: 
% \end{example}

This inspires us to revisit the $M_+$ metric.
% observation prompts a reassessment of the $M_+$ metric:
Clearly, it tends to prioritize data where $r(x,y_w) - r(x,y_l) \gg 0$ (\ie $y_w$ is strongly preferred by RM) while $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l) \ll 0$ (\ie $y_w$ is strongly disliked by LLM). 
%promote data where $r(x,y_w) - r(x,y_l) \gg 0$, \ie $y_w$ strongly preferred by RM, 
% while $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l) \ll 0$, \ie $y_w$ strongly disliked by LLM. 
% While such strong contradictions could reflect opportunities to align LLM with the reward model's preference, 
While such strong contradictions may suggest mistaken preference judgments by the LLM that necessitate alignment training, 
they also risk amplifying faulty preference annotations caused by the reward model noise. %may also indicate the risk of faulty preference annotations by the reward model.
Considering the aforementioned example again where the reward model assigns incorrect rewards, the RM assigns $r(x,y_w) - r(x,y_l) = 6.2$ while the LLM evaluates $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l) = -5.5$, resulting in an inflated $M_+$ score of $11.7$, nearly seven times the dataset's average $M_+$ score.
% For instance, in the aforementioned example where reward model assigns incorrect rewards, we have $r(x,y_w) - r(x,y_l) = 6.3$ and $\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l) = -5.6$, leading to a large $M_+$ score of $11.9$, approximately seven times the average $M_+$ score of the dataset.
As a result, this noisy data would be incorrectly rated as ``high quality'' by $M_+$. %the $M_+$ metric. %this erroneous preference data will be mistakenly rated as ``high quality'' by the $M_+$ metric.

The root issue lies in the contradictions between reward model annotations and LLM judgments, which may signal reward model noise when the LLM can accurately discern certain preferences.
To address this, we revise M+ by reintroducing absolute values as a regularizer, leading to the proposed \methodname{} metric:
% The root of this issue is that when the LLM can distinguish certain preferences, contradictions between the reward model's annotations and the LLM's judgments suggest a higher likelihood of reward model noise.
% With such contradictions naturally promoted by $M_+$, a pragmatic solution is to revise this metric to avoid such unreliable contradictions.
% Motivated by this denoising rationale, we adjust the $M_+$ metric by reintroducing absolute values as a regularizer, culminating in the proposed \methodname{} metric: (we also add an absolute value on rewards for symmetry)
\begin{align}\label{eqn:DAP_def}
    &M_{AP}(x,y_w,y_l;\theta,r) \\
    \mathrm{:=} & |r(x,y_w) - r(x,y_l)| - |\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|\notag.
\end{align}
This adjustment ensures that data with high $M_{AP}$ scores indicate substantial reward improvements, while the LLM shows uncertainty in preference determination (\ie similar implicit rewards).
It better identifies data with alignment potential and avoids unreliable contradictory annotations.
% By incorporating the absolute value regularization, data with high $M_{AP}$ scores thus indicate large reward improvements while the model remains uncertain in preference determination (\ie similar implicit rewards), not only suggesting potential for alignment enhancement but also avoiding untrustworthy contradictory annotations.

For the previous example, our \methodname{} metric yields a significantly lower score $M_{AP} = 0.7$, below the dataset's average $\overline{M}_{AP}\approx 1.3$.
% Specifically, when dealing with the previous example, the proposed \methodname{} metric yields a significantly lower score $M_{AP} = 0.7$, below the average $\overline{M}_{AP}\approx 1.3$ across the dataset. 
Consequently, this noisy preference data is rated as ``low quality'' by the $M_{AP}$ metric, offering robust data quality evaluation against reward noise. 
Furthermore, we validate the regularization term in $M_{AP}$ by applying both $M_+$ and $M_{AP}$ metrics to rate and select top-10\% preference pairs in SimPO's dataset and prompting GPT-4 to reassess the selected pairs against the original reward-model annotations. 
As shown in Figure~\ref{fig:gemma_agree}, pairs selected by $M_{AP}$ exhibit significantly higher agreement with %demonstrate a notably higher degree of agreement with 
GPT-4, indicating reduced annotation noise\footnote{While GPT-4 might also make mistakes, alignment with its preferences suggests at least a lower likelihood of annotation noise.}, and evidencing the regularization's effectiveness in $M_{AP}$.

%%%%%%%%%%%%%%% 绝对值去噪效果图 %%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/gemma_agreement.pdf}
    \vspace{-5pt}
    \caption{GPT-4 agreement with reward model's annotation on data selected by $M_+$ or $M_{AP}$ metric from \href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{SimPO's dataset}. The data selected by $M_{AP}$ has a notably higher agree rate than $M_+$, indicating less reward annotation noise.}
    \label{fig:gemma_agree}
    \vspace{-8pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Relationship with existing metrics.}
The proposed \methodname{} metric can be split into two components: the target \textit{explicit reward margin} term $M_r = |r(x,y_w) - r(x,y_l)|$, derived from aligned optimum $\pi_\theta^*$, and the current \textit{implicit reward margin} term $M_\pi = |\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|$, given by current model $\pi_\theta$:
$$
M_{AP} = \underbrace{|r(x,y_w) - r(x,y_l)|}_{\substack{\text{target explicit reward margin} \\ \text{(by aligned optimum $\pi_\theta^*$)}}} - 
\underbrace{|\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|}_{\substack{\text{current implicit reward margin} \\ \text{(by current model $\pi_\theta$)}}}
$$
While the $M_{AP}$ metric measures the gap from the current implicit reward margin to the target explicit reward margin, thereby serving as an indicator for alignment potential and data quality, each of these margins has been independently utilized as a data quality metric.
% each of these margins can independently function as a metric for data quality.
Empirical studies have validated that selecting data with \textit{larger} $M_r$ \citep{reward_diff_dpo, eva} or \textit{smaller} $M_\pi$ \citep{a_recipe} for training improves alignment performance, which is also consistent with the trend described in $M_{AP}.$
Additionally, as $\hat r_\theta(x,y)=\beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)}$ contains a hyper-parameter $\beta$, the ranking induced by $M_{AP}$ converges with $M_r$ as $\beta\to 0$, and with $-M_\pi$ as $\beta\to\infty$.
% therefore our $M_{AP}$ offers an integrated metric unifying both the explicit and implicit reward perspectives.
Therefore, the $M_{AP}$ metric offers an integrated metric unifying both the explicit and implicit reward perspectives.
More importantly, it underscores the potential for alignment by assessing the discrepancy between target and current reward margins, rather than relying on a singular metric.


\subsection{Empirical Evaluation for Data Metrics}\label{sec:metric_exp}
In this section, we conduct a preliminary experiment to evaluate different data quality metrics, using them to select preference data for alignment training. 
The results %empirical findings 
demonstrate the superior ability of our metric to select high-quality data, leading to improved alignment performance across multiple benchmarks and models.

\textbf{Models and datasets.} We focus on preference learning for the Llama-3-8b \citep{llama3report}  and Gemma-2-9b \citep{gemma2} models. Following \citet{SimPO}, we employ pre-trained instruction-tuned models as SFT models and utilize the same preference datasets in SimPO: \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm}{llama3-ultrafeedback-armorm} and 
\href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{gemma2-ultrafeedback-armorm},
for llama and gemma models respectively.
These datasets are generated by sampling responses from LLMs for prompts in the UltraFeedback dataset \citep{cui2024ultrafeedback} and then annotating preferences with the ArmoRM reward model \citep{ArmoRM}.
Additionally, SimPO provides another llama-based dataset with the preferences annotated using the PairRM reward model \citep{pairRM}: \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}{llama3-ultrafeedback}, which we include % is also utilized 
as an ablation study on different reward models.

%%%%%%%%%%%%%%% metric选数据实验图-llama %%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/perf_per_scores_llama.pdf}
    \vspace{-5pt}
    \caption{Performance of Llama-3-8b-instruct model trained on preference pairs selected by different metrics.}
    % Models are evaluated on AlpacaEval 2 and Arena-Hard benchmarks, with ``WR'' denoting the raw win rate and ``LC'' being the length-controlled win rate.}
    \label{fig:llama_select_by_metrics}
    \vspace{-15pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Data quality metrics.} 
Various data quality metrics are utilized to score preference pairs and select the top-40\% subset (approximately 24k pairs) from the SimPO datasets for further training. 
We compare the proposed \methodname{} metric $M_{AP}$ against the explicit reward margin $M_r$ metric, the negative\footnote{Since $M_\pi$ prioritizes data with smaller scores, we add a negative sign for consistency with the top-$k$ selection procedure.} implicit reward margin $-M_\pi$ metric, and a uniform baseline that randomly samples 40\% of the data for training.
Additionally, when calculating data metrics before training, the model $\pi_\theta$ used for calculating implicit rewards $\hat r_\theta$ is initialized from the reference model $\pi_\mathrm{ref}$, making the DPO's implicit reward $\hat r_\theta = \beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)}$ constant zero.
To address this, we utilize the SimPO's implicit reward $\hat r_\theta^\mathrm{Sim}(x,y)=\beta\log\pi_\theta(y|x)/|y|$ for practical implementation across this paper.
Further implementation details are provided in Appendix~\ref{appendix:implement}.

\textbf{Training and Evaluation.}
We primarily adopt SimPO for preference optimization and include DPO as an ablation study concerning different optimization objectives. 
After preference learning, evaluations are conducted using two widely recognized open-ended instruction-following benchmarks: AlpacaEval 2 \citep{alpacaeval2} and Arena-Hard \citep{arenahard}. 
These benchmarks assess the model's general conversational abilities across diverse queries and are extensively used in the research community. 
Our results report the win rate for Arena-Hard and both length-controlled (LC) and raw win rates (WR) for AlpacaEval 2.

\textbf{Empirical Results.} 
Our \methodname{} metric consistently outperforms existing data quality metrics:
As depicted in Figures~\ref{fig:llama_select_by_metrics} and \ref{fig:gemma_select_by_metrics}, while all three metrics can enhance the alignment performance, our \methodname{} metric $M_{AP}$ achieves the highest performance across all benchmarks and models.
Particularly, models trained on data selected via our metric significantly outperform the uniform baseline by 2.5 and 6.2 points on Alpaca LC, and by 4.7 and 7.1 points on Arena WR.
Such notable performance enhancement underscores the effectiveness and robustness of our metric in identifying high-quality preference data for alignment training.

\textbf{Ablation Study.}
We conduct in-depth ablation studies to rigorously evaluate various data quality metrics under different reward models and training objectives (\cf Appendix~\ref{appendix:metric_ablation}).
As illustrated in Figure~\ref{fig:metric_ablation}, our \methodname{} metric consistently surpasses existing baselines even with these variations, further validating its efficacy.

% Specifically, we conduct additional experiments using a PairRM-annotated preference dataset from SimPO, distinct from the previously utilized ArmoRM reward model. 
% Moreover, we also verify the trained models' performance with DPO loss as the optimization objective (more details are included in Appendix~\ref{appendix:metric_ablation}).
% As illustrated in Figure~\ref{fig:metric_ablation}, our \methodname{} metric consistently surpasses existing baselines even with these variations, further validating its efficacy.

%%%%%%%%%%%%%%% metric选数据实验图-gemma %%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/perf_per_scores_gemma.pdf}
    \vspace{-5pt}
    \caption{Performance of Gemma-2-9b-it model trained on preference pairs selected by different metrics.}
    \label{fig:gemma_select_by_metrics}
    \vspace{-10pt}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Theoretical Analysis}\label{sec:theoretic_analysis}

Building upon the empirical results showcasing enhanced alignment with data selected by our \methodname{} metric, we now provide a theoretical explanation for its effectiveness.
With the preference learning process transforming unaligned LLM $\pi_\theta$ into the aligned optimum $\pi_\theta^*$, our proposed metric, which selects preference data with a larger gap between $\pi_\theta$ and $\pi_\theta^*$, serves as an adversarial sampler prioritizing ``hard'' examples for training. 
Compared with uniform sampling from the dataset, such a philosophy can intuitively speed up preference learning by prioritizing data with the largest optimization gaps, which requires the most update, to train on. 
Below, we rigorously substantiate this intuition through the lens of stochastic optimization.

% Consider the alignment process transforming unaligned LLM $\pi_\theta$ into the aligned optimum $\pi_\theta^*$, the preference learning algorithms seek to minimize the discrepancy between $\pi_\theta$ and $\pi_\theta^*$.
% Since our proposed metric selects preference data with a larger gap between $\pi_\theta$ and $\pi_\theta^*$, it serves as an adversarial sampler prioritizing worst-case data for training. 
% Compared with uniform sampling from the dataset, such a philosophy can intuitively speed up preference learning by selecting data with the largest gap, which requires the most update, to train on. 
% In this section, we formalize this intuition by examining the alignment process from the perspective of stochastic optimization.
% We provide a theoretical justification for this intuition through the lens of stochastic optimization.

\textbf{Contextual bandit setting.}
Following the standard theoretical frameworks in RLHF \citep{DPO, NLHF, DNO}, we reformulate the problem as a contextual bandit, with a context (prompt) space $\mathcal X$, an arm (response) space $\mathcal Y$ and a reward function $r:\mathcal X\times\mathcal Y\to[0,1]$. 
A policy (LLM) $\pi:\mathcal X\to\Delta\mathcal Y$ maps each context to a probability simplex over the arm space, which is parameterized with $\theta:\mathcal X\times\mathcal Y\to\mathbb R$ using the softmax transformation \citep{softmax_policy} akin to LLMs: $\pi_\theta(y|x) = \frac{\exp(\theta(x,y))}{\sum_{y'\in\mathcal Y}\exp(\theta(x,y'))}$.
The optimization objective is to maximize the KL-regularized reward, as expressed in Equation~\eqref{eqn:rlhf_obj}, with the optimal condition given by: $r(x,y) = \hat r_{\theta^*}(x,y) + \log Z(x).$
To quantify the distance between $\theta$ and $\theta^*$ for theoretical analysis, we utilize the $M_1(x,y,y'|\theta,r)$ metric, which measures the discrepancy between $\pi_\theta$ and optimal policy $\pi_{\theta^*}$ on any preference pairs $(x,y,y')$, and define the following error function:
$$
\mathrm{Dist}(\theta,\theta^*) = \sqrt{\frac{1}{|\mathcal X||\mathcal Y|^2}\sum_{x\in\mathcal Y}\sum_{y,y'\in\mathcal Y}M_1(x,y,y';\theta,r)^2}.
$$
\textbf{Optimization methods.}
We investigate convergence rates of different sampling strategies when optimizing the policy $\pi_\theta$ using the DPO loss, whose population-form is defined as \citep{IPO}:
$$
\min_{\pi_\theta} \mathbb E_{(x,y,y')\sim s}[-p(y\succ y'|x)\log\sigma(\hat r_\theta(x,y) - \hat r_\theta(x,y'))],
$$
where $s: \mathcal X\times \mathcal Y^2 \to [0,1]$ represents the sampling probability, and $p(y\succ y'|x) = \sigma(r(x,y) -r(x,y'))$ denotes the preference given by BT model.
We apply stochastic gradient descent to optimize the DPO loss and compare the performance of two samplers: a uniform sampler $s_u(x,y,y') = \frac{1}{|\mathcal X||\mathcal Y|^2}$ and an adversarial sampler that selects preference data $(x,y,y')$ with the highest $M_1(x,y,y')$ score.
Note that we directly employ the original $M_1$ metric within this theoretical setting, with no need for other calibrations specially designed for practical offline preference learning scenarios (\ie $M_+$ or $M_{AP}$).

\textbf{Convergence rates.}
Our theoretical analysis reveals that by selecting preference data $(x,y,y')$ with the largest gap, \ie the $M_1(x,y,y')$ score, the adversarial sampler achieves more than 2 times faster convergence than the uniform sampler. 
Formally, we state the following theorem (the proof is provided in Appendix~\ref{appendix:proof}):
\begin{theorem}\label{thm:convergence}
Let $T_u(\varepsilon)$ and $T_{adv}(\varepsilon)$ be the (expected) iterations required for the error $\mathrm{Dist}(\theta^t,\theta^*)$ to reduce to $\varepsilon \mathrm{Dist}(\theta^0,\theta^*)$ under uniform and adversarial sampling, respectively. 
With optimal learning rates, we have:
$$
T_{adv}(\varepsilon)< 0.5 T_u(\varepsilon).
$$
\end{theorem}\vspace{-5pt}
We also conduct numerical experiments to validate this theoretical result. As illustrated in Figure~\ref{fig:converge_numerical}, the error reduction rate with the adversarial sampler significantly outperforms that of the uniform sampler, thereby confirming the theoretical acceleration. 
More importantly, such an acceleration provides a theoretical justification for selecting data with higher \methodname{} scores for training.

\section{High-Quality Data Generation}

The proposed data metric extends beyond merely selecting high-quality data subsets in \textit{existing} datasets; 
% more importantly, 
it also applies to \textit{additional} data scenarios, \ie to generate more high-quality data for alignment training.
To facilitate this, we integrate our proposed \methodname{} metric within the evolving alignment (eva) framework \citep{eva}, a self-play data generation technique that employs LLM %the training LLM 
to generate new prompts $x$ and associated response pairs $(y_w,y_l)$ to augment high-quality preference data.

Similar to SimPO, the eva framework begins by constructing %initiates with the creation of 
a preference dataset $\mathcal D=\{(x,y_w,y_l)\}$ by sampling LLM's responses $(y_1,y_2)$ to an existing prompts set $x\in\mathcal X$ (\eg UltraFeedback \citep{cui2024ultrafeedback}), and annotating preferences with a reward model such that $r(x,y_w) > r(x,y_l).$
Then eva augments a high-quality subset within this dataset through a \textbf{select-then-evolve} pipeline, which can be delineated as follows:
\begin{enumerate}[left=0pt,topsep=-2pt,itemsep=-5pt]
\item Initially, eva evaluates each preference pair $(x,y_w,y_l)$ in $\mathcal D$ using the explicit reward margin metric $M_r$.
The top-$k$ subset $\mathcal D_k$ is then selected based on their $M_r$ scores. 
\item Subsequently, eva leverages the LLM itself to rewrite selected prompts $\mathcal X_k$ in $\mathcal D_k$, yielding a set of newly generated prompts $\mathcal X_k'$. 
Therefore a new preference dataset $\mathcal D' = \{(x',y_w',y_l')\}$ can be generated with a similar sample and annotation process as $\mathcal D$.
\end{enumerate}
To generate new prompts $\mathcal X_k'$, eva employs the off-the-shell instructions from EvolInstruct \citep{EvolInstuct}, a popular method for autonomous prompt synthesis with LLM, which involves querying the LLM for in-depth and in-breadth evolving of existing prompts (\cf Appendix~\ref{appendix:implement}). 
% (\ie adding constraints, deepening, concretizing, and increasing reasoning) and in-breadth evolving (\ie diversification) of existing prompts.
The data generation methodology above allows eva to augment data selected by $M_r$ and generate additional preference datasets for alignment training.

Our proposed $M_{AP}$ metric can be seamlessly integrated into this framework for identifying high-quality data, by replacing the $M_r$ metric during the prompts selection phase of eva.
Nonetheless, the subsequent data evolving process does not currently involve data quality evaluation through any metric, thus failing to guarantee the quality of the evolved dataset. 
To address this shortcoming, we modify this self-play procedure by reordering the two phases of eva:
We first evolve the existing prompts $\mathcal X$ in $\mathcal D$ to derive additional dataset $\mathcal D'$ and then select a high-quality subset  $\mathcal D'_k$ from the newly evolved dataset.
This \textbf{evolve-then-select} modification ensures that the resultant dataset $\mathcal D_k'$ aligns with our data quality metric, offering a more rigorous validation of our metric in the data generation scenarios.
Additionally, this data generation and selection process can be iteratively conducted akin to eva, which we formally detail in Algorithm~\ref{algo:eva}.



\setlength{\textfloatsep}{5pt}
\begin{algorithm}
\caption{Eva with \textit{Evolve-then-Select} Pipeline}
\label{algo:eva}
\begin{algorithmic}
\STATE \textbf{Input:}  SFT model $\pi_{\mathrm{SFT}}$, reward model $r$, existing prompts $\mathcal{X}$ and data quality metric $M$.
\STATE Initialize $\pi_{\theta_0}$ with $\pi_{\mathrm{SFT}}$.
\FOR{iteration $t=1,\ldots,T$}
  \STATE preference dataset: $\mathcal{D}\leftarrow \mathrm{GenDataset}(\mathcal{X}, \pi_{\theta_{t-1}},r)$.
  \STATE evolved prompts: $\mathcal{X}' \leftarrow \mathrm{EvolInstruct}(\mathcal{X})$.
  \STATE additional dataset: $\mathcal{D}'\leftarrow \mathrm{GenDataset}(\mathcal{X}', \pi_{\theta_{t-1}},r)$.
  \STATE top-$k$ dataset selected by $M$ metric:\\
  \hspace{1em} $\mathcal{D}_k' = \{(x,y_w,y_l)\in\mathcal D | M(x,y_w,y_l;\theta_{t-1}) \ge \tau_k\}$,\\
  \hspace{1em} with $\tau_k$ being the $k$-th largest $M$ score in $\mathcal D'$.
  \STATE preference optimization on $\mathcal{D} \cup \mathcal{D}_k'$: \\
  \hspace{1em} $\theta_t \leftarrow \theta_{t-1} - \eta\nabla_\theta\mathcal L(\theta_{t-1};\mathcal{D} \cup \mathcal{D}_k').$
\ENDFOR
\STATE \textbf{Return:} optimized policy $\pi_{\theta_T}$
\end{algorithmic}
\end{algorithm}

% As with eva, this data generation and selection process can be iteratively conducted.
% We formally detail the iterative pipeline in Algorithm~\ref{algo:eva}, where we streamline the dataset generation process using the $\mathrm{GenDataset}(\mathcal X, \pi_\theta, r)$ function.
% This function takes the prompts $\mathcal X$, samples responses with LLM $\pi_\theta$, and annotates preferences with reward model $r$ to construct a preference dataset.
% \begin{algorithm}
% \caption{Eva with \textit{Evolve-then-Select} Pipeline}
% \label{algo:eva}
% \begin{algorithmic}
% \STATE \textbf{Input:}  SFT model $\pi_{\mathrm{SFT}}$, reward model $r$, existing prompts $\mathcal{X}$ and data quality metric $M$.
% \STATE Initialize $\pi_{\theta_0}$ with $\pi_{\mathrm{SFT}}$.
% \FOR{iteration $t=1,\ldots,T$}
%   \STATE preference dataset: $\mathcal{D}\leftarrow \mathrm{GenDataset}(\mathcal{X}, \pi_{\theta_{t-1}},r)$.
%   \STATE evolved prompts: $\mathcal{X}' \leftarrow \mathrm{EvolInstruct}(\mathcal{X})$.
%   \STATE additional dataset: $\mathcal{D}'\leftarrow \mathrm{GenDataset}(\mathcal{X}', \pi_{\theta_{t-1}},r)$.
%   \STATE top-$k$ dataset selected by $M$ metric:\\
%   \hspace{1em} $\mathcal{D}_k' = \{(x,y_w,y_l)\in\mathcal D | M(x,y_w,y_l;\theta_{t-1}) \ge \tau_k\}$,\\
%   \hspace{1em} with $\tau_k$ being the $k$-th largest $M$ score in $\mathcal D'$.
%   \STATE preference opt.: $\theta_t \leftarrow \theta_{t-1} - \eta\nabla_\theta\mathcal L(\theta_{t-1};\mathcal{D} \cup \mathcal{D}_k')$
% \ENDFOR
% \STATE \textbf{Return:} optimized policy $\pi_{\theta_T}$
% \end{algorithmic}
% \end{algorithm}
