@article{BPO,
  title={Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment},
  author={Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong},
  journal={arXiv preprint arXiv:2411.10914},
  year={2024}
}

@article{DNO,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}

@article{DPO,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{EvolInstuct,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@inproceedings{IPO,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}

@article{KTO,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@inproceedings{NLHF,
  title={Nash Learning from Human Feedback},
  author={Munos, Remi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Fiegel, C{\^o}me and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{ORPO,
  title={Reference-free monolithic preference optimization with odds ratio},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  journal={arXiv e-prints},
  pages={arXiv--2403},
  year={2024}
}

@article{RAFT,
  title={RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and KaShun, SHUM and Zhang, Tong},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@article{RRHF,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}

@inproceedings{SPIN,
  title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{SPPO,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}

@inproceedings{SimPO,
title={Sim{PO}: Simple Preference Optimization with a Reference-Free Reward},
author={Yu Meng and Mengzhou Xia and Danqi Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=3Tzcot1LKb}
}

@inproceedings{a_recipe,
  title={Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning},
  author={Yang, Sen and Cui, Leyang and Cai, Deng and Huang, Xinting and Shi, Shuming and Lam, Wai},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={6549--6561},
  year={2024}
}

@inproceedings{active_pref_learn,
  title={Active Preference Learning for Large Language Models},
  author={Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{cal_dpo,
  title={Cal-dpo: Calibrated direct preference optimization for language model alignment},
  author={Xiao, Teng and Yuan, Yige and Zhu, Huaisheng and Li, Mingxiao and Honavar, Vasant G},
  journal={arXiv preprint arXiv:2412.14516},
  year={2024}
}

@article{cringe,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}

@inproceedings{cui2024ultrafeedback,
  title={ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{curyy_dpo,
  title={Curry-dpo: Enhancing alignment using curriculum learning \& ranked preferences},
  author={Pattnaik, Pulkit and Maheshwary, Rishabh and Ogueji, Kelechi and Yadav, Vikas and Madhusudhan, Sathwik Tejaswi},
  journal={arXiv preprint arXiv:2403.07230},
  year={2024}
}

@article{eva,
  title={Evolving Alignment via Asymmetric Self-Play},
  author={Ye, Ziyu and Agarwal, Rishabh and Liu, Tianqi and Joshi, Rishabh and Velury, Sarmishta and Le, Quoc V and Tan, Qijun and Liu, Yuan},
  journal={arXiv preprint arXiv:2411.00062},
  year={2024}
}

@article{filter_dpo,
  title={Filtered direct preference optimization},
  author={Morimura, Tetsuro and Sakamoto, Mitsuki and Jinnai, Yuu and Abe, Kenshi and Ariu, Kaito},
  journal={arXiv preprint arXiv:2404.13846},
  year={2024}
}

@inproceedings{icml24bridging,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{magpie,
  title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.08464},
  year={2024}
}

@inproceedings{reward_diff_dpo,
  title={Reward Difference Optimization For Sample Reweighting In Offline RLHF},
  author={Wang, Shiqi and Zhang, Zhengze and Zhao, Rui and Tan, Fei and Cam-Tu, Nguyen},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={2109--2123},
  year={2024}
}

@article{rlhf-bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{rlhf-complex-fdu,
  title={Secrets of rlhf in large language models part i: Ppo},
  author={Zheng, Rui and Dou, Shihan and Gao, Songyang and Hua, Yuan and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Zhou, Yuhao and others},
  journal={arXiv preprint arXiv:2307.04964},
  year={2023}
}

@article{rlhf-complex-ms,
  title={Efficient rlhf: Reducing the memory usage of ppo},
  author={Santacroce, Michael and Lu, Yadong and Yu, Han and Li, Yuanzhi and Shen, Yelong},
  journal={arXiv preprint arXiv:2309.00754},
  year={2023}
}

@article{rlhf-ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rlhf-stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{rlhf-ziegler2019finetune,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@inproceedings{rs_dpo,
  title={RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models},
  author={Khaki, Saeed and Li, JinJin and Ma, Lan and Yang, Liu and Ramachandra, Prathap},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={1665--1680},
  year={2024}
}

@inproceedings{self_instruct,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13484--13508},
  year={2023}
}

@inproceedings{selfreward,
title={Self-Rewarding Language Models},
author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason E Weston},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=0NphYCmgua}
}

@article{webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

