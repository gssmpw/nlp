\section{Additional Experiments}
\subsection{Ablations study for Data Metrics Experiments}\label{appendix:metric_ablation}

For the ablation study concerning different reward models, SimPO provides an additional preference dataset generated using the Llama-3-8b-instruct model and annotated with the PairRM reward model, available at \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}{llama3-ultrafeedback}. 
Given such convenience, we directly utilize this dataset and select top-40\% subsets with different metrics for subsequent SimPO training.
The performance of trained models with different metrics is depicted in Figure~\ref{fig:metric_ablation_rm}.
Consistent with the results discussed in \S~\ref{sec:metric_exp}, our proposed \methodname{} metric $M_{AP}$ still achieves the highest performance across all benchmarks under varying reward models, demonstrating the efficacy and robustness of our metrics.

For the ablation study focused on different training objectives, we opt for DPO loss as a substitute for SimPO and employ the Gemma-2-9b-it model, noted for its high performance. 
We again apply various metrics to select top-40\% data for training.
The results, depicted in Figure~\ref{fig:metric_ablation_dpo}, showcase that our proposed metric consistently outperforms existing baselines with notable improvements.
These consistent performance gains certify the $M_{AP}$ metric's ability to identify high-quality data for alignment training, demonstrating stable improvements for varying training objectives.

%%%%%%%%%%%%%%%%%%%%% Ablation fig %%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/perf_per_scores_llama_pairrm.pdf}
        \caption{Ablation on different reward models}
        \label{fig:metric_ablation_rm}
    \end{subfigure}%
    % \hfill
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/perf_per_scores_gemma_dpo.pdf}
        \caption{Ablation on different training objectives}
        \label{fig:metric_ablation_dpo}
    \end{subfigure}
    \caption{\textbf{Ablation study for data metrics.} (\ref{fig:metric_ablation_rm}) Performance of Llama-3-8b-instruct model trained with SimPO loss on preference pairs selected by different metrics. The preference dataset is based on the PariRM reward model, instead of the ArmoRM model considered in other settings. 
    (\ref{fig:metric_ablation_dpo}) Performance of Gemma-2-9b-it model trained with DPO loss on preference pairs selected by different metrics.}
    \label{fig:metric_ablation}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{More Comparison with EVA}\label{appendix:compare_eva}

% \begin{algorithm}
% \caption{Eva with \textit{Evolve-then-Select} Pipeline}
% \label{algo:eva}
% \begin{algorithmic}
% \STATE \textbf{Input:}  SFT model $\pi_{\mathrm{SFT}}$, reward model $r$, existing prompts $\mathcal{X}$ and data quality metric $M$.
% \STATE Initialize $\pi_{\theta_0}$ with $\pi_{\mathrm{SFT}}$.
% \FOR{iteration $t=1,\ldots,T$}
%   \STATE Generate initial preference dataset: $\mathcal{D}\leftarrow \mathrm{GenDataset}(\mathcal{X}, \pi_{\theta_{t-1}},r)$.
%   \STATE Evolve prompts: $\mathcal{X}' \leftarrow \mathrm{EvolInstruct}(\mathcal{X})$.
%   \STATE Generate additional preference dataset: $\mathcal{D}'\leftarrow \mathrm{GenDataset}(\mathcal{X}', \pi_{\theta_{t-1}},r)$.
%   \STATE Select top-$k$ dataset using $M$ metric:\\
%   \hspace{1em} $\mathcal{D}_k' = \{(x,y_w,y_l)\in\mathcal D | M(x,y_w,y_l;\theta_{t-1}) \ge \tau_k\}$, with $\tau_k$ being the $k$-th largest $M$ score in $\mathcal D'$.
%   \STATE Conduct preference optimization: $\theta_t \leftarrow \theta_{t-1} - \eta\nabla_\theta\mathcal L(\theta_{t-1};\mathcal{D} \cup \mathcal{D}_k')$
% \ENDFOR
% \STATE \textbf{Return:} optimized policy $\pi_{\theta_T}$
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
\caption{Eva with \textit{Select-then-Evolve} Pipeline}
\label{algo:eva_orig}
\begin{algorithmic}
\STATE \textbf{Input:} SFT model $\pi_{\mathrm{SFT}}$, reward model $r$, existing prompts $\mathcal{X}$ and data quality metric $M$.
\STATE Initialize $\pi_{\theta_0}$ with $\pi_{\mathrm{SFT}}$.
\FOR{iteration $t=1,\ldots,T$}
  \STATE Generate initial preference dataset: $\mathcal{D}\leftarrow \mathrm{GenDataset}(\mathcal{X}, \pi_{\theta_{t-1}},r)$.
  \STATE Select top-$k$ dataset using $M$ metric:\\
  \hspace{1em} $\mathcal{D}_k = \{(x,y_w,y_l) \in \mathcal D| M(x,y_w,y_l;\theta_{t-1}) \ge \tau_k\}$, with $\tau_k$ being the $k$-th largest $M$ score in $\mathcal D$.
  \STATE Evolve prompts: $\mathcal{X}_k' \leftarrow \mathrm{EvolInstruct}(\mathcal{X}_k)$, where $\mathcal X_k$ denotes the prompts of $\mathcal D_k$ dataset.
  \STATE Generate additional preference dataset: $\mathcal{D}_k'\leftarrow \mathrm{GenDataset}(\mathcal{X}_k', \pi_{\theta_{t-1}},r)$.
  \STATE Conduct preference optimization on $\mathcal{D} \cup \mathcal{D}_k'$: $\theta_t \leftarrow \theta_{t-1} - \eta\nabla_\theta\mathcal L(\theta_{t-1};\mathcal{D} \cup \mathcal{D}_k').$
\ENDFOR
\STATE \textbf{Return:} optimized policy $\pi_{\theta_T}$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%% EVA Figure %%%%%%%%%%%%%%%%%%%%%
\newlength{\origtextsep}
\setlength{\origtextsep}{\intextsep}
\setlength{\intextsep}{0pt}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/perf_potential_ablation.pdf}
    \vspace{-5pt}
    \caption{The \methodname{} scores of different datasets and the corresponding Gemma models' performance. 
    The dataset select-evolved by our methods shows the highest \methodname{} scores and produces the best-performing results.}
    \label{fig:perf_potential_ablation}
\end{wrapfigure}
% \begin{figure}[htbp]
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our main study employs the \textit{evolve-then-select} pipeline for preference data generation, as detailed in Algorithm~\ref{algo:eva}, where we streamline the dataset generation process using the $\mathrm{GenDataset}(\mathcal X, \pi_\theta, r)$ function.
This function takes the prompts $\mathcal X$, samples responses with LLM $\pi_\theta$, and annotates preferences with reward model $r$ to construct a preference dataset.
The \textit{evolve-then-select} pipeline ensures the final dataset $\mathcal D_k'$ aligns with the evaluation of data quality metrics.

In contrast, the original eva framework operates under a \textit{select-then-evolve} pipeline\footnote{The original eva paper employs $M_r$ scores as sampling weights for $\mathcal D_k$ selection but lacks implementation guidance. Thus we substitute this sampling with a similar top-$k$ selection.}, as detailed in Algorithm~\ref{algo:eva_orig}.
Within this pipeline, data selection precedes the evolving data process.
Therefore the data quality of the final evolved dataset $\mathcal D_k'$ may not be consistent with that of $\mathcal D_k$ due to the intricate prompt evolving and preference data generation process in data evolving.
Nevertheless, we still include comparisons under this \textit{select-then-evolve} setting to ensure a comprehensive and equitable evaluation against eva.

Using the experimental setup detailed in \S\ref{sec:main_exp}, but reversing the selection and evolution order, we assess models trained on various datasets as reported in Table~\ref{table:main_exp_select_first}. 
The results indicate that datasets select-evolved with our metric continue to deliver better overall performance than the supplementary UltraFeedback datasets and those derived using the eva method. 
To corroborate that these performance improvements stem from the ability to select high-quality data, we present the average performance (Arena Hard win rate) of Gemma models trained with either DPO or SimPO, alongside the \methodname{} scores of the respective datasets, in Figure~\ref{fig:perf_potential_ablation}. 
As illustrated, the model performance continually improves as the dataset's \methodname{} score increases, with the dataset select-evolved by our metric exhibiting both the highest \methodname{} score and performance.
This correlation between \methodname{} score and performance further underscores our metric's capability of discerning high-quality data for training.

\setlength{\intextsep}{\origtextsep}
%%%%%%%%%%%%%%%%%%%%% EVA Table %%%%%%%%%%%%%%%%%%%%%
\input{tables/main_table_select_first}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convergence Rates of DPO with Different Samplers}
\subsection{Proof of Theorem~\ref{thm:convergence}}\label{appendix:proof}
Here we restate the DPO loss in its population form:
\begin{align*}
\mathbb E_{(x,y,y')\sim s}\left[\mathcal L(x,y,y';\theta)\right] &= \mathbb E_{(x,y,y')\sim s}\left[-p(y\succ y')\log\sigma(\hat r_\theta(y) - \hat r_\theta(y'))\right]\\
&= \mathbb E_{(x,y,y')\sim s}\left[-\sigma(r(x,y) - r(x,y'))\log\sigma(\hat r_\theta(y) - \hat r_\theta(y'))\right].
\end{align*}
Under stochastic gradient descent, a data $(x^t,y^t,y'^t)$ is sampled to update the parameter $\theta^t$ at each iteration $t$ with the empirical loss.
Since the sampled context $x^t$ and arms $y^t,y'^t$ can be utilized to compute both $\mathcal L(x^t,y^t,y'^t;\theta^t)$ and $\mathcal L(x^t,y'^t,y^t;\theta^t)$, we utilize both terms to derive the stochastic update rule, like previous works \citep{IPO,DNO}:
\begin{equation}\label{eqn:stoc_DPO}
    \theta^{t+1} = \theta^t -\frac12\eta^t\nabla_{\theta}\left[\mathcal L(x^t,y^t,y'^t;\theta^t) + \mathcal L(x^t,y'^t,y^t;\theta^t)\right]
\end{equation}
where $\eta^t$ denotes the time-varying learning rate.
Without loss of generality, we assume a uniform reference policy $\pi_\mathrm{ref}(y|x) = \frac1{|\mathcal Y|}$ like \citet{shi2024crucial}.
Therefore $\hat r_\theta(x,y) = \beta\log\frac{\pi_\theta(y|x)}{1/|\mathcal Y|} = \beta\theta(x,y) + \log Z(x)$ and the optimum $\theta^*$, characterized by $r(x,y) = \hat r_{\theta^*}(x,y) + \log Z^*(x)$, turns into $\beta\theta^*(x,y) = r(x,y) + c(x)$, where $c(x)$ is a constant independent of $y$.
The proof of Theorem~\ref{thm:convergence} is as follows:
\begin{proof}[Proof of Theorem~\ref{thm:convergence}]
For ease of annotation, we first consider the single context setting $|\mathcal X| = 1$ so that we can denote $\theta_y = \theta(x, y), r_y = r(x, y)$ with $x$ omitted for simplicity.
We first investigate the stochastic update rule of $\theta^t$ given by Equation~\eqref{eqn:stoc_DPO}.
Let the arms chosen at iteration $t$ being $y,y'$, we have:
\begin{align*}
    \theta_y^{t+1} &= \theta_y^t + \frac{\eta^t\beta}{2}\left[\sigma(r_y-r_{y'})\sigma(\beta\theta_{y'}^t-\beta\theta_y^t) - \sigma(r_{y'}-r_{y})\sigma(\beta\theta_{y}^t -\beta\theta_{y'}^t)\right]\\
    &= \theta_y^t + \frac{\eta^t\beta}{2}\left[\sigma(r_y-r_{y'}) - \sigma(\beta\theta_y^t - \beta\theta_{y'}^t)\right]\\
    \theta_{y'}^{t+1} &= \theta_{y'}^t +  \frac{\eta^t\beta}{2}\left[\sigma(r_{y'}-r_{y}) - \sigma(\beta\theta_{y'}^t- \beta\theta_{y}^t)\right]\\
    &= \theta_{y'}^t - \frac{\eta^t\beta}{2}\left[\sigma(r_y-r_{y'}) - \sigma(\beta\theta_y^t - \beta\theta_{y'}^t)\right],
\end{align*}
where the second equality holds by the fact that $\sigma(x)\sigma(-y) - \sigma(-x)\sigma(y) = \sigma(x) - \sigma(y).$

Since the optimal target is $\beta\theta^* = r +c$,  where $c$ is a constant, 
we denote a new vector $\xi^t = \beta\theta^t - r$.
Let:
\begin{gather*}
\Delta_{yy'}^t = \sigma(r_y-r_{y'}) - \sigma(\beta\theta_y^t - \beta\theta_{y'}^t).
% ,\\\delta_{yy'}^t = r_y-r_{y'} - (\beta\theta_y^t-\beta\theta_{y'}^t).
\end{gather*}
The update rule can be rewritten as:
\begin{equation}\label{eqn:dpo_update}
\begin{gathered}
     \theta_y^{t+1} = \theta_y^t + \frac{\eta^t\beta}{2}\Delta_{yy'}^t, 
     \theta_{y'}^{t+1} = \theta_{y'}^t - \frac{\eta^t\beta}{2}\Delta_{yy'}^t.\\
     \xi_y^{t+1} = \xi_y^t + \frac{\eta^t\beta^2}{2}\Delta_{yy'}^t, 
     \xi_{y'}^{t+1} = \xi_{y'}^t - \frac{\eta^t\beta^2}{2}\Delta_{yy'}^t.\\
\end{gathered}
\end{equation}
Note that $\theta^{t+1}_y + \theta_{y'}^{t+1} = \theta_y^t + \theta_{y'}^t$ and $\xi_{y}^{t+1} + \xi_{y'}^{t+1} = \xi_{y}^{t} + \xi_{y'}^{t}$, which indicates the mean of $\theta$ and $\xi$ remains unchanged during optimization.

For optimal $\theta^*$, the $\xi^* = \beta\theta^*-r$ should be a constant vector $c\cdot\mathbf{1}.$ Therefore, we can measure the variance of $\xi^t$ as an indicator of convergence:
$$
V^t = \frac{1}{|\mathcal Y|}\sum_{y\in\mathcal Y} (\xi_y^t - \bar\xi^t)^2, \mathrm{where}\ \bar\xi^t = \frac{1}{|\mathcal Y|}\sum_{y\in\mathcal Y}\xi_y^t.
$$
Using $V^t$, the distance function $\mathrm{Dist}(\theta^t,\theta^*)$ satisfies that:
$$
\mathrm{Dist}(\theta^t,\theta^*)^2 =\frac{1}{|\mathcal Y|^2}\sum_{y,y'\in\mathcal Y} M_1(x,y,y'|\theta,r)^2 = \frac{1}{|\mathcal Y|^2}\sum_{y,y'\in\mathcal Y} (\xi_y^t - \xi_{y'}^t)^2 = 2V^t.
$$

Now we examine how $V^t$ changes at each iteration. Assume the sampled pairs are $y,y'$ at iteration $t$, then we have:
\begin{align*}
    V^{t+1} &= V^{t} + \frac{1}{|\mathcal Y|}\left[(\xi_y^{t+1}-\bar\xi^{t+1})^2 + (\xi_{y'}^{t+1}-\bar\xi^{t+1})^2 - (\xi_y^{t}-\bar\xi^{t})^2 - (\xi_{y'}^{t}-\bar\xi^{t})^2\right]\\
    &= V^t + \frac{1}{|\mathcal Y|}\left[(\xi_y^{t+1})^2 + (\xi_{y'}^{t+1})^2-(\xi_y^{t})^2-(\xi_{y'}^{t})^2\right]\\
    &= V^t + \frac{1}{|\mathcal Y|}\left[(\xi_y^{t} + a\Delta_{yy'}^t)^2 + (\xi_{y'}^{t} - a\Delta_{yy'}^t)^2-(\xi_y^{t})^2-(\xi_{y'}^{t})^2\right]\quad \text{(let $a=\frac{\eta^t\beta^2}{2}$)}\\
    &= V^t + \frac{2a}{|\mathcal Y|}(\xi_y^t - \xi_{y'}^t + a\Delta_{yy'}^t)\Delta_{yy'}^t.
\end{align*}
where the second equation holds with $\bar \xi^{t+1} = \bar \xi^t$ and $\xi_y^{t+1} + \xi_{y'}^{t+1} = \xi_y^{t} + \xi_{y'}^{t}.$

Using the mean value theorem, we have:
$$
\Delta_{yy'}^t = \sigma'(\lambda_{yy'}^t)\left[(r_y-r_{y'}) - (\beta\theta_y-\beta\theta_{y'})\right]= -\sigma'(\lambda_{yy'}^t)(\xi_y^t - \xi_{y'}^t),
$$
where $\lambda_{yy'}^t$ is between $r_y - r_{y'}$ and $\beta\theta_y^t - \beta\theta_{y'}$. Note that $\sigma'(\cdot) \in (0,\frac14]$, and the value of $\sigma'(\lambda)$ can be further lower bounded using the update rule.

The change in $V^t$ can thus be written as:
$$
\Delta V^t = V^{t+1} - V^t = -\frac{2(1-a\sigma'(\lambda_{yy'}^t))a\sigma'(\lambda_{yy'}^t)}{|\mathcal Y|}(\xi_y^t - \xi_{y'}^t)^2
$$
Under the optimal learning rate $\eta^t = \frac{1}{\beta^2\sigma'(\lambda_{yy'}^t)}$ (we can adjust the learning rate at each iteration, similar to the line search methods), such that $a\sigma'(\lambda_{yy'}^t)= \frac12$, the change in $V^t$ satisfies:
\begin{equation}\label{eqn:error_update}
\Delta V^t = - \frac{1}{2|\mathcal Y|}(\xi_y^t - \xi_{y'}^t)^2.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% NOTE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 这里调整最优的学习率就和线搜索一样，是很自然的。
%% 如果评委觉得很怪，也可以通过 $\sigma'(\cdot)$ 在[-1,1] 范围内的值在 0.1966~0.25 之间
%% 进而通过一个固定的学习率 将系数$\frac{2(1-a\sigma'(\lambda_{yy'}^t))a\sigma'(\lambda_{yy'}^t)}$ bound住
%% 最后也还是有大约2倍的加速（可能比0.5略大一点）
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now we can examine the impact of different samplers, which determine the choices of $y,y'$ at each iteration.
For the uniform sampler, under the optimal learning rate $\eta^t$, we have:
\begin{align*}
    \mathbb E[V^{t+1}|V^t] &= V^t + \mathbb E[\Delta V^t]\\
    &= V^t - \frac{1}{2|\mathcal Y|}\mathbb E_{y,y'\sim s_u}[(\xi_y^t - \xi_{y'}^t)^2]\\
    &= V^t - \frac{1}{2|\mathcal Y|}\cdot\frac{1}{|\mathcal Y|^2}\sum_{y\in\mathcal Y}\sum_{y'\in\mathcal Y}(\xi_y^t - \xi_{y'}^t)^2\\
    &=V^t - \frac{1}{2|\mathcal Y|}\cdot2V^t\\
    &=(1-\frac1{|\mathcal Y|})V^t.
\end{align*}
So we have the following \textbf{convergence result for the uniform sampler}:
\begin{equation}\label{eqn:uniform_rate}
\mathbb E[V^t] = (1-\frac{1}{|\mathcal Y|})^tV^0.
\end{equation}

In contrast, the adversarial sampler, which chooses the pair $y^*,y'^* $ to maximize the $M_1$ score:
\begin{align*}
    y*,y'^* &= \argmax_{y,y'\in \mathcal Y} M_1(x,y, y') \\
    &= \argmax_{y,y'\in \mathcal Y} |r_y -  r_{y'} - (\beta\theta_y^t - \beta\theta_{y'}^t)|\\
    &= \argmax_{y,y'\in \mathcal Y} |\xi_y^t - \xi_{y'}^t|,
\end{align*}
which means to select the maximal and minimal elements from $\xi^t.$
Given this definition, the selected pairs at each iteration $y^*,y'^* $ satisfy that:
$$
(\xi_{y^*}^t - \xi_y^t)\cdot(\xi_{y'^*}^t - \xi_{y}^t) \le 0, \forall y\in \mathcal Y.
$$
Let $m^t = \xi_{y^*}^t - \bar \xi^t, M^t= \xi_{y'^*}^t - \bar \xi^t$ and $s_y^t = \xi_y^t - \bar \xi^t$, the inequality above can be rewritten as:
$$
(m^t-s_y^t)(M^t-s_y^t) \le 0, \forall y\in \mathcal Y.
$$
Summarize over all $y\in\mathcal Y$, we have:
\begin{align*}
    0 &\ge \sum_{y\in\mathcal Y} (m^t - s_y^t)(M^t - s_y^t)\\
    &= |\mathcal Y|\cdot m^tM^t - (m^t+M^t)\sum_{y\in\mathcal Y}s_y^t + \sum_{y\in\mathcal Y}(s_y^t)^2\\
    &= |\mathcal Y|\cdot m^tM^t + \sum_{y\in\mathcal Y}(s_y^t)^2\\
    &=|\mathcal Y|\cdot\left[(\xi_{y^*}^t - \bar \xi^t)(\xi_{y'^*}^t - \bar \xi^t) + V^t\right].
\end{align*}
where the second equality (line 2 to line 3) holds by $\sum_{y\in \mathcal Y}s_y^t = 0$.
Combing with the fact that $(a-b)^2\ge -4ab$, we have:
\begin{align*}
    (\xi_{y^*}^t - \xi_{y'^*}^t)^2 &\ge -4(\xi_{y^*}^t - \bar \xi^t)(\xi_{y'^*}^t - \bar \xi^t) \\
    &\ge 4V^t.
\end{align*}
Therefore, when updating using the selected pair  $y^*,y'^* $ at iteration $t$, we have (under the optimal learning rate in Equation~\eqref{eqn:error_update}):
\begin{align*}
V^{t+1} &= V^t  - \frac{1}{2|\mathcal Y|}(\xi_{y^*}^t - \xi_{y'^*}^t)^2\\
&\le V^t - \frac{2}{|\mathcal Y|}V^t\\
&= (1-\frac{2}{|\mathcal Y|})V^t.
\end{align*}
So we have the following \textbf{convergence result for the adversarial sampler}:
\begin{equation}\label{eqn:adv_rate}
V^t \le (1-\frac{2}{|\mathcal{Y}|})^tV^0,
\end{equation}

Eventually, we can compare the iterations required to converge to the same level of error, when applying different samplers.
Since $\mathrm{Dist}(\theta^t,\theta^*)^2 = 2V^t$, to reach $\mathrm{Dist}(\theta^t,\theta^*) = \varepsilon \mathrm{Dist}(\theta^0,\theta^*)$ for some small $\varepsilon < 1$, or equivalently, $V^t = \varepsilon^2 V^0$, the uniform sampler would require $T_u(\varepsilon) = \frac{2\log(\varepsilon)}{\log(1 - 1/|\mathcal Y|)}$ iterations according to Equation~\eqref{eqn:uniform_rate}.
In comparison, the adversarial sampler would require $T_{adv}(\varepsilon) = \frac{2\log(\varepsilon)}{\log(1 - 2/|\mathcal Y|)}$ iterations by Equation~\eqref{eqn:adv_rate}.
As one may verify that for $|\mathcal Y| > 2$:
$$
\log(1-\frac{2}{|\mathcal Y|}) > 2\log(1-\frac{1}{|\mathcal Y|}),
$$
therefore \textbf{the adversarial sampler requires less than half the iterations of the uniform counterpart}:
\begin{equation}\label{eqn:iter_ratio}
    T_{adv}(\varepsilon) < 0.5 T_u(\varepsilon).
\end{equation}

Furthermore, the above results can directly generalize to the contextual setting where $|\mathcal X| > 1$, which we briefly discuss here: With slight abuse of annotations, we can re-define $V^t = \frac{1}{|\mathcal X||\mathcal Y|}\sum_{x\in\mathcal X}\sum_{y\in\mathcal Y}(\xi_{x,y}^t - \bar \xi_x^t)^2$, and the update of $V^t$ with preference data $(x,y,y')$ at $t$ would be: 
$$
V^{t+1} = V^t + \frac{2a}{|\mathcal X||\mathcal Y|}(\xi_{xy}^t - \xi_{xy'}^t+a\Delta_{xyy'}^t)\Delta_{xyy'}^t.
$$
Then the convergence results immediately become $\mathbb E[V^t] = (1-\frac{1}{|\mathcal X||\mathcal Y|})^tV^0$ for uniform sampler and $V^t \le (1-\frac{2}{|\mathcal X||\mathcal Y|})V^0$ for the adversarial sampler, leading to the same result with Equation~\eqref{eqn:iter_ratio}.
\end{proof}

\subsection{Numerical Experiments}
Theorem~\ref{thm:convergence} establishes that the adversarial sampler requires less than half the iterations of the uniform sampler to reach the same error level. Here we conduct numerical experiments to validate this result.

We examine two scenarios: the standard bandit setting with a single context (\ie $|\mathcal X| = 1$) and a contextual setting with $|\mathcal X| = 5$. In both scenarios, the arm space is set to $|\mathcal Y| = 10$.
Rewards are initialized uniformly using $U[0,1]$ and the initial parameter $\theta^0_x$ is set to $0$.
We consider a uniform reference policy, with the KL-parameter $\beta$ fixed at $0.1$.
For the learning rate, while our proof uses the optimal learning rate $\eta^t = \frac{1}{\beta^2\sigma'(\lambda_{yy'}^t)}\ge \frac4{\beta^2}$, a fixed learning rate is set to $4/\beta^2$ for simplicity.

Figure~\ref{fig:converge_numerical} presents the error, \ie the distance to optimum $\mathrm{Dist}(\theta,\theta^*)$, averaged across 10 random initializations. As depicted, while both samplers achieve linear convergence, the adversarial sampler converges significantly more rapidly due to its strategic selection of the data with the most potential for alignment.
Notably, in our experiments, the uniform sampler required approximately six times more iterations to reach the same level of error as the adversarial sampler, which corroborates the theoretical prediction of a speed-up by a factor of more than two.


%%%%%%%%%%%%%%%%%%%%% Converge fig %%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/convergence.pdf}
        \caption{Convergence results for $|\mathcal X|=1, |\mathcal Y|=10$}
        \label{fig:converge}
    \end{subfigure}%
    % \hfill
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/convergence_contextual.pdf}
        \caption{Convergence results for $|\mathcal X|=5, |\mathcal Y|=10$}
        \label{fig:converge_contextual}
    \end{subfigure}
    \caption{\textbf{Numerical experiments for DPO under contextual bandit setting.} This figure compares learning convergence using two samplers: a uniform sampler and an adversarial sampler that selects pair data to maximize our $M_1$ metric.
    The plots display the error $\mathrm{Dist}(\theta,\theta^*)$ across 10 random initializations. Figure~\ref{fig:converge} addresses the single context scenario, while Figure~\ref{fig:converge_contextual} pertains to multiple contexts. The error lower bound of approximately 1e-15 is attributed to floating-point precision limitations.}
    \label{fig:converge_numerical}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Implementation Details}\label{appendix:implement}

\subsection{Codes \& Hyperparameters}
% Our code is open-sourced in an anonymous repository: \href{https://anonymous.4open.science/r/Alignment-Potential}{https://anonymous.4open.science/r/Alignment-Potential}. 
The code is available at: \href{https://github.com/Hesse73/Alignment-Potential-Metric}{https://github.com/Hesse73/Alignment-Potential-Metric}.

\textbf{Data quality metrics.} 
We mainly investigate three metrics in this paper: (1) the explicit reward margin metric: $M_r(x,y_w,y_l) = |r(x,y_w) - r(x,y_l)|$; (2) the implicit reward margin metric: $M_\pi(x,y_w,y_l)=|\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|$; and (3) our proposed alignment potential metric: $M_{AP} = |r(x,y_w) - r(x,y_l)| - |\hat r_\theta(x,y_w) - \hat r_\theta(x,y_l)|$.
While explicit rewards $r(x,y)$ are directly given by a reward model, computing the implicit rewards $\hat r_\theta(x,y) = \beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)}$ requires the LLM $\pi_\theta$, a reference policy $\pi_\mathrm{ref}$, and a parameter $\beta > 0$.
As discussed stated in \S\ref{sec:metric_exp}, since the implicit reward $\hat r_\theta(x,y)$ becomes constant-zero when $\pi_\theta$ is identical with $\pi_\mathrm{ref}$, we propose implementing the implicit reward via SimPO's length-normalized reward: $\hat r_\theta^\mathrm{Sim}(x,y) = \beta\log\pi_\theta(y|x)/|y|$, which can be seen as a special case of $\hat r_\theta$ with uniform reference model \citep{alphadpo}.
Regarding the parameter $\beta$, while it doesn't influence the ranking determined by $M_\pi$, it does affect the score produced by our $M_{AP}$ metric.
To effectively choose a value for $\beta$, let $\sigma_r$ and $\sigma_\pi$ denote the standard deviations of $|r(x,y_w)-r(x,y_l)|$ and $|{\log\pi_\theta(y_w|x)}/{|y_w|} - {\log\pi_\theta(y_l|x)}/{|y_l|}|$ across the dataset, respectively.
Utilizing these standard deviations, we rescale the two margins akin to Z-score normalization and introduce a parameter $\alpha\in\{0.25, 0.5, 1.0, 2.5, 5.0\}$ for computing the $M_{AP}$ metric:
$$
M_{AP}(x,y_w,y_l) = \frac{1}{\sigma_r}\left|r(x,y_w)-r(x,y_l)\right| - \frac{\alpha}{\sigma_\pi} \left|\frac{\log\pi_\theta(y_w|x)}{|y_w|} - \frac{\log\pi_\theta(y_l|x)}{|y_l|}\right|.
$$
Here we use $\alpha$ to distinguish it from the $\beta$ hyperparameter used in the DPO and SimPO loss function.

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-\baselineskip}
\centering
\caption{Hyperparameters used in our preliminary experiments.}
\label{table:params_prelim}
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{Hyperparameter} & \multicolumn{2}{c}{Gemma (ArmoRM)} & \multicolumn{2}{c}{Llama (SimPO)} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
 & SimPO & DPO & ArmoRM & PairRM \\ \midrule
learning rate & 8e-7 & 5e-7 & 1e-6 & 1e-6 \\
$\beta$ & 10 & 0.01 & 10 & 2.5 \\
$\gamma/\beta$ & 0.5 & \textbackslash{} & 0.3 & 0.55 \\ \bottomrule
\end{tabular}}
\end{wraptable}
\textbf{Preliminary experiments.} 
The preliminary experiments, outlined in Section \ref{sec:metric_exp}, employ the three metrics to select the top 40\% subsets from existing preference datasets within SimPO. 
While the explicit rewards $r(x,y)$ are present in SimPO's datasets, they didn't record the probabilities of LLM $\pi_\theta(y|x)$ when sampling the responses.
So we use the corresponding models from huggingface: \href{https://huggingface.co/google/gemma-2-9b-it}{Gemma-2-9b-it} and \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{Llama-3-8b-instruct}, input the generated responses $y$ and prompts $y$, and compute $\pi_\theta(y|x)$ to derive implicit rewards $\log\pi_\theta(y|x)/|y|$.
We use three datasets for data selection: 
two Llama-based datasets annotated by ArmoRM: \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback-armorm}{llama3-ultrafeedback-armorm} and PairRM: \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}{llama3-ultrafeedback},
and one Gemma-based dataset annotated by ArmoRM: 
\href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{gemma2-ultrafeedback-armorm}.
For the hyperparameter in $M_{AP}$, we set $\alpha = 1.0$ for Llama'
s PairRM annotated dataset and set $\alpha=2.5$ for the remaining two datasets.
After data selection,  we apply the tuned hyperparameters reported in SimPO's paper for both SimPO and DPO training.
Specifically, we set a batch size of 128, with a max sequence length of 2048 and a max prompt length of 1800. All the models are trained using a cosine learning rate scheduler with a 10\% warmup ratio, via AdamW optimizer for one epoch.
Table~\ref{table:params_prelim} details the other hyperparameters for different datasets and objectives

\textbf{Main experiments.} 
In the main experiments, we integrate our metric $M_{AP}$ into the evolving alignment (eva) framework, which involves iterative preference dataset generation and prompt evolving processes.
To generate the preference datasets $\mathcal D$ and $\mathcal D'$, we use top-p sampling (p=0.95) with a temperature of 0.8 and max sequence length of 4096 to sample responses $y$ for prompts $x$.
Following SimPO, we use five distinct random seeds to sample 5 responses $\{y_1,\dots,y_5\}$ for each prompt $x$, and use the responses with max/min reward $r(x,y)$, annotated via ArmoRM, to construct the preference pair $(x,y_w,y_l)$.
The previous sampling parameters are also used for prompt evolving, \ie query the model to rewrite existing prompts $x\in\mathcal X$ into evolved prompts $\mathcal X'$.
Regarding the instruction for LLMs to write prompts, we employ the five prompts in EvolInstruct \citep{EvolInstuct} and make some modifications for better instruction-following capabilities. 
Detailed descriptions of these revised prompts are available in Table~\ref{example:evol_prompts}.
Incorrectly formatted prompts per the evolving instruction are filtered out when constructing $\mathcal X'$.

As for the subsequent data selection and training, we use $\alpha=2.5$ for gemma and $\alpha=5.0$ for llama for our main results (Table~\ref{table:main_exp_evolve_first} and Table~\ref{table:main_exp_select_first}). For llama with DPO, we set a learning rate of 7e-7 and $\beta=0.01$, and other hyperparameters remain consistent with Table \ref{table:params_prelim}.
In the data-size scaling experiment (Figure \ref{fig:data_scaling}), the number of EvolInstruct prompt templates is reduced from 5 to 2, ensuring the processed prompt set is less than 30k$\times$2, maintaining a smaller prompt set than 60k.
The hyperparameter $\alpha=0.5$ and $\gamma/\beta = 0.3$ is employed for $M_{AP}$ metric and subsequent SimPO training. 
For the multi-iteration experiment (Figure~\ref{fig:iter_scaling}), we reduce the 3rd iteration's learning rate to 3e-7 to prevent overfitting. The $\gamma/\beta$ parameter is tuned within $\{0.1,0.3,0.5,0.8\}$, and the best-performing results based on the Arena Hard WR are reported.

\subsection{GPT-4 Annotation}
In \S\ref{sec:derive_AP_metric}, we use $M_+$ and $M_{AP}$ metrics to select the top-10\% subsets from SimPO's preference dataset and measure GPT-4's agreement on the preference annotation in data $(x,y_w,y_l)$.
Specifically, we input the prompt $x$ along with two responses $y_w,y_l$ for GPT-4, ask it to choose the better one, and check if it aligns with the preference data's judgment, \ie $y_w \succ y_l|x$.
For feasibility and financial consideration, 1,000 preference data from the selected top subsets is sampled for annotation. 
We employ the prompt template from Arena-hard \citep{arenahard} to prompt the GPT-4-1106-preview model for preference annotation. The prompts can be accessed via their \href{https://github.com/lmarena/arena-hard-auto/blob/main/config/judge_config.yaml}{config file}.

%%%%%%%%%%%%%%% llama agreement %%%%%%%%%%%%%%%%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/llama_agreement.pdf}
    \caption{GPT-4 agreement with reward model on data selected by different metrics from \href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}{SimPO's dataset with Llama model}. The data selected by $M_{AP}$ has a higher agree rate than $M_+$, indicating less reward annotation noise.}
    \label{fig:llama_agree}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In addition to the agreement results in Figure \ref{fig:gemma_agree}, which is selected and annotated on the Gemma dataset of SimPO, we also include the agreement results of the Llama dataset of SimPO in Figure~\ref{fig:llama_agree}.
Here data selected by $M_{AP}$ metric also results in a higher agree rate than $M_+$, underscoring the effectiveness of noise reduction strategies in $M_{AP}$.

\section{Illustrative Examples of Preference Data}\label{appendix:examples}

This section provides detailed examples of preference data utilized in this paper.

\textbf{Contradiction of existing metrics.}
Prior research indicates that prioritizing data with \textit{large explicit reward margins} or \textit{smaller implicit reward margins} can improve alignment performance. 
However, these two metrics often give conflicting guidance, demonstrated by two examples from the SimPO preference dataset (\href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{gemma2-ultrafeedback-armorm}) depicted in Figure~\ref{fig:teaser_a}:
The first example, where the data produces both large explicit and implicit reward margins, is rated as ``high-quality'' by the explicit reward margin but ``low-quality'' by the implicit reward margin.
Detailed information for this example is provided in Table~\ref{example:both_large}.
The second example, where the data produces both small explicit and implicit reward margins, is rated as ``low-quality'' by the explicit reward margin but ``high-quality'' by the implicit reward margin.
This data is elucidated in Table~\ref{example:both_small}.

Our proposed \methodname{} metric differentiates itself by classifying both examples as ``low-quality.'' This is due to the small disparity between explicit and implicit rewards, indicating that the LLM's preferences are already well-aligned with the data, thus negating the need for further training.

\textbf{Noisy reward model annotation.}
Reward models can introduce noisy preferences in annotation. An illustrative example from the \href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{gemma2-ultrafeedback-armorm} dataset is presented in Table~\ref{example:rm_wrong}, with its simplified version illustrated in Figure~\ref{fig:noise_example} in the main body of our paper.

As shown in the example, the prompt asks LLM about the name of Zulu soldiers and requires it to output ``No answer'' if the answer cannot be determined by the provided text.
Since the provided text does not state what Zulu soldiers are called, the correct answer would be ``No answer'', corresponding to the $y_l$ response.
However, the reward model predicts a lower reward for the correct answer and incorrectly assigns a higher reward for the wrong answer $y_w$.
This highlights the limitations of reward models and the resultant annotation noise.

Moreover, this preference data also implies that the implicit reward, given by the LLM itself, could provide correct preference annotations: the implicit reward $\hat r_\theta(x,y_l) = -3.4$ for the correct yet mislabeled response $y_l$, is much higher than the wrong answer $\hat r_\theta(x,y_w) = -8.9$.
This observation is supported by recent studies recognizing the LLM's capacity to annotate preferences \citep{boostrapping_implicit_r, CDR_annotation}. %\citep{spread_annotate, selfreward}.

\input{tables/examples/evol_prompts}

\input{tables/examples/both_large}

\input{tables/examples/both_small}

\input{tables/examples/rm_wrong}

