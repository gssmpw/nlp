[
  {
    "index": 0,
    "papers": [
      {
        "key": "rlhf-ziegler2019finetune",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "rlhf-stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "rlhf-bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "rlhf-ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rlhf-complex-ms",
        "author": "Santacroce, Michael and Lu, Yadong and Yu, Han and Li, Yuanzhi and Shen, Yelong",
        "title": "Efficient rlhf: Reducing the memory usage of ppo"
      },
      {
        "key": "rlhf-complex-fdu",
        "author": "Zheng, Rui and Dou, Shihan and Gao, Songyang and Hua, Yuan and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Zhou, Yuhao and others",
        "title": "Secrets of rlhf in large language models part i: Ppo"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "RAFT",
        "author": "Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and KaShun, SHUM and Zhang, Tong",
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"
      },
      {
        "key": "RRHF",
        "author": "Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei",
        "title": "Rrhf: Rank responses to align language models with human feedback without tears"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "DPO",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "IPO",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "KTO",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ORPO",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "SimPO",
        "author": "Yu Meng and Mengzhou Xia and Danqi Chen",
        "title": "Sim{PO}: Simple Preference Optimization with a Reference-Free Reward"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cringe",
        "author": "Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason",
        "title": "Some things are more cringe than others: Preference optimization with the pairwise cringe loss"
      },
      {
        "key": "selfreward",
        "author": "Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason E Weston",
        "title": "Self-Rewarding Language Models"
      },
      {
        "key": "DNO",
        "author": "Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang",
        "title": "Direct nash optimization: Teaching language models to self-improve with general preferences"
      },
      {
        "key": "icml24bridging",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "webgpt",
        "author": "Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others",
        "title": "Webgpt: Browser-assisted question-answering with human feedback"
      },
      {
        "key": "rlhf-bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "iclr24statistical",
        "author": "Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J Liu and Jialu Liu",
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
      },
      {
        "key": "icml24onpolicydata",
        "author": "Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar",
        "title": "Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy Data"
      },
      {
        "key": "icml24bridging",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wu2024betadpo",
        "author": "Junkang Wu and Yuexiang Xie and Zhengyi Yang and Jiancan Wu and Jinyang Gao and Bolin Ding and Xiang Wang and Xiangnan He",
        "title": "\\${\\textbackslash}beta\\$-{DPO}: Direct Preference Optimization with Dynamic \\${\\textbackslash}beta\\$"
      },
      {
        "key": "filter_dpo",
        "author": "Morimura, Tetsuro and Sakamoto, Mitsuki and Jinnai, Yuu and Abe, Kenshi and Ariu, Kaito",
        "title": "Filtered direct preference optimization"
      },
      {
        "key": "curyy_dpo",
        "author": "Pattnaik, Pulkit and Maheshwary, Rishabh and Ogueji, Kelechi and Yadav, Vikas and Madhusudhan, Sathwik Tejaswi",
        "title": "Curry-dpo: Enhancing alignment using curriculum learning \\& ranked preferences"
      },
      {
        "key": "BPO",
        "author": "Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong",
        "title": "Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rs_dpo",
        "author": "Khaki, Saeed and Li, JinJin and Ma, Lan and Yang, Liu and Ramachandra, Prathap",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models"
      },
      {
        "key": "eva",
        "author": "Ye, Ziyu and Agarwal, Rishabh and Liu, Tianqi and Joshi, Rishabh and Velury, Sarmishta and Le, Quoc V and Tan, Qijun and Liu, Yuan",
        "title": "Evolving Alignment via Asymmetric Self-Play"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "reward_diff_dpo",
        "author": "Wang, Shiqi and Zhang, Zhengze and Zhao, Rui and Tan, Fei and Cam-Tu, Nguyen",
        "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "active_pref_learn",
        "author": "Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David",
        "title": "Active Preference Learning for Large Language Models"
      },
      {
        "key": "a_recipe",
        "author": "Yang, Sen and Cui, Leyang and Cai, Deng and Huang, Xinting and Shi, Shuming and Lam, Wai",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cringe",
        "author": "Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason",
        "title": "Some things are more cringe than others: Preference optimization with the pairwise cringe loss"
      },
      {
        "key": "cal_dpo",
        "author": "Xiao, Teng and Yuan, Yige and Zhu, Huaisheng and Li, Mingxiao and Honavar, Vasant G",
        "title": "Cal-dpo: Calibrated direct preference optimization for language model alignment"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "active_pref_learn",
        "author": "Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David",
        "title": "Active Preference Learning for Large Language Models"
      },
      {
        "key": "a_recipe",
        "author": "Yang, Sen and Cui, Leyang and Cai, Deng and Huang, Xinting and Shi, Shuming and Lam, Wai",
        "title": "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "NLHF",
        "author": "Munos, Remi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Fiegel, C{\\^o}me and others",
        "title": "Nash Learning from Human Feedback"
      },
      {
        "key": "SPIN",
        "author": "Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      },
      {
        "key": "DNO",
        "author": "Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang",
        "title": "Direct nash optimization: Teaching language models to self-improve with general preferences"
      },
      {
        "key": "SPPO",
        "author": "Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan",
        "title": "Self-play preference optimization for language model alignment"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "eva",
        "author": "Ye, Ziyu and Agarwal, Rishabh and Liu, Tianqi and Joshi, Rishabh and Velury, Sarmishta and Le, Quoc V and Tan, Qijun and Liu, Yuan",
        "title": "Evolving Alignment via Asymmetric Self-Play"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "self_instruct",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "EvolQuality_Complexity",
        "author": "Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He",
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "EvolInstuct",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin",
        "title": "Wizardlm: Empowering large language models to follow complex instructions"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "magpie",
        "author": "Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen",
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "cui2024ultrafeedback",
        "author": "Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and others",
        "title": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback"
      }
    ]
  }
]