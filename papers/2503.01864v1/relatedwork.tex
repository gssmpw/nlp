\section{Related Work}
\textbf{RLHF and Preference Learning Algorithms.}
% 标准的RLHF -- RLHF复杂 & DPO系列工作 -- 迭代的工作
Despite RLHF's effectiveness in aligning language models with human preferences \citep{rlhf-ziegler2019finetune, rlhf-stiennon2020learning, rlhf-bai2022training, rlhf-ouyang2022training}, the multi-stage RL training makes it computationally complex and hard to optimize \citep{rlhf-complex-ms,rlhf-complex-fdu}. 
Researchers have been exploring more efficient and simplified alignment algorithms, by simplifying the RL training process \citep{RAFT, RRHF} or utilizing only the offline preference dataset, with DPO \citep{DPO} being a notable example.
In addition to DPO, various offline preference learning objectives have been proposed, such as IPO \citep{IPO}, KTO \citep{KTO}, ORPO \citep{ORPO}, and SimPO \citep{SimPO}.
Moreover, these offline preference optimization methods have been extended to iterative settings,
with new preference pairs continuously sampled or reference policy updated using models trained in previous iteration \citep{cringe,selfreward,DNO,icml24bridging}.

\textbf{Data Quality in Alignment.}
% RLHF中的数据 以及preference dataset数据质量的两个角度：如何采样 & 采样之后得到的数据集中那些数据是更重要的
The importance of data quality in alignment processes has been well-documented, both in RLHF \citep{webgpt,rlhf-bai2022training} and offline preference learning methods.
A significant body of research focuses on the distribution for response sampling during preference dataset construction \citep{iclr24statistical, icml24onpolicydata, icml24bridging}, while another line of work examines the quality of different preference pairs within the preference dataset \citep{wu2024betadpo, filter_dpo, curyy_dpo, BPO}. 
Of particular relevance to this study are efforts to explicitly define and utilize data quality metrics, such as leveraging the explicit reward margin to select high-quality data \citep{rs_dpo, eva} or reweighting loss \citep{reward_diff_dpo}, and using the implicit reward margin to prioritize training data \citep{active_pref_learn, a_recipe} or calibrating loss functions \citep{cringe, cal_dpo}. 
Despite the demonstrated effectiveness of these metrics, their often conflicting properties (as shown in Figure \ref{fig:teaser_a}) necessitate the development of a more universal data quality metric, which motivates us to propose the \methodname{} metric.\\
Additionally, current works based on implicit reward margin are limited to iterative preference learning settings \citep{active_pref_learn, a_recipe}, requiring the model $\pi_\theta$ to be different from $\pi_\mathrm{ref}$ to avoid constant-zero implicit reward $\hat r_\theta$. 
To overcome such a shortcoming, we propose to utilize the SimPO-based implicit reward $\hat r_\theta^\mathrm{Sim}$,
resulting in a new version of implicit reward margin applicable for standard offline preference learning scenarios.

\textbf{Self Play Alignment and Prompt Synthesis.}
% self play alignemnt 包括从y角度的（即SPIN）以及从x角度的EVA，EVA则需要使用 prompt synthesis方法: 包括evol...
The paired comparison nature of preference learning has inspired a range of self-play methods based on two-player games \citep{NLHF, SPIN, DNO, SPPO}, with both players being LMs generating responses to given prompts. Diverse from these works, eva \citep{eva} proposes an asymmetric alignment game involving a prompt creator and a response solver to augment preference data with higher reward differences (\aka informativeness in eva).
To generate new prompts, researchers have developed various prompt synthesis methods such as SelfInstruct \citep{self_instruct}, EvolQuality \citep{EvolQuality_Complexity}, EvolInstruct \citep{EvolInstuct}, Magpie \citep{magpie} and so on\footnote{Our contribution is orthogonal to different prompt synthesis techniques, and we employ EvolInstruct to generate new prompts following eva.}.
%Notably, a substantial proportion of prompts in the ultrafeedback \citep{cui2024ultrafeedback} dataset is also generated by EvolInstruct, making it a proper baseline when comparison with ultrafeedback data is required (as in our experiments).
Our work adopts the asymmetric self-play framework from eva to generate additional preference data, with a focus on evaluating various data quality metrics within this evolving data context.