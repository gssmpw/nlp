\vspace{-5pt}
\section{Discussion}
\textbf{Conclusion.}
In this work, we propose the \textit{\methodname{}} metric, $M_{AP}$, to evaluate preference data quality in alignment.
By measuring the gap from the model's current implicit reward margin to the target explicit reward margin, $M_{AP}$ quantifies the discrepancy between the current model and the aligned optimum, thereby indicating the potential for alignment enhancement.
Extensive experiments validate the efficacy of $M_{AP}$ across various training settings under offline and self-play preference learning scenarios.

\textbf{Limitations and future work}. 
Despite the performance improvements, $M_{AP}$ requires tuning a parameter $\beta$ to combine the explicit and implicit margins; future work could explore how to set this ratio automatically.
Additionally, while our experiments focus on the widely applied DPO and SimPO objectives, a broader investigation with alternative preference learning methods is crucial in future works.

% \section{Conclusion}
% In this paper, we introduce the \methodname{} metric to evaluate preference data quality in LLM alignment.
% By measuring the discrepancy between the model's current implicit reward margin to the target explicit reward margin, this metric quantifies the gap between the current model and the aligned optimum, thereby indicating the potential for alignment enhancement.
% Empirical results demonstrate that training on data selected by our metric consistently improves alignment performance, outperforming existing metrics across different base models and training objectives.
% Moreover, this metric extends to data generation scenarios (\ie self-play alignment): by identifying high-quality data from the intrinsic self-generated context, our metric yields superior results across various training settings, providing a comprehensive solution for enhancing LLM alignment through optimized
% preference data generation, selection, and utilization.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.