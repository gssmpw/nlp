\section{Preliminaries}
\textbf{RLHF with reward models.}
To align a supervised fine-tuned (SFT) model $\pi_\mathrm{SFT}$ with human preferences, the standard RLHF pipeline begins with the reward modeling phase \citep{rlhf-ziegler2019finetune, rlhf-stiennon2020learning, rlhf-ouyang2022training}.
A preference dataset $\mathcal D = \{(x,y_w,y_l)\}$ is constructed by sampling response pairs $(y_1,y_2)\sim\pi_\mathrm{SFT}(\cdot|x)$ from prompts $x$, and human annotators evaluate the preference $y_w\succ y_l|x$, where $y_w$ and $y_l$ are the preferred (winning) and less preferred (losing) responses, respectively.
% , in which a preference dataset is constructed and used to fit the reward model.
% To construct the preference dataset $\mathcal D = \{(x,y_w,y_l)\}$, the SFT model is prompted by prompts $x$ to sample response pairs $(y_1,y_2)\sim \pi_\mathrm{SFT}(\cdot|x)$, which human annotators then evaluate the preference $y_w\succ y_l|x$, with $y_w$ and $y_l$ being the preferred (win) and less preferred (lose) responses respectively.
Although the latent reward %intrinsic reward 
$r^*(x,y)$ underlying %that guides 
human preference is unknown, the Bradley-Terry (BT) model \citep{BTmodel} provides an effective proxy to capture %provides an effective framework for capturing 
human preference probabilities $p^*$ as follows:
\begin{equation}\label{eqn:BT_model}
    p^*(y_1\succ y_2|x) = \sigma(r^*(x,y_1) - r^*(x,y_2)),
\end{equation}
where $\sigma(\cdot)$ denotes the logistic function.
Therefore, a reward model (RM) $r_\phi(x,y)$ can be parameterized and trained with the preference dataset $\mathcal D$ via a negative log-likelihood loss:
\begin{equation}\label{eqn:rm_loss}
    \mathcal L_R(\phi) = -\mathbb E_{(x,y_w,y_l)\sim\mathcal D}\left[\log\sigma(r_\phi(x,y_w) - r_\phi(x,y_l))\right].
\end{equation}
With the learned reward model providing feedback, the language model $\pi_\theta$ is subsequently optimized via RL using the %trained with RL by optimizing the following 
KL-regularized objective:
\begin{equation}\label{eqn:rlhf_obj}
    \max_{\pi_\theta}\mathbb E_{\substack{x\sim\mathcal D\\y\sim\pi_\theta(\cdot|x)}}[r_\phi(x,y)] - \beta \mathbb D_\mathrm{KL}(\pi_\theta(y|x)\|\pi_\mathrm{ref}(y|x)),
\end{equation}
where $\pi_\mathrm{ref}$, typically instantiated via the SFT model $\pi_\mathrm{SFT}$, serves as the reference policy and $\beta$ is a hyperparameter controlling the deviation from the reference model.
% is a fixed reference policy, typically instantiated via the SFT model $\pi_\mathrm{SFT},$ and $\beta$ is a parameter controlling the deviation from the reference model.

\textbf{Direct Preference Optimization (DPO).} DPO \citep{DPO} is a widely adopted %one of the most popular 
offline preference optimization methods. Instead of learning an external reward model, DPO reparameterizes the reward function $r(x,y)$ using a closed-form solution to the KL-regularized reward maximization problem:
\begin{equation}\label{eqn:opt_policy}
    r(x,y) = \beta\log\frac{\pi_r(y|x)}{\pi_\mathrm{ref}(y|x)} + \log Z(x),
\end{equation}
where $Z(\cdot)$ serves as the partition function independent of $y$. 
Applying this reparameterization to the current LLM $\pi_\theta$, DPO derives an implicit reward $\hat r_\theta (x,y) = \beta\log\frac{\pi_\theta(y|x)}{\pi_\mathrm{ref}(y|x)}.$  By fitting it with the offline preference dataset like Equation \eqref{eqn:rm_loss}, the DPO loss is defined as:
\begin{equation}\label{eqn:DPO_loss}
    \mathcal L_\mathrm{DPO}(\theta) = -\mathbb E_{(x,y_w,y_l)\sim\mathcal D}\left[\log\sigma(\hat r_\theta (x,y_w) - \hat r_\theta (x,y_l))\right].
\end{equation}
\textbf{Simple Preference Optimization (SimPO).} SimPO \citep{SimPO} builds on DPO by introducing two key modifications to achieve SOTA performance.
% is another offline preference optimization method, which introduces two key modifications to the DPO loss leading to SOTA performance. 
First, it proposes a new implicit reward $\hat r_\theta^\mathrm{Sim}(x,y)=\beta\log\pi_\theta(y|x)/|y|$ to eliminate the need for a reference model, %omit the reference model, 
where $|y|$ denotes the length of response $y$. 
Such a formulation can be seen as a length-normalized special case of DPO's implicit reward with a uniform reference model \citep{alphadpo}.
Second, it introduces a classification margin term $\gamma$ into the objective. Consequently, the SimPO loss is formulated as follows:
\begin{align}\label{eqn:SimPO_loss}
    &\mathcal L_\mathrm{SimPO}(\theta) =\\
    &-\mathbb E_{(x,y_w,y_l)\sim\mathcal D}\left[\log\sigma(\hat r_\theta ^\mathrm{Sim}(x,y_w) - \hat r_\theta^\mathrm{Sim}(x,y_l) - \gamma)\right].\notag
\end{align}

\textbf{Data quality metrics.} Here we study the data quality evaluation problem in offline preference optimization. 
% Our objective is to develop a metric, 
% Our goal is to develop a metric, denoted as $M(x,y_w,y_l;\theta)$,
We denote the metric by $M(x,y_w,y_l;\theta)$ to assess the quality of preference data $(x,y_w,y_l)$ for the LLM model $\pi_\theta$.
To evaluate different metrics, we can benchmark models trained with the top-$k$ subset of the original preference dataset \citep{a_recipe}, denoted as $\mathcal D_k  \mathrm{:=}  \{(x,y_w,y_l)|M(x,y_w,y_l;\theta) \ge \tau_k\}$, where $\tau_k$ is the $k$-th highest score within the dataset.
For model training, we mainly employ DPO and SimPO as optimization methods due to their verified efficacy.

% In this paper, we study the data quality problem in the offline preference optimization frameworks, and we mainly employ DPO and SimPO as optimization methods due to their verified efficacy.
