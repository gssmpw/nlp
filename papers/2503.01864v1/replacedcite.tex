\section{Related Work}
\textbf{RLHF and Preference Learning Algorithms.}
% 标准的RLHF -- RLHF复杂 & DPO系列工作 -- 迭代的工作
Despite RLHF's effectiveness in aligning language models with human preferences ____, the multi-stage RL training makes it computationally complex and hard to optimize ____. 
Researchers have been exploring more efficient and simplified alignment algorithms, by simplifying the RL training process ____ or utilizing only the offline preference dataset, with DPO ____ being a notable example.
In addition to DPO, various offline preference learning objectives have been proposed, such as IPO ____, KTO ____, ORPO ____, and SimPO ____.
Moreover, these offline preference optimization methods have been extended to iterative settings,
with new preference pairs continuously sampled or reference policy updated using models trained in previous iteration ____.

\textbf{Data Quality in Alignment.}
% RLHF中的数据 以及preference dataset数据质量的两个角度：如何采样 & 采样之后得到的数据集中那些数据是更重要的
The importance of data quality in alignment processes has been well-documented, both in RLHF ____ and offline preference learning methods.
A significant body of research focuses on the distribution for response sampling during preference dataset construction ____, while another line of work examines the quality of different preference pairs within the preference dataset ____. 
Of particular relevance to this study are efforts to explicitly define and utilize data quality metrics, such as leveraging the explicit reward margin to select high-quality data ____ or reweighting loss ____, and using the implicit reward margin to prioritize training data ____ or calibrating loss functions ____. 
Despite the demonstrated effectiveness of these metrics, their often conflicting properties (as shown in Figure \ref{fig:teaser_a}) necessitate the development of a more universal data quality metric, which motivates us to propose the \methodname{} metric.\\
Additionally, current works based on implicit reward margin are limited to iterative preference learning settings ____, requiring the model $\pi_\theta$ to be different from $\pi_\mathrm{ref}$ to avoid constant-zero implicit reward $\hat r_\theta$. 
To overcome such a shortcoming, we propose to utilize the SimPO-based implicit reward $\hat r_\theta^\mathrm{Sim}$,
resulting in a new version of implicit reward margin applicable for standard offline preference learning scenarios.

\textbf{Self Play Alignment and Prompt Synthesis.}
% self play alignemnt 包括从y角度的（即SPIN）以及从x角度的EVA，EVA则需要使用 prompt synthesis方法: 包括evol...
The paired comparison nature of preference learning has inspired a range of self-play methods based on two-player games ____, with both players being LMs generating responses to given prompts. Diverse from these works, eva ____ proposes an asymmetric alignment game involving a prompt creator and a response solver to augment preference data with higher reward differences (\aka informativeness in eva).
To generate new prompts, researchers have developed various prompt synthesis methods such as SelfInstruct ____, EvolQuality ____, EvolInstruct ____, Magpie ____ and so on\footnote{Our contribution is orthogonal to different prompt synthesis techniques, and we employ EvolInstruct to generate new prompts following eva.}.
%Notably, a substantial proportion of prompts in the ultrafeedback ____ dataset is also generated by EvolInstruct, making it a proper baseline when comparison with ultrafeedback data is required (as in our experiments).
Our work adopts the asymmetric self-play framework from eva to generate additional preference data, with a focus on evaluating various data quality metrics within this evolving data context.