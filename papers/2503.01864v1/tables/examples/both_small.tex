\begin{table}[t]
\caption{Example from \href{https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm}{SimPO's preference dataset}, in which the explicit reward margin and implicit reward margin are both small. We rescaled the explicit reward value to a comparable range of implicit rewards.}
\label{example:both_small}
\begin{tabularx}{\textwidth}{@{}p{0.2\linewidth}X@{}}
\toprule
\textbf{Prompt} $\boldsymbol{x}$ & \lstinline[style=mystyle]{let's play a game. i say a sentence, then you make a sentence that follows up my sentence then i give a continuation to yours and so on. you ready?}  \\\midrule
\textbf{Chosen Response} $\boldsymbol{y_w}$ & \lstinline[style=mystyle]{Sounds fun! I'm ready. Give me your first sentence. \\ud83d\\ude0a  \\n\\n} \\\midrule
\textbf{Rejected Response} $\boldsymbol{y_l}$ & \lstinline[style=mystyle]{I'm ready! Let's do it. \\ud83d\\ude0a  \\n\\n**Give me your first sentence!**  \\n\\n} \\\midrule
\textbf{Explicit Rewards} &  Chosen: $r(x,y_w)=13.7$, rejected: $r(x,y_l) = 13.0$, explicit reward margin: $M_r(x,y_w,y_l) = 0.7$. \\\midrule
\textbf{Implicit Rewards} & Chosen: $\hat r_\theta^\mathrm{Sim}(x,y_w)=-3.7$, rejected: $\hat r_\theta^\mathrm{Sim}(x,y_l)=-2.9$, implicit reward margin: $M_\pi(x,y_w,y_l)=0.8$.\\\bottomrule
\end{tabularx}
\end{table}
