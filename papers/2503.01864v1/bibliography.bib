@article{rlhf-christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{rlhf-stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{rlhf-ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{rlhf-bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{rlhf-ziegler2019finetune,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
@article{3H-askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}
@article{gpt4report,
  title={Gpt-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{llama2report,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{llama3report,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{DPO,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{SimPO,
title={Sim{PO}: Simple Preference Optimization with a Reference-Free Reward},
author={Yu Meng and Mengzhou Xia and Danqi Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=3Tzcot1LKb}
}
@inproceedings{
icml24onpolicydata,
title={Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy Data},
author={Fahim Tajwar and Anikait Singh and Archit Sharma and Rafael Rafailov and Jeff Schneider and Tengyang Xie and Stefano Ermon and Chelsea Finn and Aviral Kumar},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=bWNPx6t0sF}
}
@inproceedings{icml24bridging,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@inproceedings{
iclr24statistical,
title={Statistical Rejection Sampling Improves Preference Optimization},
author={Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J Liu and Jialu Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xbjSwwrQOe}
}
@inproceedings{
wu2024betadpo,
title={\${\textbackslash}beta\$-{DPO}: Direct Preference Optimization with Dynamic \${\textbackslash}beta\$},
author={Junkang Wu and Yuexiang Xie and Zhengyi Yang and Jiancan Wu and Jinyang Gao and Bolin Ding and Xiang Wang and Xiangnan He},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=ZfBuhzE556}
}
@inproceedings{rs_dpo,
  title={RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models},
  author={Khaki, Saeed and Li, JinJin and Ma, Lan and Yang, Liu and Ramachandra, Prathap},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={1665--1680},
  year={2024}
}
@article{filter_dpo,
  title={Filtered direct preference optimization},
  author={Morimura, Tetsuro and Sakamoto, Mitsuki and Jinnai, Yuu and Abe, Kenshi and Ariu, Kaito},
  journal={arXiv preprint arXiv:2404.13846},
  year={2024}
}
@inproceedings{active_pref_learn,
  title={Active Preference Learning for Large Language Models},
  author={Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@inproceedings{a_recipe,
  title={Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning},
  author={Yang, Sen and Cui, Leyang and Cai, Deng and Huang, Xinting and Shi, Shuming and Lam, Wai},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={6549--6561},
  year={2024}
}
@article{cringe,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}
@inproceedings{reward_diff_dpo,
  title={Reward Difference Optimization For Sample Reweighting In Offline RLHF},
  author={Wang, Shiqi and Zhang, Zhengze and Zhao, Rui and Tan, Fei and Cam-Tu, Nguyen},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={2109--2123},
  year={2024}
}
@article{eva,
  title={Evolving Alignment via Asymmetric Self-Play},
  author={Ye, Ziyu and Agarwal, Rishabh and Liu, Tianqi and Joshi, Rishabh and Velury, Sarmishta and Le, Quoc V and Tan, Qijun and Liu, Yuan},
  journal={arXiv preprint arXiv:2411.00062},
  year={2024}
}
@inproceedings{SPIN,
  title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}
@inproceedings{cui2024ultrafeedback,
  title={ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{alpacaeval2,
  title={Length-controlled alpacaeval: A simple way to debias automatic evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}
@article{arenahard,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}
@article{BTmodel,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}
@article{rlhf-complex-fdu,
  title={Secrets of rlhf in large language models part i: Ppo},
  author={Zheng, Rui and Dou, Shihan and Gao, Songyang and Hua, Yuan and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Zhou, Yuhao and others},
  journal={arXiv preprint arXiv:2307.04964},
  year={2023}
}
@article{rlhf-complex-ms,
  title={Efficient rlhf: Reducing the memory usage of ppo},
  author={Santacroce, Michael and Lu, Yadong and Yu, Han and Li, Yuanzhi and Shen, Yelong},
  journal={arXiv preprint arXiv:2309.00754},
  year={2023}
}
@article{RAFT,
  title={RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and KaShun, SHUM and Zhang, Tong},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{RRHF,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
@inproceedings{IPO,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}
@article{KTO,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}
@article{ORPO,
  title={Reference-free monolithic preference optimization with odds ratio},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  journal={arXiv e-prints},
  pages={arXiv--2403},
  year={2024}
}
@article{DNO,
  title={Direct nash optimization: Teaching language models to self-improve with general preferences},
  author={Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang},
  journal={arXiv preprint arXiv:2404.03715},
  year={2024}
}
@inproceedings{selfreward,
title={Self-Rewarding Language Models},
author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason E Weston},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=0NphYCmgua}
}
@article{webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}
@article{curyy_dpo,
  title={Curry-dpo: Enhancing alignment using curriculum learning \& ranked preferences},
  author={Pattnaik, Pulkit and Maheshwary, Rishabh and Ogueji, Kelechi and Yadav, Vikas and Madhusudhan, Sathwik Tejaswi},
  journal={arXiv preprint arXiv:2403.07230},
  year={2024}
}
@article{BPO,
  title={Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment},
  author={Wang, Sizhe and Tong, Yongqi and Zhang, Hengyuan and Li, Dawei and Zhang, Xin and Chen, Tianlong},
  journal={arXiv preprint arXiv:2411.10914},
  year={2024}
}
@article{cal_dpo,
  title={Cal-dpo: Calibrated direct preference optimization for language model alignment},
  author={Xiao, Teng and Yuan, Yige and Zhu, Huaisheng and Li, Mingxiao and Honavar, Vasant G},
  journal={arXiv preprint arXiv:2412.14516},
  year={2024}
}
@inproceedings{NLHF,
  title={Nash Learning from Human Feedback},
  author={Munos, Remi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Fiegel, C{\^o}me and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@article{SPPO,
  title={Self-play preference optimization for language model alignment},
  author={Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan},
  journal={arXiv preprint arXiv:2405.00675},
  year={2024}
}
@inproceedings{self_instruct,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13484--13508},
  year={2023}
}
@inproceedings{
EvolQuality_Complexity,
title={What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
author={Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=BTKAeLqLMw}
}
@article{EvolInstuct,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}
@article{magpie,
  title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.08464},
  year={2024}
}
@inproceedings{GPO,
  title={Generalized Preference Optimization: A Unified Approach to Offline Alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, Remi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo Avila and Piot, Bilal},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@inproceedings{
REBEL,
title={{REBEL}: Reinforcement Learning via Regressing Relative Rewards},
author={Zhaolin Gao and Jonathan Daniel Chang and Wenhao Zhan and Owen Oertell and Gokul Swamy and Kiant{\'e} Brantley and Thorsten Joachims and J. Andrew Bagnell and Jason D. Lee and Wen Sun},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=yxjWAJzUyV}
}
@article{alphadpo,
  title={$\alpha$-DPO: Adaptive Reward Margin is What Direct Preference Optimization Needs},
  author={Wu, Junkang and Wang, Xue and Yang, Zhengyi and Wu, Jiancan and Gao, Jinyang and Ding, Bolin and Wang, Xiang and He, Xiangnan},
  journal={arXiv preprint arXiv:2410.10148},
  year={2024}
}
@article{sailheadwind,
  title={Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking},
  author={Rashidinejad, Paria and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.09544},
  year={2024}
}
@article{rlhf_openproblems,
title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
author={Stephen Casper and Xander Davies and Claudia Shi and Thomas Krendl Gilbert and J{\'e}r{\'e}my Scheurer and Javier Rando and Rachel Freedman and Tomek Korbak and David Lindner and Pedro Freire and Tony Tong Wang and Samuel Marks and Charbel-Raphael Segerie and Micah Carroll and Andi Peng and Phillip J.K. Christoffersen and Mehul Damani and Stewart Slocum and Usman Anwar and Anand Siththaranjan and Max Nadeau and Eric J Michaud and Jacob Pfau and Dmitrii Krasheninnikov and Xin Chen and Lauro Langosco and Peter Hase and Erdem Biyik and Anca Dragan and David Krueger and Dorsa Sadigh and Dylan Hadfield-Menell},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=bx24KpJ4Eb},
note={Survey Certification, Featured Certification}
}
@inproceedings{reward_overopt,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}
@article{boostrapping_implicit_r,
  title={Bootstrapping Language Models with DPO Implicit Rewards},
  author={Chen, Changyu and Liu, Zichen and Du, Chao and Pang, Tianyu and Liu, Qian and Sinha, Arunesh and Varakantham, Pradeep and Lin, Min},
  journal={arXiv preprint arXiv:2406.09760},
  year={2024}
}
@article{CDR_annotation,
  title={CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference Annotation},
  author={Guangxuan Xu and Kai Xu and Shivchander Sudalairaj and Hao Wang and Akash Srivastava},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.02481},
}
inproceedings{
spread_annotate,
title={Spread Preference Annotation: Direct Preference Judgment for Efficient {LLM} Alignment},
author={Anonymous},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=BPgK5XW1Nb}
}
@article{shi2024crucial,
  title={The Crucial Role of Samplers in Online Direct Preference Optimization},
  author={Shi, Ruizhe and Zhou, Runlong and Du, Simon S},
  journal={arXiv preprint arXiv:2409.19605},
  year={2024}
}
@inproceedings{ArmoRM,
  title={Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts}, 
  author={Haoxiang Wang and Wei Xiong and Tengyang Xie and Han Zhao and Tong Zhang},
  booktitle={EMNLP},
  year={2024}
}
@inproceedings{pairRM,
    title = "LLM-Blender: Ensembling Large Language Models with Pairwise Comparison and Generative Fusion",
    author = "Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen",
    booktitle = "Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023)",
    year = "2023"
}

@InProceedings{softmax_policy,
  title = 	 {On the Global Convergence Rates of Softmax Policy Gradient Methods},
  author =       {Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6820--6829},
  year = 	 {2020},
  editor = 	 {III, Hal Daum√© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/mei20b/mei20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/mei20b.html},
}
