\section{Method}

\subsection{Overall Model}

Figure~\ref{fig:overview_model} shows our model's overall structure, with the AVSR model on the left and multiple adapter-sets and the noise-scenario-classifier on the right. 

The AVSR model consist of three parts, a pre-trained, frozen ASR model (gray), a group of LoRa adapter (orange) and an AV fusion module (green). Each adapter-set on the right consists of a spicific set of LoRa adapters and an AV fusion module, which are kept to replace the adapter-set in the AVSR model if necessary. The classifier (blue) receives the noisy input mel-spectrum to determine the current noise scenario and select the optimum adapter-set.

Like \cite{AVSR_2023_selfsupervised, AVSR_2024__WhisperFlamingo}, we choose Whisper as the core element of our AVSR model, which is currently one of the most powerful ASR models. As Whisper can only process audio information, the model must be extended by the upstream AV fusion module, which is adopted from~\cite{AVSR_2023_selfsupervised}. This fusion module consists two processing levels. The first level includes two separate CNN-based feature extraction modules for audio and visual inputs. The extracted audio and visual feature vectors are processed by a multi-layer multi-head cross-attention module, which produces the inputs for the Whisper-based model. Further details about the fusion module architecture can be taken from~\cite{AVSR_2023_selfsupervised}. 

In contrast to~\cite{AVSR_2023_selfsupervised} and~\cite{AVSR_2024__WhisperFlamingo}, which perform a full fine-tuning for the pre-trained ASR model, we add LoRa adapters to the ASR model. These adapters provide the advantage that the ASR model weights are kept frozen while only the adapter weights are trained, which means that the base model's basic behavior remains unchanged. We insert adapters to all linear layers of the ASR transformer model's Query, Value, Key and Output layers with a rank of 64. 
Initially, we also tested AdaLoRa adapters. These perform an SVD to determine the optimum rank for each layer, under the constraint that the average rank across all layers remains constant. Against our expectations, it turned out that LoRa adapters yield better results for the tested parameters.

Usually AVSR models aim to simultaneously cover all noise categories and the entire Signal-to-Noise Ratio (SNR) range. We benefit from the lightweight nature of adapter-based approaches and train several noise-scenario-specific adapter-sets that share the same base model. Each adapter-set covers only a small part of the entire noise spectrum.
Due to their lightweight character, the adapter-sets can be easily exchanged when noise scenarios change. We distinguish between two basic concepts and analyze which concept provides better results:
\begin{itemize}
\item Noise-category-specific -- One specific adapter-set for each of the noise categories Babble, Music, Noise and Single Sidespeaker
\item Noise-level-specific -- One adapter-set each for the SNR range above or below 0dB
\end{itemize}

Like ~\cite{AVSR_2023_selfsupervised} we use the frozen Whisper ASR model to generate target values from clean audio inputs at the model levels mel-spectrum level, embedding level after the encoder and logit level after the decoder.
The AVSR model receives synthetically contaminated audio inputs and corresponding video frames, that contain the speaker's lip movements.
During training, we calculate a loss for all three model levels, which are added to a weighted total loss. 

\begin{equation}
\text{L}_{\text{pre}} = 0.5 \cdot L1\left(Mel_c,Mel_n\right) + L1\left(Emb_c,Emb_n\right)
\end{equation}

\begin{equation}
\text{L}_{\text{ft}} = \text{L}_{\text{pre}} + CE\left(Dec_c,Dec_n\right)
\end{equation}

$\text{L}_{\text{pre}}$ defines the loss used during pre-training. Therefore, we calculate the  Mean Absolute Error (MAE) between the model receiving clean inputs and the trained AVSR model receiving noisy inputs at the mel-spectrum and embedding level after the encoder. The mel-spectrum loss is semi-weighted to reduce its influence on the training process.
This loss aims to prevent the generated mel-spectrum from drifting too far from natural mel-spectrums.
$\text{L}_{\text{ft}}$ defines the loss during the fine-tuning. This extends $\text{L}_{\text{pre}}$ by the loss term at the logit level after the decoder. We follow the example of the original Whisper training and use a Cross-Entropy loss at this level.


\subsection{Training Details}

We use two model sizes for our approach, based on the Whisper variants base (74M) and small (244M). The applied fusion module contains 13.2M parameters. The LoRa adapters with rank 64 contain 4.8M parameters for base and 14.3M for small. For the noise-category-specific concept we use four adapter-sets and fusion modules that share one Whisper model. This results in a total of 146M model parameters, of which 72M are trainable for the base version and a total of 354M model parameters with 110M trainable parameters for the larger small version. For the noise-level-specific concept, only two adapter-sets and fusion modules share one Whisper model. This results in 110M total model parameters with 36M trainable parameters for the smaller version and 299M total model parameters with 55M trainable parameters for the larger version.

Our models are trained with a batch size of 16. For the first training step, we use the Voxceleb2 dataset for 112k iterations to pre-condition the models. This is followed by 112k iterations with the LRS3 dataset. For these two steps, we only train fusion module and encoder adapters with a learning rate of 1e-4. Afterwards, we train the the fusion module and all adapters using $\text{L}_{\text{ft}}$. For this, we also use the LRS3 dataset and start with a learning rate of 1e-5 for 21k iterations. This is followed by 21k iterations with the learning rate gradually reduced to 1e-7. Due to convergence problems, the learning rate for the larger small models was reduced by 0.5 after the first training step.
We follows the suggestion of~\cite{AVSR_2023_selfsupervised} and choose a SNR range from -15dB to 30dB during training.


\subsection{Adapter-Set Selector}

As we train individual adapter-sets for different noise scenarios, the noise category or noise level must be determined before processing. For fully automatic adapter-set selection, we built a classifier which is illustrated as the blue block in Figure~\ref{fig:overview_model} on the right. 
For this purpose, we utilize a 10-layer CNN ResNet with 64 processing channels. To determine the noise category, the CNN ResNet is followed by two fully connected linear layer and one Softmax layer. This results in four classes representing the four noise categories. For training, we use a Cross-Entropy Loss.
To determine the noise level, which is defined by the Signal-to-Noise Ratio (SNR) value, the CNN ResNet is also followed by two fully connected linear layers, with a single output neuron in the second layer. Since we predict continuous SNR values, we use a Mean Squared Error Loss (MSE) during training.
