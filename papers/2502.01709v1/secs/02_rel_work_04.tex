\section{Related Work}

\subsection{ASR and AVSR}

Automatic Speech Recognition (ASR) and Audio-Visual Speech Recognition (AVSR) approaches target the automatic transcription of input signals (audio-only or audio-visual).
Conformer and transformer architectures have established in these domains.
Beside conformer approaches~\cite{ASR_conformer_2020,ASR_conformer2_2022} espacially transformer approaches like Whisper~\cite{ASR_whisper_2022}, HuBERT~\cite{ASR_HuBERT_2021}, and Wav2Vec2~\cite{ASR_wav2vec_2020} have gained popularity in ASR. Whisper can be emphasized here as it exhibits a comparatively high robustness in moderate noise conditions~\cite{ASR_whisperAT_2023}. However, all ASR approaches suffer from increased Word Error Rates (WER) under noisy conditions.

One way, to tackle this problem is the development of transformer~\cite{AVSR_avhubert_2022, AVSR_robust_ssAVSR_avhubert2_2022, AVSR_jointly_rawD_2022, AVSR_AV_FTof_aOnlyASR} and conformer-based\cite{AVSR_autoAVSR_2023, AVSR_e2e_conformer,AVSR_AVFormer} AVSR approaches, which use visual information from lip movements to reduce the WER, espacially for noisy conditions.
The best performance for AVSR is currently achieved by the approaches RAVEn~\cite{AVSR_jointly_rawD_2022}, AV-HuBERT~\cite{AVSR_robust_ssAVSR_avhubert2_2022} and AUTO-AVSR~\cite{AVSR_autoAVSR_2023}, meaning that all of them can be considered as state-of-the-art (SOTA). 
While most common AVSR approaches train models from scratch, Whisper-Flamingo~\cite{AVSR_2024__WhisperFlamingo} and our previous work AV Fusion~\cite{AVSR_2023_selfsupervised} use pre-trained Whisper models. AV-Fusion combines the audio-visual features in a fusion module located upstream of the encoder. Whisper-Flamingo works with separate audio and visual encoders and fuses the modalities in the decoder.



\subsection{Adapter Approaches}
Adapter approaches like LoRa~\cite{adapt_lora_2022} and AdaLoRa~\cite{adapt_adalora_2023} allow transformer and conformer fine-tuning, without changing the baseline model’s parameters. For this purpose, both insert additional blocks, consisting of two serial linear layers alongside each linear layer within the baseline model’s attention blocks. The rank, which describes the dimension between both linear layer is chosen to be small in order to significantly reduce the number of trainable parameters compared to a full fine-tuning. 
Various approaches demonstrate that adapters are suitable for adapting pre-trained ASR models to changed domains~\cite{adapt_whisperChildSp_2024, adapt_SURE_2023, adapt_peft_ser_2023, adapt_distilWh_2024, adapt_resTransf_2024, AVSR_AVFormer} or to extend Large Language Models (LLM) with additional modalities~\cite{mllm_oct_2024, mllm_tunLN_2024}. The advantage of trainable adapters, apart from the reduced number of parameters to be trained, is their ability to be used as a lightweight add-on to the baseline model and falling back to the baseline model's basic behaviour if adapters are switched off.

