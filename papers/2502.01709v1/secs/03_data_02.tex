\section{Data}

\subsection{LRS3}

We utilize the LRS3-TED dataset~\cite{DATA_2018_LRS3}, the most comprehensive and challenging dataset for AVSR, to train and evaluate our models. LRS3 contains 150k sequences from TED Talks on YouTube, featuring over 9k speakers and a vocabulary of 51k words. The dataset comprises more than 430 hours of video, divided into Pretrain (407 hours), Trainval (30 hours), and Test (1 hour), which we adopt for our experiments.

\subsection{VoxCeleb2}

VoxCeleb2~\cite{DATA_2018_Voxcel2} is an audio-visual dataset originally designed for speaker recognition, without provided transcriptions. As our approach is trained self-supervised, we can utilize this dataset. It contains over one million utterances from more than 6k speakers. While the dataset is multilingual, we focus solely on the English sequences with approximately 1700 hours.

\subsection{Musan}

To systematically contaminate speech data with synthetic noise, we employ the Musan dataset~\cite{DATA_2015_musan}, which provides diverse audio samples across different categories. 60 hours of speech, 42.5 hours of music, and 6 hours of natural sounds. Following the AV-HuBERT~\cite{AVSR_avhubert_2022} methodology, we generate babble noise by merging 30 speech samples. We split each noise category into 80\% for training, 10\% for validation, and 10\% for testing to prevent any overlap.