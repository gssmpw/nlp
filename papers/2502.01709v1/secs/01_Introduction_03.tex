\section{Introduction}

Automatic Speech Recognition (ASR) and Audio-Visual Speech Recognition (AVSR) share common targets, as both aim to transcribe input sequences. ASR approaches rely on audio-only speech data, which can be particularly challenging in noisy environments, resulting in increased Word Error Rates (WER).
AVSR incorporates additional visual information, typically recordings of lip movements, which is one way to achieve a WER reduction, especially in noisy environments.

Common AVSR approaches train models from scratch on audio-visual data. In contrast, our approach builds on a pre-trained ASR model to benefit from the modeling capabilities of models that are trained on huge amounts of speech data. In this work, we utilize the SOTA ASR model Whisper, which is currently the most robust model for speech-to-text transcription. In general, our approach can be applied to any pre-trained ASR model. 

To adapt the pre-trained audio-only ASR model to the additional visual modality, we use LoRa adapters instead of full fine-tuning the model. These adapters allow a baseline model to be adapted to a specific domain without changing the underlying baseline model's basic behavior. These adapters can be switched on and off in case no visual information is available or changing environmental conditions. 

Conventional AVSR approaches train single models to handle simultaneously all noise categories and a wide range of noise levels. Due to the very lightweight adapters, several domain-specific adapter-sets can be kept available at all times and switched dynamically. We take advantage of this by training multiple specific adapter-sets that enable customized adaptation to specific noise scenarios. Additionally, we include a simple classification layer that determines the noise scenario based on the input audio signal to select the most suitable adapter-set for processing.


Our key contributions are: 
\begin{itemize} 
\item The first approach to integrate adapters into a pre-trained ASR model to enable processing of additional visual information, with a detailed analysis and comparison across multiple noise types and levels
\item The first multi-agent system for Audio-Visual Speech Recognition by developing multiple adapter-sets to optimize the performance across various noise categories and levels
\item Implementing noise category or level classification to dynamically select the most suitable adapter-set for each scenario
\end{itemize}