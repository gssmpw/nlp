\section{Related Work}
\label{sec:related-work}

\inlinesection{Diffusion models.}
Diffusion models~\cite{LSGM,LDM,song_score_based,DDPM,IDDPM,EDM} have emerged as the dominant framework for generative modeling, surpassing traditional approaches like GANs~\cite{GANs,StyleGAN} and VAEs~\cite{VAE}. Diffusion models express generation as a denoising process producing the generated content by progressively denoising an initial noise sample. Owing to their efficiency and scalability,
foundational generative models~\cite{Imagen,ImagenVideo,CogVideoX,SDXL,SVD,MovieGen} have made significant strides in synthesizing visually stunning and semantically aligned images and videos.
%foundational generative models, such as Imagen~\cite{Imagen}, Imagen~Video~\cite{ImagenVideo}, CogVideo~\cite{CogVideo}, Stable Diffusion~\cite{SDXL}, Stable Video Diffusion~\cite{SVD} and others have made significant strides in synthesizing visually stunning and semantically aligned images and videos from textual descriptions.

% PIXEL VS LATENT SPACES
Initially applied to low-resolution visual content in the pixel space~\cite{LSGM,DDPM,IDDPM,EDM}, they have soon been extended to higher resolutions. In Latent Diffusion Models (LDMs)~\cite{LSGM,LDM} high-resolution visual content is modeled in the compact latent space produced by a variational autoencoder (VAE)~\cite{VAE} within a two-stage framework. Latent Flow Models (LFMs)~\cite{LFM,Instaflow}, follow the same approach but leverage Rectified Flows (RFs) to enable faster and more stable sampling.

% EXPLICIT PROGRESSIVE GENERATION
% Some formulations of diffusion models structure generation as an explicit coarse-to-fine process by progressively increasing resolution of the generated content during sampling. Cascaded Diffusion Models (CDM)~\cite{CDM} progressively upsample low-resolution samples, % but struggle with scale consistency. 
% Matryoshka Diffusion Models (MDM)~\cite{MDM} adopt a nested hierarchical structure, %but at the cost of increased computational overhead and training complexity.
% Hierarchical Patch Diffusion Models (HPDM)~\cite{HPDM} operate on a hierarchy of localized patches. % but often fail to capture long-range dependencies.
% Pyramid Flow \cite{PyramidFlow} expresses generation as the denoising of latent representations of increasing resolution within the sampling trajectory, improving training and inference efficiency.

% IMPLICIT PROGRESSIVE GENERATION
Recent work attributes the success of diffusion models to a form of implicit spectral autoregression~\cite{InverseHeatDissipation,DCTdiff} implied by the progressive removal of noise during sampling, resulting in the generation of visual content in a coarse-to-fine manner.
Such result holds in the pixel-space of natural images, based on its pattern of decreasing spectral power~\cite{ruderman1977}.
We show that popular autoencoders~\cite{Flux,CosmosTokenizer,LTX-video,CogVideoX} have a less pronounced pattern of decreasing spectral power, inhibiting implicit spectral autoregressive generation.
Building on this observation, this work proposes a regularization scheme that re-establishes this property, consistently showing improved LDM performance and avoiding the need for explicit coarse-to-fine generation.

%\paragraph{Diffusion models (not sure if needed)}.
%Foundational papers on diffusion: DDPM/EDM/EDMv2/recflow papers. \willi{We touch on DDPM, EDM, recflow in the DM section}
%They advanced the field quite a lot.

%\paragraph{Foundational Generative Models.}

% \willi{I tried to merge it with Diffuson Models, feel free to change back}
%Foundational Generative models, such as Imagen~\cite{Imagen}, Imagen~Video~\cite{ImagenVideo}, CogVideo~\cite{CogVideo}, Stable Diffusion~\cite{SDXL}, Stable Video Diffusion~\cite{SVD} and others have made significant strides in synthesizing visually stunning and semantically aligned images and videos from textual descriptions. They were originally based on U-net-like~\cite{U-net} architectures ...
%\rameen{}

% Now, people invest
% a ton of compute: Imagen, ImagenVideo, StableDiffu-
% sion 1/2/3, Stable Video Diffusion, etc. We also need to



\inlinesection{Image and video autoencoders}.
Due to the success of LDMs, a lot of effort has been devoted to the development of better AEs. 
Image LDMs \cite{LDM} and early video diffusion models \cite{SVD,AnimateDiff} employ a spatial AE with a compression ratio of $1{\times}8{\times}8$. 
The rapid advancement of video diffusion models poses the demand for 3D AEs that jointly compress spatial and temporal dimensions to further improve efficiency \cite{CogVideo,zhou2024allegro,HunyuanVideo}. 
Among them, Open-Sora \cite{opensora} inherits the $1{\times}8{\times}8$ spatial AE and trains a decoupled $4{\times}$ temporal AE on top of its latent space, while others tend to build a hierarchical spatio-temporal AE with 3D causal convolutions \cite{xing2024videovaeplus,wu2024ivvae,chen2024deep,zhao2024cvvae,sadat2024litevae,hansen2025vitok}. 
To improve the reconstruction quality, Open-Sora-Plan~\cite{opensoraplan} and CosmosTokenizer~\cite{CosmosTokenizer} propose to employ wavelet transforms to enhance high-frequency details. 
Another popular trend is to further increase the compression ratio to reduce the number of tokens in the latent space \cite{xie2024sana,tian2024reducio,LTX-video}, thus enabling a more efficient denoising process. 
In addition to the continuous AEs explored in this work, multiple discrete AEs \cite{wang2024omnitokenizer,tang2024vidtok,CosmosTokenizer} are proposed to aid autoregressive tasks. \citet{esteves2024spectral} leverages a wavelet transform to produce latents corresponding to different frequency components. %\willi{Should we include spectral image tokenizer as it is the one that most explicitly enforces the progressive generation we are trying to achieve with the regularization?}
% All sorts of the autoencoder papers (there are 10+ of them). Wavelet-based approaches are very similar (for example Spectral Image Tokenizer \cite{esteves2024spectral}). We can also cite smth like OpenSora AE. And all the AEs from diffusers. 
% \yanyu{TODO}

\inlinesection{AEs for compression}.
Many works train neural-based AEs for image compression~\cite{balle2016end,balle2018variational,minnen2018joint,cheng2020learned}, typically with $16{\times}16$ downsampled latents which are discrete and entropy constrained.
Video compression AEs involve autoregressive AEs~\cite{DCVC,DCVC-DC,DCVC-TCM} with explicit frame-wise 
formulations that utilize motion vectors or implicit modeling~\cite{VCT}.
These approaches target high-quality reconstruction with low bitrates and adopt complex designs for learnable entropy models.
They typically employ a larger number of latent bottleneck channels ($96{-}192$), which is not generally suited for the generation task, thus we do not consider them in this work.

\inlinesection{Concurrent works}.
% DCTDiff~\citep{DCTdiff} trains a diffusion model in the DCT space, capitalizing further on their spectral autoregression property.
Independently from us, EQ-VAE~\citep{EQVAE} proposes scale equivariance regularization for autoencoders, but with a different motivation of improving their equivariance to spatial transforms.

