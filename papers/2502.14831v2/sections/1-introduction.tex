\section{Introduction}
\label{sec:intro}

\input{figures/convergence}

In recent years, diffusion models (DMs) have emerged as the dominant generative modeling paradigm in computer vision.
However, the high dimensionality of visual data poses a significant challenge, making the direct application of diffusion models impractical.
Latent diffusion models (LDMs)~\cite{LSGM,LDM} have become the main approach in mitigating this issue, demonstrating remarkable success in generating high-resolution images~\cite{Flux, DALLE-3, SD3} and videos~\cite{Sora, CogVideoX, HunyuanVideo}.
A typical LDM consists of two main components: an autoencoder and a diffusion backbone.
Most recent breakthroughs have been driven by scaling up diffusion backbones~\cite{DiT}, while autoencoders (AEs) have received comparatively less attention.

Recently, the research community has begun focusing more on improving the autoencoders, recognizing their crucial impact on overall performance, but most effort has been concentrated on enhancing reconstruction quality~\cite{Flux, CogVideo, CosmosTokenizer, LTX-video, DC-AE} and achieving higher compression ratios~\cite{CosmosTokenizer, LTX-video, DC-AE} to accelerate the diffusion process.
However, we argue that a critical yet under-explored aspect, which we refer to as \diffusability\footnote{\Diffusability describes how easily a distribution can be modeled by a diffusion process: high \diffusability indicates that the distribution is easy to fit, whereas low \diffusability makes the process more complex.}, also plays a key role in determining the utility of autoencoders.
Indeed, all three factors---reconstruction quality, compression efficiency, and diffusability---are essential for the practical effectiveness of LDMs.
Specifically, inaccurate reconstruction sets an upper bound on generation fidelity, low compression efficiency leads to slow and costly generation, and poor diffusability necessitates the use of heavier, more expensive, and sophisticated diffusion backbones, further limiting LDM quality.

Diffusion models possess a unique property of being coarse-to-fine in nature~\cite{spectral-autoregression, DCTdiff}: in the denoising process, they synthesize low-frequency signal components first and add high-frequency ones on top of them later.
It is a beneficial trait since it allows to defer error accumulation to higher frequency parts of the spectrum, which aligns well with how humans perceive quality: we are sensitive to the image structure and composition, but oblivious of its fine-grained textural details.
However, when applying the diffusion process in the latent spaces of pre-trained autoencoders, the correspondence between latent low-frequency components and their RGB counterparts may be lost, hindering the spectral autoregression property.

In this work, we identify a correlation between the spectral properties of the latent space and its diffusability.
% Inspired by recent findings~\cite{DCTdiff, spectral-autoregression} suggesting that diffusion models function as autoregressive models in spectral space,
We analyze the spectral characteristics of latent representations across several widely used image and video autoencoders.
Our investigation reveals a prominent high-frequency component in these latent spaces, deviating significantly from the spectral distribution of RGB signals.
This component becomes even more pronounced as the channel size increases, which the recent autoencoders use to improve reconstruction.
%We hypothesize that since diffusion models tend to capture low-frequency details more effectively, this high-frequency component may exhibit inherently low diffusability. 
We hypothesize that the flat spectral distribution induced by the strong high-frequency component harms the spectral autoregression property.
Moreover, we demonstrate that these high-frequency components substantially influence the final RGB result, and their inaccurate modeling can introduce noticeable visual artifacts.
Finally, we show that standard KL regularization is insufficient to address spectrum defects and, in some cases, may even amplify the issue.

To mitigate these spurious high-frequency components in latent representations, we propose a simple and effective regularization strategy.
Our approach involves aligning the latent space and the RGB space at different frequencies. 
This is achieved by enforcing scale equivariance in the decoder â€” ensuring that downsampled latents correspond to downsampled RGB representations.
Our method requires minimal modifications and few additional autoencoder fine-tuning steps, yet significantly enhances diffusability across various architectures, ultimately improving the quality of generated samples.
We validate our approach on both image and video autoencoders, including FluxAE~\cite{Flux}, CosmosTokenizer~\cite{CosmosTokenizer}, CogVideoX-AE~\cite{CogVideo}, and LTX-AE~\cite{LTX-video}, consistently demonstrating improved LDM performance on ImageNet-1K~\cite{ImageNet} $256^2$, reducing FID by 19\% for DiT-XL, and Kinetics-700~\cite{kinetics700} $17\times 256^2$, reducing FVD by at least $44\%$.

% Finally our contribution can be summarized as follows:
% \begin{itemize}
% \item We provide a detailed analysis of the relationship between spectral properties of latent spaces and diffusability in LDMs.
% \item We introduce a simple and practical method for regularizing latent spaces to improve diffusability.
% \item We conduct a comprehensive evaluation of existing autoencoders and contribute to their enhancement.
% \end{itemize}
