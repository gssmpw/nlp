\section{Experiments}
\label{sec:experiments}

\input{figures/samples}
\input{tables/imagenet}

\inlinesection{Data.}
We trained all the autoencoders on in-the-wild data which do not overlap with ImageNet-1K~\cite{ImageNet} or Kinetics-700~\cite{kinetics700} to make sure that there is no data leak in the autoencoders, and that they remain general-purpose.
For this, we used our internal image and video datasets of the $256^2$ resolution, which are similar in distribution of concepts and aesthetics to the publicly available in-the-wild datasets like COYO~\cite{COYO} and Panda-70M~\cite{Panda70M}.
To control for the impact of the data (and also the training recipe), we trained a separate autoencoder baseline for each setup without using our proposed regularization.
In several cases, just fine-tuning on such in-the-wild data already yields better diffusion performance (e.g., see DiT-XL results in \cref{table:imagenet}).

\inlinesection{Evaluation.}
We evaluate image DiT models via \fid and \dinofid (Frechet Distance computed on top of DINOv2~\cite{DINOv2} features), where the latter was shown to be a more reliable metric~\cite{DinoFID, EDMv2}.
We evaluate video DiT models with \fvd~\cite{FVD}, \fid, and \dinofid, except for ablations where we rely on 5,000 samples.
% (we found it to correlate strongly with 50,000 samples results~\cite{HPDM}).
For image models, we use 50,000 samples without any optimization for class balancing.
To evaluate autoencoders, we used PSNR, SSIM, LPIPS and \fid metrics computed on 512 samples from ImageNet and Kinetics-700 for image and video autoencoders, respectively.

\inlinesection{Training details.}
All the LDM models are trained for 400k steps with 10k warmup steps following the rectified flow diffusion parametrization~\cite{NormFlowsWithStochInterp, LFM, SD3}.
Following \citet{SD3}, we use a logit-normal training noise distribution.
We use either $2 \times 2$ or $1 \times 1$ patchification in DiT~\cite{DiT} to match the compute between DiTs trained on top autoencoders with different compression ratios.
Our video DiT is a direct adaption of the image one where we additionally unroll the temporal axis, following the recent works on video diffusion models~\cite{CogVideoX}.
We do not use patchification for the temporal axis in video DiTs.
In contrast to prior work (e.g., \citet{LDM}), we average the KL loss across the latent channels and resolutions: this has no theoretical impact, but allows to compare autoencoders with different bottleneck sizes.
% We do not use KL regularization for \regshortname-regularized autoencoders.

\inlinesection{Inference details.}
We run DiT inference with 256 steps without classifier-free guidance~\cite{CFG} for quantitative evaluations since different models are too sensitive to it and should be tuned separately~\cite{EDMv2, CFG_in_interval}.

% Additional training and inference details are provided in Appendix~\apref{ap:details}.

\subsection{Improving Existing Autoencoders}
\label{sec:experiments:main}
We apply our training pipeline on top of 3 different autoencoders.
For each autoencoder, we trained it while freezing the last output layers to avoid breaking their adversarial fine-tuning, which should have no impact on the latent space~\cite{DC-AE}.
We emphasize that \emph{none} of the explored modern autoencoders publicly released their training pipelines.
For the pretrained snapshots of autoencoders, we used the original snapshots available in the \verb|diffusers| library~\cite{diffusers}.

\inlinesection{Improving image autoencoders.}
% For image autoencoders, we used 1) FluxAE~\cite{Flux} with $8 \times 8$ compression ratio and 16 latent channels; 2) CosmosTokenizer~\cite{CosmosTokenizer} with $16 \times 16$ compression and 16 latent channels.
For the image autoencoder, we used FluxAE~\cite{Flux} with $8 \times 8$ compression ratio and 16 latent channels (since it is the most popular modern autoencoder in the community) and \cmsaei~\cite{CosmosTokenizer} with $16 \times 16$ compression ratio and 16 channels as a high-compression autoencoder.
% 2) CosmosTokenizer~\cite{CosmosTokenizer} with $16 \times 16$ compression and 16 latent channels.
For all the experiments (unless stated otherwise), we fine-tuned it for just 10,000 training steps with a batch size of 32 (320,000 total seen images) using $2\times$ and $4\times$ downsampling ratios, chosen randomly during a forward pass of the regularization loss.
DiT training on top of the unchanged Flux Autoencoder is labeled as ``vanilla''.
Autoencoders fine-tuned for 10,000 steps with our proposed \regshortname regularization is denoted via the ``+ FT-\regshortname'' suffix.
To control for the fine-tuning data and training pipeline, we fine-tuned each autoencoder without adding our regularization as an additional loss (denoted via ``+ FT'').

For the LDM benchmark, we utilized ImageNet~\cite{ImageNet} at $256 \times 256$ resolution.
We used the DiT~\cite{DiT} model as the backbone since it is the most popular modern latent diffusion backbone.
Compared to the original paper, we incorporated several recent advancements into the DiT architecture to improve the baseline performance, as described in \cref{ap:details}.
% For FluxAE, since it is more popular, we also trained DiT-L/2 and DiT-XL/2.
Qualitative samples from DiT-XL/2 are provided in \cref{fig:samples}.
The results are shown in \cref{table:imagenet}.
One can observe that our proposed regularization greatly improves the \diffusability of the downstream LDM model, allowing to achieve 19\% lower \fid compared to the vanilla Flux AE and 8\% lower \fid compared to the Flux AE, fine-tuned in our training pipeline without the \regshortname regularization.

The improvement for \cmsaei is reduced with the main reason being that our training pipeline hurts its performance (we explored over 10 different hyperparameters setups to tune the vanilla model): after fine-tuning it for 10,000 steps with only reconstruction losses (it does not use KL regularization by default), the downstream \fid performance increases by 14\% from 11.69 to 13.59.

\inlinesection{Improving video autoencoders.}
For video autoencoders, we used \cvaefull~\cite{CogVideo} (\cvae) with $4 \times 8 \times 8$ compression and 16 latent channels and LTX-AE~\cite{LTX-video} with $8 \times 32 \times 32$ compression and 32 latent channels.
The latter serves as a strong high-compression autoencoder baseline.
For all the experiments, we fine-tune them for 20,000 training steps on the joint image and video dataset with the batch size of 32.
Image batches are treated as single-frame videos which is possible due to the causal structure of the video autoencoders~\cite{MAGVITv2}.
% We use the same notation as for the image autoencoders, denoting the baseline officially pre-trained checkpoint from diffusers as ``vanilla'', the one which was fine-tuned with \regname --- ''FT-\regshortname'', and the training pipeline-controlled one --- ``FT''.

\input{tables/kinetics}
\input{tables/kl-vs-downreg}

\input{figures/reg-dct-cut-flux}
\input{figures/ablations-downreg}

Similar to the image autoencoder experiments, we train a DiT model on Kinetics-700~\cite{kinetics700} on three variants: 1) a ``vanilla'' autoencoder snapshot; 2) the vanilla autoencoder fine-tuned for 20,000 steps in our training pipeline (denoted as ``FT''); and 3) the autoencoder snapshot, fine-tuned with our downsampling regularization (denoted as ``FT-\regshortname'').
For \ltxae, we used a reduced patchification resolution of $1 \times 1$ to compensate for its extreme compression ratio.
The results are presented in \cref{table:k700}: DiT model on top \regshortname-regularized autoencoders has drastically better performance: $44$\% and $54$\% lower \fvd for \cvae and \ltxae, respectively.
Our training pipeline allowed to achieve better DiT-B training for \cvae (650.4 vs 447.3 \fvd), but led to worse scores for \ltxae (854.4 vs 876.6), which we found less stable to train.
Adding our regularization strategy greatly improves the LDM performance in each case.
One can also observe that the boost for video autoencoders is larger than in the image domain (at least $-$44\% reduced \fvd for DiT-B for video generation vs $-7$\% reduced FID for DiT-XL for image generation).
We attribute this to two factors.
First, improvements in Frechet Distances~\cite{FID} do not scale linearly (i.e., their smaller values are progressively harder to improve).
Next, causal video autoencoders have less regular latent structure: the first frame in a video is encoded into the same representation size as the subsequent chunks, which leaves more room to enhance the diffusability of the latent space.

We additionally trained a DiT-XL/2 model for the \cvae family to explore the scalability of our regularization.
For this large-scale setup, our \regshortname regularization improved the \fvd score by almost twice.

% 3) CosmosTokenizer~\cite{CosmosTokenizer} with $8 \times 16 \times 16$ compression and 16 latent channels. \willi{We say how long we finetune image AEs, but not that video AEs are finetuned 2x longer}

\subsection{Ablations}
\input{tables/ae-rec}

\inlinesection{Does \regname hurt reconstruction?}
\regname in VAEs improves downstream generation quality in terms of FID (\cref{table:imagenet,table:k700}).
We now examine its impact on AE reconstruction quality. \Cref{table:ae-rec} presents results across four reconstruction metrics â€” PSNR, SSIM, LPIPS~\cite{lpips}, and FID, on 50,000 samples from ImageNet and Kinetics for image and video autoencoders, respectively. Reconstruction quality remains similar across the models. 

\inlinesection{Can LDM performance be improved by tweaking the KL weight instead?}
In \cref{table:kl-vs-downreg}, we show that increasing the KL strength can indeed improve the LDM performance for DiT-S/2, but it inevitably hurts reconstruction, as shown by the PSNRs, which bottlenecks the scalability of larger LDM models.
In contrast, our proposed \regname allows to achieve good LDM performance without hurting the reconstruction quality of the autoencoder.
For these ablations, we trained the DiT-S/2 variants for 200K training steps and DiT-L/2 variants for 400K steps, and ran inference for 80 steps without classifier-free guidance.
One can see that for a small compute budget, increasing the KL strength is beneficial: the best LDM score is obtained with the highest KL $\beta = 0.1$.
But it severely affected the reconstruction quality of the autoencoder, which limited its scaling: the corresponding DiT-L/2 LDM variant is ranked among the worst.
At the same time, our developed regularization performs well for all DiT variants and does limit scaling.

\inlinesection{Effect of \regshortname regularization strength.}
In \cref{fig:ablations-downreg}, we show the effect of varying the loss weight on the FluxAE performance on ImageNet.
Increasing the \regshortname regularization strength naturally worsens the total reconstruction quality since the decoder is trained to generalize its performance across both low frequency and high frequency latents (via downsampling) while having the same capacity. We choose the value of 0.25 to maintain reconstruction performance compared to the base AE model while improving the generation quality as shown in~\cref{table:imagenet,table:k700}.


% See \cref{tab:kl-ablations} for details.

\inlinesection{Effect of DCT spectrum cutting.} To examine the impact of downsampling regularization, we evaluate reconstruction quality by progressively removing high-frequency DCT components from the latents. \Cref{fig:reg-dct-cut-flux} presents reconstruction metrics on 512 samples from the ImageNet validation set for the baseline FluxAE and fine-tuning, with and without regularization. As the cut ratio increases on the x-axis, indicating the removal of more high-frequency components, the AE with regularized latents consistently achieves the best reconstruction quality across all metrics.
This underscores the regularization role in aligning the spectral properties of the latent and RGB spaces.

