\section{Implementation Details}
\label{ap:details}

\inlinesection{DiT model details}.
To strengthen the baseline DiT performance, we integrated into it the latest advancements from the diffusion model literature.
Namely, we used self conditioning~\cite{RIN} and RoPE~\cite{RoPE} positional embeddings.
Besides, we switched to the rectified flow diffusion parametrization~\cite{NormFlowsWithStochInterp, RecFlow, LSGM}, which was recently shown to have better scalability with a fewer amount of inference steps~\cite{SD3}.

\inlinesection{DiT training details}.
All the DiT models are trained for 400,000 steps with 10,000 warmup steps of the learning rate from 0 to 0.0003 and then its gradual decay towards 0.00001.
We used weight decay of 0.01 and AdamW~\cite{AdamW} optimizer with beta coefficients of 0.9 and 0.99.
We used posterior sampling from the encoder distribution for VAE-based autoencoders.
In contrast to the original work, we found it helpful to do learning rate decay to 0.00001 using the cosine learning rate schedule.
We used the same model sizes for DiT-S (small), DiT-B (base), DiT-L (large) and DiT-XL (extra large), as the original work~\cite{DiT}:
\begin{itemize}
    \item DiT-S: hidden dimensionality of 384, 12 transformer blocks, and 6 attention heads in the multi-head attention.
    \item DiT-B: hidden dimensionality of 768, 12 transformer blocks, and 12 attention heads in the multi-head attention.
    \item DiT-L: hidden dimensionality of 1024, 24 transformer blocks, and 16 attention heads in the multi-head attention.
    \item DiT-XL: hidden dimensionality of 1152, 28 transformer blocks, and 16 attention heads in the multi-head attention.
\end{itemize}
We used gradient clipping with the norm of 16 for all the DiT models.
Our models were trained in the FSDP~\cite{FSDP} framework with the full sharding strategy on a single node of $8 \times$ NVidia A100 80GB GPUs or $8 \times$ NVidia H100 80GB GPUs (depending on their availability in our computational cluster).

For \cvae, since it is considerably slower than other autoencoders, we trained LDMs on pre-extracted latents.
For this, we pre-extracted them on random 17-frames clips.
In essence, this reduces

\inlinesection{Autoencoders training details}.
Since none of the autoencoders had their training pipelines released, we had to develop the training recipes for each of the autoencoder baselines individually which would not be detrimental to neither their reconstruction capability nor downstream diffusion performance.
To do this, we ablated multiple hyperparameters (the most important ones being learning rate and KL regularization strength) to arrive to a proper setup.
We chose the KL weight in such a way that the KL penalty maintains approximately the same magnitude as the pre-trained checkpoint.

Each autoencoder is trained with AdamW~\cite{AdamW} optimizer, with betas of 0.9 and 0.99, and weight decay of 0.01.
The learning rate was grid-searched individually for each autoencoder and is provided in \cref{tab:hyperparameters}.
In all the cases, we used mixed precision training with BFloat16.

During training, we maintained an exponential moving average of the weights~\cite{EDMv2}, initialized from the same parameters as the starting model, and having a half life of 5,000 steps.
% For each autoencoder, we were searching for the KL regularization coefficient in a way that the KL magnitude remains the same 

We emphasize that, when applying our regularization strategy on top of an autoencoder baseilne, we do not alter other hyperparameters (like learning rate), except for KL regularization which we disable for \regshortname-regularized models (even though we found it helpful in some of our explorations).

For each autoencoder, we freeze the last output layers of the decoder.
The motivation is the following: they were fine-tuned with the adversarial loss, which we want to exclude from the equation without hurting the ability of an autoencoder to model textural details which \fid would be sensitive to~\citep{LDM} and which do not influence the latent space properties.
Namely, we freeze the last normalization and output convolution layers.
In each case, the amount of frozen parameters constitute a negligible amount of total parameters.

Other hyperparameters for autoencoders training are provided in \cref{tab:hyperparameters}.

\input{tables/hyperparameters}
