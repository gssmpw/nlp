\section{Additional Exploration}
\label{ap:freq-reg}

In \cref{sec:method}, we outlined the base \regname strategy to regularize the spectrum of an autoencoder which has a strong advantage of being very easy to implement by a practitioner.
However, it could be beneficial to possess more advanced tools for a finer-grained control over the latent space spectral properties.
This section outlines them and provides the corresponding ablation.

\subsection{Explicitly Chopping off High-Frequency Components}

Rather than applying downsampling to produce latents and RGB targets for regualrization, it is possible to replace some ratio of high-frequency components with zeros. To do so, DCT is applied to the latents and RGB targets where a chosen set of frequency components are masked out. The modified components are then translated back to the spatial domain by inverse DCT to form the training latents and reconstruction targets.

\begin{equation}\label{eq:hard-hf-penalty}
\loss_\text{CHF}(x) = d(x, \Dec(z)) + d( {D^{-1}}(D({x}) * \mathbf{M}), \Dec({D^{-1}}(D({z}) * \mathbf{M}) ) + \loss_\text{reg},
\end{equation}
where $D$ and $D^{-1}$ represent DCT and its inverse, respectively. $\mathbf{M}$ is a $B \times B$ binary mask indicating which frequencies to zero out defined as follows:
% \willi{Are we applying this in B * AEupsamplefactor DCT space for x?}
\begin{equation}\label{eq:zigzag-mask}
\mathbf{M}(u,v) =
\begin{cases}
    1, & \text{if } \zigzag (u,v) < B^2 - N,\\
    0, & \text{otherwise}.
\end{cases}
\end{equation}
% \willi{The table has some lines that suggest M may not be cut of in zigzag order as the equation implies}
$N$ controls the frequency cutoff. We provide the ablation for this strategy in \cref{table:cuthf-ablation}.

\begin{table}[h]
\caption{Ablations for explicit high-frequency chop off for DiT-S/2 trained for 200,000 iterations on top of Flux AE with such a regularization. While it can achieve better results for some of the baselines than naive downsampling, we opt out for the latter strategy due to its simplicity. For the non-zigzag order ablation, we cut across each $x$ and $y$ axes independently}
\label{table:cuthf-ablation}
\centering
% \resizebox{1.0\linewidth}{!}{
\begin{tabular}{llcc}
\toprule
Stage II & Stage I  & \dinofidfivek \\
\midrule
DiT-S/2 & FluxAE + chop off 90\% (non-zigzag order) & 912.4 \\
DiT-S/2 & FluxAE + chop off 70\% (non-zigzag order) & 915.6 \\
DiT-S/2 & FluxAE + chop off 30\% (non-zigzag order) & 929.7 \\
DiT-S/2 & FluxAE + chop off 10\% (non-zigzag order) & 916.5 \\
DiT-S/2 & FluxAE + chop off 90\% (zigzag order) & 935.5 \\
DiT-S/2 & FluxAE + chop off 70\% (zigzag order) & 932.8 \\
DiT-S/2 & FluxAE + chop off 30\% (zigzag order) & 962.9 \\
DiT-S/2 & FluxAE + chop off 10\% (zigzag order) & 930.1 \\
\midrule
DiT-S/2 & FluxAE (vanilla) & 992.0 \\
DiT-S/2 & FluxAE with optimal (out of 8) KL $\beta$ & 929.6 \\
% \midrule
% DiT-B/2 (orig) & \multirow{4}{*}{SD-VAE-ft-MSE} & 43.47 & $-$ \\
% DiT-L/2 (orig) & & 23.33 & $-$ \\
% DiT-XL/2 (orig) & & 19.47 & $-$ \\
% ~+ 6.6M steps (orig) & & 12.03 & n/a & 121.50 \\
% ~+ 6.6M steps (orig) & & 12.03 & $-$ \\
\bottomrule
\end{tabular}
% }
\end{table}

In \cref{fig:progressive-dct-cut}, we provided the visualizations for a FluxAE resiliense with and without such chopping high-frequency regularization for $50\%$ HF dropout rate.
In \cref{fig:progressive-dct-cut-downreg}, we provide an equivalent visualization for \regshortname-fine-tuned FluxAE: while it is less resilient to frequency dropout than \regchfshortname, but is still noticeably better than the vanilla model.

\input{figures/qualitative-dct-cut-downreg}


\subsection{Soft Penalty for High-Frequency Components}

Instead of directly removing some of the components, which might become a too strict regularization signal, one can consider penalizing the amplitudes of high-frequency components in a soft manner.
Concretely, given a $B \times B$ block, we construct the following weight penalty matrix:
% amplitude = x_dct.abs() # [..., b, b]
% x_coords = torch.linspace(0, 1, steps=block_size, device=x.device, dtype=x.dtype).unsqueeze(0).expand(block_size, -1) # [b, b]
% y_coords = torch.linspace(0, 1, steps=block_size, device=x.device, dtype=x.dtype).unsqueeze(1).expand(-1, block_size) # [b, b]
% weight = (x_coords + y_coords).float().pow(power) / 4.0 # [b, b]
% weight = misc.unsqueeze_left(weight, amplitude) # [..., b, b]
% loss = (amplitude * weight).reshape(amplitude.shape[0], -1).mean(dim=1) # [batch_size]

\begin{equation}\label{eq:soft-hf-penalty-matrix}
\mathcal{W}_{uv} = (u + v)^p / B^p.
\end{equation}

Next, the soft regularization loss itself is computed as:
\begin{equation}\label{eq:soft-hf-penalty}
\loss_\text{softreg} = \sum_{u,v} D_{uv}(z) \cdot \mathcal{W}_{uv}.
\end{equation}

During training, when enabled, we add it to the main loss with the weigh $\gamma$.
We found it beneficial in some of our experiments when it is added with a small coefficient (e.g., 0.01).
While it is possible to achieve higher results with more fine-grained regularization, we opt to use the simpler version since we believe it would be easier to employ by the community.

To ablate its importance, we trained DiT-B/1 model on top of FluxAE models, fine-tuned with a different strength $\gamma$.
The results are presented in \cref{table:softregweight-abl}.

\begin{table}[ht]
\caption{Ablating the regularization strength $\alpha$ of our proposed \regname regularization.}
\label{table:softregweight-abl}
\centering
% \resizebox{1.0\linewidth}{!}{
\begin{tabular}{llcc}
\toprule
Stage II & Stage I  & \fid$_{5k}$ & \dinofid$_{5k}$ \\
\midrule
\multirow{6}{*}{DiT-B/1}
& FluxAE + FT-\regshortname $\gamma = 0.001$ & 26.43 & 497.14 \\
& FluxAE + FT-\regshortname $\gamma = 0.025$ & 25.46 & 477.61 \\
& FluxAE + FT-\regshortname $\gamma = 0.01$ & 26.72 & 487.06 \\
& FluxAE + FT-\regshortname $\gamma = 0.05$ & \cellbest{24.28} & \cellbest{458.11} \\
& FluxAE + FT-\regshortname $\gamma = 0.1$ & \cellsecond{25.84} & \cellsecond{461.97} \\
\bottomrule
\end{tabular}
% }
\end{table}


\begin{figure}
\centering
\includegraphics[width=0.4\linewidth]{assets/zigzag.pdf}
\caption{Illustration of the zigzag indexing order of DCT.}
\label{fig:zigzag}
\end{figure}

\subsection{ImageNet $512^2$ experiments}

We trained our DiT-L/2 for class-conditional $512^2$ ImageNet-1K generation for 400K steps for FluxAE~\cite{Flux}, the results are presented in \cref{table:imagenet-512}.

\input{tables/imagenet-512}

\subsection{Ablating regularization strength $\alpha$}

To ablate the importance of the regularization strength $\alpha$, we train FluxAE for 10,000 steps with a varying strength.
The results are presented in \cref{table:regweight-abl}.

\begin{table}[ht]
\caption{Ablating the regularization strength $\alpha$ of our proposed \regname regularization.}
\label{table:regweight-abl}
\centering
% \resizebox{1.0\linewidth}{!}{
\begin{tabular}{llcc}
\toprule
Stage II & Stage I  & \fid$_{5k}$ & \dinofid$_{5k}$ \\
\midrule
% 3278: "fid50k": 13.130795660857324, "dinofid50k": 249.46464787074206,
% 3279: "fid50k": 13.697963826137052, "dinofid50k": 267.7557197224488,
% 3280: "fid50k": 11.631835931219992, "dinofid50k": 203.55857327388986,
% & \fluxae + FT & 13.69 & 267.7 \\
% & \fluxae + FT-\regshortname \ours & \cellbest{11.63} & \cellbest{203.5} \\
% \midrule
% DiT-XL/2 (orig) + 3M steps & SD-VAE-ft-MSE & 12.03 & $-$ \\
\multirow{6}{*}{DiT-B/2}
& FluxAE + FT-\regshortname $\alpha = 0.01$ & 33.99 & 641.95 \\
& FluxAE + FT-\regshortname $\alpha = 0.05$ & 33.86 & 645.94 \\
& FluxAE + FT-\regshortname $\alpha = 0.1$ & \cellsecond{28.62} & \cellsecond{586.91} \\
& FluxAE + FT-\regshortname $\alpha = 0.25$ & \cellbest{26.84} & \cellbest{558.36} \\
& FluxAE + FT-\regshortname $\alpha = 0.5$ & 29.63 & 569.92 \\
& FluxAE + FT-\regshortname $\alpha = 1$ & 33.22 & 612.45 \\
\bottomrule
\end{tabular}
% }
\end{table}

