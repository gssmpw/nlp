\section{Limitations.}

We identify the following limitations of our work and the proposed regularization:
\begin{enumerate}
    \item While we did our best to verify that our framework works in the most general setup possible, testing 4 different autoencoders across 2 different domains (image and videos), our study would be more complete when verified across other diffusion parametrizations~\cite{EDM, DDPM, VDM++} or architectures~\cite{EDMv2}.
    \item We observed that our regularization still affects the reconstruction slightly: for example, \cref{table:ae-rec} shows that FluxAE \fid increased from 0.183 to 0.55 (though for some AEs, like \cvaefull, it improves). We are convinced that this \fid increase could be mitigated by training with adversarial losses, which we omitted in this work for simplicity.
    \item There is a mild sensitivity to hyperparameters: for example, we found that varying the SHF regularization weight might improve the results (see \cref{table:regweight-abl}), or adding a small KL regularization (which we disabled in the end for our regularization for simplicity).
    % \item For some setups, autoencoder training should be longer: for example, we observed that DiT-B/1 training on top of \cmsaei leads to better results after 200,000 fine-tuning steps rather than 10,000 steps as was used in the current work.
    \item None of the explored autoencoders released their training pipelines, and it is non-trivial to fine-tune them even without any extra regularization. For example, we observed that any fine-tuning of \dcae~\cite{DC-AE} was leading to divergent reconstructions in our training pipeline (we explored dozens of different hyperparameter setups). 
\end{enumerate}

We leave the exploration of these limitations for future work.
