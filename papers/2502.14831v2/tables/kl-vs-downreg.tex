\begin{table}
\caption{Ablating KL regularization weight $\beta$ for FluxAE fine-tuning in terms of the reconstruction quality and downstream DiT-S/2 and DiT-L/2~\cite{DiT} training. Increasing the KL in general improves the LDM's performance for smaller models, but at the expense of worsened AE reconstruction (and reduced training stability), which can limits scaling~\cite{SD3} and results in worse performance of DiT-L/2. Our \regshortname regularization leads to improved LDM performance without hurting the reconstruction and scales well to larger models.}
\label{table:kl-vs-downreg}
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method & DiT-S/2 \dinofidfivek & DiT-L/2 \dinofidfivek & AE \psnrsmall \\
\midrule
FluxAE (vanilla) & 992.05 & 415.87 & 30.20 \\
~+ KL $\beta = 0$ & 968.26 & 472.08 & 29.97 \\
~+ KL $\beta = 10^{-7}$ & 1018.6 & 425.35 & \cellsecond{30.29} \\
~+ KL $\beta = 10^{-6}$ & 1095.2 & 612.12 & \textcolor{crimsonred}{19.66} \\
~+ KL $\beta = 10^{-5}$ & 940.13 & \cellsecond{403.99} & 29.21 \\
~+ KL $\beta = 10^{-4}$ & 974.67 & 404.61 & 30.22 \\
~+ KL $\beta = 10^{-3}$ & 982.91 & 425.24 & 29.51 \\
~+ KL $\beta = 10^{-2}$ & 1946.5 & 1737.47 & \textcolor{crimsonred}{10.82} \\
~+ KL $\beta = 10^{-1}$ & \cellsecond{929.58} & 472.74 & \textcolor{crimsonred}{23.72} \\
\midrule
~+ FT-SE \ours & \cellbest{924.28} & \cellbest{369.15} & \cellbest{30.37} \\
\bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table}