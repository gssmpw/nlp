\begin{table*}[h]
\caption{Hyperparameters for the autoencoders explored in the current work. We had to tweak the hyperparameters for various autoencoders to prevent the divergence of the baseline training.}
\label{tab:hyperparameters}
\centering
% \resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
% Hyperparameter & FluxAE~\cite{Flux} & \cmsaei~\cite{CosmosTokenizer} & \cvae~\cite{CogVideoX} & \ltxae~\cite{LTX-video} & \cmsae~\cite{CosmosTokenizer} \\
% Hyperparameter & FluxAE & \cmsaei & \cvae & \ltxae & \cmsae~ \\
Hyperparameter & FluxAE & \cmsaei & \cvae & \ltxae \\
\midrule
% Domain & image & image & video & video & video \\
% Compression rate & $8 \times 8$ & $16 \times 16$ & $4 \times 8 \times 8$ & $8 \times 32 \times 32$ & $8 \times 16 \times 16$ \\
% Latent channels & 16 & 16 & 16 & 32 & 16 \\
% Number of fune-tuning steps & 10,000 & 10,000 & 20,000 & 20,000 & 10,000 \\
% Image batch size & 32 & 32 & 64 & 64 & 64 \\
% Video batch size & 0 & 0 & 32 & 32 & 32 \\
% % Fine-tuning domains & images & images & images + videos & images + videos & images + videos \\
% Default KL $\beta$ weight & 0.001 & 0.0 & 0.001 &  & 0.0 \\
% Learning rate & 0.0 & 0.0 & 0.0 &  & 0.0 \\
% Number of parameters & 83.8M & XXX & XXX & XXX & XXX \\
% Training resolution & $256 \times 256$ & $256 \times 256$ & $17 \times 256 \times 256$ & $17 \times 256 \times 256$ & $17 \times 256 \times 256$ \\
Domain & image & image & video & video \\
Compression rate & $8 \times 8$ & $16 \times 16$ & $4 \times 8 \times 8$ & $8 \times 32 \times 32$ \\
Latent channels & 16 & 16 & 16 & 32 \\
Number of fune-tuning steps & 10,000 & 10,000 & 20,000 & 20,000 \\
Image batch size & 32 & 32 & 64 & 64 \\
Video batch size & 0 & 0 & 32 & 32 \\
Default KL $\beta$ weight & 0.001 & 0.0 & 0.001 & 0.0001 \\
Learning rate & 0.00001 & 0.0001 & 0.0003 & 0.00005 \\
Number of parameters & 83.8M & 44M & 211.5M & 419M \\
Training resolution & $256 \times 256$ & $256 \times 256$ & $17 \times 256 \times 256$ & $17 \times 256 \times 256$ \\
MSE loss weight & 1 & 1 & 1 & 5 \\
LPIPS loss weight & 1.0 & 1.0 & 1.0 & 1.0 \\
Gradient clipping norm & 50 & 50 & 1 & 50 \\
Num upsampling blocks frozen & 1 & 3 & 0 & 0 \\
Is output convolution frozen? & Yes & Yes & Yes & Yes \\
% Frozen decoder layers & output conv + last Up block & output conv + up blocks & output conv & output conv \\
% Loss & $\mathcal{L}_2+\textnormal{LPIPS}_{\textnormal{VGG}}$& &\\
\bottomrule
\end{tabular}
% }
\end{table*}
