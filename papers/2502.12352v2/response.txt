\section{Related Work}
Our work bridges GNN explainability and mechanistic interpretability of Transformers, aiming to understand how information flows through these architectures during inference from the perspective of graph theory and network science **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**.

\textbf{GNN Explainability.} Early work on explaining GNNs focused on identifying influential subgraphs for specific predictions ____ . This spawned several approaches including concept-based methods ____, counterfactual explanations ____, and generative explanations ____ . While valuable, these methods primarily analyze input-output relationships rather than internal model dynamics. Other work has investigated physical laws learned by GNNs using symbolic regression ____ , but a systematic framework for understanding information flow in GNNs remains lacking.

\textbf{Mechanistic Interpretability of Transformers.} Recent advances in mechanistic interpretability aim to reverse-engineer neural networks into human-understandable components ____. This has led to breakthroughs in understanding model features ____, identifying computational circuits ____, and explaining emergent behaviors ____ . These insights have practical benefits - enabling better out-of-distribution generalization ____ , error correction ____ , and prediction of model behavior ____ . 
Our work aims to extend the principles of mechanistic interpretability to GNNs by leveraging their mathematical connection to Transformers. 
A complementary effort is emerging around mechanistic interpretability of biological language models ____ , sharing our goal of extracting scientific insights from AI systems trained on structured scientific data.