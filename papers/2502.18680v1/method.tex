In this study, we analyze spatial and temporal trends in hardware counter
values on GPUs, and examine the relationships between different hardware counters.

Below, we describe our methodology and metrics used in our analyses. These
metrics can be used with any hardware counter described in
Section~\ref{sec:data} and Table~\ref{table:dataset_info}.

\subsection{Analyzing the Spatial Behavior of Jobs}

Spatial behavior of individual jobs refers to how each job utilizes individual
GPUs and their resources assigned to a job.  We extend a previous metric that
quantifies spatial imbalance~\cite{li2023analyzing, peng2021holistic} to work
well with time-series data, as explained below.

\vspace{0.08in}
\noindent\textbf{Spatial Imbalance:}
This metric captures uneven values of hardware counters on GPUs within a job.
Previous studies have analyzed spatial imbalance at node level using monitoring data, but
they aggregate the imbalance over the entire runtime of the job, which may hide
some usage patterns.  Hence, we extend the definition by introducing a time-windowed
approach.  Spatial imbalance for a job $j$ within a small time window $w$ is defined
as follows:
%
\begin{equation}
    \mathit{SI}(j, w) = 1 - \frac{\sum_{g=1}^{g_j} \mathit{TC}(g,w)}{\max\limits_{1 \leq g \leq g_j} \mathit{TC}(g,w) \times g_j}
\end{equation}
%
The numerator sums up the per-GPU hardware counter values within
time window $w$ across all GPUs allocated to the job. The per-GPU values can be calculated as,
%
\[
    \mathit{TC}(g,w) = \sum_{t=1}^{t_w} C_{g,t}
\]
%
This is the sum of hardware counter values $C_{g,t}$ over all timestamps in $w$ for
GPU $g$.  The denominator in Equation 1 represents the maximum possible
counter value if all GPUs matched the highest observed value in that
window.  Spatial imbalance values close to 0 indicate low imbalance,
while values near 1 indicate significant imbalance.  The spatial imbalance
metric for a job over its entire runtime is defined as the mean spatial
imbalance across all time windows, where $w_j$ is the total number of time
windows:
%
\begin{equation}
    \mathit{SI}(j) = \frac{\sum_{w=1}^{w_j} \mathit{SI}(j, w)}{w_j}
\end{equation}

\begin{table*}[t]
    \centering
    \caption{Hardware counters and their explanations.}
    \label{table:dataset_info}
    \begin{tabular}{lcp{9cm}} \toprule
        \textbf{Counter Name}                & \textbf{Short Name} & \textbf{Description}                                                                  \\  \midrule
        DCGM\_FI\_DEV\_GPU\_UTIL             & GPU\_UTIL           & The fraction of time during which at least one kernel were executing on the GPU.      \\
        DCGM\_FI\_DEV\_MEM\_COPY\_UTIL       & MEM\_UTIL           & The fraction of time during which device memory was read or written.                  \\
        DCGM\_FI\_PROF\_PIPE\_FP16\_ACTIVE   & FP16\_ACTV          & The fraction of cycles the FP16 (half-precision) cores were active.                   \\
        DCGM\_FI\_PROF\_PIPE\_FP32\_ACTIVE   & FP32\_ACTV          & The fraction of cycles the FP32 (single-precision and integer) cores were active.     \\
        DCGM\_FI\_PROF\_PIPE\_FP64\_ACTIVE   & FP64\_ACTV          & The fraction of cycles the FP64 (double-precision) cores were active.                 \\
        DCGM\_FI\_PROF\_PIPE\_TENSOR\_ACTIVE & TNSR\_ACTV          & The fraction of cycles the tensor cores were active.                                  \\
        DCGM\_FI\_PROF\_SM\_ACTIVE           & SM\_ACTV            & The fraction of time at least one warp was active, averaged over all multiprocessors. \\
        DCGM\_FI\_DEV\_FB\_USED              & HBM\_USED           & Absolute amount high-bandwidth memory (HBM) capacity used (MB).                       \\ \bottomrule
        % Job Submissions     & Slurm data contains job ID, job name,
        % start time, end time, node ID, account, and submit line.                                                                        \\ \bottomrule
    \end{tabular}
\end{table*}

\subsection{Analyzing the Temporal Behavior of Jobs}

Temporal analysis examines the frequency and magnitude of fluctuations in
hardware counter values over time.  We use two key metrics: temporal imbalance
and burstiness.

\vspace{0.08in}
\noindent{\textbf{Temporal Imbalance:}}
Temporal imbalance quantifies the variation in hardware counter values over a job's
runtime. A high temporal imbalance indicates fluctuating usage and
a low imbalance suggests consistent behavior. We adopt the definition
from~\cite{peng2021holistic}:
%
\begin{equation}
    \mathit{TI}(j, g) = 1 - \frac{\sum_{t=1}^{t_j} C_{g,t}}{t_j \times \max_{1 \le t \le t_j} C_{g,t}}
\end{equation}
%
where $ C_{g,t} $ is the hardware counter value for GPU $g$ at time $t$. The numerator
sums the counter values over time. The denominator represents the maximum
possible value and is calculated by
multiplying the job duration by the peak observed value of the counter for that GPU.
Subtracting from 1 highlights the imbalance, where values near zero
indicate stable behavior and higher values reflect greater
fluctuations.

Equation 3 defines temporal imbalance metric for a single GPU.
The temporal imbalance of a job is defined as the maximum imbalance across
all allocated GPUs, where $g_j$ is the number of GPUs assigned to the job:
%
\[
    \mathit{TI}(j) = \max_{1 \le g \le g_j}TI(j, g)
\]

\vspace{0.08in}
\noindent{\textbf{Burstiness:}}
We use burstiness metric from ~\cite{goh2008burstiness} to quantify temporal
variability in GPU-enabled jobs. Burstiness captures
irregular fluctuations in hardware counter values by analyzing interevent times.
An event occurs when a counter value increases by more than 15\% within a 10-second
interval. For example, if GPU\_UTIL value is 50\% at a given timestamp, an event is
recorded if the next measurement exceeds 65\%. Interevent times are defined
as the time intervals between consecutive events. Burstiness of
GPU $g$ in job $j$ is defined as:
%
\begin{equation}
    \mathit{B}(j, g) = \frac{\sigma_\tau-\mu_\tau}{\sigma_\tau+\mu_\tau}
\end{equation}
%
where $\sigma_\tau$ and $\mu_\tau$ are the standard deviation and
mean of the interevent times, respectively. A value of $B=-1$ indicates perfectly
regular behavior ($\sigma_\tau=0$). $\mathit{B}=0$ represents
balanced fluctuations ($\sigma_\tau=\mu_\tau$).  A value of $B=1$ indicates
highly irregular changes in counter values ($\sigma_\tau\gg\mu_\tau$).

Equation 4 defines burstiness metric for a single GPU.
The overall burstiness of a job is the mean burstiness across all allocated GPUs,
where $g_j$ is the number of GPUs assigned to job $j$:
%
\[
    \mathit{B}(j) = \frac{\sum_{g=1}^{g_j}\mathit{B}(j, g)}{{g_j}}
\]

\subsection{Analyzing Relationships between Hardware Counters}

We investigate the relationships between temporal imbalance, spatial imbalance,
and job-level mean values of different counters to better understand
resource usage patterns. The job-level mean, $\mathit{M}(j)$, is
computed by first averaging
the counter values over time, $t_j$, for each GPU in a job,
and then taking the mean across all GPUs, $g_j$, assigned to that job:
%
\begin{equation}
    \mathit{M}(j) = \frac{1}{g_j} \sum_{g=1}^{g_j} \left( \frac{1}{t_j} \sum_{t=1}^{t_j} C_{g,t} \right)
\end{equation}
%
where $ C_{g,t} $ is the hardware counter value for GPU $g$ at time $t$. To quantify
the correlations between these derived metrics, we use Spearman
correlation~\cite{spearman}, which captures monotonic trends without assuming
linearity or normality.
