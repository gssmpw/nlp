% \begin{itemize}
%       \item IO behavior.
%             %
%       \item Storage systems.
%             %
%       \item Temporal and spatial imbalance.
%             %
%       \item System logs. Failures
% \end{itemize}

Many monitoring tools have been developed to monitor performance of HPC
systems such as LDMS~\cite{ldms}, ParMon~\cite{parmon}, and
SuperMon~\cite{supermon}.  These tools typically continuously
monitor the compute nodes and collect performance data about the jobs running
on them.  They can provide information about CPU, GPU, memory, IO usage and
power, energy consumption. The collected data they provide is typically
time-series-based.
Perlmutter utilizes the Lighweight Distributed Metric Service (LDMS) for system
monitoring and can provide GPU-level time-series data for every job
running on the compute nodes.

Several studies have leveraged monitoring data for different purposes. Some
studies use monitoring data to predict runtime of
jobs~\cite{7776517, aaziz2018modeling} and resource
usage~\cite{emeras2017evalix, sirbu2016power}.  Others apply monitoring
data for  application fingerprinting~\cite{li2023arcode, ates2018taxonomist}.
These studies show that they can identify and cluster jobs running the same
application by analyzing the monitoring data. Additionally, many studies
utilize monitoring data to identify performance anomalies. Many researchers
apply various ML techniques for automatic anomaly
detection~\cite{proctor, ruad, ozer2020characterizing}.  These studies
highlight various ways monitoring data can provide insights into
HPC system behavior ~\cite{netti2021conceptual, netti2022operational}

This paper focuses on resource usage analysis and workload characterization.
Many studies have characterized the performance and resource utilization of HPC
and cloud systems using monitoring data. Most studies have primarily
focused on the CPU and memory utilization to characterize
workloads~\cite{amvrosiadis2018diversity, cortez2017resource,
    shen2015statistical, chen2010analysis, mishra2010towards,
    reiss2012heterogeneity, ren2012workload, di2012characterization}. For example,
Peng et al. analyzed memory utilization on two HPC systems. They investigated
spatial and temporal imbalances of jobs~\cite{peng2021holistic}.  Similarly, Li
et al. analyzed temporal and spatial characteristics of jobs
by using CPU utilization, GPU utilization, and memory usage counters
in Perlmutter~\cite{li2023analyzing}.

Additionally,
several studies have focused on I/O, and storage
utilization~\cite{shen2015statistical, patel2019revisiting, ren2012workload}.
For example, Patel et al. analyzed I/O behaviors in large-scale storage systems
~\cite{patel2019revisiting}.  Several studies explored power and
energy consumption~\cite{shen2015statistical, amvrosiadis2018diversity,
    li2023analyzing, ilager2023data, chu2024generic}.  Cu et al. compared the
energy profiles of Machine Learning (ML) and traditional workloads and found
that the ML jobs have higher power usage and failure
rates~\cite{chu2024generic}.  Ilager et al. ~\cite{ilager2023data} analyzed
energy and temperature in cloud data centers.  Hu et al. characterized DL
workloads on multiple data centers. They found that single-GPU
jobs have weaker impact on cluster usage compared to multi-GPU
jobs~\cite{hu2021characterization}.

While previous work has highlighted inefficiencies such as underutilized CPU or
GPU resources and memory imbalances, they often lack detailed GPU-specific
analyses because they use a limited variety of GPU-related performance
counters. Additionally, most prior work use node-level data, whereas we analyze
GPU-level data in this study.  Our study provides a more fine-grained analysis
of GPU hardware counters with improved metrics to analyze spatial and temporal
characteristics of jobs. Moreover, we compare characteristics of ML and
non-ML workloads.


% \begin{table*}[h]
%       \centering
%       \caption{GPU Application Job Characteristics by Year. \#DC=The number of data centers, \#G=Overall GPU utilization, \#Pre=Precisions, \#Mem=Memory utilization, \#IO=File system, \#EP=Energy and Power usage, \#C=Overall CPU utilization, \#J=Job semantics}
%       \label{tab:related_work}
%       \begin{tabular}{ccp{5cm}ccccccccccc} \toprule
%             \multirow{2}{*}{\textbf{Year}} & \multirow{2}{*}{\textbf{Work}}   & \multirow{2}{*}{\textbf{Characteristics}}                                                                                      & \multicolumn{4}{c}{\textbf{Data Scope}} & \multicolumn{7}{c}{\textbf{Utilization Type}}                                                                                                                      \\ \cline{4-14}
%                                            &                                  &                                                                                                                                & \#DC                                    & Job                                           & Node       & GPU        & \#G        & \#Pre      & \#Mem      & \#IO       & \#EP       & \#C        & \#J        \\ \hline
%             2010                           & ~\cite{chen2010analysis}         & Provides system design insights through statistical analysis, clustering, and correlation of job behaviors.                    & 1                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2010                           & ~\cite{mishra2010towards}        & Workload classification methodology using clustering to group tasks based on resource usage.                                   & 1                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2012                           & ~\cite{reiss2012heterogeneity}   & Analyzes cluster usage data to understand the challenges of cloud resource scheduling.                                         & 1                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2012                           & ~\cite{ren2012workload}          & Analyzes a two-week trace to understand MapReduce workload characteristics.                                                    & 1                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \checkmark & \texttimes & \checkmark & \checkmark \\
%             2012                           & ~\cite{di2012characterization}   & Characterizes the workload in a Google Cloud data center and compares it to Grid and HPC systems.                              & 8                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2015                           & ~\cite{shen2015statistical}      & Characterizes the resource demands and usage patterns of workloads in a cloud datacenter.                                      & 1                                       & \checkmark                                    & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark & \checkmark & \texttimes & \checkmark & \checkmark \\
%             2017                           & ~\cite{cortez2017resource}       & Predicts VM behaviors using machine learning and demonstrates how these predictions can improve resource management.           & 1                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2018                           & ~\cite{amvrosiadis2018diversity} & Analyzes job behavior, compares workloads, evaluates system performance, and improves job runtime predictions.                 & 4                                       & \checkmark                                    & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \checkmark & \checkmark & \checkmark \\
%             2019                           & ~\cite{patel2019revisiting}      & Analyzes a year of I/O activity data to understand patterns in I/O behavior.                                                   & 2                                       & \texttimes                                    & \checkmark & \texttimes & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \checkmark & \texttimes \\
%             % 2020                           & ~\cite{shahrad2020serverless}    & c                                                                                                                    & 1                                       & \checkmark                                    & \checkmark & \texttimes &                                                                                          \\
%             2021                           & ~\cite{peng2021holistic}         & Analyzes memory utilization on a CPU-based and a GPU-accelerated supercomputer                                                 & 2                                       & \checkmark                                    & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark \\
%             2022                           & ~\cite{michelogiannakis2022case} & Analyzes the effectiveness of intra-rack resource disaggregation by studying utilization metrics and workload characteristics. & 1                                       & \checkmark                                    & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             2022                           & ~\cite{li2022ai}                 & Characterizes AI-enabling workloads on a GPU-accelerated system.                                                               & 1                                       & \checkmark                                    & \checkmark & \texttimes & \checkmark & \texttimes & \checkmark & \texttimes & \checkmark & \checkmark & \checkmark \\
%             2023                           & ~\cite{ilager2023data}           & Analyzes the workload, energy, and thermal characteristics of a large-scale cloud data center.                                 & 1                                       & \texttimes                                    & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark & \texttimes & \checkmark & \checkmark & \texttimes \\
%             2023                           & ~\cite{li2023analyzing}          & Analyzes resource utilization in the Perlmutter HPC system.                                                                    & 1                                       & \checkmark                                    & \checkmark & \texttimes & \checkmark & \texttimes & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\
%             % 2019                           & ~\cite{wang2019learning}         & c                                                                                                                              & 1                                       & \checkmark                                    & \checkmark & \texttimes &                                                                                          \\
%             2018                           & ~\cite{simakov2018workload}      & Analyzes the workload and resource utilization of supercomputers in the NSF Innovative HPC program.                            & 1                                       & \checkmark                                    & \checkmark & \texttimes & \texttimes & \texttimes & \checkmark & \checkmark & \texttimes & \checkmark & \checkmark \\
%             2024                           & ~\cite{chu2024generic}           & Compares the impact of Machine Learning and generic jobs.                                                                      & 1                                       & \checkmark                                    & \checkmark & \checkmark & \checkmark & \texttimes & \checkmark & \texttimes & \checkmark & \checkmark & \checkmark \\ \bottomrule
%             \textbf{2024}                  & \textbf{Our Work}                & \textbf{Analyzes GPU utilization on Perlmutter by examining a variety of previously unavailable GPU hardware counters.}        & \textbf{1}                              & \checkmark                                    & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \texttimes & \texttimes & \checkmark & \checkmark \\


%             % Example Data Rows
%             % Add more rows as needed
%       \end{tabular}
% \end{table*}

% 2024 & Analysis & High Precision & 1000 & \checkmark & \checkmark & \texttimes & 150 & 50 & 25 & 35 & 20 & 10 \\ 
