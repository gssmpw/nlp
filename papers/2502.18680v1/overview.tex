In this section, we present an overview of our monitoring data and the
system-level characteristics of Perlmutter.  The analysis includes various
aspects such as the number of GPUs used per job, GPU hours consumed,
utilization patterns, and the usage of different GPU cores. Additionally,
we compare mean of GPU\_UTIL for ML and non-ML jobs.
This section addresses the following questions:

\begin{RQcallout}
    {\bf RQ1 }{\it \RQone{}}
\end{RQcallout}

\begin{RQcallout}
    {\bf RQ2 }{\it \RQtwo{}}
\end{RQcallout}

\subsection{Overall Resource Usage}

Figure~\ref{fig:num_gpus_overview} illustrates the distribution of jobs based
on the job size (number of GPUs used) with a focus on job counts, durations,
and mean of GPU\_UTIL. The Cumulative Distribution Function (CDF), shown by the
orange line, represents the probability that a random variable $X$ takes a value
less than or equal to $x$. Red dots in each bar indicate the corresponding mean
value.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/overview/precision_upsetplot.pdf}
    \caption{The plot demonstrates the number and percentage of jobs using
        different combinations of GPU cores. FP64 is the most commonly used core,
        FP16 is rarely used, and tensor cores are often utilized alongside FP64 and
        FP32.}
    \label{fig:precision_upsetplot}
\end{figure*}

The left plot in Figure~\ref{fig:num_gpus_overview} shows the job distribution by the
number of GPUs used. Most
jobs run on a single node (first two bars) and the number of jobs decreases as the
GPU count exceeds four nodes (16 GPUs). CDF line indicates 70.5\% of jobs use
a single node. We also identify that most of these jobs allocate all four
GPUs on the node.

The middle plot in Figure~\ref{fig:num_gpus_overview} presents job durations in
hours on a logarithmic scale. Single-GPU jobs (first box) tend to be shorter on
average (\tweakedsim 11 minutes), while jobs using more GPUs generally show a wider
range of durations and run longer. In general, duration vary significantly, but the
mean duration is typically between 1-3 hours.

The right plot in Figure~\ref{fig:num_gpus_overview} shows mean of GPU\_UTIL across job
sizes. Single-GPU jobs (first box) exhibit the widest range of mean of GPU\_UTIL.
Single-GPU jobs with high mean of GPU\_UTIL are
typically short jobs (under 30 minutes). Jobs using more than 64 GPUs generally
have lower mean of GPU\_UTIL (between 20-40\%). Outliers indicate
that some jobs achieve very high GPU utilization.

\begin{figure}[h]
    \centering
    % \includegraphics[width=0.65\columnwidth]{figs/overview/gputil_duration.pdf}
    \includegraphics[width=0.65\columnwidth]{figs/overview/dist_gputil_numjobs.pdf}
    \includegraphics[width=0.65\columnwidth]{figs/overview/gputil_gpuhours.pdf}
    \caption{The plots show the number of jobs (top) and  GPU hours in log-scale
        (bottom) by mean of GPU\_UTIL with CDF. The red dots on the boxplots
        indicate the mean values. Mean of GPU\_UTIL varies greatly. 43\% of
        jobs fall within the low utilization range (0-30\%).}
    \label{fig:gputil_overview}
\end{figure}

Figure~\ref{fig:gputil_overview} presents the distribution of jobs and GPU
hours based on mean of GPU\_UTIL. The analysis highlights
how job counts and GPU hours vary across GPU\_UTIL ranges.

The top plot in Figure~\ref{fig:gputil_overview} shows the number of jobs
for each mean of GPU\_UTIL range along with their cumulative percentages.
53.8\% of jobs fall within the lower mean of GPU\_UTIL range (0-30\%). As mean
GPU\_UTIL increases, the number of jobs decreases, but higher utilization ranges
jobs still contribute to the cumulative curve.

The bottom plot Figure~\ref{fig:gputil_overview} displays GPU hours on a log
scale for each mean of
GPU\_UTIL range. The CDF line indicates that jobs in the lower mean of GPU\_UTIL
ranges (e.g., 0-19\%) contribute 22.7\% to the total GPU hours, less than their
contribution 30.5\% share of total jobs. Red dots show that
mean GPU hours remain relatively stable across mean of GPU\_UTIL ranges, while
outliers highlight jobs with exceptionally high
GPU hours in all ranges.

\subsection{Categorizing Jobs by Type}

Figure~\ref{fig:precision_upsetplot} illustrates the distribution of jobs
using different combinations of GPU cores,
including FP64\_ACTV, FP32\_ACTV, FP16\_ACTV, and TNSR\_ACTV.
The top bar chart shows the total number and percentage of jobs for each
precision combination. The bottom section visualizes unique combinations,
where filled circles and connecting lines indicate usage of specific GPU
cores. Each bar represents a unique set of jobs with no overlap.

\begin{figure*}[t]
    % \vspace{-0.1in}
    \centering
    Histogram and CDF of spatial imbalance of GPU\_UTIL \par\medskip
    \vspace{-0.05in}
    \includegraphics[width=0.65\columnwidth]{figs/imbalance/spatial/gputil_spatial_imbalance_30.pdf}
    \includegraphics[width=0.65\columnwidth]{figs/imbalance/spatial/gputil_spatial_imbalance_30_70.pdf}
    \includegraphics[width=0.65\columnwidth]{figs/imbalance/spatial/gputil_spatial_imbalance_70.pdf}
    \caption{The plots show the distribution of spatial imbalance of GPU\_UTIL
        for jobs grouped by mean of GPU\_UTIL ranges
        (0-30\%, 31-69\%, 70-100\%, left to right). Low-utilization jobs exhibit the
        highest spatial imbalance. 97.6\% of high-utilization have below 0.5 imbalance.}
    \label{fig:gputil_spatial}
\end{figure*}

FP64 is the most commonly used GPU core (61.5\% of jobs), followed
by FP32 (37.2\%). 36.6\% of jobs use only FP64 core, while 20.5\% use only FP32
cores. FP16 cores are rarely used (0.2\% of jobs). Tensor cores are being
utilized alongside FP64 (16.4\%) and FP32 (8.2\%). Even when not
explicitly used, tensor cores may be leveraged internally by CUDA libraries
such as cuBLAS. Later,  we compare job imbalance patterns for different GPU core
combinations.

\begin{figure}[h]
    % \vspace{-0.1in}
    \centering
    \includegraphics[width=0.65\columnwidth]{figs/overview/dl_vs_nondl_jobcount.pdf}
    \includegraphics[width=0.65\columnwidth]{figs/overview/dl_vs_nondl_gpuhours.pdf}
    \caption{The plots compare the number (top) and GPU hours (bottom) of ML and
        non-ML jobs by mean of GPU\_UTIL. ML jobs dominate high (70-100\%)
        and the low (0, 9\%) utilization bins. They account for more
        GPU hours. Absolute values are annotated above bars.}
    \label{fig:dl_related}
\end{figure}

Figure~\ref{fig:dl_related} illustrates the distribution of
normalized job counts and GPU hours for ML and non-ML jobs across
mean of GPU\_UTIL ranges. We normalize job counts and GPU hours separately to
compare the relative distributions of ML and non-ML jobs.
For job counts, we group jobs by mean of GPU\_UTIL and compute each group's
fraction relative to the total jobs in its category (ML or non-ML).
For GPU hours, we sum the total GPU hours in each group and normalize by the
total GPU hours in that category. The baselines are total jobs and total GPU
hours per category. Absolute values are annotated above bars.

The top plot shows normalized job counts by mean GPU\_UTIL ranges.
Non-ML jobs dominate the lower- and mid-utilization ranges (except [0-9\%]), but
decline significantly at higher utilizations.  ML jobs are most common in
high-utilization ranges (70-100\%) and the lowest utilization bin.

The bottom plot in Figure~\ref{fig:dl_related} presents the normalized GPU hours.
ML jobs dominate the 70-100\% range, while non-ML jobs contribute most in 0-39\%.
The scarcity of ML jobs in 30-70\% suggests they are either lightweight tasks
(e.g., tuning, small tests) or intensive training jobs. The mid-utilization
range is likely dominated by traditional HPC jobs that do not exhibit extreme
utilization patterns.

