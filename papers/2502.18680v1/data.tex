The Perlmutter supercomputer at the National Energy Research Scientific
Computing Center (NERSC) provides a heterogeneous architecture with both CPUs
and GPUs. The system contains 3072 CPU-only nodes and 1792 GPU-accelerated
nodes. Each GPU-accelerated node is equipped with four NVIDIA A100.  The GPU
nodes have 256 GB of DDR4 DRAM and 40 GB of HBM per GPU. Perlmutter is divided
into CPU and GPU partitions, and this study focuses solely on jobs submitted to
the GPU partition.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.65\columnwidth]{figs/overview/number_of_jobs_gpu_size.pdf}
  \includegraphics[width=0.65\columnwidth]{figs/overview/duration_gpu_size.pdf}
  \includegraphics[width=0.65\columnwidth]{figs/overview/gputil_gpu_size.pdf}
  \caption{The plots show the number of jobs with CDF, duration in hours (log-scale), and
    mean of GPU\_UTIL by the number of GPUs (left to right).
    Red dots on boxplots indicate mean values. Most jobs use a single node,
    durations vary widely but average 1-3 hours for most job sizes, and larger
    jobs tend to have lower mean GPU utilization.}
  \label{fig:num_gpus_overview}
\end{figure*}

\subsection{Data Sources}

Perlmutter utilizes LDMS~\cite{ldms} to collect system-wide monitoring
data from the compute nodes. LDMS consists of various plugins for different
components and continuously collects hardware counter measurements on
CPU, GPU, memory usage, I/O, etc. In this study, we retrieved performance
counter measurements collected by using the Data Center GPU Manager
(DCGM)~\cite{dcgm} plugin. Additionally, we retrieve Slurm~\cite{slurm_documentation}
job scheduling data, which includes job-specific metadata.

We retrieved \tweakedsim 4 months of data, spanning August 16 to December 13,
2023. The DCGM plugin was configured with a 10-second sample rate and
collected counters at this interval. Our dataset includes
information about 345,154 jobs.

Table~\ref{table:dataset_info} describes the hardware counters used in this
study. SM\_ACTV accounts for both active and stalled warps, including those
waiting on memory requests. Higher values for FP16\_ACTV, FP32\_ACTV, FP64\_ACTV,
and TNSR\_ACTV indicate higher utilization of the corresponding GPU cores.
MEM\_UTIL represents the utilization of the GPU's copy engine. GPU\_UTIL indicates
that one or more kernel functions are actively using GPU resources.

\subsection{Data Retrieval and Integration}

We retrieved LDMS data from Perlmutter using the Prometheus API and
Slurm job data with the sacct command and stored both in Parquet files.
LDMS data provides hardware counter values per GPU, including timestamps,
node IDs, GPU IDs, and counter values, but lacks job IDs. In contrast,
Slurm data contains job-level metadata without hardware counters.

To enable per-job GPU-level analysis, we merged the datasets by aligning
LDMS samples with Slurm job data. We matched LDMS timestamps to jobs running
on the same node using their start and end times from Slurm data. For each job,
we assigned LDMS samples within its time range to the corresponding job ID
and step. This process generated a dataset where each LDMS sample was linked
to a specific job and step. This integration enables us to associate
GPU-level hardware counter values with specific jobs.

\subsection{Data Cleaning and Preprocessing}

We applied several cleaning and filtering steps to ensure reliable and
accurate analyses of the merged LDMS and Slurm datasets. First, we removed the
LDMS entries not associated with any jobs during the merging process.  We excluded
the jobs running on the login nodes since they do not represent meaningful
workloads. We also removed jobs that did not use the GPU partition.

Additionally, we excluded the jobs submitted by staff accounts since these jobs
often involve maintenance, testing, or debugging rather than production
workloads. We also filtered out failed and canceled
jobs and kept only the completed jobs. We removed the jobs with durations
shorter than three minutes, as they typically lack sufficient data for
meaningful analysis.

We also applied some counter-specific filters to address data inconsistencies.
For instance, we removed jobs where counter values exceeded their
physical limits (e.g., GPU\_UTIL \textgreater 100\%) and excluded jobs with
a job-level mean counter value below 1\%.

Finally, we identified and labeled AI/ML jobs by analyzing the submit line
information in the Slurm job scripts. We classified jobs as AI/ML-related if
their submit line contained any of 18 predefined keywords \footnote{Keywords
  include `epoch`, `training`, `neural`, `cnn`, `rnn`, `lstm`, `transformer`,
  `bert`, `tensorflow`, `pytorch`, `keras`, `deep`, `inference`, `autoencoder`,
  `classification`, `detection`, `activation`, `sklearn`}.
For the remainder of this paper, we refer to these jobs as ML jobs.
