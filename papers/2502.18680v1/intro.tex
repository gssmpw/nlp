% Why is understanding resource utilization important? 
% How do we do it? 
General-purpose Graphics Processing Units (GPGPUs) have become pervasive in
compute nodes on high performance computing (HPC) systems. Gaining insights
into the resource utilization of HPC systems, and GPUs in particular, is
necessary for identifying inefficiencies, which in turn can help optimize
codes, improve overall system operation, and design more effective future
architectures. This is made possible by gathering and analyzing system-wide
monitoring data collected over extended periods across all the compute nodes on
a system. Such data provides a variety of information about different aspects
of the system such as IO, network, CPU, and GPU, as well as job metadata (e.g.,
the start and end time of each job, number of nodes/GPUs used, etc.)

% Why is it difficult?
% Despite advances in system monitoring, the process of analyzing resource
% utilization remains a challenge. Many HPC systems do not collect fine-grained
% GPU-level monitoring data. Even when available, certain counters may be
% inaccessible to the users. For example, more nuanced counters, including memory
% copy operations, SM activity, and the types of floating-point precisions used,
% have historically been unavailable, difficult to collect, or primarily used for
% hardware health monitoring rather than workload analysis. Additionally,
Libraries such as  Lightweight Distributed Metric Service (LDMS)~\cite{ldms} enable
the collection of a large amount of longitudinal system monitoring data.
However, the sheer volume of data that can be collected on large supercomputers
makes analyzing such data to find meaningful insights a formidable task.  For
example, the monitoring data gathered on Perlmutter that we analyze in this
paper is collected at 10-second intervals. Analyzing three months of this data
requires handling billions of data samples.  This requires the use of scalable
data aggregation methods, statistical techniques, and extensive manual analysis
to offer meaningful insights.

% Mention previous work. Why is missing?
% Prior studies have analyzed system-wide monitoring data to characterize HPC
% workload behavior and resource usage patterns focusing on memory, IO, and
% power~\cite{chen2010analysis, patel2019revisiting,
% simakov2018workload,ilager2023data, cortez2017resource}.  Some works involve
% analysis of GPU utilization~\cite{chu2024generic, li2023analyzing, li2022ai}.
% For example, Li et al. reported that more than 15\% of GPU hours were spent on
% idle GPUs~\cite{li2023analyzing} on Perlmutter. However, existing studies often
% examine a limited number of GPU-related counters and lack detailed temporal or
% spatial analysis. Furthermore, there has been little exploration of how DL/ML
% workloads differ from non- DL/ML workloads in terms of GPU utilization behavior
% or how different precision types (FP16, FP32, FP64, and tensor activity) are
% used across workloads. This leaves a significant gap in understanding the
% utilization behavior of workloads from different angles.

% What do we do in this work?
While some previous works~\cite{chu2024generic, li2023analyzing, li2022ai}
analyze GPU monitoring data, the analysis is limited to a few hardware
counters.  In this study, we do a comprehensive analysis of GPU-specific
hardware counters collected via LDMS on Perlmutter, a flagship open-science
supercomputer.
% collects data from the Data Center GPU Manager (DCGM) across all compute
% nodes. While DCGM has traditionally been used to monitor system health, we
% use it to explore the value of various hardware counters to understand
% resource utilization.
The gathered data includes counters such as memory copy operations, GPU units
used, streaming multiprocessor (SM) activity, and memory (framebuffer) usage
collected every 10 seconds over a period of \tweakedsim 4 months.

In this paper, we present a detailed analysis of the spatial and temporal
behavior of GPU workloads. Work distribution within a job across allocated GPUs
can result in spatial imbalances, where some GPUs are underutilized while
others are heavily loaded. Our spatial analysis focuses on quantifying these
imbalances to assess how effectively resources are allocated across the system.
GPU utilization often fluctuates over time, and capturing these temporal
variations requires specialized metrics.  Our temporal analysis includes two
metrics: burstiness and temporal imbalance.  Burstiness quantifies the
irregularity and distribution of large changes in individual hardware counters
over time.  We also analyze temporal imbalance to understand the deviation of
GPU utilization from its mean over time. It provides insight into how
consistently different GPUs are utilized during the lifetime of a job.

Additionally, we combine GPU-specific hardware counters with job metadata to
analyze this data separately for different application types. We compare machine
learning (ML) related and other (non-ML) jobs
to understand their differences in GPU usage patterns and examine how different
GPU cores (FP16, FP32, FP64 and tensor cores)  are utilized across workloads.
Finally, we explore correlations between different counters and utilization
metrics to identify key factors influencing GPU efficiency.

% By examining these GPU-specific hardware counters in tandem with job metadata,
% we reveal some inefficiencies—such as jobs that allocate multiple GPUs but only
% actively use a subset— and under explored usage patterns—for instance, the
% frequency of bursty vs. steady resource usage. By addressing these aspects,
% this study provides insights into GPU utilization patterns and opportunities
% for optimizing resource management in HPC systems.

% \red{Summarize some important findings in another paragraph}

Specifically, this work makes the following contributions:
%
\begin{itemize}
  \item We conduct a system-wide analysis of more than 300,000 jobs running on
        the GPU partition of Perlmutter over a period of \tweakedsim 4 months, and
        analyze previously unexplored hardware counters such as utilization of
        different GPU cores.
        %
  \item We adapt and extend a spatial imbalance metric for time-series data,
        using which reveals uneven usage of GPUs in multi-GPU allocations. Our analyses
        identify a non-trivial fraction of jobs that allocate multiple GPUs but
        effectively utilize only a subset of them.
        %
  \item We use existing metrics for temporal imbalance and burstiness, and our
        findings highlight substantial differences in GPU utilization over time across
        various job types.
        %
  \item We compare ML and non-ML workloads and analyze differences in GPU
        utilization, use of different types of GPU cores, spatial imbalance and
        temporal imbalance to identify workload-specific inefficiencies.
\end{itemize}
