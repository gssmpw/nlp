Our results offer a more detailed picture of how GPUs are being utilized on
Perlmutter and reveal several noteworthy patterns regarding job sizes, temporal
behavior, spatial imbalance, and relationships among GPU-specific counters.
Below, we discuss some key takeaways.

\vspace{0.08in}
\noindent\textbf{Differences in single- vs.~multi-GPU usage.}
Figure~\ref{fig:num_gpus_overview} shows that most jobs allocate a single node
(often all four GPUs on that node).
%, while jobs requesting larger numbers of GPUs become increasingly rare. 
When multiple GPUs are used, the range of job
durations widens compared to single-GPU jobs.
The mean GPU\_UTIL tends to gradually decrease for jobs that use more than
64 GPUs, which is consistent with decreased efficiency due to strong scaling.

\vspace{0.08in}
\noindent\textbf{Broad range of GPU\_UTIL.}
%A significant fraction of jobs exhibits low ( $<$ 30\%) mean GPU utilization (Figure
%\ref{fig:gputil_overview}). While these jobs make up more than half of the
%total job count, their cumulative share in total GPU hours is slightly smaller.
The GPU\_UTIL counter varies greatly across jobs.
While the mean of GPU\_UTIL exceeds 70\% for 10\% of jobs,
43\% of jobs run the GPUs with a mean of GPU\_UTIL less than 30\%
(See Figure~\ref{fig:gputil_overview}). The jobs with lower utilization may include
short, test-oriented jobs that only intermittently
invoke GPU kernels or lightweight applications where the GPU is allocated but
the computational demand remains modest.
Jobs that use a number of GPU hours but maintain low mean of GPU\_UTIL
are likely targets for optimization
and are identified using the methods we presented.

%However, as shown in the Figure~\ref{fig:spatial_extra}, 
%the jobs mostly do not utilize all GPUs evenly.

\vspace{0.08in}
\noindent\textbf{Spatial imbalance in multi-GPU jobs.} Spatial imbalance is
prevalent among many multi-GPU jobs with low mean of GPU\_UTIL.
Figure~\ref{fig:gputil_spatial} demonstrates a skew toward high imbalance for
those jobs. This is further underscored by the high CV shown in
Figure~\ref{fig:spatial_extra}, where more than half of the jobs with spatial
imbalance of GPU\_UTIL above 0.5 had a CV greater than 100\%. These findings
suggest uneven distribution of workload across GPUs in jobs and a potential for
more careful job design-- requesting fewer GPUs or better load balancing within
applications.

\vspace{0.08in}
\noindent\textbf{Temporal variations: imbalance and burstiness.} Jobs vary considerably
in their temporal usage patterns (Figures~\ref{fig:gputil_temporal}
and burstiness~\ref{fig:burstiness_gputil}).  Jobs with low mean of GPU\_UTIL
tend to be either consistently idle/underutilized (very low imbalance)
or exhibit sporadic bursts (high imbalance). High-utilization jobs generally
have lower temporal imbalance but higher burstiness which indicates mostly
consistent GPU usage with some irregular bursts. Nevertheless,
around 39\% of high-utilization jobs exhibit high burstiness.

\vspace{0.08in}
\noindent\textbf{Impact of GPU cores used and job types.} The usage of different
GPU cores (Figure~\ref{fig:precision_upsetplot}) shows that FP64 is
common, likely driven by scientific simulations, while ML jobs often
rely on FP32 or mix in tensor operations.  Over 60\% of jobs use FP64
exclusively. Although smaller in frequency, tensor operations co-occur with
both FP32 and FP64.  These findings indicate substantial demand for double
precision (scientific or engineering codes) and less demand for ML jobs
in Perlmutter. Our comparison of ML and non-ML jobs
(Figure~\ref{fig:dl_related}) shows that ML jobs are well-represented among
both high- and very low-utilization jobs but contribute more to the total
GPU hours. This could point to small-scale
experiments (very low utilization) vs. large-scale training (high utilization).

\vspace{0.08in}
\noindent\textbf{Relationships among key GPU counters.}
Spatial imbalance of
MEM\_UTIL shows stronger ties to spatial imbalance of GPU\_UTIL, suggesting
that uneven data-transfer patterns across GPUs can degrade the collective GPU
usage at the job level. By contrast, temporal imbalance of
SM\_ACTV is more strongly associated with temporal imbalance of GPU\_UTIL
than MEM\_UTIL~\ref{fig:memcopy_sm_spatial_gputil}.
This indicates that job-level computation phases are a primary driver of
inconsistent usage patterns over time.

% \textbf{Takeaway 1}: Our analysis reveals distinct patterns in job sizes, 
% utilization levels, and precision types. Single GPU jobs dominate in count 
% but often have shorter durations and higher mean utilization compared to 
% multi-GPU jobs, which exhibit longer durations and lower utilization levels. 
% The comparison between DL/ML and non-DL/ML jobs highlights the contrasting 
% GPU utilization patterns. DL/ML workloads contribute significantly to 
% high utilization ranges and GPU hours. Additionally, the precision type 
% analysis shows a strong reliance on Tensor activity which indicates its 
% widespread use across various workloads.

% \textbf{Takeaway 2}: Temporal imbalance revealed that jobs with higher GPU 
% utilization tend to have more consistent usage patterns, while low-utilization 
% jobs often exhibit uneven or sparse activity. Burstiness analysis showed that 
% high-utilization jobs generally avoid irregular spikes or idle periods, while 
% low-utilization jobs are more likely to exhibit steady or sporadic activity. 
% The relationship between these metrics highlighted that jobs with both high 
% burstiness and high temporal imbalance are the most inefficient, often 
% underutilizing GPU resources.
