% Ready to be revised by professors

\section{Related Work}

% \mamun{I think we can create a category for jailbreak attacks like attention manipulation-based, adversarial suffix-based and so on if possible. then we can detail in a paragraph at last what is the difference between us and those work, and why our work is different.}
% \pedram{Done!}

%\subsection{Attention Mechanism in LLMs}

%The introduction of the attention mechanism, first presented by \cite{vaswani2017attention}, marked an important milestone in natural language processing (NLP). By addressing key limitations of previous architectures, such as RNNs and LSTMs, attention mechanisms enabled models to process input sequences more effectively by dynamically weighing the relevance of tokens. This innovation allows models to capture rich contextual relationships, prioritizing critical elements within a sequence. For instance, attention can emphasize relationships like subject-verb pairs or semantic nuances across long-range dependencies.


%\subsection{Jailbreak Attacks in LLMs}

%\nael{This classification here seems distracting.  Is this your own classification or does it follow something that already exists?  Do we need to break them down in detail?  Each category is also not described clearly (e.g., Gradient based "by using optimization" its not clear exactly how.)}

%\pedram{This classification is based on what is mentioned in section 3.1 of jailbreak zoo survey. I just added the new category of Attention-based attacks. I think we don't need to break down them but you are right that they may not be described clearly. I will add more details to each of these classes.}

\noindent Jailbreak attacks can be broadly classified as black box attacks, those that only assume access to prompt the model and observe its output, and white box attacks, which assume access to the internal state of the model~\cite{jin2024jailbreakzoo, shayegani2023survey, yi2024jailbreak}.  The attacks can further be distinguished by the attack approach.   \textbf{Gradient-based Jailbreaks} are white box attacks that leverage model gradients to craft adversarial changes to the prompt.  The attacks backpropagate the loss between the generated text and an affirmative response agreeing to generate a Jailbreak output back to the input prompt, and adjust this input in the direction of the gradient to compel the LLM towards an affirmative response bypassing the model alignment. Examples of this approach include the Greedy Coordinate Gradient (GCG) \cite{zou2023universal} and AutoDAN \cite{liu2024autodan} both of which generate highly transferable adversarial suffixes.
Another category of attacks is the \textbf{Rule-based Jailbreaks}, which decompose malicious prompts into benign-looking inputs creating "nested scenarios" that successfully evade alignment.   Examples of this approach include ReNeLLM \cite{ding2023wolf}, CodeAttack \cite{ren2024codeattack}, and Simple Adaptive Attack \cite{andriushchenko2024jailbreaking}.  % transform malicious intents into benign-looking prompts while producing the desired adversarial output.   
    % \item \textbf{Evolutionary-based Jailbreaks}:

Other approaches include \textbf{Demonstration-based Jailbreaks}, which rely on predefined, role-playing prompt, that directs the LLM to ignore constraints and alignment. Examples include DAN (Do Anything Now) \cite{shen2024anything}, which guides models to produce specific responses through hard-coded instructions.
 \textbf{Multi-agent-based Jailbreaks} use collaborative interactions between multiple LLMs to iteratively refine and optimize jailbreak prompts (e.g., PAIR \cite{chao2023jailbreaking} and GUARD \cite{wei2023jailbreak}).

    
In contrast to these approaches, our attack manipulates attention to strengthen or weaken associations between input tokens.  Attention is central to Transformer architectures at the heart of LLMs, typically implemented through multi-head self-attention which processes input sequences in parallel and captures diverse contextual representations~\cite{wang2020linformer, radford2019language}.  Recent research has focused on making attention mechanisms more adaptive and efficient \cite{chen2021scatterbrain}. For example, Zhang et al.~\cite{zhang2023tell} propose an attention model that adjusts weights based on sequence complexity, improving both performance and efficiency. %\cite{ben2024attend} explored hierarchical attention structures that better capture multi-scale patterns in text, enhancing the model's ability to capture diverse linguistic features. %These advancements not only demonstrate the versatility of attention mechanisms but also highlight their central role in enabling LLMs to process and generate human-like text with remarkable precision.  %As attention mechanisms continue to evolve, understanding their inner workings becomes increasingly important, to improve the generation quality, but also to understand adversarial implications in contexts such as jailbreak attacks. %The ability to manipulate attention weights underscores the potential for both improving LLM performance and exploiting vulnerabilities in their alignment.

Our work, along with a concurrent work by Pu et al.~\citet{pu2024feint} is the first to exploit attention for adversarial purposes.  Specifically, Pu et al. embed harmful prompts within nested tasks to confuse alignment, with an attention manipulation adversarial suffix that focuses attention on a harmful deeply embedded task, leading to effective Jailbreaks.   Our attention manipulation approach pursues two orthogonal strategies: (1) recomposing prompt fragments into a harmful prompt in the embedding space by increasing attention between them; and (2) camouflaging adversarial suffixes generated from other Jailbreak attacks by weakening attention between them and harmful tokens to evade alignment.   As a result, our approach combines with existing attacks and uses smaller overheads in terms of adversarial suffix size and attack generation time.  In the vision transformer realm, Alam et al. demonstrated an attention manipulation adversarial attack on a deformable transformers used within an object detection pipeline~\cite{alam-23}; attention is manipulated spatially away from important tokens or towards an adversarial patch, leading to successfully fooling the transformer. % \nael{Please check.}


%\noindent Our framework represents a hybrid of Attention-Based and Gradient-Based jailbreak categories, utilizing attention-gradient information to craft highly effective jailbreak prompts. Unlike previous studies that primarily focus on adversarial prompts or automated probing, our approach systematically leverages attention weights to bypass alignment protocols. Notably, we are the first to develop a white-box jailbreak method that fully exploits all available information within the LLM. A key innovation of our work is the decomposition of prompts into harmful and safe components, a technique that provides significant advantages to attackers and will be explored further in the following sections. While prior studies, such as \cite{pu2024feint}, have investigated attention manipulation at a surface level, our approach delves deeper into the dynamics of attention loss, dynamically adjusting associations within attention layers to amplify attack success rates. This advancement not only highlights the unique contributions of our work but also uncovers new pathways for disrupting alignment in large language models.






