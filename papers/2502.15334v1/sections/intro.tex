% Ready to be revised by professors


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/Framework1_updated2.pdf}
    \caption{Two attention manipulation strategies we use to enhance existing jailbreak attacks.}
    % \pedram{This figure just shows the high-level ideas we are using, decomposition and camouflaging. I'll change the current Figure 3 to show the details of how we combine these two ideas and change attention weights to get the most benefit. The font of texts is small and I will fix it.} \nael{This figure needs improvement.  Its not clear why we need two attention triggers, and why we need this complex process of updating each of them separately.  It is also missing the adversarial token generated by GCG/ReneLLM/AutoDan (which I think is the reason for the second attention trigger?  Why use this organization rather than something else? Maybe this can be presented in two steps: how we shift attention, and then strategies for exploiting that?}
    \label{fig: General Framework}
\end{figure*}



\section{Introduction}

The development of Large Language Models (LLMs) has marked a new era in Artificial Intelligence (AI), driving significant advancements across diverse application domains. These models, combining novel attention based architectures with diverse training on vast and diverse datasets \cite{achiam2023gpt}, exhibit generalized learning, and excel in generating human-like text.  These properties have made LLMs become pivotal in applications such as conversational agents, programming assistants, federated learning systems \cite{gargary2024systematicreviewfederatedgenerative}, and DNA processing tools \cite{Sereshki2024.05.02.592257}. Prominent examples include OpenAI’s GPT-4 \cite{achiam2023gpt}, Meta’s Llama2 \cite{touvron2023llama}, and DeepSeek \cite{liu2024deepseek}.

%\nael{Why these models specifically?  What are the unique capabilities?  Perhaps omit this last sentence or find a better sentence to introduce examples of models that you want to emphaize.}

%\pedram{I think since we utilize Llama2 as an open source model for attention manipulation and use GPT-4 and DeepSeek for transferability, and GPT-4 as the judge model to calculate ASR, it is good to keep them here. What is your opinion about the sentence below instead of the previous one? \\ Examples include OpenAI's GPT-4 \cite{achiam2023gpt}, Meta's Llama2 \cite{touvron2023llama}, and DeepSeek \cite{liu2024deepseek}. GPT-4 is widely recognized for its strong reasoning abilities and multimodal processing capabilities, Llama2 offers a well-aligned open-source alternative, and DeepSeek has recently emerged as a competitive model, providing performance comparable to GPT-4 with lower computational costs and GPU requirements.}

LLM models remain vulnerable to carefully crafted inputs, known as jailbreak prompts~\cite{shayegani2023survey}, which exploit safety mechanisms and induce harmful outputs. For instance, a jailbreak prompt may force a model to generate unsafe instructions or bypass ethical safeguards \cite{deng2023jailbreaker,mamun2023deepmem}. The growing prevalence of jailbreak attacks has led to extensive research aimed at understanding these vulnerabilities and developing effective countermeasures \cite{jin2024jailbreakzoo}.  However, it is important to continue to explore more advanced attacks to enable development of effective defenses and to provide benchmarks that can be used in their evaluation.

Existing jailbreak methods can be broadly categorized into two types \cite{jin2024jailbreakzoo}: (1) \textit{white-box attacks}, where the attacker requires access to the model's internal parameters \cite{zou2023universal, wallace2019universal, wen2024hard, guo2021gradient, liu2023autodan}, and (2) \textit{black-box attacks}, which require no such access relying only on being able to prompt the model and observe the output\cite{chao2023jailbreaking, mehrotra2312tree, li2023deepinception, perez2022ignore, greshake2023not, zeng2024johnny, shen2024anything, wei2024jailbroken}.  Because white-box approaches have access to the internal state of the model, they are in general stronger attacks.  Since access to model internals is becoming more common given the increase of open-source models,  white-box access should be assumed and used to drive and evaluate defenses.


In this paper, we propose a new approach to generating jailbreak attacks by introducing an additional degree of freedom in the optimization process. Existing jailbreak attacks are output-driven, optimizing for a target output through a global loss function. Instead, we introduce an orthogonal, input-driven approach. Specifically, our method focuses on the explicit influence of prompt tokens on the model's attention, rather than solely optimizing for the final output.

An analogy can be drawn from human language processing, where tools such as punctuation, textual formatting effects, syntax usage, and voice inflections and emphasis guide interpretation by shifting attention to specific elements of the text. Similarly, our attack, which we call \textbf{Attention Eclipse}, allows an attacker to either amplify or suppress the attention among specific tokens within the prompt. We show that surgically manipulating the model’s attention to the adversary’s advantage can lead to effective jailbreak attacks that bypass alignment constraints. %circumvents alignment constraints by subtly guiding the model’s focus toward adversarial interpretations.
 
We use manipulating attention in two ways in our attacks (recognizing that there are likely to be others): \textbf{(i)} First, attention manipulation allows the \textbf{recomposition} in the latent space of a "decomposed" prompt by establishing hidden dependencies between seemingly unrelated token sequences. This enables the adversary to embed harmful content within a benign-looking prompt, where attention later bridges the safe portion with the adversarial intent.\textbf{(ii)} Second, attention manipulation facilitates \textbf{camouflaging adversarial suffixes}, where an adversarial suffix —generated through an existing jailbreak method— is made less conspicuous by controlling the model attention distribution within the prompt, causing the adversarial prompt to bypass alignment. 
% Figure~\ref{fig: Amplification} illustrates how Attention Eclipse transforms an unsuccessful AutoDAN jailbreak prompt into a successful one by exploiting attention dynamics to bypass alignment safeguards.

%In this paper, we present a new approach to generate more powerful jailbreak attacks that manipulate the attention patterns within the LLM.  %In natural language text, it is well known that small changes in emphasis, through voice inflections in spoken language or punctuation in written language can substantially change perceived meaning.  
% Existing white-box attacks work based on a loss function with respect to desired generated output, backpropagating this loss back to the input to create effective adversarial prompts. 
%While white-box approaches offer greater control over model behavior, they often fail to leverage all the available information within the LLM, limiting their success rate and computational efficiency. Conversely, black-box methods rely on trial-and-error probing, which can be computationally expensive and less precise.
%Instead, our attack, which we call \textbf{Attention Eclipse}, enhances existing jailbreak prompts by leveraging the internal attention information of LLMs to either emphasize or deemphasize attention among tokens within the prompt to the attacker's advantage. Attention Eclipse two attention shifting strategies introduced using an attention manipulation token, namely:   (1) \textit{Prompt decomposition}, which divides the jailbreak prompt into harmful tokens and a safe generic prompt that appear distant from each other within the prompt.  Attention is then used to connect the generic question to the harmful prompt, and (2) \textit{Camouflaging adversarial suffixes:} starting from an adversarial suffix generated through another Jailbreak approach, we manipulate the attention between the adversarial suffix and the harmful tokens.  We believe this attention manipulation helps hide the adversarial prompt from the alignment component of the model.    Figure~\ref{fig: Amplification} illustrates how Attention Eclipse transforms an unsuccessful AutoDAN generated jailbreak prompt into a successful one using attention manipulation to bypass the LLM’s alignment safeguards. \\

%\nael{We need a paragraph to describe what we did/how we evaluate/one or two highlight results}
%\pedram{}

We apply our attacks to a number of recent models, using the camouflage strategy to hide adversarial prompts produced using a number of recent adversarial attacks.  Across all models and adversarial attacks, our attack produces substantial improvements in the attack success rates.  Moreover, by integrating attention shifting with adversarial token generation for iterative jailbreak attacks such as GCG~\cite{zou2023universal}, we show that our framework substantially reduces the number of overall iterations needed to generate successful jailbreaks.  We also show that the attacks are highly transferrable across models.

\noindent The main contributions of this paper are as follows.

\begin{itemize}[topsep=1pt, partopsep=1pt, parsep=1pt, itemsep=1pt]

\item We introduce a new white-box adversarial attack strategy that directly manipulates attention patterns within a prompt that enables effective evasion of model alignment.

\item We propose strategies to leverage attention attacks, by composing adversarial prompts, and by shifting attention between adversarial suffixes and harmful tokens. 
%enhances existing jailbreak attacks by utilizing LLMs' internal attention mechanisms. The framework includes prompt decomposition and attention manipulator tokens to achieve significant improvements in effectiveness.

\item  Our attack strategy can compose with most existing adversarial attacks to amplify their effectiveness.  We provide a generalizable framework that amplifies existing jailbreak techniques, including ReNeLLM \cite{ding2023wolf}, GCG \cite{zou2023universal}, and AutoDAN \cite{liu2023autodan}, across various LLM architectures. Our approach demonstrates transferability within model families, highlighting its adaptability to different models and scenarios.

%\item Extensive experiments demonstrate that leveraging attention information not only improves Attack Success Rate (ASR) but also reduces the time required to craft effective jailbreak prompts. Unlike previous approaches, our method fully exploits the model's internal information, offering enhanced control and efficiency.
% \mamun{how much improvement that we gain?}



\end{itemize}




























 



















