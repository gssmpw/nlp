% Ready to be revised by professors

%Table_start%
\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multicolumn{9}{c}{\textbf{GPT-ASR(\%↑)}} \\ \midrule

\multicolumn{1}{c|}{\textbf{Dataset}} & \multicolumn{4}{c|}{\textbf{AdvBench}} & \multicolumn{4}{c}{\textbf{HarmBench}} \\ 

\multicolumn{1}{c|}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Llama2-7b}} & \multicolumn{1}{c}{\textbf{Llama2-13b}} & \multicolumn{1}{c}{\textbf{Llama2-70b}} & \multicolumn{1}{c|}{\textbf{Vicuna-13b}} & \multicolumn{1}{c}{\textbf{Llama2-7b}} & \multicolumn{1}{c}{\textbf{Llama2-13b}} & \multicolumn{1}{c}{\textbf{Llama2-70b}} & \multicolumn{1}{c}{\textbf{Vicuna-13b}} \\ \midrule

\multicolumn{1}{c|}{AutoDAN} & 30.4 & 29.6 & 31.2 & \multicolumn{1}{c|}{90.0} & 16.0 & 14.5 & 18.5 & \multicolumn{1}{c}{81.0} \\ 

\multicolumn{1}{c|}{Amplified AutoDAN} & 58.5 (+92.4\%) & 55.6 (+87.8\%) & 59.8 (+91.7\%) & \multicolumn{1}{c|}{94.6 (+5.1\%)} & 40.5 (+153.1\%) & 38.5 (+165.5\%) & 41.5 (+124.3\%) & \multicolumn{1}{c}{91.0 (+12.3\%)} \\ \midrule

\multicolumn{1}{c|}{ReNeLLM} & 51.2 & 50.1 & 62.8 & \multicolumn{1}{c|}{80.2} & 48.0  & 46.0 & 55.0 & \multicolumn{1}{c}{76.0} \\ 

\multicolumn{1}{c|}{Amplified ReNeLLM} & 76.9 (+50.2\%) & 75.8 (+51.3\%) & 79.2 (+26.1\%) & \multicolumn{1}{c|}{\textbf{99.2} (+23.7\%)} & 68.5 (+42.7\%) & 67.0 (+\%45.6) & 70.5 (+28.2\%) & \multicolumn{1}{c}{94.0 (+23.7\%)} \\  \midrule

\multicolumn{1}{c|}{GCG} & 67.9 & 64.6 & 69.0 & \multicolumn{1}{c|}{95.2} & 65.0 & 62.5 & 66.5 & \multicolumn{1}{c}{92.5} \\ 

\multicolumn{1}{c|}{Amplified GCG} & \textbf{91.2} (+34.3\%) & \textbf{87.3} (+35.1\%) & \textbf{91.7} (+32.9\%) & \multicolumn{1}{c|}{98.5 (+3.4\%)}  & \textbf{89.0} (+36.9\%) & \textbf{82.0} (+31.2\%) & \textbf{90.5} (+36.1\%) & \multicolumn{1}{c}{\textbf{96.5} (+4.3\%)} \\ \bottomrule

\end{tabular}%
}
\caption{Attack Success Rate (ASR) of baseline jailbreak attacks and their amplified versions using the Attention Eclipse framework.}
\label{table: Comparison_with_baselines}
\end{table*}
%Table_end%

\section{Experimental Evaluation}

The Attention Eclipse framework enhances existing jailbreak attacks by leveraging attention manipulation to bypass alignment. This section evaluates our method on leading open-source LLMs using multiple jailbreak attacks. We compare the performance of the original attacks with their amplified versions, demonstrating improvements in Attack Success Rate (ASR), computational efficiency, and transferability to certain closed-source LLMs.

\subsection{Experimental Setup}


\noindent \textbf{Datasets:} We use two publicly available and widely used datasets: AdvBench \cite{zou2023universal} and HarmBench \cite{mazeika2024harmbench}, which comprehensively cover adversarial strategies and attack scenarios. Further details on these datasets and their relevance to our study are provided in Appendix \ref{appendix: dataset}. \\

\noindent \textbf{Target LLMs:} We evaluate our attack on open-source LLMs \cite{kukreja2024literature}, specifically Llama2-7B-Chat, Llama2-13B-Chat, Llama2-70B-Chat \cite{touvron2023llama}, and Vicuna-13B \cite{chiang2023vicuna}. These models span diverse architectures and parameter scales, ensuring a comprehensive assessment of our approach. \\

\noindent \textbf{Evaluation Metrics:} We evaluate our approach using two primary metrics:  
1) Attack Success Rate (\textbf{ASR}): The percentage of jailbreak prompts that successfully bypass alignment. We use \textbf{GPT-ASR} \cite{ding2023wolf} with GPT-4 model as the Judge model, since filter-based ASR metrics may lead to false positives \cite{liu2023autodan, chao2023jailbreaking}.
2) Time Cost Per Prompt (\textbf{TCPP}): The average time required to generate a successful jailbreak prompt, reflects computational efficiency \cite{ding2023wolf}.  

Detailed definitions and calculation methods for these metrics are provided in Appendix \ref{appendix: Evaluation Metrics}. \\


\noindent \textbf{Baselines:} We evaluate our method by amplifying some state-of-the-art jailbreak approaches:  
- GCG \citep{zou2023universal}: Generates adversarial suffixes via backpropagation to increase the probability of target text generation. 
- AutoDAN \citep{liu2023autodan}: Utilizes a genetic algorithm to craft semantically meaningful jailbreak prompts.  
- ReNeLLM \citep{ding2023wolf}: Rewrite harmful prompts and nest them.

Attention Eclipse enhances these methods by applying attention manipulation to amplify their effectiveness, leveraging two adversarial attention-shifting strategies as shown in Figure~\ref{fig: General Framework}.



\subsection{Main Results -- Attack Effectiveness}
\label{sec: Main Results}

\noindent \textbf{Improvement in ASR:} Table \ref{table: Comparison_with_baselines} underscores the generalization capability of the Attention Eclipse framework across various models and datasets. It achieves substantial ASR gains across all evaluated models. For instance, the Amplified AutoDAN attack achieves a 153.1\% improvement on the Llama2-7B model under the HarmBench dataset, demonstrating its ability to exploit alignment weaknesses more effectively than the baseline. 

We should note that for the GCG and its amplified attack, we use the same budget for the adversarial suffix (20 tokens) and allocate additional budget to the new adversarial components: $\varphi_1$ (5 tokens) and $\varphi_2$ (10 tokens). \\


% The best case scenario achieves ASR comparable to the Prompt with Random Search attack \cite{andriushchenko2024jailbreaking}, but with significantly lower noise and better generation efficiency.  Due to differences in template configurations, we do not directly compare results with \citet{andriushchenko2024jailbreaking}, as discussed in Appendix \ref{appendix: More Jailbreak Attacks}.
%\nael{Since this attack is not comparable, why include it here?  Maybe include it in the related work section instead and refer to the appendix there?}




\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\multicolumn{8}{c}{\textbf{GPT-ASR(\%↑)}} \\ \midrule

\multicolumn{1}{c|}{\textbf{Category}} & \textbf{Illegal Activity} & \textbf{Hate Speech} & \textbf{Malware} & \textbf{Physical Harm} & \textbf{Economic Harm} & \textbf{Fraud} & \textbf{Privacy Violence} \\ \midrule

\multicolumn{1}{c|}{AutoDAN} & 28.5 & 5.1  & 48.6  & 11.6 & 59.3 & 36.2 & 52.4 \\ 
\multicolumn{1}{c|}{Amplified AutoDAN} & 62.0 (+117.5\%) & 30.8 (+503.9\%) & 56.8 (+16.9\%) & 30.2(+160.3\%) & \textbf{96.3} (+62.4\%) & 59.4 (+64.1\%) & 71.4 (+36.3\%) \\ \midrule

\multicolumn{1}{c|}{ReNeLLM} & 50.9 & 48.6 & 64.0 & 34.2 & 50.0 & 56.0 &  59.5 \\ 
\multicolumn{1}{c|}{Amplified ReNeLLM} & 83.1 (+63.3\%) & 51.3 (+5.6\%) & 89.2 (+37.8\%) & 51.2 (+49.7\%) & 59.3 (+18.6\%) & 85.5 (+52.7\%) &  66.7 (+12.1\%) \\ \midrule

\multicolumn{1}{c|}{GCG} & 65.5 & 69.2 & 62.2 & 48.8 & 66.7 & 89.9 &  76.2 \\ 
\multicolumn{1}{c|}{Amplified GCG} & \textbf{89.1} (+36.0\%) & \textbf{97.4} (+40.8\%) & \textbf{94.6} (+52.1\%) & \textbf{81.4} (+66.8\%) & 92.6 (+38.8\%) & \textbf{98.6} (+9.7\%) & \textbf{95.2} (+24.9\%)  \\ \bottomrule

\end{tabular}%
}
\caption{Performance of the amplified attacks against baseline attacks for different categories of AdvBench dataset on Llama2-7b-chat model.}
\label{table:various_types_of_harmful_prompts}
\end{table*}





\noindent \textbf{Performance Across Prompt Categories:} Table \ref{table:various_types_of_harmful_prompts} presents a breakdown of ASR across different harmful prompt categories within the AdvBench dataset \cite{ding2023wolf}. The results indicate that certain categories, such as Hate Speech and Physical Harm, exhibit lower baseline ASR, suggesting that LLMs are more resistant to these types of jailbreaks. However, the Attention Eclipse framework significantly amplifies attack success across all categories. Notably, categories with lower baseline success, such as Hate Speech and Physical Harm (5.1\% and 11.6\% ASR for AutoDAN), show substantial gains, reaching 30.8\% and 30.2\% ASR, respectively, after amplification. These improvements highlight the effectiveness of attention manipulation in bypassing alignment, even in categories where models exhibit stronger resistance. \\



\noindent \textbf{Attack Acceleration:} An effective jailbreak attack should be fast and highly successful. One of the key advantages of Attention Eclipse is its ability to accelerate jailbreak attacks. Table \ref{table: timing} presents the time cost per prompt (TCPP) for different baseline attacks and their amplified versions. We evaluate jailbreak efficiency on Llama2-7B-Chat using the AdvBench dataset (More details in Appendix \ref{sebsec: TCPP experiment}). The results show that ReNeLLM reduces the computational cost by over 60\%, improving from 136.0s to 54.46s, while GCG achieves a 3.5× speedup, cutting TCPP from 665.0s to 189.41s. Note that for an Amplified AutoDAN attack, its TCPP includes the TCPP of the baseline AutoDAN, as we first compute $J_i$ and then apply Attention Eclipse to it. By minimizing search overhead, Attention Eclipse makes jailbreak attacks faster, more efficient, and scalable to larger models. \\


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{lccc}
\toprule
\multicolumn{4}{c}{\textbf{TCPP (seconds↓)}} \\ \midrule

\multicolumn{1}{c|}{\textbf{Metric}} & \textbf{\# Queries} & \textbf{Query Time} & \textbf{TCPP} \\ \midrule

\multicolumn{1}{c|}{AutoDAN} & 100 & 9.68 & 968.0 \\  
\multicolumn{1}{c|}{Amplified AutoDAN} & 4 & 18.62 & +74.48 \\ \midrule

\multicolumn{1}{c|}{ReNeLLM} & 3 & 45.33 & 136.0 \\  
\multicolumn{1}{c|}{Amplified ReNeLLM} & 3.2 & 17.02 & \textbf{54.46} (40.0\%) \\ \midrule

\multicolumn{1}{c|}{GCG} & 500 & 1.33 & 665.0 \\  
\multicolumn{1}{c|}{Amplified GCG} & 6.2 & 30.55 & 189.41 (28.5\%) \\ \bottomrule

\end{tabular}
}
\caption{Time Cost Per Prompt (TCPP) for baseline jailbreak attacks and their amplified versions using the Attention Eclipse framework on the Llama2-7B-Chat model and a subset of AdvBench dataset.}
\label{table: timing}  
\end{table}



\noindent \textbf{Transferability:} We examine how Attention Eclipse-generated jailbreak prompts transfer to closed-source models. Using GPT-3.5-Turbo \cite{openai2023gpt35}, and GPT-4o-mini \cite{achiam2023gpt} as target models, we evaluate whether prompts optimized on Llama2-7B-Chat (source model) remain effective when transferred. For this experiment, jailbreak prompts generated from the AdvBench dataset are directly input into the target models. Table \ref{table: transferability_results} shows that amplified jailbreak prompts maintain a strong ASR across target models, despite being optimized on an open-source model. Notably, Amplified ReNeLLM achieves an ASR of 96.0\% on GPT-3.5-Turbo and 79.4\% on GPT-4o-mini, demonstrating high transferability. Amplified AutoDAN also performs well on GPT-3.5-Turbo (83.3\%), though its success drops on GPT-4o-mini (31.0\%), indicating varying levels of robustness across different architectures. Conversely, Amplified GCG shows weaker transferability, achieving 27.1\% on GPT-3.5-Turbo and only 7.5\% on GPT-4o-mini, suggesting that its attack mechanism may be more dependent on source-model-specific characteristics. These results highlight the broad applicability of Attention Eclipse in bypassing alignment across multiple LLMs, emphasizing the importance of stronger alignment mechanisms in future models to mitigate cross-model jailbreak risks.



\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{lccc}
\toprule
\multicolumn{3}{c}{\textbf{GPT-ASR(\%)}} \\ \midrule

\multicolumn{1}{c|}{\textbf{Model}} & \textbf{GPT-3.5-turbo} & \textbf{GPT-4o-mini}  \\ \midrule

\multicolumn{1}{c|}{Amplified AutoDAN} & 83.3 & 31.0  \\  \midrule
\multicolumn{1}{c|}{Amplified ReNeLLM} & \textbf{96.0} & \textbf{79.4}  \\  \midrule
\multicolumn{1}{c|}{Amplified GCG} & 27.1 & 7.5  \\  \bottomrule

\end{tabular}}
\caption{Cross-model transferability of amplified prompts from Llama2-7B-Chat to closed-source LLMs.}
\label{table: transferability_results}  
\end{table}








% -------------------------------------------------------------------------------------------------------------------------------------------------------


%Table_start%
% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Category}}} & \multicolumn{6}{c}{\textbf{GPT-ASR(\%↑)}}                                                                           \\ \cline{2-7} 

% \multicolumn{1}{c}{} & \textbf{AutoDAN} & \textbf{Amplified} & \textbf{ReNeLLM} & \textbf{Amplified} & GCG & \textbf{Amplified} \\ \hline

% Illegal Activity & 29.1 & 62.0 & 50.9 & 83.2 & 25.0 & 88.4 \\ \hline
% Hate Speech  & 0.0 & 33.3 & 48.6 & 52.4 & 48.7 & 100.0 \\ \hline
% Malware & 50.0 & 60.0 & 64.0 & 88.2 & 43.2 & 94.4 \\ \hline
% Physical Harm & 10.0 & 30.0 & 34.2 & 50.0 & 32.6 & 73.8 \\ \hline
% Economic Harm & 66.7 & 100.0 & 50.0 & 60.0 & 59.3 & 100.0 \\ \hline
% Fraud & 38.5 & 61.5 & 56.0 & 86.2 & 89.9 & 100.0 \\ \hline
% Privacy Violence & 62.5 & 75.0 & 59.5 & 61.5 & 61.9 & 100.0 \\ \hline

% \end{tabular}%
% }
% \caption{Performance of the amplified attacks against baseline attacks for different categories of AdvBench dataset on Llama2-7b-chat model.}
% \label{table:various_types_of_harmful_prompts}
% \end{table}
% %Table_end%



% %Table_start%
% \begin{table}[h]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lccc}
% \hline
% \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Attack}}} & \multicolumn{3}{c}{\textbf{TCPP(s↓)}}                                                                           \\ \cline{2-4}

% \multicolumn{1}{c}{} & \textbf{\#Queries} & \textbf{Query Time} & \textbf{TCPP}  \\ \hline

% AutoDAN & - & - & 968.0  \\ \hline
% Amplified AutoDAN & 4  & 18.62  &  74.48 \\ \hline
% ReNeLLM & - & - &  136.0 \\ \hline
% Amplified ReNeLLM & 3.2  & 17.02 &  54.46 \\ \hline
% GCG & 500 & 1.33 &  665.0 \\ \hline
% Amplified GCG & 6.2 & 30.55 & 189.41  \\ \hline

% \end{tabular}%
% }
% \caption{TCPP of several baselines with their amplified versions using Attention Eclipse on Llama2-7b-chat model.}
% \label{table:timing}
% \end{table}
% %Table_end%



% \begin{table}[h]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|ccc|}
% \toprule
% \multicolumn{4}{|c|}{\textbf{GPT-ASR(\%↑)}}  \\ \hline

% \multicolumn{1}{|l|}{\diagbox[width=15em]{\textbf{Amplified Attack}}{\textbf{Model}}} & \multicolumn{1}{c|}{\textbf{GPT-3.5-turbo}}   & \multicolumn{1}{c|}{\textbf{DeepSeek}} & \multicolumn{1}{c|}{\textbf{GPT-4o-mini}}  \\ \hline
% \multicolumn{1}{|l|}{\textbf{Amplified AutoDAN}} & 83.27 & - & 30.96 \\ \hline
% \multicolumn{1}{|l|}{\textbf{Amplified ReNeLLM}}   & 95.96 & - & 79.42 \\ \hline
% \multicolumn{1}{|l|}{\textbf{Amplified GCG}}  & 38.5 & - & \multicolumn{1}{c|}{15.6} \\ 

% \bottomrule
% \end{tabular}%
% }
% \caption{Transferability of amplified jailbreak prompts}
% \label{table: transferability_results}
% \end{table}

