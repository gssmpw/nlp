% Ready to be revised by professors

\begin{abstract}
%Large Language Models (LLMs) are increasingly being deployed across various domains where users interact through textual prompts with the model.  Ensuring that these models do not generate harmful or toxic output is an important requirement for safe deployments, leading to the creation of safety-aligned models such as Llama-2. However, 
Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. 
%While LLMs aim to be both helpful and safe, jailbreak attacks disrupt this balance, making it vital to study these vulnerabilities and develop effective countermeasures to create more robust and reliable models.
It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety.  %Existing jailbreak attacks fail to fully utilize the information available within LLMs, which limits their potential in terms of attack success rate and computational efficiency.
In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt.  %The attention can be increased to create connections between prompt components that are not semantically connected, or conversely, to break attention between those components that are.  
%We use a multi-prong attention strategy to create difficult to detect adversarial prompts that bypass alignment, and improve the effectiveness of existing attacks, while lowering their generation cost.  %Our attention manipulation strategy consists of the following steps: (1) Prompt decomposition to create semantically separated prompt components that are then connected using attention manipulation; and (2) An adversarial suffix to attempt to jailbreak the model generated using any existing adversarial algorithms; and (3) Attention manipulation to reconnect the prompt, and to also manipulate the attention on the adversarial suffix, to evade alignment.  
%We introduce \textbf{Attention Eclipse}, a framework designed to exploit attention weights to bypass alignment mechanisms with high effectiveness.
By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable.  The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2\% ASR, vs. 67.9\% for the original attack on Llama2-7B/AdvBench, using less than a third of the generation time). 

\textcolor{red!90!black}{ \textit{Warning: This paper contains potentially harmful LLM-generated content.} }


 
%These improvements significantly boost the attack success rate, and the attacks transfer across different models. 
%For instance, we achieved a high ASR of \textbf{94.23\%} on the Llama2-7b-chat using the AdvBench dataset, marking a substantial improvement over the original GCG attack, and with a significantly lower attack generation time.



% \mamun{I think we can omit to describe the approach here. 4 main things we can cover here: 1) what is the problem (jailbreak) 2) why is it important (the attack disrupts the balance between safety and helpfulness), 3) what's our novelty (our main contribution to jailbreak) and 4) finally some claims backed up by empirical results (like how efficient are we in terms of computation/effort}
% \pedram{Done!} 
% \mamun{Can we provide the exact ASR here for the best version attack and how much it improves compared to other works?}

% The code and dataset are available at \href{https://github.com/Pedramzaree/Attention_Manipulator.git}{Code and Dataset}

\end{abstract}