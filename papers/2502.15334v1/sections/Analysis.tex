\section{Ablation Study}

This section analyzes the key components of Attention Eclipse to understand their impact on ASR. We examine how Decomposition, $\varphi_1$, and $\varphi_2$ contribute individually and together, how attention manipulation shifts model behaviour, and how camouflaging adversarial suffixes improves bypassing alignment. Additionally, we explore how a well-chosen initial point enhances efficiency, leading to faster and more successful jailbreaks.


\subsection{Evaluating Individual Attack Components}

%study of each attack, we do not normalize the size of adversarial components across different prompts and maintain the same size for each adversarial component. For example, when studying J\textsubscript{i}+Decomposition+ $\varphi_1$, if $\varphi_1$ has N tokens, then in J\textsubscript{i}+Decomposition+ $\varphi_1$ + $\varphi_2$, we use N tokens for $\varphi_1$ and allocate M new tokens to $\varphi_2$. Moreover, to explore each prompt, we optimize adversarial components from scratch rather than using pre-trained ones.} \\





\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{img/HM_GCG1.png}
    \caption{Attention heatmap of amplified jailbreak prompt before and after optimization on Llama2-7b-chat model. The color of each part shows its attention on the $G_h$ obtained by $\mathcal{L}_{\text{attn}}(., G_h)$ (Equation \ref{eq:Lattn}). Darker regions indicate increased attention, demonstrating the controlled redirection of focus using Attention Eclipse.}
    \label{fig: AutoDAN_heatmap}
\end{figure*}



To analyze the contribution of individual components in Attention Eclipse, we conduct a study using 100 adversarial Goals from AdvBench as the dataset and Llama2-7B-Chat as the target model. In this study, we generate the full adversarial prompt, and then use the components individually and in combination to understand their contribution to the success of the attack.  For example, when studying J\textsubscript{i}+Decomposition+ $\varphi_1$, if $\varphi_1$ has N tokens, then in J\textsubscript{i}+Decomposition+ $\varphi_1$ + $\varphi_2$, we use N tokens for $\varphi_1$ and allocate M new tokens to $\varphi_2$. Moreover, to explore each prompt, we optimize adversarial components from scratch rather than using pre-trained ones. The results of the study is shown in Table \ref{table: ablation_study}. Without any modifications, the Goal prompt fails (0\% ASR across all attacks). Introducing Decomposition alone, without a Jailbreak prompt, achieves little success (second row), and even when we add attention to recompose the prompt, ASR remains low.  significantly boosts ASR, particularly for ReNeLLM and GCG, demonstrating that breaking down prompts helps evade alignment. Adding $\varphi_1$ further enhances ASR, especially for ReNeLLM (70.0\%), suggesting that it is successful in recomposing prompt. Similarly, $\varphi_2$ alone yields strong improvements, most notably in AutoDAN (55.0\%), indicating its effectiveness in camouflaging the adversarial suffix. The best results come from combining all components, achieving the highest ASR across all models (AutoDAN: 67.0\%, ReNeLLM: 72.0\%, GCG: 90.0\%). %This confirms that Decomposition, $\varphi_1$, and $\varphi_2$ work synergistically.



\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{ 
\begin{tabular}{lccc}
\toprule
\multicolumn{4}{c}{\textbf{GPT-ASR(\%)}} \\ \midrule

% \diagbox[width=19em]{\textbf{Prompt}}{\textbf{Attack}}

\multicolumn{1}{l|}{{\textbf{Prompt}}} & \textbf{AutoDAN} & \textbf{ReNeLLM} & \textbf{GCG} \\ \midrule

\multicolumn{1}{l|}{Goal} & 0 & 0 & 0 \\  \midrule
\multicolumn{1}{l|}{Decomposition} & 3 & 3 & 3 \\  \midrule
\multicolumn{1}{l|}{Decomposition + $\varphi_1$} & 4 & 4 & 4 \\  \midrule
\multicolumn{1}{l|}{J\textsubscript{i} + Goal} & 11 & 7 & 67 \\  \midrule
%\multicolumn{1}{l|}{J\textsubscript{i} + Decomposition} & 9 & 24 & 76 \\  \midrule
\multicolumn{1}{l|}{J\textsubscript{i} + Decomposition + $\varphi_1$} & 23 & 70 & 84 \\  \midrule
\multicolumn{1}{l|}{J\textsubscript{i} + Decomposition + $\varphi_2$} & 55 & 71 & 82 \\  \midrule
\multicolumn{1}{l|}{J\textsubscript{i} + Decomposition + $\varphi_1$ + $\varphi_2$} & \textbf{67} & \textbf{72} & \textbf{90} \\  \bottomrule

\end{tabular}}
\caption{Impact of individual components on ASR using 100 selected adversarial prompts from AdvBench. The combination of all three components achieves the highest ASR across all models.}
\label{table: ablation_study}  
\end{table}


\subsection{Attention Heatmap}

Figure \ref{fig: AutoDAN_heatmap} illustrates how embedding $\varphi_1$ and $\varphi_2$ into the prompt, followed by optimization, can shift attention patterns in a jailbreak prompt. The upper prompt represents the initial jailbreak attempt generated by Attention Eclipse, which starts with a GCG jailbreak prompt, decomposes it, and incorporates $\varphi_1$ and $\varphi_2$. However, this initial prompt fails to jailbreak the Llama2-7B-Chat model.
To overcome this, we optimize all $\varphi_1$, $\varphi_2$, and AS, resulting in the bottom prompt in Figure \ref{fig: AutoDAN_heatmap}. The colour intensity of each sentence corresponds to the summed attention weight of its tokens on G\textsubscript{H}'s tokens at all layers and heads. The heatmaps reveal that optimizing the attention loss causes G\textsubscript{S} to darken, indicating that $\varphi_1$ successfully increases G\textsubscript{S}'s attention on G\textsubscript{H}, effectively recomposing the prompt. Conversely, AS becomes lighter, suggesting that $\varphi_2$ reduces AS's attention on G\textsubscript{H}, effectively camouflaging the adversarial suffix.




\subsection{Impact of Camouflaging on Harmfulness and Jailbreak Prompts}

We investigate using amplified GCG attacks how camouflaging adversarial suffixes improves jailbreak performance. To isolate the effect of $\varphi_2$, we start with a GCG jailbreak prompt ($J_i$), decompose it, and optimize $\varphi_1$ while keeping all other elements fixed. We then embed an initial $\varphi_2$ into the jailbreak prompt and optimize it in two opposing directions: 1) Increasing Adversarial Suffix attention on G\textsubscript{H}, and 2) Decreasing Adversarial Suffix attention on G\textsubscript{H}.

Figure \ref{fig: Reverse_Optimization_Effect_OutputLoss} visualizes the results. Increasing attention causes the output loss to plateau, preventing the jailbreak by keeping the suffix detectable and suppressing its effectiveness. In contrast, reducing attention weights enables the model to successfully jailbreak within just two optimization iterations, demonstrating that camouflaging enhances the ability of the adversarial suffix to evade alignment.

Furthermore, Figure \ref{fig: Reverse_Optimization_Effect_OutputLoss} highlights that decreasing attention weights results in a steady decline in output loss, directly correlating with improved jailbreak success. This reinforces that camouflaging adversarial suffixes is a crucial mechanism for crafting more effective jailbreak prompts, making them harder to detect which results in high ASR. 



\begin{figure}[h]
    \centering
    \includegraphics[height=4cm, width=\columnwidth]{img/Reverse_Optimization_Effect_OutputLoss.pdf}
    \caption{In the blue graph, we decrease $\mathcal{L}_{\text{attn}}(AS, G_h)$ (Equation \ref{eq:Lattn}) which is the camouflaging strategy and helps output loss (GCG) to bypass alignment. The orange one shows the case we increase $\mathcal{L}_{\text{attn}}(AS, G_h)$ (Revealing Adversarial Suffix) and make the output loss flat and avoid it bypassing alignment.}
    \label{fig: Reverse_Optimization_Effect_OutputLoss}
\end{figure}



\subsection{How Initial Point Affects ASR and TCPP}
\label{sec:good_initial_point}

To explore the effect of a well-chosen initial point, we analyze its impact on ASR and TCPP in generating jailbreak prompts. Specifically, we investigate how initializing  $\varphi_1$, $\varphi_2$, and $AS$  with well-chosen values and utilizing the HotFlip method \cite{ebrahimi2017hotflip, wallace2019universal} for optimizing them enhances the performance of amplified GCG attack. We first optimize these elements on simpler Goals and use these pre-optimized points as a robust point for tackling more complex Goals, demonstrating a self-transfer effect \cite{andriushchenko2024jailbreaking}.

In an experiment on Llama2-7b/AdvBench, we compare two approaches for optimizing jailbreak prompts. In the first one, we use a well-chosen initial point for all $\varphi_1$, $\varphi_2$, and $AS$. In the second one, these tokens are initialized randomly. The first approach results in an ASR of 91.2\% (as shown in Table \ref{table: Comparison_with_baselines}) and a TCPP of 192.2 seconds, while the second one achieves an ASR of 71.2\%  and a higher TCPP of 672.7 seconds even higher than original GCG attack. %These findings highlight the importance of selecting good initial points when utilizing Attention Eclipse to maximize efficiency and ASR. 






% \nael{Did this 94.23 number change?  I cannot find it in the table (seems it is 91.7), but it used in a number of places in the paper.  I edited front end to use 91.7}






% \begin{enumerate}[itemsep=0pt, topsep=2pt, parsep=0pt, partopsep=0pt]
%     \item \textbf{Faster Optimization:} Using a strong initial point significantly reduced the number of iterations needed to generate a successful jailbreak prompt, thereby improving computational efficiency.
%     \item \textbf{Higher Success Rates:} Tokens optimized on simpler prompts provided a stable starting point, leading to better generalization across more complex prompts. This resulted in an impressive \textbf{94.23\%} ASR on the Llama2-7b-chat model with the AdvBench dataset.
% \end{enumerate}


% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|l|ccc|}
% \toprule
% \multicolumn{4}{|c|}{\textbf{GPT-ASR(\%â†‘)}}  \\ \hline

% \multicolumn{1}{|l|}{\diagbox[width=19em]{\textbf{Prompt}}{\textbf{Attack}}} & \multicolumn{1}{c|}{\textbf{AutoDAN}}   & \multicolumn{1}{c|}{\textbf{ReNeLLM}} & \multicolumn{1}{c|}{\textbf{GCG}}  \\ \hline

% \multicolumn{1}{|l|}{\textbf{Goal}} & 0.0 & - & \multicolumn{1}{c|}{-} \\ \hline

% \multicolumn{1}{|l|}{\textbf{JP\textsubscript{old}}}   & - & - & \multicolumn{1}{c|}{-} \\ \hline

% \multicolumn{1}{|l|}{\textbf{JP\textsubscript{old} + Decomposition}}  & - & - & \multicolumn{1}{c|}{-}  \\ \hline

% \multicolumn{1}{|l|}{\textbf{JP\textsubscript{old} + Decomposition + AMT\textsubscript{1}}}  & - & - & \multicolumn{1}{c|}{-} \\ \hline

% \multicolumn{1}{|l|}{\textbf{JP\textsubscript{old} + AMT\textsubscript{2}}}  & - & - & \multicolumn{1}{c|}{-} \\ \hline

% \multicolumn{1}{|l|}{\textbf{JP\textsubscript{old} + Decomposition + AMT\textsubscript{1} + AMT\textsubscript{2}}}  & - & - & \multicolumn{1}{c|}{-} \\ 

% \bottomrule
% \end{tabular}%
% }
% \caption{The ASR of different prompts as jailbreak prompts to study the effect of different components in crafting jailbreak prompts individually explored on Llama2-7b-chat model and HarmBench dataset.}
% \label{table:ablation_study}
% \end{table}






% \begin{figure}[ht]
%     \centering
%     \includegraphics[height=4cm, width=\columnwidth]{img/Reverse_Optimization_Effect.pdf}
%     \caption{Effect of modifying attention weights on harmful words. Increasing attention weights hinders jailbreak effectiveness while reducing them facilitates bypassing alignment constraints.}
%     \label{fig: Reverse_Optimization_Effect}
% \end{figure}