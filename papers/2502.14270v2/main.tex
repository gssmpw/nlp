\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, graphicx, cite, hyperref}
\usepackage{authblk}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}  
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{placeins}  % Adds \FloatBarrier for better float control
\usepackage{plantuml}
\usepackage{geometry}  % Added for margin control
\usepackage{hyperref}  % Added for link handling
\usepackage{subcaption} % Include this in your preamble
\usepackage{array}
\usepackage{multirow}
\bibliographystyle{IEEEtran} % or any other style like plain, apalike, etc.
% \bibliography{reference} % Without .bib extension
% \usepackage[T1]{fontenc}
% \usepackage{lmodern}
% % add this code for font handeling error

%\usepackage{caption}  % Ensure this package is included for fine control over captions

\title{PREDICTING FETAL BIRTHWEIGHT FROM HIGH DIMENSIONAL DATA USING ADVANCED MACHINE LEARNING}

\author{Parul Kumari$^{1}$, Harsh Joshi$^{1}$, Rajeshwari Mistri$^{1}$, Nachiket Kapure$^{1}$,  Manasi Mali$^{1}$, Seema Purohit$^{1}$, Neha Sharma$^{2}$, Mrityunjoy Panday$^{3}$, Chittaranjan S. Yajnik$^{4}$ \\
$^1$B.K. Birla College of Arts, Science and Commerce, Kalyan \\
$^2$Tata Consultancy Services, India \\
$^3$Cognizant Technologies, India \\
$^4$Diabetes Unit, KEM Hospital and Research Centre, India \\ 
\vspace{0.5cm}
\texttt{\{parulkumari2307, joshiharsh0506, rajeshwarimistri11,kapnachi1904, malimanasi2002, nvsharma1975, mrityunjoy.0113, csyajnik\}@gmail.com, seema.purohit@bkbck.edu.in} \\
    }
\begin{document}
\maketitle

\begin{abstract}
Birth weight serves as a fundamental indicator of neonatal health, closely linked to both early medical interventions and long-term developmental risks. Traditional predictive models, often constrained by limited feature selection and incomplete datasets, struggle to achieve overlooking complex maternal and fetal interactions in diverse clinipredictive settings. This research explores machine learning to address these limitations, utilizing a structured methodology that integrates advanced imputation strategies, supervised feature selection techniques, and predictive modeling. Given the constraints of the dataset, the research strengthens the role of data preprocessing in improving the model performance. Among the various methodologies explored, tree-based feature selection methods demonstrated superior capability in identifying the most relevant predictors, while ensemble-based regression models proved highly effective in capturing non-linear relationships and complex maternal-fetal interactions within the data. Beyond model performance, the study highlights the clinical significance of key physiological determinants, offering insights into maternal and fetal health factors that influence birth weight, offering insights that extend over statistical modeling. By bridging computational intelligence with perinatal research, this work underscores the transformative role of machine learning in enhancing predictive accuracy, refining risk assessment and informing data-driven decision-making in maternal and neonatal care.

\noindent \textbf{Keywords:} Birth weight prediction, maternal-fetal health, MICE, BART, Gradient Boosting, Neonatal outcomes, Clinipredictive.
\end{abstract}

\section{Introduction}
Birth weight (BW) is a crucial determinant of neonatal health outcomes, significantly influencing both immediate and long-term health trajectories. The World Health Organization (WHO) identifies low birth weight (LBW), defined as less than 2,500 grams as a substantial public health concern, impacting approximately 15\% of births globally. Figure \ref{fig:LBW} shows the distribution of LBW across the countries and correlates with elevated risks of neonatal mortality, morbidity, and enduring health complications \cite{1A, 2A}.
Conversely, high birth weight, also known as macrosomia, is associated with delivery complications and predisposes individuals to obesity and metabolic disorders later in life. Accurate prediction of BW is essential for identifying at-risk pregnancies, guides delivery mode decisions for macrosomia, prompts nutritional interventions for LBW to mitigate complications like respiratory distress syndrome, necrotizing enterocolitis, and neurodevelopmental issues \cite{3A}. In gestational diabetes, early macrosomia risk identification allows closer glycaemic monitoring and timely interventions. It prepares neonatal teams for potential complications, enabling tailored postnatal monitoring for issues like hypoglycemia in LBW infants or metabolic disorders with macrosomia \cite{4A}. 
The challenge of this research is to predict fetal BW using a constrained dataset characterized by limited observations, incomplete data quality, inconsistently distributed and restricted predictor variables. However, traditional methods for predicting BW frequently utilize a restricted set of factors, potentially omitting critical variables and leading to suboptimal performance. In such a case, machine learning (ML) is crucial for prediction due to its ability to analyze complex patterns in large datasets. Advanced techniques like ensemble methods consider an exhaustive set of variables, enhance accuracy and handle nonlinear relationships. These algorithms uncover hidden insights and make predictions beyond traditional statistical methods, enabling more informed decision-making and accelerating scientific discoveries. This research bridges the domains of medical science and technology by using ML to address challenges in epigenetic medical research. This addresses the methodological challenges of constructing reliable predictions when working with sparse maternal-fetal data, contributing to the broader discourse on predictive modeling under data limitations in perinatology. The novel integration of ML techniques with epigenetic studies highlights the transformative potential of interdisciplinary approaches in advancing medical research outcomes.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{LBW.png} 
    %\includegraphics[width=0.8\textwidth]{LBW.png} % Replace 'image.png' with your actual image filename
     % Scale down image
    \caption{ Sample of Low Birth Weight Distribution across the Countries}
    \label{fig:LBW}
\end{figure}

\subsection{OBJECTIVE}

The principal objective of this study is to enhance the predictive accuracy of neonatal weight by using ML to analyse small, incomplete and raw datasets. Specifically, the research seeks to: 
\begin{itemize}
    \item identify critical and essential attributes that impact BW,
    \item enhance predictive models through advanced imputation and supervised feature selection techniques, and
    \item improve the accuracy and interpretability of BW predictions within clinical environments.
\end{itemize}
To achieve these objectives, the study is organized into the following sections.  It begins with Section II, a comprehensive literature review, examining existing approaches to BW prediction and identifying current research gaps. The methodology in Section III outlines the data preprocessing techniques, imputation methods, feature selection strategies, and ML algorithms employed in this research. Following this, the study in Section IV presents the results, showcasing the performance of various models and highlighting the most influential predictors identified. Section V follows a discussion of the findings, contextualizing them within the broader field of prenatal care and neonatal health. Finally, Section VI concludes with a summary of key insights, an acknowledgment of limitations, and proposals for future research directions to further enhance the accuracy and clinipredictive applicability of BW prediction models.


\section{Literature Review}
ML algorithms have demonstrated significant potential in fetal BW prediction, with recent studies showing varying degrees of success across different methodological approaches. Artificial Neural Networks (ANNs) and Naive Bayes (NB) achieved 70\% accuracy on balanced datasets (n=500) in predicting BW at six months of pregnancy (Adeeba, S., Kuhaneswaran, B., \& Kumara, B., 2022).\cite{5A}, while Gaussian NaÃ¯ve Bayes implementations demonstrated 86\% accuracy in controlled studies with balanced datasets(n=445) when classifying BW into binary categories(Bekele W. T. (2022))\cite{6A}. In contrast, Decision Trees underperformed with accuracy rates below 60\% (Adeeba et al., 2022)\cite{5A}. The implementation of XGBoost showed particular promise in handling high-dimensional medical data through gradient tree boosting, especially when dealing with complex maternal health indicators (Chen \& Guestrin, 2016)\cite{7A}. Multiple Imputation by Chained Equations (MICE) demonstrated superior performance in handling missing values compared to K-Nearest Neighbors (KNN) imputation, particularly in preserving temporal consistency within longitudinal datasets, as evidenced by analysis of the Pune Maternal Nutrition Study (PMNS) dataset encompassing over 5000 variables (Varma et al., 2024)\cite{8A}.
 
The prior study, i.e., phase 1 study of birth cohort, which highlighted the effectiveness of data imputation technique by achieving superior performance in handling missing values, reducing imputation error by 23\% compared to conventional methods \cite{8A}. Their analysis of the PMNS dataset (n$>$5000) showed that MICE preserved temporal consistency in longitudinal data with 89\% accuracy, significantly outperforming K-Nearest Neighbors (KNN) imputation, which achieved 74\% accuracy in maintaining data relationships, where the present study (phase 2) employs the imputed PMNS dataset (of phase 1) to predict fetal BW. A study by (Luke Oluwaseye Joel..,2024) evaluated various imputation methods, including MICE, Mean, Median, Last Observation Carried Forward (LOCF), KNN, and Missforest imputation \cite{9A}. The findings indicated that MICE consistently outperformed other techniques in maintaining data integrity across different healthcare datasets, including those related to breast cancer and diabetes.  MICE operates under the assumption that missing values can be predicted based on observed data, thus reflecting the underlying distribution more accurately than simpler methods \cite{9A}. The Data Analytics Challenge on Missing Data Imputation (DACMI) highlighted advancements in clinical time series imputation, showcasing competitive ML models like LightGBM and XGBoost alongwith MICE, emphasizing the importance of temporal and cross-sectional features in achieving robust imputation results \cite{10A}.

The development of accurate fetal BW prediction models relies heavily on sophisticated feature selection methodologies and effective missing data imputation techniques. A comprehensive analysis by Gaillard et al. (2011)\cite{11A} used Principal Component Analysis (PCA) to identify critical maternal anthropometric measurements, revealing that gestational age, BMI, and blood pressure measurements exhibited the strongest correlations with fetal BW. The Generation R Study, which highlighted the critical role of maternal obesity indices and hypertensive disorders in influencing birth outcomes. Expanding upon this, D'souza et al. (2021) demonstrated the strong correlations between maternal vitamin B12 levels and neurodevelopmental outcomes, emphasizing the importance of incorporating nutritional factors into predictive models \cite{12A}. A more recent study by Esther Liu (2024) explored various feature selection methods, including forward selection, backward elimination, and stepwise selection, on a dataset of 1301 mother-child pairs \cite{13A}. The other feature selection approaches, such as filter methods using statistical tests like Pearson correlation, wrapper methods (which evaluate feature subsets based on model performance), and embedded methods (such as Lasso regularization and tree-based models)\cite{14A}, all of which are crucial for identifying the most relevant features for accurate predictions. 

Furthermore, the use of feature selection techniques like Boruta has been shown to optimize model outcomes, particularly in predicting LBW. Hybrid methods based on ensemble learning have also demonstrated effectiveness in predicting BW ranges, underscoring the importance of feature selection in improving prediction accuracy \cite{15A}. Additionally, studies like those of Moreira et al. (2019) and others focusing on high-risk pregnancies have reinforced the significance of robust feature selection in enhancing predictive performance \cite{16A}. Approaches like Mutual Information (MI), which captures both linear and nonlinear relationships\cite{17A}, and Kendall transformation, which preserves ranking in categorical data, offer versatile and robust methods for selecting relevant features \cite{18A}. Collectively, these studies emphasize the pivotal role of integrating advanced feature selection techniques with reliable imputation methods to develop robust and accurate predictive models for fetal BW, contributing to improved prenatal care outcomes.

Recent validation studies, by Hussain and Borah (2020), Adeeba et al. (2022) have utilized various ML evaluation techniques. In the study of Hussain and Borah's Random Forest model achieved coefficient of determination (RÂ²) of 0.87, while Adeeba et al. employed Support Vector Machines, achieving RÂ² of 0.83. These studies underscore the importance of using robust evaluation metrics like AUROC, RÂ², Root Mean Squared Error (RMSE), precision, recall, and F1 score to assess model performance\cite{19A}. These studies highlight the need for multi-center validation across diverse healthcare settings and the development of standardized protocols for model deployment in clinical workflows to realize the full potential of these technologies in improving maternal-fetal health outcomes. Dataset size constraints ($n<500$ in multiple studies), geographic and demographic homogeneity in training data, and limited integration of real-time fetal monitoring data present significant methodological limitations \cite{20A}. Technical requirements for advanced implementation include standardized APIs for healthcare system integration and automated feature selection protocols for high-dimensional medical data. The integration of Electronic Health Record (EHR) systems presents specific challenges in data standardization across diverse healthcare systems and real-time prediction capabilities for immediate clinical decision support\cite{21A}. 

The gaps analyzed from the prior studies isâ the research should focus on expanding dataset diversity, without advanced imaging data in prediction models, and developing clinical workflow integration protocols. The success of ML applications in fetal BW prediction ultimately depends on addressing these challenges while maintaining high prediction accuracy and clinical utility. As demonstrated by the reviewed studies, the field shows promising potential for enhancing prenatal care through accurate BW prediction, particularly when sophisticated imputation techniques and comprehensive feature selection methods are employed in conjunction with advanced ML algorithms.


\section{Methodology}

\subsection{Data Source}
The dataset was collected by the Diabetes Unit of the KEM Hospital, Pune through the Pune Maternal Nutrition Study (PMNS) in collaboration with the team from Southampton, UK working on fetal programming of diabetes under the guidance of Prof David Barker. The PMNS, a preconception observational birth cohort initiated in 1993 \cite{6A} in six villages near Pune, serves as an invaluable data source for this research \cite{6A}. The PMNS study represents a comprehensive dataset collected at 2â3 times during pregnancy through detailed and longitudinal assessments.
%  Figure 1 of document pmns study 1 revision 6 
% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.4]{Data Source.png}
%     %\includegraphics[width=0.8\textwidth]{Data Source.png} % Replace 'image.png' with your actual image filename
%     \caption{PMNS Data Source}
%     \label{fig:Data Source}
% \end{figure}

\begin{figure}[H]
    \centering
    \makebox[\textwidth][l]{\hspace{-2.45cm}  % Moves image 2 cm to the left
        \includegraphics[scale=0.4]{Data_Source.png}
    }
    \caption{PMNS Data Source}
    \label{fig:Data Source}
\end{figure}


Fetal growth was monitored , while paternal body size and metabolic parameters were also documented. Anthropometric measurements of newborns were performed at birth and were subsequently recorded serially over the next two decades. With over 800 pregnancies studied in these rural women from 1993-96, the comprehensive dataset encompasses maternal anthropometrics, socioeconomic status, nutrition, physical activity, metabolism, fetal growth measurements, paternal characteristics, and newborn anthropometry \cite{12A}. The study design included periodic assessments of both parents and offspring to evaluate growth and a range of cardiometabolic factors, with follow-up rates exceeding 90\%. Additionally, a biobank containing biological samples such as blood and urine has been archived, ensuring a valuable resource for future research. The data collected has been securely stored on dedicated servers, providing a repository for longitudinal and transgenerational analysis. The methodology and timeline of the PMNS process and data collected  are illustrated in Figure \ref{fig:Data Source}.
The PMNS dataset provides information for: 
\begin{itemize}
    \item Determinants of fetal growth specific to Indian populations.
    \item Long-term evolution of cardiometabolic risks, including diabetes, within the framework of the Developmental Origins of Health and Disease (DOHaD).
    \item Transgenerational influences on health outcomes, supported by the inclusion of a third generation.
    \item The development of a preconceptional micronutrient intervention, informed by maternal nutritional and metabolic data.
\end{itemize}
These foundational contributions from Phase 1 \cite{8A} provide a rich and unique platform for extending the research in Phase 2. By this data, the current study aims to build upon the existing knowledge base and further explore maternal, fetal, and transgenerational health determinants. The initial dataset for the PMNS encompassed an extensive collection of 5800 features for 800 participants. These features were broadly categorized into anthropometric measurements, socioeconomic status, obstetric history, medication, nutritional intake, and other vital parameters. For the first phase of the project, the focus was directed toward processing the anthropometric data, comprising approximately 177 columns. Subsequently, for the current analysis in this research, an imputed dataset was derived, containing 109 columns and 791 rows. This dataset includes key features such as maternal height, hip circumference, waist circumference, red blood cell count, systolic blood pressure, pulse rate, fundal height, abdominal circumference, maternal weight, BMI, lymphocyte percentage, vitamin B12 levels, red cell folate, ferritin levels, gestational age at delivery, placental weight, total protein, and total fat. The longitudinal follow-ups spanning over two decades provide a foundation for investigating the life course evolution of diabetic risk factors and other cardiometabolic outcomes, offering valuable insights for the Phase 2 research.

\subsection{EDA}

Exploratory Data Analysis (EDA) is performed to gain insights into the characteristics of the dataset and identify potential patterns and relationships between variables. EDA process involves the use of data visualization techniques, including histograms and scatter plots, to understand the distribution of variables and identify potential outliers. Summary statistics, such as means and standard deviations, are also calculated to gain insights into the central tendency and variability of the data. It reveals that the data is skewed and has a high degree of variability, which informs the decision to use non-parametric tests and robust regression models. 
The dataset contains 109 numeric columns, categorized into 89 continuous and 20 discrete variables. Among the continuous variables, 47 columns follow a Log-Normal distribution, 25 Gamma distribution, and 17 are best described by a Gaussian distribution (table \ref{tab:Data nature}). No variables exhibit characteristics of Exponential, Poisson, or Uniform distributions. The classification of continuous variables into these distributions is crucial for selecting appropriate statistical methods and preprocessing techniques. This distribution analysis aids in feature engineering, ensuring proper transformation or normalization of skewed variables, and informs the choice of models, such as regression or ML algorithms, suited for the nature of the data.

% Table 1. Data nature
% Table 1 of document pmns study 1 revision 6  
\begin{table}[h!]
\centering
\caption{Data nature}
\begin{tabular}{|c|c|}
\hline
\textbf{Distribution Type} & \textbf{Count of Data Points} \\
\hline
Gaussian (Normal) & 17 \\
\hline
Log-Normal & 47 \\
\hline
Uniform & 0 \\
\hline
Gamma & 25 \\
\hline
Discrete & 20 \\
\hline
\end{tabular}
\label{tab:Data nature}
\end{table}

In addition to distribution analysis, the dataset contains 6.78\% missing values, with a Missing Not at Random (MNAR) pattern. To further analyze the nature of missingness, the Little's MCAR Test is conducted, which tests the hypothesis that data is Missing Completely at Random (MCAR). The test yields a p-value of 0, leading to the rejection of the null hypothesis. This suggests that the missing data in the dataset is likely not missing completely at random (MCAR), and therefore, the missingness might be Missing Not at Random (MNAR) or Missing at Random (MAR).  MNAR indicates that the probability of a value being missing is related to the unobserved value itself, implying that the missingness is not random and may depend on the data characteristics. By conducting these preprocessing steps, the data is cleaned, properly scaled, and prepared for robust statistical analysis and model training.

\subsection{Data Imputations}

In this study, a multiple imputation approach is used to address missing data, utilizing the Multivariate Imputation by Chained Equations (MICE) algorithm. The MICE method is used due to its robustness and flexibility technique for handling complex data structures \cite{8A}. To further enhance the imputation process, an updated approach is adapted, where discrete data is imputed using the K-Nearest Neighbors (KNN) method, while continuous data is imputed using MICE, similar to the study done by Khan, S.I\cite{22A}. This hybrid approach is supported by recent findings, which suggests that KNN can outperform MICE for discrete data imputation, thereby improving the overall accuracy and reliability of the results.

\subsection{Supervised Feature selection}

The study utilizes an extensive feature selection (table \ref{tab:Supervised Feature Selection Methods Classification}) approach, incorporating four primary categories of supervised techniques: Filter, Wrapper, Embedded, and Hybrid methods. The objective is to identify the most relevant features for predicting fetal BW. Table \ref{tab:Supervised Feature Selection Methods Classification} shows the feature selection techniques used. 

%Table 2: Supervised Feature Selection methods classification
% Table 2 of document pmns study 1 revision 6

% \usepackage{graphicx}  % Add this in the preamble

\begin{table}[htbp]
\centering
\caption{Supervised Feature Selection Methods Classification}
\resizebox{\textwidth}{!}{ % Resize the table to fit within text width
\begin{tabular}{|c|c|c|}
\hline
\textbf{Category} & \textbf{Type} & \textbf{Method} \\
\hline
\multirow{5}{*}{Filter} & Correlation-based & Pearson correlation \\
& Statistical-based & ANOVA \\
& Information-based & Mutual Information (MI)\cite{17A} \\ & Correlation-based & Kendall\cite{18A} \\
& Selection-based & Incremental Max-Min Feature Selection (INMIFS)\cite{23A} \\
\hline
\multirow{4}{*}{Wrapper} & Forward Selection & LASSO \\
& Forward Selection & SHAP + linear regression \\
& Forward Selection & SHAP + XGBoost \\
& Backward Elimination & Recursive Feature Elimination (RFE) \\
\hline
\multirow{5}{*}{Embedded} & Regularization-based & LASSO \\
& Regularization-based & Ridge \\
& Tree-based & Decision Trees \\
& Spline-based & MARS \cite{24A} \\
& Tree-based & BART  \cite{25A}\\
\hline
\multirow{2}{*}{Hybrid} & Balancing-based & MXM \cite{26A} \\
& Network-based & HOPULAR (Modern Hopfield Networks for Tabular Data) \cite{27A} \\
\hline
\end{tabular}
}
\label{tab:Supervised Feature Selection Methods Classification}
\end{table}

The research uses a combination of these feature selection approaches to select the most pertinent features for the prediction of fetal BW. Each technique is fine-tuned through cross-validation and hyperparameter optimization, to ensure the optimal configurations for the data. The top 20 features selected by each technique are stored, and their rankings are analyzed using a consensus approach. This allows for the identification of the most consistently selected features across multiple methods, providing a comprehensive understanding of the relationships between features and the outcome variable.

\subsection{ML Model Evaluation}

The downstream evaluation of feature selection is conducted using regression techniques to assess the effectiveness of the selected features in predicting the target variable. To achieve this, the dataset is split into training and testing sets using 5-fold cross-validation, which balances bias and variance in model evaluation. A comprehensive array of 13 regression models is used, encompassing foundational techniques including Linear Regression, Ridge Regression, LASSO Regression, Bayesian Ridge Regression, and Support Vector Regression, in addition to ensemble methodologies such as Random Forest, Gradient Boosting, and AdaBoost Regression. 

These models are systematically trained on the designated training set, using diverse feature sets derived from the feature selection process, while hyperparameter optimization is executed through grid search to enhance model efficacy. The models are designed to address potential multicollinearity, non-linear relationships, and interactions among features, applying insights garnered from the feature selection phase. The evaluation of these models' performance is conducted through metrics such as Root Mean Squared Error (RMSE), Mean Squared Error (MSE), and R-squared (RÂ²), which yield critical insights into the predictive accuracy and overall goodness of fit of the models.
Specifically, RMSE and MSE measure the average magnitude of the errors made by the model, with lower values indicating higher predictive accuracy. RÂ² measures the proportion of variance in the dependent variable explained by the independent variables, with values closer to 1 indicating a better model fit. 

Each subset of features is subsequently used to train 12 distinct ML algorithms, yielding 144 distinct combinations of feature selection methods and models. The 144 model-feature combinations are thoroughly compared using RMSE and RÂ². The model exhibiting superior performance, characterized by the lowest RMSE and highest RÂ² across the entire spectrum of combinations, is selected as the optimal model, thereby substantiating the efficacy of the feature selection methodology. The generalizability of the final model is considered utilizing an additional hold-out test set, which evaluates its efficacy on unseen data, thereby ensuring its robustness.

\section{Results}
The study provides an extensive examination of the predictive capabilities of ML models concerning BW, demonstrating favourable outcomes while underscoring potential avenues for enhancing the precision of fetal weight assessment.

\subsection{Data Analysis}
\subsubsection{Summary stats}
The summary statistics (table \ref{tab:Summary Statistics}) provides an overview of the key numerical characteristics of the variables.

% Table 3 of document pmns study 1 revision 6
\begin{table}[H]
\centering
\caption{Summary Statistics}
%\begin{adjustbox}{max width=\textwidth} % Adjust table width to page width
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Statistic} & \textbf{f0\_m\_age} & \textbf{f0\_socio\_eco\_sc} & \textbf{f0\_m\_ht} & \textbf{f0\_m\_wt\_preg} & \textbf{f0\_m\_bmi\_preg} \\
\hline
Count & 784.00 & 788.00 & 789.00 & 788.00 & 782.00 \\
\hline
Mean  & 21.3239 & 26.8324 & 151.8637 & 41.3379 & 18.0494 \\
\hline
Std   & 3.5069  & 6.85    & 5.0049  & 6.1806  & 1.8731  \\
\hline
Min   & 15.0000 & 6.00    & 135.0000 & 0.0000  & 12.9500 \\
\hline
25\%   & 19.0000 & 23.00   & 148.5000 & 38.0000 & 16.7225 \\
\hline
50\%   & 21.0000 & 27.00   & 152.0000 & 41.1000 & 17.8250 \\
\hline
75\%   & 23.0000 & 31.00   & 155.4000 & 44.8000 & 19.0850 \\
\hline
Max   & 40.0000 & 48.00   & 163.9000 & 69.0000 & 30.7100 \\
\hline
\end{tabular}
%\end{adjustbox}
% Table 3. Summary statistics
\label{tab:Summary Statistics}
\end{table}
The statistical summary serves as a foundation for BW prediction where the variability in maternal health and socio-economic factors can potentially influence outcomes. For instance, `\textit{f0\_m\_wt\_prepreg}' variable has a broad range, with values from 0 to 69 kg and a mean of 41.33 kg, reflecting significant variability in maternal weight prior to pregnancy. The distribution of these variables shows variability and suggests that certain features, such as socio-economic status and BMI, may have a wider spread compared to others like maternal height. These statistics provide an essential overview of data distribution and variation, pointing to specific areas that may require further investigation, such as the wide range in maternal weight prior to pregnancy. This summary offers a quick yet comprehensive understanding of the dataset, helping identify patterns, anomalies, and areas requiring further exploration.

\subsubsection{Distribution Histogram}

% Figure 3 : Data Distribution of sample data
% Figure 2 of pmns study 1 revision 6

\begin{figure}[H]  % Use [H] for strict placement, requires \usepackage{float}
    \centering
    \includegraphics[width=16 cm]{Data_Distribution_histogram.png} % Use width for better control
    \caption{Data Distribution of Sample Data}
    \label{fig:Data_distribution}
\end{figure}

The histogram (figure \ref{fig:Data_distribution}) illustrates the distribution of data for the selected variable, with the x-axis representing the range of values and the y-axis showing the frequency of observations within each bin. The figure presents three histograms with KDE overlays for \textit{'f0\_m\_total\_v2', 'f0\_m\_calfat\_v1'}, and \textit{'f0\_m\_sys\_bp\_r2\_v2'}. \textit{'f0\_m\_total\_v2'} is slightly right-skewed, with most values between 1000â2500, peaking at 1500â2000, and some outliers beyond 3000. \textit{'f0\_m\_calfat\_v1'} is left-skewed, concentrated between 12â22, peaking at 15â17, with rare values above 30. \textit{'f0\_m\_sys\_bp\_r2\_v2'} follows a near-normal distribution but shows bimodal peaks at 100 and 120, suggesting subpopulations. This visualization reveals central tendency, variability, and deviations, with varying dispersion across histograms and skewness or bimodal trends, suggesting the need for statistical analysis or transformations for better interpretability.

\subsubsection{Missing data nature}
The heatmap provides a visual representation of missing data across selected columns in the dataset (figure \ref{fig:heatmap_missing_data}). Each column corresponds to a variable, while the rows represent individual observations. Cells in the heatmap are color-coded to indicate the presence or absence of data, where lighter shades may signify missing values, and darker shades indicate available data. 


%Figure 4 : Heatmap of missing data snapshot
% Figure 3 of document pmns study 1 revision 6

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Heatmap.png} % Replace 'image.png' with your actual image filename
    \caption{Heatmap of Missing Data Snapshot}
    \label{fig:heatmap_missing_data}
\end{figure}

For instance, \textit{'f0\_m\_sys\_bp\_r1\_v1'} column exhibits higher concentrations of missing data, suggesting potential measurement issues or systematic biases during data collection. Conversely, columns with minimal or no missing values indicate robust and complete data such as \textit{'f0\_m\_su\_prepeg'}.

\subsection{Male and Female Fetus BW Comparison}
%Figure 5. Male Female fetus bw comparison
% Figure 4 of pmns study 1 revision 6 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{sex_difference.png} % Replace 'image.png' with your actual image filename
    \caption{Male Female Fetus BW Comparison}
    \label{fig:male_female_fetus_bw}
\end{figure}

Research consistently shows that male newborns have a higher average BW compared to females. Figure \ref{fig:male_female_fetus_bw}, demonstrates the distinct weight distributions between male and female infants in PMNS data population with the expected difference of average of 130 gms \cite{28A}. 

\subsubsection{Scatter plots}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Scatter_plot.png} % Replace 'image.png' with your actual image filename
    \caption{Scatter plot of Fundal height, Abdominal circumference w.r.t Birth weight}
    \label{fig:scatter_plot}
\end{figure}
%Figure 6. Scatter plot of Fundal height, Abdominal circumference w.r.t Birth weight
% Figure 5 of pmns study 1 revision 6 

The first plot (figure \ref{fig:scatter_plot}), on the left, examines the correlation between BW (in kilograms) and fundal height (in centimeters). A clear positive trend is observed, where larger fundal heights are associated with higher BW. Similarly, the second plot in figure \ref{fig:scatter_plot}, on the right, shows a positive correlation between BW and abdominal circumference (in centimeters). Both plots categorize the data into three risk groups: low-risk (blue circles), moderate-risk (green squares), and high-risk (red diamonds). The majority of the data points belong to the moderate-risk group, clustered around the middle ranges of both fundal height and abdominal circumference. Low-risk pregnancies are spread across lower BW, while high-risk cases are relatively few, appearing sparsely within specific regions.

\subsection{Feature Selector Frequency Analysis}

This study examines the top 20 combinations (table \ref{tab:Top 10 Performing Combinations of Feature Selector and ML Algorithm Pair}) of feature selectors and their corresponding frequencies. The frequency analysis reveals that the Bart feature selector is the most prevalent, appearing in 6 out of the top 20 combinations. This suggests that the Bart feature selector is a popular choice among the examined combinations. The Forward and Lasso feature selectors also appear frequently, with 4 occurrences each. In contrast, the MXM, MARS, Decision Tree, and Pearson feature selectors appear less frequently, with 2, 1, 1, and 1 occurrences, respectively.

\subsection{ML Models}
In Table \ref{tab:Top 10 Performing Combinations of Feature Selector and ML Algorithm Pair}, the BART feature selection method, coupled with MICE for handling missing data, yields the most promising results. When integrated with Gradient Boosting Regression, this approach achieves an RÂ² of 0.62173, MSE of 61821.17, and an RMSE of 248.6386 grams. The inclusion of weight and sex as additional predictors enhances the model's predictive performance, demonstrating the value of incorporating comprehensive clinical data. 

%Table 4. Top 10 performing combinations of feature selector and ml algorithm pair
% Table 4 of pmns study 1 revision 6

%\usepackage{graphicx}  % Add this in the preamble
\begin{table}[htbp]
\centering
\caption{Top 10 Performing Combinations of Feature Selector and ML Algorithm Pair}
\resizebox{\textwidth}{!}{% Automatically resizes the table to fit the page width
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Feature Selection} & \textbf{Imputation Techniques} & \textbf{ML Model} & \textbf{R\textsuperscript{2}} & \textbf{RMSE} & \textbf{Note} \\
\hline
BART & MICE & Gradient Boosting Regression & 0.6217 & 248.64 & By adding weight and sex column \\
\hline
FORWARD & MICE & Ridge Regression & 0.6107 & 252.50 & \\
\hline
FORWARD & MICE & Linear Regression & 0.6107 & 252.52 & \\
\hline
BART & MICE & Gradient Boosting Regression & 0.6103 & 251.97 & \\
\hline
BART & MICE & Random Forest Regression & 0.6096 & 252.72 & By adding weight and sex column \\
\hline
FORWARD & MICE & Lasso Regression & 0.6075 & 253.51 & \\
\hline
FORWARD & MICE & Bayesian Ridge & 0.6046 & 254.42 & \\
\hline
FORWARD & MICE & Gradient Boosting Regression & 0.6015 & 255.59 & \\
\hline
BART & MICE & Random Forest Regression & 0.5997 & 256.20 & \\
\hline
MARS & MICE & Gradient Boosting Regression & 0.5972 & 256.65 & \\
\hline
DECISION\_TREE & MICE & Gradient Boosting Regression & 0.5955 & 257.44 & \\
\hline
PEARSON & MICE & Gradient Boosting Regression & 0.5951 & 257.26 & \\
\hline
LASSO & MICE & Bayesian Ridge & 0.5884 & 259.75 & \\
\hline
LASSO & MICE & Ridge Regression & 0.5879 & 259.86 & \\
\hline
LASSO & MICE & Linear Regression & 0.5878 & 259.87 & \\
\hline
LASSO & MICE & Lasso Regression & 0.5876 & 259.94 & \\
\hline
\end{tabular}
}
\label{tab:Top 10 Performing Combinations of Feature Selector and ML Algorithm Pair}
\end{table}

The second-best performing model utilizes forward selection with LASSO and MICE imputation, paired with Ridge Regression. This combination achieves an RÂ² of 0.61074, MSE of 63756.59, and an RMSE of 252.5007 grams. Other notable models include BART with Random Forest-based MICE imputation (RÂ² = 0.60013, RMSE = 245.8073) and MARS with MICE imputation (RÂ² = 0.59722, RMSE=256.6517).

%Table 5 . Feature State-of-The-Art Feature Selection Model Evaluation (Hopular and MXM)

%\usepackage{graphicx}  % Add this in the preamble

\begin{table}[htbp]
\centering
\caption{Feature State-of-The-Art Feature Selection Model Evaluation (Hopular and MXM)}
\resizebox{\textwidth}{!}{  % Automatically resizes the table to fit within text width
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Feature Selection} & \textbf{Imputation Techniques} & \textbf{Model} & \textbf{R\textsuperscript{2}} & \textbf{MSE} & \textbf{RMSE} \\
\hline
Hopular & MICE & Random Forest Regression & 0.5416 & 75233.55 & 274.287 \\
\hline
Hopular & MICE & Gradient Boosting Regression & 0.5409 & 75049.14 & 273.951 \\
\hline
MXM & MICE & ANN Regression & 0.3828 & 100850.6 & 317.57 \\
\hline
MXM & MICE & Random Forest Regression & 0.3720 & 103202.4 & 321.251 \\
\hline
\end{tabular}
}
\label{tab:Feature State-of-The-Art Feature Selection Model Evaluation(Hopular and MXM)}
\end{table}


Table \ref{tab:Feature State-of-The-Art Feature Selection Model Evaluation(Hopular and MXM)} presents the performance of SOTA feature selection methods, Hopular and MXM, combined with MICE imputation. Hopular demonstrates superior performance, particularly with Random Forest Regression (RÂ² = 0.54160, RMSE = 274.2873 grams) and Gradient Boosting Regression (RÂ² = 0.54093, RMSE = 273.951grams). 
MXM shows lower performance with both ANN and Random Forest Regression models. These results highlight the varying effectiveness of different feature selection methods in predictive modeling for BW. 

\subsection{Predictive Feature and respective Importance}
The maternal attributes include measurements conducted at two distinct temporal intervals: the initial visit (18 weeks of gestation) and the subsequent visit (28 weeks of gestation). During the first visit, maternal blood pressure is recorded, whereas at the second visit, various other measurements are made, including fundal height, abdominal circumference, pulse rate, and weight.
Furthermore, the fasting glucose concentration is also quantified during the second visit, providing insights into the maternal metabolic condition.


% Figure 7. Correlation analysis of best features
% Figure 6 of pmns study 1 revision 6
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{correlation_analysis.png} % Replace 'image.png' with your actual image filename
    \caption{Correlation analysis of best features}
    \label{fig:correlation analysis}
\end{figure}

The correlation analysis in figure \ref{fig:correlation analysis} supports the claim that certain features are strongly associated with BW (\textit{fl\_bw}), indicating their potential predictive value. Variables such as gestational age at delivery ('f0\_m\_GA\_Del'), abdominal circumference (\textit{f0\_m\_abd\_cir\_v2}), and fundal height (\textit{f0\_m\_fundal\_ht\_v2}) show moderate to strong positive correlations with BW, highlighting their significance in understanding and predicting neonatal outcomes. Additionally, maternal weight (\textit{f0\_m\_wt\_v2}) and placental weight (\textit{f0\_m\_plac\_wt}) also exhibit meaningful correlations, suggesting their indirect role in influencing BW. 

%Fig. 8. Feature Importance in Regression analysis
% Figure 7 of pmns study 1 revision 6

\begin{figure}[H]
    \centering
    \includegraphics[width=0.54\textwidth]{feature_importance.png} % Replace 'image.png' with your actual image filename
    \caption{Feature Importance in Regression Analysis}
    \label{fig:feature_importance}
\end{figure}

In addition to the maternal characteristics, a range of fetal characteristics are also taken into consideration. These include the gestation age at the time of delivery, which is a significant indicator in determining the BW of the fetus, along with the placental weight, which plays a essential in fetal development. Furthermore, the fetus gender is also taken into consideration for analysis, as it is known to influence BW. 
Figure \ref{fig:feature_importance} illustrates the relative importance of various features in Gradient Boosting Model, highlighting that gestational age at delivery and placental weight emerge as the preeminent factors in BW prediction, collectively representing over 78\% of the model's predictive power.

\subsection{Residual analysis}

%Table 6. Residual analysis of best ML model
%  Table 6 of pmns study 1 revision 6

\begin{table}[h]
\centering
\caption{Residual Analysis of Best ML Model}
%\begin{adjustbox}{max width=\textwidth} % Adjust table width to page width
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Error Difference Bin} & \textbf{Count} & \textbf{Feature Selector} & \textbf{Percentage} & \textbf{Imputer} \\
\hline
100-500 & 490 & BART & 62.4204 & MICE \\
\hline
0-50   & 137 & BART & 17.4522 & MICE \\
\hline
50-100 & 130 & BART & 16.5605 & MICE \\
\hline
500-1000 & 28  & BART & 3.5669  & MICE \\
\hline
\end{tabular}
%\end{adjustbox}
\label{tab:Residual analysis}
\end{table}

The residual analysis elucidates notable discrepancies in the error distribution across different error bins (Table \ref{tab:Residual analysis}). A substantial proportion of data points (62.42\%) clustered within the 100-500g error range, implying a moderate degree of prediction error in this interval. In contrast, the 0-50g error bin constitutes a relatively minor percentage of data points (17.45\%), indicating a low error rate within this range. The 50-100g error bin contributes 16.56\% of the data points, representing a moderate to low error rate. Notably, only a small fraction of data points (3.57\%) reside within the 500-1000g error range, suggesting a higher error rate in this interval.
The average error introduced by the model is approximately 173.51g, denoting a moderate level of prediction error in aggregate. These findings suggest that while the model performs reasonably well for most data points, there is room for improvement, particularly in the higher error ranges.

\section{Discussion}
This study underscores the importance of feature selection in predictive modeling for BW, particularly in clinipredictive settings where interpretability is crucial. Unlike dimensionality reduction techniques such as PCA or SVD, feature selection allows for the identification of clinically relevant predictors, which enhances the utility of models in guiding prenatal care decisions \cite{6A},\cite{29A}.

The BART-based method, combined with MICE imputation, demonstrates superior performance by identifying 8 critical predictors, including maternal placental weight, gestational age at delivery, fundal height, fasting glucose, systolic blood pressure, abdominal circumference, and pulse rate. These factors, supported by clinical literature, are essential indicators of fetal development and maternal health. The inclusion of key predictors like fetal sex and maternal weight further improved model accuracy, achieving an RÂ² of 0.6217 and an RMSE of 248.64 grams. In contrast, the forward-selection method, albeit including a broader range of features, introduces noise and irrelevant variables that diminish predictive accuracy, highlighting the trade-offs between feature inclusion and overfitting.
In this research, Gradient Boosting Regression outperforms Random Forest Regression, owing to its sequential error-correction mechanism that effectively captures complex data interactions \cite{30A}. These findings align with previous research demonstrating the value of ensemble methods in predictive analytics for healthcare applications \cite{31A} \cite{32A}.

Gestational age at delivery and placental weight emerge as the most influential predictors, collectively accounting for over 78\% of the modelâs predictive power. This reinforces well-established clinipredictive evidence regarding their critical role in determining BW \cite{31A} \cite{33A}. The identified 136.8g disparity between male and female fetal BW further corroborates prior findings, emphasizing the biological and clinical relevance of fetal sex in BW estimation \cite{32A}.

Although the modelâs overall accuracy, residual analysis reveals higher errors in extreme cases, with predictions exceeding 500g error in 3.57\% of cases. These discrepancies underscore the need for refinement, particularly in high-risk populations such as preterm or growth-restricted pregnancies. Future work should focus on integrating additional clinical, demographic, and genetic predictors to address these challenges while also exploring advanced imputation strategies to enhance data quality and model reliability.

In conclusion, the study highlights the potential of feature selection and ML methods in advancing prenatal care. By identifying and validating clinically significant predictors, these models offer valuable tools for improving risk stratification and personalized interventions, ultimately contributing to better maternal and fetal outcomes.

\section{Conclusion}
This study highlights the efficacy of ML in predicting BW, a key aspect of prenatal care. By employing robust datasets, feature selection methods, and predictive models, critical predictors such as gestational age at delivery, placental weight, and fetal sex are identified as significant contributors to model accuracy. The best-performing approach, using BART-based feature selection, MICE imputation, and Gradient Boosting Regression, achieves an RÂ² of 0.6217 and an RMSE of 248.64 grams.
Gestational age and placental weight account for over 78\% of the model's predictive power, reinforcing their critical role in determining BW. Additionally, the observed 136.8g disparity between male and female BW aligns with clinical evidence, emphasizing fetal sex's relevance in BW estimation. However, residual analysis reveals higher errors in extreme cases, particularly in high-risk pregnancies, suggesting the need for further refinement and the integration of additional clinical, demographic, and genetic predictors.
The findings underscore the potential of ML models to inform prenatal care decisions, identify high-risk pregnancies, and improve maternal and fetal outcomes. As the field advances, the integration of predictive analytics into clinical workflows will likely enhance personalized care and prenatal care protocols. This study provides a foundation for future research in developing reliable, data-driven tools for prenatal care.

\section{Acknowledgment}
The authors extend their sincere gratitude to the Diabetes Unit, KEM Hospital \& Research Centre, Pune, for their extensive help with the dataset and expertise in the domain of the research. They also thank the Society for Data Science (S4DS), for providing with this project and mentoring us through out. Additionally, they are grateful to B.K. Birla College for their unwavering support and the essential academic framework that facilitated our research.

% \bibliographystyle{IEEEtran} % or any other style like plain, apalike, etc.
\bibliography{reference} % Without .bib extension

\end{document}
