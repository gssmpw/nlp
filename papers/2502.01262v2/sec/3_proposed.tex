\section{Proposed Method}
\label{sec3.proposed}

\subsection{Motivation}
\label{sec3.1.motivation}
% We investigate the reasons for the weak transferability of conventional methods and find the following problems. Conventional segmentation attack methods~\cite{gu2022segpgd, jia2023transegpgd, xie2017adversarial, agnihotri2024cospgd, chen2023rethinking, chen2023adaptive} usually generate attacked images to disrupt output predictions, similar to image classification attack methods [refs]. However, the segmentation attacks are fundamentally different from the image classification attack. In image classification, the input image typically contains a single object (representing one class), whereas in semantic segmentation, the input image can contain multiple objects from various classes, and even multiple instances of the same class (\textit{e.g.} multiple people in the image). Since traditional image classification attack methods were developed under the assumption that there is only one class of object in the input image, they do not need to account for the spatial relationships, \ie contextual information. In contrast, segmentation attacks must consider the spatial relationships between separated objects within the input image. The most intuitive approach to attacking spatial relationships is to generate an attacked image in which objects with the same class display dissimilar features; when objects with the same class exhibit different features, it becomes challenging to make correct predictions. 


We investigate the causes of weak transferability in conventional methods and identify the following issues. Conventional segmentation attacks~\cite{gu2022segpgd, jia2023transegpgd, xie2017adversarial, agnihotri2024cospgd, chen2023rethinking, chen2023adaptive} typically aim to disrupt output predictions, similar to image classification attacks ~\cite{moosavi2016deepfool, dong2018boosting, andriushchenko2020understanding, gu2021effective}. However, segmentation attacks differ fundamentally from image classification attacks. In image classification, an input image usually contains a single object representing one class. In semantic segmentation, however, the input image can contain multiple objects from different classes or multiple instances of the same class (\textit{e.g.}, multiple people). Traditional classification attack methods, developed under the assumption of a single object class, do not need to consider spatial relationships or contextual information. In contrast, segmentation attack methods must account for spatial relationships among objects within the input image. The most intuitive approach to disrupting spatial relationships is to generate an adversarial image where objects of the same class display dissimilar features, making correct predictions challenging.

\begin{figure*}
\centering
\includegraphics[width=0.90\linewidth]{fig2.flowchart.pdf}
\caption{Overall framework of FSPGD. FSPGD employs a loss function with two components: external and internal feature similarity loss. The external feature similarity loss measures similarity between intermediate-level features of the clean image and adversarial example, whereas the internal feature similarity loss compares intermediate-level feature similarity among similar objects within adversarial example.}
\label{fig:fig2}
\end{figure*}

To validate our hypothesis, we conducted experiments to visualize feature similarity in the intermediate layer, as depicted in Fig.~\ref{fig:fig1}. Using the feature vector of the bicycle wheel region (red box) as a reference feature, we generated a map comparing feature similarity with other areas, using DeepLabV3-ResNet50 as the source model and DeepLabV3-ResNet101 as the target model. As shown in Fig.~\ref{fig:fig1}(a), the clean image reveals that the reference feature is similar to those of other bicycle wheel regions (yellow and blue boxes), indicating that the network generates similar features for objects with the same class, even when they are spatially separated. Despite the attack, as shown in Figs.~\ref{fig:fig1}(b), (c), and (d), conventional methods still produce similar features in the target model. In other words, objects with the same class continue to exhibit similar features, leading to weak attack performance (producing predictions nearly identical to those for the clean image); these results show the low transferability in conventional methods. In contrast, the proposed method performs the attack by accounting for spatial relationships, resulting in feature dissimilarity between wheel regions (red, yellow, and blue boxes). Consequently, the proposed method achieves better attack performance and demonstrates superior transferability compared to conventional methods.

\subsection{Methodology}
\begin{algorithm}[t]
    \caption{Algorithm of FSPGD}
    \textbf{Input:} Clean image $\mathrm{x}$; clean image feature map $\textit{f}_{x}(\cdot)$; adversarial example feature map $\textit{f}_{a}(\cdot)$; attack iterations $T$; the maximum magnitude of adversarial perturbation $\epsilon$; step size $\alpha$; $\phi^\epsilon(\cdot)$ is a function that clips output into the range $[\mathrm{x}-\epsilon, \mathrm{x}+\epsilon]$; $\mathcal{U}(-\epsilon, \epsilon)$ is a function that initializes random noise into the range $[-\epsilon, \epsilon]$.
    \\
    \textbf{Output:} The adversarial example $\mathrm{x}_{T}^{adv}$ 
    \begin{algorithmic}[1]
    \State \textbf{Initialize} $\mathrm{x}^{adv}_{0}=\mathrm{x}+\mathcal{U}(-\epsilon,\epsilon)$
        \For {$t \leftarrow 0 \ to \ T-1$}
            \State $\lambda_{t} \leftarrow t / T $
            \State Calculate $L_{ex}$ by using Eq.(4)
            \State Calculate $L_{in}$ by using Eq.(8)
            \State $L = \lambda_{t} L_{ex} + (1 - \lambda_{t}) L_{in}$
            \State Calculate the gradient of $L$ with respect to $\textrm{x}^{adv}_{t}$
            \State Update $\textrm{x}^{adv}_{t+1}$
            $$ \textrm{x}^{adv}_{t+1} \leftarrow \textrm{x}^{adv}_{t} + \alpha \cdot sign(\triangledown_{\textrm{x}^{adv}_{t}}L)$$
            \State Clamp on $\epsilon$-ball of clean image 
            $$\textrm{x}^{adv}_{t+1} \leftarrow \phi^\epsilon(\textrm{x}^{adv}_{t+1}) $$
        \EndFor
    \end{algorithmic}
\label{alg1}
\end{algorithm}


In the proposed method, we build \textit{L} function using the intermediate layer features $f \in \mathbb{R}^{c \times N}$, where \textit{c} and \textit{N} represent the number of channels and pixels of the feature map, respectively. In the remainder of this paper, we denote by $f_{x} \in \mathbb{R}^{c \times N}$ and $f_a \in \mathbb{R}^{c \times N}$, where the intermediate feature maps extracted from x and $\textrm{x}^{adv}_{t}$, respectively. Here, to successfully perform an attack on the source model, $f_x$ and $f_a$ should be as dissimilar as possible. Additionally, to ensure that similar objects exhibit different features in the intermediate layer of target models, similar objects within $f_a$ should have dissimilar vectors. 

Based on this hypothesis, we design our framework as illustrated in Fig.~\ref{fig:fig2}. The proposed method consists of two different loss functions, \ie $L_{ex}$ and $L_{in}$, which represent the external-feature similarity loss and internal-feature similarity loss, respectively. Specifically, $L_{ex}$ is a loss function designed to minimize the similarity between $f_x$ and $f_a$, aiming to successfully perform an attack on the source model. To achieve this, we design the loss function to reduce cosine similarity between feature vectors of each pixel in $f_x$ and $f_a$, which is formulated as follows:

\begin{equation}
\label{eq4}
    L_{ex} = \frac{1}{N}\sum\limits_{i=1}^N \Big(\frac{f_x(i)}{\vert f_x(i)\vert} \Big)^\textrm{T} \frac{f_a(i)}{\vert f_a(i)\vert},
\end{equation}
where \textit{i} indicates the pixel location. This loss function is intuitive and simple, yet exhibits outstanding performance in semantic segmentation attacks. %The superiority of the proposed $L_{ex}$ will be demonstrated in the experimental section through various experiments.
% es table edit
\begin{table*}[t]
% \captionsetup{skip=2pt}  % Adjust the skip value to reduce or increase space

\caption{Attack performance comparison on Pascal VOC 2012 in terms of mIoU. Lower mIoU means better performance and bold numbers denote the best mIoU values for each experimental setup}
\begin{center}
\setlength{\tabcolsep}{10pt}


\begin{tabular}{c | c | c  c  c  c}
\Xhline{3\arrayrulewidth}
& & \multicolumn{4}{c}{Target Models (mIoU$\downarrow$)} \\
\hline
\multirow{2}*{Source Models} & Attack Method & Source Model & PSPRes101 & DV3Res101 & FCNVGG16\\
\cline{2-6}
& Clean Images & 80.22/80.18 & 78.39 & 82.88 & 59.80 \\
\hline
\multirow{8}*{PSPRes50} & PGD~\cite{mkadry2017towards} & 7.72 & 54.73& 59.41     & 45.70  \\
& SegPGD~\cite{gu2022segpgd}& 5.41  & 54.10  & 58.95    & 45.43  \\
& CosPGD~\cite{agnihotri2024cospgd} &1.84   & 56.63 & 64.37    & 45.99   \\
& DAG~\cite{xie2017adversarial}& 65.82  & 62.67 & 66.22& 38.91         \\
& NI~\cite{lin2019nesterov}&  7.71 & 33.49   & 38.52   & 32.94  \\
& DI~\cite{xie2019improving} & 6.41& 32.00 & 35.25   & 37.34    \\
& TI~\cite{dong2019evading} & 18.28 & 64.50 & 69.60& 36.80  \\
& FSPGD (Ours) & 3.39 & \textbf{22.24} & \textbf{16.84} & \textbf{19.75} \\

\hline
\multirow{8}*{DV3Res50} & PGD~\cite{mkadry2017towards}& 9.74 & 52.96 &  56.35 & 46.39 \\
& SegPGD~\cite{gu2022segpgd} & 7.26 &52.05 & 56.50 & 46.23 \\
& CosPGD~\cite{agnihotri2024cospgd} &\textbf{1.67} & 56.82 &  61.36 & 45.94 \\
& DAG~\cite{xie2017adversarial} & 66.78 & 62.12 & 66.84 & 38.77 \\
& NI~\cite{lin2019nesterov}& 9.89 & 33.86 &36.85 & 34.92 \\
& DI~\cite{xie2019improving} & 7.35 &31.93 & 32.93 &38.30 \\
& TI~\cite{dong2019evading}  & 19.34 &64.99 & 69.80 & 37.65 \\
& FSPGD(Ours) & 3.44 & \textbf{21.89} & \textbf{16.57} & \textbf{19.36} \\
% \hline
% \hline
% \multirow{2}*{Source Models} & Attack Method & Source Model & PSPRes50 & DV3Res50 & FCNVGG16\\
% \cline{2-6}
% & Clean Images & 78.39/82.88  & 80.22 & 80.18 & 59.80 \\ 
% \hline
% \multirow{4}*{PSPRes101}& PGD~\cite{mkadry2017towards} &10.13 & 55.39 &  55.39 &  47.25 \\
% & SegPGD~\cite{gu2022segpgd} & 7.31 & 53.56 & 54.03  & 46.26 \\
% & CosPGD~\cite{agnihotri2024cospgd} &\textbf{2.87} & 57.74  &  58.50  & 47.05 \\
% & DAG~\cite{xie2017adversarial} & 63.36 & 66.28  & 66.06  & 39.10 \\
% & NI~\cite{lin2019nesterov} & 10.22& 33.50   & 34.12  & 34.41 \\
% & DI~\cite{xie2019improving} &7.21 &  29.00  & 30.58 & 39.24 \\
% & TI~\cite{dong2019evading} &22.23 & 64.64  &  64.95 &  37.29 \\
% & FSPGD(Ours) & 2.99& \textbf{12.57} & \textbf{13.65} & \textbf{21.31} \\
% \hline
% \multirow{8}*{DV3Res101}& PGD~\cite{mkadry2017towards} & 9.75 & 59.36 & 55.54 & 47.48 \\
% & SegPGD~\cite{gu2022segpgd} & 7.18 &54.47  & 53.96 & 46.53 \\
% & CosPGD~\cite{agnihotri2024cospgd} & \textbf{2.73} &58.83  & 58.54 & 47.25 \\
% & DAG~\cite{xie2017adversarial} & 67.55 & 67.09 & 67.58 & 39.48 \\
% & NI~\cite{lin2019nesterov} &  9.49  &36.41 & 34.75 & 35.62 \\
% & DI~\cite{xie2019improving} & 7.64  &34.87 & 34.11 &  40.99 \\
% & TI~\cite{dong2019evading} & 27.16  & 65.79 & 65.13 & 37.98 \\
% & FSPGD(Ours) &3.28 & \textbf{11.42} & \textbf{13.45} & \textbf{21.49} \\
\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\label{table1}
\end{table*}


% \begin{table*}[t]
% \caption{Attack performance comparison on Cityscapes in terms of mIoU. Lower mIoU means better performance and bold numbers denote the best mIoU values for each experimental setup}
% \begin{center}
% \setlength{\tabcolsep}{9pt}
% \begin{tabular}{c | c |c c c c}
% \Xhline{3\arrayrulewidth}
% & & \multicolumn{3}{c}{Target Models (mIoU$\downarrow$)}\\
% \hline
% \multirow{2}*{Source Models} & Attack Method & Source Model & PSPRes101 & DV3Res101 & Mask2former Swin-S \\
% \cline{2-6}
% & Clean Images & 60.58 & 65.90 & 65.65 & 68.24 \\
% \hline
% \multirow{7}*{Segformer} & PGD~\cite{mkadry2017towards} & 1.06 &36.07 &38.25& 48.43\\
% \multirow{7}*{MiT-B0}& SegPGD~\cite{gu2022segpgd} & 0.38 & 34.56 &36.38& 49.54\\
% & CosPGD~\cite{agnihotri2024cospgd} & \textbf{0.00} & 35.92 & 37.72& 51.51\\
% & DAG~\cite{xie2017adversarial} &50.92& 33.73 & 28.77 & 55.21\\
% & NI~\cite{lin2019nesterov} & 31.14 & 61.12 & 63.57 & 66.64 \\
% & DI~\cite{xie2019improving} & 32.44&56.99 & 59.50 & 57.02\\
% & TI~\cite{dong2019evading} & 35.58 & 58.41 & 61.06 & 60.28\\
% & FSPGD (Ours)  & 1.33 & \textbf{21.16} & \textbf{22.06} & 39.92\\
% \hline
% \hline
% \multirow{2}*{Source Models} & Attack Method & Source Model & PSPRes101 & DV3Res101 & Segformer MiT-B0 \\
% \cline{2-6}
% & Clean Images & 68.24 & 65.90 & 67.16 & 60.58 \\
% \hline
% \multirow{7}*{Mask2former} & PGD~\cite{mkadry2017towards} &0.45 &45.25 &48.35 & 49.30\\
% \multirow{7}*{Swin-S} & SegPGD~\cite{gu2022segpgd} &  0.30& 45.07&48.96&49.30\\
% & CosPGD~\cite{agnihotri2024cospgd} & \textbf{0.17} &45.23 &48.35&49.30\\
% & DAG~\cite{xie2017adversarial} & 65.59 & 42.06 & 39.42 & 54.23 \\
% & NI~\cite{lin2019nesterov} & 55.07 & 64.2  & 65.94 & 56.63 \\
% & DI~\cite{xie2019improving} & 57.27&  60.7  & 62.36 & 55.04\\
% & TI~\cite{dong2019evading} &  54.76& 63.07 & 62.67 & 57.27\\
% & FSPGD (Ours)  &2.20 &\textbf{24.29} &\textbf{25.96}&\textbf{36.87}\\
% \Xhline{3\arrayrulewidth}
% \end{tabular}
% \end{center}
% \label{table2}
% \end{table*}




On the other hand, $L_{in}$ is designed to generate dissimilar features for similar objects within the image, addressing the issues discussed in Sec.~\ref{sec3.1.motivation}. We first measure the similarity of $f_a$ between each pixel and all other pixels by constructing the Gram matrix $\textrm{S}\in \mathbb{R}^{N \times N}$ as follows:

\begin{equation}
\label{eq5}
\textrm{S}(p,q) = \Big(\frac{f_a(p)}{\vert f_a(p)\vert} \Big)^\textrm{T} \frac{f_a(q)}{\vert f_a(q)\vert},
\end{equation}
where $p=1,2,...,N$ and $q=1,2,...,N$. Note that our goal is to perform the attack only on pixels corresponding to regions with similar objects, rather than on all pixels. That means, we have to identify the locations of similar objects within the clean image based on the observation that similar objects have similar features. To this end, we design a mask matrix $\textrm{M}\in \mathbb{R}^{N \times N}$ for selecting pixels containing similar objects, where M is defined as 

\begin{equation}
\label{eq6}
\textrm{M}(p,q) = \Big(\frac{f_x(p)}{\vert f_x(p)\vert} \Big)^\textrm{T} \frac{f_x(q)}{\vert f_x(q)\vert}.
\end{equation}
Here, we build M using the $f_x$ instead of $f_a$ since $f_x$ always retains the same features, regardless of the progression of the attack. Note that when $f_x(p)$ and $f_x(q)$ have similar features due to similar objects, $\textrm{M}(p,q)$ would have a high value; that means \textit{p}-th and \textit{q}-th pixels have strong spatial relationships. Indeed, since M contains numerous components (\textit{e.g.} when \textit{N} is 1,024, \ie $32\times32$ resolution, M has approximately 1 million components), it is challenging to cover all pixels correlations. Thus, we simplify M and select specific pixels by performing binarization as follows:

\begin{equation}
\label{eq7}
  \textrm{M}_B(p,q) = \begin{cases}
     1, & \text{if $\textrm{M}(p,q) > \tau$} \\    
     0, & \text{otherwise}
  \end{cases}
\end{equation}

% -----------------------------------------
\begin{table*}[t]
% \captionsetup{skip=2pt}  % Adjust the skip value to reduce or increase space
\caption{Attack performance comparison on Cityscapes in terms of mIoU. Lower mIoU means better performance and bold numbers denote the best mIoU values for each experimental setup}
\begin{center}
\setlength{\tabcolsep}{2pt}
\begin{tabularx}{\textwidth}{c|c|>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}

\Xhline{3\arrayrulewidth}
& & \multicolumn{3}{c}{Target Models (mIoU$\downarrow$)}\\
\hline
\multirow{3}*{Source Models} & Attack & Source & PSP & DV3 & PSP & DV3 & Mask2Former \\
& Method & Model & Res50 & Res50 &  Res101 & Res101 & Swin-S \\
\cline{2-8}
& Clean Images & 60.58 &64.62 & 65.65 & 65.90 & 67.16 & 68.24 \\
\hline
\multirow{7}*{SegFormer} & PGD~\cite{mkadry2017towards} &1.06& 29.94& 36.07& 31.99& 38.25& 48.43\\
\multirow{7}*{MiT-B0}& SegPGD~\cite{gu2022segpgd} &0.38& 28.45& 34.56& 29.28& 36.38& 49.54\\
& CosPGD~\cite{agnihotri2024cospgd} & \textbf{0.00} & 29.98& 35.92& 32.19& 37.72& 51.51\\
& DAG~\cite{xie2017adversarial} &50.92& 20.84& 33.73& 32.71& 28.77& 55.21\\
& NI~\cite{lin2019nesterov} &2.06& 30.27& 37.63& 30.95& 38.24& 43.75\\
& DI~\cite{xie2019improving} & 9.13& 41.92& 45.85& 43.10& 48.06& 46.78\\
& TI~\cite{dong2019evading} & 7.66& 50.60& 52.77& 52.25& 55.88& 55.59\\
& FSPGD (Ours)  &1.33& \textbf{10.09} & \textbf{14.57} & \textbf{21.16} & \textbf{22.06} & \textbf{39.92}
\\
\hline
\hline
\multirow{3}*{Source Models} & Attack & Source&PSP & DV3&  PSP & DV3 & SegFormer\\
& Method & Model & Res50 & Res50 &  Res101 & Res101 &MiT-B0 \\
\cline{2-8}
& Clean Images &  68.24 & 64.62 & 65.65& 65.90 & 67.16 & 60.58 \\
\hline
\multirow{7}*{Mask2Former} & PGD~\cite{mkadry2017towards} &0.45 & 39.41 & 45.25& 42.15& 48.35& 49.30\\
\multirow{7}*{Swin-S} & SegPGD~\cite{gu2022segpgd} &0.30& 39.97& 45.07& 42.29& 48.96& 49.40\\
& CosPGD~\cite{agnihotri2024cospgd} &\textbf{0.17} & 39.56& 45.23& 42.36& 47.43& 49.37\\
& DAG~\cite{xie2017adversarial} & 65.59& 30.69& 42.06& 32.76& 39.42& 54.23\\
& NI~\cite{lin2019nesterov} & 0.17& 42.76& 49.41& 45.06& 50.00& 45.87\\
& DI~\cite{xie2019improving} &3.53& 50.34& 53.67& 53.16& 56.59& 50.85\\
& TI~\cite{dong2019evading} &  0.85& 56.81& 59.74& 59.95& 62.69& 59.74\\
& FSPGD (Ours)  &2.20  & \textbf{15.57} &  \textbf{18.00}&\textbf{24.29} & \textbf{25.96} & \textbf{36.87 }\\
\Xhline{3\arrayrulewidth}
\end{tabularx}
\end{center}
\label{table2}
\end{table*}

% \begin{table}[t]
% \caption{Attack performance comparison on Pascal VOC 2012 in terms of mIoU.}
% \centering
% \begin{tabular}{c | c | c  c }
% \hline
% & & \multicolumn {2}{c}{Target Models (mIoU$\downarrow$)} \\
% \hline
% \multirow{2}*{} & Attack Method & PSP Res50 & DV3 Res50 \\
% \hline
% Source& Clean Images  & 80.22 & 80.18 \\
% \hline
% \multirow{3}*{PSP} & PGD~\cite{mkadry2017towards} & 55.93 & 55.39 \\
% \multirow{3}*{Res101} & SegPGD~\cite{gu2022segpgd} & 53.56 & 54.03 \\
% & CosPGD~\cite{agnihotri2024cospgd} & 57.74 & 58.50 \\
% & FSPGD (Ours) & 12.57 & 13.65 \\
% \hline
% \multirow{3}*{DV3} & PGD~\cite{mkadry2017towards} & 56.36& 55.54 \\
% \multirow{3}*{Res101} & SegPGD~\cite{gu2022segpgd} & 54.47 & 53.96 \\
% & CosPGD~\cite{agnihotri2024cospgd} & 58.83 & 58.54 \\
% & FSPGD (Ours) & \textbf{11.44} & \textbf{13.47} \\
% \hline
% \end{tabular}
% \label{table2}
% \end{table}
% \begin{table}[t]
% \caption{
% Attack performance comparison on ADE20k in terms of mIoU.}
% \centering
% \setlength{\tabcolsep}{3pt}
% \begin{tabular}{c | c | c  c  c}
% \hline
% & & \multicolumn {2}{c}{Target Models (mIoU$\downarrow$)} \\
% \hline
% & Attack & Segformer & Mask2former \\
% & Method & MiT-B0 & Swin-S \\
% \hline
% Source& Clean Images & 36.10 & 50.22 \\
% \hline
% \multirow{3}*{Segformer} & PGD~\cite{mkadry2017towards} & & \\
% \multirow{3}*{MiT-B5} & SegPGD~\cite{gu2022segpgd} & & \\
% & CosPGD~\cite{agnihotri2024cospgd}  & & \\
% & FSPGD (Ours) & & \\
% \hline
% \multirow{3}*{Mask2former} & PGD~\cite{mkadry2017towards}& & \\
% \multirow{3}*{Swin-L} & SegPGD~\cite{gu2022segpgd} & & \\
% & CosPGD~\cite{agnihotri2024cospgd} & & \\
% & FSPGD (Ours) & & \\
% \hline
% \end{tabular}
% \label{table3}
% \end{table}

where $\tau$ is an user-defined threshold value. By using Eqs.~\ref{eq5} and~\ref{eq7}, we define $L_{in}$ as follows:
\begin{equation}
\label{eq8}
    L_{in} =  \frac{1}{2}\frac{1}{{K}}\sum\limits_{p=1}^N\sum\limits_{q=1}^N \textrm{M}_B(p,q)\otimes \textrm{S}(p,q),
\end{equation}
where $\otimes$ indicates element-wise multiplication operation and \textit{K} is the number of elements with a value of 1 in the $\textrm{M}_B$ matrix (\ie $K=\sum_{p}\sum_{q}\textrm{M}_B(p,q)$). Since both $\textrm{M}_B$ and S are symmetric Gram matrices, we divided by two to avoid double-counting values (\ie 1/2 in Eq.~\ref{eq8}).


By combining Eqs.~\ref{eq4} and~\ref{eq8}, we make our objective function \textit{L} as follows:

\begin{equation}
\label{eq9}
L = \lambda_{t} L_{ex} + (1 - \lambda_{t}) L_{in},
\end{equation}
where $\lambda_{t}$ is a value that controls the balance between $L_{ex}$ and $L_{in}$. Through extensive experiments, we found that it is beneficial to use $L_{in}$ in the early stages of attack iterations to reduce feature similarity between objects of the same class, and to apply $L_{ex}$ in the later stages to reduce the similarity between $f_x$ and $f_a$. Based on these observations, we define $\lambda_{t}=t/T$. Extensive experiments on the value of $\lambda_{t}$ are provided in the ablation study and supplementary material. We summarize the algorithm of the proposed method in Algorithm~\ref{alg1}. 






% \begin{table}[t]
% \caption{Attack performance comparison on pascalvoc in terms of mIoU.}
% \centering
% \begin{tabular}{c | c | c  c c}
% \hline
% & & \multicolumn {3}{c}{Target Models (mIoU$\downarrow$)} \\
% \hline

% \multirow{2}*{} & \multirow{2}*{Attack Method} & Source & PSP & DV3 \\
% && model&  Res101 & Res101 \\
% \hline
% Source& Clean Images & 80.22/80.18 & 78.39 & 82.88 \\
% \hline
% \multirow{3}*{PSP} & PGD~\cite{mkadry2017towards} &7.72  &54.73 &59.41   \\
% \multirow{3}*{Res50} & SegPGD~\cite{gu2022segpgd} & 5.41 &54.10 & 58.95 \\
% & CosPGD~\cite{agnihotri2024cospgd} & \textbf{1.84} &  56.63 & 64.37 \\
% & FSPGD (Ours) & 3.40& \textbf{37.48}& \textbf{18.54} \\
% \hline
% \multirow{3}*{DV3} & PGD~\cite{mkadry2017towards}& 9.74 &52.96 &56.35  \\
% \multirow{3}*{Res50} & SegPGD~\cite{gu2022segpgd} & 7.26 &52.05 &56.50  \\
% & CosPGD~\cite{agnihotri2024cospgd} & \textbf{1.67} &56.82 &61.36  \\
% & FSPGD (Ours)& 3.45 &\textbf{24.76} &\textbf{18.73} \\
% \hline
% \end{tabular}
% \label{table1}
% \end{table}



