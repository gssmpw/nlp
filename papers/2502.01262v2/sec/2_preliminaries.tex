% \begin{figure*}
% \centering
% \includegraphics[width=0.95\linewidth]{fig1.citys_sim.pdf}
% \caption{Visualization of the feature similarity. We show a feature similarity map using the features of the human face area (red box) as the reference feature. In conventional methods, high feature similarity is observed with other human face (yellow and blue boxes), whereas in the proposed method, feature similarity is notably reduced. (a) Clean image, (b) PGD~\cite{mkadry2017towards} (c) SegPGD~\cite{gu2022segpgd}, (d) CosPGD~\cite{agnihotri2024cospgd}, and (e) FSPGD (Ours).}
% \label{fig:fig1}
% \end{figure*}
% es figure edit
\begin{figure*}
\centering
\includegraphics[width=0.90\linewidth]{fig1.voc_sim.pdf}
\caption{Visualization of the feature similarity. We show a feature similarity map using the features of the bicycle wheels area (red box) as the reference feature. In conventional methods, high feature similarity is observed with other bicycle wheels (yellow and blue boxes), whereas in the proposed method, feature similarity is notably reduced. (a) Clean image, (b) PGD~\cite{mkadry2017towards} (c) SegPGD~\cite{gu2022segpgd}, (d) CosPGD~\cite{agnihotri2024cospgd}, and (e) FSPGD (Ours).}
\label{fig:fig1}
\end{figure*}

\section{Preliminaries}
\label{sec2:preliminaries}
Given a source model \textit{F} with parameters $\theta$ and a clean image x with ground-truth image y, the goal of attacker is to generate an adversarial example $\textrm{x}^{adv}$ that is indistinguishable from clean image x (\ie $\vert\vert \textrm{x}^{adv}-\textrm{x} \vert\vert_{p} \leq \epsilon$) but can fool the source model $F(\textrm{x}^{adv};\theta)\neq F(\textrm{x};\theta)=\textrm{y}$. Here, $\epsilon$ indicates the perturbation budget, and $\vert\vert \cdot \vert\vert$ means the $l_p$ norm distance. In this paper, we set \textit{p} as $\infty$ following conventional methods~\cite{gu2022segpgd, jia2023transegpgd, xie2017adversarial, agnihotri2024cospgd, chen2023rethinking, chen2023adaptive}. To generate an adversarial example, the attacker typically maximizes the objective function which is defined as follows:

\begin{equation}
\label{eq1}
    \textrm{x}^{adv} = \underset{\vert\vert \textrm{x}^{adv}-\textrm{x} \vert\vert_{p} \leq \epsilon}{\mathrm{argmax}}L(\textrm{x}^{adv}, \textrm{y}; \theta),
\end{equation}
where \textit{L} is the objective function defined by the user. For instance, in\cite{goodfellow2014explaining}, $\textrm{x}^{adv}$ is generated in an intuitive manner as follows:

\begin{equation}
\label{eq2}
    \textrm{x}^{adv} = \textrm{x} + \epsilon\cdot \textrm{sign}(\nabla_\textrm{x}L(\textrm{x}, \textrm{y}; \theta)). 
\end{equation}
This approach could efficiently produce adversarial examples but show poor attack performance. In~\cite{mkadry2017towards}, they introduce an iterative attack method, called projected gradient descent (PGD), which updates the adversarial example incrementally by adding small perturbations with a step size $\alpha$, which is expressed as 
\begin{equation}
\label{eq3}
    \textrm{x}^{adv}_{t} = \textrm{x}^{adv}_{t-1} + \alpha \cdot\textrm{sign}(\nabla_{\textrm{x}^{adv}_{t}} L(\textrm{x}^{adv}_{t}, \textrm{y}; \theta)). 
\end{equation}
Since PGD method shows better performance than single-step method defined in Eq.~\ref{eq2}, following the previous papers~\cite{gu2022segpgd, agnihotri2024cospgd, xie2017adversarial, arnab2018robustness, mkadry2017towards, nokabadi2024trackpgd, savostianova2024low, waghela2024enhancing, huang2023t}, we employ the PGD as the baseline of the proposed method. 

Recently, various adversarial attack methods~\cite{gu2022segpgd, jia2023transegpgd, xie2017adversarial, agnihotri2024cospgd, chen2023rethinking, chen2023adaptive, he2023transferable} specialized for semantic segmentation have been introduced. For instance, Guo~\etal~\cite{gu2022segpgd} enhanced the existing projected gradient descent (PGD) method~\cite{mkadry2017towards}, originally developed for image classification, and demonstrated the effectiveness of the iterative attack strategy in semantic segmentation. Jia~\etal~\cite{jia2023transegpgd} tried to further improve the transferability of the method introduced in~\cite{gu2022segpgd} by designing a novel two-stage attack process. In~\cite{chen2023rethinking}, they proposed a new attack method by theoretically analyzing the limitation of the existing attack process, while Chen~\etal~\cite{chen2023adaptive} introduced a method to enhance attack transferability using an ensemble model. These methods show strong performance in the source model, but they have not yet fully overcome the challenges of transferability. More detailed explanations of related works are provided in the supplementary material.
