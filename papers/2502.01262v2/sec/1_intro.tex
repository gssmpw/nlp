
\section{Introduction}
\label{sec:intro}

Convolutional neural networks (CNNs) have shown remarkable capabilities across a range of domains, including image classification~\cite{he2016deep, simonyan2014very, huang2017densely, szegedy2015going}, semantic segmentation~\cite{chen2017rethinking, chen2018encoder, zhao2017pyramid, long2015fully}, and image synthesis~\cite{park2024novel, goodfellow2020generative, park2024rethinking, rombach2022high, sagong2022conditional}, and have consistently achieved state-of-the-art performance. However, their vulnerability to adversarial attacks, which are strategically crafted perturbations that lead to misclassification or incorrect predictions, remains a significant concern. The presence of such vulnerabilities raises some issues, particularly in security-sensitive applications like autonomous driving~\cite{eykholt2018robust} and facial verification~\cite{sharif2016accessorize}. To address this problem, various adversarial attack methods have been studied~\cite{wang2024boosting, liang2023styless, xiaosen2023rethinking, chen2023adaptive, zhang2023improving, gu2022segpgd, li2020yet, mkadry2017towards, dong2019evading, lin2019nesterov, xie2019improving, dong2018boosting, guo2020backpropagating, huang2019enhancing, wang2021feature, zhang2022improving}, but it has not yet been fully resolved.


Adversarial attacks are categorized as white-box and black-box attacks~\cite{wang2024boosting, xiaosen2023rethinking}. In a white-box attack, the attacker has complete knowledge of the target model, including its architecture, parameters, and gradients, enabling precise crafting of adversarial examples. While white-box attacks show strong attack performance, they often exhibit lower transferability, limiting their effectiveness in real-world applications~\cite{wang2024boosting, dong2019evading, xie2019improving, guo2020backpropagating}. Conversely, black-box attacks assume no prior knowledge of the model structure or parameters. Instead, the attacker relies on querying the model and analyzing outputs to generate adversarial examples. Although more challenging, black-box attacks are more suitable for real-world applications where model specifics are unknown. This paper aims to analyze limitations in existing black-box attack methods and introduce a novel approach to address these challenges.

In the black-box attack, the ability of adversarial examples generated for source model to deceive target models, which is called transferability, is a crucial property. However, enhancing the transferability is challenging since different CNN models learn and represent distinct features. This variation makes it difficult for adversarial examples generated for a source model to generalize effectively to target models. To resolve this problem, various black-box attack methods, such as data~\cite{mkadry2017towards, dong2019evading, lin2019nesterov, xie2019improving, wang2021admix}, optimization~\cite{dong2018boosting, lin2019nesterov, guo2020backpropagating, long2024convergence}, feature~\cite{huang2019enhancing, wang2021feature, zhang2022improving, li2024improving, weng2023boosting} and model~\cite{zhu2021rethinking, li2023making, gubri2022lgv, zhu2022toward} perspectives, have been explored in the field of image classification. Although these methods show strong attack performance and transferability in image classification tasks, applying them directly to semantic segmentation, which requires classifying each pixel in the input image, is challenging.

To overcome this problem, various adversarial attack methods~\cite{gu2022segpgd, jia2023transegpgd, xie2017adversarial, agnihotri2024cospgd, chen2023rethinking, chen2023adaptive, jiao2023pearl} specifically designed for semantic segmentation have been introduced. While these methods show fine attack performance in semantic segmentation, they have not yet fully overcome the challenges of transferability. In this study, we analyze the reasons for the weak transferability of existing methods and identify the following causes: conventional methods usually calculate gradients and generate perturbations by using the output predictions of the source model. This approach exhibits strong attack performance only on the source model but fail to achieve similar performance on new target models. This limitation arises because these methods only consider pixel-wise predictions and do not effectively attack contextual information, \ie the spatial relationships between objects, which is a critical factor in semantic segmentation.

To address this problem, this paper proposes a novel black-box attack method, called the Feature Similarity Projected Gradient Descent (FSPGD) attack, which demonstrates strong attack performance and significant transferability. Unlike existing segmentation attack methods that rely solely on output predictions from the source model to compute gradients, the proposed method calculates gradients by leveraging features extracted from the intermediate layer. Specifically, we develop a novel loss function that targets local information by comparing features between clean images and adversarial examples, while also disrupting contextual information by leveraging spatial relationships between objects within the image. To validate the superiority of the proposed method, we present extensive experimental results across a variety of models, such as PSPNet-ResNet50~\cite{zhao2017pyramid}, DeepLabv3-ResNet50~\cite{chen2017rethinking}, SegFormer-MiT B0~\cite{xie2021segformer}, and Mask2Former-Swin S~\cite{cheng2022masked}. Moreover, a series of ablation studies are conducted to highlight the robust generalization capabilities of the proposed method. Quantitative evaluations clearly show that the proposed method not only achieves strong attack performance but also surpasses conventional methods in transferability, setting a new state-of-the-art benchmark. Our contribution can be summarized as follows:
 
\begin{itemize}
    \item We investigate the causes of weak transferability in existing segmentation attack methods and propose a novel method, called FSPGD, to address this issue.
    \item This paper is the first to apply intermediate feature attacks to the field of semantic segmentation. Through various experiments, we prove that intermediate feature attacks are effective not only in image classification but also in semantic segmentation.
    \item We perform extensive experiments on multiple baseline models and datasets to validate the superiority of the proposed method. In addition, we perform various ablation studies to demonstrate the generalization capability of the proposed method.
\end{itemize}

