\section{Background and Related Work}
\subsection{Neural Scaling Laws}
\vspace{-0.2cm}
Previous research has shown that the performance of Transformer-based ____ models at scale can be empirically predicted with three fundamental variables: the model size $N$, the training data size $T$, and the compute budget $B$ ____. This can be summarized by modeling the change in the cross-entropy loss $L$ when varying each variable independently:
\begin{equation}
    L(x) = L_{\infty} + {\beta_x}{x}^{\alpha_x},
    \label{eq:scaling_loss}
\end{equation}
where $x \in (N, T, B)$, $L(x)$ is the reducible loss that obeys the power-scaling law, and $L_{\infty}$ is irreducible loss. $\beta$ and $\alpha$ are thus the empirically learned variables of the power law. Varying the value of $x$ allows a practitioner to estimate the scaling behavior in different settings. When $x=N$\footnote{We assume that the model parameters are equally distributed between the encoder and decoder for encoder-decoder architectures. Otherwise, the law can also be formulated as a bivariate function w.r.t. to the encoder parameters $N_e$ and decoder parameters $N_d$  ____}, for example, the power law models the data-rich ($T \rightarrow \infty$) and compute-rich ($B \rightarrow \infty$) setting. Previous work ____ in language model re-scoring has shown that the Word Error Rate (WER) can also be modeled as a power law function of $x$. We can thus modify Equation \ref{eq:scaling_loss} as follows:
\begin{equation}
    \textsc{WER}(x) = {\beta_x}{x}^{\alpha_x}.
    \label{eq:scaling_wer}
\end{equation}
We empirically show that this power law can also generalize to the multi-modal task of ASR (Figures \ref{fig:scaling_param} and \ref{fig:time_vs_wer_vs_param}), allowing true downstream performance to be easily predicted when $x=N,B$. Furthermore, we also observe that it can be applied to ST (via $\textsc{BLEU}(x) = {\beta_x}{x}^{\alpha_x}$) and thus extends our findings to more tasks (Figures \ref{fig:param_vs_st_to_x} and  \ref{fig:param_vs_st_to_en}). 

\vspace{-0.2cm}
\subsection{Scaling Laws for text and vision}
\vspace{-0.2cm}

The impact of scaling neural models has been thoroughly studied in the domains of text and vision. Early studies in scaling text models focused on supervised tasks such as machine translation (MT) ____. The most relevant work to ours is from ____, who devised scaling laws for multilingual MT models. However, these are only trained on two translation tasks/languages. In comparison, our work evaluates on over 100 languages and tasks.

Later studies focused instead on scaling self-supervised LLMs ____. ____ empirically showed that language modeling obeys a power law w.r.t $x = N, T,$ and $B$. ____ released a suite of open-access LLMs, and showed how they can be used to understand scaling behaviors on downstream tasks. Our research can be viewed as a combination of these works, albeit applied to speech: we introduce a suite of open-access large ASR/ST models and also derive scaling laws for downstream tasks.

In vision, there is existing literature on the scalability of vision encoders on image classification tasks ____. However, these tasks do not require multi-modal understanding. Our work is thus most similar to those on text-to-image/image-to-text tasks ____. However, we focus on the speech modality while also considering multi-tasking and zero-shot behaviors.

\begin{figure*}
    \centering
    \includegraphics[width=0.85\linewidth]{charts/fleurs_wer_lang_annotated_2.png}
    \vspace{-0.3cm}
    \caption{
    \textbf{The effect of scaling model size on the 102 FLEURS languages, plotted as WER (or CER) versus available training data.} Although WER/CER generally decreases with more training data, the relationship is only moderately correlated, as indicated by the RÂ² values in the legend. Model performance is also influenced by domain alignment and orthographic transparency: for instance, more transparent languages (e.g., Spanish, Italian) often achieve lower error rates with less data than opaque languages (e.g., English, French).
    }
    \vspace{-0.3cm}
    \label{fig:multilingual_param}
\end{figure*}
\vspace{-0.1cm}
\subsection{Multilingual Processing and Scaling in Speech}
\vspace{-0.2cm}

Multilingual ASR is the concept of having a single model that can recognize speech in many languages ____. While initial investigations focused on only combining a few languages together ____, modern multilingual ASR models are capable of handling hundreds, if not thousands, of languages ____. Recent SOTA multilingual speech models have begun supporting tasks in addition to ASR. Joint language prediction and speech recognition is now a common method of developing multilingual ASR models ____. Whisper-style models ____ use a system of language and task prompts to also perform language identification, speech translation, and timestamp prediction. On the other hand, the Seamless family ____ leverages task decomposition to perform ASR within a speech-to-speech translation framework. Our work focuses on Whisper-style models, as their use of task prompts allow us to easily evaluate the effects of scale on zero/few-shot performance.

There have been few studies on neural scaling laws for speech. ____ and ____ devised neural scaling laws for self-supervised acoustics models and speech language models, respectively. However, their evaluations are limited to simple probes due to the text-less nature of these models, and cannot be easily applied to typical speech tasks. ____ and ____ experimented with scaling monolingual and multilingual models respectively to 10B parameters, but the models are trained only on internal data and remain unreleased. Neither works attempt to devise empirical scaling laws nor study the enhanced capabilities of larger models.


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{charts/fleurs_param_heat_2.png}
    \vspace{-0.4cm}
    \caption{\textbf{The effect of model scaling on WER/CER on FLEURS.} Languages are color-coded by the amount of training data. For readability, we only show the top-20 languages (by data amount) in our training corpus. We find that model scaling is consistently predictive of downstream WER/CER across languages. Scaling curves for other languages can be found in Figure \ref{fig:scaling_param_appendix} in the Appendix.}
    \vspace{-0.4cm}
    \label{fig:scaling_param}
\end{figure*}

\vspace{-0.2cm}