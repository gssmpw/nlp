
\subsection{LLM-Based Verilog Code Generation}
\label{sec:code_generation}


%In this work, we use design descriptions in English as input prompt to the GPT4\cite{OpenAI2023GPT-4} to generate Verilog code, followed by syntax checking of the generated code. If the Verilog code generated by the LLM passes the syntax check, the syntax-correct code is sent to the prediction stage. Otherwise, the error messages from the syntax checker and the corresponding erroneous parts of the LLM-generated Verilog code are placed into a correction prompt template. This correction prompt is then fed back into the LLM to regenerate the Verilog code, followed by another syntax check. Previous studies have explored the impact of prompt styles on the correctness of Verilog code generated by LLMs \cite{lu2024RTLLM}\cite{liu2023VerilogEval}. In this work, we refer to a prompt technique known as self-planning\cite{lu2024RTLLM} as our prompt template, as shown in Figure \ref{fig:input_prompt}.


As shown in Figure~\ref{fig:workfolw1}, the LLM-based Verilog code generation primarily has two components:
(1) The LLM takes English prompts as input and produces corresponding Verilog code; (2) Syntax check is performed on the code and revised prompts with feedback is fed to the LLM again for producing improved code.
The LLMs for Verilog code generation can be obtained from either existing closed-source models, such as ChatGPT~\cite{OpenAI2023GPT-4} and Gemini~\cite{Google2024Gemini}, or fine-tuning open-source models, such as Llama~\cite{meta2024Llama}. A recent analysis in \cite{Liu2024RTLCoder} shows that the best results so far were obtained from GPT4~\cite{OpenAI2023GPT-4}, a closed-source model. Therefore, we focus on using closed-source models with prompt engineering enhancements. Although the focus of our study here is closed-source models, our methodology is general and can work with fine-tuned open-source models as well.



\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/inputprompt8.pdf}
    \caption{Examples of the regulation template (left) and regulated prompts following the template(right).}
    \label{fig:input_prompt}
\end{figure}

We propose a new prompting methodology based on existing techniques. The first component of our methodology is 
{\bf Regulated Prompting}, which is derived from the self-planning technique proposed in \cite{lu2024RTLLM} and similar but different from the template approach in \cite{liu2023VerilogEval}. Examples of the regulation template and corresponding regulated prompts are provided in Figure~\ref{fig:input_prompt}. In self-planning~\cite{lu2024RTLLM}, an initial prompt asks the LLM to provide design knowledge about the intended circuit, which is similar as the template here and fed to the LLM for the Verilog generation. Since Lorecast is to provide circuit designs with quick estimates of performance and power, instead of helping a layman to design circuits, its users normally have sufficient experience to write such regulated prompting rather than the self-planning by LLM. The prompt templates in Lorecast are tidily formatted as in Figure~\ref{fig:input_prompt}, as opposed to the free narrative style templates in \cite{liu2023VerilogEval}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/errorfeedback5.pdf}
    \caption{Error messages from two different Verilog syntax checking tools.}
    \label{fig:Feedback}
\end{figure}


The second component is {\bf Iterative Prompting with Regulated Error Feedback (IPREF)}. IPREF is performed when there are syntax errors in the Verilog code generated by the LLM. Various tools are available for checking Verilog syntax correctness, including Icarus Verilog \cite{Williams2024Icarus}, Synopsys VCS \cite{Synopsys_VCS}, Xcelium \cite{Cadence2023Xcelium}, PyVerilog \cite{Yamazak2015Pyverilog}, and Verilator \cite{Snyder2004Verilator}. In Figure~\ref{fig:Feedback}, the error messages from Icarus and VCS for the same syntax error are demonstrated.  
In this work, we adopt Icarus Verilog for syntax checking. In IPREF, the error messages along with the Verilog code with syntax errors are sent back to the LLM as a new prompt for generating updated Verilog code. This process is repeated till there is no syntax error or the maximum limit $N$ is reached. Usually, $N$ is set to be 10 as improvement can rarely be obtained after 10 iterations. The original idea of taking error feedback for iterative prompting was introduced in \cite{Thakur2024AutoChip}. However, its feedback prompts are not regulated. By contrast, we propose regulated feedback prompting and an example of such regulation template is provided in Figure~\ref{fig:RegulatedFeedbackPrompt}.

%\subsubsection{Correction Check}\label{sec:correction_check} Verilog syntax checking is the process of verifying that Verilog code adheres to the language's syntactical rules, ensuring that the code is correctly formatted and free from syntax errors. Syntax checking is crucial, as any syntax errors in Verilog will prevent successful simulation or synthesis, potentially causing delays in the design process. 



%\subsubsection{Self-Correction} \label{sec:self_correction} Some researchers believe that, just as humans iteratively fix syntax errors to complete code, LLMs can also iteratively correct code errors \cite{Thakur2024AutoChip}\cite{Tsai2024RTLFixer}\cite{Gao2024AutoVCoder}. Drawing on this related work, we designed a  self-correction framework that uses error messages from syntax checks to iteratively correct Verilog code generated by LLMs, improving accuracy. When a syntax error is detected, the error message and the corresponding erroneous code are fed into a code correction prompt template, as shown in Figure \ref{fig:Correction Prompt Examplel}, prompting the LLM to fix the syntax error. This process repeats until the syntax is correct or the maximum number of correction iterations is reached.Compared to having the LLM generate Verilog code directly, using the self-correction framework significantly improves syntax correctness.



\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/correctprompt4.pdf}
    \caption{An example of a template for regulated feedback prompting.}
    \label{fig:RegulatedFeedbackPrompt}
\end{figure}

\iffalse

\begin{algorithm}
\caption{Generating Verilog Code with Self-Correction}
\begin{algorithmic}[1]
\State \textbf{Input:} descriptive\_prompt
\State \textbf{Initialize:} $iterations \gets 0$, $max\_iterations \gets N$
\Procedure{GenerateCode}{prompt}
    \State Send $prompt$ to LLM
    \State \Return LLM generated Verilog code
\EndProcedure
\Procedure{CheckSyntax}{code}
    \State Perform syntax check on $code$
    \If{syntax is correct}
        \State \Return True
    \Else
        \State \Return False
    \EndIf
\EndProcedure
\State $verilog\_code \gets \Call{GenerateCode}{descriptive\_prompt}$
\While{$iterations < max\_iterations$}
    \If{$\Call{CheckSyntax}{verilog\_code}$}
        \State \textbf{output} $verilog\_code$ to prediction phase
        \State \textbf{break}
    \Else
        \State Create Correction Prompt based on syntax error
        \State $iterations \gets iterations + 1$
        \State $verilog\_code \gets \Call{GenerateCode}{Correction Prompt}$
    \EndIf
\EndWhile
\If{not $\Call{CheckSyntax}{verilog\_code}$}
%     \State \textbf{output} "Unable to generate correct Verilog code after $iterations$ attempts"
% \Else
    \State \textbf{output} $verilog\_code$ to prediction phase
\EndIf
\end{algorithmic}
\label{alg:Generating Verilog}
\end{algorithm}


Algorithm \ref{alg:Generating Verilog} outlines the process of Verilog code generation and correction based on LLM.Lines 3 to 6 describe the interaction between the prompt and the LLM, lines 7 to 14 detail the syntax checking of the generated Verilog code, and lines 15 to 28 describe the process of generating Verilog code and using the LLM to iteratively correct erroneous code.

\fi