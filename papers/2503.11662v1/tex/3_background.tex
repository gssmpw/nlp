%structure of paper:
%1 intro
%2 backgroud: systolic array: mix size regularity, replace flow: not good at PE array placement, without using regularity info( 1 page)
%3 preliminary and motivation: formulation of mixed size placement problem, (better quality) -> ILP, runtime-> regularity -> reduce searching space( 1 page)
%4 method (1.5 page)
%5 experiment(1 page) 


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/workfolw15.pdf}
    \caption{Overview of the Lorecast methodology.}
    \label{fig:workfolw1}
\end{figure*}

% \thispagestyle{empty}
\section{Background}
\label{sec:background}

\subsection{Large Language Models }
\label{sec:graph_clustering}
Large Language Models (LLMs) are advanced AI systems designed to understand and generate human-like text by processing vast amounts of language data\cite{Radford2019LLM}. These models are typically based on Transformer architectures\cite{Vaswani2017Transformer} and employ billions of parameters to capture complex patterns and relationships within language, enabling them to perform tasks such as text generation, translation, summarization, and question answering\cite{Brown2020LLMLearners}. Prominent examples include OpenAI’s GPT-4 \cite{OpenAI2023GPT-4} and Google’s Gemini 1.5 Pro \cite{Google2024Gemini}, showcasing the immense capabilities of LLMs. With 175 billion parameters and a context capacity of 1 million tokens, these models achieve near-human performance across a variety of Natural Language Processing (NLP) tasks. The primary strength of LLMs lies in their pretraining on extensive text corpora, which gives them a general understanding of language that can be fine-tuned for specific tasks or domains \cite{Bommasani2022finite}\cite{Liu2024RTLCoder}\cite{Thakur2023Benchmarking}. As LLMs continue to evolve, their applications are expanding across various fields, including EDA, where they are increasingly employed for tasks such as code generation\cite{Wang2024ChatCPU}\cite{Wong2024VGV}\cite{Pei2024BetterV}.
\subsection{Verilog and Code Correctness}
Verilog is a hardware description language (HDL) widely used in digital design and circuit development for specifying and modeling electronic systems at various abstraction levels, from high-level functional descriptions to detailed structural representations \cite{Palnitkar2003Verilog}. Ensuring code correctness in Verilog is crucial because errors at the HDL level can lead to significant functional failures, performance inefficiencies, and increased costs when translated to physical hardware. Code correctness in Verilog typically includes both syntax correctness, ensuring code is free from syntax errors, and functional correctness, validating that the code’s behavior aligns with design specifications \cite{Harris2010CMOS}. Recent advances in machine learning and automated code generation, including the use of LLMs, have opened new avenues for improving Verilog code correctness through automated error detection and code synthesis\cite{Wang2024ChatCPU,Pei2024BetterV}.

\iffalse
\subsection{systolic array}
describe the pipeline and parallel computing function 
due to the repeating PE structure and its dataflow.
IO and control logic unmatch to the regular schematic of the PE array
\fi
