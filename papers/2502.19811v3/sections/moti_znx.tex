\section{Background and motivation}
Mixture of Experts (MoE) is key to efficiently scaling models. By sparsely activating different experts, MoE allows models to incorporate more parameters without increasing inference costs, leading to improved performance. This is particularly important in the era of large models. Notably, with the discovery in GPT-4-o1 that scaling inference time can further enhance model capabilities, MoEâ€™s inherent advantage in inference efficiency positions it as a critical component in the era of large-scale models.

\subsection{Joint optimization of computation and communication }
show that optimization the communication in the MoE is critical, can also breifly describe the parallism method in MoE (EP, TP)

\subsection{Mismatch in the granularity of computation and communication}

\paragraph{Complex data dependency}

\paragraph{Finegrained communication I/O}