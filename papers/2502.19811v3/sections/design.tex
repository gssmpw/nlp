\section{Design of \sysname{}}




In this section, we present the core design of \sysname{}, a Mixture of Experts (MoE) system optimized for efficient execution of MoE layers through pipelined execution and fine-grained overlapping of communication and computation.
Our analysis reveals that the MoE architecture has two distinct producer-consumer pipelines: the communication-computation pipeline and the computation-communication pipeline, as illustrated in~\autoref{fig:overview}.
Tokens traverse the pipelines as depicted and the operations within each pipeline are linked through a shared buffer, referred to as the \textbf{shared tensor}, serving as both the producer’s output buffer and the consumer’s input buffer.
% \bluetext{We find that the operations within each pipeline are linked through a shared buffer, referred to as the \textbf{shared tensor}. The shared tensor is crucial in the pipeline as it serves as both the producer's output and the consumer's input.}
To minimize overall latency and enhance pipeline performance, \sysname{} introduces two key mechanisms aimed at overlapping computation and communication workloads effectively.
% \NX{for the given input tokens, the whole workflow description is missing.}

1. \bluetext{Shared tensor based dependency resolving}: 
% The producer and the consumer are connected via a buffer called shared tensor.
% We identify that in the producer-consumer pipeline, it is crucial to enable the consumer to start execution as earlier as possible, thereby hiding the latency of the producer. 
% As stated previously, the complex data dependency between communication and computation hinders the seamless overlapping of these operations. 
% We analyze the data dependency through investigating the shared tensor. 
% Our finding is that a shared tensor can be decomposed and the corresponding computation can be rescheduled to overlap with communication.
% To this end, the dependency resolver is composed of two optimization steps on shared tensors:
As previously mentioned, the intricate data dependencies between communication and computation pose a challenge to achieving seamless overlap between these operations. To address this, we examine the data dependencies by analyzing the shared tensor. Our analysis reveals that the shared tensor can be decomposed, and the associated computations can be rescheduled to overlap more effectively with communication. Accordingly, \bluetext{the dependency resolving process} employs two key optimization strategies on the shared tensors as shown in~\autoref{fig:overview}: \ding{172} Decomposing the shared tensors along specific dimensions to break the coarse-grained data dependencies and, \ding{173} rescheduling the computations to enhance efficiency while ensuring effective overlapping. 
% Reschedule the compute sequence of operating on the shared tensors to maximize overlapping.

% To this end, the dependency resolver is responsible for determining \textbf{how to decompose and reorganize the shared tensor} and \textbf{how to reschedule the consumer to leverage the shared tensor}. Through shared tensor decomposition and permuted scheduling, the dependency resolver enables fine-grained communication and computation overlapping. 
% \ZSL{Kind of redundant.}

% 1. Dimension to decompose: For the shared tensor in various pipelines, we analyze its characteristic and decompose it alongside one or multiple dimensions, on which the data are independent and can be consumed by the proceeded operator in any order.
% 2. Decomposition granularity: The 
% 3. Schedule Permutation: Since the 

% 1. Dependency resolver: Facing the complex data dependency problem in processing sequences of tokens, the dependency resolver organizes and schedules tokens at fine granularity to enable efficient overlapping between communication and computation. The dependency resolver significantly enhances MoE computation efficiency.

2. Adaptive workload assignment:
% Once the pipelines are optimized by the dependency resolver, the communication-computation overlapping pattern turns regular. To hide the fine-grained communication latency, both the communication and the computation workload have to be assigned proper hardware resources.
% Since the communication and computation workloads perform differently with various input shapes, model configurations or hardware environments, the adaptive workload assignment scheme aims to balance the computation and communication to achieve optimal latency concealing, through generating highly-efficient horizontal-fused kernels for MoE.
Following pipeline optimization by the dependency \bluetext{resolving}, the pattern of communication-computation overlap becomes more consistent and regular. To effectively hide the fine-grained communication latency, it is essential to allocate appropriate hardware resources to both communication and computation workloads. Given that these workloads exhibit different performance characteristics depending on input shapes, model configurations, and hardware environments, the adaptive workload assignment scheme dynamically balances computation and communication. This approach generates highly efficient horizontally-fused kernels for the MoE system, thereby optimizing latency concealment.
% As shown in~\autoref{fig:overview}, the adaptive workload assignment mechanism generates highly-efficient horizontal-fused kernels for MoE.

% \textcolor{red}{Probably, we can have a short paragraph to show how the two components are actually used during MoE inference. I.e., explain the steps in Figure~\ref{fig:overview}.}
\browntext{As shown in~\autoref{fig:overview}, \sysname{} first leverages the shared tensor based dependency resolving method to optimize the pipelines in the MoE structure by decomposing and rescheduling the shared tensors. According to the reformed pipelines, \sysname{} then provides highly-efficient fused kernels through the adaptive workload assignment mechanism.}
% Facing the dynamic workloads at runtime, the workload assignment mechanism can also generate adaptive kernels for various input shapes.

% The configurations required to determine a MoE model is shown in~\autoref{tab:description}.
% In more detail, the tensor parallel size refer to the number of GPUs to split individual tensors across. For example, if $TP=2$, then each expert is split and spread to two devices for computation. 
% The expert parallel size refer to the number of mixture of experts parallel GPUs in each expert parallel group. 
% In the scenario shown in~\autoref{fig:moe_structure} where $TP=2$, $EP=2$ and $E=4$, each expert reside on two workers and each worker accommodate two experts. With $topk=1$, each token is routed to one expert with two slices on two devices. 
% An interesting perspective is that the communication and computing workloads in MoE are producers and consumers of each other. \textcolor{red}{Not substantial}

% \textbf{Computation in MoE.}
% The computation of an expert consists of two FFN layers and are connected via the activation function. The computation part is marked blue in~\autoref{fig:moe_structure}. Each FFN layer consists of multiple experts with each perform a matrix multiplication. 
% The shape of each expert's weight of the first FFN layer is $(N, K/TP)$ and the shape of the second FFN later is $(K/TP, N)$.

% \textbf{Communication in MoE.}
% The communication part is marked green as shown in~\autoref{fig:moe_structure} and can be achieved through various collective primitives. For example, to realize the purpose that experts on multiple devices can receive required tokens, AlltoAll and AllGather communication can be utilized. AlltoAll~\cite{megatron} sends wanted tokens to corresponding ranks. AllGather~\cite{megatron} gathers all tokens on each rank and then it is the receiver's own responsibility to select required tokens.
% After the expert computation, tensor parallelism requires an AllReduce communication call to aggregate the complete output of sliced experts. Then, another AlltoAll communication is required to bring back the tokens to their original devices. 
% Nevertheless, frameworks can customize the communication pattern as long as tokens are routed to the right places.
% % The AllReduce + AlltoAll combination can be replaced by AllGather + TopK Reduce+ Reduce Scatter \textcolor{red}{This is really hard to be understood without illustration. Maybe it should not be elaborated.}. 

% \begin{figure}[t]
% \centering
% 	\includegraphics[width=\columnwidth]{figures/moe_structure.pdf}
% 	\caption{\label{fig:moe_structure} The MoE structure (producer and consumer version).}
% \end{figure}

\subsection{Shared Tensor Based Dependency Resolving}
% As stated previously, the complex data dependency between computation and communication hinders the overlapping of computation and communication workloads.
% In this section, we identify the key operations in MoE that bring irregular and random patterns that hinders the overlapping of communication and overlapping.
% In this section, we propose the dependency resolver, which inserts two key operations (namely token.assemble and token.split) into the compute pipeline of MoE layers to match the communication and computation granularity, thereby enabling overlapping and latency hiding. 
\bluetext{We now introduce how to resolve the complex data dependency between computation and communication in MoE}.
% Specifically, in the communication-computation pipeline, the token.assemble method is applied to 
% The randomness of these operations causes the dependency mismatch problem happens between the producer and the consumer in MoE. 
% We then analyze the dependency at the granularity at token-level. 
% The key insight of the dependency resolver is to enable the consumer in the pipeline to start execution as earlier as possible.
% \ZSL{The key insight of the dependency resolver is ?}
It aims to bridge the granularity of communication and computation operations to sustain high efficiency by decomposing and rescheduling shared tensors.
% Besides, the computational efficiency should not be dragged down by the fine-grained communication.
% \ZSL{bridge}

\begin{figure}[t]
\centering
	\includegraphics[width=0.95\columnwidth]{figures/shared_tensor.pdf}
	\caption{\label{fig:shared_tensor} The producer-consumer modeling of layer0 (left) and layer1 (right) of an MoE layer. The global size of the shared tensor is $(M\times topk, N)$ for both layer0 and layer1.}
 % \vspace{-3mm}
\end{figure}


\subsubsection{How to decompose the shared tensor?}
Shared tensors, as the bridge between the producer operator and the consumer operator, is the key to enable overlapping. 
Notably, overlapping can occur only when the producer and consumer operate on independent data within the shared tensor, as illustrated in~\autoref{fig:shared_tensor}. Thus, we analyze the access pattern of operators on the shared tensor and decompose it along a specific dimension where data remain independent for the consumer operator.


For example, in the communication-computation pipeline in layer0, the consumer operator is a GEMM, with the shared tensor serving as its input matrix. In this case, tokens are independent with each other alongside the $M$ (token) dimension, allowing for decomposition of the shared tensor along $M$. 
However, since the computation of a GEMM tile involves multiplication and reduction along the token embedding dimension to produce the final outputs, decomposing the shared tensor along this dimension is not feasible.
% Since the computation of a GEMM tile requires multiplication and reduction alongside $N$ (token embedding dimension) to finalize outputs, it is then not suitable to split the shared tensor alongside $N$.
% \ZSL{Describe shared tensor as a per-expert concept?}
% \NX{confused}

As for the computation-communication pipeline in layer1, the consumer operator contains a top-K reduction, which reduces tokens along the $M$ dimension, leading to significant interdependencies between tokens along this dimension. Thus, the shared tensor can only be decomposed along the $N$ dimension where elements are independent.

% \textbf{Decomposition granularity.}
\subsubsection{How to reschedule the decomposed shared tensor?}
% \ZSL{In progress...}
% \NX{relationship between reorgnization and reschedule}
At the finest granularity, the shared tensor can be split into individual rows or columns, enabling the consumer to begin computation as soon as a single row or column is received.
However, this level of granularity results in low computational efficiency, particularly in pipelines involving compute-intensive GEMMs, which are typically organized and processed in tiles to achieve high utilization.
% Thus, after decomposing shared tensors along specific dimensions, the decomposed shared tensors should be reorganized and rescheduled as tiles for computation.
Therefore, after decomposing shared tensors along specific dimensions, the resulting sub-tensors must be reorganized and rescheduled into tiles for computation. The rescheduling of shared tensors follows two principles:
\ding{172} \bluetext{Rescheduled sub-tensors should align with the original computation tile granularity for computational efficiency.}
\ding{173} The scheduling policy should prioritize portions of the producer that can be immediately used by the consumer, allowing the consumer to begin execution as early as possible.
% Thus, to guarantee the compute efficiency of GEMM operators, the optimal decomposition granularity of shared tensors should not be finer than the granularity of GEMM tiles. 
% \NX{confused}


\begin{figure}[t]
\centering

\includegraphics[width=0.98\columnwidth]{figures/layer0.pdf}
 % \vspace{-2mm}
	\caption{\label{fig:layer0} Decompose and reschedule the shared tensor in MoE layer0. In this illustration, three experts are located on Rank 0, each requiring both local and remote data for computation.}
 % \vspace{-3mm}
\end{figure}

% The principle of reorganizing shared tensor considers the characteristic of \textbf{producers}. 
% The re-organization of decomposed shared tensors aims to minimize the impact of stragglers. 
% The re-organization of decomposed shared tensors aims to ensure the consumer to start execution as soon as possible.
\bluetext{\sysname{} leverages GroupGEMM to perform the computations for all experts on current rank.}
In the communication-computation pipeline (MoE layer0), the shared tensor, consumed by \bluetext{GroupGEMM}, is decomposed along the $M$ dimension. 
To enable early computation by the experts, tokens are sorted based on their source rank, as shown in~\autoref{fig:layer0}. The compute sequence of tiles \bluetext{in the GroupGEMM} is then designed to minimize dependency on remote data, with computation beginning from tiles containing local tokens while the transfer of other remote tokens proceeds concurrently.
% To enable the experts to start computation as early as possible, the tokens are sorted and processed according to their source rank as depicted in~\autoref{fig:layer0}. 
% The design of the computation sequence for tiles is intended to ensure minimal data dependency on remote data for each tile. Specifically, the computation begins from the tiles with tokens that are locally available. At the same time, other remote tokens can be transmitted concurrently.
% To start computation as early as possible, the tokens are sorted according to their source rank. After such shared tensor reorganization, the computation can begin from the tokens that are locally available, then at the same time the communication is proceeded concurrently to receive tokens from remote. \ZSL{Here.} The reorganized shared tensor is split in tiles \NX{confused}
% Then the compute sequence of tiles is permuted to avoid data transmission from remote ranks becoming the straggler when preparing data. 
% Specifically, the computation is rescheduled to first consume tokens that are locally available, and at the same time receiving the tokens from remote.


% To this end, the fine-grained inputs should be assembled together wisely for better compute kernel utilization.


% \subsubsection{How to reschedule operations on shared tensors?}
% Simply decomposing and reorganizing shared tensors is not sufficient to support efficient overlapping. The dependency resolver is also responsible for pointing out how to reschedule the operations on the reorganized shared tensors. 
% The principle is to prioritize the parts of the producer that can be utilized by the consumer. 
% We first discuss the methods in two separate scenarios and then provide a general methodology to schedule the decomposed tensors.
% \ZSL{What is the general methodology?}


% In the communication-computation pipeline in MoE, the shared tensor is consumed by GEMMs and can be decomposed at the M dimension. As shown in~\autoref{fig:layer0}, although the sub-tensors are organized in the granularity of tiles, the start time of the GEMM is not optimal because it has to wait until both local and remote tokens are received. To overlap the computation on received tokens and the communication of prefetching other tokens, the dependency resolver sort the tokens according to their source rank.
% In this way, the decomposed shared tensors are reorganized for computation. The dependency resolver aims to minimize the data dependency from remote devices for each GEMM, while ensuring the computational efficiency.


% \textbf{TopK-Reduce as the consumer.}
In the computation-communication pipeline (MoE layer1), \bluetext{the shared tensor undergoes a top-k reduction after processing by the GroupGEMM of experts.}
As analyzed previously, the shared tensor is decomposed along the $N$ dimension. 
% Thus, the top-k reduce operation can start execution as soon as a complete column of data is ready.
% With the concern of maximizing computational efficiency, the columns are also reorganized in the granularity of tiles.
\bluetext{The tile computation sequence is adjusted (\autoref{fig:layer1}) to enable the consumer operator to start processing before expert computations are fully completed}.
%\bluetext{The compute sequence of tiles is then adjusted as shown in~\autoref{fig:layer1} to ensure that the consumer operator can begin processing before the complete expert computation is finished.}
Instead of computing each expert sequentially, \bluetext{GroupGEMM} operations are executed column-wise.
This approach allows the reduction and communicate operations to proceed as soon as the first $T_N$ columns of the shared tensors are computed.
Without rescheduling, tokens could only be reduced after all experts have completed their computations.
% Once the first $T_N$ columns of the shared tensors are computed, the reduction operation can be performed accordingly.
% \NX{No corresponding description of the Figure}

% \autoref{} shows the TopK-reduce operation on a matrix. Each threadblock is assigned to compute for a tile. Since TopK-reduce is row-based, its computation is not dependent with the column dimension. Thus, the dependency resolver utilizes the \texttt{token.split} to split the token in the column dimension. To accommodate the token splitting and bring forward the consumer's execution, the computation sequence of the producer is adjusted. Instead of computing the tiles in the original zigzag sequence, the tiles are re-organized and computed in the column-prior order as shown in~\autoref{fig:zigzag}. In this way, the consumer can start execution as soon as the first column of tiles finish computation. 

% 1. Dimension to decompose: For the shared tensor in various pipelines, we analyze its characteristic and decompose it alongside one or multiple dimensions, on which the data are independent and can be consumed by the proceeded operator in any order.
% 2. Decomposition granularity: The 
% 3. Schedule Permutation: Since the 


% In the communication-computation pipeline, the producers are the communication operators which fill in the input buffers for computation. The consumers are the GEMM operators to read from the input buffers. As shown in~\autoref{}, since each input buffer includes tokens from different ranks, the communication of tokens can hardly be overlapped with GEMMs. The dependency resolver then assembles tokens with a two-step policy for better overlapping and compute kernel utilization.

% The dependency resolver decides how tokens are organized for computational kernels. Without special design, the experts can only begin computation once tokens from all ranks are received and no overlap can be attained. The dependency resolver thus organize tokens to maximize the GEMM kernel efficiency. The dependency resolver applies \texttt{token.assemble} method for efficient token gathering. \texttt{token.assemble} is applied on the $M$ dimension.


% \textbf{Sort tokens to enable overlapping.} Towards the target that each expert can start computation as soon as possible, the dependency resolver first sorts tokens according to their source rank.

% In the finest granularity, the consumer can start the computation for each token as long as it is received. However, such token-wise granularity makes low computational efficiency since GEMM is often organized and computed as tiles for high utilization. To this end, the fine-grained inputs should be assembled together wisely for better compute kernel utilization.


% We first determine the granularity of the gathered token to promise efficiency and then discuss the detailed method of gathering tokens. Tile is the minimum granularity to realize GEMM computation. To better reuse the weight and input, \sysname{} use the size of $n$ tiles as the basic problem granularity, then tokens are expected to fill in the $nT_m$ input buffer to begin computation.

% \textbf{Tile-based token scheduling.} It is intuitive that the tokens from the same source rank are better to be gathered together, thereby making the communication of other source ranks irrelevant with the computation of current computation. However, the number of tokens from a specific source rank may not fit in the problem size (e.g., too small or exceeds problem capacity and requires inefficient padding)

% To ameliorate the problem, we allow the problem input buffer to accept tokens from multiple ranks, as long as the tokens' source rank is sorted. In this way, we maintain the capability of overlapping and avoid padding. Specifically, on each device, the dependency resolver organizes tokens according to the received token indexes and assembles tokens to form problems in sequence. Each problem is formed following the principle that the number of dependent ranks is the smallest as shown in~\autoref{}. 

% \subsubsection{Token split for the comp-comm pipeline.} In the comp-comm pipeline, the producers are computational operators that write results to the local buffers and the consumers are communication operators that communicate the data in buffers with other ranks.

% From the token perspective, the communication operation cannot begin until all data of the token is prepared. However, since the elements within the token results are independent with each other, the token data can be split into finer granularity to better overlap the computation and communication. The dependency resolver thus applies \texttt{token.split} method to divide tokens. \texttt{token.split} is applied on the column dimension of $N$.

% \textbf{Specification for a common MoE layer}
% This part explains how compute patterns can be re-organized to enable overlapping.
% For a specific MoE layer, it consists the communication-computation pattern as well as the computation-communication pattern.



\begin{figure}[t]
\centering
	\includegraphics[width=0.9\columnwidth]{figures/layer1.pdf}
 % \vspace{-2mm}
	\caption{\label{fig:layer1} \bluetext{Rescheduled compute sequence for MoE layer1 ($E=3$ and $topk=3$). The execution order of the GroupGEMM is indicated by color (yellow $\rightarrow$ green $\rightarrow$ blue $\rightarrow$ grey). Here, $T_N$ denotes the tile size of a GroupGEMM along the $N$ dimension.}}
  % \vspace{-2mm}
\end{figure}


\subsection{Adaptive Workload Assignment}

With the decomposition and rescheduling of shared tensors, the pipelines in MoE can now achieve fine-grained overlap. To ensure effective latency hiding, the durations of fine-grained communication and computation must be closely aligned to minimize pipeline bubbles. Achieving this requires adaptive resource allocation for both computation and communication, tailored to specific tasks involved.
%to ensure efficient latency hiding, the workloads to be overlapped with each other are ought to be realized with high performance and avoid stragglers prolonging the overall latency in any circumstances.
%To design such adaptive fused kernels, the workloads are ought to be wrapped into different programming primitives to enjoy different granularity of hardware resources.


\begin{figure}[t]
\centering
	\includegraphics[width=0.86\columnwidth]{figures/cta_division.pdf}
	\caption{\label{fig:cta_division} Kernel design for the MoE layer1 on Hopper architecture. Each SM only accommodate one thread block. The red arrows indicates the route of data movement.}
  % \vspace{-2mm}
\end{figure}

% \begin{figure}[t]
% \centering
% 	\includegraphics[width=0.8\columnwidth]{figures/hierarchy.pdf}
% 	\caption{\label{fig:hierarchy} Programming hierarchy and kernel implementation on Hopper architecture.}
% \end{figure}

\subsubsection{Thread block specialization}
A straightforward approach to achieve communication-computation overlap in Mixture of Experts (MoE) is to encapsulate the entire pipeline within homogeneous thread blocks, integrating communication I/O into the prologue or epilogue of the computation (GEMM), a strategy referred to here as vertical fusion. 
Through vertical fusion, thread blocks execute concurrently, but the overlap occurs irregularly, leading to non-deterministic latencies of communication and computation, making it challenging to balance their durations for latency hiding.
% Through vertical fusion, thread blocks run concurrently and the overlapping happens in an irregular manner. The latency of communication and computation workloads is then non-deterministic and it is hard to control the ratio of communication and computation for optimal latency hiding. 
% Most importantly, the token-level fine-grained I/O in MoE can severely hinder the computational efficiency of the underlying kernels, especially on new-generation architectures like Hopper. Thus, we adopt a thread block level isolation among communication and computation workloads. This isolation enables us to precisely control the hardware resources allocated to different workloads. Thus, a balance point between computation and communication can be achieved, maximizing the hidden latency.
Furthermore, token-level fine-grained I/O in MoE can significantly reduce the computational efficiency of the underlying kernels, particularly on advanced architectures such as Hopper. To address this, we implement thread block-level isolation between communication and computation workloads. This isolation enables precise control over hardware resource allocation for each workload, facilitating a balanced distribution between computation and communication that maximizes latency hiding.


\autoref{fig:cta_division} depicts the details of the thread block specialized kernel \bluetext{on Hopper}, with the critical data path highlighted in red. 
% Since communication and computation are isolated, the GEMM thread blocks share the same implementation with the default GEMM compiler. In the case of~\autoref{fig:cta_division} where the GEMM is compiled using CUTLASS on the hopper architecture, the GEMM is realized using different warps. The producer warp loads data from global memory into shared memory buffer and the consumer warp launches tensor core MMA operations~\cite{cutlass}. The communication thread blocks read the results of the consumer warp from global memory. After topk-reducing, the DMA warps in the communication blocks either write tokens into the local global memory, or send them to the remote. This thread block-specialization programming model can be easily ported to other architectures, such as Ampere and Volta, by simply replacing the corresponding compute thread block implementation.
Due to the isolation between communication and computation, the GEMM thread blocks in \sysname{} utilize the same implementation as the default GEMM before fusion. In the scenario depicted in~\autoref{fig:cta_division}, where the GEMM is compiled using CUTLASS on the Hopper architecture, the GEMM execution is distributed across different warps. Specifically, the producer warp loads data from global memory into a shared memory buffer with the async TMA instructions, while the consumer warp initiates tensor core MMA operations~\cite{cutlass}.
The communication thread blocks subsequently read the results produced by the consumer warp from global memory. Following the top-K reduction, \bluetext{the warps} within the communication blocks either write tokens to the local global memory or transmit them to remote destinations. This thread block-specialized programming model is easily portable to other architectures, such as Ampere and Volta, requiring only a substitution of the respective compute thread block implementation.


% \textbf{Data movement with minimal cost.} 
\textbf{Hardware resource restriction.} 
% The introduced thread block specialized kernel is designed following the principle that the data movement cost is minimized. However, such design has to compromise with the hardware resource restriction. For example, it seems also possible to incorporate the communication DMA warps with the computation warps into same thread blocks to avoid redundant access to the global memory. 
The proposed thread block-specialized kernel is designed with the primary objective of minimizing data movement costs. However, this design must also contend with hardware resource limitations. For instance, it is theoretically feasible to integrate communication warps with computation warps within the same thread block to eliminate redundant global memory accesses.
However, the thread number restriction of warps constrict the communication operator to fully utilize the communication bandwidth.
From another perspective, the warps for communication also interfere with the computation warps within the same thread block.
% From another perspective, the granularity of warps (32 threads) hinders the flexible assignment of communication workloads.


% As discussed previously, fine-grained scheduling of tokens can hide the latency of communication or computation. However, it is non-trivial to realize highly efficient overlapping because workloads may interfere with each other in concurrent execution, especially when the loads are dynamic at runtime. To address this issue, we first adopt horizontal fusion within kernels to enable concurrent execution. The kernels are adaptive to dynamic runtime workloads.

% This part explains how to realize the overlapping after we re-organize the compute path.
% Multi-streaming often hurts rather than improves the overall performance~\cite{}. Thus, we applies horizontal fusion.

% \textbf{Compare with vertical fusion.}
% Different thread blocks launched on different SMs can run concurrently with each other. Thus, both vertical and horizontal fusion can impel the overlapping of workloads as shown in~\autoref{}.
% When overlapping is not enabled, the Gather+TopK reduce+RS operation is directly attached behind the tiled GEMM. The fine-grained I/O may be mismatched to the tile computation. For example, the tile is computed in the granularity of 128 tokens while the top-K reduce operation and later communication is in the granularity of 1 token. 

\subsubsection{Adaptive thread block assignment}
% To resolve the mismatch problem, we realize the division of labour at the level of thread blocks. 
% The division of thread blocks avoids imbalanced workloads of threads or warps and achieves optimal efficiency. We take the FFN2 kernel as an example. 
Suppose that there are $n$ thread blocks for the fused kernel, within which $n_p$ blocks serve as producers in the pipeline and $n_c$ blocks serve as consumers. 
Identifying an optimal division point $n_p/n_c$ is crucial for maximizing overall efficiency.
% It is important to find an optimal division point $n_p/n_c$ for highest efficiency.
% \textbf{Warp specialization}
% \textcolor{red}{Do we really need to point out the difference between Ampere and Hopper? Besides, is the content below repeated?}
% \textcolor{blue}{On devices with the Ampere architecture, the best practice of GEMM is to assign each tile's computation a threadblock so that large amount of threadblocks can saturate the SMs. Since multiple threadblocks can run concurrently, the communication and computation within them can overlap. With the hopper architecture, threadblocks can hardly be overlapped because each SM usually accommodate only one threadblock and it is the warp that is scheduled and overlapped with each other.}
% For different computation and communication configurations, the optimal division point varies. 
We demonstrate that the optimal division point is influenced by the shape of input and specific model configurations in an MoE layer. To investigate this, we measure the duration of MoE layer1 across various input sequence lengths and parallelization strategies, as shown in~\autoref{fig:cta_num}.
It is observed that there exist an optimal division point under different configurations. 

When the input token length changes, although the data sizes processed by communication and computation operations both scale with input length, the scalability of the respective resource requirements differs.
Consequently, the optimal division point shifts with changes in input length.
For example, when $\textit{TP}=8$, the optimal $n_c$ changes from 18 to 26 when $M$ is changed from 4096 to 16384.
When the model configuration (parallel strategy) is modified, the optimal division point undergoes a significant alteration. For instance, when $\textit{TP}$ is adjusted from 8 to 4, the optimal $n_c$ is transformed from 26 to 46 with $M=16384$.


% \sysname{}'s library consists of multiple pre-compiled kernels with different thread block division point. The optimal realization for each configuration is profiled and recorded as metadata before deploying. Then at runtime, \sysname{} selects the optimal kernel for execution according to the metadata.
\sysname{}'s library comprises multiple pre-compiled kernels, each with a distinct division point. Prior to deployment, the optimal configuration for each setup is profiled and stored as metadata. During runtime, \sysname{} utilizes this metadata to select the optimal kernel for execution.

% \textbf{Handling local and remote data}
% \textcolor{red}{\sysname{} does not classify and optimize the access of remote data and local data?}

\begin{figure}[t]
\centering
	\includegraphics[width=0.97\columnwidth]{figures/f1_cta_num.pdf}
	\caption{\label{fig:cta_num} Duration of the MoE layer1 kernel with varying number of thread blocks assigned for communication ($n_c$). The total number of thread blocks is identical to the number of SMs on Hopper(132). The figure shows four cases with different parallelisms.}
 % \vspace{-2mm}
\end{figure}
