\section{Introduction}
\label{sec:intro}

\bluetext{Recent advancements in large language models have revolutionized multiple domains, including natural language processing~\cite{vaswani2017attention, llama}, computer vision~\cite{liu2021swin} and multi-modal perception~\cite{liu2024visual, cao2023multi}. These achievements demonstrate that scaling up model size can significantly enhance model capacity.} However, the growth in model parameters poses substantial challenges for the deployment of such giant models,
as computational resources increasingly constrain model capacity~\cite{sharir2020cost}.

To this end, Mixture-of-Experts (MoE)~\cite{shazeer2017outrageously} introduces a sparse structure, within which only part of the parameters is activated. Instead of interacting with all parameters in dense models, MoE models allow each input to interact with only a few experts. 
% For instance, the computational demand for a single forward pass of the Mixtral-8x7B model~\cite{jiang2024mixtral} is equivalent to that of a 14 billion parameter model, even though the Mixtral-8x7B model comprises a total of 45 billion parameters.
For example, the Mixtral-8x7B model~\cite{jiang2024mixtral} comprises 45 billion parameters in total, while only 14 billion parameters are active during runtime. 
Nowadays, MoE has emerged as a key architecture for scaling models to trillion-plus parameters.

The increase in parameter size in MoE models allows for the integration of greater amounts of information, but it poses challenges in expert placement. 
A typical approach is to distribute the experts across different GPUs as a single GPU cannot store all experts~\cite{gshard}. Consequently, during the execution of MoE layers, there is an intensive need for data exchange among GPUs.
In the forward pass of several popular MoE models, the communication among devices accounts for $47\%$ of the total execution time on average, as shown in~\autoref{fig:overall_breakdown}(a). 


% A typical approach is to assign experts across GPUs, which is known as expert parallelism. Expert parallelism helps to accommodate enormous experts, but it requires data to be exchanged among devices intensively. 
% It is reported that over $40\%$ of training time in MoE setups is consumed by All2All communication, significantly affecting performance. 
% \NX{sounds like \sysname{} only supports EP.}

\begin{figure}[t]
\centering
	\includegraphics[width=\columnwidth]{figures/f1_overall_breakdown.pdf}
 % \vspace{-3mm}
	\caption{\label{fig:overall_breakdown} Analysis of the execution of MoE. (a) Time breakdown of MoE models executed on 8 H800 GPUs using Megatron-LM. \bluetext{(b) An illustration of communication-computation overlap by partitioning an expert computation kernel into two.}}

 % \vspace{-2mm}
\end{figure}

% \ZSL{Two system-level ways to reduce communication overhead.} 
% To reduce the communication overhead in MoE execution, a straight-forward approach is to design efficient communication primitives for MoE to reduce the amount of data to communicate among devices~\cite{schemoe, tutel} and utilize the inter-device bandwidth efficiently. 
%Specifically, the execution of a MoE layer in the distributed environment is composed of data receiving, expert computation and data sending as shown in~\autoref{fig:overall_breakdown}(b). 
In a distributed environment, executing an MoE layer involves data reception, expert computation, and data transmission, as depicted in in~\autoref{fig:overall_breakdown}(b).
To reduce communication overhead, one effective strategy is to pipeline the process, overlapping communication with expert computation ~\cite{tutel, fastermoe, pipemoe, schemoe}. This approach involves partitioning input data into smaller data chunks, allowing decomposed communication and computation phases to overlap.
%To mitigate the communication overhead, a reasonable approach is to overlap the communication with the expert computation through pipelining~\cite{tutel, fastermoe, pipemoe, schemoe}. 
%The key idea is to partition the input tokens into multiple data chunks, then the communication and computation operations can be decomposed for overlapping.
In the example in~\autoref{fig:overall_breakdown}(b), the received input data is divided into two chunks, and this coarse-grained overlapping reduces the overall execution time relative to non-pipelined execution.

% The communication in a MoE structure includes the data reception of experts where experts act as data consumers and the data sending where experts are data producers.
% The computation of experts are mainly GEneral Matrix Multiplications (GEMMs).
% To mitigate the communication overhead in MoE execution, a reasonable approach is to overlap the communication with the expert computation through pipelining~\cite{tutel, fastermoe, pipemoe, schemoe}. 
% The key idea is to partition the input tokens into multiple data chunks, then the communication and computation operations can be decomposed for overlapping. As shown in~\autoref{fig:overall_breakdown}(b), all tokens need to be process are split into two chunks and while the second data chunk is transmitting, the first data chunk can be consumed by the expert.  


%However, the overlapping in existing mechanisms is sub-optimal because two main obstacles still exist and hinder further optimizations. First, the efficiency of partitioned experts are degraded because the data chunks to handle becomes smaller, possibly causing the under-utilization of GPU computational resources 
\bluetext{The overlapping in existing mechanisms remains suboptimal due to two primary inefficiencies. 
First, the efficiency of partitioned experts declines as the data chunks assigned to each expert become smaller, potentially leading to under-utilization of GPU computational resources (e.g., the total compute time of experts after partitioning $t_1+t_2$ exceeds the original time $t$). 
% The coarse-grained partitioning also introduces inevitable GPU idle time during the initial and final phases of communication (e.g., the data receiving of chunk 1 and the data sending of chunk 2 are not overlapped with computation). Therefore, it is essential to minimize the non-overlapping time during the initial and final phases, while preserving computational efficiency. 
The coarse-grained partitioning results in unavoidable GPU idle time during the initial and final communication phases, such as when receiving data for chunk 1 and sending data for chunk 2, which do not overlap with computation. Consequently, minimizing the non-overlapping time in these phases while maintaining computational efficiency is crucial. 
This is challenging because the data dependency between communication and computation is complex and it is hard to be overlapped in a fine-grained granularity efficiently. 
%This presents a significant challenge due to the complex data dependencies between communication and computation, which make it difficult to achieve efficient fine-grained overlap.
% Second, the dynamic token routing in MoE induce various input shapes for experts at runtime, thereby posing diverse communication and computation burdens on GPUs. 
Second, due to the dynamic nature of MoE, the input shapes for experts are various at runtime, thereby posing diverse communication and computation burdens on GPUs. 
%Second, dynamic loads at runtime induce various input shapes, thereby posing diverse communication and computation burden on GPUs. 
% By encapsulating communication and computation tasks into separate kernels on different streams, the control over hardware resources is limited and the kernel performance is non-deterministic, impeding seamless overlap.
Encapsulating communication and computation tasks into separate kernels on different streams, like almost all the prior researches do, restricts control over hardware resources and results in non-deterministic kernel performance, thereby hindering seamless overlap (e.g., the computation of chunk 1 and the receiving of chunk 2 are misaligned). 
The second challenge, therefore, is to dynamically ensure precise allocation of hardware resources between computation and communication workloads at runtime.}
% Besides, because of the dynamic nature of MoE, the coarse-grained expert partitioning causes the workloads hard to be pipelined seamlessly and introduces bubbles.
% \NX{TODO: still not closely-related to the granularity mismatch. The root cause of the bubble is still due to a mismatch in the granularity or the coarse-grained overlapping? that's why we propose a finer one?, right?}
% Therefore, the first challenge is to reform the MoE pipeline to minimize these bubbles while preserving kernel computational efficiency. 
% Second, current systems achieve overlapping by encapsulating communication and computation tasks into separate kernels, asynchronously scheduled across distinct streams as shown in~\autoref{fig:overall_breakdown}(b). 
% However, the performance of these kernels is hard to be controlled because the hardware resources assigned to these kernels are determined beforehand at the compilation stage. When the input shape is varying because of dynamic loads, such a fixed programming paradigm would exacerbate the inadequate overlapping at runtime. There is also a requirement that the realization is adaptive to models and environments that have diverse communication or computation capability.
% However, controlling the performance of these kernels is challenging, as the hardware resources allocated to them are fixed and predetermined at the compilation stage. With dynamic loads causing variations in input shape~\cite{}, this fixed programming paradigm can worsen the inefficiency of overlapping at runtime, and is not adaptive to models and environments with varying communication or computation capabilities.
% However, this kernel-level scheduling approach offers limited control over hardware resource allocation between communication and computation tasks, often leading to inadequate overlapping.
% Moreover, frequent CPU-GPU context switching for kernel launching brings much overhead and is non-negligible compared with efficient kernels nowadays~\cite{reef}, especially in the MoE models.
%Therefore, the first challenge is how to reform the MoE pipeline to reduce bubbles while maintaining the computational efficiency of kernels.
%Second, to enable overlapping, existing systems capsulate communication and computation workloads into separated kernels and schedule them asynchronously onto different streams. 
% Second, to enable overlapping, existing systems throws much effort on scheduling kernels at CPU side and the frequent CPU-GPU context switch adversely impact the overall efficiency of experts. 
%However, such kernel-level scheduling is hard to control the hardware resources assigned to communication and computation workloads, inducing insufficient overlapping. 
%Thus, the second challenge is how to precisely allocate hardware resources for different workloads to ensure adaptive overlapping. This requires highly-efficient and adaptive workload assignment within kernels.

% First, the data dependency between the communication and computation operator is handled in coarse grained manner, which means that the consumer of each data chunk needs to wait until the whole chunk is filled to begin computation. We notice that although the computational pattern of divided experts is regular, the communication pattern is irregular because tokens are distributed across devices. The fine-grained token exchanging in communication operations becomes the bottleneck to fill in data chunks (e.g., the data receiving of the second data chunk becomes the straggler in the pipeline due to frequent remote memory access in the case in~\autoref{fig:overall_breakdown}(b)).
% Therefore, the first challenge is how to break the coarse-grained data dependency and avoid fine-grained communication I/O lagging down the expert computation.

% \ZSL{System summary.} 
\bluetext{The complex data dependency, and the dynamic computation and communication workloads in MoE impede existing systems to realize efficient communication-computation overlap.} We therefore propose \sysname{}, a system that enables fine-grained communication-computation overlapping for efficient MoE execution. \sysname{} introduces two key designs: 
%A dependency resolver that identifies the intricate data dependency between communication and computation operations in MoE and optimizes the pipelines in accordance.
1) A dependency resolving method that identifies complex data dependencies between communication and computation operations in MoE, enabling optimized computation-communication pipeline structuring.
2) An adaptive workload assignment method that dynamically allocates GPU thread blocks to different workloads within a kernel, balancing communication and computation to improve latency concealment.
%An adaptive workload assignment mechanism that assign GPU thread blocks to diverse workloads adaptively within a kernel to balance the communication and computation for better latency concealing.

% \ZSL{Some design details (1 and 2).} 



\sysname{} facilitates fine-grained overlapping in MoE by analyzing shared data buffers between communication and computation operations, referred to as \textit{shared tensor}. By decomposing the shared tensors along specific dimensions and reorganizing tensor data along with intra-operator execution order, \sysname{} eliminates the granularity mismatches between communication and computation, thereby enabling fine-grained overlapping.
To ensure precise resource allocation and effective latency concealment, \sysname{} integrates communication and computation tasks within fused GPU kernels. Through thread block specialization, 
% \sysname{} isolates the performance of communication and computation thread blocks
\bluetext{\sysname{} isolates the impact of communication on computation performance}
, maintaining high computational efficiency. By adjusting the number of thread blocks allocated to each workload, \sysname{} effectively balances communication and computation latencies and reduces bubbles in overlapping.
%To enable fine-grained overlapping of MoE, \sysname{} analyzes the data buffer shared between the communication and computation operations, which is represented as \textit{shared tensor}.
%We claim that when the shared tensor is decomposed along specific dimensions, by reorganizing shared tensors and rescheduling the intra-operator execution sequence, the granularity mismatch between communication and computation becomes insignificant, thus unlocking the potential of fine-grained overlapping.
%To provide appropriate resource assignment to workloads and realize precise latency concealing, \sysname{} incorporates the communication and computation operations in fused kernels. 
%Through thread block specialization, the performance of communication and computation thread blocks are isolated and maintain high efficiency. 
%Through adjusting the number of thread blocks assigned to different workloads, \sysname{} balances the communication and computation latency, thereby reducing the bubbles in overlapping.
% \ZSL{Implementation and experimental results.} 
%\TODO{relationship between the compile-based work}

We have integrated \sysname{} into Megatron-LM~\cite{megatron} and verified the capability of \sysname{} with various parallel strategies. Our extensive experiments on Nvidia H800 and L20 clusters show that \sysname{} delivers $1.96\times$ speedup for typical MoE layers, and $1.71\times$ speedup for end-to-end MoE model execution (Mixtral-8x7B~\cite{jiang2024mixtral}, Qwen2-MoE~\cite{bai2023qwen}, Phi3.5-MoE~\cite{abdin2024phi}) on average, compared with the SOTA MoE systems.  \bluetext{\sysname{} has been deployed to accelerate training and inference of large MoE models in production clusters comprising over ten thousand GPUs, achieving savings of millions of GPU hours.} 
% It vastly improves the throughput and increases cluster-level utilization. 
\sysname{} introduces a fine-grained pipelined programming model for computation and communication. \textbf{We will open-source COMET, aiming to inspire further optimizations}, such as implementing the programming model in \sysname{} using compilers like Triton~\cite{triton} or TVM~\cite{chen2018tvm}. 
% \NX{@shulai add citation here}
% \NX{highlight again: \sysname{} has been deployed on over ten-thousands GPUS, and achieve significant speedup, and we will opensource the implementation.}
% \NX{Shall we emphasize that our approach is orthogonal to Triton/compileâ€™s solution? It is possible to implement our programming paradigm using Triton. }



%In summary, our key contributions are as follows:
%\begin{itemize}
%    \item Provide detailed analysis on the data dependency in pipelines of MoE, based on which communication and computation are scheduled to overlap mutually in fine-grained.
    % \item Propose fine-grained decomposing, reorganizing and rescheduling method for shared tensors, thereby overlapping communication and computation without mutual impact and performance degradation. 
%    \item Design efficient and adaptive fused kernels to achieve latency concealing, regardless of the size of input, model configuration or hardware environment. The programming paradigm is orthogonal to the back-end compiler.
%    \item \sysname{} has been used to speedup large MoE models training and inference in the production environment of clusters with over ten-thousand GPUs. It vastly improves the throughput and increases cluster-level utilization.
%\end{itemize}


% A straightforward method to reduce the computational cost of large-scale models is through sparse activation. The Mixture of Experts (MoE) architecture, well-documented for achieving sparse activation, has proven effective in various studies.

% \textcolor{blue}{Unlike dense models that compute all parameters, MoE selectively activates only a subset, allowing for substantial model size increases without a proportionate rise in computational demands. 
% For instance, the MoE-based Mixtral-8x7B model~\cite{jiang2024mixtral} contains 46.7 billion parameters, yet only 12.9 billion parameters are activated during computations. 
% This model achieves performance comparable to Llama2-70B~\cite{llama} while operating six times faster.}
% To manage the large parameter sizes of MoE models, current training systems support distributed training by assigning experts across multiple GPUs, a method known as expert parallelism.


% Computation-communication overlapping is an important topic in large model training/(inference?). Techniques are evolving to either seeking finer granularity (TE) or using hand-crafted overlapping strategies (Centauri~\cite{centauri}). 

% These techniques do not work well in emerging dynamic networks (e.g., MoE) and when applying various parallelisms (e.g., expert parallelism), because pure fine granularity or static scheduling cannot handle dynamicity or load imbalancing at runtime.

% We have observed a trend that specialized jobs are dispatched to finer granularity (Architectural: CTA specialized - warp specialized, Compute Pipeline: op overlapping - tile overlapping), and it gives us opportunity to handle existing problems at kernel level.
