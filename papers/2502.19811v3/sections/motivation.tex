\section{Background and Motivation}

% In this section, we provide a background on Mixture-of-Experts (MoE) and introduces popular forms of parallelism for MoE models (expert parallelism and tensor parallelism). 
% We also introduce the common computaion-communication overlapping optimizations applied on large models.


\subsection{MoE Structure}

\begin{figure}[t]
\centering
	\includegraphics[width=\columnwidth]{figures/background_v4.pdf}
 % \vspace{-3mm}
	\caption{\label{fig:background}Example of an MoE layer across two GPUs, with two experts reside on GPU0 and two reside on GPU1. The MoE layer is composed of two feed-forward layers. In this example, for each token in the input buffer, it is dispatched to three experts ($topk=3$) in layer0 and then the results are combined in layer1. The shape of experts is $N\times K$ in layer0 and $K\times N$ in layer1.}
  % \vspace{-3mm}
\end{figure}

% Mixture-of-Experts (MoEs) are a family of neural network architectures with an interesting property that their parameter set can be made arbitrarily large without increasing their computational costs. This is achieved by employing multiple sparsely activated \textit{experts}, which deal with their own specialized sub-tasks respectively to solve the entire tasks together.

\begin{table}[ht]
\centering
\footnotesize
\caption{\label{tab:description} Description of symbols.}
\begin{tabular}{@{}ll@{}}
\toprule
Symbol & Description \\ \midrule
$L$      & Number of transformer layers \\
$E$      & Total number of experts \\
$topk$   & Number of experts that each token is routed to \\
{\it TP}     & Tensor parallel size             \\
{\it EP}     & Expert parallel size             \\
$W$      & Total parallel world size ({\it TP}$\times$ {\it EP})  \\
$M$      & \bluetext{ Input token length  $\times$ Batch size }           \\
$N$      & Embedding size of a token            \\
$K$      & Hidden size of the feed-forward layer in experts            \\
% $T_M$   & Tile size alongside $M$ dimension            \\
% $T_N$   & Tile size alongside $N$ dimension            \\ 
\bottomrule
\end{tabular}
 % \vspace{-2mm}
\end{table}


%Mixture of Experts (MoE) is key to efficiently scaling models. By sparsely activating different parameters of the entire model, MoE allows models to incorporate more parameters without increasing execution costs, leading to improved performance.
Mixture of Experts (MoE) is critical for efficiently scaling models. By enabling sparse activation of parameters, MoE allows for the integration of more parameters without increasing execution costs, thereby enhancing performance. 
% This is particularly important in the era of large models. Notably, with the discovery in GPT-4-o1 that scaling inference time can further enhance model capabilities, MoE’s inherent advantage in inference efficiency positions it as a critical component in the era of large-scale models.
The key idea of MoE is that it consists of multiple small models, namely \textit{experts} and tokens are only routed to partial experts for computation. \autoref{fig:background} shows the typical execution flow of an MoE layer and \autoref{tab:description} explains symbols to describe the execution of an MoE model.

% Transformer~\cite{attention}, as the state-of-the-art structure of large models to process sequence of tokens, consists of two parts in each transformer block, namely attention layer and multi-layer perception(MLP) layer. MoE is often applied onto the MLP layer, thereby introducing the dynamic property of the network.

% In the MoE structure, tokens from the MLP input buffer are routed to different devices, following several principles.
Each input token is assigned to one or more experts for computation, with assignments determined by various algorithms~\cite{stochastic, zhou2022mixture, liu2022gating}. A common method involves a gate network ~\cite{shazeer2017outrageously} that selects the $topk$ experts for each token, as shown in~\autoref{fig:background}, where token A is routed to Expert0, Expert1 and Expert3.
%to assign $topk$ experts for each token (e.g., token A is routed to Expert0, Expert1 and Expert3 in~\autoref{fig:background} with $topk=3$). 
After passing through two feed-forward layers of General Matrix Multiply (GEMM), the $topk$ outputs are gathered and reduced to produce the final result.

% The communication (dispatch) of tokens among GPUs and the first layer of Linear(GEMMs) constitute the operations in the layer0 of MoE. Therefore, MoE layer0 forms a communication-computation pipeline. The second layer of experts as well as the reduction of results (combine) constitute the operations of MoE's layer1. Then MoE layer1 forms a computation-communication pipeline. \NX{polish}

The operations in MoE’s layer0 comprise token communication (dispatch) across GPUs and the first layer of expert computations (GEMM operations), thereby establishing a communication-computation pipeline. MoE’s layer1 includes the second layer of expert computations, \bluetext{token undispatch and the topk reduction (combine)}, forming a computation-communication pipeline.

MoE employs two primary parallelization strategies: \textbf{Expert parallelism}~\cite{gshard} and \textbf{Tensor parallelism}~\cite{megatron}. In expert parallelism, the weights of different experts are distributed across separate GPUs, with each expert’s weights being fully intact. Tokens are routed to the corresponding devices of their respective experts. \autoref{fig:background} shows a case for expert parallelism, with Expert0 and Expert1 reside on GPU0 and others reside on GPU1. In contrast, tensor parallelism partitions the weights of all experts along the hidden dimension, with each GPU hosting a portion of the weights from all experts. Both expert and tensor parallelism are essential for the efficient execution of MoE. In practical deployment of MoE models, a hybrid parallelism approach combining both expert and tensor parallelism is often applied.

% \textbf{Dynamic routing}: Tokens are assigned multiple experts for computations. The assignment decision can be determined through various algorithms and a common solution is to let a Gate network~\cite{} to assign $topk$ experts for each token (e.g., token A is routed to both expert0 and expert1 in~\autoref{fig:background} with $topk=2$). \NX{which one is token-A, it is not labeled in the figure. Also you need to give the parallel details here. Such as Figure shows an example of TP=2, Attn is slices along the sequence dim(cite megatron). Note the token index range of each gpu in the figure, GPU0: 0-Seq/2, GPU1:Seq/2-Seq, and MoE is sliced along the expert hidden dim}
% The routing of tokens is random, thereby introducing the dynamic nature of MoE.

%There are two common strategies to accommodate experts for parallelization, namely expert parallelism and tensor parallelism.
%\textbf{Expert parallelism}: As proposed by GShard~\cite{gshard}, experts can be placed on different devices and tokens in a sequence are sent to the devices where their desired experts reside. \autoref{fig:background} shows a case for expert parallelism, with Expert0 and Expert1 reside on GPU0 and others reside on GPU1. As MoE models often have numerous experts, expert parallelism helps to scale up with model size.
%\textbf{Tensor parallelism}: Tensor parallelism~\cite{megatron} involves partitioning the computation of a neural network layer across GPUs. As the case of MoE, tensor parallelism allows an expert to be split along the expert hidden dimension and spread onto multiple GPUs. 
% Tensor parallelism is an efficient strategy to potentially reduce memory footprint and enlarge the MoE training scale~\cite{singh2023hybrid}.

%Expert parallelism and tensor parallelism are both indispensible for efficient MoE execution. Compared with tensor parallelism, expert parallelism can reduce the amount of data for communication because each expert only reside on one device. However, expert parallelism may encounter the load imbalance problem while tensor parallelism can alleviate such issue and potentially enlarge the MoE training scale~\cite{singh2023hybrid}. 
%In practical deployment of MoE models, a hybrid parallelism approach combining both expert and tensor parallelism is often applied. 

% The network dynamic characteristic as well as the parallel strategies both require tokens to be exchanged among distinct devices. 
% Once the required tokens are collected, the experts then perform calculations on their assigned inputs to produce the results. Specifically, the MLP layer consists of two matrix multiplications, connected via an activation function.
% After experts complete the computation, the tokens must be gathered and a top-k reduction operation should be performed. Subsequently, the tokens are unpermuted to their original positions on the original device.
% \ZSL{Claim that there exist the communication-computation pipeline as well as the computation-communication pipeline somewhere.}

\subsection{Computation and Communication Overlapping}
% \textcolor{red}{We should not mention \sysname{} in this part. We may say: It is more efficient to perform fine-grained overlapping, as it eliminates the ``bubbles'' in computation. However, the challenges are blabla and blabla.}

As the MoE architecture grows larger and sparser, the proportion of time spent on communication in MoE models becomes increasingly significant, as shown in~\autoref{fig:overall_breakdown}(a). As illustrated in \autoref{sec:intro}, coarse-grained overlapping of computation and communication offers limited optimization potential, and \browntext{kernel-level scheduling is not efficient for dynamic workloads. Thus, it is more efficient to perform the overlapping at a fine-grained granularity (such as token-wise) and integrates computation and communication workloads into fused GPU kernels.}
% Compared to previous methods, \sysname{} integrates all the computation and communication into a single fused GPU kernel, and performs the overlapping at a fine-grained granularity (such as token-wise).
Adopting such a finer-grained overlapping could extremely unleash further optimization opportunities. 
\bluetext{However, achieving such fine-grained overlapping in MoE is non-trivial and there are two primary obstacles in our observation.}
%The performance gains from independently optimizing computation or communication are limited, while overlapping computation and communication at a fine granularity within the MoE has the potential to unleash further optimizations. 
%However, achieving efficient overlapping of computation and communication in MoE is non-trivial.
% \autoref{fig:overall_breakdown} illustrates the ratio of computation to communication time for several popular MoE architectures. From the figure, it can be observed that communication time occupies approximately $47\%$ of the total execution time on average. 
% \NX{number misaligned with the introduction}.

%The key operations within a MoE layer in a distributed environment can be categorized as either communication-based or computation-based. The communication time in popular MoE models takes up around xx\% as tested and the communication becomes the bottleneck to achieve high GPU utilization. Therefore, it is crucial to optimize the communication-computation and the computation-communication pipeline in MoE. However, it is non-trivial to achieve this because of two challenges.
% \NX{logic gap here. }
% \paragraph{Granularity mismatch between computation and communication.} A key insight of \sysname{} is that the granularity of computation and communication in MoE is mismatched, making it challenging to finely overlap these two processes. In MoE, the token is the basic unit of data movement, as exemplified by the movement of Token A in \autoref{fig:background}. The purple block in \autoref{fig:background} represent computation tiles within GEMM/GroupGEMM kernels, using 128x128 as an example. The GEMM computation of one Expert might require 128 tokens distributed across different GPUs. Previously, when computation and communication were executed separately, token routing/unrouting bridged the mismatched granularities. However, when attempting to fuse computation and communication at a fine granularity, the granularity mismatch introduces several challenges.

\begin{figure*}[t]
\centering
	\includegraphics[width=1.7\columnwidth]{figures/overview_v3.pdf}
 % \vspace{-2mm}
	\caption{\label{fig:overview} Design overview of \sysname{}. \browntext{\sysname{} is composed of a shared tensor-based dependency resolving method and an adaptive workload assignment mechanism.}}
 % \vspace{-2mm}
\end{figure*}

\subsubsection{Granularity mismatch between computation and communication}
In MoE systems, the token serves as the fundamental unit of data movement, illustrated by the movement of Token A in \autoref{fig:background}. To maximize GPU compute efficiency, high-performance GEMM(GroupGEMM) kernels typically organize rows into tiles for processing. The purple block in \autoref{fig:background} represents such a computation tile in GEMM kernels, exemplified by a 128x128 tile. Therefore, the GEMM computations associated with a single expert may require 128 tokens distributed across multiple GPUs. When fusing computation and communication at fine granularity, the disparity between token-level data transfer and tile-level computation introduces considerable challenges\browntext{: The complex data dependency adversely affects the efficiency of overlap, prompting the use of fine-grained communication, while integrating fine-grained communication with computation within fused kernels is also challenging.}
% \textcolor{red}{A sentence should be added here to connect complex data dependency and fine-grained communication, with the challange of fine-grained overlapping.}

\textbf{Complex data dependency.}
\bluetext{The tokens needed for each computation tile, determined by the MoE’s gate at runtime, are randomly distributed across multiple devices. Computation for a tile cannot start until all required tokens are available.}
As shown in~\autoref{fig:background}, Expert0’s tile does not initiate processing until both Token A and Token B are received. Thus, with coarse-grained data communication, data preparation time for each computational tile may be prolonged because of this irregular and complicated data dependency. To mitigate this, we should employ fine-grained communication, where each computational tile reads or writes only the data it requires \bluetext{directly through the Unified Virtual Address~\cite{uva}}, and leverage the data reorganization and rescheduling to hide it with computation efficiently.


\textbf{Fine-grained communication.}
The integration of token-wise communication with tile-wise computation for overlapping is non-trivial. 
\bluetext{Remote I/O operations between GPUs exhibit significantly higher latency compared to local GPU memory access. Therefore, executing numerous fine-grained read and write operations on remote data tokens within computation thread blocks can block subsequent computational tasks, leading to a significant decline in kernel efficiency. This challenge is especially evident in the Hopper architecture, where computation kernels leverage Tensor Memory Accelerator (TMA) hardware instructions~\cite{tma} to establish asynchronous compute pipelines. The integration of long-latency remote I/O operations within these asynchronous pipelines can considerably prolong the overall execution time, adversely affecting performance.
Thus, it is critical to constrain the impact of fine-grained communication on computation kernels. }
%If threads in each thread block comprise both fine-grained communication instructions and computation instructions, the ensuing high communication costs can possibly block the computation, prolonging thread latency and introducing performance bottlenecks within GPU kernels.
%It is then necessary to separate communication and computation into different thread blocks for performance isolation. 
% Thus, efficient kernel implementations are needed to hide fine-grained I/O latency and minimize its impact on compute kernels.

Our first insight is that resolving the granularity mismatch between computation and communication in MoE models is the key to enable efficient overlap of these two processes.


\subsubsection{Diverse loads of computation and communication}
\browntext{Another characteristic of MoE is the dynamic routing of tokens to different experts, resulting in varying input shapes for experts at runtime (e.g., the token number received by Expert0 and Expert1 are different as shown in~\autoref{fig:background}).
This variability imposes differing communication and computation demands on GPUs.}
% As the input shape for the MoE model may vary at runtime, the communication and computation burden posed on GPUs is also different.
Besides, the hardware environments can also have various compute architectures or network topologies, providing different compute capacities and communication bandwidths. 
Achieving seamless overlap between computation and communication thus requires dynamically adjusting the allocation of GPU resources to different workloads, which is hard to be realized through wrapping workloads into separate kernels.

Our second insight is that the resource allocation should be adaptive within kernels at runtime to further achieve seamless communication-computation overlapping.

% Consequently, a mechanism capable of accurately and adaptively distributing hardware resources between communication and computation is critical.


% \textbf{Complex data dependency.}
% To ensure efficient Expert computation, GEMM is typically performed at the tile granularity, requiring each tile to wait until all necessary data is available before beginning. 
% As shown in \autoref{fig:background}, Expert 0’s tile does not initiate processing until both Token A and Token B are received. The tokens required for each tile are dynamically determined by the MoE’s gate and are randomly distributed across multiple devices. Therefore, when using coarse-grained data communication, data preparation time for each computational tile may be prolonged. This dynamic and irregular data dependency can introduce bubbles in computation overlapping. To mitigate this, we should employ fine-grained communication I/O, where each computational tile retrieves or writes only the data it requires,
% and leverage efficient data reorganization and scheduling to reduce overheads caused by complex data dependencies.

% \textbf{Fine-grained communication I/O.}
% Although fine-grained communication I/O can address data dependencies and eliminate bubbles during computation-communication overlap, its integration with compute kernels remains challenging, especially on new-generation architectures such as Hopper. Token-wise communication introduces performance bottlenecks within GPU kernels, prolonging thread latency and significantly reducing computational efficiency. To maintain the high performance of GEMM kernels, it is necessary to separate communication and computation into different thread blocks for performance isolation. Thus, efficient kernel implementations are needed to hide fine-grained I/O latency and minimize its impact on compute kernels.




%In MoE, the token is the basic unit of data movement, as exemplified by the movement of token A in \autoref{fig:background}. 
%To fully utilize the compute capability of GPUs, high-performance GEMM kernels often pack rows\footnote{Row and token are interchangeable terminologies for explanation in this paper.} in tiles for computation.
%The purple block in \autoref{fig:background} represents computation tiles within GEMM kernels, using 128x128 as an example. 
%However, the GEMM computation of one expert might require 128 tokens distributed across different GPUs.
%When attempting to fuse computation and communication at a fine granularity, the mismatch between the token-level data movement and the tile-level computation introduces several challenges.


%In order to ensure that an expert can compute at the maximum computational efficiency, a GEMM tile has to wait until all required rows are received. As illustrated in~\autoref{fig:background}, the tile of Expert0 can not begin computation until token A and token B are both received. 
%Since the required tokens are distributed across diverse GPUs, the data preparation time is potentially high when the data transmission is in coarse grained. Such an irregular data dependency between communication and computation hinders the experts to consume tokens in high efficiency.
% \textbf{Complex data dependency.}
% In order to ensure that an expert can compute at the maximum computational efficiency, a GEMM has to wait until all required rows are received, which can consume a significant amount of time. As illustrated in~\autoref{fig:background}, the tile of Expert0 can not begin computation until token A and token B are both received. 
% Thus, token A, B and other tokens in the tile are bonded as an non-splitable integration and dependent with each other to continue computation.
% However, token A is local while token B is remote and the preparation time for the data is potentially high.
% Thus, such an irregular data dependency between communication and computation hinders the experts to consume tokens efficiently.
%It is crucial to develop an efficient scheduling scheme to reduce the overhead caused by complex data dependency.


% As stated previously, the communication-computation and computation-communication pipeline can be largely optimized if the complex data dependency problem is alleviated. 
% The performance of these pipelines can be further enhanced by executing operations concurrently, thereby exploiting the substantial parallel processing capabilities of GPUs.
%Then these pipelines can be further boosted by running the operations within concurrently, leveraging the strong parallel capability of GPUs. 
% Such optimizations work well for the execution of dense models~\cite{centauri} because the communication and computation pattern is fixed. 
%To mitigate the bubbles in coarse-grained communication-computation overlap, employing fine-grained token-wise communication for data transmission could be a worthwhile attempt.
%However, when concatenated with tile-wise computation, the token-wise communication becomes the bottleneck and can easily prolong the latency of each thread, extending the overall latency of kernels.
%Thus, it is necessary to hide the fine-grained communication latency through efficient kernel realization.

% \textbf{Diverse loads of computation and communication.}
% The input sequence length may vary at runtime, leading to diverse computation and communication loads. Additionally, factors such as model configuration, hardware environment, and network topology significantly impact the practical performance of these operations.
% Achieving seamless overlap between computation and communication thus requires dynamically adjusting the allocation of GPU resources. Consequently, a mechanism capable of accurately and adaptively distributing hardware resources between communication and computation is critical.

% \textbf{Dynamic workload at runtime.} 
% The dynamic nature of MoE models leads to continuous fluctuations in the number of tokens assigned to each expert. So the computation and communication workloads are also changing at runtime. Achieving seamless overlap between computation and communication thus requires dynamically adjusting the allocation of GPU resources. Consequently, a mechanism capable of accurately and adaptively distributing hardware resources between communication and computation is critical.
%Nevertheless, the concurrent workloads (computation and communication) also have dynamic loads at runtime. 
%With the length of input sequence varying, it is challenging to design adaptive fused kernels to achieve satisfying overlapping and pipelining. Therefore, we also aim to design kernels with minimal interference between workloads, irrespective of the runtime workloads or system configurations.


%\subsection{Efficient kernel fusion}

% \textbf{Concurrent execution of workloads.}
% \ZSL{Explain why fuse operations into one kernel.}
%The basis for achieving communication and computation overlap lies in the GPU’s capacity for concurrent execution across thread blocks, utilizing diverse hardware resources.
%One common approach for this involves partitioning workloads into multiple kernels, which are then deployed across distinct CUDA streams~\cite{cuda_stream}.
%However, by delegating the workload scheduling to the hardware scheduler, concurrent kernels would compete for hardware resources and interfere with each other. As discussed in previous works~\cite{rammer, astra}, multi-streaming may hurt rather than improves the overall performance. From another perspective, frequent CPU-GPU context switching for kernel launching brings much overhead and is non-negligible compared with efficient kernels nowadays~\cite{reef}, especially in the MoE models. 
%To this end, it becomes more effective to wrap thread blocks with different workloads into a fused kernel, and reducing the cost of scheduling on the host side.

%\textbf{Adaptive horizontal fusion.}
% \ZSL{Maybe add some citations.}
% \ZSL{Explain why horizontal fusion and why adaptive.}
%To maintain the high performance of GEMM kernels, it is necessary to separate communication and computation into different thread blocks for performance isolation. Therefore, our goal is to design kernels for MoE to conceal the fine-grained communication I/O as much as possible, thereby maximizing the utilization of GPU hardware processing units.
%Nevertheless, the concurrent workloads (computation and communication) also have dynamic loads at runtime. 
%With the length of input sequence varying, it is challenging to design adaptive fused kernels to achieve satisfying overlapping and pipelining. Therefore, we also aim to design kernels with minimal interference between workloads, irrespective of the runtime workloads or system configurations.

% \subsection{Dynamic workload at runtime}
% Another important characteristic of MoE is that the workload is dynamic. Specifically, the 



% According to the implementation of Megatron~\cite{megatron}, the MoE structure consists of four basic phases and each phase is classified as either a computation-based phase or communication-based phase:

% \textbf{Routing}: The MoE layer’s first stage assigns tokens to experts. 
% Tokens traverse a Gate network, thereby generating a token-expert score matrix. Following softmax normalization, the top-k experts are chosen for each token.


% \textbf{Permutation}: After determining the expert indices for all input tokens, the Permutation phase scatters the tokens on each device to consolidate them for the same experts. The permutation phase requires to send tokens to expected devices and frameworks can customize the communication approaches to achieve this.

% \textbf{Experts}: Once the data is permuted, experts carry out calculations on their assigned inputs to generate the results. The calculations are composed of matrix multiplications and activation functions.
% % In the structure of transformers, experts are Multiple-layer perception (MLP) modules in common. This is a computation-based phase.

% \textbf{Un-permutation}: Following expert computation, the result vectors are redistributed to the original devices. Subsequently, a local gather and topk-reduce operation is performed to reorder the vectors in accordance with their original MoE input arrangement.

% \subsection{Parallel strategies}
% Experts are dynamically selected by routing to process input at runtime.

% Data, tensor, and expert parallelism are three commonly used
% parallel strategies in model execution.

% \textbf{Data parallelism} replicates the parameters of the model across all workers. Subsequently, each worker is assigned a distinct batch of samples. At this point, the workers operate on mutually exclusive shards of the input batch.

% \textbf{Expert parallelism} is a specific method of parallelism for MoE models, which is first proposed by GShard~\cite{gshard}. Experts are placed on different workers and each worker takes a different batch of samples. 
% For non-MoE layers, expert parallelism behaves the same as data parallelism. 
% In MoE layers, tokens in the sequence are sent to workers where their desired experts reside. 

% Similar to model parallelism, the outputs of each MoE layer are exchanged again to be organized back into original sequences for the computation of the next layer. As MoE models often have numerous experts, expert parallelism can scale up with model size better than model parallelism.

% \textbf{Tensor parallelism} involves partitioning the computation of a neural network layer across GPUs. Nvidia introduces MegatronLM, a tensor parallel algorithm to parallelize the computation of layers in a transformer neural network. Their method is aimed at parallelizing a pair of consecutive fully-connected layers, which are found in the self-attention and feedforward blocks of the transformer. Their algorithm has seen significant adoption for training many large language models.

% Expert parallelism and tensor parallelism are both indispensible for efficient MoE execution. Compared with tensor parallelism, expert parallelism can reduce the amount of data for communication because each expert only reside on one device. However, expert parallelism may encounter the load imbalance problem while tensor parallelism can alleviate such issue and potentially enlarge the MoE training scale~\cite{singh2023hybrid}. 
% In practical deployment of MoE models, a hybrid parallelism approach combining both data, expert and tensor parallelism are often applied. 


% \subsection{Motivations and Opportunities}
% \subsubsection{Computation-communication overlapping}


% Since computation and communication are using different hardware resources, they can be overlapped as long as the data dependency is resolved. 
% In the execution of dense models, existing approaches~\cite{centauri} recommend fine granularity scheduling. For example, a matrix is splited as tiles while communication and computation are scheduled in the granularity of tiles.


% In the execution of dynamic MoE, there are also similar methods~\cite{fastermoe, lina, pipemoe, schemoe} that recommend partitioning the computation into smaller chunks. 
% As depicted in~\autoref{fig:overlap}(b), dividing the input data into three chunks and subsequently scheduling their sub-tasks can overlap the expert computation with the communication operations to some extent.

% However, the communication overhead can still be large because a tile of the GEMM operator may require data from all ranks and the communication overhead is thus significant. 
% Consequently, it is hard to completely overlap the communication and the computation.

% \textcolor{red}{No substance here. The figure is also simple.}To further reduce the GPU idling, the ideal scheduling should hide the latency as much as possible as shown in~\autoref{fig:overlap}(c). To achieve such seamless MoE communication-computation fusion, the encountered \textbf{communication and computation mismatch} problem has to be resoloved delicately.

% To promise the high efficiency of computational kernels, the tokens are often gathered and computed in tiles to maximum the computational throughput. However, the dynamic nature of MoE’s network routing means that the tokens processed by experts continually changes, and are communicated in the granularity of single token. Consequently, we have to develop a wise scheduling scheme to manage token to maximize the computation efficiency.


% Second, since the data is divided into smaller tensors, these are unable to fully utilize the GPU’s computational capacity.


% For the execution of MoE models, traditional execution of expert parallelism requires communication during both the permutation and un-permutation phases, resulting in significant communication overhead.


% 2. Introduce the various collective primitives in MoE structure.

% The communication can be achieved though various collective primitives. For example, to realize the purpose that experts on multiple devices can receive required tokens, AlltoAll and AllGather communication can be utilized. AlltoAll~\cite{} sends wanted tokens to corresponding ranks. AllGather~\cite{} gathers all tokens on each rank and then it is the receiver's own responsibility to select required tokens.

% After the expert computation, tensor parallelism requires an AllReduce communication call to aggregate the complete output of sliced experts. Then, another AlltoAll communication is required to bring back the tokens to their original devices. The AllReduce + AlltoAll combination can be replaced by AllGather + TopK Reduce+ Reduce Scatter \textcolor{red}{This is really hard to be understood without illustration. Maybe it should not be elaborated.}. 

% \subsubsection{Multiple parallelisms}
% Why there are multiple ways of parallelism in MoE? 
% Why we need tensor parallelism for MoE?
% Tensor parallelism can ameliorate the token load unbalancing problems to some extent.
% As the configurations of MoE models vary, a single type of parallelism cannot meet the requirement of efficient training.


% \subsubsection{Computation-communication mismatch}
% Explain why traditional overlapping does not take effect for MoE structures. 

% Overlapping of permutation and expert computation: Each expert has to wait until all required tokens are received to keep on computation. However, the required tokens may reside on different ranks, and the overlapping is hard to be realized.

% Overlapping of expert computation and unpermutaion: The gather+Topk reduce operation has to wait until all rows are received. Such computation is hard to be pipelined with the reduce communication.



% \subsubsection{CUDA programming hierarchy}
% To this end, \textit{thread block specialized} programming paradigm prevails as another promising approach for concurrent workload execution~\cite{reef, rammer}.

% We summarize that there are three typical levels of programming hierarchies for concurrent execution:

% 1. \textbf{Kernels}: Workloads are spread into multiple kernels and are then kernels are launched onto different CUDA streams~\cite{}, then the computational kernels can be overlapped with communication kernels such as \texttt{cudaMemcpyAsync}.

% Specifically, a kernel consists of numerous thread blocks and workloads can be spread into multiple thread blocks. Thread blocks launched on different streaming multi-processors (SMs) can be executed concurrently. Such compute pattern that thread blocks are assigned different workloads is also named \textit{horizontal fusion}. A thread block is also named cooperative thread array (CTA).

% 3. \textbf{Thread warps}: Threads within a thread block is further organized and execute in warps (32 threads in common). Different workloads can then be processed by different warps within a thread block. Such a warp-specialized programming technique is especially recommended on the Hopper architecture~\cite{}. The producer-consumer programming mode is applied in CUTLASS.

% To obtain efficient realizations, workloads must be assigned to proper primitives for satisfying overlapping and pipelining. Delicate assignment of workloads to be kernel specialized, threadblock specialized or warp specialized is important. 
% The strategy should be adaptive to different hardware architectures.

% \subsubsection{Adaptive kernel fusion}
% We identify the computation-communication dependencies and claim that the overlapping is better optimized at runtime. Block specialized/warp specialized horizontal fusion. Introduce the compile 

% \subsubsection{Fine-grained communication}
% We can use NVSHMEM within kernels to support fine-grained communication.
% NVSHMEM is a shared memory communication library designed for NVIDIA GPUs. It creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine-grained GPU-initiated operations, CPU-initiated operations, and operations on CUDA streams. 
% Inspired by the traditional shared memory model (SHMEM), it partitions GPU memory into inter-GPU shared and private segments. 
% Unlike NCCL, which focuses on comprehensive communication operations, NVSHMEM is more composable, offering a lower-level API that enables more granular data access within kernels.

% Compared with NCCL, NVSHMEM offers finer-grained memory access, which in turn provides an opportunity to seamlessly fuse communication and computation within the kernel and between kernels.



