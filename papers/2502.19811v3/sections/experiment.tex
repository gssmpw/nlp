\section{Implementation}
% \sysname{} is built on CUDA xx and Pytorch xx. 
\sysname{} consists of approximately 12k lines of C++ and CUDA code and 2k lines of Python.
\sysname{} provides a suite of user-friendly Python APIs and developers can seamlessly integrate the APIs into their frameworks. In production environment, \sysname{} has been implemented in Megatron-LM for large-scale MoE training. The source code will be available on GitHub.
% Other software configurations are: NVSHMEM (vx.x), CUDA (v12.4), CUTLASS (v3.5).

\textbf{Optimized GEMM kernels for MoE.}
% \sysname{} leverages largely on the programming template of CUTLASS to generate highly efficient GEMM kernels. 
% \sysname{} also applies other optimizations to reduce the data movement. For example, the row indexes of input matrix of GEMM is accessed from global memory and have to be performed every K iteration in MoE layer0. \sysname{} caches the row indexes in registers, thereby reducing the cost to access global memory.
\sysname{} extensively utilizes the programming templates provided by CUTLASS to generate highly efficient GEMM kernels. Additionally, it incorporates various optimizations to minimize data movement overhead. For instance, in MoE layer 0, the row indices of the input matrix for GEMM operations must be accessed from global memory at each K iteration. By caching these row indices in registers, \sysname{} significantly reduces the global memory access cost.

% \subsection{Optimizing communication}
% \textcolor{red}{In this part, we may discuss something about communication optimization.}

\textbf{NVSHMEM as communication library.}
We employ NVSHMEM~\cite{nvshmem} within kernels to support fine-grained communication.
NVSHMEM is a communication library designed for NVIDIA GPUs. It creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine-grained GPU-initiated operations and CPU-initiated operations. 
% Inspired by the traditional shared memory model (SHMEM), it partitions GPU memory into inter-GPU shared and private segments. 
% Unlike NCCL~\cite{nccl}, which focuses on comprehensive communication operations, NVSHMEM is more composable, offering a lower-level API that enables more granular data access within kernels.
Unlike NCCL~\cite{nccl}, which targets high-level communication operations, NVSHMEM offers a more composable, low-level API that facilitates finer data access granularity within kernels.
% Compared with NCCL, NVSHMEM offers finer-grained memory access, which in turn provides an opportunity to seamlessly fuse communication and computation within the kernel and between kernels.


\section{Evaluation}

\begin{figure*}[t]
\centering
	\includegraphics[width=1.9\columnwidth]{figures/f1_overall_v2.pdf}
 % \vspace{-1mm}
	\caption{\label{fig:e2e} 
 End-to-end MoE model latency. 
 % \ZSL{Data updating...}
 % The data parallel size and expert parallel size are both equal to $W/TP$. 
 For the computation of MoE layers, the number of token on each device before permutation is $M\times W/\textit{TP}$.  
 The hatched region represents the identical duration of non-MoE (attention) layers in different mechanisms. Note that \textsc{FasterMoE} only supports expert parallelism for MoE layers.}
 % \vspace{-1mm}
\end{figure*}

\begin{table}[]
\caption{\label{tab:configuration} Configuration of MoE models used in experiments. \browntext{The models are open-sourced on Hugging Face~\cite{huggingface}. The meaning of symbols are explained in~\autoref{tab:description}.} 
% Note that to accommodate the execution of Qwen2 on all baselines, we replace its original four shared experts with normal experts.
}
\footnotesize
\centering
% \vspace{1mm}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
                 & L & E & topk & N & K \\ \hline
Mixtral 8x7B     & 32         & 8           & 2    & 4096         & 14336             \\ \hline
Qwen2-MoE-2.7B & 24         & 64          & 4    & 2048         & 1408              \\ \hline
Phi-3.5-MoE      & 32         & 16          & 2    & 4096         & 6400              \\ \hline
\end{tabular}
% \vspace{-2mm}
\end{table}

\subsection{Experimental Setup}
% \NX{the colors of the \sysname{} or the baselines in the legend are not unified across different figures.}
\textbf{Testbed.} We evaluate \sysname{} on a server equipped with 8 Nvidia H800 GPUs (80 GB memory each). These GPUs are interconnected through NVLink. Our software environment includes CUDA 12.3, NVSHMEM 2.11, Pytorch 2.4.0 and Megatron-LM (git-hash 6dbe4c).
% \ZSL{Polish this}.

\textbf{Comparing targets.} 
We then compare \sysname{} with several baselines. All baselines are implemented on Megatron-LM, which is a widely adopted framework for high-performance model execution, integrating hybrid parallel strategies.

The baselines are: 
(a) \textbf{\textsc{Megatron-Cutlass}}: Megatron with MoE experts that are implemented through CUTLASS grouped GEMM~\cite{groupedgemm}.
(b) \textbf{\textsc{Megatron-TE}}: Megatron with experts that use transformer engine~\cite{te}. Transformer Engine is Nvidia's library for accelerating transformer models on NVIDIA GPUs.
(c) \textbf{\textsc{FasterMoE}}~\cite{fastmoe, fastermoe}: FasterMoE is an MoE system that customizes All-to-All communication to overlap the communication and computation operations of experts.
(d) \textbf{\textsc{Tutel}}~\cite{tutel}: Tutel delivers several optimization techniques for efficient and adaptive MoE, including adaptive parallelism, the 2-dimensional hierarchical All-to-All algorithm and fast encode/decode with sparse computation on GPU.

% Note that only expert parallelism is adopted in \textsc{Tutel} and \textsc{FasterMoE} for MoE layers because these approaches can not realize arbitrary configurations of parallelism when expert number is greater than the world size. \NX{why so many constraints}

\subsection{Overall Performance}



% \textcolor{red}{We should explain L, E, N, K. in Table~\ref{tab:configuration}, and are the configurations are actually used in production..}

We evaluate the end-to-end performance of \sysname{} in multiple large MoE models, including Mixtral 8x7B~\cite{jiang2024mixtral}, Qwen2-MoE~\cite{bai2023qwen} and Phi3.5-MoE~\cite{abdin2024phi}. The configurations of these models are shown in~\autoref{tab:configuration}.
% \NX{TODO discuss}
% All benchmarks are implemented using the framework of Megatron to test the end-to-end latency. 
The experiment is conducted with various input token lengths and diverse hybrid parallel strategies. The experimental details and results are shown in~\autoref{fig:e2e}. 
% The input token length has two configurations with $M=\{4096, 8192\}$. The parallelism has four configurations with $(TP,EP) \in \{(8,1), (4,2), (2,4), (1,8)\}$. 
Note that when $\textit{TP}<W$, Megatron-LM enables data parallelism for non-MoE layers to improve overall throughput and the data parallel size is $W/\textit{TP}$. 
% The parallel strategy within the MoE layer is coherent with the default implementation of baselines, which is pure expert parallelism for \textsc{FasterMoE} and \textsc{Tutel} and hybrid tensor-expert parallelism for \textsc{Megatron} and \sysname{}.
% Thus, for the computation of MoE layer, the token number on each device before permutation is $M\times W/TP$. 
% We adopt a uniform token distribution for experts in our experiments. \NX{TODO}
The computation of attention layers are identical with different mechanisms using Megatron-LM, and only the MoE layer is implemented differently with diverse mechanisms. 
% \NX{FIXME}
% The duration of attention computation is hatched as shown in~\autoref{fig:e2e}. 

As observed, the end-to-end latencies of the benchmarks are reduced by $34.1\%$, $42.6\%$, $44.4\%$ and $31.8\%$ with \sysname{} compared with \textsc{Megatron-Cutlass}, \textsc{Megatron-TE}, \textsc{FasterMoE} and \textsc{Tutel} respectively. 
The performance gain is more prominent with the identical attention computation apart.
\sysname{} outperforms other baselines in all configurations because it realizes sufficient overlapping and the scheduling inside high-performance fused kernels greatly reduce the the overhead at CPU side.

Besides, we can also observe that \textsc{Megatron-Cutlass} and \textsc{Megatron-TE} perform similar. This is because they are identical except from the implementation of \bluetext{GEMM/GroupGEMM. Neither of them supports overlapping, while \textsc{Megatron-TE} performs worse in some cases because of the overhead in transformer engine API calls}.
% The implementation of TE is worse because the API calling overhead is high.
% Besides, it only supports expert-level computation-communication overlapping when each GPU accommodates one expert. \NX{FasterMoE only support EP and the number of expert of each EP has to be 1? This baseline sounds too weak}. 
\textsc{Tutel} performs better than other baselines because it incorporates communication into experts' computation through delicate scheduling and adaptive parallelism. Although communication and computation is overlapped partially, when the number of experts is large (Qwen2), the advantage of \textsc{Tutel} diminishes because of the large scheduling overhead.
\textsc{FasterMoE} only supports expert parallelism ($\textit{EP}=W$) and it also does not perform well on Qwen2 because the experts are small in Qwen2 and the kernel invoking time for experts dominates the MoE layer. 
% \NX{Comet conceals over 80\% of the communication on average, while other baselines only hide xx, xx.}
% It is also observed that with the increasing of $M$, \textsc{FasterMoE} and \textsc{Tutel} perform better because the scheduling overhead becomes not prominent.

%\sysname{} outperforms other baselines in all configurations because it realizes sufficient overlapping and the scheduling inside high-performance fused kernels greatly reduce the the overhead at CPU side.
% \NX{E2E is the most important experiments, show more details and dig out more performance insights}.

% With the identical attention computation apart, \sysname{} can drastically reduce the duration of MoE computation by xx, xx, xx and xx respectively.

% As analyzed in~\cite{centauri}, for communication-computation overlapping, the optimal duration reduction is $50\%$ because either communication or computation is totally overlapped. However, we observe larger than $50\%$ latency reduction compared with baselines in some scenarios.
% This is because other than communication and computation operations, the MoE layer also consists of multiple 

% \begin{figure}[t]
% \centering
% 	\includegraphics[width=\columnwidth]{figures/e2e_m4096.png}
% 	\caption{\label{fig:e2e_m4096} M=4096.}
% \end{figure}

% \begin{figure}[t]
% \centering
% 	\includegraphics[width=\columnwidth]{figures/e2e_m8192.png}
% 	\caption{\label{fig:e2e_m16384} M=8192.}
% \end{figure}

% \begin{figure}[t]
% \centering
% 	\includegraphics[width=\columnwidth]{figures/e2e_m16384.png}
% 	\caption{\label{fig:e2e_m16384} M=16384. TP=1 will encounter OOM because in this case DP=8}
% \end{figure}


\subsection{Detailed Evaluation on a Single MoE Layer}
% \textcolor{red}{We may first say that the overall performance improves because the performance of each MoE layer improves. Therefore, we perform xxx experiments ...}
We then conduct an in-depth examination of a single MoE layer to perform a detailed analysis. 
% For apples-to-apples comparison, we compare \sysname{} with the baselines solely using expert parallelism, which is supported by all baselines.

\textbf{Handling varying input token lengths.}
The latency of a single MoE layer with varying input token lengths is shown in~\autoref{fig:single_tp1}. 
% Without specification, the experts configuration in following experiments is identical to that of Mixtral 8x7B. 
With the input token number varying, \sysname{} experiences a shorter duration compared with baselines and the improvement is stable.
\sysname{} achieves a $1.28\times$ to $2.37\times$ speedup compared with the baselines on average. 
It is noted that the advantage of \sysname{} is prominent especially when $M$ is small. This is because the scheduling time on the host side predominates the overall duration when $M$ is small and \sysname{} reduces such overhead \bluetext{through kernel scheduling within the fused kernel}. The scheduling overhead increases with $topk$ and $E$ for mechanisms with kernel-level scheduling (\textsc{FasterMoE} and \textsc{Tutel}) because the experts to manage become more complicated, inducing more kernels to be scheduled.

\begin{figure}[t]
\centering
	\includegraphics[width=\columnwidth]{figures/single_test_tp1.pdf}
 % \vspace{-3mm}
	\caption{\label{fig:single_tp1} Single MoE layer duration with expert parallelism ($\textit{EP}=8$). The x-axis represents the total input token length $M$. Each device has $M/W$ tokens before token dispatching. The shape of experts are identical to that of Mixtral 8x7B.}
\end{figure}


% And here we will have a time breakdown analysis. 
% Since FastMoE does not support tensor parallelism, we examine the expert parallelism scenario for fair comparison. The results are shown in~\autoref{fig:breakdown}. 
\textbf{Time breakdown analysis of an MoE layer.}
% \textcolor{red}{Maybe we may put this part in the overall performance? It shows the sources of the performance improvement.}
The time breakdown of a specific MoE layer is shown in~\autoref{fig:breakdown}. 
% Note that the communication time includes the communication kernels as well as the time to trigger these kernels. 
% Except from the key communication and computation operations, the `schedule' time refers to the calculation time between key operations for scheduling (e.g., token index selecting or token routing for scheduling)
Note that the communication part only consists of the GPU-to-GPU communication time, and the operations of token indexing, dispatching and combining on local device are regarded as the computation part.
As revealed, \textsc{Megatron-TE} and \textsc{Megatron-cutlass} experience no overlapping between communication and computation.
% The communication takes xx\% than computation in this case.
% \textcolor{red}{Explain this in more detail:}
\textsc{FasterMoE} reduces the communication latency through customized Scatter and Gather operators, while the introduced local indexing extends the computation time.
\textsc{Tutel} reduces the communication overhead through the optimized all-to-all primitive design. However, its optimized all-to-all also exacerbates the burden of local computation.
% \textsc{Tutel} and \textsc{FastMoE} reduce the communication overhead and hide the communication latency through various ways, while the scheduling overhead is high. 
\bluetext{\textsc{Megatron-TE} has no communication overlapped}. \sysname{} hides $86.5\%$ of communication latency on average and the computational efficiency of experts is not influenced, while \textsc{FasterMoE} and \textsc{Tutel} hide only $29.2\%$ and $68.6\%$ respectively. 
% \NX{only 71.4? , change a case that can overlap 90+\%}

% However, since large calculation and indexing have to leverage CPU to compute, other operations on CPU occupy a large proportion. Since \sysname{} fuse necessary computation into large kernels, the CPU burden is small. Except from the token indexes data to be communicated, other communications are incoporated into large FFN kernels.

\begin{figure}[t]
\centering
	\includegraphics[width=\columnwidth]{figures/breakdown.pdf}
 % \vspace{-3mm}
	\caption{\label{fig:breakdown} Time breakdown of an MoE layer with expert parallelism. ($\textit{EP}=8, \textit{TP}=1, E=8, topk=2$ and $M=16384$).}
 % \vspace{-2mm}
\end{figure}

% Single MoE layer parallelism: 
% \NX{Use subsection or bold statements to organize the experiments. Currently, each experiment is presented as a single paragraph with no organization.}
\textbf{Parallelism within the MoE layer.}
% \ZSL{Still trying to enable tensor parallel for Tutel or FasterMoE...}
% To promise efficiency, \textsc{FasterMoE} and \textsc{Tutel} 
Because of the introduction of expert parallelism, the parallel strategy within the MoE layer can be different from the model's overall parallel strategy.
\autoref{fig:single_para} shows the performance of methods applying diverse parallel strategies. Among all baselines, \textsc{FasterMoE} unfortunately does not support tensor parallelism.
For other baselines (\textsc{Megatron-TE}, \textsc{Megatron-Cutlass} and \textsc{Tutel}), the MoE layer latency increases when {\it TP} grows. This is because that tensor parallelism splits each expert onto multiple devices, triggering more fragmented small GEMMs for experts and resulting in a degradation of computational efficiency.
% However, \sysname{} maintains low latency in various parallelisms because the workload assignment mechanism helps to hide fine-grained communication latency no matter how does the expert shape change. 
Nevertheless, \sysname{} maintains low latency in diverse parallelisms as \bluetext{the shared tensor is rescheduled to maintain computational efficiency} and the weight switching overhead is eliminated.
% \NX{FIXME}
% \sysname{} is proved to be adaptive to various parallel strategies and outperform other baselines.
% We only compare \sysname{} with Megatron-based MoE because other approaches do not support tensor parallelism when $E>W$.

\begin{figure}[t]
\centering
	\includegraphics[width=0.7\columnwidth]{figures/f1_single_layer_parallelism.pdf}
  % \vspace{-2mm}
	\caption{\label{fig:single_para} Single MoE layer duration under various parallelism strategies with $E=8, topk=2, M=8192, \textit{EP}\times \textit{TP}=8$.
 % \NX{confused why end2end evaluation has other baselines, and, the micro benchmark doesn't. If other baseline is not supported, how the end2end works?}
 }
\end{figure}


%\subsection{Single MoE layer scaling}
\subsection{Adaptiveness to Different Configurations}
% \textcolor{red}{We may first say that xxx factors may affect the effectiveness of \sysname{}. Therefore, we perform xxx experiments...}
We further inquire into the performance of \sysname{} when adapting different model configurations, runtime workloads and system environments.

\textbf{Performance with various MoE parameters.} We adjust the number of experts $E$ as well as $topk$ to evaluate the performance of \sysname{} in various MoE structures. The results are shown in~\autoref{fig:experts}.
With the increasing of $topk$, the duration of the MoE layer is increased because the computation amount at runtime is scaled up. 
% \sysname{} maintains privilege with various $topk$ and $E$ and a $1.52\times \sim 2.23\times$ speedup is observed compared with the baselines in this experiment.
\sysname{} consistently demonstrates superior performance across different values of $topk$ and $E$, yielding a speedup in the range of $1.16\times$ to $1.83\times$ compared to baseline implementations.
% We also observe the increase of duration with the total expert number increasing. This is because although the amount of tokens to be compute is unchanged, the target experts become more diverse and the weight switching prolongs the calculation time.
% We also observe the increase of duration with the total expert number increasing. This is because although the amount of tokens to be compute is unchanged, the target experts become more diverse and the weight switching prolongs the calculation time. When total expert number is increased from 8 to 64, the average duration is increased by $28.8\%$ on average.
% With the increasing of number of experts, the duration of MoE layer is also increased as expected.

\begin{figure}[t]
\centering
	\includegraphics[width=\columnwidth]{figures/f1_topk.pdf}
  % \vspace{-3mm}
	\caption{\label{fig:experts} Duration of a single MoE layer ($M=16384, \textit{EP}=8, \textit{TP}=1$) with various number of experts $E$ and $topk$.}
  % \vspace{-2mm}
\end{figure}


\textbf{Performance with varying token distribution.}
When using expert parallelism, the number of tokens routed to different devices varies. We evaluate the performance of \sysname{} in scenarios with imbalanced token distribution. 
The standard deviation of the token distribution across different experts is denoted as $std$.
As shown in the left panel of~\autoref{fig:scaling}, 8192 tokens are distributed across various experts with differing distributions. When $std=0$, tokens are uniformly distributed and each expert receives $M\times topk / E = 2048$ tokens. 
At $std=0.05$, the least-loaded expert is assigned only a few hundred tokens. 
In a typical training job in production, the average $std$ is $0.032$.
% and is stable after hundreds of training epochs.
When the load imbalance problem is exacerbated, the latency of the MoE layer in all systems is prolonged. \sysname{} consistently outperforms other MoE systems.


\begin{figure}[t]
\centering
	\includegraphics[width=0.9\columnwidth]{figures/f1_scaling.pdf}
 % \vspace{-3mm}
	\caption{\label{fig:scaling} Performance of a MoE layer when scaling to different scenarios. \bluetext{Left: Duration with various token distribution with expert parallelism ($E=8, topk=2, M=8192, \textit{TP}=1, \textit{EP}=8$).} Right: Duration on a L20 Cluster with diverse parallelisms ($E=8, topk=4, M=8192, \textit{EP}\times \textit{TP}=8$). }
  % \vspace{-2mm}
\end{figure}

\textbf{Scaling to distinct clusters.}
We carry out the experiments on another distinct cluster with a different network environment. The cluster is equipped with 8 Nvidia L20 GPUs (46 GB memory) and the GPUs are connected via PCIe bridges. The GPU-to-GPU bandwidth is around 25 GB/s as tested, which is much lower than the H800 cluster. The experiments on the L20 cluster represents a bandwidth-limited environment.
As shown in the right panel of~\autoref{fig:scaling}, the average speedup of \sysname{} compared with other baselines is from $1.19\times$ to $1.46\times$. The results manifest the superiority of \sysname{} under different cluster environments.


\begin{table}[]
\caption{\label{tab:memory} Required device memory size for NVSHMEM.}
\footnotesize
\centering
\vspace{1mm}
\begin{tabular}{|c|c|c|c|}
\hline
Mem(MB) & Mixtral 8x7B & Qwen2-MoE & Phi3.5-MoE \\ \hline
M=4096     & 32           & 16        & 32         \\ \hline
M=8192     & 64           & 32        & 64         \\ \hline
\end{tabular}
 \vspace{-1mm}
\end{table}


% \begin{figure}[t]
% \centering
% 	\includegraphics[width=\columnwidth]{figures/l20.pdf}
% 	\caption{\label{fig:l20} Performance of an MoE layer on Cluster B.}
% \end{figure}



\subsection{\browntext{Overhead Analysis}}
% \textcolor{red}{Why you mention memory consumption suddenly? How about other overheads?}

\sysname{} leverages NVSHMEM to allocate a shared memory buffer for communication on each device. The buffer size is dependent on the model configuration and equals to $MN$, where $M$ is the input sequence length and $N$ is the model hidden size. For datatype of BF16 or FP16, the allocated memory size is $2MN$. The communication buffer is global for the execution of the entire model, which means that it is shared across layers and experts. We list the device memory consumption of \sysname{} in~\autoref{tab:memory}, and it is negligible compared with the large device memory on current GPUs.



% \subsection{Analysis of detailed techniques}
% \sysname{} uses NVSHMEM as the communication framework. It requires xxx.