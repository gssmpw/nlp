\section{Related Work}

With the successful application of MoE in large-scale distributed training and inference, there are plenty of works focusing on the system-level optimizations of reducing the communication overhead inherited in the MoE structure.

\paragraph{Communication optimization.}
To reduce the communication overhead in MoE execution, a straight-forward approach is to leverage efficient communication algorithms~\cite{all2all, shen2022se} for faster data transmission. Recent works~\cite{tutel, rajbhandari2022deepspeed, nie2022hetumoe} also propose the 2D-hierarchical all-to-all algorithm to better utilize intra-node bandwidth and accelerate MoE communication.
% for MoE to reduce the amount of data to communicate among devices~\cite{schemoe, tutel, zhou2022accelerating} and . 
Some other works propose to reduce communication volume by data compression. For example, ScheMoE~\cite{schemoe} and Zhou et al.,~\cite{zhou2022accelerating} propose to apply data compression technologies to reduce the all-to-all communication volume while preserving the model convergence.

\paragraph{Computation-communication overlapping.}
The techniques of overlapping of computation and communication for dense models have been extensively employed in distributed training and inference~\cite{centauri, jangda2022breaking, song2023optimus, wang2022overlap, wang2023mgg, chang2024flux}.
For the MoE structure, recent studies also try to identify the pipelining opportunities for communication tasks of all-to-all operations and computing tasks of GEMMs.
FasterMoE~\cite{fastermoe} allows a pipeline degree of 2 to pipeline the expert computations and all-to-all communications. Tutel~\cite{tutel} enables a manually set degree of pipelining or a heuristic search under limited searching space, which may be sub-optimal. PipeMoE~\cite{pipemoe} 
% aim to find the optimal pipeline degree by formulating an optimization problem and building performance models to solve it.
and ScheMoE~\cite{schemoe} aim to schedule MoE operators to better utilize intra- and inter-connect bandwidths. These solutions realize overlapping through kernel-level scheduling 
and do not fully resolve the fine-grained data dependency in MoE.

% The overlapping of computation-communication is an important topic in large model training and inference. 
% Numerous works have emerged to optimize the communication and computation overlapping for dense models.
% Some frameworks~\cite{} aim to optimize the overlap scheduling of a single parallel method in a coarse-grained manner and some sophisticated compiler-style works~\cite{} generate code for fine-grained overlapping between two kernels or designing kernels using hand-crafted strategies~\cite{centauri}. However, these kernels are unavailable for MoE structures.
% Techniques are evolving to either seeking finer granularity (TE) or using hand-crafted overlapping strategies (Centauri~\cite{centauri}). 