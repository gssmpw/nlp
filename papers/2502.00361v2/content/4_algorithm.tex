\section{Soft Diffusion Actor Critic}
In this section, we propose the \algname~(\algabb), a practical maximum entropy RL algorithm leveraging RSSM to train diffusion policy to represent energy-based policies. We first address several practical issues such as sampling scheme and exploration-exploitation tradeoff and then present the overall algorithm.

\subsection{Practical Issues of Diffusion Policy Training}
\textbf{Reverse sampling distribution selection.} In the theoretical derivation, we did not specify which sampling distribution $\tilde{p}_t$ to use.
In the online RL setting, we need to query the $Q$ function when we calculate the diffusion policy loss~\eqref{eq:policy_loss}. As $Q^\pi(\sbb,\ab)$ gets more accurate, it is better to have the sampled $\tilde\ab_0$ being closer to the target policy \eqref{eq:energy_based_opt_pi}, \ie, sample $\tilde\ab_0$ such that $Q^\pi(\sbb,\tilde\ab_0)$ is of high values. In practice, we use samples generated from the reverse diffusion process in soft policy evaluation step to achieve better performance.
 % We already discussed in Example that if we select $p_t$ as $\tilde\pi_t$ then the sampled $\tilde\ab_0$ centers around $\ab_0$. As we do not know the $p_t$, we use the $t$-step in the reverse process as an approximated $p_t$ as our sampling distribution.

\textbf{Exploration and exploitation trade-off} is key to online RL performance. \citet{haarnoja2018soft2} proposed to automatically tune the regularization parameter $\lambda$ to fit a target entropy proportional to the action space dimension, but it is impractical for diffusion policy since the entropy is intractable for EBMs. 
Current diffusion online RLs either estimated entropy with Gaussian mixture model~\cite{wang2024diffusion}, or mixed sampling with uniform samples~\cite{ding2024diffusion} to encourage exploration. However, these operations significantly increase the computation cost and slow down the training, which is unnecessary. In our practical implementations, we simplified the exploration design by adding additive Gaussian noise with level $\lambda$. Therefore, the parameter $\lambda$ controls the policy randomness in two ways, the energy function scale and the additional Gaussian noise. The $\lambda$ is also auto-tuned to track a target noise level $\lambda_{\rm target}$. 
% \begin{algorithm}
%     \caption{Action sampling.}\label{alg:sampling}
%     \begin{algorithmic}[1]
%         \REQUIRE DDPM $\epsb_\theta$, state $\sbb$, noise schedule $\beta_t, \bar\alpha_t$, number of particles $M$, the $Q$ function, additive Gaussian noise level $\lambda$.
%         \STATE sample $M$ particles from $a^i_T\Ncal\rbr{0, \Ib}$ for $i=1,\dots, M$
%         \FOR{$t = T, T-1,\dots, 1$}
%         % \FOR{$i = 1,2,\dots, M$}
%             \STATE Sample a batch of actions using reverse diffusion process~\eqref{eq:annealed_langevin_2} with given $\epsb_\theta$
%             for the batch actions.
%         % \ENDFOR
%         \ENDFOR
%         \STATE Add noise $\ab\leftarrow \arg\max_{i}Q(\sbb,\ab^i_0) + \lambda\epsilon$, where $\epsilon\sim\Ncal(0,\Ib)$
%         \ENSURE $\ab$
%     \end{algorithmic}
        
% \end{algorithm}

\textbf{Numerical stability.} The loss function re-weight the common denoising score matching loss~\eqref{eq:dsm_loss} by exponential of $Q$-function. In practice, the exponential of large $Q$ functions will cause the loss to explode. Moreover, the scale of the $Q$ function might vary with different rewards or tasks. To improve numerical stability, we normalized the $Q$ function, \ie, subtracting the mean and dividing by the standard deviation. The normalization operation does not conflict with our theoretical derivation, subtracting mean values is scaling the loss, and the standard deviation can be regarded as part of the regularization hyperparameter $\lambda$.
% \textbf{Exponential moving average~(EMA) policy}. The diffusion model has superior expressiveness but is also more unstable during training. As a common practice to stabilize outcomes with EMA for the image generation with diffusion models~\cite{kingma2014adam}, we maintain an EMA of the denoising network $\epsb_{\bar\theta}$, used in soft policy evaluation step~\eqref{eq:soft_pev} and evaluations. The EMA update rules are similar to the commonly used target network trick in RL~\cite{mnih2013playing}.

\subsection{Practical Algorithm}
Combining all the discussions above, we present the practical algorithm in Algorithm \ref{alg:main} and also in \Cref{fig:demo}. 
\begin{algorithm}
    \caption{Soft Diffusion Actor-Critic~\label{alg:main}}
    \begin{algorithmic}[1]
        \REQUIRE Diffusion noise schedule $\beta_t, \bar\alpha_t$ for $t\in \{1,2,\dots T\}$, reverse sampling distribution $\tilde\pi_t$, MDP $\Mcal$, initial policy parameters $\theta_0$, initial entropy coefficient $\lambda_0$, replay buffer $\Dcal=\emptyset$, learning rate $\beta$, target entropy coefficient $\lambda_{\rm target}$
        \FOR{epoch $e=1,2,\dots$}
        \STATE \algcommentline{Sampling and experience replay.}
        \STATE Interact with $\Mcal$ using policy $\epsb_{\theta_{e-1}}$ thorough algorithm update replay buffer $\Dcal$.
        \STATE Sample a minibatch of $(\sbb, \ab, r, \sbb')$ from $\Dcal$.
        \STATE \algcommentline{Soft policy evaluation.}
        \STATE  Sample $\ab'$ via reverse diffusion process~\eqref{eq:annealed_langevin_2} with $\epsb_{\theta_{e-1}}$.\label{lst:line:sample}
        \STATE Update $Q_e$ with soft policy evaluation \eqref{eq:soft_pev}.
        \STATE \algcommentline{Soft policy improvement for diffusion policies.}
        \STATE Randomly sample $t$ and $\ab_t\sim \tilde\pi_t(\cdot|\sbb)$ .
        \STATE \emph{Reverse sampling:} Sample $\tilde\ab_0$ with \eqref{eq:a0_sample}.
        \STATE Compute $Q_e(\sbb,\tilde\ab_0)$ and normalize as $\overline Q_e(\sbb,\tilde\ab_0)$.
        \STATE Update $\theta_e$ with loss $\Lcal^\pi(\theta_{e-1};\overline{Q}_e,\lambda_{e-1})$ in~\eqref{eq:policy_loss}.
        % \STATE Update the EMA policy $\bar\theta_e\leftarrow (1 - \xi)\bar\theta_{e-1} + \xi \theta_e$
        \STATE Update entropy coefficient $\lambda_e\leftarrow \lambda_{e-1} - \beta (\lambda_e -\lambda_{\rm, target})$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
\vspace{-10pt}

\textbf{Efficiency and performance compared to other diffusion policies for online RL.} We say the proposed \algabb algorithm in~\Cref{alg:main} is efficient because of similar computation and memory cost with denoising score matching~\eqref{eq:ddpm_loss} while maintaining performance and bypassing the sampling issues, which is more efficient compared to recent diffusion policies for online RL.

\textbf{Recent works on diffusion policy online RLs} can be categorized into these families: \textbf{i) Langevin-based sampling.} With the known energy functions in~\eqref{eq:energy_based_opt_pi}, \citet{psenka2023learning,jain2024sampling} directly differentiated it to get the score function and use Langevin dynamics to sample from ~\eqref{eq:energy_based_opt_pi}. The computation is lightweight since no noise perturbation is involved (thus not diffusion policies in essence). However, the empirical performance is not good due to pitfalls mentioned in \Cref{remark:langevin}. \textbf{ii) Reverse diffusion as policy parametrizations.} The reverse process~\eqref{eq:annealed_langevin_2} can also be directly regarded as a complex parametrization of $\theta$. \citet{wang2024diffusion} backpropagate policy gradients through the reverse diffusion process, resulting in huge computation costs. \citet{ding2024diffusion} approximate the policy learning as a maximum likelihood estimation for the reverse process, which incurs approximation errors and can not handle negative $Q$-values. \textbf{iii) Others.}\citet{yang2023policy} maintained particles to approximate the policy distribution and fit it with the diffusion model. \cite{ren2024diffusion} combined the reverse process MDP with MDP in RL and conducted policy optimizations. They all induce huge memory and computation costs, thus being impractical and unnecessary. More general related works can be found in \Cref{sec:apdx_related_works}.

% Other diffusion-policy online RLs need either recurrent gradient backpropagation or complex sampling protocols.  For instance, \citet{wang2024diffusion,ren2024diffusion} require the gradient to backpropagate through the whole reverse diffusion process~\eqref{eq:annealed_langevin_2}, which incurs significantly large computation and memory cost that scales with diffusion steps $T$. \citet{ding2024diffusion,yang2023policy} requires sampling from current policy $\pi$ which is impossible. They either use another reverse process to generate or a large number of particles to estimate, which incur huge additional computation and memory costs. In summary, the proposed has minimal computation and memory cost and can be easily plugged into the soft policy iteration algorithm.

