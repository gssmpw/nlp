\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figure/demo_paper.pdf}
    \caption{Demonstration of the proposed \algname~(\algabb) algorithm. We leverage diffusion policy to represent the energy-based policy in maximum entropy RL. The diffusion policy is trained via reverse sampling score matching, an algorithm that does not sample from the target energy-based policy and only depends on the $Q$-functions, enabling efficient online RL for diffusion policy.}
    \label{fig:demo}
\end{figure*}
\section{Diffusion Policy Learning in Online RL}
\label{sec:rssm}

In this section, 
we first present the connection of energy-based models and diffusion models, justifying the expressiveness of diffusion policy, and identify the difficulties in online training of diffusion policy. 
We then introduce the reverse sampling score matching (RSSM) to make the training of diffusion policy possible with \emph{only} access to the energy function in online RL.  








\subsection{Diffusion Models as Noise-Perturbed Energy-Based Models} 
We first revisit the energy-based view of diffusion models, \ie, \emph{diffusion models are noise-perturbed EBMs}~\cite{shribak2024diffusion}, to justify that the diffusion policy can efficiently represent the energy-based $\pi_{\rm target}$. 
Given $\sbb$, consider perturbing action samples $\ab_0 \sim \pi_{\rm target}(\cdot|\sbb)$ with corruption kernel $q_{t|0}(\ab_t|\ab_0)=\Ncal(\ab_t;\sqrt{\bar\alpha_t}\ab_0, \rbr{1 - \bar\alpha_t}\Ib)$, 
which results in the noisy-perturbed policy $\tilde\pi_t(\cdot|\sbb)$ with 
$$
\tilde\pi_t(\ab_t|\sbb)= \int q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)d\ab_0
$$
for noise schedule index $t=1,2,\dots, T$.


\begin{restatable}[Diffusion models as noise-perturbed EBMs\label{prop:diff_ebm}]{proposition}{diffebm}
    The score network $s_\theta(\ab_t;\sbb, t)$ in \eqref{eq:ddpm_loss} matches noise-perturbed score functions, $\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$,
        where state $\sbb$ is added to inputs of score network $s_\theta(\ab_t;\sbb, t)$ to handle conditional distributions, $p_0(\cdot)$ in \eqref{eq:ddpm_loss} refers to $\pi_{\rm target}(\cdot|\sbb)$ in the policy learning setting. \end{restatable}

\begin{proof}
    This can be shown by checking the noise-perturbed score function $\nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb_t)$, \ie,
    \begin{align}
        & \nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb_t)\label{eq:prop1_derivation}
        \\
            = & \frac{\nabla_{\ab_t}\tilde\pi_t\rbr{\ab_t|\sbb}}{\tilde\pi_t\rbr{\ab_t|\sbb}}           =  \frac{\nabla_{\ab_t}\int q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)d\ab_0}{\tilde\pi_t\rbr{\ab_t|\sbb}}\notag\\
            = & \int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\underbrace{\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}}_{p_{0|t}(\ab_0|\ab_t, \sbb)}d\ab_0 \nonumber
    \end{align}
                        We match the noise-perturbed score function via the score network $s_\theta(\ab_t;\sbb, t)$ via optimizing the expectation of square error over $\ab_t\sim \tilde\pi_t(\cdot|\sbb)$,
    
    {\small
    \begin{align}
       &\EE_{\ab_t\sim\tilde\pi_t}\nbr{s_\theta(\ab_t;\sbb, t) - \int\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)p_{0|t}(\ab_0|\ab_t,\sbb)d\ab_0}^2\notag\\         =&\underset{\substack{\ab_0\sim\pi_{\rm target}\\\ab_t\sim q_{t|0}}}{\EE}\sbr{\nbr{s_\theta(\ab_t;\sbb, t) -\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}^2} + \texttt{constant}\label{eq:tweedie_loss}
    \end{align}
    }
    The detailed derivations of \eqref{eq:tweedie_loss} are deferred to \Cref{sec:prop_1_apdx}.
    We can see that \eqref{eq:tweedie_loss} is equivalent to the $t$-th term in DDPM loss~\eqref{eq:ddpm_loss},
    which concludes the proof of Proposition~\ref{prop:diff_ebm}.
\end{proof}


Furthermore, as the noise schedule $\beta_t$ gets close to zero when $t$ goes from $T$ to $1$ in the reverse process~\eqref{eq:annealed_langevin_2}, the noise-perturbed EBMs gradually resemble the original energy-based policies $\pi_{\rm target}$. 
Adding adaptive levels of noise perturbations encourages explorations on the energy landscape, which significantly improves the sampling quality and makes diffusion models the key breakthrough in   EBMs~\cite{song2019generative}. 

\textbf{Difficulties to train diffusion model in online RL setup.}
By the connection between EBMs and diffusion models, we justify the expressiveness of diffusion policy for maximum entropy RL. 
However, training diffusion policy is highly non-trivial in online RL because of two major challenges:

$\bullet$ \textbf{Sampling challenge:} the vanilla diffusion training with denoising score matching~\eqref{eq:ddpm_loss} requires samples from the target policy $\pi_{\rm target}$, but we cannot access $\pi_{\rm target}$ directly in online RL since we only know the energy function, \ie, the $Q$-functions.

$\bullet$ \textbf{Computational challenge:} another possible solution is to backpropagate policy gradient thorough the whole reverse diffusion process~\eqref{eq:annealed_langevin_2}. However, this recursive gradient propagation not only incurs huge computational and memory cost, but also suffers from gradient vanishing or exploding, making diffusion policy learning expensive and unstable.

These challenges hinder the performance of diffusion-based policies in online RL. 












\subsection{Learning Noise-perturbed Score Functions via Reverse Sampling Score Matching }
\label{subsec:main_theorem}




In this section, we develop our core contribution, reverse sampling score matching (RSSM), an efficient diffusion policy learning algorithm that eliminates the aforementioned difficulties. 
Following the energy-based viewpoint in \Cref{prop:diff_ebm}, we propose the following theorem,


\begin{restatable}[Reverse sampling score matching (RSSM)\label{thm.rssm}]{theorem}{rssm}
    Define $\tilde p_t(\cdot|\sbb)$ as a sampling distribution whose support contains the support of $\tilde\pi_t(\cdot|\sbb)$ given $\sbb$.
                        Then we can learn the score network $s_\theta(\ab_t;\sbb,t)$  to match with the score function of noise-perturbed policy $\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$ via minimizing
    \begin{equation}
        \underset{\substack{\ab_t\sim \tilde p_t\\ \tilde\ab_0\sim\tilde q_{0|t}}}{\EE}\sbr{\exp \rbr{Q\rbr{\sbb, \tilde\ab_0}/\lambda}\nbr{s_\theta\rbr{\ab_t; \sbb, t} - \nabla_{\ab_t}\log \tilde q_{0|t}\rbr{\tilde\ab_0 \! \mid \! \ab_t}}^2}\label{eq:thm1_reweight_loss}
    \end{equation}
    where we abbreviate $Q^{\pi_{\rm old}}$ with $Q$ for simplicity and $\tilde q_{0|t}$ is the \textbf{reverse sampling} distribution defined as
                \begin{equation}
        \tilde q_{0|t}(\tilde\ab_0|\ab_t):=\Ncal\rbr{\ab_0;\frac{1}{\sqrt{\bar\alpha_t}}\ab_t, \frac{1 - \bar\alpha_t}{\bar\alpha_t}\Ib}\label{eq:a0_sample_thm}
    \end{equation}
    which means $ \tilde\ab_0 = \frac{1}{\sqrt{\bar\alpha_t}}\ab_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb$ for $\epsb\sim\Ncal(0,\Ib)$.
\end{restatable}

The name \emph{reverse sampling} score matching comes from that we first sample $\ab_t\sim \tilde p_t$ then sample $\tilde\ab_0\sim\tilde q_{0|t}$, thus bypassing the sampling issues and not increasing computational cost. We show a sketch proof here, the full derivations can be found in \Cref{sec:appendix_derivation}.

    \textit{Proof.} The derivations consists of two major steps, reformulating the noise-perturbed score function and applying the reverse sampling trick.
    \\\textbf{Reformatting the noise-perturbed score function.} First, we slightly reformat derivations of the noise-perturbed score function in \Cref{prop:diff_ebm} starting from \eqref{eq:prop1_derivation},
    \begin{equation}
        \begin{aligned}
             \nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb)            = &\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}d\ab_0\\
            = & \frac{\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0) q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0}{Z\rbr{\ab_t;\sbb}}\\
        \end{aligned}\label{eq:noise_perturbed_score}
    \end{equation}
    where $Z(\ab_t;\sbb) := \tilde{\pi}_t(\ab_t | \sbb) \int\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0 = \int q_{t | 0}(\ab_t | \ab_0) \exp\rbr{Q(\sbb,\ab_0)/\lambda} d \ab_0 $ .
    Equation \eqref{eq:noise_perturbed_score} is obtained by substituting the energy function into $\pi_{\rm target}$.     With \eqref{eq:noise_perturbed_score}, the square error given $\ab_t$ satisfies
    \begin{equation}
        \begin{aligned}
            &\nbr{s_\theta(\ab_t;\sbb,t) - \nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)}^2
            = \frac{1}{Z(\ab_t;\sbb)}\int q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda}l_\theta(\ab_0,\ab_t;\sbb)d\ab_0
        \end{aligned}
    \end{equation}
    where {$l_\theta(\ab_0,\ab_t;\sbb) = \nbr{s_\theta\rbr{\ab_t; \sbb, t} - \nabla_{\ab_t}\log q_{t|0}\rbr{\ab_t|\ab_0}}^2$}. 
    Then we integrate the square error over a custom measure  $g(\ab_t;\sbb):=Z(\ab_t;\sbb)\ptil_t(\ab_t|\sbb)$ to compensate the $Z(\ab_t;\sbb)$ and get to, 
    \begin{equation}
        \iint\ptil_t(\ab_t|\sbb) q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda}l_\theta(\ab_0,\ab_t;\sbb)d\ab_0d\ab_t\label{eq:reformat_loss_p}
    \end{equation}
    A more rigorous derivation is deferred to Appendix \ref{sec:appendix_derivation}. 
    
    \textbf{Reverse sampling trick.} The loss function in \eqref{eq:reformat_loss_p} is still not tractable. To handle this, we introduce the \emph{reverse sampling trick}, \ie, replacing $q_{t|0}$ with a reverse sampling distribution $\qtil_{0|t}$ that satisfies
    \begin{equation}
        \begin{aligned}
        &\qtil_{0|t}(\ab_0\mid \ab_t) =\Ncal\rbr{\ab_0;\frac{1}{\sqrt{\bar\alpha_t}}\ab_t, \frac{1 - \bar\alpha_t}{\bar\alpha_t}\Ib}\\
        \propto~& q_{t|0}(\ab_t\mid \ab_0) = \Ncal\rbr{\ab_t;\sqrt{\bar\alpha_t}\ab_0, \rbr{1 - \bar\alpha_t}\Ib},
    \end{aligned}\label{eq:reverse_gaussian}
    \end{equation}
    and their score functions match
    {
    \small
    $
    \nabla_{\ab_t}\log q_{t|0}(\ab_t\mid \ab_0) = \nabla_{\ab_t}\log \qtil_{0|t}(\ab_0\mid \ab_t) = - \frac{\ab_t -\sqrt{\bar\alpha_t}\ab_0}{1 -\bar\alpha_t}
    $
    }.
    Then we can replace $q_{t|0}$ with $\tilde q_{0|t}$ in \eqref{eq:reformat_loss_p} to get a tractable loss function,
    {
                \begin{equation}
        \iint \tilde p_t(\ab_t|\sbb)\tilde q_{0|t}(\tilde\ab_0| \ab_t)\exp\rbr{Q(\sbb,\tilde\ab_0)/\lambda} \tilde l_\theta(\ab_0,\ab_t;\sbb)d\tilde\ab_0d\ab_t \label{eq:with_tidle_q}
    \end{equation}
    }
where {\small $\tilde l_\theta(\ab_0,\ab_t;\sbb)  =  \nbr{s_\theta\rbr{\ab_t;\sbb, t}  -  \nabla_{\ab_t}\log \tilde q_{0|t}\rbr{\tilde\ab_0  \mid  \ab_t}}^2$}.
In this way, we can first sample $\ab_t\sim\tilde p_t$ and then sample $\tilde\ab_0\sim\tilde q_{0|t}$ to enable tractable loss computation.    
By further algebraic operations, we can derive the loss function in \eqref{eq:thm1_reweight_loss} from \eqref{eq:with_tidle_q}. The detailed derivation can be found in Appendix \ref{sec:appendix_derivation}.\hfill$\square$

We can see that with \Cref{thm.rssm}, the loss function \eqref{eq:thm1_reweight_loss} solves both the sampling and computational difficulties mentioned previously. First, we avoid sampling from target policy $\pi_{\rm target}$, and the sampling distribution $\tilde p_t$ is some distributions we can choose. Second, we have similar computation with denoising score matching~\eqref{eq:ddpm_loss}, avoiding extra computational cost induced by diffusion policy learning.


\begin{remark}[Broader applications of RSSM.] We emphasize that although we develop RSSM for online RL problems, the RSSM has its own merit and can be applied to any probabilistic modeling problems with known energy functions. We also show a toy example in \Cref{sec:toy} where we use RSSM to train a toy diffusion model to generate samples from a Gaussian mixture distribution.
\end{remark}

\begin{remark}[Pitfalls of Langevin dynamics in online RL.]\label{remark:langevin}
    Some might question that if we already know the energy function, why not compute the gradient as the score functions and use the Langevin dynamics~\cite{parisi1981correlation} to sample from policy~\eqref{eq:energy_based_opt_pi}.  The reasons are two-fold, i) the gradient of learned $Q$-function might not match the true score function; ii) Langevin dynamics suffers from the slow mixing problem~\citep[shown in \Cref{sec:toy}]{song2019generative} even with true score functions. Both pitfalls result in bad performance and motivate the necessity of diffusion policies with known energy functions.
\end{remark}

\subsection{Practical Diffusion Policy Learning Loss}
The direct impact of Theorem~\ref{thm.rssm} is a diffusion policy learning loss that can be sampled and computed efficiently in online RL. Specifically, summing over all timestep $t$ and state $\sbb$ in Equation~\eqref{eq:thm1_reweight_loss}, we derive the diffusion policy learning loss with RSSM:

{
\small
\begin{equation}
    \begin{aligned}
        &\Lcal^\pi(\theta;Q,\lambda) := \frac{1}{T}\sum_{t=1}^T\underset{{\sbb, \ab_t,\tilde\ab_0}}{\EE}\sbr{\exp\rbr{\frac{Q(\sbb,\tilde\ab_0)}{\lambda}}\nbr{s_\theta\rbr{\ab_t, \sbb, t} - \nabla_{\ab_t}\log\qtil_{0|t}(\tilde\ab_0|\ab_t)}^2}
    \end{aligned}\label{eq:policy_loss}
\end{equation}
}
with 
\begin{equation}
    \tilde\ab_0 = \frac{1}{\sqrt{\bar\alpha_t}}\ab_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}} \epsb,\, \epsb\sim\Ncal\rbr{0, \Ib}\label{eq:a0_sample}
\end{equation}
and $\sbb$ sampled from the replay buffer, $\ab_t$ sampled from $\ptil_t(\cdot|\sbb)$. 

Obviously, such sampling protocol in~\eqref{eq:policy_loss} and~\eqref{eq:a0_sample} bypasses sampling from the target optimal policy, therefore, can be easily implemented. Meanwhile, the obtained loss avoids recursive gradient backpropagation, largely reducing computation complexity of policy gradient. These benefits perfectly echo the difficulties of sampling and computations in applying vanilla diffusion model training to online RL, enabling efficient diffusion policy learning. 






