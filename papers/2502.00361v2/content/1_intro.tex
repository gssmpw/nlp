\section{Introduction}
Recent years has witnessed a huge success of diffusion-based generative models \cite{sohl-dickstein2015deep,song2019generative,ho2020denoising}. With a dual interpretation of latent variable models and energy-based models (EBMs), diffusion models achieved superior expressiveness and multimodality in representing complex probability distributions, demonstrating unprecedented performance in image and video generation~\cite{ramesh2021zero,saharia2022photorealistic}. The superior expressiveness and multimodality naturally benefit the policies in sequential decision-making problems. Diffusion policy has improved significantly over previous deterministic or Gaussian-based policies on manipulation and locomotion tasks solved by imitation learning and offline reinforcement learning (RL),  where expert datasets are presented.


% The diffusion model 
% involves a forward diffusion process that progressively diffuses a data distribution to a noise distribution and a reverse process that conducts progressive denoising to recover the data distribution. Then the denoisers are trained to match the forward and reverse trajectories by optimizing the evidence lower bound (ELBO), enabling the generation of data distribution from noise. The progressive denoising design is the key to the success of diffusion models.

% However, it is highly non-trivial to transfer such success to diffusion policy in online RL. 
% the data distribution we want to capture with diffusion models is the optimal policy that we do not know in advance. Therefore, the forward process is intractable since we cannot sample from data distribution, making conventional training impossible for diffusion policies of online RL.

It is non-trivial to transfer such success to diffusion policy in the context of online RL. Diffusion models capture the data distribution by progressively denoising a noise distribution subject to a reverse diffusion process, which gives the diffusion model rich expressiveness. In online RL, the data distribution corresponds to the optimal policy distribution. However, the optimal policy is not known in advance, nor can we sample from it directly. Consequently, the forward diffusion process remains unknown, and so does its reverse, rendering conventional diffusion model training infeasible for online RL.

Existing studies that combined diffusion policies with online RL \cite{psenka2023learning,yang2023policy,ding2024diffusion,wang2024diffusion} proposed different methods to bypass this sampling issue. \citet{yang2023policy} approximated the policy distribution by particles. \citet{ding2024diffusion,wang2024diffusion} regarded the reverse diffusion process as policy parameterizations and trained it with policy gradient theorems. \citet{psenka2023learning} leverage the energy-based viewpoint of diffusion models, they first learned the score function by matching the derivatives of the $Q$-function and sampled with Langevin dynamics. All these methods incurred large approximation errors or huge costs on memory and computation time, preventing diffusion policies from getting superior and stable performance in online RL. 

To handle the sampling issues, we proposed reinforcement learning with reverse sampling score matching in this paper. By revisiting the energy-based viewpoint of diffusion models, we show that, although we can not sample from the policy distribution directly, we can train the diffusion policy from the known energy function, which is the soft $Q$-function in max-entropy RL~\cite{haarnoja2017reinforcement}. Specifically,
\begin{itemize}
    \item we proposed the reverse sampling score matching (RSSM) to \textbf{train diffusion models without sampling from data distributions} as long as we know the unnormalized density of the data distribution, \ie, the energy function\footnote{Even with the known energy function, the score function of noise-perturbed data distribution is non-trivial.}.
    \item We integrated the proposed RSSM with the soft policy iteration algorithm~\cite{haarnoja2017reinforcement} to train diffusion policies with the soft $Q$-function as the energy function. Moreover, we provide a practical implementation with a simplified exploration scheme.
    
    \item We demonstrated empirical results, showing that the proposed RSSM algorithm outperforms all recent diffusion-policy-based online RL baselines. Moreover, compared to the soft actor-critic~(SAC) that solves max-entropy RL with Gaussian policy, the performance is increased by more than 120\% on complex locomotion tasks such as MuJoCo Humanoid and Ant, demonstrating the true potential of diffusion policy in online RL.
\end{itemize}

\haitong{TBA: Related works, diffusion RL algorithms}
 

