
\section{Preliminaries}
In this section, we give a brief recap of the diffusion model and its energy-based viewpoints. We also introduce the Markov decision processes and max-entropy RL as our policy learning framework.

\subsection{Maximum Entropy Reinforcement Learning}
\label{sec:max_ent_rl}
We use the Markov decision process (MDP) to model the sequential decision-making problems. 
MDP is a tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, r, P, \mu_0, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $r:\Scal\times\Acal\to\RR$ is a reward function,
$P\rbr{\cdot|s, a}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ is the transition operator with $\Delta(\mathcal{S})$ as the family of distributions over $\mathcal{S}, \mu_0 \in \Delta(\mathcal{S})$ is the initial distribution and $\gamma \in(0,1)$ is the discount factor. 

Maximum entropy RL aims to justify the optimal stochastic policy in MDP. Consider the following entropy-regularized expected return as the objective function~\cite{haarnoja2017reinforcement},
\begin{equation}
    J(\pi) = \EE_{\pi}\sbr{\sum_{\tau=0}^\infty\gamma^\tau \rbr{r(\sbb_\tau, \ab_\tau) + \lambda\Hcal(\pi(\cdot | \sbb_\tau))}}\label{eq:obj_max_ent_rl}
\end{equation}
where $\Hcal\rbr{\pi(\cdot| \sbb)} = \EE_{\ab\sim\pi(\cdot | \sbb)}[-\log\pi(\ab | \sbb)]$ is the entropy, $\lambda$ is a regularization factor for the entropy. The soft policy iteration algorithm~\cite{haarnoja2017reinforcement,haarnoja2018soft} is proposed to solve the optimal max-entropy policy. Soft policy iteration algorithm iteratively conducts soft policy evaluation and soft policy improvement, where soft policy evaluation updates the soft Q-function by repeatedly applying soft Bellman update operator $\Tcal^\pi$ to arbitrary function $Q:\Scal\times\Acal\to \RR$
\begin{equation}
    \Tcal^\pi Q(\sbb_\tau, \ab_\tau) = r(\sbb_\tau, \ab_\tau) + \gamma\EE_{\sbb_{\tau+1}\sim P}\sbr{V(\sbb_{\tau+1})}\label{eq:soft_pev}
\end{equation}
where $V(\sbb_\tau) = \EE_{\ab_\tau\sim\pi}\sbr{Q(\sbb_\tau, \ab_\tau) - \lambda \log\pi(\ab_\tau\mid \sbb_\tau)}$. Then in the soft policy improvement stage, the policy is updated to fit target policy
\begin{equation}
    \pi_{\rm target}(\ab| \sbb) 
    % \frac{\exp\rbr{\frac{1}{\lambda}{Q^{\pi_{\rm old}}(\sbb, \ab)}}}{Z^{\pi_{old}}(\sbb)}
    \propto \exp\rbr{\frac{1}{\lambda}{Q^{\pi_{\rm old}}(\sbb, \ab)}} \label{eq:energy_based_opt_pi}
\end{equation}
The target policy \eqref{eq:energy_based_opt_pi} is an \emph{energy-based} policy, which means that we only know the unnormalized probability density function of the conditional distribution.

\subsection{Energy-based Models}

However, due to the difficulties in training and sampling from EBMs mentioned earlier, the well-known soft actor-critic (SAC) algorithm restricts the policy structure to be Gaussian and updates the parameters by gradient descent on the KL divergence loss $D_{KL}(\pi_\theta\|\pi_{\rm target})$~\cite{haarnoja2018soft}. Even with the Gaussian approximation, SAC showed significant performance improvement and achieved SOTA at that time, which motivated us to further improve the performance by leveraging DDPM to better fit the target policy. We will show the performance improvement compared to SAC in Section \ref{sec:exp}. 

\subsection{Denoising Diffusion Probabilistic Models} 
Denoising diffusion probabilistic models~\cite{sohl-dickstein2015deep,song2019generative,ho2020denoising} are composed of a forward process that gradually perturbs the data distribution $\xb_0\sim p_0$ to a noise distribution $\xb_T\sim p_T$, and a reverse Markov process that recovers data distribution $p_0$ from noise distribution $p_T$. The forward corruption kernel is usually Gaussian with a variance schedule $\beta_1,\dots,\beta_T$, resulting in the forward trajectories
\begin{align}
    &q_{{1:T|0}} = \prod_{t=1}^Tq_{t|t-1}\rbr{\xb_t|\xb_{t-1}} \quad \text{where}\notag\\
    &q_{t|t-1}(\xb_t|\xb_{t-1}) := \Ncal(\xb_t;\sqrt{1-\beta_t}\xb_{t-1},\beta_t\Ib)\label{eq:corruption_ddpm}
\end{align}
where $\xb_t$ is random variable at $t$ step, and $p, q$ are probability distributions\footnote{We use $p$ and $q$ interchangeably in this paper. Generally, $p$ represents intractable distributions (like the t-step marginal $p_t(\xb_t)$), and $q$ represents tractable distributions such as the Gaussian corruption $q_{t\mid t-1}(\xb_t|\xb_{t-1})$.}.
The backward Markov process aimed to recover data distribution from noise distribution $p_T$ with a series of denoiser $p_{t-1|t}(\xb_{t-1}|\xb_t)$. 
The reverse denoiser is usually intractable so we parameterize it with $\theta$, resulting in a joint distribution of the reverse process,
$$
\begin{aligned}
    &p_\theta\rbr{\xb_{0:T}}=p_T(\xb_T)\prod_{t=1}^Tp_{\theta;t-1|t}(\xb_{t-1}|\xb_{t})\quad\text{where}\\
    & p_\theta\left(\xb_{t-1} \mid \xb_t\right):=\mathcal{N}\left(\xb_{t-1} ; \boldsymbol{\mu}_\theta\left(\xb_t, t\right), \mathbf{\Sigma}_\theta\left(\xb_t, t\right)\right)
\end{aligned}
$$
, where $\boldsymbol{\mu}_\theta,\mathbf{\Sigma}_\theta$ is a reparametrization of Gaussian denoiser. Considering all $(\xb_1,\dots,\xb_T)$ as the latent variables, we can solve the denoiser parameters $\theta$ via optimizing the evidence lower bound (ELBO). After some algebraic manipulations and reparametrization, the ELBO loss function becomes~\cite{ho2020denoising},
\begin{equation}
    \EE_{t,\xb_0,\epsb}\sbr{\nbr{\epsb - \epsb_\theta\rbr{\sqrt{\bar\alpha_t}\xb_0 + \sqrt{1 - \bar\alpha_t}\epsb, t}}^2}\label{eq:ddpm_loss}
\end{equation}
where $\epsb\sim\Ncal(0, \Ib),\bar\alpha_t = \prod_{l=1}^t 1-\beta_l$, $t$ sampled uniformly from $\{1,2,\dots T\}$, $\epsb$ is isochopic Gaussian, and $\xb_0\sim p_0$. $\epsb_\theta$ is a noise prediction model reparametrized from $p_\theta$. 
Then the sampling stage in DDPM iteratively conducts
\begin{equation}
    \xb_{t-1}=\frac{1}{\sqrt{\bar\alpha_t}}\left(\xb_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta\left(\xb_t, t\right)\right)+\sigma_t \zb \label{eq:annealed_langevin_2}
\end{equation}
for $t = T, T-1,\dots, 1$ to get samples, where $\sigma_t=\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t$.

\subsection{DDPM as Energy-Based Models} 
Diffusion models can also be interpreted as energy-based models~(EBMs).
EBMs usually refer to parametrized probabilistic models that are not normalized. The density function of an EBM is generally in the formulation of
$
p_\theta(\xb) = \frac{\exp\rbr{ - E_\theta(\xb)}}{Z_\theta}
$,
where $E_\theta(x)$ is the \emph{energy function} parameterized by $\theta$, $Z_\theta = \int \exp\rbr{ - E_\theta(\xb)} d \xb$ is the normalization constant that is usually intractable. The \emph{score function} is defined as the gradient of log density, $\nabla_x\log p_\theta(x) $. 

The unnormalized nature of EBMs allows versatility and rich expressiveness compared to other probabilistic models but raises difficulties in training and sampling~\cite{song2021train}. One of the naive approaches before DDPM is to first learn the score function $\nabla_{\xb}\log p(\xb)$ via score matching~\cite{hyvarinen2005estimation,song2020sliced} and then sample via Langevin dynamics with the learned score function $f_\theta(\xb)\approx\nabla_{\xb}\log p(\xb)$~\cite{parisi1981correlation},
% , a Markov chain Monte-Carlo (MCMC) sampling procedure using only the score function,
\begin{equation}
    \xb_{i+1} \leftarrow \xb_i+\eta f_\theta(\xb)+\sqrt{2 \eta} \mathbf{z}_i, \quad i=0,\cdots, K\label{eq:langevin}
\end{equation}
where $\mathbf{z}_i \sim \mathcal{N}(0, I)$. As $\eta\to 0$ and $K\to\infty$, the sampled $\xb_K$ converges to samples from $p(\xb)$. 

\citet{song2019generative} showed that this naive approach, combining score matching with Langevin dynamics, suffered from multiple pitfalls, such as slow mixing and inaccuracy in low-density regions, which hinders the empirical performance of score-based EBMs. One remedy proposed by \citet{song2019generative} is to fit the score function of a series of noise-perturbed data distribution $\Ncal\rbr{\xb_i;\xb, \sigma_i^2 \Ib}, i=\{1,2,\dots, K\}$ with a noise schedule $\sigma_1> \sigma_2>\dots>\sigma_K$. 
The resulting models, named the noise-conditioned score networks (NCSN) $f_\theta\rbr{\xb_i;\sigma_i}$, take the noise level into the inputs and are learned by denoising score matching~\cite{vincent2011connection}
\begin{align}
    % &\EE_{\xbtil\sim \ptil}\sbr{\nbr{f_\theta\rbr{\xb_i;i} - \nabla_{\xbtil}\log \ptil_\sigma\rbr{\xbtil}}^2} =\label{eq:noisy_score_matching}\\
    &\EE_{\xb\sim p,\xb_i\sim\Ncal\rbr{\xb,\sigma_i^2 \Ib}}\sbr{\|f_\theta\rbr{\xb_i;\sigma_i} -\nabla_{\xb_i}\log q(\xb_i|\xb) \|^2}\label{eq:dsm_loss}
\end{align}
Then in the sampling stage, \citet{song2019generative} replaced the original score function in \eqref{eq:langevin} with the learned noisy score function $f_\theta(\xbtil;\sigma_i)$,
\begin{equation}
    \xb_{i+1} \leftarrow \xb_i+\eta f_\theta(\xbtil;\sigma_i)+\sqrt{2 \eta} \mathbf{z}_i, \quad i=0,\cdots, K\label{eq:annealed_langevin}
\end{equation}
named as annealed Langevin dynamics. The scheduled noise perturbation design significantly improved the image generation performance to match the state-of-the-art (SOTA) at that time~\cite{song2019generative}, which is further refined by DDPM.

We can see that the annealed Langevin dynamics~\eqref{eq:annealed_langevin} resembles the DDPM sampling~\eqref{eq:annealed_langevin_2} with different scale factors, and the denoising score matching loss~\eqref{eq:dsm_loss} is equivalent to \eqref{eq:ddpm_loss} since 
$\nabla_{\xb_i}\log q(\xb_i|\xb)=-\frac{\xb_i-\xb}{\sigma_i^2}=-\frac{\epsb}{\sigma_i}$. Therefore, DDPM can be interpreted as EBMs and is equivalent to NCSN with different designs on noise schedules and scale factors. A more thorough discussion on their equivalency can also be found in~\citet{ho2020denoising,song2021scorebased}.










