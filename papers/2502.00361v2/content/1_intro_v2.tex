\section{Introduction}
Huge successes of diffusion-based generative models have been witnessed recently~\cite{sohl-dickstein2015deep,song2019generative,ho2020denoising}. With a dual interpretation of latent variable models and energy-based models (EBMs), diffusion models achieved superior expressiveness and multimodality in representing complex probability distributions, demonstrating unprecedented performance in image and video generation~\cite{ramesh2021zero,saharia2022photorealistic}. The superior expressiveness and multimodality naturally benefit the policies in sequential decision-making problems. In fact, diffusion policy has been introduced in imitation learning and offline reinforcement learning (RL), where expert datasets are presented. Due to the flexibility of diffusion models, it improved significantly over previous deterministic or unimodal policies on manipulation~\cite{chi2023diffusion,ke20243d,scheikl2024movement} and locomotion tasks~\cite{huang2024diffuseloco}.

% The diffusion model 
% involves a forward diffusion process that progressively diffuses a data distribution to a noise distribution and a reverse process that conducts progressive denoising to recover the data distribution. Then the denoisers are trained to match the forward and reverse trajectories by optimizing the evidence lower bound (ELBO), enabling the generation of data distribution from noise. The progressive denoising design is the key to the success of diffusion models.

% However, it is highly non-trivial to transfer such success to diffusion policy in online RL. 
% the data distribution we want to capture with diffusion models is the optimal policy that we do not know in advance. Therefore, the forward process is intractable since we cannot sample from data distribution, making conventional training impossible for diffusion policies of online RL.

Meanwhile, online RL has long been seeking expressive policy families.
Specifically, \citet{haarnoja2017reinforcement} showed that the optimal stochastic policy of online RL lies in energy-based models~(EBMs), \ie, unnormalized probabilistic models. EBMs are inherently flexible and multimodal due to their unnormalized nature.
% has primarily been solved by using expressive parametric models to represent energy-based policies, \ie, policies represented by unnormalized densities. 
However, sampling and evaluation of EBMs are notoriously difficult due to their intractable likelihood~\cite{song2021train}. A variety of parametrized probabilistic models have been introduced for efficient sampling and learning but with the payoff of approximation error. 
For example, a representative maximum entropy RL algorithm is the well-known soft actor-critic~\citep[SAC, ][]{haarnoja2018soft} that has been the state-of-the-art in online RL. However, SAC restricts the policy space to Gaussian distributions, thereby losing the inherent expressiveness and multimodality of energy-based models (EBMs).
% indicating a potential performance improvement with more expressive policy structures. 

Diffusion models have been shown to be closely related to energy-based models (EBMs), as they can be interpreted as EBMs with multi-level noise perturbations~\cite{song2019generative, shribak2024diffusion}. The multi-level noise perturbations significantly improve the sampling quality, making diffusion policies the perfect fit to represent the energy-based policies. 
However, it is highly non-trivial to train diffusion policies in the context of online RL. The vanilla procedure to train diffusion models requires sampling from target data distribution, which refers to the optimal policy in online RL. However, we can not sample from the optimal policies in online RL directly. Several studies have explored alternative approaches to overcome this limitation.
For example, \citet{psenka2023learning,jain2024sampling} directly match the score functions with derivatives of the learned $Q$-functions and sample with Langevin dynamics.
\citet{yang2023policy} maintain a large number of action particles and fit it with diffusion models. \citet{wang2024diffusion} backpropagate the policy gradient thorough the whole reverse diffusion process. \citet{ding2024diffusion} approximate the policy optimization by maximum likelihood estimation reweighted by the $Q$-function. 
All these methods still encounter various challenges due to inaccurate approximations and/or huge memory and computation costs, limiting the true potential of diffusion policies in online RL.

To handle these challenges, we propose the \algname~(\algabb), an efficient algorithm to train diffusion policies in online RL without sampling from optimal policies. 
% By revisiting the energy-based viewpoint of diffusion models, we show that, although we can not sample from the policy distribution directly, we can train the diffusion policy from the known energy function, which is the soft $Q$-function in max-entropy RL~\cite{haarnoja2017reinforcement}. 
Specifically, 
% \Bo{revise later to echo the main body logic. }\haitong{revised.}
\begin{itemize}[leftmargin=10pt, parsep=5pt]
% \vspace{-10pt}
    \item Developing upon the viewpoint of diffusion models as noise-perturbed EBMs, we first propose the reverse sampling score matching (RSSM), an algorithm to train diffusion models with only access to the energy function (\ie, unnormalized density) while \emph{bypassing sampling from the data distribution}.
    \item We then show that the RSSM enables efficient diffusion policy training when fit into online RL, which leads to a practical implementation named \algname~(\algabb). No specific sampling protocol or recurrent gradient backpropagation is needed during the policy learning stage. 
    We also address practical issues such as exploration and numerical stability.
    \item We demonstrated empirical results, showing that the proposed \algabb outperforms all recent diffusion policy online RL baselines in most OpenAI Gym MuJoCo benchmarks. Moreover, the performance is increased by \textbf{more than 120\% over SAC} on complex locomotion tasks such as MuJoCo Humanoid and Ant, demonstrating the true potential of diffusion policy in online RL.
\end{itemize}

% \Bo{is this related work? it is too succinct, we may need expand this section. this can be postpone to later part, right before experiments section. At that stage, the method is specified clearly and we can make comparison to the existing diffusion policy methods.}
% Applications of diffusion models for RL and other decision-making problems can be categorized into three major fields depending on the learning objectives: \textbf{i),} Diffusion policies~\cite{wang2022diffusion,chi2023diffusion} learns the policies as mappings from observations to actions as the decision makers; \textbf{ii),} Diffusion-based planning~\cite{janner2022planning,du2024learning} learns to solve a long-horizon trajectory optimization as the guidance for the policies; and \textbf{iii),} Diffusion world models~\cite{ding2024diffusionworldmodel,rigter2023world,shribak2024diffusion} learn the environment transition dynamics. Our scope falls into the diffusion policies that are learned by online RL, specifically. 
 

