
\section{Preliminaries}
We introduce the necessary preliminaries in this section. First, we introduce Markov decision process and maximum entropy reinforcement learning as our policy learning framework, followed by a recap of diffusion models.

\subsection{Maximum Entropy Reinforcement Learning}
\label{sec:max_ent_rl}
\textbf{Markov Decision Processes (MDPs).}~We consider Markov decision process~\citep{puterman2014markov} specified by a tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, r, P, \mu_0, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $r:\Scal\times\Acal\to\RR$ is a reward function,
$P\rbr{\cdot|s, a}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ is the transition operator with $\Delta(\mathcal{S})$ as the family of distributions over $\mathcal{S}, \mu_0 \in \Delta(\mathcal{S})$ is the initial distribution and $\gamma \in(0,1)$ is the discount factor. 

\textbf{Maximum entropy RL.} We follow the maximum entropy RL to learn our diffusion policies~\cite{haarnoja2017reinforcement}. We consider the following entropy-regularized expected return as the policy learning objective,
\begin{equation}
    \arg\max_\pi J(\pi) := \EE_{\pi}\sbr{\sum_{\tau=0}^\infty\gamma^\tau \rbr{r(\sbb_\tau, \ab_\tau) + \lambda\Hcal(\pi(\cdot | \sbb_\tau))}}\label{eq:obj_max_ent_rl}
\end{equation}
where $\Hcal\rbr{\pi(\cdot| \sbb)} = \EE_{\ab\sim\pi(\cdot | \sbb)}[-\log\pi(\ab | \sbb)]$ is the entropy, $\lambda$ is a regularization coefficient for the entropy. 
The soft policy iteration algorithm~\cite{haarnoja2017reinforcement,haarnoja2018soft} is proposed to solve the optimal max-entropy policy. Soft policy iteration algorithm iteratively conducts soft policy evaluation and soft policy improvement, where soft policy evaluation updates the soft $Q$-function by repeatedly applying soft Bellman update operator $\Tcal^\pi$ to current value function $Q:\Scal\times\Acal\to \RR$, \ie, 
\begin{equation}
    \Tcal^\pi Q(\sbb_\tau, \ab_\tau) = r(\sbb_\tau, \ab_\tau) + \gamma\EE_{\sbb_{\tau+1}\sim P}\sbr{V(\sbb_{\tau+1})}\label{eq:soft_pev}
\end{equation}
where $V(\sbb_\tau) = \EE_{\ab_\tau\sim\pi}\sbr{Q(\sbb_\tau, \ab_\tau) - \lambda \log\pi(\ab_\tau\mid \sbb_\tau)}$ \cite{haarnoja2018soft}. Then in the soft policy improvement stage, the policy is updated to fit the target policy
\begin{equation}
    \pi_{\rm target}(\ab| \sbb) 
    % \frac{\exp\rbr{\frac{1}{\lambda}{Q^{\pi_{\rm old}}(\sbb, \ab)}}}{Z^{\pi_{old}}(\sbb)}
    \propto \exp\rbr{\frac{1}{\lambda}{Q^{\pi_{\rm old}}(\sbb, \ab)}} \label{eq:energy_based_opt_pi}
\end{equation}
where ${\pi_{\rm old}}$ is the current policy and $Q^{\pi_{\rm old}}$ is the converged result of \eqref{eq:soft_pev} with $\Tcal^{\pi_{\rm old}}$.
% \Bo{delete this section. we discuss the connection between ebm and diffusion in our method section. as we discuss on last friday, please follow my diffusion spectral representatio paper logic. }

\textbf{Soft Actor-Critic.} 
Although we have closed-form policy~\eqref{eq:energy_based_opt_pi}, it is a \emph{unnormalized} distribution, often referred to as an \emph{energy-based} policy since the unnormalized density is called energy function in literature, which is notoriously difficult to sample from and learn. To enable efficient computation, a natural idea is to approximate the energy-based policies~\eqref{eq:energy_based_opt_pi} with a parametrized distribution. 
A representative algorithm is the well-known soft actor-critic (SAC), which restricts the policy to be a parametrized Gaussian, \ie, $\pi_\theta\rbr{a|s} = \Ncal\rbr{ \mu_{\theta_1}(s), \sigma^2_{\theta_2}(s)}$ and updates the parameters $\theta = [\theta_1, \theta_2]$ by optimizing the $KL$-divergence to the target policy $D_{KL}(\pi_\theta\|\pi_{\rm target})$~\cite{haarnoja2018soft} via policy gradient with parametrization trick, \ie, % which equals a Q-learning style loss with data samples from $\Dcal$
$$
J^\pi_{\rm SAC}(\theta)=\EE_{\sbb\sim\Dcal,\ab\sim\pi_\theta}\sbr{\lambda\log\pi_\theta(\ab|\sbb)-Q^{\pi_{\rm old}}(\sbb,\ab)}. 
$$ 
% \Bo{with explicit SAC policy update objective here. }\haitong{done.}
% Even with the Gaussian reduction, SAC has been SOTA till now, showing the huge potential of the maximum entropy RL. 
The Gaussian approximation loses the inherent expressiveness and multimodality of energy-based policies, thus limiting the performance of maximum entropy RL algorithms. This limitation motivates the pursuit of more expressive policy structures to further enhance performance.


% \textbf{Abstact problem notations.} From now on, we focus on an abstract version of representing energy-based policies in \eqref{eq:energy_based_opt_pi}.
% Specifically, we study how to train and sample from an EBM with the given energy function \emph{only}, \ie, we have a target distribution $p_0$ with known energy function $E$ with $p_0(\xb_0)\propto\exp\rbr{-E(\xb_0)}$. Only means we cannot sample from $p_0$.

\subsection{Denoising Diffusion Probabilistic Models} 
\label{sec.diffusion}
Denoising diffusion probabilistic models~\citep[DDPMs,][]{sohl-dickstein2015deep,song2019generative,ho2020denoising} are powerful tools to represent and generate complex probability distributions. \emph{Given data samples from the data distribution} $p_0$, DDPMs are composed of a forward diffusion process that gradually perturbs the data distribution $\xb_0\sim p_0$ to a noise distribution $\xb_T\sim p_T$, and a reverse diffusion process that reconstructs data distribution $p_0$ from noise distribution $p_T$.
 The forward corruption kernel is usually Gaussian with a variance schedule $\beta_1,\dots,\beta_T$, resulting in the forward trajectories with joint distribution
\begin{align}
    &q_{{0:T}}(\xb_{0:T}) = p_0(\xb_0)\prod_{t=1}^Tq_{t|t-1}\rbr{\xb_t|\xb_{t-1}} \quad \text{where}\notag\\
    &q_{t|t-1}(\xb_t|\xb_{t-1}) := \Ncal(\xb_t;\sqrt{1-\beta_t}\xb_{t-1},\beta_t\Ib)\label{eq:corruption_ddpm}
\end{align}
where $\xb_t$ is random variable at $t$ step, and $p, q$ are probability distributions\footnote{We use $p$ and $q$ interchangeably as density function in this paper. Generally, $p$ represents intractable distributions (like the t-step marginal $p_t(\xb_t)$), and $q$ represents tractable distributions such as the Gaussian corruption $q_{t\mid t-1}(\xb_t|\xb_{t-1})$.}.
The backward process recovers the data distribution from a noise distribution $p_T$ with a series of reverse kernels $p_{t-1|t}(\xb_{t-1}|\xb_t)$. 
The reverse kernels are usually intractable so we parameterize it with neural networks denoted as $p_{\theta;t-1|t}\left(\xb_{t-1} \mid \xb_t\right)$, resulting in a joint distribution of the reverse process,
$$
% \begin{aligned}
    % &
    % \text{where}\\
    % & p_\theta\left(\xb_{t-1} \mid \xb_t\right):=\mathcal{N}\left(\xb_{t-1} ; \boldsymbol{\mu}_\theta\left(\xb_t, t\right), \mathbf{\Sigma}_\theta\left(\xb_t, t\right)\right)
% \end{aligned}
p_\theta\rbr{\xb_{0:T}}=p_T(\xb_T)\prod_{t=1}^Tp_{\theta;t-1|t}(\xb_{t-1}|\xb_{t})\quad
$$
Considering all $(\xb_1,\dots,\xb_T)$ as the latent variables, we can solve the parameters $\theta$ via optimizing the evidence lower bound (ELBO) over $\xb_0$,
$$
{\rm ELBO}(\theta) = \EE_{\xb_0\sim p_0}\EE_{\xb_{1:T}\sim q}\sbr{\log \frac{p_\theta(\xb_{0:T})}{q(\xb_{1:T}|\xb_0)}}.
$$
After fixing $p_{\theta;t-1|t}$ to be Gaussian and reparametrizing $p_{\theta;t-1|t}$ with a score network\footnote{Some paper reparameterize it as the noise prediction network $\epsb_\theta$, but they are the same in essence since $\nabla_{\xb_t} \log q_{t|0}(\xb_t|\xb_0) = -\frac{\xb_t -\sqrt{\bar\alpha_t}\xb_0}{1 -\bar\alpha_t}=-\frac{\epsb}{\sqrt{1 -\bar\alpha_t}}$ for Gaussian noise $\epsb$.} $s_\theta(\xb_t;t)$, maximizing the ELBO is equivalent to minimizing a collection of denoising score matching loss over multiple noise levels indexed by $t$~\cite{vincent2011connection,ho2020denoising},
\begin{equation}
    \begin{aligned}
        &\Lcal_{\rm DSM}(\theta):=\frac{1}{T}\sum_{t=0}^T(1 -\bar\alpha_t)\underset{\substack{\xb_0\sim p_0\\\xb_t\sim q_{t|0}}}{\EE}\sbr{\nbr{ s_\theta\rbr{\xb_t; t} - \nabla \log q_{t|0}(\xb_t|\xb_0)}^2}\label{eq:ddpm_loss}
    \end{aligned}
\end{equation}
where $q_{t|0}(\ab_t|\ab_0):=\Ncal(\ab_t;\sqrt{\bar\alpha_t}\ab_0, \rbr{1 - \bar\alpha_t}\Ib)$ and $\bar\alpha_t = \prod_{l=1}^t (1-\beta_l)$. 
After learning the $s_\theta$ by minimizing \eqref{eq:ddpm_loss}, we can draw sample via the reverse diffusion process by iteratively conducting
\begin{equation}
    \xb_{t-1}=\frac{1}{\sqrt{\bar\alpha_t}}\left(\xb_t+\beta_t s_\theta\left(\xb_t, t\right)\right)+\frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t}\beta_t \zb_t\label{eq:annealed_langevin_2}
\end{equation}
for $t = T, T-1,\dots, 1$ and $\zb_t\sim\Ncal\rbr{0,\Ib}$.

% \subsection{DDPM as Energy-Based Models} 
% Diffusion models can also be interpreted as energy-based models~(EBMs).
% EBMs usually refer to parametrized probabilistic models that are not normalized. The density function of an EBM is generally in the formulation of
% $
% p_\theta(\xb) = \frac{\exp\rbr{ - E_\theta(\xb)}}{Z_\theta}
% $,
% where $E_\theta(x)$ is the \emph{energy function} parameterized by $\theta$, $Z_\theta = \int \exp\rbr{ - E_\theta(\xb)} d \xb$ is the normalization constant that is usually intractable. The \emph{score function} is defined as the gradient of log density, $\nabla_x\log p_\theta(x) $. 

% The unnormalized nature of EBMs allows versatility and rich expressiveness compared to other probabilistic models but raises difficulties in training and sampling~\cite{song2021train}. One of the naive approaches before DDPM is to first learn the score function $\nabla_{\xb}\log p(\xb)$ via score matching~\cite{hyvarinen2005estimation,song2020sliced} and then sample via Langevin dynamics with the learned score function $f_\theta(\xb)\approx\nabla_{\xb}\log p(\xb)$~\cite{parisi1981correlation},
% % , a Markov chain Monte-Carlo (MCMC) sampling procedure using only the score function,
% \begin{equation}
%     \xb_{i+1} \leftarrow \xb_i+\eta f_\theta(\xb)+\sqrt{2 \eta} \mathbf{z}_i, \quad i=0,\cdots, K\label{eq:langevin}
% \end{equation}
% where $\mathbf{z}_i \sim \mathcal{N}(0, I)$. As $\eta\to 0$ and $K\to\infty$, the sampled $\xb_K$ converges to samples from $p(\xb)$. 

% \citet{song2019generative} showed that this naive approach, combining score matching with Langevin dynamics, suffered from multiple pitfalls, such as slow mixing and inaccuracy in low-density regions, which hinders the empirical performance of score-based EBMs. One remedy proposed by \citet{song2019generative} is to fit the score function of a series of noise-perturbed data distribution $\Ncal\rbr{\xb_i;\xb, \sigma_i^2 \Ib}, i=\{1,2,\dots, K\}$ with a noise schedule $\sigma_1> \sigma_2>\dots>\sigma_K$. 
% The resulting models, named the noise-conditioned score networks (NCSN) $f_\theta\rbr{\xb_i;\sigma_i}$, take the noise level into the inputs and are learned by denoising score matching~\cite{vincent2011connection}
% \begin{align}
%     % &\EE_{\xbtil\sim \ptil}\sbr{\nbr{f_\theta\rbr{\xb_i;i} - \nabla_{\xbtil}\log \ptil_\sigma\rbr{\xbtil}}^2} =\label{eq:noisy_score_matching}\\
%     &\EE_{\xb\sim p,\xb_i\sim\Ncal\rbr{\xb,\sigma_i^2 \Ib}}\sbr{\|f_\theta\rbr{\xb_i;\sigma_i} -\nabla_{\xb_i}\log q(\xb_i|\xb) \|^2}\label{eq:dsm_loss}
% \end{align}
% Then in the sampling stage, \citet{song2019generative} replaced the original score function in \eqref{eq:langevin} with the learned noisy score function $f_\theta(\xbtil;\sigma_i)$,
% \begin{equation}
%     \xb_{i+1} \leftarrow \xb_i+\eta f_\theta(\xbtil;\sigma_i)+\sqrt{2 \eta} \mathbf{z}_i, \quad i=0,\cdots, K\label{eq:annealed_langevin}
% \end{equation}
% named as annealed Langevin dynamics. The scheduled noise perturbation design significantly improved the image generation performance to match the state-of-the-art (SOTA) at that time~\cite{song2019generative}, which is further refined by DDPM.

% We can see that the annealed Langevin dynamics~\eqref{eq:annealed_langevin} resembles the DDPM sampling~\eqref{eq:annealed_langevin_2} with different scale factors, and the denoising score matching loss~\eqref{eq:dsm_loss} is equivalent to \eqref{eq:ddpm_loss} since 
% $\nabla_{\xb_i}\log q(\xb_i|\xb)=-\frac{\xb_i-\xb}{\sigma_i^2}=-\frac{\epsb}{\sigma_i}$. Therefore, DDPM can be interpreted as EBMs and is equivalent to NCSN with different designs on noise schedules and scale factors. A more thorough discussion on their equivalency can also be found in~\citet{ho2020denoising,song2021scorebased}.










