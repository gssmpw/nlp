
\section{Experimental Results}
\label{sec:exp}
This section presents the experimental results. We first use a toy example, generating a 2D Gaussian mixture, to verify the effectiveness of the proposed RSSM as diffusion model training. Then we show the empirical results of the proposed \algabb algorithm evaluated with OpenAI Gym MuJoCo tasks.

\begin{figure*}[h]
    \subfigure[\label{fig:subfig:data}]{\includegraphics[width=0.19\linewidth]{figure/data.png}}
    \subfigure[\label{fig:subfig:rssm1}]{\includegraphics[width=0.19\linewidth]{figure/generate_gaussian.png}}
    % \subfigure[RSSM with $p_{{\rm sample}, t}:=p_t$.]{\includegraphics[width=0.19\linewidth]{figure/generate_pt.pdf}}
    \subfigure[\label{fig:subfig:rssm2}]{\includegraphics[width=0.19\linewidth]{figure/generate_uniform.png}}
    \subfigure[\label{fig:subfig:dsm}]{\includegraphics[width=0.19\linewidth]{figure/generate_dsm.png}}\subfigure[\label{fig:subfig:langevin}]{\includegraphics[width=0.19\linewidth]{figure/generate_langevin.png}}
    
    \caption{The scatter plots of generating 2D Gaussian mixture, the histograms show the partition on each axis. \Cref{fig:subfig:data} shows the true data samples with mixing coefficients [0.8, 0.2]. Fig.  \ref{fig:subfig:rssm1},\ref{fig:subfig:rssm2},\ref{fig:subfig:dsm} show that both the proposed RSSM (with two sample schemes) and denoising score matching can approximately recover the true data distribution. \Cref{fig:subfig:langevin} shows the slow mixing of Langevin dynamics that the mixing coefficients can not be correctly recovered. }
\end{figure*}

\begin{table*}[ht]
    \centering
    \caption{Performance on OpenAI Gym MuJoCo environments. The numbers show the best mean returns and standard deviations over 200k iterations and 5 random seeds.}
    \label{tab:performance}
    
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lllllll}
    \toprule
    & & \textsc{HalfCheetah} &
    \textsc{Reacher} &
    \textsc{Humanoid} & 
    \textsc{Pusher} &
    \textsc{InvertedPendulum} \\
    \midrule
    \multirow{3}{*}{\textbf{\begin{tabular}{@{}l@{}}Classic \\ Model-Free RL\end{tabular}}}
     & PPO 
       & $4852 \pm 732$
       & $-8.69 \pm 11.50$
       & $952 \pm 259$
       & $-25.52 \pm 2.60$
       & $\mathbf{1000 \pm 0}$ \\
     & TD3 
       & $8149 \pm 688$
       & $\mathbf{-3.10 \pm 0.07}$
       & $5816 \pm 358$
       & $\mathbf{-25.07 \pm 1.01}$
       & $\mathbf{1000 \pm 0}$ \\
     & SAC 
       & $8981 \pm 370$ 
       & $-65.35 \pm 56.42 $ 
       & $2858 \pm 2637$
       & $-31.22 \pm 0.26$
       & $\mathbf{1000 \pm 0}$ \\
    \midrule
    \multirow{5}{*}{\textbf{Diffusion Policy RL}}
     & QSM 
     & $10740 \pm 444$
     & $-4.16 \pm 0.28 $
     & $5652 \pm 435$
     & $-80.78 \pm 2.20$
     & $\mathbf{1000 \pm 0}$ \\
     & DIPO 
     & $9063 \pm 654$ 
     & $-3.29 \pm 0.03 $
     & $4880 \pm 1072$
     & $-32.89 \pm 0.34$
     & $\mathbf{1000 \pm 0}$ \\
     & DACER &  $11203 \pm 246$
     & $-3.31 \pm 0.07 $
     & $2755 \pm 3599$
     & $-30.82 \pm 0.13$
     & $801 \pm 446 $ \\
     & QVPO 
     & $7321 \pm 1087$
     & $-30.59 \pm 16.57$
     & $421 \pm 75$
     & $-129.06 \pm 0.96$
     & $\mathbf{1000 \pm 0}$                   \\
     & DPPO 
     & $1173 \pm 392 $
     & $-6.62 \pm 1.70 $
     & $484 \pm 64$
     & $-89.31 \pm 17.32 $
     & $\mathbf{1000 \pm 0}$\\
     & \textbf{SDAC~(ours)} &  $\mathbf{11924 \pm 609}$
     & $\mathbf{-3.14 \pm 0.10}$
     & $\mathbf{6959 \pm 460}$
     & $\mathbf{-30.43 \pm 0.37}$
     & $\mathbf{1000 \pm 0}$ \\
    \toprule
    & & \textsc{Ant} & \textsc{Hopper} 
       & \textsc{Swimmer} 
       & \textsc{Walker2d}& \textsc{Inverted2Pendulum}  \\
    \midrule
    \multirow{3}{*}{\textbf{\begin{tabular}{@{}l@{}}Classic \\ Model-Free RL\end{tabular}}}
     & PPO 
       & $3442 \pm 851$
       & $3227 \pm 164$
       & $84.5 \pm 12.4$
       & $4114 \pm 806 $
       & $9358 \pm 1$ \\
     & TD3 
       & $3733 \pm 1336$
       & $1934 \pm 1079$
       & $71.9 \pm 15.3$
       & $2476 \pm 1357$
       & $\mathbf{9360 \pm 0}$ \\
     & SAC
       & $2500 \pm 767$
       & $3197 \pm 294 $
       & $63.5 \pm 10.2$
       & $3233 \pm 871$
       & $9359 \pm 1$\\
    \midrule
    \multirow{5}{*}{\textbf{Diffusion Policy RL}}
     & QSM   
     & $938 \pm 164 $
     & $2804 \pm 466 $
     & $57.0 \pm 7.7$
     & $2523 \pm 872 $
     & $2186 \pm 234$\\
     & DIPO  
     & $965 \pm 9 $
     & $1191 \pm 770 $
     & $46.7 \pm 2.9$
     & $1961 \pm 1509 $
     & $9352 \pm 3$
     \\
     & DACER 
     & $4301 \pm 524 $
     & $3212 \pm 86$
     & $103.0 \pm 45.8$
     & $3194 \pm 1822 $
     & $6289 \pm 3977$\\
     & QVPO  
     & $718 \pm 336$
     & $2873 \pm 607$
     & $53.4 \pm 5.0$
     & $2337 \pm 1215$
     & $7603 \pm 3910$\\
     & DPPO
     & $ 60 \pm 15$
     & $ 2175\pm 556$
     & $ \mathbf{106.1\pm 6.5}$
     & $ 1130\pm 686$
     & $ 9346\pm 4$\\
     & \textbf{SDAC~(ours)} 
     & $\mathbf{5683 \pm 138}$
     & $\mathbf{3275 \pm 55}$
     & $\mathbf{79.3 \pm 52.5}$
     & $\mathbf{4365 \pm 266}$
     & $\mathbf{9360 \pm 0}$\\
    \bottomrule
    \end{tabular}%
    }
    \end{table*}
    
\subsection{Toy Example}
\label{sec:toy}
We first show a toy example of generating a 2D Gaussian mixture dataset to verify the effectiveness of the proposed RSSM loss in training diffusion models as generative models. The Gaussian mixture model is composed of two modes whose mean values are $[3, 3]$ and $[-3, -3]$ and mixing coefficients are $0.8$ and $0.2$ shown in \Cref{fig:subfig:data}. The detailed training setup can be found in \Cref{sec:apdx_toy_example}.

We compare diffusion models trained with 
 three loss functions: \textbf{a.} proposed RSSM loss~\eqref{eq:thm1_reweight_loss} with $\ptil_t=\Ncal\rbr{0, 4\Ib}$ for all $t$ in \Cref{fig:subfig:rssm1}. \textbf{b.} proposed RSSM loss ~\eqref{eq:thm1_reweight_loss} with uniform sampling distribution $\ptil_t=\operatorname{Unif}\rbr{[-6, 6]}$ for all $t$ in \ref{fig:subfig:rssm2}. Both two RSSM algorithms have access to the true energy function but cannot sample directly from the true data.  \textbf{c.}~the commonly used denoising score matching loss~\eqref{eq:ddpm_loss} in \Cref{fig:subfig:dsm}, which has access to sample from the Gaussian mixture. We also show the naive Langevin dynamics~\cite{parisi1981correlation} samples as a reference in \Cref{fig:subfig:langevin}, which has access to the true score function.

 Empirical results showed that both two diffusion models trained by RSSM and the one trained by vanilla denoising score matching can approximately recover both two modes and the mixing coefficients, which verifies the effectiveness of the proposed RSSM algorithm. Moreover, with a uniform $\ptil_t$, the proposed RSSM achieves the lowest KL divergence to true data samples in the three loss functions.

Moreover, the Langevin dynamics samples in \Cref{fig:subfig:langevin} show that even with the true score function, Langevin dynamics can not correctly recover the mixing coefficient in finite steps~(20 steps in this case), demonstrating the slow mixing problem and further verifying the necessity of diffusion models even with given energy or score functions. 
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.19\linewidth]{figure/HalfCheetah.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Reacher.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Humanoid.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Pusher.pdf}
    \includegraphics[width=0.19\linewidth]{figure/InvertedPendulum.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Ant.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Hopper.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Swimmer.pdf}
    \includegraphics[width=0.19\linewidth]{figure/Walker2d.pdf}
    \includegraphics[width=0.19\linewidth]{figure/InvertedDoublePendulum.pdf}
    \includegraphics[width=0.8\linewidth]{figure/legend.pdf}
    
    \caption{Average return over 20 evaluation episodes every 25k iterations (125k for Humanoid) during training. We select the top 5 baselines ranked by average performance over all tasks for clarity. The errorbars are standard deviations over 5 random seeds.}
    \label{fig:main}
    
\end{figure*}
\subsection{OpenAI Gym MuJoCo Tasks}
\subsubsection{Experimental Setup}
We implemented our algorithm with the JAX package\footnote{The implementation will be published upon acceptance.} and evaluated the performance on 10 OpenAI Gym MuJoCo v4 tasks. All environments except Humanoid-v4 are trained over 200K iterations with a total of 1 million environment interactions while Humanoid-v4 has five times more. 

\textbf{Baselines.} The baselines include two families of model-free RL algorithms. The first family is diffusion policy RL, which includes a collection of recent diffusion-policy online RLs including  QSM~\cite{psenka2023learning}, QVPO~\cite{ding2024diffusion}, DACER~\cite{wang2024diffusion}, DIPO~\cite{yang2023policy} and DPPO~\cite{ren2024diffusion}. The second family is classic model-free online RL baselines including PPO~\cite{schulman2017proximal}, TD3~\cite{fujimoto2018addressing} and SAC~\cite{haarnoja2018soft}. A more detailed explanation to the baselines can be found in Appendix \ref{sec:apdx_baselines}.


\subsubsection{Experimental Results}
The performance and training curves are shown in \Cref{tab:performance} and \Cref{fig:main}, which shows that our proposed algorithm outperforms all the baselines in all OpenAI Gym MuJoCo environments except \texttt{Swimmer}. Especially, for those complex locomotion tasks including the \texttt{HalfCheetah}, \texttt{Walker2d}, \texttt{Ant}, and \texttt{Humanoid}, we obtained \textbf{32.8\%, 35.0\%, 127.3\%, 143.5\%} performance improvement compared to SAC and \textbf{at least 6.4\%, 36.7\%, 32.1\%, 23.1\%} performance improvement compared to other diffusion-policy online RL baselines (not the same for all environments), respectively, demonstrating the superior and consistent performance of our proposed algorithm and the true potential of diffusion policies in online RL.

Moreover, the performance of the proposed \algabb is very stable and consistently good for all the tasks, demonstrating the superior robustness of \algabb. On the contrary, every diffusion-policy RL baseline performed badly on one or some tasks. For example, QSM failed the InvertedDoublePendulum, possibly because its true value function is known to be highly non-smooth. The non-smooth nature results in bad score function estimations since QSM matches the score function by differentiating the $Q$-functions. QVPO failed Reacher and Pusher since it cannot handle negative $Q$-functions. DACER failed InvertedPendulum despite its good performance in some complex tasks, probably due to the gradient instability when backpropagated recursively. 

\textbf{Computation and memory cost.}
We count the GPU memory allocations and total computation time listed in \Cref{tab:time}. The computation is conducted on a desktop with AMD Ryzen 9 7950X CPU, 96 GB memory, and NVIDIA RTX 4090 GPU. We achieve low memory consumption and faster computations compared to other diffusion-policy baselines. Note that the QSM essentially conducts the Langevin dynamics, which does not involve diffusion policies with the multi-level noise perturbation in essence. We can still achieve a comparable computation time and memory cost with QSM, indicating the proposed \algabb does not add much extra computational cost due to the diffusion policies.

\begin{table}[ht] 
    \centering
    % 
    \caption{GPU memory allocation and total compute time of 200K iterations and 1 million environment interactions. *QSM did not learn diffusion policies essentially thus the computation is lightweight.\label{tab:time}}
    {
    \small
        \begin{tabular}{r|c | c}
    \toprule
         Algorithm &
         % \begin{tabular}{@{}c@{}}GPU Memory \\ (MB)\end{tabular}
         GPU Memory (MB)
         & 
         % \begin{tabular}{@{}c@{}}Total training time \\ (min)  \end{tabular}
         Training time (min)
          \\
         \midrule
         QSM*& 997 & 14.23\\
         \midrule
         QVPO & 5219 & 30.90\\
         DACER &1371 & 27.61\\
         DIPO & 5096 & 19.31\\
         DPPO & 1557 & 95.16\\
         \textbf{SDAC~(ours)} & \textbf{1113} & \textbf{16.10}\\
         \bottomrule
    \end{tabular}
    }
\end{table}

    \textbf{Sensitivity analysis.}
    In Figure~\ref{fig:ablation-study}, we perform sensitivity analyses of different diffusion steps and diffusion noise schedules. Results show that 10 and 20 diffusion steps obtain comparable results, both outperforming the 30-step setting. The linear and cosine noise schedules perform similarly, and both outperform the variance preserve schedule. Therefore we choose 20 steps and cosine schedules for all tasks. The results also shows that \algabb is robust to diffusion process hyperparameters.
    % \vspace{-5pt}
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.24\linewidth]{figure/Ant_steps.pdf}
        \includegraphics[width=0.24\linewidth]{figure/Ant_schedule.pdf}\\
        \caption{Sensitivity analysis on diffusion steps and diffusion noise schedule on Ant-v4.}
        \label{fig:ablation-study}
        % \vspace{-15pt}
    \end{figure}
