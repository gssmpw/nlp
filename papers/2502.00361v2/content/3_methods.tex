\section{Efficient Diffusion Policy Learning in Online RL}
\label{sec:rssm}
\haitong{TODO:\\
- repeat contribution from RL,\\
- add section afterward connecting back to RL,\\
- add before 3.2 to say we abstract our problem, unable to sample, to training the diffusion model ...\\
- problem statement more clear\\
}

We introduce the proposed diffusion policy training algorithm in this section. First, we revisit the problem formulation and the difficulty of diffusion policy learning in online RL.
Then we introduce our core contribution, the reverse sampling score matching (RSSM), and the equivalence to denoising score matching.

\subsection{Problem Formulation and Online RL Setup}
We try to learn a diffusion model $\epsb_\theta $ to represent energy-based policy $\pi(\ab|\sbb)\propto \exp\rbr{\frac{1}{\lambda}{Q^{\pi_{\rm old}}(\sbb, \ab)}}$ in \eqref{eq:energy_based_opt_pi} using sampling scheme \eqref{eq:annealed_langevin_2} with learned $Q^{\pi_{\rm old}}$.
The problem can be abstracted as learning a diffusion model to represent a data distribution whose density $p_0$ satisfies
\begin{equation}
    p_0(\xb_0)\propto\exp\rbr{\frac{-E(\xb_0)}{\lambda}}\label{eq:prob_abstract}
\end{equation}
where the energy function $E$ corresponds to soft $Q$ function $-Q^{\pi_{\rm old}}$ in \eqref{eq:energy_based_opt_pi} and the $\xb_0$ refers to $\ab$ when given $\sbb$.

\textbf{Difficulties specific to the online RL setup}. Note that in the online RL problem, we were \emph{given a learned energy function $E$}, but \emph{can not sample from $p_0$}. 
Compared to common diffusion model training setup in \Cref{sec.diffusion}, not being able to sample from $p_0$ makes both the ELBO and loss function in \eqref{eq:ddpm_loss} intractable, thus training diffusion model via the denoising score matching loss~\eqref{eq:ddpm_loss} is impossible.
    
\subsection{Diffusion Model as Sufficient Representations of Energy-based Policy}
Before delving deeper, some might ask

{
\centering
\textit{Can diffusion policy effectively represent the energy-based policy~\eqref{eq:energy_based_opt_pi}?}
}

We give an affirmative answer since many studies have already pointed out the EBM viewpoint of diffusion models~\cite{song2019generative,ho2020denoising,song2021scorebased}.  

First, the noise prediction model $\epsb_\theta$ implicitly computes multi-level noise perturbed score functions indexed by $t$, \ie,
\begin{equation}
    \epsb_\theta(\xb_t, t)\approx-\sqrt{1 - \bar\alpha_t}~\nabla_{\xb_t}\log p_t(\xb_t)\label{eq:goal}.
\end{equation}
where $p_t(\xb_t) = \int q_{t|0}(\xb_t|\xb_0)p_0(\xb_0)d\xb_0$
is the $p_0$ perturbed by noise level $\beta_t$ for $t=1,2,\dots, T$. 
We show by revisiting a key equivalence proposed in \citet{vincent2011connection},
\begin{proposition}[Denoising score matching equivalent to noise-perturbed sore matching. \citet{vincent2011connection}]
The step $t$ term in DDPM loss function~\eqref{eq:ddpm_loss} can be written as
\begin{equation}
    \underset{\substack{\xb_0\sim p_0\\\xb_t\sim p_t}}{\EE}\sbr{\nbr{\epsb_\theta\rbr{\xb_t, t} + \sqrt{1 -\bar\alpha_t}\nabla_{\xb_t}\log q(\xb_t|\xb_0)}^2}\label{eq:ddpm_loss_t_step}
\end{equation}
    which is equivalent to 
    \begin{equation}
    \EE_{\xb_t\sim p_t}\sbr{\nbr{\epsb_\theta\rbr{\xb_t, t} + \sqrt{1 -\bar\alpha_t} \nabla_{\xb_t}\log p_t(\xb_t)}^2}\label{eq:original_sm_loss}
    \end{equation}
\end{proposition}
We repeated the proof in Appendix \haitong{tba}. It is easy to see that minimizing loss function \eqref{eq:original_sm_loss} leads to \eqref{eq:goal}.

For the sampling stage, the diffusion model sampling scheme~\eqref{eq:annealed_langevin_2} resembled Langevin dynamics~\cite{parisi1981correlation}, a Markov chain Monte-Carlo sampling method of EBMs, 
via replacing the score function $\nabla_\xb\log p_0(\xb)$ with learned $\epsb_\theta$~\cite{ho2020denoising}. 


In fact, the idea of multi-level noise perturbations, also called simulated annealing or stochastic localization \haitong{add ref}, is the key to the success of diffusion models and significantly improves the performance over score matching with Langevien dynamics~\cite{song2019generative,ho2020denoising}. We will demonstrate the empirical advantages of diffusion models over naive Langevin dynamics in \Cref{sec:exp}.


\subsection{Reverse Sampling Score Matching~(RSSM)}
\label{subsec:main_theorem}
Despite the fact that , we still face the difficulty of not being able to sample from $p_0$ and then compute denoising score matching loss~\eqref{eq:ddpm_loss}. To obtain a tractable training framework, we first show that the learning goal of noise prediction model $\epsb_\theta$ is to fit a scaled noise-perturbed score function,
\ie,
\begin{equation}
    \epsb_\theta(\xb_t, t)\approx-\sqrt{1 - \bar\alpha_t}~\nabla_{\xb_t}\log p_t(\xb_t)\label{eq:goal}.
\end{equation}


Now we introduce our training algorithm, reverse sampling score matching. Instead of taking the expectation of the squared error on $p_t(\xb_t)$ like \eqref{eq:original_sm_loss}, we select arbitrary properly defined\footnote{$g$ is strictly positive everywhere on the support of $p_t(\xb_t)$ \haitong{bounded?}.} weight function $g:\Xcal\to\RR^+$ to define a general loss function to learn \eqref{eq:goal},
\begin{equation}
    \int g(\xb_t)\nbr{\epsb_\theta\rbr{\xb_t;t} +\sqrt{1 - \bar\alpha_t} \nabla_{\xb_t}\log p_t\rbr{\xb_t}}^2d\xb_t \label{eq:reweighted_score_matching}
\end{equation}

Then we propose our training algorithm by properly selecting the function $g$. Specifically,
\begin{theorem}[Reverse sampling score matching (RSSM)]
    Define $p_{{\rm sample},t}(\xb_t)$ is the sampling distribution that has the same support as $p_t(\xb_t)$.
    Construct the reweighting function $g$ as 
    \begin{equation}
        g(\xb_t) = Z(\xb_t)p_{{\rm sample},t}(\xb_t)\label{eq:reweighting_g}
    \end{equation}
    where $Z(\xb_t) = \int\exp\rbr{-E(\xb_0)}q_{t|0}(\xb_t\mid \xb_0)d\xb_0$. Then the loss function in \eqref{eq:reweighted_score_matching} is equivalent to
    {
    \small
    \begin{equation}
        \underset{\xb_t,\epsb}{\EE}\sbr{\exp \rbr{-E\rbr{\frac{1}{\sqrt{\bar\alpha_t}}\xb_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb}}\nbr{\epsb_\theta\rbr{\xb_t;t} - \epsb}^2}\label{eq:thm1_reweight_loss}
    \end{equation}
    }
    where $\xb_t\sim p_{{\rm sample},t},\epsb\sim\Ncal(0, \Ib)$.
\end{theorem}
\begin{proof}
    First, the constructed $g$ is strictly positive everywhere on the support of $p_t(\xb_t)$ since $Z(\xb_t)$ is the unnormalized $p_t(\xb_t)$ and we select $p_{{\rm sample},t}(\xb_t)$ to have the same support as $p_t$. 
    Then by algebraic operations, we can show that the loss function in \eqref{eq:reweighted_score_matching} is equivalent to
    \begin{equation}
        \iint \frac{g(\xb_t)}{Z(\xb_t)}q_{t|0}(\xb_t\mid \xb_0)\exp\rbr{-E(\xb_0)}l_\theta(\xb_0,\xb_t)d\xb_0d\xb_t \label{eq:reformat_loss_p}
    \end{equation}
    where 
    $l_\theta(\xb_0,\xb_t) = \nbr{\epsb_\theta\rbr{\xb_t;t} +\sqrt{1 - \bar\alpha_t} \nabla_{\xb_t}\log q_{t|0}\rbr{\xb_t|\xb_0}}^2$. 
    The detailed derivation is deferred to Appendix \ref{sec:appendix_derivation} for space limit.
    
    The loss function in \eqref{eq:reformat_loss_p} is still not tractable. To handle this, we use a \emph{reverse sampling trick}, \ie, replacing $q_{t|0}$ with a reverse sampling distribution $\qtil_{0|t}$ that satisfies
    \begin{equation}
        \begin{aligned}
        &\qtil_{0|t}(\xb_0\mid \xb_t) =\Ncal\rbr{\xb_0;\frac{1}{\sqrt{\bar\alpha_t}}\xb_t, \frac{1 - \bar\alpha_t}{\bar\alpha_t}\Ib}\\
        \propto~& q_{t|0}(\xb_t\mid \xb_0) = \Ncal\rbr{\xb_t;\sqrt{\bar\alpha_t}\xb_0, \rbr{1 - \bar\alpha_t}\Ib}
    \end{aligned}\label{eq:reverse_gaussian}
    \end{equation}
    and thus
    $$
    \nabla_{\xb_t}\log q_{t|0}(\xb_t\mid \xb_0) = \nabla_{\xb_t}\log \qtil_{0|t}(\xb_0\mid \xb_t) = - \frac{\xb_t -\sqrt{\bar\alpha_t}\xb_0}{1 -\bar\alpha_t}
    $$
    Then we can replace $q_{t|0}$ with $\tilde q_{0|t}$ and substitute the definition of $g$ in \eqref{eq:reformat_loss_p}, we have 
    {
    \small
    $$
    \iint p_{{\rm sample,}t}(\xb_t)\tilde q_{0|t}(\tilde\xb_0\mid \xb_t)\exp\rbr{-E(\tilde\xb_0)}l_\theta(\tilde\xb_0,\xb_t)d\tilde\xb_0d\xb_t 
    $$
    }
    where
    $$
    l_\theta(\tilde\xb_0,\xb_t) = \nbr{\epsb_\theta\rbr{\xb_t;t} +\sqrt{1 - \bar\alpha_t} \nabla_{\xb_t}\log \tilde q_{0|t}\rbr{\tilde\xb_0\mid \xb_t}}^2
    $$
    Then we can reformulate to derive the loss function in \eqref{eq:thm1_reweight_loss}. The detailed derivation can be found in Appendix \ref{sec:appendix_derivation}.
\end{proof}



Combining all timestep $t$ together, we arrive at the final loss function of RSSM,
\begin{subequations}
    \begin{align}
        & \Lcal_{\rm RSSM}(\theta) := \notag\\
        &\frac{1}{T}\sum_{t=0}^T\EE_{\xb_t,\epsb}\sbr{\exp\rbr{-E(\tilde\xb_0)}\nbr{\epsb_\theta\rbr{\xb_t, t} - \epsb}^2}\label{eq:our_final_loss}\\
        &\text{where}~ \tilde\xb_0 = \frac{1}{\sqrt{\bar\alpha_t}}\xb_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb,\ \epsb\sim\Ncal\rbr{0,\Ib}\xb_t\sim p_{{\rm sample},t}\label{eq:reverse_sample}
    \end{align}
\end{subequations}

% Note that although the proportional in \eqref{eq:reverse_gaussian} hides a constant relevant with $t$,  \citet{ho2020denoising} showed that the weights on $t$ do not matter in the empirical performance. Therefore, we just ignore the $t$-dimension weights and uniformly sampled $t$. 

\begin{remark}[Reverse sampling]
    The name \emph{reverse sampling} score matching comes from that we first sample $\xb_t\sim p_{{\rm sample}, t}$ and then sample $\tilde\xb_0$ following \eqref{eq:reverse_sample}, a similar rule when reconstructing $\xb_0$ from $\xb_t$ with given model output $\epsb_\theta(\xb_t, t)$. 
\end{remark}

\subsection{RSSM for Diffusion Policy Learning in Online RL}
we apply our RSSM loss function in \eqref{eq:our_final_loss} to conditional distribution $\pi$ and take expectation over the states,
{
\small
\begin{equation}
    \Lcal^{\pi}_{\rm RSSM}(\theta;Q,\lambda) :=\underset{{\sbb, t,\ab_t,\epsb}}{\EE}\sbr{\exp\rbr{\frac{Q(\sbb,\tilde\ab_0)}{\lambda}}\nbr{ \epsb_\theta\rbr{\ab_t,\sbb,  t} - \epsb}^2}\label{eq:policy_loss}
\end{equation}
}
with 
\begin{equation}
    \tilde\ab_0 = \frac{1}{\sqrt{\bar\alpha_t}}\ab_t + \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb\label{eq:a0_sample}
\end{equation}
and $\sbb$ sampled from the replay buffer, $t$ uniformly sampled from $\{1,2,\dots,T\}$, $\ab_t$ sampled from a sampling distribution $p_{{\rm sample}, t}(\cdot\mid \sbb)$, and $\epsilon\sim\Ncal\rbr{0, \Ib}$. 

\textbf{Sampling distribution selection.} In the online RL setting, we need to query the $Q$ function in \eqref{eq:policy_loss}. As the $Q^\pi(\sbb,\ab)$ gets more accurate when $\ab$ is closed to the current policy, it is better to have the sampled $\tilde\xb_0$ closed to the intractable target policy \eqref{eq:energy_based_opt_pi}. We already discussed in Example \ref{example:sample_dist} that if we select $p_t$ as $p_{{\rm sample}, t}$ then the sampled $\tilde\xb_0$ centers around $\xb_0$. As we do not know the $p_t$, we use the $t$-step in the reverse process as an approximated $p_t$ as our sampling distribution.
 \haitong{add another section connect back to reinforcement learning, }


