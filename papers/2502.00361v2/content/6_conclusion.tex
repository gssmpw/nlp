\section{Conclusion}
In this paper, we proposed \algname~(\algabb), an efficient diffusion policy training algorithm tailored for online RL. Regarding diffusion models as noise-perturbed EBMs, we develop the reverse sampling score matching to train diffusion models with access only to the energy functions and bypass sampling from the data distribution. In this way, we can train a diffusion policy with only access to the soft $Q$-function as the energy functions in online maximum entropy RL. Empirical results have shown superior performance compared to SAC and other recent diffusion policy online RLs. Possible future directions include improving the stability of diffusion policies and efficient exploration scheme design.
