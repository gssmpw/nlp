\section{Related works}
\label{sec:apdx_related_works}

\textbf{Diffusion models for decision making.} Due to their rich expressiveness in modeling complex and multimodal distributions, diffusion models have been leveraged to represent stochastic policies~\cite{wang2022diffusion,chen2022offline,hansen2023idql}, plan trajectories~\cite{janner2022planning,chi2023diffusion,du2024learning} and capture transition dynamics~\cite{rigter2023world,ding2024diffusionworldmodel,shribak2024diffusion}. Specifically, we focus on the diffusion policies. Diffusion policies have been primarily used on offline RL with expert datasets, where the denoising score matching~\eqref{eq:ddpm_loss} is still available and the learned $Q$-function only provides extra guidance such as regularization~\cite{wang2022diffusion} or multiplication in the energy function.
However, in online RL we do not have the dataset, thus denoising score matching is impossible.

\textbf{Diffusion Models.}
Diffusion models have a dual interpretation of EBMs and latent variable models. The latent variable interpretation is motivated by the solving reverse-time diffusion thermodynamics via multiple layers of decoder networks \cite{sohl-dickstein2015deep}. It was later refined by \citet{ho2020denoising} via simplified training loss. The EBM interpretation aims to solve pitfalls in Langevin dynamics sampling by adding progressively decreasing noise~\cite{song2019generative}. Then the two viewpoints are merged together with viewpoints from stochastic differential equations~\cite{song2021scorebased}, followed by numerous improvements on the training and sampling design~\cite{song2022denoising,karras2022elucidating}.

% The success of diffusion models is credit to the multiple decoder network structures. In the generation process of diffusion models, multiple decoder networks progressively subtract noise from the latent space samples (usually Gaussian). \citet{song2019generative,ho2020denoising} showed that the progressive denoising is equivalent to the \emph{annealed }Langevin dynamics sampling in EBMs. Compared to the Langevin dynamics sampling, a standard Markov chain Monte-Carlo (MCMC) sampling method, \emph{annealed} Langevin dynamics adds decreasing levels of noise perturbations in the MCMC sampling. Noise perturbation significantly improves sampling efficiency and quality of EBMs, which is the underlying reason of the success of diffusion models.

\textbf{Noise-conditioned score networks.}
A equivalent approaches developed by \citet{song2019generative} simultaneously with diffusion models is to fit the score function of a series of noise-perturbed data distribution $\Ncal\rbr{\xb_i;\xb, \sigma_i^2 \Ib}, i=\{1,2,\dots, K\}$ with a noise schedule $\sigma_1> \sigma_2>\dots>\sigma_K$. 
The resulting models, named the noise-conditioned score networks (NCSN) $f_\theta\rbr{\xb_i;\sigma_i}$, take the noise level into the inputs and are learned by denoising score matching~\cite{vincent2011connection}
\begin{align}
    % &\EE_{\xbtil\sim \ptil}\sbr{\nbr{f_\theta\rbr{\xb_i;i} - \nabla_{\xbtil}\log \ptil_\sigma\rbr{\xbtil}}^2} =\label{eq:noisy_score_matching}\\
    &\EE_{\xb\sim p,\xb_i\sim\Ncal\rbr{\xb,\sigma_i^2 \Ib}}\sbr{\|f_\theta\rbr{\xb_i;\sigma_i} -\nabla_{\xb_i}\log q(\xb_i|\xb) \|^2}\label{eq:dsm_loss}
\end{align}
% \begin{equation}
%     \ab_i = \ab_{i-1} + \frac{\eta}{2} \nabla_{\ab_{i-1}}\log \pi(\ab_{i-1}|\sbb) + \sqrt{\eta}\zb_i,\,i=1,2,\dots K \label{eq:langevin}
% \end{equation}
Then in the sampling stage, \citet{song2019generative} uses the Langevin dynamics $\xb_{i+1} = \xb_{i} + \eta \nabla_{\xb_{i}}\log q(\xb_{i} \mid \xb) + \sqrt{2 \eta}\zb_i$ to sample from energy function. \citet{song2019generative} additionally replace the original score function $\nabla_{\xb_{i}}\log q(\xb_{i} \mid \xb)$ in the Langevin dynamics with the learned noisy score function $f_\theta(\xbtil;\sigma_i)$:
\begin{equation}
    \xb_{i+1} \leftarrow \xb_i+\eta f_\theta(\xbtil;\sigma_i)+\sqrt{2 \eta} \mathbf{z}_i, \quad i=0,\cdots, K\label{eq:annealed_langevin}
\end{equation}
named as annealed Langevin dynamics. The scheduled noise perturbation design significantly improved the image generation performance to match the state-of-the-art (SOTA) at that time~\cite{song2019generative}, which is further refined by DDPM.

We can see that the annealed Langevin dynamics~\eqref{eq:annealed_langevin} resembles the DDPM sampling~\eqref{eq:annealed_langevin_2} with different scale factors, and the denoising score matching loss~\eqref{eq:dsm_loss} is equivalent to \eqref{eq:ddpm_loss}. Therefore, DDPM can be interpreted as EBMs with multi-level noise perturbations. A more thorough discussion on their equivalency can also be found in~\citet{ho2020denoising,song2021scorebased}.

\section{Derivations}

\subsection{Derivations of \Cref{prop:diff_ebm}}
\label{sec:prop_1_apdx}
We repeat \Cref{prop:diff_ebm} here,
\diffebm*
% \begin{proposition}[Diffusion models as noise-perturbed EBMs]
%     The score network $s_\theta(\ab_t;\sbb, t)$ in \eqref{eq:ddpm_loss} matches noise-perturbed score functions, $\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$,
%     % $$s_\theta(\ab_t;\sbb, t)\text{ matches }\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb),$$
%     where state $\sbb$ is added to inputs of score network $s_\theta(\ab_t;\sbb, t)$ to handle conditional distributions, $p_0(\cdot)$ in \eqref{eq:ddpm_loss} refers to $\pi_{\rm target}(\cdot|\sbb)$ in our problems.\label{prop:ebm}
% \end{proposition}
% \vspace{-6mm}
\begin{proof}
    This can be shown by checking the gradient estimator of $\nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb_t)$, \ie,
    \begin{align}
    \nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb_t) % \label{eq:prop1_derivation}
            = & \frac{\nabla_{\ab_t}\tilde\pi_t\rbr{\ab_t|\sbb}}{\tilde\pi_t\rbr{\ab_t|\sbb}}           =  \frac{\nabla_{\ab_t}\int q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)d\ab_0}{\tilde\pi_t\rbr{\ab_t|\sbb}}\notag\\
            = & \int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\underbrace{\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}}_{p_{0|t}(\ab_0|\ab_t, \sbb)}d\ab_0 \nonumber
    \end{align}
    % where the last equality comes from Gaussian perturbation~\eqref{eq:gaussian_perturb}
    % $$
    % \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0) = -\frac{\ab_t - \sqrt{\bar\alpha_t}\ab_0}{{1-\bar\alpha_t}} = -\frac{\epsb}{\sqrt{1-\bar\alpha_t}}
    % $$
    % where $\epsb\sim\Ncal\rbr{0,\Ib}$.
    We match the noise-perturbed score function via a neural network $s_\theta(\ab_t;\sbb, t)$ via optimizing the expectation of square error over $\ab_t\sim \tilde\pi_t(\cdot|\sbb)$,
    
    {\small
    \begin{align}
       &\EE_{\ab_t\sim\tilde\pi_t}\sbr{\nbr{s_\theta(\ab_t;\sbb, t) - \nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)}^2}\notag \\
       =&\EE_{\ab_t\sim\tilde\pi_t}\sbr{\nbr{s_\theta(\ab_t;\sbb, t) - \int\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}d\ab_0}^2}\notag\\ 
       = & \EE_{\ab_t\sim\tilde\pi_t}\sbr{\nbr{s_\theta(\ab_t;\sbb, t)}^2} - 2\EE_{\ab_t\sim\tilde\pi_t}\sbr{\inner{s_\theta(\ab_t;\sbb, t)}{\int\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}d\ab_0}} + \texttt{constant}\\
       = & \EE_{\ab_t\sim\tilde\pi_t}\sbr{\nbr{s_\theta(\ab_t;\sbb, t)}^2} - 2\iint{\inner{s_\theta(\ab_t;\sbb, t)}{\tilde\pi_t(\ab_t|\sbb)\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}}}d\ab_0 d\ab_t + \texttt{constant}\\
        = & \EE_{\ab_t\sim\tilde\pi_t}\sbr{\nbr{s_\theta(\ab_t;\sbb, t)}^2} - 2~\EE_{\ab_0\sim\pi_{\rm target}, \ab_t\sim q_{t|0}}\sbr{\inner{s_\theta(\ab_t;\sbb, t)}{\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}} + \texttt{constant}\\
       =&\underset{\substack{\ab_0\sim\pi_{\rm target}\\\ab_t\sim q_{t|0}}}{\EE}\sbr{\nbr{s_\theta(\ab_t;\sbb, t) -\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}^2} + \texttt{constant}\label{eq:tweedie_loss_apdx}
    \end{align}
    }
    where the \texttt{constant} denotes constants that are irrelevant with $\theta$. Therefore, minimizing the difference between $s_\theta(\ab_t;\sbb, t)$ and $\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$ also implies minimizing the difference between $s_\theta(\ab_t;\sbb, t)$ and $\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)$.
    % The last equation holds since $\tilde\pi_t$
    This concludes the proof of Proposition~\ref{prop:diff_ebm}.
\end{proof}
\subsection{Derivations of \Cref{thm.rssm}}
\label{sec:appendix_derivation}
% Given a loss function to fit $\epsb_\theta(\ab_t;\sbb, t)\approx -\sqrt{1-\bar\alpha_t}\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$ with a properly defined weight function $g$,
% \begin{align}
%     \Lcal^g(\theta;\sbb,t)  \coloneqq \int g(\ab_t;\sbb)\nbr{\epsb_\theta\rbr{\ab_t,t} + \sqrt{1 - \bar\alpha_t} \nabla_{\ab_t} \log \tilde\pi_t\rbr{\ab_t|\sbb}}^2d\ab_t\label{eq:reweighted_score_matching_apdx}
% \end{align}
% This provides a loss function to learn the score function, which still requires sampling from the target distribution and thus intractable in the online RL setting.

\Cref{thm.rssm} shows that we can match the score network $s_\theta(\ab_t;\sbb, t)$ with noise-perturbed policy score function $\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)$ without sampling from $\pi_{\rm target}$ like denoising score matching~\eqref{eq:ddpm_loss}. Let us restate \Cref{thm.rssm} and provide the proof below.

\rssm*
% \begin{theorem}[Reverse sampling score matching (RSSM, \Cref{thm.rssm} in the main text).]
%     Define $\tilde p_t(\cdot|\sbb)$ as a sampling distribution whose support contains the support of $\tilde\pi_t(\cdot|\sbb)$ given $\sbb$.
%     Construct the weight function $g$ as 
%     \begin{equation}
%         g(\ab_t;\sbb) = Z(\ab_t;\sbb)\tilde p_t(\cdot|\sbb)\label{eq:reweighting_g_apdx}
%     \end{equation}
%     where $Z(\ab_t;\sbb) = \int\exp\rbr{Q(\sbb,\ab_0)/\lambda}q_{t|0}(\ab_t| \ab_0)d\ab_0$. Then the loss function in \eqref{eq:reweighted_score_matching_apdx} is equivalent to
%     \begin{equation}
%         \underset{\substack{\ab_t\sim \tilde p_t \\ \epsb\sim\Ncal(0, \Ib)}}{\EE}\sbr{\exp \rbr{Q\rbr{\sbb, \tilde\ab_t}/\lambda}\nbr{\epsb_\theta\rbr{\ab_t, \sbb, t} - \epsb}^2}\label{eq:thm1_reweight_loss}
%     \end{equation}
%     where 
%     \begin{equation}
%         \tilde\ab_0 = \frac{1}{\sqrt{\bar\alpha_t}}\ab_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb \label{eq:a0_sample_thm_apdx}
%     \end{equation}
% \end{theorem}

\textbf{Proof.} 
The proof sketch consists of two major steps, reformulating the noise-perturbed score function and applying the reverse sampling trick.
    \\\textbf{Reformatting the noise-perturbed score function.} First, we slightly reformat derivations of the noise-perturbed score function in \Cref{prop:diff_ebm} starting from \eqref{eq:prop1_derivation},
    \begin{equation}
        \begin{aligned}
            & \nabla_{\ab_t}\log\tilde\pi_t(\ab_t|\sbb)\\
            = &\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)\frac{q_{t|0}(\ab_t|\ab_0)\pi_{\rm target}(\ab_0|\sbb)}{\tilde\pi_t\rbr{\ab_t|\sbb}}d\ab_0\\
            = & \frac{\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0) q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0}{Z_t\rbr{\ab_t;\sbb}}\\
        \end{aligned}\label{eq:noise_perturbed_score_apdx}
    \end{equation}
where the second equality holds since we multiply both the denominator and numerator with normalization constant $\int \exp(Q(\sbb,\ab_0)/\lambda)d\ab_0$ with:
\begin{align}
    Z_t(\ab_t; \sbb) & := \tilde{\pi}_t(\ab_t | \sbb) \int \exp(Q(\sbb,\ab_0)/\lambda)d\ab_0 \\
    &= \int\frac{\exp(Q(\sbb, \ab_0)/\lambda)}{\int\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0}q_{t|0}(\ab_t|\ab_0)d\ab_0\int\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0\\
        &= \int\exp(Q(\sbb, \ab_0)/\lambda)q_{t|0}(\ab_t|\ab_0)d\ab_0
    % & = \left(\int q_{t | 0}(\ab_t | \ab_0) \pi_{\rm target} (\ab_0 | \sbb) d \ab_0 \right) \left( \int \exp(Q(\sbb,\ab_0)/\lambda)d\ab_0 \right) \\
    % & = 
\end{align}

 We use the score network $s_\theta$ to learn \eqref{eq:noise_perturbed_score_apdx} via minimizing the square error, resulting the following loss term given $\ab_t$,
% Since $$p_0(\xb_0)\propto \exp\rbr{-\frac{E(\xb_0)}{\lambda}},p_t(\xb_t) = \int q_{t|0}(\xb_t|\xb_0)p_0(\xb_0)d\ab_0
% $$, we have
\begin{align}
    &\nbr{s_\theta(\ab_t,\sbb, t) -\nabla_{\ab_t}\log \tilde\pi_t(\ab_t|\sbb)}^2\notag\\ % d\ab_t
    % & \Lcal^g(\theta;\sbb,t)\notag\\  \int g(\ab_t;\sbb)
    =&\nbr{s_\theta(\ab_t,\sbb, t) -\frac{\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0) q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0}{Z_t\rbr{\ab_t;\sbb}}}^2\notag\\ % d\ab_t
    = &~
    \frac{1}{Z(\ab_t;\sbb)}{\nbr{s_\theta(\ab_t,\sbb, t)}^2 \underbrace{\rbr{\int \exp\rbr{Q(\sbb,\ab_0)/\lambda}\,q_{t|0}(\ab_t|\ab_0)\,d\ab_0}}_{Z(\ab_t;\sbb)} }\notag \\
    % = &\nbr{s_\theta(\ab_t,\sbb, t)}^2\\
     & -
    \frac{2}{Z(\ab_t;\sbb)}{\inner{s_\theta(\ab_t,\sbb, t)}{\int \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0) q_{t|0}(\ab_t|\ab_0)\exp\rbr{Q(\sbb,\ab_0)/\lambda} d\ab_0}} + \texttt{constant}\label{eq:zx_apdx}\\
    = &
    \int\frac{1}{Z(\ab_t;\sbb)}q_{t|0}(\ab_t|\ab_0)\rbr{{\exp\rbr{Q(\sbb,\ab_0)/\lambda}}\rbr{\nbr{s_\theta(\ab_t,\sbb, t)}^2 - 2\inner{s_\theta(\ab_t,\sbb, t)}{\nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)} } }d\ab_0 + \texttt{constant} \notag \\
    = & \int\frac{1}{Z(\ab_t;\sbb)}q_{t|0}(\ab_t|\ab_0)\rbr{{\exp\rbr{Q(\sbb,\ab_0)/\lambda}}\nbr{s_\theta(\ab_t,\sbb, t) - \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}^2  } d\ab_0 + \texttt{constant} \label{eq:apdx_last_line_derive}
\end{align}
where we abbreviate the constants that are irrelevant to $\theta$ in the calculation.

\textbf{Weight function $g$.} For more rigorous derivations, we denote a weight function $g:\Acal\times\Scal\to \RR^+$ constructed by 
\begin{equation}
    g(\ab_t;\sbb) = Z(\ab_t;\sbb)\ptil(\ab_t|\sbb) \label{eq:def_g_apdx}
\end{equation}
As $Z(\ab_t;\sbb)$ is $\tilde\pi_t$ scaled by normalization constant $\int\exp\rbr{Q(\sbb,\ab_0)}d\ab_0$ and the support of $\ptil(\cdot|\sbb)$ contains $\tilde\pi_t(\cdot|\sbb)$, we know that $g(\ab_t;\sbb)$ is strictly positive on the support of $\tilde\pi(\cdot|\sbb)$. Therefore, we can optimize the squared error over $\ab_t$ via the 
\begin{align}
    \Lcal^g(\theta;\sbb,t)  :=& \int g(\ab_t;\sbb)\nbr{s_\theta\rbr{\ab_t,t} - \nabla_{\ab_t} \log \tilde\pi_t\rbr{\ab_t|\sbb}}^2d\ab_t\notag\\
    =& \iint\frac{g(\ab_t;\sbb) }{Z(\ab_t;\sbb)}q_{t|0}(\ab_t|\ab_0)\rbr{{\exp\rbr{Q(\sbb,\ab_0)/\lambda}}\nbr{s_\theta(\ab_t,\sbb, t) - \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}^2  } d\ab_0d\ab_t + \texttt{constant}\notag\\
     =& \iint\ptil(\ab_t|\sbb)q_{t|0}(\ab_t|\ab_0)\rbr{{\exp\rbr{Q(\sbb,\ab_0)/\lambda}}\nbr{s_\theta(\ab_t,\sbb, t) - \nabla_{\ab_t}\log q_{t|0}(\ab_t|\ab_0)}^2  } d\ab_0d\ab_t + \texttt{constant} \label{eq:before_reverse_sample_apdx}
\end{align}
where the equalities come from the results in \eqref{eq:apdx_last_line_derive} and definition of $g$ in \eqref{eq:def_g_apdx}.

\textbf{Reverse sampling trick.} Then we replace $q_{t|0}$ with a reverse sampling distribution $\qtil_{0|t}$ that satisfies
    \begin{equation}
        \begin{aligned}
        &\qtil_{0|t}(\ab_0\mid \ab_t) =\Ncal\rbr{\ab_0;\frac{1}{\sqrt{\bar\alpha_t}}\ab_t, \frac{1 - \bar\alpha_t}{\bar\alpha_t}\Ib}
        \propto q_{t|0}(\ab_t\mid \ab_0) = \Ncal\rbr{\ab_t;\sqrt{\bar\alpha_t}\ab_0, \rbr{1 - \bar\alpha_t}\Ib},  \end{aligned}\label{eq:reverse_gaussian_apdx}
    \end{equation}
    and thus
    \begin{equation}
        \nabla_{\ab_t}\log q_{t|0}(\ab_t\mid \ab_0) = \nabla_{\ab_t}\log \qtil_{0|t}(\ab_0\mid \ab_t) = - \frac{\ab_t -\sqrt{\bar\alpha_t}\ab_0}{1 -\bar\alpha_t}\label{eq:reverse_sampling_score_apdx}
    \end{equation}

Then we continue via replacing $q$ with $\qtil$ in \eqref{eq:apdx_last_line_derive}, resulting in \eqref{eq:apdx_last_line_derive} equals 
\begin{align}
    % & \iint\frac{g(\ab_t;\sbb)}{Z(\ab_t;\sbb)}\qtil_{0|t}(\tilde\ab_0\mid \ab_t)\rbr{{\exp(Q(\sbb,\tilde\ab_0)/\lambda)}\nbr{s_\theta(\ab_t,\sbb, t) -  \nabla_{\xb_t}\log \qtil_{0|t}(\tilde\ab_0\mid \ab_t)}^2  } d\tilde\ab_0d\ab_t\notag \texttt{constant} \\
    \Lcal^g(\theta;\sbb,t) = & \iint \ptil_t(\ab_t|\sbb)\qtil_{0|t}(\tilde\ab_0\mid \ab_t)\rbr{{\exp\rbr{Q(\sbb,\ab_0)/\lambda}}\nbr{s_\theta(\ab_t,\sbb, t) -  \nabla_{\xb_t}\log \qtil_{0|t}(\tilde\ab_0\mid \ab_t)}^2  } d\tilde\ab_0d\ab_t+\texttt{constant}\notag \\
    = &~\EE_{\ab_t\sim \ptil_t, \tilde\ab_0\sim \qtil_{0|t}}\sbr{{\exp(Q(\sbb,\tilde\ab_0)/\lambda)}\nbr{s_\theta(\ab_t,\sbb, t) -  \nabla_{\xb_t}\log \qtil_{0|t}(\tilde\ab_0\mid \ab_t)}^2  } +\texttt{constant}\label{eq:appendix_derivation_second_last}
\end{align}

% From \eqref{eq:a0_sample_thm_apdx} and \eqref{eq:reverse_sampling_score_apdx} we have 
% % \begin{equation}
% %     \tilde{\xb}_0 = \frac{1}{\sqrt{\bar\alpha_t}}\xb_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb,\ \epsb\sim\Ncal(0, \Ib) \label{eq:appendix_reverse_sample}
% % \end{equation}
% % Note that both plus and minus signs in the \eqref{eq:appendix_reverse_sample} hold since the Gaussian is symmetric. Then 
% $$
% \nabla_{\xb_t}\log \qtil_{0|t}(\tilde\ab_0\mid \ab_t) = - \frac{\ab_t -\sqrt{\bar\alpha_t}\ab_0}{1 -\bar\alpha_t} = - \frac{\epsb}{\sqrt{1 - \bar\alpha_t}}
% $$
% Substitute in \eqref{eq:appendix_derivation_second_last}, we have,
% $$
% \Lcal^g(\theta;\sbb,t) = \EE_{\ab_t\sim \ptil_t, \tilde\ab_0\sim \qtil_{0|t}}\sbr{{\exp(Q(\sbb,\tilde\ab_0)/\lambda)}\nbr{\epsb_\theta(\ab_t,\sbb, t) - \epsb}^2  }  + C_2
% $$
which concludes the proof of \Cref{thm.rssm}. \hfill$\square$

% \section{Additional Discussions on Sampling Distribution}
% \label{sec}
%  We emphasize that sampled $\tilde\xb_0$ is irrelevant with and different from $\xb_0$. Consider the following example of the sampling distribution $\ptil_t$,
 
%  \begin{example}[Choice of the sampling distribution.]\label{example:sample_dist}
% Assume we set $\ptil_t := p_t$ then 
% $$
% \begin{aligned}
%     \tilde\xb_0 & = \frac{1}{\sqrt{\bar\alpha_t}}\xb_t - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb_1 \\
%     & =  \frac{1}{\sqrt{\bar\alpha_t}}\rbr{\sqrt{\bar\alpha_t}\xb_0 + \sqrt{1 -\bar\alpha_t}\epsb_2} - \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}\epsb_1\\
%     &=  \xb_0 + \sqrt{\frac{1 - \bar\alpha_t}{\bar\alpha_t}}(\epsb_1 - \epsb_2)\sim \Ncal \rbr{\xb_0, 2 \frac{1-\bar\alpha_t}{\bar\alpha_t}\Ib}
% \end{aligned}
% $$
% where $\epsb_1\sim\epsb_2\sim\Ncal(0,\Ib)$. Note that $\epsb_1 - \epsb_2\sim\Ncal(0, 2\Ib)$. We can see that the sampled $\tilde\xb_0$ does not follow the data distribution but follows a Gaussian-perturbed data distribution in this case.     
%  \end{example}

\section{Additional Experimental Setup}


\subsection{Training Setups for the Toy Example}
\label{sec:apdx_toy_example}
Consider Gaussian mixture model with density function
\begin{equation}
    p_0(\xb) = 0.8 * \frac{1}{2\pi}\exp\rbr{-\frac{\nbr{\xb-[3;3]}^2}{2}} + 0.2 * \frac{1}{2\pi}\exp\rbr{-\frac{\nbr{\xb+[3;3]}^2}{2}}
\end{equation}
where the RSSM optimizes
\begin{equation}
        \underset{\substack{\xb_t\sim \ptil \\\xb_0\sim\qtil_{0|t}}}{\EE}\sbr{p_0(\tilde\xb_0)\nbr{s_\theta\rbr{\xb_t;t} - \nabla_{\xb_t}\log \qtil_{0|t}(\tilde\xb_0|\xb_t)}^2} % \label{eq:thm1_reweight_loss}
\end{equation}
for the Gaussian sampling, $\ptil(\xb_t)=\Ncal(0, 4\Ib)$ for all $t$ and for uniform sampling, $\ptil_t$ is a uniform distribution from $[-6, 6]$ on both dimensions.
The score network is trained via the hyperparameters listed in \Cref{tab:hyperpara_toy}. The Langevin dynamics has direct access to the true score function $\nabla_\xb\log p_0(\xb)$.
\begin{table}[h]
    \centering
    \caption{Hyperparameters for the toy example.}
    \vspace{10pt}
    \begin{tabular}{l|c|l|c}
    \toprule
      \textbf{Name}   & \textbf{Value}&\textbf{Name}   & \textbf{Value} \\
      \midrule
      Learning rate & 3e-4 & Diffusion noise schedule & linear\\
      Diffusion steps & 20&Diffusion noise schedule start & 0.001\\
      Hidden layers & 2&Diffusion noise schedule end & 0.999\\
      Hidden layer neurons & 128&Training batch size & 1024\\
      Activation Function & LeakyReLU & Training epoches & 300 \\
         \bottomrule
    \end{tabular}
    \label{tab:hyperpara_toy}
\end{table}

\subsection{Baselines}
\label{sec:apdx_baselines}
We include two families of methods as our baselines. For the first family of methods, we select 5 online diffusion-policy RL algorithms:  QSM~\cite{psenka2023learning}, QVPO~\cite{ding2024diffusion}, DACER~\cite{wang2024diffusion}, DIPO~\cite{yang2023policy} and DPPO~\cite{ren2024diffusion}. We include both off-policy (QSM, QVPO, DACER, DIPO) and on-policy (DPPO) diffusion RL methods among this group of algorithms. QSM follows a similar idea with \Cref{remark:langevin} to use the Langevin dynamic with derivatives of learned $Q$-function as the score function. QVPO derives a Q-weighted variational objective for diffusion policy training, yet this objective cannot handle negative rewards properly. DACER directly backward the gradient through the reverse diffusion process and proposes a GMM entropy regulator to balance exploration and exploitation. DIPO utilizes a two-stage strategy, which maintains a large number of state-action particles updated by the gradient of the Q-function, and then fit the particles with a diffusion model.  DPPO constructs a two-layer MDP with diffusion steps and environment steps, respectively, and then performs Proximal Policy Optimization on the overall MDP. In our experiments, we use the training-from-scratch setting of DPPO to ensure consistency with other methods. 

The second family of baselines includes 3 classic model-free RL methods: PPO~\cite{schulman2017proximal}, TD3~\cite{fujimoto2018addressing} and SAC~\cite{haarnoja2018soft}. For PPO, we set the replay buffer size as 4096 and use every collected sample 10 times for gradient update. Across all baselines, we collect samples from 5 parallel environments in a total of 1 million environment interactions and 200k epoches/iterations. The results are evaluated with the average return of 20 episodes across 5 random seeds.
\subsection{Hyperparameters}
\label{sec:params}
\begin{table}[h]
    \centering
    \caption{Hyperparameters}
    \begin{tabular}{l|c}
    \toprule
      \textbf{Name}   & \textbf{Value} \\
      \midrule
      Critic learning rate & 3e-4 \\
      Policy learning rate & 3e-4, linear annealing to 3e-5\\
      Diffusion steps & 20\\
      Diffusion noise schedules & Cosine \\
      % Number of particles (Hopper) & 8\\
      % Number of particles (all other environments)& 32\\
      Policy network hidden layers & 3\\
      Policy network hidden neurons & 256\\
      Policy network activation & Mish\\
      Value network hidden layers & 3\\
      Value network hidden neurons & 256\\
      Value network activation & Mish\\
      replay buffer size (off-policy onoy) & 1 million\\
         \bottomrule
    \end{tabular}
    \label{tab:main_hyper}
\end{table}

where the Cosine noise schedule means 
$\beta_t = 1 - \frac{\bar\alpha_t}{\alpha_{t-1}}$ with $\bar{\alpha}_t=\frac{f(t)}{f(0)}$, $f(t)=\cos \left(\frac{t / T+s}{1+s} * \frac{\pi}{2}\right)^2$.



