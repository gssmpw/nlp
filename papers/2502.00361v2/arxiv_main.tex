\documentclass[11pt]{article}
\usepackage[letterpaper, margin=2cm]{geometry}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{parskip}
\setlength{\abovedisplayskip}{10pt}  % Space before equations
\setlength{\belowdisplayskip}{10pt}  % Space after equations
\setlength{\parindent}{0pt}
\setlength{\parskip}{10pt} % Adjust the space as needed
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[affil-it]{authblk}
\usepackage[textsize=tiny]{todonotes}



\usepackage{amsthm}
% \usepackage{generic}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{algorithmic}
\usepackage{natbib}
% if you use cleveref..
\usepackage{textcomp}
\usepackage{color}
\usepackage{xurl}
\usepackage{bm}
% \usepackage{algorithm2e}
\usepackage[normalem]{ulem}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{wrapfig}
\usepackage{booktabs, multirow}
\let\labelindent\relax
\usepackage{enumitem}
\usepackage{thmtools} 
\usepackage{thm-restate}
% \newcommand{\citep}{\cite}
% \newcommand{\citet}{\cite}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{Definitions}

% \usepackage{xr-hyper}
\usepackage[colorlinks,citecolor=blue]{hyperref}
% \hypersetup{hidelinks=true}
\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\algname}{{Soft Diffusion Actor-Critic}\xspace}
\newcommand{\algabb}{\texttt{SDAC}\xspace}
\newcommand{\algcommentline}[1]{\texttt{\textit{\textcolor{gray}{\# #1}}}}

\allowdisplaybreaks


\title{Soft Diffusion Actor-Critic: \\
Efficient Online Reinforcement Learning for Diffusion Policy}

% \author{Haitong Ma\textsuperscript{1}, Tianyi Chen\textsuperscript{2}, Kai Wang\textsuperscript{2}, Na Li\textsuperscript{1,*}, Bo Dai\textsuperscript{2,*}}



\begin{document}

\author[1]{Haitong Ma} 
  \author[2]{Tianyi Chen} 
  \author[2]{Kai Wang}
    \author[1]{Na Li$^*$}
  \author[2]{Bo Dai\thanks{Equal supervision.}}
  \affil[1]{School of Engineering and Applied Sciences, Harvard University}
  \affil[2]{School of Computational Science and Engineering, Georgia Institute of Technology\thanks{Emails: Haitong (haitongma@g.harvard.edu), Tianyi Chen (tchen667@gatech.edu), Kai Wang (kwang692@gatech.edu), Na Li (nali@seas.harvard.edu), Bo Dai (bodai@cc.gatech.edu)}}
    \date{}

\maketitle
\begin{abstract}
Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the vanilla diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy, making training diffusion policies highly non-trivial in online RL. 
Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and impractical. 
To enable efficient diffusion policy training for online RL, we propose {\algname~(\algabb)}, exploiting the viewpoint of diffusion models as noise-perturbed energy-based models.
The proposed \algabb relies solely on the state-action value function as the energy functions to train diffusion policies, bypassing sampling from the optimal policy while maintaining lightweight computations. 
% Specifically, we first propose \emph{reverse sampling score matching} that can train diffusion models with only access to energy functions via \emph{reverse sampling trick}. 
% We apply the technique toz online maximum entropy RL to propose the SDAC algorithm, which learns a diffusion policy that only requires the soft $Q$-function as the energy function to significantly reduce the computation cost and achieve state-of-the-art performance.
% We then apply it to online maximum entropy RL, named soft diffusion actor-critic (SDEC), to learn a diffusion policy that only requires the soft $Q$-function as the energy function, which significantly reduces the computation cost and achieves state-of-the-art performance.
% Unlike existing methods that approximated the data distribution or backpropagated the policy gradient through the whole diffusion process, we achieved state-of-the-art performance with lightweight computation.
We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that \algabb outperforms all recent diffusion-policy online RLs on most tasks, and improves more than 120\% over soft actor-critic on complex locomotion tasks such as Humanoid and Ant.

\end{abstract}


\setlength{\abovedisplayskip}{2pt}
\setlength{\abovedisplayshortskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\setlength{\belowdisplayshortskip}{1pt}
\setlength{\jot}{1pt}
%
\setlength{\floatsep}{1ex}
\setlength{\textfloatsep}{1ex}



\input{content/1_intro_v2}
\input{content/2_preliminaries_v4}
\input{content/3_methods_v4}
\input{content/4_algorithm.tex}
\input{content/5_experiments.tex}
\input{content/6_conclusion}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{content/8_appendix.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
\end{document}