\section{Results on units of measure redefinition}
\label{app:units_redefinition}

% \begin{figure*}[h!]
%     \centering
%     \subfloat[Results of Mistral models on FF format.]{ % Subfigure 1
%         \includegraphics[width=0.47\linewidth]{images/mistral_large_units.png}
%         \label{fig:mistral}
%     }     
%     \subfloat[Results of Llama models on FF format.]{ % Subfigure 2
%         \includegraphics[width=0.47\linewidth]{images/llama_units.png}
%         \label{fig:llama}
%     } \\
%     \subfloat[Results of Mistral models on MC format.]{ % Subfigure 2
%         \includegraphics[width=0.47\linewidth]{images/mistral_large_untis_MC.png}
%         \label{fig:mistral_mc}
%     } 
%     \subfloat[Results of Llama models on MC format.]{ % Subfigure 2
%         \includegraphics[width=0.47\linewidth]{images/llama_units_MC.png}
%         \label{fig:llama_mc}
%     }
%     \caption{The results of the different Mistral and Llama models on $Q_3$ questions using ZS prompting for the redefinition task of units of measurement.}
%     \label{fig:size_comparison_units}
% \end{figure*}



An overview of response accuracy is presented in Table \ref{tab:anchored_table_units}, where we consider the hardest redefinitions corresponding to $R_{a}3$ and $R_{s}2$ for \textit{assignment} and \textit{swapping} respectively, as well as all three question levels, together with FF and MC response formats regarding units of measurement redefinitions. Additionally, Table \ref{tab:anchored_table_NR_units} presents the number of correct responses in the NR case alongside the anchoring rate for FF responses regarding units redefinitions. Anchoring is less prominent for some LLMs in comparison to their anchored responses in the constants redefinition task; for instance, Command r+, light text, text achieve even 0\% anchoring in some cases, even in $Q_3$ questions over the hardest $R_a3$ unit redefinitions. Nevertheless, anchoring still persists in many instances, with large rates concerning models in the Mistral family for the hardest question and redefinition levels. Moreover, Titan models present high anchoring even for $R_a2$ unit redefinitions, even in the easier $Q_1$ level. Surprisingly, anchoring for Titan models reduces as questions and redefinitions become harder, but this does not indicate an improvement in producing correct responses and therefore an advancement in reasoning capability; instead, the anchoring reduction is attributed to the generation of more completely wrong responses, indicating those models' inability of solving the unit of measure redefinition task appropriately.

\input{tables/res_ff_mc}


Figure \ref{fig:size_comparison_units} shows the results of the different Mistral and Llama models for the $Q_3$ question level in the ZS prompting setup for units redefinitions. The conclusions are similar to those for the redefinition of constants, where the number of anchored responses is significantly higher in the MC setup compared to the FF setup --a rather expected pattern, since LLMs are exposed to the default response. Additionally, once again, it is observable that the larger models are more prone to providing anchored responses compared to the smaller ones, regardless the response format and the model family.

Furthermore, Figures \ref{fig:mistral_all-units} and \ref{fig:llama_all-units} illustrate the results of Mistral7B and Mistral Large, as well as Llama8B and Llama 405B, respectively, regarding units of measure redefinitions. Once again, Mistral7B tends to provide \textit{fewer anchored responses} compared to its larger counterpart. The same trend holds for Llama, although for $Q_1$ and $Q_2$ responses, the increase in anchoring is less pronounced for the larger model. 

Additionally, the performance of the larger LLama405B model in the NR case is excellent in the $Q_1$  question level, achieving a response accuracy close to 100\% in most cases, denoting that this model is adequately knowledgeable regarding the default meanings of units of measure. For $Q_2$ questions in Llama405B, an interesting pattern emerges. The number of correct responses in the NR task remains close to 100\%, indicating that the model is also an excellent reasoner in this difficulty level. However, when units are redefined, the model's accuracy \textit{declines considerably}, coinciding with a noticeable increase in the number of anchored responses. Therefore, Llama405B exploits memorized patterns to be able to handle unit redefinitions, even though memorization almost useless in the $Q_1$ level. The anchoring rate further increases in the $Q_3$ level, even though the NR correct response rate (and therefore the model's reasoning ability in the default setting) is decreased in comparison to the easier question levels.

Lastly, Tables \ref{tab:correlation_zs-units}, \ref{tab:correlation_fs-units}, and \ref{tab:correlation_cot-units} present the correlation between average model performance for all LLMs in the NR case and the number of anchored responses for unit of measure redefinitions using ZS, FS, and CoT prompting, respectively. These correlations mostly exhibit a similar pattern to those observed in the constant redefinition task. However, unlike constants, where high positive correlations were found due to \textit{swapping}, unit of measure redefinitions are implemented using  \textit{assignment} exclusively. 

Nevertheless, in both ZS and FS prompting, we observe a high positive correlation for $Q_3$-level questions, similar to the constant redefinition case, denoting increased anchoring for more potent reasoners. This trend holds for the MC response format, but not for the FF format (where correlations are weak), contradicting constants redefinition findings. Apparently, anchoring becomes less prominent with respect to reasoning capability when the LLMs have to generate responses over redefined units of measure. 


On the other hand, CoT is evidently capable of reducing anchoring of more potent reasoners, leading to weak or negative correlations in all cases, regardless the redefinition or question difficulty. This is  a contradictory fact in comparison to constants redefinitions, revealing that CoT can assist LLMs in reasoning more properly and thus anchor less to their prior knowledge, aligning to basic CoT claims \cite{step-by-step}.

\begin{figure}[h!]
    \centering
    \subfloat[Response breakdown for Mistral models.]{ % Subfigure 1
        \includegraphics[width=0.8\linewidth]{images/mistral-anchored-units.png}
        \label{fig:mistral}
    }   \\
    \subfloat[Response breakdown for Llama models.]{ % Subfigure 2
        \includegraphics[width=0.8\linewidth]{images/llama-anchored-units.png}
        \label{fig:llama}
    } \\
        \vskip -0.01in
    \caption{Results for the different Mistral and Llama models on $Q_3$ questions using ZS prompting for the
redefinition task of units of measure redefinitions. The order of the bars per redefinition type/level corresponds to increasing model size.}
    \label{fig:size_comparison_units}
\end{figure}



\begin{table}[h!]
\small
    \centering
    \begin{tabular}{l|ccc}
        \hline
        Level & $R_{a1}$ & $R_{a2}$ & $R_{a3}$ \\ \hline
        & \multicolumn{3}{c}{Free-Form (FF)} \\ 
        \hline
        $Q_1$ & -0.295 & \cellcolor{lightdustygreen} -0.403   & \cellcolor{lightdustygreen} -0.33 \\ 
        $Q_2$ & \cellcolor{lightdustygreen} -0.361 & -0.247 & \cellcolor{lightdustygreen} -0.479 \\ 
        $Q_3$ & -0.063 & 0.19 & 0.14 \\
        
        \hline
        & \multicolumn{3}{c}{Multiple Choice (MC)} \\ \hline
        $Q_1$ & \cellcolor{lightdustygreen} -0.49 & -0.149  & \cellcolor{lightdustygreen} -0.542 \\ 
        $Q_2$ & -0.159 & -0.023 & 0.08 \\ 
        $Q_3$ & 0.248 & \cellcolor{lightdustypink} 0.338 & -0.127 \\

        \hline
    \end{tabular}
    \caption{Correlation between model performance before redefinition with the percentage of anchored answers for each type of unit of measure redefinition and question level in ZS setup. 
    Cells highlighted in \textcolor{lightdustypink}{pink} indicate a \textbf{high positive correlation} ($>0.3$), while cells in \textcolor{lightdustygreen}{green} indicate a \textbf{high negative correlation} ($<-0.3$).}
    \label{tab:correlation_zs-units}
\end{table}



\begin{table}[h!]
\small
    \centering
    \begin{tabular}{l|ccc}
        \hline
        Level & $R_{a1}$ & $R_{a2}$ & $R_{a3}$ \\ \hline
        & \multicolumn{3}{c}{Free-Form (FF)} \\ 
        \hline
        $Q_1$ & \cellcolor{lightdustygreen} -0.32 & \cellcolor{lightdustygreen} -0.442 & -0.161 \\
        $Q_2$ & \cellcolor{lightdustygreen} -0.404 & -0.231 & 0.039 \\
        $Q_3$ & 0.128 & -0.042 & 0.279 \\
        
        \hline
                & \multicolumn{3}{c}{Multiple Choice (MC)} \\ \hline

                
$Q_1$ & \cellcolor{lightdustygreen} -0.332 & 0.058 & \cellcolor{lightdustygreen} -0.593 \\
$Q_2$ & 0.135 & 0.131 & 0.266 \\
$Q_3$ & \cellcolor{lightdustypink} 0.314 & \cellcolor{lightdustypink} 0.49 & 0.101 \\

        \hline
    \end{tabular}
    \caption{Correlation between model performance before redefinition with the percentage of anchored answers for each type of unit of measure redefinition and question level in FS setup. 
    Cells highlighted in \textcolor{lightdustypink}{pink} indicate a \textbf{high positive correlation} ($>0.3$), while cells in \textcolor{lightdustygreen}{green} indicate a \textbf{high negative correlation} ($<-0.3$).}
    \label{tab:correlation_fs-units}
\end{table}



\begin{table}[h!]
\small
    \centering
    \begin{tabular}{l|ccc}
        \hline
        Level & $R_{a1}$ & $R_{a2}$ & $R_{a3}$ \\ \hline
        & \multicolumn{3}{c}{Free-Form (FF)} \\ 
        \hline
$Q_1$ & \cellcolor{lightdustygreen} -0.502 & \cellcolor{lightdustygreen} -0.598 & \cellcolor{lightdustygreen} -0.529 \\
$Q_2$ & \cellcolor{lightdustygreen} -0.465 & \cellcolor{lightdustygreen} -0.3 & -0.174 \\ 
$Q_3$ & -0.232 & -0.181 & -0.079 \\ 
        
        \hline
                & \multicolumn{3}{c}{Multiple Choice (MC)} \\ \hline

                
$Q_1$ & \cellcolor{lightdustygreen} -0.528 & -0.023 & \cellcolor{lightdustygreen} -0.523 \\ 
$Q_2$ & 0.015 & -0.091 & -0.016 \\
$Q_3$ & -0.127 & 0.013 & -0.242 \\

        \hline
    \end{tabular}
    \caption{Correlation between model performance before redefinition with the percentage of anchored answers for each type of unit of measure redefinition and question level in CoT setup. 
    Cells highlighted in \textcolor{lightdustypink}{pink} indicate a \textbf{high positive correlation} ($>0.3$), while cells in \textcolor{lightdustygreen}{green} indicate a \textbf{high negative correlation} ($<-0.3$).}
    \label{tab:correlation_cot-units}
\end{table}





\begin{figure*}[h]
\begin{subfigure}{\textwidth}
        \centering
         \includegraphics[width=0.86\linewidth]{images/mistral7b_stacked_bars-units.png}
        \caption{Response breakdown for Mistral7B before and after units of measure redefinitions.}
        \label{fig:mistral7b_MC-units}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.86\linewidth]{images/mistral_large_stacked_bars-units.png}
        \caption{Response breakdown for Mistral Large before and after units of measure redefinitions.}
        \label{fig:mistral_large_MC-units}
    \end{subfigure}
    \caption{Comparison of Mistral7B and Mistral Large (123B)  responses on the MC response format for units of measure redefinitions.}
    \label{fig:mistral_all-units}
\end{figure*}



\begin{figure*}[h]
\begin{subfigure}{\textwidth}
        \centering
         \includegraphics[width=0.86\linewidth]{images/llama8b_stacked_bars-units.png}
        \caption{Response breakdown for Llama8B before and after units of measure redefinitions.}
        \label{fig:llama8b_MC-units}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.86\linewidth]{images/llama405_stacked_bars-units.png}
        \caption{Response breakdown for Llama405B before and after units of measure redefinitions.}
        \label{fig:llama405b_large_MC-units}
    \end{subfigure}
    \caption{Comparison of Llama8B and Llama405B  responses on the MC response format for units of measure redefinitions.}
    \label{fig:llama_all-units}
\end{figure*}
