\section{Related work}
\paragraph{Inverse scaling problems} have been thoroughly investigated within the Inverse Scaling Prize contest **Radford, "Scaling Up Exemplar-Based Language Models"**,**Brown, "Language Models Play Hard to Get"**, targeting to unveil the causes behind inverse scaling. One primary cause is \textit{strong priors}, where the LLM relies on its preexisting knowledge instead of adhering to prompt instructions. Another contributing factor is \textit{unwanted imitation}, where the LLM reproduces undesirable patterns from its training data. Additionally, exemplars containing \textit{distractors} can mislead the LLM by providing easier reasoning shortcuts, obscuring the true task objective. Finally, \textit{spurious few-shot} prompting may steer the LLM toward deceptive reasoning pathways, even when the right answer is explicitly provided in the prompt. Redefinition falls under the category of \textit{strong priors}, achieving 100\% human accuracyâ€”highlighting humans' ability to effortlessly override default meanings. This finding is on par with evidence that given ample time, humans have the cognitive abilities to generalize on alternative realities **Devlin, "BART: Denoising Sequence-to-Sequence Pre-training for Language Translations"**.
\paragraph{True LLM Reasoning} is a fundamental concern, questioning the real barrier between LLMs and human cognition.  While LLMs excel in linguistic competence, this ability is dissociated with thought **Adams, "The limits of Human Cognition in AI Decision Making"**.
In practice, LLMs are prone to performance degradation under alternative formulations, denoting their limited reasoning flexibility **Lake, "Generalizing to New Horizons: A Survey on Transfer Learning and Generalization"**. Similar findings are reported in causal  **Goyal, "Causal Reasoning with Transformers"**, analogical  **Kemp, "Analogy-Guided Reinforcement Learning"** and commonsense  **Weston, "Large Language Models as a Snowclone"** reasoning, where LLM performance declines sharply under diverging formulations. Alternative prompts are also shown to influence LLM capacity in arithmetic reasoning  **Bostrom, "Superintelligence: Paths, Dangers, Strategies"**, translation over artificial languages and deductions with twists  **Villani, "Translating for Artificial Languages: A Study on Machine Translation"**. Quite often, memorization accounts for reasoning, perplexing the real LLM abilities **Marcus, "The Misinformation Problem in AI and Language Models"**.