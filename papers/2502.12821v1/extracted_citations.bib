@article{Ball2024CanWC,
  title={Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities},
  author={Thomas Ball and Shuo Chen and Cormac Herley},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.07638},
  url={https://api.semanticscholar.org/CorpusID:272600130}
}

@inproceedings{Gendron2024CanLL,
  title={Can Large Language Models Learn Independent Causal Mechanisms?},
  author={Ga{\"e}l Gendron and Bao Trung Nguyen and Alex Yuxuan Peng and Michael Witbrock and Gillian Dobbie},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:267412032}
}

@article{Lewis2024UsingCT,
  title={Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models},
  author={Martha Lewis and Melanie Mitchell},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.08955},
  url={https://api.semanticscholar.org/CorpusID:267657861}
}

@article{Mahowald2024DissociatingLA,
  title={Dissociating language and thought in large language models},
  author={Kyle Mahowald and Anna A. Ivanova and Idan Asher Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
  journal={Trends in Cognitive Sciences},
  year={2024},
  volume={28},
  pages={517-540},
  url={https://api.semanticscholar.org/CorpusID:268551442}
}

@article{Nezhurina2024AliceIW,
  title={Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models},
  author={Marianna Nezhurina and Lucia Cipolina-Kun and Mehdi Cherti and Jenia Jitsev},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.02061},
  url={https://api.semanticscholar.org/CorpusID:270226508}
}

@article{Stevenson2024CanLL,
  title={Can Large Language Models generalize analogy solving like people can?},
  author={Claire E. Stevenson and Alexandra Pafford and Han L. J. van der Maas and Melanie Mitchell},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.02348},
  url={https://api.semanticscholar.org/CorpusID:273821010}
}

@misc{jin2024largelanguagemodelsinfer,
      title={Can Large Language Models Infer Causation from Correlation?}, 
      author={Zhijing Jin and Jiarui Liu and Zhiheng Lyu and Spencer Poff and Mrinmaya Sachan and Rada Mihalcea and Mona Diab and Bernhard Sch√∂lkopf},
      year={2024},
      eprint={2306.05836},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05836}, 
}

@inproceedings{li-etal-2024-challenging,
    title = "Challenging Large Language Models with New Tasks: A Study on their Adaptability and Robustness",
    author = "Li, Chenxi  and
      Tian, Yuanhe  and
      Zerong, Zhaxi  and
      Song, Yan  and
      Xia, Fei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.485/",
    doi = "10.18653/v1/2024.findings-acl.485",
    pages = "8140--8162",
    abstract = "Recent progress in large language models (LLMs) has marked a notable milestone in the field of artificial intelligence. The conventional evaluation of LLMs primarily relies on existing tasks and benchmarks, raising concerns about test set contamination and the genuine comprehension abilities of LLMs. To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs' adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies. We investigate the capacity of LLMs to adapt to new but simple tasks, especially when they diverge from the models' pre-existing knowledge. Our methodology emphasizes the creation of straightforward tasks, facilitating a precise error analysis to uncover the underlying causes of LLM failures. This strategic approach also aims to uncover effective strategies for enhancing LLM performance based on the detailed error analysis of system output."
}

@misc{lou2024quantifyingincontextreasoningeffects,
      title={Quantifying In-Context Reasoning Effects and Memorization Effects in LLMs}, 
      author={Siyu Lou and Yuntian Chen and Xiaodan Liang and Liang Lin and Quanshi Zhang},
      year={2024},
      eprint={2405.11880},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.11880}, 
}

@misc{mckenzie2024inversescalingbiggerisnt,
      title={Inverse Scaling: When Bigger Isn't Better}, 
      author={Ian R. McKenzie and Alexander Lyzhov and Michael Pieler and Alicia Parrish and Aaron Mueller and Ameya Prabhu and Euan McLean and Aaron Kirtland and Alexis Ross and Alisa Liu and Andrew Gritsevskiy and Daniel Wurgaft and Derik Kauffman and Gabriel Recchia and Jiacheng Liu and Joe Cavanagh and Max Weiss and Sicong Huang and The Floating Droid and Tom Tseng and Tomasz Korbak and Xudong Shen and Yuhui Zhang and Zhengping Zhou and Najoung Kim and Samuel R. Bowman and Ethan Perez},
      year={2024},
      eprint={2306.09479},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09479}, 
}

@misc{wang2024generalizationvsmemorizationtracing,
      title={Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data}, 
      author={Xinyi Wang and Antonis Antoniades and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang},
      year={2024},
      eprint={2407.14985},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.14985}, 
}

@inproceedings{wu-etal-2024-reasoning,
    title = "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
    author = {Wu, Zhaofeng  and
      Qiu, Linlu  and
      Ross, Alexis  and
      Aky{\"u}rek, Ekin  and
      Chen, Boyuan  and
      Wang, Bailin  and
      Kim, Najoung  and
      Andreas, Jacob  and
      Kim, Yoon},
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.102/",
    doi = "10.18653/v1/2024.naacl-long.102",
    pages = "1819--1862",
    abstract = "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on {\textquotedblleft}counterfactual{\textquotedblright} task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects."
}

@misc{xie2024memorizationlargelanguagemodels,
      title={On Memorization of Large Language Models in Logical Reasoning}, 
      author={Chulin Xie and Yangsibo Huang and Chiyuan Zhang and Da Yu and Xinyun Chen and Bill Yuchen Lin and Bo Li and Badih Ghazi and Ravi Kumar},
      year={2024},
      eprint={2410.23123},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23123}, 
}

