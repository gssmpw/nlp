\section{Related work}
\paragraph{Inverse scaling problems} have been thoroughly investigated within the Inverse Scaling Prize contest \cite{mckenzie2024inversescalingbiggerisnt}, targeting to unveil the causes behind inverse scaling. One primary cause is \textit{strong priors}, where the LLM relies on its preexisting knowledge instead of adhering to prompt instructions. Another contributing factor is \textit{unwanted imitation}, where the LLM reproduces undesirable patterns from its training data. Additionally, exemplars containing \textit{distractors} can mislead the LLM by providing easier reasoning shortcuts, obscuring the true task objective. Finally, \textit{spurious few-shot} prompting may steer the LLM toward deceptive reasoning pathways, even when the right answer is explicitly provided in the prompt. Redefinition falls under the category of \textit{strong priors}, achieving 100\% human accuracyâ€”highlighting humans' ability to effortlessly override default meanings. This finding is on par with evidence that given ample time, humans have the cognitive abilities to generalize on alternative realities \cite{wu-etal-2024-reasoning}.
\paragraph{True LLM Reasoning} is a fundamental concern, questioning the real barrier between LLMs and human cognition.  While LLMs excel in linguistic competence, this ability is dissociated with thought \cite{Mahowald2024DissociatingLA}.
In practice, LLMs are prone to performance degradation under alternative formulations, denoting their limited reasoning flexibility \cite{wu-etal-2024-reasoning}. Similar findings are reported in causal  \cite{jin2024largelanguagemodelsinfer, Gendron2024CanLL}, analogical \cite{Lewis2024UsingCT, Stevenson2024CanLL} and commonsense \cite{Nezhurina2024AliceIW} reasoning, where LLM performance declines sharply under diverging formulations. Alternative prompts are also shown to influence LLM capacity in arithmetic reasoning \cite{Ball2024CanWC, li-etal-2024-challenging}, translation over artificial languages and deductions with twists \cite{li-etal-2024-challenging}. Quite often, memorization accounts for reasoning, perplexing the real LLM abilities \cite{xie2024memorizationlargelanguagemodels, lou2024quantifyingincontextreasoningeffects, wang2024generalizationvsmemorizationtracing}.