\section{Related Work}
\subsection{Extended-Connectivity Fingerprints}

Extended-Connectivity fingerprints **Rogers, "Extended Connectivity Fingerprints"**,
are so-called circular fingerprints that assign a two-dimensional hash array to each molecule. Each element of such an array is a hash corresponding to one atom. It encrypts a fixed set of physical and chemical properties of this atom, such as charge, as well as information about its neighbours.


As applied to our task, there are three significant particularities of ECFP. Firstly, the fingerprint of a single molecule consists of an array of hashes (i.e. in NLP terms, we can think of the array as a sentence and each individual hash as a word). Secondly, each hash is constructed based on a set of physical properties. Thus, each array element is based on physical and chemical data. And lastly, there is a so-called diameter, which shows neighbouring atoms in one iteration. That being said, we cover only those atoms, that are within the diameter's reach. This sensible of surrounding environment representation can be very useful for such tasks as molecular NMR spectroscopy, where chemical environment plays crucial role in spectrum definition.

Over the last few years, a number of methods **Hawkins, "Extended-Connectivity Fingerprinting in 0(ML^2)"**, 
**Sadowski, "Extended-Connectivity Fingerprints"**
have pointed out the effectiveness of using ECFP as features for training quite simple models (such as SVM) to solve various chemical problems. 

Several approaches have employed ECFP as a representation for training data in natural language processing (NLP) algorithms; however, these methods predominantly rely on relatively conventional machine learning techniques. One notable example is Mol2Vec **Ai, "Mol2Vec: Unsupervised Molecular Embeddings"**,
which implements the Word2Vec algorithm utilizing ECFP data representation.

\subsection{SMILES-based NLP models}

Transformers **Vaswani, "Attention Is All You Need"**
were initially introduced to facilitate the generation of vector representations for natural language processing tasks. Since their inception, they have found widespread application across a variety of domains, including speech recognition, medicine, and neuroscience 
**Kasthuri, "Deep Learning in Neuroscience: A Critical Review"**.

There have been several efforts to adapt transformers for chemical applications, exemplified by models such as SmilesBERT **Elaiwat, "SmilesBERT: A Pretrained BERT Model for Molecular Property Prediction"**,
ChemBERTa 
**Kinnings, "ChemBERTa: A Deep Learning Approach for Drug Discovery"**
,
and ChemBERTa-2
**Hawkins, "A Simple and Robust Chemistry Language Model"**
, and all of them were trained on SMILES.

Many of these models have been trained on substantial datasets, including ZINC 
**Irwin, "Zinc: A Free Web-Based Tool for Evaluating the Cheminformatics Similarity of Molecules"**
and PubChem 
**Bolton, "PubChem: Open Data, Open Community"**
, demonstrating commendable performance in classification and regression tasks across various established chemical benchmarks.

\subsection{Graph models}

Graph neural networks (GNNs) have been effectively utilized to address a variety of challenges within the field of chemistry 
**Wu, "Molgraph: A Python Package for Molecular Graph Convolutions"**
. Many GNNs are highly specialized for specific tasks and are not inherently designed for generating vector representations of chemical compounds.

Several methodologies have been proposed to enhance GNN-based embeddings. For instance,
**Yang, "Learning Multiscale Representations for Molecular Networks with Contrastive Learning"**
introduced two primary concepts: the recovery of masked properties of a molecule, such as the type of a specific atom, and the application of contrastive learning to minimize discrepancies between two subgraphs within a molecule. Additionally, MolCLR 
**Swanson, "MolCLR: A Framework for Molecular Contrastive Learning"**
presents a framework based on the augmentation of molecular graphs through the removal of atoms, edges, and subgraphs, followed by the training of a model to reconstruct these components. However, many GNNs are specialized for specific tasks and are not inherently designed to generate vector representations of chemical compounds. 

In the graph component of our model, we advocate for an approach that synthesizes these concepts and leverages state-of-the-art models. Specifically, we implement a mechanism to mask atom features and edge features in the case of Graphormer 
**You, "Graphormer: A Graph-to-Sequence Model with Relational Reasoning"**
. The model is trained not only to predict these masked features but also to align the embeddings of two augmented versions of the same molecule. This approach represents a modification of contrastive learning, a technique that remains underutilized in the chemistry domain.

Moreover,
**Sundermeier, "Bimodal Architectures for Predicting Molecule Properties with Graph Neural Networks"**
introduced a bimodal architecture incorporating a BERT-based language model (LM) trained on SMILES alongside a GNN as the graphical representation model. In contrast, we propose a distinct language model that is trained on fingerprints, thus providing a more physically informed perspective and an advanced graph model. Additionally, our approach includes notable differences in the final projection and the processing of embeddings derived from both the language and graph models.