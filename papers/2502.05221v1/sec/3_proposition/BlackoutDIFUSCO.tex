\subsection{Blackout Diffusco}
\paragraph{Forward Process.}  
The forward process in Blackout Diffusion progressively corrupts the original adjacency matrix \( x_0 \in \{0, 1\}^{N \times N} \), representing the binary edge connectivity, into a fully degenerate blackout state \( x_T = 0 \). The dynamics are governed by a pure-death Markov process, where each edge state transitions from active (\(1\), edge is in the tour) to inactive (\(0\), edge is not in the tour) over a continuous time interval \([0, T]\). The forward process can be expressed as:
\[
q(x_t | x_0) = \mathrm{Binomial}(x_t; x_0, e^{-t}),
\]
where \( e^{-t} \) represents the decay rate of edge states over time. 

As \( t \to T \), the corrupted matrix \( x_t \) converges to a black image, effectively destroying all structural information in \( x_0 \). To ensure complete corruption, we set \( T \) to a sufficiently large value of 15.0, as done in this work.

\paragraph{Reverse Process.}  
The reverse process reconstructs the adjacency matrix \( x_0 \) by simulating a birth-only process, effectively reversing the pure-death dynamics of the forward process. Each element of the matrix, corresponding to an edge state \((i, j)\), is processed independently. The reverse transition probability for edge state \((i, j)\) is modeled as:


\begin{equation}
p(x_{s}(i, j) | x_t(i, j), x_0(i, j)) = 
\binom{o - n}{m - n} \cdot r^{m-n} (1 - r)^{o-m},
\label{eq:blackout_reverse}
\end{equation}


where the variables are defined as follows:
\begin{itemize}
    \item \( n = x_t(i, j) \): The corrupted edge state at timestep \( t \),
    \item \( m = x_{s}(i, j) \): The intermediate reconstructed edge state at timestep \( s \),
    \item \( o = x_0(i, j) \): The original binary edge state,
    \item \( r = \frac{e^{-s} - e^{-t}}{1 - e^{-t}} \): The binomial transition parameter.
\end{itemize}

This formulation, derived from the \textit{binomial bridge}, ensures a smooth and probabilistically consistent transition from the corrupted state \( x_t \) to the original state \( x_0 \).

To approximate \( \hat{x}_0 \) from \( \hat{x}_t \), a neural network \(\text{NN}_\theta\) is used to predict the change in state \(\Delta x_{t \to 0}\), representing the difference between \( \hat{x}_t \) and the original state \( \hat{x}_0 \), as:
\[
\Delta x_{t \to 0} = \text{NN}_\theta(\hat{x}_t, t)
\]
where:
\begin{itemize}
    \item \( \text{NN}_\theta \): A parameterized neural network,
    \item Inputs: The corrupted adjacency matrix \( \hat{x}_t \) and the timestep \( t \),
    \item Outputs: The predicted change \(\Delta x_{t \to 0}\) between \( \hat{x}_t \) and \( \hat{x}_0 \).
\end{itemize}

The reconstruction of \( x_s \) is then performed iteratively as:
\[
\hat{x}_0 = \hat{x}_t + \Delta x_{t \to 0}.
\]
The probability \( p(x_{s} | \hat{x}_t, \hat{x}_0) \) follows a similar formulation to the binomial transition probability shown in Equation~\ref{eq:blackout_reverse}, ensuring consistency with the dynamics of the reverse process.

This iterative refinement ensures that the adjacency matrix \( x_0 \) is reconstructed accurately, element by element, while respecting the probabilistic structure modeled by the binomial bridge. Each edge state evolves independently during the reverse process, guided by the neural network's predictions and the binomial transition probabilities.

\paragraph{Training Objective.}  
The training objective is designed to minimize the discrepancy between the predicted and true adjacency matrice, incorporating the temporal dynamics of the diffusion process. The loss function for each element \(i\) is formulated as:
\begin{equation}
l_i = (t_k - t_{k-1}) e^{-t_k} \left[ y_i - \left( (x_0 - x_{t_k})_i \log y_i \right) \right],
\label{eq:training_objective}
\end{equation}
where:
\begin{itemize}
    \item \(t_k\) and \(t_{k-1}\) represent successive timesteps,
    \item \(y_i\) is the difference between two states for the element \(i\), which is \(\Delta x_{t \to 0}\),
    \item \(x_0\) is the ground truth adjacency matrix,
    \item \(x_{t_k}\) is the corrupted adjacency matrix at timestep \(t_k\).
\end{itemize}

This loss function ensures that the model learns to predict transitions in the adjacency matrix while respecting the temporal dynamics introduced during the diffusion process. By incorporating the time-dependent weight \(e^{-t_k}\) and the logarithmic term \(\log y_i\), the objective penalizes large deviations in the reconstructed probabilities and encourages convergence towards the true solution.

\paragraph{Simplified Loss.}  
The training objective can also be simplified to focus directly on the discrepancy between the predicted and true adjacency tensors, with less emphasis on the temporal dynamics. The simplified loss function for each element \(i\) is expressed as:
\begin{equation}
l_i = \left[ y_i - \left( (x_0 - x_{t_k})_i \log y_i \right) \right],
\label{eq:Simplified_Loss}
\end{equation}

This formulation simplifies the original loss by removing the explicit time-dependent weighting terms, allowing the focus to remain on accurately reconstructing the true adjacency tensor while still leveraging the diffusion dynamics for guidance.
