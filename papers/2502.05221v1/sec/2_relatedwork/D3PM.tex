\subsubsection{Discrete Denoising Diffusion Probabilistic Models (D3PM)}

D3PM \cite{austin2021structured} generalizes diffusion models to discrete state spaces, making it particularly effective for tasks where data is inherently categorical or binary. In graph-based combinatorial optimization problems, such as those addressed by DIFUSCO \cite{sun2023difusco}, the data is represented as an $N \times N \times 2$ tensor, where each entry corresponds to the state of an edge between two nodes. The tensor encodes probabilities for the two discrete states: \(k = 0\) for no edge (\(1 - p_{\text{edge}}\)) and \(k = 1\) for an edge (\(p_{\text{edge}}\)).

\paragraph{Forward Process.}  
The forward process progressively corrupts the original adjacency tensor \(x_0\) by applying a transition matrix \(Q_t \in \mathbb{R}^{2 \times 2}\) independently to each edge state at timestep \(t\). The transition matrix \(Q_t\) is defined as:
\[
Q_t = \begin{bmatrix} 
(1 - \beta_t) & \beta_t \\ 
\beta_t & (1 - \beta_t)
\end{bmatrix},
\]
where \(\beta_t \in (0, 1)\) governs the corruption level at timestep \(t\):
\begin{itemize}
    \item \((1 - \beta_t)\): Probability of maintaining the current state (edge or no edge),
    \item \(\beta_t\): Probability of transitioning to the opposite state.
\end{itemize}

At each timestep \(t\), the forward process is represented as:
\[
q(x_t | x_{t-1}) = \mathrm{Cat}(x_t; \tilde{x}_{t-1} Q_t),
\]
where \(\tilde{x}_{t-1} Q_t\) computes the probabilities of transitioning to each state based on the prior state and the corruption level \(\beta_t\). The recursion is defined as:
\[
x_t = \tilde{x}_{t-1} Q_t,
\]
applying \(Q_t\) to all entries of \(\tilde{x}_{t-1}\). Over multiple timesteps, the adjacency tensor \(x_t\) becomes increasingly corrupted, transitioning between edge and no-edge states with probabilities governed by \(\beta_t\). By the final timestep \(T\), the corrupted state \(x_T\) converges to a uniform or degenerate categorical distribution, effectively destroying all structural information in \(x_0\).

This formulation ensures systematic corruption where the degree of randomness introduced at each timestep is precisely controlled by \(\beta_t\), maintaining consistency across the \(N \times N \times 2\) tensor structure.

\paragraph{Reverse Process.}  
The reverse transition \(q(x_{t-1} | x_t, \tilde{x}_0)\) is derived using Bayes' theorem and is defined as:
\[
q(x_{t-1} | x_t, \tilde{x}_0) = \frac{q(x_t | x_{t-1}, \tilde{x}_0) q(x_{t-1} | \tilde{x}_0)}{q(x_t | \tilde{x}_0)},
\]
where \(q(x_t | x_{t-1}, \tilde{x}_0)\) represents the forward corruption step from \(x_{t-1}\) to \(x_t\), \(q(x_{t-1} | \tilde{x}_0)\) encodes the marginal transition from \(\tilde{x}_0)\) to \(x_{t-1}\) and \(q(x_t | \tilde{x}_0)\) normalizes the posterior over \(x_{t-1}\).

The forward transition across multiple timesteps is captured by the cumulative transition matrix \(\bar{Q}_t = Q_1 Q_2 \cdots Q_t\), where each \(Q_t \in \mathbb{R}^{2 \times 2}\) represents the corruption probabilities at timestep \(t\). The entries of \(\bar{Q}_t\) encode the overall probabilities of transitioning between the two states (edge or no edge) over the first \(t\) steps. This allows the marginal \(q(x_t | \tilde{x}_0)\) to be expressed as:
\[
q(x_t | \tilde{x}_0) = \mathrm{Cat}\left( x_t; p = \tilde{x}_0 \bar{Q}_t \right).
\]

Using this cumulative formulation, the posterior distribution \(q(x_{t-1} | x_t, \tilde{x}_0)\) is given by:
\[
q(x_{t-1} | x_t, \tilde{x}_0) = \mathrm{Cat}\left( x_{t-1}; p = \frac{\tilde{x}_t Q_t^\top \odot \tilde{x}_0 \bar{Q}_{t-1}}{\tilde{x}_0 \bar{Q}_t \tilde{x}_t^\top} \right),
\]
where \(\tilde{x}_t\) is the discretized adjacency tensor at timestep \(t\), \(Q_t^\top\) is the transpose of the transition matrix for timestep \(t\) and \(\odot\) denotes element-wise multiplication.

This formulation ensures that the reverse sampling process systematically reconstructs \(x_0\) while incorporating both the corrupted state \(x_t\) and the clean prediction \(\tilde{x}_0\).

The neural network \(p_\theta\) is trained to predict \(\tilde{x}_{0, \theta}\) (discretized version of \(x_{0, \theta}\)), the clean adjacency tensor, given the corrupted \(x_t\). The reverse process then substitutes the predicted \(\tilde{x}_0\) into the posterior to compute:
\[
p_\theta(x_{t-1} | x_t) = \sum_{\tilde{x}_0} q(x_{t-1} | x_t, \tilde{x}_0) p_\theta(\tilde{x}_0 | x_t).
\]

By iteratively applying this reverse sampling from \(x_T\) to \(x_0\), the process reconstructs the original adjacency tensor while maintaining consistency with the forward corruption model. The integration of the cumulative transition matrix \(\bar{Q}_t\) ensures that the overall corruption and reconstruction remain aligned across all timesteps.

\paragraph{Training Objective.}  
The model is trained to minimize the binary cross-entropy loss between the predicted discretized tensor \(\hat{x}_0\) (output by the neural network) and the ground truth discretized tensor \(x_0\). The process involves uniformly sampling a timestep \(t \in \{1, 2, \dots, T\}\), applying the forward corruption process to obtain \(x_t\), and using the neural network to predict \(x_{0, \theta} (x_t)\) from \(x_t\). The loss function is defined as:
\begin{align}
\mathcal{L} = \mathbb{E}_{t, x_0} \bigg[ 
& -\sum_{i=1}^N \sum_{j=1}^N \sum_{k=0}^1 \big( 
x_0(i, j, k) \log x_{0, \theta}(i, j, k) \nonumber \\
& + (1 - x_0(i, j, k)) \log (1 - x_{0, \theta}(i, j, k)) 
\big) 
\bigg],
\end{align}
where \(x_0(i, j, k) \in \{0, 1\}\) is the ground truth binary state for the edge between nodes \(i\) and \(j\) and \(x_{0, \theta}(i, j, k)\) is the predicted probability output by the neural network for the same edge state.

The training process ensures that the neural network learns to predict the ground truth discretized tensor \(x_0\) by minimizing the binary cross-entropy loss. By comparing \(x_{0, \theta}\) and \(x_0\) directly, the model aligns its predictions with the underlying structure of the original adjacency tensor, enabling accurate reconstruction during the reverse process.
