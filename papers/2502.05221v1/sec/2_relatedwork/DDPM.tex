\subsubsection{Denoising Diffusion Probabilistic Models (DDPM)}

\paragraph{Forward Process.}  
DDPM \cite{ho2020ddpm} operates on continuous data distributions by introducing Gaussian noise in a forward process. The process starts with the original data \(x_0\), representing the final heatmap, and iteratively adds Gaussian noise at each timestep \(t\) until the data becomes random noise \(x_T\), approximated as \(x_T \sim \mathcal{N}(0, I)\). The forward process is defined as:
\[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I),
\]
where \(x_t\) represents the noisy data at timestep \(t\), and \(\beta_t \in (0, 1)\) is the variance controlling the noise level. Over multiple timesteps, the structure in \(x_0\) is progressively corrupted, resulting in \(x_T\), which serves as the starting point for the reverse process.

\paragraph{Reverse Process.}  
The reverse process reconstructs \(x_0\) from \(x_T\) by iteratively denoising through intermediate steps \(x_{T-1}, x_{T-2}, \dots, x_0\). At each timestep \(t\), the reverse distribution is parameterized as:
\[
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)),
\]
where \(\mu_\theta\) is the predicted mean, \(\Sigma_\theta = \beta_t I\) is the fixed variance, and both are predicted by a neural network. The mean \(\mu_\theta\) is computed as:
\[
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} \epsilon_\theta(x_t, t) \right),
\]
where \(\alpha_t = \prod_{s=1}^t (1 - \beta_s)\) represents the cumulative noise schedule, and \(\epsilon_\theta(x_t, t)\) is the predicted noise at timestep \(t\). Starting from \(x_T\), the reverse process iteratively refines the noisy data, reducing the noise at each step to reconstruct \(x_0\).

The reverse process outputs either the predicted noise \(\epsilon_\theta\) at each timestep, which is used to refine \(x_t\), or the reconstructed data \(x_0\) at the end of the process. This iterative denoising ensures that \(x_0\) is a high-quality approximation of the original data distribution, making DDPM an effective generative framework.

\paragraph{Training Objective.}  
The training process optimizes the variational lower bound (VLB) to ensure that the reverse process accurately reconstructs the original data. The objective is expressed as:
\begin{align}
\mathcal{L} = \mathbb{E}_{q(x_0)} \bigg[ 
& D_{\mathrm{KL}}(q(x_T | x_0) \| p(x_T)) \nonumber \\
& + \sum_{t=2}^{T} D_{\mathrm{KL}}(q(x_{t-1} | x_t, x_0) \| p_\theta(x_{t-1} | x_t)) \nonumber \\
& - \log p_\theta(x_0 | x_1)
\bigg],
\end{align}
where \(D_{\mathrm{KL}}\) is the Kullback-Leibler divergence, \(q(x_T | x_0)\) represents the forward processâ€™s noise distribution, and \(p_\theta(x_{t-1} | x_t)\) parameterizes the reverse process.

\paragraph{Simplified Loss.}  
For practical implementation, the training objective is often simplified to directly predict the noise \(\epsilon\) added during the forward process, leading to the following reparameterized loss:
\[
\mathcal{L}_{\mathrm{simple}} = \mathbb{E}_{t, x_0, \epsilon} \big[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \big],
\]
where \(\epsilon \sim \mathcal{N}(0, I)\) is the ground truth noise. This simplified loss allows efficient optimization of the neural network to predict noise accurately.
