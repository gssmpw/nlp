\section{Evaluation} \label{sec:evaluation}
Evaluating concept erasure methods is essential for quantifying their effectiveness and enabling fair comparisons across different approaches. This section reviews widely adopted metrics and datasets to assess both the success of concept removal and the preservation of general model capabilities.

\subsection{Metrics}
Concept erasure methods are typically evaluated in two key aspects: erasure effectiveness and model fidelity. 

The Erasure Success Rate (ESR) measures how effectively a method removes a target concept. This is commonly assessed using classification accuracy, where a pre-trained classifier determines whether the erased concept remains present in the generated images. Formally, ESR is defined as:
\begin{equation}\label{eq:esr}
    \text{ESR} = \frac{1}{N} \sum_{i=1}^{N} \mathbbm{1}\left( f\left(SD'(y_i)\right) = c_{\text{erase}} \right),
\end{equation}
where \( y_i \) represents the prompt, \( c_{\text{erase}} \) is the erased concept, \( f \) is a classifier, and \( N \) is the total number of prompts. Lower ESR values indicate more successful erasure. ESR can also be extended to evaluate robustness against adversarial attacks by replacing standard prompts \( y_i \) with adversarially optimized prompts \( y_{\text{adv.}} \) or modifying latent variables \( z_{\text{adv.}} \), allowing an assessment of how well the model resists attempts to reconstruct erased concepts.

To ensure that erasure does not degrade the model's ability to generate non-erased content, model fidelity is evaluated by measuring both image quality and text-image alignment before and after concept removal. Fr√©chet Inception Distance (FID)~\cite{fid} is widely used to quantify changes in the perceptual quality of generated images. In addition to image quality, maintaining alignment between textual prompts and generated outputs is crucial. CLIP score~\cite{hessel2021clipscore} is commonly employed for this purpose, providing a similarity measure between the generated image and its corresponding textual prompt. Furthermore, ESR can also serve as a fidelity metric by computing it on prompts unrelated to the erased concept, denoted as \( y_{\text{non-erase}} \).


\subsection{Datasets}
Dataset selection depends on the nature of the concepts being erased, with commonly used datasets categorized according to their evaluation objectives. For assessing NSFW content removal, the I2P dataset~\cite{Schramowski2022SafeLD}, which consists of 4,703 real-world user-generated prompts, is widely employed. Object concept removal is typically evaluated using structured prompts such as ``A photo of [object class]'', enabling controlled experiments on whether erased objects still appear in generated images. Artistic style erasure often relies on ESD's artist prompt dataset~\cite{esd}, which provides standardized prompts referencing specific artistic styles.

To evaluate model fidelity, the COCO dataset~\cite{lin2014microsoft} is commonly used. This dataset enables FID-based image quality assessment and supports CLIP score evaluation for measuring text-image alignment. Beyond standard datasets, robustness evaluation requires datasets explicitly designed for testing adversarial vulnerabilities. 
For example, the MMA-Diffusion benchmark~\cite{Yang2023MMADiffusionMA} and Ring-A-Bell dataset~\cite{Tsai2023RingABellHR} feature adversarial prompts designed to evade concept erasure and systematically test its vulnerabilities.


Together, these metrics and datasets establish a comprehensive framework for evaluating concept erasure, ensuring that methods are assessed not only for their effectiveness in removing targeted concepts but also for their ability to maintain generative quality and resist adversarial attacks.
