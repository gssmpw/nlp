\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/taxonomy_v3.pdf}
    \caption{
    Taxonomy of Concept Erasers. Concept erasure methods are categorized based on their optimization strategy (first level) and the model components they modify (second level). A detailed discussion is provided in Sec.~\ref{sec:method}.
    }
    \label{fig:taxonomy}
\end{figure}

\section{Backgrounds} \label{sec:preliminaries}

This section presents an overview of the Text-to-Image (T2I) diffusion model with a particular focus on Stable Diffusion (SD)~\cite{stable_diffusion}. %, which serves as the foundational framework for evaluating concept erasure methods. 
As shown in Fig.~\ref{fig:overview}, SD comprises three main components: a vision decoder for reconstructing images from latent representations, a latent diffusion model for iterative denoising, and a conditional text encoder that transforms textual prompts into conditioning vectors.
We outline both the training and inference mechanisms of SD, which are essential for understanding how concept erasure techniques modify key model components or inference steps to suppress undesired concepts.

% \begin{table*}[t!]
%   \caption{Taxonomy of Concept Erasure Methods in T2I Models. Methods are categorized based on their optimization strategies and the specific model components they modify. CA denotes the cross-attention layers within the U-Net, while CFG refers to Classifier-Free Guidance adjustments. A comprehensive discussion of these methods is provided in Sec.~\ref{sec:method}.
% }
%   \centering
%   % \footnotesize
%   % \small
%   \scriptsize
%   \setlength{\tabcolsep}{3pt} % Reduce spacing for the first three columns
%   \begin{adjustbox}{width=\textwidth,center}
%     \begin{tabular}{p{1.8cm}p{3.6cm}cccccp{7.0cm}} 
%       \toprule
%       \textbf{Category} & \textbf{Representative Works} & \multicolumn{5}{c}{\textbf{Optimization Space}} & \textbf{Optimization Strategy} \\ 
%       \cmidrule(lr){3-7} 
%        & & \textbf{U-Net} & \textbf{CA} & \textbf{CLIP} & \textbf{LLM} & \textbf{CFG} & \\ 
%       \midrule
%       \multirow{13}{*}{\textbf{Fine-tuning}}  
%       & FMN~\cite{Zhang2023ForgetMeNotLT} &  & \cmark &  &  &  & Attention reweighting \\ 
%       & AC~\cite{Ablating_Concept} &  & \cmark &  &  &  & Remapping erased concepts \\ 
%       & SALUN~\cite{fan2024salun} & \cmark &  &  &  &  & Saliency-guided tuning \\ 
%       & ESD~\cite{esd} & \cmark & \cmark &  &  &  & Concept removal in generative noise process \\ 
%       & DT~\cite{Ni2023DegenerationTuningUS} & \cmark &  &  &  &  & Targeted concept degradation \\ 
%       & Geom-Erasing~\cite{Liu2023ImplicitCR} & \cmark &  &  &  &  & Targeted concept degradation \\ 
%       & SA~\cite{Heng2023SelectiveAA} & \cmark &  &  &  &  & Targeted concept degradation \\ 
%       & IMMA~\cite{Zheng2023IMMAIT} & \cmark &  &  &  &  & Prevents unauthorized fine-tuning \\ 
%       & SAFE-CLIP~\cite{safe_clip} &  &  & \cmark &  &  & Adversarial robustness for CLIP \\ 
%       & Latent Guard~\cite{Liu2024LatentGA} &  &  & \cmark &  &  & Targeted feature suppression \\ 
%       & AdvUnlearn~\cite{Zhang2024DefensiveUW} &  &  & \cmark &  &  & Adversarial fine-tuning for CLIP \\
%       & Receler~\cite{Huang2023RecelerRC} & \cmark  &  &  &  &  & Introduces adapter for robustness in U-Net \\
%       & R.A.C.E~\cite{RACE} & \cmark  &  &  &  &  & Adversarial fine-tuning for U-Net \\
%       \midrule
%       \multirow{7}{*}{\textbf{Closed-form}}  
%       & ReFACT~\cite{Arad2023ReFACTUT} &  &  & \cmark &  &  & Low-rank memory update in CLIP MLP layers \\ 
%       & TIME~\cite{Orgad2023EditingIA} &  & \cmark &  &  &  & Projection matrix updates in cross-attention \\ 
%       & UCE~\cite{Gandikota2023UnifiedCE} &  & \cmark &  &  &  & Multi-concept projection learning \\ 
%       & MACE~\cite{Lu2024MACEMC} &  & \cmark &  &  &  & LoRA-based parameter refinement for erasure \\ 
%       & EMCID~\cite{Xiong2024EditingMC} & \cmark & \cmark &  &  &  & Two-stage closed-form editing (self-distillation + projection) \\ 
%       & MUNBa~\cite{Wu2024MUNBaMU} &  & \cmark & \cmark &  &  & Nash bargaining-based concept unlearning \\ 
%       & RECE~\cite{Gong2024ReliableAE} & \cmark  &  &  &  &  & Adversarial fine-tuning \\
%       \midrule
%       \multirow{6}{*}{\textbf{Inference-Time}}  
%       & SLD~\cite{sld} &  & \cmark &  &  & \cmark & Adjusts latent denoising dynamics \\ 
%       & AMG~\cite{Chen2024TowardsMD} &  & \cmark &  &  & \cmark & Prevents overfitting to erased concepts \\ 
%       & SAFREE~\cite{safree} &  &  & \cmark &  &  & Prevents undesired text-image associations \\ 
%       & Content Suppression~\cite{Li2024GetWY} &  &  & \cmark &  &  & Enforces embedding constraints \\ 
%       & ORES~\cite{ores} &  &  &  & \cmark &  & LLM-based adversarial filtering \\ 
%       & GuardT2I~\cite{Yang2024GuardT2IDT} &  &  &  & \cmark &  & Detects circumvention prompts \\ 
%       \bottomrule
%     \end{tabular}
%   \end{adjustbox}
%   \label{tab:taxonomy}
% \end{table*}

\begin{table*}[t!]
  \caption{Taxonomy of Concept Erasure Methods in T2I Models. Methods are categorized based on their optimization strategies and the specific model components they modify. In the third column, "CA" denotes the cross-attention layers within the latent diffusion model, while "CFG" refers to Classifier-Free Guidance adjustments. A comprehensive discussion of these methods is provided in Sec.~\ref{sec:method}.}
  \centering
  \scriptsize
  \setlength{\tabcolsep}{3pt} % Reduce spacing for the first three columns
  \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{p{1.8cm}p{3.6cm}cccccp{6.5cm}} 
      \toprule
      \textbf{Category} & \textbf{Representative Works} & \multicolumn{5}{c}{\textbf{Optimization Space}} & \hspace{5mm} \textbf{Description} \\ 
      \cmidrule(lr){3-7} 
       & & \textbf{U-Net} & \textbf{CA} & \textbf{CLIP} & \textbf{LLM} & \textbf{CFG} & \\ 
      \midrule
      \multirow{13}{1.8cm}{\textbf{Fine-Tuning}}  
      & FMN~\cite{Zhang2023ForgetMeNotLT} &  & \cmark &  &  &  & Minimize attention activation to erase concepts. \\ 
      & AC~\cite{Ablating_Concept} &  & \cmark &  &  &  & Remaps erased concepts to general concepts. \\ 
      & SALUN~\cite{fan2024salun} & \cmark &  &  &  &  & Modifies influential weights to remove concepts. \\ 
      & ESD~\cite{esd} & \cmark & \cmark &  &  &  & Edits noise prediction to remove concepts. \\ 
      & DT~\cite{Ni2023DegenerationTuningUS} & \cmark &  &  &  &  & Degrades model’s ability to reconstruct erased concepts. \\ 
      & Geom-Erasing~\cite{Liu2023ImplicitCR} & \cmark &  &  &  &  & Uses geometric constraints for concept removal. \\ 
      & SA~\cite{Heng2023SelectiveAA} & \cmark &  &  &  &  & Continual learning-based forgetting approach. \\ 
      & IMMA~\cite{Zheng2023IMMAIT} & \cmark &  &  &  &  & Enhances robustness against unauthorized fine-tuning. \\ 
      & SAFE-CLIP~\cite{safe_clip} &  &  & \cmark &  &  & Fine-tunes CLIP with safe and unsafe text-image quadruplets. \\ 
      & Latent Guard~\cite{Liu2024LatentGA} &  &  & \cmark &  &  & Fine-tunes the CLIP text encoder with safe and unsafe pairs. \\ 
      & AdvUnlearn~\cite{Zhang2024DefensiveUW} &  &  & \cmark &  &  & Adversarial finetuning for CLIP text encoder. \\
      & Receler~\cite{Huang2023RecelerRC} &  & \cmark &  &  &  & Uses adapters to enhance robustness. \\
      & R.A.C.E~\cite{RACE} & \cmark  & \cmark &  &  &  & Adversarially fine-tunes U-Net for resilience. \\
      \midrule
      \multirow{7}{1.8cm}{\textbf{Closed-form Model Editing}}  
      & ReFACT~\cite{Arad2023ReFACTUT} &  &  & \cmark &  &  & Updates CLIP’s memory via low-rank edits. \\ 
      & TIME~\cite{Orgad2023EditingIA} &  & \cmark &  &  &  & Modifies CA projection matrices for concept editing. \\ 
      & UCE~\cite{Gandikota2023UnifiedCE} &  & \cmark &  &  &  & Simultaneously erases multiple concepts. \\ 
      & MACE~\cite{Lu2024MACEMC} &  & \cmark &  &  &  & Utilize adapters for large-scale concept erasure. \\ 
      & EMCID~\cite{Xiong2024EditingMC} &  &  & \cmark  &  &  & Large-scale concept erasure via two-stage closed-form editing \\ 
      & MUNBa~\cite{Wu2024MUNBaMU} &  &  & \cmark &  &  & Uses Nash bargaining for controlled concept removal. \\ 
      & RECE~\cite{Gong2024ReliableAE} &  & \cmark &  &  &  & Integrates adversarial fine-tuning with closed-form editing. \\
      \midrule
      \multirow{6}{1.8cm}{\textbf{Inference-Time Intervention}}  
      & SLD~\cite{sld} &  &  &  &  & \cmark & Incorporates safety guidance to mitigate undesired concepts. \\ 
      & AMG~\cite{Chen2024TowardsMD} &  & &  &  & \cmark & Introduces three guidance strategies to prevent memorization. \\ 
      & SAFREE~\cite{safree} &  &  & \cmark &  &  & Self-validating filtering and re-attention for safe generation.\\ 
      & Content Suppression~\cite{Li2024GetWY} &  &  & \cmark &  &  & Adjusts embeddings to suppress concept generation. \\ 
      & ORES~\cite{ores} &  &  &  & \cmark & \cmark & Utilizes LLMs to filter and rewrite prompts for safer generation. \\ 
      & GuardT2I~\cite{Yang2024GuardT2IDT} &  &  &  & \cmark &  & Propose conditional LLM to detect adversarial prompts. \\ 
      \bottomrule
    \end{tabular}
  \end{adjustbox}
  \label{tab:taxonomy}
\end{table*}

% \subsection{Comparison with Related Work}  
% Concept erasure differs from machine unlearning, image editing. While machine unlearning focuses on removing specific data points from a model to comply with privacy regulations, concept erasure targets entire content categories, such as explicit or copyrighted styles, preventing their regeneration. Unlike image editing, which modifies specific attributes of an input image based on auxiliary inputs, concept erasure alters a model’s ability to generate certain concepts across all inputs.

\subsection{Three Components of Stable Diffusion }

Stable Diffusion comprises three primary components:

%~\cite{esser2021taming}
\paragraph{(1) Image Autoencoder.} The model leverages a pre-trained autoencoder to compress high-dimensional image data into a low-dimensional latent representation. The encoding network $\mathcal{E}(\cdot)$ maps an image $x$ to a latent variable $z = \mathcal{E}(x)$, and the decoding network $\mathcal{D}(\cdot)$ reconstructs the image from the latent space such that $\mathcal{D}(z) = \hat{x} \approx x$. This design ensures effective data compression while minimizing reconstruction error, preserving essential image features critical for generative tasks.



\paragraph{(2) Latent Diffusion Model.} The core generative process in SD is governed by a U-Net-based Latent Diffusion Model (LDM) that progressively refines noisy latent representations toward high-fidelity outputs. The training objective is formulated as:
\begin{equation}
    L_{\text{SD}} = \mathbb{E}_{n \sim \mathcal{N}(0,1), z, c, t} \left[
    \| n - \Phi_{\theta}(z_t, c) \|_2^2
    \right],
\end{equation}
where $c$ is the text embedding derived from the input prompt and integrated via cross-attention, $t$ denotes the diffusion timestep, $n$ is a noise vector sampled from a standard Gaussian distribution $\mathcal{N}(0,1)$, and $z_t$ is the noisy latent variable at timestep $t$. The LDM \( \Phi_{\theta} \), parameterized by \( \theta \), is trained to predict and remove noise at each step, progressively refining the latent variable along the diffusion trajectory.


\paragraph{(3) Conditional Text Encoding.} The model employs a text encoder to transform user-provided text prompts into conditioning vectors, enabling fine-grained control over the generation process. Specifically, the textual prompt $y$ is embedded as $c = \mathcal{E}_{\text{txt}}(y)$, where $\mathcal{E}_{\text{txt}}$ typically textual encoder of CLIP~\cite{CLIP}. These text embeddings are integrated through the cross-attention layers within the latent diffusion model~\cite{stable_diffusion}, allowing the textual context to dynamically influence each denoising step.

\subsection{Inference in Stable Diffusion}
Classifier-free guidance~\cite{ho2022classifier} enhances the conditionality of the image synthesis process during the inference phase of SD. The process starts with initializing latent representations $z_T$ sampled from a Gaussian distribution. The denoising trajectory is steered by classifier-free guidance, which modifies the denoising function as follows:
\begin{equation}\label{eq:classifier-free-guidance}
    \Tilde{\Phi}_{\theta}(z_t, c) = \Phi_{\theta}(z_t, \phi) + \alpha \left( \Phi_{\theta}(z_t, c) - \Phi_{\theta}(z_t, \phi) \right),
\end{equation}
where $\Phi_{\theta}(z_t, c)$ and $\Phi_{\theta}(z_t, \phi)$ represent the conditioned and unconditioned latent noises, respectively. The guidance scale $\alpha > 1$ amplifies the influence of the conditioned path, embedding the textual information into the generative process. 
Iterative refinement reduces noise through sequential calculations of $z_{t-1} = \Tilde{\Phi}_{\theta}(z_t, c)$, progressing until $t=0$. The final coherent image representation $z_0$ is transformed into the output image $\hat{x}$ by the decoder, $\hat{x} = \mathcal{D}(z_0)$. The T2I generation process can be succinctly expressed as $SD(y) = \mathcal{D}(\Tilde{\Phi}_{\theta}(z_T, \mathcal{E}_{\text{txt}}(y)))$.

