\section{Methods} \label{sec:method}

\subsection{Concept Erase}
Concept erasure in T2I models, particularly SD, involves modifying model parameters or adjusting inference procedures to selectively suppress or eliminate the generation of specific, unwanted concepts. This technique is crucial for addressing the risks associated with generating potentially harmful or copyrighted content in the model's outputs. The primary goal of concept erasure is to condition the model so that it does not produce images corresponding to undesired prompts. For instance, to erase the influence of a copyrighted artist's style, the model is adjusted such that a prompt like ``A painting in the style of [artist]'' results in outputs that bear no resemblance to that artist's work. This objective can be succinctly stated as \(SD(y_{\text{erase}}) \not\in  \{ x_{\text{erase}}\}\), where \(y_{\text{erase}}\) is the prompt that includes the concept to be erased, and \(x_{\text{erase}}\) denotes any image typically representative of that concept.



Concept erasure can be achieved through various optimization methods. And in each optimization group, methods can get classified by which components are modified to achieve the goal. This section,  therefore, categorizes existing methods by their optimization strategies and components they modify.
A comprehensive taxonomy with detailed explanations is provided in Tab.~\ref{tab:taxonomy}.
For a detailed explanation of each component and inference stage, please refer to Sec.~\ref{sec:preliminaries}.


\subsection{Fine-tuning Methods} \label{subsec:finetuning}
Fine-tuning is one of the most intuitive methods to erase undesired concepts from the T2I models. These methods iteratively optimize weights of component of Stable Diffusion (SD) to match erasing concept to its designed corresponding concept. For example, match representation of erasing concept $c_{erase}$, ``Van Gogh'' to $c_{target}$, ``Artist''. We categorize this by which component is updated to erase concept. 

% \paragraph{Fine-tuning Latent Diffusion Model.}
% Fine-tuning the latent diffusion model (LDM) is a widely used approach for concept erasure, as it allows precise modification of generative capabilities at the image synthesis level. Unlike fine-tuning the text encoder, which modifies how prompts are interpreted, LDM fine-tuning directly alters how concepts are manifested in generated images. These methods primarily target U-Net's cross-attention layers, denoising network parameters, or classifier-free guidance terms to remove or suppress undesired content.

% Several works have explored targeted concept removal through fine-tuning U-Net's attention layers. Forget-Me-Not~\cite{forgetmenot} selectively fine-tunes cross-attention projection matrices to steer attention away from erased concepts while preserving generalization. ESD~\cite{esd} fine-tunes the denoising process of diffusion models to prevent the generation of specific objects, styles, or NSFW content. Selective Amnesia~\cite{selectiveamnesia} introduces a regularization-based continual learning approach, modifying the model incrementally to unlearn specific visual patterns while maintaining general generative ability.

% Overall, fine-tuning methods in LDMs enable direct control over concept erasure at the generative process level, ensuring that removed concepts are not reconstructable through adversarial prompting. However, these methods often require computationally expensive retraining and may introduce unintended degradation in model diversity and quality. Future work could explore more efficient fine-tuning strategies, including hybrid approaches that combine fine-tuning with closed-form updates for faster adaptation.

\paragraph{Fine-tuning  Latent Diffusion Model.}  To edit SD's Latent Diffusion Models (LDM) component, 
fine-tuning-based concept erasure methods selectively update model parameters to remove undesired concepts while preserving overall generative capabilities. These approaches can get categorized further based on the specific LDM's model components they modify, as different architectural elements govern distinct aspects of the image synthesis process.

A general formulation of fine-tuning for concept erasure in LDMs is as follows: \begin{equation}
    \min_{\theta} \left\| \Phi_{\theta}(z_t, c_{\text{erase}}) - \Phi_{0}(z_t, c_{\text{target}}) \right\|^{2}_2,
\end{equation}
where \( \Phi_{0} \) represents the pretrained LDM model, and \( \Phi_{\theta} \) denotes the fine-tuned LDM with updated parameters \( \theta \). The terms \( c_{\text{erase}} \) and \( c_{\text{target}} \) correspond to the text embeddings \( \mathcal{E}_{\text{txt}}(y_{\text{erase}}) \) and \( \mathcal{E}_{\text{txt}}(y_{\text{target}}) \), respectively. The objective enforces alignment between the erased concept \( c_{\text{erase}} \) and the target concept \( c_{\text{target}} \), ensuring that the model learns to replace undesired representations in the latent space.

One class of methods targets to update the \textbf{cross-attention layers in LDM}, which determine how textual prompts influence the generated visual output. FMN~\cite{Zhang2023ForgetMeNotLT} fine-tunes cross-attention module to re-steer attention mechanisms to eliminate certain concepts while maintaining generative quality. 
% All but One~\cite{Hong2023AllBO} modifies cross-attention layers by fine-tuning classifier guidance, ensuring that erased concepts are suppressed while maintaining the model’s ability to generate diverse and high-quality images. 
 AC~\cite{Ablating_Concept} introduces an anchor-based fine-tuning strategy, aligning erased concepts with broader semantic categories to suppress their stylistic or object-based representations.

Another set of methods fine-tunes the \textbf{LDM backbone}, directly modifying the denoising process to eliminate specific concepts from the model’s latent representations. 
ESD~\cite{esd} fine-tunes the LDM to match the noise prediction of \( c_{\text{erase}} \) to that of \( c_{\text{target}} \), ensuring erased concepts remain irrecoverable. This optimization is guided by classifier-free guidance (Eq.~\eqref{eq:classifier-free-guidance}), which directs the model's learning signal. During fine-tuning, ESD modifies either cross-attention or non-attention modules to reinforce robustness against adversarial prompts.
SALUN~\cite{fan2024salun} applies saliency-guided erasing, selectively updating high-impact weights to maximize forgetting while minimizing unintended side effects. 
DT~\cite{Ni2023DegenerationTuningUS} conditions the model to generate structurally degraded outputs when prompted with erased concepts, effectively neutralizing their representation in the latent space.

Several approaches incorporate geometric constraints, continual learning, or robustness against personalization to enhance fine-tuning methods. Geom-Erasing~\cite{Liu2023ImplicitCR} removes implicit visual concepts, such as watermarks and hidden signals, by introducing geometric constraints that disrupt structured artifacts without degrading unrelated content. SA~\cite{Heng2023SelectiveAA} leverages continual learning techniques, employing regularization-based forgetting to erase targeted concepts while preserving generalization and mitigating catastrophic forgetting. Lastly, IMMA~\cite{Zheng2023IMMAIT} adopts a preventive fine-tuning approach, modifying model weights preemptively to resist unauthorized adaptation via fine-tuning techniques, thereby preventing the downstream personalization of diffusion models for unethical or restricted purposes. Additionally, SPM~\cite{Lyu2023OnedimensionalAT} introduces a lightweight, one-dimensional adapter that enables precise and transferable concept erasure across different diffusion models.



\paragraph{Fine-tuning CLIP.}
Fine-tuning latent diffusion model showed great success for concept erasure, plus their interpretability to understand erasing concepts. However, to apply these methods to the updated LDM or other structured LDM, they have to get changed or redesigned since these methods are designed only to specific LDM. As one of the strength of fine-tuning CLIP model for concept erase, the CLIP text encoder whose concepts get erased, $\mathcal{E}_{\text{txt}}$, can transfer to the other structure LDM as long as they still depends on CLIP model. To fine-tune the CLIP model, \cite{safe_clip} generates a dataset composed of quadruplets of safe and unsafe text-image pairs (ViSU dataset). 
Similarly, \cite{Liu2024LatentGA} generates a dataset composed of safe and unsafe text pairs (CoPro dataset), where unsafe prompts are synthesized using a large language model, and safe counterparts are created by removing harmful concepts while preserving context.
After generating datasets, this study finetunes CLIP based on designed loss inspired by contrastive loss. This finetuned CLIP text encoder, $\mathcal{E'_{\text{txt}}}$, leads $\mathcal{E'}_{\text{txt}}(y_{erase}) \approx \mathcal{E}_{\text{txt}}(y_{target})$. Even if $y_{erase}$ is given to the SD model, it will generate images that are not aligned with concept to erase, $SD'(y_{erase}) \approx x_{target}$, where $SD'(y) = \mathcal{D}(\Tilde{\Phi}_{\theta}(z_T, \mathcal{E'}_{\text{txt}}(y)))$.
These methods offer better adaptability than LDM fine-tuning approaches. However, this approach requires a carefully curated and extensive dataset to fine-tune the existing text encoder, unlike other concept erasure methods.


\subsection{Closed-form Model Editing Methods} \label{subsec:closed}
Fine-tuning methods are intuitive and effective for modifying SD. However, they require iterative optimization through gradient descent, making them computationally expensive and time-consuming. Moreover, fine-tuning introduces risks of overfitting and unintended degradation of the model’s capabilities, necessitating careful hyperparameter tuning. In contrast, closed-form solutions provide a direct mathematical update to model parameters without iterative training. This enables faster application of model modifications while eliminating the need for extensive hyperparameter tuning. 

A general formulation of closed-form model editing follows a least squares based optimization:
\begin{equation} \label{eq:closed_form}
\min_{W} \left\| W c_{\text{erase}} - W_0 c_{\text{target}} \right\|_2^2,
\end{equation}
where \( W \) denotes the editable parameters of the model, primarily the key and value projection matrices in the cross-attention module, while \( W_0 \) represents the pre-trained weights. Closed-form solutions directly compute the optimal update for \( W \) directly, enabling efficient modification while preserving overall model coherence. To enhance stability and prevent unintended interference, regularization terms are incorporated into Eq.~\eqref{eq:closed_form}, balancing alignment with the target concept while minimizing deviations from the original model.

Notable closed-form methods include ReFACT~\cite{Arad2023ReFACTUT}, which applies a low-rank memory update to the CLIP text encoder, ensuring persistent factual knowledge updates while minimizing interference by unrelated concepts. TIME~\cite{Orgad2023EditingIA} modifies the LDM’s cross-attention projection matrices, aligning implicit assumptions in generated images with desired attributes. Unified Concept Editing (UCE)~\cite{Gandikota2023UnifiedCE} introduces a closed-form method for simultaneous multi-concept editing in T2I models, enabling scalable erasure, moderation, and debiasing by modifying cross-attention projections while minimizing interference against unedited concepts. MACE~\cite{Lu2024MACEMC} further refines cross-attention weights by integrating adapter-based concept erasure, achieving precise removal of up to 100 concepts in a more memory-efficient manner.

Other recent works have explored additional extensions of closed-form model editing. EMCID~\cite{Xiong2024EditingMC} introduces a two-stage framework combining self-distillation and closed-form updates, scaling to over 1,000 concurrent modifications. MUNBa~\cite{Wu2024MUNBaMU} formulates concept erasure as a Nash bargaining problem, deriving an equilibrium update that balances forgetting and preservation objectives.

Overall, closed-form methods offer a computationally efficient alternative to fine-tuning by providing direct parameter updates. These methods ensure fast and stable modifications. 





% \subsection{Inference Stage Control Methods} \label{subsec:inference}
% Both of fine-tuning methods and closed-form methods shows their intuitive and effective performance to erase concepts. Fine-tuning LDM and closed-form methods shows it is possible to edit diffusion process for mapping random noise to visual latent. And CLIP finetuning achieves its flexibility to plug-and-play to the other T2I models, which are using CLIP text encoder. 

% However, previous three categories of concept erasing methods commonly requires weight updates of SD's component. Inference stage control methods are developed for erasing concepts without modification of components of SD. Instead, edit diffusion process~\eqref{eq:classifier-free-guidance} or textual embedding $c$ or sanitizing prompt $y$ using LLM, such as LLaMA\cite{Touvron2023LLaMAOA}.



\subsection{Inference-time Intervention Methods} \label{subsec:inference}
Both fine-tuning and closed-form methods demonstrate intuitive and effective performance in concept erasure. Fine-tuning LDM and closed-form parameter updates enable direct control over the generative process.
%, modifying the mapping from random noise to visual latents. 
Among them,  CLIP fine-tuning provides a flexible plug-and-play concept erasure solution for T2I models that rely on CLIP encoders.

However, these approaches require weight modifications to SD components, limiting their adaptability and deployment efficiency. In contrast, inference-stage control methods enable concept erasure without modifying SD’s parameters. These methods instead intervene at the inference stage by modifying classifier-free guidance (Eq.~\eqref{eq:classifier-free-guidance}), editing textual embeddings \( c \), or sanitizing input prompts \( y \) using large language models.


\paragraph{Modifying Classifier-Free Guidance.}  
A core approach for inference-stage concept erasure is adjusting classifier-free guidance (Eq.~\eqref{eq:classifier-free-guidance}) to steer the generative process away from undesired content. Safe Latent Diffusion~\cite{sld} modifies the classifier-free guidance signal in SD’s denoising process, redirecting latent activations to prevent the generation of unsafe concepts. Anti-Memorization Guidance~\cite{Chen2024TowardsMD} introduces despecification and dissimilarity constraints that adjust classifier-free guidance dynamically, ensuring that models do not overfit to specific training instances or regenerate memorized images. Both methods leverage guidance re-weighting strategies to suppress undesired features while maintaining high image quality.

\paragraph{Editing Textual Embeddings.}  
Rather than modifying the diffusion process, another class of inference-stage methods operates on text embeddings to enforce concept erasure. SAFREE~\cite{safree} applies subspace projection and adaptive re-attention to detect and suppress undesirable content within CLIP text embeddings before they get used for image synthesis. Similarly, Content Suppression in T2I Models~\cite{Li2024GetWY} employs soft-weighted regularization to refine textual embeddings during sampling, ensuring that forbidden concepts do not appear in generated outputs. These methods enable fine-grained, token-level control over the generation process while preserving overall model flexibility.

\paragraph{Sanitizing Input Prompts Using LLMs.}  
A third category of inference-stage control leverages Large Language Models (LLM) to preprocess prompts, ensuring that user inputs do not contain prohibited content before the diffusion process begins. ORES ~\cite{ores} employs LLM-based query rewriting to automatically sanitize user prompts, replacing restricted terms with conceptually aligned yet safe alternatives. On the other hand, GuardT2I~\cite{Yang2024GuardT2IDT} detects adversarial prompts that attempt to bypass safety mechanisms, leveraging a fine-tuned LLM to analyze and reject unsafe queries before image generation. 

By operating externally to the SD components and modifying only the diffusion process at inference, these methods remain model-agnostic and scalable across different T2I model architectures.



% \subsection{Inference Stage Control Methods} \label{subsec:inference}
% Both fine-tuning and closed-form methods demonstrate intuitive and effective performance in concept erasure. Fine-tuning latent diffusion models (LDMs) and closed-form parameter updates enable direct control over the generative process, modifying the mapping from random noise to visual latents. Additionally, CLIP fine-tuning provides a flexible plug-and-play solution for text-to-image (T2I) models that rely on CLIP encoders.

% However, all three of these approaches require weight modifications to Stable Diffusion (SD) components. In contrast, inference-stage control methods have been developed to erase concepts without altering SD’s parameters. These methods instead manipulate the diffusion process during sampling by modifying classifier-free guidance Eq.~\eqref{eq:classifier-free-guidance}, editing the textual embedding \( c \), or sanitizing the input prompt \( y \) using large language models (LLMs) such as LLaMA~\cite{Touvron2023LLaMAOA}.

% One class of inference-stage methods directly modifies the diffusion process to steer the generation away from undesired content. SLD~\cite{sld} introduces classifier-free guidance modification to suppress unsafe content in latent space, ensuring that harmful concepts are naturally omitted from generated images. Similarly, SAFREE~\cite{safree} utilizes subspace projection and adaptive re-attention to detect and suppress unsafe features in both text embeddings and denoising steps, preventing undesired generations.

% Another approach operates by modifying the input prompt or its textual embedding to erase concepts. ORES~\cite{ores} leverages LLM-based query rewriting to sanitize unsafe user prompts, replacing prohibited content with more appropriate alternatives during diffusion sampling. Similarly, GuardT2I~\cite{guardt2i} detects adversarial prompts that attempt to bypass safety mechanisms by translating latent embeddings into natural language using a fine-tuned LLM, filtering out manipulative queries before image generation.

% Beyond safety-oriented methods, inference-stage control is also applied to prevent model memorization and enforce negative constraints. AMG (Anti-Memorization Guidance)~\cite{amg} introduces despecification and dissimilarity constraints during inference to prevent models from reproducing copyrighted or sensitive training data. Content Suppression in T2I Models~\cite{content_suppression} refines textual embedding optimization by applying soft-weighted regularization to ensure that forbidden content remains absent, even under adversarial prompting.

% Inference-stage control methods provide a training-free, model-agnostic alternative to fine-tuning-based concept erasure. These techniques enable dynamic intervention during generation, offering scalability and adaptability across different T2I architectures.

