\section{Robustness} \label{sec:robustness}
Concept erasure methods aim to prevent T2I models from generating undesired concepts. For instance, when a model undergoes fine-tuning to erase a copyrighted artist’s style (e.g., Van Gogh), prompts such as ``cypresses by Van Gog'' should ideally produce outputs that bear no resemblance to the artist’s original style, as shown in Fig.~\ref{fig:overview}.

However, studies demonstrate that minor perturbations to the prompt—such as the addition of unrelated tokens or imperceptible modifications—can effectively circumvent concept erasure, allowing target T2I model to regenerate removed concepts. In some cases, even semantically meaningless inputs would  exploit the underlying representations within SD models to reconstruct erased concepts.

We categorize adversarial attacks based on whether they require access to the Latent Diffusion Model (LDM) of Stable Diffusion (SD). Following this, we also discuss existing defense strategies against such attacks.

\subsection{Adversarial Attacks} \label{subsec:adv_atk}
Adversarial attacks against T2I models manipulate prompt inputs or textual embeddings to reconstruct erased concepts. Given a concept-erased model, an adversarial prompt is optimized to elicit outputs that closely resemble those generated by an unaltered model. A general formulation of adversarial attacks against concept erasure is:
\begin{equation}
\underset{z_{\text{adv.}} \text{ or } y_{\text{adv.}}}{\mathrm{argmin}}  || \mathcal{SD'}(z_{\text{adv.}},y_{\text{adv.}}) - x_{\text{erase}} ||,    
\end{equation}
where \( \mathcal{SD'} \) denotes the concept-erased Stable Diffusion model, and \( z_{\text{adv.}} \) and \( y_{\text{adv.}} \) represent adversarial latents and prompts, respectively, that restore the erased concept within \( \mathcal{SD'} \).


\paragraph{Attacks with LDM Access}
These attacks access LDM’s latent representations, enabling adversaries to design strategies that systematically bypass concept erasure mechanisms. While such access is rare in real-world scenarios, these attacks remain essential for stress-testing erasure techniques.

One class of such attacks invert concept erasure transformations to recover removed concepts.  
Circumventing Concept Erasure~\cite{pham2024circumventing} optimizes adversarial embeddings via inversion using LDM, successfully retrieving erased concepts. Similarly, Concept Arithmetics~\cite{Petsiuk2024ConceptAF} reconstructs erased concepts by manipulating latent representations and leveraging semantic composition to synthesize forbidden attributes through linear combinations of concept embeddings.

Another category of attacks exploits prompt tuning and adversarial optimization to bypass safety-driven concept removal. P4D~\cite{p4d} systematically tunes adversarial prompts by iteratively refining textual inputs based on model feedback, demonstrating that safety filters and erasure techniques remain vulnerable to adversarial prompt engineering. Additionally, UnlearnDiffAtk~\cite{to_generate_or_not} specifically targets models trained with safety-driven unlearning by crafting prompts that reverse-engineer unlearning constraints, showing that adversarially optimized inputs can still induce the generation of prohibited content.

Although these attacks require privileged access to LDM, they provide valuable insights into the transferability of adversarial prompts across models. By assessing how an attack generalizes to different versions of SD and CLIP, these studies reveal broader vulnerabilities in concept erasure methods.

\paragraph{Attacks without LDM Access}  
Even without LDM access, adversarial methods effectively bypass concept erasures by manipulating prompt or textual embeddings, without interacting with the diffusion model’s internal denoising process.

PEZ~\cite{hard_prompt} formulates a discrete optimization problem to recover a text prompt that closely aligns with a given erased concept image, \( x_{\text{erase}} \), leveraging CLIP’s vision-language similarity for optimization without requiring access to the underlying diffusion model. 
% Similarly, MMA-Diffusion~\cite{Yang2023MMADiffusionMA} introduces adversarial attacks that operate within the CLIP text encoder embedding space or a multi-modal space, bypassing safety mechanisms by optimizing adversarial prompts and applying imperceptible perturbations to generated images. \qnote{unsure the left sentence, how can blackbox method add pertubation on the generated images?}
Similarly, MMA-Diffusion~\cite{Yang2023MMADiffusionMA}, like PEZ, employs a surrogate model for adversarial attacks, operating within CLIP’s text encoder embedding space or its multi-modal space. By optimizing adversarial prompts and introducing imperceptible perturbations, MMA-Diffusion successfully bypasses safety mechanisms, demonstrating its effectiveness in both text-to-image generation and image editing tasks.


Ring-A-Bell~\cite{Tsai2023RingABellHR} proposes a model-agnostic red-teaming framework that reconstructs erased concepts by optimizing adversarial prompts using a genetic algorithm in the text embedding space. Likewise, RIATIG~\cite{Liu2023RIATIGRA} employs a genetic optimization strategy to iteratively refine adversarial queries, enabling content moderation evasion across multiple T2I models. 
Meanwhile, UPAM~\cite{Peng2024UPAMUP} utilizes gradient-based prompt tuning combined with semantic-enhancing learning to systematically generate adversarial prompts capable of bypassing API-level safety mechanisms.

These approaches expose a fundamental vulnerability in concept erasure techniques—even without LDM access, adversarial optimization in the prompt space alone can effectively reconstruct erased content. This underscores the need for stronger prompt filtering mechanisms and adversarially resilient diffusion models to prevent circumvention through external manipulations.


% \subsection{Defensive Methods}
% For the necessity of developing robust methods against adversarial attacks~\ref{subsec:adv_atk}, concept erase methods inspired by adversarial training are proposed with following the categories in concept erasing~\ref{sec:method}. 

% RACE~\cite{race} proposes single diffusion time step adversarial attack to improve the efficiency of adversarial attack for SD. And they show that such attack can be used for adversarial finetuning for latent diffusion model. 
% Receler~\cite{receler} do something.
% AdvUlearn~\cite{advunlearn} do adversarial finetuning for CLIP-text encoder.
% RECE~\cite{rece} do adversarial finetuning on the top of close-form method.

% \subsection{Defensive Methods}
% To mitigate the vulnerabilities of concept-erased models against adversarial prompt attacks (Sec.~\ref{subsec:adv_atk}), recent research integrates adversarial training into concept erasure methods. These approaches aim to enhance robustness while maintaining the generation fidelity of diffusion models. We categorize these methods based on their modifications component (Sec.~\ref{sec:method}).

% R.A.C.E.~\cite{race} introduces a single-timestep adversarial attack strategy to efficiently identify vulnerabilities in SD. By leveraging this attack mechanism, the method performs adversarial fine-tuning to strengthen concept erasure. Receler~\cite{receler} employs a lightweight adversarial eraser embedded within the cross-attention layers of the diffusion model. It integrates concept-localized regularization to maintain generation quality while selectively erasing undesired concepts. Additionally, adversarial prompt learning is used to generate paraphrased attack prompts, improving the robustness of the model against semantic perturbations in textual prompts.

% AdvUnlearn~\cite{advunlearn} advances the robust erasing paradigm by introducing adversarial training on the CLIP text encoder rather than modifying the UNet of diffusion models. This approach enhances prompt-space robustness, making the model resistant to embedding-space adversarial attacks while preserving the original text-to-image alignment. 

% RECE~\cite{rece} extends closed-form method by incorporating adversarial finetuning on top of parameter-efficient matrix modifications in cross-attention layers. Unlike iterative fine-tuning-based methods, RECE efficiently discovers adversarial embeddings that can reconstruct erased concepts and removes them in a fully closed-form manner. In inference stage control method, SAFREE~\cite{safree} shows robust performances comparing with other difffense methods even such method does not employ adversarial training.
 
% By integrating adversarial robustness into concept erasure, these methods significantly improve the reliability of T2I safety mechanisms. However, challenges remain in optimizing the balance between robustness, generation quality, and computational efficiency, warranting further exploration in adaptive adversarial training strategies for future diffusion models.


\subsection{Defensive Methods}
% To mitigate the vulnerabilities of concept-erased models against adversarial prompt attacks (Sec.\ref{subsec:adv_atk}), recent research integrates adversarial training into concept erasure methods, aiming to enhance robustness while maintaining generation fidelity. These methods primarily target different architectural components, such as LDM, cross-attention weights, and text encoders, each playing a crucial role in enhancing robustness and mitigating adversarial vulnerabilities (Sec.~\ref{sec:method}).
To strengthen concept-erased models against adversarial prompt attacks (Sec.\ref{subsec:adv_atk}), recent research integrates adversarial training into concept erasure techniques, enhancing robustness while preserving generation fidelity. These defenses align with the categorization of concept erasure methods (Fig.\ref{fig:taxonomy}, Tab.\ref{tab:taxonomy}), targeting distinct architectural components. 
By addressing vulnerabilities across these optimization spaces, defensive methods improve the reliability of concept erasure while maintaining the expressiveness of T2I models.


R.A.C.E.\cite{RACE} employs a single-timestep adversarial attack to efficiently identify vulnerabilities in SD and leverages this attack for adversarial fine-tuning, significantly reducing attack success rates in both white-box and black-box settings. Receler\cite{Huang2023RecelerRC} integrates a lightweight robust eraser within cross-attention layers of LDM, utilizing concept-localized regularization and adversarial prompt learning to improve robustness against paraphrased attacks while preserving non-target concepts. AdvUnlearn~\cite{Zhang2024DefensiveUW} advances the robust erasing paradigm by applying adversarial training on the CLIP text encoder, enhancing prompt-space robustness while maintaining the alignment between textual prompts and image generation. RECE~\cite{Gong2024ReliableAE} extends closed-form concept erasure by incorporating adversarial fine-tuning on matrix-modified cross-attention layers, efficiently discovering and erasing adversarial embeddings in a fully closed-form manner. Additionally, in inference-stage control, SAFREE~\cite{safree} demonstrates strong robustness compared to other defense methods, despite not employing adversarial training.

By integrating adversarial robustness into concept erasure, these methods significantly improve the reliability of T2I safety mechanisms. However, challenges remain in optimizing the balance between robustness, generation quality, and computational efficiency, warranting further exploration in adaptive adversarial training strategies for future diffusion models.

% \qnote{can we make a table in the appendix summarizing the defense methods discussed in this section and get them categorized according to Figure 2 possible? }