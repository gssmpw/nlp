\section{Proofs}

\subsection{Proof of Lemma 1:}
\label{sec:p_1}

From Equations \eqref{eq:imm_rew} and \eqref{eq:bell_post}, the Bellman equation of the pre-decision state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ at time step $t = 0,1,...,T$ is equal to:
\begin{align}
V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t) = & \max_{\mathbf{c_t}} \{ \! \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\!\beta_i + (1 - \mathbbm{1}_{|\mathcal{G}_t|=1})  V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) + \mathbbm{1}_{|\mathcal{G}_t|=1}\sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}{P^i_{t}(\mathbf{c_t})(r_i - c_t^i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}}) \notag \\ 
& + \mathbbm{1}_{|\mathcal{G}_t|=1} \! \sum_{i \in \mathcal{R}\hspace{0.05em}_{t} \cup \{\emptyset\}} \! P^i_{t}(\mathbf{c_t}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\}) \}
\end{align}

\noindent We can reformulate this expression as follows: 
\begin{align}
V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t) = & \max_{c_t} \{ \mathbbm{1}_{|\mathcal{G}_t|=1} [ \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})(r_i - c_t^i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}) +  \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\}) + P^\emptyset_{t}(\mathbf{c_t}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) ] \notag \\
&  +  (1 - \mathbbm{1}_{|\mathcal{G}_t|=1}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) \} + \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\beta_i \label{eq:using_p0}
\end{align}

\noindent We express $P^\emptyset_{t}(c_t^i)$ as $ 1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})$ and therefore using Equation \eqref{eq:using_p0} we have:
\begin{align}
V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t) = & max_{c_t} \{ \mathbbm{1}_{|\mathcal{G}_t|=1} [ \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})(r_i - c_t^i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}) + \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\}) +  (1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) ] \notag \\
&   + (1 - \mathbbm{1}_{|\mathcal{G}_t|=1}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) +\sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} \beta_i \}  \\
& = max_{c_t} \{ \mathbbm{1}_{|\mathcal{G}_t|=1} \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})(r_i - c_t^i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} + V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\}) - V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'})) +  V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) + \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\! \beta_i \} \\
& = max_{c_t} \{ \mathbbm{1}_{|\mathcal{G}_t|=1} \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})(r_i - c_t^i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) ) + V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) + \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\! \beta_i \}
\end{align}

\noindent Therefore: 
\begin{align} 
V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t) = \max_{c_t} \{ \phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t}) \} + V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'})  + \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\! \beta_i \end{align}
where $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t}) = \mathbbm{1}_{|\mathcal{G}_t|=1} \mathbb{E}_{i \sim P_t(\mathbf{c_t})} [r_i - c_t^i - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) -\beta_i\mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}]$  and $ \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) = V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) - V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\})$

\subsection{Proof of Lemma 2:}
\label{sec:p_2}

Using expression \ref{eq:mnl3} we reformulate $\phi_t$ as a function of $P_t$ as follows:
\begin{align}
\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t, P_t) = \mathbbm{1}_{|\mathcal{G}_t|=1} [\sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(r_i + u_{ij} - u_0 - \mu_j \ln{P_t^i} + \mu_j \ln{P_t^{\emptyset}}  - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) -\beta_i\mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} ) ]
\end{align}

\noindent The gradient of $\phi_t$ is given by:
\begin{align} \frac{\partial \phi_t}{\partial P^i_t} = \mathbbm{1}_{|\mathcal{G}_t|=1} (r_i + u_{ij} - u_0 - \mu_j \ln{P^i_t} + \mu_j \ln{P^\emptyset_t} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \mu_j) \text{, } \forall i \in \mathcal{R}\hspace{0.05em}_{t}, \end{align}
\begin{align} \frac{\partial \phi_t}{\partial P^\emptyset_t} = \frac{\mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} P^i_t}{P^\emptyset_t} \end{align}

\noindent Additionally:
\begin{align} \frac{\partial^2 \phi_t}{\partial P^i_t \partial P^i_t} = -\frac{\mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j}{P^i_t} \text{, } \forall i \in \mathcal{R}\hspace{0.05em}_{t}, \end{align} 
\begin{align} \frac{\partial^2 \phi_t}{\partial P^i_t \partial P^j_t} = 0 \text{, } \forall i,j \in \mathcal{R}\hspace{0.05em}_{t}: i \neq j,\end{align}
\begin{align} \frac{\partial^2 \phi_t}{\partial P^\emptyset_t \partial P^\emptyset_t} = -\frac{\mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} P^i_t}{(P^\emptyset_t)^2}, \end{align} 
\begin{align} \frac{\partial^2 \phi_t}{\partial P^\emptyset_t \partial P^i_t} = \frac{\mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j}{P^\emptyset_t} \text{, } \forall i \in \mathcal{R}\hspace{0.05em}_{t} \end{align}

\noindent Therefore the Hessian of $-\phi_t$ is given by:
\begin{align} \mathbb{H}_{-\phi_t} = \mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j \begin{bmatrix}
\frac{1}{P^1_t} & 0 & \dots & 0 & -\frac{1}{P^\emptyset_t} \\
0 & \frac{1}{P^2_t} & \dots & 0 & -\frac{1}{P^\emptyset_t} \\
\vdots &  & \ddots &  & \vdots \\
0 & 0 & \dots & \frac{1}{P^{|\mathcal{R}\hspace{0.05em}_{t}|}_t} & -\frac{1}{P^\emptyset_t} \\
-\frac{1}{P^\emptyset_t} & -\frac{1}{P^\emptyset_t} & \dots & -\frac{1}{P^\emptyset_t} & \frac{\sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} P^i_t}{(P^\emptyset_t)^2} 
\end{bmatrix}
\end{align}
\noindent Therefore:
\begin{align} y\mathbb{H}_{-\phi_t}y^T = \mathbbm{1}_{|\mathcal{G}_t|=1} \mu_j ( \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}\frac{y_i^2}{P^i_t} + \frac{y_{|\mathcal{R}\hspace{0.05em}_{t}|+1}}{(P^\emptyset_t)^2}(\sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} P^i_t)) \geq 0 \text{ } \forall y \in \mathbb{R}^{|\mathcal{R}\hspace{0.05em}_{t}|+1} \end{align} 
As a result, we conclude that $\phi_t$ is a concave function of $P_t$ for $P_t \in (0,1)^{|\mathcal{R}\hspace{0.05em}_{t}|+1}$. Since $ \Psi = \{ (P^1,...,P^{|\mathcal{R}\hspace{0.05em}_{t}|},P^\emptyset) \in (0,1)^{|\mathcal{R}\hspace{0.05em}_{t}|+1} : \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} P^i_t = 1 \}$ is a convex subset of $(0,1)^{|\mathcal{R}\hspace{0.05em}_{t}|+1}$, therefore $\phi_t $ is concave in $\Psi$.

\subsection{Proof of Lemma 3:}
\label{sec:p_3}
To find the optimal $P^*_t \in \Psi$ that maximizes $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,P_t)$ we solve the first order condition. To do so we replace $P^\emptyset_{t}(c_t^i)$ by $1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})$ and take the gradient of $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,P_t)$:
\begin{align}
\frac{\partial \phi_t}{\partial P^i_t} = & \mathbbm{1}_{|\mathcal{G}_t|=1} [r_i + u_{ij} - u_0 - \mu_j \ln{P^i_t} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) + \mu_j \ln{(1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^i)} - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \mu_j + \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^i( -\frac{\mu_j}{1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^i}) ] \\
& = \mathbbm{1}_{|\mathcal{G}_t|=1} [r_i + u_{ij} - u_0 - \mu_j \ln{P^i_t} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) + \mu_j \ln{(1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^i)}  - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} -\frac{\mu_j}{1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^i} ]
\end{align}

\noindent Therefore:
\begin{align}
\frac{\partial \phi_t}{\partial P^i_t} = 0  \Leftrightarrow & P^{i*}_t = \frac{\mu_j}{m_t} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}  - m_t)\}, \forall i \in \mathcal{R}\hspace{0.05em}_{t} \label{eq:pi} 
\end{align}

\noindent where: 
\begin{align}
m_t = \frac{\mu_j}{1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^{i*}} = \frac{\mu_j}{P_t^{\emptyset*}} \label{eq:mt1}
\end{align}
% \Leftrightarrow P_t^{\emptyset*} = \frac{m_t}{\mu_j} 
\noindent We have:
\begin{align}
& \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P_t^{i*} =  1 - P_t^{\emptyset*} = 1 - \frac{\mu_j}{m_t} \Leftrightarrow \\ 
& 1 - \frac{\mu_j}{m_t} = \frac{\mu_j}{m_t} \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - m_t)\} \Leftrightarrow \\
&(\frac{m_t}{\mu_j} - 1) \exp\{ \frac{m_t}{\mu_j} - 1\} = \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - \mu_j)\} \Leftrightarrow \\
& m_t =  \mu_j \left( W_0(\sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - \mu_j)\}) + 1 \right) \label{eq:mt2}
\end{align}

\noindent where $W_0$ is the Lambert's W function for k = 0.

\noindent From Equation \ref{eq:mnl3} using Equations \ref{eq:pi} and \ref{eq:mt1} we result in:
\begin{align}
& c^{i*}_t = -u_{ij} + u_0 - \mu_j \ln( \frac{\mu_j}{m_t} ) + \mu_j \ln ( \frac{\mu_j}{m_t} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - m_t)\} )  \Rightarrow \\
& c^{i*}_t = r_i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - m_t
\end{align}

\noindent We have therefore derived an analytical solution. 

\section{Experimental Design}


\subsection{Neural Network Training Details}
\label{sec:nn_app}

In our implementation, we incorporate a target network parameterized by $\hat{\theta}$ for the post-decision value function approximation. This mitigates oscillating or divergent updates that can occur with frequent updates. The target network's parameters ($\hat{\theta}$) are periodically synchronized with those of the main network ($\theta$) at intervals defined by \(K\) (see Table \ref{tab:training-details}), leading to smoother value function updates and improved convergence during training.

\begin{table}[h]
    \centering
    \caption{Neural Network Training Details}
    \begin{tabular}{ll}
        \hline
        \textbf{Parameter} & \textbf{Value} \\
        \hline
        Optimizer (clip value) & Adam (0.5) \\
        Loss function & Huber \\
        Network size & [16,16] \\
        Attention embedding size & 32 \\
        Activation function & Swish \\
        Discount factor & 0.95 \\
        Batch size & 512 \\
        Initial learning rate & $1 \times 10^{-5}$ \\
        Learning rate decay rate & $1 \times 10^{-2}$ \\
        Learning rate decay steps & 10000 \\
        Epochs & 30 \\
        C ($\theta$ update frequency) & $80 \times 50$ \\
        K ($\hat{\theta}$ update frequency) & $80 \times 50 \times 5$ \\
        Initial $\delta$ (e-greedy parameter) & 10 \\
        $\delta$ decay per time step & $1 \times 10^{-4}$ \\
        \hline
    \end{tabular}
    \label{tab:training-details}
\end{table}

\subsection{Formula-based compensation (FP)}
\label{sec:fbp_app}

\begin{table}[h]
  \centering
  \caption{Weights for each attribute in the formula-based compensation strategy}
  \begin{tabular}{lc}
    \hline
    \textbf{Attribute} & \textbf{Weights} \\
    \hline
    Reward       & \( \{ 0, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95 \} \) \\
    Distance     & \( \{ 0, 5, 10, 15, 20 \} \) \\
    Penalty      & \( \{ -0.1, -0.05, 0, 0.05, 0.1 \} \) \\
    Exp. Boost   & \( \{ 0, 0.1, 0.2, 0.3 \} \) \\
    \hline
  \end{tabular}
  \label{table:fbp_val}
\end{table}

\subsection{Full information solution}
\label{sec:fi_app}

Consider $\mathcal{R}$ and $\mathcal{G}$ to be the sets of all requests and gig workers that arrive within the full time horizon. Let $r_i,\beta_i, t_i^{\mathrm{arr}}, t_i^{\mathrm{exp}}$ be the reward, penalty, arrival time and expiration time of request $i \in \mathcal{R}$, $t_j^{\mathrm{arr}}$ be the arrival time of gig worker $ j \in \mathcal{G}$. Lastly, let $u_{ij} + \hat{e}_j$ be the attractiveness of request $i$ for gig worker $j$, where $\hat{e}_j$ is the realization of the gig worker's utility noise variable $e_j$. The full information solution is given by solving the following MILP: 
\begin{align}
    \text{maximize} \quad & \sum_{i \in \mathcal{R}} \sum_{j \in \mathcal{G}} \left( r_i - \beta_i - u_{ij} -  \hat{e}_j \right) x_{ij}  \\
    \text{subject to} \quad & \sum_{j \in \mathcal{G}} x_{ij} \leq 1 \quad \forall i \in \mathcal{R}  \\ % Each request assigned to at most one gig worker
    & \sum_{i \in \mathcal{R}} x_{ij} \leq 1 \quad \forall j \in \mathcal{G} \\ % Each gig worker gets assigned at most one request
    & t_j^{\mathrm{arr}} x_{ij} \leq t_i^{\mathrm{exp}} \quad \forall i \in \mathcal{R}, \forall j \in \mathcal{G} \\ % \text{Assigned gig worker arrives before request expires}
    & t_i^{\mathrm{arr}} x_{ij} \leq t_j^{\mathrm{arr}} \quad \forall i \in \mathcal{R}, \forall j \in \mathcal{G} \\ % \text{Assigned gig worker arrives after request arrives}
    & x_{ij} \in \{0, 1\} \quad \forall i \in \mathcal{R}, \forall j \in \mathcal{G}
\end{align}

\subsection{Mean Bias Error calculation and interpretation}
\label{sec:mbe}

For a set of requests $I$, we calculate the \gls{acr:mbe} between the predicted utilities of requests $ \{ \hat{u}_i \}_{i \in I} $ and the true  of requests $ \{ u_i \}_{i \in I} $ as:
\begin{align}
\text{MBE}(I) = \frac{1}{|I|} \sum_{i \in I} (u_i - \hat{u}_i)
\end{align}
A positive $\text{MBE}$ indicates that, on average, the estimated utility is lower than the true utility of the gig workers, suggesting an underestimation of the requests' attractiveness. Conversely, a negative $\text{MBE}$ suggests an overestimation of the attractiveness.