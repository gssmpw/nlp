\section{Methodology}
\label{sec:method}

Solving the \gls{acr:mdp} as defined in Section \ref{sec:prob_form} presents several key challenges from a dynamic decision-making perspective. A primary challenge is the need for a model that describes the environment's transition dynamics, which is often complex and challenging to characterize. Another significant challenge is the intractable state space arising from the stochastic arrival of requests and gig workers, which renders exhaustive computation of optimal policies infeasible. Therefore, effective generalization across states is essential, as explicitly enumerating all possible states is unrealistic in many real-world scenarios.

Our proposed method offers several solutions to address these challenges. First, we utilize the \gls{acr:mnl} model to describe gig workers' decision-making process, capturing essential transition dynamics without requiring a fully known environment model. To manage the complexity of large state spaces, we leverage a post-decision state formulation, which eases computing optimal policies from a tractability perspective. This formulation also facilitates solving the inner optimization problem and deriving an exact form for optimal prices based on the post-decision state values. Our method is designed to ensure scalability, making it suitable for real-world applications where exhaustive enumeration of all possible states is infeasible. To this end, we incorporate value function approximators to generalize effectively across states, which enables us to estimate value functions without exhaustive state enumeration.  

This section details our approach. Section \ref{sec:bellman} introduces the Bellman equation for the \gls{acr:mdp} as defined in Section~\ref{sec:prob_form}. Section \ref{sec:mnl} discusses the \gls{acr:mnl} model for capturing essential transition dynamics, addresses the state space intractability by deriving the post-decision state formulation and presents an analytical solution to the optimization problem with optimal prices based on post-decision state values. Section \ref{sec:statistical_models} presents value function approximators and an algorithm for learning appropriate parameterizations. Section \ref{sec:training} details the statistical model used for gig worker utility estimation and presents the training procedure for the full algorithm. Finally, Section \ref{sec:meth_as} discusses methodological assumptions. 

\subsection{Bellman equation}
\label{sec:bellman}
In the following, we derive the Bellman equation for the \gls{acr:mdp} as defined in Section \ref{sec:prob_form}. To do so, we first elaborate on optimal value functions in general to elucidate the connection between pre- and post-decision states, which then allows us to derive the pre- and post-decision state value function accordingly. 

\noindent \textit{Optimal value functions:} The optimal value function of a pre-decision state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ or a post-decision state $\mathcal{R}\hspace{0.05em}_t^{p}$, denoted as $V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ and $V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_{t}^{p})$, describes the maximum expected reward achievable from that state onward by following the optimal policy. In this context, the transition from a post-decision state to a successor pre-decision state exclusively involves the stochastic realization of the arrival processes of on-demand requests and gig workers. Therefore, a straightforward relationship between the two value functions is the following: 
\begin{align}
V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{p}) = \mathbb{E}_{\mathcal{R}^{\mathrm{new}},\mathcal{G}^{\mathrm{new}}}[V_{t+1}(\mathcal{R}\hspace{0.05em}_t^{p} \cup \mathcal{R}^{\mathrm{new}},\mathcal{G}^{\mathrm{new}})] \label{eq:value_function}
\end{align}
where the expectation over $\mathcal{R}^{\mathrm{new}},\mathcal{G}^{\mathrm{new}}$ indicates the dynamics of new request and new gig worker arrivals in the system correspondingly (see Figure~\ref{fig:state_trans}). Essentially, Equation \ref{eq:value_function} states that the value of a post-decision state $\mathcal{R}\hspace{0.05em}_{t}$ is equal to the expected value over all possible successor pre-decision states. 
%The expected immediate reward at time step $t$ when following the optimal policy. 

\noindent \textit{Bellman equation using the pre-decision state value function:}
The classic definition of the Bellman equation of the pre-decision state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ at time step $t = 0,1,...,T$ reads: 
\begin{align}
V_t(S\hspace{-0.1em}_t) & = \max_{\mathbf{c_t}} \{  R_t(\mathbf{c_t}) + \mathbb{E}_{S' \sim P(\cdot|S\hspace{-0.1em}_t,\mathbf{c_t})}[V_{t+1}(S')] \}
\end{align}
where $R_t(\mathbf{c_t})$ accounts for the expected immediate reward of being in state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ and following action $\mathbf{c_t}$, while the expectation $\mathbb{E}_{S' \sim P(\cdot|s,\mathbf{c_t})}[V_{t+1}(S')]$ accounts for the expected future reward that can be obtained from successor pre-decision states $S'$. In this standard definition, the value of each pre-decision state is decomposed using the value of the successor pre-decision state. Notably, the transitions from one pre-decision state to the next include two different sources of stochasticity. The first source of stochasticity results from the utility of the gig workers while the second source results from the stochastic nature of the request and gig worker arrivals. 
This formulation of the Bellman equation is highly intractable due to the complexities introduced by both sources of stochasticity. The primary challenge arises from the second source of stochasticity, as the arrival of new requests and gig workers leads to an explosion in the state transition space, making it impossible to model or track transitions to the next state accurately. In contrast, the first source of stochasticity, which depends solely on the gig workers' choice model, is less complex. To address this issue, we can alternatively formulate the Bellman equation using the post-decision state value function.

\noindent \textit{Bellman equation using the post-decision state value function:} Let us denote as $\mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}$ the set of requests in~$\mathcal{R}\hspace{0.05em}_{t}$ that expire, and let $\mathcal{R}\hspace{0.05em}_t^{'}$ correspond to the set of requests in $\mathcal{R}\hspace{0.05em}_t$ that do not expire after time step $t$, i.e., $\mathcal{R}\hspace{0.05em}_t^{'} = \mathcal{R}\hspace{0.05em}_{t} \setminus \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}$. Furthermore, from here on, we assume for ease of notation that $\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{\emptyset\}$ is equivalent to $\mathcal{R}\hspace{0.05em}_t^{'} $. Then, the Bellman equation of the pre-decision state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ at time step $t = 0,1,...,T$ is given by: 
\begin{align}
V_t(S\hspace{-0.1em}_t) & = \max_{\mathbf{c_t}} \big\{  R_t(\mathbf{c_t}) + \underbrace{(1 - \mathbbm{1}_{|\mathcal{G}_t|=1})  V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'})\vphantom{\sum_{i \in \mathcal{R}\hspace{0.05em}_{t} \cup \{\emptyset\}}}}_{\text{(I)}}  + \underbrace{\mathbbm{1}_{|\mathcal{G}_t|=1} \! \sum_{i \in \mathcal{R}\hspace{0.05em}_{t} \cup \{\emptyset\}} \! P^i_{t}(\mathbf{c_t}) V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\})}_{\text{(II)}}  \big\}
\label{eq:bell_post}
\end{align}

\noindent where the term $R_t(\mathbf{c_t})$ accounts for the expected immediate reward of being in state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ and following action $\mathbf{c_t}$, while the remaining terms account for the expected future reward obtained from the successor post-decision state. Specifically, term (I) accounts for the expected future reward in the case where the pre-decision state does not contain any gig workers. Conversely, term (II) represents the expected future reward in the case where the pre-decision state contains a gig worker. In this case, the probabilities of request selection made by the gig worker determine the transition to post-decision states. Consequently, with a probability of \(P^i_{t}\), we transition to the post-decision state \(\mathcal{R}_{t}^{'} \setminus \{i\}\). 

Bellman Equation \ref{eq:bell_post} makes an explicit distinction between the two different sources of stochasticity in the \gls{acr:mdp}. The source of uncertainty which results from the gig workers' stochastic utility is reflected by the transition to all possible post-decision states, while the second source of uncertainty is embedded within the value function of the post-decision states. By separating the transition dynamics in this manner, the complexity of the Bellman equation is reduced, making the problem more tractable by directly depending only on the transition into the post-decision states.

\subsection{Representing gig worker's utility using the \gls{acr:mnl} model and deriving an analytical solution}
\label{sec:mnl}

To effectively utilize the tractability offered by the post-decision state formulation of the Bellman equation, it is essential to model the probability distribution for transitioning to each post-decision state. This necessitates a model that accurately describes gig worker behavior, and the \gls{acr:mnl} model is one of the most commonly used approaches in the literature on discrete choice behavior for both research and practical applications. Its popularity stems from the fact that it offers a closed-form expression for acceptance probabilities and its balance of simplicity and performance, which renders it both versatile and effective. Accordingly, we adopt the \gls{acr:mnl} model as the foundation for our analysis.

\noindent\textit{Multinomial Logit models:} In the \gls{acr:mnl} model, the random variables $\{e_{ij}\}_{i \in \mathcal{R}\hspace{0.05em}_{t}}$ of the utility function (see Equation \ref{eq:util}) are independent and identically distributed (i.i.d), following a Gumbel distribution. We utilize a special case of the MNL model which assumes a linear relationship between the offered compensation and Gumbel-distributed random variables with zero mean. Formally, this variation considers that the utility function of a gig worker $j$ for a request $i$ is:
\begin{equation} U_{ij} = u_{ij} + c_{i} + e_i \end{equation} where $u_{ij}$ is a value indicating the attractiveness of request $i$ to gig worker $j$, and whose value is determined by observable characteristics of the request, $c_i$ is the offered compensation to the gig worker for accepting the request, and $e_i$ are i.i.d. zero mean Gumbel variables with variance equal to $(\mu_j\pi)^2/6$ for some $\mu_j> 0$. Under this \gls{acr:mnl} model, when the request state is $\mathcal{R}\hspace{0.05em}_{t}$, the probability of a gig worker $j$ in time step $t$ accepting request $i$ for offered compensations $\mathbf{c_t} = (c^i_t)_{i \in \mathcal{R}\hspace{0.05em}_{t}}$ equals:
\begin{align} P^i_{t}(\mathbf{c_t}) = \frac{\exp((u_{ij} + c_t^i)/\mu_j)}{\sum_{l \in \mathcal{R}\hspace{0.05em}_{t}}(\exp((u_{lj} + c_t^j)/\mu_j) + \exp(u_0/\mu_j)},\label{eq:mnl1}\end{align} 
while the probability of gig worker $j$ not accepting any requests reads:
\begin{align} P^\emptyset_{t}(\mathbf{c_t}) = \frac{ \exp(u_0/\mu_j)}{\sum_{l \in \mathcal{R}\hspace{0.05em}_{t}}(\exp((u_{lj} + c_t^j)/\mu_j) + \exp(u_0/\mu_j)},\label{eq:mnl2}\end{align} 
where $u_0 \geq 0$ is a constant indicating the attractiveness of the all-reject alternative. 

In the simplest case, all gig workers share the same utility function. Formally, $u_{ij} = u_i$ for all requests $i \in \mathcal{R}\hspace{0.05em}_{t}$ and for all gig workers $j \in \mathcal{G}_t$, and $\mu_j = \mu$ for all gig workers $j \in \mathcal{G}_t$. However, to introduce variability in preferences among the population while maintaining model simplicity, we can assume that the population is divided into $D$ gig worker groups. This formulation corresponds to a Mixed Multinomial Logit (Mixed MNL) model, where within each group, individuals exhibit similar preferences. Formally, for each group $d \in [1,\dots,D]$, the utility function is $u_{ij} = u_i^d$  for all requests $i \in \mathcal{R}\hspace{0.05em}_{t}$ and all gig workers $j \in \mathcal{G}^d_t$, and $\mu_j = \mu_d$ for all gig workers $j \in \mathcal{G}^d_t$.

To solve Bellman equation \ref{eq:bell_post}, we adapt and extend the theory from \cite{dong2009dynamic}, which studies dynamic pricing in the context of inventory control of substitute products, to our problem setting. Specifically, we demonstrate that there exists an alternative formulation of the optimization problem defined by the post-decision state Bellman equation. This alternative formulation uses acceptance probabilities as decision variables and is concave. We then derive the optimal solution by solving the first-order condition.

To this end, we first reformulate the value function by applying Equation \eqref{eq:imm_rew} and representing $P^\emptyset_{t}(c_t^i)$ as $ 1 - \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}}P^i_{t}(\mathbf{c_t})$ and derive the following lemma.
\begin{lemma}\label{lem:dynamic_programming}
The value of the pre-decision state $S\hspace{-0.1em}_t = (\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t)$ is equal to:
$$ V_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t) = \max_{c_t} \{ \phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t}) \} + V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'})  + \sum_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}\! \beta_i $$
where 
$\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t}) = \mathbbm{1}_{|\mathcal{G}_t|=1} \mathbb{E}_{i \sim P_t(\mathbf{c_t})} [r_i - c_t^i - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) -\beta_i\mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}]$ and
$ \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) = V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'}) - V^{\mathrm{p}}_{t}(\mathcal{R}\hspace{0.05em}_t^{'} \setminus \{i\}).$

\end{lemma}
The proof of Lemma 1 can be found in Appendix \ref{sec:p_1}. 

As a result of this reformulation, we observe that finding the optimal prices $\mathbf{c_t}$ reduces to optimizing $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t})$. The function $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t})$ concerns only scenarios where a gig worker is available, as no decision is required otherwise. It accounts for the probability of each request $i \in \mathcal{R}\hspace{0.05em}_{t}$ being accepted given a compensation $c_t^i$, the corresponding reward and penalty, and an additional term $\Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'})$. The term $\Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'})$ reflects the difference in expected reward when request $i \in \mathcal{R}\hspace{0.05em}_{t}$ is accepted immediately versus when it remains unfulfilled, commonly referred to as the opportunity cost of request $i \in \mathcal{R}\hspace{0.05em}_{t}$. As demonstrated by \cite{hanson1996optimizing}, $\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,\mathbf{c_t})$ is not guaranteed to be concave in $\mathbf{c_t}$. 

We can address such optimization problems by expressing the compensation decision $c^t_i$ in terms of the acceptance probabilities. Using the \gls{acr:mnl} model to describe gig worker behavior, we combine Equations \eqref{eq:mnl1} and \eqref{eq:mnl2} to establish a bijection between the compensation \(c^t_i\) and the acceptance probabilities \(P_t^i\). Specifically, the compensation for each request can be expressed as a function of acceptance probabilities as follows:
\begin{align}
\frac{P_t^i(\mathbf{c_t})}{P_t^{\emptyset}(\mathbf{c_t})} = \exp((u_{ij} + c_t^i - u_0)/\mu_j) \Leftrightarrow c_t^i = -u_{ij} + u_0 + \mu_j \ln{P_t^i} - \mu_j \ln{P_t^{\emptyset}}.
\label{eq:mnl3}
\end{align}
Consequently, using the above expression we reformulate $\phi_t$ as a function of $P_t$ and establish the following result.
\begin{lemma}\label{lem:concavity}
$\phi_t(\mathcal{R}\hspace{0.05em}_{t},\mathcal{G}_t,P_t) = \mathbbm{1}_{|\mathcal{G}_t|=1}\mathbb{E}_{i \sim P_t}[r_i + u_{ij} - u_0 - \mu_j \ln{P_t^i} + \mu_j \ln{P_t^{\emptyset}}  - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) -\beta_i\mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}}]$ is concave in~$P_t$.
\end{lemma}
The proof of Lemma 2 can be found in Appendix \ref{sec:p_2}. 

By proving the concavity of $\phi_t$ as a function of $P_t$ we can solve the first order condition of $\phi_t$ and derive an optimal solution as a function of the acceptance probabilities $P_t$.
\begin{lemma}\label{lem:exact}
The optimal price $c^{i*}_t$ is given by:
$c^{i*}_t = r_i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} 
 - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - m_t$ where $m_t$ results from solving: 
 $(\frac{m_t}{\mu_j} - 1) \exp\{ \frac{m_t}{\mu_j} - 1\} = \sum_{i \in \mathcal{R}\hspace{0.05em}_{t}} \exp\{ \frac{1}{\mu_j} (r_i + u_{ij} - u_0 - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'}) - \mu_j)\}$
\end{lemma}
The proof of Lemma \ref{lem:exact} can be found in Appendix \ref{sec:p_3}. 

Lemma \ref{lem:exact} states that the optimal price for a request~$i$ is equal to the reward of that request, reduced by the following terms: its penalty in the case that it expires within the current time step, the expected value discrepancy $\Delta^i_{V_{t}}(\mathcal{R}\hspace{0.05em}_t^{'})$ and the term $m_t$, which is a state-specific factor that adjusts pricing based on the overall system state rather than on individual requests. In essence, $m_t$ captures system-wide influences on the optimal pricing decision at any given time.

While we define feasible compensations as non-negative in our problem setting, the optimal compensation described in Lemma \ref{lem:exact} may result in negative values under certain conditions, e.g., due to specific worker-request dynamics or model assumptions. In practice, when implementing the optimal compensations, we set any negative values of $\mathbf{c_t} = (c^i_t)_{i \in \mathcal{R}\hspace{0.05em}_{t}}$ to zero to ensure all compensations remain feasible. Since gig workers would not accept requests with either negative or zero compensation, this adjustment aligns with realistic worker behavior and does not compromise the model's practical effectiveness. This adjustment allows for practical feasibility and ensures the model’s applicability in real-world scenarios, where negative compensations would be infeasible, while retaining most of the theoretical solution’s structure and insights.

\subsection{Value function approximation}
\label{sec:statistical_models}
Although our optimization problem has an analytical solution, derived in Section \ref{sec:mnl}, its dependence on the post-decision value function makes precise computation infeasible due to the vast and intractable state space. To overcome this, we employ statistical models to approximate the post-decision value function.

\noindent\underline{Statistical models for the post-decision value function:} A statistical model for the post-decision value function is defined as a function $\hat{V}$ parameterized by $\theta$ which receives as input a post-decision state $\mathcal{R}^{post}\hspace{0.05em}$ and predicts a value $\hat{v}$ for that state: $\hat{V}^\theta: \mathcal{R}^{post}\hspace{0.05em} \mapsto \hat{v} \in \mathbbm{R}$. The primary challenge in designing an effective statistical model to approximate the post-decision state value lies in the fact that the request state is represented as a set, containing a variable number of requests. Consequently, an effective statistical model for the post-decision value function approximation must be able to handle this dynamic structure. To account for such a variability, we utilize a neural network architecture that incorporates an attention mechanism.

\begin{figure}[b]
\centering
\resizebox{\textwidth}{!}{ % Use full width of the page
\begin{tikzpicture}[node distance=2cm]

    % Input node on the left, outside the enclosing box
    \node [block] (input) {$i \in \mathcal{R}^{post}\hspace{0.05em}$};

    % Horizontal structure of nodes inside the enclosing box
    \node [block, right=of input] (mlp_emb) {MLP$^{emb}$};
    \node [block, right=of mlp_emb] (mlp_w) {MLP$^W$};
    \node [block, right=of mlp_w] (cell) {Context \\[-0.3cm] $C = \sum_{i \in \mathcal{R}^{post}\hspace{0.05em}} \beta_{i} \cdot e_{i}$};

    % Enlarged enclosing box around MLP^emb, MLP^W, and Context nodes, including the extra arrow
    \node[fit=(mlp_emb) (mlp_w) (cell), draw, inner sep=10pt, rounded corners,  yshift=-0.18cm] (enclosing_box) {};

    % Arrows
    \draw [arrow] (input) -- (mlp_emb);
    \draw [arrow] (mlp_emb) -- node[above] {$e_i$} (mlp_w);
    \draw [arrow] (mlp_w) -- node[above] {$\beta_i$} (cell);
    
    % Direct arrow from MLP^emb to Context with centered label
    \draw [arrow] (mlp_emb) -- ++(0,-0.8cm) -| node[pos=0.12, above] {$e_i$} (cell);
\end{tikzpicture}
}
\caption{\textnormal{Attention mechanism}}
\label{fig:tikz_diagram}
\end{figure}

\noindent \textit{Attention mechanism:} The attention mechanism (see Figure \ref{fig:tikz_diagram}) calculates an embedding vector $e_i$ for each request $i \in \mathcal{R}^{post}\hspace{0.05em}$ using a multi-layer perceptron (MLP). Subsequently, $e_i$ is processed by a second MLP which performs the following calculation: $\beta_{i} = \sigma(w \cdot \tanh(W \cdot e_{i}))$, where $\sigma$ is the sigmoid function, $w$ is a trainable vector of weights, and $W$ is a trainable matrix of weights. Lastly, it calculates the context vector $C$ as $C = \sum_{i \in \mathcal{R}^{post}\hspace{0.05em}} \beta_{i} \cdot e_{i}$. The context vector is then given as an input to the rest of the neural network architecture. 

\noindent \textit{Neural network architecture:} The neural network for the post-decision value function approximation consists of an attention mechanism that comprises a feedforward layer with 32 units and a Swish activation \citep{ramachandran2017searching}, which provides the embedding vector $e_i$ for each request $i \in \mathcal{R}^{post}\hspace{0.05em}$. It then calculates the context vector using weights $w \in \mathbbm{R}^{64}$ and $W \in \mathbbm{R}^{64 \times 32}$. This context vector, along with other relevant state features (e.g., the number of requests, the most urgent deadline), constitutes the input to the subsequent layers of the network. These layers include two feedforward layers, each with 16 units and a Swish activation. Additional information about neural network training and hyperparameters can be found in Appendix \ref{sec:nn_app}. 

\noindent \underline{Algorithm for training the value function approximator:} To learn the parameters of the statistical model, we follow an approximate value iteration for the post-decision state value function, adapted from \cite{powell2021reinforcement}, to accommodate the continuous action space of our problem setting and the probabilistic transitions to post-decision states. Algorithm~1 shows a pseudocode of our learning procedure: initially, the algorithm receives estimates of the \gls{acr:mnl} parameters $\hat{\mathit{u}}$ and $\hat{\mu}$ (L.1). Then we randomly initialize a parameterization $\theta$ for the statistical model of the post-decision value function (L.2), and repeat the following procedure: first, we gather experiences by interacting with a simulated or real-world environment (L.3). During this time, we select the compensation by considering the estimated optimal price $\hat{c}_{t}$ using the latest estimate of the value function approximation, and we perturb the estimated optimal value using a Gaussian perturbation (L.4) to ensure exploration. After a sufficient amount of experiences is gathered, we update the parameterization $\theta$ (L.5-L.8). For updating the post-decision value function approximation around the observed post-decision state $\mathcal{R}\hspace{0.05em}_{t-1}^{post}$ of time step $t-1$, we use the (estimated) optimal value $v_t^*$ of the observed pre-decision state $S\hspace{-0.1em}_t$ of the following time step $t$.

\begin{algorithm}[b!]
\caption{Approximate value iteration using the post-decision value function}
\begin{algorithmic}[1]  % The [1] here enables line numbering
\REQUIRE MNL model utilities $\hat{\mathit{u}}$, and Gumbel parameter $\hat{\mu}$
\STATE Initialize a value function parameterization $\theta$ for $\hat{V}^\theta$ 
\FOR{episode = 1,\ldots,M}
    \STATE Step 1. Gather experiences using the e-greedy policy:
    \STATE \hspace{\algorithmicindent} $\begin{aligned}[t]
            \mathbf{\hat{c}}_{t} = (\max(0,r_i - \beta_i \mathbbm{1}_{i \in \mathcal{R}\hspace{0.05em}_t^{\mathrm{exp}}} - \Delta_{\hat{V}}^i(\mathcal{R}\hspace{0.05em}_t^{'}) - m_t + \varepsilon_{i}))_{i \in \mathcal{R}\hspace{0.05em}_{t}} 
            \end{aligned}$
            where $\varepsilon_{i} \sim \mathcal{N}(0,\delta)$  $\forall i \in \mathcal{R}\hspace{0.05em}_{t}$
    \STATE Step 2. Update $\theta$ using the target:
    \STATE \hspace{\algorithmicindent} $\begin{aligned}[t]
            \hat{V}^{\theta}(\mathcal{R}\hspace{0.05em}_{t-1}^{post}) \leftarrow v_t^*
            \end{aligned}$
    \STATE where $\mathcal{R}\hspace{0.05em}_{t-1}^{post}$ is the \underline{observed} post-decision state at time step $t-1$ and $v_t^*$ is the estimated optimal value of the successor pre-decision state $S\hspace{-0.1em}_t$:     
    \STATE \hspace{\algorithmicindent} $\begin{aligned}
            v_t^* = \max_{\mathbf{c_t}} \{ R_t(\mathbf{c_t}) +  \gamma \mathbbm{1}_{|\mathcal{G}_t|=1} \sum_{i \in \mathcal{R}\hspace{0.05em}_t\cup\{\emptyset\}}  P_t^i(\mathbf{c_t}) \cdot \hat{V}^{\theta}_t(\mathcal{R}\hspace{0.05em}_t^{'}\backslash\{i\}) \} + (1 - \mathbbm{1}_{|\mathcal{G}_t|=1}) \cdot \hat{V}^{\theta}_t(\mathcal{R}\hspace{0.05em}_t^{'}).
            \end{aligned}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Training procedure}
\label{sec:training}
\noindent \underline{Statistical model of the gig worker's utility:} In practice, the true \gls{acr:mnl} model parameters for the utilities $u_{ij}$ and the Gumbel parameter $\mu$ are not known. To estimate the parameters of the \gls{acr:mnl} model for each gig worker group, we use observed accept/reject gig worker decisions based on the characteristics of the on-demand request $\mathbf{x}_i$ and offered compensation $c_i$. This data comes either from interactions with the environment under any reasonable policy or from pre-existing historical data. For each gig worker group $d~\in~[1,\dots,D]$ we train a logistic regression model using the log-odds function: $(\mathbf{\hat{w}}_d^T\mathbf{x}_i + c_i)/\beta_d$ where $\mathbf{\hat{w}}_d$ and $\beta_d$ are trainable parameters. Finally, we estimate the utility $u_{ij}$ of a gig worker $j$ for request $i$ using the mapping $\hat{\mathit{u}}_j : \mathbf{x}_i \mapsto \mathbf{\hat{w}}_d^T \mathbf{x}_i$, so that $\hat{u}_{ij} = \hat{\mathit{u}}_j(\mathbf{x}_i)$, and $\beta_d$ by $\hat{\mu}_d = \beta_d$.

\noindent \underline{Training pipeline:} Our training procedure for each scenario involves the following steps: Initially, we gather experiences by interacting with training scenarios. The compensation policy employed sets the compensation for each request $i$  by randomly selecting a value between 40\%-85\% of the request's reward. From these experiences, we train the \gls{acr:mnl} estimator as previously defined, using stochastic gradient descent for optimization. Since the gig worker always chooses the offer that maximizes their utility, the data skews toward higher-compensation offers, especially when the sampling policy is non-optimal and tends to over-offer. Therefore, we use \(L2\) regularization on the weight $\beta_d$ to prevent the weight $\beta_d$ from increasing excessively. We then proceed to train the post-decision value function approximation as outlined in Algorithm 1. In order to mitigate the effect of the network weight initialization, we repeat this process with 5 different random seeds, resulting in 5 distinct models. We select the final model based on the one that demonstrates the best performance on the validation scenarios.

\subsection{Discussion}
\label{sec:meth_as}
Our proposed algorithmic paradigm adopts a model-free approach in many aspects, while it relies on the \gls{acr:mnl} model to capture gig worker behavior. As a result, it requires some knowledge of gig worker groups or at least observable characteristics of gig workers. Below, we discuss key considerations and limitations of our algorithm.

\noindent \textit{Modeling gig worker utility:} Modeling all stochastic aspects of the environment is impractical due to the complexity of real-world scenarios. Instead, selectively modeling key elements provides a practical balance between model-free and model-based approaches. For example, while the variability in gig worker arrivals and requests (e.g., weekday vs. weekend patterns) is too complex to be modeled precisely, focusing on gig worker decision-making is reasonable. In our approach, we use the \gls{acr:mnl} model to represent gig worker decisions. This model is widely used in research and practice, offering flexibility to capture diverse preferences across gig worker groups. However, it assumes a specific mathematical structure (e.g., independence of irrelevant alternatives), which may not fully reflect real-world decision-making. Despite this, its practicality and generalizability justify its use in our algorithm.

\noindent \textit{Knowledge of gig worker groups:} When the gig worker population displays heterogeneous preferences, i.e., when more that one gig worker group exists, the performance of our algorithm relies on having knowledge of the different groups. However, the availability of such information varies across platforms, influencing their ability to identify groups with similar preferences. Platforms seeking to identify potential groups can refer to existing studies, such as \cite{marcucci2017connected}, \cite{bathke2023occasional}, and \cite{miller2017crowdsourced}, which offer valuable insights into gig worker decision-making and subgroup behavior.
