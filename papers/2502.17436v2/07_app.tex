The appendix is organized as follows. We first provide a proof of \cref{the:pvgivenxt} (the velocity distribution given $x_t$) in \cref{sec:proofpvgivenxt}. We then provide a proof of \cref{clm:velocitydistribution} (velocity distribution for the special case of a mixture of Gaussians target distribution) in \cref{sec:proof_claim_1}. Afterwards we provide the proof of \cref{the:1} (correctness of the marginals) in \cref{sec:proof_thm_1}. 
Then we discuss density estimation for HRF models in \cref{sec:density}.
Next we provide more details regarding the  hierarchical rectified flow formulation in \cref{sec:HRFformulationdetails}. 
Subsequently, we discuss experimental and implementation details in \cref{sec:exp_details}. Finally, we provide additional ablation studies in \cref{sec:ablation} and additional experimental results in \cref{sec:app:additional}.

\section{Proof of \cref{the:pvgivenxt}}
\label{sec:proofpvgivenxt}
\textbf{Proof of~\cref{the:pvgivenxt}:}
The velocity at location $x_t$ and time $t$ is $v = x_1 - x_0 = \frac{x_1 - x_t}{1-t}$. The last equality holds because $(1-t) x_0 + tx_1 = x_t$. Recall that for a random variable $Y = \alpha X + \beta$ with $\alpha, \beta \in \mathbb{R}$ and $\alpha \neq 0$, we have $p_Y(y) = \frac{1}{\alpha} p_X\left( \frac{y - \beta}{\alpha}\right)$. Since the random variable $V$ is a linear transform of the random variable $X_1$, we get
\begin{equation}
\label{eq:pvgivenxt}
\pi_1(v; x_t, t) = p_{V | X_t} (v | x_t) = (1-t) p_{X_1 | X_t}\left( (1-t)v + x_t | x_t \right).
\end{equation}
Therefore, we need to evaluate $p_{X_1 | X_t}$. Using Bayes' formula, 
\begin{equation}
\label{eq:bayes}
p_{X_1 | X_t} (x_1 | x_t) = \frac{p_{X_t | X_1}(x_t | x_1) p_{X_1}(x_1)}{p_{X_t}(x_t)},
\end{equation}
assuming that $p_{X_t}(x_t) \neq 0$. It is undefined if $p_{X_t}(x_t) \neq 0$. 
Now it remains to find $p_{X_t | X_1}$ and we have
\begin{align}
\label{eq:pxtgivenx1}
p_{X_t | X_1} (x_t | x_1) & = p_{(1-t)X_0 + t x_1}(x_t) =\frac{1}{1-t}p_{X_0}\left( \frac{x_t -t x_1}{1-t}\right). 
\end{align}
Plugging~\cref{eq:bayes} and~\cref{eq:pxtgivenx1} into~\cref{eq:pvgivenxt} and using $x_1 = x_t + (1-t) v$, we have 
\begin{align}
\pi_1(v; x_t, t)  = p_{V|X_t} (v | x_t) 
& = \frac{p_{X_0}(x_t - tv) p_{X_1}(x_t + (1-t)v)}{p_{X_t} (x_t)} \nonumber \\
& = \frac{\rho_0(x_t - tv) \rho_1(x_t + (1-t)v)}{\rho_t (x_t)}.
\end{align}

Since the random variable $X_t$ is a linear combination of two independent random variables $X_0$ and $X_1$ as defined in~\cref{eq:lin_int}, we have 
\begin{align}
\label{eq:pxt}
 \rho_t (x_t) & = p_{(1-t)X_0} (x_t) * p_{tX_1} (x_t)   = \int p_{(1-t)X_0} (z) p_{tX_1} (x_t-z)  dz \nonumber \\
 & = \int \frac{1}{1-t} p_{X_0} \left( \frac{z}{1-t} \right) \frac{1}{t}p_{X_1} \left(\frac{x_t-z}{t} \right) d z \nonumber \\
 & = \frac{1}{t(1-t)} \rho_0 \left( \frac{x_t}{1-t} \right) * \rho_1 \left( \frac{x_t}{t}\right), \quad \text{for }t \in (0, 1).
\end{align}
At $t = 0$, $\rho_t = \rho_0$ since $x_t = x_0$. At $t = 1$, $\rho_t = \rho_1$, since $x_t = x_1$. $ \pi_1(v; x_t, t) $ is undefined if $\rho_t(x_t) = 0$.
This completes the proof.
\hfill$\blacksquare$


\section{Proof of \cref{clm:velocitydistribution}}
\label{sec:proof_claim_1}
\citet{bromiley2003products} summarizes a few useful properties for the product and convolution of Gaussian distributions. We state the relevant results here for our proof of \cref{clm:velocitydistribution}.  
\begin{lemma}
\label{lem:GLT}
For the linear transform of a Gaussian random variable, we have $$\mathcal{N}(ax + b; \mu, \sigma^2) = \frac{1}{a} \mathcal{N}\left(x; \frac{\mu - b}{a} , \frac{\sigma^2}{a^2} \right).$$
\end{lemma}

\begin{lemma}
\label{lem:Gconv}
 For the convolution of two Gaussian distributions, we have $$\mathcal{N}(x; \mu_1, \sigma_1^2) * \mathcal{N}(x; \mu_2, \sigma_2^2) = \mathcal{N}(x; \mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2).$$
\end{lemma}

\begin{lemma}
\label{lem:Gprod}
 For the product of two Gaussian distributions, we have $$ \mathcal{N}(x; \mu_1, \sigma_1^2) \cdot \mathcal{N}(x; \mu_2, \sigma_2^2) = \frac{1}{\sqrt{2\pi(\sigma_1^2 + \sigma_2^2)}} \exp\left[- \frac{(\mu_1 - \mu_2)^2}{\sigma_1^2 + \sigma_2^2} \right] \mathcal{N}\left (x; \frac{\mu_1 \sigma_2^2 + \mu_2 \sigma_1^2}{\sigma_1^2 + \sigma_2^2} , \frac{\sigma_1^2 \sigma_2^2}{\sigma_1^2 + \sigma_2^2} \right). $$
\end{lemma}
The proofs of the Lemmas are detailed by \citet{bromiley2003products}.


\textbf{Proof of \cref{clm:velocitydistribution}:}
We first compute the density of $X_t$ using~\cref{the:pvgivenxt} with the specific $\rho_0$ and $\rho_1$:  %
\begin{align}
\label{eq:pxtgaussian}
 \rho_t(x_t) %
 & = \frac{1}{t(1-t)} \rho_0 \left( \frac{x_t}{1-t} \right)* \rho_1\left( \frac{x_t}{t}\right)   \nonumber \\
 & =  \frac{1}{t(1-t)} \mathcal{N}\left(\frac{x_t}{1-t}; 0, 1  \right) *\left(\sum_{k = 1}^K w_k \mathcal{N}\left(\frac{x_t}{t}; \mu_k, \sigma_k^2  \right) \right). 
\end{align}
By applying \cref{lem:GLT} and \cref{lem:Gconv} to \cref{eq:pxtgaussian}, we get 
\begin{align}
\label{eq:pxt2}
 \rho_t (x_t)  & = \mathcal{N}\left(x_t; 0, (1-t)^2  \right) * \left(\sum_{k = 1}^K w_k \mathcal{N}\left(x_t; t\mu_k, t^2\sigma_k^2  \right) \right)  \nonumber \\ 
 &= \sum_{k = 1}^K w_k  \left(   \mathcal{N}\left(x_t; 0, (1-t)^2  \right)  * \mathcal{N}\left(x_t; t\mu_k, t^2\sigma_k^2  \right) \right) \nonumber \\
 & = \sum_{k = 1}^K w_k \mathcal{N} \left(x_t; t\mu_k, \tilde{\sigma}_{k, t}^2 \right).
\end{align}

Using \cref{the:pvgivenxt} and~\cref{eq:pxt2}, %
we have
\begin{align}
    \label{eq:pvgivenxt2}
    p_{V | X_t} (v | x_t) %
    & = \frac{\mathcal{N}\left( x_t - tv; 0, 1 \right) \left( \sum_{k=1}^K w_k \mathcal{N}\left(x_t + (1-t)v; \mu_k, \sigma_k^2 \right) \right) }{ \sum_{k' = 1}^K w_{k'} \mathcal{N} \left(x_t; t\mu_{k'}, \tilde{\sigma}_{k', t}^2 \right)} \nonumber \\
    & \stackrel{a}{=} \frac{\mathcal{N}\left( v; \frac{x_t}{t}, \frac{1}{t^2} \right) \left( \sum_{k=1}^K w_k \mathcal{N}\left(v; \frac{\mu_k - x_t}{1-t}, \frac{\sigma_k^2}{(1-t)^2} \right) \right) }{ \sum_{k' = 1}^K w_{k'} \mathcal{N} \left(x_t; t\mu_{k'}, \tilde{\sigma}_{k', t}^2 \right)} \nonumber \\
    & =  \frac{\sum_{k=1}^K w_k\mathcal{N}\left( v; \frac{x_t}{t}, \frac{1}{t^2} \right) \mathcal{N}\left(v; \frac{\mu_k - x_t}{1-t}, \frac{\sigma_k^2}{(1-t)^2} \right) }{ t(1-t)\sum_{k' = 1}^K w_{k'} \mathcal{N} \left(x_t; t\mu_{k'}, \tilde{\sigma}_{k', t}^2 \right)} \nonumber \\
    & \stackrel{b}{=} \frac{\sum_{k = 1}^K w_k \frac{t(1-t)}{\sqrt{2\pi((1-t)^2 + t^2 \sigma_k^2)}} \exp\left( -\frac{(x_t - t\mu_k)^2}{(1-t)^2 + t^2 \sigma_k^2}\right) \mathcal{N}\left(v; \frac{(1-t)(\mu_k - x_t) + t\sigma_k^2 x_t }{\tilde{\sigma}_{k, t}^2}, \frac{\sigma_k^2}{\tilde{\sigma}_{k, t}^2} \right)}{t(1-t)\sum_{k' = 1}^K w_{k'} \mathcal{N} \left(x_t; t\mu_{k'}, \tilde{\sigma}_{k', t}^2 \right)} \nonumber \\
    & \stackrel{c}{=} \frac{\sum_{k = 1}^K w_k \mathcal{N}\left(x_t; t\mu_{k}, \tilde{\sigma}_{k, t}^2 \right) \mathcal{N}\left(v; \frac{(1-t)(\mu_k - x_t) + t\sigma_k^2 x_t }{\tilde{\sigma}_{k, t}^2}, \frac{\sigma_k^2}{\tilde{\sigma}_{k, t}^2} \right)}{\sum_{k' = 1}^K w_{k'} \mathcal{N} \left(x_t; t\mu_{k'}, \tilde{\sigma}_{k', t}^2 \right)} \nonumber \\
    & = \sum_{k = 1}^K \tilde{w}_{k, t} \mathcal{N}\left(v; \frac{(1-t)(\mu_k - x_t) + t\sigma_k^2 x_t }{\tilde{\sigma}_{k, t}^2}, \frac{\sigma_k^2}{\tilde{\sigma}_{k, t}^2} \right).
\end{align}
The equality $a$ holds by applying \cref{lem:GLT}. %
The equality $b$ is derived by applying \cref{lem:Gprod} to the product of two Gaussian distributions. Simplifying the expressions, we get equality $c$ and the final expression of $p_{V|X_t}(v|x_t)$. This completes the proof. 
\hfill$\blacksquare$



\input{07_app_proofthm2}


\input{03_density}

\section{Hierarchical Rectified Flow Formulation Details}
\label{sec:HRFformulationdetails}
In this section, we show how \cref{eq:opt} can be derived from \cref{eq:opt:hierarchy}.
For convenience we re-state \cref{eq:opt:hierarchy}:
\begin{equation}
\label{eq:opt:hierarchy_copy}
\inf_f \mathbb{E}_{{\bm x}_0\sim{\bm \rho}_0,x_1\sim\rho_1,{\bm t}\sim U[0,1]^D}\left[\left\|\left(x_1 - {\bm 1}_D^T{\bm x}_0\right) - f\left({\bm x}_{\bm t},{\bm t}\right)\right\|^2_2\right].
\end{equation}

For $D=2$, we note that $x_1 - {\bm 1}_D^T{\bm x}_0$ is equivalent to $x_1-x_0^{(1)}-x_0^{(2)}$. Letting $x_0 = x_0^{(1)}$ and $v_0 = x_0^{(2)}$, we obtain  $x_1 - {\bm 1}_D^T{\bm x}_0 = x_1-x_0-v_0$. 

Further note that we obtain the time variables $\bm t=[t^{(1)}, t^{(2)}]=[t, \tau]\sim U[0,1]^2$, since $t$ and $\tau$ are drawn independently  from a uniform distribution $U[0,1]$. Also, ${\bm x}_0=[x_0^{(1)},x_0^{(2)}] = [x_0, v_0]\sim {\bm \rho}_0$, where $x_0$ and $v_0$ are drawn independently from standard Gaussian source distributions $\rho_0$ and $\pi_0$ because ${\bm \rho}_0$ is a $D$-dimensional standard Gaussian. 

Based on the general expression $x^{(d)}_{\bm t} = (1-t^{(d)})x_0^{(d)} + t^{(d)}(x_1 - \sum_{k=1}^{d-1} x_0^{(k)})$ and the previous results, we have 
$x_t = x^{(1)}_{\bm t} = (1-t^{(1)})x_0^{(1)} + t^{(1)}x_1=(1-t)x_0 + tx_1$ 
and $v_{\tau}=x^{(2)}_{\bm t} = (1-t^{(2)})x_0^{(2)} + t^{(2)}(x_1-x_0^{(1)}) = (1-\tau)v_0 + \tau v_1$. This is identical to the computation of $x_t$ and $v_{\tau}$. Combining all of these results while renaming the function from $f$ to $a$, we arrive at
\begin{equation}
\label{eq:opt_copy}
\inf_a \mathbb{E}_{x_0\sim\rho_0,x_1\sim \mathcal{D},t\sim U[0,1],v_0\sim\pi_0,\tau\sim U[0,1]}\left[\|(x_1 - x_0 - v_0) - a(x_t,t,v_\tau, \tau)\|^2_2\right].
\end{equation}
This program is identical to the one stated in \cref{eq:opt}. 




\section{Experimental and Implementation Details}
\label{sec:exp_details}
\subsection{Low Dimensional Experiments}
For the 1D and 2D experiments, we use the same neural network. It consists of two parts. The first part processes the space and time information separately using sinusoidal positional embedding and linear layers. In the second part, the processed information is concatenated and passed through a series of linear layers to produce the final output. Compared to the baseline, our HRF model with depth $D$ takes $D$ times more space and time information as input. Therefore, the first part of the network has $D$ times more embedding and linear layers to handle spatial and temporal information from different depths. However, by adjusting the dimensions of the hidden layers, we reduced the network size to just one-fourth of the baseline, while achieving superior performance. For each dataset in the low-dimensional experiments, we use 100,000 data points for training and another 100,000 data points for evaluation. For each set of experiments, we train five different models using five random seeds. During the evaluation, we performed a total of 125 experiments and averaged the results to ensure the fairness and validity of our findings. 

\subsection{High Dimensional Experiments}
In the high-dimensional image experiments, we used the U-Net architecture described by \citet{LipmanICLR2023} for the baseline model. %
To handle extra inputs $v_\tau$ and $\tau$, we designed new U-Net-based network architectures for MNIST, CIFAR-10, and ImageNet-32 data. 

\noindent\textbf{MNIST.} For MNIST, we use a single U-Net and modify the ResNet block. Similar to the neural network used in our low-dimensional experiments, each ResNet block has two parts. In the first part, we handle two sets of space-time information, i.e., $(x_t,t)$ and $(v_\tau,\tau)$, separately with 2 distinct pathways: convolutional layers for spatial data and linear layers for time embeddings. In the second part, all the spatial data and time embeddings are added together and passed through a series of linear layers to capture the space-time dependencies. For a fair evaluation, we adjusted the number of channels such that the model sizes approximately match (ours: 1.07M parameters vs.\ baseline: 1.08M parameters). We note that the HRF formulation significantly outperforms the baseline. The results were shown in \cref{fig:img_data}. More results are provided in \cref{sec:add_results}.

\noindent \textbf{CIFAR-10.} For CIFAR-10, we use two U-Nets with the same number of layers but different channel sizes. We use a larger U-Net with channel size 128 to process the velocity $v_{\tau}$ and time $\tau$. We use another smaller U-Net with channel size 32 to process the location $x_t$ and time $t$. We merge the output of each ResNet block of the smaller U-Net with the corresponding ResNet block of the bigger U-Net. %
The size of this new U-Net structure is $1.25\times$ larger than the baseline (44.81M parameters in our model and 35.75M parameters in the baseline). Our model achieves a slightly better generation quality (see~\cref{fig:img_data} in~\cref{sec:exp} and~\cref{tab:performance} in~\cref{sec:add_results}).

\noindent \textbf{ImageNet-32.} For ImageNet-32, we adopt the same architectural setup as for CIFAR-10 but modify the attention resolution to ``16,8'' instead of just ``16'' to better capture the increased multimodality of the ImageNet-32 dataset. Our U-Net model has a parameter size of 46.21M, compared to 37.06M for the baseline. It demonstrates slightly improved generation quality (see \cref{fig:img_data} in \cref{sec:exp} and \cref{tab:performance} in \cref{sec:add_results}). 

For training, we adopt the procedure and parameter settings from \citet{tong2023improving} and \citet{LipmanICLR2023}. We use the Adam optimizer with $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$, with no weight decay. For MNIST, the U-Net has channel multipliers $ [1,2,2]$, while for CIFAR-10 and ImageNet-32, the channel multipliers are $ [1,2,2,2]$. The learning rate is set to $1\times 10^{-4}$ with a batch size 128 for MNIST and CIFAR-10. For ImageNet-32, we increase the batch size to 512 and adjust the learning rate to $2\times 10^{-4}$. We train all models on a single NVIDIA RTX A6000 GPU. For MNIST, we train both the baseline and our model for 150,000 steps while we use 400,000 steps for CIFAR-10. 



\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{3pt}
\begin{tabular}{ccccccc}
\toprule
\textbf{Total NFEs} & 
\textbf{Sampling Steps} & 
$\mathcal{N}\to2\mathcal{N}$ & 
$\mathcal{N}\to5\mathcal{N}$ & 
$2\mathcal{N}\to2\mathcal{N}$ &
$\mathcal{N}\to6\mathcal{N} (2D)$ &
$8\mathcal{N}\to$ moon \\
& & 1-WD & 1-WD & 1-WD & 2-SWD & 2-SWD \\
\midrule
100 & $(1,100)$ & \textbf{0.020} & 0.031 & 0.045 & 0.070 & 0.172 \\
100 & $(2,50)$ & 0.025 & \underline{0.019} & \underline{0.011} & \textbf{0.037} & \textbf{0.107} \\
100 & $(5,20)$ & \underline{0.022} & 0.020 & \textbf{0.010} & \underline{0.045} & \underline{0.119} \\
100 & $(10,10)$ & 0.025 & \underline{0.019} & 0.017 & 0.053 & 0.163 \\
100 & $(20,5)$ & 0.026 & \textbf{0.017} & 0.030 & 0.062 & 0.201 \\
100 & $(50,2)$ & 0.047 & 0.030 & 0.075 & 0.081 & 0.222 \\
100 & $(100,1)$ & 0.032 & 0.030 & 0.050 & 0.085 & 0.177 \\
\bottomrule
\end{tabular}
}
\caption{HRF2 performance for low dimensional experiments under the same $\text{NFE}=100$ budget with different choices of sampling steps. Sampling steps $(J,L)$ indicates that we use $J$ steps to integrate $x$ and $L$ steps to integrate $v$. 1-WD refers to the 1-Wasserstein distance and 2-SWD refers to the Sliced 2-Wasserstein distance. \textbf{Bold} for the best. \underline{Underline} for the runner-up. }
\label{tab:ablation_nfe}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{fig/fig9_ablation_D/loss_convergence_v1.pdf}
    \caption{Training losses of HRF with different depths on 1D data, a standard Gaussian source distribution to a mixture of 2 Gaussians target distribution. We observe training to remain stable.}
    \label{fig:loss_convergence}
\end{figure}


\section{Ablation Studies}
\label{sec:ablation}
\subsection{Ablation Study for NFE}
The sampling process of HRF with depth $D$ involves integrating $D$ ODEs using Euler's method. The total number of neural function evaluations (NFE) is defined as $\text{NFE}=\prod_d N^{(d)}$ where $N^{(d)}$ is the number of integration steps at depth $d$. Note, for a constant NFE budget, varying the $N^{(d)}$ values can lead to different results. Therefore, we conduct an ablation study to understand suitable choices for $N^{(d)}$. 

As shown in \cref{fig:emp_v_dist}, increasing the number of integration steps improves the sampling of the velocity distribution. %
However, beyond a certain threshold, the benefit of additional steps does not justify the increased computational cost. \cref{tab:ablation_nfe} further illustrates that, for a fixed NFE budget, a compelling strategy is to allocate a sufficient number of steps to accurately sample $v$ for a precise velocity distribution while using fewer steps to integrate over $x$. 

\subsection{Ablation Study for Depth}
\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{Training} 
& \multicolumn{3}{c}{\textbf{1D data}} 
& \multicolumn{3}{c}{\textbf{2D data}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7}
& \textbf{RF (0.30M)} & \textbf{HRF2 (0.07M)} & \textbf{HRF3 (0.67M)} & \textbf{RF (0.33M)} & \textbf{HRF2 (0.08M)} & \textbf{HRF3 (0.71M)} \\
\midrule
Time ($\times 10^{-2}$ s/iter) & 1.292 & 0.736 & 2.202 & 1.503 & 0.737 & 2.252 \\
Memory (MB) & 2011 & 1763 & 2417 & 2091 & 1803 & 2605 \\
Param.\ Counts & 297,089 & 74,497 & 673,793 & 329,986 & 76,674 & 711,042\\
\bottomrule
\end{tabular}
}
\caption{Computational requirements for training on synthetic datasets. All models in this table are trained for 15000 iterations with a batch size of 51200. }
\label{tab:training_syn}
\end{table}

\begin{table}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{Inference Time (s)} 
& \multicolumn{3}{c}{\textbf{1D data}} 
& \multicolumn{3}{c}{\textbf{2D data}} \\
\cmidrule(r){2-4} \cmidrule(r){5-7}
\textbf{Total NFEs} & \textbf{RF (0.30M)} & \textbf{HRF2 (0.07M)} & \textbf{HRF3 (0.67M)} & \textbf{RF (0.33M)} & \textbf{HRF2 (0.08M)} & \textbf{HRF3 (0.71M)} \\
\midrule
5 & 0.030 ± 0.014 & 0.014 ± 0.005 & 0.037 ± 0.030 & 0.035 ± 0.017 & 0.017 ± 0.006 & 0.041 ± 0.034 \\
10 & 0.069 ± 0.020 & 0.033 ± 0.000 & 0.128 ± 0.001 & 0.078 ± 0.025 & 0.039 ± 0.000 & 0.145 ± 0.001 \\
50 & 0.372 ± 0.024 & 0.164 ± 0.000 & 0.642 ± 0.001 & 0.440 ± 0.001 & 0.193 ± 0.000 & 0.727 ± 0.001 \\
100 & 0.755 ± 0.001 & 0.327 ± 0.000 & 1.291 ± 0.002 & 0.884 ± 0.001 & 0.385 ± 0.000 & 1.455 ± 0.003 \\
\bottomrule
\end{tabular}
}
\caption{Inference time comparison for synthetic data using a varying NFE budget. For HRF2, we used sampling step combinations: $(1,5), (2,5), (5,10), (10,10)$. For HRF3, we used sampling step combinations: $(1,1,5), (1,2,5), (1,5,10), (2,5,10)$. For all experiments, we set our batch size to 100,000. } 
\label{tab:infer_time_syn}
\end{table}

Our HRF framework can be extended to an arbitrary depth $D$. Here, we compare the training loss convergence of HRF with depths ranging from 1 to 5, where HRF1 corresponds to the baseline RF. As illustrated by the training losses shown in \cref{fig:loss_convergence}, training stability remains consistent across different depths, with higher-depth HRFs demonstrating comparable stability to lower-depth models. Importantly, note that \cref{fig:loss_convergence} mainly serves to compare convergence behavior and not loss magnitudes as those magnitudes reflect different objects, i.e., velocity for a depth of 1, acceleration for a depth of 2, etc. Moreover, the deep net structure for the functional field of directions $f$ depends on the depth, which makes a comparison more challenging. 
\cref{tab:training_syn} and \cref{tab:infer_time_syn} indicate that increasing the depth results in manageable model size, training time, and inference time. These trade-offs are justified by the significant performance improvements observed in \cref{fig:1d_data} and \cref{fig:2d_data}. See \cref{sec:additional1d} for details regarding the training data. 


\section{Additional Experimental Results}
\label{sec:app:additional}
\subsection{Additional 1D Results}
\label{sec:additional1d}
The results for experiments used in \cref{fig:teaser} and \cref{fig:emp_v_dist} are shown in \cref{fig:more_1d_data}. 
\label{sec:exp_more_results}
\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cccc}
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/1to2_dist.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/new_1to2_WD_NFE.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/1to2_traj_400.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/1to2_traj_20_20.pdf}\\
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/2to2_dist.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig7_more_1d/new_2to2_WD_NFE.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig1_traj/2to2_traj_400.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig1_traj/2to2_traj_20_20.pdf}\\
    (a) Data distribution & (b) Metrics & (c) RF trajectories & (d) HRF trajectories
    \end{tabular}
    \caption{More experiments on 1D data: top row shows results for a standard Gaussian source distribution and a mixture of 2 Gaussians target distribution; bottom row shows results for a mixture of 2 Gaussians source distribution and the same mixture of 2 Gaussians target distribution. }
    \label{fig:more_1d_data}
\end{figure}

\input{07_app_batchOT}

\input{07_app_results}
