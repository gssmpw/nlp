\section{Towards Hierarchical Rectified Flow}
\label{sec:method}

In the following \cref{sec:method:casestudy}, we first discuss the multimodality of the velocity distribution and provide a case study with Gaussian mixtures. The case study is designed to provide insights regarding the velocity distribution. We then discuss in \cref{sec:method:approach} a simple way to capture the multimodal velocity distribution and how to use it to sample from the data distribution. Then, we show in \cref{sec:method:theory} that the proposed procedure indeed faithfully captures the data marginals. Finally, we discuss in \cref{sec:method:extension} an extension towards a hierarchical rectified flow formulation.


\subsection{Velocity distribution and case study with Gaussian Mixtures}
\label{sec:method:casestudy}
The linear interpolation in~\cref{eq:lin_int} defines a time-differentiable stochastic process with the random velocity field $v(x_t, t) = x_1 - x_0$, where  $x_0 \sim \rho_0$ and $x_1 \sim \rho_1$. %
Note, the source and target distributions are independent.  The following theorem characterizes the distribution of the velocity at a specific space time location $(x_t,t)$:
\begin{theorem}
\label{the:pvgivenxt}
The velocity distribution $\pi_1(v; x_t, t)$ at the space time location $(x_t, t)$ induced by the linear interpolation in~\cref{eq:lin_int} is
\begin{align} 
\label{eq:vgxt}
\pi_1(v; x_t, t) = p_{V|X_t} (v | x_t)
& = \frac{\rho_0(x_t - tv) \rho_1(x_t + (1-t)v)}{\rho_t (x_t)},
\end{align}
for $\rho_t(x_t) \neq 0$ with (`*' denotes convolution)
\begin{equation}
\rho_t(x_t) =  
\begin{cases}
\rho_0 (x_0) &\text{for }t = 0, \\
\frac{1}{t(1-t)} \rho_0 \left( \frac{x_t}{1-t} \right) * \rho_1\left( \frac{x_t}{t}\right) & \text{for }t \in (0, 1),  \\
\rho_1 (x_1) &\text{for }t = 1.
\end{cases}
\end{equation}
The distribution $\pi_1(v; x_t, t)$ is undefined if $\rho_t(x_t) =0$. 
\end{theorem}
The proof of~\cref{the:pvgivenxt} is deferred to~\cref{sec:proofpvgivenxt}.
Note that since $\rho_1$ is typically multimodal, the resulting $\pi_1(v; x_t, t)$ is also multimodal. At $t = 0$, we have $\pi_1(v; x_t, t) = \rho_1(x_t + v)$, which corresponds to the data distribution shifted by $-x_t$. At $t = 1$, we have $ \pi_1(v; x_t, t) = \rho_0(x_t - v)$, which corresponds to the flipped source distribution shifted by $x_t$. 

To illustrate the multimodality of the velocity distribution, we consider a simple 1-dimensional example. The source distribution is a standard Gaussian (zero mean, unit variance). The target distribution is a Gaussian mixture. The following corollary provides the ``ground-truth'' velocity distribution at any location $x_t$.

\begin{corollary}
\label{clm:velocitydistribution}
Assume $\rho_0 = \mathcal{N}(x;0,1)$ and $\rho_1 = \sum_{k=1}^K w_k \mathcal{N}(x;\mu_k,\sigma_k^2)$, then
\begin{equation}
\label{eq:true_v_pdf}
 \pi_1(v; x_t, t) = \sum_{k=1}^K \tilde{w}_{k, t}\mathcal{N}\left(v; \frac{(1-t)(\mu_k - x_t) + t\sigma_k^2 x_t }{\tilde{\sigma}_{k, t}^2}, \frac{\sigma_k^2}{\tilde{\sigma}_{k, t}^2} \right),
\end{equation}
where $\tilde{\sigma}_{k, t}^2 = (1-t)^2 + t^2 \sigma_k^2$ and $\tilde{w}_{k, t} = \frac{w_k \mathcal{N}(x_t; t\mu_k,\tilde{\sigma}^2_{k, t})}{\sum_{k' = 1}^K w_{k'} \mathcal{N}(x_t; t\mu_{k'},\tilde{\sigma}^2_{k', t}) } $.
\end{corollary}

We defer the proof of \cref{clm:velocitydistribution} to  \cref{sec:proof_claim_1}. To empirically check the fit of \cref{clm:velocitydistribution}, in \cref{fig:velocitydistributions}, we compare the derived velocity distribution with empirical estimates at different locations $(x_t,t)$. We observe a great fit and very clearly multimodal distributions.

\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cccc}
    \includegraphics[width=0.25\linewidth]{fig/fig2_v_analy/vdist_samples_-1_0.0.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig2_v_analy/vdist_samples_0_0.4.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig2_v_analy/vdist_samples_0.5_0.6.pdf}&
    \includegraphics[width=0.25\linewidth]{fig/fig2_v_analy/vdist_samples_1_1.0.pdf}\\
    (a) $(x_t,t)=(-1.0, 0.0)$ & (b) $(x_t,t)=(0.0, 0.4)$ & (c) $(x_t,t)=(0.5, 0.6)$ & (d) $(x_t,t)=(1.0, 1.0)$
    \end{tabular}
    \caption{We verify the derived velocity distribution by comparing its probability density function (blue) to the empirical sample histogram (orange) at different times $t$ and locations $x_t$. }
    \label{fig:velocitydistributions}
\end{figure}

It is very much worthwhile to study these distributions a bit more. In particular, we observe that the velocity distribution at time $t=1$ collapses to a single Gaussian, more specifically a shifted source distribution. This can be seen from \cref{fig:velocitydistributions}(d). Further, at time $t=0$, we observe the velocity distribution to be identical to a shifted data distribution. This can be seen from \cref{fig:velocitydistributions} (a). %

This is valuable to know as it suggests that the velocity distribution is at least as complex as the data distribution. Indeed, at $t=0$, the velocity distribution is identical to a shifted data distribution.


\subsection{Modeling the Velocity Distribution}
\label{sec:method:approach}
The previous section showed that the velocity distributions can be multimodal. Knowing that the optimal velocity model $v^\ast(x_t,t)$ of classic rectified flow averages ``ground-truth'' velocities, we can't expect classic rectified flow to capture this distribution. We hence wonder: 1) is it possible to capture the multimodal velocity distribution at each time $t$ and at each location $x_t$; 2) are there any benefits to capturing the multimodal velocity distribution as opposed to `just' capturing its mean as done by classic rectified flow.

Intuitively, an accurate characterization of the velocity distribution might be beneficial because we obtain straighter integration paths, which in turn may lead to easier integration with fewer neural function evaluations (NFE). In addition, capturing the velocity distribution provides additional modeling flexibility (an additional time axis), which might yield to improved results. Notably, modeling of the velocity distribution does not lead to modeling of a simpler distribution. As mentioned in \cref{sec:method:casestudy}, at time $t=0$ the velocity distribution is identical to a shifted data distribution. %

To accurately model the ``ground-truth'' velocity distribution, we can  use rectified flow for velocities rather than locations, which are used in the classic rectified flow formulation. This is equivalent to learning the acceleration. To see this, first, consider classic rectified flow again: we construct a time-dependent location $x_t$ from pairs $(x_0,x_1)$, compute the ``ground-truth'' velocity $v_{\mathrm{gt}}(x_t,t) = \partial x_t/\partial t$, and train a velocity model $v_\theta(x_t,t)$ to match this ``ground-truth'' velocity $v_\mathrm{gt}(x_t,t)$.

To learn the acceleration, we introduce a source velocity sample $v_0\sim\pi_0$ drawn from a known source velocity distribution $\pi_0$. We also construct a target velocity sample $v_1(x_t,t)\sim \pi_1(v; x_t, t)$ at time $t$ and at location $x_t$, which follows the target velocity distribution $ \pi_1(v; x_t, t)$ at time $t$ and at location $x_t$. Note, the target velocity sample at time $t$ and at location $x_t = (1-t)x_0 + tx_1$ is obtained via $v_1(x_t,t) = x_1 - x_0$, when considering a rectified flow. The samples $v_1(x_t,t)$ follow the ``ground-truth'' velocity distribution $\pi_1(v; x_t, t)$ at time $t$ and at location $x_t$.

Using both the source velocity sample $v_0$ and the target velocity sample $v_1(x_t,t)$, and following classic rectified flow, we introduce a new time-axis $\tau\in[0,1]$ and construct a time-dependent velocity $v_\tau(x_t,t) = (1-\tau)v_0 + \tau v_1(x_t,t)$ at time $t$ and at location $x_t$. Using it, we obtain the ``ground-truth'' acceleration from the time-dependent velocity $v_\tau(x_t,t)$ via $a_{\mathrm{gt}}(x_t,t,v_\tau,\tau) = \partial v_\tau/\partial \tau = v_1(x_t,t) - v_0 = x_1 - x_0 - v_0$. 

Note, for a specific $(x_t, t)$, we can get the following ODE induced from the linear interpolation of the target velocity distribution to convert $u_0 \sim \pi_0$ to $u_1 \sim \pi_1(v; x_t, t)$, %
\begin{equation}
\label{eq:udiffeq}
   du_\tau(x_t, t) = a (x_t, t, u_\tau, \tau) d\tau, \quad \text{with } u_0 \sim \pi_0.
\end{equation}
Here, $a (x_t, t, u_\tau, \tau)\! =\! \E_{\pi_0, \pi_1(v; x_t, t)}[ V_1 \!-\! V_0 | V_\tau \!= \! u_\tau ] \!=\! \E_{\pi_0, \rho_0, \rho_1}[ X_1 \!- \!X_0 \!-\! V_0 | V_\tau \! = \! u_\tau , X_t \!=\! x_t]$ is the expected acceleration vector field.


\begin{algorithm}[t]
\SetKwComment{Comment}{//}{}
\caption{Hierarchical Rectified Flow Training}\label{alg:training}
 The source distributions $\rho_0$ and $\pi_0$ and the dataset $\mathcal{D}$ \\
\While{stopping conditions not satisfied}{
 Sample $x_0 \sim \rho_0, x_1 \sim {\mathcal{D}}$, and $v_0 \sim \pi_0$   \Comment*[r]{better to sample a mini-batch}
 Sample $t \sim U[0,1]$ and $\tau \sim U[0,1]$  
 \Comment*[r]{different $t$ and $\tau$ for each mini-batch sample}
Compute loss following~\cref{eq:opt}\;
Perform gradient update on $\theta$
}
\end{algorithm}

Our approach aims to learn the acceleration vector field $a$ though flow matching for all $(x_t, t)$, i.e., matching the ``ground-truth'' acceleration by addressing 
\begin{equation}
\label{eq:opt}
\inf_a \mathbb{E}_{x_0\sim\rho_0,x_1\sim \mathcal{D},t\sim U[0,1],v_0\sim\pi_0,\tau\sim U[0,1]}\left[\|(x_1 - x_0 - v_0) - a(x_t,t,v_\tau, \tau)\|^2_2\right].
\end{equation}
In practice, we use a parametric model $a_\theta(x_t, t, v_\tau, \tau)$ to match the target ``ground-truth'' acceleration by minimizing the objective w.r.t.\ the trainable parameters $\theta$. Training of the parametric acceleration model is straightforward. It is summarized in \cref{alg:training}.

It remains to answer how we use the trained acceleration model $a_\theta(x_t,t,v_\tau,\tau)$ during sampling. We have the following coupled ODEs induced from the coupled linear interpolations:
\begin{equation}
\label{eq:vflow}
\begin{cases}
   du_\tau(z_t, t) = a (z_t, t, u_\tau, \tau) d\tau, & \text{with } u_0(z_t, t) \sim \pi_0, \quad \tau \in [0, 1],  \\
   dz_t = u_1(z_t, t) dt, & \text{with } z_0 \sim \rho_0, \quad t \in [0, 1].
\end{cases}
\end{equation}
Those coupled ODEs convert $z_0 \in \rho_0$ to $z_1 \in \rho_1$. After training, the ODEs in~\cref{eq:vflow} are simulated using the vanilla Euler method and $a_\theta$, as detailed in \cref{alg:sampling}. We first draw two random samples: $v_0\sim\pi_0$ from the source velocity distribution and $x_0\sim\rho_0$ from the source location distribution. We then integrate the velocity forward to time $\tau=1$ to obtain a sample from the modeled velocity distribution $v_1(x_0,0)$. Subsequently, we use this sample to perform one integration step on the location. We continue this procedure until we arrive at a sample $x_1$. 

\textit{Remark.}
The generation of data is governed by a random differential equation (RDE) with the random velocity field, where~\cref{eq:vflow} can be viewed as
\begin{equation}
    \label{eq:rde}
    dz_t = g(z_t, t, u_0(z_t, t)) dt, \quad \text{where } u_0(z_t, t) \sim \pi_0, z_0 \sim \rho_0,  
\end{equation}
and $g$ is a deterministic function. The randomness comes from the initial conditions for data and velocity. This is different from the sampling process governed by a stochastic differential equation (SDE) used in diffusion models~\citep{SongICLR2021}, where the randomness comes from the Wiener process and the initial condition for data.   

\begin{algorithm}
\SetKwComment{Comment}{//}{}
\SetKwInOut{input}{Input}
\SetKwInOut{output}{Output}
\caption{Hierarchical Rectified Flow Sampling}\label{alg:sampling}
\input{The source distributions $\rho_0$ and $\pi_0$, the number of $t$-discretization steps $J$, the number of $\tau$-discretization steps $L$, and the trained network parameters $\theta$. }
Sample $z_0\sim \rho_0$ and $u_0 \sim \pi_0$\;
Compute $\Delta t = \frac{1}{J-1}$ and $\Delta \tau = \frac{1}{L-1}$\; 
 \For{$j =1,\dots, J$}
      {\For{$l = 1,\dots,L$}
         {Compute $u_l = u_{l-1} + a_\theta(z_{t_{j-1}}, t_{j-1}, u_{l-1}, \tau_{l-1}) \cdot \Delta \tau$}      
       Compute $z_j = z_{j-1} + u_L \cdot \Delta t$}
\end{algorithm}



Note that our use of the term acceleration %
is not due to second-order derivatives of the location, but rather due to two hierarchically coupled linear processes. We hence refer to this construction as a hierarchical rectified flow. 

It remains to show that the obtained samples indeed follow the target data distribution. We will dive into this topic next.












\subsection{Discussions on the generated data distribution}
\label{sec:method:theory}
We discuss below the property of the hierarchical rectified flow defined in~\cref{eq:vflow}. According to  rectified flow theory, we can generate samples from the velocity distribution using the expected acceleration field. The following theorem states that the generation process defined in \cref{eq:vflow}, which uses the velocity distribution, leads to correct marginals for all times $t\in[0,1]$. 
\begin{theorem}
\label{the:1}
The time-differentiable stochastic process $\bm{Z} = \{Z_t: t \in [0, 1] \}$ generated by \cref{eq:vflow} has the same marginal distribution as the time-differentiable stochastic process $\bm{X} = \{X_t: t \in [0, 1] \}$ generated by the linear interpolation in~\cref{eq:lin_int}.
\end{theorem}
We defer the proof of \cref{the:1} to  \cref{sec:proof_thm_1}. Intuitively, the marginal preserving property is because at each time $t \in [0, 1]$, we can express $z_t$ as the linear interpolation of an $x_0 \sim \rho_0$ and an $x_1 \sim \rho_1$ according to~\cref{eq:lin_int}. 

A key benefit of our approach is that the process $\bm{Z}$ can be piece-wise straight. Starting with samples $z_t$ from $\rho_t$ for $t \in [0, 1]$, we propagate  each sample by $v(z_t, t)\Delta t$, where $v(z_t, t)\sim  \pi_1(v; z_t, t)$. Since $v(z_t, t) = x_1 - x_0$, where $tx_1 + (1-t) x_0 = z_t$, the straight path following $v(z_t, t)$ will lead to a sample from the data distribution. In other words, $\Delta t$ can be chosen arbitrarily in the interval $(0, 1-t]$.  In practice, the learned velocity distribution is not perfect. Therefore, instead of one-step generation from the initial distribution, we choose to propagate the samples for a couple of steps. As shown in Section~\ref{sec:exp}, we typically only use 2-5 steps in the numerical integration for data generation. Computationally, straight paths are very attractive as trajectories with nearly straight paths incur small time-discretization error in numerical simulation.

 




\subsection{Extending Towards Hierarchical Rectified Flow}
\label{sec:method:extension}

Consider the training objective for acceleration matching discussed in \cref{eq:opt}, and further consider the coupled ODE solved when sampling from the constructed process as specified in \cref{eq:vflow}. It is straightforward to extend both to an arbitrary depth. I.e., instead of modeling the velocity distribution by matching accelerations, we can model the acceleration distribution by matching jerk or go even deeper towards snap, crackle, pop, and beyond.

Formally, the training objective of a hierarchical rectified flow of depth $D$ is given by
\begin{equation}
\label{eq:opt:hierarchy}
\inf_f \mathbb{E}_{{\bm x}_0\sim{\bm \rho}_0,x_1\sim\rho_1,{\bm t}\sim U[0,1]^D}\left[\left\|\left(x_1 - {\bm 1}_D^T{\bm x}_0\right) - f\left({\bm x}_{\bm t},{\bm t}\right)\right\|^2_2\right].
\end{equation}
Here, ${\bm 1}_D$ is the $D$-dimensional all-ones vector and ${\bm t} = \left[t^{(1)}, \dots, t^{(D)}\right]^T$ is a $D$-dimensional vector of time variables drawn from a $D$-dimensional unit cube $U[0,1]^D$. Moreover, we use the $D$-dimensional vector of source distribution samples ${\bm x}_0 = [x_0^{(1)}, \dots, x_0^{(D)}]^T$, drawn from a $D$-dimensional source distribution ${\bm \rho}_0$, e.g., a $D$-dimensional standard Gaussian. We further use the $D$-dimensional location vector ${\bm x}_{\bm t} = [x^{(1)}_{\bm t}, \dots, x^{(D)}_{\bm t}]^T$, with its $d$-th entry given as $x^{(d)}_{\bm t} = (1-t^{(d)})x_0^{(d)} + t^{(d)}(x_1 - \sum_{k=1}^{d-1} x_0^{(k)})$. In addition, we refer to $f$ as the functional field of directions. Note that \cref{eq:opt:hierarchy} is identical to \cref{eq:lin_rect_flow} if $D=1$ or \cref{eq:opt} if $D=2$.

Before discussing inference we want to highlight the importance of the first term in \cref{eq:opt:hierarchy}. Subtracting a large number of Gaussians from a data sample $x_1$ leads to a smoothed distribution. This is another potential benefit of a hierarchical rectified flow formulation. 

Given a trained functional field of directions $f$ we  sample from the defined process via numerical simulation according to the following coupled ODEs:  
\begin{equation}
\label{eq:hflow}
\begin{cases}
   d z_{\bm{t}}^{(D)}\left(\bm{z}^{(1:D-1)}_{\bm{t}}, \bm{t}^{(1:D-1)}  \right)  = f (\bm{z}_{\bm{t}}, \bm{t}) dt^{(D)}, & \text{with } z_0^{(D)} \sim \boldsymbol{\rho}_0^{(D)},  \\
    d z_{\bm{t}}^{(D-1)} \left(\bm{z}^{(1:D-2)}_{\bm{t}}, \bm{t}^{(1:D-2)}  \right) = z_1^{(D)} \left(\bm{z}^{(1:D-1)}_{\bm{t}}, \bm{t}^{(1:D-1)}  \right) dt^{(D-1)}, & \text{with } z_0^{(D-1)} \sim \boldsymbol{\rho}^{(D-1)}_0,  \\
    \quad  \quad \vdots\\
   d z^{(1)}_{\bm t} = z_1^{(2)}\left (z^{(1)}_{\bm t}, t^{(1)} \right)dt^{(1)}, & \text{with } z_0^{(1)} \sim \boldsymbol{\rho}^{(1)}_0.
\end{cases}
\end{equation}
Note that \cref{eq:hflow} is identical to \cref{eq:RF} if $D=1$ or \cref{eq:vflow} if $D=2$.

Again, note that our use of the terms acceleration, jerk, etc.\ is not due to second, third, and higher-order derivatives of the location, but rather due to hierarchically coupled linear processes. %
