\section{Related Work}
\label{sec:rel}
\textbf{Generative Modeling:} %
GANs~\citep{goodfellow2014generative,arjovsky2017wasserstein}, VAEs~\citep{KingmaICLR2014}, and normalizing flows~\citep{tabak2013family,rezende2015variational,dinh2017density,huang2018neural,durkan2019neural} are classic methods for learning deep generative models. GANs excel in generating high-quality images but face challenges like training instability and mode collapse due to their min-max update mechanism. VAEs and normalizing flows rely on maximum likelihood estimation (MLE) for training, which necessitates architectural constraints or special approximations to ensure manageable likelihood computations. VAEs often employ a conditional Gaussian distribution alongside variational approximations, while the discrete normalizing flows utilize specifically designed invertible architectures and require costly Jacobian matrix calculations. Extending the discrete normalizing flow to continuous cases enabled the Jacobian to be unstructured yet estimatable using trace estimation methods~\citep{hutchinson1989stochastic,ChenARXIV2018,grathwohl2019scalable}. However, using maximum likelihood estimation (MLE) for this mapping requires costly backpropagation through numerical integration. Regularizing the path can minimize solver calls~\citep{finlay2020train,onken2021ot}, but it doesn't resolve the fundamental optimization challenges. \citet{rozen2021moser,ben2022matching} considered simulation-free training by fitting a velocity field, but still present scalability issues~\citep{rozen2021moser} and biased optimization~\citep{ben2022matching}.

Recent research has utilized diffusion processes, particularly the Ornstein-Uhlenbeck (OU) process, to link the target distribution $\rho_1$ with a source distribution $\rho_0$. This involves a stochastic differential equation (SDE) that evolves over infinite time, framing  generative model learning as fitting the reverse evolution of the SDE from Gaussian noise to $\rho_1$~\citep{sohl2015deep,ho2020denoising,SongICLR2021}. This method learns the velocity field by estimating the score function $\nabla \log(\rho_t(x))$ using the Fischer divergence instead of the maximum likelihood estimation (MLE) objective. Although diffusion models have demonstrated significant potential for modeling high-dimensional distributions~\citep{rombach2022high,hoogeboom2022equivariant,saharia2022photorealistic}, the requirement for infinite time evolution, heuristic time step parameterization~\citep{xiao2022tackling}, and the unclear significance of noise and score~\citep{bansal2024cold,lu2022maximum} pose challenges. Notably, the score-based diffusion models typically require a large number of time steps to generate data samples. In addition, calculating the actual likelihoods necessitates using the ODE probability flow linked to the SDE~\citep{SongICLR2021}. These highlight the need for further exploration of effective ODE-driven methods for learning the data distribution.  

\textbf{Flow Matching:}
Concurrently,~\citet{liu2023flow,LipmanICLR2023,albergo2023building} presented an alternative to score-based diffusion models by learning the ODE velocity through a time-differentiable stochastic process defined by interpolating between samples from the source and data distributions, i.e., $x_t = \psi_{t} (x_0, x_1)$, 
with $x_0 \sim \rho_0$ and $x_1 \sim \rho_1$, 
instead of the OU process. This offers greater simplicity and flexibility by enabling precise connections between any two densities over finite time intervals. \citet{liu2023flow} concentrated on a linear interpolation with $\psi_{t} (x_0, x_1) = (1-t) x_0 + t x_1$, i.e., straight paths connecting points from the source and the target distributions.~\citet{LipmanICLR2023} introduced the interpolation through the lens of conditional probability paths leading to a Gaussian. Extensions of~\citet{LipmanICLR2023} were detailed by~\citet{tong2023improving}, generalizing the method beyond a Gaussian source distribution.~\citet{albergo2023building,albergo2023stochastic} introduced stochastic interpolants with more general forms. 

\textbf{Straightening Flows:}  
\citet{liu2023flow} outlined an iterative process called ReFlow for coupling the points from the source and target distributions to straighten the transport path and demonstrated that repeating this procedure leads to an optimal transport map. Other related studies bypass the iterations by modifying how noise and data are sampled during training. For example, \citet{pooladian2023multisample,tong2023improving} calculated mini-batch optimal transport couplings between the Gaussian and data distributions to minimize transport costs and gradient variance. %
Note that these approaches are orthogonal to our approach and can be adopted in our formulation (see~\cref{sec:app:additional}). %

\textbf{Modeling Velocity Distribution:} Concurrently, \citet{GuoARXIV2025} also study a method to model multi-modal velocity vector fields. In this paper, we discuss  use of a hierarchy of ordinary differential equations. Differently, \citet{GuoARXIV2025} study how to use a lower-dimensional latent space to enable modeling of the velocity distribution via a variational approach. The hierarchy of ordinary differential equations permits to more accurately model the velocity distribution while use of the variational approach enables to capture semantics.
\vspace{-5pt}