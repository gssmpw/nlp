\section{Preliminaries}
\label{sec:prelim}

Given a dataset ${\cal D} = \{(x_1)\}$ consisting of samples $x_1\sim\rho_1$, e.g., images, drawn from an unknown target data distribution $\rho_1$, the goal of generative modeling is to learn a model that faithfully captures the unknown target data distribution $\rho_1$ and permits to sample from the learned distribution. %

Since we focus primarily on rectified flow, we provide its formulation in the following. %
At inference time, 
rectified flow starts from samples $x_0\sim\rho_0$ drawn from a known source distribution $\rho_0$, e.g., a standard Gaussian. The source distribution samples are pushed forward from time $t=0$ to target time $t=1$ via integration along a trajectory specified via a learned velocity field $v(z_t,t)$. This learned velocity field depends on the current time $t$ and the sample location $z_t$ at time $t$. Formally, we obtain samples by numerically solving the ordinary differential equation (ODE)
\begin{equation}
    \label{eq:RF}
    d z_t = v (z_t, t) dt, \, \text{with }z_0 \sim \rho_0, \quad t \in [0, 1]. 
\end{equation}
Notably, this sampling procedure is able to capture multimodal dataset distributions, as one expects from a generative model.

To learn the velocity field, at training time, rectified flow constructs random pairs $(x_0,x_1)$, consisting of a source distribution sample $x_0\sim\rho_0$ and a target distribution sample $x_1\sim{\cal D}$. The latter is drawn from a given dataset ${\cal D}$ consisting of samples which are assumed to be drawn from the unknown target distribution $\rho_1$. For a uniformly drawn time $t\sim U[0,1]$, the time-dependent location $x_t$ %
is computed from the pair $(x_0,x_1)$ using linear interpolation of $(x_0, x_1)$, i.e., 
\begin{equation}
\label{eq:lin_int}
x_t = (1-t)x_0 + tx_1, \quad \text{where} \, x_0 \sim \rho_0, \, x_1 \sim {\cal D}.
\end{equation}
At this location $x_t$ and time $t$, the ``ground-truth'' velocity $v_\text{gt}(x_t,t) = \partial x_t/\partial t = x_1 - x_0$ is readily available. It is then matched during training with a velocity model $v(x_t,t)$ via a standard $\ell_2$ loss, i.e., during training we address
\begin{equation}
\label{eq:lin_rect_flow}
\inf_{v} \mathbb{E}_{x_0\sim\rho_0,x_1\sim{\cal D},t\sim U[0,1]}\left[\| x_1 - x_0 - v(x_t,t) \|^2_2 \right],
\end{equation}
where the optimization is over the set of all measurable velocity fields. 
In practice, the functional velocity model $v(x_t,t)$ is often parameterized via a deep net with trainable parameters $\theta$, i.e., $v(x_t,t)\approx v_\theta(x_t,t)$, and the infimum resorts to a minimization over parameters $\theta$.

Considering the training procedure more carefully, it is easy to see that different random pairs $(x_0,x_1)$ can lead to different ``ground-truth'' velocity directions at the same time $t$ and at the same location $x_t$. The aforementioned $\ell_2$ loss hence asks the functional velocity model $v(x_t,t)$ to regress to  different ``ground-truth'' velocity directions. This leads to averaging, i.e., the optimal functional velocity model $v^\ast(x_t,t) = \mathbb{E}_{\{(x_0,x_1,t):(1-t)x_0+tx_1 = x_t\}}\left[v(x_t,t)\right]$.

According to Theorem 3.3 by~\cite{liu2023flow}, if we use $v^\ast$ for the ODE in~\cref{eq:RF}, then the stochastic process associated with \cref{eq:RF} has the same marginal distributions for all $t \in [0, 1]$ as the stochastic process associated with the linear interpolation characterized in~\cref{eq:lin_int}.

Nonetheless, to avoid the averaging, in this paper we wonder whether it is possible to capture the multimodal velocity distribution at each time $t$ and at each location $x_t$, and whether there are any potential benefits to doing so.


