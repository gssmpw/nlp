Large-scale imitation learning efforts have resulted in generalist policies able to perform manipulation tasks~\citep{robomimic2021,ravichandar2020recent}. However, these methods require lots of data, which is expensive and time-consuming to collect on real robots, and have difficulty generalizing to new tasks.
Our goal is to drastically reduce the data needed for real-world robotics while also enabling better generalization to new tasks.


One approach to reducing real-world data is training policies in simulation and transferring these policies to the real world. 
While this approach has worked well for navigation with restricted observation spaces [CITE RMA, Zurich paper, etc.], it's much more difficult to do so with visual manipulation policies as visual features trained in simulation don't transfer well to the real world, requiring very specific, hard-to-build simulators that reflect the exact real world scene [CITE STUFF]. Designing such sims for each new robot embodiment or environment setting is not scalable.
What if we could instead use arbitrary simulators that don't need to directly align with our downstream tasks and embodiment?
\caelan{This paragraph reads like the contribution is on the simulation side, not on the policy side}

To enable this, our key insight is to instead separate the policy into two levels, one that reasons about the task and one that actually performs low-level control. With this separation, the higher-level reasoner can be trained on general robotic simulation data to learn \emph{what} high-level actions to perform for various robot manipulation tasks. Meanwhile, the low-level control policy can instead learn \emph{how} to execute those high-level actions rather than focusing on specific visual features that do not generalize from sim to real.
\caelan{We'll need to cite a few hierarchical policy learning papers}

Specifically, we fine-tune Vision Language Models (VLMs)~\citep{bommasani2021opportunities}, pre-trained on large amounts of visual data, on general robotic simulation data to learn \emph{what} high-level actions are useful in robot manipulation. In contrast with works that directly fine-tune VLMs for low-level actions [RT-2~\citep{brohan2022rt,zitkovich2023rt}, OpenVLA~\citep{kim2024openvla}, LLARVA~\citep{niu2024llarva}]---which still require far too much data to perform well---we fine-tune VLMs to output easy-to-generalize, high-level trajectory sketches that can be fed to any downstream visual policy. These sketch-level representations have been shown to enable better task generalization while requiring less robot-specific data~\citep{gu2023rttrajectory}.

Our key contribution is a pipeline that can be trained on \emph{general} robotic simulation data, then with very small amounts of downstream real-robot data, guides any visual policy to simply focus on the low-level actions needed for control.
\caelan{If the focus is is on the hybrid simulation and real training, the most critical thing to do is obtain some real-world results that back up that it works}

\textbf{Our contributions are as follows:}
\begin{itemize}
    \item A novel hierarchical policy architecture that uses a VLMs to make high-level decisions and, conditioned on these decisions, deploys RVT for low-level control.
    \item A data efficient training regime that leverages simulation data to train the high level and real data to train the low level.
    \item Simulation and real experiments ...
\end{itemize}