Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, ``off-domain'' data such as action-free videos, hand-drawn sketches or simulation data. 
In this work, we posit that \emph{hierarchical} vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. 
In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. 
We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. 
In the real-robot experiments, we observe an average of 20\% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50\% relative gain. 
Visual results are
provided at: \url{https://hamster-robot.github.io/}


% Large models such as VLMs and LLMs have shown strong open-world generalization to complex problems in vision and language, but they have been relatively more difficult to deploy in robotics. One major challenge that has made the deployment of such models challenging in robotics is the lack of scalable robotic training data. Typically, robot datasets must be obtained on hardware, through an expensive on-robot data collection process. For scalable training, it's important that these models are also able to use cheaper, ``off-domain" data such as videos, hand-drawn sketches, or data from simulation besides just high-quality, on-robot data. In this work, we posit that hierarchical vision-language-action models can be more effective in utilizing cheap, off-domain data than standard monolithic vision-language-action models, showing considerable cross-domain transfer. In particular, we study a class of hierarchical vision-language-action models, where high-level vision-language models (VLMs) are trained on relatively cheap data sources to produce semantically meaningful intermediate predictions such as 2D paths indicating desired behavior. These predicted 2D paths can then serve as guidance for low-level, 3D aware control policies capable of precise manipulation. Doing so alleviates the high level model from the burden of fine-grained motion prediction, while providing guidance that can significantly improve low-level generalization. In this work, we show that this separation of prediction into semantic high-level predictions, and 3D-aware low-level predictions allows \emph{hierarchical} VLA policies to transfer across significant domain gaps, from simulation to the real world or across scenes with widely varying visual appearance. Doing so allows for the usage of cheap, abundant data sources beyond high-quality on-robot data, thereby enabling broad semantic and visual generalization. We demonstrate how hierarchical architectures trained on such cheap off-domain data can enable robotic manipulation with semantic, visual, and geometric generalization through experiments in simulation and the real world.