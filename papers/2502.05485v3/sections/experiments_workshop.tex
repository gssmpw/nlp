\section{Experimental Evaluation}


To test the hypotheses proposed in Section~\ref{sec:method}, we perform empirical evaluations in both simulation and the real world. %The experiments primarily aim to answer the following questions: (1) do hierarchical VLA models enable behavioral generalization to unseen scenarios? (2) do hierarchical VLA models show more effective cross-domain generalization than monolithic VLA models or low-level imitation learning methods? (3) is behavior learned by hierarchical VLA models robust to significant degrees of visual and semantic variations? (4) does including cross-domain data from settings like simulation really help with model generalization? (5) does explicitly finetuning the high-level VLM yield benefits in terms of spatial and semantic reasoning?

%\ankit{shall we remove this paragraph}
%To validate these hypotheses, we perform experiments in simulation by training both the high-level VLM and the low-level 3D policy on data from the RLBench~\citep{james2020rlbench} benchmark environment and testing on visual and semantic variations in the related Colloseum benchmark~\cite{pumacay2024colosseum}. In the real world, we perform experiments by training the base VLM model, VILA-1.5-13b~\citep{vila2024}, on the data mixture proposed in Section~\ref{sec:method:vlm:datasets}, and testing on a completely unseen tabletop manipulation setting that is depicted in Fig [TODO]. Further details of the experimental setup in both simulation and the real world is provided in the following sections. 
%\ag{We need to have a figure of the table top setting clearly indicated}

\subsection{Real World Evaluation on Tabletop Manipulation}
\input{tables/real}
Our real-world evaluation experiments aim to test the generalization capability of hierarchical VLA models across significant semantic and visual variations. In particular, we consider a variant of \method\ that uses a VLM (VILA-1.5-13b) finetuned on the data mixture in Section~\ref{sec:method:vlm:datasets} as the high-level predictor, with a 3-D policy architecture - RVT-2~\citep{goyal2024rvt} as the choice of low-level policy, as described in Section~\ref{sec:method:policy}. The low-level 3D policy is trained with 320 episodes collected via teleoperation directly on the table-top manipulation setup shown in Fig.~\ref{fig:experiments:training_data}. Importantly, the high-level VLM in \method\ is not finetuned on any in-domain data and is directly transferred only from the cheap data sources described in Section~\ref{sec:method:vlm:datasets}. This suggests that any generalization that the VLM sees does not result from in-domain training data rather than from cross-domain transfer. 

% \ag{Ensure we have something showing the difference between training domains and testing setup. This is important to show the tranfer results.}

\textbf{Baseline comparisons.} We compare \method\ to a state-of-the-art monolithic VLA, OpenVLA~\citep{kim2024openvla}, as well as a non-VLM 3D imitation learning policy. For fair comparison, we finetune OpenVLA on the collected in-domain trajectory data described above since OpenVLA showed poor zero-shot generalization. For the 3D imitation learning policy, we use RVT-2~\citep{goyal2024rvt} as our baseline, as it is effective in learning robust policies with few demonstrations. The RVT-2 baseline is trained with the same teleoperation data used to train the low-level policy in \method\ but without the intermediate 2D path representation from \method 's VLM.

\begin{figure}[!tb]
    \centering
    %\includegraphics[width=\linewidth,trim={0in 4.8in 0in 0.5in},clip]{figs/main_exp_fig_short.pdf}
    \includegraphics[width=\linewidth]{figs/main_exp_fig_short.pdf}
    % \includegraphics[width=\linewidth,trim={1in 1.5in 3in 0.8in},clip]{figs/main_exp_fig.pdf}
    \caption{\footnotesize{Quantitative evaluation results on a real-world robot, evaluated across different axes of generalization. Across all generalization axes, \method \ outperforms monolithic VLAs and 3D imitation learning.}}
    \label{fig:experiments:main_exp}
    \vspace{-4mm}
\end{figure}

\textbf{Results.} Figure~\ref{fig:experiments:main_exp} summarizes our real-world results. We compile results for multiple task types, including `pick and place,' and nonprehensile tasks such as `push buttons' and `knock down objects.' Similar to prior work~\citep{kim2024openvla}, we test generalization across various axes: \emph{arrangement}: new positions and object combinations with seen objects; \emph{obj and goal:} unseen object-goal combinations; \emph{visual:} visual changes in table texture, lighting, distractor objects; \emph{language:} unseen language instructions (e.g., candy $\rightarrow$ sweet object); \emph{spatial:} unseen spatial object relationships in the instruction; \emph{novel object:} unseen objects; and lastly, \emph{multiple:} a combination of multiple variations. 
In total, we evaluate each model on 74 tasks for 222 total evaluations.
% \ag{It'd be really useful to have a figure demonstrating each of these variations}

We find that \method~is able to outperform monolithic VLA models as well as 3D imitation learning methods. This is significant because this improved performance is in the face of considerable visual and semantic changes in the test setting, showing the ability of \method~to transfer much more effectively than monolithic VLA models or non-VLM base models. See \Cref{sec:appendix:real_world_exp_details} for evaluation conditions, a task list, and other experiment details, and \Cref{sec:appendix:failure_modes} for failure modes.
Finally, see \Cref{sec:appendix:simulation_results} for results in simulation.
%We refer readers to the supplementary website for additional details on the failure modes and evaluation conditions. \jz{TODO: replace this with link to appendix section}

\subsection{Impact of Design Decisions on VLM performance}
\label{sec:experiments:vlm_design}
To better understand the transfer and generalization performance of the proposed hierarchical VLA model, we analyze the impact of various decisions involved in training the high-level VLM. We conduct a human evaluation of different variants of a trained high-level VLM on a randomly collected dataset of real-world test images, as shown in \Cref{fig:vlm_generalization}. We ask each model to generate 2D path traces corresponding to instructions such as ``move the block on the right to Taylor Swift'' 
%or ``screw the light bulb in the lamp'' 
(the full set is in \Cref{sec:appendix:generalization}). We then provide the paths generated by each method to human evaluators who have not previously seen any of the models' predictions. The human evaluators then rank the predictions for each method; we report the average rank across the samples in \Cref{tab:experiments:vlm}. 

We evaluate the following VLM models: (1) zero-shot state-of-the-art closed-source models such as GPT-4o using a similar prompt to ours (shown in \Cref{fig:gpt4o-prompt}), (2) zero-shot state-of-the-art closed-source models such as GPT-4o but using Code-as-Policies~\citep{liang2023code} to generate paths as described in \citet{gu2023rttrajectory} (prompt in \Cref{fig:cap-prompt}),
%(3) zero-shot open-source models (VILA-1.5-13b), without any finetuning, 
(3) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section~\ref{sec:method:vlm:datasets}, but excluding the simulation trajectories from the RLBench dataset, (4) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section~\ref{sec:method:vlm:datasets}, including path sketches from the RLBench dataset. The purpose of these evaluations is to first compare with closely related work that generates 2D trajectories using pretrained closed source VLMs~\cite{gu2023rttrajectory} (Comparison (1) and (2)). 
%Secondly, the comparison with (3) is meant to analyze the impact of finetuning over using zero-shot open-source VLMs for path generation. 
The comparison between (3) and (4) (our complete method) is meant to isolate the impact of including the simulation path sketches from the RLBench dataset. In doing so, we analyze the ability of the VLM to predict intermediate paths to transfer across significantly varying domains (from RLBench to the real world). 

The results suggest that: (1) zero-shot path generation, even from closed-source VLMs~\cite{gu2023rttrajectory} such as GPT-4o with additional help through Code-as-Policies~\citep{liang2023code}, underperforms VLMs finetuned on cross-domain data as in \method; (2) inclusion of significantly different training data such as low-fidelity simulators during finetuning improves the real-world VLM performance. This highlights the transferability displayed by \method\ across widely varying domains. These results emphasize that despite considerable perceptual differences, the \method's hierarchical VLA can effectively utilize diverse sources of cheap prior data for path predictions. 


\input{tables/vlm_workshop}


% We first study how fine-tuning \method s VLM on our curated datasets affects its performance. We find the fine-tuning the VLM on our curated robotic dataset significantly improves performance as shown in Table.~\ref{tab:vlm}.

% We also compare \method s fine-tuned VLM to techniques used in prior works for generating intermediate representations like 2D paths. Specifically, we compare to techniques used in RT-Trajectory: zero-shot VLM prompting and Code-as-policies. To accommodate the best-case scenario in prior works, we allow them to use state-of-the-art closed-source VLM, while our method is limited to open-source methods that can be finetunes. As illustrated in Table~\ref{tab:real}, we find that...

% Further, we examine the cross-domain generalization of \method s VLM. This is done by training on the RLBench simulation, which looks very different from the real world. We then test in the real world on tasks with semantics similar to tasks present in RLBench. Surprisingly, we find that \method is able to understand the underlying semantics of tasks from data is visually very different. This paves a path of effectively utilizing diverse sources of data.

%\paragraph{Significantly Out-of-Domain Scenarios}
%We further probe the VLM to study its generalization on setups that are ``significantly" out-of-domain. Figure~\ref{fig:vlm_generalization} shows some samples. Overall, we find the finetuned VLM in \method~ to be surprisingly robust on data that is significantly out-of-domain. For example, the finetuned VLM can draw reasonable 2D paths on hand-drawn sketches which were neither in the pre-training nor the finetuning data. We also ask the VLM to create 2D paths in cases that require world knowledge, like ``move the left block on Jensen Huang's photo". This is interesting as it shows that \method s VLM can utilize its knowledge from different domains (like knowledge about people). 

% \ag{I don't quite get this celebrity thing, can someone clarify this?}


% \ag{Overall we need to go through the experiments and really illustrate how different training and test are, illustrate the different testing conditions, illustrate some paths drawn by the VLM, and for the human comparison provide one illustrative example.}