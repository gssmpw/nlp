\section{\method: Hierarchical Action Models for Robotic Learning}
\label{sec:method}

In this work, we examine how VLA models can be trained on relatively abundant data to demonstrate cross-domain transfer capabilities, as opposed to training on expensive image-action data collected on a robot. \method\ is a family of hierarchical action models designed for this purpose, exhibiting generalizable and robust manipulation. It consists of two interconnected models: first, a higher-level VLM that is fine-tuned on large-scale, cross-modal data to produce intermediate guidance (detailed in \Cref{sec:method:vlm}), and second, a low-level policy that produces actions conditioned on the VLM's predicted guidance (detailed in \Cref{sec:method:policy}). The finetuned VLM and the low-level policy communicate using a 2D path representation. \Cref{fig:method} provides an overview of \method's design. Crucially, we study the ability of such a hierarchical design to enable training on cheap, abundantly available data such as simulation and videos. 

\textbf{Background: Imitation Learning via Supervised Learning.} The goal of imitation learning is to train a probabilistic policy $\pi_\theta(a \mid s, o, z)$ from an expert-provided dataset. This policy $\pi_\theta$ outputs the probability of producing action $a$ conditioned on proprioceptive states $s$, perceptual observations $o$, and language instructions $z$ that specify the task. In the typical imitation learning setting, a dataset of expert in-domain trajectories is provided, consisting of observation-action-language tuples $\mathcal{D} = \{(s_i, a_i, o_i, z_i)\}_{i=1}^N$. This dataset can be utilized to learn the parameters of the policy $\pi_\theta$. While $\pi$ can take on a variety of architectures with various training objectives~\citep{goyal2023rvt, ke20243d, zhao23aloha, chi23diffusion}, most imitation learning algorithms are trained via supervised learning to maximize the objective: $\mathbb{E}_{(s_i, a_i, o_i, z_i) \sim \mathcal{D}} \left[ \log \pi_\theta\left(a_i \mid s_i, o_i, z_i \right) \right].$ This core objective can be modified with rich architectural choices such as 3D policy architectures~\citep{goyal2023rvt, ke20243d} or more expressive policy distribution classes~\citep{zhao23aloha, chi23diffusion}, but generalization to out-of-domain to settings with semantic or visual variations is still challenging. We study how vision-language models can be used to aid the generalization of such low-level imitation learning-based policies.

\textbf{Problem Definition.}
Rather than operating in the pure imitation learning setting as described earlier, we study a scenario where cross-domain data is utilized to train VLA models. While the typical imitation learning setting uses a dataset of optimal in-domain, on-robot tuples $\mathcal{D} = \{(s_t, a_t, o_t, z_t)\}_{t=1}^N$ to learn a near-optimal policy $\pi_\theta$, in this setting we additionally assume access to a much larger dataset(s) of ``off-domain" approximately optimal data $\vlmdata = \{(o_i^o, z_i^o)\}_{i=1}^M$, where $M \gg N$, such as video or simulation data. This ``off-domain" data $\vlmdata$ is different from in-domain data $\mathcal{D}$ in several important ways: \emph{1)} Off-domain perceptual observations $o_i^o$ may be considerably different than in-domain perceptual observations $o_i$, even when the underlying physical state of the system is similar
% \footnote{Viewed from the lens of a partially observable Markov decision process, this involves differing emission functions across domains.}.
An illustrative example of this is the marked difference between simulation and real-world scene appearance (see Figure.~\ref{fig:vlm_generalization}). \emph{2)} The underlying physical dynamics of the system can be potentially different, i.e the transition dynamics $\mathds{P}(s'|s, a)$ between states $s', s$ may be different between off-domain sources such as video or simulation than the test-time deployment setting. While the dynamics may show level differences, we assume the higher-level coarse strategies to solve the task remain invariant. \emph{3)} Off-domain data may not have access directly to actions $a$ or proprioception $p$, for instance in video based datasets. This poses challenges to directly applying the standard imitation learning paradigm for these datasets.
% \anqi{Do we want to talk about different modalities, e.g., 2d single images versus 3d point clouds at all here?}

The goal is to leverage the combination of a small amount of ``expensive" in-domain data $\mathcal{D}$ and a large amount of relatively ``cheap" off-domain data $\vlmdata$ to obtain a generalizable policy $\pi_\theta$ that can be successfully deployed over various initial conditions, task variations, and visual variations in the in-domain robot environment. Without additional assumptions, this problem is arduous due to the lack of alignment between the in-domain and off-domain settings. In this work, we assume access to an intermediate \emph{path-labeler} $p_i = h(o_i, z_i)$ at training time, that accepts an observation $o_i$ and a language instruction $z_i$ from either the off-domain or in-domain datasets, to produce an intermediate path label $p_i$ that indicates \emph{how} to optimally perform the task $z_i$ from the observation $o_i$. In this work, we choose this intermediate path label $p_i$ to be a sequence of points, a 2D path, on the image that indicates coarse end-effector motion to solve the designated task. 
% \anqi{Should mention that it additionally include gripper actions. And ideally refer to a figure.} 
This path-labeler at training time can come from different sources --  a projection of known proprioception if available, human-drawn trajectory annotations on images, point-tracked end-effector or hand positions from video, and so on. Applying such a path labeler to the off-domain dataset yields $\Tilde{\vlmdata} = \{(o_i^o, z_i^o, p_i^o)\}_{i=1}^M$.
% while applying it to the in-domain dataset correspondingly yields $\Tilde{\mathcal{D}} = \{(s_i, a_i, o_i, z_i, p_i)\}_{i=1}^N$.
% \anqi{a small detail, for in-domain dataset, shouldn't eef projection be sufficient?}
% \jz{the path labeler $h$ notation is incorrect and doesn't align with the method description. notation here needs to change and be incorporated in \Cref{sec:method:policy:training}}\anqi{Ideally, we may actually want to use a different notation for datasets with path labels and without, e.g., $\mathcal{D}_\text{off}$ and $\Tilde{\mathcal{D}}_\text{off}$.}

% In the following sections, we propose and discuss a family of VLA architectures that finetune high-level VLMs to produce the above mentioned path labels $p_i$ from off-domain data $\Tilde{\vlmdata}$, that can then provide guidance to low-level policies trained from in-domain data $\mathcal{D}$. Importantly, we show that these types of architectures enjoy the ability to train on more easily available data sources such as video or simulation and demonstrate significant cross-domain generalization. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/robolines_method.pdf}
    \caption{\footnotesize{Depiction of \method 's execution. The high-level VLM is called once to generate the 2D path. The low-level policy is conditioned on the 2D path and interacts with the environment sequentially to execute low-level actions. The path predicted by the VLM enhances the low-level policy generalization capability.}
    }
    \vspace{-4mm}
    \label{fig:method}
\end{figure*}

\subsection{\method 's VLM for producing 2D Paths Trained from Off-Domain Data}
\label{sec:method:vlm}

The first stage of building a \method\ VLA model is finetuning a high-level VLM that predicts coarse 2D paths $p$ given a language instruction $z$ and observation $o$. This path represents the approximate trajectory of the robot end-effector on the input camera image. It also contains information about the gripper state (where to open the gripper and where to close it) as subsequently explained.

Although, conceptually, any VLM can be used to predict such a 2D path by casting an appropriate prompt, we find that standard pre-trained VLMs struggle with predicting such a path in a zero-shot manner (see \Cref{tab:experiments:vlm}). Therefore, we finetune pre-trained VLMs on datasets that ground VLMs to robot scenes and path predictions collected from easier-to-obtain sources, i.e., internet visual-question-answering data, robot data from other modalities, and simulation data.
% \footnote{Data sources such as human image annotations or end effector trackers on robot videos can also be used.} 
The primary advantages of finetuning such a hierarchical VLM that produces intermediate representations $\hat{p}_i$ compared to producing actions $a$ with a monolithic model~\citep{kim2024openvla, zitkovich2023rt} are twofold: 1) the lack of actions in certain off-domain datasets (such as videos) makes it impossible to even train monolithic pixel-to-action models, 2) we find that hierarchical VLMs producing intermediate cross-domain predictions generalize more effectively than monolithic VLAs. 

\textbf{Finetuning Objective and Datasets.}
\label{sec:method:vlm:datasets}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figs/robolines_training_data.pdf}
%     \caption{(a): Examples of training data in $\Tilde{\mathcal{D}}_\text{off}$ used to train \method's VLM. (b): The data used to train \method's low-level policies
%     % \ankit{show one example of each of the training data domain used in training the VLM (3 images like in the text); show a couple of examples of the training data used for training the policy. show the input/output for both cases.}
%     }
%     \label{fig:experiments:training_data}
% \end{figure}
% The 2D path that the VLM predicts is represented sequence of tokens corresponding to points on an image that detail where the robot gripper should move and close/open to complete the task (see \Cref{fig:method}(a)).
We use VILA-1.5-13b~\citep{vila2024}, a 13-billion-parameter vision language model trained on interleaved image-text datasets and video captioning data, as our base VLM. We then curate a multi-domain dataset to finetune this model for effective 2D path prediction. Predicting the 2D path of the end-effector requires understanding \emph{what} objects to manipulate in a given task in terms of their pixel positions, but also reasoning about \emph{how} a robot should perform the task. To enable this understanding, we collate a diverse off-domain dataset $\vlmdata$ from a wide range of modalities, including real-world data, visual question-answering data, and simulation data. Importantly, \emph{none} of this off-domain data used to train the VLM comes from the deployment environment, thereby emphasizing generalizability. However, as outlined in \Cref{sec:method:policy}, the predictions of this trained VLM are used to guide a low-level policy at inference time. 
%are paired with a small amount of in-domain trajectory data $\mathcal{D}$ to train low-level control policies. \anqi{Maybe a stupid Q since I am not fully aware of the techinical details. Did we use the VLM outputs to train the low-level policy or the GT eef 2D paths?}

We assemble a dataset $\Tilde{\mathcal{D}}_\text{off} = \{(o_i^o, z_i^o, p_i^o)\}_{i=1}^M$ of image inputs $o^o_i$, language prompts $z_i^o$, and path labels $p_i^o$ consisting of three types of data: (1) pixel point prediction tasks (\emph{what}); (2) simulated robotics tasks (\emph{what and how}); (3) a real robot dataset consisting of trajectories (\emph{what and how}). 
We detail each dataset below; see \Cref{fig:experiments:training_data} for visualization of each dataset's prompts and labels.

\textbf{Pixel Point Prediction.} For pixel point prediction, we use the dataset released by RoboPoint~\citep{yuan2024robopoint} with 1.4 million VQA tasks, with most answers represented as a list of 2D points corresponding to locations on the image. A sample consists of a prompt $z^o$ like $\texttt{Find all instances of cushions}$, an input image $o^o$ and labels $p^o$ like \texttt{$[(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)]$}.
%\footnote{Note that this is not a temporally ordered path, but rather simply a set of unordered points of interest in an image. We overload notation here for the sake of notational convenience.} 
This dataset consists of data automatically generated in simulation and collected from existing real-world datasets; its diversity and tasks enable the \method\ VLM to reason about pixel-object relationships across diverse scenes while retaining its semantic generalization capabilities.

\textbf{Robot Simulation Data.} We additionally generate a dataset of simulated robotics tasks from RLBench~\citep{james2020rlbench}, a simulator of a Franka robot performing tabletop manipulation for a wide array of both prehensile and non-prehensile tasks.
We use the simulator's built-in planning algorithms to automatically generate successful manipulation trajectories and construct ground-truth 2D path labels $p^o$.
Each trajectory contains a sequence of 3D coordinates of the robot's gripper in world space, as well as whether the gripper is open or closed at a given time step. 
We use known camera intrinsics and extrinsics to project these points on the front image and construct labels $p^o = [(x_\text{image}, y_\text{image}, \texttt{gripper\_open}), \ldots]$ where $x_\text{image}, y_\text{image} \in [0, 1]$ are relative pixel locations of the end effector's position on the image.
% \anqi{should also explain what {\tt gripper\_open} is}. 
The front camera image of the initial state forms the image input $o^o$ and the prompt $z^o$ for the VLM is to provide a sequence of points denoting the trajectory of the robot gripper to achieve the given instruction (see Figure~\ref{fig:method})
We generate 1000 episodes for each of 79 robot manipulation tasks in RLBench, each episode with $\sim$4 language instructions, for a total of $\sim$300k $(o^o, z^o, p^o) $ tuples for $\Tilde{\vlmdata}$.

\textbf{Real Robot Data.} Using real robot data allows us to ensure the VLM can reason about objects and robot gripper paths when conditioned on scenes, including real robot arms.
We use existing, online robot datasets \emph{not from the deployment environment} to enable this VLM ability.
We source 10k trajectories from the Bridge dataset~\citep{walke2023bridgedata, open_x_embodiment_rt_x_2023} consisting of a WidowX arm performing manipulation tasks and \~45k trajectories from DROID~\citep{khazatsky2024droid}. 
For both datasets, we use the given end-effector trajectories and given (or estimated) camera matrices to convert robot gripper trajectories to 2D paths $p^o$. We use a camera image from the first timestep of each robot trajectory as $o^o$ and a similar text prompt $z^o$ as the simulation dataset. Note that we essentially utilize the robot data as video data, where the end effector is tracked over time. In principle, this could be done with any number of point-tracking methods~\citep{doersch2023tapir} on raw video as well, with no action or proprioceptive labels. 

\textbf{VLM Training.}
We finetune the \method\ VLM on all three datasets by randomly sampling from all samples in the entire dataset with equal weight.
%\jz{Yi fill in weighting scheme} \yi{most of the strategy is similar to sft step in VILA. we use VILA1.5-13B as pretrained weight which uses Vicuna-1 as LLM and siglip-so400m-patch14-384 as vision encoder. 
%We finetune both LLM and vision encoder. since we are using equivalent batch size 256 instead of 2048, we use LR=1e-5 instead of 1e-4 to maintain stable loss curve. It takes 30 hours on an 8xA100 node.} 
%sampling weights with the standard VLM training objective of minimizing the negative log-likelihood of the answer tokens $p_\text{ans}$ conditioned on prompt tokens $p_\text{prompt}$ and an input image $o$:
One problem with directly training on the path labels $p^o$ is that many paths may be extremely long, e.g., exceeding one hundred points. 
Since we want the \method\ VLM to reason at a \emph{high level} instead of on the same scale as the low-level control policy.
Therefore, we simplify the paths $p^o$ with the Ramer-Douglas-Peucker algorithm~\citep{RAMER1972244, douglas_pecker_1973} that reduces curves composed of line segments to similar curves composed of fewer points. 
% This results in an average 2D path length consisting of \jz{yi insert} points.
We train with the standardized supervised prediction loss to maximize the log-likelihood of the language labels $p^o$:
% \anqi{ground truth labels or 2D points/paths labels? With language, one may confuse it with $z$.}:
%\begin{equation*}    
$\mathbb{E}_{(o^o_i, z_i^o, p_i^o) \sim \Tilde{\vlmdata}}  \log \text{VLM} \left( p_i^o \mid z_i^o, o^o_i \right).$
%\end{equation*}

\subsection{Path Guided Low-Level Policy Learning}
\label{sec:method:policy}
After training the \method\ VLM to predict paths, we train a low-level policy to utilize these paths to predict actions. While a low-level control policy \emph{can} learn to solve the task without access to 2D path predictions, providing it with 2D paths can make the task easier. The paths allow the low-level policy to forgo long-horizon and semantic reasoning and focus on local and geometric predictions to produce low-level actions. As we find empirically (see \Cref{fig:experiments:main_exp}), 2D paths allow for considerably improved visual and semantic generalization of low-level policies. We train low-level policies based on rich 3-D perceptual information, available at test time on a robotic platform with standard depth cameras. Then the question becomes---how do we incorporate 2D path information $\hat{p}$ produced by the VLM in \Cref{sec:method:vlm:datasets} onto the 3D inputs to enable generalizable robot manipulation? 
% We first provide a description of low-level policy training guided by path information and then detail how the VLM and low-level policy are combined into our \method\ VLA for evaluation in the real world.

% \subsubsection{Training Path-Conditioned Policies}
% \label{sec:method:policy:training}
\textbf{Conditioning on Paths.}
We convert 2D paths of the form $p = \{(x_i, y_i, \texttt{gripper\_open})\}_{t=1}^L$ into a format that is easy to incorporate into any language ($z$), proprioception ($s$), and image ($o$) conditioned policy $\pi_\theta(a \mid s, o, z)$.
% while also ensuring the paths are represented in a natural manner that is easy to learn from? 
While one could concatenate the path with the proprioception or language input, paths are of varied lengths, and this could prevent the integration of such paths into existing policy architectures that cannot take in varied proprioceptive or language inputs. Instead, we directly draw the 2D path points onto the image input to the policy, which is not only generalizable across policy architectures but also may provide easier-to-follow path guidance as the policy does not have to learn how to associate path points with their corresponding image locations~\citep{gu2023rttrajectory}. 
During training, we use oracle paths constructed by projecting end-effector points to the camera plane as described for simulation and real robot data in \Cref{sec:method:vlm:datasets}.

Formally, we iterate 
% \anqi{need to clarify how the paths are obtained. Is it through path-labeler or VLM? Need to use $\hat{l}_i$ later on if we use VLM predictions.} 
through each trajectory $\tau_i = \{s_i^t, a_i^t, o_i^t, z_i)\}_{t=1}^T$ on the in-domain dataset $\mathcal{D}$ to obtain the path $p_i$. 
% \anqi{somehow similar confusion here} describing $\tau_i$. 
\citet{gu2023rttrajectory} proposed using colored trajectories to guide a policy's actions, and we largely follow their method of coloring trajectories to indicate gripper status and progression through time. 
These paths are drawn onto all images in the trajectory $o_i^1...o_i^T$ by drawing points at each $(x, y)$ and connecting them with line segments to obtain $\{\tilde{o}_i^t\}_{t=1}^T$.
We use a color gradient to indicate progression through time (see \Cref{fig:method}(b) for an example).
We plot circles for change in gripper status: e.g., \textcolor{green}{green} for closing the gripper and \textcolor{blue}{blue} for opening.
This constructs the final in-domain path-labeled dataset $\mathcal{D}_\text{path} = \{ (s_i, a_i,\tilde{o}_i, z_i)\}_{i=1}^N$.
% \anqi{Earlier in 4.1, $\mathcal{D}$ is used to also refer to the path-labeled dataset, and I suggested to change it to $\Tilde{\mathcal{D}}$. Either way, we need to make it consistent.}


%$ = \{ (s_i, a_i, o_i, z_i) \}_{i=1}^N$ 

%\ag{TODO: we need to describe this more formally and potentially for both 3D-DA type methods and for RVT style methods}
%\ag{We need to argue why this is better than just concatenating etc}


%Describe the shallow head architecture and how exactly the trajectory is fed into the low-level policy (image overlay). Describe how this can be trained on in-robot data. I wouldn't make it too specific to RVT or 3DDA or anything. Just as a general low-level policy. 

%\subsubsection{Training the Policy}
\textbf{Imitation Learning.} Finally, we train a policy $\pi_\theta(a \mid s, \tilde{o}, z)$ 
% \anqi{is it conditioned on $z$ or $l$? Isn't $l$ given by $\tilde{o}$ already?} 
conditioned on proprioception and other sensor information $s$, path-annotated image observations $\tilde{o}$, and a task language instruction $z$ on $\mathcal{D}_\text{path}$.
% \anqi{reminder on notation consistency for this dataset}. 
\method 's general path-conditioning framework allows for using arbitrary lower-level control policies as they do not need to condition on the same inputs as the VLM.
Therefore, we train 3D low-level policies, such as RVT-2~\citep{goyal2024rvt} and 3D-DA~\citep{ke20243d}, for low-level control.
Here, we assume $s$ includes additional sensor information (i.e., depth),
% \anqi{I think earlier in Section 3, $p$ is explicitly refered to as proprioceptive information, which by definition does not include perception input. Maybe explicitly adding depth or instead say that depth is also included in observation $o$?}
which 3D-DA and RVT-2 utilize to construct point clouds and virtual camera renderings, respectively, for more accurate control and data-efficient imitation learning.
We directly train these policies, with no necessary architectural changes,
%\footnote{We ignore the language instruction for RVT-2 it is already encoded in the 2D path.} 
with their supervised imitation learning objectives on $\mathcal{D}_\text{path}$
% \anqi{reminder on notation consistency for this dataset} 
to maximize log-likelihoods of the dataset actions: $\mathbb{E}_{(s_t, a_t, \tilde{o}_t, z_t)\sim \mathcal{D}_\text{path}} \log \pi_\theta(a \mid s_t, \tilde{o}_t, z_t)$. 
% \anqi{reminder on notation consistency for this dataset}
For further implementation details, see \Cref{sec:appendix:implementation}.



\textbf{Online Evaluation.}
% \label{sec:method:evaluation}
% Finally, we describe how we generate low-level actions with the \method-VLA architecture at inference time. 
Standard VLA architectures query the VLM for every low-level action~\citep{kim2024openvla, rt22023arxiv}, which can be very expensive with large VLMs---for example, OpenVLA's 7B param VLA only runs at 6Hz on an RTX 4090~\citep{kim2024openvla}.
Instead, \method 's hierarchical design allows us to query the VLM just once at the beginning of the episode to generate a 2D path $\hat{l}$ that we draw onto every subsequent image.
%\footnote{\method\ is not inherently limited to being queried once per episode, but for simplicity and computational efficiency we query just once per episode in our experiments.}
Therefore, \method\ can be scaled to large VLM backbones without needing end-users to be concerned about inference speed.

% % During downstream evaluation in the target environment, we query the \method\ VLM  once at the start of the episode to generate a 2D path given the first observation $o_1$. 
% These 2D paths are then drawn onto all future frames $\tilde{o}_{1:N}$ for $\pi_\theta$ as described in \Cref{sec:method:policy:training}. See \Cref{fig:method} for a visualization of the full online evaluation pipeline.

% To summarize, we finetune a VLM on cross-modal, cheaply obtained data from the internet to predict trajectory keypoints \anqi{2d paths} to annotate images for an image-conditioned policy to receive high-level trajectory guidance to perform low-level control. \anqi{Avoid one-sentence paragraphs.}
