\section{Introduction}
% VLM based learning methods
\begingroup
\renewcommand{\thefootnote}{}
\endgroup
Developing general robot manipulation policies has been notoriously difficult. With the advent of large vision-language models (VLMs) that display compelling examples of generalization, there is optimism that similar techniques can be helpful for robotic manipulation. A line of prior work~\citep{team2024octo, kim2024openvla, gu2023rttrajectory} builds open-world vision-language-action models (VLAs) by finetuning off-the-shelf, pretrained VLMs. \anqi{does rt-trajectory really fall under this category?} The recipe for training many of these VLA models has been to collect and curate a large-scale robotics-specific dataset, complete with images and corresponding on-robot actions\anqi{what about proprio}, and then finetune a VLM to directly produce actions~\citep{kim2024openvla,rt22023arxiv}. Such VLAs have shown robustness on simple tasks and controlled environmental variations. However, these models display limited generalization in terms of environment, object, task, and semantic variation. This issue could be attributed to the relative scarcity of diverse, in-domain training data\anqi{seems conflicting with previous sentence, claiming that they've already curated `large-scale' datasets}. The data needed to train these models is expensive since it requires end-to-end image-action pairs \anqi{isn't it the same for imitation learning models?} that must all be collected directly on-robot through an expensive medium such as teleoperation. A solution for training VLA models must be developed to instead learn from easy-to-collect ``cheap" sources of data. \anqi{should probably mention about 3D as well}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/robolines_teaser.pdf}
    \caption{\footnotesize{Overview of \method, VLAs and ``smaller" imitation learning methods. \method 's hierarchical design results in better generalization with a small amount of in-domain data. \method\ is able to utilize cheap training sources such as videos or simulations for enhanced generalization.}}
    \label{fig:teaser}
\vspace{-8mm}
\end{figure*}

% Imitation learning methods and their flaws
On the other hand, relatively ``small" \anqi{excessive use of quotation marks?} imitation learning models have shown impressive dexterity and geometric robustness. Such models have demonstrated promise across a range of complex tasks involving contact-rich manipulation and 3D reasoning, spanning domains from tabletop manipulation~\citep{shridhar2023perceiver,goyal2023rvt} to fine dexterous manipulation~\citep{zhao23aloha}\anqi{is aloha appropriate here? maybe?}. Trained on relatively small datasets, these models show local robustness and stable control \anqi{stable is vague} but typically lack semantic or visual generalization. They are often brittle to changes in the environment, semantic description of the tasks, or changes in the objects being manipulated~\citep{pumacay2024colosseum}. Reliable, generalizable robotic learning techniques must marry the generalization benefits of large VLMs, with the efficiency, local robustness and dexterity of small imitation learning policies, all while being able to train from abundant and cheap sources of data\anqi{seems over-claiming as the low-level policy is not trained with these cheap data}. In this work, we ask -- can we design VLA models that train on relatively abundant and cheap data sources\anqi{again, over-claiming}, showing broad visual and semantic generalization, while retaining the low-level geometric and 3D understanding displayed by small imitation learning models? \anqi{3D is not neccesarily guaranteed in ALOHA is an instance of a small model}

We propose that a hierarchical architecture for vision language action models, \method (\methodlong), can serve as an effective way to learn from abundant and inexpensive off-domain sources of data such as videos or simulation\anqi{Again, could be confusing with data source}. We study a family of \method s, where finetuned VLMs are connected to low-level 3D policy learning methods \anqi{3D policy seems to come out of nowhere} via intermediate 2D path representations. Since these 2D paths can easily be obtained in abundance from data sources such as videos or simulations (either with point tracking, hand-sketching, or proprioceptive projection)\anqi{cite}, these can be used to finetune the larger \anqi{? than what} higher-level VLM in \method. These 2D paths can then serve as guidance for a low-level policy that operates on rich 3D and proprioceptive inputs, alleviating the burden of long-horizon planning and semantic reasoning\anqi{this is important but not mentioned in the previous paragraph}, allowing low-level policies to focus on robustly generating precise, spatially-aware actions. 

Representations similar to 2D paths has been explored in the robot learning literature~\citep{gu2023rttrajectory}, primarily as a technique for flexible task specification. However, the key hypothesis explored in this paper is distinct -- we posit that using cheap data such as videos or simulation to finetune \emph{hierarchical} path generating VLMs can enable a surprising degree of cross-domain transfer \anqi{fact check this} as compared to the direct transfer of monolithic vision-language-action models ~\citep{brohan2022rt, kim2024openvla}. Here the focus is less on using paths as a scalable \anqi{?} technique for task specification, and more on using hierarchy as a mechanism for robust cross-domain transfer across settings with considerable visual and semantic differences. Specifically, we find that VLMs trained to predict 2D path representation can transfer to the real world from simulations that look very different from the real world\anqi{confusing sentence}, or across real-world scenarios with widely varying appearance\anqi{mention hand tracking?}. Hence, the hierarchical design of \method\ provides a way to utilize cheaper, but perceptually varying sources of ``off-domain'' data (such as simulation or cross-embodiment data) to benefit real-world control policies. 

% Ankit: Another option, although not very crisp and clear
The hierarchical design presented in \method\ can also offer additional advantages through the decoupling of VLM training and low-level action prediction. Specifically, since the higher-level VLM is predicting semantically meaningful trajectories from monocular RGB camera inputs, the lower-level control policies can operate from rich 3D and proprioceptive inputs. In doing so, \method\ inherits the semantic reasoning benefits of VLMs along with the 3D reasoning and spatial awareness benefits of 3D imitation learning policies ~\citep{goyal2024rvt, ke20243d}. Finally, since \method\ is built on both open-source VLMs and low-level policies, it can serve as a fully open-sourced enabler for the community-building vision-language-action models. \anqi{need merge with previous paragraph}

\anqi{mention viable action frequency of HAMSTER and monolithic VLA model, resp.}