\section{\method: Hierarchical Action Models for Robotic Learning}
\label{sec:method}
\begin{figure*}[!t]
    \centering
    %\includegraphics[width=0.9\linewidth]{figs/robolines_method.pdf}
    \includegraphics[width=0.85\linewidth]{figs/hamster_method_new.pdf}
    \caption{\footnotesize{Depiction of \method 's execution. The high-level VLM is called once to generate the 2D path. The low-level policy is conditioned on the 2D path and interacts with the environment sequentially to execute low-level actions. The path predicted by the VLM enhances the low-level policy generalization capability.}
    }
    \vspace{-4mm}
    \label{fig:method}
\end{figure*}
In this work, we examine how VLA models can leverage relatively abundant data and demonstrate cross-domain transfer capabilities, as opposed to relying purely on expensive observation-language-action data collected on a robot. \method\ is a family of hierarchical VLA models designed for this purpose, exhibiting generalizable and robust manipulation. It consists of two interconnected models: first, a higher-level VLM that is finetuned on large-scale, off-domain data to produce intermediate 2D path guidance (detailed in \Cref{sec:method:vlm}), and second, a low-level policy that produces actions conditioned on 2D paths (detailed in \Cref{sec:method:policy}).

The primary advantages of finetuning such a hierarchical VLM that produces intermediate representations as opposed to directly producing actions $a$ with a monolithic model~\citep{kim2024openvla, zitkovich2023rt,black2024pi0} are threefold: \emph{1)} our hierarchical VLM can leverage off-domain datasets lack of precise actions, e.g., simulation and videos; \emph{2)} we find empirically that hierarchical VLMs producing 2D paths generalize more effectively cross-domain than monolithic VLA models; and \emph{3)} the hierarchical design provides more flexibility on the sensory modality, and allows for asynchronous query of large high-level VLA models and small low-level policy models.


\subsection{\method's VLM for producing 2D Paths Trained from Off-Domain Data}
\label{sec:method:vlm}


The high-level VLM of \method\ predicts a coarse 2D path $p$ to achieve the task given a monocular RGB image $\tt img$ and language instruction $z$, i.e., $\hat p \sim {\tt VLM}({\tt img},z)$. The 2D path $p$ describes a coarse trajectory of the robot end-effector, or human hand in the case of human videos, on the input camera image. It also contains information about the gripper state. Formally, the 2D path is defined as $p = [(x_t, y_t, \texttt{gripper\_open}_t)]_t$ where $x_t, y_t \in [0, 1]$ are \emph{normalized pixel locations} of the end effector's (or hand) position at step $t$, and $\texttt{gripper\_open}_t$ is a binary value indicating the gripper state, i.e., open and close. 

Although, any pretrained text-and-image-input VLM~\citep{vila2024, liu2024visual,openai2024gpt4} can be used to predict such a 2D path by casting an appropriate prompt, we find that pre-trained VLMs struggle with predicting such a path in a zero-shot manner (see \Cref{tab:experiments:vlm}). Therefore, we finetune pre-trained VLMs on datasets that ground VLMs to robot scenes and path predictions collected from easier-to-obtain sources, i.e., internet visual-question-answering data, robot data from other modalities, and simulation data. This is in contrast to work such as ~\cite{gu2023rttrajectory}, where pre-trained VLMs are tasked with directly performing spatially relevant path generation.

We use VILA-1.5-13b~\citep{vila2024} as our base VLM, a 13-billion-parameter vision language model trained on interleaved image-text datasets and video captioning data. Although it is possible to curate a dataset on path prediction $\{({\tt img}_i,z_i,p_i)\}_i$ and train the VLM \emph{only} on the dataset, the literature~\citep{rt22023arxiv,yuan2024robopoint} has shown that \emph{co-training} the VLM on a variety of relevant tasks, all framed as VQA tasks, can help retain the VLM's generalization capability. 
To this end, we curate a multi-domain dataset to finetune this model for effective 2D path prediction. 

\subsubsection{Finetuning Objective and Datasets.}% \label{sec:method:vlm:datasets}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/dataset_mix.pdf}
\caption{\footnotesize\textbf{Off Domain Training Data}: $\mathcal{D}_\text{off}$ contains (a) Pixel Point Prediction: 770k object location tasks from RoboPoint. (b) Simulated Robot Data: 320k 2D end-effector paths from RLBench environment. (c) Real Robot Data: 110k 2D end-effector paths from Bridge and DROID trajectories.}

    \label{fig:experiments:training_data}
\end{figure}
Predicting the 2D path of the end-effector requires understanding \emph{what} objects to manipulate in a given task in terms of their pixel positions, but also reasoning about \emph{how} a robot should perform the task. To enable this understanding, we collate a diverse off-domain dataset $\vlmdata$ from a wide range of modalities, including real-world data, visual question-answering data, and simulation data. Importantly, \emph{none} of this off-domain data used to train the VLM comes from the deployment environment, thereby emphasizing generalizability. 

We assemble a dataset ${\mathcal{D}}_\text{off} = \{({\tt img}_i, z_i, {\tt ans}_i)\}_{i=1}^M$ of image inputs ${\tt img}_i$, language prompts $z_i$, and answer ${\tt ans}_i$ consisting of three types of \emph{off-domain} data: (1) pixel point prediction tasks (\emph{what}); (2) simulated robotics tasks (\emph{what and how}); (3) a real robot dataset consisting of trajectories (\emph{what and how}). 
We detail each dataset below; see \Cref{fig:experiments:training_data} for visualization of each dataset's prompts and corresponding answers. 

\textbf{Pixel Point Prediction.} For pixel point prediction, we use the RoboPoint dataset~\citep{yuan2024robopoint} with 770k pixel point prediction tasks, with most answers represented as a list of 2D points corresponding to locations on the image. A sample consists of a prompt $z$ like $\texttt{Locate object between the marked items}$, an input image $\tt img$ and answer $\tt ans$ like \texttt{$[(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)]$}.\footnote{Note that this is not a temporally ordered path, but rather a set of unordered points of interest in an image.} See the left of \Cref{fig:experiments:training_data} for an example. This dataset consists of data automatically generated in simulation and collected from existing real-world datasets; its diverse tasks 
enable the \method\ VLM to reason about pixel-object relationships across diverse scenes while retaining its semantic generalization capabilities.

\textbf{Simulated Robot Data.} We additionally generate a dataset of simulated robotics tasks from RLBench~\citep{james2020rlbench}, a simulator of a Franka robot performing tabletop manipulation for a wide array of both prehensile and non-prehensile tasks.
We use the simulator's built-in planning algorithms to automatically generate successful manipulation trajectories. Given a trajectory, we use the first frame from the front camera as the image input $\tt img$. We construct prompt $z$ to instruct the VLM to provide a sequence of points denoting the trajectory of the robot gripper to achieve the given language instruction (see Figure~\ref{fig:method}). The ground-truth 2D path $p = [(x_t, y_t, \texttt{gripper\_open}_t)]_t$ is given by propriceptive projection using forward kinematics and camera parameters. 

We generate $1000$ episodes for each of $81$ robot manipulation tasks in RLBench, each episode with $\sim$4 language instructions, for a total of around $320k \,({\tt img}, z, {\tt ans}) $ tuples, where ${\tt ans}=p$. See the middle of \Cref{fig:experiments:training_data} for an example. %  for $\Tilde{\vlmdata}$.


\textbf{Real Robot Data.} Using real robot data allows us to ensure the VLM can reason about objects and robot gripper paths when conditioned on scenes, including real robot arms.
We use existing, online robot datasets \emph{not from the deployment environment} to enable this VLM ability.
We source 10k trajectories from the Bridge dataset~\citep{walke2023bridgedata, open_x_embodiment_rt_x_2023} consisting of a WidowX arm (different embodiment from test robot) performing manipulation tasks and around 45k trajectories from DROID~\citep{khazatsky2024droid}. 
We covert both datasets to VQA dataset in as similar way as the simulated RL-Bench data, where the 2D paths are extracted from proprioception and camera parameters (see the right of \Cref{fig:experiments:training_data} for an example).
% For both datasets, we use the given end-effector trajectories and given (or estimated) camera parameters to convert robot gripper trajectories to 2D paths $p$. We use a camera image from the first timestep of each robot trajectory as $o^o$ and a similar text prompt $z^o$ as the simulation dataset. 
Note that we essentially utilize the robot data as video data, where the end effector is tracked over time. In principle, this could be done with any number of point-tracking methods~\citep{doersch2023tapir} on raw video as well, with no action or proprioceptive labels.

We finetune the \method\ VLM on all three types of data by randomly sampling from all samples in the entire dataset with equal weight. We also include a 660k-sample VQA dataset~\citep{liu2024improved} for co-training to preserve world knowledge. We train with the standardized supervised prediction loss to maximize the log-likelihood of the answers $\tt ans$:
% \anqi{ground truth labels or 2D points/paths labels? With language, one may confuse it with $z$.}:
%\begin{equation*}    
$\mathbb{E}_{({\tt img}_i, z_i, {\tt ans}_i) \sim \vlmdata}  \log \text{VLM} \left( {\tt ans}_i \mid {\tt img}_i, z_i\right)$. 
%\end{equation*}

% \subsubsection{VLM Training}

%\jz{Yi fill in weighting scheme} \yi{most of the strategy is similar to sft step in VILA. we use VILA1.5-13B as pretrained weight which uses Vicuna-1 as LLM and siglip-so400m-patch14-384 as vision encoder. 
%We finetune both LLM and vision encoder. since we are using equivalent batch size 256 instead of 2048, we use LR=1e-5 instead of 1e-4 to maintain stable loss curve. It takes 30 hours on an 8xA100 node.} 
%sampling weights with the standard VLM training objective of minimizing the negative log-likelihood of the answer tokens $p_\text{ans}$ conditioned on prompt tokens $p_\text{prompt}$ and an input image $o$:
\textbf{Remark.} One issue with simulation and real robot data is that the extracted 2D paths $p$ can be extremely long, e.g., exceeding one hundred steps. 
Since we want the \method\ VLM to reason at a \emph{high level} instead of on the same scale as the low-level control policy, 
we simplify the paths $p^o$ with the Ramer-Douglas-Peucker algorithm~\citep{RAMER1972244, douglas_pecker_1973} that reduces curves composed of line segments to similar curves composed of fewer points. We refer readers to \Cref{sec:rdp_vs_20p} for an ablation study.
% This results in an average 2D path length consisting of \jz{yi insert} points.

\subsection{Path Guided Low-Level Policy Learning}
\label{sec:method:policy}
The low-level policy of \method\ $\pi_\theta (a\mid s, o, z, p)$ is conditioned on proprioceptive and perceptive observations, (optional) language instruction and, importantly, 2D path. 
% 
% After training the \method\ VLM to predict paths, we train a low-level policy to utilize these paths to predict actions\anqi{``after''?}. 
% 
While a low-level control policy \emph{can} learn to solve the task without 2D path, % providing it with 2D paths can make the task easier\anqi{unnecessary sentence?}. T
the paths allow the low-level policy to forgo long-horizon and semantic reasoning and focus on local and geometric predictions to produce robot actions. As we find empirically (see \Cref{fig:experiments:main_exp}), 2D paths allow for considerably improved visual and semantic generalization of low-level policies. 

\method's general path-conditioning framework allows lower-level policies to take in proprioceptive and perceptual (e.g., depth images) observations, that are not input to the high-level VLM. 
% 
We consider low-level policies based on 3D perceptual information, i.e., $o=({\tt{img}}, {\tt{pointcloud}})$, available at test time on a robotic platform with standard depth cameras. We study two choices of policy architecture, RVT-2~\citep{goyal2024rvt} and 3D-DA~\citep{ke20243d} which has shown state-of-the-art results on popular robot manipulation benchmark~\citep{james2020rlbench}.

% \textbf{Conditioning on Paths.} Most existing policy architectures are in the form of $\pi_\theta (a\mid s, o, z)$, without taking 2D paths as an input. 
% % 
% Then the question becomes -- how do we incorporate 2D path information $\hat{p}$ produced by the VLM onto the input of the low-level policy, without major modifications to existing policy architectures? 
% One way is to concatenate the path with the proprioception or language input. Since 2D paths are of varied lengths, this requires the underlying policy architecture to be able to take in varied-length proprioceptive or language instructions. 
% % 
% Another possibility is to draw the 2D path points onto the image to the policy~\citep{gu2023rttrajectory}. With this design choice, 2D paths can be directly added to the image observation without any changes to policy architectures. % We also hypothesize that path drawings provide easier-to-follow path guidance as the policy does not have to learn how to associate path points with their corresponding image locations. 
% Our implementation of drawing conditioning large follow \citet{gu2023rttrajectory}'s method 
% % proposed using colored trajectories to guide a policy's actions, and we largely follow their method of 
% of coloring trajectories to indicate gripper status and progression through time. 
% These paths are drawn onto all images in the trajectory $o_i^1...o_i^T$ by drawing points at each $(x_t, y_t)$ and connecting them with line segments.
% We use a color gradient to indicate progression through time (see \Cref{fig:method}(b) for an example).
% We plot circles for change in gripper status: e.g., \textcolor{green}{green} for closing the gripper and \textcolor{blue}{blue} for opening.
% % This constructs the final in-domain path-labeled dataset $\mathcal{D}_\text{path} = \{ (s_i, a_i,\tilde{o}_i, z_i)\}_{i=1}^N$.
% % \anqi{Earlier in 4.1, $\mathcal{D}$ is used to also refer to the path-labeled dataset, and I suggested to change it to $\Tilde{\mathcal{D}}$. Either way, we need to make it consistent.}
% % 
% We empirically study the above two choices of path conditioning, i.e., concatenation and drawing, in \cref{sec:appendix:additional_ablations}.

\textbf{Conditioning on Paths.} Most policy architectures use the form \(\pi_\theta(a \mid s, o, z)\) without 2D path inputs. One na\"ive option is to concatenate the path with proprioceptive or language inputs. However, because 2D paths vary in length, the architecture must handle variable-length inputs. To incorporate the 2D path \(\hat{p}\) from the VLM without major modifications, we alternatively overlay the 2D path onto the image observation~\citep{gu2023rttrajectory}. Our implementation follows this approach by drawing colored trajectories on all images in the trajectory \(o_i^1, \ldots, o_i^T\): points at each \((x_t, y_t)\) are connected with line segments using a color gradient to indicate temporal progression (see \Cref{fig:method}(b)), and circles mark changes in gripper status (e.g., \textcolor{green}{green} for closing, \textcolor{blue}{blue} for opening). If the policy architecture allows images with more than three channels, we can also include path drawing as separate channels, instead of overlaying it on the RGB channel. We empirically study both drawing strategies, overlay and concatenating channels, in \cref{sec:appendix:additional_ablations}.

%$ = \{ (s_i, a_i, o_i, z_i) \}_{i=1}^N$ 

%\ag{TODO: we need to describe this more formally and potentially for both 3D-DA type methods and for RVT style methods}
%\ag{We need to argue why this is better than just concatenating etc}

%Describe the shallow head architecture and how exactly the trajectory is fed into the low-level policy (image overlay). Describe how this can be trained on in-robot data. I wouldn't make it too specific to RVT or 3DDA or anything. Just as a general low-level policy. 

%\subsubsection{Training the Policy}
\textbf{Policy Training.} 
To train the policy, we collect a relatively small-scale task-specific dataset $\mathcal{D}=\{(s_i, o_i, z_i, a_i)\}_{i=1}^N$ on the robot hardware. 
During training, we use \emph{oracle} 2D paths constructed by proprioception projection, similar to how the 2D paths are constructed for the VLM training data, and construct path-labeled dataset $\mathcal{D}_\text{path}=\{(s_i, o_i, z_i, p_i, a_i)\}_{i=1}^N$. % as described for simulation and real robot data \anqi{do we use bridge or droid for training the low-level policy?}. % in \Cref{sec:method:vlm:datasets}.
% 
% Formally, we iterate 
% \anqi{need to clarify how the paths are obtained. Is it through path-labeler or VLM? Need to use $\hat{l}_i$ later on if we use VLM predictions.} 
% through each trajectory $\tau_i = \{s_i^t, a_i^t, o_i^t, z_i)\}_{t=1}^T$ on the in-domain dataset $\mathcal{D}$ to obtain the path $p_i$. 
% \anqi{somehow similar confusion here} describing $\tau_i$. 
% 
We train a policy $\pi_\theta(a \mid s, {o}, z, p)$ 
% \anqi{is it conditioned on $z$ or $l$? Isn't $l$ given by $\tilde{o}$ already?} 
% conditioned on proprioception $s$ and path-annotated image observations $\tilde{o}$, and a task language instruction $z$. %  on $\mathcal{D}_\text{path}$. We directly train these policies, with no necessary major architectural modifications,%\footnote{We ignore the language instructions for real-world experiments as the task information is fully encoded in the 2D path.} 
with standard supervised imitation learning objectives on $\mathcal{D}_\text{path}$
% \anqi{reminder on notation consistency for this dataset} 
to maximize the log-likelihood of the dataset actions: $\mathbb{E}_{(s_i, o_i, z_i, p_i, a_i)\sim \mathcal{D}_\text{path}} \log \pi_\theta(a_i \mid s_i, o_i, z_i, p_i)$. 
For further implementation details, see \Cref{sec:appendix:implementation}.



\textbf{Inference Speed.}
% \label{sec:method:evaluation}
% Finally, we describe how we generate low-level actions with the \method-VLA architecture at inference time. 
Monolithic VLAs query the VLM at every action step~\citep{kim2024openvla, rt22023arxiv}, which can be very expensive with large VLMs. For example, OpenVLA's 7B-parameter VLA only runs at 6Hz on an RTX 4090~\citep{kim2024openvla}.
Instead, \method 's hierarchical design allows us to query the VLM only one or few times during an episode to generate 2D paths $\hat{p}$ that can be followed by low-level policy for multiple steps. % \footnote{\method\ is not inherently limited to being queried once per episode, but for simplicity and computational efficiency we query just once per episode in our experiments.}
Therefore, \method\ can be scaled to large VLM backbones without needing end-users to be concerned about inference speed.

% % During downstream evaluation in the target environment, we query the \method\ VLM  once at the start of the episode to generate a 2D path given the first observation $o_1$. 
% These 2D paths are then drawn onto all future frames $\tilde{o}_{1:N}$ for $\pi_\theta$ as described in \Cref{sec:method:policy:training}. See \Cref{fig:method} for a visualization of the full online evaluation pipeline.

% To summarize, we finetune a VLM on cross-modal, cheaply obtained data from the internet to predict trajectory keypoints \anqi{2d paths} to annotate images for an image-conditioned policy to receive high-level trajectory guidance to perform low-level control. \anqi{Avoid one-sentence paragraphs.}


% ------------------------------
% \textbf{Problem Definition.}
% Rather than operating in the pure imitation learning setting as described in \Cref{sec:prelim}, we study a scenario where cross-domain data is utilized to train VLA models. While the typical imitation learning setting uses a dataset of optimal in-domain, on-robot tuples $\mathcal{D} = \{(s_t, a_t, o_t, z_t)\}_{t=1}^N$ to learn a near-optimal policy $\pi_\theta$, in this setting we additionally assume access to a much larger dataset(s) of ``off-domain" approximately optimal data $\vlmdata = \{(o_i^o, z_i^o)\}_{i=1}^M$, where $M \gg N$, such as video or simulation data. This ``off-domain" \anqi{why quotation marks?} data $\vlmdata$ is \anqi{can be?} different from in-domain data $\mathcal{D}$ in several important ways: \emph{1)} Off-domain perceptual observations $o_i^o$ may be considerably different than in-domain perceptual observations $o_i$, even when the underlying physical state of the system is similar.
% % \footnote{Viewed from the lens of a partially observable Markov decision process, this involves differing emission functions across domains.}.
% An illustrative example of this is the marked difference between simulation and real-world scene appearance (see ~\Cref{fig:vlm_generalization}). \emph{2)} The underlying physical dynamics of the system can be potentially different, i.e., the transition dynamics 
% % $\mathds{P}(s'|s, a)$ between states $s', s$   # unncessary
% may be different between off-domain sources such as video or simulation than the test-time deployment setting\anqi{embodiment difference, for example}. While the dynamics may show level differences, we assume the higher-level coarse strategies to solve the task remain invariant. \emph{3)} Off-domain data may not have access directly to actions $a$ or proprioception state $s$\anqi{may not be even available}, for instance in video based datasets. This poses challenges to directly applying the standard imitation learning paradigm for these datasets.\anqi{hand?}

% The goal is to leverage the combination of a small amount of ``expensive" in-domain data $\mathcal{D}$ and a large amount of relatively ``cheap" off-domain data $\vlmdata$ to obtain a generalizable policy $\pi_\theta$ that can be successfully deployed over various initial conditions, task variations, and visual variations in the in-domain robot environment. Without additional assumptions, this problem is arduous due to the lack of alignment between the in-domain and off-domain settings. In this work, we assume access to an intermediate \emph{path-labeler} $p_i = h(o_i, z_i)$ at training time, that accepts an observation $o_i$ and a language instruction $z_i$ from either the off-domain or in-domain datasets, to produce an intermediate path label $p_i$ that indicates \emph{how} to optimally perform the task $z_i$ from the observation $o_i$. In this work, we choose this intermediate path label $p_i$ to be a sequence of points, a 2D path, on the image that indicates coarse end-effector motion to solve the designated task. 
% % \anqi{Should mention that it additionally include gripper actions. And ideally refer to a figure.} 
% This path-labeler at training time can come from different sources --  a projection of known proprioception if available, human-drawn trajectory annotations on images, point-tracked end-effector or hand positions from video, and so on. Applying such a path labeler to the off-domain dataset yields $\Tilde{\vlmdata} = \{(o_i^o, z_i^o, p_i^o)\}_{i=1}^M$. \anqi{$i$ is the indexing of trajectories here?}
% % while applying it to the in-domain dataset correspondingly yields $\Tilde{\mathcal{D}} = \{(s_i, a_i, o_i, z_i, p_i)\}_{i=1}^N$.
% % \anqi{a small detail, for in-domain dataset, shouldn't eef projection be sufficient?}
% % \jz{the path labeler $h$ notation is incorrect and doesn't align with the method description. notation here needs to change and be incorporated in \Cref{sec:method:policy:training}}\anqi{Ideally, we may actually want to use a different notation for datasets with path labels and without, e.g., $\mathcal{D}_\text{off}$ and $\Tilde{\mathcal{D}}_\text{off}$.}

% % In the following sections, we propose and discuss a family of VLA architectures that finetune high-level VLMs to produce the above mentioned path labels $p_i$ from off-domain data $\Tilde{\vlmdata}$, that can then provide guidance to low-level policies trained from in-domain data $\mathcal{D}$. Importantly, we show that these types of architectures enjoy the ability to train on more easily available data sources such as video or simulation and demonstrate significant cross-domain generalization. 