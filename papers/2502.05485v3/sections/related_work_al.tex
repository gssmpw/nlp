\section{Related Work}\label{sec:related_work}
\textbf{LLMs and VLMs for robotics.} 
Early attempts in leveraging LLMs and VLMs for robotics are through pretrained language~\citep{jang2022bc,shridhar2023perceiver,singh2023progprompt} and visual~\citep{shah2021rrl,parisi2022unsurprising,nair2023r3m,ma2023vip} representations. However, these are not sufficient for complex semantic reasoning and generalization to the open world~\citep{brohan2022rt,zitkovich2023rt}. Recent research has focused on directly leveraging open world reasoning and generalization capability of LLMs and VLMs, by 
prompting or fine-tuning them to, e.g., generate plans~\citep{huang2022language,huang2023inner,lin2023text2motion,liang2023code,singh2023progprompt,brohan2023can}, construct value~\citep{huang2023voxposer} and reward functions~\citep{kwon2023reward,RoboCLIP,yu2023language,ma2024eureka,wang2024rl}. Our work is more closely related to the literature on VLA models, summarized below.

\textbf{Monolithic VLA models as language-conditioned robot policies.}
Monolithic VLA models have been proposed to produce robot actions given task description and image observations directly \citep{brohan2022rt,jiang2023vima,zitkovich2023rt,team2024octo,kim2024openvla,radosavovic2023robot}. Monolithic VLA models are often constructed from VLMs~\citep{liu2024visual,bai2023qwen,driess2023palm,vila2024}, and are trained on large-scale on-robot data~\citep{brohan2022rt,open_x_embodiment_rt_x_2023,khazatsky2024droid} to predict actions as text or special tokens. However, due to the lack of coverage in existing robotics datasets, they must be finetuned in-domain on expensive on-robot data. Their action frequency is also constrained by inference frequency, limiting their capability to achieve dexterous and dynamic tasks. The most relevant monolithic VLA model to our work is LLARVA~\citep{niu2024llarva}, which predicts end-effector trajectories in addition to robot actions. However, LLARVA does not use trajectory prediction to control the robot; rather, it uses it as an auxiliary task to improve action prediction. Therefore, LLARVA still suffers from the limitations of monolithic VLA models. In contrast, our work takes a hierarchical approach, enabling us to use specialist lower-level policies that take in additional inputs the VLMs cannot support, such as 3D pointclouds, to enable better imitation learning. Our predicted paths then enable these lower-level policies to generalize more effectively.

\textbf{VLMs for predicting intermediate representations.}
Our work bears connections to prior methods using vision-language models to predict intermediate representations. These methods can be categorized by the choice of predicted representations:

% \anqi{I actually feel  that we should uncomment the previous sentence to so that we don't have a one-sentence paragraph.}

\emph{Point-based predictions:} A common intermediate prediction interface has been keypoint affordances~\citep{stone2023open,sundaresan2023kite,nasiriany2024pivot,yuan2024robopoint,kuang2024ram}. Keypoint affordances can be obtained through using open-vocabulary detectors~\citep{minderer2022simple}, iterative prompting of VLMs~\citep{nasiriany2024pivot}, or fine-tuning detectors to identify certain parts of an object by semantics~\citep{sundaresan2023kite}. Perhaps most related to our work, \cite{yuan2024robopoint} finetune a VLM to predict objects of interest as well as free space for placing an object, and \cite{liu2024moka} propose a mark-based visual prompting procedure to predict keypoint affordances as well as a fixed number of waypoints. As opposed to these, our work finetunes a VLM model to not just predict points but rather entire 2D paths, making it more broadly applicable across robotic tasks. % \yi{it is not easy to add new images while controling the number of pages.} \anqi{hopefully shown in the wiping task}

\textbf{Trajectory-based predictions:} The idea of using trajectory-based task specifications to condition low-level policies was proposed in RT-trajectory~\citep{gu2023rttrajectory}, largely from the perspective of flexible task specification. This work also briefly discusses the possibility of combining trajectory-conditioned model with trajectory sketches generated by a pre-trained VLM. Complementary to RT-Trajectory, the focus of this work is less on the use of trajectory sketches for task specification, but rather a hierarchical design of VLAs such that the high-level VLM can be fine-tuned with relative cheap and abundant data sources. This could include data such as action-free videos, or simulation data that look very different from the real world. We show that the emergent generalization capability of VLMs from its web-scale pretraining allows it transfer to test scenarios of interest with considerable visual and semantic variations.  While RT-trajectory uses human effort or off-the-shelf pre-trained VLMs to generate trajectories, we show that fine-tuning VLM models on cheap data sources can generate significantly more accurate and generalizable trajectories (see Table.~\ref{tab:experiments:vlm}).  Moreover, our instantiation of this architecture enables the incorporation of rich 3D and proprioceptive information, as compared to monocular 2D policies~\citep{gu2023rttrajectory}. 

\textbf{Leveraging simulation data for training robot policies.}
There has been extensive work on leveraging simulation for robot learning. Simulation data is popular in reinforcement learning (RL), as RL on real robotic systems is often impractical due to high sample complexity and safety concerns~\citep{lee2020learning,handa2023dextreme,torne2024reconciling}. Recently, simulation has been also exploited to directly generate~\citep{fishman22mpn} or bootstrap~\citep{mandlekar2023mimicgen} large-scale datasets for imitation learning, to reduce the amount of expensive robot teleoperation data needed. Our work takes a different approach -- using simulation data to finetune a VLM, and showing that VLM is able to transfer the knowledge learned from simulation data to real robot systems, despite considerable visual differences. A related observation is recently made by~\citep{yuan2024robopoint}, but they use keypoint affordances as the interface between the VLM and the low-level policy as opposed to more general expressive 2D path representations.

% ====== AL old text ==== AG edit ==== start 
% \textbf{Hierarchical VLA models as language-conditioned robot policies.}
% Hierarchical VLA models interface VLMs and low-level policies through intermediate representation which summarizes VLM's reasoning. The benefits of the hierarchical model are \emph{1)} the intermediate representation can be obtained through either directly prompting the VLMs~\citep{stone2023open,nasiriany2024pivot} or fine-tuning the VLMs with non-robot data or cheap, e.g., actionless or simulated, robot data~\citep{yuan2024robopoint}; and \emph{2)} the low-level policy can incorporate sensory inputs that are not straightforward to be leveraged by VLMs such as 3D point clouds~\citep{sundaresan2023kite,yuan2024robopoint}. One common intermediate representation is keypoint affordances~\citep{stone2023open,sundaresan2023kite,nasiriany2024pivot,yuan2024robopoint}. \anqi{details below, can be omitted to save space} \cite{stone2023open} obtain keypoints through directly prompting an open-vocabulary object detector~\cite{minderer2022simple}. \cite{nasiriany2024pivot} propose an iterative prompting mechanism to refine keypoint predictions given by VLMs. ~\cite{sundaresan2023kite} fine-tune an open-vocabulary object detector to identify certain parts of an object by semantics, e.g., the trunk of an elephant toy. \cite{yuan2024robopoint} leverage simulation data to instruct-tune a VLM to predict objects of interest as well as free space for placing an object. \cite{liu2024moka} propose a mark-based visual prompting procedure to predict keypoint affordances as well as a fixed number of waypoints. 

% The main challenges for using keypoint as intermediate representation are \emph{1)} it is difficult to handle nonprehensile tasks; and \emph{2)} multiple keypoint queries are required for long-horizon tasks, making the model subject to cascading errors~\citep{sundaresan2023kite}. Different from existing approaches, our VLM predicts \emph{trajectories} of the end-effector of the robot. Our approach inherents the benefits of hierarichical VLA models, while providing broader support on the type and horizon of tasks.

% \textbf{Trajectory-conditioned robot policies.}
% RT-trajectory~\citep{gu2023rttrajectory} is highly related to our work, as it also proposes to train a trajectory-conditioned policy.~\cite{gu2023rttrajectory} show that the trained policy can be conditioned on human sketches and human hand demonstrations. They also lightly explore the possibility of combining RT-Trajectory with trajectories generated from prompting a VLM. Complementary to RT-Trajectory, our work focuses less on the trajectory-conditioned policy, but instead study how to construct a hieraricical VLA, such that the VLM can provide meaningful guidance to trajectory-conditioned policies. We show that our approach significantly outperform prompting a state-of-the-art VLM model with various strategies \anqi{including code-as-policy used by~\cite{gu2023rttrajectory}}. Additionally, we find out that it is crucial to incorporate language instructions in the trajectory-conditioned policy, as the language instruction helps to resolve ambiguities and fix inaccuracy in the predicted trajectories. This observation is only made possible as we study the VLM and the trajectory-conditioned policy jointly, instead of focusing only on training a trajectory-following policy. \anqi{is it true??}

% With the emergent of track any-point (TAP) models~\citep{doersch2023tapir,wang2023tracking}, researcher has also explored policies that are conditioned on future trajectories of objects~\citep{yuan2024general,xu2024flow,bharadhwaj2024track2act} and points sampled from a fixed grid on the image~\citep{wen2023any}. While our current formulation uses end-effector trajectories, it is possible to extend our formulation such that the VLM predicts, e.g., objects trajectories, instead. 

% ====== AL old text ==== AG edit ==== end 

% \paragraph{Other hierarchical robot policies.}
% \textbf{HITL-TAMP~\citep{garrett2021integrated,mandlekar2023human}}: hierarchical policy that uses Task and Motion Planning (TAMP) at the high level and BC agents at the low level


% \textbf{3D Imitation Learning Methods}
% Imitation learning methods for manipulation that utilize explicit 3D scene representations have proven to be both data-efficient and robust to geometric variations~\cite{shridhar2023perceiver}. One example is PerAct~\cite{shridhar2023perceiver}, which employs a voxel-based scene representation processed by a perceiver transformer. RVT~\cite{goyal2023rvt} introduced a "virtual" multi-view representation, resulting in significant improvements in speed and performance over voxel-based approaches. Building on this, RVT-2~\cite{goyal2023rvt} further improved precision with a multi-stage design, achieving state-of-the-art results. At the same time, Act3D~\cite{gervet2023act3d} and 3D Diffuser Actor~\cite{ke20243d} proposed an alternative strategy by projecting features from pre-trained RGB models and processing them as feature point clouds. \method capitalizes on the advantages of these 3D techniques while enabling superior generalization in open-world scenarios.
