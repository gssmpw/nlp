%\setcounter{section}{0}
%\renewcommand{\thesection}{A}
%\section{Appendix}
%\setcounter{table}{0}
%\renewcommand{\thetable}{A\arabic{table}}
%\setcounter{figure}{0}
%\renewcommand{\thefigure}{A\arabic{figure}}

\appendix
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/robolines_training_data_2.pdf}
    \caption{(a): Examples of training data in $\mathcal{D}_\text{off}$ used to train \method's VLM. (b): The data used to train \method's low-level policies.
    % \ankit{show one example of each of the training data domain used in training the VLM (3 images like in the text); show a couple of examples of the training data used for training the policy. show the input/output for both cases.}
    }
    \label{fig:appendix:training_data}
\end{figure}
%For extended supplementary details and results, please see \url{https://sites.google.com/view/hamster-iclr}.
\section{VLM Finetuning Dataset Details}
\label{sec:appendix:vlm_training_details}

\paragraph{Pixel Point Pred Data.} Our point prediction dataset comes from Robopoint~\citep{yuan2024robopoint}. 
770k samples in our point prediction dataset contain labels given as a set of unordered points such as $p^o = [(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)]$, or bounding boxes in $[(cx, cy, w, h)]$ style. Other than that, following Robopoint~\citep{yuan2024robopoint}, we use the VQA dataset~\cite{liu2024improved} with 660k samples which answer VQA queries in natural language such as ``What is the person feeding the cat?'' 
We keep these data as is because these VQA queries are likely to benefit a VLM's semantic reasoning and visual generalization capabilities; we fine-tune \method 's VLM on the entire Robopoint dataset as given.

\paragraph{Simulation Data.}  We selected 81 RLBench tasks out of 103 to generate data by removing tasks with poor visibility on the \texttt{front\_cam} view in RLBench. We use the first image in each episode combined with each language instruction. The final dataset contains around 320k trajectories.
%The output of this process is the prediction of a simplified 2D trajectory representing the end-effector's movement, starting when the gripper is in a closed state.
\paragraph{Real Robot Data.}
For the Bridge~\citep{walke2023bridgedata} dataset, which only provides RGB images, we extract trajectories by iteratively estimating the extrinsic matrix for each episode. In each scene, we randomly sample a few frames and manually label the center of the gripper fingers. Using the corresponding end-effector poses, we compute the 3D-2D projection matrix with a PnP (Perspective-n-Point) approach. We then apply this projection matrix to the episodes and manually check for any misalignments between the projected gripper and the actual gripper. Episodes exhibiting significant deviations are filtered out, and a new round is started to estimate their extrinsic matrix.

For DROID~\citep{khazatsky2024droid}, a large portion of the dataset contains noisy camera extrinsics information that do not result in good depth alignment.
Therefore, we filter out trajectories with poor-quality extrinsics as measured by the alignment between the projected depth images and the RGB images. 
This results in $\sim$45k trajectories ($\sim$22k unique trajectories as trajectories each have 2 different camera viewpoints) which we use for constructing the VLM dataset $\vlmdata$ as described in \Cref{sec:method:vlm}.

\section{Implementation and Architecture Details}
\label{sec:appendix:implementation}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figs/robolines_training_data.pdf}
%     \caption{(a): Examples of training data in $\Tilde{\mathcal{D}}_\text{off}$ used to train \method's VLM. (b): The data used to train \method's low-level policies.
%     % \ankit{show one example of each of the training data domain used in training the VLM (3 images like in the text); show a couple of examples of the training data used for training the policy. show the input/output for both cases.}
%     }
%     \label{fig:experiments:training_data}
% \end{figure}

\begin{figure}[h]
\small
\begin{mdframed}[frametitle=\method \ Prompt, frametitlealignment=\centering,]
In the image, please execute the command described in $\langle$quest$\rangle$\{quest\}$\langle/$quest$\rangle$.

Provide a sequence of points denoting the trajectory of a robot gripper to achieve the goal.

Format your answer as a list of tuples enclosed by $\langle$ans$\rangle$ and $\langle/$ans$\rangle$ tags. For example:

\texttt{
$\langle$ans$\rangle$[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), 
$\langle$action$\rangle$Open Gripper$\langle/$action$\rangle$, 
(0.74, 0.21), 
$\langle$action$\rangle$Close Gripper$\langle/$action$\rangle$, ...]$\langle/$ans$\rangle$
}

The tuple denotes the $x$ and $y$ location of the end effector of the gripper in the image. The action tags indicate the gripper action.

The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.
\end{mdframed}
\caption{The full text prompt we use to train \method\ with on simulation and real robot data (\Cref{sec:method:vlm}). We also use this prompt for inference.}

\label{fig:vila-prompt}
%See the full prompt in 
%appendix, \mysec{sec:appendix:prompt}.}
\end{figure}

\subsection{VLM Implementation Details}
\paragraph{VLM Prompt.} We list the prompt for both fine-tuning on sim and real robot data and evaluation in \Cref{fig:vila-prompt}. We condition the model on an image and the prompt, except when training on Pixel Point Prediction data (i.e., from Robopoint~\citep{yuan2024robopoint}) where we used the given prompts from the dataset. 
Note that we ask the model to output gripper changes as separate language tokens, i.e., \texttt{Open Gripper/Close Gripper}, as opposed to as a numerical value as shown in simplified depictions like \Cref{fig:method}. 

\paragraph{VLM Trajectory Processing.} As mentioned in \Cref{sec:method:vlm}, 
one problem with directly training on the path labels $p^o$ is that many paths may be extremely long.
Therefore, we simplify the paths $p^o$ with the Ramer-Douglas-Peucker algorithm~\citep{RAMER1972244, douglas_pecker_1973} that reduces curves composed of line segments to similar curves composed of fewer points. We run this algorithm on paths produced by simulation and real robot data to generate the labels $p^o$ for $\vlmdata$. We use tolerance $\epsilon=0.05$, resulting in paths that are around 2-5 points for each short horizon task.

\paragraph{VLM Training Details.} 
We train our VLM, VILA1.5-13B~\cite{vila2024}, on a node equipped with eight NVIDIA A100 GPUs, each utilizing approximately 65\,GB of memory. The training process takes about 30 hours to complete. We use an effective batch size of 256 and a learning rate of $1 \times 10^{-5}$. During fine-tuning, the entire model---including the vision encoder---is updated.

\subsection{Low-level Policy Training Details}
We train RVT2~\citep{goyal2024rvt} and 3D-DA~\citep{ke20243d} as our lower-level policies.
We keep overall architecture and training hyperparameters the same as paper settings.
Specific details about how the inputs were modified other than the 2D path projection follow.

For low-level policy training, we train the policies on ground truth paths constructed by projecting trajectory end-effector points to the camera image. 
In order to also ensure the policies are robust to possible error introduced by \method\ VLM predictions during evaluation, we add a small amount of random noise ($N(0, 0.01)$) to the 2D path $(x, y)$ image points during training to obtain slightly noisy path drawings. No noise was added to the gripper opening/closing indicator values.

\paragraph{RVT2 \citep{goyal2024rvt}.} We remove the language instruction for RVT-2 when conditioning on \method\ 2D paths.
%Given a 2D sketch, we overlay the corresponding 2D trajectory onto the input RGB image. 
%The RGB image, along with the 2D sketch, is then projected into 3D space for further processing. We 
\paragraph{3D-DA \citep{ke20243d}.} In simulated experiments in Colosseum, no changes were needed. In fact, we saw a performance drop for HAMSTER+3D-DA when removing language for Colosseum tasks and a small drop in performance when using simplified language instructions. 
This is likely due to 3D-DA's visual attention mechanism which cross attends CLIP language token embeddings with CLIP visual features, therefore detailed language instructions are beneficial.

In real-world experiments, we simplify the language instruction in the same way as for RVT2 when conditioning on \method\ 2D paths to encourage following the trajectory more closely with limited data. In addition, we reduced the embedding dimension of the transformer to $60$ from $120$, removed proprioception information from past timesteps, and reduced the number of transformer heads to $6$ from $12$ in order to prevent overfitting.

\section{Real World Experiment Details}
\label{sec:appendix:real_world_exp_details}
\begin{figure}
\small
\begin{mdframed}[frametitle=RT-Trajectory GPT-4o Prompt, frametitlealignment=\centering,]
In the image, please execute the command described in '\{quest\}'.

Provide a sequence of keypoints denoting a trajectory of a robot gripper to achieve the goal. Keep in mind these are keypoints, so you do not need to provide too many points.

Format your answer as a list of tuples enclosed by \texttt{<ans>} and \texttt{</ans>} tags. For example:

\texttt{<ans>[(0.25, 0.32), (0.32, 0.17), (0.13, 0.24), <action>Open Gripper</action>, 
(0.74, 0.21), <action>Close Gripper</action>, ...]</ans>}

The tuple denotes point $x$ and $y$ location of the end effector of the gripper in the image. The action tags indicate the gripper action.

The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.

The current position of the robot gripper is: \{current\_position\}. Do not include this point in your answer.
\end{mdframed}
\caption{The full text prompt we use to prompt RT-Trajectory with GPT4-o.}

\label{fig:gpt4o-prompt}
%See the full prompt in 
%appendix, \mysec{sec:appendix:prompt}.}
\end{figure}

\begin{figure}
\small
\begin{mdframed}[frametitle=RT-Trajectory Code as Policies Prompt, frametitlealignment=\centering,]
Task Instruction: \{task\_instruction\}

Robot Constraints:
\begin{itemize}
    \item The robot arm takes as input 2D poses with gripper open/closing status of the form $(x, y, \text{gripper\_open} == 1)$
    \item The gripper can open and close with only binary values
    \item The workspace is a $1 \times 1$ square centered at $(0.5, 0.5)$
    \item The x-axis points rightward and y-axis points downward.
\end{itemize}

Please write Python code that generates a list of 2D poses and gripper statuses for the robot to follow. Include Python comments explaining each step. Assume you can use \texttt{numpy} or standard Python libraries, just make sure to import them.

Enclose the start and end of the code block with \texttt{<code>} and \texttt{</code>} so that it can be parsed. Make sure that it is a self-contained script such that when executing the code string, there is a variable named \texttt{robot\_poses} which is a list of poses of the form: \texttt{[(x, y, gripper), (x, y, gripper), ...]}.

Scene Description:

\begin{verbatim}
<code>
{scene_description}
</code>
\end{verbatim}
\end{mdframed}
\caption{The full text prompt we use for RT-Trajectory with Code-as-Policies on top of GPT4-o. The scene description at the bottom comes from an open-vocabulary object detector describing each detected object and its bounding box in the image based on the task instruction.}

\label{fig:cap-prompt}
%See the full prompt in 
%appendix, \mysec{sec:appendix:prompt}.}
\end{figure}

% \jz{Insert the exact list of training and testing tasks, how the data was collected, number of trajectories for each type, etc.}
\subsection{Training Tasks and Data Collection} For our real-world experiments, we collected all data using a Franka Panda arm through human teleoperation, following the setup described in \citet{khazatsky2024droid}. Below, we describe the training tasks:

\paragraph{Pick and place.} We collected 220 episodes using 10 toy objects. In most of the training data, 2 bowls were placed closer to the robot base, while 3 objects were positioned nearer to the camera. The language goal for training consistently followed the format: \texttt{pick up the \{object\} and put it in the \{container\}}.

\paragraph{Knock down objects.} We collected 50 episodes with various objects of different sizes. Typically, 3 objects were arranged in a row, and one was knocked down. The language goal for training followed the format: \texttt{push down the \{object\}}.

\paragraph{Press button.} We collected 50 episodes with 4 colored buttons. In each episode, the gripper was teleoperated to press one of the buttons. The language goal followed the format: \texttt{press the \{color\} button}.

When training RVT2, which requires keyframes as labels, in addition to labeling frames where the gripper performs the \texttt{open gripper} and \texttt{close gripper} actions, we also included frames that capture the intermediate motion as the gripper moves toward these keyframes.

% \jz{also mention keyframe extraction }
\subsection{Baseline Training Details}
\label{sec:baseline_details}
\paragraph{OpenVLA~\citep{kim2024openvla}.} Following \citet{kim2024openvla}, we only utilize parameter efficient fine-tuning (LoRA) for all of our experiments, since they showed that it matches full fine-tuning performance while being much more efficient. We follow the recommended default rank of $r$=32. We opt for the resolution of 360 x 360 to match all of the baseline model's resolutions. We also follow the recommended practice of training the model until it surpasses 95\% token accuracy. However, for some fine-tuning datasets, token accuracy converged near 90\%. We selected the model checkpoints when we observed that the token accuracy converged, which usually required 3,000 to 10,000 steps using a global batch size of either 16 or 32. Training was conducted with 1 or 2 A6000 gpus (which determined the global batch size of 16 or 32). Emprically, we observed that checkpoints that have converged showed very similar performance in the real world. For example, when we evaluate checkpoint that was trained for 3,000 steps and showed convergence, evaluating on a checkpoint trained for 5,000 steps of the same run resulted in a very similar performance.

\paragraph{RT-Trajectory~\citep{gu2023rttrajectory}.} We implement two versions of RT-Trajectory for the comparison in \Cref{tab:experiments:vlm}. The first (0-shot GPT-4o) directly uses GPT-4o to generate 2D paths with a prompt very similar to the one we use for \method, displayed in \Cref{fig:gpt4o-prompt}. 

The second version implements RT-Trajectory on top of a Code-as-Policies~\citep{liang2023code}, as described in RT-Trajectory.
We use OWLv2~\citep{minderer2023scaling} to perform open-vocabulary object detection on the image to generate a list of objects as the scene description and then prompt RT-Trajectory with the prompt shown in \Cref{fig:cap-prompt}. We also use GPT-4o as the backbone for this method.
\subsection{Evaluation Tasks} 
\input{tables/evaluation_tasks}
\label{sec:appendix_evaluation_tasks}

We evaluate our method on the tasks of \texttt{pick and place}, \texttt{knock down object}, and \texttt{press button} across various generalization challenges, as illustrated in \Cref{fig:experiments:main_exp}. Detailed results are available in \Cref{table:detailed_real}. Following \citep{kim2024openvla}, we assign points for each successful sub-action. For VLM, human experts are employed to assess the correctness of the predicted trajectories.
\section{Extended Results}
\input{tables/colosseum_extended}

\subsection{Impact of Design Decisions on VLM performance}
\label{sec:experiments:vlm_design}
To better understand the transfer and generalization performance of the proposed hierarchical VLA model, we analyze the impact of various decisions involved in training the high-level VLM. We conduct a human evaluation of different variants of a trained high-level VLM on a randomly collected dataset of real-world test images, as shown in \Cref{fig:vlm_generalization}. We ask each model to generate 2D path traces corresponding to instructions such as ``move the block on the right to Taylor Swift'' or ``screw the light bulb in the lamp'' (the full set is in \Cref{sec:appendix:generalization}). We then provide the paths generated by each method to human evaluators who have not previously seen any of the models' predictions. The human evaluators then rank the predictions for each method; we report the average rank across the samples in \Cref{tab:experiments:vlm}. 

We evaluate the following VLM models: (1) zero-shot state-of-the-art closed-source models such as GPT-4o using a similar prompt to ours (shown in \Cref{fig:gpt4o-prompt}), (2) zero-shot state-of-the-art closed-source models such as GPT-4o but using Code-as-Policies~\citep{liang2023code} to generate paths as described in \citet{gu2023rttrajectory} (prompt in \Cref{fig:cap-prompt}),
%(3) zero-shot open-source models (VILA-1.5-13b), without any finetuning, 
(3) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section~\ref{sec:method:vlm}, but excluding the simulation trajectories from the RLBench dataset, (4) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section~\ref{sec:method:vlm}, including path sketches from the RLBench dataset. The purpose of these evaluations is to first compare with closely related work that generates 2D trajectories using pretrained closed source VLMs~\cite{gu2023rttrajectory} (Comparison (1) and (2)). 
%Secondly, the comparison with (3) is meant to analyze the impact of finetuning over using zero-shot open-source VLMs for path generation. 
The comparison between (3) and (4) (our complete method) is meant to isolate the impact of including the simulation path sketches from the RLBench dataset. In doing so, we analyze the ability of the VLM to predict intermediate paths to transfer across significantly varying domains (from RLBench to the real world). 

The results suggest that: (1) zero-shot path generation, even from closed-source VLMs~\cite{gu2023rttrajectory} such as GPT-4o with additional help through Code-as-Policies~\citep{liang2023code}, underperforms VLMs finetuned on cross-domain data as in \method; (2) inclusion of significantly different training data such as low-fidelity simulation during finetuning improves the real-world performance of the VLM. This highlights the transferability displayed by \method\ across widely varying domains. These results emphasize that the hierarchical VLA approach described in \method\ can effectively utilize diverse sources of cheap prior data for 2D path predictions, despite considerable perceptual differences. 


\input{tables/vlm}


\subsection{VLM Real World Generalization Study}
\label{sec:appendix:generalization}
The full list of task descriptions for this study is below (see \Cref{sec:experiments:vlm_design} for the main experiment details). Duplicates indicate different images for the same task. We plot some additional comparison examples in \Cref{fig:human_eval}. Note that the path drawing convention in images for this experiment differ from what is given to the lower-level policies as described in \Cref{sec:method:policy} as this multi-colored line is easier for human evaluators to see.
\begin{enumerate}
    \item screw in the light bulb on the lamp
    \item screw in the light bulb on the lamp
    \item screw in the light bulb on the lamp
    \item screw out the light bulb and place it on the holder
    \item screw out the light bulb and place it on the holder
    \item screw in the light bulb
    \item screw in the light bulb on the lamp
    \item move the blue block on Taylor Swift
    \item pick up the left block and put it on Jensen Huang
    \item move the block on the right to Taylor Swift
    \item place the yellow block on Kobe
    \item pick up the blue block and place it on Jensen Huang
    \item move the red block to Kobe
    \item press the button on the wall
    \item press the button to open the left door
    \item press the button to open the right door
    \item open the middle drawer
    \item open the bottom drawer
    \item open the top drawer
    \item open the middle drawer
    \item open the bottom drawer
    \item press the button
    \item press the button
    \item press the orange button
    \item press the orange button with black base
    \item press the button
    \item pick up the SPAM and put it into the drawer
    \item pick up the orange juice and put it behind the red box
    \item pick up the tomato soup and put it into the drawer
    \item pick up the peach and put it into the drawer
    \item move the mayo to the drawer
    \item move the dessert to the drawer
    \item pick up the object on the left and place it on the left
    \item pick up the fruit on the left and put it on the plate
    \item pick up the milk and put it on the plate
    \item press the button with the color of cucumber, then press the button with color of fire
    \item press the button with color of banana
    \item press the button with color of leaf
    \item press the button with color of leaf, then press the one with color of banana
    \item press left button
    \item pick up the left block on the bottom and stack it on the middle block on top
    \item make I on top of C
    \item put number 2 over number 5
    \item stack block with lion over block with earth
    \item pick up the left block on the bottom and stack it on the middle block on top
    \item stack the leftest block on the rightest block
    \item stack the block 25 over block L
    \item put the left block on first stair
\end{enumerate}

\subsection{Human Ranking}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/human_ranking_example.jpg}
    \caption{An example of results for human ranking. The trajectory is from blue to red with blue circle and red circle denotes gripper close point and open point respectively. The grader is asked to provide a rank to these trajectory about which trajectory has highest chance to succeed.}
    \label{fig:human_rank_example}
\end{figure}
Due to the variety of possible trajectories that accomplish the same task, we use human rankings to compare how likely produced trajectories are to solve the task instead of quantitative metrics such as MSE. To do that, we generate trajectories for 48 image-question pairs with HAMSTER w/o RLBench, HAMSTER, Code-as-Policy~\citep{liang2023code}, and GPT4o~\citep{openai2024gpt4}. See \Cref{fig:human_rank_example} for an example. 

We recruit 5 human evaluators, who are robot learning researchers that have not seen the path outputs of \method, to grade these 4 VLMs based on the instruction: \textit{``Provide a rank for each method (1 for best and 4 for worst). In your opinion, which robot trajectory is most likely to succeed. Traj goes from blue to red, blue circle means close gripper, red circle means open gripper.''} The evaluators are allowed to give multiple trajectories the same score if they believe those trajectories are tied.
As they are robot learning researchers, they are familiar with the types of trajectories that are more likely to succeed. Therefore, these rankings act as a meaningful trajectory quality metric.


% \begin{figure}[H] % [H] forces the figure to appear exactly where it is placed
%     \centering
%     \includegraphics[width=0.4\textwidth]{figs/failure_distribution.png}
%     \caption{Performance Distribution of RVT2+Sketch and 3DDA+Sketch}
%     \label{fig:Performance Distribution}
% \end{figure}

% \input{tables/failure_disitrbution}
% \section{Failure Analysis} 

% \label{sec:appendix:failure_modes}
% Our experiments encountered various failure modes, which can be categorized as VLM fails to predict the correct trajectory, low-level action model fails to follow that trajectory and low-level action model fails to execute the action. 
% \subsection{VLM Failure}
% VLM can fail to predict trajectory due to multiple reasons and we group them as:
% \paragraph{VLM failed to understand the language goal} Although the VLM demonstrates strong capability in handling different task descriptions, it struggles when the training set lacks similar tasks. In such cases, the model may fail to understand the goal and make accurate predictions.

% \paragraph{Incorrect trajectory prediction} Another common failure mode is the VLM predicting an incorrect trajectory for the given language goal. This may occur when the model either interacts with the wrong objects or misinterprets the direction of the affordance.

% \paragraph{Dynamic changes in the environment} Our approach generates trajectories at the start of each task. If the environment changes significantly after the trajectory is generated, the task will likely fail, as the model lacks the ability to update the trajectory during execution and cannot identify the object initially referenced.

% \subsection{Fail to Follow Trajectory}

% \paragraph{3D ambiguity} Since we predict 2D trajectories, ambiguities can arise in determining whether a point is positioned above or behind an object, leading to errors in execution.

% \paragraph{Incorrect Object to Interact} The action model is not explicitly constrained to follow the predicted trajectory, which can lead to failures. The low-level action model may deviate from the trajectory's guidance, a primary source of error in our experiments.

% \subsection{Failure to execute the action} 
% Despite covering various actions during training, the action model may still fail due to several factors. For example, in grasping tasks, an incorrect grasp angle can cause the object to slip, resulting in a failed grasp.
% \subsection{Failure Analysis}
% We provide our analysis of our experiment in \Cref{failure_distribution}. We observed distinct tendencies across methods. For RVT, in cases where the VLM prediction is correct:
% 72\% of failures are due to RVT not following the trajectory.
% 28\% are due to action execution failures.
% For 3DDA, we found that:
% Only 10\% of failures are due to not following the trajectory.
% 90\% are due to action execution failures.
% We believe this discrepancy arises because RVT includes a re-projection step, which complicates trajectory adherence. In contrast, 3DDA utilizes a vision tower that processes the original 2D image, making it easier to interpret the trajectory.

\section{Failure Analysis}
\label{sec:appendix:failure_modes}
\input{tables/failure_disitrbution}
This section outlines the failure modes observed during our experiments and provides a detailed breakdown of the causes. Failures can be attributed to issues in \textbf{trajectory prediction}, \textbf{trajectory adherence}, and \textbf{action execution}.
\subsection{Different Failure Modes}
\paragraph{Trajectory Prediction Failures}
The Vision-Language Model (VLM) may fail to predict the correct trajectory due to several factors:

- \textit{Failure to understand the language goal:} 
  Although the VLM demonstrates strong capabilities in handling diverse task descriptions, it struggles when the training set lacks similar tasks. This can cause the model to misunderstand the goal and make inaccurate predictions.

- \textit{Incorrect trajectory prediction:} 
  In some cases, the VLM predicts an incorrect trajectory, either by interacting with the wrong objects or misinterpreting the direction of the affordance.

- \textit{Dynamic changes in the environment:} 
  Since trajectories are generated at the beginning of a task, significant environmental changes during execution can lead to failure. The model lacks the ability to dynamically adjust the trajectory or reidentify the object initially referenced.

\paragraph{Trajectory Adherence Failures}
Failures in adhering to the predicted trajectory arise primarily due to:

- \textit{3D ambiguity:} 
  The use of 2D trajectory predictions introduces ambiguities, such as determining whether a point is positioned above or behind an object, leading to execution errors.

- \textit{Incorrect object interaction:} 
  The low-level action model is not explicitly constrained to strictly follow the predicted trajectory. As a result, it may deviate, interacting with the wrong object and causing task failures.

\paragraph{Action Execution Failures}
Even when the trajectory is correctly predicted and adhered to, action execution may still fail due to:

- \textit{Execution-specific issues:} 
  Despite training on a diverse set of actions, the model may fail during execution. For example, in grasping tasks, an incorrect grasp angle can cause the object to slip, resulting in a failed grasp.

\subsection{Failure Analysis}
Our analysis in \Cref{fig:failure_distribution} reveals distinct failure tendencies across methods.

For RVT, 72\% of failures stemmed from the low-level model failing to follow the trajectory, while 28\% were due to execution failures. In contrast, for 3DDA, only 10\% of failures were related to trajectory adherence, with 90\% attributed to execution failures.

We hypothesize that this discrepancy arises because RVT incorporates a re-projection step, complicating trajectory adherence. In contrast, 3DDA leverages a vision tower that processes the original 2D image, simplifying trajectory interpretation.
\section{Simulation Experiment Details}
\label{sec:appendix:simulation_details}

\begin{wrapfigure}{R}{0.35\textwidth}
    \includegraphics[width=\linewidth]{figs/colosseum_variations.png}
    \caption{Colosseum benchmark variations. Figure from \citet{pumacay2024colosseum}, taken with permission.}
\label{fig:colosseum_overview} 
\end{wrapfigure}

Our simulation experiments are performed on Colosseum~\citep{pumacay2024colosseum}, a simulator built upon RLBench~\citep{james2020rlbench} containing a large number of visual and task variations to test the generalization performance of robot manipulation policies (see \Cref{fig:colosseum_overview} for a visualization of a subset of the variations). 
We use the \texttt{front\_camera} and remove all tasks in which the camera does not provide a clear view of the objects in the task, resulting in 14 out of 20 colosseum tasks (we remove \texttt{basketball\_in\_hoop}, \texttt{empty\_drawer}, \texttt{get\_ice\_from\_fridge}, \texttt{move\_hanger}, \texttt{open\_drawer}, \texttt{turn\_oven\_on}).

Colosseum contains 100 training episodes for each task, without any visual variations, and evaluates on 25 evaluation episodes for each variation. We follow the same procedure other than using just the \texttt{front\_camera} instead of multiple cameras. 
We report results in \Cref{tab:experiments:colosseum} after removing variations with no visual variations (e.g., object friction). 

\input{tables/grouped_real_table}







\section{Different ways of representing 2D Paths}
\label{sec:rdp_vs_20p}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/RDP_vs_20p.jpg}
    \caption{The task is to pick up the lid and close it on the jar with correct color. Task description is located on the top-left corner of each image. The trajectory goes from blue to red where blue circles denotes where the gripper should close and red circles denotes where the gripper should open. GT denotes ground truth, 3B and 13B denotes VILA1.5-3B and VILA1.5-13B, RDP denotes paths simplified using Ramer–Douglas–Peucker algorithm while 20p denotes paths reprensented using 20 points.}
    \label{fig:rdp_vs_20p}
\end{figure}
To investigate the effect of the number of points on the 2D path, we train the VLM to predict 1. paths simplified using RDP algorithm, which simplify paths in short horizon tasks to 3-5 points and is what we used in the paper. We denote these paths as RDP in the following; 2. Paths represented with 20 points sampled on the path with same step size, denoted as 20p in the following. We keep points where the gripper is executing operation of open or close in both methods.

We train the network on RLBench 80 tasks with 1000 episodes for each task and test it on 25 episodes on the task of close jar. We tried both VILA1.5-3B (denoted as 3B) and VILA1.5-13B (denoted as 13B) as our backbone. Thus we have in total 4 combinations over 2 backbones and 2 designs of path representations. We visualize the result in this \Cref{fig:rdp_vs_20p}. 

From this result we can see that when using smaller models, like VILA1.5-3B, paths represented using points extracted using RDP algorithm outperforms paths represented with a fixed number of 20 points significantly. When the network becomes larger to the level of 13B, the VLM is able to handle the representation using 20 points and both two path representations work perfectly. We believe that is because when points are simplified using the RDP algorithm, we usually need less points to represent the path and helps the model to pay more attention to predict the accurate position for the gripper open/close points.
%\jz{todo}.

%\input{tables/ablation}