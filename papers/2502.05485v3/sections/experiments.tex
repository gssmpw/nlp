\vspace{-2mm}
\section{Experimental Evaluation}
\vspace{-2mm}
% for camera ready
%We perform empirical evaluations in both simulation and the real world. The experiments aim to answer the following questions: (Q1) do hierarchical VLA models enable behavioral generalization to unseen scenarios? (Q2) do hierarchical VLA models show more effective cross-domain generalization than monolithic VLA models or low-level imitation learning methods? (Q3) is behavior learned by hierarchical VLA models robust to significant degrees of visual and semantic variations? (Q4) can hierarchical VLA models enable learning of non-prehensile and long-horizon tasks? (Q5) does explicitly finetuning the high-level VLM yield benefits in terms of spatial and semantic reasoning?

% for arxiv
We evaluate our approach in both simulation and real-world experiments to the following key questions. Do hierarchical VLAs:
\begin{enumerate}[label=\textbf{Q\arabic*}, nosep]
    \item \label{q1} Generalize behaviors to unseen scenarios with significant visual and semantic variation?
    \item \label{q2} Achieve stronger cross-domain generalization than monolithic VLAs and low-level imitation learning methods?
    %\item \label{q3} Exhibit robustness to significant visual and semantic variations?
    \item \label{q3} Facilitate learning of non-prehensile and long-horizon tasks?
    \item \label{q4} Exhibit strong demonstration efficiency?
    \item \label{q5} Have improved visual + semantic reasoning due to hierarchy and VLM fine-tuning?
\end{enumerate}


%\ankit{shall we remove this paragraph}
%To validate these hypotheses, we perform experiments in simulation by training both the high-level VLM and the low-level 3D policy on data from the RLBench~\citep{james2020rlbench} benchmark environment and testing on visual and semantic variations in the related Colloseum benchmark~\cite{pumacay2024colosseum}. In the real world, we perform experiments by training the base VLM model, VILA-1.5-13b~\citep{vila2024}, on the data mixture proposed in Section~\ref{sec:method:vlm:datasets}, and testing on a completely unseen tabletop manipulation setting that is depicted in Fig [TODO]. Further details of the experimental setup in both simulation and the real world is provided in the following sections. 
%\ag{We need to have a figure of the table top setting clearly indicated}

\vspace{-2mm}
\subsection{Real World Evaluation on Tabletop Manipulation}
\vspace{-2mm}
\input{tables/real}
\begin{figure}[!tb]
    \centering
    %\includegraphics[width=\linewidth,trim={0in 4.8in 0in 0.5in},clip]{figs/main_exp_fig_short.pdf}
    %\includegraphics[width=\linewidth,trim={0in 6.5in 0in 0.5in}]{figs/main_exp_fig_short_v2.pdf}
    \includegraphics[width=\linewidth]{figs/main_exp_fig_short_v3_small.pdf}
    % \includegraphics[width=\linewidth,trim={1in 1.5in 3in 0.8in},clip]{figs/main_exp_fig.pdf}
    \caption{\footnotesize{Depiction of quantitative real-world policy execution results on a real-world robot, evaluated across different axes of generalization and across both prehensile and non-prehensile tasks. Across all generalization axes, \method \ outperforms monolithic VLAs and the base 3D imitation learning policies.}}
    \label{fig:experiments:main_exp}
    \vspace{-4mm}
\end{figure}
To answer \ref{q1}, our real-world evaluation experiments aim to test the generalization capability of hierarchical VLA models across significant semantic and visual variations. In particular, we consider a variant of \method\ that uses a VLM (VILA-1.5-13b~\citep{vila2024}) finetuned on the data mixture in Section~\ref{sec:method:vlm} as the high-level predictor, with two low-level 3D policy architectures - RVT-2~\citep{goyal2024rvt} and 3D Diffuser Actor (3D-DA)~\citep{ke20243d} as choices of the low-level policy, as described in Section~\ref{sec:method:policy}. The low-level 3D policies are trained with $320$ episodes collected via teleoperation % directly on the table-top manipulation setup 
shown in Fig.~\ref{fig:experiments:training_data}. Importantly, the high-level VLM % in \method\ 
has not seen any in-domain data and is only finetuned on the off-domain data described in Section~\ref{sec:method:vlm}. This suggests that any generalization that the VLM shows result from % in-domain training data, but instead from 
cross-domain transfer. 

% \ag{Ensure we have something showing the difference between training domains and testing setup. This is important to show the tranfer results.}

\textbf{Baseline comparisons.} To answer \ref{q2}, we compare \method\ with a state-of-the-art monolithic VLA, OpenVLA~\citep{kim2024openvla} as well as non-VLM 3D policies, RVT-2~\citep{goyal2024rvt} and 3D-DA~\citep{ke20243d}. For fair comparison, we finetune OpenVLA on the collected in-domain data described above since OpenVLA showed poor zero-shot generalization. 
%We also discuss an additional OpenVLA baseline fine-tuned on the same RLBench simulation data as \method's VLM below.
The 3D policy (RVT-2, 3D-DA) baselines are trained with the same teleoperation data used to train the low-level policy in \method\ but without the intermediate 2D path representation from \method 's VLM.

\textbf{Finetuning OpenVLA with RLBench.}  
To ensure our method's advantage over OpenVLA~\citep{kim2024openvla} is not solely due to RLBench data, we fine-tuned OpenVLA on the same RLBench dataset used for \method's VLM—1,000 episodes per task across 81 tasks (using only episodes with good front-camera visibility)—until achieving over 90\% token accuracy~\citep{kim2024openvla}. We then fine-tuned this model on our tasks following the procedure in \Cref{sec:baseline_details}. In real-world pick-and-place experiments (6 trials over 6 ``Basic'' tasks as shown in \Cref{table:detailed_real}), RLBench-finetuned OpenVLA averaged a success score of 0.54 versus 0.58 for the model without RLBench fine-tuning. This suggests that monolithic VLA architectures like OpenVLA gain little benefit from RLBench data, likely due to mismatches in action and observation spaces relative to the real-world setup.

\textbf{Quantitative Results.} Figure~\ref{fig:experiments:main_exp} summarizes our real-world results. To answer \ref{q3}, we evaluate across multiple task types, including `pick and place,' and nonprehensile tasks such as `press buttons' and `knock down objects.' We also test generalization across various axes (\ref{q1}) -- \emph{obj and goal:} unseen object-goal combinations; \emph{visual:} visual changes in table texture, lighting, distractor objects; \emph{language:} unseen language instructions (e.g., candy $\rightarrow$ sweet object); \emph{spatial:} unseen spatial object relationships in the instruction; \emph{novel object:} unseen objects; and lastly, \emph{multiple:} a combination of multiple variations. 
In total, we evaluate each model on 74 tasks for 222 total evaluations. Detailed results and the success score metric are provided in Appendix \Cref{table:detailed_real}. %Additional qualitative experiments showcasing various skills can be found in \Cref{sec:various_tasks}.
% \ag{It'd be really useful to have a figure demonstrating each of these variations}

\textbf{Qualitative Eval on Various Tasks.} In addition to the quantitative evaluation conducted for comparison with OpenVLA, we also present qualitative results that demonstrate how HAMSTER’s hierarchical structure enables low-level policy models to generalize to more complex tasks. \Cref{fig:various_tasks} illustrates the diverse tasks HAMSTER can handle, including unfolding a towel, opening and closing drawers, pressing buttons, wiping surfaces, and cleaning tables. These tasks present challenges such as varying lighting conditions, cluttered backgrounds, and semantic understanding requiring external world knowledge. Additionally, HAMSTER demonstrates the ability to perform long-horizon tasks—none of which are part of the in-domain training set used to train the policy model.

\begin{figure}[t!]
    \centering
% \vspace{-0.4cm}
    \includegraphics[width=0.8\linewidth]{figs/various_tasks_480p.jpg}
% \vspace{-0.5cm}
    \caption{\footnotesize Examples of various robot tasks and environments that \method\ can handle. See more details in our teaser video at \href{https://hamster-robot.github.io/}{https://hamster-robot.github.io/}.}
    \label{fig:various_tasks}
% \vspace{-1.0cm}
\end{figure}

Overall, we find that \method\ significantly outperforms monolithic VLA models and (non-VLM) 3D policies by over \textbf{2x} and \textbf{3x}, respectively, on average. This is significant because this improved performance is in the face of considerable visual and semantic changes in the test setting, showing the ability of \method~to generalize better % transfer more effectively 
than monolithic VLA models or non-VLM base models. 
We further group results by task type in \Cref{tab:grouped_task_comparison}, where we see \method\ outperforms OpenVLA across all task types (pick and place, press button, and knock down). See \Cref{sec:appendix:real_world_exp_details} for evaluation conditions, a task list, and other experiment details, and \Cref{sec:appendix:failure_modes} for failure modes.
%We refer readers to the supplementary website for additional details on the failure modes and evaluation conditions. \jz{TODO: replace this with link to appendix section}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth,trim={0in 1.5in 0in 1.8in},clip]{figs/real_robot_policy.pdf}
    \caption{\footnotesize{Example real-world \method\ rollouts demonstrate its strong performance in novel scenes achieved by leveraging VLMs' generalization capabilities and the robust execution of low-level 3D policies.}}
    \label{fig:experiments:real_robot_policy}
    \vspace{-2mm}
\end{figure}

\vspace{-2mm}
\subsection{Simulation Evaluation}
\vspace{-2mm}
\textbf{Overall Results.}
For further investigation into \ref{q1}, \ref{q2}, and \ref{q3}, we conducted a controlled simulation evaluation using Colosseum~\citep{pumacay2024colosseum}, which provides significant visual and semantic variations across pick-place and non-prehensile tasks. Pairing our high-level VLM with the state-of-the-art 3D-DA~\citep{ke20243d} policy on RLBench, we compared {\method} against a vanilla 3D-DA implementation without path guidance. As shown in \Cref{tab:experiments:colosseum} over 5 seeds, {\method} outperforms the vanilla approach by an average of 31\%. This improvement stems from training with path-drawn images, which encourages the policy to focus on the path rather than extraneous visual features, thereby enhancing robustness to visual variations.
%This shows that the 2D paths produced by the VLM in {\method} can help low-level policies to generalize better to novel unseen variations. 
We refer readers to \citet{pumacay2024colosseum} for details on the variations and \Cref{sec:appendix:simulation_details} for further simulation experiment details.

\input{tables/cole_rebuttal_and_view}
% \input{tables/view_invariance}
\textbf{HAMSTER with Fewer Demonstrations.}
We also test \method's ability to work well with limited demonstrations to answer \ref{q4}.
We test on a subset of 5 Colosseum tasks, namely, \textsc{slide\_block\_to\_target}, \textsc{place\_wine\_at\_rack\_location}, \textsc{insert\_onto\_square\_peg}, \textsc{stack\_cups}, \textsc{setup\_chess}.
Results in \Cref{tab:rebuttal_sim_table} demonstrate that \method+3D-DA with just 50\% of the data still achieves 2x the success rate of standard 3D-DA, demonstrating that \method\ is demonstration-efficient for the downstream imitation learning tasks. 

\input{tables/colesseum}

\vspace{-2mm}
\subsection{VLM Generalization Studies}
\vspace{-2mm}
\label{sec:appendix:additional_ablations}
Finally, we perform additional experiments to answer \ref{q5} on whether \method's hierarchy enables superior visual and semantic reasoning. %specifically test \method's ability to generalize to novel views various ways to represent the paths
%, and the demonstration efficiency of \method~.

\begin{wrapfigure}[15]{R}{0.3\textwidth}
    \centering
    \vspace{-0.4cm}
    \includegraphics[width=\linewidth]{figs/new_camera.jpg}
    \vspace{-0.7cm}
    \caption{\footnotesize Camera positions for view invariance: old (right) and new (left).}
    \label{fig:camera_angle}
\end{wrapfigure}
\textbf{Camera View Invariance.}
We test \method+RVT2 against OpenVLA from a new camera angle (\Cref{fig:camera_angle}) across 10 pick-and-place trials using 6 training objects and 3 training containers to check \method's visual spatial reasoning.
% 
The results in \Cref{tab:camera_comparison} show that \method\ significantly outperforms OpenVLA and remains robust to new camera angles, benefiting from its VLM trained on diverse \emph{off-domain} tasks across various viewpoints. 
Additionally, we compare \method+RVT2 (Concat), where instead of overlaying the path on the input RGB image, we modify RVT-2 to accept a 6-channel input by concatenating the original RGB image with a separate RGB image containing only the drawn path.
We can easily apply this due to \method's hierarchical nature.
%This approach is less easily applied to arbitrary imitation learning policies (for example, it cannot be easily applied to 3D-DA as it uses a pre-trained CLIP image encoder expecting 3 input channels), but allows us to represent paths in a different way.
Concatenated paths actually achieve the best performance, demonstrating the effectiveness of this path representation, though it is less general and not compatible with all imitation learning policy architectures (such as 3D-DA as it uses a pre-trained image encoder expecting 3 input channels).
One possible explanation is that RVT2’s virtual reprojection can fragment the 2D path when it is directly drawn on the image, making it harder for RVT2 to decode. By providing a dedicated path channel (via concatenation), path guidance is preserved more effectively.
%We also test another path representation, using just gripper open/close points instead of drawing the full path.
%We test on a subset of 5 Colosseum tasks, namely, \texttt{slide\_block\_to\_target}, \texttt{place\_wine\_at\_rack\_location}, \texttt{insert\_onto\_square\_peg}, \texttt{stack\_cups}, \texttt{setup\_chess}.
%in \Cref{tab:rebuttal_sim_table}. We see that \method+3D-DA outperforms \method with only gripper open/close points on these tasks, and this performance gain likely will extend further on longer-horizon tasks. 



\textbf{VLM Generalization.} We further demonstrate the benefit of \method's hierarchy by demonstrating that the VLM generalizes well to visually unique and semantically challenging tasks due to its off-domain fine-tuning. 
We visualize example \method\ path drawings in \Cref{fig:vlm_generalization}, demonstrating \method's VLM itself effectively reasons semantically and visually for unseen tasks. 
We further investigate VLM performance in \Cref{sec:experiments:vlm_design}, where we find that (1) \method\ outperforms zero-shot path generation from closed-source VLMs \citep{gu2023rttrajectory, liang2023code} and (2) that inclusion of simulation data improves \method's real-world performance. 
Both results point to the benefit of explicit hierarchy: off-domain VLM fine-tuning that improves its performance.
See \Cref{sec:experiments:vlm_design} for further details.



\begin{figure}[t]
    \centering
    %\includegraphics[height=80pt]{figs/fig5.pdf}
    %\vspace{-0.5cm}
    \includegraphics[width=\linewidth]{figs/fig5_v2.pdf}
    \vspace{-0.7cm}
    \caption{\footnotesize{\method's VLM demonstrates strong generalization to unseen scenarios. From left to right: (a) leveraging world knowledge for user-specified tasks, (b) handling out-of-domain inputs like human-drawn sketches, and (c) transferring from diverse simulations to visually distinct real-world tasks. Blue-to-red lines indicate motion, with blue and red circles marking grasp and release points, respectively.} }
    \label{fig:vlm_generalization}
    \vspace{-4mm}
\end{figure}




% \ag{I don't quite get this celebrity thing, can someone clarify this?}
% \ag{Overall we need to go through the experiments and really illustrate how different training and test are, illustrate the different testing conditions, illustrate some paths drawn by the VLM, and for the human comparison provide one illustrative example.}