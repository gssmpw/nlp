\section{Background}
\label{sec:prelim}


% \textbf{Imitation Learning via Supervised Learning.} The goal of imitation learning is to train a probabilistic policy $\pi_\theta(a \mid s, o, z)$ from an expert-provided dataset. This policy $\pi_\theta$ outputs the probability of producing action $a$ conditioned on proprioceptive observations $s$, perceptual observations $o$, and language instructions $z$ that specify the task. Here we assume that the perceptual observation $o=({\tt{img}}, {\tt{other}})$ includes a nominal monocular RGB image observation and potential other sensing modalities, e.g., depth image. In the typical imitation learning setting, a dataset of in-domain expert trajectories is provided, consisting of observation-language-action tuples $\mathcal{D} = \{(s_i, o_i, z_i, a_i)\}_{i=1}^N$. This dataset can be utilized to learn the parameters of the policy $\pi_\theta$. Most imitation learning algorithms are trained via maximum likelihood (or related methods) to maximize the objective: $\mathbb{E}_{(s_i, o_i, z_i, a_i) \sim \mathcal{D}} \left[ \log \pi_\theta\left(a_i \mid s_i, o_i, z_i \right) \right].$ This core objective can be combined with rich architectural choices such as 3D policy architectures~\citep{goyal2023rvt, ke20243d} or more expressive policy distribution classes~\citep{zhao23aloha, chi23diffusion}. However, generalization to out-of-domain to settings with semantic or visual variations has remained challenging. We study how VLMs can be used to aid the generalization of such low-level imitation learning-based policy models. %, discussed in \Cref{sec:method:vlm}. 

\textbf{Imitation Learning via Supervised Learning.}  
Imitation learning trains a policy $\pi_\theta(a \mid s, o, z)$ from expert demonstrations, where $s$ denotes proprioceptive inputs, $o$ includes perceptual observations (e.g., RGB images, depth), and $z$ provides task instructions. Given an expert dataset $\mathcal{D} = \{(s_i, o_i, z_i, a_i)\}_{i=1}^N$, the policy is optimized via maximum likelihood estimation, maximizing $\mathbb{E}_{(s_i, o_i, z_i, a_i) \sim \mathcal{D}} \left[ \log \pi_\theta\left(a_i \mid s_i, o_i, z_i \right) \right]$. Despite advancements in architectures such as 3D policy representations~\citep{goyal2023rvt, ke20243d}, generalizing to novel semantic or visual variations remains challenging. In this paper, we explore how VLMs can enhance imitation learning models for better generalization.

% \textbf{Vision Language Models.} Typical vision language models (VLMs)~\citep{vila2024, liu2024visual} are large transformers~\citep{vaswani2023attentionneed} that take vision \& text tokens as input and produce text responses. These models are pre-trained on large multimodal datasets ~\citep{zhu2023multimodal,kakaobrain2022coyo-700m}, and then finetuned on targeted high-quality datasets~\citep{Shen2021IncorporatingVL,lu2022learn}. These models tokenize each modality into a shared space to produce a sequence of output tokens corresponding to text or other output modalities. In this work, we assume access to a pre-trained, text-and-image-input VLM ~\citep{vila2024, liu2024visual}, that autoregressively outputs a sequence of text tokens conditioned on an image and previous text tokens. These pretrained VLMs can typically be finetuned using a supervised prediction loss that minimizes the negative log-likelihood of the answer text tokens.


\textbf{Vision-Language Models.}  
VLMs~\citep{vila2024, liu2024visual} are large transformer models~\citep{vaswani2023attentionneed} that accept both vision and text tokens to generate text responses. They are pre-trained on extensive multimodal datasets~\citep{zhu2023multimodal,kakaobrain2022coyo-700m} and later fine-tuned on high-quality, task-specific data~\citep{Shen2021IncorporatingVL,lu2022learn}. By tokenizing each modality into a shared space, these models autoregressively produce sequences of text tokens conditioned on an image and prior tokens. In our work, we assume access to such a pre-trained, text-and-image VLM~\citep{vila2024, liu2024visual}, further fine-tuned via a supervised loss that minimizes the negative log-likelihood of the target tokens.


