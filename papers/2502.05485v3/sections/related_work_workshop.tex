\section{Related Work}
\textbf{LLMs and VLMs for robotics.} 
Early attempts in leveraging LLMs and VLMs for robotics are through pretrained language~\citep{jang2022bc,shridhar2023perceiver,singh2023progprompt} and visual~\citep{shah2021rrl,parisi2022unsurprising,nair2023r3m,ma2023vip} models. However, these are not sufficient for complex semantic reasoning and generalization to the open world~\citep{brohan2022rt,zitkovich2023rt}. Recent research has focused on directly leveraging open world reasoning and generalization capability of LLMs and VLMs, by 
prompting or fine-tuning them to, e.g., generate plans~\citep{huang2022language,huang2023inner,lin2023text2motion,liang2023code,singh2023progprompt,brohan2023can}, construct value~\citep{huang2023voxposer} and reward functions~\citep{kwon2023reward,RoboCLIP,yu2023language,ma2024eureka,wang2024rl}. Our work is more closely related to the literature on VLA models, summarized below.

\textbf{Monolithic VLA models as language-conditioned robot policies.}
Monolithic VLA models have been proposed to produce robot actions given task description and image observations directly \citep{brohan2022rt,jiang2023vima,zitkovich2023rt,team2024octo,kim2024openvla,radosavovic2023robot}. Monolithic VLA models are often constructed from VLMs~\citep{liu2024visual,bai2023qwen,driess2023palm,vila2024}, and are trained on large-scale robot teleoperation data~\citep{brohan2022rt,open_x_embodiment_rt_x_2023,khazatsky2024droid} to predict actions as text or special tokens. However, due to the lack of coverage in existing robotics datasets, they must be finetuned in-domain on expensive teleoperated data. The most relevant monolithic VLA model is LLARVA~\citep{niu2024llarva}, which predicts end-effector trajectories in addition to robot actions. However, LLARVA does not use trajectory prediction to control the robot; rather, it uses it as an auxiliary task to improve action prediction. Therefore, LLARVA still suffers from the limitations of monolithic VLA models. In contrast, our work takes a hierarchical approach, drastically reducing the number of demonstrations needed for learning downstream tasks and the number of VLM calls per episode. Moreover, our proposed hierarchical architecture enables training from cheaper data sources while enabling considerable cross-domain transfer. 

\textbf{VLMs for predicting intermediate representations.}
Our work bears connections to prior methods using vision-language models for intermediate prediction. These methods can be categorized by the choice of predicted representation:

% \anqi{I actually feel that we should uncomment the previous sentence to so that we don't have a one-sentence paragraph.}

\emph{Point-based predictions:} A common intermediate prediction interface has been keypoint affordances~\citep{stone2023open,sundaresan2023kite,nasiriany2024pivot,yuan2024robopoint}. Some examples include using open-vocabulary detectors~\citep{minderer2022simple}, iterative prompting of VLMs~\citep{nasiriany2024pivot}, or fine-tuning detectors to identify certain parts of an object by semantics~\citep{sundaresan2023kite}. Perhaps most related, \cite{yuan2024robopoint} finetunes a VLM to predict objects of interest as well as free space for placing an object, and \cite{liu2024moka} propose a mark-based visual prompting procedure to predict keypoint affordances as well as a fixed number of waypoints. As opposed to these, our work finetunes a VLM model to not just predict points but rather entire 2D paths, making it more broadly applicable across robotic tasks. 

\emph{Trajectory-based predictions:} The idea of using trajectory-based task specifications to condition low-level policies was proposed in RT-trajectory~\citep{gu2023rttrajectory}, largely from the perspective of flexible task specification. This work also briefly discusses the possibility of combining RT-Trajectory with trajectory sketches generated from prompting a pre-trained vision language model. Complementary to RT-Trajectory, the focus of this work is less on the use of trajectory sketches for task specification, but rather on the abilities of a hierarchical VLA model to finetune the high-level VLM on cheap and abundant sources. %This could include training data such as videos or simulation data, and show transfer to test scenarios of interest with considerable visual and semantic variation. 
While RT-trajectory uses human effort or off-the-shelf pre-trained models to generate trajectories, we show that finetuning VLM models on cheap data sources can generate more accurate and generalizable trajectories (see Table.~\ref{tab:experiments:vlm}). Moreover, our instantiation of this architecture enables the incorporation of rich 3D and proprioceptive information, as compared to monocular 2D policies~\citep{gu2023rttrajectory}. 