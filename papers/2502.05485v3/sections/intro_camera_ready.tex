\section{Introduction}
% VLM based learning methods
\begingroup
\renewcommand{\thefootnote}{}
\endgroup
Developing general robot manipulation policies has been notoriously difficult. With the advent of large vision-language models (VLMs) that display compelling generalization capabilities, there is optimism that the same recipe is directly applicable to robot manipulation. A line of prior work~\citep{rt22023arxiv,kim2024openvla,black2024pi0} builds open-world vision-language-action models (VLAs) by finetuning off-the-shelf pretrained VLMs to directly produce robot actions. These VLA models, which we refer to in this work as \emph{monolithic} VLA models, rely crucially on large robotics datasets, complete with on-robot observations, e.g., images and proprioceptive states, and actions. However, on-robot data is expensive, since end-to-end observation-action pairs are typically collected on the robot hardware through, e.g., teleoperation. Despite recent community-wide efforts in building large-scale robotics datasets~\citep{open_x_embodiment_rt_x_2023,khazatsky2024droid}, the size, quality, and diversity of existing robotics datasets are still limited, and monolithic VLA models have yet to demonstrate emergent capability comparable to VLMs and LLMs in other domains of study. Moreover, monolithic VLA models are constrained by their inference frequency to achieve dexterous and dynamic manipulation tasks~\citep{rt22023arxiv, kim2024openvla}.

\begin{figure*}[t]
    \centering
    %\includegraphics[width=\linewidth]{figs/robolines_teaser.pdf}
    \includegraphics[width=\linewidth]{figs/hamster_teaser_new.pdf}
    \caption{\footnotesize{Overview of \method, VLAs and ``smaller" imitation learning methods. \method 's hierarchical design results in better generalization with a small amount of in-domain data. \method\ is able to utilize cheap training sources such as videos or simulations for enhanced generalization.}}
    \label{fig:teaser}
\vspace{-4mm}
\end{figure*}

% Imitation learning methods and their flaws
On the other hand, relatively small robot policy models have shown impressive dexterity and % geometric 
robustness. Such models have demonstrated promise across a range of complex tasks involving contact-rich manipulation and 3D reasoning, spanning domains from tabletop manipulation~\citep{shridhar2023perceiver,goyal2023rvt,goyal2024rvt,ke20243d} to fine dexterous manipulation~\citep{chi23diffusion,zhao23aloha}. Trained on relatively small datasets, these models show local robustness, and can achieve dexterous and high-precision control. However, they are often brittle to drastic changes in the environment or semantic description of the tasks~\citep{pumacay2024colosseum}. These models also can struggle to effectively leverage simulation data for real-world manipulation tasks due to sim-to-real gaps in visual appearances and system dynamics~\citep{li2024evaluating, robomimic2021}.

In this work, we ask -- how can we marry the generalization benefits of large VLMs, with the efficiency, local robustness, and dexterity of small policy models? 
% 
Our key insight is that, instead of directly predicting robot actions, VLMs can be fine-tuned to produce intermediate representations as high-level guidance on solving the robot manipulation task. The intermediate representation can then be consumed by the low-level policy model to produce actions, alleviating the low-level policy from the burden of long-horizon planning and complex, semantic reasoning. Further, if the intermediate representations are chosen such that they are \emph{1)} easily obtainable from image sequences; \emph{2)} largely embodiment agnostic; and \emph{3)} sufficiently robust to subtle changes in dynamics, the VLM can be fine-tuned with \emph{off-domain} data where robot actions are unavailable or inaccurate. Such off-domain data does not need to be collected on the actual robot hardware. Examples of off-domain data include action-free video data, simulation data, human videos, and videos of robot with different embodiments. These off-domain data are generally easier to collect and may already be abundant in existing datasets. We hypothesize, and show experimentally in Fig~\ref{fig:vlm_generalization}, that this hierarchical separation can allow VLA models to more effectively bridge the domain gap between off-domain data and in-domain robotic manipulation. 

% Since the quality and diversity of data has been shown to be keys to VLM generalization, we hypothesize that such \emph{hierarchical} VLAs, where the high-level VLM and low-level policy model are interfaced with an appropriate intermediate representation, can generalize better than monolithic VLAs.

To this end, we propose a hierarchical architecture for VLAs, \method\ (\methodlong), where large fine-tuned VLMs are connected to low-level policy models via 2D path representations\footnote{Representations similar to 2D paths has been explored in the robot learning literature~\citep{gu2023rttrajectory}, primarily as a technique for flexible task specification. We refer readers to~\cref{sec:related_work} for a detailed discussion.}. A 2D path is a coarse trajectory of the 2D image-plane position of the robot end-effector\footnote{For human video, this corresponds to the position of the palm center or fingertips.}, as well as where the gripper state changes, i.e., opens and closes (see Fig.~\ref{fig:method}). These 2D paths can be obtained cheaply and automatically from data sources such as action-free videos or physics simulations, using point tracking~\citep{doersch2023tapir,karaev2025cotracker}, hand-sketching~\citep{gu2023rttrajectory}, or proprioceptive projection. This allows \method\ can effectively leverage these abundant and inexpensive off-domain data when fine-tuning the high-level VLM. 
% Ankit: Another option, although not very crisp and clear
The hierarchical design presented in \method\ also offers additional advantages through the decoupling of VLM training and low-level action prediction. Specifically, while the higher-level VLM is predicting semantically meaningful trajectories from monocular RGB camera inputs, the lower-level policy models can additionally operate from rich 3D and proprioceptive inputs. In doing so, \method\ inherits the semantic reasoning benefits of VLMs along with the 3D reasoning and spatial awareness benefits of 3D policy models ~\citep{goyal2024rvt, ke20243d}. Moreover, the high-level VLM and low-level policy model can be queried at different frequencies 

In summary, we study a family of hierarchical VLA models \method s, where finetuned VLMs are connected to low-level 3D policy models~\citep{goyal2024rvt,ke20243d}. The 2D paths produced by high-level VLMs serve as guidance for a low-level policy that operates on rich 3D and proprioceptive inputs, allowing low-level policies to focus on robustly generating precise, spatially-aware actions. In our experiments, we observe an average of 20\% improvement in success rate over seven different axes of generalization over OpenVLA~\citep{kim2024openvla}, which amounts to 50\% relative gain, as shown in \Cref{tab:grouped_task_comparison}. Since \method\ is built on both open-source VLMs and low-level policies, it can serve as a fully open-sourced enabler for the community-building vision-language-action models. It is important to note that while we are certainly not the first to propose hierarchical VLA models~\citep{gu2023rttrajectory, nasiriany2024rt}, we propose the novel insight that this type of hierarchical decomposition allows for these models to make use of abundant off-domain data for improving real-world control. This opens the door to alternative ways of training large vision-language-action models using cheaper and more abundant data sources. 