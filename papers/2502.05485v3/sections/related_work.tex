

\section{Related Work}

\paragraph{Brainstorming below.}
Maybe: Training VLMs to output trajectory-level representations can help make simulation data useful for the real world.
\begin{itemize}
    \item \textbf{RoboPoint}~\citep{yuan2024robopoint}: This work focuses on training a VLM for predicting relevant points on an image. These points could potentially be used for downstream applications like Manipulation. In contrast, we predict end-effector trajectory using a VLM and then use it to aid a downstream manipulation trajectory. Points could be sufficient in some applications like reaching a certain location but are insufficient for describing manipulation scenarios that require a specific movement of the end-effector like wiping a cloth.
    % Trains on sim and real data, draws points on the image for M2T2. Can train on general VQA data, but points are likely not as helpful as full trajectories as an interface between VLMs and policies in general (we should show this). \yi{I think it is OK since we can just say point is another format of cheap data}
    
    \item \textbf{LLARVA}~\citep{niu2024llarva}: LLARVA also explores the use of trajectories. They 2D-trajectory prediction task could serve as an additional source of supervision. However, they predict action via an end-to-end Vision Action Model. We on the other hand study an explicit hierarchical design, where 2D-trajectory from a VLM is fed to an action model for predicting actions. 
    % Pre-trained on Open-X~\citep{o2024open}, then fine-tuned for the specific downstream task. Needs a crazy amount of downstream fine-tuning data. Predicts 2D traces as an auxiliary loss for using the VLM as a VLA with robot actions. Ablations show it helps.
    \item \textbf{RT-Trajectory}~\citep{gu2023rttrajectory}: RT-Trajectory is a closely related work which proposes to use a trajectory for communication. However, there are a couple of differences:
    -- they do not explore or develop a VLM for trajectory generation; use off the shelf which is worse; 
    -- they only consume the trajectory with a large action model like RT. In our design any action head can be used including relatively small 3D action head. Our proposal is to use the design as being more general than being specific to RT. \caelan{Are there specific aspects of training our own VLM that make it more adept in the hierarchy (other than just training on the trajectory dataset)}
    % \emph{proposes} sketches for policy conditioning. Trains with hindsight-labels but does inference exps with human drawings, human hand capture, and LLMs/VLMs. VLM: image $\rightarrow$ 2d image pos tokens, without fine-tuning. Just evaluates on 1 RT-1 style policy arch.
    % \item \textbf{MotIF: Motion Instruction Fine-tuning}~\citep{hwang2024motifmotioninstructionfinetuning}: Fine-tunes VLMs for detecting success on drawn trajectory motions of robots rather than single image states. Not too too related. [too recent, no need to compare]
    \item \textbf{KITE: Keypoint-Conditioned Policies for Semantic Manipulation}: Yes, they also have a two stage design like us. But only marks points instead of trajectory; explore the use of a prefixed library of action primitives instead of a learned policy model.
    \item \textbf{Robot-Moo: Open-World Object Manipulation using Pre-Trained Vision-Language Models} Similar to KITE -- marks only object bounding boxes.
    \item \textbf{Flow as the Cross-Domain Manipulation Interface} Generates object-centric flow; We instead create robot-centric trajectory; [To Check: Does not consider open-world generalization?]
    \item \textbf{Any-point trajectory modeling for policy learning} \yi{this and the above one, these two are prediction short term flow, which still needs full trajectory and not easy to scale up}
    \item \textbf{MOKA: Open-World Robotic Manipulation through Mark-Based Visual Prompting} Another way of prompting VLMs, complementary to our idea and can be used within our design, we compare training a VLM vs prompting.
    \item \textbf{PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs} Another way of prompting VLMs, complementary to our idea and can be used within our design, we compare training a VLM vs prompting.
    \item \textbf{RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation} different type of method - retrieval vs learning
    \item \textbf{OPTIMUS~\citep{dalal2023imitating}}: train BC agents on cheap data procedurally generated in simulation (using a task and motion planning system)
    \item \textbf{HITL-TAMP~\citep{garrett2021integrated,mandlekar2023human}}: hierarchical policy that uses Task and Motion Planning (TAMP) at the high level and BC agents at the low level
    % Also, MimicGen, SkillGen, etc.
\end{itemize}

Other:
\begin{itemize}
    \item Peract/RVT~\citep{shridhar2023perceiver,goyal2023rvt,goyal2024rvt} ...
    \item 3DDA~\citep{gervet2023act3d,ke20243d} ...
    \item GPT-4V~\citep{openai2024gpt4} ...
    \item SayCan~\citep{brohan2023can} ...
    \item ProgPrompt~\citep{singh2023progprompt} ...
\end{itemize}
