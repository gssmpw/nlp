\vspace{-10pt}
\section{Introduction}
\label{intro}

% Photorealistic 3D head avatars are essential for authentic communication in virtual and augmented environments, where capturing subtle expressions and head movements is crucial. High-quality avatars enhance experiences in telecommunication, social VR, and virtual training by ensuring accurate geometry and appearance, especially in dynamic scenarios involving face and hair deformations.

% \cc{
% I think the structure of introduction should be:
% \begin{itemize}
%     \item Codec Avatar (HQLP, MVP): +photo-realistic, -slow in rendering on device
%     \item PiCA: + fast rendering speed, high-precise geometry; - person-specific
%     \item Universal avatar: + universal, generalize to unseen id; - cannot handle hair well
%     \item Our method: + universal, layered, handle hair etc.
% \end{itemize}
% }

% Pixel Codec Avatars (PiCA)~\cite{ma2021pixel} have made significant strides in real-time avatar rendering on mobile device. PiCA uses a pixel-level decoder to dynamically adjust texture resolution in screen space, achieving adaptive rendering with efficient computation. This framework enables real-time performance by directly decoding per-pixel color, unlike traditional models that rely on fixed texture maps or vertex-based representations. 
% ~\di{not sure if we need to mention the effiency of pica since adding gaussian will degrade this. But somehow we need to motivate the use of PiCA.} However, PiCA is a personalized model, requiring per-identity training, which limits its scalability. Additionally, PiCA’s single-mesh representation struggles with accurate hair reconstruction, often producing artifacts such as hair tails growing from the shoulders or hair that fails to deform naturally with head movements.

% Recent universal avatar reconstruction models, such as Cao \etal~\cite{cao2022authentic} and \textit{URAvatar}~\cite{li2024uravatar}, enable cross-identity generalization but face challenges in accurate hair reconstruction and dynamic scenarios. Inaccurate guide meshes and simplistic representations in these methods often lead to misaligned geometry and limited hair modeling capabilities, preventing natural deformation and accurate hair reconstruction during head movements.

% To address these challenges, we propose \textbf{LUCAS}, which stands for \textbf{L}ayered \textbf{U}niversal \textbf{C}odec \textbf{A}vatars. LUCAS introduces a layered representation that separates the face and hair components, allowing them to deform independently and ensuring precise alignment. This design enables more accurate hair dynamics—using the same encoding features, but decoding them separately for the face and hair. For instance, in Fig.~\ref{fig:vis_frown}, the subject's upward gaze and frowning cause the hair to lower slightly toward the eyebrows. A single-mesh representation struggles to handle such subtle movements, as the expression code influences the entire mesh uniformly. However, LUCAS's layered structure ensures that the face and hair respond independently, leading to more realistic deformation.
% We further extend PiCA by proposing a Universal Prior Model (UPM), enabling cross-identity scalability while maintaining real-time efficiency. Additionally, LUCAS can optionally integrate Gaussian splatting with the layered structure to improve the fidelity of the reconstructed avatars at the cost of computational efficiency. Our contributions are summarized as follow:
% \begin{itemize}
% \item We present LUCAS, the first pixel codec avatar that unifies universal modeling and layered representation for face and hair, enabling both cross-identity generalization and improved face-hair alignment while maintaining real-time performance on device.
% %\item We propose a novel dehairing method to generate bald head avatars, providing the foundation for the layered approach.
% \item LUCAS's architecture allows for optional integration with Gaussian splatting to achieve higher visual fidelity, particularly for complex hairstyles.
% \item LUCAS exhibits enhanced dynamic performance in handling head pose changes, expression transfer, and hairstyle variations, including on held-out subjects in zero-shot driving scenarios.
% \end{itemize}


Photorealistic 3D head avatars are vital for authentic communication in virtual and augmented environments, where capturing subtle expressions and head movements is crucial~\cite{lombardi2018deep,wei2019vr,richard2021audio}. High-quality avatars enhance experiences in telecommunications, social VR, virtual training, and healthcare by ensuring accurate geometry and appearance, particularly in dynamic scenarios involving facial and hair deformations~\cite{chu2020expressive,schwartz2020eyes,liu2021label,liu2021refined,gao2022data,zhangli2022region,chang2022deeprecon,liu2022transfusion,he2023dealing,martin2023deep,gao2024training,liu2024lepard,liu2023deformer,liu2023deep,liu2024Instantaneous,he2024dice,han2024proxedit,zhangli2024layout,dao2025improved}.
Recent advances in Codec Avatars~\cite{lombardi2021mixture,li2023megane} have achieved remarkable photorealism through sophisticated rendering techniques and volumetric primitives. But these methods often demand significant computational resources, posing challenges for real-time rendering on mobile devices.
%Recent advances in Codec Avatars \cite{lombardi2018deep, lombardi2021mixture}
%have achieved remarkable photorealism through sophisticated rendering techniques and volumetric primitives. However, these methods often require significant computational resources, making real-time rendering on mobile devices challenging.

Pixel Codec Avatars (PiCA)~\cite{ma2021pixel} tackle performance challenges by introducing a pixel-level decoder that dynamically adjusts texture resolution in screen space. This approach enables efficient real-time rendering on mobile devices through direct per-pixel color decoding, eliminating the need for fixed texture maps or vertex-based representations. However, PiCA is a personalized model that requires time-consuming per-identity training, which limits its scalability. Additionally, its single-mesh representation struggles with accurately reconstructing hair, often resulting in artifacts such as hair tails appearing on shoulders or unnatural hair deformation during head movements.

% Universal avatar reconstruction approaches, such as~\cite{cao2022authentic} and URAvatar~\cite{li2024uravatar}, 
% Universal avatar reconstruction approaches~\cite{cao2022authentic,li2024uravatar} have made progress in cross-identity generalization, allowing the codec avatar generalzed from the universal prior model trained on multiple users data. However, these methods face significant challenges in hair modeling and dynamic scenarios. Their simplistic representations and inaccurate guide meshes often result in misaligned geometry and limited hair modeling capabilities, preventing natural deformation during expressions and head movements.

Universal avatar reconstruction approaches~\cite{cao2022authentic,li2024uravatar} have advanced cross-identity generalization, enabling codec avatars to generalize from universal prior models (UPM) trained on data from multiple users. However, these methods encounter significant challenges in hair modeling and dynamic scenarios. Their simplistic representations and inaccurate guide meshes often lead to misaligned geometry and limited hair modeling capabilities, hindering natural deformation during expressions and head movements.

%To address these challenges, we propose \textbf{LUCAS}, \textbf{L}ayered \textbf{U}niversal \textbf{C}odec \textbf{A}vatar\textbf{s}. LUCAS introduces a layered representation that separates face and hair components, allowing them to deform independently while ensuring precise alignment. This design enables more accurate hair dynamics—using the same encoding features but decoding them separately for face and hair. For instance, in Fig.~\ref{fig:vis_frown}, when the subject gazes upward and frowns, the hair naturally lowers toward the eyebrows. While a single-mesh representation struggles with such subtle movements due to uniform expression code influence, LUCAS's layered structure ensures independent and realistic face and hair deformation.
%
% To address these challenges, we propose \textbf{LUCAS}, or \textbf{L}ayered \textbf{U}niversal \textbf{C}odec \textbf{A}vatar\textbf{s}, a layered representation that separates face and hair components for independent deformation while maintaining precise alignment.  This design enables more accurate hair dynamics by using the same encoding features but decoding them separately for the face and hair. For instance, as shown in Fig.~\ref{fig:vis_frown}, when the subject gazes upward and frowns, the hair naturally lowers toward the eyebrows. While a single-mesh representation is not flexible and cannot faithfully disentangle the movement of face and hair as separate factors, LUCAS's layered structure ensures their independent and realistic deformation.

To address these challenges, we propose \textbf{LUCAS} (\textbf{L}ayered \textbf{U}niversal \textbf{C}odec \textbf{A}vatar\textbf{s}), a layered representation that separates face and hair components, allowing them to deform independently while maintaining precise alignment. This design enables more accurate hair dynamics by using shared encoding features but decoding them separately for the face and hair. For instance, as shown in Fig.~\ref{fig:vis_frown}, when the subject gazes upward and frowns, the hair naturally lowers toward the eyebrows. In contrast, single-mesh representations lack the flexibility to disentangle face and hair movements as separate factors, leading to interdependent deformations. Smoothness regularization further exacerbate this issue by enforcing coupled motion. LUCAS overcomes these limitations, enabling realistic and independent deformation for natural movement.
%
LUCAS follows a universal training strategy, training the UPM on data from multiple users, which allows it to generalize easily to unseen users and generate realistic codec avatars. 
% Additionally, LUCAS can optionally integrate 3D Gaussian Splatting with its layered structure to enhance the fidelity of reconstructed avatars, albeit at the cost of computational efficiency.
Additionally, we show that our layered mesh design improves the anchor geometry for precise and visually appealing Gaussian renderings~\cite{kerbl20233d}.
% Moreover, we propose a Universal Layered Prior Model for our layered approach, enabling cross-identity scalability while maintaining real-time efficiency. Additionally, LUCAS can optionally integrate 3D Gaussian Splatting~\cite{kerbl20233d} with the layered structure to improve the fidelity of reconstructed avatars at the cost of computational efficiency. Our contributions are:
In summary, our contributions are:
\begin{itemize}
% \item We present LUCAS, the first codec avatar that unifies universal modeling and layered representation for face and hair, enabling both cross-identity generalization and improved face-hair alignment while maintaining real-time performance on device.
% \item LUCAS's architecture allows for optional integration with Gaussian splatting to achieve higher visual fidelity, particularly for complex hairstyles.
% \item LUCAS exhibits enhanced dynamic performance in handling head pose changes, expression transfer, and hairstyle variations, including on held-out subjects in zero-shot driving scenarios.

\item We introduce LUCAS, the first mesh-based Universal Prior Model that enables cross-identity generalization while maintaining real-time rendering on devices.
\item The first compositional Universal Prior Model for the head, featuring a layered representation for both the hairless head and hair, which significantly enhances the quality of both face and hair rendering.
\item LUCAS demonstrates improved dynamic performance in managing head pose changes, expression transfers, and hairstyle variations, even on unseen subjects in zero-shot driving scenarios.

\end{itemize}

% \cc{
% I think the structure of introduction should be:
% \begin{itemize}
%     \item Codec Avatar (HQLP, MVP): +photo-realistic, -slow in rendering on device
%     \item PiCA: + fast rendering speed, high-precise geometry; - person-specific
%     \item Universal avatar: + universal, generalize to unseen id; - cannot handle hair well
%     \item Our method: + universal, layered, handle hair etc.
% \end{itemize}
% }

% ~\di{I rewrote the intro section following chen's suggestion. Giljoo please check.}

\begin{figure}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/vis_frown.pdf}
\vspace{-20pt}
\caption{
\textbf{Layered representation enables adaptive alignment between face and hair.} LUCAS's independent face and hair deformation captures subtle hair movements in response to facial expressions, unlike single-mesh avatars that are globally controlled.
}
\label{fig:vis_frown}
\vspace{-20pt}
\end{figure}