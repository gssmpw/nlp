\section{Related Works}
\noindent \textbf{3D Head Avatar Reconstruction.}
Early approaches to head avatar reconstruction were primarily based on 3D Morphable Face Models (3DMFMs)~\cite{blanz2023morphable}, which used linear combinations of prototype vectors for shape and texture generation, later extended with blendshapes for animation~\cite{lewis2014practice}. However, these manual blendshape-based approaches were limited in expressiveness and required significant effort to create. Deep learning has revolutionized this field, introducing non-linear models through VAEs~\cite{ranjan2018generating} and GANs~\cite{shamai2019synthesizing} for more complex facial representations. Lombardi \etal~\cite{lombardi2018deep} pioneered joint modeling of shape and appearance using VAEs, while works like Bagautdinov \etal~\cite{bagautdinov2018modeling} and Ranjan \etal~\cite{ranjan2018generating} employed mesh convolutions for detailed geometry capture. FLAME~\cite{li2017learning} incorporated linear blend skinning for jaw and neck movements but had difficulty conveying subtle expressions.
%
Recent advances have focused on improving rendering quality and generalization. The Pixel Codec Avatar (PiCA)~\cite{ma2021pixel} introduced dynamic texture resolution through pixel-based decoding, departing from traditional fixed texture maps~\cite{lombardi2018deep} and vertex-based representations~\cite{zhou2019dense}. However, PiCA's subject-specific nature and single-mesh representation limit its scalability and hair modeling capabilities. Universal models like LatentAvatar~\cite{xu2023latentavatar} and URAvatar~\cite{li2024uravatar} have attempted to address generalization across identities, but often struggle with preserving person-specific details and handling large deformations, particularly in hair regions. Cao \etal~\cite{cao2022authentic} proposed a shared expression space across identities, but accurate guide meshes remained challenging, affecting reconstruction quality.
Our work addresses these limitations by combining PiCA's efficient and accurate pixel-based rendering with a Universal Prior Model for cross-identity generalization. Crucially, we introduce a layered representation that separates face and hair components, enabling better alignment and optimization compared to single-mesh approaches while maintaining high visual fidelity across different identities, expressions and poses.

\noindent \textbf{3D Hair Modeling.}
Hair modeling in 3D avatar reconstruction has been explored through various approaches. Traditional strand-based methods, whether using multiview stereo~\cite{paris2008hair, luo2012multi} or single-view inference~\cite{chai2012single, zhou2018hairnet, zheng2023hairstep,wu2024monohair}, focus on explicit strand geometry recovery. While these methods can achieve high geometric accuracy, they are often computationally intensive and impractical for mobile VR applications. Alternative approaches have explored different representations for hair modeling. HeadCraft~\cite{sevastopolsky2025headcraft} combines parametric head models with StyleGAN-generated displacement maps for animation control and detail preservation. Volumetric methods like Neural Volumes~\cite{lombardi2019neural} and MVP~\cite{lombardi2021mixture} have demonstrated impressive results in hair rendering, with HVH~\cite{wang2022hvh} and NeuWigs~\cite{wang2023neuwigs} further improving hair animation through layered modeling. However, these person-specific models often struggle with generalization to novel identities.
While recent works have attempted to address generalization through pixel-aligned information~\cite{raj2021pixel} or cross-identity hypernetworks~\cite{cao2022authentic}, they either face challenges with complex geometry or depend heavily on precise head mesh tracking. Our method focuses on a universal compositional representation that separately models face and hair components using efficient mesh-based representations, enabling real-time rendering while maintaining visual quality across diverse hairstyles and identities.


\noindent \textbf{Compositional Avatar Representation.}
Compositional modeling has emerged as a promising direction for improving the quality and controllability of 3D avatars.
% Recent works have explored this approach for different aspects of human representation. 
For face and accessories, MEGANE~\cite{li2023megane} demonstrated the benefits of compositional modeling by combining surface geometry and volumetric representation for eyeglasses, enabling accurate geometric and photometric interactions with faces. DELTA~\cite{feng2023learning} proposed a hybrid explicit-implicit representation to disentangle face and hair components, but primarily focused on static reconstruction and hairstyle transfer.
For head avatars, RGCA~\cite{saito2024relightable} and URAvatar~\cite{li2024uravatar} have shown the advantages of separately modeling head and eye regions for better eye dynamics and relighting effects.
% In the domain of full-body reconstruction, w
Works like GALA~\cite{kim2024gala} and LayGA~\cite{lin2024layga} have introduced layered representations that decompose body and clothing, showing improved results in clothing dynamics and detail preservation. TECA~\cite{zhang2024teca} further extends compositional modeling to text-guided avatar generation.
% , using different representations (meshes and NeRFs) for different components to better handle their distinct structural qualities.
Unlike previous works that treat the head as a single entity or focus on static composition, we introduce a layered representation specifically designed to capture the complex dynamic interactions between face and hair during expressions and pose changes. Our approach uniquely combines compositional modeling with a universal prior model, enabling consistent expression transfer and pose-dependent hair dynamics across different identities.

% \di{Giljoo please help refine and shorten the related work.}