% \section{Demo Video}

% We provide a demo video in the supplementary material, which includes more visual results of our work. Specifically, it contains:
% \begin{itemize}
%     \item Evaluation of the expression code on synchronized control of face and hair deformation.
%     \item Evaluation of the proposed layered representation on expression, pose and hair animation.
%     \item Visualization and comparison of avatar driving against the state-of-the-art.
%     \item Visualization of avatar driving on unseen identities.
%     \item Visualization of avatar driving with hairstyle changing.
% \end{itemize}

% \section{Data acquisition.} 
% %We adopt a similar setup as~\cite{cao2022authentic}, capturing calibrated and synchronized multi-view images at a resolution of 4096 × 2668 using 110 cameras and 460 white LED lights, operating at 90 Hz. 
% We adopt a similar setup as~\cite{cao2022authentic}, running calibrated and synchronized multi-view captures using 110 cameras and 460 white LED lights.
% %
% Image are captured at a resolution of 4096 $\times$ 2668 at 90 Hz.
% %
% Participants are asked to perform a predefined set of facial expressions, sentences, and gaze motions, generating approximately 144,000 frames per subject. 
% %
% To collect diverse illumination patterns while ensuring stable facial tracking, we use time-multiplexed illumination. Specifically, every third frame is fully illuminated to facilitate tracking, while the remaining frames are lit using random or grouped sets of 5 lights. 
% %
% As in~\cite{cao2022authentic}, we track a topologically consistent coarse mesh using the fully illuminated frames and further stabilize head pose using the mode pursuit method from~\cite{lamarre2018face}. 
% %
% The tracked meshes, head poses, and averaged unwrapped textures are interpolated to the partially lit frames for subsequent avatar training.
% \rongyu{We may not need so many details for the data capturing.}
% \di{moved to supp.}
\section{Network Architecture}
In this section, we provide more details of our network architecture and hyperparameters for our id-conditioned hypernetwork, appearance deocder, geometry decoder, Gaussian hypernetwork and Gaussian decoder, respectively.

\noindent \textbf{Identity-conditioned hypernetwork.}
We adopt a U-Net~\cite{ronneberger2015u} architecture as our identity-conditioned hypernetwork that takes as input the neutral geometry and texture of a subject to predict subject-specific decoder parameters. The network consists of two parallel downsampling branches that process geometry and texture features separately, followed by a joint upsampling branch. The input geometry and texture are first normalized by subtracting their respective means and dividing by their standard deviations. The geometry branch processes the normalized geometry (scaled by 0.2) while the texture branch processes the normalized texture (scaled by 0.4) through a series of downsampling blocks. 
The network's channel dimensions progressively increase through the layers with sizes of (3, 32, 64, 128, 256, 256, 512, 512, 512, 512, 256). Features from both branches are concatenated at each scale and processed through the upsampling branch. The network generates three types of outputs through Transfer modules (Fig.~\ref{fig:supp_block}(a)): untied bias parameters ($\Theta_{\text{id}}$), per-identity geometry displacement ($d$), and per-identity positional encoding ($f$). Each Transfer module consists of two weight-normalized convolution (Conv2DWN) layers with learnable biases, mapping the concatenated features through a hidden dimension of 512 channels, followed by LeakyReLU activation ($\alpha=0.2$), before outputting the final parameters.
For both face and hair branches, six texture bias parameters are generated for the appearance decoder, corresponding to different resolution levels. The geometry decoder receives five geometry displacement parameters for its Block modules, with an additional displacement map ($d$) for the output layer. The pixel decoder receives a positional encoding ($f$) of dimension [4, 1024, 1024] generated by processing the concatenation of the final upsampled features and the input texture through a Transfer module.
The architecture leverages weight normalization throughout its convolution and linear layers for stable training, with careful initialization using Glorot initialization scaled by 0.2 for most layers and 1.0 for the final convolution layers in the Transfer modules.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/supp_block.pdf}
\caption{\textbf{Architecture of (a) Transfer module and (b) Block module.} (a) Transfer Block processes features through two Conv2dWN layers with intermediate LeakyReLU activation and scaled learnable bias. (b) Block module combines Conv2dWN, LeakyReLU activation, and pixel shuffle operations for feature transformation and upsampling.}
\label{fig:supp_block}
\end{figure}

\noindent \textbf{Appearance decoder.}
The face appearance decoder $\mathcal{D}_e^\text{face}$ processes expression encoding $z$, view-dependent conditions $\omega$ and neck pose $\eta$ to generate detailed textures. Specifically, $z \in \mathbb{R}^{16}$ is first processed by a Block module Fig.~\ref{fig:supp_block}(b)) to obtain features in $\mathbb{R}^{128}$, while the view direction encoding $\omega \in \mathbb{R}^3$ and neck pose $\eta \in \mathbb{R}^6$ are transformed through a linear layer and LeakyReLU activation to $\mathbb{R}^{16}$. These combined features are then processed through a cascade of Block modules that progressively upsample the spatial resolution from $8\times8$ to $256\times256$ while reducing the channel dimensions ($160, 64, 32, 16, 12, 8, 4$). The view-dependent conditioning enables the network to capture view-dependent effects like specular highlights and shading variations. For hair appearance decoder $\mathcal{D}_e^\text{hair}$, we use a similar network architecture with additional head pose $h$ encoded to $\mathbb{R}^{16}$ through linear layers and LeakyReLU activations, resulting in a $176$-dimensional feature vector after concatenation.

\noindent \textbf{Geometry decoder.}
The face geometry decoder $\mathcal{D}_g^\text{face}$ combines expression encoding $z$ with neck pose parameters ${\eta}$ to capture expression-dependent geometry deformations and neck movements. The input $z \in \mathbb{R}^{16}$ is first processed by a Block module, expanding the features to $\mathbb{R}^{128}$. Meanwhile, the neck pose ${\eta}$ is encoded through a linear layer and LeakyReLU to $\mathbb{R}^{16}$. These concatenated features are processed through the same progressive upsampling architecture as the appearance decoder. Notably, the geometry decoder includes a mask output that helps handle occlusions or invalid regions that may occur due to extreme neck poses or expressions. For hair geometry decoder $\mathcal{D}_g^\text{hair}$, the network additionally conditions on head pose $h$, which is encoded similarly to $\eta$, resulting in a $160$-dimensional feature vector after concatenation.

\noindent \textbf{Gaussian-splatting hypernetwork.}
As shown in Fig.~\ref{fig:supp_framework_gs_hyper}, our Gaussian hypernetwork $\mathcal{E}_\text{id}^\text{gs}$ shares the same U-Net backbone as the PiCA hypernetwork, consisting of two parallel downsampling branches that process geometry $\textbf{G}_{\text{neu}}$ and texture $\textbf{T}_{\text{neu}}$ features separately. Through the dedicated downsampling branches, the input geometry and texture are first normalized ($0.2\times$ and $0.4\times$ respectively) and processed into multi-scale feature maps. The resulting features are concatenated at each scale and processed through an upsampling network. Through a sequence of transfer modules, the network predicts the identity-specific bias map $\Theta_\text{id}^{\text{gs}}$ and extracts mean color attributes $d^c_\text{mean}$ from the neutral appearance data, as formulated in Eq.~{\color{red}7} of the main paper. These bias parameters control the position, rotation, scale, opacity and color attribute of the splatted Gaussians anchored at mesh vertices $\{\hat{t}_k\}_{k=1}^M$, allowing for person-specific rendering characteristics. The same architecture is employed for both face and hair branches, enabling joint optimization of all rendering components through a unified hypernetwork backbone.
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{figs/supp_framework_gs_hyper.pdf}
\caption{\textbf{Overview of Gaussian hypernetwork.} The network processes normalized geometry $\textbf{G}_{\text{neu}}$ and texture $\textbf{T}_{\text{neu}}$ through parallel branches to predict identity-specific bias parameters $\Theta_\text{id}^{\text{gs}}$ and mean color attributes $d^c_\text{mean}$ for Gaussian rendering. The same architecture is used for both face and hair branches.}
\label{fig:supp_framework_gs_hyper}
\end{figure}

\noindent \textbf{Gaussian decoder.} 
The Gaussian decoder $\mathcal{D}^{\text{gs}}$ consists of a progressive upsampling network that transforms input features to Gaussian attributes. Taking as input a 128-dimensional expression encoding concatenated with a 16-dimensional neck pose encoding $\eta$, the network first processes them through two MLP layers to obtain $8\times8$ feature maps. These features are then gradually upsampled through a series of transposed convolution layers with LeakyReLU activations ($\alpha=0.2$), expanding the spatial resolution from $8\times8$ to $1024\times1024$ while progressively reducing the channel dimensions ($256, 128, 64, 32, 16, 59$). The final layer outputs a 59-channel feature map, where the first 49 channels encode the spherical harmonics coefficients for appearance, and the remaining 10 channels encode the Gaussian attributes: position delta $\delta t_k$, rotation quaternion $q_k$, and scale $s_k$. The quaternions are normalized and scales are constrained through a softplus activation multiplied by 0.5. An additional sigmoid activation is applied to the first spherical harmonic coefficient to obtain the opacity $o_k$. The face and hair branches share the same architecture but operate independently to handle their respective geometry and appearance characteristics.

\noindent \textbf{Gaussian Rendering.} 
After obtaining the Gaussian attributes from both face and hair decoders, we concatenate their features for joint rendering. Let $N = N_f + N_h$ denote the total number of Gaussians, where $N_f$ and $N_h$ represent face and hair Gaussians respectively. We combine their position deltas $\delta t \in \mathbb{R}^{N\times3}$, rotation quaternions $q \in \mathbb{R}^{N\times4}$, scales $s \in \mathbb{R}^{N\times3}$, opacities $o \in \mathbb{R}^{N\times1}$, and spherical harmonics coefficients $d^c \in \mathbb{R}^{N\times48}$. 
For RGB rendering, the color attribute $c_k \in \mathbb{R}^{48}$ for the $k$-th Gaussian is computed as:
\begin{equation}
c_k = 
\begin{bmatrix}
    d_k^{c,\text{base}} + d^c_\text{mean} \\
     \beta \cdot d_k^{c,\text{ho}}
\end{bmatrix}
\end{equation}
%
where $d_k^{c,\text{base}} \in \mathbb{R}^3 $ is the base color component (first three coefficients), $d_{\text{mean}}^c \in \mathbb{R}^3 $ is the mean color vector, $d_k^{c,\text{ho}} \in \mathbb{R}^{45}$ represents the higher-order coefficients (remaining 45 coefficients). $\beta$ is a scaling factor for the higher-order terms where we set as 0.05 in our experiment.
% For segmentation mask rendering, we assign face and hair Gaussians with different ID values (1 for face, 2 for hair) to obtain per-pixel part labels. The rendering is performed using a GPU-accelerated EGL-based OpenGL renderer, which rasterizes each Gaussian splat according to its 3D position, scale, and rotation under the given camera parameters, and composites them in a front-to-back order to generate the final RGB image or segmentation mask.



\section{Training details}
% We first train personalized model for all identities. Then we calculate the average geometry and texture for each identity and take them as static assets for the universal prior model training.


% \noindent \textbf{Training strategy.} 
Our model follows a two-phase training process. In the first phase, we train only the PiCA branch using reconstruction loss $\mathcal{L}_{\text{pica}}$ and dehairing loss $\mathcal{L}_{\text{dehair}}$. Subsequently, we freeze these weights and train the Gaussian branch with loss $\mathcal{L}_{\text{gs}}$.
The loss weights are configured as follows:
\begin{itemize}
    \item Gaussian loss $\mathcal{L}_{\text{gs}}$: $\lambda_{\text{render}}$ = 10.0, $\lambda_{\text{scale}}$ = 0.2, $\lambda_{\Delta}$ = 0.01;
    \item PiCA loss $\mathcal{L}_{\text{pica}}$: $\lambda_I$ = 4.0, $\lambda_D$ = 10.0, $\lambda_{N}$ = 1.0, $\lambda_{M}$ = 0.1, $\lambda_{S}$ = 4.0, $\lambda_{KL}$ = 0.001, $\lambda_{seg}$ = 2.0;
    \item Dehairing loss $\mathcal{L}_{\text{dehair}}$: initially $\lambda_{\text{dehair}}$ = 20.0, decaying to 0 between 70k and 80k iterations;
    \item $\lambda_{\text{pica}}$ = $\lambda_{\text{gs}}$ = $\lambda_{\text{dehair}}$ = 1.0.
\end{itemize}
We adopt L1 loss for both $\mathcal{L}_{I}$ and $\mathcal{L}_{\text{render}}$ due to its effectiveness in preserving fine hair details. Hair segmentation regularization $\mathcal{L}_{\text{seg}}$ is restricted to the first phase to avoid Gaussian blur artifacts between hair strands. The high initial weight of dehairing loss $\mathcal{L}_{\text{dehair}}$ accelerates the convergence of bald geometry, ensuring accurate dehaired results without interference from the hair mesh. The model is trained using Adam optimizer with a learning rate of 0.001 for 300k (PiCA branch) + 300k  (GS branch) iterations on 4 NVIDIA A100 GPUs.
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/supp_dehair.pdf}
\caption{\textbf{More visualization of avatar dehairing.} Our method successfully removes hair while preserving the underlying head geometry across subjects with diverse hairstyles. }
\label{fig:supp_dehair}
\end{figure}

\section{Discussion}
\noindent \textbf{Avatar dehairing.}
We provide more visualization of the dehairing results in Fig.~\ref{fig:supp_dehair}. The examples demonstrate that our method can effectively remove diverse hairstyles while maintaining accurate head shape and facial features.

\noindent \textbf{Impact of training strategy.} We investigate two different training strategies for our Gaussian model: two-stage training and joint training. In the two-stage approach, we first train the mesh model with $\mathcal{L}_\text{pica}$ and $\mathcal{L}_\text{dehair}$ for 300k iterations, then freeze the mesh branch and train the Gaussian branch with $\mathcal{L}_\text{pica}$ for another 300k iterations. In the joint training approach, we train both branches simultaneously with all losses enabled. As shown in Table~\ref{tab:training_strategy}, both strategies achieve comparable performance across all metrics, with differences being negligible (PSNR: ±0.003, SSIM: ±0.0005, LPIPS: ±0.0007). However, we observe that joint training exhibits significant instability during the initial 10k iterations, often leading to training explosions that substantially delay the convergence process. Given these findings, we adopt the two-stage training strategy in our final model for its superior training stability while maintaining equivalent performance.
\begin{table}[t]
\centering
\caption{\textbf{Ablation study} on training strategy. While both strategies achieve similar final performance, two-stage training offers better stability during the optimization process.
}

\renewcommand\tabcolsep{12pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Strategy & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule
Two-stage training & \textbf{34.5607} & 0.9196 & \textbf{0.2387} \\
Joint training &{34.5579} & \textbf{0.9201} & {0.2394} \\
\bottomrule
\end{tabular}}
% 
\label{tab:training_strategy}
\end{table}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/supp_dp.pdf}

\caption{\textbf{Impact of pixel decoder $\mathcal{D}_p$.} The pixel decoder helps achieve better facial details and overall fidelity.}
\label{fig:supp_dp}
\end{figure}

\noindent \textbf{Impact of pixel decoder.} To understand the role of the pixel decoder in our Gaussian model, we conduct experiments by removing $\mathcal{D}_p$ and supervising the reconstruction solely through Gaussian rendering loss. Note that this ablation is conducted with joint training of the mesh and Gaussian branches. As shown in Fig.~\ref{fig:supp_dp}, models with $\mathcal{D}_p$ achieve noticeably better visual fidelity compared to those without. The quantitative results in Table.~\ref{tab:ab_p} further support this observation, with our full model outperforming the variant without $\mathcal{D}_p$ across most metrics, particularly on unseen subjects. We attribute this improvement to the pixel decoder's ability to enhance the underlying avatar geometry, which in turn provides better spatial anchoring for Gaussian rendering. Notably, the performance gap widens on unseen subjects, suggesting that $\mathcal{D}_p$ contributes to better generalization of our model. Furthermore, even without the pixel decoder, our model significantly outperforms URAvatar~\cite{li2024uravatar}, demonstrating the inherent advantage of our layered representation design.




\begin{table}[t]
\centering
\caption{\textbf{Ablation study} on pixel decoder $\mathcal{D}_p$. We evaluate our universal model on both training and unseen subjects. The top two techniques are highlighted in \textcolor{red!50}{red} and \textcolor{yellow!100}{yellow}, respectively.}

\renewcommand\tabcolsep{2pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Training subjects} & \multicolumn{3}{c}{Unseen subjects} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\multirow{1}{*}{Method}  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule
w/o $\mathcal{D}_p$ & \cellcolor{yellow!25}34.2113 & \cellcolor{yellow!25}0.9164 & \cellcolor{red!25}0.2390 & \cellcolor{yellow!25}32.1328 & \cellcolor{yellow!25}0.8993 & \cellcolor{yellow!25}0.2607 \\

URAvatar & 33.1209 & 0.9021 & 0.2493 & 31.4462 & 0.8922 & 0.2625\\

Full model & \cellcolor{red!25}{34.5579} & \cellcolor{red!25}{0.9201} & \cellcolor{yellow!25}{0.2394}  & \cellcolor{red!25}{32.5847} & \cellcolor{red!25}{0.9087} & \cellcolor{red!25}{0.2496} \\
\bottomrule
\end{tabular}}

\label{tab:ab_p}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/supp_gs_delta.pdf}

\caption{\textbf{Impact of Gaussian position prior regularization.} The position delta loss $\mathcal{L}_\Delta$ effectively constrains the Gaussians to stay within their respective regions, leading to clean boundaries between hair and bald head areas and faithful reproduction of the ground truth appearance.}
\label{fig:supp_gs_delta}
\end{figure}

% \begin{table}[t]
% \centering
% \caption{Ablation study on the number of training subjects. The top three techniques are highlighted in \textcolor{red!50}{red}, \textcolor{orange!50}{orange}, and \textcolor{yellow!100}{yellow}, respectively. }
% 
% \renewcommand\tabcolsep{17pt}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
% \midrule
% 4 & 30.5284 & 0.8749 & 0.2690\\
% 8 & 31.5512 & 0.8825 & 0.2608   \\
% 16 & 32.0852 & 0.9022 & 0.2557\\
% 32 & 33.5858 & 0.9089 & 0.2492 \\
% 48 & 33.9327 & 0.9136 & 0.2457 \\
% 64 & 34.2785 & 0.9175 & 0.2424\\
% 76 & 34.5579  & 0.9201 & 0.2394\\

% \bottomrule
% \end{tabular}}
% 
% \label{tab:subject_number}
% \end{table}

% \begin{table}[t]
% \centering
% \caption{Ablation study on the number of training subjects. The top three techniques are highlighted in \textcolor{red!50}{red}, \textcolor{orange!50}{orange}, and \textcolor{yellow!100}{yellow}, respectively. }
% 
% \renewcommand\tabcolsep{17pt}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{lccc}
% \toprule
% Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
% \midrule
% 4 \\
% 8 & 32.0512 & 0.8895 & 0.2678   \\
% 16 \\
% 32 \\
% 48 \\
% 64 \\
% 76 & 33.0254 & 0.9073  & 0.2537 \\


% \bottomrule
% \end{tabular}}
% 
% \label{tab:subject_number}
% \end{table}

\noindent \textbf{Impact of Gaussian position prior regularization.} In Fig.~\ref{fig:supp_gs_delta} we show the Gaussian shapes for the hair and bald head regions. With the position delta loss $\mathcal{L}_\Delta$, our model effectively maintains the Gaussian primitives in their designated regions - hair Gaussians properly represent the hair volume while face Gaussians accurately cover the bald head area. As shown in the visualization, the Gaussian distributions align well with the ground truth appearance, leading to high-quality rendered results. The clear separation between hair and bald head Gaussians demonstrates the effectiveness of our position regularization.




\begin{table}[t]
\centering
\caption{\textbf{Inference time comparison} of different avatar reconstruction methods. All times measured on NVIDIA A100.}

\renewcommand\tabcolsep{28pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l cc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Inference Time (ms)} \\
\cmidrule{2-3}
& $1024^2$ & $512^2$ \\
\midrule
% $^{\dagger}$PiCA (mesh)~\cite{ma2021pixel} & - & - \\
% $^{\dagger}$LUCAS (mesh) & - & - \\
% $^{\dagger}$LUCAS & 11.98 & 6.55 \\
% \midrule
% $^*$uPiCA (mesh) & - & - \\
% $^*$LUCAS (mesh) & - & - \\
URAvatar~\cite{li2024uravatar} & 12.84 & 6.40 \\
LUCAS & 12.01 & 6.42 \\
\bottomrule
\end{tabular}}
\label{tab:inference_time}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/supp_hair_switch.pdf}
\caption{\textbf{Application on hairstyle switching.} We combine face condition from subject B, hair condition from subject C, and expressions from subject A. Note how our model maintains high-fidelity facial details while accurately preserving the characteristics of both the chosen face and hairstyle.}
\label{fig:supp_hair_switch}
\end{figure}

\noindent \textbf{Impact of training data.} 
To understand how the number of training identities impacts our model's performance, we conducted experiments with varying numbers of subjects in the training set. Specifically, we tested our model using seven different training scales: 4, 8, 16, 32, 48, 64, and 76 identities. The model's performance shows consistent enhancement as we increase the training set size. This trend highlights the importance of diverse training samples in building robust prior representations.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figs/subject_num_comparison.png}
\caption{\textbf{Ablation study} on the number of training subjects. The model's performance improves consistently with larger training sets, demonstrating the importance of diverse subjects for learning robust priors. }
\label{fig:num_ids}
\end{figure*}
\noindent \textbf{Rendering Performance.}
For all identities, we use a layered structure with separate Gaussian representations for face and hair. We experiment with two configurations: a high-quality setting using $M$ = 1024 × 1024 = 1 Mi Gaussians total (0.5 Mi each for face and hair), and a faster setting with $M$ = 512 × 512 = 0.25 Mi Gaussians total. We observe that increasing the number of Gaussians leads to quality improvement at the cost of slower rendering. As shown in Tab.~\ref{tab:inference_time}, our complete model with 1 Mi Gaussians takes 12.01 ms for rendering, while reducing to 0.25 Mi Gaussians achieves faster rendering at 6.42 ms on NVIDIA A100. Our method achieves comparable rendering speed to other Gaussian Splatting-based approaches. 
% All Gaussian-based models require 600 K iterations for convergence, which is approximately twice the training time compared to mesh-based approaches.



\section{Application}
We can independently control the hair and face appearance by using condition data from different identities. In Fig.~\ref{fig:supp_hair_switch}, we demonstrate this capability by combining the face from one subject, hair from another, and driving expressions using a third subject. Our model successfully preserves facial details like wrinkles while maintaining the distinct characteristics of the chosen face and hair styles, demonstrating its ability to decompose and recombine these elements effectively.


