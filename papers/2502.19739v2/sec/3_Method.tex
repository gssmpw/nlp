\section{Methods} Our approach starts with generating assets for our captured and tracked datasets (Sec.\ref{sec:dataset}), which support the universal layered prior model (Sec.\ref{sec:upm_layered}), based on a novel mesh-based UPM (Sec.\ref{sec:upm_pica}). Our layered design improves anchor geometry for precise Gaussian renderings (Sec.\ref{sec:urgca}). Loss functions and training details are outlined in Sec.~\ref{sec:training_losses}.
\begin{figure}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/dehair.pdf}
\vspace{-15pt}
\caption{
\textbf{Dehaired Head and Hair Geometries.} Our method precisely disentangles dehaired head from hair for different users.}
\label{fig:dehair}
\vspace{-10pt}
\end{figure}

\subsection{Dataset and Assets}
\label{sec:dataset}
% We employ the multi-view capture system described by Cao \etal~\cite{cao2022authentic} to record the facial performances of 76 participants. Each participant's dataset comprises approximately 14,000 frames captured from 110 distinct camera angles. Notably, 12 of these participants are bald. Initially, we tracked the geometries in the datasets of these 12 bald participants and constructed a Principal Component Analysis (PCA) model~\cite{abdi2010principal} based on these geometries. This PCA model serves as a re-projection loss during the tracking of geometries in the datasets of other participants, ensuring precise supervision of the bald head geometry by using the dehaired mesh as the ground truth. Additionally, we deform a hair template with 20k vertices to track each participant's hair on top of these dehaired geometries. Examples of dehaired head and hair geometries are illustrated in~Fig.~\ref{fig:dehair}.

We use the multi-view capture system from Cao \etal~\cite{cao2022authentic} to record facial performances of 76 identities 
%which comprises approximately of 14,000 frames 
captured from 110 distinct cameras.
%
To learn 2D hair segmentation textures for each identity, we add predicted segmentation masks from HRNet~\cite{Wang2019DeepHR}, trained on our in-house dataset. 
%
Notably, 5 identities are bald.
% 
Starting with these bald individuals, we iteratively build a linear deformable model~\cite{Blanz1999AMM} of bald head geometry, gradually expanding by “dehairing” the next participant with least amount of hair until covering all 76 identities, see Fig.~\ref{fig:dehair}.
% 
We learn the linear deformable model by using Expectation Maximization (EM) for factor analysis\cite{Ghahramani1996TheEA}, following Torresani~\cite{NIPS2003_8db92642}, and find that adding Laplacian smoothness loss to the M-step can help regularize shapes yielding better results compared to vanilla PCA~\cite{abdi2010principal}.
%
Dehairing is performed by computing the expected values of latent variables similar to the E-step by only using the observed data (excluding hair-covered areas) which in turn is used to infer the hidden bald geometry which we use to inpaint the hair regions, stitched using~\cite{SorkineHornung2007AsrigidaspossibleSM}.
% 
% Examples of dehaired head and hair geometries are illustrated in~Fig.~\ref{fig:dehair}.

%\cc{Di could you fill the numbers here?}~\di{done.} 
% \di{BTW, we had this dataset section but moved to supp due to space limit.}

\begin{figure*}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/framework.pdf}
\vspace{-20pt}
\caption{
\textbf{Overview of LUCAS.} (a) Our identity-conditioned hypernetwork $\mathcal{E}_\text{id}^\text{face}$/$\mathcal{E}_\text{id}^\text{hair}$ generates identity-specific features $\{f, d\}$ and untied biases $\Theta_\text{id}$ from neutral geometry and appearance data. (b) The expression encoder $\mathcal{E}_\text{exp}$ learns a unified expression code space that enables consistent expression transfer across identities. (c) Given expression code $z$, view direction $\omega$, and poses $\{h, \eta\}$, our compositional avatar decoder $\mathcal{D}^\text{face}$/$\mathcal{D}^\text{hair}$ produces separate geometry and appearance maps for face and hair. These are combined with mean geometry and geometry displacement for multi-mesh rendering, followed by separate pixel decoders for the final avatar image generation.
}
\label{fig:flowchart}
% \vspace{-2mm}
\end{figure*}

% (a) Our universal layered prior model takes neutral geometry and appearance data as input, and produces identity-specific constant features $f$, geometry displacements $d$, and untied biases $\Theta_\text{id}$ for person specific avatar generation. (b) Given an expression code $z$, view direction $\omega$, head and neck poses, $h$ and $\eta$, our model decodes the geometry displacements ($\mathbf{g}^\text{face}$, $\mathbf{g}^\text{hair}$) and appearance maps ($\mathbf{e}^\text{face}$, $\mathbf{e}^\text{hair}$) for face and hair. We incorporate the corresponding mean geometry ($\mathbf{g}_\text{mean}^\text{face}$, $\mathbf{g}_\text{mean}^\text{hair}$) and displacement bias to obtain the final geometry for multi-mesh rendering. The rendered feature image is further passed through two separate pixel decoders,$\mathcal{D}_p^\text{face}$ and $\mathcal{D}_p^\text{hair}$ , with the corresponding masks and constant features $f^\text{face}$, $f^\text{hair}$, to decode the final avatar image.

% Our goal is to build a universal layered shape and appearance model for human heads that can be rendered for any identity in real-time. To  this end, we learn a relightable appearance model from cross- identity light-stage captures based on grouped point lights. In  this  section,  we  will  introduce  our  learning  framework, LUCAS, which learns to relight target hands in dif- ferent  poses  and  views.   The  core  of  our  model  is  a  compositional that preserves the lin- earity of light transport, which enables the generalization to arbitrary illuminations by training with monochrome group lights  only.    Our  model  consists  of  two  parallel  render- ing  branches,  physical  and  neural.   The  physical  branch (Sec. 4.1) focuses on refining geometry and providing accu- rate shading features as an illumination proxy for the neural branch. The neural branch (Sec. 4.2) learns the final appear- ance of hands with global illumination. These two branches are trained jointly in an end-to-end manner with our tailored loss functions (Sec. 4.3). Finally, we use this universal prior to quickly personalize a relightable hand model from few- shot observations (Sec. 5.4).

%\subsection{Assets Generation}
%\label{sec:assets_generation}
%\noindent \textbf{Avatar dehairing.}
%To enable the creation of a layered representation, we first generate a dehairing model by isolating the bald head geometry.
%~\rongyu{Which avatar it is? uPiCA, coarse mesh tracking or something else?}
%
%We build a Principal Component Analysis (PCA)~\cite{abdi2010principal} model using subjects without hair, which serves as a re-projection loss when training a dehairing model on subjects with hair.
%~\rongyu{What's this dehairing model? What's the input and output? Is the output per-frame or per-identity.} 
%This process ensures accurate supervision for the bald head geometry by using the dehaired mesh as ground truth. An example of avatar dehairing is shown in~Fig.~\ref{fig:dehair}.
%~\rongyu{Some details are missing here, we need to clarify the inputs and outputs of dehairing and the method used (more details on reprojection loss)}~\di{need some help from Stan and Chen.}



\subsection{Universal Prior Model for Pixel Codec Avatars}
\label{sec:upm_pica}

Pixel Codec Avatars (PiCA)~\cite{ma2021pixel} offers precise mesh tracking and real-time rendering but limited to personalized models. 
%
%Inspired by~\cite{cao2022authentic}, we propose a Universal Prior Model (UPM) for PiCA. 
Inspired by~\cite{cao2022authentic}, we extend Personalized PiCA with cross-identity capacity, powered by a Universal Prior Model (UPM). We call the new model as \textbf{uPiCA}.
% we propose a Universal Prior Model (UPM) for PiCA.
%Similarly, we employ an identity-conditioned hypernetwork~\cite{ha2016hypernetworks} to generate person-specific avatars. The hypernetwork $\mathcal{E}_\text{id}$ takes identity features as inputs and produces a subset of person-specific model weights.
Similarly, uPiCA adopts a Variational AutoEncoder (VAE)~\cite{kingma2013auto} architecture with an expression encoder, and an avatar decoder. Besides, an identity-conditioned hypernetwork~\cite{ha2016hypernetworks} is added to generate person-specific avatars.
%uPiCA is composed of a expression encoder, an avatar decoder, and an identity-conditioned hypernetwork~\cite{ha2016hypernetworks} to generate person-specific avatars.

\noindent \textbf{Identity-conditioned hypernetwork}
%To enable the extraction of person-specific details, 
$\mathcal{E}_\text{id}$ takes a neutral texture map $\textbf{T}_{\text{neu}}$ and a neural geometry image (mapping vertex position to texture UV space), $\textbf{G}_{\text{neu}}$, and generates bias maps $\Theta_{\text{id}}$ for each level of the avatar decoder $\mathcal{D}$ via a set of skip connections. $\mathcal{E}_\text{id}$ also generates a per-identity positional encoding $f$ for pixel decoder and per-identity geometry displacement $d$ for geometry decoder:
%, similar to a U-Net architecture \cite{ronneberger2015u}:
\begin{equation}
f, d, \Theta_{\text{id}} = \mathcal{E}_\text{id}(\textbf{T}_{\text{neu}}, \textbf{G}_{\text{neu}}; \Phi_{\text{id}}). 
\label{hypernet}
\end{equation}
$\Phi_{\text{id}}$ is the trainable parameters for the identity encoder. 
%
% The decoder is also conditioned on view direction $\omega$, and neck pose $\eta$, used for rendering, to allow explicit control over view- and pose-dependent appearance changes.~\rongyu{Should been moved to decoder.}

\noindent \textbf{Expression encoder.}  
The expression code \( z \) are generated by the expression encoder \( \mathcal{E}_{\text{exp}} \), which takes the differences between the current and neutral geometry and texture maps as input:  
\(
\Delta \textbf{G}_{\text{exp}} = \textbf{G}_{\text{exp}} - \textbf{G}_{\text{neu}},  
\Delta \textbf{T}_{\text{exp}} = \textbf{T}_{\text{exp}} - \textbf{T}_{\text{neu}},
\)  
where \( \textbf{G}_{\text{exp}} \) and \( \textbf{T}_{\text{exp}} \) are the current geometry and texture maps, respectively. \( z \in \mathbb{R}^{16 \times 4 \times 4} \) is defined as:
\begin{equation}
%\begin{split}
    z =  \mathcal{N}(\mu, \sigma); \ \mu, \sigma = \mathcal{E}_{\text{exp}}(\Delta \textbf{T}_{\text{exp}}, \Delta \textbf{G}_{\text{exp}}; \Phi_{\text{exp}}),
%\end{split}
\end{equation}
where 
%\( \mathcal{N}(0, 1) \) is the unit normal distribution, and
\( \Phi_{\text{exp}} \) are the trainable parameters of $\mathcal{E}_{\text{exp}}$. Since the model is trained end-to-end on multi-identity data, the same expression code can be reused across different identities for driving, ensuring consistent expression transfer.

\noindent \textbf{Avatar decoder.} 
We use a set of multiview images $\mathbf{I}_{c,t}$ (\ie, images from camera $c$ at frame $t$) with calibrated intrinsics $\mathbf{K}_c$ and extrinsics $\mathbf{R}_c \mid \mathbf{t}_{c}$.
% \rongyu{Extrinsics should be $\mathbf{R} \mid \mathbf{t}_{c}$, without frame $t$?} ~\di{fixed.}
%
% We compute the camera viewing direction as $\omega = \mathbf{R}_c ^\top \mathbf{t}_{c} $ and transform this vector into an $16 \times 8 \times 8$ grid through a linear layer. 
To condition the decoder on the view direction, we compute $\omega = \mathbf{R}_c^\top \mathbf{t}_c$ (approximating the viewing direction based on a head-centered coordinate system). This vector is transformed into a $16 \times 8 \times 8$ grid via a linear layer.
%
Additionally, we enhance the geometry to encompass the shoulder region and use linear blend skinning to model neck pose $\eta \in \mathbb{R}^{6}$. We use a similar decoder architecture $\mathcal{D}$ as PiCA, which consists of an appearance decoder $\mathcal{D}_e$, a geometry decoder $\mathcal{D}_g$, and a pixel decoder $\mathcal{D}_p$. 
%
Noted that the outputs of geometry and appearance decoder are both expression-dependent.
%
The geometry decoder takes the latent code $z$ and neck pose $\eta$ as input and decodes a head-centered 3D dense position map.
%
The appearance decoder uses the latent code $z$, viewing direction $\omega$, and neck pose $\eta$ to decode a low-resolution, view-dependent map of local appearance codes:
\begin{equation}
\begin{split}
    % \mathbf{g} & = \mathcal{D}_g\left(z, \eta; \Theta_{\text{id}}^g, \Phi_g\right), \\
    % \mathbf{e} & = \mathcal{D}_e\left(z, \omega, \eta; \Theta_{\text{id}}^e, \Phi_e\right),
    \mathbf{g}  = \mathcal{D}_g\left(z, \eta; \Theta_{\text{id}}^g, \Phi_g\right); \quad
    \mathbf{e} = \mathcal{D}_e\left(z, \omega, \eta; \Theta_{\text{id}}^e, \Phi_e\right),
\end{split}
\label{ge}
\end{equation}
where $\mathbf{g} \in \mathbb{R}^{256 \times 256 \times 3}$ is a map of geometry displacement, and $\mathbf{e} \in \mathbb{R}^{256 \times 256 \times 4}$ is a map of appearance codes. $\Theta_{\text{id}}^*$ are identity-specific biases from Eq.~\ref{hypernet} and are related to the corresponding decoders $\mathcal{D}_*$. $\Phi_*$ are their corresponding network training parameters.
We define the final geometry as
$\mathbf{G} = \mathbf{g}_\text{mean} + d + \mathbf{g}$,
% ~\rongyu{Shall we also bold $d$?} ~\di{I think no.}
where we apply a Laplacian preconditioning~\cite{Nicolet2021Large} to the gradients of mean geometry \(\mathbf{g}_\text{mean} \) to bias gradient steps towards smooth solutions. 
%
\( d \) and \( \mathbf{g} \) are the per-identity and expression-dependent geometry displacement, respectively.
The final geometry \( \mathbf{G} \) is sampled at each vertex’s UV coordinates to produce a mesh for rasterization with $\mathbf{e}$. 
%
Rasterization assigns to a pixel at screen position $\textbf{s}$ its corresponding UV coordinates $\textbf{u}$ and head-centered $xyz$ coordinates $\textbf{x}$, and produces the feature image $\hat {\mathbf{I}}_{c,t}^f(\mathbf{s})$. 
%
The pixel decoder further decodes the color at each pixel to produce the rendered image through:
\begin{equation}
        \hat {\mathbf{I}}_{c,t}(\mathbf{s}) = \mathcal{D}_p\left(\hat {\mathbf{I}}_{c,t}^f(\mathbf{s}), f, \textbf{x}, \textbf{u}; \Phi_p\right),   
\label{p}
\end{equation}
where $\Phi_p$ are the training parameters of $\mathcal{D}_p$. $f$ is the positional encoding from Eq~\ref{hypernet}. 
Note that $\mathcal{D}_p$ uses shared weights across subjects, avoiding identity-specific biases from the hypernetwork. Appearance variations are effectively captured by the feature inputs, enhancing network efficiency for runtime deployment and eliminating the need to manage multiple shaders for different users.


\subsection{Universal Layered Prior Model}
\label{sec:upm_layered}

To enable a universal prior model for compositional face and hair avatars across identities, we extend uPiCA to a layered approach, as shown in Fig.~\ref{fig:flowchart}. 
%
In this model, we employ two parallel hypernetworks for face and hair, \ie, $\mathcal{E}_\text{id}^{\text{face}}$ and $\mathcal{E}_\text{id}^{\text{hair}}$.
%to generate untiled bias maps for the face and hair decoders. 
%
This separation allows the model to capture intricate details, such as hair deformation due to head movements or facial expressions.
% , while maintaining the geometric coherence between the face and hair regions. 
%
We employ a unified expression encoder that extracts shared features from the tracked data, enabling synchronized control of both face and hair deformations through a common expression space.
%
The encoded information is then passed in parallel to two independent decoders, $\mathcal{D}^\text{face}$ and $\mathcal{D}^\text{hair}$, allowing each part to adapt to its unique geometry and appearance.


\noindent \textbf{Compositional avatar decoder.}
We use the same decoder architecture as uPiCA from Sec.~\ref{sec:upm_pica} for the face decoders, and denote them as $\mathcal{D}_g^\text{face}$, $\mathcal{D}_e^\text{face}$, and $\mathcal{D}_p^\text{face}$. For the hair geometry decoder $\mathcal{D}_g^\text{hair}$, we use both the head pose \( h \) and neck pose \( \eta \) as inputs since these factors influence hair movement. 
%
Additionally, we include the latent code \( z \) as the input of $\mathcal{D}_g^\text{hair}$, as our experiments reveal that hair deforms with certain facial expressions, such as frowning.
% ~\rongyu{Need to make sure we have ablation study for this claim}.~\di{yes we have.}
This behavior arises because the skin beneath the hair shifts with facial movements, causing the hair to adjust accordingly.
%
For the hair appearance decoder $\mathcal{D}_e^\text{hair}$, we take all inputs of $\mathcal{D}_g^\text{hair}$ along with the view direction \( \omega \) to account for view-dependent appearance variations, ensuring that both the geometry and texture adapt seamlessly across viewing angles. The layered hair decoders are formulated as:
\begin{equation}
\begin{split}
    \mathbf{g}^{\text{hair}} & = \mathcal{D}_g^{\text{hair}}(z, \eta, h; \Theta^{g,\text{hair}}_\text{id}, \Phi_g^{\text{hair}}); \\
    \mathbf{e}^{\text{hair}} & = \mathcal{D}_e^{\text{hair}}(z, \omega, \eta, h; \Theta^{e,\text{hair}}_\text{id}, \Phi_e^{\text{hair}}),
\end{split}
\label{ge_hair}
\end{equation}
where \( \mathbf{g}^{\text{hair}} \in \mathbb{R}^{256 \times 256 \times 3} \) and \( \mathbf{e}^{\text{hair}} \in \mathbb{R}^{256 \times 256 \times 4} \) are the position and texture map of the hair mesh, respectively. 
% The hair geometry decoder extracts a mesh with 20k vertices, which captures detailed hair shapes and ensures smooth integration with the face geometry.

\noindent \textbf{Multi-mesh joint rendering.}
After decoding the face and hair components, we obtain two geometry maps: \( \mathbf{G}^\text{face} \) and \( \mathbf{G}^{\text{hair}} \). These maps are concatenated and jointly processed with the texture maps \( \mathbf{e}^\text{face} \) and \( \mathbf{e}^{\text{hair}} \) using a differentiable renderer~\cite{Pidhorskyi2024RasterizedEG} to produce a unified feature vector for the entire screen image. We further apply the face and hair mask \( m^{\text{face}} \) and \( m^{\text{hair}} \) to the rendered feature map and feed the masked feature images $\hat {\mathbf{I}}_{c,t}^{f,\text{face}}(\mathbf{s})$ and $\hat {\mathbf{I}}_{c,t}^{f,\text{hair}}(\mathbf{s})$, along with their corresponding \( \mathbf{x} \) and \( \mathbf{u} \) into separate pixel decoders $\mathcal{D}_p^\text{face}$ and $\mathcal{D}_p^\text{hair}$. The final rendered image is given by:
\begin{equation}
\begin{aligned}
    \hat{\mathbf{I}}_{c,t}^\text{l}(\mathbf{s})
    &= \mathcal{D}_p^\text{face}(\hat {\mathbf{I}}_{c,t}^{f,\text{face}}(\mathbf{s}), \mathbf{x}, \mathbf{u}; \Phi_p^\text{face}) \odot m^{\text{face}} \\
    &+ \mathcal{D}_p^{\text{hair}}(\hat {\mathbf{I}}_{c,t}^{f,\text{hair}}(\mathbf{s}), \mathbf{x}, \mathbf{u}; \Phi_p^{\text{hair}}) \odot m^{\text{hair}}.
\end{aligned}
\end{equation}
% where \( m^{\text{face}} \) and \( m^{\text{hair}} \) are the masks for the face and hair regions, respectively, obtained from the multi-mesh rasterization. 
% This layered design enables precise alignment between the face and hair components, ensuring high-fidelity reconstruction while supporting complex hairstyles and dynamic movements.
\subsection{Layered Meshes for Gaussian Rendering}
\label{sec:urgca}
We show that our layered mesh design improves the anchor geometry for precise and visually appealing Gaussian renderings. 
Building on prior works~\cite{li2024uravatar,cao2022authentic}, we parameterize and anchor Gaussians on the vertices of our layered PiCA guide mesh. 
We employ parallel face and hair branches for the Gaussian hypernetwork and decoder, which share the same architecture. For simplicity, we denote the hypernetwork and decoder for each branch as $\mathcal{E}_\text{id}^\text{gs}$ and $\mathcal{D}^\text{gs}$. The hypernetwork is formulated as:
\begin{equation}
d^c_\text{mean}, \Theta_\text{id}^{\text{gs}} = \mathcal{E}_\text{id}^{\text{gs}}(\textbf{T}_{\text{neu}}, \textbf{G}_{\text{neu}}; \Phi_{\text{id}}^\text{gs}),
\label{e_id}
\end{equation}
where $d^c_\text{mean}$ represents the mean color attribute from neutral appearance data, and $\Theta_\text{id}^{\text{gs}}$ is the identity-specific bias map.
We denote the vertex positions of the layered PiCA guide mesh as \( \{ \hat{t}_k \}_{k=1}^M \), which serve as anchors for the Gaussians. The Gaussian decoder \( \mathcal{D} ^{\text{gs}} \) takes the expression code \( z \) and neck pose \( \eta \) as input, and is conditioned on the identity untied bias map \( \Theta_{\text{id}}^{\text{gs}} \). It outputs the following attributes:
%
\begin{equation}
\left\{ \delta t_k, q_k, s_k, d_k^c, o_k \right\}_{k=1}^M = 
\mathcal{D}^{\text{gs}}\left(z, \eta; \Theta_{\text{id}}^{\text{gs}}, \Phi^{\text{gs}}\right), \tag{9}
\end{equation}
where \( \delta t_k \) is the position delta, \( q_k \) is the rotation quaternion, \( s_k \) is the scale, \( d_k^c \) is the color attribute, and \( o_k \) is the opacity. 
% Here, \( \Phi^{\text{gs}} \) represents the learnable parameters of the Gaussian decoder. 
The final Gaussian positions are computed as $t_k = \hat{t}_k + \delta t_k$, and colors as $d^c_\text{mean} + d^c_k$ for rendering.


\subsection{Training and Losses}
\label{sec:training_losses}

We jointly optimize all the trainable network parameters $\Phi$ using a total loss \( \mathcal{L}_{\text{total}} \) consisting of:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{\text{pica}} \mathcal{L}_{\text{pica}} + \lambda_{\text{gs}} \mathcal{L}_{\text{gs}} + \lambda_{\text{dehair}} \mathcal{L}_{\text{dehair}},
\end{equation}
where \( \mathcal{L}_{\text{pica}} \) and \( \mathcal{L}_{\text{gs}} \) are the PiCA reconstruction and Gaussian losses, respectively, and \( \mathcal{L}_{\text{dehair}} \) is the dehairing loss. \( \lambda_* \)  are their corresponding loss weights. For the dehairing loss \( \mathcal{L}_{\text{dehair}} \), a large initial weight is applied with a decay during training to accelerate the convergence of bald geometry, ensuring accurate dehaired geometry without interference from the hair mesh. The dehaired avatar serves as the foundation for adding a hair layer, allowing joint optimization of both face and hair with precise alignment.

\noindent \textbf{PiCA reconstruction loss.} We extend the original PiCA losses in \cite{ma2021pixel} to a layered reconstruction loss, defined as:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{pica}} & = \lambda_I \mathcal{L}_I + \lambda_D \mathcal{L}_D + \lambda_N \mathcal{L}_N + \lambda_M \mathcal{L}_M \\
& + \lambda_S \mathcal{L}_S + \lambda_{\text{KL}} \mathcal{L}_{\text{KL}} + \lambda_{\text{seg}} \mathcal{L}_{\text{seg}}.
\end{split}
\end{equation}
Here, \( \mathcal{L}_I \), \( \mathcal{L}_D \), \( \mathcal{L}_N \), and \( \mathcal{L}_{\text{KL}} \) correspond to photometric, depth, normal, and KL divergence losses, respectively, as defined in the original PiCA paper~\cite{ma2021pixel}. The photometric loss \( \mathcal{L}_I \) measures the $L_1$ difference between predicted and ground truth images. 
The mesh tracking loss \( \mathcal{L}_M \) handles hair and face meshes separately by leveraging the tracked hair mesh and the dehaired geometry from avatar dehairing. Smoothness terms \( \mathcal{L}_S \), including Laplacian and general smoothness regularization, are applied independently to both hair and face meshes to prevent artifacts caused by noisy depth inputs, incomplete depth supervision, or stochastic gradient descent noise. The segmentation loss \( \mathcal{L}_{\text{seg}} \) ensures accurate reconstruction of hair regions, particularly thin strands along the sides of the head, preventing them from blending into the face mesh. Notably, we refine the hair segmentation mask through erosion and dilation, creating a mask that extends beyond exact boundaries to account for inaccuracies, and weights \( \mathcal{L}_{\text{seg}} \).

\begin{figure*}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/comparison_mesh.pdf}
\vspace{-20pt}
\caption{
\textbf{Qualitative comparison (mesh).} Our layered representation enables better reconstruction of long hair compared to uPiCA's single-mesh approach. While uPiCA struggles with hair-shoulder intersections and loses hair tail details during head movement, our method maintains clean geometry with accurate hair shape and positioning across different head poses.}
\label{fig:comparison_mesh}
% \vspace{-2mm}
\end{figure*}


\noindent \textbf{Gaussian loss.} The parameters of the Gaussian branch are optimized using the following loss:
\vspace{-5pt}
\begin{equation}
\mathcal{L}_{\text{gs}} = \lambda_\text{render} \mathcal{L}_\text{render} + \lambda_\text{scale} \mathcal{L}_\text{scale} + \lambda_\Delta \mathcal{L}_\Delta. 
\end{equation}
The Gaussian render loss \( \mathcal{L}_{\text{render}} \) applies the $L_1$ loss on the rendered image, following the original 3DGS paper~\cite{kerbl20233d}. To regularize the scale of the Gaussian primitives, we define a pre-clamped scale regularization loss as:
\begin{equation}
\begin{split}
\mathcal{L}_\text{scale} & = \frac{1}{M} \sum_{k=1}^{M} 
\left( 
\frac{1}{\max(r_\text{min}, s_k)} \cdot \mathbb{I}(s_k < r_\text{min}) \right. \\
& \quad + \left. \left( \max(0, s_k - r_\text{max}) \right)^2 
\right),
\end{split}
\end{equation}
% \begin{equation}
% \mathcal{L}_\text{scale} = \frac{1}{N} \sum_{k=1}^{N} 
% \left( 
% \frac{\mathbb{I}(s_k < r_\text{min}) }{\max(r_\text{min}, s_k)} + \left( \max(0, s_k - r_\text{max}) \right)^2
% \right),
% \end{equation}
where \( s_k \) is the scale value of the \( k \)-th Gaussian primitive along any axis, and \( M \) is the total number of primitives. The variables \( r_\text{min} \) and \( r_\text{max} \) represent the lower and upper bounds of the primitive scale, set to 0.1 and 5.0 in our experiments. Note that the regularization loss is computed on the original, unclamped scales \( s_k \) to penalize deviations effectively. We clamp the primitive scale values to the range \([r_\text{min}, r_\text{max}]\) before passing them to the Gaussian renderer, ensuring the rendered Gaussians remain within a controlled range. \( \mathbb{I}(\cdot) \) denotes the indicator function, which equals 1 if the condition is true and 0 otherwise. This formulation ensures stable optimization by keeping the Gaussian scales within appropriate bounds.
Moreover, we apply a delta position loss \( \mathcal{L}_\Delta \) to both the hair and face Gaussians to prevent them from drifting too far from their guide mesh. This loss ensures that hair Gaussians stay within the hair area, and Gaussians on the bald regions of the face mesh do not migrate into the hair region. Specifically, the loss penalizes position deviations as follows:
\begin{equation}
\mathcal{L}_\Delta = \mathbb{E}\left[ \left( \delta t^{\text{hair}} \right)^2 \right] 
+ \mathbb{E}\left[ \left( \delta t^{\text{face}} \odot (1 - m^{\text{face}}) \right)^2 \right],
\end{equation}
where \( \delta t^{\text{hair}} \) and \( \delta t^{\text{face}} \) represent the position deltas of the hair and face Gaussians, respectively, and \( m^{\text{face}} \) is the face mask used to ensure that the delta loss is only applied to the bald head region. More training details are given in \textit{suppl.}

% This formulation ensures that the hair Gaussians remain confined to their designated area, while the Gaussians on the bald regions of the head do not encroach into the hair region, maintaining the separation between face and hair areas throughout training.

% The relative weights are:
% $\lambda_{\text{geo}} = 1, \quad \lambda_{l1} = 10, \quad \lambda_{\text{ssim}} = 0.2, \quad \lambda_s = \lambda_{c-} = \lambda_{\text{es}} = 1.0 \times 10^{-2}, \quad \lambda_{\text{eo}} = \lambda_{\text{ev}} = 1.0 \times 10^{-4}, \quad \lambda_{\text{kl}} = 2.0 \times 10^{-3}.$



