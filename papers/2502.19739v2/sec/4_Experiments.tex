\begin{figure*}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/comparison_gs.pdf}
\vspace{-15pt}
\caption{
\textbf{Qualitative comparison.} Left: Comparison with personalized models shows our method achieves more precise hair reconstruction than PiCA's mesh results. Right: In comparison with universal models, while uPiCA exhibits artifacts such as hair growing from shoulders, our LUCAS (mesh) achieves cohesive reconstruction. When rendered with Gaussian splatting, LUCAS (gs) demonstrates superior detail preservation compared to URAvatar, particularly in complex hairstyles.}
\label{fig:comparison}
\vspace{-5pt}
\end{figure*}


\section{Experiments}

\textbf{Evaluation protocols.} We adopt three widely-used metrics for evaluation: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM)~\cite{wang2004image}, and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}. We restrict the evaluation to the foreground regions, as defined by masks derived from the reconstructed geometry.
% For a fair comparison with prior works, all methods are trained and evaluated under a multi-identity setting, unless explicitly stated otherwise.
% For ablation studies, we train with 76 identities and test on 1) unseen segments from a train subject, 2) unseen segments from an unseen subject, and 3) an unseen illumination from a train subject, which orthogonally eval- uate the generalization of our model to novel poses, identi- ties, and illuminations.

\noindent \textbf{Baselines.} 
For mesh-based methods, we primarily compare with Universal PiCA (uPiCA), which extends Pixel Codec Avatars (PiCA)\cite{ma2021pixel} by incorporating our proposed Universal Prior Model (UPM), as detailed in Sec.~\ref{sec:upm_pica}. Additionally, we perform per-identity comparisons with PiCA to evaluate personalized reconstruction performance.
For Gaussian-based methods, our main comparison is with URAvatar~\cite{li2024uravatar}, benchmarking our modelâ€™s ability to capture fine-grained visual details with Gaussian splatting.


% We also equipped uPiCA with gaussian branch, named UGS-PiCA for comparison.

% \subsection{Qualitative results}
% Fig.~\ref{cover_image} shows that our universal layered avatars generalize to novel identities, views, poses, expressions, and hairstyles. As our avatars share the same global latent space, we illustrate that our universal avatars can be driven consistently across identities under different expressions and poses in real time.



\subsection{Evaluation of the layered representation}
% Comparision with uPiCA


\noindent \textbf{Disentangled representation enhances mesh quality.} A key contribution of our work is the compositional representation of the face and hair as two separate meshes.  This design addresses a fundamental limitation of single-mesh avatars: their constrained UV space allocation, where hair is restricted to a small portion of the UV map while the face dominates.  By allowing separate UV maps for face and hair, our approach enables more accurate representation of complex hairstyles, particularly for long hair. As shown in Fig.~\ref{fig:comparison_mesh}, the comparison across various head poses demonstrates our method's superior capability in reconstructing long hair details. While uPiCA struggles with hair reconstruction, especially during head movement, our approach maintains precise geometry and reduces common artifacts such as hair color bleeding onto shoulders. This improvement becomes particularly evident when the avatar tilts or lowers its head, where our layered representation ensures the hair remains correctly positioned, resulting in visually coherent and realistic renderings.
% A significant reason for this superiority lies in the way UV space is allocated. In single-mesh models, the hair occupies only a small portion of the UV map, with most of the map dedicated to the face. This allocation often results in substantial mesh distortion for the hair, as the limited UV space fails to represent complex hairstyles effectively. In contrast, our layered approach allows each mesh to utilize its own UV map, avoiding distortions that can compromise visual fidelity. 
\begin{figure}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/animation.pdf}
\vspace{-15pt}
\caption{
\textbf{Comparison on dynamic hair animation.} Our LUCAS mesh tracks hair strand deformation and aligns with head and neck movements, outperforming uPiCA in dynamic scenarios.}
\label{fig:animation}
\vspace{-10pt}
\end{figure}

\noindent \textbf{Improved hair deformation during animation.}
In Fig.~\ref{fig:animation}, we demonstrate the advantage of our method in more dynamic scenarios. The examples show how our LUCAS mesh deforms to match hair strand movement in response to head and neck poses, accurately tracking the motion of long hair. In contrast, uPiCA struggles to adapt the hair strands to the changing head positions, resulting in less natural deformations. This comparison highlights the benefit of our layered approach, which provides better control over hair dynamics and improves realism during animation.

\begin{table}[t]
\centering
\caption{\textbf{Quantitative comparisons} on per-subject ($^{\dagger}$) and cross-subject ($^*$) optimization against the state-of-the-art. 
The top three techniques are highlighted in \textcolor{red!50}{red}, \textcolor{orange!50}{orange}, and \textcolor{yellow!100}{yellow}, respectively.
}
\vspace{-5pt}
\renewcommand\tabcolsep{12pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule

$^{\dagger}$PiCA (mesh)~\cite{ma2021pixel} & 32.0512 & 0.8895 & 0.2678   \\

$^{\dagger}$LUCAS (mesh) & \cellcolor{yellow!25}33.5211 & 0.9044 & 0.2479 \\

$^{\dagger}$LUCAS (gs) & \cellcolor{red!25}35.2027 & \cellcolor{red!25}0.9286 & \cellcolor{orange!25}0.2407 \\

\midrule

$^*$uPiCA (mesh) & 32.5623 & 0.8971 & 0.2594 \\

$^*$LUCAS (mesh) & 33.0254 & \cellcolor{yellow!25}0.9073  & 0.2537 \\

$^*$URAvatar (gs)~\cite{li2024uravatar} & 33.1227 & 0.9034 & \cellcolor{yellow!25}0.2464 \\

$^*$LUCAS (gs) & \cellcolor{orange!25}{34.5579} & \cellcolor{orange!25}{0.9201} & \cellcolor{red!25}{0.2394} \\

% $^*$LUCAS (gs) & \textbf{40.0202} & \textbf{0.9711} & \textbf{0.1585} \\

\bottomrule
\end{tabular}}
\vspace{-10pt}
\label{tab:comparison}
\end{table}




\noindent \textbf{Enhanced Gaussian avatars through better meshes.}
The improved mesh structure strengthens the foundation for Gaussian avatars, as the Gaussian splatting process relies heavily on the underlying mesh geometry. While Gaussian splatting can mitigate some errors inherent in single-mesh models, our layered approach further enhances visual fidelity, particularly for intricate hairstyles.
As shown in Fig.~\ref{fig:comparison}, we demonstrate improvements over both personalized and universal models. Compared to PiCA, our approach achieves more detailed hair reconstruction even at the mesh level, with Gaussian splatting further enhancing the visual fidelity. In the universal model comparison, while uPiCA suffers from artifacts like disconnected hair growing from shoulders, LUCAS's mesh representation achieves more cohesive reconstruction. When comparing Gaussian-based methods, LUCAS (gs) demonstrates clear advantages over URAvatar in preserving fine details. These visual improvements are quantitatively validated in Table~\ref{tab:comparison}.
% In cross-subject optimization, LUCAS (gs) achieves the best performance across all metrics, significantly outperforming both mesh- and Gaussian-based methods. Similar improvements are observed in per-subject optimization, demonstrating the consistent advantages of our layered approach.

% \begin{table*}[t]
% \centering
% \caption{Quantitative comparisons.  We evaluate our method for both per-subject ($^{\dagger}$) and cross-subject ($^*$) optimization against the state-of-the-art methods in head avatar reconstruction. The top three techniques are highlighted in \textcolor{red!25}{red}, \textcolor{orange!25}{orange}, and \textcolor{yellow!100}{yellow}, respectively. \color{red}{Placeholder. Numbers to be updated}}
% \vspace{-5pt}
% \renewcommand\tabcolsep{10pt}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{lccccccccc}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Subject 1}} & \multicolumn{3}{c}{\textbf{Subject 2}} & \multicolumn{3}{c}{\textbf{Subject 3}} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ 
%  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ 
%  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
% \midrule

% $^{\dagger}$PiCA (mesh)~\cite{ma2021pixel} &  \\

% $^{\dagger}$LUCAS (mesh) & \\

% % $^{\dagger}$GS-PiCA (gs) & 23.44 & 0.9062 & 0.1708 
% % & 23.25 & 0.9154 & \cellcolor{yellow!100}0.1715 
% % & 24.90 & 0.9216 & 0.1510 \\

% $^{\dagger}$LUCAS (gs) &\\

% \midrule

% $^*$uPiCA (mesh) &  \\

% $^*$LUCAS (mesh) &  \\

% $^*$URAvatar (gs)~\cite{} & 33.7893 & 0.9294 & 0.3031\\

% $^*$LUCAS (gs) & \\
% % \cellcolor{red!25}
% \bottomrule
% \end{tabular}}
% \vspace{-5pt}
% \label{tab:comparison}
% \end{table*}


\begin{figure}[t]
% \vspace{-15pt}
\centering
\includegraphics[width=1\linewidth]{figs/ab_seg_expcode.pdf}
\vspace{-15pt}
\caption{
\textbf{Ablation study.} (a) Expression code improves face-hair synchronization during expressions. (b) Hair segmentation regularization preserves fine hair details.}
\label{fig:ab_seg_expcode}
% \vspace{-2mm}
\end{figure}



% \noindent \textbf{Compositional avatar representation.} layered head/hair meshes vs. single head mesh.

% \noindent \textbf{Impact of training strategy.} 

\subsection{Ablation study}

\noindent \textbf{Impact of expression code.}
In Fig.~\ref{fig:ab_seg_expcode}(a), we compare results with and without the expression code for hair. Without the expression code, the hair mesh fails to move naturally with facial movements, particularly during expressions like frowning. This observation aligns with the findings in Fig.~\ref{fig:vis_frown}, where a subject looks upward and frowns, the hair should lower slightly toward the eyebrows. Our layered representation enables this natural movement by sharing the same expression code $z$ but decoding it separately for face and hair, allowing each component to deform independently and precisely. This advantage is further validated by the quantitative improvements shown in Table~\ref{tab:ab}.

\noindent \textbf{Impact of segmentation regularization.}
In Fig.~\ref{fig:ab_seg_expcode}(b), we assess the effect of hair segmentation regularization. This component is particularly crucial for reconstructing thin hair, as seen in the example, where the hair on both sides is quite fine. Without segmentation regularization, the mesh struggles to capture these thin strands, resulting in blurred renderings. Adding the segmentation term significantly improves the mesh reconstruction, allowing the fine hair to appear correctly in the final render. Quantitative results are also shown in Table~\ref{tab:ab}.

\begin{table}[t]
\centering
\caption{\textbf{Ablation study} on expression code and hair segmentation regularization, evaluated on both training and unseen subjects.
The top two techniques are highlighted in \textcolor{red!50}{red} and \textcolor{yellow!100}{yellow}, respectively.
}
\vspace{-5pt}
\renewcommand\tabcolsep{2pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{3}{c}{Training subjects} & \multicolumn{3}{c}{Unseen subjects} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\multirow{1}{*}{Method}  & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\ 
\midrule
w/o exp. code & \cellcolor{yellow!25}34.1014 & 0.9129 & 0.2498 & \cellcolor{yellow!25}31.9128 & 0.8874 & 0.2601 \\
w/o hair seg & 34.0285 & \cellcolor{yellow!25}0.9140 & \cellcolor{yellow!25}0.2485 & 31.7964 & \cellcolor{red!25}{0.9098} & \cellcolor{yellow!25}0.2554 \\
Full model & \cellcolor{red!25}{34.4981} & \cellcolor{red!25}{0.9189} & \cellcolor{red!25}{0.2402}  & \cellcolor{red!25}{32.5847} & \cellcolor{yellow!25}{0.9087} & \cellcolor{red!25}{0.2496} \\
\bottomrule
\end{tabular}}
\vspace{-5pt}
\label{tab:ab}
\end{table}



\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/zero_shot.pdf}
\vspace{-15pt}
\caption{\textbf{Visualization of avatar driving.} (a) Expression retargeting from a source identity (top left) to multiple avatars demonstrates precise transfer of facial annd hair details. (b) Zero-shot driving on \textit{unseen subjects} shows accurate preservation of fine details around eyes and mouth regions.}
\label{fig:drive}
\end{figure}

\subsection{Evaluation of avatar driving}

% Our LUCAS avatars is drivable by various conditioning data. Expressions, poses, hairstyles.

% Fig. 1 right and Fig. 26 show some retargeting examples. Here, we choose one identity from our dataset (1st column in Fig. 26), pass the tracked mesh and texture into the expression encoder, to obtain the expres- sion code, and feed it into the decoder of each personalized avatar. These results show that the expression of the source identity is transferred to the different avatars, while details such as teeth and wrinkles are preserved.

\noindent \textbf{Driving avatars with diverse inputs.} Our universal model demonstrates versatile driving capabilities across different types of inputs as shown in Fig.\ref{cover_image}. More specifically, in Fig.~\ref{fig:drive}(a), expressions from a source identity (top left) are accurately transferred to multiple personalized avatars, preserving fine details in both wrinkles and hair. This precision stems from our universal layered prior model, where separate decoding of face and hair enables better reconstruction of intricate details.

% Additionally, we demonstrate hairstyle switching by changing the identity conditioning for the hair, showcasing the flexibility of our model to adapt different hairstyles seamlessly. 


\noindent \textbf{Testing on zero-shot driving.} To further evaluate generalization, we test our model on unseen subjects through zero-shot driving. Fig.~\ref{fig:drive}(b) demonstrates that our model successfully transfers novel expressions to untrained identities while maintaining precise facial features, particularly around the eyes and mouth regions.







\section{Conclusion}
We present LUCAS, the first universal compositional representation for 3D head avatars that disentangles face and hair components. This separation allows independent deformation, resolving issues like misplaced hair and misaligned dynamics. It also improves the anchor
geometry for precise and visually appealing Gaussian ren-
derings. Our Universal Layered Prior Model enables effective cross-identity generalization and avatar driving, even for unseen subjects.

\noindent \textbf{Limitation and future work.}
Our layered approach improves face and hair reconstruction but struggles with extreme hair deformations. Unseen poses during driving can degrade long hair deformation, especially in zero-shot scenarios. Future work will focus on relighting, training with a broader range of hairstyles for a more robust universal prior, and fine-tuning on real-world data to enhance applicability.



