\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\Rone}{{\color{purple}{R\_2ms6}}}
\newcommand{\Rtwo}{{\color{blue}{R\_B5w1}}}
\newcommand{\Rthree}{{\color{orange}{R\_6phU}}}
\newcommand{\Rfour}{{\color{red}{R\_GwUX}}}
\newcommand{\todoa}{{\color{brown}{To Do}}}
% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{1549} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{\LaTeX\ Guidelines for Author Response}  % **** Enter the paper title here

% \maketitle
\title{\vspace{-0.3cm} LUCAS: Layered Universal Codec Avatars\vspace{-0.3cm}}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
We thank the reviewers for the valuable comments, noting our work ``novel", ``solid'', ``valuable'', ``well-written'', ``easy to understand'', ``simple but well-evaluated'', and ``address a difficult problem'', with ``high quality'', ``noticeable'', ``sufficient'' and ``strictly better'' results.

%and recognition of our novel problem setup (\Rone, \Rtwo, \Rthree), well paper writing (\Rone, \Rtwo), novel method (\Rone, \Rtwo), comprehensive experiments (\Rone, \Rtwo), and SOTA performance (\Rone, \Rtwo). 



\noindent \textbf{Q1.~[\Rone, \Rtwo, \Rfour] Real-time performance.} 
Our model achieves 45 FPS rendering performance on our mobile device after quantization-aware-training. QAT is not included in the paper as our focus is the layered representation. Hardware configuration will be provided. % We will include it in the revised version.

% Our universal mesh avatars achieve 4 ms rendering time (25 FPS) on our mobile device, comparable to PiCA. We will elaborate with detailed hardware configuration.


% \noindent \textbf{Q10.~[\Rtwo] Modeling as mesh not strand.} We opted for this approach to ensure real-time rendering capabilities essential for practical applications. While strand-based models offer more detail, they require higher computational power, limiting their accessibility.

\noindent \textbf{Q2.~[\Rtwo, \Rthree] Strand-based methods} cannot be easily generated by hypernetworks, making it computationally demanding and not meet the compactness requirements for mobile VR applications. In contrast, mesh representation used in our approach are better supported by modern devices, thus providing a more efficient solution for dynamic avatar modeling.

% face limitations in producing strands efficiently using hypernetworks, are computationally expensive, and lack the compactness required for real-world VR applications. By using a disentangled mesh representation, our method achieves better long-term temporal tracking and offers a more efficient solution for dynamic avatar modeling.



\noindent \textbf{Q3.~[\Rone] Inconsistent hair segments.} 
% We refine the hair segmentation mask through erosion and dilation, creating a mask that extends beyond exact boundaries to account for inaccuracies. This enhanced mask weights the hair segmentation loss, ensuring the model prioritizes identified hair areas and addresses common issues like stray hairs. We also implement a dynamic weight decay, starting with a lower weight and gradually increasing to 1.
% We refine the hair segmentation mask with erosion and dilation to address inaccuracies and weight the segmentation loss. A dynamic weight decay is applied, starting low and gradually increasing to 1, to prioritize hair areas effectively. 
% To address the inaccuracy, hair mask is refined with erosion and dilation. 
% Besides, the segmentation loss is weighted dynamically with weights gradually increased from 0.01 to 1.
We refine the hair segmentation mask through erosion and dilation, creating a mask that extends beyond exact boundaries to account for inaccuracies, and weights the hair segmentation loss. 

% This enhanced mask weights the hair segmentation loss, ensuring the model prioritizes identified hair areas and addresses common issues like stray hairs.

\noindent \textbf{Q4.~[\Rone] Dehairing loss} $\mathcal{L}_{\text{dehair}}$ 
%is strategically utilized during the early training iterations to ensure geometric consistency between the predicted bald head geometry $\mathbf{G}^\text{face}$, and a reference geometry $\mathbf{G}^\text{ref}$ derived from the dehairing process described in Sec.~3.1. This is expressed as $\mathcal{L}_{\text{dehair}} = \text{MSE}(\mathbf{G}^\text{ref} - \mathbf{G}^\text{face})$ and will be elaborated.
is used in the early training iterations to ensure geometric consistency between the predicted bald head geometry $\mathbf{G}^\text{face}$ and the reference geometry $\mathbf{G}^\text{ref}$ derived from the dehairing process described in Sec.~3.1. This is expressed as $\mathcal{L}_{\text{dehair}} = \text{MSE}(\mathbf{G}^\text{ref} - \mathbf{G}^\text{face})$ and will be elaborated.

\noindent \textbf{Q5.~[\Rone] The absence of identity biases in $\mathcal D _\text{p}$} aims to enhance efficiency during runtime deployment, not necessarily for ``real-time'' purposes. Our model uses a single shader with the learned MLP weights in $\mathcal D _\text{p}$ for runtime, eliminating the need to manage multiple shaders for multiple users. We agree that the geometry decoder already takes the per-identity bias into account, and is indeed a reason for the absence of identity bias here. We will make it clear.

% The absence of identity biases in $\mathcal D _\text{p}$} aims to enhance efficiency during runtime deployment, not necessarily for ``real-time'' purposes. This means we do not need to customize MLP weights in $\mathcal D _\text{p}$ for each user during testing. We agree that the geometry decoder already takes the per-identity bias into account, and is indeed a reason for the absence of identity bias here. We will make it clear.

\noindent \textbf{Q6.~[\Rone] Geometric coherence.} While this separation provides greater detail and flexibility, we do not claim it ensures perfect coherence in extreme deformations or at skin-hair intersections without additional measures. We rely on the described losses with image loss being the most reliable and accurate regularizer. The intended meaning of geometric coherence refers to synchronizing deformations through a unified latent encoder. We will clarify this.

\noindent \textbf{Q7.~[\Rone] Notation.} $\Theta^{g,\text{hair}}_\text{id}$/$\Theta^{e,\text{hair}}_\text{id}$ are per-id bias for hair geometry/appearance decoders, with $\Phi_g^{\text{hair}}$/$\Phi_e^{\text{hair}}$ being their network parameters.

\noindent \textbf{Q8.~[\Rone] Relighting.} Adopting the URavatar formulation to enable relighting is feasible and is in our plan.

% \noindent \textbf{Q11.~[\Rone] Typo.} Corrected. Thank you.



\noindent \textbf{Q9.~[\Rtwo] Dataset.} Our model demonstrates notable cross-identity generalization with 76 identities. Scaling the dataset with greater diversity is on our agenda and will further enhance the model's generalizability.

\noindent \textbf{Q10.~[\Rtwo] Contribution 3} underscores our modelâ€™s advancements in dynamic hair-face interactions during head pose changes, expression transfers, and hairstyle variations. Comprehensive experiments on unseen subjects in zero-shot driving scenarios demonstrate LUCAS's adaptability and practicality, setting it apart from previous methods.

\noindent \textbf{Q11.~[\Rtwo, \Rthree]} \textbf{Sec.~3.1} will be elaborated.

\noindent \textbf{Q12.~[\Rthree]} Our \textbf{motivation for using mesh-based representations} is rooted in their proven efficiency and practicality, as demonstrated by models like PiCA. Meshes are inherently compact, computational memory efficient, and well-suited for DSP/GPU hardware, making them widely adopted in the real-world VR applications. Building upon PiCA, our method enhances geometry, dynamic hair-face interactions, and cross-id generalization ability without compromising efficiency, unlike computationally intensive Gaussian and strand-based methods.

%\textbf{motivation} for using mesh-based representations is rooted in their proven efficiency and practicality, as demonstrated by models like PiCA, which are widely adopted in real-world VR applications. Meshes are inherently compact, computational and memory efficient, and well-suited for DSP/GPU hardware. They enable disentangling face and hair geometry for coherent layered rendering. Building upon PiCA, our method enhances geometry, dynamic hair-face interactions, and cross-id generalization ability without compromising efficiency, unlike computationally intensive Gaussian and strand-based methods.

% Mesh is more compact to disentangle face and hair, and memory, computation, efficient representation on both DSP and GPU setups. as illustrated in PiCA and URAvatar. Gaussian and strand based methods s often demand significant computational resources
% training on a100s,
% testing performance is comparable to pica

% 2. run on device requires model quantization which is not included in this paper.
% 3. mesh is more compact to disentangle face and hair
% 4.pica uravatar 
% we keep design for pica and comparable

\noindent \textbf{Q13.~[\Rthree] Narrative of Sec.3.4.} This section highlights how our layered mesh design improves anchor geometry for precise and visually appealing Gaussian renderings. We will revise to better articulate its role and benefits.

\noindent \textbf{Q14.~[\Rthree] Zero-shot driving} refers to transferring expressions $z$ from a trained identity to unseen identities without requiring retraining.
% Specifically, for trained identities ABC, we might use expression code $z$ from A to animate B and C. For an unseen identity D, $z$ from A would similarly drive D without D's data being included in the training set. 
% for trained identities ABC and unseen identity D, we use expression code $z$ from A to drive D

% This demonstrates our modelâ€™s capability to generalize to new subjects without requiring retraining.

\noindent \textbf{Q15.~[\Rthree] Hair reconstruction quality} is benchmarked below by computing metrics on the hair regions.

\begin{table}[!htbp]
\vspace{-10pt}
\centering
% \caption{\textbf{Quantitative Comparisons}.  
% %\text{PCA}^{\dagger}$ indicates we use GT ego-motion in PCA training. All results are trained and evaluated under flow threshold 0.05. }
% }
% \renewcommand{\arraystretch}{1.15}
\renewcommand\tabcolsep{25pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
% \multirow{2}{*}{Method}
\multirow{1}{*}{}
& PSNR ($\uparrow$) & SSIM ($\uparrow$) & LPIPS ($\downarrow$) 
% & Infer. Time (fps) ($\downarrow$)  
\\ 
% \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
\midrule
% FastNSF & 0.2045 & 1.5176 & 0.6180 \\
% ICP
% & 0.0554 &  0.4499 & 0.7456\\
% uPiCA (mesh) \\
% LUCAS (mesh)  \\
% URAvatar (gs) \\
% LUCAS (gs) \\

uPiCA (mesh) & 31.5626 & 0.8877 & 0.2693 \\

LUCAS (mesh) & 32.0252 & 0.8970  & 0.2631 \\

URAvatar (gs)& 32.1227 & 0.8932 & 0.2569 \\

LUCAS (gs) & \textbf{{33.5578}} & \textbf{{0.9109}} & \textbf{{0.2490}} \\
\bottomrule
\end{tabular}} %\vspace{-1em}
\label{tab_comp}
\vspace{-10pt}
\end{table}


\noindent \textbf{Q16.~[\Rthree] Visualization of dynamic interaction.} Please refer to the \textit{suppl.} video for the animation results.

\noindent \textbf{Q17.~[\Rthree] Gaussian Head Avatar} is a personalized model requiring 3DMM coefficient transfer for expression transfer. In contrast, LUCAS is a universal model capable of generalizing to unseen identities, making it more versatile for dynamic, real-world applications. 

\noindent \textbf{Q18.~[\Rthree] Mesh resolution.} 
% Unlike the original PiCA, our approach uses the same resolution mesh vertices (65K) for both input and output. We will clarify this.
We use the same 65K mesh vertices for input and output, unlike the original PiCA.
% We will clarify this.

\noindent \textbf{Q19.~[\Rthree] Hair UV} is a 256$^2$ map defined as
$\mathbf{G^\text{hair}} = \mathbf{g}_\text{mean}^\text{hair} + d^\text{hair} + \mathbf{g}^\text{hair}$, where $\mathbf{g}^\text{hair}_\text{mean}$ is a mean hair geometry, $f^\text{hair}$ and $d^\text{hair}$ are a per-identity positional encoding and geometry displacement, respectively. 


\noindent \textbf{Q20.~[\Rfour] Limitation.} Our method represents a remarkable improvement over current techniques, particularly in capturing finer details, dynamic interactions, and cross-id generalization ability. However, we acknowledge its limitations, including dependency on training data, challenges with extremely complex hairstyles, lack of physics-based priors and dynamic relighting capabilities. We are committed to addressing these issues in future work.

\noindent \textbf{Q21. Related works} on approaches using hair segmentation~[\textbf{\Rone}], point-based and one-shot models~[\textbf{\Rthree}] and MeGA~[\textbf{\Rfour}] will be discussed.





%%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\end{document}
