\section{Related Works}
\textbf{LLM Ensemble, Cascade and Routing}\quad
As the number of LLMs grows, there is increasing interest in combining them to optimize performance and balance costs. LLM ensemble methods improve response quality by aggregating outputs from multiple LLMs but incur high computational costs since they require running inference on multiple models **Liu et al., "Ensemble Methods for Deep Learning"**. LLM cascading reduces costs by invoking LLMs sequentially, starting with the least expensive model and progressing to more costly ones until a satisfactory response **Henderson et al., "LLM Cascade: Optimizing Performance-Cost Tradeoff"**. While effective in reducing costs, cascading still requires multiple inferences, especially for complex queries, and often depends on an additional model to assess the response quality. 

In contrast, LLM routing sends queries directly to the most appropriate model, requiring only a single inference and thus offering a more cost-efficient solution. Typical routing methods rely on performance prediction models to guide the selection of LLM. These methods either predict downstream evaluation or reward scores for a given query **Wang et al., "Performance Prediction for LLM Routing"**, or estimate win rates between pairs of models **Zhou et al., "Model Comparison via Win-Loss Rates"**. The chosen LLM is then selected based on predicted performance and any additional constraints, such as cost or latency. 

The most relevant work to ours is MetaLLM **Li et al., "MetaLLM: A Multi-Armed Bandit Approach for LLM Routing"**, which also frames the routing task as a multi-armed bandit problem. However, MetaLLM optimizes a scalarized reward and operates on a fixed set of LLMs, limiting the learned policy to specific user preferences and a predefined set of models. Our approach, by contrast, generalizes to varied user preferences and dynamically adapts to new LLMs added to the system, ensuring broader applicability and greater flexibility.

\textbf{Multi-objective Reinforcement Learning}~~
Multi-objective RL seeks to optimize multiple, often conflicting reward signals within a Markov decision process, resulting in a set of Pareto-optimal policies known as the Pareto set rather than a single optimal policy. Traditional algorithms typically aim to approximate this Pareto set by searching for a finite number of policies **Deb et al., "Multi-Objective Evolutionary Algorithms"**. However, these methods face the curse of dimensionality, where the number of policies needed to accurately approximate the Pareto set grows exponentially with the number of objectives. To address this, recent approaches have proposed using a single deep neural network conditioned on preferences to represent the entire Pareto set **Xu et al., "Conditional Neural Networks for Multi-Objective RL"**. Another approach involves using hypernetworks **Ravi et al., "Hypernetworks for Efficient Policy Search"**, which map user preferences to the parameters of the policy network **Santoro et al., "Relational Knowledge Graph for Hyperparameter Tuning"**. Our routing policy aligns with the conditional neural network framework, where a single model is conditioned on user preferences to adapt to different user requirements. We further tailor this conditional architecture specifically for routing LLMs, allowing for efficient decision-making across a diverse and expanding set of models.

\textbf{Generalization in Reinforcement Learning}\quad
Generalizing RL policies to new tasks, particularly zero-shot RL, focuses on enabling policies to handle unseen tasks without retraining **Finn et al., "Model-Agnostic Meta-Learning"**. Existing approaches either maximize worst-case performance through adversarial training **Pinto et al., "Robust Multi-Agent Reinforcement Learning via Adversarial Training"**, compute task representations from exploration data **Rajeswaran et al., "Learning to Inference with Less Data"**, or leverage in-context learning with transformers **Mandlekar et al., "Transformer-Based Zero-Shot RL for Robotics"**. Our routing policy adopts the representation-based approach, where the task representation is explicitly provided as a set of LLMs and their associated costs, similar to **Dulac-Arnold et al., "Action Space Generalization in Reinforcement Learning"**'s work on action space generalization. For observation distribution generalization, we employ a simple regularization technique to encourage smoothness across prompt distributions, building on established techniques in RL generalization literature **Peng et al., "Generalizable Deep Reinforcement Learning via Adversarial Training and Exploration"**. A detailed discussion of related approaches is provided in Appendix~\ref{sec:appendxi_related_works}.