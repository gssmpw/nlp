\section{Related Works}
\textbf{LLM Ensemble, Cascade and Routing}\quad
As the number of LLMs grows, there is increasing interest in combining them to optimize performance and balance costs. LLM ensemble methods improve response quality by aggregating outputs from multiple LLMs but incur high computational costs since they require running inference on multiple models \citep{jiang2023llm,wang2023fusing,lu2024blending}. LLM cascading reduces costs by invoking LLMs sequentially, starting with the least expensive model and progressing to more costly ones until a satisfactory response \citep{chen2023frugalgpt,madaan2023automix,ramirez2024optimising}. While effective in reducing costs, cascading still requires multiple inferences, especially for complex queries, and often depends on an additional model to assess the response quality. 

In contrast, LLM routing sends queries directly to the most appropriate model, requiring only a single inference and thus offering a more cost-efficient solution. Typical routing methods rely on performance prediction models to guide the selection of LLM. These methods either predict downstream evaluation or reward scores for a given query \citep{shnitzer2023large,lu2023routing,hari2023tryage,vsakota2024fly}, or estimate win rates between pairs of models \citep{ding2024hybrid,ong2024routellm}. The chosen LLM is then selected based on predicted performance and any additional constraints, such as cost or latency. 

The most relevant work to ours is MetaLLM \citep{nguyen2024metallm}, which also frames the routing task as a multi-armed bandit problem. However, MetaLLM optimizes a scalarized reward and operates on a fixed set of LLMs, limiting the learned policy to specific user preferences and a predefined set of models. Our approach, by contrast, generalizes to varied user preferences and dynamically adapts to new LLMs added to the system, ensuring broader applicability and greater flexibility.

\textbf{Multi-objective Reinforcement Learning}~~
Multi-objective RL seeks to optimize multiple, often conflicting reward signals within a Markov decision process, resulting in a set of Pareto-optimal policies known as the Pareto set rather than a single optimal policy. Traditional algorithms typically aim to approximate this Pareto set by searching for a finite number of policies \citep{van2014multi,parisi2014policy,xu2020prediction}. However, these methods face the curse of dimensionality, where the number of policies needed to accurately approximate the Pareto set grows exponentially with the number of objectives. To address this, recent approaches have proposed using a single deep neural network conditioned on preferences to represent the entire Pareto set \citep{yang2019generalized,abels2019dynamic,basaklar2022pd}. Another approach involves using hypernetworks \citep{chauhan2023brief}, which map user preferences to the parameters of the policy network \citep{shu2024learning}. Our routing policy aligns with the conditional neural network framework, where a single model is conditioned on user preferences to adapt to different user requirements. We further tailor this conditional architecture specifically for routing LLMs, allowing for efficient decision-making across a diverse and expanding set of models.

\textbf{Generalization in Reinforcement Learning}\quad
Generalizing RL policies to new tasks, particularly zero-shot RL, focuses on enabling policies to handle unseen tasks without retraining \citep{korkmaz2024survey}. Existing approaches either maximize worst-case performance through adversarial training \citep{moos2022robust,dong2023robust}, compute task representations from exploration data \citep{touati2021learning,agarwal2021contrastive,benjamins2022contextualize,ingebrand2024zero}, or leverage in-context learning with transformers \citep{melo2022transformers,brohan2022rt}. Our routing policy adopts the representation-based approach, where the task representation is explicitly provided as a set of LLMs and their associated costs, similar to \citet{jain2020generalization}'s work on action space generalization. For observation distribution generalization, we employ a simple regularization technique to encourage smoothness across prompt distributions, building on established techniques in RL generalization literature \citep{cobbe2019quantifying,zhang2021generalization}. A detailed discussion of related approaches is provided in Appendix~\ref{sec:appendxi_related_works}.

\vspace{-4pt}