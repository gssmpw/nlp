%Version 3 December 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
% \documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
% \documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
% \documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style 
% \documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
% \usepackage{algorithm}%
% \usepackage{algorithmicx}%
\usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algpseudocode}%
\usepackage{listings}%
% \usepackage{lmodern}
\usepackage{anyfontsize}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
%% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
% \newtheorem{theorem}{Theorem}%  meant for continuous numbers
% %%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
% %% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
% \newtheorem{proposition}[theorem]{Proposition}% 
% %%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
% \newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Enhancing Collaborative Filtering-Based Course Recommendations by
Exploiting Time-to-Event Information with Survival Analysis]{Enhancing Collaborative Filtering-Based Course Recommendations by
Exploiting Time-to-Event Information with Survival Analysis}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1,2]{Alireza Gharahighehi}\email{alireza.gharahighehi@kuleuven.be}
\equalcont{These authors contributed equally to this work.}
\author*[1,2]{Achilleas Ghinis}\email{achilleas.ghinis@.kuleuven.be}
\equalcont{These authors contributed equally to this work.}

\author[1,2]{Michela Venturini}\email{michela.venturini@kuleuven.be}

\author[2,3]{Frederik Cornillie}\email{frederik.cornillie@kuleuven.be}

\author[1,2]{Celine Vens}\email{celine.vens@kuleuven.be}

\affil*[1]{\orgdiv{Department of Public Health and Primary Care}, \orgname{KU Leuven, Campus Kulak}, \orgaddress{\street{Etienne Sabbelaan 53}, \city{Kortrijk}, \postcode{8500}, \country{Belgium}}}

\affil[2]{\orgdiv{Itec, imec research group at KU Leuven}, \orgaddress{\street{Etienne Sabbelaan 51}, \city{Kortrijk}, \postcode{8500}, \country{Belgium}}}

\affil[3]{\orgdiv{Department of Linguistics}, \orgname{KU Leuven}, \orgaddress{\street{Etienne Sabbelaan, 53}, \city{Kortrijk}, \postcode{8500}, \country{Belgium}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Massive Open Online Courses (MOOCs) are emerging as a popular alternative to traditional education, offering learners the flexibility to access a wide range of courses from various disciplines, anytime and anywhere. Despite this accessibility, a significant number of enrollments in MOOCs result in dropouts. To enhance learner engagement, it is crucial to recommend courses that align with their preferences and needs. Course Recommender Systems (RSs) can play an important role in this by modeling learners' preferences based on their previous interactions within the MOOC platform. Time-to-dropout and time-to-completion in MOOCs, like other time-to-event prediction tasks, can be effectively modeled using survival analysis (SA) methods. In this study, we apply SA methods to improve collaborative filtering recommendation performance by considering time-to-event in the context of MOOCs. Our proposed approach demonstrates superior performance compared to collaborative filtering methods trained based on learners' interactions with MOOCs, as evidenced by two performance measures on three publicly available datasets. The findings underscore the potential of integrating SA methods with RSs to enhance personalization in MOOCs.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{recommendation systems, survival analysis, massive open online course, personalized learning, dropout}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Massive Open Online Courses (MOOCs) platforms offer a diverse range of online courses to learners around the globe, promoting equitable education by breaking down barriers related to geography and time. However, despite their significant advantages, many MOOC enrollments end in dropouts. Reports indicate that dropout rates for courses from renowned institutions such as MIT and Harvard can reach up to 90\%~\cite{chen2022systematic}. Dropouts may occur for various reasons, including accessing only the free parts of the courses, perceiving the course or topic as irrelevant, or lacking necessary competencies. This dropout information is crucial for modeling users' preferences and needs on MOOC platforms. Recommender Systems (RSs) are machine learning models that leverage users' past interactions to suggest the most suitable items to be recommended to the target user. Typically, RSs are divided into two main categories: Content-based filtering (CB) and collaborative filtering (CF). CB filtering RSs suggest items with features similar to those that the user has previously expressed interest in, while CF RSs predict users' preferences based on the similarities between users' and items' past interactions.

In a MOOC platform, a CF-based RS can be used to recommend courses based on users' previous enrollments. Although prior enrollments provide valuable data for modeling user preferences, they do not incorporate time-to-event information such as time-to-dropout or time-to-completion. Given the high dropout rates in MOOCs, incorporating time-to-event information can enhance the understanding of users' needs and preferences regarding courses. 

Survival analysis (SA) is a branch of statistics concerned with modeling the time until a particular event, such as death or machinery failure, occurs~\cite{clark2003survival}. A key aspect of survival data is that some events remain unobserved, known as censored data. Right-censoring, the most frequent type of censoring in SA, occurs when the target event is not witnessed during the follow-up period or if the instance is lost before the follow-up ends. The primary advantage of SA lies in its ability to use such partial data during the learning process by including instances with censored events which are often disregarded in classification and regression tasks. Our hypothesis is that incorporating the time-to-event (dropout or completion) is highly informative for modeling user preferences in MOOC recommendations, as it offers critical insights into students' engagement in MOOCs~\cite{room2021dropout}.

% In this paper we propose a post-processing approach based on survival analysis to enhance the performance of collaborative filtering methods in the context of MOOCs. Specifically, we would like to recommend courses that user will enroll with high probability and will complete in short time or will have very long predicted time for dropout. This is illustrated in Figure~\ref{fig:mexample}. Assume that the target user in the figure has enrolled in course 1 to 5, has completed course 1 and 3 and dropout from course 2, 4, and 5. If we use their interactions with course 1 and 2 to train typical CF methods, with high probability course 3,4 and 5 would be recommended, i.e., all of them will have high probabilities to be enrolled compared to course 6. But these methods are unable to provide reasonable ranking among these three courses. Ideally courses that are likely to be completed in short time or courses with long predicted dropout time should be ranked earlier in the recommendation list. The idea of this paper is to use the time-to-event information (time-to-dropout or time-to-completion) to train a SA method and to re-rank the courses with the high probability of enrolment based on their predicted dropout risk. 

In this paper\footnote{The source code is available at \url{https://anonymous.4open.science/r/mocc_cf_sa-85C9}}, we introduce a post-processing strategy utilizing SA to improve the effectiveness of CF techniques in the context of MOOCs. Our goal is to recommend courses that users are likely to enroll in with a high probability, and either complete swiftly or have a long time before dropout. This concept is demonstrated in Figure~\ref{fig:mexample}. Suppose a user, as shown in the figure, has enrolled in courses 1 to 5, completed courses 1 and 3, and dropped out of courses 2, 4, and 5. The target user, for whom we want to provide recommendations, has also enrolled in courses 1 and 2. Given the similar enrollments between the training and target users, standard CF methods would likely recommend courses 3, 4, and 5, suggesting they have higher predicted enrollment probabilities compared to course 6. However, these methods cannot effectively rank these courses among themselves. Ideally, courses expected to be completed fast or those with longer predicted dropout time should be ranked higher in the recommendation list. An SA model, trained on time-to-event information, will better quantify the ordering between the courses most likely to be enrolled in by the target user. The approach presented in this paper exploits time-to-event information (time-to-dropout or time-to-completion) to train an SA method and re-rank highly probable courses for enrollment based on their predicted time to dropout or completion.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{MOOCRS_m.jpg}
    \caption{Illustration of time-to-event data in the context of MOOCs}
    \label{fig:mexample}
\end{figure}

The paper is organized as follows: relevant studies about MOOC recommendations and dropout prediction are described in Section~\ref{sec:LR}. Next, in Section~\ref{sec:method}, we illustrate our proposed approach, explaining the collaborative filtering task, the time-to-event prediction task and the final post-processing step to generate the final recommendation lists. In Section~\ref{sec:ES}, we describe three publicly available datasets and the experimental setup in designing and testing the proposed approach. Next, in Section~\ref{sec:result} we present and discuss the results of comparing our proposed approach against some CF methods on these three datasets. Finally, we conclude and outline some directions for future research in Section~\ref{sec:con}.

\section{Related work}
\label{sec:LR}
\subsection{MOOC recommendation}
% Various types of RSs have been used in the context of MOOCs such as collaborative, knowledge- and content-based filtering. Among these different types, CF RSs have been extensively applied, either individually or in combination with other RSs, since these types of RSs do not require item or user meta-data to come-up with recommendations and therefore can rely solely on the logs of learners~\cite{uddin2021systematic}. Among numerous studies which applied CF for MOOC recommendations, nearest neighbors-~\cite{fu2015undergraduate,he2017design,lu2019research,yang2014peer,song2016research,pang2018adaptive,yin2020mooc} and matrix factorization~\cite{wu2021collaborative,gharahighehi2023extending,chao2019collaborative}-based approaches are the most popular ones. Few studies considered time-information in forming their MOOC RS. In~\cite{pardos2017enabling}, learners' dwelling time in the MOOC page in edX is considered to provide personalized recommendations. In another study on the edX MOOCs, time-augmented Recurrent Neural Network (RNN) has been applied to consider the amount of time that a learner spent on each course pages in order to provide personalized recommendations. While this simple way of applying time-information enhance RSs performance. to our knowledge, time-to-event data, i.e., time to completion and time-to-dropout, have not been used to generate more informed recommendations.

Various types of RSs have been utilized in the context of MOOCs, including collaborative, knowledge-based, and content-based filtering. Among these, CF RSs have been extensively applied, either individually or in combination with other types, since they do not require item or user metadata to generate recommendations and can rely solely on learners' logs~\cite{uddin2021systematic}. Numerous studies have applied CF for MOOC recommendations, with nearest neighbors~\cite{fu2015undergraduate,he2017design,lu2019research,yang2014peer,song2016research,pang2018adaptive,yin2020mooc} and matrix factorization~\cite{wu2021collaborative,gharahighehi2023extending,chao2019collaborative} approaches being the most popular. Although time-related information provides relevant insights into learners' preferences and needs in MOOCs, few studies have incorporated this data into their MOOC recommendations. For instance, one study used learners' dwell time on the MOOC page in edX\footnote{\url{https://www.edx.org/}} to provide personalized recommendations~\cite{pardos2017enabling}. Another similar study~\cite{tang2017personalized} applied a time-augmented Recurrent Neural Network (RNN) to consider the amount of time learners spent on each course page for making personalized recommendations in edX. In our previous study~\cite{gharahighehi2023extending}, we demonstrated that SA can improve the performance of a specific RS, namely Bayesian Personalized Ranking (BPR), when the predictions of a SA method, trained based on time to dropouts, are embedded in the BPR algorithm. While SA based on time-to-dropout improved the quality of recommendations, it has only used in a specific algorithm, namely BPR.

While using time information has proven to have a positive effect on RS performance, to our knowledge, time-to-event data, such as time-to-completion and time-to-dropout, have not been utilized to provide more informed recommendations, specifically in CF RSs.

\subsection{Time-to-event prediction in MOOCs}
The task of dropout prediction in the context of MOOCs has been mainly modeled as a classification task~\cite{dalipi2018mooc, chen2022systematic}. While in these studies the task was predicting the event of dropout, the authors ignored the time information in their predictions. SA can be used to incorporate the time information in modeling dropout in MOOCs and there are some promising examples in the literature. The authors in~\cite{gitinabard2018your} used SA, specifically Cox proportional hazards method, to model dropout risk in the context of MOOCs and unveil social and behavioral features impacting the outcome. Xie~\cite{xie2019modelling} utilized survival analysis to examine the hazard function of dropout, employing the learner's course viewing duration on a course in MOOCs. Labrador et al.~\cite{labrador2019survival} specified the fundamental factors attached to learners' dropout in an online MOOC platform using Cox proportional hazard regression. Wintermute et al.~\cite{wintermute2021survival} applied a Weibull survival function to model the certificate rates of learners in a MOOCs platform, assuming that learners “survive” in a course for a particular time before stochastically dropping out. In~\cite{pan2022survival} a more sophisticated SA deep learning approach was proposed to tackle volatility and sparsity of the data, that moderately outperformed the Cox model. Masci et al.~\cite{masci2024modelling} applied shared frailty Cox models to model dropout of students who enrolled in engineering programs. 

% While SA has been applied to model dropout in MOOCs, to the best of our knowledge, it hasn't been used to model user preferences and needs in MOOC recommendations. The research gap that we aim to fill is to investigate the merits of SA to model time-to-dropout and time-to-completion and use it to enhance performance of typical CF RSs, which are the most common type of RSs, in the context of MOOCs. Although SA has been used to model dropout in MOOCs, it has not been employed to understand user preferences and needs within MOOC recommendations. Our research aims to bridge this gap by exploring the benefits of using SA to model time-to-dropout and time-to-completion in MOOCs. We will leverage this information to enhance the performance of CF RSs, which are the most prevalent type of RSs, in the context of MOOCs.

Although SA has been applied to model dropout in MOOCs, to the best of our knowledge, it hasn't been used to model user preferences and needs in MOOC recommendations. The research gap that we aim to fill is to investigate the merits of SA to model time-to-events in the context of MOOCs, specifically time-to-dropout and time-to-completion, and use it to enhance the performance of typical CF RSs. 

\section{Methodology}
\label{sec:method}

\subsection{Problem formulation}

In recommendation tasks there are two main sets of entities, the users, who receive the recommendations, and the items, which can be recommended to the users. Let $U = \{u_{1}, u_{2}, ..., u_{m}\} $ and $I = \{i_{1}, i_{2}, ..., i_{n}\}$ be two finite sets, representing users and items, respectively. The already known interactions between such items and users are stored in an interaction matrix $\mathbf{M}$, which in the context of our study on MOOCS can contain tuples where the first element in the tuple contains the time to the event, and the second element of the tuple is the event between the user and course (``c" completed or ``d" dropout):
% (e.g. $Y_{1,1}=(10, c)$ means that user "1" completed course "1" in 10 days):

\begin{equation}
\label{eq:hyperdef}
 M_{ui} = \left\{ 
  \begin{array}{l l}
    (t_{ui},c), & \quad \mbox{if user $u$ completed course $i$}\\
    (t_{ui},d), & \quad \mbox{if user $u$ dropout from course $i$}\\
    0, & \quad \mbox{if user $u$ hasn't enrolled in course $i$}.
  \end{array} \right.
\end{equation}

\noindent where $t_{ui}$ is time-to-completion or time-to-dropout for user ``u" and item ``i". To represent learners enrolments in MOOCs, we consider enrolment matrix $E$ by binarizing the interaction matrix $M$. The task of MOOC recommendation is to provide a ranked $top@k$ recommendation list, i.e., the first $k$ items in the ordered list, to each user.

\subsection{Collaborative filtering}
\label{method:cf}
The task of a CF-based RS is to model user preferences over unseen items and generate ranked lists of recommendations using a sparse interaction matrix between users and items. CF RSs either form neighborhoods around users or items (UKNN or IKNN\footnote{User- or Item-based K Nearest Neighbors}) or learn latent features (e.g. SVD\footnote{Singular Value Decomposition} and NMF\footnote{Non-negative Matrix Factorization}) to infer preferences. In the context of MOOCs, the RS is trained on the enrollment matrix, which contains user enrollments. Once trained, the CF-based recommendation system can predict the missing values in the matrix, i.e., the courses that learners haven't yet enrolled in, thereby reconstructing the entire matrix. This allows the system to identify the MOOCs that learners are most likely to enroll in, and the courses will then be ranked based on these predictions with the $top@k$ courses recommended to the learner.

\subsubsection{Memory-based collaborative filtering}
User-based and item-based KNN (UKNN and IKNN) are memory-based CF methods that infer missing interactions between users and items by leveraging the data of neighboring users or items. UKNN and IKNN predict missing values in the interaction matrix by calculating a weighted average of the scores from similar users or items. The weights assigned to each neighbor represent the similarity between their interaction vector and that of the target user or item.

\subsubsection{Model-based collaborative filtering}
Model-based CF RSs learn latent features for items and users, and then use these features to construct the interaction matrix. For example, Pure Singular Value Decomposition (SVD)~\cite{cremonesi2010performance} and Non-negative Matrix Factorization (NMF)~\cite{cichocki2009fast} are CF RSs that decompose the interaction matrix into two low-rank matrices for users and items. In NMF, the user and item learned matrices contain only non-negative values. Given $\mathbf{P_{u}}$ and $\mathbf{Q_{i}}$ as the learned latent features of users and items respectively, the enrolment matrix can be reconstructed by multiplying $\mathbf{P_{u}}$ and $\mathbf{Q_{i}}$. 

\subsection{Survival Analysis}
\label{method:sa}
%The number of days a student spends on a course contains valuable information on the their level of engagement and interest beyond a simple binary interaction. This time information can be exploited using SA which consists of a set of statistical and machine learning methods used to model the time until an event of interest occurs \cite{clark2003survival}.
\subsubsection{Defining time-to-event and censoring}
\label{sec:survival_definitions}
In this context, we can define the time-to-event variable as the number of days elapsed between a users' first and last interactions with a given course. The definition of censoring and event times is dependent on whether the event of interest is course dropout or completion. For example, if the time-to-event variable of interest is course completion, then event times are defined as the total number days elapsed between a student's first and last interactions for a course they have successfully completed, while students who have not yet completed that course at their last interaction are considered to be censored.

Survival data contains two key components: a time $Y$ which denotes the time an individual was followed up for, and a binary event variable $\delta$ which denotes whether $Y$ corresponds to an event time when the event of interest occurred, or a censoring time where the individual was last observed without the event having occurred. % If $T$ denotes the event time while $C$ denotes censoring time, then the data comes in the set of $(\delta,Y)$ where $Y=\min(T,C)$ and $\delta=I(T\le c)$. Therefore, if $\delta=1$ then the event time is known for an individual, while if $\delta=0$ then the event had not occurred at the time of last contact. 
Using the definitions from equation \ref{eq:hyperdef}, if the event of interest is defined as course completion, the tuple $(t_{ui},c)$ would correspond to a user who has experienced the event while the tuple $(t_{ui},d)$ corresponds to a user who is censored. If the event of interest is defined as course dropout, then the opposite is true where $(t_{ui},d)$ corresponds to a user who has experienced the event while $(t_{ui},c)$ corresponds to a censored user. Additionally, a set of covariates $X$ which could be predictive of a user's likelihood of successfully completing a course is often available on both the user and course levels. These covariates can be used as features in various SA models, such as like Regularized Cox Proportional Hazards Models (CoxNet), Gradient Boosted Ensembles (XGB), and Random Survival Forests (RSF), to predict the time to dropout or completion.


%In this context, we can define the time-to-event variable as the number of days elapsed between a users' first and last interactions with a given course, and the event of interest can be defined as either completion or dropout from that course. 

%In survival and time-to-event data right-occurs when certain individuals drop out from a study or do not experience the event of interest during the study period. Survival data therefore contains two key components: a time $Y$ which denotes the time an individual was followed up for, and a binary event variable $\delta$ which denotes whether $Y$ corresponds to an event time when the event of interest occurred, or a censoring time where the individual was last observed without the event having occurred. A key assumption of most SA methods is that all individuals will experience the event of interest even if outside the study period~\cite{Amico2018}. 

% A core feature of survival and time-to-event data is the presence of censoring which occurs when information on the time of the event of interest is missing. This often comes in the form of right-censoring which happens when certain individuals drop out from a study or do not experience the event of interest during the study period. Survival data therefore contains two key components: a time $Y$ which denotes the time an individual was followed up for, and a binary event variable $\delta$ which denotes whether $Y$ corresponds to an event time when the event of interest occurred, or a censoring time where the individual was last observed without the event having occurred. 
%If $T$ denotes the event time while $C$ denotes censoring time, then the data comes in the set of $(\delta,Y)$ where $Y=\min(T,C)$ and $\delta=I(T\le c)$. Therefore, if $\delta=1$ then the event time is known for an individual, while if $\delta=0$ then the event had not occurred at the time of last contact. 


%In this context, the definition of censoring and event times is dependent on whether the event of interest is course dropout or completion. If the time-to-event variable of interest is course completion, then event times are defined as the total number days elapsed between a student's first and last interactions with a course they have successfully completed, while students who have not yet completed that course at their last interaction are considered to be censored. Using the definitions from equation \ref{eq:hyperdef}, if the event of interest is defined as course completion, the tuple $(t_{ui},c)$ would correspond to a user who has experienced the event while the tuple $(t_{ui},d)$ corresponds to a user who is censored. If the event of interest is defined as course dropout, then the opposite is true where $(t_{ui},d)$ corresponds to a user who has experienced the event while $(t_{ui},c)$ corresponds to a censored user. 

%\subsubsection{Including Covariates}
% Additionally, a set of covariates $X$ which could be predictive of a user's likelihood of successfully completing a course are often available on both the user and course level and can be used as features in a number of survival analysis models to predict the time-to-dropout or time-to-completion like Regularized Cox Proportional Hazards Models (CoxNet), Gradient Boosted Ensembles (XGB), and Random Survival Forests (RSF) %. On the user level, such variables could include their age, level of education, subject interest, and prior user behavior like the number of courses completed and the time they spent interacting with them. On the course level, such variables could include the subject matter, a difficulty rating or amount of pre-requisites, as well as the behavior of other users like the average completion rate and time spent interacting with the given course. These covariates can be used as features in a number of survival analysis models to predict the time-to-dropout or time-to-completion like Regularized Cox Proportional Hazards Models (CoxNet), Gradient Boosted Ensembles (XGB), and Random Survival Forests (RSF). 

\subsubsection{Survival analysis definitions}
The Cox Proportional Hazards (CPH)~\cite{cox1972regression} is a semi-parametric method for estimating the hazard function $h(t,x)$ which measures the instaneous risk of experiencing the event at time $t$ given that the individual has not yet experienced it at time $t$. The hazard function can also be expressed as  $h(t)=\frac{d}{dt}H(t)$ where $H(t)$ is the Cumulative Hazard Function. While $H(t)$ does not have intuitive interpretation, the Survival Function $S(t)=P(T>t)=1-F(t)$ which denotes the probability that an individual does not experience the event before time $t$ can be expressed as $S(t)=\exp(-H(t))$. The CPH model is then defined as:

\begin{equation}
\label{eq:cph}
h(t,X_i) = h_0(t)\exp(X_i^T\beta)
\end{equation}

\noindent where $h_0(t)$ corresponds to a baseline hazard function which is common for all individuals and $\exp(X^T_i\beta)$ serves as a multiplicative factor affecting that baseline hazard based on an individuals covariates. In terms of estimation, the baseline hazard $h_0(t)$ is treated as a nuisance factor while the coefficients of $\beta$ are the main parameters of interest. If we define $T_1<\dots<T_J$ as the $J$ ordered distinct event times and assume there are no ties in event times, it can be shown~\cite{cox1975partial} that estimation for $\beta$ can be achieved by maximizing the log-partial likelihood:

%Given that $h(T)=\frac{d}{dt}H(t)$ and $S(t)=\exp(-H(t))$, the corresponding survival function can then be written as $S(t) =S_0(t)^{\exp(X^T\beta)}$ where $S_0(t)$ now corresponds to a baseline survival function.

% \begin{equation}
% S(t) = \exp(-H_0(t)\exp(X^t\beta)) =S_0(t)^{\exp(X^T\beta)}
% \end{equation}

\begin{equation}
\label{eq:cph_like}
LL(\beta) = \sum_{j=1}^N \delta_j \Big[X_j^T\beta - \log\Big(\sum_{i\in R_j}\exp(X_i^T\beta)\Big)\Big]
\end{equation}

\noindent where $X_j$ corresponds to the covariates of the individual who experienced the event at time $T_j$, while $R_j$ corresponds to the set of individuals still at risk of experiencing the event at time $T_j$. 

\subsubsection{Regularized Cox Proportional Hazards Model}

CoxNet~\cite{simon2011regularization} combines the well known $\ell_1$ lasso and $\ell_2$ ridge penalties on the coefficients of the CPH model in an elastic-net~\cite{zou2005regularization} fashion to introduce sparsity in high-dimensional problems and avoid overfitting. Given a set of covariates $p$, equation \ref{eq:cph_like} is modified to the corresponding objective function:

\begin{equation}
\arg \max_\beta \quad LL(\beta) - \alpha \Big( r\sum_{k=1}^p|\beta_k| + \frac{1-r}{2}\sum_{k=1}^p\beta_k^2\Big)
\end{equation}

\noindent where $r\in (0,1)$ controls the relative weight of the $\ell_1$ and $\ell_2$ penalties while $\alpha \in(0,1)$ controls the overall shrinkage.

\subsubsection{Gradient Boosted Ensembles}
Gradient Boosting is a common framework for predictive modeling which uses an ensemble of weak learners~\cite{Friedman2001}. In the context of SA, ~\cite{ridgeway_state_1999} proposed replacing the linear regression component of equation \ref{eq:cph_like} with a boosted ensemble of regression based estimators $f(x)$ to maximize the log-partial likelihood:
\begin{equation}
    LL(\beta) = \sum_{j=1}^N \delta_j \Big[f(x) - \log\Big(\sum_{i\in R_j}\exp(f(x)\Big)\Big]
\end{equation}

% A number of regression-based estimators can be considered for the weak-learners, and a popular implementation of boosted Cox models uses regression trees \cite{polsterl2020scikit}.
\noindent where a popular implementation of boosted Cox models uses regression trees for the weak-learners~\cite{polsterl2020scikit}.

\subsubsection{Random Survival Forests}
Random Survival Forests (RSF)~\cite{Ishwaran2008} are an extension of Random Forests~\cite{Breiman2001} to specifically model time-to-event outcomes with censored observations where individual trees within an RSF are grown to maximize the survival difference between nodes. The most common splitting criterion makes use of the log-rank test~\cite{Bland2004} between the resulting nodes. Once a tree is grown, the Cumulative Hazard Function within each terminal node $h$ is calculated using the non-parametric Nelson-Aalen estimator as:
% A Random Forest \cite{Breiman2001} is a tree-ensemble method  which aggregates the output of many trees grown on bootstrapped samples of the dataset using recursive partitioning of random subsets of the feature space based on a certain splitting criterion. 
%To appropriately handle time-to-event outcomes and censoring, splits in the individual trees within a RSF are selected to maximize the survival difference between the two nodes. The most common splitting criterion makes use of the log-rank test \cite{Bland2004} between the resulting nodes which is often used for comparing the survival outcomes of two groups. Unlike CoxNet and the Gradient Boosted Trees which make use of the log-partial likelihood, once a tree is grown, the Cumulative Hazard Function within each terminal node $h$ is calculated using the non-parametric Nelson-Aalen estimator [ref?] as:

\begin{equation}
H(t|x_i)=\hat{H}_h(t) = \sum_{t_{j,h}\le t} \frac{d_{j,h}}{R_{j,h}}
\end{equation}

\noindent where $d_{j,h}$ is the number of events at time $t_{j,h}$ and $R_{j,h}$ is the number of individuals at risk at time $t_{j,h}$. The ensemble estimator for the Cumulative Hazard Function is then obtained by averaging all the individual trees. %$B^{-1}\sum_{b=1}^B H_{b}(t|x_i)$.

% \begin{equation}
% H_e=\frac{1}{B} \sum_{b=1}^B H_{b}(t|x_i)
% \end{equation}


\subsubsection{Model Fitting and Interpretation}

The survival models can be fit using the known interactions of users and courses in a MOOC dataset. Each training instance is defined on an observed user-course interaction and the covariates for that instance can consist of both user-level and course-level information for the given user-course pair (the applied features in the experiments are discussed at the end of Section~\ref{sec:ES_d}). The target event times and indicators can be constructed based on whether the event of interest is course dropout or course completion as described in section \ref{sec:survival_definitions}. To avoid biased predictions based on the overall duration of a course, the time-to-event variable can be normalized within each course such that the minimum and maximum days elapsed between the first and last interactions of users within that course are the same across all courses in the database while information on the duration can be included as a covariate in the model. A prediction set of instances consisting of unseen user-course interactions can be constructed in a similar fashion as in the training stage. Now, each row corresponds to user-course pairs which are unobserved with the same user-level and course-level covariates as in the training stage. The survival model can now be used to predict a risk score for each of these unobserved interactions. 

The key distinction between modeling time-to-dropout and time-to-completion lies in the interpretation of the risk predictions. When modeling time-to-completion, a higher risk score for a user between course $A$ and course $B$ indicates that the student is likely to complete course $A$ faster, relative to the average student, compared to course $B$. A recommender would prioritize courses where a user has a high risk score, which corresponds to courses that the user is more likely to complete quickly. Conversely, when modeling time-to-dropout, a higher risk score for a user between course $A$ and $B$ means that the student is likely to drop out of course $A$ faster, relative to the average student, compared to course $B$. Therefore, a recommender would prioritize courses where the user has a low risk score, which corresponds to courses that the user is less likely to drop out of quickly and more likely to engage with for a longer duration. %Completion speed may not necessarily be the best metric for recommending courses, as taking longer to complete a course does not necessarily imply that a student did not enjoy taking the course. Similarly, longer dropout times could also be misleading as this does not necessarily imply that the student enjoyed the taking course even though they did not complete it. 

The two models thus capture two distinct user behaviors: how quickly they will complete a course and how quickly they will drop out of a course.  Instead of choosing between the time-to-completion and time-to-dropout models, predictions from both models can be utilized to identify courses that a user has both a high probability of completing quickly and a low probability of dropping out quickly. This can be achieved by aggregating the ranks of courses based on their dropout risk scores from lowest to highest and their completion risk scores from highest to lowest, and then ordering the courses based on the average ranks from these two lists.

\subsection{Re-ranking}
The main idea of this paper is to enhance the performance of CF-based RSs using predictions from a SA model trained on time-to-event data in the context of MOOCs. As discussed in Section~\ref{method:cf}, CF-based RSs are designed to rank the courses that learners are most likely to enroll in next (Step~1 in Algorithm~\ref{alg:alg}), based on their previous enrollments. By incorporating predictions from SA methods, which are trained on time-to-dropout ($\hat Y_{d}$ in Algorithm~\ref{alg:alg}) or time-to-completion data ($\hat Y_{c}$ in Algorithm~\ref{alg:alg}), or their aggregated ranked list ($L_{SA_{CD}}$ in Algorithm~\ref{alg:alg}), the initial list generated by a CF-based RS can be re-ranked. This re-ranking prioritizes courses that, among the initially ranked ones with the CF recommender, have shorter predicted time-to-completion or longer predicted time-to-dropout. The entire concept is illustrated in Algorithm~\ref{alg:alg}.

\begin{algorithm}[tb]
\caption{Enhancing Course Recommendations Using Survival Analysis}
\label{alg:alg}

\KwIn{user-item interaction matrix $M$, initial list length $l$, final recommendation list length $k$}
\KwOut{top@k recommendation list for each user}

% Step 1
\BlankLine
\textbf{Step 1: Collaborative filtering} \\
$E \gets binarize(M)$ \\
$\hat E \gets CF.fit\_predict(E)$ \\
$ L_{CF} \gets rank(\hat E, l)$ \tcp*[h]{Rank courses based on CF predictions and keep the first $l$ for each user} 
% Step 2
\BlankLine
\textbf{Step 2: Survival analysis} \\
$X \gets get\_features(M)$ \\
$\hat Y_{c} \gets SA_{c}.fit\_predict(X, M)$ \tcp*[h]{SA model based on time-to-completion}
$\hat Y_{d} \gets SA_{d}.fit\_predict(X, M)$ \tcp*[h]{SA model based on time-to-dropout}
$ L_{SA_{C}} \gets rank(\hat Y_{c})$ \tcp*[h]{Rank courses based on time-to-completion SA predictions}
$ L_{SA_{D}} \gets rank(\hat Y_{d})$ \tcp*[h]{Rank courses based on time-to-dropout SA predictions}
$ L_{SA_{CD}} \gets aggregate\_rank(L_{SA_{c}}, L_{SA_{D}})$ \\
% Step 3
\BlankLine
\textbf{Step 3: Re-ranking} \\
$top@k = re\_rank(L_{CF}, L_{SA_{(C/D/CD)}})$ \\

\BlankLine
\textbf{Return} $top@k$

\end{algorithm}

\section{Experimental design}
\label{sec:ES}
\subsection{Datasets}\label{sec:ES_d}
Publicly available datasets generated from MOOCs are scarce and most of them are described by Lohse et al. in~\cite{lohse2019surveying}. To evaluate our approach, we used three widely recognized publicly available datasets: XuetangX~\cite{feng2019understanding}, KDDCUP~\cite{feng2019understanding}, and Canvas~\cite{canvas}. Both the KDDCUP and XuetangX datasets are anonymized and provided by the XuetangX platform\footnote{\url{https://www.xuetangx.com/}}. The Canvas dataset contains de-identified data from Canvas Network\footnote{\url{https://www.canvas.net/}} open courses from January 2014 to September 2015. Table~\ref{tab:data} describes the three preprocessed publicly available MOOC datasets that were used to evaluate the proposed approach.
The raw JSON files containing logs of all interactions an individual had with a course for the XuetangX and KDDCUP datasets were processed to extract the first and last interactions a user had with a given course. The time-to-event variable was defined as the difference between the dates of these actions and binary indicators denoting whether a user dropped out or completed the course were provided. The Canvas dataset was in tabular format and already contained that information. In all three datasets, the time-to-event variable was normalized within each course such that the minimum and maximum days elapsed between the first and last interactions of users within that course were the same for each course. 

Due to the lack of consistent high-quality metadata at both the student and course levels across the three datasets—such as age, education, or course descriptions—the covariates for the models were kept relatively simple. For each user, the number of courses taken, percentage of courses completed, and average completion and dropout times in the training set were included as features in the SA models. Additionally, for each student-course pair, the student level user-item interaction vector containing all enrollment and time-to-event data for that student in the training-set, as well as the course level item-user interaction vector containing all enrollments and time-to-event data students had with that course in the training set were included after performing dimensionality reduction using Principal Components Analysis and retaining the components containing 80\% of the total variance. 

\begin{table*}
\centering
  \caption{Datasets descriptions}
    \label{tab:data} 
   
  \begin{tabular}{lccc}
    \toprule
    & XuentangX&KDDCUP&Canvas\\
    \midrule
    Number of Users &2417&1944&959\\
   Number of Items &246&39&193\\
   Sparsity &$95.5\%$&$87.1\%$&$95.4\%$ \\
   Avg number of completed courses per user&4.6&4.8&4.3\\
   Avg number of dropout courses per user&5.9&4.4&4.5\\
   % Avg time-to-completion&77.7&18.1&0.015\\ 
   % Avg time-to-dropout&46.6&6.6&0.069\\ 

  \bottomrule
\end{tabular}
\end{table*}

\subsection{Experimental setup}
\label{setup}
Before processing the datasets the cold-start users and courses that have less than 5 interactions where at least three of them should be course completion, were dropped. Then each dataset was split into three disjoint sets: training, validation and test sets. Test and validation sets contained three (at least one completed course) and one (either completed or dropout course) interaction per user respectively. The rest of interactions were used for training. The validation set was used to tune the hyperparameters (the details about hyperparameter tuning is reported in Appendix~\ref{hyper}). 

% To evaluate the performance of SA methods, concordance index (C-Index), which is a popular evaluation measure for SA methods, is used. To evaluate the final recommendations, two variants of NDCG\footnote{Normalized Discounted Cumulative Gain} are considered. NDCG is a rank-sensitive evaluation measure that penalizes the score of recommendations if the relevant items appear in the lower ranks in the recommendation list. To incorporate the time-to-event information in the evaluation measure, we considered a variant of NDCG (NDCG-t), where the relevance scores are linearly decayed based on the maximum time-to-dropout or minimum time-to-completion for each course since the courses with long dropout time or short completion time are more preferred. 

The five-fold cross-validated concordance index (C-Index) on the training set was used to tune and evaluate the performance of SA methods. The C-index can be seen as a generalization of the Area Under the Curve (AUC) in classification models, particularly when dealing with survival data that includes censored information. The metric essentially evaluates whether individuals with higher risk scores experience the event faster than those with lower risk scores~\cite{Longato2020}. Similar to AUC, the C-index ranges from 0 to 1. Additionally, two variants of the Normalized Discounted Cumulative Gain (NDCG) were considered to assess the final recommendations. NDCG is a rank-sensitive evaluation measure that penalizes recommendation scores if relevant items appear lower in the list. Apart from the regular NDCG, and in order to incorporate time-to-event information, we introduced a variant (NDCG-t), where relevance scores are linearly decayed based on either the maximum time-to-dropout for dropout courses or the minimum time-to-completion for completed courses. This approach prioritizes courses with longer dropout times or shorter completion times as more preferred.

\subsection{Competing approaches}
\label{baselines}
For each step mentioned in Algorithm~\ref{alg:alg}, different competing methods are applied. For the first step, we selected the CF baselines based on the results of the award winning paper~\cite{dacrema2019we}, which showed that simple CF RSs such as memory-based approaches (UKNN and IKNN), and SLIM outperform more recent complex deep neural network based approaches. Therefore, the following baselines are selected:

\begin{itemize}
    \item \textbf{UKNN} and \textbf{IKNN}: user- and item-based KNN~\cite{sarwar2001item,lops2011content} are memory-based CF methods that impute missing interactions between users and items based on the interactions of neighbor users/items. 
    % \item \textbf{IKNN}: item-based (IKNN) collaborative filtering (CF)~\cite{lops2011content} is a memory-based CF methods that impute missing interactions between users and items based on the interactions of neighbor items.
    \item \textbf{SVD}: Singular Value Decomposition (SVD)~\cite{cremonesi2010performance} can be applied to decompose the interaction matrix to two low-rank matrices for users and items. 
    \item \textbf{NMF}: Non-negative Matrix Factorization (NMF)~\cite{cichocki2009fast} is similar to SVD but the learned user and item matrices contain non-negative values.
    \item \textbf{WRMF}: weighted regularized matrix factorization (WRMF)~\cite{pan2008one} is a model-based CF method that utilizes the alternating-least-squares optimization algorithm to learn its parameters.
    \item \textbf{EASE}: Embarrassingly Shallow Autoencoders (EASE)~\cite{steck2019embarrassingly} is a linear collaborative filtering model based on shallow auto-encoders~\cite{cheng2016wide}.
    \item \textbf{SLIM}: Sparse LInear Method (SLIM)~\cite{ning2011slim} is a method that learns the sparse aggregation coefficient square matrix using the optimization problem regularized with L1 and L2 norms.
\end{itemize}

For the second step in Algorithm~\ref{alg:alg}, the following SA methods are considered:
\begin{itemize}
    \item \textbf{CoxNet} is a penalized variant of the Cox Proportional Hazards Model.
    \item \textbf{RSF} is random forest extension to survival or time-to-event outcomes.
    \item \textbf{XGB} is a boosting method using regression trees as base learners with a cox partial likelihood.
\end{itemize}

Finally, for the re-ranking step (the third step in Algorithm~\ref{alg:alg}), we followed three options to include SA predictions, re-ranking based on time-to-completion (re-ranking based on $ L_{SA_{C}}$), time-to-dropout (re-ranking based on $ L_{SA_{D}}$) and their combined ranks (re-ranking based on $ L_{SA_{CD}}$).

\section{Results and Discussion}
\label{sec:result}
Five-fold cross-validation on the training set was used to select the best parameters for each survival method to model time-to-completion or time-to-dropout. The cross-validated C-index  for each dataset and method is reported in Table~\ref{tab:SA} with the optimal parameters in Table~\ref{tab:hyper}. $XGB$ outperforms both $CoxNet$ and $RSF$ in terms of the C-index across all three datasets, for both time-to-completion and time-to-dropout prediction tasks. Except for the case where $XGB$ is applied to the Canvas dataset, SA methods trained on time-to-dropout generally perform better than those trained on time-to-completion. This suggests that time-to-dropout is more informative when modeling time-to-event in MOOCs. We chose $XGB$ to model time-to-event in our experiments due to its better C-index based on 5-fold cross-validation compared to $CoxNet$ and $RSF$. 

Table~\ref{tab:rerank_full} shows the results of applying several CF-based RSs and the proposed post-processing approaches on three MOOCs datasets, evaluated using two variants of the NDCG measure. In this table, the `Baseline' column includes CF RSs performance without post-processing with SA models predictions. `+D', `+C,' and `+DC' in the table stand for post-processing based on time-to-dropout, time-to-completion, and their combination, respectively. The best-performing approach in each dataset and for each measure is represented with the underlined numbers.

For the first step in Algorithm~\ref{alg:alg}, among the CF RSs, SLIM performs best for XuetangX and KDD, while UKNN is the top performer for Canvas according to both evaluation measures. As shown in the table, all three post-processing approaches perform better compared to the corresponding CF-based RS baseline. Post-processing based on both time-to-dropout and time-to-completion (`+DC') performs better in most cases compared to post-processing based on only one type of event, which implies that using SA predictions based on both events, i.e., dropout and completion, are more effective to model user preferences and needs.

% The re-ranking approach using both time-to-dropout and time-to-completion predictions achieved the best performance in $XuetangX$ and $KDD$ datasets and the re-ranking approach using only time-to-dropout predictions performs the best for Canvas dataset w.r.t. both evaluation measures. We have included the results of more extensive settings where we re-ranked the $UKNN$ recommendations using all three SA methods ($CoxNet$, $XGB$ and $RSF$) based on different initial list lengths (In Table~\ref{tab:results} the initial list length is fixed to 8) in Appendix~\ref{all_SA}. Additionally, the results of re-ranking the recommendations of all mentioned CF RSs based on $XGB$ predictions are provided in Appendix~\ref{all CFs}.

\begin{table*}
\centering
  \caption{Survival analysis methods comparisons w.r.t. C-index}
    \label{tab:SA} 
  \begin{tabular}{@{}lcccccc@{}}
\toprule
       & \multicolumn{2}{c}{XuetangX}                                        & \multicolumn{2}{c}{Canvas}                                   & \multicolumn{2}{c}{KDD}                                      \\ \midrule
       & \multicolumn{1}{l}{Dropout} & \multicolumn{1}{l}{Completion} & \multicolumn{1}{l}{Dropout} & \multicolumn{1}{l}{Completion} & \multicolumn{1}{l}{Dropout} & \multicolumn{1}{l}{Completion} \\ \midrule
Coxnet & 0.7119                      & 0.7117                         & 0.7355                      & 0.7355                         & 0.7061                      & 0.6885                         \\
RSF    & 0.7223                      & 0.7064                         & 0.7827                      & 0.7722                         & 0.7475                      & 0.7148                         \\
XGB    & \textbf{0.7479}                      & \textbf{0.7269}                         & \textbf{0.7956}                      & \textbf{0.8079}                         & \textbf{0.8083}                      & \textbf{0.7309}                         \\ \bottomrule
\end{tabular}
\end{table*}
\newcommand{\cg}{\cellcolor{gray!20}}

\begin{table*}
\caption{Re-ranking with XGb for all baseline reccomenders}
\label{tab:rerank_full}
\centering
\scalebox{0.79}{
\begin{tabular}{cllcccc|cccc} 
\toprule
\multicolumn{1}{l}{}             &                             &                                    & \multicolumn{4}{c}{\textbf{ndcg}}         & \multicolumn{4}{c}{\textbf{ndcg-t}}                             \\ 
\midrule
\multirow{22}{*}{\textbf{Top 3}} & \multicolumn{1}{c}{Dataset} & \multicolumn{1}{c}{CF Model} & Baseline & + D   & + C   & + DC  & Baseline & + D                        & + C   & + DC   \\ 
\cmidrule{2-11}
                                 & \multirow{7}{*}{Canvas}     & EASE                               & 0.159    & 0.259 & 0.256 & \cg{0.262} & 0.16     & 0.259                      & 0.256 & \cg{0.262}  \\
                                 &                             & WRMF                               & 0.149    & 0.264 & 0.252 & \cg{\underline{0.275}} & 0.15     & 0.264                      & 0.252 & \cg{\underline{0.275}}  \\
                                 &                             & IKNN                               & 0.159    & 0.254 & 0.254 & \cg{0.263} & 0.16     & 0.254                      & 0.254 & \cg{0.263}  \\
                                 &                             & NMF                                & 0.072    & 0.173 & 0.205 & \cg{0.206} & 0.074    & 0.173                      & 0.205 & \cg{0.206}  \\
                                 &                             & SLIM                               & 0.152    & \cg{0.261} & 0.255 & 0.252 & 0.152    & \cg{0.261} & 0.255 & 0.252  \\
                                 &                             & SVD                                & 0.139    & \cg{0.242} & 0.217 & 0.231 & 0.141    & \cg{0.242}                      & 0.217 & 0.231  \\
                                 &                             & UKNN                               & 0.167    & \cg{0.259} & 0.249 & 0.247 & 0.166    & \cg{0.26}                       & 0.249 & 0.247  \\ 
\cmidrule{2-11}
                                 & \multirow{7}{*}{KDD}        & EASE                               & 0.429    & 0.628 & 0.596 & \cg{0.632} & 0.425    & 0.628                      & 0.596 & \cg{0.633}  \\
                                 &                             & WRMF                               & 0.296    & 0.535 & \cg{0.631} & 0.607 & 0.293    & 0.535                      & \cg{0.631} & 0.607  \\
                                 &                             & IKNN                               & 0.396    & 0.623 & 0.609 & \cg{0.645} & 0.392    & 0.623                      & 0.609 & \cg{0.645}  \\
                                 &                             & NMF                                & 0.32     & 0.589 & 0.556 & \cg{0.609} & 0.318    & 0.589                      & 0.555 & \cg{0.609}  \\
                                 &                             & SLIM                               & 0.438    & 0.62  & 0.614 & \cg{\underline{0.646}} & 0.435    & 0.621                      & 0.613 & \cg{\underline{0.646}}  \\
                                 &                             & SVD                                & 0.41     & 0.597 & 0.598 & \cg{0.633} & 0.407    & 0.597                      & 0.598 & \cg{0.632}  \\
                                 &                             & UKNN                               & 0.413    & 0.617 & 0.588 & \cg{0.633} & 0.417    & 0.589                      & \cg{0.633} & 0.632  \\ 
\cmidrule{2-11}
                                 & \multirow{7}{*}{XuetangX}   & EASE                               & 0.209    & 0.373 & 0.331 & \cg{0.392} & 0.244    & 0.373                      & 0.331 & \cg{0.392}  \\
                                 &                             & WRMF                               & 0.215    & 0.400 & 0.371 & \cg{0.411} & 0.253    & 0.4                        & 0.372 & \cg{0.411}  \\
                                 &                             & IKNN                               & 0.237    & 0.399 & 0.388 & \cg{0.413} & 0.234    & 0.399                      & 0.387 & \cg{0.413}  \\
                                 &                             & NMF                                & 0.158    & 0.359 & 0.362 & \cg{0.384} & 0.157    & 0.359                      & 0.362 & \cg{0.384} \\
                                 &                             & SLIM                               & 0.253    & 0.434 & 0.411 & \cg{\underline{0.441}} & 0.249    & 0.434                      & 0.411 & \cg{\underline{0.441}}  \\
                                 &                             & SVD                                & 0.22     & 0.373 & 0.377 & \cg{0.387} & 0.219    & 0.373                      & 0.377 & \cg{0.387}  \\
                                 &                             & UKNN                               & 0.243    & 0.407 & 0.379 & \cg{0.408} & 0.24     & 0.407                      & 0.379 & \cg{0.408} \\ 
\midrule
\multirow{21}{*}{\textbf{Top 5}} & \multirow{7}{*}{Canvas}     & EASE                               & 0.189    & \cg{0.307} & 0.304 & \cg{0.307} & 0.229    & \cg{0.308}                      & 0.304 & 0.306  \\
                                 &                             & WRMF                               & 0.183    & 0.313 & 0.317 & \cg{0.323} & 0.183    & 0.313                      & 0.317 & \cg{0.324}  \\
                                 &                             & IKNN                               & 0.19     & 0.307 & 0.303 & \cg{0.308} & 0.19     & \cg{0.307}                      & 0.303 & \cg{0.307}  \\
                                 &                             & NMF                                & 0.094    & 0.213 & \cg{0.239} & 0.234 & 0.095    & 0.213                      & \cg{0.239} & 0.234  \\
                                 &                             & SLIM                               & 0.183    & 0.309 & \cg{0.312} & 0.31  & 0.183    & 0.308                      & \cg{0.313} & 0.31   \\
                                 &                             & SVD                                & 0.167    & \cg{0.292} & 0.273 & 0.285 & 0.167    & \cg{0.292}                      & 0.273 & 0.285  \\
                                 &                             & UKNN                               & 0.198    & \cg{\underline{0.32}}  & 0.309 & 0.311 & 0.198    & \cg{\underline{0.32}}   & 0.309 & 0.311  \\ 
\cmidrule{2-11}
                                 & \multirow{7}{*}{KDD}        & EASE                               & 0.508    & \cg{0.653} & 0.623 & \cg{0.653} & 0.503    & \cg{0.653}                      & 0.621 & 0.652  \\
                                 &                             & WRMF                               & 0.373    & 0.573 & \cg{0.645} & 0.629 & 0.368    & 0.573                      & \cg{0.643} & 0.629  \\
                                 &                             & IKNN                               & 0.472    & 0.648 & 0.638 & \cg{\underline{0.666}} & 0.468    & 0.647                      & 0.636 & \cg{0.664}  \\
                                 &                             & NMF                                & 0.392    & 0.622 & 0.598 & \cg{0.636} & 0.388    & 0.622                      & 0.596 & \cg{0.635}  \\
                                 &                             & SLIM                               & 0.518    & 0.647 & 0.643 & \cg{\underline{0.666}} & 0.515    & 0.647                      & 0.641 & \cg{\underline{0.666}}  \\
                                 &                             & SVD                                & 0.487    & 0.628 & 0.623 & \cg{0.647} & 0.482    & 0.627                      & 0.622 & \cg{0.647}  \\
                                 &                             & UKNN                               & 0.489    & 0.646 & 0.621 & \cg{0.655} & 0.486    & 0.645                      & 0.62  & \cg{0.654}  \\ 
\cmidrule{2-11}
                                 & \multirow{7}{*}{XuetangX}   & EASE                               & 0.244    & 0.42  & 0.394 & \cg{0.433} & 0.241    & 0.418                      & 0.394 & \cg{0.432}  \\
                                 &                             & WRMF                               & 0.251    & 0.447 & 0.415 & \cg{0.448} & 0.248    & 0.446                      & 0.416 & \cg{0.448}  \\
                                 &                             & IKNN                               & 0.275    & 0.453 & 0.44  & \cg{0.458} & 0.272    & 0.451                      & 0.44  & \cg{0.458}  \\
                                 &                             & NMF                                & 0.195    & 0.399 & 0.404 & \cg{0.419} & 0.193    & 0.398                      & 0.404 & \cg{0.419}  \\
                                 &                             & SLIM                               & 0.297    & 0.483 & 0.464 & \cg{\underline{0.487}} & 0.293    & 0.482                      & 0.464 & \cg{\underline{0.487}}  \\
                                 &                             & SVD                                & 0.257    & 0.425 & \cg{0.431} & \cg{0.431} & 0.255    & 0.424                      & \cg{0.431} & \cg{0.431}  \\
                                 &                             & UKNN                               & 0.284    & 0.456 & 0.438 & \cg{0.457} & 0.281    & 0.454                      & 0.438 & \cg{0.457}  \\
\bottomrule
\end{tabular}}
\end{table*}

Beyond its superior performance compared to other competing methods, our proposed approach offers the added benefit of providing more clear explanations for recommendations. This allows us to determine the extent to which recommendations are influenced by enrollment likelihood (based on the course ranking in $L_{CF}$), time-to-completion (based on the course ranking in $L_{SA_{C}}$), or time-to-dropout (based on the course ranking in $L_{SA_{d}}$). For example, the explanation like: ``\textit{This course is recommended because learners similar to you have enrolled in these MOOCs"}, can be extended with ``\textit{We believe you will complete this course swiftly since we expect you finish this course X hours/days faster than the average student}, " or ``\textit{You will be more engaged with this course since you have X\% lower chance of dropout from the course relative to the average student}". Consequently, learners can effectively control various elements affecting recommendations and can disable any factors they deem irrelevant based on the provided explanations.

% Despite the promising results reported in this section, we acknowledge several limitations of our study. The proposed method was validated on only three datasets, which, to the best of our knowledge, are the only relevant datasets available for MOOC recommendations. Another limitation was the lack of features from both learners and courses in the SA methods. A richer feature space would enable the SA method to better model learners' time-to-event predictions. Additionally, our validation used historical data through offline evaluation, meaning we cannot confirm if MOOC learners will perceive the reported better performance. A user study should be conducted for further validation.

% \begin{table}
% \centering
% \caption{Results ("+D", "+C", and "+DC" refers to re-ranking based on dropout, completion and both respectively.)}
% \label{tab:results}
% \resizebox{\columnwidth}{!}{%
% \begin{tblr}{
%   cell{1}{2} = {c=4}{c},
%   cell{1}{6} = {c=4}{c},
%   cell{1}{10} = {c=4}{c},
%   cell{2}{2} = {c=2}{c},
%   cell{2}{4} = {c=2}{c},
%   cell{2}{6} = {c=2}{c},
%   cell{2}{8} = {c=2}{c},
%   cell{2}{10} = {c=2}{c},
%   cell{2}{12} = {c=2}{c},
%   cell{3}{2} = {c},
%   cell{3}{3} = {c},
%   cell{3}{4} = {c},
%   cell{3}{5} = {c},
%   cell{3}{6} = {c},
%   cell{3}{7} = {c},
%   cell{3}{8} = {c},
%   cell{3}{9} = {c},
%   cell{3}{10} = {c},
%   cell{3}{11} = {c},
%   cell{3}{12} = {c},
%   cell{3}{13} = {c},
%   hline{1,14} = {-}{0.08em},
%   hline{2-3} = {2-13}{0.03em},
%   hline{4,11} = {-}{0.05em},
% }
%         & XuetangX       &                &                &                & Canvas         &                &                &                & KDD            &                &                &                \\
%         & Top3           &                & Top5           &                & Top3           &                & Top5           &                & Top3           &                & Top5           &                \\
%         & ndcg           & ndcg-t         & ndcg           & ndcg-t         & ndcg           & ndcg-t         & ndcg           & ndcg-t         & ndcg           & ndcg-t         & ndcg           & ndcg-t         \\
% UKNN    & 0.243          & 0.240          & 0.284          & 0.281          & 0.167          & 0.166          & 0.198          & 0.198          & 0.413          & 0.410          & 0.489          & 0.486          \\
% IKNN    & 0.237          & 0.234          & 0.275          & 0.272          & 0.159          & 0.160          & 0.190          & 0.192          & 0.396          & 0.392          & 0.472          & 0.468          \\
% SVD     & 0.220          & 0.219          & 0.257          & 0.255          & 0.139          & 0.141          & 0.167          & 0.167          & 0.410          & 0.407          & 0.487          & 0.482          \\
% NMF     & 0.158          & 0.157          & 0.195          & 0.193          & 0.072          & 0.074          & 0.094          & 0.095          & 0.320          & 0.318          & 0.392          & 0.388          \\
% WRMF    & 0.215          & 0.253          & 0.251          & 0.248          & 0.149          & 0.150          & 0.183          & 0.183          & 0.296          & 0.293          & 0.373          & 0.368          \\
% EASE    & 0.209          & 0.208          & 0.244          & 0.241          & 0.159          & 0.160          & 0.189          & 0.190          & 0.429          & 0.425          & 0.508          & 0.503          \\
% SLIM    & 0.253          & 0.249          & 0.297          & 0.293          & 0.152          & 0.152          & 0.183          & 0.183          & 0.438          & 0.435          & 0.518          & 0.515          \\
% UKNN+D  & 0.407          & 0.407          & 0.456          & 0.454          & \textbf{0.259} & \textbf{0.260} & \textbf{0.320} & \textbf{0.320} & 0.617          & 0.617          & 0.646          & 0.645          \\
% UKNN+C  & 0.379          & 0.379          & 0.438          & 0.438          & 0.249          & 0.249          & 0.309          & 0.309          & 0.588          & 0.589          & 0.621          & 0.620          \\
% UKNN+DC & \textbf{0.408} & \textbf{0.408} & \textbf{0.457} & \textbf{0.457} & 0.247          & 0.247          & 0.311          & 0.311          & \textbf{0.633} & \textbf{0.633} & \textbf{0.655} & \textbf{0.654} 
% \end{tblr}}
% \end{table}
% \end{table*}

\section{Conclusion}
\label{sec:con}
Time-to-event information such as time-to-completion and time-to-dropout in MOOCs provides valuable insights into learners' preferences and needs. In this paper, we proposed modeling time-to-event in MOOCs—specifically, time-to-completion and time-to-dropout—using survival analysis methods. We sought to leverage these predictions to improve collaborative filtering recommender systems. This enhancement enables the recommender system to recommend courses that learners are both more likely to enroll in and complete quickly or stay engaged with for longer periods. As detailed in Section~\ref{sec:result}, our approach outperforms competing collaborative filtering-based recommender systems on three publicly available datasets, with better performance according to two variants of the NDCG measure.

There are two main directions for future work: (i) In this paper, we used survival analysis predictions to post-process the collaborative filtering-based recommendations. A promising direction for future work is to develop an ensemble model that can model user preferences and needs from different perspectives~\cite{gharahighehi2021ensemble}, or to model the problem as a multi-task learning problem to simultaneously learn two tasks: how likely a learner will enroll in a MOOC and how long it will take to complete the course or drop out from it. (ii) In this paper, we created two separate survival models for time-to-completion and time-to-dropout. A key assumption of most survival models is that all individuals will eventually experience the event of interest. In this context, a student who completes a course will never drop out of it when building a time-to-dropout model. We would like to investigate the merits of survival analysis methods with cured fraction information \cite{Amico2018} which remedies this by recognizing that some individuals will never experience the event of interest and explicitly models the probability of the event occurring inside the survival model. 

% There are two main directions for future work: (i) In this paper, we used survival analysis predictions to post-process the collaborative filtering-based recommendations. A promising direction for future work is to model the problem as a multi-task learning problem to simultaneously learn two tasks: how likely a learner will enroll in a MOOC and how long it will take to complete the course or drop out from it. (ii) In this paper, we created two separate survival models for time-to-completion and time-to-dropout. A key assumption of most survival models is that all individuals will eventually experience the event of interest. In this context, a student who completes a course will never drop out of it when building a time-to-dropout model. We would like to investigate the merits of survival analysis methods with cured fraction information \cite{Amico2018} which remedies this by recognizing that some individuals will never experience the event of interest and explicitly models the probability of the event occurring inside the survival model. 



% \section{sazdlts}\label{sec2}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \section{This is an example for first level head---section head}\label{sec3}

% \subsection{This is an example for second level head---subsection head}\label{subsec2}

% \subsubsection{This is an example for third level head---subsubsection head}\label{subsubsec2}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. 

% \section{Equations}\label{sec4}

% Equations in \LaTeX\ can either be inline or on-a-line by itself (``display equations''). For
% inline equations use the \verb+$...$+ commands. E.g.: The equation
% $H\psi = E \psi$ is written via the command \verb+$H \psi = E \psi$+.

% For display equations (with auto generated equation numbers)
% one can use the equation or align environments:
% \begin{equation}
% \|\tilde{X}(k)\|^2 \leq\frac{\sum\limits_{i=1}^{p}\left\|\tilde{Y}_i(k)\right\|^2+\sum\limits_{j=1}^{q}\left\|\tilde{Z}_j(k)\right\|^2 }{p+q}.\label{eq1}
% \end{equation}
% where,
% \begin{align}
% D_\mu &=  \partial_\mu - ig \frac{\lambda^a}{2} A^a_\mu \nonumber \\
% F^a_{\mu\nu} &= \partial_\mu A^a_\nu - \partial_\nu A^a_\mu + g f^{abc} A^b_\mu A^a_\nu \label{eq2}
% \end{align}
% Notice the use of \verb+\nonumber+ in the align environment at the end
% of each line, except the last, so as not to produce equation numbers on
% lines where no equation numbers are required. The \verb+\label{}+ command
% should only be used at the last line of an align environment where
% \verb+\nonumber+ is not used.
% \begin{equation}
% Y_\infty = \left( \frac{m}{\textrm{GeV}} \right)^{-3}
%     \left[ 1 + \frac{3 \ln(m/\textrm{GeV})}{15}
%     + \frac{\ln(c_2/5)}{15} \right]
% \end{equation}
% The class file also supports the use of \verb+\mathbb{}+, \verb+\mathscr{}+ and
% \verb+\mathcal{}+ commands. As such \verb+\mathbb{R}+, \verb+\mathscr{R}+
% and \verb+\mathcal{R}+ produces $\mathbb{R}$, $\mathscr{R}$ and $\mathcal{R}$
% respectively (refer Subsubsection~\ref{subsubsec2}).

% \section{Tables}\label{sec5}

% Tables can be inserted via the normal table and tabular environment. To put
% footnotes inside tables you should use \verb+\footnotetext[]{...}+ tag.
% The footnote appears just below the table itself (refer Tables~\ref{tab1} and \ref{tab2}). 
% For the corresponding footnotemark use \verb+\footnotemark[...]+

% \begin{table}[h]
% \caption{Caption text}\label{tab1}%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Column 1 & Column 2  & Column 3 & Column 4\\
% \midrule
% row 1    & data 1   & data 2  & data 3  \\
% row 2    & data 4   & data 5\footnotemark[1]  & data 6  \\
% row 3    & data 7   & data 8  & data 9\footnotemark[2]  \\
% \botrule
% \end{tabular}
% \footnotetext{Source: This is an example of table footnote. This is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote. This is an example of table footnote.}
% \footnotetext[2]{Example for a second table footnote. This is an example of table footnote.}
% \end{table}

% \noindent
% The input format for the above table is as follows:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% \begin{table}[<placement-specifier>]
% \caption{<table-caption>}\label{<table-label>}%
% \begin{tabular}{@{}llll@{}}
% \toprule
% Column 1 & Column 2 & Column 3 & Column 4\\
% \midrule
% row 1 & data 1 & data 2	 & data 3 \\
% row 2 & data 4 & data 5\footnotemark[1] & data 6 \\
% row 3 & data 7 & data 8	 & data 9\footnotemark[2]\\
% \botrule
% \end{tabular}
% \footnotetext{Source: This is an example of table footnote. 
% This is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote.
% This is an example of table footnote.}
% \footnotetext[2]{Example for a second table footnote. 
% This is an example of table footnote.}
% \end{table}
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%

% \begin{table}[h]
% \caption{Example of a lengthy table which is set to full textwidth}\label{tab2}
% \begin{tabular*}{\textwidth}{@{\extracolsep\fill}lcccccc}
% \toprule%
% & \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]} & \multicolumn{3}{@{}c@{}}{Element 2\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
% Project & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
% \midrule
% Element 3  & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$\\
% Element 4  & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$\\
% \botrule
% \end{tabular*}
% \footnotetext{Note: This is an example of table footnote. This is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
% \footnotetext[1]{Example for a first table footnote.}
% \footnotetext[2]{Example for a second table footnote.}
% \end{table}

% In case of double column layout, tables which do not fit in single column width should be set to full text width. For this, you need to use \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ instead of \verb+\begin{table}+ \verb+...+ \verb+\end{table}+ environment. Lengthy tables which do not fit in textwidth should be set as rotated table. For this, you need to use \verb+\begin{sidewaystable}+ \verb+...+ \verb+\end{sidewaystable}+ instead of \verb+\begin{table*}+ \verb+...+ \verb+\end{table*}+ environment. This environment puts tables rotated to single column width. For tables rotated to double column width, use \verb+\begin{sidewaystable*}+ \verb+...+ \verb+\end{sidewaystable*}+.

% \begin{sidewaystable}
% \caption{Tables which are too long to fit, should be written using the ``sidewaystable'' environment as shown here}\label{tab3}
% \begin{tabular*}{\textheight}{@{\extracolsep\fill}lcccccc}
% \toprule%
% & \multicolumn{3}{@{}c@{}}{Element 1\footnotemark[1]}& \multicolumn{3}{@{}c@{}}{Element\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
% Projectile & Energy	& $\sigma_{calc}$ & $\sigma_{expt}$ & Energy & $\sigma_{calc}$ & $\sigma_{expt}$ \\
% \midrule
% Element 3 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
% Element 4 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
% Element 5 & 990 A & 1168 & $1547\pm12$ & 780 A & 1166 & $1239\pm100$ \\
% Element 6 & 500 A & 961  & $922\pm10$  & 900 A & 1268 & $1092\pm40$ \\
% \botrule
% \end{tabular*}
% \footnotetext{Note: This is an example of table footnote this is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
% \footnotetext[1]{This is an example of table footnote.}
% \end{sidewaystable}

% \section{Figures}\label{sec6}

% As per the \LaTeX\ standards you need to use eps images for \LaTeX\ compilation and \verb+pdf/jpg/png+ images for \verb+PDFLaTeX+ compilation. This is one of the major difference between \LaTeX\ and \verb+PDFLaTeX+. Each image should be from a single input .eps/vector image file. Avoid using subfigures. The command for inserting images for \LaTeX\ and \verb+PDFLaTeX+ can be generalized. The package used to insert images in \verb+LaTeX/PDFLaTeX+ is the graphicx package. Figures can be inserted via the normal figure environment as shown in the below example:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% \begin{figure}[<placement-specifier>]
% \centering
% \includegraphics{<eps-file>}
% \caption{<figure-caption>}\label{<figure-label>}
% \end{figure}
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.9\textwidth]{fig.eps}
% \caption{This is a widefig. This is an example of long caption this is an example of long caption  this is an example of long caption this is an example of long caption}\label{fig1}
% \end{figure}

% In case of double column layout, the above format puts figure captions/images to single column width. To get spanned images, we need to provide \verb+\begin{figure*}+ \verb+...+ \verb+\end{figure*}+.

% For sample purpose, we have included the width of images in the optional argument of \verb+\includegraphics+ tag. Please ignore this. 

% \section{Algorithms, Program codes and Listings}\label{sec7}

% Packages \verb+algorithm+, \verb+algorithmicx+ and \verb+algpseudocode+ are used for setting algorithms in \LaTeX\ using the format:

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{verbatim}
% \begin{algorithm}
% \caption{<alg-caption>}\label{<alg-label>}
% \begin{algorithmic}[1]
% . . .
% \end{algorithmic}
% \end{algorithm}
% \end{verbatim}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%

% You may refer above listed package documentations for more details before setting \verb+algorithm+ environment. For program codes, the ``verbatim'' package is required and the command to be used is \verb+\begin{verbatim}+ \verb+...+ \verb+\end{verbatim}+. 

% Similarly, for \verb+listings+, use the \verb+listings+ package. \verb+\begin{lstlisting}+ \verb+...+ \verb+\end{lstlisting}+ is used to set environments similar to \verb+verbatim+ environment. Refer to the \verb+lstlisting+ package documentation for more details.

% A fast exponentiation procedure:

% \lstset{texcl=true,basicstyle=\small\sf,commentstyle=\small\rm,mathescape=true,escapeinside={(*}{*)}}
% \begin{lstlisting}
% begin
%   for $i:=1$ to $10$ step $1$ do
%       expt($2,i$);  
%       newline() od                (*\textrm{Comments will be set flush to the right margin}*)
% where
% proc expt($x,n$) $\equiv$
%   $z:=1$;
%   do if $n=0$ then exit fi;
%      do if odd($n$) then exit fi;                 
%         comment: (*\textrm{This is a comment statement;}*)
%         $n:=n/2$; $x:=x*x$ od;
%      { $n>0$ };
%      $n:=n-1$; $z:=z*x$ od;
%   print($z$). 
% end
% \end{lstlisting}

% \begin{algorithm}
% \caption{Calculate $y = x^n$}\label{algo1}
% \begin{algorithmic}[1]
% \Require $n \geq 0 \vee x \neq 0$
% \Ensure $y = x^n$ 
% \State $y \Leftarrow 1$
% \If{$n < 0$}\label{algln2}
%         \State $X \Leftarrow 1 / x$
%         \State $N \Leftarrow -n$
% \Else
%         \State $X \Leftarrow x$
%         \State $N \Leftarrow n$
% \EndIf
% \While{$N \neq 0$}
%         \If{$N$ is even}
%             \State $X \Leftarrow X \times X$
%             \State $N \Leftarrow N / 2$
%         \Else[$N$ is odd]
%             \State $y \Leftarrow y \times X$
%             \State $N \Leftarrow N - 1$
%         \EndIf
% \EndWhile
% \end{algorithmic}
% \end{algorithm}

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{minipage}{\hsize}%
% \lstset{frame=single,framexleftmargin=-1pt,framexrightmargin=-17pt,framesep=12pt,linewidth=0.98\textwidth,language=pascal}% Set your language (you can change the language for each code-block optionally)
% %%% Start your code-block
% \begin{lstlisting}
% for i:=maxint to 0 do
% begin
% { do nothing }
% end;
% Write('Case insensitive ');
% Write('Pascal keywords.');
% \end{lstlisting}
% \end{minipage}

% \section{Cross referencing}\label{sec8}

% Environments such as figure, table, equation and align can have a label
% declared via the \verb+\label{#label}+ command. For figures and table
% environments use the \verb+\label{}+ command inside or just
% below the \verb+\caption{}+ command. You can then use the
% \verb+\ref{#label}+ command to cross-reference them. As an example, consider
% the label declared for Figure~\ref{fig1} which is
% \verb+\label{fig1}+. To cross-reference it, use the command 
% \verb+Figure \ref{fig1}+, for which it comes up as
% ``Figure~\ref{fig1}''. 

% To reference line numbers in an algorithm, consider the label declared for the line number 2 of Algorithm~\ref{algo1} is \verb+\label{algln2}+. To cross-reference it, use the command \verb+\ref{algln2}+ for which it comes up as line~\ref{algln2} of Algorithm~\ref{algo1}.

% \subsection{Details on reference citations}\label{subsec7}

% Standard \LaTeX\ permits only numerical citations. To support both numerical and author-year citations this template uses \verb+natbib+ \LaTeX\ package. For style guidance please refer to the template user manual.

% Here is an example for \verb+\cite{...}+: \cite{bib1}. Another example for \verb+\citep{...}+: \citep{bib2}. For author-year citation mode, \verb+\cite{...}+ prints Jones et al. (1990) and \verb+\citep{...}+ prints (Jones et al., 1990).

% All cited bib entries are printed at the end of this article: \cite{bib3}, \cite{bib4}, \cite{bib5}, \cite{bib6}, \cite{bib7}, \cite{bib8}, \cite{bib9}, \cite{bib10}, \cite{bib11}, \cite{bib12} and \cite{bib13}.


% \section{Examples for theorem like environments}\label{sec10}

% For theorem like environments, we require \verb+amsthm+ package. There are three types of predefined theorem styles exists---\verb+thmstyleone+, \verb+thmstyletwo+ and \verb+thmstylethree+ 

% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%
% \bigskip
% \begin{tabular}{|l|p{19pc}|}
% \hline
% \verb+thmstyleone+ & Numbered, theorem head in bold font and theorem text in italic style \\\hline
% \verb+thmstyletwo+ & Numbered, theorem head in roman font and theorem text in italic style \\\hline
% \verb+thmstylethree+ & Numbered, theorem head in bold font and theorem text in roman style \\\hline
% \end{tabular}
% \bigskip
% %%=============================================%%
% %% For presentation purpose, we have included  %%
% %% \bigskip command. Please ignore this.       %%
% %%=============================================%%

% For mathematics journals, theorem styles can be included as shown in the following examples:

% \begin{theorem}[Theorem subhead]\label{thm1}
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. Example theorem text. Example theorem text. Example theorem text. Example theorem text. 
% Example theorem text. 
% \end{theorem}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{proposition}
% Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
% Example proposition text. Example proposition text. Example proposition text. Example proposition text. Example proposition text. 
% \end{proposition}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{example}
% Phasellus adipiscing semper elit. Proin fermentum massa
% ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
% at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
% \end{example}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{remark}
% Phasellus adipiscing semper elit. Proin fermentum massa
% ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend
% at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. 
% \end{remark}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{definition}[Definition sub head]
% Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. Example definition text. 
% \end{definition}

% Additionally a predefined ``proof'' environment is available: \verb+\begin{proof}+ \verb+...+ \verb+\end{proof}+. This prints a ``Proof'' head in italic font style and the ``body text'' in roman font style with an open square at the end of each proof environment. 

% \begin{proof}
% Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
% \end{proof}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text. Sample body text.

% \begin{proof}[Proof of Theorem~{\upshape\ref{thm1}}]
% Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. Example for proof text. 
% \end{proof}

% \noindent
% For a quote environment, use \verb+\begin{quote}...\end{quote}+
% \begin{quote}
% Quoted text example. Aliquam porttitor quam a lacus. Praesent vel arcu ut tortor cursus volutpat. In vitae pede quis diam bibendum placerat. Fusce elementum
% convallis neque. Sed dolor orci, scelerisque ac, dapibus nec, ultricies ut, mi. Duis nec dui quis leo sagittis commodo.
% \end{quote}

% Sample body text. Sample body text. Sample body text. Sample body text. Sample body text (refer Figure~\ref{fig1}). Sample body text. Sample body text. Sample body text (refer Table~\ref{tab3}). 

% \section{Methods}\label{sec11}

% Topical subheadings are allowed. Authors must ensure that their Methods section includes adequate experimental and characterization data necessary for others in the field to reproduce their work. Authors are encouraged to include RIIDs where appropriate. 

% \textbf{Ethical approval declarations} (only required where applicable) Any article reporting experiment/s carried out on (i)~live vertebrate (or higher invertebrates), (ii)~humans or (iii)~human samples must include an unambiguous statement within the methods section that meets the following requirements: 

% \begin{enumerate}[1.]
% \item Approval: a statement which confirms that all experimental protocols were approved by a named institutional and/or licensing committee. Please identify the approving body in the methods section

% \item Accordance: a statement explicitly saying that the methods were carried out in accordance with the relevant guidelines and regulations

% \item Informed consent (for experiments involving humans or human tissue samples): include a statement confirming that informed consent was obtained from all participants and/or their legal guardian/s
% \end{enumerate}

% If your manuscript includes potentially identifying patient/participant information, or if it describes human transplantation research, or if it reports results of a clinical trial then  additional information will be required. Please visit (\url{https://www.nature.com/nature-research/editorial-policies}) for Nature Portfolio journals, (\url{https://www.springer.com/gp/authors-editors/journal-author/journal-author-helpdesk/publishing-ethics/14214}) for Springer Nature journals, or (\url{https://www.biomedcentral.com/getpublished/editorial-policies\#ethics+and+consent}) for BMC.

% \section{Discussion}\label{sec12}

% Discussions should be brief and focused. In some disciplines use of Discussion or `Conclusion' is interchangeable. It is not mandatory to use both. Some journals prefer a section `Results and Discussion' followed by a section `Conclusion'. Please refer to Journal-level guidance for any specific requirements. 

% \section{Conclusion}\label{sec13}

% Conclusions may be used to restate your hypothesis or research question, restate your major findings, explain the relevance and the added value of your work, highlight any limitations of your study, describe future directions for research and recommendations. 

% In some disciplines use of Discussion or 'Conclusion' is interchangeable. It is not mandatory to use both. Please refer to Journal-level guidance for any specific requirements. 

\backmatter

% \bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here. 

% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

% Please refer to Journal-level guidance for any specific requirements.

% \bmhead{Acknowledgements}

% Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

% \section*{Statements and Declarations}

% \subsection*{Funding}

% The work received funding from the Flemish Government (AI Research Program).

% \subsection*{Competing Interests}
% The authors have no relevant financial or non-financial interests to disclose.

% \subsection*{Author Contributions}
% All authors contributed to the study conception and design. Data collection and analysis were performed by Alireza Gharahighehi, Achilleas Ghinis and Michela Venturini. The first draft of the manuscript was written by Alireza Gharahighehi and Achilleas Ghinis. All authors read and approved the final manuscript.
% \subsection*{Code availability}

% The source code is available at \url{https://anonymous.4open.science/r/mocc_cf_sa-85C9}.



\bibliography{sn-article}% common bib file
\begin{appendices}

\section{Hyperparameter tuning}
\label{hyper}
Table~\ref{tab:hyper} provides the final tuned hyperparameters for the collaborative filtering and survival analysis models in our experiments. 
% In this Appendix, details about the hyperparameter tuning procedure, both for first and second steps in Algorithm~\ref{alg:alg}, are reported in Table~\ref{tab:hyper}.

\begin{table*}[hbt!]
\caption{Selected hyperparameters}
\label{tab:hyper}
\scalebox{0.95}{
\begin{tabular}{lllllll}
\toprule
\multicolumn{4}{l}{}                                                    & \multicolumn{3}{c}{datasets} \\ \cmidrule{3-7} 
\multicolumn{2}{l}{}                                & parameter & range & XuetangX  & KDDCUP  & Canvas \\ \midrule
\multirow{15}{*}{CF} & UKNN    &\# neighbors & (20, 800)& 301 & 488 & 128 \\
                                          &         &shrink term& (0,1000)& 178& 907 & 8 \\
 \cmidrule{3-7}
                                          & IKNN    &\# neighbors&(20, 800)&  70   &  37       & 789        \\
                                          &         &shrink term&(0,1000)&   350 &  194       &   793     \\\cmidrule{3-7}
                                          & SVD     &\# latent features&(3, 50)&    5      &   3      &  8      \\\cmidrule{3-7}
                                          & NMF     &\# latent features & (10, 300) &    202       &   43      &     242   \\
                                          &         & L1\_ratio&(0.1,0.9)       &     0.214      &     0.846    &     0.232   \\\cmidrule{3-7}
                                          & WRMF    & epochs & (10,200)      &   217        &    200  &   10     \\
                                          &         & \# latent features &  (10,100)     &   21        &    45     &  43      \\
                                          &         &  regularization         & (1e-5, 1e-1)&   0.008        &  0.097       &0.093         \\\cmidrule{3-7}
                                          & EASE    & l2\_norm & (1e0, 1e7)      & 95109 & 2540 &  9325573      \\\cmidrule{3-7}
                                          & SLIM    & topk & (50, 600)      &   380    &    486    & 321  \\
                                          &         & l1\_norm  & (1e-5,1.0)  &  0.0002     &  0.593     & 0.039   \\
                                          &         & l2\_norm & (1e-3, 1.0) &  0.310  &   0.006      &    0.179 \\ \midrule
\multirow{10}{*}{$SA_{D}$}                       & CoxNet  & alpha          &   (0,1)    &  0.0039         &   0.00941      &    0.335    \\ \cmidrule{3-7}
                                          & RSF     &   n\_estimators         &  (25,100)     & 100          & 100        & 80       \\
                                          &         &  min\_samples\_leaf         &  (10,20)     & 13          & 12        & 18       \\
                                          &         &  min\_samples\_split         &  (10,20)     & 11          & 18        & 20       \\
                                          &         &  max\_depth         &  (2,12)     & 12          & 12       & 9       \\\cmidrule{3-7}
                                          & XGBoost &   Learning Rate       &  (0.1,1)     &     0.2777      &  0.4124       &    0.1247    \\
                                          &         &  n\_estimators         &  (25,200)     & 184          & 121        & 199       \\
                                          &         &  min\_samples\_leaf         &  (5,20)     & 16          & 16        & 17       \\
                                          &         &  min\_samples\_split         &  (5,20)     & 6          & 6        & 16       \\
                                          &         &  max\_depth         &  (2,20)     & 7          & 16       & 12       \\ \midrule
\multirow{10}{*}{$SA_{C}$}                       & CoxNet  & alpha          &   (0,1)    &     0.0050      &   0.0639      &     0.236   \\ \cmidrule{3-7}
                                          & RSF     &   n\_estimators         &  (25,100)     & 78          & 75        & 98       \\
                                          &         &  min\_samples\_leaf         &  (10,20)     & 17          & 13        & 19       \\
                                          &         &  min\_samples\_split         &  (10,20)     & 13          & 17        & 15       \\
                                          &         &  max\_depth         &  (2,12)     & 12          & 6       & 7       \\\cmidrule{3-7}
                                          & XGBoost &   Learning Rate       &  (0.1,1)     &     0.3835      &  0.1495       &    0.1117    \\
                                          &         &  n\_estimators         &  (25,200)     & 173          & 124        & 153       \\
                                          &         &  min\_samples\_leaf         &  (5,20)     & 10          & 8        & 10       \\
                                          &         &  min\_samples\_split         &  (5,20)     & 12          & 6        & 12       \\
                                          &         &  max\_depth         &  (2,20)     & 10          & 12       & 10       \\
                                          \bottomrule
\end{tabular}}
\end{table*}

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

% \bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
