\section{Background and Related Work}
\label{sec:background}

\subsection{Evolution of Long Context in LLMs}
Early language models (e.g., BERT, GPT-2) were constrained by input lengths of a few hundred to a couple thousand tokens. More recent models have dramatically increased their capacity: GPT-3 scaled to 2{,}048 tokens, and subsequent architectures, such as GPT-3.5 Turbo, Claude 2, and Gemini 1.5, now claim support for input contexts beyond 100{,}000 tokens ____. Nevertheless, empirical studies reveal that these long-context capabilities do not necessarily translate to effective \emph{utilization} of the entire input. Despite large or even multimillion-token windows, models often attend to only a fraction of the provided text, leading to performance degradation on reasoning tasks where critical information is scattered far apart ____.

Multiple benchmarks have emerged to evaluate how well LLMs leverage extended context. For instance, ____ introduced \emph{BABILong}, which provides tasks specifically designed to test needle-in-a-haystack retrieval and multi-step reasoning within extensive inputs. Similarly, ____ presented \emph{NeedleBench}, extending context lengths up to one million tokens to assess how models handle single-needle or multi-needle retrieval challenges. Other datasets like \emph{NovelQA} ____, \emph{One Thousand and One Pairs} ____, and \emph{CLongEval} ____ test performance on massive text corpora, including entire novels and Chinese-language benchmarks. Summarization techniques for large texts can be considered complementary or integrated with retrieval ____.

The overarching conclusion from these works is that \emph{merely expanding context size} does not guarantee robust comprehension and multi-fact integration. Key issues include \emph{context confusion} and \emph{context dilution} in the attention mechanism, making it difficult for the model to prioritize and recall crucial facts ____. Consequently, the quest for effective \emph{long-context understanding} has shifted toward techniques such as retrieval-based solutions, chunking, hierarchical modeling, or refined prompt engineering to handle large text inputs ____.

\subsection{Retrieval-Augmented Generation (RAG)}
\label{subsec:rag}
One widely adopted strategy is \emph{Retrieval-Augmented Generation (RAG)}, where a language model is paired with an external retriever that fetches relevant snippets from an indexed corpus ____. Rather than providing the entire document, the retriever selects a small subset of text presumably related to the user's query. While RAG effectively narrows the attention to relevant content, it typically treats retrieval as a one-shot step ____. Multi-hop reasoning tasks requiring multiple disjoint facts can falter if the initial top-$k$ retrieval does not capture all necessary segments ____.

Various iterative or tool-augmented retrieval methods have emerged to address these limitations, interleaving generation with refined retrieval calls ____. However, these multi-round pipelines add complexity and computational overhead. Our approach aims to preserve multi-hop \emph{retrieval} within a single pass of the model (see Section~\ref{sec:proposed}), thus offering a lightweight alternative to full-scale external RAG.

\subsection{Chain-of-Thought Reasoning}
\label{subsec:cot}
\emph{Chain-of-thought (CoT) prompting} has shown that LLMs can significantly improve on complex tasks by producing intermediate reasoning steps ____. However, when crucial facts are missing or buried in large contexts, CoT by itself may lead to incomplete or hallucinated chains of reasoning ____. Consequently, an important line of work explores combining CoT with external retrieval calls ____. Yet such an approach can require multiple model passes or a separate retriever.

In this paper, we incorporate CoT reasoning \emph{within} a single forward pass that also emulates retrieval via prompt engineering. Rather than calling an external index, we ask the model to identify relevant text segments in the prompt itself, then chain those segments into a multi-hop solution.

\subsection{Emulating RAG via Prompt Engineering}
\label{subsec:emulating_rag}
The main idea behind \emph{emulating RAG} is to unify the benefits of retrieval-based focusing and CoT-based multi-step reasoning within a single prompt. Instead of orchestrating separate retrieval calls, we instruct the model to \emph{locate and tag} relevant portions of the input text, then walk through these tagged segments in a CoT style ____.

This approach hinges on carefully structured prompts (Section~\ref{sec:proposed}), where the language model is guided to:
\begin{enumerate}
    \item Identify relevant snippets.
    \item Tag them explicitly (e.g., with \texttt{<relevant\_section>}).
    \item Summarize or analyze those snippets in a chain-of-thought manner.
    \item Produce the final answer, referencing the tagged evidence.
\end{enumerate}
By encapsulating retrieval-like selection and multi-hop reasoning in one pass, we reduce reliance on external retrievers while retaining robust coverage of potentially large input contexts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 3. Proposed Method
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%