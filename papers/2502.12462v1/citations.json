[
  {
    "index": 0,
    "papers": [
      {
        "key": "georgiev2024gemini",
        "author": "Georgiev, Petko and Lei, Ving Ian and et al.",
        "title": "{Gemini 1.5}: Unlocking Multimodal Understanding Across Millions of Tokens of Context"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kuratov2024babilong",
        "author": "Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Igorevich Sorokin and Artyom Sorokin and Mikhail Burtsev",
        "title": "{BABILong}: Testing the Limits of {LLM}s with Long Context Reasoning-in-a-Haystack"
      },
      {
        "key": "zhang2024infinitybench",
        "author": "Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and Sun, Maosong",
        "title": "{$\\infty$Bench}: Extending Long Context Evaluation Beyond 100K Tokens"
      },
      {
        "key": "levy-etal-2024-task",
        "author": "Levy, Mosh and Jacoby, Alon and Goldberg, Yoav",
        "title": "Same Task, More Tokens: The Impact of Input Length on the Reasoning Performance of Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kuratov2024babilong",
        "author": "Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Igorevich Sorokin and Artyom Sorokin and Mikhail Burtsev",
        "title": "{BABILong}: Testing the Limits of {LLM}s with Long Context Reasoning-in-a-Haystack"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2024needlebench",
        "author": "Li, Mo and Zhang, Songyang and et al.",
        "title": "{NeedleBench}: Can {LLMs} Do Retrieval and Reasoning in 1 Million Context Window?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2025novelqa",
        "author": "Cunxiang Wang and Ruoxi Ning and Boqi Pan and Tonghui Wu and Qipeng Guo and Cheng Deng and Guangsheng Bao and Xiangkun Hu and Zheng Zhang and Qian Wang and Yue Zhang",
        "title": "{NovelQA}: Benchmarking Question Answering on Documents Exceeding 200K Tokens"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "karpinska2024one",
        "author": "Karpinska, Marzena and Thai, Katherine and et al.",
        "title": "One Thousand and One Pairs: A \"Novel\" Challenge for Long-Context Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "qiu-etal-2024-clongeval",
        "author": "Qiu, Zexuan and Li, Jingjing and Huang, Shijue and Jiao, Xiaoqi and Zhong, Wanjun and King, Irwin",
        "title": "{CLongEval}: A Chinese Benchmark for Evaluating Long-Context Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "koh2022empirical",
        "author": "Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui",
        "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lee2024can",
        "author": "Lee, Jinhyuk and Chen, Anthony and et al.",
        "title": "Can Long-Context Language Models Subsume Retrieval, {RAG, SQL, and More?}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shaham2022scrolls",
        "author": "Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer",
        "title": "{SCROLLS}: Standardized Comparison Over Long Language Sequences"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "lee2024can",
        "author": "Lee, Jinhyuk and Chen, Anthony and et al.",
        "title": "Can Long-Context Language Models Subsume Retrieval, {RAG, SQL, and More?}"
      },
      {
        "key": "reddy-etal-2024-docfinqa",
        "author": "Reddy, Varshini and Koncel-Kedziorski, Rik and Lai, Viet Dac and Krumdick, Michael and Lovering, Charles and Tanner, Chris",
        "title": "{DocFinQA}: A Long-Context Financial Reasoning Dataset"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bai-etal-2024-longbench",
        "author": "Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and Dong, Yuxiao and Tang, Jie and Li, Juanzi",
        "title": "{LongBench}: A Bilingual, Multitask Benchmark for Long Context Understanding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2024needlebench",
        "author": "Li, Mo and Zhang, Songyang and et al.",
        "title": "{NeedleBench}: Can {LLMs} Do Retrieval and Reasoning in 1 Million Context Window?"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lee2024can",
        "author": "Lee, Jinhyuk and Chen, Anthony and et al.",
        "title": "Can Long-Context Language Models Subsume Retrieval, {RAG, SQL, and More?}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and et al.",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "agarwal2024manyshot",
        "author": "Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Rosias, Luis and Chan, Stephanie CY and Zhang, Biao and Faust, Aleksandra and Larochelle, Hugo",
        "title": "Many-Shot In-Context Learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "shaham2022scrolls",
        "author": "Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and Levy, Omer",
        "title": "{SCROLLS}: Standardized Comparison Over Long Language Sequences"
      },
      {
        "key": "lee2024can",
        "author": "Lee, Jinhyuk and Chen, Anthony and et al.",
        "title": "Can Long-Context Language Models Subsume Retrieval, {RAG, SQL, and More?}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "li2024needlebench",
        "author": "Li, Mo and Zhang, Songyang and et al.",
        "title": "{NeedleBench}: Can {LLMs} Do Retrieval and Reasoning in 1 Million Context Window?"
      }
    ]
  }
]