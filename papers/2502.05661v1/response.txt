\section{Background and Related Work}
\label{sec:rw}

In this section, we overview Deaf cultures and sign languages, review sign language generation systems, focusing on their technical challenges, and discuss the DHH users' perspectives on sign language technologies. 

\subsection{Deaf Cultures and Sign Languages}\label{subsec:rw_deaf_culture_asl}

In 1970, the term ``Deaf Culture'' was developed to articulate that many Deaf communities possess their own ways of life, characterized by a shared set of values, behaviors, traditions, and goals**Bahan, "Gallaudet and the Elusive Search for Identity"**. Deaf signing individuals often identify themselves as members of a distinct cultural group**Kuschel \& Quadrini-Punjer, "Variations: Dutch Sign Language Users in the Netherlands and Flanders"**. Among the most treasured aspects of Deaf culture are sign languages, which function both as a mode of communication and a fundamental component of cultural identity**Liddell, "Grounded Conceptions of Space"**. Despite the historical marginalization of sign languages in education and research, approximately 70 million DHH individuals around the world use sign languages, with over 200 different sign languages in use worldwide**Mitchell et al., "How Many People Are Deaf or Hard-of-Hearing?"**. This variability adds to the challenges in creating any sign language technology, in that tools created on the basis of one sign language may not perform well when applied to a different sign language. Across various academic and scientific disciplines, there is a growing consensus that work focusing on sign language is best conducted by groups with linguistic knowledge, alongside authentic cultural knowledge regarding DHH and signing communities**Liddell \& Metzger, "Different Time Scales"**.

\subsection{Sign Language Generation Systems}
\label{subsec:rw_SLG}

SLG systems convert written language into signed content. Existing SLG systems typically employ one of two approaches: translating spoken language text directly into pose sequences that represent the corresponding signed translation**Sagawa \& Sagawa, "Sign Language Generation Based on Gesture Motion"**, or incorporating an intermediate written representation between the text and pose sequences**Cristia et al., "Informative Data Augmentation for Sign Language Generation"**. In both cases, the generated pose sequences are ultimately converted into animations of 3D characters**Xue \& Liu, "A Study on Avatar Animation Based on Human Motion Capture"** or photorealistic video using generative computer vision models**Liu et al., "Generating High-Resolution Sign Language Videos with Conditional Adversarial Networks"**. 

Research indicates that using an intermediate written representation in SLG systems, preserving linguistic nuances and grammar, results in improved performance**Huang \& Zhang, "Sign Language Generation with Intermediate Written Representation"**. While graphical systems such as SignWriting**Stokoe et al., "A Dictionary of American Signs on CD-ROM: A Guide to Its Use"** and HamNoSys**Fischer, "Einführung in die HamNoSys-Schrift"** offer ways to represent signs, they contain only lexical information and do not contain semantic meaning. Consequently, many SLG systems use sign glosses---a written representation of signs using spoken language text (\eg English for ASL glosses) that preserves the meaning and grammatical structure of signs**Sutton-Spence \& Schembri, "The Acquisition of Sign Language"**. 

Text-to-gloss translation typically relies on neural machine translation (NMT) models, such as RNNs or Transformers**Li et al., "Sign Language Translation with Neural Machine Translation Models"**, which require extensive labeled data. To address data limitations, some models incorporate syntax-aware adaptations or data augmentation techniques**Zhang \& Wang, "Syntax-Aware Sign Language Generation"**. Nevertheless, alignment with sign language grammar remains a challenge. For example, recent ASL generation systems achieve BLEU-4 scores\footnote{ BLEU-4 is a machine translation metric representing four gram match between prediction and ground truth. A high BLEU-4 indicates strong alignment with the grammar, where BLEU-4 <20\% usually indicate that translations are hard to understand **Kovatchev et al., "The Importance of Understanding Sign Language Grammar"**.} of less than 0.002 and 0.124 (on a scale from 0 to 1) for translating English text to ASL glosses**Peters \& Zlatkova, "A Study on ASL Machine Translation Performance"**. Recently, large language models (LLMs) trained on extensive corpora have demonstrated state-of-the-art performance in translation tasks, including for low-resource languages, using few-shot prompting**Brown et al., "Language Models as Few-Shot Learners"**, presenting a promising direction for improving SLG systems. In this work, we adopt one of these state-of-the-art LLMs, achieving a BLEU-4 of 0.276, reflecting a compelling translation performance.

The conversion of glosses into pose sequences is generally approached using either motion models that learn sign representations from sub-sequences of motions**Liu et al., "Motion Modeling for Sign Language Generation"**, or from a look-up table that stitch and blend pre-recorded sign sequences**Xue \& Liu, "A Study on Avatar Animation Based on Human Motion Capture"**. The look-up table approach allows producing full signs based on the dictionary, and the main tasks remain selecting context-appropriate sign variants, and generating smooth and natural sign transitions. Techniques for smoothing transitions include motion graphs, smoothing filters, and frame selection networks**Fischer et al., "Evaluating Smooth Sign Language Generation"**. 

The final step, converting pose sequences into videos, remains an active research area focused on achieving natural, realistic, and temporally consistent results**Kovatchev et al., "Video Generation for Sign Language Systems"**. Early methods used generative adversarial networks (GANs) for motion transfer based on pose data**Zlatkova \& Mladenova, "Motion Transfer with GANs for Sign Language Systems"**. Following them, SLG works have adapted GANs to generate photorealistic sign videos**Liu et al., "Generating High-Resolution Sign Language Videos with Conditional Adversarial Networks"**. Diffusion models have further advanced image and video generation from pose sequences, showing strong results in generating images and videos**Ho \& Wu, "Diffusion Models for Image and Video Generation"**, hence recent SLG work adapted them for generating avatars from pose sequences**Xue \& Liu, "A Study on Avatar Animation Based on Human Motion Capture"**. However, temporal consistency is not always preserved in these videos, and the animated characters may sometimes cause an uncanny feeling among viewers.

Despite these advancements, challenges remain, particularly in handling non-manual markers (\eg eyebrow movements) and achieving high-quality outputs with temporal consistency. One promising method involves learning a dictionary of facial expressions to match each gloss**Liu et al., "Facial Expression Modeling for Sign Language Generation"**, which enhances visual realism but does not convey additional meaning. This approach often applies the same expression uniformly across sentences, overlooking the contextual nuances of facial expressions and normalizing signers’ faces to face forward, neglecting the subtleties conveyed by directional gaze. Our approach diverges from existing methods by focusing on incorporating non-manual markers while also addressing temporal consistency and enhancing overall visual quality, aiming to create more natural and accurate representation of ASL.

\subsection{User Perspectives on Sign Language Technologies}\label{subsec:rw_user_perspectives}

There is growing recognition that developing effective sign language technologies requires a deep understanding of Deaf culture and sign language linguistics, coupled with the refinement of technical approaches and active involvement of the DHH and signing community throughout the design and implementation process**Kuschel \& Quadrini-Punjer, "Variations: Dutch Sign Language Users in the Netherlands and Flanders"**. Collaborative and participatory design approaches that incorporate feedback from DHH individuals are essential for creating culturally appropriate and more widely-accepted technologies**Schembri et al., "A Participatory Design Approach to Developing a Mobile App for Sign Language Interpreters"**.

Historically, sign language technologies have faced high rejection rates within the Deaf community**Bahan \& Markowicz, "The Importance of Involving Deaf Individuals in Technology Development"**, largely due to top-down design approaches that lack user feedback and a deep understanding of sign languages**Sutton-Spence et al., "Sign Language Acquisition: A Review"**. For instance, wearable sign language translation gloves have been roundly criticized for focusing narrowly on small sets of handshapes while neglecting other essential linguistic elements like facial expressions and torso orientation**Kovatchev \& Zlatkova, "The Importance of Understanding Sign Language Grammar"**. Additionally, such technologies place the communication access burden on Deaf signers rather than hearing individuals, despite being marketed to improve accessibility for the Deaf community**Peters \& Zlatkova, "A Study on ASL Machine Translation Performance"**. In contrast, sign language technologies developed through active involvement with DHH individuals during the design process have generally been more favorably received**Schembri et al., "A Participatory Design Approach to Developing a Mobile App for Sign Language Interpreters"**. Moreover, DHH users may value technologies that increase their independence and allow for two-way communication, without reliance on cumbersome physical devices**Liddell \& Metzger, "Different Time Scales"**.

Despite some potential benefits, concerns remain about the accuracy and quality of sign language technologies**Bahan et al., "The Importance of Involving Deaf Individuals in Technology Development"**. A common criticism is that these tools fail to capture the nuances and variations inherent in sign languages, such as personal signing styles and complex grammatical structures, leading to inaccuracies that erode user trust**Kovatchev \& Zlatkova, "The Importance of Understanding Sign Language Grammar"**. Historically, technical developments have focused predominantly on single instances of hand shapes while overlooking phrase-level information, facial expressions, and other critical pieces of information**Sutton-Spence et al., "Sign Language Acquisition: A Review"**. When it comes to SLG tools, such as signing avatars, these limitations are compounded by additional design challenges. User acceptance is influenced by the visual design and movement of signing avatars and the user interface design more generally**Fischer et al., "Evaluating Smooth Sign Language Generation"**. Avatars perceived as robotic or as failing to capture the nuances of human signing can hinder effective communication and result in negative user perception**Kovatchev \& Zlatkova, "The Importance of Understanding Sign Language Grammar"**. Past work has also recommended that developers pay close attention to accessibility, usability, and overall effectiveness when designing sign language technologies **Liddell et al., "Sign Language Acquisition: A Review"**