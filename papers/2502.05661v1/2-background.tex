\section{Background and Related Work}\label{sec:rw}

In this section, we overview Deaf cultures and sign languages, review sign language generation systems, focusing on their technical challenges, and discuss the DHH users' perspectives on sign language technologies. 

\subsection{Deaf Cultures and Sign Languages}\label{subsec:rw_deaf_culture_asl}

In 1970, the term ``Deaf Culture'' was developed to articulate that many Deaf communities possess their own ways of life, characterized by a shared set of values, behaviors, traditions, and goals~\cite{bragg2021fate,ladd2003understanding}. Deaf signing individuals often identify themselves as members of a distinct cultural group~\cite{obasi2008seeing,padden1988deaf}. Among the most treasured aspects of Deaf culture are sign languages, which function both as a mode of communication and a fundamental component of cultural identity~\cite{glickman1993deaf,bragg_sign_2019,bda}. Despite the historical marginalization of sign languages in education and research, approximately 70 million DHH individuals around the world use sign languages, with over 200 different sign languages in use worldwide~\cite{wfd,huenerfauth2009sign,bragg2021fate}. This variability adds to the challenges in creating any sign language technology, in that tools created on the basis of one sign language may not perform well when applied to a different sign language. Across various academic and scientific disciplines, there is a growing consensus that work focusing on sign language is best conducted by groups with linguistic knowledge, alongside authentic cultural knowledge regarding DHH and signing communities~\cite{desai2024systemic,bragg2021fate}.

\subsection{Sign Language Generation Systems}
\label{subsec:rw_SLG}

SLG systems convert written language into signed content. Existing SLG systems typically employ one of two approaches: translating spoken language text directly into pose sequences that represent the corresponding signed translation~\cite{hwang2024universal,hwang2024gloss}, or incorporating an intermediate written representation between the text and pose sequences~\cite{stoll_text2sign_2020, saunders_progressive_2020, moryossef2023open, walsh_sign_2024, xie2024g2p, saunders2020adversarial, arkushin2023ham2pose}. In both cases, the generated pose sequences are ultimately converted into animations of 3D characters~\cite{kim2022sign,kipp2011sign} or photorealistic video using generative computer vision models~\cite{saunders_signing_2022,stoll_text2sign_2020,saunders2020everybody}. 

Research indicates that using an intermediate written representation in SLG systems, preserving linguistic nuances and grammar, results in improved performance~\cite{hwang2024universal, ma2024multi, camgoz_neural_2018}. While graphical systems such as SignWriting~\cite{sutton1974signwriting} and HamNoSys~\cite{hanke2004hamnosys} offer ways to represent signs, they contain only lexical information and do not contain semantic meaning. Consequently, many SLG systems use sign glosses---a written representation of signs using spoken language text (\eg English for ASL glosses) that preserves the meaning and grammatical structure of signs~\cite{liddell2003grammar,desai2024systemic, bragg_sign_2019, muller_considerations_2023}. 

Text-to-gloss translation typically relies on neural machine translation (NMT) models, such as RNNs or Transformers~\cite{stoll_text2sign_2020, stoll2018sign, egea_gomez_syntax-aware_2021, saunders_progressive_2020, walsh_sign_2024, zhu_neural_2023, saunders2022signing}, which require extensive labeled data. To address data limitations, some models incorporate syntax-aware adaptations or data augmentation techniques~\cite{egea_gomez_syntax-aware_2021, zhu_neural_2023}. Nevertheless, alignment with sign language grammar remains a challenge. For example, recent ASL generation systems achieve BLEU-4 scores\footnote{ BLEU-4 is a machine translation metric representing four gram match between prediction and ground truth. A high BLEU-4 indicates strong alignment with the grammar, where BLEU-4 <20\% usually indicate that translations are hard to understand \cite{papineni_bleu_2002}.} of less than 0.002 and 0.124 (on a scale from 0 to 1) for translating English text to ASL glosses~\cite{inan2024generating,zhu_neural_2023}. Recently, large language models (LLMs) trained on extensive corpora have demonstrated state-of-the-art performance in translation tasks, including for low-resource languages, using few-shot prompting~\cite{brown2020language, hendy2023good, peng2023towards}, presenting a promising direction for improving SLG systems. In this work, we adopt one of these state-of-the-art LLMs, achieving a BLEU-4 of 0.276, reflecting a compelling translation performance.

The conversion of glosses into pose sequences is generally approached using either motion models that learn sign representations from sub-sequences of motions~\cite{saunders_progressive_2020, saunders2021mixed, xie2024g2p}, or from a look-up table that stitch and blend pre-recorded sign sequences~\cite{moryossef2023open, stoll_text2sign_2020, stoll2018sign, saunders2022signing, walsh_sign_2024}. The look-up table approach allows producing full signs based on the dictionary, and the main tasks remain selecting context-appropriate sign variants, and generating smooth and natural sign transitions. Techniques for smoothing transitions include motion graphs, smoothing filters, and frame selection networks~\cite{stoll_text2sign_2020, saunders2022signing, moryossef2023open, walsh_sign_2024}. 

The final step, converting pose sequences into videos, remains an active research area focused on achieving natural, realistic, and temporally consistent results~\cite{chan2019everybody, aberman2019deep, liu2019neural, wang2018video, hu2024animate, wang2024disco}. Early methods used generative adversarial networks (GANs) for motion transfer based on pose data~\cite{chan2019everybody, aberman2019deep, liu2019neural, wang2018video}. Following them, SLG works have adapted GANs to generate photorealistic sign videos~\cite{saunders_signing_2022, walsh_sign_2024}. Diffusion models have further advanced image and video generation from pose sequences, showing strong results in generating images and videos~\cite{ramesh2022hierarchical, saharia2022photorealistic, zhang2023adding, huang2023composer, mou2024t2i, hu2024animate, feng2023dreamoving}, hence recent SLG work adapted them for generating avatars from pose sequences~\cite{fang2023signdiff, fang2024signllm}. However, temporal consistency is not always preserved in these videos, and the animated characters may sometimes cause an uncanny feeling among viewers.

Despite these advancements, challenges remain, particularly in handling non-manual markers (\eg eyebrow movements) and achieving high-quality outputs with temporal consistency. One promising method involves learning a dictionary of facial expressions to match each gloss~\cite{walsh_sign_2024}, which enhances visual realism but does not convey additional meaning. This approach often applies the same expression uniformly across sentences, overlooking the contextual nuances of facial expressions and normalizing signersâ€™ faces to face forward, neglecting the subtleties conveyed by directional gaze. Our approach diverges from existing methods by focusing on incorporating non-manual markers while also addressing temporal consistency and enhancing overall visual quality, aiming to create more natural and accurate representation of ASL.

\subsection{User Perspectives on Sign Language Technologies}\label{subsec:rw_user_perspectives}

There is growing recognition that developing effective sign language technologies requires a deep understanding of Deaf culture and sign language linguistics, coupled with the refinement of technical approaches and active involvement of the DHH and signing community throughout the design and implementation process~\cite{prietch_systematic_2022,desai2024systemic,kipp2011assessing}. Collaborative and participatory design approaches that incorporate feedback from DHH individuals are essential for creating culturally appropriate and more widely-accepted technologies~\cite{bragg_sign_2019}.

Historically, sign language technologies have faced high rejection rates within the Deaf community~\cite{Hsu2024unintel,vogel2024factors,gugenheimer2017impact}, largely due to top-down design approaches that lack user feedback and a deep understanding of sign languages~\cite{kipp2011assessing,mohr2017three,zhang2024illuminating,prietch2022systematic}. For instance, wearable sign language translation gloves have been roundly criticized for focusing narrowly on small sets of handshapes while neglecting other essential linguistic elements like facial expressions and torso orientation~\cite{michael2017why}. Additionally, such technologies place the communication access burden on Deaf signers rather than hearing individuals, despite being marketed to improve accessibility for the Deaf community~\cite{visual2019}. In contrast, sign language technologies developed through active involvement with DHH individuals during the design process have generally been more favorably received~\cite{inan2024generating,kipp2011assessing,boudreault2024closed,anindhita2016designing,kipp2011sign}. Moreover, DHH users may value technologies that increase their independence and allow for two-way communication, without reliance on cumbersome physical devices~\cite{michael2017why,hill2020deaf}.

Despite some potential benefits, concerns remain about the accuracy and quality of sign language technologies~\cite{lee2021american,kipp2011sign,huenerfauth2009linguistically,kipp2011assessing,ebling2016building}. A common criticism is that these tools fail to capture the nuances and variations inherent in sign languages, such as personal signing styles and complex grammatical structures, leading to inaccuracies that erode user trust~\cite{kipp2011assessing,lee2021american,kipp2011sign,huenerfauth2009linguistically,ebling2016building}. Historically, technical developments have focused predominantly on single instances of hand shapes while overlooking phrase-level information, facial expressions, and other critical pieces of information~\cite{ebling2016building,huenerfauth2009linguistically,kipp2011assessing}. When it comes to SLG tools, such as signing avatars, these limitations are compounded by additional design challenges. User acceptance is influenced by the visual design and movement of signing avatars and the user interface design more generally. Avatars perceived as robotic or as failing to capture the nuances of human signing can hinder effective communication and result in negative user perception~\cite{kipp2011assessing,tran2023us,huenerfauth2009sign,quandt2022attitudes}. Past work has also recommended reducing the reliance on extensive text-based instructions and offering customizable features, such as for avatar appearance and signing style~\cite{quandt2022attitudes,muir2005perception,tran2023us}. Building on existing literature, this work integrates linguistic and cultural feedback to refine our design choices and improve system performance. 