\subsubsection{Model Selection} We experimented with various versions of GPT and tested multiple configurations to identify the optimal model. As shown in Table \ref{tab:llm_experiment_results}, GPT-4o-2024-05-13 (our adopted model) outperformed other GPT-4 variants under identical settings. Additionally, we fine-tuned two versions of GPT models capable of fine-tuning, but their performance was lower than that of few-shot prompting with the adopted model. However, fine-tuning GPT-4 models with larger datasets could hold promise, and exploring this option when the feature becomes more widely available may yield further improvements.

\subsubsection{Prompting Examples} For Module 1, we varied the prompts for the ``SYSTEM'' in different setups for the English Text-to-ASL Gloss task (depicted on the left side of Figure \ref{fig:module_1}), while maintaining consistency in the ``ASSISTANT'' and ``USER'' prompts. No additional prompt engineering was performed for generating linguistic information (task on the right side of Figure \ref{fig:module_1}). A summary of these setups is provided in Table \ref{tab:prompt_engineering}.
