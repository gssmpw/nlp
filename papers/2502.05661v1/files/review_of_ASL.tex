Similar to other sign languages, ASL is also a visual-based natural language, expressed by using both manual and non-manual markers~\cite{Stokoe1961SignLS}.  A common misconception is that substituting each written English word with a corresponding ASL sign would be enough as a translation~\cite{aslgrammar}. However, this approach does not produce true ASL~\cite{hanson2012computers}, as ASL has its own grammar and lexicon, distinct from English~\cite{lucas2001sociolinguistics,valli2000linguistics}. Moreover, there is no one-to-one mapping between English words and ASL signs, which makes direct substitution less appropriate~\cite{neidle2007signstream}. 

% \subsubsection{ASL linguistics}
% ASL consists of various linguistic aspects, including phonology, morphology, syntax, semantics, and pragmatics, each contributing to the languageâ€™s unique characteristics~\cite{valli2000linguistics}. ASL phonology involves the study of the smallest units, or parameters, such as handshape, movement, location, palm orientation, and non-manual signals like facial expressions~\cite{sandler2006sign}. ASL morphology involves the formation of signs from smaller units, demonstrating both derivational and inflectional processes~\cite{aronoff2005paradox}. The syntax of ASL often follows a topic-comment order rather than the subject-verb-object structure typical of English~\cite{liddell2021american,aslgrammar}, and relies heavily on spatial grammar, where the location and movement of signs correspond to syntactic roles~\cite{neidle2000syntax}. ASL semantics and pragmatics involve the use of signs in context, with meaning often modified through facial expressions, body language, and the specific use of signing space~\cite{wilbur2013phonological}.

\subsubsection{ASL Written Representation}
ASL-LEX~\cite{caselli2017asl} has been used as a gloss reference for annotation of ASL in several works (\eg~\cite{desai2024asl,joze2018ms,ma2018signfi,bragg2021asl}). However, ASL-LEX glosses often lack representation of non-manual markers, such as facial expressions and body movement, which can limit the naturalness and understandability of generated signs when used in SLG~\cite{huenerfauth_evaluation_2008}. To address this, ASL linguists have developed conventions to capture non-manual markers in addition to manual behaviors~\cite{neidle2001signstream,neidle2007signstream,neidle2002signstream}. These include behaviors such as head position and movements, eye gaze and aperture, eyebrow position and movements, and body movements. 
% In this work, we adopt these conventions for our annotations.
% \rotem{only leave this part if we end up using the meta-data}

% \input{fig_tab/asl_datasets}

\subsubsection{ASL Datasets} 
\label{subsubsec:asl_datasets}
Sign language datasets often pose a bottleneck for SLG research~\cite{bragg_sign_2019}. Reviewing ASL datasets reveals substantial variation in vocabulary size, recording duration, number of signers, image resolution, modalities, gloss annotation conventions, and annotation tools~\cite{zahedi2005combination,dreuw2008benchmark,martinez2002purdue,joze2018ms,neidle2012new,neidle2022asl,desai2024asl,signingsavvy,duarte_how2sign_2021,shi2022open,uthus2023youtubeasl} (Table \ref{tab:asl_datasets}). For instance, OpenASL~\cite{shi2022open} and YouTube-ASL~\cite{uthus2023youtubeasl} stand out with their extensive vocabularies of approximately 33,000 and 60,000 signs, respectively, offering a broad lexical base. However, these datasets provide only videos and English captions, without their corresponding written representations. 

RWTH-BOSTON-50~\cite{zahedi2005combination} and Purdue RVL-SLLL~\cite{martinez2002purdue} are among the earliest publicly available ASL datasets. Despite their pioneering role, their relatively small vocabularies, lack of detailed gloss annotations, non-expert human annotators, and variable image quality limit their utility for more advanced ASL research and applications. MS-ASL~\cite{joze2018ms} and ASL Citizen~\cite{desai2024asl} provide word-level isolated ASL signs from a wide range of signers, serving as valuable resources for sign language recognition research. However, for tasks such as generating ASL signs from English sentences, word-level datasets lack crucial contextual information, such as sentence structure, non-manual markers, and signer consistency.

Datasets like NCSLGR~\cite{neidle2012new}, ASLLRP\cite{neidleboston}, and DSP~\cite{neidle2022asl}, resulting from collaborations among multiple universities, as well as the How2Sign~\cite{duarte_how2sign_2021} dataset collected with higher resolution cameras, offer more comprehensive data. These datasets include English sentences with corresponding written representations, detailed annotation conventions (\eg \cite{neidle2001signstream,neidle2007signstream}), and videos featuring both continuous and citation-form signs. These advancements have allowed some of these datasets, such as NCSLGR and How2Sign datasets, to be used as benchmarks for ASL processing research (\eg~\cite{zhu_neural_2023,moryossef_data_2021,baltatzis2024neural}). While these datasets address some of the critical gaps in earlier resources, issues such as their relatively small sizes (\eg \cite{neidle2012new,neidle2022asl}), inconsistent annotation conventions across datasets, and limited accessibility of the DSP and How2Sign gloss datasets make some tasks of ASL processing both promising and challenging. 