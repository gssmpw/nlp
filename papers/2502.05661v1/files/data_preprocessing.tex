\paragraph{Step 1: Data Extraction} We obtained the ASLLRP dataset from the project web interface\footnote{DAI 2: \url{https://dai.cs.rutgers.edu/dai/s/cart}, login required.}. The dataset includes ASL sentence-level signed videos and XML files\footnote{These XML files are generated from the SignStream annotation tool. More details about these files can be found here: \url{https://dai.cs.rutgers.edu/downloads/XML-Export-format.pdf}.} containing corresponding English translations and annotations. For the translation task in Module 1, we focused on extracting manual information from the textual annotations to capture the primary meaning of the English translations. Specifically, we extracted existing English sentences from the XML files and systematically spliced English-based annotations, including vocabulary and compound symbols, fingerspelling, name signs, classifiers, locative words, and gestures, in chronological order. In total, we extracted 2,119 English sentences with corresponding English-based glosses. Additionally, we trimmed the signing videos based on the XML data so that each English sentence corresponds to a specific sign language video (utterance) for our subsequent tasks.

\paragraph{Step 2: Data Cleaning} Following a similar approach to prior work~\cite{amin_sign_2021}, we removed gloss annotations that did not alter the overall meaning of the sentences when omitted, such as repetition (annotated as a single or multiple ``+'' signs), number of signing hands (annotated as ``(1h)'' and ``(2h)''), and signs indicator that both hands move in an alternating manner (annotated as ``alt.''). To reduce translation errors, we standardized all fingerspelling-related glosses from fs-XXX to fs-X-X-X (\eg from ``fs-JOHN'' to ``fs-J-O-H-N'') and unified annotations for spatial locations (\eg ``i:GIVE:j'' and ``i:GIVE:k'' were standardized to ``i:GIVE:j''). While classifiers play a crucial role in ASL, we excluded them from this work because they typically appear only once or very few times in the datasets, so there was insufficient data for effective model prompting. After data cleaning, we retained 1,843 English sentences with corresponding English-based glosses for the remaining experiments. 

\paragraph{Step 3: Text-to-Gloss Dictionary Generation} To improve consistency in sign representations across different sentences and datasets, we constructed a text-to-gloss dictionary using the ASLLRP Sign Bank\footnote{\url{https://dai.cs.rutgers.edu/dai/s/signbank}}, which contains isolated signs along with their corresponding English-based glosses and translations. We then systematically unified the glosses based on step 2 to ensure consistency between the dictionary and the gloss annotations for the sentences. During the dictionary generation, we observed that some words may have variants of glosses depending on the context (\eg ``ask, inquire, query, question'' can be annotated as ``ASK'', ``ASK:i'', or ``i:ASK:j'', depending on whether the previous and following words are signed in a neutral location). Therefore, our dictionary employs a one-to-multiple mapping, accommodating the variability in gloss annotations. In total, the dictionary contains 3,915 text-to-gloss pairs. Notably, we identified 43 words that do not have corresponding glosses (\ie out-of-vocab words). For these words, which lack corresponding videos, fingerspelling is used as an alternative. 

\paragraph{Step 4: Ground True Correction} During the process of extracting ground truth from XML files to determine whether a sentence is a yes/no question, wh- question, conditional statement, and/or contains negation, we discovered that the ground truth labels were based on the signing rather than the English text, leading to some misalignments between the English text and the linguistic labels. For example, ``I guarantee that the parents will be mad if the children dye their hair orange'' was originally labeled as a negation statement, because the signing of it contains negation, although the English sentence does not. To address these issues, four of our researchers iteratively re-labeled and discussed the test set sentence categories, refining the labels to better reflect the text content. These revised labels were then used as the ground truth, allowing us to calculate precision and recall for each sentence type predictions and to identify patterns in the model's errors.