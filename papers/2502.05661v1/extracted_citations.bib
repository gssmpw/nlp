@inproceedings{bragg_sign_2019,
	address = {Pittsburgh PA USA},
	title = {Sign {Language} {Recognition}, {Generation}, and {Translation}: {An} {Interdisciplinary} {Perspective}},
	isbn = {978-1-4503-6676-2},
	shorttitle = {Sign {Language} {Recognition}, {Generation}, and {Translation}},
	url = {https://dl.acm.org/doi/10.1145/3308561.3353774},
	doi = {10.1145/3308561.3353774},
	language = {en},
	urldate = {2024-03-20},
	booktitle = {The 21st {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Bragg, Danielle and Koller, Oscar and Bellard, Mary and Berke, Larwan and Boudreault, Patrick and Braffort, Annelies and Caselli, Naomi and Huenerfauth, Matt and Kacorri, Hernisa and Verhoef, Tessa and Vogler, Christian and Ringel Morris, Meredith},
	month = oct,
	year = {2019},
	keywords = {notion},
	pages = {16--31},
}

@inproceedings{camgoz_neural_2018,
	address = {Salt Lake City, UT},
	title = {Neural {Sign} {Language} {Translation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578910/},
	doi = {10.1109/CVPR.2018.00812},
	abstract = {Sign Language Recognition (SLR) has been an active research ﬁeld for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar.},
	language = {en},
	urldate = {2024-03-28},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Camgoz, Necati Cihan and Hadfield, Simon and Koller, Oscar and Ney, Hermann and Bowden, Richard},
	month = jun,
	year = {2018},
	keywords = {notion},
	pages = {7784--7793},
}

@inproceedings{egea_gomez_syntax-aware_2021,
	address = {Online (Virtual Mode)},
	title = {Syntax-aware {Transformers} for {Neural} {Machine} {Translation}: {The} {Case} of {Text} to {Sign} {Gloss} {Translation}},
	shorttitle = {Syntax-aware {Transformers} for {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2021.bucc-1.4},
	abstract = {It is well-established that the preferred mode of communication of the deaf and hard of hearing (DHH) community are Sign Languages (SLs), but they are considered low resource languages where natural language processing technologies are of concern. In this paper we study the problem of text to SL gloss Machine Translation (MT) using Transformer-based architectures. Despite the significant advances of MT for spoken languages in the recent couple of decades, MT is in its infancy when it comes to SLs. We enrich a Transformer-based architecture aggregating syntactic information extracted from a dependency parser to word-embeddings. We test our model on a well-known dataset showing that the syntax-aware model obtains performance gains in terms of MT evaluation metrics.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 14th {Workshop} on {Building} and {Using} {Comparable} {Corpora} ({BUCC} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Egea Gómez, Santiago and McGill, Euan and Saggion, Horacio},
	editor = {Rapp, Reinhard and Sharoff, Serge and Zweigenbaum, Pierre},
	month = sep,
	year = {2021},
	keywords = {notion},
	pages = {18--27},
}

@inproceedings{muller_considerations_2023,
	address = {Toronto, Canada},
	title = {Considerations for meaningful sign language machine translation based on glosses},
	url = {https://aclanthology.org/2023.acl-short.60},
	doi = {10.18653/v1/2023.acl-short.60},
	abstract = {Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation. To address these issues, we put forward concrete recommendations for future research on gloss translation. Our suggestions advocate awareness of the inherent limitations of gloss-based approaches, realistic datasets, stronger baselines and convincing evaluation.},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Müller, Mathias and Jiang, Zifan and Moryossef, Amit and Rios, Annette and Ebling, Sarah},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {682--693},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	keywords = {notion},
	pages = {311--318},
}

@article{prietch_systematic_2022,
	title = {A {Systematic} {Review} of {User} {Studies} as a {Basis} for the {Design} of {Systems} for {Automatic} {Sign} {Language} {Processing}},
	volume = {15},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/3563395},
	doi = {10.1145/3563395},
	abstract = {Deaf persons, whether or not they are sign language users, make up one of various existing marginalized populations that historically have been socially and politically underrepresented. Unfortunately, this also happens in technology design. Conducting user studies in which marginalized populations are represented is a step towards guaranteeing their right to participate in choices and decisions that are made for, with, and by them. This article presents and discusses results from a Systematic Literature Review (SLR) of user studies in the design of systems for Automatic Sign Language Processing (ASLP). Following our SLR protocol, from 2,486 papers initially found, we applied inclusion and exclusion criteria to finally select 37 papers in our review. We excluded publications that were not full papers, were not related to our main topic of interest, or that reported results that had been updated by more recent papers. All the selected papers focus on user studies as a basis for the design of three major aspects of ASLP: generation (ASLG), recognition (ASLR), and translation (ASLT). With regard to our specific area of interest, we analyzed four areas related to our research questions: goals and research methods, types of user involvement in the interaction design life cycle, cultural and collaborative aspects, and other lessons learned from the primary studies under review. Salient findings from our analysis show that numerical scale questionnaires are the most frequently used research instruments, co-designing ASLP systems with sign language users is not a common practice (as potential users are included mostly in the evaluation phase), and only seldom are Deaf persons who are sign language users included as members of research teams. These findings point to the need of conducting more inclusive and qualitative research for, with and by Deaf persons who are sign language users.},
	language = {en},
	number = {4},
	urldate = {2024-07-22},
	journal = {ACM Transactions on Accessible Computing},
	author = {Prietch, Soraia and Sánchez, J. Alfredo and Guerrero, Josefina},
	month = dec,
	year = {2022},
	keywords = {notion},
	pages = {1--33},
}

@misc{saunders_progressive_2020,
	title = {Progressive {Transformers} for {End}-to-{End} {Sign} {Language} {Production}},
	url = {http://arxiv.org/abs/2004.14874},
	abstract = {The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences. In this paper, we propose Progressive Transformers, a novel architecture that can translate from discrete spoken language sentences to continuous 3D skeleton pose outputs representing sign language. We present two model configurations, an end-to-end network that produces sign direct from text and a stacked network that utilises a gloss intermediary. Our transformer network architecture introduces a counter that enables continuous sequence generation at training and inference. We also provide several data augmentation processes to overcome the problem of drift and improve the performance of SLP models. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future research.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
	month = jul,
	year = {2020},
	note = {arXiv:2004.14874 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@inproceedings{saunders_signing_2022,
	address = {New Orleans, LA, USA},
	title = {Signing at {Scale}: {Learning} to {Co}-{Articulate} {Signs} for {Large}-{Scale} {Photo}-{Realistic} {Sign} {Language} {Production}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	shorttitle = {Signing at {Scale}},
	url = {https://ieeexplore.ieee.org/document/9879134/},
	doi = {10.1109/CVPR52688.2022.00508},
	abstract = {Sign languages are visual languages, with vocabularies as rich as their spoken language counterparts. However, current deep-learning based Sign Language Production (SLP) models produce under-articulated skeleton pose sequences from constrained vocabularies and this limits applicability. To be understandable and accepted by the deaf, an automatic SLP system must be able to generate co-articulated photo-realistic signing sequences for large domains of discourse.},
	language = {en},
	urldate = {2024-07-23},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
	month = jun,
	year = {2022},
	keywords = {notion},
	pages = {5131--5141},
}

@article{stoll_text2sign_2020,
	title = {{Text2Sign}: {Towards} {Sign} {Language} {Production} {Using} {Neural} {Machine} {Translation} and {Generative} {Adversarial} {Networks}},
	volume = {128},
	issn = {1573-1405},
	shorttitle = {{Text2Sign}},
	url = {https://doi.org/10.1007/s11263-019-01281-2},
	doi = {10.1007/s11263-019-01281-2},
	abstract = {We present a novel approach to automatic Sign Language Production using recent developments in Neural Machine Translation (NMT), Generative Adversarial Networks, and motion generation. Our system is capable of producing sign videos from spoken language sentences. Contrary to current approaches that are dependent on heavily annotated data, our approach requires minimal gloss and skeletal level annotations for training. We achieve this by breaking down the task into dedicated sub-processes. We first translate spoken language sentences into sign pose sequences by combining an NMT network with a Motion Graph. The resulting pose information is then used to condition a generative model that produces photo realistic sign language video sequences. This is the first approach to continuous sign video generation that does not use a classical graphical avatar. We evaluate the translation abilities of our approach on the PHOENIX14T Sign Language Translation dataset. We set a baseline for text-to-gloss translation, reporting a BLEU-4 score of 16.34/15.26 on dev/test sets. We further demonstrate the video generation capabilities of our approach for both multi-signer and high-definition settings qualitatively and quantitatively using broadcast quality assessment metrics.},
	language = {en},
	number = {4},
	urldate = {2024-03-29},
	journal = {International Journal of Computer Vision},
	author = {Stoll, Stephanie and Camgoz, Necati Cihan and Hadfield, Simon and Bowden, Richard},
	month = apr,
	year = {2020},
	keywords = {notion},
	pages = {891--908},
}

@misc{walsh_sign_2024,
	title = {Sign {Stitching}: {A} {Novel} {Approach} to {Sign} {Language} {Production}},
	shorttitle = {Sign {Stitching}},
	url = {http://arxiv.org/abs/2405.07663},
	abstract = {Sign Language Production (SLP) is a challenging task, given the limited resources available and the inherent diversity within sign data. As a result, previous works have suffered from the problem of regression to the mean, leading to under-articulated and incomprehensible signing. In this paper, we propose using dictionary examples and a learnt codebook of facial expressions to create expressive sign language sequences. However, simply concatenating signs and adding the face creates robotic and unnatural sequences. To address this we present a 7step approach to effectively stitch sequences together. First, by normalizing each sign into a canonical pose, cropping, and stitching we create a continuous sequence. Then, by applying filtering in the frequency domain and resampling each sign, we create cohesive natural sequences that mimic the prosody found in the original data. We leverage a SignGAN model to map the output to a photo-realistic signer and present a complete Text-to-Sign (T2S) Sign Language Production (SLP) pipeline. Our evaluation demonstrates the effectiveness of the approach, showcasing state-of-the-art performance across all datasets. Finally, a user evaluation shows our approach outperforms the baseline model and is capable of producing realistic sign language sequences.},
	language = {en},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Walsh, Harry and Saunders, Ben and Bowden, Richard},
	month = may,
	year = {2024},
	note = {arXiv:2405.07663 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, notion},
}

@inproceedings{zhu_neural_2023,
	address = {Toronto, Canada},
	title = {Neural {Machine} {Translation} {Methods} for {Translating} {Text} to {Sign} {Language} {Glosses}},
	url = {https://aclanthology.org/2023.acl-long.700},
	doi = {10.18653/v1/2023.acl-long.700},
	abstract = {State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).},
	urldate = {2024-03-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Dele and Czehmann, Vera and Avramidis, Eleftherios},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {12523--12541},
}

