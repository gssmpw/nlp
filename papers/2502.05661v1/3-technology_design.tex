\section{Sign Language Generation Prototype}\label{sec:design} 

In this section we describe our SLG prototype, which generates ASL videos with manual markers---such as hand shape, location, movements, and palm orientation---as well as non-manual markers, including facial expressions and eyebrow movements. 
Our focus is on context-free settings, where each sentence is translated independently. We used a modular approach for our system design (Figure \ref{fig:system_arch}), allowing increased flexibility and interpretability of each module.

The prototype consists of three components: \textbf{Module 1: English Text to ASL Representations}, which leverages a Large Language Model (GPT-4o~\cite{achiam2023gpt}) to translate an English sentence into English-based ASL glosses and to detect linguistic information relevant to non-manual markers; \textbf{Module 2: ASL Representations to Skeletal Pose Sequence}, which takes the LLM outputs and employs a Motion Matching approach to synthesize a skeletal pose sequence; and \textbf{Module 3: Skeletal Pose Sequence to ASL Signed Video}, which generates signed video frames representing a photorealistic ASL signer. 
This modular approach allows for future improvement of the system as the technology advances, by allowing each part to be changed separately. This prototype was iteratively refined within the research team. Insights from these researchers and other collaborators fluent in ASL helped to guide improvements in translation quality, visual and motion quality, and information conveyance.
% Importantly, for future work each of these modules could be swapped out with other methods (\eg a task-specific LLM for Module 1 or a styled avatar representation for Module 3). 

\input{fig_tab/system_arch}

\subsection{Module 1: English Text to ASL Representations}\label{sebsec:module1}

We used an enhanced gloss-based approach that translates an English sentence into an intermediate ASL gloss, including both manual and non-manual information, which is then utilized by subsequent modules. Given the ability of LLMs to naturally absorb and generate grammatical rules, structures, and nuances~\cite{brown2020language,radford2019language}, we used GPT-4o\footnote{Specifically, we used the model gpt-4o-2024-05-13.}~\cite{achiam2023gpt}, a state-of-the-art LLM, to perform two key tasks: (1) translate an English sentence into English-based glosses and (2) detect if the English sentence contains linguistic features associated with specific facial expressions (Module 1 in Figure~\ref{fig:system_arch}). GPT-4o was selected based on our preliminary experiments with various versions of the GPT models. Detailed experimental results are presented in Appendix \ref{appendix:llm_experiments}. 

For the first task, we adopted a prompting-based approach using LLMs with ``in-context learning''~\cite{xu2024misconfidence}, inspired by recent work on low-resource machine translation~\cite{guo2024teaching}, where dataset sizes are too small to train large-scale translation models. This approach allows the model to adapt and perform specific tasks by interpreting examples or instructions directly embedded in the input text, without requiring explicit retraining~\cite{brown2020language}. To improve performance, we added 1,494 in-context examples of English sentence-gloss pairs to our prompt from the ASLLRP dataset (representing 80\% of the dataset). Given the limited window of GPT-4o (\ie 128,000 input tokens), which restrict the number of examples that can be included in a single prompt, we used a ``multi-prompting'' approach. This method involved splitting the examples into multiple batches and iteratively prompting GPT-4o with each batch. In addition, we asked the LLM to constrain its output by generating glosses within the vocabulary established by our text-to-gloss dictionary described below.
% To enhance our translation capabilities, we implemented RAG (Retrieval Augmented Generation) \cite{lewis2020retrieval} with anonymized embeddings. First, as a pre-process, we anonymized all train sentences by converting name references into pronouns. Next, we embed the anonymized sentences using an OpenAI embeddings model. Finally, at inference, for each test sentence, we embed it as well and look for the $N$ most similar examples to this sentence based on the cosine similarity between the embedding of the test example, and the embeddings of the anonymized train examples. This way, the model is presented with the most accurate and relevant examples. As table Table \ref{tab:text-to-gloss_RAG_eval_results} shows, when using RAG the results are better than using all of the train examples. Moreover, using fewer examples and anonymized embeddings yields better results in most cases. The reason for using anonymization, is that names are given high weight in the embedding, which leads to less relevant examples in some cases. For examples, the 3 most similar sentence for the sentence "Which college did Mary go to?" before anonymization, are: "Which college does Mary go to?", "What did Mary's name used to be?", "Mary used to live in Boston.", While after anonymization they are: "Which college does Mary go to?", "Which high school did you go to?", "Where did you go to high school?", which are more relevant and similar examples.
% Given the limited context window of GPT-4o (\ie 128,000 input tokens), which restrict the number of examples that can be included in a single prompt, we used a ``multi-prompting'' approach. This method involved splitting the examples into multiple batches and iteratively prompting GPT-4o with each batch. 

For the second task, we adopted a zero-shot prompting approach, asking the model to predict linguistic features associated with specific facial expressions without any in-context examples. The idea of linguistic predictions was inspired by prior research suggesting that non-manual expressions corresponding to specific grammatical markers, such as raised eyebrows or head tilts, typically involve a consistent set of behaviors that convey meaning within sign language~\cite{neidle2002signstream,baker1983microanalysis}. In this work, we focus primarily on eyebrow movements. To this end, we asked the model to predict whether a given English sentence: is (1) a yes-no question, (2) a wh-question, (3) a conditional statement, and/or (4) contains negation. The outputs from both tasks are then used to generate skeletal poses that are compatible with the subsequent modules, enhancing the integration of non-manual markers. 

This approach addresses two common limitations of gloss-based ASL representations: (1) their tendency to deviate from ASL grammar, and (2) their inability to fully capture the context and expressiveness necessary for conveying the full semantics of a sentiment. 

\paragraph{Dataset and Implementation Details} After reviewing the available ASL datasets (see Appendix ~\ref{appendix:ASL} for more details), we selected the ASLLRP~\cite{neidle_asl_2022} dataset for Module 1. The ASLLRP dataset contains continuous sentence-level ASL videos, isolated ASL videos, ASL glosses, and corresponding English translations. This dataset provides detailed annotations, including textual annotations (\eg English-based glosses for lexical signs, fingerspelling, classifiers, name signs, and gestures), manual markers (\eg number of hands used, alternating hand movements), and non-manual markers (\eg head position and movements, eye gaze, and mouth movements). 

\paragraph{Data Preprocessing} While ASLLRP provides the most comprehensive information required for our task, the data is dispersed across various resources and editions. To make effective use of this dataset, we first consolidated these disparate resources into a unified framework, extracting 2,119 English sentence-gloss pairs along with their corresponding signing videos. The signing videos were then trimmed to isolate specific sign language utterances for our subsequent tasks. To minimize translation errors, we removed gloss annotations that did not alter the overall meaning of the sentence when omitted and standardized all glosses related to fingerspelling. All these changes were done by consulting team members fluent in ASL. We also excluded glosses for classifiers due to their limited sample sizes. After data cleaning, we retained 1,843 English sentence-gloss pairs. Next, we developed a word-gloss dictionary to improve consistency in sign representations of words across different sentences, resulting in 3,915 word-gloss pairs. For the 43 out-of-vocabulary (OOV) words that lacked corresponding videos, we employed fingerspelling as an alternative representation. Finally, four of our researchers conducted a ground truth correction to resolve misalignments between the linguistic labels for the four types of non-manual information and the English text, ensuring the labels more accurately reflected the text content. A more detailed description of our data preprocessing process can be found in Appendix \ref{appendix:data_prep}. The conventions used for re-annotating the glosses in this work are summarized in Table \ref{tab:gloss_convention}.


\input{fig_tab/module_1}

\paragraph{LLM Translation and Classification} 
We used few-shot and zero-shot prompting over GPT-4o \cite{achiam2023gpt} to perform these tasks.
Our prompts were designed to ensure the outputs could be directly used 
% to generate CSV files 
for downstream tasks and systematic evaluation. 
% and further use; 
Figure \ref{fig:module_1} overviews the process and prompts, and shows a usage example. More examples of prompts can be found in Table \ref{tab:prompt_engineering}. For the translation task, we structured the process by first defining the task for the system. Next, we provided the model with context using English word-gloss pair examples for few-shot learning. Finally, we asked the model to translate each English sentence into ASL glosses, while restricting the translation to our word-gloss dictionary as its vocabulary. For the linguistic features task, we also started by defining the task for the system, and then, using zero-shot prompting, asked the model to classify the linguistic features in the English sentence, \ie if it contains a yes/no question, a wh-question, a condition, and/or a negation. Zero-shot was enough in this case, because GPT was extensively trained over English text. The exact prompts used for this process are shown in Figure \ref{fig:module_1}.
% ------------------------------------------------------------

\subsection{Module 2: ASL Representations to Skeletal Pose Sequence}\label{subsubsec:module2} 
The goal of this module is to take the gloss and non-manual LLM outputs and generate a sequence of skeletal poses at video frame rate, which expresses the input English phrase. We based our approach on Motion Matching, a widely used technique in the Computer Graphics community~\cite{buttner2015motion,clavet2016motion,holden2020learned}, which takes a large dictionary of short character animations and an input signal and intelligently blends clips together to form a cohesive video. 
Given the gloss input, a sequence of reference clips is chosen from the dictionary using an optimization function that minimizes the signing concept of ``economy of motion.'' This principle prioritizes the ``best'' sign by minimizing the distance between the body position at the end of the previous sign and the start of the next. The selected clips are then linearly blended together to create a cohesive sequence. 
The non-manual predictions are used as input to an expression blending part of the model which takes the glossed output and augments the facial expressions, in particular targeting eyebrow motion. 
Our signing dictionary derived from ASLLRP contains 12,681 signed pose sequences, with many repetitions of each sign, which are labeled with the 3,915 glosses noted above. 

Motion Matching typically comprises of three components: (1) a definition for how we represent pose sequences and how they are used for generating the pose sequence dictionary, (2) similarity and optimization functions for identifying the ``best'' elements for a sequence, and (3) a blending function to create the resulting pose sequence. 
See Figure \ref{fig:system_arch} (Module 2) for a visual description. The first step chooses and blends the best sign variants. A second step applies expression blending, which augments the pose sequences with non-manual markers to refine facial expressions. 

\paragraph{Skeletal Pose Representation \& Sign Dictionary}
Whole body, face, and hand skeletal keypoints are extracted from all isolated sign videos in ASLLRP using Mediapipe~\cite{lugaresi2019mediapipe}, using 3D information for hands and 2D information for the others.
We preprocessed this data in three ways. First, we imputed keypoints that were missing due to occlusion issues and poor tracking. For missing keypoints at the beginning or end of a sequence, we filled in points with neutral poses where the hands were positioned together just below the viewpoint from the camera. All other missing keypoints were linearly interpolated using valid keypoints from timesteps before and after within that sequence. One exception was with fingerspelling, where we intentionally kept the non-dominant hand in the same neutral position to avoid jumps between letters in a word. Second, we normalized all keypoints in space so that position and scale of the body and head were consistent across sequences. This alleviated differences in camera position between videos and body shapes between signers. For positioning, we relied on the first frame of each sign with the average shoulder position in subsequent frames relative to that first frame. Lastly, we trimmed the start and end of each sign using annotations from the ASLLRP dataset. For fingerspelling, we sped up the clips to account for the discrepancy between the slower performance in the isolated sign video clips and the faster pace typically used in-situ \cite{quinto2010rates}. 

\paragraph{Optimization functions}
There are many different ways to articulate the same sign for emphasis, style, and convenience~\cite{baker1991american,brentari1998prosodic}. For most signs in our dictionary we have multiple examples of each sign. Often these variants convey the same meaning, but are performed by different signers. Sometimes the meaning does vary. For example, ``big'' might have versions that convey a medium-big size and a large-big size or a sign might be shown using newer and antiquated styles. In short of having sufficient linguistic information to differentiate sign variations, we select sign variants based on minimizing movement rather than incorporating other linguistic factors. In the signing community this is sometimes referred to as minimizing the ``economy of motion,'' where an individual may blend together sign variations based on which is physically more efficient.  Mathematically, given a vector of keypoint locations $x_{i,t}^p$ where $i$ is a valid gloss index, $t$ is a frame index within a clip, and $p$ is a body part (body, face, hands), we compared the Euclidian distance using a weighted average of the current gloss $i$ and a candidate subsequent gloss indexed by $j$:
\begin{equation}
	d(i, j) = \sum_{p \in \{body,\ face,\ hands\}} \alpha_{p} \cdot \left\|x_{i,T}^p - x_{j,0}^p\right\|_2^2,
\end{equation}
where $\alpha_p$ is a weighting value for each body part, and $T$ is the final frame in the clip. Values of $alpha$ were chosen to prioritize importance of the body and prevent large changes in posture.

The final sequence of sign videos was determined by minimizing the differences (maximizing the similarity) across all glosses output from the LLM. This was achieved by a greedy algorithm that selected sign videos with the correct gloss labels, prioritizing those where the beginning of the clip was most similar to the end of the previous clip. 

\paragraph{Gloss \& Expression Blending}
We generated a preliminary pose sequence by linearly blending together the start and end of the pose sequence from each chosen gloss instance, using the first and last 20 frames of each clip (at 90 Hz). To increase smooth transitions, we appended half-second neutral pose to the beginning and end of each sequence of videos, which was also interpolated with the gloss videos. We then used the predicted non-manual marker information to augment the facial expressions holistically after stitching the videos together. Specifically, we adjusted the position of the eyebrows throughout the video to reflect whether a sentence was a yes/no question, wh-question, or neither. The output of this module is a sequence with body, face, and hand keypoint poses for a full video. 

% ------------------------------------------------------------
\subsection{Module 3: Skeletal Poses to Video Frames}\label{subsec:module3}
The last module converts the generated pose sequences into a sequence of photorealistic images. 
First, the input 2/3D skeleton poses are rasterized by drawing the skeletal positions onto an image. 
Second, these skeletal images are used as input to an image-to-image neural network, which outputs photorealistic images. We choose to generate videos that resemble ``live'' signers, in an effort to mitigate confounds that could arise in accurately representing signs with more stylized avatars.

 % \todo{I have moved both the description of the baseline and the proposed rasterization function here instead of the experimental section. Another option would be to have only the proposed here and leave the baseline's description in the experimental section.}
The design decisions regarding the rasterization function---the way the skeleton is drawn---play a critical role in the performance of the image-to-image model. In the baseline rasterization function used by previous work~\cite{zhang2023adding, hu2024animate}, each landmark position was represented by a circle on a 2D image with a monochromatic (black) background, with straight lines connecting the hand and torso landmarks. This is consistent with the commonly used drawing functions within the Mediapipe~\cite{lugaresi2019mediapipe} library. In contrast, in our drawing function, instead of scattered, connected circles, each body part (\ie hands, body, face) was represented as a convex polygon, with additional connections drawn between the face and the entire body. Each body surface was drawn with a different shade of gray and each hand uses a different color palette where each finger is a different shade. We use the 3D data from each hand to determine the palm orientation (``in'' versus ``out'') using the surface normal of landmarks surrounding the palm and augment hand colors based on this orientation. Moreover, the background in our dataset varies per person and we find that using a rasterized background color with shades of black-to-red going from top to bottom and black-to-green going from left to right improves the stability of the generated images. The proposed rasterization function significantly improves the image quality and background stability with emphasis to differentiating the hands and individual fingers, disambiguating occlusions originating in overlap between body parts, and differences in the backgrounds of each image.

Although there have been large advances in photorealistic image generation of humans using diffusion models (\eg ControlNet~\cite{zhang2023adding}), results tend to lack temporal consistency and often do not represent hands accurately. Hence, our work builds on image-to-image translation models~\cite{isola2017image, brooks2023instructpix2pix, tumanyan2023plug, hertz2022prompt, zhang2023adding}, while adding modern architectures and loss functions. 
% Specifically, we used a UNet-like model~\cite{ronneberger2015u}, using neural building blocks from Imagen architecture~\cite{saharia2022photorealistic} along with an LPIPS perceptual loss.
Specifically, we used a U-Net architecture ~\cite{ronneberger2015u}, with the encoder and decoder backbones using neural building blocks from the architecture in Imagen~\cite{saharia2022photorealistic}. 
% Imagen is an image generation model that is conditioned on a text description. 
Unlike Imagen, which uses text as an additional input to the system, we condition the decoder using the signers' identity. 
This is especially important because we use data from many different Signer IDs as part of the same model.
This enables the network to output different visual appearances for each Signer ID in the dataset, which is used when training the network and at inference time. 
% \todo{CL: Consider adding a couple more details, such as the specific Imagen block types. }
% Inspired by this, we leverage this architecture to condition the model on the identity of the signer that we want to be depicted in the photo-realistic video. 

The model is trained with a combination of three losses. These are an L1 term between the entire generated output frame and the target input frame, an L1 term only on the hand region, and an LPIPS term~\cite{zhang2018unreasonable}, which is a learned metric that measures perceptual similarity between the output frame and the target input frame. 
% The first loss is an L1 loss between the entire generated output frame $\mathbf{y}$ and the target input frame $\mathbf{x}$:
% \begin{equation}
%     \mathcal{L}_{\text{L1\_whole}} = \frac{1}{N} \sum_{i=1}^{N} \left| y_i - x_i \right|
% \end{equation}
% where $N$ is the total number of pixels in the image.
% The second loss is again an L1 term but this time we leverage a binary hand mask $M_{\text{hands}}$ that identifies the hand regions in the image. This loss term ensures that particular emphasis is given on hand generation. The loss is computed per pixel and the weighted by the hand mask:
% \begin{equation}
%     \mathcal{L}_{\text{L1\_hands}} = \frac{\sum_{i=1}^{N} \left| y_i - x_i \right| \cdot M_{\text{hands}, i}}{\sum_{i=1}^{N} M_{\text{hands}, i}}
% \end{equation}
% Here, the numerator computes the L1 loss over the hand region, and the denominator normalizes by the total number of pixels in the hand mask.
% The third loss is the LPIPS loss~\cite{zhang2018unreasonable}, which is a learned perceptual similarity metric that measures the perceptual similarity between the output frame $\mathbf{y}$ and the target input frame $\mathbf{x}$. The third loss term is denoted as $\mathcal{L}_{\text{perceptual}} = \text{LPIPS}(\mathbf{x}, \mathbf{y})$.
The total loss used to train the model is the sum of the whole frame L1 loss, the hand-specific L1 loss, and the perceptual loss.
% : $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{L1\_whole}} + \mathcal{L}_{\text{L1\_hands}} + \mathcal{L}_{\text{perceptual}}$


\paragraph{Dataset and Implementation Details} 
% \todo{Information about internal dataset (depending on which person we end up showing on Figure 1 as well)} 

Our primary dataset for image generation experiments is How2Sign~\cite{duarte_how2sign_2021}\footnote{There is ambiguity as to which individuals in How2Sign gave permission to use their likeness in publications. Thus, for visualization purposes within this paper and supplemental material, we trained additional models that contain the identity of two other people who have given their permission. Qualitatively, these results are representative of the How2Sign results.}.
Although it doesn't contain glosses, in contrast to ASLLRP, which has varied quality across videos, How2Sign contains 35K high-resolution clips of ASL with a vocabulary size of over 16K word tokens. The high resolution and overall data quality of How2Sign helps the model to learn fine-grained and high-quality visual representations of ASL. 

While the overall image quality is generally high, there are problems with skeleton tracking, especially when there is significant motion blur or there is ambiguity in hand pose. 
Thus, when training the model, we discard lower quality frames
% First, we do not want to create a distribution shift between train and test distributions. 
in efforts to learn more precise mappings between skeletons and photo-realistic humans.
% , without having to make low quality predictions in case some landmarks are missing from a frame of the training set.
We accomplish this by performing automated visual checks in both image and skeletal pose spaces. 
In image space, we use optical flow to detect motion blur by analyzing the flow vectors between two consecutive frames using Farneback's method ~\cite{farneback2003two}. 
In pose space, we check for sudden large changes in landmark positions between consecutive frames, which might indicate inaccuracies due to motion blur. 
% This can \rotem{what do you mean can? are we doing it or not?} also be extended over a short history of frames in order to track the historical stability of landmark positions. 
Specifically, we compared the current landmark positions to the mean landmark positions over a sliding window of predefined size. While signing, hands tend to move more than the body, so the pose conditions are imposed only for each hand instead of including the entire body and face.
% \todo{CL: What is the percent of frames uses? Can we also include the number of signers and some relevant stats based on what (and how much) data we actually ended up using}

% , and considered factors such as motion blur and stability to improve the visual quality of the generated videos. 
% As described in the previous section, the output poses are in the format of a flattened vector of landmark positions. 
% However, these raw landmark positions lack visual context, spatial relationships, and other visual cues inherent in photorealistic frames, making the task of transforming skeletal poses into video frames challenging for machine learning models.

% This transformation converts the task to an Image-to-Image translation, allowing us to leverage a model architecture designed specifically for image processing. 
% While the ultimate goal is to generate a video, the approach used in \textit{ASL Representation to Skeletal Poses} handles the temporal modeling of motion, enabling the approach used in \textit{Skeletal Poses to Video Frames} to focus on generating individual frames only. This separation reduces the complexity of modeling temporal and spatial information simultaneously. 
% \rotem{missing how is temporal consistency preserved after converting to video frames?}

% An additional consideration involves managing missing or inconsistent landmarks. This means that at inference time, this model will be only receiving frames of good quality. 

% Let $I_{t-1}(x,y) $ and $I_t(x,y)$ represent the pixel intensities of two consecutive frames. The optical flow $\mathbf{F}(x,y)$ represents the displacement vector for each pixel from frame $t-1$ to frame $t$, and is defined as:

% \begin{equation}
%     \mathbf{F}(x,y) = \begin{bmatrix} u(x,y) \\ v(x,y) \end{bmatrix}
%     \label{eq:flow}
% \end{equation}
% where:

% \begin{itemize}
%     \item $u(x,y)$ is the displacement in the $x$-direction, 
%     \item $v(x,y)$ is the displacement in the $y$-direction.
% \end{itemize}

% The algorithm minimizes the difference between the frames while also penalizing large variations in the flow field, resulting in:

% \begin{equation}
% \min_{\mathbf{F}} \sum_{x,y} \left[I_t(x+u,y+v) - I_{t-1}(x,y) \right ]^2 + \lambda \sum_{x,y} \|\nabla \mathbf{F}(x,y)\|^2
% \label{eq:farneback}
% \end{equation}
% where $\lambda$ is a regularization parameter controlling the smoothness of the flow field.
% Motion blur is detected by analyzing the optical flow magnitudes. Given the optical flow $\mathbf{F}(x,y)$ for each pixel, the magnitude of the flow is computed as:
% \begin{equation}
%     \|\mathbf{F}(x,y)\| = \sqrt{u(x,y)^2 + v(x,y)^2}
%     \label{eq:flow_magnitude}
% \end{equation}

% The mean magnitude across all pixels is used to determine the presence of motion blur:

% \begin{equation}
%     \text{Mean Flow Magnitude} = \frac{1}{N} \sum_{x,y} \|\mathbf{F}(x,y)\|
% \end{equation}
% where $N$ is the total number of pixels in the frame.
% The condition for detecting motion blur is:
% \begin{equation}
%     \text{Motion Blur} = 
%     \begin{cases}
%         \text{True}, & \text{if } \frac{1}{N} \sum_{x,y} \|\mathbf{F}(x,y)\| > \varepsilon_{blur} \\
%         \text{False}, & \text{otherwise}
%     \end{cases}
% \end{equation}
% where $\varepsilon_{blur}$ is a predefined threshold.
% On landmark space, we check for sudden large changes in landmark positions between consecutive frames, which might indicate inaccuracies due to motion blur. 
% Given the landmarks of the current frame $/mathbf{L}_t$, and the landmarks of the previous frame $\mathbf{L}_{t-1}$, the Euclidean distance between corresponding landmarks is calculated as:

% \begin{equation}
%     d_i = \|\mathbf{L}_t^i - \mathbf{L}_{t-1}^i\|_2
% \end{equation}
% where:
% \begin{itemize}
%     \item $\mathbf{L}_t^i = (x_t^i, y_t^i, z_t^i)$ represents the coordinates of the $i$-th landmark at time $t$
%     \item $d_i$ is the Euclidean distance between the $i$-th landmarks at time $t$ and $t-1$
% \end{itemize}
% The condition for detecting a sudden change is:
% \begin{equation}
%     \text{Sudden Change} = 
%     \begin{cases}
%         \text{True}, & \text{if } \max(d_i) > \varepsilon_{sudden\_change} \\
%         \text{False}, & \text{otherwise}
%     \end{cases}
% \end{equation}
% where $\varepsilon_{sudden\_change}$ is a predefined threshold.
% This can \rotem{what do you mean can? are we doing it or not?} also be extended over a short history of frames in order to track the historical stability of landmark positions. The stability of landmarks over time is assessed by comparing the current landmark positions to the mean landmark positions over a sliding window of predefined size.
% $w$. Let $\mathbf{L}_{t-w}$, $\mathbf{L}_{t-w+1}$, $\mathbf{L}_{t}$ represent the landmarks over the past $w$ frames. The mean landmark position is computed as:
% \begin{equation}
%     \bar{\mathbf{L}} = \frac{1}{w} \sum_{k=t-w+1}^{t} \mathbf{L}_k 
% \end{equation}

% The stability is measured as the mean Euclidean distance between the current landmarks and the mean landmarks:
% \begin{equation}
%     \text{Stability} = \frac{1}{w} \sum_{k=t-w+1}^{t} \|\mathbf{L}_k - \bar{\mathbf{L}}\|_2
% \end{equation}
% The condition for stable landmarks is:
% \begin{equation}
%     \text{Stable Landmarks} = 
%     \begin{cases}
%         \text{True}, & \text{if } \frac{1}{w} \sum_{k=t-w+1}^{t} \|\mathbf{L}_k - \bar{\mathbf{L}}\|_2 < \varepsilon_{stability} \\
%         \text{False}, & \text{otherwise}
%     \end{cases}
% \end{equation}
% where $\varepsilon_{stability}$ is a predefined threshold.
% Given that in ASL hands tend to move more than the body, the conditions on landmark space are imposed explicitly on each of the hands instead of the entirety of landmark positions.


% \rotem{I'm missing some more technical details of how are we analyzing the flow, what is the check for sudden changes in lmks..}


% The How2Sign~\cite{duarte_how2sign_2021} dataset was used for training and generating the final signed videos in Module 3. This approach allowed us to focus on generating ASL signs that include both manual markers---such as hand shape, location, movements, and palm orientation---as well as non-manual markers, including facial expressions and eyebrow movements. 


% \rotem{need to add gradient background details}

