\section{Discussion}\label{sec:discussion}

Our goal was to develop a prototype ASL generation system, addressing key challenges limiting real-world applicability of existing SLG systems, and to explore whether DHH signers would find this technology useful. Below, we reflect on our design process, provide key insights learned, identify areas for improvement, and discuss computational and ethical considerations in the use of our system.

\subsection{Technical Insights from the Design and Evaluation Process}

During the design process and evaluations with DHH participants, we gained valuable technical insights that informed our choices and identified areas for future improvement. One key finding was the importance of careful data handling for translation tasks. Our ablation study results, as shown in Table~\ref{tab:text-to-gloss_eval_results}, highlight the importance of data preprocessing, increasing the number of examples provided to the model, and constraining the translation within the pre-generated vocabulary to improve model translation performance in the low-resource settings. Considerable effort was dedicated to creating an annotation scheme that not only accurately represents ASL signs and sentences but also functions effectively when used with the LLM and the rest of our prototype. This points to a fundamental challenge with glossing: the diverse definitions and interpretations of ASL glosses. Standardization across datasets could mitigate this issue and improve accuracy by allowing the combination of different data resources~\cite{bragg_sign_2019}.

Our use of an LLM for generating both manual and non-manual information demonstrate potential, with the model achieving a BLEU-4 of 0.276 for translating English sentences from the ASLLRP dataset into ASL glosses. While direct comparisons---such as running our dataset on other systems or applying our system to other datasets---are challenging due to the inaccessibility of other datasets and systems, this represents highest reported score for such translation task in the literature, highlighting the effectiveness of few-shot prompting techniques in handling low-resource languages. More than half of the time, DHH participants found the meaning of the generated videos ``Acceptable'', ``Similar'' or ``The Same'' when compared to the English text. However,  in close to 50\% of the examples, they rated our translations as ``Poor'' or ``Very poor'' concerning ASL grammar and style, indicating a need for further improvement in aligning the output with native signing conventions. 

Our additional experiments on English Text-to-ASL gloss translation using Retrieval Augmented Generation (RAG)~\cite{lewis2020retrieval} demonstrated improved performance, achieving a BLEU-4 score of 0.279 $\pm$ 0.003. These results suggest potential for further enhancement in translation accuracy. Detailed descriptions of the experiments are provided in Appedix \ref{appendix:additional_text-to-gloss}. Beyond translation accuracy, our innovation on extracting non-manual markers directly from the English text using zero-shot prompting, could potentially enhance the naturalness and grammatical accuracy of the generated videos. Nonetheless, some linguistic features were misidentified due to inconsistencies between gloss annotations and English sentences (as discussed in Section \ref{subsubsec:linguistic_predictions}), suggesting the need for prompt fine-tuning or more targeted examples. 

The use of a Motion Matching approach for generating skeletal pose sequences offered both promise and challenges. By optimizing for ``economy of motion,'' this method enabled smoother transitions between signs, contributing to more fluid and natural signing overall. However, we encountered issues with fingerspelling, where unintended movements appeared between letters, disrupting the continuity of motion. This challenge was also noted in user feedback, highlighting gaps in achieving the desired naturalness in coarticulations, particularly for complex cases such as fingerspelling. The noticeable naturalness rating difference between the full model and the retargeted approach---where only in 34.5\% of the cases participants perceived the naturalness of our videos as ``Neutral'' or better, compared to 71.1\% for the retargeted version (results shown in Figure~\ref{fig:visual_motion_quality})---emphasizes the need for refining our skeletal motion generation method. 

A key factor limiting the adoption of existing SLG systems by DHH users is the low quality of the generated signing videos, which are often described as robotic or blurry ~\cite{kipp2011assessing,tran2023us,huenerfauth2009sign,quandt2022attitudes}. Our technical evaluations, as detailed in Table~\ref{tab:pose-to-video_eval_results}, demonstrate that our approach improves the visual quality of the generated videos by systematically eliminating data errors, such as missing landmarks, blurriness, and temporal inconsistencies in landmark positioning, through using only the highest-quality frames. However, we still observe a gap between these technical improvements and practical usability, as in 77.8\% of the time DHH participants found the visual quality of our signing videos to be ``Poor'' or ``Very Poor''. Additionally, participants noted that head movements did not consistently align with the camera. 
Future work could explore integrating more advanced generative models such as diffusion models~\cite{croitoru2023diffusion,yang2023diffusion} to enhance video quality.

\subsection{A Need for Larger, High-quality, and Comprehensive ASL Datasets}

Despite using the largest and highest-quality ASL datasets available, the chosen datasets still suffer from several limitations. The ASLLRP dataset is advantageous in that it contains tens of thousands of videos with comprehensive annotations (\eg glosses, English sentences, non-manual markers). However, the dataset suffers from limited visual quality due to issues such as image resolution and motion blur, which proved challenging for generating compelling image-to-image models during our initial experiments. When we turned to the How2Sign dataset for training image-to-image models, we found that the visual quality was significantly better. However, this introduced inconsistencies between datasets. For example, signers in the ASLLRP dataset tend to be seated and looking at prompts away from the camera; while signers in the How2Sign dataset maintain direct eye contact and are looking closer at the camera, slightly sideways. These discrepancies, coupled with the relatively small size of these datasets, highlight the need for more comprehensive and consistent ASL datasets.

Furthermore, the reliance on human-labeled gloss annotations in existing ASL datasets introduces multiple sources of errors and inconsistencies. 
% Our evaluation of predictions of linguistic features from a given English sentence, particularly non-manual from English sentences (Section \ref{subsubsec:linguistic_predictions}), revealed several discrepancies, especially in identifying negation. These inconsistencies appear to result from the annotation process itself; annotations were based on the signing videos in the ASLLRP dataset rather than the corresponding English sentences, leading to misalignment. Moreover, 
While many English sentences in the ASLLRP dataset are derived from context-free ASL utterances translated into glosses and English text, others come from longer narrative videos. In these cases, accurate translation requires full contextual understanding, which the annotations may not always provide~\cite{tanzer2024reconsidering}. Consequently, for many of these context-dependent sentences, our text-to-gloss translations may be more accurate than the original human annotated glosses. This is reflected in our model's performance, where we achieved a BLEU-4 score of 0.305 without these context-dependent sentences (52 in the test set), compared to 0.276 with them. Further supporting this observation, our user study indicated that DHH users rated the quality of our translations, specifically regarding the meaning of video compared to the English text, to be more acceptable than the manually annotated glosses provided by the ASLLRP dataset. These findings highlight the critical need for more robust, high-quality datasets with standardized annotation practices to support the development of effective SLG systems~\cite{bragg_sign_2019}.

\subsection{Addressing the Complexities of ASL in Sign Language Generation Technologies}

The complexities of ASL grammar present challenges for developing effective SLG technologies. While general guidelines for ASL grammar exist, the language, like all natural languages, does not always adhere to rigid grammatical structures in everyday use. This complexity is evident in the mixed results from our experiments, where attempts to provide grammar guidelines to the LLM did not consistently enhance translation performance. Many examples in the ASLLRP dataset, while grammatically correct, diverge from these general guidelines (as shown in Table \ref{tab:text-to-gloss_eval_results}). Feedback from our user study, which highlighted stylistic and grammatical errors, emphasizes the need for a more nuanced computational understanding of how ASL is used in diverse, real-world contexts to improve. Furthermore, regional variations within a single language and differences across multiple sign languages introduce additional layers of complexity that remain to be addressed. 
 
This work studies several aspects of both manual and non-manual markers in ASL morphology, lexicon, and syntax, such as compounds, agreement verbs (directional verbs indicating agreement with the subject and object), fingerspelling, and name signs (more details can be found in Table~\ref{tab:gloss_convention}). However, these linguistic features are analyzed only within the context of the dataset used in this study, which does not capture the full range of their usage in ASL. Additionally, several other facets of ASL grammar and usage remain unexplored. For example, we excluded one type of manual marker, classifiers, due to limited data available to model them accurately. Classifiers, which are essential for conveying nuanced meanings and spatial relationships in ASL, require context-aware data and more sophisticated modeling approaches. As SLG systems evolve towards context-dependent applications, incorporating classifiers will be critical for enhancing the naturalness and expressiveness of the generated signs. Additionally, our work focuses primarily on eyebrow movements, one type of facial expressions within non-manual markers, used to indicate questions, conditional statements, and negation. However, non-manual markers in ASL consist of a wide range of features, including head tilts, mouth shapes, and body posture, which also contribute to the grammar and meaning of signed sentences~\cite{Stokoe1961SignLS,klima1979signs,brentari2002prosody}. Future work is needed to expand the modeling of these additional markers to capture the full complexity of ASL.

Moreover, our study focused on context-free SLG, where each sentence is generated independently. However, sign languages heavily use indexing and spatial referencing, such as referencing people or places mentioned earlier in a conversation~\cite{winston1991spatial,friedman1975space}. Our current prototype system lacks the capacity to remember or track these spatial references over multiple utterances. Additionally, types of signing like storytelling often involve more extensive use of expressions, classifiers, spatial references, and role shifting than our prototype can currently support. Addressing these challenges will require more data, modeling, and interdisciplinary collaboration with ongoing feedback from the DHH and signing communities. 

\subsection{Computational and Ethical Considerations}

While both technical and human evaluations demonstrate the potential of our prototype system, and the modular approach offers flexibility by enabling individual components to be improved or replaced as technologies advance, there are several computational and ethical considerations that should be carefully addressed when using or further improving the system. First, the current prototype requires running GPT-4o inference for every generation instance with longer prompts, which introduces computational and financial costs, as well as scalability challenges, particularly for real-time or large-scale applications. Optimization techniques or lighter models may need to be explored to address this issue. Second, the nature of modular approach can lead to the loss of information between stages, computational inefficiencies, or biases imposed by external constraints at each module. Addressing these shortcomings will require careful integration of modules. Third, the use of LLMs might pose a risk of generating inappropriate or offensive language, which could introduce harm to the DHH community or undermine their trust in using such system. As emphasized in both academia and industry (\eg Apple's Responsible AI white paper~\cite{applewhitepaper}), designing AI tools with care to proactively mitigate potential harms must be a top priority. This includes implementing content filtering mechanisms, rigorous validation processes, and culturally sensitive design practices to ensure that the system outputs are respectful, inclusive, and aligned with community expectations.



% \todo{need to mention somewhere in the text the difficulty of recruiting DHH participants, while also emphasizing the importance of their involvement in evaluating the system for its real-world usability}

% \todo{CL: consider adding paragraph about how this work was developed, interactions with the DHH community, and limitations or suggestions for how others might think about this. }
% Ultimately, while this work was done in collaboration with people in the Deaf community, the majority of authors area hearing and only half of them are fluent or have taken classes in ASL. 
% Continued work should ...
