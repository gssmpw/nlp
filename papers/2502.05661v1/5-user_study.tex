\section{User Evaluation with DHH Signers}\label{sec:user_study}

We conducted a user study with 30 DHH participants to further evaluate our prototype system by assessing the perceived quality of our generated signed videos, with a focus on their ASL grammatical correctness---both with and without non-manual markers---understandability, and naturalness of movement. Additionally, we gathered participants' interest in this technology and its potential use cases. All English sentences were derived from continuous sentence-level signing videos in the ASLLRP dataset. The signed videos presented to participants were either generated from the How2Sign dataset or presented as raw, unprocessed human-signed videos from the ASLLRP dataset.

\subsection{Study Design}\label{subsec:study_design}
The survey was conducted online via a web-based survey tool and consisted of two main sections. Participants provided responses through 5-point rating scales and open-ended feedback, allowing for both quantitative and qualitative insights. To minimize bias that might arise from visual aesthetics influencing translation quality evaluations, we intentionally structured the survey to first evaluate visual and motion quality, followed by translation quality. This design choice was inspired by the aesthetic-usability effect, which indicates that users often perceive visually appealing or high-quality visual designs as more functional or accurate~\cite{tractinsky2000beautiful,hoegg2010good}. We chose 5-point semantic differential scales, a survey rating scale designed to capture respondents' attitudes, approaches, and perspectives~\cite{osgood1964semantic,osgood1957measurement,semantic_diff}, to gauge DHH participants' perceptions of the quality of the generated signed videos. A detailed summary of the user study questions is provided in Appendix \ref{appendix:survey}.

\subsubsection{Section 1: Visual and Motion Quality}\label{subsubsec:sec1} This section evaluated Modules 2 (ASL Representations to Skeletal Pose Sequence) and 3 (Skeletal Poses to Video Frames) of our system, focusing on the motion and visual quality of the generated signed videos. The goal was to explore alignments between technical and human evaluations while providing additional assessment of Module 2, which was not fully evaluated during the technical phase due to the lack of established metrics for this module. To achieve this, we presented two types of models for evaluation. 

The first type, \textit{AI (Annotations)}, uses human-annotated English-based glosses (manual markers) from the ASLLRP dataset, along with our manually annotated linguistic information (non-manual markers), as input for Modules 2 and 3 of our system. This approach assumes a high-quality text translation and focuses on evaluating the performance of our motion and image models. The second type, \textit{Video Retargeting}, takes skeletal poses extracted from ASLLRP sentence videos as input for Module 3, representing a best-case scenario between these two types. This approach assumes high quality text translation and skeletal extraction, focusing solely on assessing the performance of our visual model and identifying potential issues when retargeting data from the ASLLRP dataset to the How2Sign dataset. Notably, all models using our Module 3 were trained exclusively on the How2Sign dataset (detailed in Section \ref{subsec:module3}).

Each participant viewed and rated three videos for each type, where videos were randomly sampled from a larger set of 27 sentences. Participants rated each of the videos on a 5-point rating scale for understandability, visual quality, and naturalness of movement---criteria commonly referenced in the literature~\cite{huenerfauth2007evaluating,quandt2022attitudes}. These evaluations were captured across multiple bipolar dimensions, with scale options such as ``0 (Very Hard), 1 (Hard), 2 (Neutral), 3 (Easy), 4 (Very Easy)'' or ``0 (Very Poor), 1 (Poor), 2 (Neutral), 3 (Good), 4 (Excellent).'' Note that ``N/A'' was provided as a default option, but participants were asked to select another response. After completing each rating scale question, participants had the option to provide open-ended feedback for additional insights. Figure \ref{fig:user_study_sec1} provides an example of the survey interface used for this section as presented to the participants. 

\input{fig_tab/user_study_sec1_example}

\subsubsection{Section 2: Translation Quality}\label{subsubsec:sec2}
This section evaluated Module 1 (English text to ASL Representations) of our system, focusing on the translation quality. We aimed to assess how closely our generated ASL aligns with correct ASL grammar and style, and to examine the impact of non-manual markers, specifically facial expressions, on the overall quality and comprehensibility of the signed output. We achieved this by comparing four types of models.

The first type, \textit{AI (Annotations)}, is identical to the approach described in Section \ref{subsubsec:sec1}. This approach allows us to compare the translation quality of our system with human-annotated ASL representations.  The second type, \textit{AI w/o Expr}, uses our full system (Modules 1-3) but with expression blending model turned off to specifically evaluate our system's effectiveness in generating non-manual markers. The third type, \textit{AI (Full)}, uses our full prototype with the LLM predictions (Modules 1-3), containing both manual and non-manual information. The last type, \textit{Raw Video}, consists of original human-signed videos from the ASLLRP dataset without any processing or modeling. The raw videos serve as the best-performing benchmark, providing a reference point for understanding the gap between our system-generated outputs and natural, human-signed videos.


Each participant viewed 21 videos taken from six sentence types. These included a wh-question, a yes/no question, a question that could be mistaken for a statement without non-manual markers, a simpler statement without non-manual markers, a more complex statement involving negation or conditional, and one random sentence with fingerspelling. Within a survey, videos were randomized so that the same sentence was only used once, with one exception. To analyze the expression blending part of our model, we showed each participant the three ``question'' sentences twice: once using our full system (\textit{AI (Full)}) and once without expression blending (\textit{AI w/o Expr}). 

For each video, participants first provided English translations for the ASL content shown. They were then presented with the ``true'' English translation from the ASLLRP dataset and rated three aspects on a 5-point rating scale: the similarity between the video's meaning and the ``true'' English, the quality of the ASL translation (including grammar and signing style), and the accuracy of the facial expressions. To encourage decisive responses and minimize central tendency bias, we adapted scales from prior work~\cite{zhu_neural_2023}, excluding the neutral option and using choices such as ``0 (Very Poor), 1 (Poor), 2 (Acceptable), 3 (Good), 4 (Excellent).'' After completing these ratings, participants used checkbox options and an open-ended text box to report issues with the translations. They also had the options to provide feedback on translation quality and share their ASL interpretation of the English sentence. To maintain consistency and reliability in the evaluation process, each video in both sections was reviewed by at least three participants. 
% Figure X \todo{add the figure for section 2} provides an example of the survey interface used for this section. 

% \subsubsection{Section 3: Interest in Our System}\label{subsubsec:sec3} This section aimed to gauge participants' interest in AI signing technology based on our system and its potential usability. Participants were asked to rate the likelihood of using AI signers as a supplement to live interpreters in situations where interpreters are unavailable or impractical. They were also asked to share their opinions on potential scenarios and specific use cases for AI signers, as well as their thoughts on the suitability of the current quality of AI signers. Finally, participants were asked to indicate their preferences between ``live'' (photrealistic) human-realistic signers and 3D avatar representations.

\subsubsection{Follow-Up Questions and Demographics}\label{subsubsec:sec4} At the end of the survey, participants were asked about their general interest in AI signing technology and its potential use cases. Additionally, demographic information was collected, including gender, age group, the age at which they began learning ASL, their proficiency in both English and ASL, and the frequency of their communication in ASL and spoken English.

\subsection{Data Collection}\label{subsec:data_collection}

Participants in this study were recruited outside the research group to ensure impartiality and avoid potential biases. To qualify for participation, individuals had to self-identify as DHH, use ASL as their primary language, and be over the age of 18. To ensure participants met these criteria and had the necessary proficiency in ASL, we further implemented a screening process. This process involved prospective participants watching three ASL videos and selecting the corresponding English translations from a set of multiple-choice options. This study went through our organization's internal user study review process.

After the screening and recruitment process, we enrolled a total of 30 DHH signers who met all eligibility criteria. For demographics, 11 participants were aged 20-29, 10 between 30-39, 7 between 40-49, and 2 between 50-59. Twenty-one participants identified as female, and 9 identified as male. Twenty-four participants learned ASL before age 10, while the remaining learned it later. Regarding proficiency, 23 participants rated their ASL comprehension and production as excellent, while the others rated themselves as good. Fifteen rated their English proficiency as excellent, 11 as good, and 3 as acceptable. All except one reported using ASL daily, with one reporting weekly use. The survey took 45-60 minutes for most individuals to complete.

\input{fig_tab/user_study_results1}

\subsection{Data Analysis}

For the rating questions, we report descriptive statistics showing the proportions of each response option for each model type. To account for both fixed and random effects in our data, and to address small sample sizes and deviations from normality in data distributions, we conducted parametric bootstrap linear mixed model (LMM) analyses~\cite{davison1997bootstrap,pinheiro2006mixed}. These models include model type, sentence type, and participants' demographic variables---including gender, age category, ASL age, ASL proficiency, and frequency of ASL use---as fixed effects to assess their influence on the ratings. Participant ID was treated as a random effect to capture individual variability. For visual and motion quality evaluations, we conducted three LMM analyses--one each for understanding, visual quality, and naturalness of motion. Similarly, for translation quality evaluations, we conducted another three LMM analyses to assess the similarity of meaning between the generated videos and the English text, the signing quality (focusing on ASL grammar and style), and the accuracy of facial expressions in matching the English text. For open questions, we summarize participants' feedback to provide insight into their experiences and perceptions.

To further evaluate our system's translation quality, three authors with ASL experience (1 fluent Deaf signer; 1 fluent hearing signer; 1 novice hearing signer) independently rated the participant-provided translations relative to the English annotations from the ASLLRP dataset. This evaluation assessed whether each translation was semantically equivalent to the target phrase. A 5-point scale was used, defined as follows: 4 = the idea is the same (The same); 3 = the idea is evident but contains one error, such as question changed to a statement, one word error, or one missing element (Similar); 2 = the idea is somewhat similar but unclear or contains multiple errors (Acceptable); 1 = some semblance of the idea is present (Poor); 0 = little to no resemblance to the target (Completely different). Pairwise Pearson correlations~\cite{lee1988thirteen} were conducted and showed the high agreement among the ratings of the three evaluators, with Pearson's correlation coefficients ranging from $r = 0.860$ to $0.946$ ($p < .001$). For all LMM analyses with model type as a fixed effect, additional pairwise post-hoc comparisons with Holm corrections~\cite{holm1979simple} were conducted to identify specific factors influencing translation quality.

% Quantitative results from each of the ratings questions across the study can be found in Figure \ref{fig:userstudyresults}. We report descriptive statistics, including proportions, for rating questions and we summarize open-ended responses. 
%For open-ended responses, we performed inductive coding to identify patterns.


\subsection{Findings}\label{subsec:study_findings}

\subsubsection{Visual and Motion Quality Evaluation Findings}
Figure \ref{fig:visual_motion_quality} shows results for Section \ref{subsubsec:sec1}. Regarding the understandability of the generated signed videos from two model types, in the best-case scenario, where raw ASLLRP skeleton data was retargeted using the pose-to-video model from Module 3 (\textit{Video Retargeting)}, participants found 60.0\% of videos to be \textit{easy} or \textit{very easy} to understand, with 73.3\% to be at least \textit{neutral}. Results for \textit{naturalness} were very similar. For visual quality, perceptions were lower, with 32.3\% ratings being at least \textit{good} and 60.1\% with at least \textit{neutral}. When using our full model with human annotations from the ASLLRP dataset combined with linguistic information from our LLM (\textit{AI (Annotations)}), only 21.1\% of ratings indicated the videos were \textit{easy} or \textit{very easy} to understand. Naturalness and visual quality were both rated with lower scores compared to the \textit{Video Retargeting} approach. However, in open-ended responses, some participants commented positively about the body and face movements (\eg \textit{``Good Body Movements and some lip syncing their words (helpful for those who don't understand [the ASL sign])''}). Negative sentiments focused on issues like blurriness, cut off fingers, and the need for improved facial expressions. For example, \textit{``Blurred background is hard to read the signer''} and \textit{``[...] fingers cut off sometimes, needs more movement in the facial.''}


Our parametric bootstrap LMM analyses revealed significant main effects of model type and sentence type on understandability, visual quality, and naturalness of motion, with few demographic variables showing significant effects. For example, in visual quality ratings, model type showed a significant effect, $\chi^2(1)=54.53, p<0.001$, with a bootstrap $p$-value of 0.002, indicating that the retargeted model received significantly higher visual quality ratings than the \textit{AI (Annotations)} model. Sentence type also had a significant impact on visual quality ratings, $\chi^2(4) = 18.59, p < .001$. 
\input{fig_tab/visual_quality_sentence_type}
As illustrated in Figure \ref{fig:visual_quality_sentence_type}, Wh-questions and yes-no questions were rated highest in visual quality, while conditional sentences received the lowest ratings (Holm-corrected post-hoc tests: all $z > 3.7$, all $p$ < .001). Among demographic variables, no significant effects were found for gender ($\chi^2(1) = 0.029$, p = 0.972), age ($\chi^2(3) = 1.94, p = 0.584$), ASL age ($\chi^2(2) = 2.54, p = 0.281$), or ASL proficiency ($\chi^2(2) = 2.95, p = 0.229$). 

\subsubsection{Translation Quality Evaluation Findings} Figure \ref{fig:translation_quality} shows results for Section \ref{subsubsec:sec2}. As expected, the raw videos from the ASLLRP dataset were easiest to understand and had the highest similarity with the English sentences that were shown. Surprisingly, there were a small number of ASLLRP videos that had ``poor'' or ``different'' ratings. One participant noted that one of these raw videos had a \textit{``Lack of grammar and sentence structure but I can understand what he mean[s].''} For the other three models, the differences in translation quality were modest overall---except that the results using our translated glosses (\textit{AI (Full)}) achieved significantly higher ratings than the manually annotated glosses from the ASLLRP dataset (\textit{AI (Annotations)})s in terms of the meaning of the translation. Furthermore, incorporating non-manual markers (\ie facial expressions) in our full model resulted in higher acceptance compared to the same model without non-manual markers (\textit{AI (w/o Expr)}). The \textit{quality} of the translation, which focuses on ASL grammar and style, was rated as at least \textit{acceptable} in 65.3\% of cases with our full model. Similarly, the \textit{meaning} of the translation was at least acceptable 53.8\% of the time and facial quality was at least acceptable 48.5\% of the time.
% Emphasizing the potential benefits of adding facial expressions to SLG systems, the trend among the AI-generated videos was that our full model with added facial expressions is best and the version using the ``true'' ASLLRP glosses does second best.
% , and our translation model without added facial expressions performs notably worse. 

% Additional fixed effects, including sentence type and participant demographic variables, were also tested. Sentence type showed a significant effect, $\chi^2(4)=19.33, p<0.001$, indicating that different sentence types impacted understanding independently of model type. Further examination of the sentence types, as depicted in Figure \ref{fig:per_sentence} shows that Wh-questions were rated the highest, while sentences that included fingerspelling or conditional statements were rated as hardest to understand. No significant effects were observed for gender ($\chi^2$(1)=1.81, p=0.179), age group ($\chi^2$(3)=0.93, p=0.818), ASL age ($\chi^2$(2)=1.73, p=0.421), or ASL proficiency ($\chi^2$(2)=1.96, p=0.375), suggesting these factors did not influence understanding of the models.

Similar to the evaluation of visual and motion quality, our LMM analyses revealed a significant main effect of model type on all three aspects of translation quality (all $p < .001$, with bootstrap $p$-values of 0.002). Contrast analyses showed that the videos generated by the \textit{AI (Annotations)} model were significantly less similar in meaning to the provided English text compared to those produced by our full model (\textit{AI (Full)}; $z = 2.73$, $p < .05$). However, raw videos consistently received higher ratings than all model-generated outputs. For signing quality and the accuracy of facial expressions, contrast analyses indicated a significant difference between raw videos and all model outputs; however, no significant differences were observed among \textit{AI (Annotations)}, \textit{AI (Full)}, and \textit{AI (w/o Expr)}. Sentence type was also identified as a significant factor influencing translation quality. For example, signing quality ratings exhibited a significant main effect of sentence type ($\chi^2(3) = 227.27$, $p < .001$, with a bootstrap $p$-value of 0.002). However, the limited sample size for each sentence type restricted the scope for more detailed analyses. No demographic variables were  significant predictors of signing quality ratings.

\input{fig_tab/translation_quality_ratings}
Our additional LMM analysis, aimed at understanding how well participants' translations aligned with the intended sentences, revealed a strong effect of model type on average translation quality ratings, $\chi^2(3) = 50.45, p < .001$, with a bootstrap $p$-value of 0.002. As shown in Figure \ref{fig:translation_quality_ratings}, the quality of the translations provided by participants showed significant differences between model types, with \textit{AI (Annotations)} performing significantly worse than both \textit{AI (Full)} and \textit{AI (w/o Expr)} ($z = 3.200$, Holm-corrected $p < .01$). Although participants rated translations with non-manual markers as more acceptable, no significant differences were observed between the two models using our system, with and without non-manual markers ($z = 0.907, p = 0.364$). Sentence type also had a significant effect on the translation quality ratings, $\chi^2(7) = 23.34, p < .001$, with a bootstrap $p$-value of 0.002. In contrast, all demographic variables did not significantly influence translation quality ratings. 

% \han{Did we perform a comparison analysis between the raw video and our full model?}

% Three of the authors with ASL experience (1 Deaf; 1 expert; 1 novice) rated the quality of all translations that each participant wrote relative to the English annotations input into our model. The goal was to assess whether each translation was semantically the same as the target phrase. A 5-points scale was used with the following definitions: 4=idea is the same; 3=idea is evident but one element is wrong (\eg question to statement, one word error, one element missing); 2=idea is somewhat similar but unclear/has errors (Acceptable); 1=some semblance of the idea is present (Poor); 0=little to no resemblance to target (Completely different). 
% There was high correlation between each annotator (Pairwise Pearson $r$ between 0.860-0.946, p<.001). Rating averages are shown in  Figure \ref{fig:translation_ratings} where on average the raw videos had a score of 3.634, AI (Full) was 3.170, AI (w/o Expr) was 3.194, and AI (Annotations) was 2.905. The results between each model or raw video are statistical significant, except for between AI (Full) and AI (w/o Expression) which are not significantly different. 

% \input{fig_tab/userstudy_ratings}

% Participants had checkbox options along with an open-ended text box for reporting issues. 
Participants reported several issues with both our full model and our model using ASLLRP human gloss annotations, with the majority of concerns pertained to image and motion quality. However, there were a small number of comments on ``missing information'' and ``wrong signs.'' One participant noted, \textit{``The signing in the beginning looks very laggy, maybe avoid spelling out the words,''} referring to limitations in fingerspelling where individual letters appeared to jump between locations or were signed  slowly compared to natural signing. 

\subsubsection{Interest of AI Signing Technology and Its Use Cases} 
% Follow-up questions asked about participant's general interest in AI Signing technology and primarily consisted of open-ended feedback.
Participants expressed varying levels of interest in AI signing technology. One highly enthusiastic participant remarked, 
\textit{``Everything looks good so far, most of the ASL is correct, definitely on the right path. This would be a great tool and technology for those who struggle with communication in the hearing community. It's super convenient and I can't wait. Thank you for allowing me to be a part of this,''} indicating their inclination to sign up to use such a technology in the future. The least interested person highlighted that the quality of the technology is far from being useful, stating, \textit{``I am not interested seeing AI signing technology because it's too complicated to understand the ASL signer.''} Despite this, the same participant later expressed that the technology could be valuable for certain use cases. 

When asked about their interest in photorealistic, cartoon, or 3D avatars to represent AI signers, participants provided mixed feedback, but with a lean towards photorealistic styles. One participant emphasized the value of realism, stating,
\textit{``Realistic and Authentic[---]it is simpler for viewers to relate to and believe in the content when a live signer offers an honest and realistic experience. It better for training and teaching other people ASL.''} 
Stylized signers could be of interest for social media, advertisements, or children's content, but multiple people noted the importance of ensuring the stylized depiction is capable of conveying nuance of sign language: \textit{``I think more stylized appearance can do, but needs [to be] clear in image and facial expressions.''}
% In contrast, another participant expressed a preference for cartoon avatars over 3D models, noting, \textit{``[...] would prefer a cartoon over a 3D avatar.''} The most detailed comment highlighted that preferences are context-dependent, suggesting that different aesthetics might be more suitable for varying scenarios, such as a doctor's office versus a TV show. \todo{I would add the comment here}

Participants mentioned a wide range of potential use cases for AI signing technology. Many of these examples related to simultaneous recognition and generation of ASL for real-time social interactions. Others focused on one-sided interactions, such as ASL generation of live presentations. 
% , which we categorized into two main groups: social interactions and one-sided interactions. 
% For social interactions, common contexts included: Doctors office (mentioned 5 times); Video or in-person conversations (5x); Customer service (4x); Education (3x); Smart assistant (2x); Drive-thru services (2x); Live presentations (1x). For one-sided interactions, participants reported use cases including: Transportation (5x); Internet “captions” (5x); Public announcements (3x); Emergency info (3x); Museum (2x); Entertainment (1x); Radio (1x); Books (1x); For hearing people to practice/learn (1x).

% Participants suggested a range of potential use cases for AI signing technology, which we categorized into two main groups: social interactions and one-sided interactions. For social interactions, common contexts included: Doctors office (mentioned 5 times); Video or in-person conversations (5x); Customer service (4x); Education (3x); Smart assistant (2x); Drive-thru services (2x); Live presentations (1x). For one-sided interactions, participants reported use cases including: Transportation (5x); Internet “captions” (5x); Public announcements (3x); Emergency info (3x); Museum (2x); Entertainment (1x); Radio (1x); Books (1x); For hearing people to practice/learn (1x).

% It is important to note that this study is preliminary, but feedback on translation quality thus far suggests that our prototype is heading towards a good direction, and with some improvements, participants could find it compelling for situations where a live interpreter is not available. These responses also suggest that much more work is needed to achieve a high-enough motion and visual quality that DHH signers would find valuable. 