\section{Technical Evaluation}\label{sec:technical_eval}

We conducted technical evaluations to assess the performance of our proposed system in translating English text into intermediate ASL representations and generating signed videos. The following sections provide a detailed account of each evaluation, including the experimental procedures and the corresponding evaluation results. Note that a direct quantitative comparison with previous work is challenging due to the use of different datasets~\cite{inan2024generating,zhu_neural_2023,moryossef_data_2021}, output modalities, or gloss-less approaches~\cite{baltatzis2024neural}. For instance, while benchmarks for English text-to-ASL gloss translation often use datasets from other languages, benchmarks specific to ASL gloss translation are lacking. Additionally, for video generation, \cite{baltatzis2024neural} employs the How2Sign dataset and produces SMPL-X 3D human body model poses, whereas our system generates photorealistic videos. These differences in output (3D models vs. photorealistic videos) and their end-to-end design, which precludes comparison of intermediate components, make direct comparisons impractical.

\subsection{English Text to ASL Represenetations}\label{subsec:technical_eval_exp1} 

\subsubsection{English Text-to-ASL Gloss Translation}\label{subsubsec:english_to_gloss}

We conducted ablation studies to determine the optimal model configuration for translating English sentences into English-based glosses (as illustrated on the left side of Module 1 in Figure \ref{fig:system_arch}). Specifically, we examined four key factors: the impact of data preprocessing, the number of in-context examples fed to GPT, the effectiveness of generating glosses within the vocabulary established in our word-to-gloss dictionary, and the necessity of guiding GPT to learn ASL grammar rules\footnote{The ASL grammar rules we provided to GPT-4o can be found in \ref{asl_grammar_rules} in Appendix.}. For the number of English-to-gloss examples, we experimented with 600 (33\% of dataset) and 1,474 (80\% of dataset) sentences from ASLLRP. The dataset was randomly split into a 80/20 ratio to mitigate inconsistencies in distribution. We report BLEU~\cite{papineni_bleu_2002} scores (1 to 4 grams) and ROUGE-L~\cite{lin_rouge_2004} scores, two widely used metrics in the machine translation community~\cite{baltatzis2024neural,saunders_progressive_2020,saunders2020adversarial,fang2024signllm}. Additionally, for a more comprehensive evaluation, we include METEOR~\cite{banerjee_meteor_2005}, CHrF~\cite{popovic_chrf_2015}, TER~\cite{snover_study_2006}, and SacreBLEU~\cite{post2018call}, which are also commonly applied in the literature to assess text-to-gloss translation quality~\cite{egea_gomez_syntax-aware_2021,zhu_neural_2023,forster_extensions_nodate}.
\input{fig_tab/text-to-gloss_eval_results}

As shown in Table~\ref{tab:text-to-gloss_eval_results}, our ablation study results indicate that data preprocessing improves the LLM's performance in translating English text to English-based glosses. Similarly, providing the LLM with more examples, when they are chosen randomly, and limiting the generated glosses to those within the word-to-gloss dictionary results in higher BLEU (1 to 4 grams), ROUGE-L, METEOR, and CHrF scores, along with lower TER scores, all of which suggest enhanced model performance. 

\input{fig_tab/non_manual_results} 

Interestingly, most experiments showed that adding grammar rules did not improve the model’s translation ability, however, there were some exceptions. 
% This may stem from the fact the grammar rules were too strict, while the dataset examples did not always follow these rules. Table~\ref{tab:text-to-gloss_RAG_eval_results_appx} shows the effectiveness of giving more relevant examples to the model using RAG, rather than using all train examples without considering their relevance to the sentence at hand. Moreover, it shows the contribution of using anonymized embeddings, where in most cases it improved the results and allowed using fewer examples to achieve better results.
For example, when data preprocessing was applied and the LLM was provided with 80\% of the entire dataset without limiting the generated glosses to the word-to-gloss vocabulary, we observed mixed results. Specifically, BLEU (1 to 4 grams) scores suggested better model performance without adding grammar rules to the LLM, while other metrics indicated the opposite trend. Furthermore, although direct comparisons are challenging, our system demonstrates compelling translation performance compared to existing results reported in the literature, achieving a BLEU-4 score improvement from 0.191 to 0.276. Table \ref{tab:text-to-gloss_sota} in Appendix \ref{appendix:existing_text-to-gloss_results} summarizes the existing English Text-to-ASL gloss translation results reported in the literature.

\subsubsection{Linguistic Predictions}\label{subsubsec:linguistic_predictions}

Falsely predicting linguistic features for a sentence could result in unnecessary non-manual markers added to the sequential poses and video frames, potentially leading to confusion in the generated ASL videos. To evaluate the performance of GPT-4o in detecting linguistic features regarding the four questions---whether a sentence is a yes/no question, wh-question, conditional statement, and/or contains negation---we calculate precision and recall for each type of prediction. 

Figure \ref{fig:non_manual_results} summarizes the model’s performance in detecting linguistic features within a given English sentence across the four conditions. Overall, the model demonstrates high accuracy across these tasks, particularly in identifying questions and conditional statements. The relatively low precision for negation (precision=0.79) suggests that the model occasionally incorrectly identified negation in sentences where the human-labeled ground truth did not indicate negation presence. We analyzed these cases and discovered that in most, the sentences include negative sentiment, \eg "Why do you hate video games?" or "My sister blamed me but I am innocent!"  


\subsection{Video Generation}
\label{subsec:technical_eval_exp3}

We evaluated our system's performance in generating signed videos (Modules 2 and 3) using quantitative metrics commonly used for human video generation~\cite{wang2024disco}. These metrics evaluate the generations at either image-level (single-level) or video-level. Image-level metrics include L1, PSNR~\cite{hore2010image}, SSIM~\cite{wang2004image}, LPIPS~\cite{zhang2018unreasonable} and FID~\cite{heusel2017gans}, while video-level metrics include FID-VID~\cite{balaji2019conditional} and FVD~\cite{unterthiner2018towards}. Following prior research~\cite{wang2024disco}, we calculated video-level metrics for sequences of 16 consecutive frames. The dataset contains around 60,000 frames from the How2Sign dataset for training and 15,000 for testing, which correspond to about 40 and 10 minutes of video, respectively. To account for variations in appearance such as clothing, we treated the same signer across different recording sessions as distinct signer identities, resulting in a total of 13 Signer IDs.

We performed several ablation studies to evaluate the efficacy of our design choices. The first ablation study focused on the rasterization function, comparing our proposed enhanced rasterization function with the simpler baseline. The second ablation experiment focused on checking frame quality. Specifically, we reported metrics for our Pose-to-Video model under three conditions: (1) ``All frames'', where no frames were excluded from training; (2) ``Valid frames'', where frames with missing landmarks were excluded from the training set, and (3) ``Proposed'', where frames with missing landmarks, blurry frames, and frames that contain landmarks that indicate temporal inconsistencies were excluded, as detailed in the final paragraph of Section \ref{subsec:module3}. 

\input{fig_tab/pose-to-video_eval_results}

Table \ref{tab:pose-to-video_eval_results} presents the evaluation results, demonstrating the proposed approach improves all metrics across the board. The effectiveness of the rasterization function is evident, as the baseline approach produced outputs that resembled a reconstructed skeletal pose rather than a photorealistic human version. The proposed rasterization function provides a better anatomical representation of a given pose, enabling the model to learn a more robust mapping between skeletal poses and photorealistic human images. In terms of frame quality, removing lower quality frames progressively improves the model’s performance, reinforcing the conclusion that data quality is just as important as quantity. 

