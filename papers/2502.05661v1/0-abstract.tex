
Sign languages are essential for the Deaf and Hard-of-Hearing (DHH) community. Sign language generation systems have the potential to support communication by translating from written languages, such as English, into signed videos. However, current systems often fail to meet user needs due to poor translation of grammatical structures, the absence of facial cues and body language, and insufficient visual and motion fidelity. 
We address these challenges by building on recent advances in LLMs and video generation models to translate English sentences into natural-looking AI ASL signers.
The text component of our model extracts information for manual and non-manual components of ASL, which are used to synthesize skeletal pose sequences and corresponding video frames. 
Our findings from a user study with 30 DHH participants and thorough technical evaluations demonstrate significant progress and identify critical areas necessary to meet user needs. 
% Sign languages are essential for the Deaf and Hard-of-Hearing (DHH) community. Sign language generation systems have the potential to support communication by translating from written languages, such as English, into signed videos. However, current systems often fail to meet user needs due to misalignment with grammatical structures, the absence of facial information, and insufficient visual and motion fidelity. To address these challenges we introduce an ASL video generation prototype that aims to produce natural-looking AI signers from English text. Leveraging LLMs and image-to-image models, our system translates English sentences into written representations that contain both the manual and non-manual components of ASL, which are then transformed into skeletal poses using a Motion Matching approach and synthesized into sign video frames. Through technical evaluations and a user study with 30 DHH participants, our findings demonstrate the system's potential and indicate areas for further development necessary to meet user needs.