
@misc{walsh_select_2024,
	title = {Select and {Reorder}: {A} {Novel} {Approach} for {Neural} {Sign} {Language} {Production}},
	shorttitle = {Select and {Reorder}},
	url = {http://arxiv.org/abs/2404.11532},
	abstract = {Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S\&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88\% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.},
	language = {en},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Walsh, Harry and Saunders, Ben and Bowden, Richard},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11532 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{walsh_sign_2024,
	title = {Sign {Stitching}: {A} {Novel} {Approach} to {Sign} {Language} {Production}},
	shorttitle = {Sign {Stitching}},
	url = {http://arxiv.org/abs/2405.07663},
	abstract = {Sign Language Production (SLP) is a challenging task, given the limited resources available and the inherent diversity within sign data. As a result, previous works have suffered from the problem of regression to the mean, leading to under-articulated and incomprehensible signing. In this paper, we propose using dictionary examples and a learnt codebook of facial expressions to create expressive sign language sequences. However, simply concatenating signs and adding the face creates robotic and unnatural sequences. To address this we present a 7step approach to effectively stitch sequences together. First, by normalizing each sign into a canonical pose, cropping, and stitching we create a continuous sequence. Then, by applying filtering in the frequency domain and resampling each sign, we create cohesive natural sequences that mimic the prosody found in the original data. We leverage a SignGAN model to map the output to a photo-realistic signer and present a complete Text-to-Sign (T2S) Sign Language Production (SLP) pipeline. Our evaluation demonstrates the effectiveness of the approach, showcasing state-of-the-art performance across all datasets. Finally, a user evaluation shows our approach outperforms the baseline model and is capable of producing realistic sign language sequences.},
	language = {en},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Walsh, Harry and Saunders, Ben and Bowden, Richard},
	month = may,
	year = {2024},
	note = {arXiv:2405.07663 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, notion},
}

@inproceedings{saunders_signing_2022,
	address = {New Orleans, LA, USA},
	title = {Signing at {Scale}: {Learning} to {Co}-{Articulate} {Signs} for {Large}-{Scale} {Photo}-{Realistic} {Sign} {Language} {Production}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	shorttitle = {Signing at {Scale}},
	url = {https://ieeexplore.ieee.org/document/9879134/},
	doi = {10.1109/CVPR52688.2022.00508},
	abstract = {Sign languages are visual languages, with vocabularies as rich as their spoken language counterparts. However, current deep-learning based Sign Language Production (SLP) models produce under-articulated skeleton pose sequences from constrained vocabularies and this limits applicability. To be understandable and accepted by the deaf, an automatic SLP system must be able to generate co-articulated photo-realistic signing sequences for large domains of discourse.},
	language = {en},
	urldate = {2024-07-23},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
	month = jun,
	year = {2022},
	keywords = {notion},
	pages = {5131--5141},
}

@article{lopez-ludena_methodology_2014,
	title = {Methodology for developing an advanced communications system for the {Deaf} in a new domain},
	volume = {56},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705113003754},
	doi = {10.1016/j.knosys.2013.11.017},
	abstract = {A methodology for developing an advanced communications system for the Deaf in a new domain is presented in this paper. This methodology is a user-centred design approach consisting of four main steps: requirement analysis, parallel corpus generation, technology adaptation to the new domain, and finally, system evaluation. During the requirement analysis, both the user and technical requirements are evaluated and defined. For generating the parallel corpus, it is necessary to collect Spanish sentences in the new domain and translate them into LSE (Lengua de Signos Española: Spanish Sign Language). LSE is represented by glosses and using video recordings. This corpus is used for training the two main modules of the advanced communications system to the new domain: the spoken Spanish into the LSE translation module and the Spanish generation from the LSE module. The main aspects to be generated are the vocabularies for both languages (Spanish words and signs), and the knowledge for translating in both directions. Finally, the field evaluation is carried out with deaf people using the advanced communications system to interact with hearing people in several scenarios. In this evaluation, the paper proposes several objective and subjective measurements for evaluating the performance. In this paper, the new considered domain is about dialogues in a hotel reception. Using this methodology, the system was developed in several months, obtaining very good performance: good translation rates (10\% Sign Error Rate) with small processing times, allowing face-to-face dialogues.},
	urldate = {2024-07-23},
	journal = {Knowledge-Based Systems},
	author = {López-Ludeña, V. and González-Morcillo, C. and López, J. C. and Ferreiro, E. and Ferreiros, J. and San-Segundo, R.},
	month = jan,
	year = {2014},
	keywords = {Advanced communications system for Deaf, Development methodology, Hotel reception for Deaf, Speech into LSE translation, User-centred methodology, notion},
	pages = {240--252},
}

@article{sanchez_r_nodate,
	title = {R. {San}-{Segundo}, {J}.{M}. {Montero}, {R}. {Córdoba}, {V}. {Sama}, {F}. {Fernández}, {L}.{F}. {D}'{Haro}.},
	abstract = {This paper describes the design, development and field evaluation of a Spanish into Spanish Sign Language (LSE: Lengua de Signos Española) translation system. The developed system focuses on helping deaf people when they want to renew their Driver's License. The system is made up of a speech recognizer (for decoding the spoken utterance into a word sequence), a natural language translator (for converting a word sequence into a sequence of signs belonging to the sign language), and a 3D avatar animation module (for playing back the signs). For the natural language translator, three technological proposals have been implemented and evaluated: an example-based strategy, a rule-based translation method and a statistical translator. For the final version, the implemented language translator combines all the alternatives into a hierarchical structure. This paper includes a detailed description of the field evaluation carried out. This evaluation has been carried out in the Local Traffic Office in Toledo involving real government employees, and deaf people from Madrid and Toledo. The evaluation includes objective measurements from the system and subjective information from questionnaires. The paper reports a detailed analysis of the main problems found and a discussion on how to solve them (some of them specific for Spanish Sign Language).},
	language = {en},
	author = {Sánchez, D and Garcia, A},
	keywords = {notion},
}

@article{huenerfauth_evaluation_2008,
	title = {Evaluation of {American} {Sign} {Language} {Generation} by {Native} {ASL} {Signers}},
	volume = {1},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/1361203.1361206},
	doi = {10.1145/1361203.1361206},
	abstract = {There are many important factors in the design of evaluation studies for systems that generate animations of American Sign Language (ASL) sentences, and techniques for evaluating natural language generation of written texts are not easily adapted to ASL. When conducting user-based evaluations, several cultural and linguistic characteristics of members of the American Deaf community must be taken into account so as to ensure the accuracy of evaluations involving these users. This article describes an implementation and user-based evaluation (by native ASL signers) of a prototype ASL natural language generation system that produces sentences containing classifier predicates, which are frequent and complex spatial phenomena that previous ASL generators have not produced. Native signers preferred the system's output to Signed English animations -- scoring it higher in grammaticality, understandability, and naturalness of movement. They were also more successful at a comprehension task after viewing the system's classifier predicate animations.},
	language = {en},
	number = {1},
	urldate = {2024-07-22},
	journal = {ACM Transactions on Accessible Computing},
	author = {Huenerfauth, Matt and Zhao, Liming and Gu, Erdan and Allbeck, Jan},
	month = may,
	year = {2008},
	keywords = {notion},
	pages = {1--27},
}

@article{prietch_systematic_2022,
	title = {A {Systematic} {Review} of {User} {Studies} as a {Basis} for the {Design} of {Systems} for {Automatic} {Sign} {Language} {Processing}},
	volume = {15},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/3563395},
	doi = {10.1145/3563395},
	abstract = {Deaf persons, whether or not they are sign language users, make up one of various existing marginalized populations that historically have been socially and politically underrepresented. Unfortunately, this also happens in technology design. Conducting user studies in which marginalized populations are represented is a step towards guaranteeing their right to participate in choices and decisions that are made for, with, and by them. This article presents and discusses results from a Systematic Literature Review (SLR) of user studies in the design of systems for Automatic Sign Language Processing (ASLP). Following our SLR protocol, from 2,486 papers initially found, we applied inclusion and exclusion criteria to finally select 37 papers in our review. We excluded publications that were not full papers, were not related to our main topic of interest, or that reported results that had been updated by more recent papers. All the selected papers focus on user studies as a basis for the design of three major aspects of ASLP: generation (ASLG), recognition (ASLR), and translation (ASLT). With regard to our specific area of interest, we analyzed four areas related to our research questions: goals and research methods, types of user involvement in the interaction design life cycle, cultural and collaborative aspects, and other lessons learned from the primary studies under review. Salient findings from our analysis show that numerical scale questionnaires are the most frequently used research instruments, co-designing ASLP systems with sign language users is not a common practice (as potential users are included mostly in the evaluation phase), and only seldom are Deaf persons who are sign language users included as members of research teams. These findings point to the need of conducting more inclusive and qualitative research for, with and by Deaf persons who are sign language users.},
	language = {en},
	number = {4},
	urldate = {2024-07-22},
	journal = {ACM Transactions on Accessible Computing},
	author = {Prietch, Soraia and Sánchez, J. Alfredo and Guerrero, Josefina},
	month = dec,
	year = {2022},
	keywords = {notion},
	pages = {1--33},
}

@misc{tanzer_reconsidering_2024,
	title = {Reconsidering {Sentence}-{Level} {Sign} {Language} {Translation}},
	url = {http://arxiv.org/abs/2406.11049},
	abstract = {Historically, sign language machine translation has been posed as a sentence-level task: datasets consisting of continuous narratives are chopped up and presented to the model as isolated clips. In this work, we explore the limitations of this task framing. First, we survey a number of linguistic phenomena in sign languages that depend on discourse-level context. Then as a case study, we perform the first human baseline for sign language translation that actually substitutes a human into the machine learning task framing, rather than provide the human with the entire document as context. This human baseline—for ASL to English translation on the How2Sign dataset—shows that for 33\% of sentences in our sample, our fluent Deaf signer annotators were only able to understand key parts of the clip in light of additional discourse-level context. These results underscore the importance of understanding and sanity checking examples when adapting machine learning to new domains.},
	language = {en},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Tanzer, Garrett and Shengelia, Maximus and Harrenstien, Ken and Uthus, David},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11049 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{saunders_progressive_2020,
	title = {Progressive {Transformers} for {End}-to-{End} {Sign} {Language} {Production}},
	url = {http://arxiv.org/abs/2004.14874},
	abstract = {The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences. In this paper, we propose Progressive Transformers, a novel architecture that can translate from discrete spoken language sentences to continuous 3D skeleton pose outputs representing sign language. We present two model configurations, an end-to-end network that produces sign direct from text and a stacked network that utilises a gloss intermediary. Our transformer network architecture introduces a counter that enables continuous sequence generation at training and inference. We also provide several data augmentation processes to overcome the problem of drift and improve the performance of SLP models. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future research.},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
	month = jul,
	year = {2020},
	note = {arXiv:2004.14874 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@inproceedings{kim_signbleu_2024,
	address = {Torino, Italia},
	title = {{SignBLEU}: {Automatic} {Evaluation} of {Multi}-channel {Sign} {Language} {Translation}},
	shorttitle = {{SignBLEU}},
	url = {https://aclanthology.org/2024.lrec-main.1289},
	abstract = {Sign languages are multi-channel languages that communicate information through not just the hands (manual signals) but also facial expressions and upper body movements (non-manual signals). However, since automatic sign language translation is usually performed by generating a single sequence of glosses, researchers eschew non-manual and co-occurring manual signals in favor of a simplified list of manual glosses. This can lead to significant information loss and ambiguity. In this paper, we introduce a new task named multi-channel sign language translation (MCSLT) and present a novel metric, SignBLEU, designed to capture multiple signal channels. We validated SignBLEU on a system-level task using three sign language corpora with varied linguistic structures and transcription methodologies and examined its correlation with human judgment through two segment-level tasks. We found that SignBLEU consistently correlates better with human judgment than competing metrics. To facilitate further MCSLT research, we report benchmark scores for the three sign language corpora and release the source code for SignBLEU at https://github.com/eq4all-projects/SignBLEU.},
	urldate = {2024-06-05},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Kim, Jung-Ho and Huerta-Enochian, Mathew John and Ko, Changyong and Lee, Du Hui},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	keywords = {notion},
	pages = {14796--14811},
}

@inproceedings{cao_attention_2022,
	address = {Seattle, United States},
	title = {Attention {Fusion}: a light yet efficient late fusion mechanism for task adaptation in {NLU}},
	shorttitle = {Attention {Fusion}},
	url = {https://aclanthology.org/2022.findings-naacl.64},
	doi = {10.18653/v1/2022.findings-naacl.64},
	abstract = {Fine-tuning a pre-trained language model using annotated data has become the de-facto standard for adapting general-purpose pre-trained models like BERT to downstream tasks. However, given the trend of larger pre-trained models, fine-tuning these models for each downstream task is parameter-inefficient and computationally-expensive deeming this approach sub-optimal for adoption by NLU systems. In recent years, various approaches have been proposed for parameter efficient task adaptation such as Adaptor, Bitfit, Prompt tuning, Prefix tuning etc. However, most of these efforts propose to insert task specific parameters in-between or inside intermediate layers of the pre-trained encoder resulting in higher computational cost due to back-propagation of errors to all layers. To mitigate this issue, we propose a light but efficient, attention based fusion module which computes task-attuned token representations by aggregating intermediate layer representations from a pre-trained network. Our proposed fusion module trains only 0.0009\% of total parameters and achieves competitive performance to the standard fine-tuning approach on various tasks. It is also decoupled from the pre-trained network making it efficient during computation and scalable during deployment. Last but not the least, we demonstrate that our proposed attention-fusion mechanism can transfer effectively to different languages for further re-use and expansion.},
	urldate = {2024-06-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Jin and Satya Prakash, Chandana and Hamza, Wael},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	keywords = {notion},
	pages = {857--866},
}

@misc{edalati_krona_2022,
	title = {{KronA}: {Parameter} {Efficient} {Tuning} with {Kronecker} {Adapter}},
	shorttitle = {{KronA}},
	url = {http://arxiv.org/abs/2212.10650},
	abstract = {Fine-tuning a Pre-trained Language Model (PLM) on a speciﬁc downstream task has been a well-known paradigm in Natural Language Processing. However, with the ever-growing size of PLMs, training the entire model on several downstream tasks becomes very expensive and resource-hungry. Recently, different Parameter Efﬁcient Tuning (PET) techniques are proposed to improve the efﬁciency of ﬁnetuning PLMs. One popular category of PET methods is the low-rank adaptation methods which insert learnable truncated SVD modules into the original model either sequentially or in parallel. However, low-rank decomposition suffers from limited representation power. In this work, we address this problem using the Kronecker product instead of the low-rank representation. We introduce KronA, a Kronecker product-based adapter module for efﬁcient ﬁne-tuning of Transformer-based PLMs. We apply the proposed methods for ﬁne-tuning T5 on the GLUE benchmark to show that incorporating the Kronecker-based modules can outperform state-of-the-art PET methods.},
	language = {en},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Edalati, Ali and Tahaei, Marzieh and Kobyzev, Ivan and Nia, Vahid Partovi and Clark, James J. and Rezagholizadeh, Mehdi},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10650 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@inproceedings{reza_abscribe_2024,
	address = {Honolulu HI USA},
	title = {{ABScribe}: {Rapid} {Exploration} \& {Organization} of {Multiple} {Writing} {Variations} in {Human}-{AI} {Co}-{Writing} {Tasks} using {Large} {Language} {Models}},
	isbn = {9798400703300},
	shorttitle = {{ABScribe}},
	url = {https://dl.acm.org/doi/10.1145/3613904.3641899},
	doi = {10.1145/3613904.3641899},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Reza, Mohi and Laundry, Nathan M and Musabirov, Ilya and Dushniku, Peter and Yu, Zhi Yuan “Michael” and Mittal, Kashish and Grossman, Tovi and Liut, Michael and Kuzminykh, Anastasia and Williams, Joseph Jay},
	month = may,
	year = {2024},
	keywords = {notion},
	pages = {1--18},
}

@inproceedings{hohman_talaria_2024,
	title = {Talaria: {Interactively} {Optimizing} {Machine} {Learning} {Models} for {Efficient} {Inference}},
	shorttitle = {Talaria},
	url = {http://arxiv.org/abs/2404.03085},
	doi = {10.1145/3613904.3642628},
	abstract = {On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Hohman, Fred and Wang, Chaoqun and Lee, Jinmook and Görtler, Jochen and Moritz, Dominik and Bigham, Jeffrey P. and Ren, Zhile and Foret, Cecile and Shan, Qi and Zhang, Xiaoyi},
	month = may,
	year = {2024},
	note = {arXiv:2404.03085 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, notion},
	pages = {1--19},
}

@inproceedings{hohman_model_2024,
	title = {Model {Compression} in {Practice}: {Lessons} {Learned} from {Practitioners} {Creating} {On}-device {Machine} {Learning} {Experiences}},
	shorttitle = {Model {Compression} in {Practice}},
	url = {http://arxiv.org/abs/2310.04621},
	doi = {10.1145/3613904.3642109},
	abstract = {On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today’s large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Hohman, Fred and Kery, Mary Beth and Ren, Donghao and Moritz, Dominik},
	month = may,
	year = {2024},
	note = {arXiv:2310.04621 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, notion},
	pages = {1--18},
}

@inproceedings{taeb_axnav_2024,
	title = {{AXNav}: {Replaying} {Accessibility} {Tests} from {Natural} {Language}},
	shorttitle = {{AXNav}},
	url = {http://arxiv.org/abs/2310.02424},
	doi = {10.1145/3613904.3642777},
	abstract = {Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag accessibility issues (e.g., Text size not increasing with Large Text enabled, VoiceOver navigation loops). We evaluate this system through a 10 participant user study with accessibility QA professionals who indicated that the tool would be very useful in their current work and performed tests similarly to how they would manually test the features. The study also reveals insights for future work on using LLMs for accessibility testing.},
	language = {en},
	urldate = {2024-06-04},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	author = {Taeb, Maryam and Swearngin, Amanda and Schoop, Eldon and Cheng, Ruijia and Jiang, Yue and Nichols, Jeffrey},
	month = may,
	year = {2024},
	note = {arXiv:2310.02424 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, I.2, notion},
	pages = {1--16},
}

@article{supalla_why_2017,
	title = {Why {American} {Sign} {Language} {Gloss} {Must} {Matter}},
	volume = {161},
	issn = {0002-726X},
	url = {https://www.jstor.org/stable/26235305},
	abstract = {RESPONDING TO AN ARTICLE by Grushkin on how deaf children best learn to read, published, along with the present article, in an American Annals of the Deaf special issue, the authors review American Sign Language gloss. Topics include how ASL gloss enables deaf children to learn to read in their own language and simultaneously experience a transition to written English, and what gloss looks like and how it underlines deaf children’s learning and mastery of English literacy through ASL. Rebuttal of Grushkin’s argument includes data describing a deaf child’s engagement in reading aloud (entirely in ASL) with a gloss text, which occurred without the breakdown implied by Grushkin. The authors characterize Grushkin’s argument that deaf children need to learn to read through a conventional ASL writing system as limiting, asserting that ASL gloss contributes more by providing a path for learning and mastering English literacy.},
	number = {5},
	urldate = {2024-05-31},
	journal = {American Annals of the Deaf},
	author = {Supalla, Samuel J. and Cripps, Jody H. and Byrne, Andrew P. J.},
	year = {2017},
	note = {Publisher: Gallaudet University Press},
	keywords = {notion},
	pages = {540--551},
}

@inproceedings{zhu_neural_2023,
	address = {Toronto, Canada},
	title = {Neural {Machine} {Translation} {Methods} for {Translating} {Text} to {Sign} {Language} {Glosses}},
	url = {https://aclanthology.org/2023.acl-long.700},
	doi = {10.18653/v1/2023.acl-long.700},
	abstract = {State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).},
	urldate = {2024-03-27},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Dele and Czehmann, Vera and Avramidis, Eleftherios},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {12523--12541},
}

@misc{gheini_cross-attention_2021,
	title = {Cross-{Attention} is {All} {You} {Need}: {Adapting} {Pretrained} {Transformers} for {Machine} {Translation}},
	shorttitle = {Cross-{Attention} is {All} {You} {Need}},
	url = {http://arxiv.org/abs/2104.08771},
	abstract = {We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.},
	language = {en},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Gheini, Mozhdeh and Ren, Xiang and May, Jonathan},
	month = sep,
	year = {2021},
	note = {arXiv:2104.08771 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{lialin_scaling_2023,
	title = {Scaling {Down} to {Scale} {Up}: {A} {Guide} to {Parameter}-{Efficient} {Fine}-{Tuning}},
	shorttitle = {Scaling {Down} to {Scale} {Up}},
	url = {http://arxiv.org/abs/2303.15647},
	abstract = {This paper presents a systematic overview and comparison of parameter-efﬁcient ﬁnetuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of ﬁne-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a speciﬁc focus on real-life efﬁciency and ﬁne-tuning multibillion-scale language models.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
	month = mar,
	year = {2023},
	note = {arXiv:2303.15647 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full ﬁne-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of ﬁne-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B ﬁne-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than ﬁnetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deﬁciency in language model adaptation, which sheds light on the efﬁcacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	language = {en},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@misc{kreutzer_joey_2020,
	title = {Joey {NMT}: {A} {Minimalist} {NMT} {Toolkit} for {Novices}},
	shorttitle = {Joey {NMT}},
	url = {http://arxiv.org/abs/1907.12484},
	abstract = {We present Joey NMT, a minimalist neural machine translation toolkit based on PyTorch that is speciﬁcally designed for novices. Joey NMT provides many popular NMT features in a small and simple code base, so that novices can easily and quickly learn to use it and adapt it to their needs. Despite its focus on simplicity, Joey NMT supports classic architectures (RNNs, transformers), fast beam search, weight tying, and more, and achieves performance comparable to more complex toolkits on standard benchmarks. We evaluate the accessibility of our toolkit in a user study where novices with general knowledge about Pytorch and NMT and experts work through a self-contained Joey NMT tutorial, showing that novices perform almost as well as experts in a subsequent code quiz. Joey NMT is available at https://github. com/joeynmt/joeynmt.},
	language = {en},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Kreutzer, Julia and Bastings, Jasmijn and Riezler, Stefan},
	month = jun,
	year = {2020},
	note = {arXiv:1907.12484 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, notion},
}

@article{chen_two-stream_nodate,
	title = {Two-{Stream} {Network} for {Sign} {Language} {Recognition} and {Translation}},
	abstract = {Sign languages are visual languages using manual articulations and non-manual elements to convey information. For sign language recognition and translation, the majority of existing approaches directly encode RGB videos into hidden representations. RGB videos, however, are raw signals with substantial visual redundancy, leading the encoder to overlook the key information for sign language understanding. To mitigate this problem and better incorporate domain knowledge, such as handshape and body movement, we introduce a dual visual encoder containing two separate streams to model both the raw videos and the keypoint sequences generated by an off-the-shelf keypoint estimator. To make the two streams interact with each other, we explore a variety of techniques, including bidirectional lateral connection, sign pyramid network with auxiliary supervision, and frame-level selfdistillation. The resulting model is called TwoStream-SLR, which is competent for sign language recognition (SLR). TwoStream-SLR is extended to a sign language translation (SLT) model, TwoStream-SLT, by simply attaching an extra translation network. Experimentally, our TwoStream-SLR and TwoStream-SLT achieve stateof-the-art performance on SLR and SLT tasks across a series of datasets including Phoenix-2014, Phoenix-2014T, and CSL-Daily.},
	language = {en},
	author = {Chen, Yutong and Zuo, Ronglai and Wei, Fangyun and Wu, Yu and Liu, Shujie and Mak, Brian},
	keywords = {notion},
}

@misc{liu_multilingual_2020,
	title = {Multilingual {Denoising} {Pre}-training for {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/2001.08210},
	abstract = {This paper demonstrates that multilingual denoising pre-training produces signiﬁcant performance gains across a wide variety of machine translation (MT) tasks. We present mBART – a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the ﬁrst method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly ﬁne tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-speciﬁc modiﬁcations. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.},
	language = {en},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08210 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
}

@misc{hu_corrnet_2024,
	title = {{CorrNet}+: {Sign} {Language} {Recognition} and {Translation} via {Spatial}-{Temporal} {Correlation}},
	shorttitle = {{CorrNet}+},
	url = {http://arxiv.org/abs/2404.11111},
	abstract = {In sign language, the conveyance of human body trajectories predominantly relies upon the coordinated movements of hands and facial expressions across successive frames. Despite the recent impressive advancements of sign language understanding methods, they often solely focus on individual frames, inevitably overlooking the inter-frame correlations that are essential for effectively modeling human body trajectories. To address this limitation, this paper introduces a spatial-temporal correlation network, denoted as CorrNet+, which explicitly identifies and captures body trajectories across multiple frames. In specific, CorrNet+ employs two parallel modules to build human body trajectories: a correlation module and an identification module. The former captures the cross-spacetime correlations in local spatial-temporal neighborhoods, while the latter dynamically constructs human body trajectories by distinguishing informative spatial regions. Afterwards, a temporal attention module is followed to adaptively evaluate the contributions of different frames in the whole video. The resultant features offer a holistic perspective on human body movements, facilitating a deeper understanding of sign language. As a unified model, CorrNet+ achieves new state-of-the-art performance on two extensive sign language understanding tasks, including continuous sign language recognition (CSLR) and sign language translation (SLT). Especially, CorrNet+ surpasses previous methods equipped with resource-intensive pose-estimation networks or pre-extracted heatmaps for hand and facial feature extraction. Compared with CorrNet, CorrNet+ achieves a significant performance boost across all benchmarks while halving the computational overhead, achieving a better computation-accuracy tradeoff. A comprehensive comparison with previous spatial-temporal reasoning methods verifies the superiority of CorrNet+. Code is available at https://github.com/hulianyuyy/CorrNet Plus.},
	language = {en},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Hu, Lianyu and Feng, Wei and Gao, Liqing and Liu, Zekang and Wan, Liang},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11111 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
}

@misc{gong_llms_2024,
	title = {{LLMs} are {Good} {Sign} {Language} {Translators}},
	url = {http://arxiv.org/abs/2404.00925},
	doi = {10.48550/arXiv.2404.00925},
	abstract = {Sign Language Translation (SLT) is a challenging task that aims to translate sign videos into spoken language. Inspired by the strong translation capabilities of large language models (LLMs) that are trained on extensive multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT. In this paper, we regularize the sign videos to embody linguistic characteristics of spoken language, and propose a novel SignLLM framework to transform sign videos into a language-like representation for improved readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The Vector-Quantized Visual Sign module converts sign videos into a sequence of discrete character-level sign tokens, and (2) the Codebook Reconstruction and Alignment module converts these character-level tokens into word-level sign representations using an optimal transport formulation. A sign-text alignment loss further bridges the gap between sign and text tokens, enhancing semantic compatibility. We achieve state-of-the-art gloss-free results on two widely-used SLT benchmarks.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Gong, Jia and Foo, Lin Geng and He, Yixuan and Rahmani, Hossein and Liu, Jun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.00925 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, notion},
}

@inproceedings{ebling_bridging_2015,
	address = {Dresden, Germany},
	title = {Bridging the gap between sign language machine translation and sign language animation using sequence classification},
	url = {https://aclanthology.org/W15-5102},
	doi = {10.18653/v1/W15-5102},
	urldate = {2024-05-07},
	booktitle = {Proceedings of {SLPAT} 2015: 6th {Workshop} on {Speech} and {Language} {Processing} for {Assistive} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Ebling, Sarah and Huenerfauth, Matt},
	editor = {Alexandersson, Jan and Altinsoy, Ercan and Christensen, Heidi and Ljunglöf, Peter and Portet, François and Rudzicz, Frank},
	month = sep,
	year = {2015},
	keywords = {notion},
	pages = {2--9},
}

@incollection{vedaldi_stochastic_2020,
	address = {Cham},
	title = {Stochastic {Fine}-{Grained} {Labeling} of {Multi}-state {Sign} {Glosses} for {Continuous} {Sign} {Language} {Recognition}},
	volume = {12361},
	isbn = {978-3-030-58516-7 978-3-030-58517-4},
	url = {https://link.springer.com/10.1007/978-3-030-58517-4_11},
	abstract = {In this paper, we propose novel stochastic modeling of various components of a continuous sign language recognition (CSLR) system that is based on the transformer encoder and connectionist temporal classiﬁcation (CTC). Most importantly, We model each sign gloss with multiple states, and the number of states is a categorical random variable that follows a learned probability distribution, providing stochastic ﬁne-grained labels for training the CTC decoder. We further propose a stochastic frame dropping mechanism and a gradient stopping method to deal with the severe overﬁtting problem in training the transformer model with CTC loss. These two methods also help reduce the training computation, both in terms of time and space, signiﬁcantly. We evaluated our model on popular CSLR datasets, and show its eﬀectiveness compared to the state-of-the-art methods.},
	language = {en},
	urldate = {2024-05-07},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Niu, Zhe and Mak, Brian},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58517-4_11},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {172--186},
}

@article{forster_extensions_nodate,
	title = {Extensions of the {Sign} {Language} {Recognition} and {Translation} {Corpus} {RWTH}-{PHOENIX}-{Weather}},
	abstract = {This paper introduces the RWTH-PHOENIX-Weather 2014, a video-based, large vocabulary, German sign language corpus which has been extended over the last two years, tripling the size of the original corpus. The corpus contains weather forecasts simultaneously interpreted into sign language which were recorded from German public TV and manually annotated using glosses on the sentence level and semi-automatically transcribed spoken German extracted from the videos using the open-source speech recognition system RASR. Spatial annotations of the signers’ hands as well as shape and orientation annotations of the dominant hand have been added for more than 40k respectively 10k video frames creating one of the largest corpora allowing for quantitative evaluation of object tracking algorithms. Further, over 2k signs have been annotated using the SignWriting annotation system, focusing on the shape, orientation, movement as well as spatial contacts of both hands. Finally, extended recognition and translation setups are deﬁned, and baseline results are presented.},
	language = {en},
	author = {Forster, Jens and Schmidt, Christoph and Koller, Oscar and Bellgardt, Martin and Ney, Hermann},
	keywords = {notion},
}

@inproceedings{manakhimova_linguistically_2023,
	address = {Singapore},
	title = {Linguistically {Motivated} {Evaluation} of the 2023 {State}-of-the-art {Machine} {Translation}: {Can} {ChatGPT} {Outperform} {NMT}?},
	shorttitle = {Linguistically {Motivated} {Evaluation} of the 2023 {State}-of-the-art {Machine} {Translation}},
	url = {https://aclanthology.org/2023.wmt-1.23},
	doi = {10.18653/v1/2023.wmt-1.23},
	abstract = {This paper offers a fine-grained analysis of the machine translation outputs in the context of the Shared Task at the 8th Conference of Machine Translation (WMT23). Building on the foundation of previous test suite efforts, our analysis includes Large Language Models and an updated test set featuring new linguistic phenomena. To our knowledge, this is the first fine-grained linguistic analysis for the GPT-4 translation outputs. Our evaluation spans German-English, English-German, and English-Russian language directions. Some of the phenomena with the lowest accuracies for German-English are idioms and resultative predicates. For English-German, these include mediopassive voice, and noun formation(er). As for English-Russian, these included idioms and semantic roles. GPT-4 performs equally or comparably to the best systems in German-English and English-German but falls in the second significance cluster for English-Russian.},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Manakhimova, Shushen and Avramidis, Eleftherios and Macketanz, Vivien and Lapshinova-Koltunski, Ekaterina and Bagdasarov, Sergei and Möller, Sebastian},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {224--245},
}

@inproceedings{avramidis_challenging_2023,
	address = {Singapore},
	title = {Challenging the {State}-of-the-art {Machine} {Translation} {Metrics} from a {Linguistic} {Perspective}},
	url = {https://aclanthology.org/2023.wmt-1.58},
	doi = {10.18653/v1/2023.wmt-1.58},
	abstract = {We employ a linguistically motivated challenge set in order to evaluate the state-of-the-art machine translation metrics submitted to the Metrics Shared Task of the 8th Conference for Machine Translation. The challenge set includes about 21,000 items extracted from 155 machine translation systems for three language directions, covering more than 100 linguistically-motivated phenomena organized in 14 categories. The metrics that have the best performance with regard to our linguistically motivated analysis are the Cometoid22-wmt23 (a trained metric based on distillation) for German-English and MetricX-23-c (based on a fine-tuned mT5 encoder-decoder language model) for English-German and English-Russian. Some of the most difficult phenomena are passive voice for German-English, named entities, terminology and measurement units for English-German, and focus particles, adverbial clause and stripping for English-Russian.},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Avramidis, Eleftherios and Manakhimova, Shushen and Macketanz, Vivien and Möller, Sebastian},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	keywords = {notion},
	pages = {713--729},
}

@inproceedings{phillips_american_2023,
	address = {Mount Pleasant, MI, USA},
	title = {American {Sign} {Language} {Translation} {Using} {Transfer} {Learning}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350322347},
	url = {https://ieeexplore.ieee.org/document/10291026/},
	doi = {10.1109/AIBThings58340.2023.10291026},
	abstract = {Sign language translation (SLT) is the process of recognizing signs, generating glosses, and translating glosses to a spoken language. Our work focuses on translating American Sign Language (ASL) glosses to English sentences. We use five multitask, transfer learning models to perform these translations. Our method outperforms the state-of-the-art model’s approach by 14.58 points in terms of BLEU-4 on the ASLG-PC12 dataset. Furthermore, we assess the complexity and usefulness of ASLGPC12 for SLT. Our work indicates its synthetic generation does not accurately reflect the complexity of ASL. In turn, we introduce a new dataset, ASLG-EC23, that better reflects the grammatical and topical diversity of ASL. Our method achieves a BLEU-4 score of 40.10 on ASLG-EC23, setting a baseline for future research.},
	language = {en},
	urldate = {2024-05-02},
	booktitle = {2023 {IEEE} {International} {Conference} on {Artificial} {Intelligence}, {Blockchain}, and {Internet} of {Things} ({AIBThings})},
	publisher = {IEEE},
	author = {Phillips, Hunter and Lasch, Steven and Maddumala, Mahesh},
	month = sep,
	year = {2023},
	keywords = {notion},
	pages = {1--5},
}

@misc{ouargani_advancing_2023,
	title = {Advancing {Text}-to-{GLOSS} {Neural} {Translation} {Using} a {Novel} {Hyper}-parameter {Optimization} {Technique}},
	url = {http://arxiv.org/abs/2309.02162},
	abstract = {In this paper, we investigate the use of transformers for Neural Machine Translation of text-to-GLOSS for Deaf and Hard-of-Hearing communication. Due to the scarcity of available data and limited resources for textto-GLOSS translation, we treat the problem as a low-resource language task. We use our novel hyper-parameter exploration technique to explore a variety of architectural parameters and build an optimal transformer-based architecture specifically tailored for text-to-GLOSS translation. The study aims to improve the accuracy and fluency of Neural Machine Translation generated GLOSS. This is achieved by examining various architectural parameters including layer count, attention heads, embedding dimension, dropout, and label smoothing to identify the optimal architecture for improving text-to-GLOSS translation performance. The experiments conducted on the PHOENIX14T dataset reveal that the optimal transformer architecture outperforms previous work on the same dataset. The best model reaches a ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score of 55.18\% and a BLEU-1 (BiLingual Evaluation Understudy 1) score of 63.6\%, outperforming state-of-the-art results on the BLEU1 and ROUGE score by 8.42 and 0.63 respectively.},
	language = {en},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Ouargani, Younes and Khattabi, Noussaima El},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02162 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7, notion},
}

@inproceedings{snover_study_2006,
	address = {Cambridge, Massachusetts, USA},
	title = {A {Study} of {Translation} {Edit} {Rate} with {Targeted} {Human} {Annotation}},
	url = {https://aclanthology.org/2006.amta-papers.25},
	abstract = {We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU—even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as—or better than—a second human judgment does.},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the 7th {Conference} of the {Association} for {Machine} {Translation} in the {Americas}: {Technical} {Papers}},
	publisher = {Association for Machine Translation in the Americas},
	author = {Snover, Matthew and Dorr, Bonnie and Schwartz, Rich and Micciulla, Linnea and Makhoul, John},
	month = aug,
	year = {2006},
	keywords = {notion},
	pages = {223--231},
}

@inproceedings{popovic_chrf_2015,
	address = {Lisbon, Portugal},
	title = {{chrF}: character n-gram {F}-score for automatic {MT} evaluation},
	shorttitle = {{chrF}},
	url = {https://aclanthology.org/W15-3049},
	doi = {10.18653/v1/W15-3049},
	urldate = {2024-05-02},
	booktitle = {Proceedings of the {Tenth} {Workshop} on {Statistical} {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Popović, Maja},
	editor = {Bojar, Ondřej and Chatterjee, Rajan and Federmann, Christian and Haddow, Barry and Hokamp, Chris and Huck, Matthias and Logacheva, Varvara and Pecina, Pavel},
	month = sep,
	year = {2015},
	keywords = {notion},
	pages = {392--395},
}

@inproceedings{post_call_2018,
	address = {Brussels, Belgium},
	title = {A {Call} for {Clarity} in {Reporting} {BLEU} {Scores}},
	url = {https://aclanthology.org/W18-6319},
	doi = {10.18653/v1/W18-6319},
	abstract = {The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to “the” BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.},
	urldate = {2024-04-30},
	booktitle = {Proceedings of the {Third} {Conference} on {Machine} {Translation}: {Research} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Post, Matt},
	editor = {Bojar, Ondřej and Chatterjee, Rajen and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Monz, Christof and Negri, Matteo and Névéol, Aurélie and Neves, Mariana and Post, Matt and Specia, Lucia and Turchi, Marco and Verspoor, Karin},
	month = oct,
	year = {2018},
	keywords = {notion},
	pages = {186--191},
}

@misc{othman_statistical_2011,
	title = {Statistical {Sign} {Language} {Machine} {Translation}: from {English} written text to {American} {Sign} {Language} {Gloss}},
	shorttitle = {Statistical {Sign} {Language} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1112.0168},
	abstract = {This works aims to design a statistical machine translation from English text to American Sign Language (ASL). The system is based on Moses tool with some modifications and the results are synthesized through a 3D avatar for interpretation. First, we translate the input text to gloss, a written form of ASL. Second, we pass the output to the WebSign Plug-in to play the sign. Contributions of this work are the use of a new couple of language English/ASL and an improvement of statistical machine translation based on string matching thanks to Jaro-distance.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Othman, Achraf and Jemni, Mohamed},
	month = dec,
	year = {2011},
	note = {arXiv:1112.0168 [cs]},
	keywords = {notion},
}

@inproceedings{papineni_bleu_2002,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Bleu: a {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {Bleu},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	urldate = {2024-04-02},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	editor = {Isabelle, Pierre and Charniak, Eugene and Lin, Dekang},
	month = jul,
	year = {2002},
	keywords = {notion},
	pages = {311--318},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2024-04-02},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	keywords = {notion},
	pages = {74--81},
}

@inproceedings{chen_simple_2022,
	address = {New Orleans, LA, USA},
	title = {A {Simple} {Multi}-{Modality} {Transfer} {Learning} {Baseline} for {Sign} {Language} {Translation}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879103/},
	doi = {10.1109/CVPR52688.2022.00506},
	abstract = {This paper proposes a simple transfer learning baseline for sign language translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily) contain only about 10K-20K pairs of sign videos, gloss annotations and texts, which are an order of magnitude smaller than typical parallel data for training spoken language translation models. Data is thus a bottleneck for training effective sign language translation models. To mitigate this problem, we propose to progressively pretrain the model from generaldomain datasets that include a large amount of external supervision to within-domain datasets. Concretely, we pretrain the sign-to-gloss visual network on the general domain of human actions and the within-domain of a sign-to-gloss dataset, and pretrain the gloss-to-text translation network on the general domain of a multilingual corpus and the within-domain of a gloss-to-text corpus. The joint model is fine-tuned with an additional module named the visuallanguage mapper that connects the two networks. This simple baseline surpasses the previous state-of-the-art results on two sign language translation benchmarks, demonstrating the effectiveness of transfer learning. With its simplicity and strong performance, this approach can serve as a solid baseline for future research.},
	language = {en},
	urldate = {2024-04-02},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Yutong and Wei, Fangyun and Sun, Xiao and Wu, Zhirong and Lin, Stephen},
	month = jun,
	year = {2022},
	keywords = {notion},
	pages = {5110--5120},
}

@misc{noauthor_glossing_nodate,
	title = {Glossing: transcription symbols in sign language},
	url = {https://www.handspeak.com/learn/3/},
	urldate = {2024-04-02},
	keywords = {notion},
}

@misc{moryossef_data_2021,
	title = {Data {Augmentation} for {Sign} {Language} {Gloss} {Translation}},
	url = {http://arxiv.org/abs/2105.07476},
	abstract = {Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss-to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low-resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on the thus obtained synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Moryossef, Amit and Yin, Kayo and Neubig, Graham and Goldberg, Yoav},
	month = may,
	year = {2021},
	note = {arXiv:2105.07476 [cs]},
	keywords = {notion},
}

@article{zahedi_continuous_nodate,
	title = {Continuous {Sign} {Language} {Recognition} – {Approaches} from {Speech} {Recognition} and {Available} {Data} {Resources}},
	abstract = {In this paper we describe our current work on automatic continuous sign language recognition. We present an automatic sign language recognition system that is based on a large vocabulary speech recognition system and adopts many of the approaches that are conventionally applied in the recognition of spoken language. Furthermore, we present a set of freely available databases that can be used for training, testing and performance evaluation of sign language recognition systems. First results on one of the databases are given, we show that the approaches from spoken language recognition are suitable, and we give directions for further research.},
	language = {en},
	author = {Zahedi, Morteza and Dreuw, Philippe and Rybach, David and Deselaers, Thomas and Bungeroth, Jan and Ney, Hermann},
	keywords = {notion},
}

@article{othman_english-asl_nodate,
	title = {English-{ASL} {Gloss} {Parallel} {Corpus} 2012: {ASLG}-{PC12}},
	abstract = {A serious problem facing the community of researchers in the field of sign language is the absence of a large parallel corpus for signs language. The ASLG-PC12 project proposes a rule-based approach for building a big parallel corpus of English written texts and American Sign Language glosses. We present a novel algorithm that transforms an English part-of-speech sentence to an ASL gloss. This project was started in the beginning of 2011 as a part of the project WebSign, and it offers today a corpus containing more than one hundred million pairs of sentences between English and ASL glosses. It is available online for free to promote development and design of new algorithms and theories for American Sign Language processing, for example statistical machine translation and related fields. In this paper, we present tasks for generating ASL sentences from the Gutenberg Project corpus that contains only English written texts.},
	language = {en},
	author = {Othman, Achraf and Jemni, Mohamed},
	keywords = {notion},
}

@article{wong_sign2gpt_2024,
	title = {{SIGN2GPT}: {LEVERAGING} {LARGE} {LANGUAGE} {MOD}- {ELS} {FOR} {GLOSS}-{FREE} {SIGN} {LANGUAGE} {TRANSLA}-},
	abstract = {Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos. We also propose a novel pretraining strategy that directs our encoder to learn sign representations from automatically extracted pseudo-glosses without requiring gloss order information or annotations. We evaluate our approach on two public benchmark sign language translation datasets, namely RWTH-PHOENIX-Weather 2014T and CSL-Daily, and improve on stateof-the-art gloss-free translation performance with a significant margin.},
	language = {en},
	author = {Wong, Ryan and Camgoz, Necati Cihan and Bowden, Richard},
	year = {2024},
	keywords = {notion},
}

@misc{ye_scaling_2023,
	title = {Scaling {Back}-{Translation} with {Domain} {Text} {Generation} for {Sign} {Language} {Gloss} {Translation}},
	url = {http://arxiv.org/abs/2210.07054},
	abstract = {Sign language gloss translation aims to translate the sign glosses into spoken language texts, which is challenging due to the scarcity of labeled gloss-text parallel data. Back translation (BT), which generates pseudo-parallel data by translating in-domain spoken language texts into sign glosses, has been applied to alleviate the data scarcity problem. However, the lack of large-scale high-quality domain spoken language text data limits the effect of BT. In this paper, to overcome the limitation, we propose a Prompt based domain text Generation (PGEN) approach to produce the large-scale in-domain spoken language text data. Specifically, PGEN randomly concatenates sentences from the original in-domain spoken language text data as prompts to induce a pre-trained language model (i.e., GPT-2) to generate spoken language texts in a similar style. Experimental results on three benchmarks of sign language gloss translation in varied languages demonstrate that BT with spoken language texts generated by PGEN significantly outperforms the compared methods. In addition, as the scale of spoken language texts generated by PGEN increases, the BT technique can achieve further improvements, demonstrating the effectiveness of our approach. We release the code and data for facilitating future research in this field.},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Ye, Jinhui and Jiao, Wenxiang and Wang, Xing and Tu, Zhaopeng},
	month = feb,
	year = {2023},
	note = {arXiv:2210.07054 [cs]},
	keywords = {notion},
}

@inproceedings{cihan_camgoz_sign_2020,
	address = {Seattle, WA, USA},
	title = {Sign {Language} {Transformers}: {Joint} {End}-to-{End} {Sign} {Language} {Recognition} and {Translation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72817-168-5},
	shorttitle = {Sign {Language} {Transformers}},
	url = {https://ieeexplore.ieee.org/document/9156773/},
	doi = {10.1109/CVPR42600.2020.01004},
	abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classiﬁcation (CTC) loss to bind the recognition and translation problems into a single uniﬁed architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to signiﬁcant performance gains.},
	language = {en},
	urldate = {2024-03-28},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cihan Camgoz, Necati and Koller, Oscar and Hadfield, Simon and Bowden, Richard},
	month = jun,
	year = {2020},
	keywords = {notion},
	pages = {10020--10030},
}

@inproceedings{camgoz_neural_2018,
	address = {Salt Lake City, UT},
	title = {Neural {Sign} {Language} {Translation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578910/},
	doi = {10.1109/CVPR.2018.00812},
	abstract = {Sign Language Recognition (SLR) has been an active research ﬁeld for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar.},
	language = {en},
	urldate = {2024-03-28},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Camgoz, Necati Cihan and Hadfield, Simon and Koller, Oscar and Ney, Hermann and Bowden, Richard},
	month = jun,
	year = {2018},
	keywords = {notion},
	pages = {7784--7793},
}

@misc{noauthor_english_nodate,
	title = {English to {ASL} {Gloss} {Machine} {Translation} - {ProQuest}},
	url = {https://www.proquest.com/docview/2489585468?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2024-03-28},
	keywords = {notion},
}

@article{dafnis_isolated_nodate,
	title = {Isolated sign recognition using {ASL} datasets with consistent text-based gloss labeling and curriculum learning},
	language = {en},
	author = {Dafnis, Konstantinos M and Chroni, Evgenia and Neidle, Carol and Metaxas, Dimitris N},
	keywords = {notion},
}

@inproceedings{hassan_isolated-signing_2020,
	address = {Marseille, France},
	title = {An {Isolated}-{Signing} {RGBD} {Dataset} of 100 {American} {Sign} {Language} {Signs} {Produced} by {Fluent} {ASL} {Signers}},
	isbn = {979-10-95546-54-2},
	url = {https://aclanthology.org/2020.signlang-1.14},
	abstract = {We have collected a new dataset consisting of color and depth videos of fluent American Sign Language (ASL) signers performing sequences of 100 ASL signs from a Kinect v2 sensor. This directed dataset had originally been collected as part of an ongoing collaborative project, to aid in the development of a sign-recognition system for identifying occurrences of these 100 signs in video. The set of words consist of vocabulary items that would commonly be learned in a first-year ASL course offered at a university, although the specific set of signs selected for inclusion in the dataset had been motivated by project-related factors. Given increasing interest among sign-recognition and other computer-vision researchers in red-green-blue-depth (RBGD) video, we release this dataset for use by the research community. In addition to the RGB video files, we share depth and HD face data as well as additional features of face, hands, and body produced through post-processing of this data.},
	language = {English},
	urldate = {2024-03-27},
	booktitle = {Proceedings of the {LREC2020} 9th {Workshop} on the {Representation} and {Processing} of {Sign} {Languages}: {Sign} {Language} {Resources} in the {Service} of the {Language} {Community}, {Technological} {Challenges} and {Application} {Perspectives}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Hassan, Saad and Berke, Larwan and Vahdani, Elahe and Jing, Longlong and Tian, Yingli and Huenerfauth, Matt},
	editor = {Efthimiou, Eleni and Fotinea, Stavroula-Evita and Hanke, Thomas and Hochgesang, Julie A. and Kristoffersen, Jette and Mesch, Johanna},
	month = may,
	year = {2020},
	keywords = {notion},
	pages = {89--94},
}

@incollection{rosso_linguistically_2022,
	address = {Cham},
	title = {Linguistically {Enhanced} {Text} to {Sign} {Gloss} {Machine} {Translation}},
	volume = {13286},
	isbn = {978-3-031-08472-0 978-3-031-08473-7},
	url = {https://link.springer.com/10.1007/978-3-031-08473-7_16},
	abstract = {In spite of the recent advances in Machine Translation (MT) for spoken languages, translation between spoken and Sign Languages (SLs) or between Sign Languages remains a difficult problem. Here, we study how Neural Machine Translation (NMT) might overcome the communication barriers for the Deaf and Hard-of-Hearing (DHH) community. Namely, we approach the Text2Gloss translation task in which spoken text segments are translated to lexical sign representations. In this context, we leverage transformer-based models via (1) injecting linguistic features that can guide the learning process towards better translations; and (2) applying a Transfer Learning strategy to reuse the knowledge of a pre-trained model. To this aim, different aggregation strategies are compared and evaluated under Transfer Learning and random weight initialization conditions. The results of this research reveal that linguistic features can successfully contribute to achieve more accurate models; meanwhile, the Transfer Learning procedure applied conducted to substantial performance increases.},
	language = {en},
	urldate = {2024-03-27},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer International Publishing},
	author = {Egea Gómez, Santiago and Chiruzzo, Luis and McGill, Euan and Saggion, Horacio},
	editor = {Rosso, Paolo and Basile, Valerio and Martínez, Raquel and Métais, Elisabeth and Meziane, Farid},
	year = {2022},
	doi = {10.1007/978-3-031-08473-7_16},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {172--183},
}

@article{amin_sign_2021,
	title = {Sign {Language} {Gloss} {Translation} using {Deep} {Learning} {Models}},
	volume = {12},
	doi = {10.14569/IJACSA.2021.0121178},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Amin, Mohamed and Hefny, Hesahm and Mohammed, Ammar},
	month = jan,
	year = {2021},
	keywords = {notion},
}

@inproceedings{li_word-level_2020,
	address = {Snowmass Village, CO, USA},
	title = {Word-level {Deep} {Sign} {Language} {Recognition} from {Video}: {A} {New} {Large}-scale {Dataset} and {Methods} {Comparison}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72816-553-0},
	shorttitle = {Word-level {Deep} {Sign} {Language} {Recognition} from {Video}},
	url = {https://ieeexplore.ieee.org/document/9093512/},
	doi = {10.1109/WACV45572.2020.9093512},
	abstract = {Vision-based sign language recognition aims at helping the deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Li, Dongxu and Opazo, Cristian Rodriguez and Yu, Xin and Li, Hongdong},
	month = mar,
	year = {2020},
	keywords = {notion},
	pages = {1448--1458},
}

@article{neidle_challenges_nodate,
	title = {Challenges in {Development} of the {American} {Sign} {Language}},
	abstract = {The American Sign Language Lexicon Video Dataset (ASLLVD) consists of videos of {\textgreater}3,300 ASL signs in citation form, each produced by 1-6 native ASL signers, for a total of almost 9,800 tokens. This dataset, including multiple synchronized videos showing the signing from different angles, will be shared publicly once the linguistic annotations and verifications are complete. Linguistic annotations include gloss labels, sign start and end time codes, start and end handshape labels for both hands, morphological and articulatory classifications of sign type. For compound signs, the dataset includes annotations for each morpheme. To facilitate computer vision-based sign language recognition, the dataset also includes numeric ID labels for sign variants, video sequences in uncompressed-raw format, camera calibration sequences, and software for skin region extraction. We discuss here some of the challenges involved in the linguistic annotations and categorizations. We also report an example computer vision application that leverages the ASLLVD: the formulation employs a HandShapes Bayesian Network (HSBN), which models the transition probabilities between start and end handshapes in monomorphemic lexical signs. Further details and statistics for the ASLLVD dataset, as well as information about annotation conventions, are available from http://www.bu.edu/asllrp/lexicon.},
	language = {en},
	author = {Neidle, Carol and Thangali, Ashwin and Sclaroff, Stan},
	keywords = {notion},
}

@book{efthimiou_proceedings_2022,
	address = {Paris},
	title = {Proceedings of the {LREC} 2022 {Workshop} on the {Representation} and {Processing} of {Sign} {Languages}: {Multilingual} {Sign} {Language} {Resources}},
	isbn = {979-10-95546-86-3},
	shorttitle = {Proceedings of the {LREC} 2022 {Workshop} on the {Representation} and {Processing} of {Sign} {Languages}},
	language = {en},
	publisher = {European Language Resources Association (ELRA)},
	editor = {Efthimiou, Eleni and Fotinea, Stavroula-Evita and Hanke, Thomas and Hochgesang, Julie A. and Kristoffersen, Jette and Mesch, Johanna and Schulder, Marc},
	year = {2022},
	keywords = {notion},
}

@inproceedings{thangali_exploiting_2011,
	address = {Colorado Springs, CO, USA},
	title = {Exploiting phonological constraints for handshape inference in {ASL} video},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995718/},
	doi = {10.1109/CVPR.2011.5995718},
	abstract = {Handshape is a key linguistic component of signs, and thus, handshape recognition is essential to algorithms for sign language recognition and retrieval. In this work, linguistic constraints on the relationship between start and end handshapes are leveraged to improve handshape recognition accuracy. A Bayesian network formulation is proposed for learning and exploiting these constraints, while taking into consideration inter-signer variations in the production of particular handshapes. A Variational Bayes formulation is employed for supervised learning of the model parameters. A non-rigid image alignment algorithm, which yields improved robustness to variability in handshape appearance, is proposed for computing image observation likelihoods in the model. The resulting handshape inference algorithm is evaluated using a dataset of 1500 lexical signs in American Sign Language (ASL), where each lexical sign is produced by three native ASL signers.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Thangali, Ashwin and Nash, Joan P. and Sclaroff, Stan and Neidle, Carol},
	month = jun,
	year = {2011},
	keywords = {notion},
	pages = {521--528},
}

@misc{noauthor_american_nodate,
	title = {American {Sign} {Language} {Lexicon} {Video} {Dataset} ({ASLLVD})},
	url = {https://www.bu.edu/asllrp/av/dai-asllvd.html#terms},
	urldate = {2024-03-26},
	keywords = {notion},
}

@inproceedings{duarte_how2sign_2021,
	address = {Nashville, TN, USA},
	title = {{How2Sign}: {A} {Large}-scale {Multimodal} {Dataset} for {Continuous} {American} {Sign} {Language}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-509-2},
	shorttitle = {{How2Sign}},
	url = {https://ieeexplore.ieee.org/document/9577749/},
	doi = {10.1109/CVPR46437.2021.00276},
	abstract = {One of the factors that have hindered progress in the areas of sign language recognition, translation, and production is the absence of large annotated datasets. Towards this end, we introduce How2Sign, a multimodal and multiview continuous American Sign Language (ASL) dataset, consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation. To evaluate the potential of How2Sign for real-world impact, we conduct a study with ASL signers and show that synthesized videos using our dataset can indeed be understood. The study further gives insights on challenges that computer vision should address in order to make progress in this ﬁeld.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Duarte, Amanda and Palaskar, Shruti and Ventura, Lucas and Ghadiyaram, Deepti and DeHaan, Kenneth and Metze, Florian and Torres, Jordi and Giro-i-Nieto, Xavier},
	month = jun,
	year = {2021},
	keywords = {notion},
	pages = {2734--2743},
}

@article{neidle_new_nodate,
	title = {{NEW} {Shared} \& {Interconnected} {ASL} {Resources}: {SignStream}® 3 {Software}; {DAI} 2 for {Web} {Access} to {Linguistically} {Annotated} {Video} {Corpora}; and a {Sign} {Bank}},
	language = {en},
	author = {Neidle, Carol and Opoku, Augustine and Dimitriadis, Gregory and Metaxas, Dimitris},
	keywords = {notion},
}

@misc{neidle_asl_2022,
	title = {{ASL} {Video} {Corpora} \& {Sign} {Bank}: {Resources} {Available} through the {American} {Sign} {Language} {Linguistic} {Research} {Project} ({ASLLRP})},
	shorttitle = {{ASL} {Video} {Corpora} \& {Sign} {Bank}},
	url = {http://arxiv.org/abs/2201.07899},
	abstract = {The American Sign Language Linguistic Research Project (ASLLRP) provides Internet access to high-quality ASL video data, generally including front and side views and a close-up of the face. The manual and non-manual components of the signing have been linguistically annotated using SignStream(R). The recently expanded video corpora can be browsed and searched through the Data Access Interface (DAI 2) we have designed; it is possible to carry out complex searches. The data from our corpora can also be downloaded; annotations are available in an XML export format. We have also developed the ASLLRP Sign Bank, which contains almost 6,000 sign entries for lexical signs, with distinct English-based glosses, with a total of 41,830 examples of lexical signs (in addition to about 300 gestures, over 1,000 fingerspelled signs, and 475 classifier examples). The Sign Bank is likewise accessible and searchable on the Internet; it can also be accessed from within SignStream(R) (software to facilitate linguistic annotation and analysis of visual language data) to make annotations more accurate and efficient. Here we describe the available resources. These data have been used for many types of research in linguistics and in computer-based sign language recognition from video; examples of such research are provided in the latter part of this article.},
	urldate = {2024-03-26},
	publisher = {arXiv},
	author = {Neidle, Carol and Opoku, Augustine and Metaxas, Dimitris},
	month = jan,
	year = {2022},
	note = {arXiv:2201.07899 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
}

@misc{moryossef_open-source_2023,
	title = {An {Open}-{Source} {Gloss}-{Based} {Baseline} for {Spoken} to {Signed} {Language} {Translation}},
	url = {http://arxiv.org/abs/2305.17714},
	abstract = {Sign language translation systems are complex and require many components. As a result, it is very hard to compare methods across publications. We present an open-source implementation of a text-to-gloss-to-pose-to-video pipeline approach, demonstrating conversion from German to Swiss German Sign Language, French to French Sign Language of Switzerland, and Italian to Italian Sign Language of Switzerland. We propose three different components for the text-to-gloss translation: a lemmatizer, a rule-based word reordering and dropping component, and a neural machine translation system. Gloss-to-pose conversion occurs using data from a lexicon for three different signed languages, with skeletal poses extracted from videos. To generate a sentence, the text-to-gloss system is first run, and the pose representations of the resulting signs are stitched together.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Moryossef, Amit and Müller, Mathias and Göhring, Anne and Jiang, Zifan and Goldberg, Yoav and Ebling, Sarah},
	month = may,
	year = {2023},
	note = {arXiv:2305.17714 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, notion},
}

@article{ormel_glossing_2010,
	title = {Glossing a multi-purpose sign language corpus},
	abstract = {This paper describes the strategies that have been developed for creating consistent gloss annotations in the latest update to the Corpus NGT. Although the project aims to embrace the plea for ID-glosses in Johnston (2008), there is no reference lexicon that could be used in the creation of the annotations. An idiosyncratic strategy was developed that involved the creation of a temporary ‘glossing lexicon’, which includes conventions for distinguishing regional and other variants, true and apparent homonymy, and other difficulties that are specifically related to the glossing of two-handed simultaneous constructions on different tiers.},
	language = {en},
	author = {Ormel, Ellen and Crasborn, Onno and Forster, Jens and Stein, Daniel},
	year = {2010},
	keywords = {notion},
}

@article{lucie_survey_2020,
	title = {A survey on the animation of signing avatars: {From} sign representation to utterance synthesis},
	abstract = {Signing avatars make it possible for deaf people to access information in their preferred language. How-
ever, sign language synthesis represents a challenge for the computer animation community as the mo-
tions generated must be realistic and have a precise semantic meaning. In this article, we distinguish
the synthesis of isolated signs deprived of any contextual inflections from the generation of full sign lan-
guage utterances. In both cases, the animation engine takes as input a representation of the synthesis
objective to create the final animation. Because of their spatiotemporal characteristics, signs and utter-
ances cannot be described by a sequential representation like phonetics in spoken languages. For this
reason, linguistic and gestural studies have aimed to capture the typical and special features of signs and
sign language syntax to promote different sign language representations. Those sign representations can
then be used to produce an avatar animation thanks to sign synthesis techniques based on keyframes,
procedural means or data-driven approaches. Novel utterances can also be generated using concatenative
or articulatory techniques.
This article constitutes a survey of (i) the challenges specific to sign languages avatars, (ii) the sign rep-
resentations developed in order to synthesize isolated signs, (iii) the possible sign synthesis approaches,
(iv) the different utterance specifications, and (v) the challenges and animation techniques for generating
sign language utterances.},
	author = {Lucie, Naet and Caroline, Larboulette and Sylvie, Gibet},
	year = {2020},
	keywords = {notion},
}

@inproceedings{bragg_sign_2019,
	address = {Pittsburgh PA USA},
	title = {Sign {Language} {Recognition}, {Generation}, and {Translation}: {An} {Interdisciplinary} {Perspective}},
	isbn = {978-1-4503-6676-2},
	shorttitle = {Sign {Language} {Recognition}, {Generation}, and {Translation}},
	url = {https://dl.acm.org/doi/10.1145/3308561.3353774},
	doi = {10.1145/3308561.3353774},
	language = {en},
	urldate = {2024-03-20},
	booktitle = {The 21st {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Bragg, Danielle and Koller, Oscar and Bellard, Mary and Berke, Larwan and Boudreault, Patrick and Braffort, Annelies and Caselli, Naomi and Huenerfauth, Matt and Kacorri, Hernisa and Verhoef, Tessa and Vogler, Christian and Ringel Morris, Meredith},
	month = oct,
	year = {2019},
	keywords = {notion},
	pages = {16--31},
}

@misc{peng_better_2023,
	title = {Better {Sign} {Language} {Translation} with {Monolingual} {Data}},
	url = {http://arxiv.org/abs/2304.10844},
	abstract = {Sign language translation (SLT) systems, which are often decomposed into video-to-gloss (V2G) recognition and gloss-to-text (G2T) translation through the pivot gloss, heavily relies on the availability of large-scale parallel G2T pairs. However, the manual annotation of pivot gloss, which is a sequence of transcribed written-language words in the order in which they are signed, further exacerbates the scarcity of data for SLT. To address this issue, this paper proposes a simple and efficient rule transformation method to transcribe the large-scale target monolingual data into its pseudo glosses automatically for enhancing the SLT translation. Empirical results show that the proposed approach can significantly improve the performance of SLT, especially achieving state-of-the-art results on two SLT benchmark datasets PHEONIX-WEATHER 2014T and ASLG-PC12. Our code has been released at: https://github.com/pengr/Mono{\textbackslash}\_SLT.},
	urldate = {2024-04-05},
	publisher = {arXiv},
	author = {Peng, Ru and Zeng, Yawen and Zhao, Junbo},
	month = apr,
	year = {2023},
	note = {arXiv:2304.10844 [cs]
version: 1},
	keywords = {notion},
}

@inproceedings{banerjee_meteor_2005,
	address = {Ann Arbor, Michigan},
	title = {{METEOR}: {An} {Automatic} {Metric} for {MT} {Evaluation} with {Improved} {Correlation} with {Human} {Judgments}},
	shorttitle = {{METEOR}},
	url = {https://aclanthology.org/W05-0909},
	urldate = {2024-04-04},
	booktitle = {Proceedings of the {ACL} {Workshop} on {Intrinsic} and {Extrinsic} {Evaluation} {Measures} for {Machine} {Translation} and/or {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Satanjeev and Lavie, Alon},
	editor = {Goldstein, Jade and Lavie, Alon and Lin, Chin-Yew and Voss, Clare},
	month = jun,
	year = {2005},
	keywords = {notion},
	pages = {65--72},
}

@article{stoll_text2sign_2020,
	title = {{Text2Sign}: {Towards} {Sign} {Language} {Production} {Using} {Neural} {Machine} {Translation} and {Generative} {Adversarial} {Networks}},
	volume = {128},
	issn = {1573-1405},
	shorttitle = {{Text2Sign}},
	url = {https://doi.org/10.1007/s11263-019-01281-2},
	doi = {10.1007/s11263-019-01281-2},
	abstract = {We present a novel approach to automatic Sign Language Production using recent developments in Neural Machine Translation (NMT), Generative Adversarial Networks, and motion generation. Our system is capable of producing sign videos from spoken language sentences. Contrary to current approaches that are dependent on heavily annotated data, our approach requires minimal gloss and skeletal level annotations for training. We achieve this by breaking down the task into dedicated sub-processes. We first translate spoken language sentences into sign pose sequences by combining an NMT network with a Motion Graph. The resulting pose information is then used to condition a generative model that produces photo realistic sign language video sequences. This is the first approach to continuous sign video generation that does not use a classical graphical avatar. We evaluate the translation abilities of our approach on the PHOENIX14T Sign Language Translation dataset. We set a baseline for text-to-gloss translation, reporting a BLEU-4 score of 16.34/15.26 on dev/test sets. We further demonstrate the video generation capabilities of our approach for both multi-signer and high-definition settings qualitatively and quantitatively using broadcast quality assessment metrics.},
	language = {en},
	number = {4},
	urldate = {2024-03-29},
	journal = {International Journal of Computer Vision},
	author = {Stoll, Stephanie and Camgoz, Necati Cihan and Hadfield, Simon and Bowden, Richard},
	month = apr,
	year = {2020},
	keywords = {notion},
	pages = {891--908},
}

@article{desai_asl_nodate,
	title = {{ASL} {Citizen}: {A} {Community}-{Sourced} {Dataset} for {Advancing} {Isolated} {Sign} {Language} {Recognition}},
	abstract = {Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the first crowdsourced Isolated Sign Language Recognition (ISLR) dataset, collected with consent and containing 83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their webcam to retrieve matching signs from a dictionary. Through our generalizable baselines, we show that training supervised machine learning classifiers with our dataset achieves competitive performance on metrics relevant for dictionary retrieval, with 63\% accuracy and a recall-at-10 of 91\%, evaluated entirely on videos of users who are not present in the training or validation sets.},
	language = {en},
	author = {Desai, Aashaka and Berger, Lauren and Minakov, Fyodor O and Milan, Vanessa and Singh, Chinmay and Pumphrey, Kriston and Ladner, Richard E and Iii, Hal Daumé and Lu, Alex X and Caselli, Naomi and Bragg, Danielle},
	keywords = {notion},
}

@inproceedings{rastgoo_sign_2021,
	address = {Nashville, TN, USA},
	title = {Sign {Language} {Production}: {A} {Review}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66544-899-4},
	shorttitle = {Sign {Language} {Production}},
	url = {https://ieeexplore.ieee.org/document/9522839/},
	doi = {10.1109/CVPRW53098.2021.00384},
	abstract = {Sign Language is the dominant yet non-primary form of communication language used in the deaf and hearingimpaired community. To make an easy and mutual communication between the hearing-impaired and the hearing communities, building a robust system capable of translating the spoken language into sign language and vice versa is fundamental. To this end, sign language recognition and production are two necessary parts for making such a two-way system. Sign language recognition and production need to cope with some critical challenges. In this survey, we review recent advances in Sign Language Production (SLP) and related areas using deep learning. This survey aims to brieﬂy summarize recent achievements in SLP, discussing their advantages, limitations, and future directions of research.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio and Sabokrou, Mohammad},
	month = jun,
	year = {2021},
	keywords = {notion},
	pages = {3446--3456},
}

@inproceedings{egea_gomez_syntax-aware_2021,
	address = {Online (Virtual Mode)},
	title = {Syntax-aware {Transformers} for {Neural} {Machine} {Translation}: {The} {Case} of {Text} to {Sign} {Gloss} {Translation}},
	shorttitle = {Syntax-aware {Transformers} for {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2021.bucc-1.4},
	abstract = {It is well-established that the preferred mode of communication of the deaf and hard of hearing (DHH) community are Sign Languages (SLs), but they are considered low resource languages where natural language processing technologies are of concern. In this paper we study the problem of text to SL gloss Machine Translation (MT) using Transformer-based architectures. Despite the significant advances of MT for spoken languages in the recent couple of decades, MT is in its infancy when it comes to SLs. We enrich a Transformer-based architecture aggregating syntactic information extracted from a dependency parser to word-embeddings. We test our model on a well-known dataset showing that the syntax-aware model obtains performance gains in terms of MT evaluation metrics.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 14th {Workshop} on {Building} and {Using} {Comparable} {Corpora} ({BUCC} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Egea Gómez, Santiago and McGill, Euan and Saggion, Horacio},
	editor = {Rapp, Reinhard and Sharoff, Serge and Zweigenbaum, Pierre},
	month = sep,
	year = {2021},
	keywords = {notion},
	pages = {18--27},
}

@article{nunez-marcos_survey_2023,
	title = {A survey on {Sign} {Language} machine translation},
	volume = {213},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422020115},
	doi = {10.1016/j.eswa.2022.118993},
	abstract = {Sign Languages (SLs) are employed by deaf and hard-of-hearing (DHH) people to communicate on a daily basis. However, the communication with hearing people still faces some barriers, mainly because of the scarce knowledge about SLs among hearing people. Hence, tools to allow the communication between users of either sign or spoken languages must be encouraged. A stepping stone in this direction is the research of the sign language translation (SLT) task, which aims to produce a spoken language translation of a sign language video or vice versa. By implementing these types of translators in portable devices, we will make considerable progress towards a barrier-free communication between DHH and hearing people. That is why, in this work, we focus on reviewing the literature on SLT and provide the necessary background about SLs. Besides, we summarise the available datasets and the results found in the literature for one of the most used datasets, the RWTH-PHOENIX-2014T. Moreover, the survey lists the challenges that need to be tackled within the SLT research and also for the adoption of SLT technologies, and proposes future research lines.},
	language = {en},
	urldate = {2024-03-22},
	journal = {Expert Systems with Applications},
	author = {Núñez-Marcos, Adrián and Perez-de-Viñaspre, Olatz and Labaka, Gorka},
	month = mar,
	year = {2023},
	keywords = {notion},
	pages = {118993},
}

@incollection{goos_machine_2000,
	address = {Berlin, Heidelberg},
	title = {A {Machine} {Translation} {System} from {English} to {American} {Sign} {Language}},
	volume = {1934},
	isbn = {978-3-540-41117-8 978-3-540-39965-0},
	url = {http://link.springer.com/10.1007/3-540-39965-8_6},
	abstract = {Research in computational linguistics, computer graphics and autonomous agents has led to the development of increasingly sophisticated communicative agents over the past few years, bringing new perspective to machine translation research. The engineering of language-based smooth, expressive, natural-looking human gestures can give us useful insights into the design principles that have evolved in natural communication between people. In this paper we prototype a machine translation system from English to American Sign Language (ASL), taking into account not only linguistic but also visual and spatial information associated with ASL signs.},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {Envisioning {Machine} {Translation} in the {Information} {Future}},
	publisher = {Springer Berlin Heidelberg},
	author = {Zhao, Liwei and Kipper, Karin and Schuler, William and Vogler, Christian and Badler, Norman and Palmer, Martha},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and White, John S.},
	year = {2000},
	doi = {10.1007/3-540-39965-8_6},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {notion},
	pages = {54--67},
}

@inproceedings{zelinka_neural_2020,
	address = {Snowmass Village, CO, USA},
	title = {Neural {Sign} {Language} {Synthesis}: {Words} {Are} {Our} {Glosses}},
	isbn = {978-1-72816-553-0},
	shorttitle = {Neural {Sign} {Language} {Synthesis}},
	url = {https://ieeexplore.ieee.org/document/9093516/},
	doi = {10.1109/WACV45572.2020.9093516},
	abstract = {This paper deals with a text-to-video sign language synthesis. Instead of direct video production, we focused on skeletal models production. Our main goal in this paper was to design a fully end-to-end automatic sign language synthesis system trained only on available free data (daily TV broadcasting). Thus, we excluded any manual video annotation. Furthermore, our designed approach even do not rely on any video segmentation. A proposed feed-forward transformer and recurrent transformer were investigated. To improve the performance of our sequence-to-sequence transformer, soft non-monotonic attention was employed in our training process. A beneﬁt of character-level features was compared with word-level features. We focused our experiments on a weather forecasting dataset in the Czech Sign Language.},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Zelinka, Jan and Kanis, Jakub},
	month = mar,
	year = {2020},
	keywords = {notion},
	pages = {3384--3392},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {notion},
}

@article{ananthanarayana_deep_2021,
	title = {Deep {Learning} {Methods} for {Sign} {Language} {Translation}},
	volume = {14},
	issn = {1936-7228, 1936-7236},
	url = {https://dl.acm.org/doi/10.1145/3477498},
	doi = {10.1145/3477498},
	abstract = {Many sign languages are bona fide natural languages with grammatical rules and lexicons hence can benefit from machine translation methods. Similarly, since sign language is a visual-spatial language, it can also benefit from computer vision methods for encoding it. With the advent of deep learning methods in recent years, significant advances have been made in natural language processing (specifically neural machine translation) and in computer vision methods (specifically image and video captioning). Researchers have therefore begun expanding these learning methods to sign language understanding. Sign language interpretation is especially challenging, because it involves a continuous visual-spatial modality where meaning is often derived based on context.
            
              The focus of this article, therefore, is to examine various deep learning–based methods for encoding sign language as inputs, and to analyze the efficacy of several machine translation methods, over three different sign language datasets. The goal is to determine which combinations are sufficiently robust for sign language translation
              without
              any gloss-based information.
            
            To understand the role of the different input features, we perform ablation studies over the model architectures (input features + neural translation models) for improved continuous sign language translation. These input features include body and finger joints, facial points, as well as vector representations/embeddings from convolutional neural networks. The machine translation models explored include several baseline sequence-to-sequence approaches, more complex and challenging networks using attention, reinforcement learning, and the transformer model. We implement the translation methods over multiple sign languages—German (GSL), American (ASL), and Chinese sign languages (CSL). From our analysis, the transformer model combined with input embeddings from ResNet50 or pose-based landmark features outperformed all the other sequence-to-sequence models by achieving higher BLEU2-BLEU4 scores when applied to the controlled and constrained GSL benchmark dataset. These combinations also showed significant promise on the other less controlled ASL and CSL datasets.},
	language = {en},
	number = {4},
	urldate = {2024-03-22},
	journal = {ACM Transactions on Accessible Computing},
	author = {Ananthanarayana, Tejaswini and Srivastava, Priyanshu and Chintha, Akash and Santha, Akhil and Landy, Brian and Panaro, Joseph and Webster, Andre and Kotecha, Nikunj and Sah, Shagan and Sarchet, Thomastine and Ptucha, Raymond and Nwogu, Ifeoma},
	month = dec,
	year = {2021},
	keywords = {notion},
	pages = {1--30},
}

@inproceedings{findlater_deaf_2019,
	address = {Glasgow Scotland Uk},
	title = {Deaf and {Hard}-of-hearing {Individuals}' {Preferences} for {Wearable} and {Mobile} {Sound} {Awareness} {Technologies}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300276},
	doi = {10.1145/3290605.3300276},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Findlater, Leah and Chinh, Bonnie and Jain, Dhruv and Froehlich, Jon and Kushalnagar, Raja and Lin, Angela Carey},
	month = may,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
}

@inproceedings{saha_visualizing_2022,
	address = {New Orleans LA USA},
	title = {Visualizing {Urban} {Accessibility}: {Investigating} {Multi}-{Stakeholder} {Perspectives} through a {Map}-based {Design} {Probe} {Study}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Visualizing {Urban} {Accessibility}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517460},
	doi = {10.1145/3491102.3517460},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Saha, Manaswi and Patil, Siddhant and Cho, Emily and Cheng, Evie Yu-Yen and Horng, Chris and Chauhan, Devanshi and Kangas, Rachel and McGovern, Richard and Li, Anthony and Heer, Jeffrey and Froehlich, Jon E.},
	month = apr,
	year = {2022},
	keywords = {notion},
	pages = {1--14},
}

@inproceedings{sharma_disability-first_2023,
	address = {Hamburg Germany},
	title = {Disability-{First} {Design} and {Creation} of {A} {Dataset} {Showing} {Private} {Visual} {Information} {Collected} {With} {People} {Who} {Are} {Blind}},
	isbn = {978-1-4503-9421-5},
	url = {https://dl.acm.org/doi/10.1145/3544548.3580922},
	doi = {10.1145/3544548.3580922},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Sharma, Tanusree and Stangl, Abigale and Zhang, Lotus and Tseng, Yu-Yun and Xu, Inan and Findlater, Leah and Gurari, Danna and Wang, Yang},
	month = apr,
	year = {2023},
	keywords = {notion},
	pages = {1--15},
}

@inproceedings{jin_how_2022,
	address = {New Orleans LA USA},
	title = {How {Will} {VR} {Enter} {University} {Classrooms}? {Multi}-stakeholders {Investigation} of {VR} in {Higher} {Education}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {How {Will} {VR} {Enter} {University} {Classrooms}?},
	url = {https://dl.acm.org/doi/10.1145/3491102.3517542},
	doi = {10.1145/3491102.3517542},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Jin, Qiao and Liu, Yu and Yarosh, Svetlana and Han, Bo and Qian, Feng},
	month = apr,
	year = {2022},
	keywords = {notion},
	pages = {1--17},
}

@article{bhandari_multi-stakeholder_2022,
	title = {Multi-stakeholder {Perspectives} on {Digital} {Tools} for {U}.{S}. {Asylum} {Applicants} {Seeking} {Healthcare} and {Legal} {Information}},
	volume = {6},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3555642},
	doi = {10.1145/3555642},
	abstract = {There is a concerning lack of clear and accurate information around accessing public benefits for asylum applicants in the United States (U.S.), which has been shown to negatively affect their healthcare engagement. Digital tools such as websites and mobile applications can be a potentially promising way to disseminate public benefits information to asylum applicants. The goal of this study is to understand the current informational needs of asylum applicants in the U.S. seeking legal information and resources regarding their individual rights to public health benefits and services. Through semi-structured interviews with 24 asylum applicants currently in the U.S. and 13 healthcare and legal professionals working with asylum applicants and other immigrants, we identify four key challenges and barriers to using currently available digital tools: information uncertainty, accessibility, emotional barriers, and contextual sensitivity. Our findings highlight the importance of considering multiple stakeholders' perspectives when designing tools within the immigration informational space. We provide targeted design recommendations to create digital tools for asylum seekers and the stakeholders who support them.},
	language = {en},
	number = {CSCW2},
	urldate = {2024-03-21},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Bhandari, Aparajita and Freed, Diana and Pilato, Tara and Taki, Faten and Kaur, Gunisha and Yale-Loehr, Stephen and Powers, Jane and Long, Tao and Bazarova, Natalya N.},
	month = nov,
	year = {2022},
	keywords = {notion},
	pages = {1--21},
}

@inproceedings{hassan_design_2022,
	address = {New Orleans LA USA},
	title = {Design and {Evaluation} of {Hybrid} {Search} for {American} {Sign} {Language} to {English} {Dictionaries}: {Making} the {Most} of {Imperfect} {Sign} {Recognition}},
	isbn = {978-1-4503-9157-3},
	shorttitle = {Design and {Evaluation} of {Hybrid} {Search} for {American} {Sign} {Language} to {English} {Dictionaries}},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501986},
	doi = {10.1145/3491102.3501986},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {{CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Hassan, Saad and Amin, Akhter Al and Gordon, Alexis and Lee, Sooyeon and Huenerfauth, Matt},
	month = apr,
	year = {2022},
	keywords = {notion},
	pages = {1--13},
}

@inproceedings{bohacek_sign_2023,
	address = {New York NY USA},
	title = {Sign {Spotter}: {Design} and {Initial} {Evaluation} of an {Automatic} {Video}-{Based} {American} {Sign} {Language} {Dictionary} {System}},
	isbn = {9798400702204},
	shorttitle = {Sign {Spotter}},
	url = {https://dl.acm.org/doi/10.1145/3597638.3614497},
	doi = {10.1145/3597638.3614497},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {The 25th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Bohacek, Matyas and Hassan, Saad},
	month = oct,
	year = {2023},
	keywords = {notion},
	pages = {1--5},
}

@inproceedings{tang_community-driven_2023,
	address = {Hamburg Germany},
	title = {Community-{Driven} {Information} {Accessibility}: {Online} {Sign} {Language} {Content} {Creation} within d/{Deaf} {Communities}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Community-{Driven} {Information} {Accessibility}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581286},
	doi = {10.1145/3544548.3581286},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Tang, Xinru and Chang, Xiang and Chen, Nuoran and Ni, Yingjie (MaoMao) and Lc, Ray and Tong, Xin},
	month = apr,
	year = {2023},
	keywords = {notion},
	pages = {1--24},
}

@inproceedings{bleakley_exploring_2022,
	address = {Athens Greece},
	title = {Exploring {Smart} {Speaker} {User} {Experience} for {People} {Who} {Stammer}},
	isbn = {978-1-4503-9258-7},
	url = {https://dl.acm.org/doi/10.1145/3517428.3544823},
	doi = {10.1145/3517428.3544823},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 24th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Bleakley, Anna and Rough, Daniel and Roper, Abi and Lindsay, Stephen and Porcheron, Martin and Lee, Minha and Nicholson, Stuart Alan and Cowan, Benjamin R. and Clark, Leigh},
	month = oct,
	year = {2022},
	keywords = {notion},
	pages = {1--10},
}

@inproceedings{lea_user_2023,
	address = {Hamburg Germany},
	title = {From {User} {Perceptions} to {Technical} {Improvement}: {Enabling} {People} {Who} {Stutter} to {Better} {Use} {Speech} {Recognition}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {From {User} {Perceptions} to {Technical} {Improvement}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581224},
	doi = {10.1145/3544548.3581224},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Lea, Colin and Huang, Zifang and Narain, Jaya and Tooley, Lauren and Yee, Dianna and Tran, Dung Tien and Georgiou, Panayiotis and Bigham, Jeffrey P and Findlater, Leah},
	month = apr,
	year = {2023},
	keywords = {notion},
	pages = {1--16},
}

@inproceedings{ohshiro_how_2022,
	address = {Athens Greece},
	title = {How people who are deaf, {Deaf}, and hard of hearing use technology in creative sound activities},
	isbn = {978-1-4503-9258-7},
	url = {https://dl.acm.org/doi/10.1145/3517428.3550396},
	doi = {10.1145/3517428.3550396},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 24th {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Ohshiro, Keita and Cartwright, Mark},
	month = oct,
	year = {2022},
	keywords = {notion},
	pages = {1--4},
}

@inproceedings{dingman_interview_2021,
	address = {Virtual Event USA},
	title = {Interview and {Think} {Aloud} {Accessibility} for {Deaf} and {Hard} of {Hearing} {Participants} in {Design} {Research}},
	isbn = {978-1-4503-8306-6},
	url = {https://dl.acm.org/doi/10.1145/3441852.3476526},
	doi = {10.1145/3441852.3476526},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 23rd {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Dingman, Becca and Tigwell, Garreth W. and Shinohara, Kristen},
	month = oct,
	year = {2021},
	keywords = {notion},
	pages = {1--3},
}

@misc{yin_including_2021,
	title = {Including {Signed} {Languages} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2105.05222},
	abstract = {Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Yin, Kayo and Moryossef, Amit and Hochgesang, Julie and Goldberg, Yoav and Alikhani, Malihe},
	month = jul,
	year = {2021},
	note = {arXiv:2105.05222 [cs]},
	keywords = {notion},
}

@misc{desai_systemic_2024,
	title = {Systemic {Biases} in {Sign} {Language} {AI} {Research}: {A} {Deaf}-{Led} {Call} to {Reevaluate} {Research} {Agendas}},
	shorttitle = {Systemic {Biases} in {Sign} {Language} {AI} {Research}},
	url = {http://arxiv.org/abs/2403.02563},
	abstract = {Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by what decisions are the most convenient or perceived as important to hearing researchers. We end with a call to action: the field must make space for Deaf researchers to lead the conversation in sign language AI.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Desai, Aashaka and De Meulder, Maartje and Hochgesang, Julie A. and Kocab, Annemarie and Lu, Alex X.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02563 [cs]},
	keywords = {notion},
}

@inproceedings{amershi_guidelines_2019,
	address = {Glasgow Scotland Uk},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {https://dl.acm.org/doi/10.1145/3290605.3300233},
	doi = {10.1145/3290605.3300233},
	language = {en},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
	month = may,
	year = {2019},
	keywords = {notion},
	pages = {1--13},
}

@inproceedings{muller_considerations_2023,
	address = {Toronto, Canada},
	title = {Considerations for meaningful sign language machine translation based on glosses},
	url = {https://aclanthology.org/2023.acl-short.60},
	doi = {10.18653/v1/2023.acl-short.60},
	abstract = {Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation. To address these issues, we put forward concrete recommendations for future research on gloss translation. Our suggestions advocate awareness of the inherent limitations of gloss-based approaches, realistic datasets, stronger baselines and convincing evaluation.},
	urldate = {2024-03-21},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Müller, Mathias and Jiang, Zifan and Moryossef, Amit and Rios, Annette and Ebling, Sarah},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	keywords = {notion},
	pages = {682--693},
}

@misc{han_machine_2018,
	title = {Machine {Translation} {Evaluation} {Resources} and {Methods}: {A} {Survey}},
	shorttitle = {Machine {Translation} {Evaluation} {Resources} and {Methods}},
	url = {http://arxiv.org/abs/1605.04515},
	abstract = {We introduce the Machine Translation (MT) evaluation survey that contains both manual and automatic evaluation methods. The traditional human evaluation criteria mainly include the intelligibility, fidelity, fluency, adequacy, comprehension, and informativeness. The advanced human assessments include task-oriented measures, post-editing, segment ranking, and extended criteriea, etc. We classify the automatic evaluation methods into two categories, including lexical similarity scenario and linguistic features application. The lexical similarity methods contain edit distance, precision, recall, F-measure, and word order. The linguistic features can be divided into syntactic features and semantic features respectively. The syntactic features include part of speech tag, phrase types and sentence structures, and the semantic features include named entity, synonyms, textual entailment, paraphrase, semantic roles, and language models. The deep learning models for evaluation are very newly proposed. Subsequently, we also introduce the evaluation methods for MT evaluation including different correlation scores, and the recent quality estimation (QE) tasks for MT. This paper differs from the existing works {\textbackslash}cite\{GALEprogram2009,EuroMatrixProject2007\} from several aspects, by introducing some recent development of MT evaluation measures, the different classifications from manual to automatic evaluation measures, the introduction of recent QE tasks of MT, and the concise construction of the content. We hope this work will be helpful for MT researchers to easily pick up some metrics that are best suitable for their specific MT model development, and help MT evaluation researchers to get a general clue of how MT evaluation research developed. Furthermore, hopefully, this work can also shine some light on other evaluation tasks, except for translation, of NLP fields.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {Han, Lifeng},
	month = sep,
	year = {2018},
	note = {arXiv:1605.04515 [cs]},
	keywords = {notion},
}

@misc{noauthor_neidle-et--2022-signlang-proceedings-2pdf_nodate,
	title = {Neidle-et-al-2022-signlang-proceedings-2.pdf},
	url = {https://open.bu.edu/ds2/stream/?#/documents/434508/page/2},
	urldate = {2024-03-26},
}
