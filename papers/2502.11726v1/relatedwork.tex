\section{Related Works}
\label{sec:Related Works}

This section first reviews objective PCQA approaches and subjective PCQA datasets.
Then, typical rank learning-based IQA methods are discussed.

\subsection{Objective PCQA Methods}
\label{subsec:spcqa}

Objective PCQA methods \citep{liuq22,oufz21} can be classified into geometry-only and general-purpose approaches from the perspective of application scope.
Geometry-only PCQA methods evaluate the geometry quality of point clouds based on geometry attributes alone, whereas general-purpose PCQA approaches take geometry and color attributes into consideration simultaneously.

\subsubsection{Geometry-only PCQA Methods}
\label{subsec:fr_method_details}
Up to now, geometry-only PCQA methods  \citep{Alexioue18,Dumice18,Meynetg20,Yangq22,Dinizr21,liuq22} are all full-reference point-based approaches, which evaluate the geometry quality of point clouds based on geometry attributes in the 3D space directly.
These methods can be classified into Po2Po, Po2Pl, Po2D, and Pl2Pl metrics.

The Po2Po metric measures the degree of distortion by quantifying the Euclidean distances between corresponding points, such as Hausdorff distance (HD) \citep{Lavoueg10}, root mean square error (RMSE) \citep{Javaheria17a}, mean city-block distance (MCD) \citep{Zengj20}, peak signal to noise ratio (PSNR) \citep{Tiand17}, and Chamfer distance (CD) \citep{Zhangdb21}.
The Po2Pl metric \citep{Tiand17} improves over Po2Po by projecting the obtained Po2Po distances along the surface normal direction.%句子润色
The Po2D \citep{Javaheria20a} distance exploits a new type of correspondence between two point clouds and employs the Mahalanobis distance to measure the distance between a point and a distribution of points in a limited point cloud region.
The Pl2Pl metric \citep{Alexioue18icme} is built on the angular similarity of tangent planes that correspond to associated points between the reference and the degraded point cloud.

\subsubsection{General-purpose PCQA Methods}

General-purpose PCQA methods cover point-based and projection-based approaches, which take both geometry and color attributes into consideration.

\textbf{Point-based approaches} predict absolute quality scores by evaluating the quality of point clouds directly in the point space. 
Viola et al. \citep{Violai20} extracted color statistics from both reference and degraded point clouds, and employed color histograms to drive objective metrics.
Meynet et al. \citep{Meynetg20} also selected several geometry-based and color-based features and combined them linearly by logistic regression.
In \citep{Yangq22}, Yang et al. proposed a metric, called GraphSIM, to predict the human perception of colored point clouds with superimposed geometry and color impairments.
They utilized graph signal processing to extract point cloud color gradients to yield robust quality prediction. 
Chai et al. \citep{Chai2024} introduced a potent no-reference point cloud quality evaluation technique, which combines visual and geometric characteristics, leading to a substantial improvement in the precision and applicability of the assessment.
They compared and combined the statistics of color and geometry information of the reference and original point cloud to estimate the perceived quality of the degraded point cloud.
The proposed coloured point cloud based on geometric segmentation and colour transformation (CPC-GSCT) \citep{Hual21} metric employed geometric segmentation and color transformation respectively to construct geometric and color features for estimating the point cloud quality.

In addition to the aforementioned full-reference point-based methods, no-reference point-based approaches only utilize degraded point clouds and do not require information about reference point clouds.
However, there is currently less research on no-reference quality assessment in the 3D space directly.
The proposed blind quality evaluator for colored point cloud based on visual perception (BQE-CVP) method \citep{Hual21cvp} utilized handcrafted features to develop a no-reference quality evaluator.  
It characterized the distortion of distorted point clouds from geometric, color, and joint perspectives.
Liu et al. \citep{Liuyp22} proposed a no-reference metric ResSCNN based on sparse convolutional neural network (SCNN) with residual connections, which is a type of deep learning architecture designed to process grid-like data, to accurately estimate the subjective quality of point clouds.
The proposed ResSCNN adopts a hierarchical feature extraction module to extract both geometry and color attributes of point clouds, which takes the entire point cloud as input.
Shan et al. \citep{Shanzy23} proposed a novel no-reference PCQA metric, called GPA-Net, to attentively extract perturbation of structure and texture via a multi-task graph convolutional network.
They introduced a coordinate normalization module to achieve the shift, scale, and rotation invariance.


\textbf{Projection-based methods}, which have dominated the field of PCQA, strive to project point clouds onto a set of 2-dimensional (2D) planes and then predict absolute quality scores using existing IQA techniques \citep{Zhangzc22, Alexioue19mex, wuxj21, Yangq21, Yang_2022_CVPR, liuq22, wang2024zoom}.

Most existing projection-based methods \citep{Zhangzc22} employ handcrafted features to characterize the distortion of degraded point clouds.
Alexiou et al. \citep{Alexioue19mex} investigated the impact of the number of viewpoints employed to assess the visual quality of point clouds and proposed to assign weights to the projected views based on interactivity information obtained during subjective evaluation experiments.
In \citep{wuxj21}, using a head-mounted display (HMD) with six degrees of freedom, Wu et al. proposed two projection-based objective quality evaluation methods: a weighted view projection based model and a patch projection-based model. 
He et al. \citep{Hezy21} projected the texture information and curvature of colored point clouds onto 2D planes, and extracted texture and geometric statistical features, respectively.
Their method combined both colored texture and curvature projection.
Yang et al.\citep{Yangq21} projected the 3D point cloud onto six perpendicular image planes of a cube to obtain both 2D texture and depth images to represent the photometric and geometric information of the original point cloud.
Then, they utilized features extracted from these images for objective metric development.
Yang et al. \citep{Yang_2022_CVPR} also leveraged the rich prior knowledge from the field of IQA and applied the domain adaptation to point cloud quality assessment.
Liu et al. \citep{liuq22} employed an attention mechanism and a variant of information content-weighted structural similarity to develop a novel objective model, which significantly outperforms existing metrics. 

\begin{table*}[!t]
	\centering
	\caption{Selected publicly accessible PCQA datasets.}
	\label{Tab:existing-dataset}
\resizebox{\textwidth}{!}{
	\begin{tabular}{lllll}
		\hline
		\multicolumn{1}{l}{Types}       & \multicolumn{1}{l}{Datasets} & \multicolumn{1}{l}{Reference samples} & \multicolumn{1}{l}{Distortion type}            & \multicolumn{1}{l}{Distorted samples} \\ \hline
		\multirow{2}{*}{Colorless} & G-PCD \citep{Alexioue17}     & 5   & Octree-puring, Gaussian noise & 40   \\
		& RG-PCD \citep{Alexioue18mex}   & 6   & Octree-puring  & 24  \\ \hline
		\multirow{6}{*}{Colored}   & CPCD 2.0 \citep{Hual21}   & 10    & G-PCC, V-PCC, Gaussian noise& 360    \\
		& SIAT-PCQD \citep{wuxj21}  & 20     & V-PCC   & 340       \\
		& SJTU-PCQA \citep{Yangq21}   & 10     & Octree, downsampling, color and geometry noise & 420    \\
		& WPC \citep{liuq22}   & 20    & Gaussian noise, dowsampling, G-PCC, V-PCC& 740                                   \\
		& LS-PCQA \citep{Liuyp22}     & 104      & G-PCC, V-PCC, color and geometry noise         & 22,568      \\ 
      & BASICS \citep{BASICS2024}     & 75      & V-PCC, G-PCC-Predlift, G-PCC-Raht, GeoCNN         & 1494      \\ 
  \hline
	\end{tabular}}
\end{table*}

Recently, deep learning-based feature extraction methods have also been introduced into projection-based approaches.
Liu et al. \citep{liuq21tcsvt} proposed a deep learning-based no reference point cloud quality assessment method, called PQA-Net, consisting of three modules: a multi-view-based joint feature extraction and fusion module, a distortion type identification module, and a quality vector prediction module.
The entire network is jointly trained for quality prediction. 
Tao et al. \citep{Taowx21} projected 3D point clouds into 2D color projection maps and geometric projection maps and designed a multi-scale feature fusion network to blindly evaluate the visual quality. 
However, the selection of projection directions may significantly influence the overall assessment performance \citep{Yangq22,Liuyp22}.
Consequently, the 3D-to-2D projection cannot characterize the 3D points distribution very well, resulting in information loss.

All in all, the majority of existing PCQA methods are specially designed for colored point clouds, which cannot be directly used for geometry-only quality assessment.
Moreover, current geometry-only PCQA methods are all full-reference and hand-crafted approaches.
In addition, all existing  PCQA methods are dedicated to predicting absolute quality scores of input point clouds.
This paper is dedicated to developing a deep list-wise rank learning framework for no-reference GQA in the 3D space.

\subsection{Subjective PCQA Datasets}

Subjective PCQA datasets are widely used to train, validate, and test learning-based PCQA methods.
Existing datasets can be classified into colored and colorless PCQA datasets, according to the color attribute of their original point clouds, while also containing various types of distortions, such as Gaussian noise, downsampling, geometry-based point cloud compression (G-PCC), and video-based point cloud compression (V-PCC).
Table \ref{Tab:existing-dataset} outlines existing publicly available subjective PCQA datasets.



Colored PCQA datasets, such as CPCD 2.0 \citep{Hual21}, SJTU-PCQA \citep{Yangq21}, SIAT-PCQD \citep{wuxj21}, WPC \citep{liuq22}, LS-PCQA \citep{Liuyp22} and BASICS \citep{BASICS2024}, are built for general-purpose PCQA methods.
These datasets contain numerous forms of colored point cloud (i.e., cultural heritages, computer-generated objects, and human figures) that are degraded by diverse geometric and color distortions \citep{Liuyp22}.
Since the color attribute is involved in the subjective quality evaluation experiment, colored PCQA datasets cannot be directly used for geometry-only PCQA methods. 
This is because these datasets are based on MOS ratings, which reflect human visual perception influenced by color. 
As a result, point clouds with the same geometric structure but different color distortions can yield significantly different MOS scores, making these labels unreliable for evaluating geometric differences alone.
By contrast, only a few small-scale colorless PCQA datasets have been constructed for geometry-only PCQA algorithms, such as G-PCD \citep{Alexioue17} and RG-PCD \citep{Alexioue18mex}.
Among them, the largest RG-PCD dataset only contains 6 reference point clouds and 24 geometrically degraded samples.
Therefore, the number of reference point clouds and included distorted samples in existing colorless PCQA datasets are insufficient for the proposed geometry-only ranked-based PCQA algorithm.
Besides, it is also challenging to collect reliable MOS for a large number of point clouds, since the subjective experiment is time-consuming and expensive. 

To alleviate the lack of large-scale subjective datasets, the concept of pseudo MOS is introduced in the IQA \citep{Wujj20,oufz21} and PCQA \citep{Liuyp22}. 
The pseudo MOS, which differs from the MOS rated by subjective experiments, is automatically calculated utilizing selected existing quality assessment algorithms (i.e., using the high-performance full-reference metrics) based on well-designed annotation criteria.
The reliability of the pseudo MOS 
 \citep{Liuyp22} can been verified via conducting various validation experiments.

\subsection{Rank Learning-based Image Quality Assessment}

Rank learning refers to learning a function that can predict the ranking list of a given set of stimuli \citep{Liuty11,Guojf20}, which has been widely used in IQA, since Gao et al. \citep{Gaof15} firstly proposed to apply rank learning to IQA.
Rank learning-based IQA methods aim to rank images instead of assigning absolute quality scores, which can be roughly classified into pairwise-based and list-wise-based methods.

\begin{figure*}[tbp]
	\centering
	\includegraphics[width=0.9\textwidth]{Tex/samples2.eps}
	\caption{Snapshots of some reference point clouds in the LRL dataset.}
	\label{fig:samples}
\end{figure*}

\textbf{Pairwise-based approaches} assume that the relative quality order between two images can be inferred from other ground-truth formats. 
They focus on optimizing the relative quality preferences between images by minimizing the number of misclassified instance pairs.
Gao et al. \citep{Gaof15} first explored and exploited preference image pairs and utilized natural scene statistics based features to train a robust regression model, in which the preference label represents the relative quality of two images.
In \citep{Mal16}, Ma et al. extracted global image scene textures (GIST) features and resorted to the pairwise rank learning approach to discriminate the perceptual quality between the retargeted image pairs.
Hu et al. \citep{Hub19} proposed to employ DoG-based structure representation, where difference of Gaussian (DoG) is used to decompose the restored image into a set of DoG signals at various octaves and scales, spatial-domain local binary pattern (LBP) feature extraction and frequency-domain log-Gabor feature extraction to extract and concatenate feature vectors to predict the relative quality ranking of restored images.
These methods assess the quality of distorted images via artificially designed unnaturalness expression, which often fail to capture distorted artifacts.
Recently, the siamese network \citep{Liuxl17,Mengxd21,Fuzq22} has been introduced to design deep rank learning-based IQA methods.
They adopt a siamese deep architecture, which takes a pair of degraded images as input and outputs their rank order. 
Each branch is designed to capture deep features of a degraded image using deep neural networks, such as CNN.

\textbf{List-wise-based methods} target constructing loss functions that directly reflect the model's final performance in ranking. 
Gu et al. \citep{Guj19} formulated the no-reference IQA as a recursive list-wise ranking problem that directly learns to rank a list of images with implicit quality measures.
Ou et al. \citep{oufz21} proposed a controllable list-wise ranking loss function to train a CNN by setting an upper and lower bound of rank range and introducing an adaptive margin to tune rank interval. 
They also designed a novel imaging-heuristic approach to generate the rank image samples.
In \citep{Wangxj22}, Wang et al. explored the perceptual quality-related factors and designed a modified list-wise ranking algorithm to achieve a more consistent evaluation with 3D perception and image degradation mechanism in the stereoscopic image re-targeting process.

Inspired by existing list-wise-based IQA methods, this paper reformulates the no-reference GQA as a list-wise ranking problem, which directly learns to rank a list of point clouds with implicit quality measures.