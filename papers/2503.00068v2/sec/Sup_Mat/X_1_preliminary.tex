\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{images/overall_pipeline.pdf}
  \caption{Our data flow includes three stages: (1) pre-train: knowledge distillation-based cross-modal pre-training; (2) train \& evaluation: train the PI-HMR network with pressure sequences; (3) post-process: improve the estimates with the Test-Time Optimization strategy.} 
  % \vspace{-0.5cm}
  \label{fig: sup_overall_pipeline}
\end{figure*}

\section{Introduction}
In this material, we provide additional details regarding the network and implementation of our methods, as well as compared SOTAs. We further present more qualitative results to show the performance of PI-HMR and our re-generated p-GTs for TIP~\cite{wu2024seeing} and to explore their failure scenarios. The details include:
\begin{itemize}
    \item Implementation details for SMPLify-IB, PI-HMR, cross-modal knowledge distillation, VQ-VAE, test-time optimization, and SOTA methods compared to PI-HMR.
    \item More quantitative and qualitative results about SMPLify-IB, PI-HMR, and failure cases.
    \item Limitations and future works.
\end{itemize}

The overall pipeline of our pressure-to-motion flow is shown in~\cref{fig: sup_overall_pipeline}, and detailed architecture and implementation details will be elaborated below.

\section{Preliminary}

\textbf{Body Model.} The SMPL~\cite{loper2023smpl} model provides a differentiable function $V = \mathcal{M}(\theta, \beta, t)$ that outputs a posed 3D mesh with $N=6890$ vertices. The pose parameter $\theta \in \mathbb{R}^{24 \times 3}$ includes a $\mathbb{R}^3$ global body rotation and the relative rotation of 23 joints with respect to their parents. The shape parameter $\beta \in \mathbb{R}^{10}$ represents the physique of the body shape. And $t \in \mathbb{R}^3$ means the root translation w.r.t the world coordinate.

\section{Network and Implementation Details}

\subsection{Implementation details for SMPLify-IB}

\label{sec:appendix_section}
\subsubsection{The first stage}
In the first stage of our optimization algorithm, we jointly optimize body shape $\beta$, pose parameters $\theta$, and translation $t$ using a sliding-window~(set as 128) approach, with overlap~(set as 64) between adjacent windows. We minimize the following objective function:
\begin{align}
L_{s1}(\theta, \beta, t)&=\lambda_{J}\mathcal{L}_{J}+
\lambda_{p}\mathcal{L}_{p}+\lambda_{sm}\mathcal{L}_{sm}+\lambda_{cons}\mathcal{L}_{cons}\nonumber\\
&\quad+\lambda_{bc}\mathcal{L}_{bc}+\lambda_{g}\mathcal{L}_{g}+\lambda_{sc}\mathcal{L}_{\sc}
\end{align}

1. \textbf{Reprojection constraint term $\mathcal{L}_{J}$:} This term penalizes the weighted robust distance between the projections of the estimated 3D joints and the annotated 2D joint ground truths. Instead of the widely used weak-perspective projection in~\cite{bogo2016keep} with presumed focal length, we apply the perspective projection with calibrated focal length and camera-bed distance provided by TIP.

2. \textbf{Prior constraint term $\mathcal{L}_{p}$:} This term impedes the unrealistic poses while allowing possible ones. $\mathcal{L}_{pose}$, $\mathcal{L}_{shape}$ penalizes the out-of-distribution estimated postures and shapes, which is similar to terms in SMPLify, and $\mathcal{L}_{torso}$ ensures correct in-bed torso poses, where the height of hips should be less than shoulders and the height of waist is below the mean height of shoulders and hips.
\begin{align}
\mathcal{L}_{p}&= \mathcal{L}_{pos} + \mathcal{L}_{sha} + \mathcal{L}_{tor} \\
\mathcal{L}_{pos} &= \sum^{T}_{i}(\lambda^{pos}_{1} (\mathcal{G}(\theta(i)) + \sum_j \lambda^{pos}_{2, j} \cdot e^{\gamma_j \cdot \theta(i)_j})) \nonumber \\
\mathcal{L}_{sha} &= \lambda^{sha}_{} \sum^{T}_{i}||\beta(i)||^2 \nonumber \\
\mathcal{L}_{tor} &= \sum^T_i (\lambda^{tor}_{1} \cdot e^{\omega_{hip}d_{hip}(i)}+ \lambda^{tor}_{2} \cdot e^{\omega_{wai}d_{wai}(i)}) \nonumber \\
d_{hip}(i) &= z_{hip}(i) - z_{sho}(i) \nonumber \\
d_{wai}(i) &= z_{wai}(i) - mean(z_{hip}(i), z_{sho}(i)) \nonumber
\end{align}
where $\mathcal{G}$ is the Gaussian Mixture Model pre-trained in SMPLify, and the second term in $\mathcal{L}_{pos}$ penalizes impossible bending of limbs, neck and torso, such as shoulder twist.  $z_{hip}$, $z_{sho}$, $z_{wai}$ are the height of hip joints, shoulder joints, and waist joint, and $\omega_{hip}$, $\omega_{wai}$ are both set to 100.

3. \textbf{Smooth constraint term $\mathcal{L}_{sm}$:} This term reduces the jitters by minimizing the 3D joints velocity, acceleration and SMPL parameter differences.
\begin{align}
\mathcal{L}_{smo}&=\mathcal{L}_{par}+ \mathcal{L}_{vel}+\mathcal{L}_{acc}  \\
\mathcal{L}_{par}&=\sum^{T-1}_{i=1} (\lambda^{par}_{1} ||\beta (i+1)-\beta (i)||^2 \nonumber \\
&\quad+ \lambda^{par}_{2} ||\theta (i+1)-\theta (i)||^2+\lambda^{par}_{2} ||t (i+1)-t(i)||^2) \nonumber \\
\mathcal{L}_{vel}&=\sum^{T-1}_{i=1}(\lambda^{vel}_{1} ||J(i+1)_{3D}-J(i)_{3D}||^2  \nonumber\\
&\quad+ \lambda^{vel}_{2} ||V(i+1)-V(i)||^2)  \nonumber \\
\mathcal{L}_{acc}&=\sum^{T-1}_{i=2} ||2J(i)_{3D}-J(i-1)_{3D}-J(i+1)_{3D}||^2 \nonumber
\end{align}
where $V(i)$ and $J(i)$ are the coordinates of SMPL vertex set $V$ and 3D joints $J$ in the frame $i$. 

4. \textbf{Consistency constraint term $\mathcal{L}_{cons}$:} This term enhances the consistency between the overlapping parts of the current window and the previously optimized window.
\begin{align}
\mathcal{L}_{cons}&=\sum_{\substack{i\in overlap \\ frames}} (\lambda^{cons}_{1}||\theta(i, b_{1})-\theta(i, b_{2})||^2  \nonumber\\
&\quad+\lambda^{cons}_{2} ||t(i, b_{1})-t(i, b_{2})||^2  \nonumber \\
&\quad+\lambda^{cons}_{3} ||V(i, b_{1}) - V(i, b_{2})||^2 \nonumber \\
&\quad+\lambda^{cons}_{4} ||J(i, b_{1})_{3D} - J(i, b_{2})_{3D}||^2) 
\end{align}
where $t(i, b)$, $\theta(i, b)$ is the translation parameters and pose parameters in frame $i$ of window $b$, and  $V(i, b)$, $J(i, b)_{3D}$ is the coordinates of vertex set $V$, 3D joints $J$ in frame $i$ of window $b$. $b_1$, $b_2$ means the previous window and the present window, respectively.

5. \textbf{Bed contact constraint term $\mathcal{L}_{bc}$:} This term improves the plausibility of human-scene contact. We consider vertices that are close to the bed to be in contact with bed and encourage those vertices to contact with the bed plane while penalizing human-bed penetration.
\begin{align}
\mathcal{L}_{bc} &= \sum^{T}_{i} (\lambda^{in\_bed}_{}  \sum_{0<z(i)_{v}<thre_{bed}} \text{tanh}^2(\omega_{in\_bed} z(i)_v) \nonumber\\ 
&\quad+ \lambda^{out\_bed}_{} \sum_{z(i)_{v}<0} \text{tanh}^2(-\omega_{out\_bed} z(i)_{v})) 
\end{align}
where $z(i)_v$ is the signed distance to the bed plane of vertex $v$ in frame $i$, and $thre_{bed}$ is the contact threshold and set to 0.02m.

6. \textbf{Gravity constraint term $\mathcal{L}_{g}$:} This term penalizes abnormal limb-lifting and reduces depth ambiguity.
\begin{align}
\mathcal{L}_{g}&=\sum^{T}_{i}\sum_{\substack{j\in G_{J}\\z(i)_j>0}}\mathbb{I}(vel(i)_{j}< thre_{vel})e^{\omega(i)_{j}(z(i)_j)} 
\end{align}

where $thre_{vel}$ is set to $\sqrt{110}$, and $vel(i)_j$ denotes the velocity of joint $j$ in frame $i$, which is calculated from 2D annotations. $\omega(i)_j$ is a dynamic weight depends on the state of annotated 2D joint ground truths. Specifically, in addition to the velocity-based criterion, we have more complicated settings for potential corner cases. For example, when a person is seated on the bed, supporting the bed surface with both hands, the shoulders will be incorrectly judged as implausible lifts by sole velocity-based criterion~(This scenario is rarely encountered in TIP, yet it still exists). In that case, we alleviate the impact of gravity constraints on this scenario by dynamically adjusting $\omega(i)_j$. In practice, when the 2D projection lengths of limbs are less than 60\% of the projection lengths in the rest pose, according to geometry, we consider the corresponding limb to be normally lifted even if the corresponding joint speed is below $thre_{v}$, and thus $\omega(i)_j$ takes a smaller value. Besides, $\omega(i)_j$ takes a smaller value for hand joints whose 2D projections are inside the torso to avoid severe hand-torso intersection.

7. \textbf{Self-contact constraint term $\mathcal{L}_{sc}$:} This term is proposed to obtain plausible self-contact and abbreviate self-penetration. In the first stage, we only deal with the intersection between the hand and the torso. The self-contact between other body parts is optimized in the second stage.
\begin{align}
\mathcal{L}_{sc}&=\lambda^{p\_con}_{} \mathcal{L}_{p\_con}+ \lambda^{p\_isect}_{} \mathcal{L}_{p\_isect}+ \lambda^{pull}_{} \mathcal{L}_{pull}  \nonumber \\
&\quad+ \lambda^{push}_{} \mathcal{L}_{push}  \\
\mathcal{L}_{p\_con}&=\sum_{\substack{0<sdf_v<thre_{dist}}} \text{tanh}^2(\omega_{p\_con}sdf_v) \nonumber \\
\mathcal{L}_{p\_isect}&=\sum_{sdf_v<0} \text{tanh}^2(\omega_{p\_isect}|sdf_v|) \nonumber
\end{align}
where $sdf_v$ is the value of the signed distance field(SDF) at vertex $v$, which is calculated by our self-penetration detection algorithm. The details of $\mathcal{L}_{pull}$, $\mathcal{L}_{push}$ are given in the main body of the manuscript.

\subsubsection{The second stage}
We treat the results of the first stage as initialization for the second stage. Specifically, we use the mean $\beta$ of each subject and fix the shape parameters in the second stage. We optimize $\theta$ and $t$ to obtain more plausible human meshes. The objective function $L_{s2}$ is as follows:
\begin{align}
L_{s2}(\theta, \beta, t)&=\lambda_{J}\mathcal{L}_{J}+
\lambda_{p}\mathcal{L}_{p}+\lambda_{sm}\mathcal{L}_{sm}+\lambda_{cons}\mathcal{L}_{cons}\nonumber\\
&\quad+\lambda_{bc}\mathcal{L}_{bc}+\lambda_{g}\mathcal{L}_{g}+\lambda_{sc}\mathcal{L}_{sc}
\end{align}
$\mathcal{L}_J$, $\mathcal{L}_p$, $\mathcal{L}_{sm}$, $\mathcal{L}_{cons}$, $\mathcal{L}_{g}$, $\mathcal{L}_{bc}$ are the same as the first stage, while $L_{sc}$ penalizes self-intersection in all body segments rather than only hands and torso.

\subsubsection{Implementation details}
We use Adam as the optimizer with a learning rate of 0.01, and each stage involves 500 iterations. The length of the sliding window is 128, with 50\% overlapping to prevent abrupt changes between windows. The joints and virtual joints we use for the segmentation of SMPL mesh is listed in~\cref{tab:opt_SEG}.
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\begin{table}[]
\centering
\begin{tabular}{m{7em}|m{15em}}
\toprule[2pt]
body parts & joints \& virtual joints \\
\midrule[1.2pt]
head & left ear, right ear, nose \\
\hline
torso-upper arm & left shoulder, right shoulder, spine2 \\
\hline
\par left arm & left elbow, left hand, \par mid of left elbow and hand, \par $\frac{2}{5}$ point from left elbow to shoulder \\
\hline
\par right arm & right elbow, right hand, \par mid of right elbow and hand, \par $\frac{2}{5}$ point from right elbow to shoulder \\
\hline
torso-thigh & left hip, right hip \\
\hline
\par left thigh & left knee, left ankle, \par mid of left knee and ankle, \par $\frac{2}{5}$ point from left ankle to hip \\
\hline
\par right thigh & right knee, right ankle, \par mid of right knee and ankle, \par$\frac{2}{5}$ point from right ankle to hip \\
\bottomrule[2pt]
\end{tabular}
\caption{\textbf{Positions of our selected segment centers}.}
\label{tab:opt_SEG}
\end{table}

\subsection{Implementation details for PI-HMR}

Before the aforesaid modules in the main body of our manuscript, PI-HMR also contains three different Transformer blocks for AttentionPooling, cross-attention for sampling features in MFF, and temporal consistency extraction. We will provide detailed designs of these Transformer layers.  (1) For AttentionPooling, we use the same structure in CLIP~\cite{radford2021learning}. (2) For the cross-attention module in MFF, we apply a one-layer Transformer block as the attention module with one attention head and  Dropout set as 0. (3) For the temporal encoder, we apply a two-layer Transformer block to extract the temporal consistency from the fusion feature sequence. In detail, each transformer layer contains a multi-head attention module with $N=8$ heads. These learned features are then fed into the feed-forward network with 512 hidden neurons. Dropout~($p=0.1$) and DropPath~($p_d=0.2$) are applied to avoid overfitting.

The loss of PI-HMR is defined as:
\begin{equation} \label{eq: sup_overall_func}
\small
   \mathcal{L}_{pi} = \lambda_{\text{SMPL}} \mathcal{L}_{\text{SMPL}} + \lambda_{3D} \mathcal{L}_{3D} + \lambda_{2D} \mathcal{L}_{2D} 
\end{equation}
where $\mathcal{L}_{\text{SMPL}}$, $\mathcal{L}_{3D}$, $\mathcal{L}_{2D}$ are calculated as:

\begin{equation}
\mathcal{L}_{\text{SMPL}} = \omega_{s}^{\text{SMPL}} ||\beta - \hat{\beta}||^2 + \omega_{p}^{\text{SMPL}} ||\theta - \hat{\theta}||^2  + \omega_{t}^{\text{SMPL}} ||t - \hat{t}||^2 \nonumber 
\end{equation}

\begin{equation}
\mathcal{L}_{3D} = || J_{3D} - \hat{J}_{3D} ||^2 \nonumber 
\end{equation}

\begin{equation}
\mathcal{L}_{2D} = || J_{2D} - \hat{J}_{2D} ||^2 \nonumber 
\end{equation}
where $\hat{x}$ represents the ground truth for the corresponding estimated variable $x$, and $\lambda$ and $\omega$ are hyper-parameters. We set $\lambda_{\text{SMPL}}=1$, $\lambda_{3D}=300$, $\lambda_{2D}=0.5$, $\omega_{t}^{\text{SMPL}}=\omega_{p}^{\text{SMPL}}=60$, and $\omega_{s}^{\text{SMPL}}=1$ for PI-HMR's training.

Before training, we first pad pressure images to $64 \times 64$ and set $T=15$ as the sequence length. No data augmentation strategy is applied during training. During the training process, we train PI-HMR for 100 epochs with a batchsize of 16, using the AdamW optimizer with a learning rate of 3e-4 and weight decay of 5e-3. We adopt a warm-up strategy in the initial 5 epochs and schedule periodically in a cosine-like function as~\cite{wu2024seeing}. The weight decay is set to 5e-3 to abbreviate overfitting. All implementation codes are implemented in the PyTorch 2.0.1 framework and run on an RTX4090 GPU. 

\subsection{Implementation details for cross-modal KD}\label{sec:sup_imp_kd}
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_KD.pdf}
  \caption{\textbf{An overview of our KD-based network.}} 
  % \vspace{-0.5cm}
  \label{fig: sup_kd_structure}
\end{figure*}
We conduct a HMR-based network~(with a ResNet50 as encoder and an IEF~\cite{kanazawa2018end} SMPL regressor) to pre-train the ResNet50 encoder with SOTA vision-based method Cliff~\cite{li2022cliff}. The detailed structure is presented in~\cref{fig: sup_kd_structure} where we concurrently introduce label supervision, as well as distillation from Cliff's latent feature maps and prediction outcomes, to realize cross-modal knowledge transfer.

To train the KD-based network, like PI-HMR, we first pad pressure images to $64 \times 64$. No data augmentation strategy is applied during training. The training process is performed for 100 epochs with an AdamW optimizer in a minibatch of $256$ on the same training and validation dataset of PI-HMR. We adopt a warm-up strategy in the initial 5 epochs and schedule periodically in a cosine-like function. The weight decay is set to 5e-3 to abbreviate overfitting. All implementation codes are implemented in the PyTorch 2.0.1 framework and run on an NVIDIA. RTX4090 GPU. 

\subsection{Implementation details for VQ-VAE}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_vqvae.pdf}
  \caption{\textbf{An overview of our VQ-VAE network.}} 
  % \vspace{-0.5cm}
  \label{fig: sup_kd_structure}
\end{figure}
The VQ-VAE follows the architecture in~\cite{feng2024stratified}, which incorporates two 4-layer Transformer blocks as the encoder and decoder, respectively, and a $\mathbb{R}^{512 \times 384}$ codebook with 512 entries and $\mathbb{R}^{384}$ for the discrete latent of each entry. Each Transformer layer consists of a 4-head self-attention module and a feedforward layer with 256 hidden units.

To train the VQ-VAE network, we only input the pose parameter sequence $\Theta=\{\theta_1, ..., \theta_T\}$, without the translation and shape parameters, to push the model learning the motion continuity of the turn-over process. The pose sequences will firstly be encoded to motion features $H$ in the Transformer encoder, quantized into discrete latent sequence $Z$ by finding its closest element in the codebook, and reconstruct the input motion sequence in the follow-up Transformer decoder. We follow the loss setting in~\cite{feng2024stratified} and minimize the following loss function in~\cref{eq: sup_loss_vqvae}.

\begin{align} \label{eq: sup_loss_vqvae}
\mathcal{L}_{vq} = \lambda^{vq}_{\theta}&\text{Smooth}_{L1}(\Theta, \hat{\Theta}) \\
&\quad+ \lambda^{vq}_{J}\text{Smooth}_{L1}(J_{3D}(\Theta), J_{3D}(\hat{\Theta})) \nonumber\\
&\quad+ \lambda^{vq}_{d}(||sg[Z] - H||_2 + \omega^{vq}_{b} ||Z - sg[H]||_2) \nonumber
\end{align}
where $\hat{x}$ represents the ground truth for the corresponding estimated variable $x$, $\mathcal{J}(\Theta)$ means 3D joint locations of given SMPL pose parameter sequences $\Theta$~($\beta$ and $t$ are default all-0 tensors), $sg$ denotes the stop gradient operator, and $\lambda$ and $\omega$ are hyper-parameters. We set $\lambda^{vq}_{\theta}=1$, $\lambda^{vq}_{J}=5$, $\lambda^{vq}_{d}=0.25$, and $\omega^{vq}_{b}=0.5$. 

The VQ-VAE is trained with a batchsize of 64 and a sequence length of 64 frames for 100 epochs on the same training and validation dataset of PI-HMR. Adam optimizer is adapted for training, with a fixed learning rate of 1e-4, and [0.9, 0.999] for $\beta$ of the optimizer. All implementation codes are implemented in the PyTorch 2.0.1 framework and run on an NVIDIA. RTX4090 GPU. 

\subsection{Implementation details for Test-Time Optimization}

We use the VQ-VAE to act as the only motion prior and supervision in our TTO routine. For terminological convenience, given a VQ-VAE $\mathbb{M}$ and PI-HMR initial predictions~$\Theta^0 = \{\theta^0_1, ... \theta^0_T\}$. For the $i_{th}$ iteration, we calculate the loss by~\cref{eq: sup_tto} and update the $\Theta$ by stochastic gradient descent. The result of $i_{th}$ iteration will be input into $\mathbb{M}$ and optimized in the ${i + 1}_{th}$ iteration:

\begin{equation} 
    \mathcal{L}_{TTO}^i = \alpha \mathcal{L}_{m}(\Theta^i, \Theta^0) + (1 - \alpha)\mathcal{L}_{m}(\Theta^i, \mathbb{M}(\Theta^i)) + \mathcal{L}_{sm}(\Theta^i)
\label{eq: sup_tto}
\end{equation}
where each term is calculated as:

\begin{align} 
 \mathcal{L}_{m}(\Theta_1, \Theta_2) =  \lambda_{\text{smpl}}^{TTO}  ||\Theta_1, \Theta_2||^2 \\
&+ \lambda_{3D}^{TTO} ||\mathcal{J}(\Theta_1) - \mathcal{J}(\Theta_2)||^2 \nonumber
\end{align}

\begin{equation}
\begin{split}
    \mathcal{L}_{sm}(\Theta) = \lambda_{sm}^{TTO} \frac{1}{T-1} \sum_{t=2}^{T} (|\Theta(t) - \Theta(t-1)| \\
    + |\mathcal{J}(\Theta(t)) - \mathcal{J}(\Theta(t-1)| \nonumber
\end{split}
\end{equation}
where $\mathcal{J}(\Theta)$ means 3D joint locations of given SMPL pose parameters $\Theta$~($\beta$ and $t$ are the initial predictions and won't be updated during the optimization), $\alpha$ is a balance weight to balance initial PI-HMR predictions and reconstructions of VQ-VAE, and $\lambda$s are hyperparameters. We set $\alpha=0.5$, $\lambda_{\text{smpl}}^{TTO}=0.5$, $\lambda_{3D}^{TTO}=0.1$, and $\lambda_{sm}^{TTO}=0.1$ for the test-time optimization.

During the optimization, we freeze the shape parameters $\beta$ and translation parameters $t$ of the initial PI-HMR's outputs, and only optimize pose parameters $\theta$. We employ a sliding window of size 64 to capture the initial PI-HMR predictions and  update them in 30 iterations with a learning rate of 0.01 and Adam as the optimizer. All optimization codes are implemented in the PyTorch 1.11.0 framework and run on an NVIDIA. RTX3090 GPU.

\subsection{Implementation details for SOTA methods}

In this section, we will provide implementation details of compared SOTA networks.

\textbf{HMR~\cite{kanazawa2018end} and HMR + KD}: The implementation details of HMR series are introduced in~\cref{sec:sup_imp_kd}. The distinction between the two lies in whether knowledge distillation supervision is employed during the training process.

\textbf{TCMR~\cite{choi2021beyond} and MPS-NET~\cite{wei2022capturing}}: We choose TCMR and MPS-NET as the compared vision-based architecture because they follow the same paradigm of VIBE~\cite{kocabas2020vibe}, which incorporates a static encoder for texture feature extraction, a temporal encoder for temporal consistency digestion, and a regressor for final SMPL predictions. We use the same architecture and loss weights of the default setting, except converting the initial ResNet50 input to a single channel and adjusting the first convolution layer's kernel size to $5\times 5$ to fit the single-channel pressure images.

\textbf{PI-Mesh~\cite{wu2024seeing}}: PI-Mesh is the first-of-its-kind temporal network to predict in-bed human motions from pressure image sequences. We follow the codes and implementation details provided in~\cite{wu2024seeing} with a ResNet50 as the static encoder and a two-layer Transformer block as the temporal encoder.  

\textbf{BodyMAP-WS}: BodyMap~\cite{tandon2024bodymap} is a SOTA dual-modal method to predict in-bed human meshes and 3D contact pressure maps from both pressure images and depth images. We realize a substitute version provided in their paper, named BodyMap-WS, because we don't have 3D pressure map labels. It is worth mentioning that we notice the TIP dataset fails to converge on the algorithm provided in their GitHub repository. So we remove part of the codes including rotation data augmentation and post-processing of the network outputs~(Line 139-150 and Line 231-242 in the \textit{PMM/MeshEstimator.py} of the GitHub repository) to ensure convergence. 

All methods are trained on the same training-validation dataset of PI-HMR. For TCMR, MPS-NET, and PI-Mesh, we adopt the same training routine as PI-HMR. To be specific, we first pad pressure images to $64 \times 64$ and set $T=15$ as the sequence length. No data augmentation strategy is applied during training. During the training process, we train these approaches for 100 epochs with a batchsize of 16, using the AdamW optimizer with the learning rate of 3e-4 and weight decay of 5e-3~(we firstly conduct a simple grid-search for the best learning rate selection on these methods), and adopt a warm-up strategy in the initial 5 epochs and scheduled periodically in a cosine-like function. For BodyMap-WS, we follow the training routine provided in~\cite{tandon2024bodymap}, resize the pressure images to $224\times224$, and apply RandomAffine, RandomCutOut, and PixelDropout as data augmentation strategies. The training process is performed for 100 epochs with an Adam optimizer in a minibatch of $32$, a learning rate of 1e-4 and weight decay of 5e-4. All codes are implemented in the PyTorch 2.0.1 framework and run on an NVIDIA. RTX4090 GPU.


\section{More ablations}

\subsection{Discussion on TopK sampling} The sampling functions as a low-value filter, freeing the model's attention from redundant, noisy backgrounds and focusing more on high-value regions. We provide a visualization in~\cref{fig: topk_sampling}, where, with 128 points, the pressure image can retain the human's outline while highlighting the core contact areas.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{images/exp_selection_principle.pdf} 
  \caption{\textbf{Visualization of TopK Sampling.}} 
  % \vspace{-0.5cm}
  \label{fig: topk_sampling}
\end{figure}

\subsection{Comparisons with single-input models} For vision methods, single-image models usually exhibit lower MPJPE compared to temporal models~(\eg CLIFF vs PMCE). However, for pressure data, temporal models show superiority, likely due to their ability to leverage temporal context, mitigating information ambiguity. This implies the strength of temporal models in pressure data processing compared to single ones. For fair comparisons, we implemented a single-input-based PI-HMR, achieving a 62.01mm MPJPE~(71.48mm for BodyMAP-WS), showing the efficacy of our architecture framework.


\subsection{Results on the original TIP dataset}
The results are shown in~\cref{tab: results_on_o_tip}, which demonstrate a comparable magnitude of MPJPE reduction, proving the efficacy of PI-HMR.

\begin{table}[t] 
\small
\centering
\begin{tabular}{l|c|c|c}
\hline
Method      & TCMR  & PI-Mesh & PI-HMR\\
\hline
MPJPE/ACC-ERR & 67.9/14.6 & 79.2/18.2 & \textbf{68.38/5.24}\\
\hline
\end{tabular} 
\caption{\textbf{Quantitative results on the original TIP dataset.}} \label{tab: results_on_o_tip} 
\end{table}

\subsection{Ablations of TTO.} 
We conducted ablations involving the selection of the balance weight $\alpha$ in~\cref{tab: ablations_alpha} and the number of iterations in~\cref{tab: ablations_iters}. We also explored integrating the pre-trained VQ-VAE into PI-Mesh during training~(as it regresses the sequence rather than the mediate frame, making it suitable for VQ-VAE) and calculating the reconstruction loss. However, MPJPE drops limitedly (0.06mm). We will explore more potential methods~(\eg SPIN-like) in the future work.

\begin{table}[] 
\centering
\begin{tabular}{l|ccccc}
\hline
$\alpha$      & 0.1  & 0.3 & 0.5 & 0.7 & 0.9\\
\hline
MPJPE      &   56.94  &   55.93   &   55.50   & \textbf{ 55.43}   &   55.67  \\
\hline
\end{tabular}
\caption{\textbf{Ablations on balance weight $\alpha$.}} 
\label{tab: ablations_alpha}
\end{table}

\begin{table}[] 
\centering
\begin{tabular}{l|ccccc}
\hline
iters      & 10  & 30 & 50 & 70 & 90\\
\hline
MPJPE      &  56.14   &  55.50    &   55.25   &  55.15   &   \textbf{55.10 } \\
\hline
\end{tabular}
\caption{\textbf{Ablations on the number of iterations.}} 
\label{tab: ablations_iters}
\end{table}

\subsection{Generalization of SMPLify-IB on the SLP dataset.}  
We implemented SMPLify-IB on the SLP dataset. Results show the 2D MPJPE drops from 37.6 to 6.9 pixels compared to Cliff's outputs. \cref{fig: ib_slp} shows our pros in alleviating depth ambiguity. Meanwhile, we observed limb distortions in the optimization results, which may stem from erroneous initial estimations (CLIFF exhibits notable domain adaptation issues in an in-bed scene). In the absence of temporal context, these mis-predictions could exacerbate the likelihood of unreasonable limb angles, underscoring the significance of temporal information in in-bed human shape annotations.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/ib_on_slp_1.pdf}
  \caption{\textbf{Visualizations of SMPLify-IB on SLP.}} 
  \label{fig: ib_slp}
\end{figure}



\section{Visualization results}

In this section, we present additional visualization results to verify the efficiency of our general framework for the in-bed HPS task.
\subsection{Visualizations for Time Consumption of self-penetration algorithms}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_smplify_time.pdf}
  \caption{\textbf{Time consumption when deploying the two self-penetration detection and computation algorithms in our optimization routine.} We count the time taken in an optimization stage with 500 iterations on a single batch (128 frames) and document the proportion of time spent by the self-penetration modules in the overall duration~(in deep blue).} 
  % \vspace{-0.5cm}
  \label{fig: sup_smplify_time}
\end{figure}
\cref{fig: sup_smplify_time} provides quantitative comparisons on time consumption of our optimization routines with SOTA self-penetration algorithm~(Self-Contact in SMPLify-XMC~\cite{muller2021self}) and our proposed light-weight approach~(downsample 1/3 version). The experiment is conducted on a NVIDIA. 3090 GPU, with each optimization performing with 500 iterations on a single batch (128 frames). While the Self-Contact algorithm yields high detection accuracy, it comes at a significant time and computational expense~(\ie, nearly 100s per frame on a RTX3090 GPU). Our detection module brings nearly 450 times speed while archiving comparable self-penetration refinement.

\subsection{Visualizations for gravity-based constraints.}
~\cref{fig: sup_smplify_gravity} provides more visual evidence on the efficiency of our gravity constraints in SMPLify-IB. Traditional single-view regression-based method~(yellow meshes by Cliff) and optimization-based method~(red meshes by a SMPLify-like approach adopted in TIP) face serious depth ambiguity in the in-bed scene, especially when limbs overlap from the camera perspective, thus leading to implausible limb lifts~(\eg, hand lifts in the first and third rows in~\cref{fig: sup_smplify_gravity}, and leg lifts when legs contact and overlap in the third row). Our proposed gravity constraints, accompanied by a strong self-penetration detection and penalty term, effectively alleviate the depth ambiguity issue while maintaining reasonable contact. This validates the feasibility of alleviating depth ambiguity issues with physical constraints in specific scenarios.
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/sup_smplify_gravity.pdf}
  \caption{\textbf{Qualitative comparisons on the p-GTs generated by Cliff~(predicted on images), TIP and our generations by SMPLify-IB.} We highlight the implausible limb lifts by single-view depth ambiguity in red ellipses and our refinement with yellow ellipses.} 
  % \vspace{-0.5cm}
  \label{fig: sup_smplify_gravity}
\end{figure}

\subsection{Failure cases for SMPLify-IB}

About 1.6\% samples of our optimization results might fail due to severely false initialization by CLIFF, wrong judgment in gravity constraints, and trade-offs in the multiple-term optimization, as presented in~\cref{fig: sup_smplify_failure_case}. Thus we manually inspected all generated results and carried out another round of optimization to address these errors, aiming at generating reliable p-GTs for the TIP dataset. The refinement is highlighted with yellow ellipses in~\cref{fig: sup_smplify_failure_case}. 
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{images/sup_smplify_failure_case.pdf}
  \caption{\textbf{Typical failure cases of SMPLify-IB.} We highlight the wrong generations with red markers and our refinement in the yellow ellipses.} 
  % \vspace{-0.5cm}
  \label{fig: sup_smplify_failure_case}
\end{figure}

\subsection{Failure cases for PI-HMR}
In~\cref{fig: sup_pihmr_failure_case}, we show a few examples where PI-HMR fails to reconstruct reasonable human bodies. The reason mainly falls in the information ambiguities, ranging from (a) PI-HMR mistakenly identifies the contact pressure between the foot and the bed as originating from the leg~(shown in the red ellipse), (b) hand lifts and (c) leg lifts.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_pihmr_failure_case.pdf}
  \caption{\textbf{Typical failure cases of PI-HMR.} We highlight the mispredictions and corresponding pressure regions with red markers.} 
  % \vspace{-0.5cm}
  \label{fig: sup_pihmr_failure_case}
\end{figure}

\subsection{More Qualitative Visualizations}

We present more qualitative visualizations on the performance of our proposed optimization strategy SMPLify-IB in~\cref{fig: sup_simplify_quali} and PI-HMR in~\cref{fig: sup_pihmr_quali} and~\cref{fig: sup_pihmr_quali_2}.

\section{Limitations and Future works}

we conclude our limitations and future works in three main aspects:

(1) \textbf{Hand and foot parametric representations:} More diverse and flexible tactile interactions exist in the in-bed scenarios. For instance, the poses of the hands and feet vary with different human postures, thereby influencing the patterns of localized pressure. However, the SMPL model fails to accurately depict the poses of hands and feet, thereby calling for more fine-grained parametric body representations~\cite{pavlakos2019expressive, osman2022supr} to precisely delineate the contact patterns between human bodies and the environment.

(2) \textbf{Explicit constraints from contact cues:} In this work, we propose an end-to-end learning approach to predict human motions directly from pressure data. The learning-based pipeline can rapidly sense the pressure distribution patterns and generate high-quality predictions from pressure sequences, yet it may lead to underutilization of contact priors from pressure sensors and cause misalignment between limb position and contact regions~(\eg, torso and limbs lift). In future works, we aim to explicitly incorporate contact priors through learning or optimization methods~\cite{shimada2023decaf} to further enhance the authenticity of the model's predictions.

(3) \textbf{Efforts for information ambiguity:} In this work, we aspire to mitigate the information ambiguity issue through pressure-based feature sampling and habit-based Test-Time Optimization strategies, yielding accuracy improvement; however, challenges persist. Building upon the observation that users perform movements in certain habitual patterns, we expect to develop a larger-scale motion generation model reliant on VQ-VAE~\cite{van2017neural} or diffusion~\cite{ho2020denoising} techniques, to address the deficiencies in single-pressure modality based on users' motion patterns.

\newpage
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_simplify_quali.pdf}
  \caption{\textbf{Qualitative results of our generated p-GTs on the TIP dataset.} We compare our results with SOTA vision-based methods Cliff and TCMR~(predicted on RGB images) and p-GTs provided in TIP.} 
  % \vspace{-0.5cm}
  \label{fig: sup_simplify_quali}
\end{figure*}

\newpage
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_pihmr_quali.pdf}
  \caption{\textbf{Qualitative results of PI-HMR's performance on the TIP dataset.} We compare our results with SOTA vision-based methods Cliff~(predicted on RGB images) and pressure-based method PI-Mesh.} 
  % \vspace{-0.5cm}
  \label{fig: sup_pihmr_quali}
\end{figure*}

\newpage
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{images/sup_pihmr_quali_2.pdf}
  \caption{\textbf{More qualitative results of PI-HMR's performance on the TIP dataset.}} 
  % \vspace{-0.5cm}
  \label{fig: sup_pihmr_quali_2}
\end{figure*}






