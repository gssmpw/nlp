\section{Method}
\subsection{PI-HMR}
Our motivation is to utilize pressure data nature. So our efforts fall into three stages: alleviating the dataset bottleneck and learning cross-dataset human and motion priors in the pre-training stage; pressure-based PI-HMR's design; and learning user's habits to overcome information ambiguity in the TTO. Thus, the data flow includes: (1) pre-train: KD-based pre-training with the training set; (2) train: train the PI-HMR and VQ-VAE with the training set; (3) test: test with PI-HMR on the test set and improve the estimates with the TTO strategy. \cref{fig: pihmr_architecture} shows the framework of PI-HMR. The details of each module will be elaborated as follows:

\subsubsection{Overall Pipeline of PI-HMR}
Given an input pressure image sequence $V=\{I_i \in \mathbb{R}^{H\times W}\}^T_{t=1}$ with $T$ frames, PI-HMR outputs the SMPL predictions of the mid-frame by a three-stage feature extraction and fusion modules. Following~\cite{kocabas2020vibe, choi2021beyond, wei2022capturing}, we first use ResNet50 to extract the static feature of each frame to form a static representation sequence $X = \{x_t \in \mathbb{R}^{2048 \times H_1 \times W_1}\}^T_{t=1}$. The extracted $X$ is then fed into our Multi-scale Feature Fusion module~(MFF) to generate the fusion feature sequences $G = \{g_t\}^T_{t=1}$, with two-layer Transformer blocks behind to learn their long-term temporal dependencies and yield the temporal feature sequence $Z = \{z_t\}^T_{t=1}$. Finally, We use the mean feature of $Z$ as the integrated feature representation of the mid-frame and produce final estimations with an IEF SMPL regressor~\cite{kanazawa2018end}.   

\subsubsection{Multi-Scale Feature Fusion Module}
To exploit the characteristics of pressure images, our core insight lies in that both large-pressure regions and human joint projections are essential for model learning: large-pressure regions represent the primary contact areas between humans and environments, directly reflecting user's posture and movement tendencies; 2D joint positions, always accompanied by inherent information ambiguity, serve to assist the model in learning the local pressure distribution pattern between small and large pressure zones. Following the insight, we present the Multi-scale Feature Fusion module~(MFF), shown in~\cref{fig: sec7_pimesh_structure}. MFF extracts multi-scale features from the static feature $x_i$ with the supervision of high-pressure masks and human joints, and generates the fusion feature $g_i$ for the next-stage temporal encoder. Before delving into MFF, we first introduce our positional encoding and high-pressure sampling strategy.

\textbf{Spatial Position Embedding.} We introduce a novel position embedding approach to fuse spatial priors into model learning. Compared with visual pixels, we could acquire the position of each sensing unit and their spatial relationships, given that the sensors remain fixed during data collection. Specifically, for a sensing unit located in pixel $(i, j)$ of a pressure image, we could get its position representation $[i, j, i \cdot d_h, j \cdot d_w]$, with $d_h$, $d_w$ being the sensor intervals along x-axis and y-axis~($d_h=0.0311m$ and $d_w=0.0195m$ in TIP). The first two values mean its position within image, while the latter ones denote the position in the world coordinate system (with its origin at the top-left pixel position of the pressure image). The representation is then transformed into spatial tokens $P \in \mathbb{R}^{256}$ using a linear layer. During the training, we could generate the spatial position map for the whole pressure image, noted as $P_i \in \mathbb{R}^{256 \times H \times W}$.

\textbf{TopK-Mask and Learnable Mask.} We employ a Top-K selection algorithm to generate  high-pressure 0-1 masks for each pressure image~(elements larger than K-largest value is set as 1). The mask, noted as $H^K$, will be fed into MFF as contour priors. Besides, we incorporate a learnable mask $H^{LK}$ into our model, utilizing the initial pressure input $I_i$ and the TopK-Mask matrix $H_i^K$ to learn an attention distribution that evaluates the contribution of features in the feature map. The learnable mask is computed as:
\begin{equation} 
    H^{LK}_i = \text{Softmax}(\text{Conv}([I_i \odot H_i^K, H^K_i]))
\end{equation}
where $\odot$ is the Hadamard product. The product result will be stacked with the TopK-mask and fed into a 1-layer convolution layer and Softmax layer to generate the attention matrix $H^{LK}_i \in \mathbb{R}^{H \times W}$. We aim to explicitly integrate these pressure distributions to enhance learnable masks' quality. The K is set as 128 in PI-HMR, and we also conduct ablations to discuss the selection of K in~\cref{tab: ablations for PI-HMR}.

\textbf{Auxiliary Joint Regressor.} We use an auxiliary joint regressor to provide 2D joints for the multi-scale feature extraction~(shown in~\cref{fig: sec7_pimesh_structure}). The regressor takes the static feature $x_i$ as input and returns the 2D positions of 12 joints in the pressure image, noted as $J_i^{2D}$. The 2D regressor will be trained in conjunction with the entire model.

\textbf{Multi-Scale Feature Fusion.}
\begin{figure}[tbp]
  \centering
  \includegraphics[width=\linewidth]{images/pihmr_mff.pdf}
  \caption{\textbf{Framework of our multi-scale feature fusion module.}} 
  % \vspace{-0.5cm}
  \label{fig: sec7_pimesh_structure}
\end{figure}
We extract the global feature $g_i^g$, local feature $g_i^l$, and sampling feature $g_i^s$ from the static feature $x_i$, without replying on the temporal consistency. Firstly for global feature, we apply average pooling and downsampling to the static features $x_i \in \mathbb{R}^{2048 \times H_1 \times W_1}$ to generate global representation $g_i^g \in \mathbb{R}^{512}$. 

Subsequently, we perform dimension-upsampling on $x_i$ to obtain upsampled feature $x_i^{up} \in \mathbb{R}^{256 \times H \times W}$ that aligned with the initial pressure input scale, facilitating us to apply spatial position embedding and feature sampling. For local features, we add $x_i^{up}$ to the spatial position map $P_i$ we have learned, multiply it point-wise with the Learnable Mask $H^{LK}_i$, and then subject it to AttentionPooling to derive the local features $g_i^l \in \mathbb{R}^{256}$. 

As for the sampling features, we employ a feature sampling process on $x_i^{up}$ based on the pre-obtained TopK-Masks and 12 2D keypoint positions obtained from a auxiliary 2D keypoint regressor and get a medium feature $g_i^{mid} \in \mathbb{R}^{(K + 12) \times 256}$. After the same spatial position embedding, the medium feature will be input into a 1-layer Transformer layer to learn its spatial semantics, with the mean of the results serving as the sampling feature $g_i^s \in \mathbb{R}^{256}$.

Finally we get the fusion feature $ g_i \in \mathbb{R}^{1024}$ by concatenating aforesaid global, local, and sampling features.  

\subsubsection{Training Strategy}

The overall loss function can be expressed as follows:
\begin{equation} \label{eq: overall_func}
\small
   \mathcal{L}_{pi} = \lambda_{\text{SMPL}} \mathcal{L}_{\text{SMPL}} + \lambda_{3D} \mathcal{L}_{3D} + \lambda_{2D} \mathcal{L}_{2D} 
\end{equation}
where $\mathcal{L}_{\text{SMPL}}$ and $\mathcal{L}_{3D}$ presents the deviations between the estimated SMPL parameters and 3d joints with GTs,  and $\mathcal{L}_{2D}$ minimize errors in 2D joints for the auxiliary regressor.

\subsection{Encoder pre-train by cross-modal KD}
We employ a cross-modal KD framework to pretrain our PI-HMR's feature encoder, aiming at learning motion and shape priors from vision-based methods on paired pressure-RGB images. Specifically, we implement a HMR~\cite{kanazawa2018end} architecture as the student network $\mathcal{F}_S$~(with a ResNet50 as encoder and a IEF~\cite{kanazawa2018end} SMPL regressor), and choose CLIFF~(ResNet50)~\cite{li2022cliff} as the teacher model $\mathcal{F}_T$~(a HMR-based network). During pre-training, we apply extra feature-based and response-based KD~\cite{gou2021knowledge} to realize fine-grained knowledge transfer. Given input pressure-RGB-label groups~$(I_P, I_R, y)$, and 4 pairs of hidden feature maps from $\mathcal{F}_T$ and $\mathcal{F}_S$~(ResNet50 has 4 residual blocks, so we extract the feature maps after each residual block), i.e., $M_T$ from $\mathcal{F}_T$ and $M_S$ from $\mathcal{F}_S$, the loss function is:
\begin{equation}
\small
\begin{split}
    L_{KD} = \lambda_{kd}^y L_{pi}(\mathcal{F}_S(I_P), y) + \lambda_{kd}^T L_{pi}(\mathcal{F}_S(I_P), \mathcal{F}_T(I_R)) \\
    + \lambda_{kd}^F \sum_{i=1}^4||M_S^i - M_T^i|| 
\end{split}
\end{equation}
where $L_{pi}$ is the same as~\cref{eq: overall_func}, and $\lambda$ is the hyperparamter. After training and convergence, the ResNet50 encoder from $\mathcal{F}_S$ will be adopted as PI-HMR's pre-trained static encoder and finetuned in the following training process.

\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\begin{table*}[t] 
\small
\centering
\begin{tabular}{l|c|c|cccc}
\toprule[2pt]
Method       & Input  & Modalities      & MPJPE  & PA-MPJPE & MPVE  & ACC-ERR \\
\hline
% CLIFF        &  single                     & \multirow{3}{*}{RGB} &       &         &          &      &       &           \\ \cline{1-2} \cline{4-9}
% TCMR         & \multirow{2}{*}{sequence}     &                    &       &         &          &      &       &           \\
% MPS-NET      &                        &                    &       &         &          &      &       &           \\
% \hline
HMR~\cite{kanazawa2018end} & \multirow{3}{*}{single} & \multirow{9}{*}{Pressure} & 75.06 & 57.97 &  89.11 &   31.52  \\
HMR-KD       &            &         &   66.30    &          52.41      &  83.01        &     24.41      \\
% PressureNet  &                        &                    &       &                   &      &       &           \\
% BodyMap      &                        &                    &       &         &          &      &       &           \\
BodyMap-WS~\cite{tandon2024bodymap}   &           &        &   71.48    &     \textbf{40.91}      &   80.08       &   27.98     \\ \cline{1-2} \cline{4-7}
TCMR~\cite{choi2021beyond}         & \multirow{7}{*}{sequence}     &     & 64.37 &   46.76      &     74.66         &       20.12    \\
MPS-NET~\cite{wei2022capturing}      &               &         &         160.59    &    112.12     &     187.13        &      28.73         \\
PI-Mesh~\cite{wu2024seeing}      &        &         &    76.47   &     54.65     &   90.54       &    21.86  \\
 \cline{1-1} \cline{4-7}
PI-HMR (ours)       &         &      & 59.46         &        44.53      &      69.92    &       \textbf{9.12}     \\
PI-HMR + KD (ours)       &        &     &   \underline{57.13}  & 42.98         &  \underline{67.22}    &        9.84     \\
PI-HMR + TTO (ours)       &       &       &    57.76        &     43.31    &   67.76   &         \underline{9.83}    \\
PI-HMR + KD + TTO (ours) &        &       &      \textbf{55.50}     &    \underline{41.81}     &   \textbf{65.15}       &   9.96        \\
\bottomrule[2pt]
\end{tabular}
\caption{\textbf{Overall results of PI-HMR with SOTA methods}} \label{tab: overall_results}
\end{table*}
\begin{figure*}[t] \vspace{-0.1cm}
  \centering
  \includegraphics[width=\linewidth]{images/quali_results.pdf}
  \caption{\textbf{Qualitative visualization for PI-HMR.} PI-HMR and PI-Mesh's results are generated by pressure images, while CLIFF's outputs are generated by RGB images for cross-modal comparison. Predictions are rendered on RGB images for comparison convenience}
  \label{fig: overall_vis}
\end{figure*} 

\subsection{Test-Time Optimization}
We also explore a TTO routine to further enhance prediction quality of PI-HMR. Considering that there hasn't been a general 2D keypoint regressor for pressure images, we are inclined toward seeking an unsupervised, prior-based optimization strategy. We notice that humans exhibit similar movement patterns across various postural states~(\eg, timing, which hand to support, and leg movements). This inspires us to pre-learn such a motion habit as motion prior, playing as supplement cues to refine PI-HMR's prediction. 

We apply a VQ-VAE as the motion prior learner. The selection is rooted in our assumption that the distribution of bed-bound movements is rather constrained. In that case, for a noised motion prediction, VQ-VAE could match it to the closest pattern, thereby re-generating habit-based results. The VQ-VAE is based on Transformer blocks and show similar architecture with~\cite{feng2024stratified}. During training,  we only auto-reconstruct the pose sequences~($\theta$ in SMPL). More details are provided in Supplementary Materials.

The VQ-VAE will act as the only motion prior and supervision in our TTO routine. For terminological convenience, given a VQ-VAE $\mathbb{M}$ and PI-HMR initial predictions~$\Theta^0 = \{\theta^0_1, ... \theta^0_T\}$, the $i_{th}$ iteration objectives follows:
\begin{equation} 
\small
    \mathcal{L}_{TTO}^i = \mathcal{L}_{m}(\Theta^i, \Theta^0) + \mathcal{L}_{m}(\Theta^i, \mathbb{M}(\Theta^i)) + \mathcal{L}_{sm}(\Theta^i)
\end{equation}
$\mathcal{L}_{m}$ is the SMPL and joint error term, and $\mathcal{L}_{sm}$ is the smooth loss. The result of $i_{th}$ iteration will be input into $\mathbb{M}$ and optimized in the ${i + 1}_{th}$ iteration. The TTO will help maintain a balance between initial PI-HMR outputs and the reconstruction by VQ-VAE, thus learning robust motion priors.

% Vision-based approaches usually employ a robust 2D keypoint regressor~(\eg OpenPose~\cite{cao2017realtime}, ViTPose~\cite{xu2022vitpose}) as weak supervision to encourage the predictions fit to image cues.

% as a result of information ambiguity, the prediction noise is primarily concentrated during turning-over movements with less contact between humans and beds. Meanwhile,

