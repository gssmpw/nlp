\section{Related work}
\label{sec:RelatedWorks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{SDE Approximations and Applications.} In ____, a formal theoretical framework was proposed to derive SDEs that accurately capture the stochastic nature inherent in optimization methods commonly used in machine learning. Since then, SDEs have been applied in various areas of machine learning, including \emph{stochastic optimal control} to optimally adjust both stepsize ____ and batch size ____. Importantly, they have been used to characterize \emph{convergence bounds} and \emph{stationary distributions} ____, \emph{scaling rules} ____, and provided insights in the context of \emph{implicit regularization} ____.

\textbf{Two Classes of Compression.} The current theory focuses \textit{either} on unbiased ____ \textit{or} biased ____ compression without discussing the conceptual differences between the two classes. However, biased compressors typically outperform their unbiased counterparts in practical applications ____. Therefore, there is a gap between theory and practice which we aim to reduce in this paper.

\textbf{Heavy-tailed Noise.} Recent empirical evidence suggests that the noise in several deep learning setups follows a heavy-tailed distribution ____. In contrast, previous studies mostly focused on more restricted bounded variance assumptions. Therefore, there is a growing interest in analyzing the convergence of algorithms under heavy-tailed noise  ____. Earlier works ____ combined compression and clipping to make the algorithm communication-efficient and robust to heavy-tailed noise: We show that the \textit{sign} compressor alone effectively addresses both issues without introducing additional hyperparameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%