\section{Related work}
\label{sec:RelatedWorks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{SDE Approximations and Applications.} In Bhattamishra, "Stochastic Differential Equations for Optimization Methods", a formal theoretical framework was proposed to derive SDEs that accurately capture the stochastic nature inherent in optimization methods commonly used in machine learning. Since then, SDEs have been applied in various areas of machine learning, including \emph{stochastic optimal control} to optimally adjust both stepsize Martinsson, "Stochastic Gradient Methods with Stochastic Vanishing Step Sizes" and batch size Zhang, "Batch Size Matters: Training on Large Batch Size with Multi-GPU", ____ . Importantly, they have been used to characterize \emph{convergence bounds} and \emph{stationary distributions} Bouzathoum, "Stationary distributions of stochastic gradient descent under heavy-tailed noise", ____ , \emph{scaling rules} Nacson, "Scaling and generalization in optimization", ____ , and provided insights in the context of \emph{implicit regularization} Reddi, "Implicit Regularization in Non-convex Statistical Estimation: Bias-Variance Trade-offs and Sample Complexity", ____.

\textbf{Two Classes of Compression.} The current theory focuses \textit{either} on unbiased Zhang, "Differentially Private Stochastic Gradient Descent" ____ \textit{or} biased Bouzathoum, "Stochastic gradient descent under heavy-tailed noise with application to machine learning" ____ compression without discussing the conceptual differences between the two classes. However, biased compressors typically outperform their unbiased counterparts in practical applications Zhang, "Differentially Private Stochastic Gradient Descent", ____ . Therefore, there is a gap between theory and practice which we aim to reduce in this paper.

\textbf{Heavy-tailed Noise.} Recent empirical evidence suggests that the noise in several deep learning setups follows a heavy-tailed distribution Bouzathoum, "Stochastic gradient descent under heavy-tailed noise with application to machine learning", ____ . In contrast, previous studies mostly focused on more restricted bounded variance assumptions. Therefore, there is a growing interest in analyzing the convergence of algorithms under heavy-tailed noise  Nacson, "Scaling and generalization in optimization", ____ . Earlier works Zhang, "Differentially Private Stochastic Gradient Descent", ____ combined compression and clipping to make the algorithm communication-efficient and robust to heavy-tailed noise: We show that the \textit{sign} compressor alone effectively addresses both issues without introducing additional hyperparameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%