@inproceedings{Malladi2022AdamSDE,
 author = {Malladi, Sadhika and Lyu, Kaifeng and Panigrahi, Abhishek and Arora, Sanjeev},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {On the {SDEs} and Scaling Rules for Adaptive Gradient Algorithms},
 year = {2022}
}

@article{PD2025,
  author  = {Constantin Philippenko and Aymeric Dieuleveut},
  title   = {Compressed and distributed least-squares regression: convergence rates with applications to federated learning},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {288},
  pages   = {1--80},
  url     = {http://jmlr.org/papers/v25/23-1040.html}
}

@inproceedings{compagnoni2023sde,
  title={An sde for modeling sam: Theory and insights},
  author={Compagnoni, Enea Monzio and Biggio, Luca and Orvieto, Antonio and Proske, Frank Norbert and Kersting, Hans and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  pages={25209--25253},
  year={2023},
  organization={PMLR}
}

@inproceedings{compagnoni2024sde,
  title={SDEs for Minimax Optimization},
  author={Compagnoni, Enea Monzio and Orvieto, Antonio and Kersting, Hans and Proske, Frank and Lucchi, Aurelien},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4834--4842},
  year={2024},
  organization={PMLR}
}

@article{condat2022ef,
  title={{EF-BV}: A unified theory of error feedback and variance reduction mechanisms for biased and unbiased compression in distributed optimization},
  author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{fatkhullin2024momentum,
  title={Momentum provably improves error feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{gao2023econtrol,
  title={{EControl}: Fast Distributed Optimization with Compression and Error Control},
  author={Gao, Yuan and Islamov, Rustem and Stich, Sebastian},
  journal={arXiv preprint arXiv:2311.05645},
  year={2023}
}

@article{gorbunov2023high,
  title={High-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavy-tailed noise},
  author={Gorbunov, Eduard and Sadiev, Abdurakhmon and Danilova, Marina and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2310.01860},
  year={2023}
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in {SGD}},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@inproceedings{islamov2021distributed,
  title={Distributed second order methods with fast rates and compressed communication},
  author={Islamov, Rustem and Qian, Xun and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  pages={4617--4628},
  year={2021},
  organization={PMLR}
}

@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={ICANN 2018},
  year={2018}
}

@article{khirirat2023clip21,
      title={{Clip21}: Error Feedback for Gradient Clipping}, 
      author={Sarit Khirirat and Eduard Gorbunov and Samuel Horváth and Rustem Islamov and Fakhri Karray and Peter Richtárik},
      year={2023},
      journal={arXiv preprint: arXiv 2305.18929}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  year={2019}
}

@article{li2023convergenceandprivacy,
  title={Convergence and privacy of decentralized nonconvex optimization with gradient clipping and communication compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.09896},
  year={2023}
}

@article{mishchenko2024distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={Optimization Methods and Software},
  year={2024}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNNs}.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  year={2014}
}

@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{smith2021origin,
  title={On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  author={Samuel L. Smith and Benoit Dherin and David G. T. Barrett and Soham De},
  journal={arXiv preprint arXiv: 2101.12176},
  year={2021}
}

@article{sun2023distributed,
  title={Distributed Stochastic Optimization under Heavy-Tailed Noises},
  author={Sun, Chao},
  journal={arXiv preprint arXiv:2312.15847},
  year={2023}
}

@article{yang2022taming,
  title={Taming Fat-Tailed (“Heavier-Tailed” with Potentially Infinite Variance) Noise in Federated Learning},
  author={Yang, Haibo and Qiu, Peiwen and Liu, Jia},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{yu2023smoothed,
  title={Smoothed Gradient Clipping and Error Feedback for Distributed Optimization under Heavy-Tailed Noise},
  author={Yu, Shuhua and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.16920},
  year={2023}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{zhao2022batch,
    title={Batch size selection by stochastic optimal control},
    author={Jim Zhao and Aurelien Lucchi and Frank Norbert Proske and Antonio Orvieto and Hans Kersting},
    booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
    year={2022}
}

