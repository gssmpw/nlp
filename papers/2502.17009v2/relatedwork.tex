\section{Related work}
\label{sec:RelatedWorks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{SDE Approximations and Applications.} In \citep{li2017stochastic}, a formal theoretical framework was proposed to derive SDEs that accurately capture the stochastic nature inherent in optimization methods commonly used in machine learning. Since then, SDEs have been applied in various areas of machine learning, including \emph{stochastic optimal control} to optimally adjust both stepsize \citep{li2017stochastic,li2019stochastic} and batch size \citep{zhao2022batch}. Importantly, they have been used to characterize \emph{convergence bounds} and \emph{stationary distributions} \citep{compagnoni2023sde,compagnoni2024sde,compagnoni2025adaptive}, \emph{scaling rules} \citep{jastrzkebski2017three,Malladi2022AdamSDE,compagnoni2025adaptive}, and provided insights in the context of \emph{implicit regularization} \citep{smith2021origin,compagnoni2023sde}.

\textbf{Two Classes of Compression.} The current theory focuses \textit{either} on unbiased \citep{condat2022ef,PD2025, mishchenko2024distributed, islamov2021distributed} \textit{or} biased \citep{gao2023econtrol, fatkhullin2024momentum} compression without discussing the conceptual differences between the two classes. However, biased compressors typically outperform their unbiased counterparts in practical applications \citep{seide20141}. Therefore, there is a gap between theory and practice which we aim to reduce in this paper.

\textbf{Heavy-tailed Noise.} Recent empirical evidence suggests that the noise in several deep learning setups follows a heavy-tailed distribution \citep{simsekli2019tail, zhang2020adaptive, gurbuzbalaban2021heavy, kunstner2024heavy}. In contrast, previous studies mostly focused on more restricted bounded variance assumptions. Therefore, there is a growing interest in analyzing the convergence of algorithms under heavy-tailed noise  \citep{devlin2018bert, sun2023distributed, yang2022taming, gorbunov2023high}. Earlier works \citep{khirirat2023clip21, li2023convergenceandprivacy, yu2023smoothed} combined compression and clipping to make the algorithm communication-efficient and robust to heavy-tailed noise: We show that the \textit{sign} compressor alone effectively addresses both issues without introducing additional hyperparameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%