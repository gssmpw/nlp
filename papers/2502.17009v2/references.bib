@article{gess2024stochastic,
  title={Stochastic modified flows, mean-field limits and dynamics of stochastic gradient descent},
  author={Gess, Benjamin and Kassing, Sebastian and Konarovskyi, Vitalii},
  journal={Journal of Machine Learning Research},
  year={2024}
}

@article{liu2018diffusion,
  title={A Diffusion Approximation Theory of Momentum Stochastic Gradient Descent in Nonconvex Optimization},
  author={Tianyi Liu and Zhehui Chen and Enlu Zhou and Tuo Zhao},
  journal={Stochastic Systems},
  year={2021}
}

@article{qian2021basis,
  title={Basis matters: better communication-efficient second order methods for federated learning},
  author={Qian, Xun and Islamov, Rustem and Safaryan, Mher and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2111.01847},
  year={2021}
}

@article{hardt2018gradient,
  title={Gradient descent learns linear dynamical systems},
  author={Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={29},
  pages={1--44},
  year={2018}
}

@inproceedings{islamov2025towards,
title={Towards Faster Decentralized Stochastic Optimization with Communication Compression},
author={Rustem Islamov and Yuan Gao and Sebastian U Stich},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=CMMpcs9prj}
}

@article{liu2024aiming,
  title={Aiming towards the minimizers: fast convergence of SGD for overparametrized problems},
  author={Liu, Chaoyue and Drusvyatskiy, Dmitriy and Belkin, Misha and Davis, Damek and Ma, Yian},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{islamov2024loss,
  title={Loss Landscape Characterization of Neural Networks without Over-Parametrization},
  author={Islamov, Rustem and Ajroldi, Niccol{\`o} and Orvieto, Antonio and Lucchi, Aurelien},
  journal={arXiv preprint arXiv:2410.12455},
  year={2024}
}

@inproceedings{islamov2021distributed,
  title={Distributed second order methods with fast rates and compressed communication},
  author={Islamov, Rustem and Qian, Xun and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  pages={4617--4628},
  year={2021},
  organization={PMLR}
}

@inproceedings{hu2019global,
  title={On the global convergence of continuous--time stochastic heavy--ball method for nonconvex optimization},
  author={Hu, Wenqing and Li, Chris Junchi and Zhou, Xiang},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)},
  year={2019}
}


@article{maulen2021continuous,
  title={A continuous-time model of stochastic gradient descent: convergence rates and complexities under Lojasiewicz inequality},
  author={Maul{\'e}n Soto, Rodrigo Ignacio},
  year={2021},
  journal={Universidad de Chile}
}


@inproceedings{paquette2021sgd,
  title={{SGD} in the large: Average-case analysis, asymptotics, and stepsize criticality},
  author={Paquette, Courtney and Lee, Kiwon and Pedregosa, Fabian and Paquette, Elliot},
  booktitle={Conference on Learning Theory},
  year={2021},
  organization={PMLR}
}


@article{gu2021adversarial,
  title={Adversarial Training for Gradient Descent: Analysis Through its Continuous-time Approximation},
  author={Gu, Haotian and Guo, Xin and Li, Xinyu},
  journal={arXiv preprint arXiv:2105.08037},
  year={2021}
}


@article{ankirchner2024comparison,
  title={A comparison of continuous-time approximations to stochastic gradient descent},
  author={Ankirchner, Stefan and Perko, Stefan},
  journal={Journal of Machine Learning Research},
  year={2024}
}


@article{zhou2020stochastic,
  title={Stochastic modified equations for continuous limit of stochastic ADMM},
  author={Zhou, Xiang and Yuan, Huizhuo and Li, Chris Junchi and Sun, Qingyun},
  journal={arXiv preprint arXiv:2003.03532},
  year={2020}
}


@article{su2023accelerated,
  title={Accelerated Federated Learning Over Wireless Fading Channels With Adaptive Stochastic Momentum},
  author={Su, Liqun and Lau, Vincent KN},
  journal={IEEE Internet of Things Journal},
  year={2023},
  publisher={IEEE}
}


@article{an2020stochastic,
  title={Stochastic modified equations for the asynchronous stochastic gradient descent},
  author={An, Jing and Lu, Jianfeng and Ying, Lexing},
  journal={Information and Inference: A Journal of the IMA},
  year={2020}
}


@inproceedings{fontaine2021convergence,
  title={Convergence rates and approximation results for {SGD} and its continuous-time counterpart},
  author={Fontaine, Xavier and De Bortoli, Valentin and Durmus, Alain},
  booktitle={Conference on Learning Theory},
  year={2021}
}

@article{zhu2020sharp,
  title={A sharp convergence rate for a model equation of the asynchronous stochastic gradient descent},
  author={Yuhua Zhu and Lexing Ying},
  journal={Communications in Mathematical Sciences},
  year={2021}
}



@article{bercher2020weak,
  title={Weak error analysis for stochastic gradient descent optimization algorithms},
  author={Bercher, Aritz and Gonon, Lukas and Jentzen, Arnulf and Salimova, Diyora},
  journal={arXiv preprint arXiv:2007.02723},
  year={2020}
}


@inproceedings{ayadi2021stochastic,
  title={Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent},
  author={Ayadi, Imen and Turinici, Gabriel},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)},
  year={2021}
}


@article{lanconelli2022note,
  title={A note on diffusion limits for stochastic gradient descent},
  author={Lanconelli, Alberto and Lauria, Christopher SA},
  journal={arXiv preprint arXiv:2210.11257},
  year={2022}
}


@article{soto2022sde,
  title={An {SDE} perspective on stochastic convex optimization},
  author={Soto, Rodrigo Maulen and Fadili, Jalal and Attouch, Hedy},
  journal={arXiv preprint arXiv:2207.02750},
  year={2022}
}


@article{maulen2024stochastic,
  title={Stochastic Inertial Dynamics Via Time Scaling and Averaging},
  author={Maulen-Soto, Rodrigo and Fadili, Jalal and Attouch, Hedy and Ochs, Peter},
  journal={arXiv preprint arXiv:2403.16775},
  year={2024}
}



@article{wang2020asymptotic,
  title={Asymptotic analysis via stochastic differential equations of gradient descent algorithms in statistical and computational paradigms},
  author={Wang, Yazhen and Wu, Shang},
  journal={Journal of machine learning research},
  year={2020}
}


@article{dambrine2024stochastic,
  title={Stochastic Differential Equations for modeling first order optimization methods},
  author={Dambrine, Marc and Dossal, Ch and Puig, B{\'e}n{\'e}dicte and Rondepierre, Aude},
  journal={SIAM Journal on Optimization},
  year={2024}
}


@inproceedings{li2023fast,
  title={Fast Equilibrium of {SGD} in Generic Situations},
  author={Li, Zhiyuan and Wang, Yi and Wang, Zhiren},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{li2022uniform,
  title={On uniform-in-time diffusion approximation for stochastic gradient descent},
  author={Li, Lei and Wang, Yuliang},
  journal={arXiv preprint arXiv:2207.04922},
  year={2022}
}


@article{wang2022two,
  title={Two facets of {SDE} under an information-theoretic lens: Generalization of {SGD} via training trajectories and via terminal states},
  author={Wang, Ziqiao and Mao, Yongyi},
  journal={arXiv preprint arXiv:2211.10691},
  year={2022}
}


@article{cui2020momentum,
  title={Momentum methods for stochastic optimization over time-varying directed networks},
  author={Cui, Zhuo-Xu and Fan, Qibin and Jia, Cui},
  journal={Signal Processing},
  year={2020}
}


@article{bardi2022deep,
  title={Deep relaxation of controlled stochastic gradient descent via singular perturbations},
  author={Bardi, Martino and Kouhkouh, Hicham},
  journal={arXiv preprint arXiv:2209.05564},
  year={2022}
}


@article{kunin2023limiting,
  title={The Limiting Dynamics of {SGD}: Modified Loss, Phase-Space Oscillations, and Anomalous Diffusion},
  author={Kunin, Daniel and Sagastuy-Brena, Javier and Gillespie, Lauren and Margalit, Eshed and Tanaka, Hidenori and Ganguli, Surya and Yamins, Daniel LK},
  journal={Neural Computation},
  year={2023}
}


@article{sun2023scheduling,
  title={Scheduling hyperparameters to improve generalization: From centralized {SGD} to asynchronous {SGD}},
  author={Sun, Jianhui and Yang, Ying and Xun, Guangxu and Zhang, Aidong},
  journal={ACM Transactions on Knowledge Discovery from Data},
  year={2023}
}

@article{chen2022approximation,
  title={Approximation to stochastic variance reduced gradient Langevin dynamics by stochastic delay differential equations},
  author={Chen, Peng and Lu, Jianya and Xu, Lihu},
  journal={Applied Mathematics \& Optimization},
  year={2022}
}

@article{zhang2023stochastic,
  title={Stochastic modified equations and dynamics of dropout algorithm},
  author={Zhang, Zhongwang and Li, Yuqing and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.15850},
  year={2023}
}

@inproceedings{pan2023toward,
title={Toward Understanding Why {Adam} Converges Faster Than {SGD} for Transformers},
author={Yan Pan and Yuanzhi Li},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022}
}


@inproceedings{zhou2022win,
  title={Win: Weight-decay-integrated nesterov acceleration for adaptive gradient algorithms},
  author={Zhou, Pan and Xie, Xingyu and Shuicheng, YAN},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{deng2012mnist,
  title={The {MNIST} database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  year={2012}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  year={2011}
}

@book{10.5555/1593511,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 journal       = {Nature}
}


@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  journal={Toronto, ON, Canada}
}


@article{zhou2024towards,
  title={Towards understanding convergence and generalization of {AdamW}},
  author={Zhou, Pan and Xie, Xingyu and Lin, Zhouchen and Yan, Shuicheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024}
}


@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  year={2009}
}

@article{noci2022signal,
  title={Signal propagation in transformers: Theoretical perspectives and the role of rank collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27198--27211},
  year={2022}
}

@article{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{de2018convergence,
  title={Convergence guarantees for {RMSProp} and {Adam} in non-convex optimization and an empirical comparison to Nesterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}

@article{yang2024two,
  title={Two sides of one coin: the limits of untuned sgd and the power of adaptive methods},
  author={Yang, Junchi and Li, Xiang and Fatkhullin, Ilyas and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{li2024convergence,
  title={Convergence of {Adam} under relaxed assumptions},
  author={Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}



@inproceedings{chen2018convergence,
title={On the Convergence of A Class of Adam-Type Algorithms  for Non-Convex Optimization},
author={Xiangyi Chen and Sijia Liu and Ruoyu Sun and Mingyi Hong},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{wang2022provable,
  title={Provable adaptivity in {Adam}},
  author={Wang, Bohan and Zhang, Yushun and Zhang, Huishuai and Meng, Qi and Ma, Zhi-Ming and Liu, Tie-Yan and Chen, Wei},
  journal={arXiv preprint arXiv:2208.09900},
  year={2022}
}

@misc{
OTIBA2024,
title={On the Implicit Bias of Adam},
author={Matias D. Cattaneo and Jason Matthew Klusowski and Boris Shigida},
year={2024},
url={https://openreview.net/forum?id=ZA9XUTseA9}
}

@inproceedings{
bernstein2018signsgdd,
title={sign{SGD} with Majority Vote is Communication Efficient and Fault Tolerant},
author={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJxhijAcY7},
}


@article{xiao2024exact,
  title={Exact Risk Curves of signSGD in High-Dimensions: Quantifying Preconditioning and Noise-Compression Effects},
  author={Xiao, Ke Liang and Marshall, Noah and Agarwala, Atish and Paquette, Elliot},
  journal={arXiv preprint arXiv:2411.12135},
  year={2024}
}

@article{PD2025,
  author  = {Constantin Philippenko and Aymeric Dieuleveut},
  title   = {Compressed and distributed least-squares regression: convergence rates with applications to federated learning},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {288},
  pages   = {1--80},
  url     = {http://jmlr.org/papers/v25/23-1040.html}
}

@article{smith2021origin,
  title={On the Origin of Implicit Regularization in Stochastic Gradient Descent},
  author={Samuel L. Smith and Benoit Dherin and David G. T. Barrett and Soham De},
  journal={arXiv preprint arXiv: 2101.12176},
  year={2021}
}


@inproceedings{dosovitskiy2020image,
title={An Image is Worth 16x16 Words: {T}ransformers for Image Recognition at {Scale}},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{pythia,
author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and Van Der Wal, Oskar},
title = {Pythia: a suite for analyzing large language models across training and scaling},
year = {2023},
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {102},
numpages = {34},
location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
series = {ICML'23}
}

@article{
defossez2020simple,
title={A Simple Convergence Proof of {Adam} and {Adagrad}},
author={Alexandre D{\'e}fossez and Leon Bottou and Francis Bach and Nicolas Usunier},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022}
}

@article{zhang2022adam,
  title={{Adam} can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in neural information processing systems},
  year={2022}
}


@InProceedings{bernstein2018signsgd,
  title = 	 {Sign{SGD}: Compressed Optimisation for Non-Convex Problems},
  author =       {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year={2018}
}

@article{balles2020geometry,
      title={The Geometry of Sign Gradient Descent}, 
      author={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},
      year={2020},
      journal={arXiv preprint arXiv: 2002.08056}
}


@InProceedings{karimireddy2019errorfeedback,
  title = 	 {Error Feedback Fixes {S}ign{SGD} and other Gradient Compression Schemes},
  author =       {Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year = 	 {2019}
}


@InProceedings{safaryan2021signsgd,
  title = 	 {Stochastic Sign Descent Methods: New Algorithms and Better Theory},
  author =       {Safaryan, Mher and Richtarik, Peter},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021}
}


@inproceedings{li2023convergence,
title={Convergence of Adam Under Relaxed Assumptions},
author={Haochuan Li and Alexander Rakhlin and Ali Jadbabaie},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@misc{zhang2024convergence,
      title={Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance}, 
      author={Qi Zhang and Yi Zhou and Shaofeng Zou},
      year={2024},
      journal={arXiv preprint arXiv:2404.01436}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@article{wang2023closing,
  title={Closing the gap between the upper bound and lower bound of Adam's iteration complexity},
  author={Wang, Bohan and Fu, Jingwen and Zhang, Huishuai and Zheng, Nanning and Chen, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{
zhang2019gradient,
title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{mai2021stability,
  title={Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  year={2021}
}


@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  year={2013},
}

@inproceedings{puchkin2024breaking,
  title={Breaking the heavy-tailed noise barrier in stochastic optimization problems},
  author={Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2024}
}


@inproceedings{crawshaw2022robustness,
	author = {Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Robustness to Unbounded Smoothness of Generalized SignSGD},
	year = {2022}
}

@book{karatzas2014brownian,
  title={Brownian motion and stochastic calculus},
  author={Karatzas, Ioannis and Shreve, Steven},
  volume={113},
  year={2014},
  publisher={springer}
}



@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}


@article{popov1980modification,
  title={A modification of the Arrow-Hurwicz method for search of saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={28},
  pages={845--848},
  year={1980},
  publisher={Springer}
}




@inproceedings{ma2022qualitative,
  title={A qualitative study of the dynamic behavior for adaptive gradient algorithms},
  author={Ma, Chao and Wu, Lei and Weinan, E},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={671--692},
  year={2022},
  organization={PMLR}
}



@article{loizou2021stochastic,
  title={Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity},
  author={Loizou, Nicolas and Berard, Hugo and Gidel, Gauthier and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19095--19108},
  year={2021}
}


@article{chen1997convergence,
  title={Convergence rates in forward--backward splitting},
  author={Chen, George HG and Rockafellar, R Tyrrell},
  journal={SIAM Journal on Optimization},
  volume={7},
  number={2},
  pages={421--444},
  year={1997},
  publisher={SIAM}
}

@article{noor2003new,
  title={New extragradient-type methods for general variational inequalities},
  author={Noor, Muhammad Aslam},
  journal={Journal of Mathematical Analysis and Applications},
  volume={277},
  number={2},
  pages={379--394},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{
gidel2018variational,
title={A Variational Inequality Perspective on Generative Adversarial Networks},
author={Gauthier Gidel and Hugo Berard and Gaëtan Vignoud and Pascal Vincent and Simon Lacoste-Julien},
booktitle={International Conference on Learning Representations},
year={2019}
}

@inproceedings{vlatakis2023stochastic,
  title={Stochastic methods in variational inequalities: Ergodicity, bias and refinements},
  author={Vlatakis-Gkaragkounis, Emmanouil Vasileios and Giannou, Angeliki and Chen, Yudong and Xie, Qiaomin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4123--4131},
  year={2024},
  organization={PMLR}
}



@book{mao2007stochastic,
  title={Stochastic differential equations and applications},
  author={Mao, Xuerong},
  year={2007},
  publisher={Elsevier}
}

@article{oksendal1990stochastic,
  title={When is a stochastic integral a time change of a diffusion?},
  author={{\O}ksendal, Bernt},
  journal={Journal of theoretical probability},
  year={1990}
}




@article{hong2023high,
  title={High probability convergence of Adam under unbounded gradients and affine variance noise},
  author={Hong, Yusu and Lin, Junhong},
  journal={arXiv preprint arXiv:2311.02000},
  year={2023}
}

@inproceedings{compagnoni2024sde,
  title={SDEs for Minimax Optimization},
  author={Compagnoni, Enea Monzio and Orvieto, Antonio and Kersting, Hans and Proske, Frank and Lucchi, Aurelien},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4834--4842},
  year={2024},
  organization={PMLR}
}



@inproceedings{ijcai2018p307,
author = {He, Li and Meng, Qi and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
title = {Differential Equations for Modeling Asynchronous Algorithms},
year = {2018},
publisher = {AAAI Press},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2220–2226},
numpages = {7},
series = {IJCAI'18}
}

@article{chen2015convergence,
  title={On the convergence of stochastic gradient MCMC algorithms with high-order integrators},
  author={Chen, Changyou and Ding, Nan and Carin, Lawrence},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@book{ljung2012stochastic,
  title={Stochastic approximation and optimization of random systems},
  author={Ljung, Lennart and Pflug, Georg and Walk, Harro},
  volume={17},
  year={2012},
  publisher={Birkh{\"a}user}
}




@article{kaddour2022flat,
  title={When Do Flat Minima Optimizers Work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16577--16595},
  year={2022}
}

@inproceedings{hsieh2021limits,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={4337--4348},
  year={2021},
  organization={PMLR}
}


@inproceedings{
ujvary2022rethinking,
title={Rethinking Sharpness-Aware Minimization as Variational Inference},
author={Szilvia Ujv{\'a}ry and Zsigmond Telek and Anna Kerekes and Anna M{\'e}sz{\'a}ros and Ferenc Husz{\'a}r},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022}
}



@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}



@article{bahri2021sharpness,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={ACL 2022},
  year={2022}
}




@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}


@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={ICLR 2021},
  year={2021}
}

@article{du2021efficient,
  title={Efficient sharpness-aware minimization for improved training of neural networks},
  author={Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent YF},
  journal={ICLR 2022},
  year={2022}
}



@article{levy2016power,
  title={The power of normalization: Faster evasion of saddle points},
  author={Levy, Kfir Y},
  journal={arXiv preprint arXiv:1611.04831},
  year={2016}
}


@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={International Conference on Machine Learning},
  year={2017}
}


@article{kohatsu1997stochastic,
  title={Stochastic differential equations with random coefficients},
  author={Kohatsu-Higa, Arturo and Le{\'o}n, Jorge A and Nualart, David},
  journal={Bernoulli},
  pages={233--245},
  year={1997},
  publisher={JSTOR}
}


@article{barakat2018convergence,
  title={Convergence and dynamical behavior of the ADAM algorithm for nonconvex stochastic optimization},
  author={Barakat, Anas and Bianchi, Pascal},
  journal={SIAM Journal on Optimization},
  volume={31},
  number={1},
  pages={244--274},
  year={2021},
  publisher={SIAM}
}



@article{bishop2019stability,
  title={Stability properties of systems of linear stochastic differential equations with random coefficients},
  author={Bishop, Adrian N and Del Moral, Pierre},
  journal={SIAM Journal on Control and Optimization},
  volume={57},
  number={2},
  pages={1023--1042},
  year={2019},
  publisher={SIAM}
}


@inproceedings{kunin2019loss,
  title={Loss landscapes of regularized linear autoencoders},
  author={Kunin, Daniel and Bloom, Jonathan and Goeva, Aleksandrina and Seed, Cotton},
  booktitle={International Conference on Machine Learning},
  pages={3560--3569},
  year={2019},
  organization={PMLR}
}




@article{rangwani2022escaping,
  title={Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data},
  author={Rangwani, Harsh and Aithal, Sumukh K and Mishra, Mayank and Babu, R Venkatesh},
  journal={NeurIPS 2022},
  year={2022}
}


@article{wen2022does,
  title={How Does Sharpness-Aware Minimization Minimize Sharpness?},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={ICLR 2023},
  year={2023}
}




@article{folland2005higher,
  title={Higher-order derivatives and Taylor’s formula in several variables},
  author={Folland, Gerald B},
  journal={Preprint},
  pages={1--4},
  year={2005}
}




@inproceedings{daneshmand2018escaping,
  title={Escaping saddles with stochastic gradients},
  author={Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={1155--1164},
  year={2018},
  organization={PMLR}
}





@article{graversen2000maximal,
  title={Maximal inequalities for the Ornstein-Uhlenbeck process},
  author={Graversen, S and Peskir, Goran},
  journal={Proceedings of the American Mathematical Society},
  volume={128},
  number={10},
  pages={3035--3041},
  year={2000}
}



@article{mandt2015continuous,
  title={Continuous-time limit of stochastic gradient descent revisited},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M and others},
  journal={NIPS-2015},
  year={2015}
}


@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={ICLR 2017},
  year={2017}
}


@inproceedings{jiang2019fantastic,
  title={Fantastic Generalization Measures and Where to Find Them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  booktitle={International Conference on Learning Representations},
  year={2019}
}



@incollection{risken1996fokker,
  title={Fokker-planck equation},
  author={Risken, Hannes},
  booktitle={The Fokker-Planck Equation},
  pages={63--95},
  year={1996},
  publisher={Springer}
}


@book{gardiner1985handbook,
  title={Handbook of stochastic methods},
  author={Gardiner, Crispin W and others},
  volume={3},
  year={1985},
  publisher={springer Berlin}
}


@inproceedings{
xie2020diffusion,
title={A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
author={Zeke Xie and Issei Sato and Masashi Sugiyama},
booktitle={International Conference on Learning Representations},
year={2021}
}


@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}



@inproceedings{Dziugaite2017PacBayes,
  author = {Gintare K. Dziugaite and Daniel M. Roy},
  title = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
  booktitle = {Uncertainty in Artificial Intelligence},
  year = {2017}
}




@article{Jin2021pgd,
    author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
    title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
    year = {2021},
    publisher = {Association for Computing Machinery},
    volume = {68},
    number = {2},
    journal = {J. ACM},
}



@InProceedings{simsekli2019tailindex,
  title = 	 {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author =       {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2019},
}


@book{kushner2003stochastic,
  title={Stochastic approximation and recursive algorithms and applications},
  author={Kushner, Harold and Yin, G George},
  volume={35},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{helmke1994optimization,
    title = {Optimization and Dynamical Systems},
    author={Helmke, Uwe and Moore, John B},
    edition = {1st},
    publisher = {Springer London},
    year = {1994}
}


@inproceedings{Su2014nesterov,
 author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {A Differential Equation for Modeling {Nesterov’s} Accelerated Gradient Method: Theory and Insights},
 year = {2014}
}


@inproceedings{kingma2014adam,
	title={Adam: A Method for Stochastic Optimization},
	author={Diederik P. Kingma and Jimmy Ba},
	booktitle={International Conference on Learning Representations},
	year={2015}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@misc{hinton2012rmsprop,
author={Tijmen Tieleman and Geoffrey Hinton},
title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude.},
organization={COURSERA: Neural networks for machine learning},
year={2012},
pages={26--31},
url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
}

@article{higham2001algorithmic,
  title={An algorithmic introduction to numerical simulation of stochastic differential equations},
  author={Higham, Desmond J},
  journal={SIAM review},
  year={2001}
}

@book{milstein2013numerical,
  title={Numerical integration of stochastic differential equations},
  author={Milstein, Grigorii Noikhovich},
  year={2013},
  publisher={Springer Science \& Business Media}
}




@inproceedings{Li2020reconciling,
    title={Reconciling modern deep learning with traditional optimization analyses: the intrinsic learning rate},
    author={Zhiyuan Li and Kaifeng Lyu and Sanjeev Arora},
    booktitle={Advances in Neural Information Processing Systems},
    year={2020}
}

@inproceedings{kunin2021neural,
    title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},
    author={Daniel Kunin and Javier Sagastuy-Brena and Surya Ganguli and Daniel LK Yamins and Hidenori Tanaka},
    booktitle={International Conference on Learning Representations},
    year={2021}
}



@article{mandt2017SGDasABI,
    author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
    title = {Stochastic Gradient Descent as Approximate {Bayesian} Inference},
    year = {2017},
    journal = {Journal of Machine Learning Research},
}

@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={ICANN 2018},
  year={2018}
}

@article{
sagun2018empirical,
title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
author={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},
journal={ICLR 2018 Workshop Track},
url={https://openreview.net/forum?id=rJrTwxbCb},
year={2018}
}

@article{zhu2018anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={ICML 2019},
  year={2019}
}




@misc{Dua:2019,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@inproceedings{smith2020sde,
  title={On the Generalization Benefit of Noise in Stochastic Gradient Descent},
  author={Smith, Samuel and Elsen, Erich and De, Soham},
  booktitle={International Conference on Machine Learning},
  year={2020},
}


@article{panigrahi2019non,
  title={Non-{G}aussianity of stochastic gradient noise},
  author={Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  journal={SEDL workshop at NeurIPS 2019},
  year={2019}
}

@inproceedings{lucchi2022fractional,
    title={On the Theoretical Properties of Noise Correlation in Stochastic Optimization},
    author={Aurelien Lucchi and Frank Proske and Antonio Orvieto and Francis Bach and Hans Kersting},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}

@article{GM,
  title={On stochastic differential
equations with locally unbounded drift},
  author={Gy\"{o}ngy, I. and Mart\'{\i}nez, T.},
  journal={Czechoslovak Mathematical Journal,
No. 4, p. 763--783, Vol. 51},
  year={2001}
}

@article{kunstner2023noise,
  title={Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be},
  author={Kunstner, Frederik and Chen, Jacques and Lavington, Jonathan Wilder and Schmidt, Mark},
  journal={ICLR},
  year={2023}
}



@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@phdthesis{xu2022experimental,
  title={Experimental Evaluation of Iterative Methods for Games},
  author={Xu, Mengtong and others},
  year={2022},
  school={Johns Hopkins University}
}



@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}


@article{popov1980modification,
  title={A modification of the Arrow-Hurwicz method for search of saddle points},
  author={Popov, Leonid Denisovich},
  journal={Mathematical notes of the Academy of Sciences of the USSR},
  volume={28},
  pages={845--848},
  year={1980},
  publisher={Springer}
}


@article{loizou2021stochastic,
  title={Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity},
  author={Loizou, Nicolas and Berard, Hugo and Gidel, Gauthier and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19095--19108},
  year={2021}
}


@article{chen1997convergence,
  title={Convergence rates in forward--backward splitting},
  author={Chen, George HG and Rockafellar, R Tyrrell},
  journal={SIAM Journal on Optimization},
  volume={7},
  number={2},
  pages={421--444},
  year={1997},
  publisher={SIAM}
}

@article{noor2003new,
  title={New extragradient-type methods for general variational inequalities},
  author={Noor, Muhammad Aslam},
  journal={Journal of Mathematical Analysis and Applications},
  volume={277},
  number={2},
  pages={379--394},
  year={2003},
  publisher={Elsevier}
}

@article{gidel2018variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal={ICLR},
  year={2019}
}



@article{vlatakis2023stochastic,
  title={Stochastic Methods in Variational Inequalities: Ergodicity, Bias and Refinements},
  author={Vlatakis-Gkaragkounis, Emmanouil-Vasileios and Giannou, Angeliki and Chen, Yudong and Xie, Qiaomin},
  journal={arXiv preprint arXiv:2306.16502},
  year={2023}
}

@article{du2022optimal,
  title={Optimal extragradient-based bilinearly-coupled saddle-point optimization},
  author={Du, Simon S and Gidel, Gauthier and Jordan, Michael I and Li, Chris Junchi},
  journal={arXiv preprint arXiv:2206.08573},
  year={2022}
}


@inproceedings{gorbunov2022stochastic,
  title={Stochastic extragradient: General analysis and improved rates},
  author={Gorbunov, Eduard and Berard, Hugo and Gidel, Gauthier and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7865--7901},
  year={2022},
  organization={PMLR}
}



@article{chavdarova2019reducing,
  title={Reducing noise in GAN training with variance reduced extragradient},
  author={Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran{\c{c}}ois and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{beznosikov2021distributed,
  title={Distributed saddle-point problems: Lower bounds, optimal algorithms and federated gans},
  author={Beznosikov, Aleksandr and Samokhin, Valentin and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2010.13112},
  year={2021}
}


@article{chavdarova2022continuous,
  title={Continuous-time Analysis for Variational Inequalities: An Overview and Desiderata},
  author={Chavdarova, Tatjana and Hsieh, Ya-Ping and Jordan, Michael I},
  journal={arXiv preprint arXiv:2207.07105},
  year={2022}
}

@article{li2022nesterov,
  title={Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization},
  author={Li, Chris Junchi and Yuan, Angela and Gidel, Gauthier and Jordan, Michael I},
  journal={ICML},
  year={2023}
}

@inproceedings{loizou2020stochastic,
  title={Stochastic hamiltonian gradient methods for smooth games},
  author={Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={International Conference on Machine Learning},
  pages={6370--6381},
  year={2020},
  organization={PMLR}
}


@article{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{li2022convergence,
  title={On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging},
  author={Li, Chris Junchi and Yu, Yaodong and Loizou, Nicolas and Gidel, Gauthier and Ma, Yi and Le Roux, Nicolas and Jordan, Michael},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9793--9826},
  year={2022},
  organization={PMLR}
}



@article{,
  title={},
  author={},
  journal={arXiv preprint arXiv:2112.13826},
  year={2021}
}

@inproceedings{chavdarova2021last,
  title={Last-iterate convergence of saddle point optimizers via high-resolution differential equations},
  author={Chavdarova, Tatjana and Jordan, Michael I and Zampetakis, Manolis},
  booktitle={Minimax Theory and its Applications 08 (2023), No. 2},
  pages={333--380},
  year={2023},
  organization={Heldermann Verlag}
}

@article{ryu2019ode,
  title={Ode analysis of stochastic gradient methods with optimism and anchoring for minimax problems},
  author={Ryu, Ernest K and Yuan, Kun and Yin, Wotao},
  journal={arXiv preprint arXiv:1905.10899},
  year={2019}
}


@inproceedings{mishchenko2020revisiting,
  title={Revisiting stochastic extragradient},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4573--4582},
  year={2020},
  organization={PMLR}
}


@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}



@inproceedings{rosca2021discretization,
  title={Discretization drift in two-player games},
  author={Rosca, Mihaela C and Wu, Yan and Dherin, Benoit and Barrett, David},
  booktitle={International Conference on Machine Learning},
  pages={9064--9074},
  year={2021},
  organization={PMLR}
}


@article{hajizadeh2023linear,
  title={On the Linear Convergence of Extragradient Methods for Nonconvex--Nonconcave Minimax Problems},
  author={Hajizadeh, Saeed and Lu, Haihao and Grimmer, Benjamin},
  journal={INFORMS Journal on Optimization},
  year={2023},
  publisher={INFORMS}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{soen2021variance,
  title={On the Variance of the Fisher Information for Deep Learning},
  author={Soen, Alexander and Sun, Ke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5708--5719},
  year={2021}
}

@book{lehmann2006theory,
  title={Theory of point estimation},
  author={Lehmann, Erich L and Casella, George},
  year={2006},
  publisher={Springer Science \& Business Media}
}



@article{hsieh2020explore,
  title={Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16223--16234},
  year={2020}
}


@article{lu2022sr,
  title={An o (sr)-resolution ode framework for understanding discrete-time algorithms and applications to the linear convergence of minimax problems},
  author={Lu, Haihao},
  journal={Mathematical Programming},
  volume={194},
  number={1-2},
  pages={1061--1112},
  year={2022},
  publisher={Springer}
}


@inproceedings{balduzzi2018mechanics,
  title={The mechanics of n-player differentiable games},
  author={Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  booktitle={International Conference on Machine Learning},
  pages={354--363},
  year={2018},
  organization={PMLR}
}


@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@inproceedings{compagnoni2023sde,
  title={An sde for modeling sam: Theory and insights},
  author={Compagnoni, Enea Monzio and Biggio, Luca and Orvieto, Antonio and Proske, Frank Norbert and Kersting, Hans and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  pages={25209--25253},
  year={2023},
  organization={PMLR}
}


@article{zhuang2022surrogate,
  title={Surrogate gap minimization improves sharpness-aware training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},
  journal={ICML 2022},
  year={2022}
}




@inproceedings{ijcai2018p307,
author = {He, Li and Meng, Qi and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
title = {Differential Equations for Modeling Asynchronous Algorithms},
year = {2018},
publisher = {AAAI Press},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2220–2226},
numpages = {7},
series = {IJCAI'18}
}

@article{chen2015convergence,
  title={On the convergence of stochastic gradient MCMC algorithms with high-order integrators},
  author={Chen, Changyou and Ding, Nan and Carin, Lawrence},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@book{ljung2012stochastic,
  title={Stochastic approximation and optimization of random systems},
  author={Ljung, Lennart and Pflug, Georg and Walk, Harro},
  volume={17},
  year={2012},
  publisher={Birkh{\"a}user}
}


@article{kim2023stability,
  title={Stability Analysis of Sharpness-Aware Minimization},
  author={Kim, Hoki and Park, Jinseong and Choi, Yujin and Lee, Jaewook},
  journal={arXiv preprint arXiv:2301.06308},
  year={2023}
}


@article{kaddour2022flat,
  title={When Do Flat Minima Optimizers Work?},
  author={Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16577--16595},
  year={2022}
}

@inproceedings{hsieh2021limits,
  title={The limits of min-max optimization algorithms: Convergence to spurious non-critical sets},
  author={Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
  booktitle={International Conference on Machine Learning},
  pages={4337--4348},
  year={2021},
  organization={PMLR}
}


@inproceedings{
ujvary2022rethinking,
title={Rethinking Sharpness-Aware Minimization as Variational Inference},
author={Szilvia Ujv{\'a}ry and Zsigmond Telek and Anna Kerekes and Anna M{\'e}sz{\'a}ros and Ferenc Husz{\'a}r},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022}
}



@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}



@article{bahri2021sharpness,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={ACL 2022},
  year={2022}
}


@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}


@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={ICLR 2021},
  year={2021}
}

@article{du2021efficient,
  title={Efficient sharpness-aware minimization for improved training of neural networks},
  author={Du, Jiawei and Yan, Hanshu and Feng, Jiashi and Zhou, Joey Tianyi and Zhen, Liangli and Goh, Rick Siow Mong and Tan, Vincent YF},
  journal={ICLR 2022},
  year={2022}
}


@article{mil1986weak,
  title={Weak approximation of solutions of systems of stochastic differential equations},
  author={Mil’shtein, GN},
  journal={Theory of Probability \& Its Applications},
  year={1986}
}


@inproceedings{kunin2019loss,
  title={Loss landscapes of regularized linear autoencoders},
  author={Kunin, Daniel and Bloom, Jonathan and Goeva, Aleksandrina and Seed, Cotton},
  booktitle={International Conference on Machine Learning},
  pages={3560--3569},
  year={2019},
  organization={PMLR}
}


@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21285--21296},
  year={2020}
}


@article{rangwani2022escaping,
  title={Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data},
  author={Rangwani, Harsh and Aithal, Sumukh K and Mishra, Mayank and Babu, R Venkatesh},
  journal={NeurIPS 2022},
  year={2022}
}


@article{wen2022does,
  title={How Does Sharpness-Aware Minimization Minimize Sharpness?},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={ICLR 2023},
  year={2023}
}

@article{bartlett2022dynamics,
  title={The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima},
  author={Bartlett, Peter L and Long, Philip M and Bousquet, Olivier},
  journal={arXiv preprint arXiv:2210.01513},
  year={2022}
}


@article{folland2005higher,
  title={Higher-order derivatives and Taylor’s formula in several variables},
  author={Folland, Gerald B},
  journal={Preprint},
  pages={1--4},
  year={2005}
}


@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}


@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  year={2019}
}


@inproceedings{daneshmand2018escaping,
  title={Escaping saddles with stochastic gradients},
  author={Daneshmand, Hadi and Kohler, Jonas and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={1155--1164},
  year={2018},
  organization={PMLR}
}



@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  year={2015}
}

@article{graversen2000maximal,
  title={Maximal inequalities for the Ornstein-Uhlenbeck process},
  author={Graversen, S and Peskir, Goran},
  journal={Proceedings of the American Mathematical Society},
  volume={128},
  number={10},
  pages={3035--3041},
  year={2000}
}


@article{mandt2017stochastic,
  title={Stochastic gradient descent as approximate bayesian inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={JMLR},
  year={2017}
}

@article{mandt2015continuous,
  title={Continuous-time limit of stochastic gradient descent revisited},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M and others},
  journal={NIPS-2015},
  year={2015}
}


@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={ICLR 2017},
  year={2017}
}


@inproceedings{jiang2019fantastic,
  title={Fantastic Generalization Measures and Where to Find Them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  booktitle={International Conference on Learning Representations},
  year={2019}
}



@incollection{risken1996fokker,
  title={Fokker-planck equation},
  author={Risken, Hannes},
  booktitle={The Fokker-Planck Equation},
  pages={63--95},
  year={1996},
  publisher={Springer}
}


@book{gardiner1985handbook,
  title={Handbook of stochastic methods},
  author={Gardiner, Crispin W and others},
  volume={3},
  year={1985},
  publisher={springer Berlin}
}


@inproceedings{
xie2020diffusion,
title={A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
author={Zeke Xie and Issei Sato and Masashi Sugiyama},
booktitle={International Conference on Learning Representations},
year={2021}
}


@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}



@inproceedings{Dziugaite2017PacBayes,
  author = {Gintare K. Dziugaite and Daniel M. Roy},
  title = {Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data},
  booktitle = {Uncertainty in Artificial Intelligence},
  year = {2017}
}




@article{Jin2021pgd,
    author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
    title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
    year = {2021},
    publisher = {Association for Computing Machinery},
    volume = {68},
    number = {2},
    journal = {J. ACM},
}



@InProceedings{simsekli2019tailindex,
  title = 	 {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author =       {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2019},
}


@book{kushner2003stochastic,
  title={Stochastic approximation and recursive algorithms and applications},
  author={Kushner, Harold and Yin, G George},
  volume={35},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{helmke1994optimization,
    title = {Optimization and Dynamical Systems},
    author={Helmke, Uwe and Moore, John B},
    edition = {1st},
    publisher = {Springer London},
    year = {1994}
}


@inproceedings{Su2014nesterov,
 author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {A Differential Equation for Modeling {Nesterov’s} Accelerated Gradient Method: Theory and Insights},
 year = {2014}
}

@inproceedings{Malladi2022AdamSDE,
 author = {Malladi, Sadhika and Lyu, Kaifeng and Panigrahi, Abhishek and Arora, Sanjeev},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {On the {SDEs} and Scaling Rules for Adaptive Gradient Algorithms},
 year = {2022}
}

@inproceedings{Li2021validity,
    title={On the Validity of Modeling {SGD} with Stochastic Differential Equations ({SDE}s)},
    author={Zhiyuan Li and Sadhika Malladi and Sanjeev Arora},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
    year={2021}
}

@inproceedings{zhao2022batch,
    title={Batch size selection by stochastic optimal control},
    author={Jim Zhao and Aurelien Lucchi and Frank Norbert Proske and Antonio Orvieto and Hans Kersting},
    booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
    year={2022}
}

@inproceedings{Li2020reconciling,
    title={Reconciling modern deep learning with traditional optimization analyses: the intrinsic learning rate},
    author={Zhiyuan Li and Kaifeng Lyu and Sanjeev Arora},
    booktitle={Advances in Neural Information Processing Systems},
    year={2020}
}

@inproceedings{kunin2021neural,
    title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},
    author={Daniel Kunin and Javier Sagastuy-Brena and Surya Ganguli and Daniel LK Yamins and Hidenori Tanaka},
    booktitle={International Conference on Learning Representations},
    year={2021}
}



@article{mandt2017SGDasABI,
    author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
    title = {Stochastic Gradient Descent as Approximate {Bayesian} Inference},
    year = {2017},
    journal = {Journal of Machine Learning Research},
}

@article{zhu2019anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={ICML},
  year={2019}
}


@InProceedings{Xie2021,
  title = 	 {Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization},
  author =       {Xie, Zeke and Yuan, Li and Zhu, Zhanxing and Sugiyama, Masashi},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  year = 	 {2021},
}


@article{stephan2017stochastic,
  title={Stochastic gradient descent as approximate bayesian inference},
  author={Stephan, Mandt and Hoffman, Matthew D and Blei, David M and others},
  journal={Journal of Machine Learning Research},
  year={2017}
}


@article{ahn2012bayesian,
  title={Bayesian posterior sampling via stochastic gradient {F}isher scoring},
  author={Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
  journal={arXiv preprint arXiv:1206.6380},
  year={2012}
}


@inproceedings{mandt2016variational,
  title={A variational analysis of stochastic gradient algorithms},
  author={Mandt, Stephan and Hoffman, Matthew and Blei, David},
  booktitle={International conference on machine learning},
  year={2016}
}


@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as {SGD}},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  year={2020}
}



@inproceedings{chen2014stochastic,
  title={Stochastic gradient hamiltonian monte carlo},
  author={Chen, Tianqi and Fox, Emily and Guestrin, Carlos},
  booktitle={International conference on machine learning},
  pages={1683--1691},
  year={2014},
  organization={PMLR}
}



@article{sagun2018empirical,
title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
author={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},
journal={ICLR 2018 Workshop Track},
url={https://openreview.net/forum?id=rJrTwxbCb},
year={2018}
}

@article{zhu2018anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={ICML 2019},
  year={2019}
}


@inproceedings{j2018on,
title={On the Convergence of {Adam} and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
}


@article{poggio2017theory,
  title={Theory of deep learning {III}: explaining the non-overfitting puzzle},
  author={Poggio, Tomaso and Kawaguchi, Kenji and Liao, Qianli and Miranda, Brando and Rosasco, Lorenzo and Boix, Xavier and Hidary, Jack and Mhaskar, Hrushikesh},
  journal={arXiv preprint arXiv:1801.00173},
  year={2017}
}


@article{MN,
  title={A Taylor expansion of the square root matrix function},
  author={Del Moral, Pierre and Niclas, Angele},
  journal={Journal of Mathematical Analysis and Applications},
  volume={465},
  number={1},
  pages={259--266},
  year={2018},
  publisher={Elsevier}
}


@article{orvieto2019continuous,
  title={Continuous-time models for stochastic optimization algorithms},
  author={Orvieto, Antonio and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}



@inproceedings{smith2020sde,
  title={On the Generalization Benefit of Noise in Stochastic Gradient Descent},
  author={Smith, Samuel and Elsen, Erich and De, Soham},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@article{zhao2018proximal,
  title={Proximal {SCOPE} for distributed sparse learning},
  author={Zhao, Shenyi and Zhang, Gong-Duo and Li, Ming-Wei and Li, Wu-Jun},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{deng2024distributed,
  title={Distributed personalized empirical risk minimization},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavinia, Pouria and Mahdavi, Mehrdad},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}


@inproceedings{marfoq2023federated,
  title={Federated learning for data streams},
  author={Marfoq, Othmane and Neglia, Giovanni and Kameni, Laetitia and Vidal, Richard},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2023}
}


@article{aviv2021learning,
  title={Learning under delayed feedback: Implicitly adapting to gradient delays},
  author={Aviv, Rotem Zamir and Hakimi, Ido and Schuster, Assaf and Levy, Kfir Y},
  journal={ICML},
  year={2021}
}


@inproceedings{wang2017memory,
  title={Memory and communication efficient distributed stochastic optimization with minibatch prox},
  author={Wang, Jialei and Wang, Weiran and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  year={2017}
}


@article{yu2019double,
  title={Double quantization for communication-efficient distributed optimization},
  author={Yu, Yue and Wu, Jiaxiang and Huang, Longbo},
  journal={Advances in neural information processing systems},
  year={2019}
}


@article{ayache2023walk,
  title={Walk for learning: A random walk approach for federated learning from heterogeneous data},
  author={Ayache, Ghadir and Dassari, Venkat and El Rouayheb, Salim},
  journal={IEEE Journal on Selected Areas in Communications},
  year={2023}
}



@article{panigrahi2019non,
  title={Non-{G}aussianity of stochastic gradient noise},
  author={Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  journal={SEDL workshop at NeurIPS 2019},
  year={2019}
}

@inproceedings{shamir2014distributed,
  title={Distributed stochastic optimization and learning},
  author={Shamir, Ohad and Srebro, Nathan},
  booktitle={2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  year={2014}
}

@inproceedings{lucchi2022fractional,
    title={On the Theoretical Properties of Noise Correlation in Stochastic Optimization},
    author={Aurelien Lucchi and Frank Proske and Antonio Orvieto and Francis Bach and Hans Kersting},
    booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}

@inproceedings{
compagnoni2025adaptive,
title={Adaptive Methods through the Lens of {SDE}s: Theoretical Insights on the Role of Noise},
author={Enea Monzio Compagnoni and Tianlin Liu and Rustem Islamov and Frank Norbert Proske and Antonio Orvieto and Aurelien Lucchi},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=ww3CLRhF1v}
}

@article{WS,
  title={Bounds for eigenvalues using traces},
  author={Wolkowicz, Henry and Styan, George PH},
  journal={Linear algebra and its applications},
  volume={29},
  pages={471--506},
  year={1980},
  publisher={Elsevier}
}


@book{IW,
  title={Stochastic differential equations and diffusion processes},
  author={Ikeda, Nobuyuki and Watanabe, Shinzo},
  year={2014},
  publisher={Elsevier}
}


@article{GM,
  title={On stochastic differential
equations with locally unbounded drift},
  author={Gy\"{o}ngy, I. and Mart\'{\i}nez, T.},
  journal={Czechoslovak Mathematical Journal,
No. 4, p. 763--783, Vol. 51},
  year={2001}
}

@inproceedings{abadi2016tensorflow,
  title={{TensorFlow}: a system for {Large-Scale} machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  year={2016}
}

@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  year={2012}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  year={2017}
}

@inproceedings{sapio2021scaling,
  title={Scaling distributed machine learning with $\{$In-Network$\}$ aggregation},
  author={Sapio, Amedeo and Canini, Marco and Ho, Chen-Yu and Nelson, Jacob and Kalnis, Panos and Kim, Changhoon and Krishnamurthy, Arvind and Moshref, Masoud and Ports, Dan and Richt{\'a}rik, Peter},
  booktitle={18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
  year={2021}
}

@inproceedings{alistarh2018convergence,
  title={The convergence of stochastic gradient descent in asynchronous shared memory},
  author={Alistarh, Dan and De Sa, Christopher and Konstantinov, Nikola},
  booktitle={Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing},
  year={2018}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNNs}.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  year={2014}
}

@article{mishchenko2024distributed,
  title={Distributed learning with compressed gradient differences},
  author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
  journal={Optimization Methods and Software},
  year={2024}
}

@inproceedings{gorbunov2021local,
  title={{Local SGD}: Unified theory and new efficient methods},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized {SGD} with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{islamov2024asgrad,
  title={{AsGrad}: A sharp unified analysis of asynchronous-{SGD} algorithms},
  author={Islamov, Rustem and Safaryan, Mher and Alistarh, Dan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2024}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the {P}olyak-{\L}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16},
  pages={795--811},
  year={2016},
  organization={Springer}
}


@article{vogels2019powersgd,
  title={{PowerSGD}: Practical low-rank gradient compression for distributed optimization},
  author={Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{islamov2023distributed,
  title={Distributed Newton-type methods with communication compression and bernoulli aggregation},
  author={Islamov, Rustem and Qian, Xun and Hanzely, Slavom{\'\i}r and Safaryan, Mher and Richt{\'a}rik, Peter},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@article{safaryan2021fednl,
  title={{FedNL}: Making Newton-type methods applicable to federated learning},
  author={Safaryan, Mher and Islamov, Rustem and Qian, Xun and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2106.02969},
  year={2021}
}

@article{beznosikov2023biased,
  title={On biased compression for distributed learning},
  author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
  journal={Journal of Machine Learning Research},
  year={2023}
}

@article{kunstner2024heavy,
  title={Heavy-tailed class imbalance and why adam outperforms gradient descent on language models},
  author={Kunstner, Frederik and Yadav, Robin and Milligan, Alan and Schmidt, Mark and Bietti, Alberto},
  journal={arXiv preprint arXiv:2402.19449},
  year={2024}
}

@inproceedings{balles2018dissecting,
  title={Dissecting {Adam}: The sign, magnitude and variance of stochastic gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{chen2024symbolic,
  title={Symbolic discovery of optimization algorithms},
  author={Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and others},
  journal={Advances in neural information processing systems},
  year={2024}
}

@article{fatkhullin2024momentum,
  title={Momentum provably improves error feedback!},
  author={Fatkhullin, Ilyas and Tyurin, Alexander and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{li2023convergenceandprivacy,
  title={Convergence and privacy of decentralized nonconvex optimization with gradient clipping and communication compression},
  author={Li, Boyue and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.09896},
  year={2023}
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in {SGD}},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{gorbunov2023high,
  title={High-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavy-tailed noise},
  author={Gorbunov, Eduard and Sadiev, Abdurakhmon and Danilova, Marina and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2310.01860},
  year={2023}
}

@article{yang2022taming,
  title={Taming Fat-Tailed (“Heavier-Tailed” with Potentially Infinite Variance) Noise in Federated Learning},
  author={Yang, Haibo and Qiu, Peiwen and Liu, Jia},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{sun2023distributed,
  title={Distributed Stochastic Optimization under Heavy-Tailed Noises},
  author={Sun, Chao},
  journal={arXiv preprint arXiv:2312.15847},
  year={2023}
}

@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{khirirat2023clip21,
      title={{Clip21}: Error Feedback for Gradient Clipping}, 
      author={Sarit Khirirat and Eduard Gorbunov and Samuel Horváth and Rustem Islamov and Fakhri Karray and Peter Richtárik},
      year={2023},
      journal={arXiv preprint: arXiv 2305.18929}
}


@article{yu2023smoothed,
  title={Smoothed Gradient Clipping and Error Feedback for Distributed Optimization under Heavy-Tailed Noise},
  author={Yu, Shuhua and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.16920},
  year={2023}
}

@article{condat2022ef,
  title={{EF-BV}: A unified theory of error feedback and variance reduction mechanisms for biased and unbiased compression in distributed optimization},
  author={Condat, Laurent and Yi, Kai and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{horvoth2022natural,
  title={Natural compression for distributed deep learning},
  author={Horv{\'a}th, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richt{\'a}rik, Peter},
  booktitle={Mathematical and Scientific Machine Learning},
  year={2022}
}

@article{richtarik2021ef21,
  title={{EF21}: A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{horvath2023stochastic,
  title={Stochastic distributed learning with gradient quantization and double-variance reduction},
  author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Richt{\'a}rik, Peter and Stich, Sebastian},
  journal={Optimization Methods and Software},
  year={2023}
}

@article{khirirat2018distributed,
  title={Distributed learning with compressed gradients},
  author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  journal={arXiv preprint arXiv:1806.06573},
  year={2018}
}

@article{condat2024locodl,
  title={{LoCoDL}: Communication-Efficient Distributed Learning with Local Training and Compression},
  author={Condat, Laurent and Maranjyan, Artavazd and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2403.04348},
  year={2024}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of {SGD}: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020}
}

@article{gao2023econtrol,
  title={{EControl}: Fast Distributed Optimization with Compression and Error Control},
  author={Gao, Yuan and Islamov, Rustem and Stich, Sebastian},
  journal={arXiv preprint arXiv:2311.05645},
  year={2023}
}

@article{wen2017terngrad,
  title={{Terngrad}: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  journal={Advances in neural information processing systems},
  year={2017}
}

@article{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={Advances in neural information processing systems},
  year={2018}
}

@article{mishchenko2022asynchronous,
  title={Asynchronous {SGD} beats minibatch {SGD} under arbitrary delays},
  author={Mishchenko, Konstantin and Bach, Francis and Even, Mathieu and Woodworth, Blake E},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{li2020unified,
  title={A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.07013},
  year={2020}
}

@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}