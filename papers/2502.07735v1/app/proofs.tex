\section{Proofs}\label{app:proofs}


\subsection{Proof of Lemma~\ref{th:fixed_pb}}\label{app:pb_proof}

Consider a random walk on $\cG$ with reversed edges and backward policy as a transition probability. In particular, we define a stochastic process $\{X_n\}_{n=0}^\infty$, such that $X_0 = s_f$ a.s. and $\P[X_i = s \mid X_{i-1} = s'] = \PB(s \mid s')$ and also $\P[X_i = s_0 \mid x_{i-1} = s_0] = 1$, i.e., $s_0$ is an absorbing state. We want to show that
\begin{enumerate}
    \item The random walk terminates with probability 1: $\P[\exists n : X_n = s_0 ] = 1$;
    \item The expected length of a walk is finite: $\E[n_{\tau}] = \E[\sum_{n=0}^{\infty} \ind\{X_i \not = s_0 \}] < \infty$.
\end{enumerate}
In particular, the first statement implies that $\cP(\cdot)$ is a correct probability measure over finite trajectories since for any $\tau = (s_0,s_1\ldots,s_{n_{\tau}}, s_f)$ it holds
\[
    \P[(X_{n_{\tau}+1},\ldots,X_{0}) = \tau] = \prod_{t=0}^{n_\tau} \PB(s_{t} \mid s_{t+1}) = \cP(\tau)\,,
\]
and we have
\[
    \sum_{\tau \in \cT} \P[(X_{n_{\tau}+1},\ldots,X_{0}) = \tau] = \P[\exists \tau \in \cT:  (X_{n_{\tau}+1},\ldots,X_{0}) = \tau] ] = \P[\exists n: X_n = s_0]\,,
\]
since the family of events $\{(X_{n_{\tau}+1},\ldots,X_{0}) = \tau\}$ do not intersect for different $\tau$.

Let us consider any intermediate state $s \in \cS \setminus \{s_0, s_f\}$ and the same random walk $\{Y_n\}_{n=0}^\infty$ that starts from a state $s$. We define $p_s \triangleq \P[\exists n > 0: Y_n = s]$, i.e. the probability that the random walk returns to $s$ at some point. First, notice that $p_s < 1$. Indeed, based on our assumptions about $\cG$, there exists at least one path $\tau$ from $s_0$ to $s$, and furthermore, we can only consider paths that do not contain any cycles, as these can be effectively eliminated. In this case, we have
\[
    \P[(Y_{n_{\tau}+1},\ldots,Y_{0}) = \tau] = \prod_{t=0}^{n_\tau} \PB(s_{t} \mid s_{t+1}) > 0
\]
by the condition on $\PB(s \mid s') > 0$ for $s \to s' \in \cE$. Notice that $\{ (Y_{n_{\tau}+1},\ldots,Y_{0}) = \tau\} \cap \{ \exists n > 0: Y_n = s\} = \emptyset$, since if the trajectory of the random walk has already reached $s_0$, it will never return. Thus, $p_s = \P[\exists n > 0: Y_n = s] < 1$. 

Next, for each state $s \in \cS \setminus \{s_0, s_f\}$ we define $N_s \triangleq \sum_{i=0}^\infty \ind\{ X_i = s \}$ as a number of visits of a state $s$ during the original backward random walk, and also let us define $N'_s \triangleq \sum_{i=0}^\infty\{ Y_i = s\}$ a number of visits of a state $s$ during the backward random walk that starts at $s$. We notice that $\E[N_{s}] \leq \E[N_{s}']$ since after the first visit of $s$, by Markovian property we can replace a random walk $\{X_n\}$ with a random walk $\{Y_n\}$. At the same time, we have $\P[N'_s > k] = \P[\exists n_1, \ldots, n_k > 0: Y_{n_j} = s ]$ and, by Markovian property, we have $\P[N'_s > k] = p_s^k$. Thus
\[
    \E[N'_s] = \sum_{k=0}^\infty \P[N'_s > k] = \sum_{k=0}^\infty p_s^k = \frac{1}{1-p_s} < +\infty\,.
\]
Finally, we have
\[
    \E[n_{\tau}] = \E\left[\sum_{k=0}^\infty \ind\{X_k \not = s_0\}\right] = \E\left[\sum_{s \in \cS \setminus\{s_0, s_f\}} \sum_{k=0}^\infty \ind\{X_k = s\}\right] = \sum_{s \in \cS \setminus\{s_0, s_f\}} \E[N_s] \leq \sum_{s \in \cS \setminus\{s_0, s_f\}} \E[N_s'] < +\infty\,.  
\]






\subsection{Proof of Proposition~\ref{th:flow_eqs}}\label{app:flow_eq_proof}
Firstly, we prove flow matching conditions. We have
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s \to s') & =\cF(s_f)\;\E_{\tau }\left[ \sum_{t = 0}^{n_{\tau}} \ind\{s_t = s, s_{t + 1} = s'\}\right] = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}+1} \ind\{s_{t} = s, s_{t + 1} = s'\} \mid n_\tau\right] \right],  \\
\cF(s) & = \cF(s_f)\;\E_{\tau }\left[ \sum_{t = 0}^{n_{\tau}+1} \ind\{s_t = s\}\right] = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}+1} \ind\{s_{t} = s\} \mid n_\tau\right] \right].
\end{split}
\end{equation*}
Next, note that the following equations hold for any trajectory $\tau$ and any $s \in \cS \setminus \{s_0, s_f\}$:
$$
    \ind\{s_t = s\} = \sum_{s'' \in \vin(s)} \ind \{s_{t - 1} = s'', s_{t} = s\} = \sum_{s' \in \vout(s)} \ind\{s_t = s, s_{t + 1} = s'\}.
$$
Then for $s \in \cS \setminus \{s_0\}$:
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s) & = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 1}^{n_{\tau}+1} \sum_{s'' \in \vin(s)} \ind\{s_{t - 1} = s'', s_{t} = s\} \mid n_\tau\right] \right]  \\
 & = \sum_{s'' \in \vin(s)} \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 1}^{n_{\tau}+1} \ind\{s_{t - 1} = s'', s_{t} = s\} \mid n_\tau\right] \right] = \sum_{s'' \in \vin(s)} \cF(s'' \to s).
\end{split}
\end{equation*}
Similarly for any $s \in \cS \setminus \{s_f\}$:
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s) & = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}} \sum_{s' \in \vout(s)} \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right]  \\
 & = \sum_{s' \in \vout(s)} \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}} \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right] = \sum_{s' \in \vout(s)} \cF(s \to s'). \\
\end{split}
\end{equation*}


Next, we prove the key relation $\cF(s \to s') = \cF(s')\PB(s \mid s')$. We have
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s \to s') & = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}} \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right]  \\
 & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \E_\tau\left[  \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right] \\
 & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t} = s, s_{t+1} = s' \mid n_\tau\right) \right]  \\
  & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t+1} = s' \mid n_\tau\right) \P\left(s_{t} = s \mid s_{t+1} = s', n_\tau\right) \right]  \\
  & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t+1} = s' \mid n_\tau\right) \right] \PB(s \mid s') = \cF(s') \PB(s \mid s').\\
\end{split}
\end{equation*}
In the penultimate equation we used Markovian property of trajectory distribution.

Finally, ${\cF}(s_0)$ is equal to, by definition, $\cF(s_f)$ multiplied by the expected number of times $\tau \sim \cP$ visits $s_0$, where the latter is always $1$, so we have ${\cF}(s_0) = \cF(s_f)$.

\subsection{Proof of Proposition~\ref{th:pb_from_flow}}\label{app:pb_from_flow_proof}

Since $\cF(s \to s')$ satisfies flow matching conditions, we will define
$$\cF(s) = \sum\limits_{s' \in \vout(s)} \cF(s \to s') = \sum\limits_{s'' \vin(s)} \cF(s'' \to s).$$
Next, take $\PB(s \mid s') = \cF(s \to s') / \cF(s')$. Let us denote $\hat{\cF}$ to be flows from Definition~\ref{def:cyclic_flows} that are induced by $\PB$ and $\cF(s_f)$ (which correspond to expected number of visits in regard to the trajectory distribution $\cP$ induced by $\PB$). In essence, we want to prove that $\cF$ and $\hat{\cF}$ coincide. 

By Proposition~\ref{th:flow_eqs} and definition of $\PB$ we have
$$\hat{\cF}(s \to s') = \hat{\cF}(s')\PB(s \mid s') = \frac{\hat{\cF}(s')}{\cF(s')}\cF(s \to s') = C(s')\cF(s \to s').$$
Where we denote $C(s) = \hat{\cF}(s) / \cF(s)$. In addition, by Proposition~\ref{th:flow_eqs}, $\hat{\cF}$ satisfies flow matching conditions, thus
$$\hat{\cF}(s) = \sum\limits_{s' \in \vout(s)} \hat{\cF}(s \to s') = \sum\limits_{s'' \vin(s)} \hat{\cF}(s'' \to s).$$
Combining these statements, for any $s \in \cS \setminus \{s_f\}$ we have
$$
\hat{\cF}(s) = C(s)\cF(s) = \sum\limits_{s' \in \vout(s)} C(s')\cF(s \to s').
$$
The first equation is by definition of $C(s)$ and the second equation is due to flow matching conditions. Thus we have a system of linear equations with respect to $C(s)$:
$$
\forall s \in \cS \setminus \{s_f\}, \; \sum\limits_{s' \in \vout(s)} C(s')\cF(s \to s') - C(s)\cF(s) = 0.
$$
In addition, $\hat{\cF}(s_f)$ is equal to, by definition, $\cF(s_f)$ multiplied by the expected number of times $\tau \sim \cP$ visits $s_f$, where the latter is $1$, so we have $\hat{\cF}(s_f) = \cF(s_f)$, thus an additional equation is $C(s_f) = 1$. In total, we have $|\cS|$ variables and $|\cS|$ equations, and are interested in strictly positive solutions. Firstly, there exists a trivial solution $C(s) = 1$ for each $s \in \cS$, which is an only constant solution since $C(s_f) = 1$. 

Next, suppose there exists a non-constant solution $C'(s)$. Denote $\cS_{\operatorname{max}} = \operatorname{argmax}\limits_{s \in \cS} C'(s)$, which will be a proper subset of $\cS$. Let us consider two cases. First, suppose $s_f \not\in \cS_{\operatorname{max}}$. Let $\tau = \left(s_0 \to s_1 \to \ldots \to s_{n_{\tau}} \to s_f\right)$ be any trajectory that visits some state in $\cS_{\operatorname{max}}$. Then there exists an index $t \le n_\tau$ such that $s_t \in \cS_{\operatorname{max}}$ and $s_{t+1} \not\in \cS_{\operatorname{max}}$. Then we have
$$
1 = \frac{\sum_{s' \in \vout(s_t)} C(s')\cF(s_t \to s') }{C(s_t)\cF(s_t)} < \frac{\sum_{s' \in \vout(s_t)} C(s_t)\cF(s_t \to s') }{C(s_t)\cF(s_t)} = \frac{\sum_{s' \in \vout(s_t)} \cF(s_t \to s') }{\cF(s_t)} = 1.
$$
The inequality is due to three facts: (i) $C(s) > 0 \; \forall s \in \cS$, (ii) $s_t \in \cS_{\operatorname{max}}$, and thus $C(s_t) \geq C(s')$ for any $s' \in \cS$,  and (iii) the inequality is strict for at least one edge $s_t \to s_{t+1}$ such that $C'(s_{t+1}) < C'(s_t)$, and it implies a contradiction.

Second, suppose $s_f \in \cS_{\operatorname{max}}$. Then, denote $\cS_{\operatorname{min}} = \operatorname{argmin}\limits_{s \in \cS} C'(s)$, which will be a proper subset of $\cS$. Similarly to the previous case, let $\tau$ be any trajectory that visits some state in $\cS_{\operatorname{min}}$. Then there exists an index $t \le n_\tau$ such that $s_t \in \cS_{\operatorname{min}}$ and $s_{t+1} \not\in \cS_{\operatorname{min}}$. Then we have
$$
1 = \frac{\sum_{s' \in \vout(s_t)} C(s')\cF(s_t \to s') }{C(s_t)\cF(s_t)} > \frac{\sum_{s' \in \vout(s_t)} C(s_t)\cF(s_t \to s') }{C(s_t)\cF(s_t)} = \frac{\sum_{s' \in \vout(s_t)} \cF(s_t \to s') }{\cF(s_t)} = 1.
$$
Thus, in this case, there is also a contradiction. Therefore $C(s) = 1$ is a unique solution, meaning that $\hat{\cF}(s) = \cF(s)$. Finally for any $s \to s' \in \cE$
$$
\hat{\cF}(s \to s') = C(s')\cF(s \to s') = \cF(s \to s').
$$
$\cF$ and $\hat{\cF}$ coincide, thus the proposition is proven.

%Next, suppose there exists a non-constant solution $C'(s)$. Denote $s_m = \operatorname{argmax}\limits_{s \in \cS} C'(s)$. Suppose $s_m \neq s_f$, then we have
%$$
%1 = \frac{\sum_{s' \in \vout(s_m)} C(s')\cF(s_m \to s') }{C(s_m)\cF(s_m)} < \frac{\sum_{s' \in \vout(s_m)} C(s_m)\cF(s_m \to s') }{C(s_m)\cF(s_m)} = \frac{\sum_{s' \in \vout(s_m)} \cF(s_m \to s') }{\cF(s_m)} = 1.
%$$
%Thus there is a contradiction.



\subsection{Proof of Proposition~\ref{th:pf_db}}\label{app:pf_db_proof}

Let us proof existence and uniqueness of a corresponding forward policy. Let $\cF$ be the flow induced by the backward policy~\eqref{def:cyclic_flows}.

\textbf{Uniqueness.} Suppose that such a forward policy $\PF$ exists, then probability distributions over $\cT$ induced by $\PB$ and $\PF$ coincide. Then
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s \to s') & = \cF(s_f)\E_{n_\tau} \left[ \E_\tau\left[ \sum_{t = 0}^{n_{\tau}} \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right]  \\
 & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \E_\tau\left[  \ind\{s_{t} = s, s_{t+1} = s'\} \mid n_\tau\right] \right] \\
 & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t} = s, s_{t+1} = s' \mid n_\tau\right) \right]  \\
  & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t} = s \mid n_\tau\right) \P\left(s_{t+1} = s' \mid s_{t} = s, n_\tau\right) \right]  \\
  & = \cF(s_f)\E_{n_\tau} \left[ \sum_{t = 0}^{n_{\tau}} \P\left(s_{t} = s \mid n_\tau\right) \right] \PF(s' \mid s) = \cF(s') \PF(s' \mid s).\\
\end{split}
\end{equation*}


Then we have $\cF(s') \PB(s \mid s') = \cF(s)\PF(s' \mid s)$ and $\PF(s' \mid s) = \cF(s') \PB(s \mid s') / \cF(s)$, thus we finish the proof of uniqueness presented above.


\textbf{Existence.} Take 
$$\PF(s' \mid s) = \frac{\cF(s') \PB(s \mid s')}{\cF(s)} = \frac{\cF(s \to s')}{\cF(s)}.$$ 
This is always a valid probability distribution since $\cF(s) = \sum_{s' \in \vout(s)} \cF(s \to s')$. Next, for any $\tau \in \cT$ we have 
$$\prod_{t = 0}^{n_\tau} \PB(s_t \mid s_{t + 1}) = \prod_{t = 0}^{n_\tau} \frac{\cF(s_t \to s_{t + 1})}{\cF(s_{t+1})} = \frac{\prod_{t = 0}^{n_\tau} \cF(s_t \to s_{t + 1})}{\prod_{t = 0}^{n_\tau} \cF(s_{t+1})}.$$
where the first equation is due to Proposition~\ref{th:flow_eqs}. Next, note that $$\cF(s_0) = \cF(s_f)\;\E_{\tau }\left[ \sum_{t = 0}^{n_{\tau}+1} \ind\{s_t = s_0\}\right] = \cF(s_f) \cdot 1 = \cF(s_f).$$
Then 
$$\frac{\prod_{t = 0}^{n_\tau} \cF(s_t \to s_{t + 1})}{\prod_{t = 0}^{n_\tau} \cF(s_{t+1})} = \frac{\cF(s_0)}{\cF(s_f)}\frac{\prod_{t = 0}^{n_\tau} \cF(s_t \to s_{t + 1})}{\prod_{t = 0}^{n_\tau} \cF(s_{t})} = \frac{\prod_{t = 0}^{n_\tau} \cF(s_t \to s_{t + 1})}{\prod_{t = 0}^{n_\tau} \cF(s_{t})} = \prod_{t = 0}^{n_\tau} \PF(s_{t+1} \mid s_{t}).$$
Thus the existence is proven. Finally, detailed balance conditions follow from the proof of uniqueness.

\subsection{Proof of Proposition~\ref{th:pf_db_reverse}}\label{app:pf_db_reverse_proof}

Consider an edge function $\bar{\cF}(s \to s') = \cF(s)\PF(s' \mid s)$. It is positive since $\cF(s) > 0$ and $\PF(s' \mid s) > 0$ by the statement of the proposition. Since $\PF(\cdot \mid s)$ is a valid probability distribution over $\vout(s)$, we have 
$$\sum_{s' \in \vout(s)}\bar{\cF}(s \to s') = \sum_{s' \in \vout(s)}\cF(s)\PF(s' \mid s) = \cF(s)\sum_{s' \in \vout(s)}\PF(s' \mid s) = \cF(s).$$
Similarly, since $\PB(\cdot \mid s)$ is a valid probability distribution over $\vin(s)$, and $\cF$, $\PF$ and $\PB$ satisfy detailed balance conditions, we have
$$\sum_{s'' \in \vin(s)}\bar{\cF}(s'' \to s) = \sum_{s'' \in \vin(s)}\cF(s'')\PF(s \mid s'') = \sum_{s'' \in \vin(s)}\cF(s)\PB(s'' \mid s) = \cF(s)\sum_{s'' \in \vin(s)}\PB(s'' \mid s) = \cF(s).$$
Thus $\bar{\cF}$ satisfies flow matching conditions. In addition 
$$
\PB(s \mid s') = \frac{\cF(s)\PF(s' \mid s)}{\cF(s')} = \frac{\bar{\cF}(s \to s')}{\cF(s')} = \frac{\bar{\cF}(s \to s')}{\sum_{s'' \in \vin(s')}\bar{\cF}(s'' \to s')}.
$$
Thus, applying Proposition~\ref{th:pb_from_flow} to $\bar{\cF}$, we get that it is an edge flow induced by $\PB$, thus $\cF$ is also the state flow induced by $\PB$ and $\cF(s_f)$.

Next, consider any trajectory $\tau = (s_0, s_1, \dots, s_{n_\tau}, s_f) \in \cT$. By detailed balance conditions we have
$$
\prod_{t = 0}^{n_\tau} \PB(s_t \mid s_{t + 1}) = \prod_{t = 0}^{n_\tau} \frac{\cF(s_t)\PF(s_{t+1} \mid s_t)}{\cF(s_{t+1})} = \frac{\cF(s_0)}{\cF(s_f)} \prod_{t = 0}^{n_\tau} \PF(s_{t+1} \mid s_t) = \prod_{t = 0}^{n_\tau} \PF(s_{t+1} \mid s_t).
$$

The final equation is due to the fact that state flow $\cF$ is induced by $\PB$ and $\cF(s_f)$, so we have $\cF(s_f) = \cF(s_0)$ by Proposition~\ref{th:flow_eqs}. Thus the proposition is proven.

\subsection{Proof of Proposition~\ref{th:total_flow}}\label{app:total_flow_proof}

We first note that not including $s_0$ and $s_f$ in the sum is just a matter of the definition of trajectory length presented in~\ref{sec:background_gflow}, where we do not count the first and the final state towards it. Using the fact that the length of a trajectory is the sum of the numbers of visits to each individual state in the graph, we obtain that
$$
\E_{\tau \sim \cP}[n_\tau] =\E_{\tau }\left[ \sum_{s \in \cS \setminus \{s_0, s_f\}} \sum_{t = 0}^{n_{\tau}+1} \ind\{s_t = s\}\right] = \sum_{s \in \cS \setminus \{s_0, s_f\}} \E_{\tau }\left[  \sum_{t = 0}^{n_{\tau}+1} \ind\{s_t = s\}\right] = \sum\limits_{s \in \cS \setminus \{s_0, s_f\}} \frac{\cF(s)}{\cF(s_f)}.
$$



\subsection{Entropy-Regularized RL and Theorem~\ref{th:rl_reduction}}\label{app:rl_reduction_proof}

\paragraph{Background on Entropy-Regularized RL.}

Let $\cM_{\cG}$ be a deterministic MDP induced by a graph $\cG$ with a state space $\cS$ corresponding to vertices of $\cG$, the action space $\cA_s$ for each state $s$ corresponds to outgoing edges of $s$, associated with $\vout(s)$, and let $\lambda > 0$ be a regularization coefficient. We define a policy $\pi$ as a mapping from each state $s \in \cS$ to a probability measure $\pi(\cdot|s)$ over $\cA_s$. 

Then, for any policy $\pi$, we define the regularized value function as follows
\begin{equation}\label{eq:reg_value_function_def}
    V^{\pi}_{\lambda}(s) \triangleq \E_{\tau \sim \Ptraj{\pi}}\left[ \sum_{t=0}^\infty r(s_t, s_{t+1}) + \lambda \cH(\pi(\cdot| s_t))  \mid s_0 = s \right] \,,
\end{equation}
where $\cH$ is Shannon entropy, $\Ptraj{\pi}$ is a trajectory distribution induced by the following the policy $\pi$: $s_t \sim \pi(\cdot | s_{t-1})$  for all $t \geq 1$ and the starting state $s_0$ is fixed as $s$ (not to be confused with the initial state in $\cG$). Overall, it is not clear if the value function is a well-defined function in the case of $\gamma = 1$. We call this type of problem \textit{regularized shortest path}.

\begin{lemma}\label{lem:uniqeness_reg_shortest_path}
    Assume that (i) a graph $\cG$ satisfies \Cref{assumption} and (ii) for any $s \in \cS, s' \in \cA_s$ it holds $r(s,s') \leq 0$ and $r(s,s') = 0$ if and only if $|\cA_s| = 1$. Then, a regularized shortest path problem admits a unique solution, and the value of its solution satisfies soft optimal Bellman equations
    \begin{equation}\label{eq:reg_optimal_bellman}
        \Qstar_{\lambda}(s,s') \triangleq r(s,s') + \Vstar(s')\,, \qquad \Vstar_{\lambda}(s) \triangleq \lambda \log\left( \sum_{s' \in \vout(s)} \exp\left\{ \frac{1}{\lambda} \Qstar_{\lambda}(s,s') \right\} \right)\,,
    \end{equation}
    where the optimal policy can be derived as $\pistar(a|s) \propto \exp\{1/\lambda \cdot \Qstar_{\lambda}(s,a)\}$. 
\end{lemma}
\begin{proof}
    Without loss of generality, we can assume that for all states, it holds $|\cA_s| \geq 2$, because we can shrink edges such that $|\cA_s| = 1$ since they will provide zero reward and zero entropy. Thus, we can assume that $r(s,s') < 0$ for all $s \in \cS, s' \in \cA_s$.

    Then, we can rewrite the value function in the following way
    \[
        V^{\pi}_{\lambda}(s_0) = \sum_{s\in \cS}\sum_{s' \in \vout(s)} \E_{\pi}[n_\tau(s,s')] r(s,s') + \lambda \sum_{s\in \cS} \E_{\pi}[n_\tau(s)] \cH(\pi(\cdot | s)) \,.
    \]
    where $n_\tau(s) = \sum_{t=0}^\infty \ind\{s_t = s\}$ and $n_\tau(s, s') = \sum_{t=0}^\infty \ind\{s_t = s, s_{t+1} = s'\}$ is a number of visits of a particular state and edge in $\cG$. In the analogy with occupancy measures in RL, we employ the notation $d^{\pi}(s) = \E_{\pi}[n_\tau(s)]$ and $d^{\pi}(s,s') = \E_{\pi}[n_\tau(s,s')]$ for an expected number of visits. However, this definition also corresponds to the flow function in the reversed graph (see \Cref{def:cyclic_flows}) with the "backward policy" $\pi$.
    
    Given this expression, we notice that if we consider any policy such that $\pi(s'|s) > 0$ for all $s \in \cS$, then, according to \Cref{th:fixed_pb} in the reversed graph, $d^{\pi}(s,s'), d^{\pi}(s) < +\infty$, and it implies that the value function is finite.
    
    Additionally, any policy such that $\E_{\pi}[n_\tau(s,s')] = +\infty$ have $V^{\pi}_{\lambda}(s_0) = -\infty$ and thus cannot be optimal since there exists at least one policy with a finite value. Therefore, we can consider only a class of policies such that $d^{\pi}(s,s') \in [0, +\infty)$.

    Then, we notice that $d^{\pi}(s,s') = \pi(s'|s) d^{\pi}(s)$ thus we can rewrite the value in the following form
    \[
       V^{\pi}_{\lambda}(s_0) =  \sum_{s\in \cS}\sum_{s' \in \vout(s)} d^{\pi}(s,s') r(s,s') - \underbrace{\lambda \sum_{s\in \cS} \sum_{s' \in \vout(s)} d^{\pi}(s,s') \log\left( d^{\pi}(s,s') / \sum_{s' \in \vout(s)} d^{\pi}(s,s') \right)}_{R(d^{\pi})}\,.
    \]
    As a function of $d^{\pi}(s,s')$, we see that the first term in the expression above is linear whereas the second one is relative conditional entropy \cite{neu2017unified} that is strongly concave. Given that the set of all admissible $d^{\pi}(s,s')$ is a polytope that is defined as a family of negative functions that satisfies the flow matching constraints (see \Cref{th:flow_eqs})
    \[
        \cK \triangleq \left\{ d \colon \cS \times \cS \to \mathbb{R}_+ \bigg| \sum_{s' \in \vout(s)} d^{\pi}(s,s') = \sum_{s'' \in \vin(s)} d^{\pi}(s'', s), \sum_{s' \in \vout(s_0)}d(s_0, s') = 1, \sum_{s'' \in \vin(s_f)} d(s'',s_f) = 1  \right\},
    \]
    where the flow and policy have one-to-one corresponds due to \Cref{th:pb_from_flow} in the reversed graph. Since the set $\cK$ is a polytope, optimization of $V^{\pi}_{\lambda}(s_0)$ over occupancy measures is a strictly convex problem and thus admits a unique solution $d^\star$ that corresponds to a unique policy $\pistar$.
    
    Before proving the optimal Bellman equations, we want to show that $\pistar$ satisfies $\pistar(s'|s) > 0$ for any $s \in \cS, s' \in \vout(s)$. To do it, we explore the gradients of the regularizer, using computations done in \cite{neu2017unified}, Section A.4:
    $
        \frac{\partial R(d^\pi)}{\partial d^\pi(s,s')} = \log \pi(s'|s)\,.
    $
    In particular, it implies that as $\pi(s'|s) \to 0$, then $\norm{\nabla_{d^{\pi}} \partial R(d^\pi)} \to +\infty$, thus the optimal policy $\pistar$ cannot satisfy $\pistar(s'|s) = 0$ since it will violate the optimality conditions.


    Next, we want to prove that the value of the optimal policy satisfies soft optimal Bellman equations. First, we notice that the usual Bellman equations still hold
    \[
        Q^{\pi}_{\lambda}(s,s') = r(s,s') + V^{\pi}_{\lambda}(s'), \qquad V^{\pi}_{\lambda}(s) = \sum_{s' \in \vout(s)} \pi(s'|s) Q^{\pi}_\lambda(s,s') + \lambda \cH(\pi(\cdot | s))\,.
    \]
    Let us consider a regularized policy improvement operation, defined as
    \[
        \pi'(\cdot|s) \triangleq \arg\max_{p}  \left\{ \sum_{s' \in \vout(s)} p(s') Q^{\pi}_\lambda(s,s') + \lambda \cH(p) \right\} \propto \exp\left\{ \frac{1}{\lambda} Q^{\pi}_{\lambda}(s, \cdot) \right\} \,.
    \]
    Then we want to show that $V^{\pi'}_{\lambda}(s_0) \geq V^{\pi}_{\lambda}(s_0)$ if the policy $\pi$ is positive: $\pi(s'|s) > 0$ for all $s \in \cS, s'\in \vout(s)$. 
    We start from a general inequality that holds for any $s \in \cS$
    \begin{align*}
        V^{\pi'}_{\lambda}(s) - V^{\pi}_{\lambda}(s) &= \left(\sum_{s' \in \vout(s)} \pi'(s'|s) Q^{\pi'}_\lambda(s,s') + \lambda \cH(\pi'(\cdot | s))\right)
        -  \left( \sum_{s' \in \vout(s)} \pi(s'|s) Q^{\pi}_\lambda(s,s') + \lambda \cH(\pi(\cdot | s))\right) \\
        &= \left(\sum_{s' \in \vout(s)} \pi'(s'|s) Q^{\pi'}_\lambda(s,s') + \lambda \cH(\pi'(\cdot | s))\right) - \left(\sum_{s' \in \vout(s)} \pi'(s'|s) Q^{\pi}_\lambda(s,s') + \lambda \cH(\pi'(\cdot | s))\right) \\
        &+ \left(\sum_{s' \in \vout(s)} \pi'(s'|s) Q^{\pi}_\lambda(s,s') + \lambda \cH(\pi'(\cdot | s))\right) 
         -\left(\sum_{s' \in \vout(s)} \pi(s'|s) Q^{\pi}_\lambda(s,s') + \lambda \cH(\pi(\cdot | s))\right) \\
         &\geq \sum_{s' \in \vout(s)} \pi'(s'|s) \left[ Q^{\pi'}_{\lambda}(s,s') - Q^{\pi}_{\lambda}(s,s') \right] = \sum_{s' \in \vout(s)} \pi'(s'|s) \left[ V^{\pi'}_{\lambda}(s') - V^{\pi}_{\lambda}(s') \right]\,.
    \end{align*}
    After $t$ rollouts, we have
    \[
        V^{\pi'}_{\lambda}(s_0) - V^{\pi}_{\lambda}(s_0) \geq \E_{\tau \sim \Ptraj{\pi'}} \left[ V^{\pi'}_{\lambda}(s_t) - V^{\pi}_{\lambda}(s_t)  \right]\,,
    \]
    Since the policy $\pi$ is positive, then \Cref{th:fixed_pb} in the reversed graph implies that $d^{\pi}(s,s'), d^{\pi}(s) < +\infty$ and thus all values and Q-values are finite: $Q^{\pi}(s,s') > -\infty$ for any $s \in \cS, s' \in \vout(s)$ It implies that $\pi'$ is also positive. Thus, its trajectories are finite with probability 1 and yields $V^{\pi'}_{\lambda}(s_0) \geq V^{\pi}_{\lambda}(s_0)$. Finally, applying policy improvement to $\pistar$ we conclude the statement.
\end{proof}



\begin{proof}[Proof of Theorem~\ref{th:rl_reduction}.]
Let $\cP$ be the trajectory distribution induced by the GFlowNet backward policy and $\Ptraj{\pi}$ be the trajectory distribution induced by RL policy $\pi$. Then we rewrite the value function \eqref{eq:reg_value_function_def} in the following form using the tower property of conditional expectation to replace entropy with negative logarithm of the policy
$$
    V^{\pi}_{\lambda=1}(s_0) = \E_{\tau \sim \Ptraj{\pi}} \left[ \sum_{t = 0}^{n_\tau} r(s_t, s_{t + 1}) - \log \pi(s_{t+1} \mid s_t) \right]\,.
$$
Notice that there is no coefficient in front of entropy and reward because we set $\gamma = 1, \lambda=1$ by the theorem statement. Using simple algebraic manipulations
$$
    V^{\pi}_{\lambda=1}(s_0) = \E_{\tau \sim \Ptraj{\pi}} \left[ \sum_{t = 0}^{n_\tau} \log\exp(r(s_t, s_{t + 1})) - \log \pi(s_{t + 1} \mid s_t)\right] = \E_{\tau \sim \Ptraj{\pi}} \left[ \log  \frac{\prod_{t = 0}^{n_\tau} \exp (r(s_t, s_{t + 1}))}{\prod_{t = 0}^{n_\tau} \pi(s_{t + 1} \mid s_t)}\right]\,.
$$
Next, we notice that $r(s,s') = \log \PB(s|s')$ for all non-terminal $s'$ and, $r(s,s_f) = \log \cR(s) = \log \PB(s \mid s_f) + \log \cZ$ for terminal transitions due to the reward matching condition. Thus,
\[
    V^{\pi}_{\lambda=1}(s_0)  = \log \cZ -\E_{\tau \sim \Ptraj{\pi}} \left[ \log  \frac{\prod_{t = 0}^{n_\tau} \pi(s_{t + 1} \mid s_t)}{\prod_{t = 0}^{n_\tau} P_B(s_t \mid s_{t + 1})}\right] = \log \cZ -\operatorname{KL}(\Ptraj{\pi} \vert \cP)\,.
\]
Here $\cP$ is a trajectory distribution induced by $\PB$~\eqref{eq:backward_distribution}. We note that the final equation is the same as the one in Proposition 1 of~\cite{tiapkin2024generative} for the acyclic case.

Thus, the optimal policy $\pistar$ that maximizes $V^{\pi}_{\lambda=1}(s_0)$ is the one that minimizes $\operatorname{KL}(\Ptraj{\pi} \vert \cP)$. By Proposition~\ref{th:pf_db}, there exists a unique forward policy $\PF$ that induces the same trajectory distribution as $\PB$, which is equivalent to achieving zero KL-divergence. Thus, $\pistar$ coincides with $\PF$, and we conclude the statement by the uniqueness of the solution (see \Cref{lem:uniqeness_reg_shortest_path}). To apply \Cref{lem:uniqeness_reg_shortest_path}, without loss of generality, we can assume that the GFlowNet reward function $\cR$ is normalized, i.e., $\cZ = 1$ and $\log \cR(x) \leq 0$. Indeed, since a terminating transition $x \to s_f$ is always visited exactly once, it is equivalent to subtracting $\log \cZ$ from all terminal rewards, which does not change the optimal policy and modifies all values by the same constant.

Next, consider soft optimal Bellman equations \eqref{eq:reg_optimal_bellman} for non-terminating transitions
$$
\Qstar_{\lambda=1}(s, s') = \log \PB(s \mid s') + \log \sum_{s'' \in \vout(s')} \exp(\Qstar_{\lambda=1}(s', s''))\,.
$$
Let us show that $\Qstar_{\lambda=1}(s, s') = \log \cF(s \to s')$ will satisfy the equations.
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\cF(s \to s')  = \log\cF(s') + \log \PB(s \mid s')& =   \log\sum_{s'' \in \vout(s')} \cF(s' \to s'') + \log \PB(s \mid s')  \\
& = \log \PB(s \mid s') + \log \sum_{s'' \in \vout(s')} \exp\left(\log \cF(s' \to s'')\right).
\end{split}
\end{equation*}
Here we used equations from Proposition~\ref{th:flow_eqs}. For terminating transitions we simply have $\Qstar_{\lambda=1}(s, s_f) = r(s, s_f) = \log \cR(s) = \log \cF(s \to s_f)$. Since there exists a unique solution to soft optimal Bellman equations, we have proven $\Qstar_{\lambda=1}(s, s') = \log \cF(s \to s')$. As for state flows, we have
$$
\Vstar_{\lambda=1}(s) = \log \sum_{s' \in \vout(s)} \exp(\Qstar_{\lambda=1}(s, s')) =  \log \sum_{s' \in \vout(s)} \exp\left(\log \cF(s \to s')\right) =  \log \cF(s)\,.
$$
Thus the proof is concluded.
\end{proof}