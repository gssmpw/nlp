\section{Experimental Details}\label{app:exp_details}

\subsection{Loss Choice}\label{app:loss_choice}

While~\cite{brunswic2024theory} used the original flow matching loss~\cite{bengio2021flow} for experimental evaluation, it was previously shown to be less computationally efficient and provide slower convergence than other GFlowNet losses~\cite{malkin2022gflownets, madan2023learning} in acyclic case, so we carry out experimental evaluation with the more broadly employed detailed balance loss~\cite{bengio2023gflownet}. Moreover, flow matching loss does not admit explicit parameterization of a backward policy, as well as training with fixed backward policies, thus not allowing us to study some of the phenomena we explore in the experiments. 

In addition, we note that the proposed state flow regularization~\eqref{eq:RDB_loss} can be potentially applied with other GFlowNet losses that learn flows, e.g. $\SubTB$~\cite{madan2023learning}, or with the modification of $\DB$ proposed in~\cite{deleu2022bayesian} that implicitly parametrizes flows as $\cF(s) = \cR(s) / \PF(s_f \mid s)$.

\subsection{Hypergrids}\label{app:exp_grids}

Formally, $\cS \setminus \{s_0, s_f\}$ is a set of points with integer coordinates inside a $D$-dimensional hypercube with side length $H$: $\left\{\left(s^1, \ldots, s^D\right) \mid s^i \in\{0,1, \ldots, H-1\}\right\}$. $s_0$ and $s_f$ are auxiliary states that do not correspond to any point inside the grid. Possible transitions correspond to increasing or decreasing any coordinate by $1$ without exiting the grid. In addition, for each state $s \in \cS \setminus \{s_0, s_f\}$ there is a terminating transition $s \to s_f$. GFlowNet reward at $s = (s^1, \ldots, s^D)$ is defined as
\begin{align*}
\cR(s) \triangleq R_0 &+ R_1 \prod_{i = 1}^D \mathbb{I}\left[0.25 < \left|\frac{s^i}{H-1}-0.5\right|\right] + R_2 \prod_{i = 1}^D \mathbb{I}\left[0.3 < \left|\frac{s^i}{H-1}-0.5\right| < 0.4\right]\eqsp,
\end{align*}
%\vspace{-0.3cm}
where $0<R_0 \ll R_1<R_2$. \cite{brunswic2024theory} do not specify reward parameters used in their experiments, so we use the parameters from the acyclic version of the environment studied in~\cite{malkin2022trajectory}, i.e. $(R_0 = 10^{-3}, R_1 = 0.5, R_2 = 2.0)$. 

The utilized metric is:
$$\frac{1}{2}\sum_{x \in \cX} |\cR(x) / \cZ - \pi(x)|,$$
where $\pi(x)$ is empirical distribution of last $2 \cdot 10^5$ samples seen in training (endpoints of trajectories sampled from $\PF$).


All models are parameterized by MLP with 2 hidden layers and 256 hidden size, which accept a one-hot encoding of $s$ as input. $\cF_{\theta}(s), \PF(s' \mid s, \theta), \PB(s \mid s', \theta)$ share the same backbone, with different linear heads predicting logarithm of the state flow, logits of the forward policy and logits of the backward policy. In case of the fixed $\PB$, $s_{\text{init}}$ corresponds to the center of the grid and we take $\varepsilon = 10^{-8}$ (see Appendix~\ref{app:fix_and_learn_pb}).

We train all models on-policy. We use Adam optimizer with a learning rate of $10^{-3}$ and a batch size 16 (number of trajectories sampled at each training step). For $\log \cZ_{\theta}$ we use a larger learning rate of $10^{-2}$ (see  \cite{malkin2022trajectory}). All models are trained until $2 \cdot 10^6$ trajectories are sampled, and the empirical sample distribution $\pi(x)$ is computed over the last $2 \cdot 10^{5}$ samples seen in training. For $\SDB$ we set $\varepsilon=1.0$ and $\eta = 10^{-3}$. We found that using larger values of $\eta$ can lead to smaller expected trajectory length, but also significantly interfere with sampling fidelity of the learned GFlowNet, thus we opt for these values in our experiments.


\subsection{Permutations}\label{app:exp_perms}


All models are parameterized by MLP with 2 hidden layers and 128 hidden size, which accept a one-hot encoding of $s$ as input. $\cF_{\theta}(s), \PF(s' \mid s, \theta), \PB(s \mid s', \theta)$ share the same backbone, with different linear heads predicting logarithm of the state flow, logits of the forward policy and logits of the backward policy. In case of the fixed $\PB$, $s_{\text{init}}$ corresponds to the permutation $(n, n - 1, \dots, 2, 1)$ and we take $\varepsilon = 10^{-8}$ (see Appendix~\ref{app:fix_and_learn_pb}). 

We train all models on-policy. We use Adam optimizer with a learning rate of $10^{-3}$ and a batch size 512 (number of trajectories sampled at each training step). We found that using small batch sizes can significantly hinder training stability for this environment; thus, we opt for a larger value. All models are trained for $10^5$ iterations. For $\log \cZ_{\theta}$ we use a larger learning rate of $10^{-2}$ (see \cite{malkin2022trajectory}). Empirical distribution $\hat{C}(k)$ is computed over the last $10^{5}$ samples seen in training. For $\SDB$ we set $\varepsilon=1.0$ and $\eta = 10^{-3}$. We found that using larger values of $\eta$ can potentially lead to smaller expected trajectory length, but also significantly interfere with sampling fidelity of the learned GFlowNet, thus we opt for these values in our experiments.

Suppose that $x_1, \dots, x_m$ is a set of GFlowNet samples (terminal states of trajectories sampled from $\PF$). Then, the empirical $L_1$ error of fixed point probabilities is defined as:
$$
\sum_{k=0}^N \left| C(k) - \frac{1}{m}\sum_{i=1}^m \mathbb{I}\{x_i(k)=k\}\right|,
$$
and the relative error of mean reward is defined as 
$$
\left|\frac{\E[\cR(x)] - \frac{1}{m}\sum_{i=1}^m \cR(x_i)}{\E[\cR(x)]}\right|,
$$
where $\E[\cR(x)] = \sum_{x \in \cX} \cR(x) \frac{\cR(x)}{\cZ}$.

\subsubsection{Reward Distribution Properties}\label{app:exp_perms_vals}

We define the GFlowNet reward as $\cR(s) = \exp(\frac{1}{2}\sum_{k=1}^{n} \mathbb{I}\{s(k) = k\})$. We are interested in true values of three quantities:
\begin{enumerate}
    \item normalizing constant $\cZ = \sum_{x \in \cX} \cR(x)$,
    \item true expected reward $\E[\cR(x)] = \sum_{x \in \cX} \cR(x) \frac{\cR(x)}{\cZ}$,
    \item fixed point probabilities $C(k) = \mathbb{P}\left(\left(\sum_{i=1}^{n} \mathbb{I}\{x(i) = i\}\right) = k\right)$ with respect to the reward distribution.
\end{enumerate}

While computing sums over all permutations is intractable for $n$ above some threshold, below, we show that for this particular reward, analytical expressions for these quantities can be derived. 

First, we will derive the formula for the total number of permutations of length $n$ with exactly $k$ fixed points, which we will denote as $D(k, n)$. In combinatorics, such permutations are known as partial derangements, and the quantity is known as rencontres numbers \citep[p.180]{comtet1974advanced}. Note that 
$$D(k, n) = \binom{n}{k}D(0, n-k),$$
since choosing a permutation with $k$ fixed points coincides with choosing $k$ positions for fixed points, and permuting the remaining elements such that there are no fixed points among them. So let us start with derivation of $D(0, n)$. Denote $S_i$ to be the set of permutations on $n$ elements that has a fixed point on position $i$. Then, by inclusion-exclusion principle we have 
\begin{equation*}
\begin{split}
    \left|S_1 \cup \cdots \cup S_n\right| & =\sum_i\left|S_i\right|-\sum_{i<j}\left|S_i \cap S_j\right|+\sum_{i<j<k}\left|S_i \cap S_j \cap S_k\right|+\cdots+(-1)^{n+1}\left|S_1 \cap \cdots \cap S_n\right| \\
    & =\binom{n}{1}(n-1)!-\binom{n}{2}(n-2)!+\binom{n}{3}(n-3)!-\cdots+(-1)^{n+1}\binom{n}{n} 0! \\
    & =\sum_{i=1}^n(-1)^{i+1}\binom{n}{i}(n-i)! = n!\sum_{i=1}^n \frac{(-1)^{i+1}}{i!}.\\
\end{split}
\end{equation*}
Then 
$$D(0, n) = n! - \left|S_1 \cup \cdots \cup S_n\right| = n! - n!\sum_{i=1}^n \frac{(-1)^{i+1}}{i!} = n!\sum_{i=0}^n \frac{(-1)^{i}}{i!}.$$
Thus, we have 
$$D(k, n) = \binom{n}{k}D(0, n - k) = \frac{n!}{k!(n-k)!}(n-k)!\sum_{i=0}^{n-k} \frac{(-1)^{i}}{i!} = n!\sum_{i=0}^{n-k} \frac{(-1)^{i}}{i!k!}.$$

Finally, all of the quantities we are interested in are easily expressed through $D(k, n)$:

\begin{enumerate}
    \item $\cZ = \sum_{k = 0}^n D(k, n)\exp(k/2)$.
    \item $\E[\cR(x)] = \sum_{k = 0}^n D(k, n)\exp(k/2)\frac{\exp(k/2)}{\cZ}$.
    \item $C(k) = D(k, n)\frac{\exp(k/2)}{\cZ}$.
\end{enumerate}

For reference, the formula yields values of $\log \cZ \approx$ $3.8262$, $11.2533$, $42.9843$ for $n=4$, $n=8$, and $n=20$ respectively.

