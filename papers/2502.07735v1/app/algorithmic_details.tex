\section{Algorithmic Details}\label{app:algo_details}

\subsection{Training Policy and Flow Weighting}\label{app:flow_weighting}


Recall the optimization problem in~\eqref{eq:opt_cyclic_gflow}:
\begin{align*}
\label{eq:opt_cyclic_gflow}
%\left\{
\min\limits_{\cF, \PF, \PB} &\; \sum\limits_{s \in \cS \setminus \{s_0, s_f\}} \cF(s) \\
\text{subject to}&\; \left( \log\frac{\cF(s)\PF(s' \mid s)}{\cF(s')\PB(s \mid s')}\right)^2 = 0\eqsp, & \forall s \to s' \in \cE \eqsp, \notag \\
& \cF(s_f) \PB(x | s_f) = \cR(x)\eqsp,&  \forall x \to s_f \in \cE\eqsp.\notag
%\right.
\end{align*}

Now, suppose that training with $\DB$ loss~\eqref{eq:DB_loss} and state flow regularization~\eqref{eq:RDB_loss} is done on-policy, i.e. trajectories are collected using the trained policy $\PF$. Let us write down the expected gradient of the loss summed over a trajectory (note that regularization is not applied to $\cF(s_0)$ and $\cF(s_f)$)
$$
\E_{\tau \sim \PF}\left[\sum_{t = 0}^{n_\tau} \nabla_\theta \left(\log \frac{\cF_{\theta}(s) \PF(s_{t+1} \mid s_t, \theta)}{\cF_{\theta}(s_{t+1})\PB(s_t \mid s_{t+1}, \theta)} \right)^2 + \sum_{t = 1}^{n_\tau} \lambda \nabla_\theta \cF_\theta(_t)\right],
$$
which can be rewritten as
$$
\E_{\tau \sim \PF}\left[\sum_{t = 0}^{n_\tau} \nabla_\theta \mathcal{L}_{\mathrm{DB}}(s_t \to s_{t+1}) \right] + \lambda \E_{\tau \sim \PF}\left[\sum_{t = 1}^{n_\tau} \nabla_\theta \cF_\theta(s_t) \right].
$$

The first term is the expected gradient of the standard $\DB$ loss. As for the second term, we note that if $\cF_{\theta}$ is exactly the state flow induced by $\PF$, we have
\begin{equation*} %\label{eq:cyclic_flows}
\begin{split}
\E_{\tau \sim \PF}\left[\sum_{t = 1}^{n_\tau} \nabla_\theta \cF_\theta(s_t) \right] & = \E_{\tau \sim \PF}\left[\sum_{s \in \cS \setminus \{s_0, s_f\}} \sum_{t = 0}^{n_\tau} \mathbb{I}\{s_t = s\} \nabla_\theta \cF_\theta(s)  \right]  \\
& = \sum_{s \in \cS \setminus \{s_0, s_f\}} \nabla_\theta \cF_\theta(s) \E_{\tau \sim \PF}\left[\sum_{t = 0}^{n_\tau} \mathbb{I}\{s_t = s\} \right]  \\
& = \sum_{s \in \cS \setminus \{s_0, s_f\}} \frac{\cF_{\theta}(s)}{\cF_\theta(s_f)}\nabla_\theta \cF_\theta(s) = \frac{1}{2\cF_\theta(s_f)} \nabla_\theta \left( \sum_{s \in \cS \setminus \{s_0, s_f\}} \cF_{\theta}(s)^2 \right).
\end{split}
\end{equation*}

This implies that on-policy training tries to minimize the sum of squared state flows rather than the sum of state flows. This happens due to the fact that the trajectory distribution that is used to collect data for training (induced by $\PF$ in this case) visits certain states more often then others, thus a weight is given to the flow in each state equal to the expected number of visits. However, if $\PF(s \mid s_0)$ is fixed to be uniform over $\cS \setminus \{s_0, s_f\}$ (see Section~\ref{sec:experiments} and Appendix~\ref{app:fix_and_learn_pb}), this issue can be circumvented by applying flow regularizer only in the first state of each sampled trajectory. Then, equal weight will be given to $\cF_\theta(s)$ in each state in the expected loss, thus we will be minimizing the sum of state flows. However, in our experiments we noticed that this does not significantly influence the results, thus we leave exploring this phenomenon as a further research direction.

\subsection{Loss Scaling and Stability}\label{app:scaling_stability}

In this section, we provide a more detailed explanation of our scaling hypothesis (see Section~\ref{sec:experiments}). Let us consider a GFlowNet that learns $\cF$, $\PF$ and $\PB$. Since these quantities are predicted by a neural network, a standard way is to make it predict logits for the forward policy, logits for the backward policy, and logarithm of the state flow. Flow functions are always positive, thus predicting them in log scale is a natural approach~\cite{bengio2021flow, bengio2023gflownet}. Then, for any transition $s \to s'$, define two quantities:
\begin{equation}% \label{eq:flow_errors}
\begin{split}
\Delta_{\log \cF}(s,s',\theta) &\triangleq \log\cF_{\theta}(s) + \log\PF(s' | s, \theta) 
 - \log \cF_{\theta}(s') - \log\PB(s | s', \theta)\eqsp, \\\
\Delta_{\cF}(s,s',\theta) &\triangleq \exp\left(\log\cF_{\theta}(s) + \log\PF(s' | s, \theta)\right) 
 - \exp\left(\log \cF_{\theta}(s') + \log\PB(s | s', \theta)\right)\eqsp.
\end{split}
\end{equation}
The first is difference between predicted logarithms of the flows in the forward and backward direction $\log \cF_F - \log \cF_B$, while the second is difference between predicted flows in the forward and backward direction $ \cF_F - \cF_B$.
Then, the standard $\DB$ loss~\eqref{eq:DB_loss} is
$$
\mathcal{L}_{\DB}(s \to s') =  \Delta_{\log \cF}(s,s',\theta)^2, 
$$
and the $\SDB$ loss~\eqref{eq:DB_loss} proposed in~\cite{brunswic2024theory} is
$$
\mathcal{L}_{\SDB}(s \to s') =  \log\left(1 + \varepsilon \Delta_{\cF}(s,s', \theta)^2\right) \cdot (1 + \eta \cF_\theta(s)).
$$

However, for both losses one can either replace $\Delta_{\log \cF}$ with $\Delta_{\cF}$ or the other way around. For visualization, let us fix the predicted log backward flow $\cF_B$ to be, e.g., $1$, and plot the losses with respect to the varying value of the predicted log forward flow $\cF_F$. The plots are presented in Figure~\ref{fig:losses}. One can note that as argument $\log \cF_F$ decreases, both losses in $\Delta_{\cF}$ scale quickly plato, thus their derivative goes to zero. From the optimization perspective this means that when the predicted log flow needs to be \textit{increased}, the gradient step will be very small since the derivative of the loss is almost zero. On the other hand, when the predicted log flow needs to be \textit{decreased}, the gradient step will be larger since losses have much higher derivatives in the corresponding regions. In combination with Proposition~\ref{th:total_flow}, this gives a possible explanation to stability of $\Delta_{\cF}$ scale losses: \textit{they are biased towards underestimation of the flows, and, as a result, biased towards solutions with smaller expected trajectory length.} We note that the same reasoning can be applied to stable flow matching loss proposed in~\cite{brunswic2024theory} since it also operates with differences between flows in $\Delta_{\cF}$ scale.

However, as we show in our experimental evaluation (Section~\ref{sec:experiments}), \textit{this comes at the cost of learning GFlowNets that match the reward distribution less accurately}.


\begin{figure}[t!]
    %\vspace{-0.1cm}
    \centering
    \includegraphics[width=0.47\linewidth]{figures/losses_db.pdf}
    \includegraphics[width=0.47\linewidth]{figures/losses_sdb.pdf}
    \caption{Plots for $\DB$ and $\SDB$ losses in $\Delta \cF$ and $\Delta \log \cF$ scales with fixed predicted log backward flow $= 1$ and varying predicted log forward flow. More specifically, \textcolor{Green}{green} curve is $y = (x - 1)^2$, \textcolor{red}{red} curve is $y = \left(e^x - e^1\right)^2$, \textcolor{Brown}{brown} curve is $y = \log\left( 1+ (x - 1)^2 \right) \cdot (1 + 0.001e^x)$, \textcolor{blue}{blue} curve is $y = \log\left( 1+ \left(e^x - e^1\right)^2 \right) \cdot (1 + 0.001e^x)$}.  
    \label{fig:losses}
\end{figure}

\subsection{Fixed $\PB$ and Trainable $\PB$}\label{app:fix_and_learn_pb}

In non-acyclic environments, $s_0$ and $s_f$ generally are fictive states that do not correspond to any object. Then $\PF(s_f \mid s)$ corresponds to probability to terminate a trajectory in state $s$, while $\PF(s \mid s_0)$ corresponds to the probability that a trajectory starts in the state $s$. Thus, the choice of $\vout(s_0)$ is crucial in the design of the environment. If this set is large, e.g., coincides with $\in \cS \setminus \{s_0, s_f\}$, one has to fix $\PF(s \mid s_0)$ to some distribution, e.g. uniform, otherwise learning becomes intractable. However, in this case $\PB(s_0 \mid s)$ has to be trainable, otherwise it may be impossible to satisfy detailed balance conditions for transitions $s_0 \to s$.


In our experiments, we consider two settings: training with a fixed $\PB$ and using a trainable $\PB$. 

In case of fixed $\PB$, we consider the case when $\vout(s_0) = \{ s_{\text{init}} \}$, where $s_{\text{init}}$ is some fixed state $\in \cS \setminus \{s_0, s_f\}$. Thus the first transition for all trajectories is to go from $s_0$ to $s_{\text{init}}$. Then, for any $s \in \cS \setminus \{s_0, s_f, s_{\text{init}}\}$, $\PB(\cdot \mid s)$ is uniform over the parents of $s$, while $\PB(s_0 \mid s_{\text{init}}) = 1 - \varepsilon$ for some small $\varepsilon > 0$ and $\PB(s \mid s_{\text{init}}) = \varepsilon / (\vin(s_{\text{init}}) - 1)$ for other transitions $s \to s_{\text{init}}$.



For a trainable $\PB$, we consider the case when $\vout(s_0) = \cS \setminus \{s_0, s_f\}$. Here we fix the first forward transition probability $\PF(s \mid s_0)$ to be uniform over $\cS \setminus \{s_0, s_f\}$. In this case, $\DB$ loss for the first transition takes a special form:
%\begin{small}
\begin{equation}\label{eq:first_db}
{\cL_{\DB}(s_0 \to s) \triangleq \bigg(\log \cZ_{\theta} - \log | \cS \setminus \{s_0, s_f\}| - \log \PB(s_0 \mid s, \theta) - \log \cF_{\theta}(s)  \bigg)^2, }
\end{equation}
where $\log \cZ_{\theta} - \log | \cS \setminus \{s_0, s_f\}|$ corresponds to $\log \cF_\theta(s_0) + \log \PF(s \mid s_0)$. An important note is that $\log \cF_\theta(s_0)$ for optimal solutions always coincides with $\log \cZ$; thus, it is usually harmful to apply state flow regularization~\eqref{eq:RDB_loss} to it.

\subsection{Solving Small Environments Exactly}\label{app:small_env_solution}

Suppose we have a fixed backward policy $\PB$ and a final flow $\cF(s_f)$. Then, induced flows $\cF$ and the corresponding forward policy $\PF$ can be obtained exactly for small environments. Consider the following system of linear equations with respect to $\hat{\cF}(s)$ that arises from Proposition~\ref{th:flow_eqs}:
%$$
%\hat{\cF}(s) = \sum_{s' \in \vout(s)} \PB(s \mid s')\hat{\cF}(s'), \; \forall s \in \cS \setminus \{s_f\}.
%$$
\begin{equation}\label{eq:state_flow_system}
\left\{
\arraycolsep=1.5pt\def\arraystretch{2.2}
\begin{array}{l}
\hat{\cF}(s) = \sum_{s' \in \vout(s)} \PB(s \mid s')\hat{\cF}(s'), \; \forall s \in \cS \setminus \{s_f\}, \\
\hat{\cF}(s_f) = \cF(s_f).
\end{array}
\right.
\end{equation}
The system has $|\cS|$ variables and $|\cS|$ equations. $\hat{\cF}(s) = \cF(s)$ is a solution, where $\cF(s)$ are state flows induced by $\PB$ and $\cF(s_f)$, and the uniqueness of the solution follows from Proposition~\ref{th:pb_from_flow}. Thus, by solving the system, one can exactly find induced state flows. Then, by Proposition~\ref{th:flow_eqs} and Proposition~\ref{th:pf_db}, edge flows and $\PF$ can also be exactly expressed as
$$
\cF(s \to s') = \PB(s \mid s')\cF(s'), \;\; \PF(s' \mid s) = \PB(s \mid s')\cF(s') / \cF(s).
$$
Finally, by Corollary~\ref{th:total_flow}, one can find the expected trajectory length of the induced trajectory distribution $\cP$ as:
$$
    \E_{\tau \sim \cP}[n_\tau] = \frac{1}{\cF(s_f)}\sum\limits_{s \in \cS \setminus \{s_0, s_f\}} \cF(s).
$$

Interestingly, the system~\eqref{eq:state_flow_system} can also be explained from Markov Chain perspective. Let us take the graph $\cG$ with reversed edges, add a loop from $s_0$ to itself, and use $\PB$ to define a Markov Chain: $P(s_0 \mid s_0) = 1$, $P(s \mid s') = \PB(s \mid s')$ if there is an edge $s \to s'$, and $P(s \mid s') = 0$ otherwise. It will be an absorbing Markov Chain, with an only absorbing state $s_0$ since it is reachable from any other state by Assumption~\ref{assumption}. Its transition matrix can be written in the following way:
\[P = \left[\begin{array}{ c | c }
    Q & R \\
    \hline
    \mathbf{0} & 1
  \end{array},\right]\]
where $Q$ is a $|\cS| - 1$ by $|\cS| - 1$ matrix and $R$ is a $|\cS| - 1$ by $1$ matrix. Its fundamental matrix $N$, i.e., such matrix that $N_{s, s'}$ is equal to the expected number of visits to a non-absorbing state $s'$ before being absorbed when starting from a non-absorbing state $s$, can be obtained as:
$$
N = \sum_{k=0}^{+\infty}Q^k = (I - Q)^{-1},
$$
where $I - Q$ is always invertible (\citealp{kemeny1969finite}, Theorem 3.2.1). One can note that normalized flows $\cF(s)/\cF(s_f)$ coincide with the expected number of visits to $s$ when starting from $s_f$, thus coincide with the row of matrix $N$ corresponding to $s_f$. Finally, notice that $(I - Q)$ coincides with the transposed matrix of the truncated system~\eqref{eq:state_flow_system} (with the exception of the variable and the equation corresponding to $s_0$), thus such system has a unique solution $\cF(s_f)(I - Q)^{-T} e_{s_f} = \cF(s_f)N^{T} e_{s_f}$, where $e_{s_f}$ is a vector of size $|\cS| - 1$ that has $1$ on the position corresponding to $s_f$ and $0$ on all others. Variable corresponding to $s_0$ should be handled separately, but it is easy to see $\hat{\cF}(s_0) = \sum_{s' \in \vout(s)} \PB(s \mid s'){\cF}(s') = \cF(s_0)$.

