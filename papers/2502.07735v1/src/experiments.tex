

\vspace{-0.2cm}
\section{Experiments}
\label{sec:experiments}

In addition to verifying our theoretical findings, one of the goals of our experimental evaluation is to examine the \textit{scaling hypothesis} that we put out:

%\vspace{-0.1cm}
\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
%\textit{
\textbf{Scaling hypothesis.} When $\PB$ is trainable, the main factor that plays a crucial role in loss stability in practice, i.e., controlled mean trajectory length of the trained non-acyclic GFlowNet, is the scale in which the error between flows is computed. Indeed, the standard $\DB$ loss~\eqref{eq:DB_loss} operates in log-flow scale $\Delta \log \cF$, while standard $\SDB$~\eqref{eq:StableDB_loss} operates in flow scale $\Delta \cF$. We hypothesize that using log-flow scale losses without regularization can lead to arbitrary large trajectory length, while flow scale losses are biased towards solutions with smaller flows and thus do not suffer from this issue.
%}
%\vspace{-0.1cm}
\end{tcolorbox}

In this section, we use $\DB$ or $\SDB$ to specify the utilized loss, $\Delta \log \cF$ or $\Delta \cF$ to specify the flow scale used to compute the error, and use $\lambda = C$ to specify the strength of the proposed state flow regularization (see Section~\ref{sec:learn_non_acyclic}). For example, $(\DB, \Delta \log \cF)$ in the legend correspond to~\eqref{eq:DB_loss}, $(\SDB, \Delta \cF)$ corresponds to~\eqref{eq:StableDB_loss} and $(\DB, \Delta \log \cF, \lambda=C)$ corresponds to~\eqref{eq:RDB_loss}. Detailed discussion on loss scaling and stability is provided in \Cref{app:scaling_stability}. 

Source code: \href{https://github.com/GreatDrake/non-acyclic-gfn}{github.com/GreatDrake/non-acyclic-gfn}.



%\vspace{-0.1cm}
\subsection{Experimental Setting}
\label{sec:exp_setup}
%\vspace{-0.1cm}

We consider two discrete environments for experimental evaluation: 1) a non-acyclic version of the hypergrid environment~\cite{bengio2021flow} that was introduced in~\cite{brunswic2024theory}; 2) non-acyclic permutation generation environment from~\cite{brunswic2024theory} with a harder variant of the reward function. Experimental details are presented in Appendix~\ref{app:exp_details}.

Mean sample reward was used as a metric in~\cite{brunswic2024theory}, with higher values considered better. However, \textit{we point out that this does not always represent sampling accuracy from the reward distribution $\cR/\cZ$}. Indeed, the model that learned to sample from the highest-reward mode still achieves a high average reward despite resulting in mode collapse. For instance, recent works argue that measuring the deviation of mean sample reward from the true expected reward $\sum_{x \in \cX} \cR(x) \frac{\cR(x)}{\cZ}$ results in a better metric, see, e.g., \cite{shen2023towards} for detailed motivation. In addition, we employ other metrics to track sampling accuracy depending on the environment, which we discuss in detail below. 

In both environments, we consider two settings: training with a fixed backward policy $\PB$ that is almost uniform and using a trainable $\PB$. In the second case, the initial log flow $\log \cF_{\theta}(s_0) = \log \cZ_{\theta}$ is also being learned. Thus, we can examine its convergence to the true normalizing constant $\log \cZ$. See Appendix~\ref{app:fix_and_learn_pb} for details.


%\vspace{-0.1cm}
\subsection{Hypergrids}
%\vspace{-0.1cm}
\label{sec:grids}

\begin{figure}[t!]

    %\vspace{-0.1cm}
  \centering
    \includegraphics[width=0.95\linewidth]{figures/grid_7_2.pdf} 

    %\vspace{-0.5cm}
  \caption{Comparison of non-acyclic GFlowNet training losses on a small hypergrid environment. We use $\DB$ or $\SDB$ to specify the utilized loss, $\Delta \log \cF$ or $\Delta \cF$ to specify the flow scale used to compute the error in the loss, and use $\lambda = C$ to specify the usage of the proposed state flow regularization. \textit{Top:} evolution of $L^1$ distance between an empirical distribution of samples and target distribution. \textit{Bottom:} evolution of mean length of sampled trajectories.}\label{fig:mol_results}
  %\vspace{-0.2cm}
\label{fig:small_grid}

\end{figure}



We start with non-acyclic hypergrid environments~\cite{brunswic2024theory}. These environments are small enough that the normalizing constant $\cZ$ can be computed exactly, and the trained sampler can be efficiently evaluated against the exact reward distribution. States are points with integer coordinates $s \in \{0,\ldots,H-1\}^{D}$ inside a $D$-dimensional hypercube with side length $H$, plus two auxiliary states $s_0$ and $s_f$. Possible transitions correspond to increasing or decreasing any coordinate by $1$ without exiting the grid. Moreover, each state has a terminating transition $s \to s_f$, thus $\cX = \cS \setminus \{s_0, s_f\}$. The reward function has modes near the grid corners, separated by wide troughs with very small rewards. To measure sampling accuracy, total variation distance is computed between the true reward distribution $\cR(x) / \cZ$ and an empirical distribution of the last $2 \cdot 10^5$ samples seen in training, which coincides with $\frac{1}{2}$ of $L^1$ distance on discrete domains.% (which coincides with half of the $L^1$-distance).

\begin{figure*}[!t]

    %\vspace{-0.1cm}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/grid_20_4.pdf}

    %\vspace{-0.15cm}
    \caption{Comparison of non-acyclic GFlowNet training losses on a larger hypergrid environment. We use $\DB$ or $\SDB$ to specify the utilized loss, $\Delta \log \cF$ or $\Delta \cF$ to specify the flow scale used to compute the error in the loss, and use $\lambda = C$ to specify the usage of the proposed state flow regularization. \textit{Left:} evolution of $L^1$ distance between an empirical distribution of samples and target distribution. \textit{Middle:} evolution of mean length of sampled trajectories. \textit{Right:} evolution of the trained initial log flow $\log \cZ_\theta$. \vspace{-0.05cm}} 
    %\vspace{-0.4cm}
\label{fig:big_grid}
\end{figure*}

{\renewcommand{\arraystretch}{1.0}
\setlength{\tabcolsep}{3pt}
\begin{table*}[t]
%\vspace{-0.35cm}
\caption{Comparison on the permutation environment. $C(k) \; L^1$ is $L^1$ distance between true and empirical distribution of fixed point probabilities $C(k)$, $\Delta \cR$ is relative error of mean reward proposed in \cite{shen2023towards}, $\Delta \log \cZ$ is $|\log \cZ_\theta - \log \cZ|$. Mean and std values are computed over 3 random seeds. \highlight{Blue} indicates the best metric, \highlightr{red} indicates the smallest expected trajectory length.}
\vspace{-0.25cm}
\centering
\begin{center}
\begin{footnotesize}
%\begin{sc}
\begin{tabular}{@{}l|cccc|cccc@{}}
%\label{resnets}
    \toprule
      &
      \multicolumn{4}{c}{$n=8$} & \multicolumn{4}{c}{$n=20$}    \\
     \cmidrule(l){2-9}  
     Loss
               & $C(k) \; L^1 \downarrow$ & $\Delta \cR \downarrow$
               & $\Delta \log \cZ \downarrow$ & $\E[n_\tau]$ 
               & $C(k) \; L^1 \downarrow$ & $\Delta \cR \downarrow$
               & $\Delta \log \cZ \downarrow$ & $\E[n_\tau]$\\
     \midrule
    \DB, $\Delta \cF$
    & $0.215$ \scalebox{0.7}{$\!\pm\!0.198$}
    & $0.214$ \scalebox{0.7}{$\!\pm\!0.086$}
    & $0.814$ \scalebox{0.7}{$\!\pm\!0.826$}
    & \highlightr{$2.43$ \scalebox{0.7}{$\!\pm\!0.28$}}
    %----------------
    & $0.453$ \scalebox{0.7}{$\!\pm\!0.002$}
    & $0.343$ \scalebox{0.7}{$\!\pm\!0.000$}
    & $42.98$ \scalebox{0.7}{$\!\pm\!0.000$}
    & \highlightr{$2.00$ \scalebox{0.7}{$\!\pm\!0.00$}}\\
    %\cmidrule(l){1-11} 
    \SDB, $\Delta \cF$
    & $0.031$ \scalebox{0.7}{$\!\pm\!0.012$}
    & $0.046$ \scalebox{0.7}{$\!\pm\!0.023$}
    & $0.074$ \scalebox{0.7}{$\!\pm\!0.025$}
    & $3.32$ \scalebox{0.7}{$\!\pm\!0.15$}
    %----------------
    & $0.452$ \scalebox{0.7}{$\!\pm\!0.001$}
    & $0.343$ \scalebox{0.7}{$\!\pm\!0.000$}
    & $42.98$ \scalebox{0.7}{$\!\pm\!0.000$}
    & \highlightr{$2.01$ \scalebox{0.7}{$\!\pm\!0.00$}}\\
   %\cmidrule(l){1-11} 
    \DB, $\Delta \log \cF$, $\lambda=10^{-3}$
    & $0.036$ \scalebox{0.7}{$\!\pm\!0.015$}
    & $0.056$ \scalebox{0.7}{$\!\pm\!0.024$}
    & $0.018$ \scalebox{0.7}{$\!\pm\!0.010$}
    & {$2.80$ \scalebox{0.7}{$\!\pm\!0.04$}}
    %----------------
    & $0.041$ \scalebox{0.7}{$\!\pm\!0.002$}
    & $0.064$ \scalebox{0.7}{$\!\pm\!0.000$}
    & $0.023$ \scalebox{0.7}{$\!\pm\!0.005$}
    & $3.23$ \scalebox{0.7}{$\!\pm\!0.00$}\\
    \SDB, $\Delta \log \cF$, $\lambda=10^{-3}$
    & $0.037$ \scalebox{0.7}{$\!\pm\!0.013$}
    & $0.056$ \scalebox{0.7}{$\!\pm\!0.019$}
    & $0.020$ \scalebox{0.7}{$\!\pm\!0.015$}
    & {$2.79$ \scalebox{0.7}{$\!\pm\!0.04$}}
    %----------------
    & $0.041$ \scalebox{0.7}{$\!\pm\!0.002$}
    & $0.064$ \scalebox{0.7}{$\!\pm\!0.000$}
    & $0.026$ \scalebox{0.7}{$\!\pm\!0.003$}
    & $3.22$ \scalebox{0.7}{$\!\pm\!0.00$}\\
    \DB, $\Delta \log \cF$, $\lambda=10^{-5}$
    & \highlight{${0.005}$ \scalebox{0.7}{$\!\pm\!0.001$}}
    & \highlight{$0.001$ \scalebox{0.7}{$\!\pm\!0.000$}}
    & \highlight{$0.005$ \scalebox{0.7}{$\!\pm\!0.004$}}
    & $4.31$ \scalebox{0.7}{$\!\pm\!0.05$}
    %----------------
    & $0.017$ \scalebox{0.7}{$\!\pm\!0.002$}
    & $0.035$ \scalebox{0.7}{$\!\pm\!0.002$}
    & \highlight{$0.003$ \scalebox{0.7}{$\!\pm\!0.003$}}
    & $7.55$ \scalebox{0.7}{$\!\pm\!0.50$}\\
    \SDB, $\Delta \log \cF$, $\lambda=10^{-5}$
    & \highlight{$0.005$ \scalebox{0.7}{$\!\pm\!0.001$}}
    & $0.002$ \scalebox{0.7}{$\!\pm\!0.000$}
    & \highlight{$0.006$ \scalebox{0.7}{$\!\pm\!0.006$}}
    & $4.36$ \scalebox{0.7}{$\!\pm\!0.09$}
    %----------------
    & \highlight{$0.014$ \scalebox{0.7}{$\!\pm\!0.001$}}
    & \highlight{$0.025$ \scalebox{0.7}{$\!\pm\!0.001$}}
    & \highlight{$0.005$ \scalebox{0.7}{$\!\pm\!0.005$}}
    & $7.31$ \scalebox{0.7}{$\!\pm\!0.07$}\\
    \bottomrule
    \end{tabular}
%\end{sc}
\end{footnotesize}
\end{center}
\vspace{-0.25cm}
%\vskip -0.1in
\label{perms_table}
\end{table*}}


We begin our analysis with a $7 \times 7$ grid to study the effects of learning under a fixed backward policy compared to a trainable backward policy. Since the environment is small, it is possible to find the flows induced by the fixed $\PB$ exactly, thus also its expected trajectory length, see Appendix~\ref{app:small_env_solution}. Figure~\ref{fig:small_grid} presents the results. First, we note that both $(\DB, \Delta \log \cF)$ and $(\SDB, \Delta \cF)$ with fixed $\PB$ converge to the true expected trajectory length induced by the fixed backward policy, which is in line with Corollary~\ref{th:fix_pb_learning}. However, for all losses, using trainable $\PB$ allows us to find a solution with a smaller trajectory length. In addition, we observe that using a loss in $\Delta \cF$ scale results in slower convergence and a slight bias in the learned forward policy than in case of $\Delta \log \cF$ scale, for both fixed and learned $\PB$. Finally, an interesting note is that using an unstable $\DB$ loss in $\Delta \log \cF$ scale without state flow regularization \textit{can} still result in a small expected trajectory length, as we see in this experiment. However, we further show that this is not the case for a larger environment.

Next, we consider a larger $20 \times 20 \times 20 \times 20$ hypergrid. An expected trajectory length induced by the chosen fixed backward policy is several orders of magnitude larger than for a smaller grid, making this approach impractical. While one can try to manually find a fixed $\PB$ with a smaller expected trajectory length, this is generally a challenging problem, thus we consider only the setting of trainable $\PB$ here. Our findings are presented in Figure~\ref{fig:big_grid}. Similarly to $7 \times 7$ grid, we find that learning in $\Delta \cF$ scale results in a biased policy both for $\DB$ and $\SDB$, and this bias is noticeably larger than in the smaller grid. In $\Delta \log \cF$ scale, both $\DB$ and $\SDB$ employed with state flow regularization learn to correctly sample from the reward distribution. While all methods converge to similar expected trajectory length, $\Delta \cF$ scale losses have smaller $n_\tau$ in the middle of the training even when employed without regularization, which supports our scaling hypothesis. In addition, Figure~\ref{fig:big_grid_app} in \Cref{app:add_plots} shows that for both losses in $\Delta \log \cF$ scale, a mean length of sampled trajectories tends to infinity when the training is done without state flow regularization. Finally, we note that $\Delta \log \cF$ losses correctly learn the true normalizing constant $\cZ$, while $\Delta \cF$ losses perform worse.
 

%\vspace{-0.15cm}
\subsection{Permutations}
\label{sec:perms}
%\vspace{-0.15cm}


Next, we consider the environment corresponding to the Cayley Graph of the group of permutations on $n$ elements $\{1, 2, \dots, n\}$ from~\cite{brunswic2024theory}. Each state $s \in \cS \setminus \{s_0, s_f\}$ is a permutation of fixed length $(s(1), \dots, s(n))$, and there are $n - 1$ possible transitions that correspond to swapping a pair of adjacent elements $s(k)$ and $s(k+1)$, plus a transition that corresponds to a circular shift of the permutation to the right $(s(n), s(1), \dots, s(n-1))$. In addition, each state has a terminating transition $s \to s_f$. GFlowNet reward utilized in the experiments of~\cite{brunswic2024theory} is $\mathbb{I}[s(1) = 1]$. We argue that this results in a fairly simple task, and a trivial forward policy exists that just applies circular shift until $1$ is on the first position. We opt for using a more complex reward distribution in our experiments and define GFlowNet reward in terms of the number of fixed points in a permutation $\cR(s) = \exp\left(\frac{1}{2}\sum_{k=1}^n\mathbb{I}\{s(k) = k\}\right)$. 

Since with the growth of $n$, the number of states $n!$ quickly becomes too large to compute total variation distance as it was done for hypergrid, we track convergence of a number of statistics to their true respective values. Firstly, we compute the relative error between mean reward of GFlowNet samples and true expected reward as it was proposed in~\cite{shen2023towards}. Secondly, denote $C(k)$ as the probability that a permutation sampled from the reward distribution has $k$ fixed points. We compute $L^1$ error between the vector $(C(0), C(1), \dots, C(n))$ and its empirical estimate over last $10^5$ samples seen in training. Finally, we track the convergence of the trained $\log \cZ_{\theta}$ to the true value of $\log \cZ$. In Appendix~\ref{app:exp_perms_vals}, we show how true reference values of these quantities can be efficiently computed.

Table~\ref{perms_table} presents the results for $n=8$ and $n=20$. Here, $\PB$ is trained in all cases. While for $n=8$, the environment is still relatively small, $n=20$ results in a more challenging environment with $\approx 2.4 \cdot 10^{18}$ states, thus the trained neural network needs to generalize to states unseen during training in order to match the reward distribution. Firstly, we note that while $\Delta \cF$ scale losses can learn the reward distribution to some capacity for $n=8$, they fail for $n=20$. However, in all cases, they converge to small $\E[n_\tau]$, supporting our scaling hypothesis. On the other hand, we find that GFlowNets training with $\Delta \log \cF$ losses and state flow regularization converges to low values of reward distribution approximation errors for both $n=8$ and $n=20$. In addition, we see that using a smaller regularization coefficient $\lambda$ on the one hand results in a model with a larger expected trajectory length, but on the other hand, results in a model that better matches the reward distribution.  Finally, we perform the same experiment as for hypergrids (Figure~\ref{fig:small_grid}) with a fixed $\PB$ compared to a trainable $\PB$ on small permutations of length $n=4$, and make similar observations to the ones presented in Section~\ref{sec:grids}. The results are presented in Figure~\ref{fig:small_perms_app} in \Cref{app:add_plots}. 

The key observations from our experimental evaluation are: 
\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]%
    \begin{enumerate}[itemsep=-2pt,leftmargin=5pt]
        \item Learning with a fixed $\PB$ is possible without stable losses and regularization, however, manually picking $\PB$ with small $\E[n_\tau]$ is challenging;
        \item When $\PB$ is trained, our results empirically support the scaling hypothesis, showing that even the standard $\DB$ in $\Delta \cF$ scale is stable; however, non-acyclic GFlowNets trained with $\Delta \cF$ scale losses often fail to accurately match the reward distribution; 
        \item Both $\DB$ and $\SDB$ in $\Delta \log \cF$ scale result in better matching the reward distribution but need to be utilized with state flow regularization to ensure small expected trajectory length $\E[n_\tau]$.
    \end{enumerate}
\end{tcolorbox}


