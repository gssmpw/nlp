
\vspace{-0.2cm}
\section{Theory of GFlowNets in Discrete Non-Acyclic Environments}
\label{sec:construction}


\subsection{Environment}\label{sec:cyclic_env}
All definitions regarding the environment can be introduced similarly to the setting of acyclic GFlowNets (see Section~\ref{sec:background_gflow}) with one main difference: graph $\cG$ can now contain cycles. We make two technical assumptions on the structure of $\cG$:
\begin{assumption}
\label{assumption}
    1) There is a special initial state $s_0$ with no incoming edges and a special sink state $s_f$ with no outgoing edges; 2) For any state $s \in \cS$ there exists a path from $s_0$ to $s$ and a path from $s$ to $s_f$.
\end{assumption}


Next, we formally define trajectories:


\begin{definition}
\label{def:trajectory}
A sequence $\tau = (s_0 \to s_1 \to \ldots \to s_{n_{\tau}} \to s_{n_{\tau}+1} = s_f)$ is called a trajectory of length $n_{\tau} \in \mathbb{N}$ if all transitions $s_{t} \to s_{t+1} \in \mathcal{E}, t \in \{0,\ldots,n_{\tau}\}$. Then $\cT$ is a set of all finite length trajectories which start in $s_0$ and finish in $s_f$.
\end{definition}

\vspace{-0.1cm}
In the above definition and further, we use a convention $s_{n_{\tau} + 1} = s_f$. The main difference with acyclic GFlowNets is that $\cT$ can be infinite, and $\cT$ can contain trajectories of arbitrary length. 

\subsection{Backward Policy and Trajectory Distribution}
There are several equivalent ways to introduce probability distributions over trajectories in GFlowNets. One of the common approaches is to start by introducing trajectory flows~\cite{bengio2023gflownet}. The main theoretical advantage of the approach based on trajectory flows is that it allows for non-Markovian flows, see \cite{bengio2023gflownet}. At the same time, Markovian flows are primarily considered in GFlowNets literature, and in our paper, we only consider this setting. Instead of starting from the definition of the flow, a more intuitive approach is to begin with the definitions of the \emph{forward} and \emph{backward} policies.
\begin{definition}
\label{def:backward_policy}
A forward policy $\PF(s' \mid s)$ consistent with $\cG$ is a family of conditional probability distributions over $s' \in \vout(s)$ defined for each $s \in \cS \setminus \{s_f\}$, Similarly, a backward policy $\PB(s \mid s')$ consistent with $\cG$ is a family of conditional probability distribution over $s \in \vin(s')$, defined for each $s' \in \cS \setminus \{s_0\}$.
\end{definition}

\vspace{-0.1cm}
In the subsequent parts of the paper, we always assume that the considered $\PF$ or $\PB$ are consistent with $\cG$ and do not specify this fact explicitly. \Cref{def:backward_policy} is consistent with the definitions of forward and backward policies in acyclic GFlowNets (\citealp{bengio2023gflownet}, Definition~4). Note that the structure of $\cG$ is symmetric with respect to an interchange between initial state $s_0$ and sink state $s_f$ and reversion of all edges in $\cG$. Thus, starting with either $\PF$ or $\PB$ is equivalent. We prefer to start from a backward policy $\PB$ in our subsequent derivations. Using $\PB$, we define a probability distribution $\cP$ over $\tau = \left(s_0 \to s_1 \to \ldots \to s_{n_{\tau}} \to s_f\right) \in \cT$ according to
\vspace{-0.2cm}
\begin{equation}
\label{eq:backward_distribution}
%\textstyle
\cP(\tau) \triangleq \prod_{t = 0}^{n_\tau} \PB(s_t \mid s_{t + 1})\eqsp.
\end{equation}
\vspace{-0.4cm}

In such a case, we say that the trajectory distribution $\cP(\tau)$ is \textit{induced by $\PB$}. In the following lemma, we show that $\cP(\tau)$ is a correctly defined probability distribution over $\cT$. 

\begin{lemma}
\label{th:fixed_pb}
Let $\PB(s \mid s')$ be a backward policy, such that $\PB(s \mid s') > 0$ for any edge $(s \to s') \in \cE$. Then
\begin{itemize}[noitemsep, nolistsep]
    \item $\cP(\tau)$ defined in \eqref{eq:backward_distribution} is a probability distribution over $\cT$, that is, $\sum_{\tau \in \cT} \cP(\tau) = 1$.
    \item Moreover, its expected trajectory length is always finite $\E_{\bf{\tau} \sim \cP}[n_{\bf{\tau}}] = \sum_{\tau \in \cT} n_{\tau} \cP(\tau) < +\infty$.
\end{itemize}
\end{lemma}
In fact, the condition $\PB(s' \mid s) > 0$  together with \Cref{assumption} allows us to ensure that the sequence $s_i$ is a finite state-space absorbing Markov chain. Given this assumption, the proof of \Cref{th:fixed_pb} almost coincides with the proof of the fact that the states of such a Markov chain are non-recurrent, see, e.g., \cite{levin2017markov}. For completeness, we provide the proof in Appendix~\ref{app:pb_proof}.

\subsection{State and Edge Flows}
Given a probability distribution $\cP(\tau)$ induced by $\PB$, our next aim is to define state and edge flows. Before proceeding with a valid construction, we provide some intuition about our definitions.  Let us first show that, contrary to the acyclic GFlowNets, we can not define edge flows as \textit{visitation probabilities} $\cP(\{\tau \in \cT \mid s \to s' \in \tau\})$.
%\begin{equation}
%\label{eq:flow_mathcing_wrong}
%\textstyle 
%\cP(\{\tau \in \cT \mid s \to s' \in \tau\})\eqsp,
%\end{equation}
In particular, we show that such a definition does not satisfy the flow matching conditions \eqref{eq:fm}. Indeed, consider an example from~\cite{brunswic2024theory}:
\[
%\textstyle 
\xymatrix{
 s_0  \ar[r]^{1}   &a  \ar[r]^{0.5}  &b \ar@/_1pc/[r]^{1} &c  \ar@/_1pc/[l]^{0.5}\ar[r]^{1}&s_f
}
\]
The number above each edge $(s \to s')$ is $\PB(s \mid s')$. Consider the distribution $\cP(\tau)$ defined by \eqref{eq:backward_distribution}. Let us plot the {visitation probability} for each edge:
$$
\xymatrix{
 s_0  \ar[r]^{1}   &a  \ar[r]^{\textcolor{red}{1}}  &\textcolor{red}{b} \ar@/_1pc/[r]^{\textcolor{red}{1}} &\textcolor{red}{c}  \ar@/_1pc/[l]^{\textcolor{red}{0.5}}\ar[r]^{\textcolor{red}{1}}&s_f
}
$$
One can see that the flow matching condition \eqref{eq:fm} does not hold for states $b$ and $c$ since $1 \neq 1 + 0.5$. Instead, let us calculate the \textit{expected number of visits} for each edge $s \to s'$
\[
%\textstyle 
\E_{\tau \sim \cP}\left[ \sum_{t = 0}^{n_\tau} \ind\{s_{t} = s, s_{t + 1} = s'\}\right]\eqsp.
\]
We visualize the corresponding numbers on the plot below:
$$
\xymatrix{
 s_0  \ar[r]^{1}   &a  \ar[r]^{\textcolor{blue}{1}}  &\textcolor{blue}{b} \ar@/_1pc/[r]^{\textcolor{blue}{2}} &\textcolor{blue}{c}  \ar@/_1pc/[l]^{\textcolor{blue}{1}}\ar[r]^{\textcolor{blue}{1}}&s_f
}
$$
It is easy to check that the flow matching constraints \eqref{eq:fm} are now satisfied. Next, we formally define:
\begin{definition}
\label{def:cyclic_flows}
Let $\PB(s \mid s')$ be a backward policy, such that $\PB(s \mid s') > 0$ for any edge $(s \to s') \in \cE$. Then, given a \emph{final flow} $\cF(s_f) > 0$, we define state and edge flows as
\begin{align} 
\cF(s \to s') &\triangleq \cF(s_f) \cdot \;\E_{\tau \sim \cP}\left[ \sum_{t = 0}^{n_\tau} \ind\{s_{t} = s, s_{t + 1} = s'\}\right], \nonumber  \\
\cF(s) & \triangleq \cF(s_f) \cdot \;\E_{\tau \sim \cP(\tau)}\left[ \sum_{t = 0}^{n_{\tau}+1} \ind\{s_t = s\}\right] \label{eq:flow_eqs}\eqsp.
\end{align}
We say that the flows defined above are induced by the backward policy $\PB$ and final flow $\cF(s_f)$.
\end{definition}
It is important to note that if $\cG$ does not contain cycles, the expected number of visits in \eqref{eq:flow_eqs} coincides with visitation probability, thus Definition~\ref{def:cyclic_flows} agrees with the usual understanding of flows in the acyclic GFlowNet literature. Next, we show that state and edge flows defined in \eqref{def:cyclic_flows} satisfy the detailed balance and flow matching conditions \eqref{eq:db} --  \eqref{eq:fm}.

\begin{proposition}
\label{th:flow_eqs}
Flows $\cF$ defined in~\Cref{def:cyclic_flows} satisfy:
\begin{enumerate}%[noitemsep, nolistsep]
    \item $\cF(s) \overset{(a)}{=} \sum\limits_{s' \in \vout(s)} \cF(s \to s') \overset{(b)}{=} \sum\limits_{s'' \vin(s)} \cF(s'' \to s),$ \\
    for each $s \in \cS \setminus \{s_0, s_f\}$. Moreover, identity (a) holds for $s_0$, and (b) holds for $s_f$. 
    \item $\cF(s \to s') = \cF(s')\PB(s \mid s')$ for any $(s \to s') \in \cE$.
    \item $\cF(s_0) = \cF(s_f)$.
\end{enumerate}
\end{proposition}

We provide the proof in Appendix~\ref{app:flow_eq_proof}. In the next proposition, we show that there is a one-to-one correspondence between a pair ($\PB, \cF(s_f)$) and edge flows $\cF$. Its proof is provided in \Cref{app:pb_from_flow_proof}.
\begin{proposition}
\label{th:pb_from_flow}
Let $\cF : \cE \to \mathbb{R}_{>0}$ be a function that satisfies flow matching conditions~\eqref{eq:fm}. Define the corresponding backward policy by the relation
\begin{equation*}
%\textstyle
\PB(s \mid s') = \cF(s \to s') \;\; / \sum_{s'' \in \vin(s')} \cF(s'' \to s')\eqsp.
\end{equation*}
Then $\cF$ are edge flows induced by $\PB$ and $\cF(s_f) =  \sum_{s'' \in \vin(s_f)} \cF(s'' \to s_f)$.
%and the corresponding probability $\cP(\tau)$ defined in \eqref{eq:backward_distribution}. Then $\cF$ satisfy equations \eqref{eq:flow_eqs}. 
\end{proposition}


\subsection{Forward Policy and Detailed Balance}
It is well-known in acyclic GFlowNets theory \cite{bengio2023gflownet} that there exists a unique forward policy $\PF$ for any backward policy $\PB$ that induces the same probability distribution over $\cT$. The main implication of this fact is that by fixing rewards $\cR(x), x \in \cX$ and a backward policy $\PB(s \mid s')$ for each state $s' \in \cS \setminus \{s_0, s_f\}$, one automatically fixes a trajectory distribution $\cP(\tau)$ that satisfies the reward matching condition~\cite{malkin2022trajectory}. However, sampling from such distribution is intractable since it requires starting from a terminal state sampled from the reward distribution. Thus, during GFlowNet training, one tries to find a forward policy, which always allows tractable sampling of trajectories by construction, that will match this trajectory distribution $\cP(\tau)$. One can note that this bears similarities with hierarchical variational inference~\cite{malkin2023gflownets}. In the following proposition, we show that this result holds also for non-acyclic GFlowNets.

\begin{proposition}\label{th:pf_db}
Given any backward policy $\PB(s \mid s') > 0$, there exists a unique forward policy $\PF(s' \mid s)$ such that
$$
\cP(\tau) = \prod_{t = 0}^{n_\tau} \PB(s_t \mid s_{t + 1}) = \prod_{t = 0}^{n_\tau} \PF(s_{t+1} \mid s_{t})\eqsp, \; \forall \tau \in \cT.
$$
Moreover, it satisfies the detailed balance conditions
$$
\cF(s)\PF(s' \mid s) = \cF(s')\PB(s \mid s'), \; \forall s \to s' \in \cE
$$
with the state flow $\cF$ defined in \eqref{eq:flow_eqs}. 
\end{proposition}
The proof is provided in Appendix~\ref{app:pf_db_proof}. Conversely, the following proposition shows that if a triplet $\cF$, $\PF$, $\PB$ satisfies detailed balance conditions \eqref{eq:db}, it will be consistent with all previous definitions and propositions.
\begin{proposition}\label{th:pf_db_reverse}
Let $\cF\colon \cS \to \mathbb{R}_{>0}$, and let $\PF(s'|s) > 0$, $\PB(s|s') > 0$ be forward and backward policies, such that detailed balance conditions \eqref{eq:db} are satisfied. Then $\PF$ and $\PB$ induce the same trajectory distribution:
$$
\cP(\tau) = \prod_{t = 0}^{n_\tau} \PB(s_t \mid s_{t + 1}) = \prod_{t = 0}^{n_\tau} \PF(s_{t+1} \mid s_{t})\eqsp, \; \forall \tau \in \cT.
$$
Moreover, then $\cF$ are state flows induced by $\PB$ and $\cF(s_f)$.
\end{proposition}
 For proof, we refer to Appendix~\ref{app:pf_db_reverse_proof}. The above propositions directly generalize their counterparts from the non-acyclic setting \cite{bengio2023gflownet}. Note that, due to the symmetries between $s_0$ and $s_f$ in $\cG$ up to changing direction of edges, we could start from the forward policy and trajectory distribution induced by it in \eqref{eq:backward_distribution}, and then prove the uniqueness of the corresponding backward policy $\PB$. 

\subsection{Training Non-Acyclic GFlowNets}
\label{sec:learn_non_acyclic}

Now, we proceed with the main learning problem in GFlowNets: finding a forward policy that matches the reward distribution over terminal states $\cR(x) / \cZ, x \in \cX$. The following Proposition shows how the reward matching condition can be formulated in terms of flows.
\begin{proposition}\label{prop:reward_matching}
    Let $\PB(s \mid s') > 0$ be a backward policy, $\cF(s_f) > 0$ a final flow, and $\cR(x) > 0$ GFlowNet rewards. If edge flows $\cF(s \to s')$ induced by $\PB$ and $\cF(s_f)$ satisfy: 
    \begin{equation}\label{eq:flow_reward_matching}
        \cF(x \to s_f) = \cR(x) \;\; \forall (x \to s_f) \in \cE\eqsp,
    \end{equation}
    the trajectory distribution $\cP$ induced by $\PB$~\eqref{eq:backward_distribution} satisfies the reward matching condition, i.e. $\P_{\tau \sim \cP}[s_{n_\tau} = x] = \cR(x) / \cZ$. Then, the same trajectory distribution is induced by the unique corresponding forward policy $\PF$, thus also satisfying the reward matching condition.
\end{proposition}
\begin{proof}
Notice that an edge leading into $s_f$ can be visited only once; thus, $\cF(x \to s_f)$ coincides with a probability $\P_{\tau \sim \cP}[s_{n_\tau} = x]$ that a trajectory terminates in $x$ times the final flow $\cF(s_f)$. In addition, we have $\cF(s_f) = \sum_{x \in \vin(s_f)} \cF(x \to s_f) = \sum_{x \in \cX} \cR(x) = \cZ$, thus $\P_{\tau \sim \cP}[s_{n_\tau} = x] = \cF(x \to s_f) / \cF(s_f) = \cR(x) / \cZ$.
\end{proof}

Proposition~\ref{prop:reward_matching} also implies $\cF(s_0) = \cF(s_f) = \cZ$ and $\PB(x \mid s_f) = \cR(x) / \cZ$ by Proposition~\ref{th:flow_eqs}.


An important fact from the literature on acyclic GFlowNets~\cite{malkin2022trajectory, bengio2023gflownet} that was overlooked in the work of~\cite{brunswic2024theory}, but holds in non-acyclic case as well, is that it is generally easy to manually pick a backward policy such that the induced trajectory distribution will satisfy reward matching condition. A simple and natural choice is to take $\PB(x \mid s_f) = \cR(x) / \cZ$ for $s_f$ and fix $\PB(s \mid s') = 1 / |\vin(s')|$ for all other states. It is worth mentioning that $\cZ$ is generally unknown, but this issue is circumvented in GFlowNets by learning unnormalized flows or making $\cZ$ itself a learnable parameter depending on the chosen loss function~\cite{malkin2022trajectory, bengio2023gflownet, madan2023learning}. Moreover, Proposition~\ref{th:pf_db} shows the uniqueness of the corresponding $\PF$. Thus, we state the main practical corollary of this result:
\begin{corollary}\label{th:fix_pb_learning}
    When a backward policy $\PB > 0$ is fixed, any loss from the acyclic GFlowNet literature~\cite{bengio2021flow, malkin2022trajectory, bengio2023gflownet, madan2023learning} can be used to learn the corresponding forward policy $\PF$ in the non-acyclic case as well. Lemma~\ref{th:fixed_pb} and Proposition~\ref{th:pf_db} imply that there is always a unique solution with a finite expected trajectory length, thus the stability of the loss~\cite{brunswic2024theory} does not play a factor.
\end{corollary}

The main disadvantage of learning with a fixed backward policy in non-acyclic GFlowNets that does not arise in acyclic GFlowNets is the fact that expected trajectory length $\E_{\tau \sim \cP}[n_{\tau}]$ of a manually chosen $\PB$ can be large. A natural way to circumvent this issue is to consider a learnable backward policy, which is also a widely employed choice in acyclic GFlowNet literature \cite{malkin2022trajectory, jang2024pessimistic,gritsaev2024optimizing}. However, \cite{brunswic2024theory} made an important discovery by pointing out that standard losses from acyclic GFlowNet literature are not stable (Theorem 3), meaning that the expected trajectory length can grow uncontrollably during training. The concept of stability was introduced with respect to learnable edge flows (Definition 3), which implies that the corresponding backward policy also changes during training. Using a stable loss, e.g.~\eqref{eq:StableDB_loss}, was proposed as way to approach this issue. At the same time, we argue that efficient training of a non-acyclic GFlowNet with controlled expected trajectory length in case of a learnable $\PB$ is possible without utilizing stable losses. The next proposition is a simple corollary of Definition~\ref{def:cyclic_flows}:
\begin{proposition}
\label{th:total_flow}
    Given a trajectory distribution $\cP$, its expected trajectory length is equal to the normalized total flow:
    \begin{equation}
    \label{eq:sample_time_flows_equality}
    %\textstyle 
    \E_{\tau \sim \cP}[n_\tau] = \frac{1}{\cF(s_f)} \sum\limits_{s \in \cS \setminus \{s_0, s_f\}} \cF(s).
    \end{equation}
\end{proposition}
The proof is presented in Appendix~\ref{app:total_flow_proof}.
This result is a refinement of Theorem 2 of \cite{brunswic2024theory}, which states only '$\leq$' inequality in \eqref{eq:sample_time_flows_equality}. Thus, we believe one of our key contributions to be pointing out the following fact:
%Recall that the reward matching condition implies $\cF(s_f) = \cZ$, hence, minimizing total normalized flow and minimizing total flow in \Cref{th:total_flow} is equivalent.

\begin{tcolorbox}[colback=colorblue,
    colframe=black,
    arc=4pt,
    boxsep=0.3pt,
]Learning a non-acyclic GFlowNet with the smallest expected trajectory length is \textit{equivalent} to learning a non-acyclic GFlowNet with the smallest total flow.
\end{tcolorbox}

We also believe that exploiting this equivalence is a crucial direction for future research on non-acyclic GFlowNets. We further explore a particular solution, which suggests the use of a state flow as a regularizer in the existing GFlowNet losses. We consider an example of the detailed balance loss \DB~\eqref{eq:DB_loss}. In this case, \Cref{th:pf_db_reverse} implies that learning a non-acyclic GFlowNet with the smallest expected trajectory length can be formulated as the following constraint optimization problem:
\begin{align}
\label{eq:opt_cyclic_gflow}
%\left\{
\min\limits_{\cF, \PF, \PB} &\; \sum\limits_{s \in \cS \setminus \{s_0, s_f\}} \cF(s) \\
\text{subject to}&\; \left( \log\frac{\cF(s)\PF(s' \mid s)}{\cF(s')\PB(s \mid s')}\right)^2 = 0\eqsp, & \forall s \to s' \in \cE \eqsp, \notag \\
& \cF(s_f) \PB(x \mid s_f) = \cR(x)\eqsp,&  \forall x \to s_f \in \cE\eqsp.\notag
%\right.
\end{align}


As an approximate way to solve \eqref{eq:opt_cyclic_gflow}, one can use \DB~\eqref{eq:DB_loss} with \textit{state flow regularization}:
\begin{equation}
\label{eq:RDB_loss}
\left(\log \frac{\cF_{\theta}(s) \PF(s' \mid s, \theta)}{\cF_{\theta}(s')\PB(s \mid s', \theta)} \right)^2 + \lambda \cF_\theta(s) \eqsp,
\end{equation}
where $\lambda > 0$ is a hyperparameter that controls a trade-off between an expected trajectory length and an accuracy of matching the reward distribution. As in~\eqref{eq:DB_loss}, reward matching is enforced by substituting $\cF_{\theta}(s_f)\PB(x  \mid s_f, \theta) = \cR(x)$.

Note that the $\DB$ loss is defined on individual transitions, and during training, it is optimized across transitions collected by a training policy. A standard choice is to optimize it over transitions from trajectories sampled using $\PF$, yet the training policy can be chosen differently, %as long as it has full support.
or, in RL terms, training can be done in an on-policy or off-policy fashion, see \cite{tiapkin2024generative}. Note that different states $s$ might appear with different frequencies in the loss depending on a training policy, which can lead to
a bias in the optimization problem~\eqref{eq:opt_cyclic_gflow}. We discuss this phenomenon in more detail, as well as potential ways to mitigate it, in Appendix~\ref{app:flow_weighting}.

Finally, it is important to mention that flow-based regularizers in a non-acyclic case were already proposed in Theorem 1 of \cite{brunswic2024theory}, but only for the stable loss setup. Moreover, they were introduced in order to find an acyclic flow. Our paper further explores and sheds new light on this phenomenon, showing that training can be carried out even when an unstable loss is utilized with regularization. Moreover, when the total flow is minimized, one can ensure the smallest possible expected trajectory length.

%\vspace{-0.1cm}
\subsection{Connections with Entropy-Regularized RL}
A recent line of works \cite{tiapkin2024generative,deleu2024discrete} studied connections between GFlowNets and RL, showing that the GFlowNet learning problem is equivalent to an entropy-regularized RL~\cite{neu2017unified, geist2019theory} problem in an appropriately constructed deterministic MDP, given that the backward policy is fixed. We show that the same result holds for non-acyclic GFlowNets as well. 

Let $\cG$ be a graph of a non-acyclic GFlowNet, $\cR$ a GFlowNet reward, and $\PB > 0$ a fixed backward policy that satisfies the reward matching condition. Let $\cF$ be the flow induced by $\PB$ with $\cF(s_f) = \cZ$, and $\PF$ be a unique forward policy corresponding to $\PB$ (see Proposition~\ref{th:pf_db}). Define a deterministic MDP $\cM_{\cG}$ induced by a graph $\cG$, where the state space $\cS$ coincide with vertices of $\cG$, action space $\cA_s$ for each state $s$ correspond to $\vout(s)$, and transition kernel is defined as transition in the graph $\text{P}(s' \mid s, a) = \ind\{a = s'\}$, $a \in \vout(s)$. We use no discounting ($\gamma = 1.0$) and set RL rewards for terminating transitions $r(x, s_f) = \log \cR(x)$, and for all other transitions $r(s, s') = \log \PB(s \mid s')$. Then, the following statement holds.

\begin{theorem}[Generalization of Theorem 1 \cite{tiapkin2024generative}]\label{th:rl_reduction}
    The optimal policy $\pistar_{\lambda=1}(s' \mid s)$ for the entropy-regularized MDP $\cM$ with coefficient $\lambda=1$ is equal to $\PF(s' \mid s)$ for all $s \in \cS \setminus \{ s_f \}, s' \in \cA_s$. Moreover, regularized optimal value $\Vstar_{\lambda=1}(s)$ and Q-value $\Qstar_{\lambda=1}(s,s')$ coincide with $\log \cF(s)$ and $\log \cF(s \to s')$ respectively.
\end{theorem}
%\vspace{-0.1cm}
The proof and all missing definitions are provided in \Cref{app:rl_reduction_proof}. Note that the proof of \cite{tiapkin2024generative} cannot be directly transferred to the non-acyclic setting since it is based on induction over the topological ordering of vertices of $\cG$, which exists only for acyclic graphs. 
