%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}
\usepackage{hyperref}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}

% inlined bib file
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{filecontents}
\usepackage{wasysym}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{ising_v1}
\usepackage{algpseudocode}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{cleveref}
\usepackage{subfigure}
\usepackage{threeparttable}

\usepackage{listings}
\usepackage{xcolor}

\renewcommand{\lstlistingname}{Code}
\newcommand{\jyl}[1]{{\color{brown}[jyl: #1]}}

\Crefname{listing}{Code}{Codes}

\lstdefinestyle{mystyle}{frame=tb,
    language=C,
    backgroundcolor=\color{white},   
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    basicstyle=\ttfamily\scriptsize,
    % basicstyle=\ttfamily\footnotesize\fontfamily{pcr}\selectfont,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    % numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    escapeinside={(*@}{@*)},
}
\lstset{style=mystyle}

% \usepackage{xr}
% \externaldocument{appendix}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\solution}{Collider\xspace}

\newcommand{\mycircle}{\scalebox{0.7}{\CIRCLE}}
\newcommand{\myleftcircle}{\scalebox{0.7}{\LEFTcircle}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\newcommand\mycommfont[1]{\small{#1}}
\SetCommentSty{mycommfont}

\def\submission{0}

\definecolor{CodeHighlight}{RGB}{255, 220, 220}

% % inlined bib file
% \usepackage{filecontents}

% %-------------------------------------------------------------------------------
% \begin{filecontents}{\jobname.bib}
% %-------------------------------------------------------------------------------
% @Book{arpachiDusseau18:osbook,
%   author =       {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau Andrea C.},
%   title =        {Operating Systems: Three Easy Pieces},
%   publisher =    {Arpaci-Dusseau Books, LLC},
%   year =         2015,
%   edition =      {1.00},
%   note =         {\url{http://pages.cs.wisc.edu/~remzi/OSTEP/}}
% }
% @InProceedings{waldspurger02,
%   author =       {Waldspurger, Carl A.},
%   title =        {Memory resource management in {VMware ESX} server},
%   booktitle =    {USENIX Symposium on Operating System Design and
%                   Implementation (OSDI)},
%   year =         2002,
%   pages =        {181--194},
%   note =         {\url{https://www.usenix.org/legacy/event/osdi02/tech/waldspurger/waldspurger.pdf}}}
% \end{filecontents}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
% \title{\Large \bf Accelerating Token Filtering for Efficient Large Langeuage Model Training}

% \title{\Large \bf Unlocking Full Efficiency of Token Filtering in Large Language Model Training}
\title{\Large \bf Enhancing Token Filtering Efficiency in Large Language Model Training with \solution} % Arxiv

% Unlock fully accelerating token filtering for efficient large language model training
% Unlocking the efficiency 

%for single author (just remove % characters)
% \author{
% % {Submission ID: 336}
% {\rm Your N.\ Here}\\
% Your Institution
% \and
% {\rm Second Name}\\
% Second Institution
% copy the following lines to add more authors
% \and
% {\rm Name}\\
% Name Institution
% } % end author

% \author{
% {\rm Di Chai$^1$, Pengbo Li$^1$, Feiyuan Zhang$^1$, Yilun Jin$^1$, Han Tian$^2$, Junxue Zhang$^{1\dag}$ and Kai Chen$^{1\dag}$}\\
% $^1$Hong Kong University of Science and Technology\\
% $^2$University of Science and Technology of China \ \ \ \ $^\dag$Corresponding Authors
% }

\makeatletter
\renewcommand{\@fnsymbol}[1]{\textcolor{black}{\ifcase#1\or \dagger\else\@arabic{#1}\fi}}
\makeatother

\author{
{\rm Di Chai$^1$, Pengbo Li$^1$, Feiyuan Zhang$^1$, Yilun Jin$^1$, Han Tian$^2$, Junxue Zhang$^{1}$\thanks{Junxue Zhang and Kai Chen are the corresponding authors.}, Kai Chen$^{1}$\textsuperscript{\dag}}\\
$^1$\textit{Hong Kong University of Science and Technology}\\
$^2$\textit{University of Science and Technology of China}
}

\maketitle

% \renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[1]{These authors contributed equally to this work.}

%-------------------------------------------------------------------------------
\begin{abstract}
    % Training high-quality large language models (LLMs) is computationally expensive, requiring substantial training tokens and resources. 
    Token filtering has been proposed to enhance utility of large language models (LLMs) by eliminating inconsequential tokens during training. While using fewer tokens should reduce computational workloads, existing studies have not succeeded in achieving higher efficiency. This is primarily due to the insufficient sparsity caused by filtering tokens only in the output layers, as well as inefficient sparse GEMM (General Matrix Multiplication), even when having sufficient sparsity.
    
    This paper presents \solution\footnote{A collider is a modern system in physics used for accelerating and colliding particles to uncover deeper insights into the fundamental nature of matter.}, a system unleashing the full efficiency of token filtering in LLM training. At its core, \solution filters activations of inconsequential tokens across all layers to maintain sparsity. Additionally, it features an automatic workflow that transforms sparse GEMM into dimension-reduced dense GEMM for optimized efficiency. Evaluations on three LLMs—TinyLlama-1.1B~\cite{tinyllama}, Qwen2.5-1.5B~\cite{qwen2}, and Phi1.5-1.4B~\cite{Phi1.5}—demonstrate that \solution reduces backpropagation time by up to 35.1\% and end-to-end training time by up to 22.0\% when filtering 40\% of tokens. Utility assessments of training TinyLlama on 15B tokens indicate that \highlight{\solution sustains the utility advancements of token filtering by relatively improving model utility by 16.3\% comparing to} regular training, and reduces training time from 4.7 days to 3.5 days using 8 GPUs. \solution is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code.

\end{abstract}
%-------------------------------------------------------------------------------

\input{sections/introduction}
\input{sections/background_motivation}
\input{sections/system}
\input{sections/discussion}
\input{sections/implementation}
\input{sections/evaluation}
\input{sections/related_work}
\input{sections/conclusion}

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{TokenFilter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks