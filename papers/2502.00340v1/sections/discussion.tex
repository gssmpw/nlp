\section{Discussion}

Apart from the existing designs in \solution, we would also like to discuss the following aspects of potential improvements brought by \solution (\eg, communication efficiency) and future directions of enhancing the performance of token fitlering systems.

\highlight{
\subsection{Filtering Activations Necessitates Different Training Parameters} \label{sec:dis:diff_training_params}

To retain sparsity and achieve efficiency improvements, \solution further filters the activations of inconsequential tokens across all layers during the backward computation. From the perspective of the chain rule in gradient propagation, the gradients of the remaining tokens (\ie, tokens that are not filtered out) are backpropagated to the front layers through all previous tokens (\ie, based on causal language modeling), including the filtered tokens.
Filtering the activations of inconsequential tokens implicitly eliminates the gradients transferred from the remaining tokens to filtered ones, resulting in a reduced amount of gradients backpropagated to the front layers. Given the same inputs, \solution passes a smaller amount of gradients to update the models compared to methods that only filter the loss (\ie, Rho~\cite{RHO}). Therefore, \solution (\ie, after filtering the activations) necessitates different training parameters from existing work~\cite{RHO} to achieve optimal performance. For example, \solution is more likely to require a larger batch size to increase stability, and layer-wise learning rate adjustments~\cite{DBLP:conf/icmla/SinghDZGT15,DBLP:journals/corr/abs-2006-13484} could also be beneficial in inproving model utility in \solution.

}

\subsection{\solution Reduces Communication Overheads in Distributed Training} \label{sec:discussion:reducing_communication_overheads}

\solution optimizes computational efficiency in backward token filtering by transforming sparse GEMM into dense GEMM, which significantly reduces communication overheads in distributed LLM training. Specifically, \solution updates the entire computation graph, where the gradients and activations are reduced along the sequence dimension. Existing parallelism strategies that enable LLM training on distributed systems typically transfer gradients between different nodes or GPUs during backward computation. Therefore, reducing the sequence dimension of gradients can linearly decrease communication overheads in distributed training. We discuss the advantages of \solution across different parallelism strategies as follows.

\begin{icompact}
    \item Tensor Parallel (TP)~\cite{MegatronLM}. In TP, the transformer models are typically partitioned along the multiple heads and hidden dimensions. All-reduce of gradients on inputs are required twice (\ie, inputs of FFN and attention block) in each layer's backward computation. \solution can linearly reduce the amount of data transferred in each all-reduce operation.
    \item Sequence Parallel (SP)~\cite{SP}. SP is designed to be combined with TP to further reduce the memory usage caused by the redundant activations of dropout and layer normalization. In SP, the sequence dimension is partitioned on multiple devices through all-gather during computing dropout and layer normalization and recovered to partition on hidden dimensions through reduce-scatter. \solution can reduce the communication overhead in the corresponding all-gather and reduce-scatter operations.
    \item Pipeline Parallel (PP)~\cite{MegatronLM}. The advantages of using \solution in PP is straightforward as PP sequentially transfers the gradients in the graph which are linearly reduced by \solution.
    \item Mixture-of-Experts (MoE). The integration of \solution in MoE helps in optimizing communication between experts during the backward passes, thereby minimizing the data transferred across devices and enhancing throughput. Specifically, \solution ensures that only gradients of important tokens are routed to the corresponding experts.
\end{icompact}

Apart from reducing communication costs in various parallelism strategies, \solution is also beneficial for activation offloading methods, such as ZeRO-R~\cite{deepspeed}, by minimizing input/output (I/O) costs.

\subsection{Reducing Overhead of Reference Model} \label{sec:discussion:reducing_refmodel_overheads}

Existing studies~\cite{RHO} select filtered tokens based on a reference model, which may introduce additional computational overhead. The reference model uses the same architecture as the target training model and is fine-tuned on a manually curated, high-quality dataset~\cite{RHO}. Intuitively, the reference model helps filter out tokens that do not conform to the distribution of the high-quality dataset, which implicitly represents the domain on which we expect the target model to be trained. Although the approach to use a reference model has been proven effective, it has two significant drawbacks: 1) Training the reference model incurs additional costs, and it is computationally inefficient to update the knowledge within the reference model (\eg, updating the manually curated high-quality dataset requires retraining the model); 2) While inference results from the reference model can be prepared offline to reduce overhead during training, the inference process still consumes significant resources since the model has the same architecture as the target model (\ie, the same number of parameters).

Thus, it is necessary to reduce the overhead of the reference model to improve the practicality of token filtering systems. The key insight is that the reference model is used for filtering tokens and should therefore be loss-tolerant in the system (\eg, some deviation in output loss may yield the same filtering results). Here, we discuss two potential directions to reduce the overhead of the reference model.

\parab{Model compression.} We can employ model compression techniques to reduce the inference overhead of the reference model. For example, we can distill knowledge into a smaller model or directly prune the model to decrease the number of parameters. Other techniques, such as quantizing the reference model to low-bit precision, can also be utilized to further reduce inference overhead.

\parab{Combining with n-gram models.} We can also combine the reference model with n-gram models to reduce overhead. The n-gram approach is an early and quintessential method in statistical linguistics, based on the central assumption that the current word is influenced solely by the previous $n-1$ words. For a given sentence $w_1, w_2, ..., w_m$, the n-gram model calculates its probability by: 
\begin{equation}
	\vspace{-1mm}
    P(w_1, w_2, \dots, w_m) \approx \prod_{i=1}^{m} P(w_i \mid w_{i-1}, \dots, w_{i-n+1}).
	\vspace{-1mm}
\end{equation}
It is worth noting that the mathematical principle of n-gram models is similar to the concept of causal language modeling in LLMs, allowing n-gram models to serve as a substitute. In particular, n-gram models generally require much less time for training and inference, which is negligible compared to transformer-based models. Thus, we can quickly update the knowledge in the n-gram model.
We have also conducted prior experiments to validate the performance of n-gram models. We measured the Pearson correlation between the loss of n-gram and transformer-based models trained on the same dataset, with the results presented in \Cref{fig:dis_pearson_ngram_ref}. Additionally, we assessed the common ratio of filtered tokens between n-gram and transformer-based models, with the results shown in \Cref{fig:dis_common_ratio_ref_bi}. The findings indicate that the n-gram model exhibits high similarity to the transformer-based model in terms of token filtering, which supports the viability of using n-gram models to reduce the overhead of the reference model.

\begin{figure}[t!]
	\small
	\centering
	\subfigure[Distribution of pearson correlation between the loss of n-gram and transformer-based models.]{
		\centering
		\label{fig:dis_pearson_ngram_ref}
		\includegraphics[width=0.45\linewidth]{figures/dis_pearson_ngram_ref.pdf}
	}
	\hfil
	\subfigure[Distribution of common ratio of filtered tokens between n-gram and transformer-based models.]{
		\centering
		\label{fig:dis_common_ratio_ref_bi}
		\includegraphics[width=0.45\linewidth]{figures/dis_common_ratio_ref_bi.pdf}
	}
	\caption{The n-gram model shows high similarity with the transformer-based model in terms of token filtering.}
	\vspace{+1mm}
	\label{fig:dis:n-gram}
\end{figure}

In our system, we assume that the reference model is already trained and that the inference results are prepared offline. We leave the exploration of reducing the overhead of the reference model for future work.
