\section{Introduction}

Training high-quality large language models (LLMs) is notably resource-intensive, requiring substantial investments in both data and computational power. For example, the training of foundational models such as LLaMA 3-70B necessitates approximately 7 million GPU hours and over 15 trillion high-quality tokens \cite{dubey2024llama}. \emph{Token filtering} represents an emerging paradigm aimed at enhancing the cost-efficiency of LLM training by systematically discarding less significant tokens early in the training process\footnote{This paper primarily focuses on backward filtering, as it demonstrates superior performance in enhancing the capabilities of LLMs. Further details can be found in \S\ref{sec:background_llm}}. This methodology enables the model to concentrate on the most pertinent tokens, resulting in up to 30\% absolute improvement in model utility across various tasks~\cite{RHO}.

While the effectiveness of token filtering in enhancing model utility is well recognized within the AI community\footnote{Rho-1~\cite{RHO} received the Best Paper Runner-up Award at NeurIPS 2024.}, its potential to improve computational efficiency in training remains largely unexplored. In principle, by significantly reducing the number of tokens processed in the computational pipeline, token filtering should decrease computational demands and expedite training. However, our experiments, which combine token filtering with existing LLM training systems, demonstrate only a modest 1.2\% speedup in training time, even when 40\% of the tokens are eliminated (\S\ref{sec:efficiency_motivation}). This limited enhancement in training efficiency constrains the broader advantages of token filtering for large-scale LLM training. Therefore, we pose the question: \emph{Can we fully unlock the efficiency of token filtering while simultaneously achieving greater utility than conventional training?}

To address this question, we first investigate the key factors limiting the efficiency gains of existing token filtering system: (1) Insufficient sparsity after token filtering: Existing approaches fail to create true sparsity following token filtering~\cite{RHO}. These methods drop tokens only at the first stage of backpropagation---the loss computation layer---along with their corresponding loss values, without modifying the generated gradients. As a result, in subsequent backpropagation steps, the hidden states of the dropped tokens are still updated because they contribute to the computed gradients (via attention). This leads to dense matrix computations, undermining the efficiency of token filtering. (2) Inefficiency of sparse matrix implementations: Even when sparse matrices are employed, existing sparse GEMM (General Matrix Multiplication) implementations fail to fully exploit the sparsity introduced by token filtering. Current implementations are effective only when sparsity exceeds $95\%$ (\S\ref{sec:efficiency_motivation}), whereas the sparsity achieved through backward token filtering typically ranges between $30\%$ and $40\%$~\cite{RHO}. Our experiments demonstrate that directly applying existing GEMM implementations to backward token filtering even significantly increase the training time (\S\ref{sec:efficiency_motivation}).

To tackle the two aforementioned challenges and fully unlock the training efficiency enabled by token filtering, we propose \solution. To the best of our knowledge, \solution is the first system designed to unleash the performance potential of token filtering. At its core, \solution integrates two key ideas:
\begin{icompact}
\item[1.] \highlight{\solution carefully analyzes the backward computation graph and proposes further filtering activations of inconsequential tokens during backpropagation to retain sufficient sparsity. \solution sustains the utility advancements of existing token filtering method~\cite{RHO} and increases the opportunity for efficiency improvement.}
\item[2.] \solution leverages the characteristics of token filtering—specifically, the sparsity of matrices in either columns or rows—to transform sparse GEMM into dimension-reduced dense GEMM, maximizing performance on existing hardware. However, PyTorch's dynamic graph nature complicates global updates to dimensions and variables, as graph variability and node differences prevent static rules for correctness. To overcome this, we design an automatic workflow leveraging the runtime stability (\ie, the graph remains stable during the training) to dynamically identify and update the necessary dimensions and variables before backpropagation.
\end{icompact}

We implement \solution as a PyTorch C++ extension that can be easily integrated into existing training pipelines with minimal code changes. Systems already using backward token filtering only need to add one line of code to achieve efficiency improvement. We leverage Torchgen\footnote{Torchgen is a tool used to autogenerate wrappers for the torch package. In particular, the node processing codes in autograd graph are generate using \textit{gen\_autograd.py} in Torchgen.} to automatically generate graph updating code for different models, ensuring compatibility with a wide range of LLM architectures. Our system is compatible with widely-used efficient attention implementations (\eg, FlashAttention) and can also work with parallelism strategies like tensor parallelism to further reduce communication costs (dicsussed in \S\ref{sec:discussion:reducing_communication_overheads}).

We evaluate \solution comprehensively regarding utility and efficiency using three tiny but mighty models: TinyLlama~\cite{tinyllama}, Qwen2.5-1.5B~\cite{qwen2}, and Phi-1.5~\cite{Phi1.5}.
\highlight{Using the same parameter settings from previous work~\cite{RHO}}, utility assessments of training TinyLlama on open-web-math with 15 billion tokens show that the \solution improves model utility by an average of 16.3\% across nine tasks comparing to regular training and reduces training duration from 4.5 to 3.7 days using eight NVIDIA 3090 GPUs. 
While \solution offers significant efficiency improvements over existing backward filtering~\cite{RHO}, \highlight{it shows slightly decreased model utility as it necessitates different optimal parameter settings and more details are discussed in \S\ref{sec:dis:diff_training_params}.}
Efficiency evaluation on three LLMs indicate that the \solution can reduce the backward computation by up to 35.1\% and lower overall training time up to 22\% when filtering 40\% of tokens. The efficiency improvements of \solution is more pronounced with longer contexts and higher filtering ratios. 
