\section{Evaluation} \label{sec:evaluation}

In this section, we comprehensively evaluate \solution with the following key results:

\begin{icompact}
    \item Utility evaluations of training TinyLlama~\cite{tinyllama} on open-web-math with 15B tokens~\cite{DBLP:conf/iclr/PasterSAB24} show that \solution not only improves the model utility by \highlight{16.3\% (averaged relative improvements on nine tasks)} and also reduces the end-to-end training time from 4.5 days to 3.7 days compared with regular training. Compared with Rho filtering~\cite{RHO}, \solution shows significant efficiency improvements but slightly decreases in model utility, \highlight{due to requiring different optimal parameter settings}.
    \item Efficiency evaluations on three LLMs show that \solution can reduce the backward computation up to 35.1\% and reduce end-to-end training cost by up to 22\% when filtering 40\% tokens. The speedup of \solution becomes more pronounced as the context length increases and the filtering ratio increases. The cost of the \solution operator does not increase with the filtering ratio is negligible compared with the time saved in the backward stage.
\end{icompact}

\begin{table*}[t!]
    \centering
    \setlength{\tabcolsep}{0.5em}
    \renewcommand\arraystretch{1}
    \small
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
    \multirow{2}{*}{\tabincell{c}{\\Method}} & \multirow{2}{*}{\tabincell{c}{Finetuning\\Dataset}} & \multicolumn{10}{c|}{Evaluation tasks} & \multirow{2}{*}{\tabincell{c}{Time\\(days)}} \\\cline{3-12}
     &  & \tabincell{c}{GSM8K} & \tabincell{c}{MATH} & \tabincell{c}{SVAMP} & ASDiv & MAWPS & TAB & MQA & MMLU & SAT & Average & \\\hline
    TinyLlama & NA & 2.3 & 2.4 & 9.9 & 18.1 & 20.2 & 8.8 & 22.1 & 17.9 & 21.9 & 13.7 & NA \\\hline
    TinyLlama & \tabincell{c}{OWM\\(Full)} & 3.6 & 4.2 & 19.1 & 31.5 & 36.2 & 14.7 & 10.3 & 21.7 & 18.8 & 17.8 & $\sim$4.5 \\\hline
    RHO & \tabincell{c}{OWM\\(Filter 40\%)} & 11.7 & 8 & 35.9 & 48.1 & 63.2 & 19 & 16 & 15.5 & 6.2 & 24.8 & $\sim$4.5 \\\hline
    \tabincell{c}{\solution\\(Ours)} & \tabincell{c}{OWM\\(Filter 40\%)} & 6.7 & 5.8 & 28.0 & 37.4 & 46.7 & 14.9 & 10.2 & 14.5 & 21.9 & 20.7 & $\sim$3.7 \\\hline
    \end{tabular}
    \caption{Utility evaluation on different tasks. Compared with regular training, \solution improves the model utility and significantly reduces the end-to-end training time. Compared with Rho filtering, \solution shows significant efficiency improvements but has some decrease of model utiltiy, which potentially could be solved by tuning the training parameters (\S\ref{sec:eval:utility}).} 
    \label{tab:eval:utility}
\end{table*}

\begin{table*}[t!]
	\centering
	\setlength{\tabcolsep}{0.5em}
	\renewcommand\arraystretch{1.1}
	\small
	\begin{tabular}{c|c|c|c|c|c|c}
		\hline
        \multirow{2}{*}{\tabincell{c}{Model}} & \multirow{2}{*}{\tabincell{c}{Training\\Method}} & \multicolumn{5}{c}{Detailed Time Consumption (seconds)} \\\cline{3-7}
		& & \tabincell{c}{Forward} & \tabincell{c}{Computing Loss} & \tabincell{c}{\solution Operator} & \tabincell{c}{Backward} & Total \\\hline
        \multirow{3}{*}{\tabincell{c}{TinyLlama\\(1.1B, 4K)}}& \tabincell{c}{Regular Training} & 13.43 & 0.023 & NA & 29.36 & 42.82 \\\cline{2-7}
        & \tabincell{c}{Rho Filtering} & 13.41 & 0.094 & NA & 29.21 & 42.73 \\\cline{2-7}
        & \tabincell{c}{\solution (Ours)} & 13.44 & 0.099 & 0.87 & \tabincell{c}{18.96 ($\downarrow$ 35.1\%)} & \tabincell{c}{33.37($\downarrow$ 22.0\%)} \\\hline
        
        \multirow{3}{*}{\tabincell{c}{Qwen2.5\\(1.5B, 3K)}}& \tabincell{c}{Regular Training} & 12.93 & 0.039 & NA & 31.31 & 44.27 \\\cline{2-7}
        & \tabincell{c}{Rho Filtering} & 12.90 & 0.321 & NA & 31.22 & 44.48 \\\cline{2-7}
        & \tabincell{c}{\solution (Ours)} & 12.90 & 0.317 & 1.14 & \tabincell{c}{23.27 ($\downarrow$ 25.7\%)} & \tabincell{c}{37.65($\downarrow$ 15.0\%)} \\\hline

        \multirow{3}{*}{\tabincell{c}{Phi1.5\\(1.4B, 3K)}}& \tabincell{c}{Regular Training} & 12.68 & 0.027 & NA & 26.97 & 39.32 \\\cline{2-7}
        & \tabincell{c}{Rho Filtering} & 12.77 & 0.126 & NA & 26.90 & 39.38 \\\cline{2-7}
        & \tabincell{c}{\solution (Ours)} & 12.75 & 0.129 & 0.794 & \tabincell{c}{18.68 ($\downarrow$ 30.5\%)} & \tabincell{c}{32.03($\downarrow$ 18.7\%)} \\\hline
        
	\end{tabular}
	\caption{The detailed time consumption of one iteration (\ie, 512 samples) in three models using 8 GPUs.} %between the servers or between the server and data contributors.}
    \vspace{-2mm}
\label{tab:eval:eff_detail}
\end{table*}

\subsection{Experimental Setup} \label{sec:eval:setup}

\parab{Testbed setup.} We evaluate \solution on Ubuntu servers, each equipped with 8 NVIDIA RTX 3090 GPUs (24GB), 40 CPU cores, and 256GB of memory. We use PyTorch 2.5.0 and CUDA 12.6 for implementation. Following previous studies~\cite{RHO}, we utilize small but powerful LLMs in our experiments (\eg, TinyLlama~\cite{tinyllama} with 1.1B parameters). To maximize training efficiency, we perform single-node multi-GPU training using ZeRO-1 in DeepSpeed~\cite{deepspeed}. We implement gradient accumulation to facilitate large batch training and use mixed precision with BF16 to reduce memory consumption and accelerate training.


\parab{Datasets, models, and tasks.} Our experiments focus on fine-tuning pre-trained foundation models for downstream tasks. We utilize two small but powerful LLMs in the experiments: TinyLlama~\cite{tinyllama}, Qwen2.5-1.5B~\cite{qwen2}, and Phi1.5~\cite{Phi1.5}. Following previous work~\cite{RHO}, we first train a reference model on small yet high-quality datasets, and then use the loss from the reference model to filter tokens during the training of the target model. 
We focus on mathematical reasoning as the main task, employing a blend of synthetic and manually curated math-related tokens~\cite{yu2023metamath,yue2023mammoth,mitra2024orcamath,DBLP:conf/naacl/AminiGLKCH19,DBLP:conf/acl/WangLSXDLCWS24} as high-quality data to train the reference model. For large-scale datasets to train the target model, we use open-web-math (OWM)~\cite{DBLP:conf/iclr/PasterSAB24}. We follow~\cite{RHO} and use the same architecture for both the reference and target models.
To evaluate the utility of the models, we use the following tasks: GSM8K~\cite{GSM8K}, MATH~\cite{DBLP:conf/iclr/LightmanKBEBLLS24}, SVAMP~\cite{SVAMP}, ASDiv~\cite{ASDiv}, MAWPS~\cite{MAWPS}, TabMWP~\cite{TabMWP}, MathQA~\cite{MathQA}, SATMath~\cite{SATMath}, and MMLU~\cite{MMLU}.

\parab{Baselines and hyperparameters.} We compare \solution with the following baselines: (1) regular training, which involves training the model without token filtering; and (2) Rho filtering~\cite{RHO}, an existing method of backward filtering that does not significantly enhance efficiency.
For the utility evaluation, we largely adhere to the hyperparameters outlined in \cite{RHO}. Specifically, we aggregate different samples into a context length of 2048, set the batch size to one million tokens, and utilize a learning rate of $5 \times 10^{-5}$ with cosine decay. We train all models for 1 epoch during the utility evaluation.
For the efficiency evaluation, we report throughput or time consumption per iteration, averaged over 10 iterations. Due to limited resources, we only report the utility evaluation on the TinyLlama, while the efficiency evaluation encompasses all three models---TinyLlama, Qwen2.5-1.5B, and Phi1.5.

\subsection{Utility Evaluation} \label{sec:eval:utility}

In the utility evaluation, we compare \solution with regular training and Rho filtering~\cite{RHO} on the TinyLlama model. We filter 40\% of the tokens during the training of \solution and Rho filtering, while regular training utilizes all the tokens. \Cref{tab:eval:utility} displays the models' performance on different tasks.
Compared to regular training, \solution not only improves model utility by 16.3\% (\ie, relative accuracy improvement) but also reduces the end-to-end training time from 4.5 to 3.7 days. For single tasks, the model utility of \solution surpasses that of regular training by up to 10.5\% (\eg, absolute accuracy improvements in MAWPS). In comparison to Rho filtering, \solution demonstrates significant efficiency improvements, though it exhibits a slight decrease in model utility. We analyze the utility decrease as follows.

\parab{Analyzing the utility decrease of \solution compared with Rho.} 
\highlight{It is worth noting that the utility assessments are based on a direct comparison to provide readers with an intuitive impression: both Rho~\cite{RHO} and \solution use the same training parameter settings. However, as discussed in \S\ref{sec:dis:diff_training_params}, \solution further filters the activations to improve efficiency and passes a smaller amount of gradients during backpropagation given the same inputs. Thus, it requires different parameters to achieve optimal performance. We believe that with more optimal parameter settings, the drop in model utility would be minimal.}

\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.57]{figures/eff_diff_seq_Tinyllama.pdf}
	\caption{Throughput of regular training, Rho filtering, and \solution on TinyLlama model with different context lengths. \solution shows superior performance, with its efficiency advantage growing at longer contexts.}
	\label{fig:eff_diff_seq_Tinyllama}
    \vspace{+2mm}
\end{figure}

\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.57]{figures/eff_vs_ratio_all.pdf}
	\caption{The end-to-end speedup of \solution compared with Rho filtering on three models with different filtering ratios. The speedup of \solution linearly increases with the filtering ratio.} %\highlight{pending check 20\% results}}
	\label{fig:eff_vs_ratio_all}
    \vspace{+2mm}
\end{figure}

\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.54]{figures/eff_filter_cost.pdf}
	\caption{The cost of \solution operator (\ie, updating the graph) under different filtering ratios. The cost does not increase with the filtering ratio and is negligible compared with the time saved in the backward stage.}
	\label{fig:eff_filter_cost}
    \vspace{+1mm}
\end{figure}

\subsection{Efficiency Evaluation}

We compare the training efficiency of \solution with regular training and Rho filtering on three models: TinyLlama, Qwen2.5-1.5B, and Phi1.5. We divide the training process into four stages: forward, computing loss, \solution operator (\ie, updating the graph), and backward. \Cref{tab:eval:eff_detail} presents a detailed time comparison for these four stages across the three models.
When filtering 40\% of the tokens, \solution reduces end-to-end time consumption by 22.0\%, 15.0\%, and 18.7\% compared to regular training and Rho filtering for the three models, respectively. In particular, \solution decreases the backward time consumption by 35.1\%, 25.7\%, and 30.5\%, respectively. The additional cost of updating the graph (\ie, the \solution operator) is relatively small compared to the time saved during the backward stage, resulting in a significant overall improvement in efficiency.
While the time consumption for loss computation is increased in token filtering methods compared to regular training—due to the additional process of selecting filtered tokens—the increased time is negligible and does not impact overall efficiency.

\parab{Impact of the context length.} 
The context length is a critical factor influencing the training efficiency of LLMs, as the training complexity increases quadratically with the context length. We evaluate the efficiency of \solution using different context lengths on the TinyLlama model. \Cref{fig:eff_diff_seq_Tinyllama} illustrates the throughput of regular training, Rho filtering, and \solution across various context lengths ranging from 1K to 4K.
The throughput of \solution consistently exceeds that of both regular training and Rho filtering, with the efficiency improvement becoming more pronounced as the context length increases. At a context length of 4K, \solution achieves a 1.28$\times$ higher throughput than that of the other methods. These results demonstrate that \solution can effectively enhance efficiency in computationally intensive scenarios, such as long-context training.

\parab{Impact of the filtering ratio.} 
To further investigate the efficiency improvement of \solution, we evaluate the end-to-end speedup compared to Rho filtering under different filtering ratios, with the results shown in \Cref{fig:eff_vs_ratio_all}. The speedup of \solution increases linearly with the filtering ratio, demonstrating the effectiveness of \solution's system design and indicating potential performance gains in scenarios with higher filtering ratios (\eg, long-context training).
Additionally, the cost associated with the \solution operator (\ie, updating the graph) at different filtering ratios is illustrated in \Cref{fig:eff_filter_cost}. Notably, this cost does not increase with the filtering ratio and remains negligible when compared to the time saved during the backward stage. These results demonstrate that \solution can effectively enhance the training efficiency of LLMs through token filtering across varying filtering ratios.