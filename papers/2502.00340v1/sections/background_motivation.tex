\section{Background and Motivation} \label{sec:background_motivation}

\subsection{LLM Training \& Token Filtering} \label{sec:background_llm}

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.55]{figures/llm_training.pdf}
	\caption{An overview of LLM training.}
	\label{fig:llm_training}
    % \vspace{+2mm}
\end{figure}

Training LLMs is a computationally intensive process that that demands substantial computational resources. Figure~\ref{fig:llm_training} shows an overview of the LLM training process. The training data is first tokenized and fed into the LLM, which consists of multiple transformer layers. The model processes the input data and generates predictions, which are compared to the ground truth labels (\ie, next tokens) to compute the loss. Finally, gradients are computed based on the loss to update the model parameters. 
Two primary factors significantly influence the computational cost: the size of the model (\eg, the number of layers) and the number of training tokens. For instance, training foundation models like LLaMA3-70B requires approximately 7 million GPU hours and involves processing more than 15 trillion tokens. Additionally, LLM-based applications necessitate extensive domain knowledge to fine-tune the model, which can also be computationally expensiveâ€”particularly for applications that require frequent updates (\eg, LLM-based recommender systems~\cite{DBLP:conf/sigir/LinWLYFWC24}).
Accelerating LLM training is crucial to enable faster application development, reducing costs, and minimizing the environmental impact of training LLMs.

Existing studies have explored various techniques to accelerate LLM training. However, many of them either leave limited room for further improvement or adversely affect model utility. Specifically, distributed training systems~\cite{MegatronLM,MegaScale} have been proposed to effectively leverage computational resources in parallel and reduce idle time in the computation pipeline by overlapping communication and data input/output (I/O) with computations. State-of-the-art LLM training systems~\cite{MegaScale} have achieved 55.2\% model FLOPs utilization (MFU) while training on more than 10,000 GPUs. Further enhancing the utilization rate of hardware remains a challenging task with limited room for improvement.
Techniques such as layer freezing~\cite{SmartFRZ,yiding-layer-freezing}, model pruning~\cite{DBLP:conf/nips/MaFW23,DBLP:conf/iclr/Sun0BK24}, and low-rank fine-tuning~\cite{LoRA} have been explored to reduce the number of trainable model parameters and improve training efficiency. However, decreasing the number of trainable parameters may negatively impact the model's utility and generalization ability~\cite{DBLP:journals/natmi/DingQYWYSHCCCYZWLZCLTLS23}.

Token filtering is a recently proposed technology that has been well recognized by the AI community. The core idea is to identify and filter out tokens that are either noisy or unlikely to contribute meaningfully to the training process, which implicitly improves the quality of training data to benefit the model utility. Moreover, by reducing the total number of tokens to be trained, token filtering also brings opportunity for efficiency improvement.

Existing token filtering works can be categorized into two types: \emph{forward token filtering} and \emph{backward token filtering}. As illustrated in \Cref{fig:token_filter_intro}, forward token filtering techniques remove training tokens during the forward pass, whereas backward token filtering methods eliminate tokens exclusively during the backward pass.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.6]{figures/token_filter_intro.pdf}
	\caption{An overview of existing token filter studies. Forward token filtering methods (a) filter hidden states during forward process, while backward token filtering methods (b) filter training loss during the backward process.}
	\label{fig:token_filter_intro}
    \vspace{+2mm}
\end{figure}

Forward token filtering methods have been extensively studied in previous works~\cite{DBLP:conf/acl/HouPZWSSZ22,DBLP:conf/acl/ZhongDL0ZDT23,DBLP:journals/corr/abs-2211-11586,DBLP:journals/corr/abs-2401-15293}. However, they typically underperform compared to backward filtering methods due to semantic losses~\cite{DBLP:conf/acl/ZhongDL0ZDT23,DBLP:journals/corr/abs-2211-11586,RHO}. As shown in \Cref{fig:token_filter_intro}, forward token filtering methods filter tokens at each layer of the forward computation, such that each layer of the model only processes partial context. However, this approach has been shown to cause semantic loss and potential harm model utility~\cite{DBLP:conf/acl/ZhongDL0ZDT23,DBLP:journals/corr/abs-2211-11586}. Evaluations in existing forward filtering studies~\cite{DBLP:conf/acl/HouPZWSSZ22,DBLP:conf/acl/ZhongDL0ZDT23,DBLP:journals/corr/abs-2211-11586,DBLP:journals/corr/abs-2401-15293} report only similar or lower model utilities and fail to achieve the improvements in utility seen with backward filtering methods~\cite{RHO}.

Backward token filtering is an effective solution for enhancing model utility and is widely accepted within the AI community. Existing work~\cite{RHO} has demonstrated that backward filtering methods can not only reduce the number of training tokens processed during the backward pass but also improve model utility by eliminating inconsequential tokens. As illustrated in \Cref{fig:token_filter_intro}, the backward filtering method maintains standard forward computation while performing selective token training in the output layer.
Existing studies leverage a reference model to assess the importance of each token. For instance, when training a target model to enhance mathematical reasoning, the reference model is trained on a small but high-quality mathematical corpus (\eg, clean datasets with clear instructions and derivations). During the training process, the loss of the target model (\ie, the model being trained) is compared to the loss of the reference model. Tokens with high excessive loss (\ie, the loss of the target model minus the loss of the reference model) are considered important, while those with lower excessive loss are filtered out during the backward pass.
Empirically, tokens with high excessive loss have larger room to be trained, and lower loss in the reference model also indicates that the tokens match the distribution of high-quality data. Mathematically, backward token filtering can be formulated as follows~\cite{RHO}:
\begin{equation}
    \mathcal{L}_{filter} = -\frac{1}{N \times k\%} \sum^N_{i=1} I_{k\%}(\mathbf{x}_i) \log P_{\theta}(\mathbf{x}_i|\mathbf{x}_{<i};\theta)
\end{equation}
\begin{equation}
	I_{k\%}(\mathbf{x}_i) = \left\{
	\begin{aligned}
		1, & \ if \ \mathbf{x}_i \ \in \ top \ k\% \ of \ (\mathcal{L}_{\theta}(\mathbf{x}_i)-\mathcal{L}_{ref}(\mathbf{x}_i)) \\
		0, & \ \text{otherwise}
	\end{aligned}
	\right.
\end{equation}
where $\mathcal{L}_{\theta}$ is the loss of the target model, $\mathcal{L}_{ref}$ is the loss of the reference model, and $\mathcal{L}_{filter}$ is the actual loss to train the target model while keeping $k\%$ of tokens.

In this paper, we mainly focus on backward token filtering due to its aforementioned advantages.

\subsection{Existing Token Filtering Fails to Improve Efficiency} \label{sec:efficiency_motivation}

Although backward token filtering has shown promising results in improving model utility, its potential of improving training efficiency remains unexplored. In principle, reducing the number of training tokens should bring significant efficiency improvement due to the reduced computation workload. However, existing studies fail to improve training efficiency due to the following two reasons: (1) insufficient sparsity after token filtering; and (2) inefficiency of sparse GEMM implementations.

\parab{Insufficient sparsity after token filtering.} 
The potential for efficiency improvements in token filtering methods arises from the sparsity achieved by filtering out unimportant tokens. The gradients of the filtered tokens become zero, allowing for a reduction in computational costs during the backward process. Essentially, backpropagation involves computations between gradients and activations, which are intermediate results specifically stored for the backward pass.
Current methods filter the loss of unimportant tokens at the output layer, resulting in sparse gradients. However, they leave all dense activations unchanged. Consequently, after being multiplied by these dense activations, the gradients are no longer sparse once they pass through the final attention block. Therefore, existing backward filtering methods~\cite{RHO} exhibit insufficient sparsity, even after filtering the loss at the output layer.

\Cref{eq:attention} shows the forward computation of the attention block in the transformer model, where $softmax(\mathbf{Q}\mathbf{K}^T/\sqrt{d})$ is stored as activation\footnote{Eager implementation of attention block in PyTorch stores the softmax as intermediate results. The FlashAttention recomputes the the softmax matrix during backward, which is mathematically equalivent to storing the matrix.}.
\begin{equation} \label{eq:attention}
	\mathbf{X} = softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}}) \times \mathbf{V}
\end{equation}
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.45]{figures/dv.pdf}
	\vspace{+0.5mm}
	\caption{Leaving the activation (\ie, $softmax$) of filtered tokens unchanged makes the $\mathbf{V}$'s gradients computed by the attention block not sparse anymore after the backpropagation. The dense gradients $\mathbf{G}_V$ will be passed to the front layers, undermining sparsity in all the rest computations .}
	\label{fig:dv}
    \vspace{+1mm}
\end{figure}
\Cref{fig:dv} illustrates the process of computing gradients for $\mathbf{V}$ (\ie, $\mathbf{G_V}$) using sparse gradients while maintaining unchanged activations (\ie, activations of all tokens are retained). After filtering the tokens based on loss, the gradients of the corresponding tokens become zero, as depicted in \Cref{fig:dv}. However, because the activations of the filtered tokens remain unchanged, the gradients of $\mathbf{V}$ are no longer sparse. Consequently, the backward computation following the first attention block lacks sparsity, limiting efficiency improvements solely within the output layer.

Following the setting in existing work~\cite{RHO}, we can estimate the upper bound of efficiency improvement with existing token filtering schemes. Taking TinyLlama, a model with 22 layers and 1.1B parameters, as an example. Filtering 40\% tokens will only linearly improve the efficiency on backward propagation of the last layer, while no front layers can be improved. Thus, the overall backward efficiency can only be improved by 1.8\%. Given that backpropagation consumes 66\% of the whole training~\cite{MegatronLM}, the end-to-end efficiency improvement is only 1.2\%.

To unlock the full efficiency of token filtering, we propose to further filter the activations to retain the sparsity in the whole backpropagation, as we illustrate in \S\ref{sec:system:filter_activation}.

\parab{Inefficient sparse GEMM.} Existing sparse GEMM implementations are not well-suited for token filtering training. Although sparse GEMM is a hot research topic and PyTorch has provided a sparse tensor implementation (\ie, \texttt{torch.sparse}), the efficiency of existing sparse GEMM is only improved when the data has very high sparsity (\eg, 95\%). Furthermore, \texttt{torch.sparse} does not fully support model training. For instance, the commonly used Compressed Sparse Row (CSR) format only accommodates 2D tensors, whereas the data in transformer models is typically represented as 3D or 4D tensors.
% Moreover, \texttt{torch.sparse} fails to provide full support for modrts 2D tensors while the data in transformer models is typically 3D or 4D tensors.el training. For example, the frequently used CSR-sparse format only suppo

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.52]{figures/sparse_gemm_eff.pdf}
	\caption{PyTorch sparse GEMM outperforms regular GEMM only when filtering more than 95\% tokens and cannot improve efficiency of token filtering training which typically drops 30\% $\sim$ 40\% tokens \cite{RHO}.}
	\label{fig:sparse_gemm_eff}
    % \vspace{+2mm}
\end{figure}

To demonstrate the problem, we perform experiments on our testbed (details in \S\ref{sec:eval:setup}).
We compare the efficiency of sparse GEMM in PyTorch and regular GEMM in the scenario of token filtering, \ie, the matrix is sparse by row or columns. \Cref{fig:sparse_gemm_eff} shows the comparison results under different ratios of token filtering and batch sizes. The sparse GEMM is more efficient only when over 95\% of all tokens are filtered, which is unrealistic for token filtering. Under the typical filtering rate of 40\%, sparse GEMM is even 10$\times$ slower than regular GEMM. 

