%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{url}
\usepackage{comment}
\usepackage[normalem]{ulem}
\usepackage{scalerel}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
%\usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\newcommand*{\paral}{\stretchrel*{\parallel}{\perp}}
\newcommand*{\Sslash}{\stretchrel*{\sslash}{\perp}}
\newcommand{\En}{\mathcal{E}}

% Uncomment the next line to enable comments
\def\showcomments{}

\ifdefined\showcomments
  \newcommand\bluebf[1]{\textcolor{blue}{\textbf{#1}}}
  \newcommand\alb[1]{\textcolor{magenta}{[ab: #1]}}
  \newcommand\matt[1]{\textcolor{cyan}{[ms: #1]}}
  \newcommand\ams[1]{\textcolor{blue}{[ams: #1]}}
\else
    \newcommand\bluebf[1]{}
    \newcommand\alb[1]{}
    \newcommand\matt[1]{}
    \newcommand\ams[1]{}
\fi

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}
%\usepackage{[preprint]{icml2025}

%\makeatletter
%\def\icml@copyright{}
%\makeatother

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bbm}

\newtheorem{prop}{Proposition}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{In-context denoising with minimal transformers as associative memory retrieval}
\icmltitlerunning{In-context denoising with one-layer transformers: connections between attention and associative memory retrieval}

%\makeatletter
%\renewcommand{\@copyrightspace}{}  % Removes ICML footer
%\fancyfoot[C]{}
%\makeatother

\makeatletter
\def\icml@copyrighttext{\phantom{Placeholder text}} % Removes ICML footer
\makeatother

\makeatletter
\def\icml@copyrighttext{} % Completely removes ICML footer
\makeatother

\begin{document}

% ===========================================================================================
%\title{In-context denoising with minimal \\transformers}
%\title{In-context denoising with one-layer transformers and connections to associative memory retrieval}
\twocolumn[
\icmltitle{In-context denoising with one-layer transformers: \\ connections between attention and associative memory retrieval}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}

\icmlauthor{Matthew Smart}{xxx}
\icmlauthor{Alberto Bietti}{yyy}
\icmlauthor{Anirvan M. Sengupta}{yyy,www,zzz}

\end{icmlauthorlist}

%
%Department of Physics and Astronomy, Rutgers University, Piscataway, New Jersey 08854, USA 
%Center for Computational Quantum Physics, Flatiron Institute, New York, New York 10010, USA 
%Center for Computational Mathematics, Flatiron Institute, New York, New York 10010, USA

\icmlaffiliation{xxx}{
    Center for Computational Biology, Flatiron Institute, New York, NY, USA}
\icmlaffiliation{yyy}{
    Center for Computational Mathematics, Flatiron Institute, New York, NY, USA}
    \icmlaffiliation{www}{
    Center for Computational Quantum Physics, Flatiron Institute, New York, NY, USA}
\icmlaffiliation{zzz}{
    Department of Physics and Astronomy, Rutgers University, Piscataway, NJ, USA}
    
\icmlcorrespondingauthor{Matthew Smart}{msmart@flatironinstitute.org}
\icmlcorrespondingauthor{Anirvan M. Sengupta}{asengupta@flatironinstitute.org}
%\texttt{asengupta@flatironinstitute.org} \\

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{attention, in-context learning, denoising, associative memory, Hopfield network, transformers}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.

% we could say "serve as transiently stored associative memories"

% Original abstract 25/01/23
%We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory networks (DAMs), also known as modern Hopfield networks. We show that certain restricted denoising problems can be solved optimally even by a one-layer transformer. We then demonstrate that gradient descent updates of the corresponding DAMs, initialized with noisy inputs, produce qualitatively similar denoising solutions. In particular, simplified transformers trained on the denoising objective leverage the expressive power of the attention mechanism to guide corrupted initial states into low-energy valleys of the associated DAM. This behavior yields better solutions than exact retrieval of either a context token or a spurious local minimum. Our results therefore provide a concrete example of dense associative memory networks extending beyond the standard memory retrieval paradigm, offering practical applications for in-context learning and other tasks requiring generalization. Taken together, this work solidifies the connection between DAMs and attention mechanisms first identified by Ramsauer et al., which continues to stimulate significant interest in shared principles.

%\matt{TLDR sentence: (remove at submission)} \\
%  In-context learning of denoising tasks is possible with one-layer transformers, and the optimal denoiser can be viewed as an update rule of a dense associative memory network.

% attempt to rebalance the abstract
%We introduce in-context denoising, a task that provides a principled framework for studying Bayes-optimal in-context learning using attention-based architectures. We derive Bayes optimal predictors for certain restricted denoising problems and show that such solutions can be expressed by a single-layer transformer. Through both theoretical analysis and empirical results, we establish that attention layers trained on in-context denoising can recover Bayes-optimal predictors. We explore the relationship between the attention layers trained on the in-context denoising task and dense associative memory (DAM) networks, also known as modern Hopfield networks. We demonstrate that gradient-based updates in DAMs, initialized with noisy inputs, produce qualitatively similar denoising solutions, with attention mechanisms guiding corrupted states into low-energy regions of the associated DAM model. Our results provide a concrete example of DAM networks extending beyond memory retrieval to in-context learning tasks. This behavior yields better solutions than exact retrieval of either a context token or a spurious local minimum. Our results therefore provide a concrete example of DAM networks extending beyond the standard memory retrieval paradigm. 
%Overall, this work sheds new light on the implicit inferential capabilities of transformers and clarifies their connection to associative memory models.

\end{abstract}
% TLDR sentence: incontext denoising via one layer transformers can be viewed as spurious solutions of modern hopfield networks / DAMs

% TITLES:
% In-context denoising with one-layer transformers and connections to associative memory retrieval
% In-context denoising with one-layer transformers: connections between attention and associative memory retrieval

% Alberto Jan 22 email
% Also, some papers to cite regarding memorization vs generalization (though these two things mean many different things each time...)
% https://arxiv.org/abs/2306.15063
% https://arxiv.org/abs/2405.11751
% https://arxiv.org/abs/2412.00104
% [regarding the refs, the "memorization" in the in-context regression papers is actually referring to retrieving specific tasks/regression vectors seen often during training, which is pretty different than retrieval of individual tokens, so not sure it's that related actually]

% ===========================================================================================

\section{Introduction}
\label{sec:intro}
%In recent years, the transformer architecture \cite{Vaswani2017} has seen many successes in applications, from natural language processing \cite{zhang2023turing,patwardhan2023transformers}, in particular, large language models \cite{raiaan2024review}, to computer vision tasks \cite{khan2022transformers,han2022survey}. Understanding the mechanism of transformer-based networks is a major research goal at this moment. In an interesting development, \citet{bietti2024birth} discusses transformer performance in terms of associative memories.

The transformer architecture \cite{Vaswani2017} has achieved remarkable success across diverse domains, from natural language processing \cite{devlin2019bert,brown2020language,touvron2023llama} 
to computer vision \cite{dosovitskiy2020image}. 
Despite their practical success, understanding the mechanisms behind transformer-based networks remains an open challenge. 
This challenge is exacerbated by the growing scale and complexity of modern large networks. 
Toward addressing this, researchers studying simplified architectures have identified connections between the attention operation that is central to transformers and associative memory models \cite{ramsauer2021iclr}, providing not only an avenue for understanding how such architectures encode and retrieve information but also potentially ways to improve them further.
%\cite{zhang2023turing, patwardhan2023transformers}, particularly in large language models \cite{raiaan2024review}, and 
%\cite{khan2022transformers, han2022survey}. 

The most celebrated model for associative memories in systems neuroscience is the so-called Hopfield model \cite{amari1972learning, nakano1972associatron, little1974existence, Hopfield1982}. This model has a capacity to store ``memories" (stable fixed points of a recurrent update rule) proportional to the number of nodes \cite{Hopfield1982, Amit1985}. In the last decade, new energy functions \cite{krotov2016hopfield, demircigil2017} were proposed for dense associative associative memories with much higher capacities. These energy functions are often referred to as modern Hopfield models. 
\citet{ramsauer2021iclr} pointed out the similarity between the one-step update rule of a certain modern Hopfield network \cite{demircigil2017} and a particular one-layer transformer map, generating interest in the statistical physics and the systems neuroscience community \cite{krotov2021large,krotov2023new,lucibello2024prl}. 
However, the construction in~\cite{ramsauer2021iclr} appears to emphasize the specific task of exact retrieval (converging to a fixed point), while in practice transformers may tackle many other tasks.
%\alb{However, the construction in~\cite{ramsauer2021iclr} appears somewhat taylored to the specific task of memory retrieval, while in practice transformers may tackle many other tasks.}
%However, the specific task that the particular transformer in \cite{ramsauer2021iclr} is performing appears to be memory retrieval \alb{this last sentence would benefit from more clarification}\ams{Trying to be concrete!}\matt{might need to hedge more, not sure they assume its operating as retrieval}, which is one among many tasks tranformers are capable of tackling.

To explore this connection beyond retrieval, we introduce \emph{in-context denoising}, a task that bridges the behavior of trained transformers and associative memory networks through the lens of in-context learning (ICL). In standard ICL, a sequence model is trained to infer an unknown function $g$ from contextual examples, predicting $g(X_{L+1})$ given a sequence of input-output pairs $E = ((X_1, g(X_1)), ..., (X_L, g(X_L)), (X_{L+1},-))$. Crucially, $g$ is implied solely through the context and differs across prompts -- performant models are therefore said to ``learn $g(x)$ in context".
While ICL has been extensively studied in supervised settings \citep{garg2022neurips, bartlett2024jmlr, akyurek2023, reddy2024iclr}, recent work suggests that transformers may internally emulate gradient descent over a context-specific loss function during inference \citep{vonOswald2023mordvintsev, dai2023gptlearnicl, ahn2023transformers}. This general perspective aligns with our findings. 

In this work, we generalize ICL to an unsupervised setting where the prompt consists of $L$ samples from a random distribution and the query is a noise-corrupted sample from the same distribution. This shift allows us to probe how trained transformers internally approximate Bayes-optimal inference, while deepening the connection to associative memory models which are prototypical denoisers. 
By setting up this problem in this way, we also attempt to answer a few questions. One of them concerns the memorization-generalization dilemma in denoising: a Hopfield model's success is usually measured by successful memory recovery, while in-context learning may have to solve a completely new problem. Another question has to do with the number of iterations of the corresponding Hopfield model: why does the \citet{ramsauer2021iclr} correspondence involve only one iteration of Hopfield energy minimization and not many?

%Rather than focusing on memory retrieval, we introduce and study a denoising task that bridges the behaviors of trained transformers and associative memory networks through the lens of in-context learning (ICL).  
%ICL normally refers to supervised tasks where a sequence model is trained to perform next-token prediction on inputs of the form $E=((X_1, g(X_1)), ..., (X_L, g(X_L)), (X_{L+1},-))$ providing and approximation to $g(X_{L+1})$, where $g:\mathbb{R}^n \rightarrow \mathbb{R}$ 
%\alb{just $g$ is better, so that it could be interpreted as language maps as well?} 
%is drawn from a predetermined function class, such as the family of linear maps $g(x)= w^T x$. 
%Crucially, $g$ is implied solely through the context and differs across prompts -- performant models are therefore said to ``learn $g(x)$ in context".

%ICL for simple problems like linear regression \citep{garg2022neurips, bartlett2024jmlr, akyurek2023} has taught us much about the expressivity of one-layer transformers. Indeed, an exciting emerging perspective views in-context learning in a pre-trained transformer as gradient descent on a context-specific objective in the forward pass \citep{vonOswald2023mordvintsev, dai2023gptlearnicl, ahn2023transformers}. 
%The number of layers corresponds to the number of iterations of gradient descent on the context-only loss (e.g. $L(\beta)= \sum_{i=1}^n (y_i - \beta^T x_i)^2$). 
%More generally, transformers learn preconditioned gradient descent \cite{ahn2023transformers}.
% \alb{I would skip this part about multiple layers, or they might ask us}. \matt{ok! also care to not say gradient descent in forward pass is 'solved'}
%The results we present in this work are in alignment with this perspective. 



% a sequence of input-output pairs $\{(x_i,g(x_i))\}$ (called a prompt) are provided as an input, and the model can take a test example $(x,-)$ and convert it to something $\approx (x,g(x))$, without having to make any updates to the parameters in the model. Of course, the model can achieve this only after having trained on a sufficiently large and diverse set of supervised learning problems.
%We could also view this setting as a special unsupervised task, regarding $(x_i,g(x_i))$ as $z_i$ and regarding $(x,-)$ as $\tilde z$ an augmentation or a corruption of $z=(x,g(x))$ 
% \alb{do we really need this last point? it feels fine to skip this and just say that we can similarly consider unsupervised problems?}.
% \matt{do you mean to just remove the next sentence?}\ams{I think he means viewing the supervised problem as an unsupervised one. Changed!} 

\textbf{In summary, our contributions are as follows:}
In Section~\ref{sec:results}, we introduce in-context denoising as a framework for understanding how transformers perform implicit inference beyond memory retrieval. In Section \ref{sec:experiments}, we establish that single-layer transformers with one attention head are expressive enough to optimally solve certain denoising problems. We then empirically demonstrate that standard training from random weights can recover the Bayes optimal predictors. 
The trained attention layers are mapped back to dense associative memory networks in Section \ref{sec:assoc-mem}. Our results refine the general connection pointed out in previous work, offer new mechanistic insights into attention, and provide a concrete example of dense associative memory networks extending beyond the standard memory retrieval paradigm to solve a novel in-context learning task.
  
\section{Problem formulation: In-context denoising}
\label{sec:results}

% To study the generalized denoising problems introduced below, we adapt the setup of \citep{garg2022neurips, akyurek2023, bartlett2024jmlr} to an unsupervised learning problem. These (and related) works have investigated in-context learning of minimal sequence models by presenting elementary mathematical queries (e.g. linear regression problems) as prompts. 
% In particular, sequence models are trained to perform next-token prediction on inputs of the form $E=((X_1, g(X_1)), ..., (X_L, g(X_L)), (X_{L+1},0))$ where $g:\mathbb{R}^n \rightarrow \mathbb{R}$ is drawn from a predetermined function class, such as the family of linear maps $g(x)= w^T x$. Crucially, $g(x)$ is implied solely through the context and differs across prompts -- performant models are therefore said to ``learn $g(x)$ in context".  


% We generalize the previous approach to our problem using the following setting.
In this section, we describe our general setup. Recurring common notation is described in Appendix \ref{appendix:notation}.
\subsection{Setup}
Each task corresponds to a distribution $D$ over the probability distribution of data: $p_X\sim D$.
Let $X_1,\cdots,X_{L+1} \overset{\mathrm{iid}}{\sim}  p_X$, define the sampling of the tokens. Let the noise corruption be defined by $\tilde X\sim p_\text{noise}(\cdot|X_{L+1})$. The random sequence $E=(X_1, X_2, ..., X_L, \tilde X)$ are given as ``context" (input) to a sequence model $F(\cdot;\theta)$ which outputs an estimate $\hat X_{L+1}$ of the original $(L+1)$-th token . The task is to minimize the expected loss $\E[l(\hat X_{L+1},X_{L+1})]$ for some loss function $l(\cdot,\cdot)$. Namely, our problem is to find
\begin{equation}
    \min_\theta \E_{p_X\sim D,X_{1:L\!+\!1}\sim p_X^{L\!+\!1},\tilde X\sim p_\text{noise}(\cdot|X_{L\!+\!1}) }[l(F(E,\theta),X_{L\!+\!1})].
\end{equation}

% \matt{any way to abbreviate or partially move in-line this heavy-duty subscript? (I am fine with it as-is though!)}
In practice, we choose $\tilde X= X_{L+1} + Z$, a pure token corrupted by the addition of isotropic Gaussian noise $Z \sim \mathcal{N}(0, \sigma_{Z}^2 I_n)$, and our objective function to minimize is the mean squared error (MSE) $\E[||\hat X_{L+1}-X_{L+1}||^2]$. 

In the following subsection, we explain the pure token distributions for three specific tasks. These tasks are of course structured so that a one-layer transformer has the expressivity to capture a solution, which, as $L\to\infty$, provides an optimal solution, in some sense. To that end, we derive Bayes optimal estimators for each of the three tasks, under the assumption that we know the original distribution $p_X$ of pure tokens. In Section \ref{sec:experiments}, we use these estimators as baselines to evaluate the performance of the denoiser $f(E,\theta)$ based on a one-layer transformer trained on finite datasets. 

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.65\textwidth]{figs/fig1-problem-setup-linear-v3.pdf}
% \caption{
%   Problem setup for the case of denoising linear subspaces and baseline estimators (linear projections).
%   \matt{consider splitting the two panels and moving the baseline one lower?}
%   }
% \label{fig:setup}
% \end{figure*}

\begin{figure*}[h!]
\centering
\includegraphics[width=0.99\textwidth]{fig-overview-cases-formulation-v3.pdf}
\caption{
  (a) Problem formulation for a general in-context denoising task. 
  (b) The three denoising tasks considered here include instances of linear and non-linear manifolds as well as Gaussian mixtures. 
  In each case, the context $E^{(i)}$ consists of a sequence of pure tokens from the data distribution $p_X^{(i)} \sim D$ where $D$ denotes the task distribution, along with a single query token that has been corrupted by Gaussian noise. The objective is to predict the target (i.e. \emph{denoise} the query) given information contained only in the prompt.
  }
\label{fig:setup}
\end{figure*}


\subsection{Task-specific token distributions}
We consider three elementary in-context denoising tasks, where the data (vectors in $\mathbb R^n$) comes from:
\begin{enumerate}
  \item Linear manifolds ($d$-dimensional subspaces)
  \item Nonlinear manifolds ($d$-spheres)
  \item Small noise Gaussian mixtures (clusters) where the component means have fixed norm
\end{enumerate}

Below we describe the task-specific distributions $p_X$ and the process for sampling tokens $\{x_t\}$. The general setup and the three special cases are represented in Fig. \ref{fig:setup}. 
The same corruption process applies to all cases: $\tilde X = X_{L+1} + Z, Z \sim \mathcal{N}(0, \sigma_{Z}^2 I_n)$. 

\subsubsection{Case 1 - Linear manifolds}\label{case1}
A given training prompt consists of pure tokens sampled from a random $d$-dimensional subspace $S$ of $\mathbb{R}^n$. 
\begin{itemize}
  \item Let $P$ be the orthogonal projection operator to a random $d$-dimensional linear subspace $S$ of $\mathbb{R}^n$, sampled according to the uniform measure, induced by the Haar measure on the coset space $O(n)/O(n-d)\times O(d)$, on the Grassmanian $G(d,n)$, the manifold of all $d$-dimensional subspaces of $\mathbb{R}^n$. 
  %\bluebf{note: reversed $G$ notation}
  % swapped notation: G(n,d) -> G(d, n)
  % Thanks!-A
  %
  \item Let $Y \sim \mathcal{N}(0, \sigma_0^2 I_n)$ and define $X = P Y$; we use this process to construct the starting sequences $(X_1, ..., X_{L+1})$ of $L+1$ independent tokens.
\end{itemize}
We thus have $p_X = \mathcal{N}(0, \sigma_0^2 P)$, with the Haar distribution of $P$ characterizing the task ensemble associated with $D$. 

\subsubsection{Case 2 - Nonlinear manifolds}\label{case2}
We focus on the case of $d$-dimensional spheres of fixed radius $R$ centered at the origin in $\mathbb{R}^n$. 
\begin{itemize}
  \item Choose a random $d+1$-dim subspace $V$ of $\mathbb{R}^n$, sampled according to the uniform measure, as before, on the Grassmanian $G(d+1, n)$.
  The choice of this random subspace generates the distribution of tasks $D$.
  %, and the corresponding orthogonal projector $P$. 
  % swapped notation: G(n,d+1) -> G(d+1, n)
  %
  \item %Let $z \sim \mathcal{N}(0, \sigma_z^2 P)$ and define $x = R z/||z||$, projecting the point to the $d$ dimensional sphere of radius $R$, $S\subset V$, resampling if $z=0$. 
  Inside $V$, sample uniformly from the radius $R$ sphere (once more, a Haar induced measure on a coset space $O(d+1)/O(d)$).
  We use this process to construct input sequences $X_{1:L+1}=(x_1, ..., x_{L+1})$ of $L+1$ independent tokens.
\end{itemize}

In practice, we uniformly sample points with fixed norm in $\mathbb{R}^d$ and embed them in $\mathbb{R}^n$ by concatenating zeros. We then rotate the points by selecting a random orthogonal matrix $Q \in \mathbb{R}^{n \times n}$. 

\subsubsection{Case 3 - Gaussian mixtures (Clustering)}
Pure tokens are sampled from a weighted mixture of isotropic Gaussians in $n$-dimensions, 
$\{w_{a}, (\mu_{a}, \sigma_{a}^2)\}_{a=1}^K$. The density is

$$p_X(x) =  
    \sum_{a=1 } ^K  w_{a} C_{a}
        e^{- \lVert x - \mu_{a} \rVert ^2 / 2 \sigma_{a}^2}, $$

where $C_{a} = (2 \pi \sigma_{a}^2)^{-n/2}$ are normalizing constants. 
%We choose $\sigma_\alpha=\sigma, \:\,\forall\: \alpha$. 
The $\mu_a$ are independently chosen from a uniform distribution on the radius $R$ sphere of dimension $n-1$, centered around zero. The distribution of tasks $D$, is decided by the choice of $\{\mu_a\}_{a=1}^K$.

For our ideal case, we will consider the limit that the variances go to zero. In that case, the density is simply

$$p_{X_0}(x) =  
    \sum_{a=1 }^K w_{a} \delta(x - \mu_{a}). $$

\subsection{Bayes optimal denoising baselines for each case}
\label{sec:bayes-optimal-predictors}
% based on notes 24-11-21
The first $L$ tokens in $E$ are ``pure samples" from $p$ that should provide information about the distribution for our denoising task. Our performance is expected to be no better than that of the best method, in the case that the token distribution and also the corrupting process are exactly known. This is where the Bayesian optimal baseline comes in.
As is well-known, the Bayes optimal predictor of a quantity is given by the posterior mean. We use that fact to compute the Bayes optimal loss.

% We refer to corrupted samples $\tilde x$ as $p_{\text{query}}$. Remarks:

% \begin{itemize}
%   \item $p_{X}(x)$ is task-dependent (the three scenarios considered here are introduced above).
  
%   \item $p_{\tilde X}(\tilde x)$ where $\tilde x = x + \eta$. 
%   For a sum of independent random variables, $Y=X_1+X_2$, their pdf is a convolution $p_Y(y)=\int p_{X_1}(x) p_{X_2}(y-x)dx$. Thus:
%     \begin{equation*}
%     \begin{aligned}
%       p_{\tilde X}(\tilde x) 
%         & = \int p_{\eta}(z) p_{X}(\tilde x - z) dz \\
%         & = C_{\eta} \int e ^{- \lVert z \rVert ^2 / 2 \sigma_{\eta}^2} p_{X}(\tilde x - z) dz
%     \end{aligned}
%     \end{equation*}

%     where $C_\eta = (2 \pi \sigma_{\eta}^2)^{-n/2}$ is a constant. E.g. in the linear denoising problem:
%     \begin{equation*}
%     \begin{aligned}
%       p_{\tilde X}(\tilde x) & = C_{\eta} C_{X} \int 
%         e ^{- \lVert z \rVert ^2 / 2 \sigma_{\eta}^2}
%         e ^{- \lVert z - \tilde X \rVert ^2 / 2 \sigma_{z}^2}
%       dz.
%     \end{aligned}
%     \end{equation*}

%   \item $p_{\tilde X \mid X}(\tilde x \mid x)$. 
%     This is simply 
%     $$p_{\eta}(\tilde x - x) = C_{\eta} e ^{- \lVert \tilde x - x \rVert ^2 / 2 \sigma_{\eta}^2}.$$

%   \item $p_{X \mid \tilde X}(x \mid \tilde x)$. By Bayes' theorem, this is

%     \begin{equation*}
%     \begin{aligned}
%       p_{X \mid \tilde X}(x \mid \tilde x) 
%         & = \frac{p_{\tilde X \mid X}(\tilde x \mid x) p_{X}(x)} { p_{\tilde X}(\tilde x) }\\
%         & = \frac
%             {e ^{- \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} p_X(x) }
%             {\int e ^{- \lVert z - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} p_X(z) dz}.
%     \end{aligned}
%     \end{equation*}

% \end{itemize}

 In particular, we seek a function $f(\tilde X): \mathbb{R}^n \rightarrow \mathbb{R}^n$ such that $\E_{X, \tilde X} \left[ \Vert X - f(\tilde X) \rVert^2 \right]$ is minimized. 
 %Below we will drop the subscript when the expectation is over the joint distribution $p_{X, \tilde X}$. 
% \bluebf{TODO cleanup notation}.
Since the perturbation~$Z$ is Gaussian, the posterior distribution of $X$, given $\tilde X$ is
\begin{equation*}
  p_{X \mid \tilde X}(x \mid \tilde x) = C(\tilde x) p_X(x) e ^{  
    - \lVert x - \tilde x \rVert ^2 / 2 \sigma_{Z}^2
  },    
\end{equation*}
 where $C(\tilde x)$ is a normalizing factor (see Appendix \ref{appendix:Bayes-notation} for more explanation). 
% The square loss objective is minimized by the posterior mean 
% \begin{equation*}
% %\begin{aligned}
%   f_\text{opt}(\tilde X) = \E_{X \mid \tilde X} [ X \mid \tilde X ]
%               = \int x\: p_{X \mid \tilde X}(x \mid \tilde X) dx %\\
%              % &= \frac{1}{p_{\tilde X}(\tilde X)} \int x\: p_{X, \tilde X}(x, \tilde X) dx.
% %\end{aligned}
% \end{equation*}
The following proposition sets up a baseline to which we expect to compare our results as $L\to\infty$. The proof is in Appendix \ref{appendix:performance-bound}.
% Proof: Using the law of total expectation, observe that 
% \bluebf{[matt: cross-terms cancel when going from 1st to 2nd line; can just state the result and  (a) cite the scalar version, or (b) move this mini derivation to the appendix]} 
% NOTE: The same basic result is shown for scalar r.v. on Wikipedia here:
%    https://en.wikipedia.org/wiki/Conditional_variance
\begin{prop}
\label{prop:performance-bound}
For each task, specified by the input distribution $p_X$, and the noise model $p_{\tilde X|X}$,
\begin{equation}
  \E_{X,\tilde X} \left[ \Vert X - f(\tilde X) \rVert^2 \right] 
 \ge   \E_{\tilde X} \left[ \Tr{ \Cov (X \mid \tilde X)} \right].
\end{equation}

%Note the final line is independent of $f$. 
This lower bound is met when $f(\tilde X) = \E [ X \mid \tilde X ]$. 
\end{prop}

Thus, the Bayes optimal denoiser is the posterior expectation for $X$ given $\tilde X$. The expected loss is found by computing the posterior sum of variances. 

These optimal denoisers can be computed analytically for both the linear and nonlinear manifold cases (given the variances and dimensionalities). In the Gaussian mixture (clustering) case, it depends on the choice of the centroids which then needs to be averaged over. 

\paragraph{Linear case.}
%The PCA baseline \alb{this is the first time PCA comes up? maybe we just leave this connection to  PCA as a comment below?} 
%\matt{I agree, although if we want to mention PCA more directly, maybe pointing to smth like Fig. \ref{fig:linear-baselines} helps?} 
%for the linear denoising task is a special case of this result. 

For the linear denoising task, pure samples $X$ are drawn from an isotropic Gaussian in a restricted subspace. 
The following result provides the Bayes optimal predictor in this case, the proof of which is in Appendix \ref{appendix:Bayes-optimal-linear}. 

% \begin{equation*}
%   p_{X \mid \tilde X}(x \mid \tilde x) = C(\tilde x) p_X(x) e ^{  
%     - \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2
%   }    
% \end{equation*}
%  where $C(\tilde x)$ is a normalizing factor. 

%The noise can be decomposed into parallel and perpendicular parts using the projection $P$ onto $S$, i.e.
% \begin{equation*}
%   \tilde X = \tilde X_{\paral} + \tilde X_{\perp}= P \tilde X + (I-P) \tilde X,
% \end{equation*}
% so that 
% \begin{equation*}
%   e^{- \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} = 
%     e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \:\:
%     e^{- \lVert \tilde x_{\perp} \rVert ^2 / 2 \sigma_{\eta}^2}.
% \end{equation*}

% Only the first factor matters for $p_{X \mid \tilde X}(x \mid \tilde x)$ since it depends on $x$. Then, inside the subspace $S$ (dropping $\tilde X_{\perp}$),   \\
% \begin{equation*}
% \begin{aligned}
% p_X(x)e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2}
%   & \propto e^{- \lVert x \rVert ^2 / 2 \sigma_{z}^2
%          - \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \\
%   & \propto \exp 
%     \left( - \frac{\lVert x - \frac{\sigma_{z}^2}{\sigma_{z}^2 + \sigma_{\eta}^2} \tilde x_{\paral} \rVert ^2}
%                    {2 \frac{\sigma_{z}^2 \sigma_{\eta}^2}{\sigma_{z}^2 + \sigma_{\eta}^2}
%                   } 
%     \right). 
% \end{aligned}
% \end{equation*}
\begin{prop}
\label{prop:Bayes-optimal-linear}
For $p_X$ corresponding to Subsection \ref{case1}, the Bayes optimal answer is 
 \begin{equation}
    f_\text{opt}(\tilde X)=\E[X|\tilde X] 
    = \frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} P \tilde X,
 \end{equation}
 and the expected loss is
 \begin{equation}
    \label{eq:linear_predictor_errorval}
    \E \left[ \lVert P \tilde X - X_{L+1} \rVert^2 \right] 
    = d \sigma_{0}^2 \sigma_Z^2 / (\sigma_0^2 + \sigma_{Z}^2).
\end{equation}
\end{prop}   



\begin{figure}[h!]
\centering
\includegraphics[width=0.38\textwidth]{fig-linear-baselines-wide.pdf}
\caption{
  Baseline estimators for the case of random linear manifolds with projection operator $P^{(i)}$. 
  }
\label{fig:linear-baselines}
\end{figure}



\paragraph{Manifold case.}
In the nonlinear manifold denoising problem, we focus on the case of lower dimensional spheres $S$ (e.g. the circle $S^1 \subset \mathbb{R}^2$). 
For such manifolds, the Bayes optimal answer is given by the following proposition.

\begin{prop}
\label{prop:Bayes-optimal-manifold}
For $p_X$ defined as in Subsection \ref{case2}, with $P$ being the orthogonal projection operator to $V$, the $d+1$ dimensional linear subspace, with $R$ being the radius of sphere $S$, the Bayes optimal answer is
\begin{align}
&f_\text{opt}(\tilde X)=\E [ X \mid \tilde X ]  \nonumber\\
  & = 
  \frac{\int e^{\langle x, \tilde X_{\paral} \rangle / \sigma_{Z}^2} \:x\, d S_x}
       {\int e^{\langle x, \tilde X_{\paral} \rangle / \sigma_{Z}^2} \: d S_x}\\
  &= 
  \frac{I_\frac{d+1}{2} \left(R \frac{\lVert \tilde X_{\paral} \rVert}{\sigma_{Z}^2} \right)}
       {I_\frac{d-1}{2} \left(R \frac{\lVert \tilde X_{\paral} \rVert}{\sigma_{Z}^2} \right)} 
  R \frac{\tilde X_{\paral}}{\lVert \tilde X_{\paral} \rVert},
\end{align}
where $\tilde X_{\paral}=P\tilde X$ and $I_\nu$ is the modified Bessel function of the first kind.
\end{prop}

% we have
% \begin{equation*}
% \begin{aligned}
% \E [ X \mid \tilde X ] 
%   & = 
%   \frac{\int e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \: x\: p_X(x) dx}
%        {\int e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \: p_X(x) dx} \\
%   & = 
%   \frac{\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{\eta}^2} \:x\, d S_x}
%        {\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{\eta}^2} \: d S_x}.
% \end{aligned}
% \end{equation*}

% From this formulation, two observations can be made.
% First, this predictor can be represented by a single softmax self-attention layer,

% \begin{equation*}
% \E [ X \mid \tilde X ] 
%  = W_{PV} X \textrm{softmax}(X^T W_{KQ} \tilde x) 
% \end{equation*}

% if we select $W_{KQ}=\frac{1}{\sigma_{\eta}^2} I$, $W_{PV}=I$, and provide sufficiently large context $X$ to approximate the integrals. Below we will see that this solution is readily identified via SGD. 
% \bluebf{(check and compare to trained weights, which seem to settle along a hyperbolic curve | what about gradient flow)}

% Second, the integrals can be evaluated directly once the parameters are specified. If $S$ is a $d_x$--sphere of radius $R_x$, then the optimal predictor is again a shrunk projection of $\tilde x$ onto $S$,

% \begin{equation*}
% \begin{aligned}
%   \frac{\int_0^{\pi} e^{R_x \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{\eta}^2} \: \cos \theta \sin^{(d_x - 1)} \theta \: d\theta}
%        {\int_0^{\pi} e^{R_x \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{\eta}^2} \: \sin^{(d_x - 1)} \theta \: d\theta}
%   R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert} \\
%   = 
%   \frac{I_\frac{d_x+1}{2} \left(R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2} \right)}
%        {I_\frac{d_x-1}{2} \left(R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2} \right)} 
%   R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert},
% \end{aligned}
% \end{equation*}

% where $I_a(y)$ is a modified Bessel function and 
% $R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert}$
% is the point on $S$ in the direction of $x_{\paral}$. \\
% \bluebf{(see Anirvan diagram in notes)}

% \bluebf{CITE Gradshteyn and Ryzhik 5th Ed (integral book) Sec 3.915 p517 and Sec 8.486 p981...\\
% Anirvan notes p5-8: ``more than one way of expressing ratio in terms of the modified Bessel function"
% }

% Denoting $\beta = R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2}$, we arrive at \bluebf{(check this)}:
% \begin{equation*}
% \begin{aligned}
% \E_{S}[\cos{\theta}] 
%     = \frac{-\frac{d}{\beta}I_d(\beta) + I_d^{\prime}(\beta)}{I_d(\beta)} 
%     = \frac{I_{d+1}(\beta)}{I_d(\beta)}.
% \end{aligned}
% \end{equation*}
  

% Alternative note: \bluebf{cleanup, debugging}: 
% \begin{itemize}
%   \item Modified Bessel function 
%   $$I_n(\beta) \equiv \frac{1}{{\pi}}\int_0^{\pi}  e^{\beta \cos \theta} \: \cos (n \theta) d \theta $$
%   \item Recursive identity (for $d-1$ being even)
% $$\int_0^{\pi} e^{\beta \cos \theta} \: \cos \theta \sin^{2m} \theta \: d\theta
% =...$$

% \item Check case of $d_x=1$ (circle). Ratio should match 
%     $$\E_{S}[\cos{\theta}] = \frac{I_1(\beta)}{I_0(\beta)}.$$

% \item Check case of $d_x=2$ (sphere). Ratio should match  
%     $$\E_{S}[\cos{\theta}] = \frac{1}{\beta}(\beta \coth(\beta) - 1) 
%                          \neq \frac{I_2(\beta)}{I_1(\beta)}.$$

% \end{itemize}




\paragraph{Clustering case.}
For clustering with isotropic Gaussian mixtures 
$\{w_{a}, (\mu_{a}, \sigma_{a}^2)\}_{a=1}^p$, the Bayes optimal predictors for some important special cases are as follows. See Appendix \ref{appendix:bayes-case3} for the general case.
\begin{prop}
\label{prop:bayes-case3}
For general isotropic Gaussian model with $\sigma_a=\sigma_0, ||\mu_a||=R$ for all $a=1,\ldots,K$.
 %\begin{equation}
 %\label{eq:bayes-case3-nonzero}
 \begin{align}
    &f_\text{opt}(\tilde X)=\E[X|\tilde X] \nonumber\\
    &= \frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} \tilde X 
    +\frac{\sigma_{Z}^2}{\sigma_{0}^2 + \sigma_{Z}^2} \frac
    {\sum_{a } w_a
         e^{\langle \mu_{a}, \tilde X \rangle /(\sigma_0^2 +\sigma_{Z}^2)}
        \:\: \mu_{a}
        }
    {\sum_{a }w_a
         e^{\langle \mu_{a}, \tilde X \rangle / (\sigma_0^2 +\sigma_{Z}^2)}}.
 \end{align}
 %\end{equation}
If $\sigma_0\to 0$,
\begin{equation}
\label{eq:bayes-case3-zerolimit}
 f_\text{opt}(\tilde X)=\E [X \mid \tilde X]  = \frac
    {\sum_{a } w_a
         e^{\langle \mu_{a}, \tilde X \rangle / \sigma_{Z}^2}
        \:\: \mu_{a}
        }
    {\sum_{a }w_a
         e^{\langle \mu_{a}, \tilde X \rangle / \sigma_{Z}^2}}. 
\end{equation}         
\end{prop}

In all three cases, we notice similarities between the form of the Bayes optimal predictor, and attention operations in transformers, a connection which we explore below.

% consider the limit that the variances go to zero. Then, 
% \begin{equation*}
%   p_{X \mid \tilde X}(x \mid \tilde x) = \frac
%     {\sum_{\alpha }
%          w_{\alpha} \delta (x - \mu_{\alpha}) 
%          e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}}
%     {\sum_{\alpha }
%         w_{\alpha} e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}},
% \end{equation*}
% which gives the conditional expectation 
% \begin{equation*}
%   \E [X \mid \tilde X] = \frac
%     {\sum_{\alpha } 
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: w_{\alpha} \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \: w_{\alpha}
%         }.
% \end{equation*}

% Note that when the mixtures are equal-weight and the center norms $\lVert \mu_{\alpha} \rVert $ are the same, this is the softmax solution: 
% \begin{equation*}
% \begin{aligned}
% \E [X \mid \tilde X] & = \frac
%     {\sum_{\alpha } 
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}} 
%   = \sum_{\alpha} p_{\alpha} \mu_{\alpha} \\
%   & = W_{PV} X \textrm{softmax}(X^T W_{KQ} \tilde x) .
% \end{aligned}
% \end{equation*}
% where $W_{PV} = I$, $W_{KQ} = \frac{1}{\sigma_\eta^2} I$, and $X$ is a sequence of tokens uniformly sampled from the mixture. 
% \bluebf{TODO check this and elaborate + connect further}

% \bluebf{Jan 23 notes below - finite cluster variance case - does not appear to be expressible by one-layer softmax attention}

% On the other hand, for non-zero cluster variances $\sigma_{\alpha}^2$,
% \begin{equation*}
% \E [X \mid \tilde X] = 
%   \frac
%    {\int e^{-\frac{\|x - \tilde{x}\|^2}{2 \sigma_\eta^2}} \sum_\alpha \left( w_{\alpha} e^{-\frac{\|x - \mu_\alpha\|^2}{2 \sigma_\alpha^2}}\right) 
%      x \, dx
%    }
%    {\int e^{-\frac{\|x - \tilde{x}\|^2}{2 \sigma_\eta^2}} \sum_\alpha \left( w_{\alpha} e^{-\frac{\|x - \mu_\alpha\|^2}{2 \sigma_\alpha^2}} \right)
%      \, dx
%    }.
% \end{equation*}

% We can simplify this expression by completing the square in the exponent and using the fact that the integral of a Gaussian about its mean is zero. This yields
% \begin{equation*}
% \begin{aligned}
%   \E [X \mid \tilde X] 
%   & = 
%   \frac
%    {\sum_{\alpha} w_{\alpha} m_{\alpha} \int \exp(g_{\alpha}) \,dx}
%    {\sum_{\alpha} w_{\alpha} \int \exp(g_{\alpha}) \,dx}
% \end{aligned}
% \end{equation*}

% where we have introduced
% \begin{equation*}
% g_{\alpha} = -\frac{1}{2}\Bigl(\frac{1}{\sigma_\eta^2} + \frac{1}{\sigma_\alpha^2}\Bigr) \,\|x - m_\alpha\|^2
% \, 
% + C_{\alpha},
% \end{equation*}
% with ``constant" 
%   $C_{\alpha} = -\frac{\lVert \tilde x - \mu_{\alpha}\rVert}{2 (\sigma_\eta^2 + \sigma_\alpha^2)}$

% and
% \begin{equation*}
% m_\alpha =
% \frac{\frac{1}{\sigma_\eta^2} \,\tilde{x} + \frac{1}{\sigma_\alpha^2}\,\mu_\alpha}
%      {\frac{1}{\sigma_\eta^2} + \frac{1}{\sigma_\alpha^2}}
%  = \frac{\sigma_\alpha^2 \, \tilde{x} + \sigma_\eta^2 \,\mu_\alpha}
%      {\sigma_\alpha^2 + \sigma_\eta^2}.
% \end{equation*}

% % By symmetry, the integral of a Gaussian about its mean is zero, so the numerator simplifies to
% % \begin{equation*}
% % \begin{aligned}
% %   \int \exp(g_{\alpha}) x \,dx 
% %    & = m_{\alpha} \int \exp(g_{\alpha}) \,dx \\
% %    & = m_{\alpha} \int \exp(g_{\alpha}) \,dx.
% % \end{aligned}
% % \end{equation*}

% In the case that the center norms $\lVert \mu_{\alpha} \rVert$ and variances $\sigma_{\alpha}^2$ are each equivalent, we have
% \begin{equation*}
% \begin{aligned}
%   \E & [X \mid \tilde X] \\
% %  & = 
% %  \frac{\sigma_\alpha^2}
% %     {\sigma_\alpha^2 + \sigma_\eta^2} \, \tilde{x}
% %    + 
% %\frac{\sigma_\eta^2}
% %     {\sigma_\alpha^2 + \sigma_\eta^2}
% %  \frac
% %   {\sum_{\alpha} w_{\alpha} \mu_{\alpha} \int \exp(g_{\alpha}) \,dx}
% %   {\sum_{\alpha} w_{\alpha} \int \exp(g_{\alpha}) \,dx} \\
%   & = 
%   \frac{\sigma_\alpha^2}
%      {\sigma_\alpha^2 + \sigma_\eta^2} \, \tilde{x}
%     + 
% \frac{\sigma_\eta^2}
%      {\sigma_\alpha^2 + \sigma_\eta^2}
%   \frac
%    {\sum_{\alpha} w_{\alpha} \mu_{\alpha} 
%             \exp\left( \frac{\langle \tilde{x}, \mu_\alpha \rangle}{\sigma_\eta^2 + \sigma_\alpha^2} \right)
%      }
%    {\sum_{\alpha} w_{\alpha} 
%             \exp\left( \frac{\langle \tilde{x}, \mu_\alpha \rangle}{\sigma_\eta^2 + \sigma_\alpha^2} \right)
%             %e^{\frac{\langle \tilde{x}, \mu_\alpha \rangle}{(\sigma_\eta^2 + \sigma_\alpha^2)}}
%      }.
% \end{aligned}
% \end{equation*}



% \matt{Need to check this} Which can be expressed as a softmax operation:
% \begin{equation*}
%   \E [X \mid \tilde X] = a \tilde{x} + W_{PV} M \textrm{softmax}(M^T W_{KQ} \tilde x + \ln w),
% \end{equation*}
% where $a = \frac{\sigma_\alpha^2}{\sigma_\eta^2 + \sigma_\alpha^2}$, 
% $W_{PV} = \frac{\sigma_\eta^2}{\sigma_\eta^2 + \sigma_\alpha^2} I$, 
% $W_{KQ} = \frac{1}{\sigma_\eta^2 + \sigma_\alpha^2} I$, and $M$ is a matrix with the cluster centers as columns.

% \matt{For this case we could maybe present this finite sigma result first, then say in small $\sigma$ limit can be approximated by a one layer transformer, becoming exact as $\sigma \rightarrow 0$.}

% Note that in the limit that $\sigma_{\alpha} \rightarrow 0$ with equal mixture weights, this becomes expressible by one-layer self-attention, since one can simply replace $M$ with the context $X$ itself. 

% \subsection{Analysis for each case (just linear currently)}

% \subsubsection{Analysis for linear case and connection to principle component analysis} 

% \bluebf{[A lot of this is now a bit redundant with the Bayesian generalization; cleanup]}

% Given corrupt samples $\tilde \rvx$, we seek an optimal linear estimator $\hat \rvx = V^* \tilde \rvx$ satisfying 
%   $\textrm{argmin}_{V} \, \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right]$.
%   %; $F$ indicates Frobenius norm (which we omit below).

% We first note $\lVert V \tilde \rvx - \rvx \rVert^2 = \lVert (V - I_n) P \rvz + V \boldsymbol{\eta} \rVert^2$. Furthermore, we have by definition that $\E[\rvx \rvx^T]=P \E[\rvz \rvz^T] P^T = \sigma_Z^2 P$. Thus, the sum $\frac{1}{L}\sum_{t=1}^L x_t x_t^T = \frac{1}{L} XX^T$, where $x_t$ are independent samples of $\rvx$, converges in probability to $\sigma_Z^2 P$ as $L \rightarrow \infty$. Finally, since $\rvz$ has zero mean and isotropic covariance, $\E [ \rvz^T A \rvz ] = \sigma_z^2 \Tr [ A ]$ for any matrix $A$. The same holds for $\boldsymbol{\eta}$. 

% Leveraging the above properties as well as the independence of both random variables, we have: 

% \begin{equation*}
%   \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \sigma_{\eta}^2 \Tr (V^T V) + \sigma_{z}^2 \Tr((V-I_n)^T(V-I_n)P).
% \end{equation*}

% One is free to choose a basis, so select one which gives $P = \begin{pmatrix} I_d & 0 \\ 0 & 0 \end{pmatrix}$ and denote $V = \begin{pmatrix} V_\parallel & V_1 \\ V_2 & V_\perp \end{pmatrix}$ accordingly. 
% Carrying out the multiplications, we have
% \begin{equation*}
% \begin{aligned}
%   \E \left[  & \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \\
%     &  \: \sigma_{\eta}^2 \bigl[ \Tr (V_\parallel^T V_\parallel) + \Tr (V_\perp^T V_\perp)
%     + \Tr (V_1^T V_1) + \Tr (V_2^T V_2) \bigr]   \\
%   & + \sigma_{z}^2 
%     \left[\Tr((V_\parallel - I_d)^T (V_\parallel - I_d)) + \Tr(V_1^T V_1) \right].
% \end{aligned}
% \end{equation*}

  
% Observe that setting $V_1, V_2, V_\perp$ to zero solely benefits the objective. The question remains to identify $V_\parallel$ by solving 
%   $\textrm{argmin}_{V_\parallel} \, 
%   \sigma_{\eta}^2 \lVert V_\parallel \rVert_F^2 
%     + \sigma_{z}^2 \lVert V_\parallel - I_d \rVert_F^2$. 
%   %\item 
% Extremizing this by differentiation gives 
% $V_\parallel^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} I_d$, which when combined with the aforementioned zero blocks gives
% %demonstrating that the optimal linear denoiser is the ``shrunk" projection operator,

% %$V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P$.
% \begin{equation}
%     \label{eq:shrunk_projector}
%   V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P
% \end{equation}
  
% Thus, given a sequence of independent samples $X=(x_1,...,x_{L-1}, \tilde x_L)$ from $S$ -- where the last token $\tilde x_L = x_L + \eta$ has been corrupted out of $S$ -- a good \bluebf{(standard notation? MMSE?)} linear estimator for $x_L$ is

% \begin{equation}
%     \label{eq:linear_predictor}
%     %\hat x_L = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{\sigma_z^2}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
%     \hat x_L = \frac{1}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{1}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
% \end{equation}

% The optimal linear denoiser is therefore a shrunk projection operator, tending to zero as the corruption scale $\sigma_{\eta}$ becomes large relative to the scale of pure samples $\sigma_z$.

% \bluebf{Check/show that same minima are found with gradient flow from particular or random initial conditions...} 
% $d \theta / dt = - \nabla C(\theta)$ given an initialization $\theta_0$ (or distribution of $\theta_0$) as in \citet{bartlett2024jmlr}. 

% From the above analysis, it is evident that the optimal weights for the linear self-attention transformer are scaled identity matrices (Fig. \ref{fig:empirical-training-linear}): 
% \begin{equation}
%     \label{eq:linear_predictor_weights}
% W_{KQ}^*=\alpha_1 I, \: \: W_{PV}^*=\alpha_2 I
%     \:\:\:
%     \textrm{with} \:\:\:
%     \alpha_1 \alpha_2 = \frac{1}{ \sigma_{\eta}^2 + \sigma_{z}^2}.
% \end{equation}
% %$W_{KQ}^*=\alpha_1 I$, $W_{PV}^*=\alpha_2 I$ with $\alpha_1 \alpha_2 = \frac{\sigma_{z}^2}{ \sigma_{\eta}^2 + \sigma_{z}^2}$.

% Furthermore, substituting $V^*$ shows that the minimized expected error is 
% \begin{equation}
%     \label{eq:linear_predictor_errorval}
%     \E \left[ \lVert V^* \tilde \rvx - \rvx \rVert^2 \right] 
%     = d \sigma_{\eta}^2 \sigma_z^2 / (\sigma_z^2 + \sigma_{\eta}^2),
% \end{equation}
% which grows linearly with the subspace dimension $d$.
% %\bluebf{(note: need to divide by $1/n$ when comparing to torch MSE which auto scales loss by $1/n$ i.e. number of features/dimensions)}.

% \begin{itemize}

%   \item \bluebf{Note: sum should be to L-1 but in practice we go to L and include the corrupt token? Should try to subtract the contribution of last token in steps above.}
%   Note (``correction term"): find $\E[\beta XX^T] = \beta L \E[x x^T] + \beta \E[\eta \eta^T] = P \sigma_z^2 + \beta \sigma_{\eta}^2 I_n$. Thus: $\Tr [\hat P] =  d \sigma_z^2 + \frac{n}{L} \sigma_{\eta}^2$.
  
%   % Good
%   %\item \bluebf{TODO: check that Eq. \ref{eq:linear_predictor_errorval} matches our MSE after training (use as baseline)}

%   \item \bluebf{TODO: comparison to PCA and case of orthogonal corruption; code already there for it} (introduce a scalar $\gamma \in [0,1]$ which selects between orthogonal and isotropic noise).

%   \item \bluebf{TODO: generalize to affine subspaces; code already there for it.}

% \end{itemize}

\section{In-context denoising with one-layer transformers - empirical results}
\label{sec:experiments}

In this section, we provide simple constructions of one-layer transformers that approximate (and under certain conditions, exactly match) the Bayes optimal predictors above.

\textbf{Input:}
Let $p_X^{(1)},\ldots, p_X^{(N)}\overset{\mathrm{iid}}{\sim}D$, be distributions sampled for one of the tasks. For each distribution $p_X^{(i)}$, we sample $E^{(i)}:=(X_1^{(i)},\ldots,X_L^{(i)},\tilde X^{(i)})$ taking value in $\mathbb{R}^{n \times (L+1)}$ be an input to a sequence model. We also retain the true $(L+1)$-th token $X_{L+1}^{(i)}$ for each $i$.  

\textbf{Objective:}
Given an input sequence $E^{(i)}$, return the uncorrupted final token $X_{L+1}^{(i)}$. We consider the mean-squared error loss over a collection of $N$ training pairs, $\{E^{(i)}, X_{L+1}^{(i)}\}_{i=1}^{N}$, 
\begin{equation}
\label{eq:cost}
    %\textrm{argmin}_{\theta} \: L(\theta) \:\: \textrm{where} \:\: 
    C(\theta) = \sum_{i=1}^{N} \lVert F(E^{(i)},\theta) - x_{L+1}^{(i)} \rVert^2,
\end{equation}
where $F(E^{(i)},\theta)$ denotes the parametrized function predicting the target final token based on input sequence $E^{(i)}$.

% \textbf{Input:}
% Let $X = (x_1, x_2, ..., x_{L}, x_\text{query}) \in \mathbb{R}^{n \times (L+1)}$ be an input to a sequence model. The tokens $x_t \in \mathbb{R}^n$ are sampled from a task-specific distribution $p_X(x)$ as introduced in Section \ref{sec:results}. The final token $x_\text{query}$ is a pure token that has been corrupted by additive Gaussian noise. 
% %For instance, in Case 1 (Linear subspaces), the first $L-1$ tokens lie exactly on a subspace $S$ of $\mathbb{R}^n$ with dimension $d \ll L$, while the final token $x_\text{query}$ is a corrupted version of a pure token $x_L$ drawn from $S$. 

% \textbf{Objective:}
% Given an input sequence $X$, return the denoised final token $x_{L+1}$. We consider the mean-squared error loss over a collection of $M$ training pairs, $\{X^{(i)}, x_{L+1}^{(i)}\}_{i=1}^{M}$, with $x_{L+1}^{(i)}$ denoting the uncorrupted final token (the target), 
% \begin{equation}
% \label{eq:cost}
%     %\textrm{argmin}_{\theta} \: L(\theta) \:\: \textrm{where} \:\: 
%     C(\theta) = \sum_{i=1}^{M} \lVert f_{\theta}(X^{(i)}) - x_{L+1}^{(i)} \rVert^2,
% \end{equation}
% where $f_{\theta}(X^{(i)})$ denotes the denoised final token predicted by the model for input sequence $X^{(i)}$.
% \bluebf{Q: should we consider explicit objective that pure samples are not corrupted (i.e. include it in cost)? I.e., the first $L-1$ tokens should be invariant under $f(X)$, only the last token should be alterred by $f$.}

%\matt{``$M$ training pairs" clashes with mask matrix notation; \\More importantly, wondering if the mask notation is too heavy. We could e.g. introduce it then immediately state that we will work with a truncated form $f(E_0, \tilde x)$ where $E_0 = [x_1, ... , x_L]$, or start from a truncated form and refer to the appendix that it can be represented via masking of full self-attention}

\subsection{One-layer transformer and the attention between the query and pure tokens}

%To motivate our choice of transformer architecture, let us start by discussing the linear case. 
To motivate our choice of architecture, let us start by discussing the linear case. 

There we have $f_\text{opt}(\tilde X)=\tfrac{\sigma_0^2}{\sigma_0^2+\sigma_Z^2}P\tilde X$. Note that, by the strong law of large numbers, $\hat P=
\tfrac{1}{\sigma_0^2L}\sum_{t=1}^LX_tX_t^T$ is a random matrix that almost surely converges component-by-component to the orthogonal projection $P$ as $L\to \infty$, since, for each $t$, $X_tX_t^T$ has the expectation $\sigma_0^2P$ and that $X_t$ is a Gaussian random variable with zero mean and a finite covariance matrix. So we could propose

\begin{equation}
    f(\tilde X)=\frac{\sigma_0^2}{\sigma_0^2+\sigma_Z^2}\hat P\tilde X=\frac{1}{(\sigma_0^2+\sigma_Z^2)L}\sum_{t=1}^LX_t\langle X_t,\tilde X\rangle.
\end{equation}


\begin{figure*}[h!]
\centering
%\includegraphics[width=0.9\textwidth]{figs/fig-training-triple.pdf}
%\includegraphics[width=0.99\textwidth]{figs/fig-training-triple-smaller.pdf}
\includegraphics[width=0.99\textwidth]{fig-training-triple-smaller-noLin.pdf}
\caption{
  (a) Training dynamics for the studied cases using one-layer softmax attention (circles) as well as linear attention (triangles). 
  Solid lines represent the average loss over six seeds, with the shaded area indicating the range for cases 2 and 3. 
  For each case, the grey dashed baseline indicates the 0-predictor, and the pink line indicates the Bayes optimal predictor.
  All cases use a context length of $L=500$, ambient dimension $n=16$, and are trained with Adam on a dataset of size 800 with batch size 80 and standard weight initialization $w_{ij} \sim U[-1/\sqrt{n}, 1/\sqrt{n}]$. 
  (b) Final attention weights $W_{KQ}$ and $W_{PV}$ are shown. For each, we indicate the mean of the diagonal elements. Initial weights are displayed for the second and third case.
  }
\label{fig:empirical-training-triple}
\end{figure*}

 We now consider a simplified one-layer linear transformer (see Appendices \ref{appendix:self-attention-general} and \ref{appendix:self-attention-denoising} for more detailed discussions) which still has sufficient expressive power to capture our finite sample approximation to the Bayes optimal answer. We define 
\begin{equation}
\label{eq:transformer-linear-attn}
\hat X = F_\textrm{Lin}(E,\theta) := \frac{1}{L} W_{PV} X_{1:L} X_{1:L}^T W_{KQ} \tilde X    
\end{equation}
taking values in $\mathbb{R}^{n}$,
where $X_{1:L}:=[X_1,\ldots,X_L]$ taking values in $\R^{n\times L}$, 
% is a masking matrix of the form
% \begin{equation}
% M_\text{Lin}=\begin{bmatrix}
%   I_L & 0_{L\times 1}\\
%   0_{1\times L} & 0
%   \end{bmatrix},
% \end{equation}
% preventing  $W_{PV}\tilde X$ from being added to the output \matt{rephrase, do we mean the value operation}\ams{Is it any clearer?}. \\
% \matt{Mostly! I had in mind to present it the direct/short way without masking, but maybe the masking is expected by our readers.
% \\
% The expression used in the code is to take the last column of this operation
% $$
% \textrm{Attn}_\text{Lin}(E,W_{PV},W_{KQ}) :=\frac{1}{L} W_{PV} E_{0} E_{0}^T W_{KQ} E
% $$
% i.e. just state
% $$ 
% \hat x = f_\textrm{Lin}(E,\theta) := \frac{1}{L} W_{PV} E_{0} E_{0}^T W_{KQ} \tilde x
% $$
% where $E_{0} = E_{:,1:L}, \tilde x = E_{:, L+1}$. 
% Your full expression effectively does the same thing since $E M_\text{Lin}E^T = E_{0} E_{0}^T$ where $E_{0} = E M_\textrm{Lin}$. 
% }
with learnable weights $W_{KQ}, W_{PV} \in \mathbb{R}^{n \times n}$ abbreviated by $\theta$.
%, we define 
% \matt{should this be $F_\textrm{Lin}$ to distinguish from below?} 
% \begin{equation}
%   F(E,\theta):=[\textrm{Attn}_\text{Lin}(E,W_{PV},W_{KQ})]_{:,L+1}.    
% \end{equation}
Note that, when $W_{PV}=\alpha I_n,W_{KQ}=\beta I_n$, and 
$\alpha\beta=\tfrac{1}{\sigma_0^2+\sigma_Z^2}$, $F(E,\theta)$ should approximate the Bayes optimal answer $f_\text{opt}(\tilde X)$ as $L\to \infty$.

Similarly, we could argue that the second two problems, the $d$-dimesional spheres and the $\sigma_0\to 0$ zero limit of the Gaussian mixtures could be addressed by  softmax attention
\begin{equation}
\label{eq:transformer-softmax-attn}
  \hat X = F(E,\theta) :=  W_{PV} X_{1:L}\textrm{softmax} (X_{1:L}^T W_{KQ} \tilde X) 
\end{equation}
taking values in $\mathbb{R}^{n }$. 
% where 
% $M\in\bar\R^{(L+1)\times(L+1)}$ is a masking matrix  of the form
% \begin{equation}
% M=\begin{bmatrix}
%   0_{L\times(L+ 1)}\\
%   (-\infty) 1_{1\times L+1} 
%   \end{bmatrix},
% \end{equation}
% once more, preventing the contribution of $\tilde X$ value to the output.  
The function $\textrm{softmax}(z):=\frac{1}{\sum_{i=1}^n e^{z_i}}(e^{z_1}, \ldots, e^{z_n})^T \in \mathbb{R}^n$ is applied column-wise. 
% \matt{
% Mirroring the linear attn comment above, the softmax code takes the last column of this operation:
% $$
% \textrm{Attn}(E,W_{PV},W_{KQ}) := W_{PV} E_{0} \softmax(E_{0}^T W_{KQ} E)
% $$
% which is 
% $$
% \hat x = F(E,\theta) := W_{PV} E_{0} \softmax(E_{0}^T W_{KQ} \tilde x)
% $$
% where $E_0 = E_{:, 1:L}$. 
% I now see that your expression does the same thing!
% I find the notation/presentation more complex personally, but it is not a big deal.
% }

% We then define 
% \begin{equation}
%   F(E,\theta):=[\textrm{Attn}(E,W_{PV},W_{KQ})]_{:,L+1}.    
% \end{equation}
For both problems, namely the spheres and the $\sigma_0\to 0$ Gaussian mixtures, we could have $W_{PV}=\alpha I_n,W_{KQ}=\beta I_n$ with $\alpha=1, \beta=1/\sigma_Z^2$ providing Bayes optimal answers as $L\to\infty$. 
% \matt{only $W_{KQ}$ acts like an inverse temp $\beta$ in the softmax -- can maybe lighten notation by using $W_{PV}=\alpha I_n, W_{KQ}=\beta I_n $?}


In fact, we could make a more general statement about distributions $p_X$ where the norm of $X$ is fixed.
\begin{theorem}
\label{theorem:convergence}
If we have a task distribution $D$ so that the support of each $p_X$ is the subset of some sphere, centered around the origin, with a $p_X$-dependent radius $R$, then the function 
\begin{equation}
F((\{X_t\}_{t=1}^L,\tilde x),\theta^*)=\frac{\sum_{t=1}^LX_te^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}{\sum_{t=1}^Le^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}
\end{equation}
converges almost surely to the Bayes optimal answer $f_\text{opt}(\tilde x)$ for all $\tilde x\in \R^n$, as $L\to\infty$. The optimal parameter $\theta^*$ refers to $W_{PV}= I_n,W_{KQ}=\tfrac{1}{\sigma_Z^2}I_n$.
\end{theorem}
The proof of the theorem is in Appendix \ref{appendix:convergence}.
Note that the condition of $p_X$ being supported on a sphere is not artificial as, in many practical transformers, pre-norm with RMSNorm gives you inputs on the sphere, up to learned diagonal multipliers.
% \alb{comment on layernorm (pre-norm with RMSNorm gives you inputs on the sphere, up to learned diagonal multipliers)} \matt{minor note: query would also be on sphere after pre-norm}

For the linear case, we use linear attention, but that may not be essential. Informally speaking, the softmax attention model has the capacity to subsume the linear attention model. 
\begin{proposition}
\label{prop:attention-limit}
As $\epsilon\to 0$,
\begin{align}
&F\Big(E,\big(\frac{1}{\epsilon}W_{PV},\epsilon W_{KQ}\big)\Big)=\frac{1}{\epsilon} W_{PV}\bar X\nonumber\\
&+ \frac{1}{L}W_{PV}\sum_{t=1}^LX_t(X_t-\bar X)^TW_{KQ}\tilde X+O(\epsilon),
\end{align}
where $\bar X=\tfrac{1}{L}\sum_{t=1}^LX_t$ is the empirical mean. 
\end{proposition}
See Appendix \ref{appendix:expansion-softmax} for the details of small $W_{KQ}$ expansion and Appendix \ref{appendix:attention-limit} for the proof of Proposition \ref{prop:attention-limit}. 

For case 1, note that $\E[X_t]=0$ and covariance of $X_t$ is finite, $E[\bar X]=0$, and $E[||\bar X||^2]=O(\tfrac{1}{L})$, allowing us to drop $\bar X$ as $L\to \infty$. If, in addition, $\epsilon$ is small, only the second term survives. Thus, $F\big(E,(\frac{1}{\epsilon}W_{PV},\epsilon W_{KQ})\big)$ starts to approximate $F_{\text{Lin}}\big(E,(W_{PV},W_{KQ})\big)$ when $L$ is large and $\epsilon$ is small, with $\epsilon\sqrt{L}$ large. 
% \alb{why do you need $L$ and $||W_{PV}||$ large? isn't small~$W_{KQ}$ sufficient?}\ams{Does that convince you, Alberto?} \matt{I'm also confused why we need large $W_{PV}$}\alb{yes, thanks!}. 
We therefore could use the softmax model for all three cases. 


\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{fig3-icl.pdf}
\caption{
  (a) Trained linear attention network converges to Bayes optimal estimator as context length increases ($n=16$, $d=8$, $\sigma_0^2=2, \sigma_z^2=1$). 
  (b) A network trained to denoise subspaces of dimension $d=8$ can accurately denoise subspaces of different dimensions presented at inference time, given sufficient context.
}
\label{fig:empirical-ICL}
\end{figure*}

%where $\beta\equiv\frac{1}{L}$ and $W_{KQ}, W_{PV} \in \mathbb{R}^{n \times n}$ are learnable weights abbreviated by $\theta$. %See Appendix for details. 

%For the map $\sigma(\cdot):\mathbb{R}^{L \times L} \rightarrow \mathbb{R}^{L \times L}$ we consider column-wise softmax 
 
% We also consider linearized self-attention
% %Softmax attention: $\sigma(Z)$ denotes column-wise softmax, i.e. for each column $z$, $\sigma(z)=\frac{1}{\sum_{i=1}^n e^{z_i}}(e^{z_1}, \ldots, e^{z_n}) \in \mathbb{R}^n$.
% %Linear attention:  $\sigma(\cdot)=I$
% %$\sigma(\cdot)=I$ so that
% \begin{equation}
%    F_\textrm{LSA}(X) = W_{PV} \beta X X^T W_{KQ} X.
% \end{equation}

% %%\bluebf{Note: \citet{bartlett2024jmlr} divides both expressions by $\rho \equiv L-1$} \\

% In either case, the network prediction $f(X) \in \mathbb{R}^n$ is simply the last column $F(X)_{:, L}$ of the attention layer, $f(X) = W_{PV} X \softmax (X^T W_{KQ} \tilde x_L) = W_{PV} \sum_{t=1}^L a_{tL} x_t$:

% Softmax case: 
% $$f(X) = \frac{W_{PV} \sum_{t=1}^L \exp{(x_t^T W_{KQ} \tilde x_L)} x_t}
%               {\sum_{i=1}^L \exp{(x_i^T W_{KQ} \tilde x_L)}} . $$

% Linear case:  
% $$f_\textrm{LSA}(X)=W_{PV} \hat P W_{KQ} \tilde x_L$$ where $\hat P \equiv \beta \sum_{t=1}^L x_t x_t^T$. 

% \bluebf{Note:} \cite{bartlett2024jmlr} Eq. (3) has the residual term $X + f(X)$.

% Notation to incorporate above:
% \begin{itemize}
%   \item $A \equiv \softmax(X^T W_{KQ} X)  \in \mathbb{R}^{L \times L}$ -- attention matrix (provided $n_\textrm{attn} > n$).
%   \item $a_{ij} = \exp{(x_i^T W_{KQ} x_j)} / \sum_{t=1}^L \exp{(x_t^T W_{KQ} x_j)}$ -- softmax attention coefficients 
% \end{itemize}

\subsection{Case 1 - Linear manifolds}
The Bayes optimal predictor for the linear denoising task from Section \ref{sec:bayes-optimal-predictors} suggests that the linear attention weights should be scaled identity matrices with their product satisfying
$\alpha \beta = \frac{1}{\sigma_0^2 + \sigma_Z^2}$. 
Fig. \ref{fig:empirical-training-triple} shows that a one-layer network of size $n=16$ trained on tasks with $\sigma_Z^2=1, \sigma_0^2=2, d=8, L=500$ indeed achieves this bound, training to nearly diagonal weights with the appropriate scale
%$\left\langle w_{KQ}^{(ii)} \right\rangle \left\langle w_{PV}^{(ii)} \right\rangle = 0.327 \approx \tfrac{1}{3}$. 
$\langle w_{KQ}^{(ii)} \rangle \langle w_{PV}^{(ii)} \rangle = 0.327 \approx 1/3$ (similar weights are learned for each seed, up to a sign flip). 

%\bluebf{Could maybe end section 4 on an ICL subsection with this figure, if we want to keep it?}
%\matt{is this point worth making, or is it sort of trivial given a PCA interpretation? maybe still interesting to remark}
Fig. \ref{fig:empirical-ICL}(a) displays how this bound is approached as the context length $L$ of training samples is increased. In Fig. \ref{fig:empirical-ICL}(b) we study how the performance of a model trained to denoise random subspaces of dimension $d=8$ is affected by shifts in the subspace dimension at inference time. We find that when provided sufficient context, such models can adapt with mild performance loss to solve more challenging tasks not present in the training set.

It is evident from Fig. \ref{fig:empirical-training-triple}(a) that the softmax network performs similarly to the linear one for this task. We can understand this through the small argument expansion of the softmax function mentioned above. The learned weights displayed in Fig. \ref{fig:empirical-training-triple}(b) indicate that $\beta^\textrm{softmax}\approx 0.194$ becomes small (note it decreases by a factor $\epsilon \approx 0.344$ relative to $\beta^\textrm{linear}$), 
while the value scale $\alpha^\textrm{softmax}\approx 1.607$ becomes larger by a similar factor $\sim 1/\epsilon$ to compensate.
Thus, although the optimal denoiser for this case is intuitively expressed through linear self-attention, it can also be achieved with softmax self-attention in the appropriate limit.

\subsection{Case 2 - Nonlinear manifolds}
% Note: a 10k run for same settings got beta=3.295, alpha=1.692 with optimal loss (L=500, dSeed1)

%The Bayes optimal predictor for the nonlinear manifold (subsphere) denoising task from Section \ref{sec:bayes-optimal-predictors} suggests that the softmax self-attention weights should satisfy $\beta_{PV}=I$, $\beta_{KQ}=\frac{1}{\sigma_Z^2}$. 

Fig. \ref{fig:empirical-training-triple} (case 2) shows networks of size $n=16$ trained to denoise subspheres of dimension $d=8$ and radius $R=1$, with corruption $\sigma_Z^2=0.1$ and context length $L=500$. 
Once again, the network trains to have scaled identity weights. 

We note that although the network nearly achieves the optimal MSE on the test set, the weights appear at first glance to deviate slightly from the Bayes optimal predictor of Section \ref{sec:bayes-optimal-predictors}, which indicated $W_{PV}=\alpha I$, $W_{KQ}=\beta I$ with $\alpha=1, \beta= 1/\sigma_Z^2$. To better understand this, we consider a coarse-grained MSE loss landscape by scanning over $\alpha$ and $\beta$. See Fig. \ref{fig:loss-landscape-MSE-2d}(a) in Appendix \ref{appendix:loss-landscapes}. We find that the 2D loss landscape has roughly hyperbolic level sets which is suggestive of the linear attention limit, where the weight scales become constrained by their product $\alpha \beta$. Reflecting the symmetry of the problem, we also note mirrored negative solutions  (i.e. one could also identify $\alpha = -1$, $\beta = -1/ {\sigma_Z^2}$ from the analysis in Section \ref{sec:bayes-optimal-predictors}). Importantly, the plot shows that the trained network lies in the same valley of the loss landscape as the optimal predictor, in agreement with Fig. \ref{fig:empirical-training-triple}. 

%observe that (1) the softmax network is operating in the linear regime 
%\matt{TODO check/show this}; 
%and (2) in the linear regime, there is a hyperbolic family of solutions satisfying $\alpha \beta \approx ...$ ... therefore multiple solutions... 

%This MSE loss landscape for the latter two cases is depicted graphically in the Appendix in Fig. \ref{fig:loss-landscape-MSE-2d}.

%\matt{TODO if time, check for hyperbolic levels sets relating to softmax expansion, once details are resolved.}

\subsection{Case 3 - Gaussian mixtures}
%\matt{need to cleanup this case 3 section}
% Note: a 10k run for sig2z=0.1 got beta=4.1,   alpha=1.0   with imperfect loss    @ L=50 (dSeed1, wSeed0)
% Note: a 10k run for sig2z=0.1 got beta=4.385, alpha=1.036 with near perfect loss @ L=500 (dSeed1, wSeed0)
% Note: a 10k run for sig2z=0.1 got beta=4.422, alpha=1.031 with near perfect loss @ L=500 (dSeed0, wSeed0)
% Note: a 10k run for sig2z=0.2 got beta=3.615, alpha=0.995 with near perfect loss @ L=500 (dSeed0, wSeed0) 
%     -> see that doubling sig_z^2 does not halve beta
% Note: a 10k run for sig2z=0.1 ***p=2*** (not p=8) got beta=0.377, alpha=0.995 with near perfect loss @ L=500 (dSeed0, wSeed0) 
%     -> eqn has no explicit dependence on p, but we see a shift in beta (not alpha)

Figure \ref{fig:empirical-training-triple} (case 3) shows networks of size $n=16$ trained to denoise balanced Gaussian mixtures with $p=8$ components that have isotropic variance $\sigma_0^2=0.02$ and centers randomly placed on the unit sphere in $\mathbb{R}^n$. The corruption magnitude is $\sigma_Z^2=0.1$ and context length is $L=500$. The baselines show the zero predictor (dashed grey line) as well as the optimum from Proposition (\ref{prop:bayes-case3}) (pink) and its $\sigma_0^2 \rightarrow 0$ approximation Eq. (\ref{eq:bayes-case3-zerolimit}) (grey). 

 %Since $\sigma_0^2 \neq 0$, the test MSE is not expected to reach the optimum because the network cannot directly express the residual term $\delta \tilde X$. 
%\matt{maybe it can indirectly express it via the residual term in the softmax expansion?}

The trained weights qualitatively approach the optimal estimator for the zero-variance limit but with a slightly different scaling: while the scale of $W_{PV}$ is $\alpha \approx 1$, the $W_{KQ}$ scale is $\beta \approx 5.127 < 1/\sigma_Z^2$. To study this, we provide a corresponding plot of the 2D loss landscape in Fig. \ref{fig:loss-landscape-MSE-2d}(a) in Appendix \ref{appendix:loss-landscapes}. While the symmetry of the previous case has been broken (the context cluster centers $\{\mu_{a}\}$ will not satisfy $\langle \mu \rangle = 0$), we again find that the trained network lies in the anticipated global valley of the MSE loss landscape. 

%achieve a similar error as the simplified $\sigma_0^2 \rightarrow 0$ predictor Eq. (\ref{eq:bayes-case3-zerolimit}) (grey line), while exhibiting a $W_{KQ}$ scale of $\beta \approx 5.127 < 1/\sigma_Z^2$ and the expected $W_{PV}$ scale $\alpha \approx 1$. 

%although with smaller than expected $\beta_{KQ}$ scaling $\langle w_{KQ}^{(ii)} \rangle \approx 2.94 < 1/{\sigma_Z^2}$. 
%%%\bluebf{Potentially relevant Q: for GMM, what is context mean $\bar X$ conditioned on having seen $\tilde X$? Clearly for $p=1$ (one cluster) its non-zero, what about $p>1$? This affects how we interpret the softmax expansion probably.}. 

%\matt{do we need a figure showing scaling vs. MSE for $\sigma_0 \rightarrow 0$}


%\begin{figure}[h]
%\centering
%\includegraphics[width=0.47\textwidth]{figs/fig2-training-baselines-diag.pdf}
%\caption{
%  \bluebf{Remove this figure if we are keeping a ``three-case" one like above.}
%  Linear case. Training dynamics and linear self-attention weights (NOTE: colorbar inaccurate for initial weights on left).
%  }
%\label{fig:empirical-training-linear}
%\end{figure}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.47\textwidth]{figs/fig-training-case-spheres.pdf}
% \caption{
%   \bluebf{Remove this figure if we are keeping a ``three-case" one like above.} [PLACEHOLDER]
%   Nonlinear manifolds case. Training dynamics for one-layer softmax attention. Solid lines show the average loss over six initialization seeds (dashed lines). Right: Initial and final learned weights for two seeds. \matt{Six final weights not identical; possibly fall onto hyperbola in loss landscape}
%   }
% \label{fig:empirical-training-spheres}
% \end{figure}

\section{Connection to dense associative memory networks}
% \section{Trained denoisers through the lens of associative memory networks}
%Energy landscape of the trained denoiser (associative memory perspective)
\label{sec:assoc-mem}

In each of the denoising problems studied above, we have shown analytically and empirically that the optimal weights of the one-layer transformer are scaled identity matrices $W_{PV} \approx \alpha I, W_{KQ} \approx \beta I$. In the softmax case, the trained denoiser can be concisely expressed as 
%\matt{should we have lower case $x_{1:L}$, since we have lowercase query? alternatively, should it be $E_{1:L}$?}\ams{Perhaps, we could say, it is over one realization of the pure tokens $X_{1:L}=x_{1:L}$. One the other hand $X_1:L$, totally unintentionally, looks like a matrix, which happen to be true ;)}
$$\hat x = g(X_{1:L}, \tilde x):=\alpha X_{1:L} \textrm{softmax}(\beta X_{1:L}^T \tilde x),$$
re-written such that $X \in \mathbb{R}^{n \times L}$ stores pure context tokens.

We now demonstrate that such denoising corresponds to one-step gradient descent (with specific step sizes) of energy models related to dense associative memory networks, also known as modern Hopfield networks \cite{ramsauer2021iclr, demircigil2017, krotov2016hopfield}.

Consider the energy function:
\begin{equation}
\label{eq:LSE-Hopfield-alt}
   \En(X_{1:L},s) = \frac{1}{2 \alpha} \| s \|^2 - \frac{1}{\beta}\log \left( 
    \sum_{t=1}^L e^{\beta X_t^T s}
    \right),
\end{equation}

which mirrors the \citet{ramsauer2021iclr} construction but with a Lagrange multiplier added to the first term. 

An operation inherent to the associative memory perspective is the recurrent application of a denoising update. 
Gradient descent iteration $s(t+1) =  s(t) - \gamma \;\nabla_s \En \bigl(X_{1:L}, s(t)\bigr)$ yields

\begin{equation}
\label{eq:DAM-GD-update}
\begin{aligned}
    s(t+1) &= 
      \left(1 - \frac{\gamma}{\alpha}\right) s(t) 
      + \gamma X_{1:L} \mathrm{softmax} \bigl( \beta X_{1:L}^T s(t) \bigr).
\end{aligned}
\end{equation} 

It is now transparent that initializing the state to the query $s(0)=\tilde x$ and taking a single step with size $\gamma=\alpha$ recovers the behavior of the trained attention model (Fig. \ref{fig:energy-landscape-denoising}). On the other hand, one could consider alternative step sizes and recurrent iteration. However, as Fig. \ref{fig:energy-landscape-denoising} shows, this has the potential to degrade performance. 

Additional details are provided in Appendix \ref{appendix:sec-mapping-attn-assocmem}. In particular, the energy model for linear attention is discussed in Appendix \ref{appendix:linear-attention-trad-Hopfield}.

% \subsection{Section 4 layout option 2}
% We have considered trained networks, relating the query $\tilde X$ and the estimate of the target $\hat X$, of the form 
% \begin{equation}
% \hat X=f(\tilde X):=\gamma\sum_{t=1}^LX_t\langle X_t,\tilde X\rangle
% \label{linear-net}
% \end{equation}
% for the first case (with $\gamma=\tfrac{1}{(\sigma_0^2+\sigma_Z^2)L}$) and of the form
% \begin{equation}
% \hat X=f(\tilde X):=\frac {\sum_{t=1}^LX_te^{\beta\langle X_t,\tilde X\rangle}}{\sum_{t=1}^Le^{\beta\langle X_t,\tilde X\rangle}}
% \label{softmax-net}
% \end{equation}
% for the two other cases (with $\beta=\tfrac{1}{\sigma_Z^2}$). We now demonstrate that these relations could be thought of as one-step gradient descent (with specific step sizes) of energy models related to DAM networks, also known as Hopfield models.
 
% Consider energy functions of the form
% \begin{equation}
%     \En(s) = \frac{\lambda}{2} \|s\|^{2} - \phi(s)
% \label{eq:Energy}
% \end{equation}
% where $s\in\R^n$. The differentiable function $\phi$ would be chosen later and would depend upon $\{X_t\}_{t=1}^L$, a dependence that is suppressed in the notation \alb{and~$\phi$ will be chosen later?}\ams{Incorporated in the text}. A one-step gradient descent update with step size $\gamma$ gives
% \begin{align}
%     s^{\text{new}} &=s^{\text{old}}-\gamma\nabla \En(s^{\text{old}})\nonumber\\
%     &=(1-\gamma\lambda)s^{\text{old}} +\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD}
% \end{align}
% Choosing  $\gamma\lambda=1$, we get
% \begin{equation}
%     s^{\text{new}} =\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD-fixed-step}
% \end{equation}

% Now note that choosing $\phi(s):=\tfrac{1}{2}\sum_{t=1}^L(\langle X_t,s\rangle)^2$ or 
% \begin{equation}
%     \En(s) := \frac{1}{2\gamma} \|s\|^{2} - \frac{1}{2}s^T(\sum_{t=1}^LX_tX_t^T)s
% \label{eq:spherical-Hopfield}
% \end{equation}
% gives us $s^{\text{new}} =\gamma\sum_tX_t\langle X_t,s^{\text{old}}\rangle$ which is Eq. (\ref{linear-net}), once we identify the query $\tilde X$ to be $s^{\text{old}}$ and the predicted target $\hat X$ to be $s^{\text{new}}$. We will call this energy function the Naive Spherical Hopfield model which does not have a retrieval phase \cite{fischer1993spin} (see \citet{bolle2003spherical} for similar model that does) \alb{refs?}\ams{Refs added!}. Also, See Appendix.

% On the other hand, choosing $\phi(s):=\tfrac{1}{\beta}\sum_{t=1}^Le^{\beta \langle X_t,s\rangle}$ or
% \begin{equation}
%     \En(s) := \frac{1}{2} \|s\|^{2} - {\beta}\sum_{t=1}^Le^{\beta \langle X_t,s\rangle}
% \label{eq:LSE-Hopfield}
% \end{equation}
% with the step size $\gamma=1$ gives us Eq. (\ref{softmax-net}), the same identifications for the query and the target as before. The energy in Eq. (\ref{eq:LSE-Hopfield}) is the log-sum-exp function-based energy defined by \citet{ramsauer2021iclr}, starting with work of \citet{demircigil2017}.

% \subsection{Connection to dense associative memory networks with exponential kernel to softmax self-attention}

% In the main text, we find that training one-layer self-attention networks on the denoising task leads to weights that resemble scaled identity matrices $W_{PV}\approx \alpha I$, $W_{KQ}\approx \beta I$. 

% To recover the trained network, consider the generalized LSE energy function \alb{point to an equation where it is defined?}, 

% \begin{equation}
%     E(q) = \frac{\lambda}{2} \|q\|^{2} - \frac{1}{\beta}\,\mathrm{LSE} \bigl( \beta X^{T} q \bigr)
% \label{eq:DAM}
% \end{equation}

% where $q$ denotes an input query vector and $\lambda$ need not be 1. 

% This is a dense associative memory network (also known as modern Hopfield network), similar to what was introduced in the inspirational work of \cite{ramsauer2021iclr}. It can be seen as a continuous-state relaxation of the associative memory network with binary states and exponential storage capacity discussed in \cite{demircigil2017}.

% From Eq. (\ref{eq:DAM}), note that gradient descent
%     $q(t+1) =  q(t) - \gamma \;\nabla E\bigl(q(t)\bigr)$ 
% with $\gamma=1/\lambda$ yields the iteration
% \begin{equation}
%     q(t+1) = 
%     \gamma X \mathrm{softmax} \bigl( \beta X^T q(t) \bigr),
% \end{equation}
% matching the trained denoising update rule in the main text when we also select $\lambda=1/\alpha$. 

% As an interesting generalization, we note that using any other gradient descent step size 
%     $\gamma \neq 1/\lambda$ 
% leads to a ``residual" term 
% \begin{equation}
% \begin{aligned}
%     q(t+1) &=  q(t) - \gamma \;\nabla E\bigl(q(t)\bigr)  \\
%            &= (1 - \gamma \lambda) q(t) + \gamma X \mathrm{softmax} \bigl( \beta X^T q(t) \bigr).
% \end{aligned}
% \end{equation} 
% Residual terms are commonly used in multilayer variants of transformer architectures to help propagate information to downstream attention heads. 

% \bluebf{TODO: as a bridge to the ``linear" sections below, consider Taylor expansion of the softmax update / LSE energy function (look at Jacobian as in Ramsauer2021 appendix)}

% \textbf{Connecting ``Spherical Hopfield model" to linear self-attention}

% Consider the problem of finding the ground state of a `spherical' Hopfield model. Let $$E_0(s)=-\frac{1}{2}s^T J s=-\frac{1}{2}\sum_{ab} s_a J_{ab} s_b,$$ with $$J=\sum_ix_ix_i^T$$ for some prescribed set of vectors $\{x_i\} \in \mathbb{R}^n$. In components, we have the more familiar $J_{ab} = \frac{1}{L}\sum_{i=1}^L x_{ia} x_{ib}$. We want to find $$\argmin_{s} E_0(s)$$ subject to the constraint $$s^Ts=nv^2,$$ $v$ being a constant. In the associative memory literature, the $\{x_i\}$ are seen as patterns or memories that have been encoded in the couplings $J_{ab}$. 
% The model is ``spherical" in the sense that the norm of $s$ is fixed, so $s$ lives on a sphere.

% The solution to the problem is obvious: $s$ has to belong to the eigenvector subspace corresponding to the largest eigenvalue of $J$.

% We can reformulate this constrained optimization problem using a Lagrange multiplier: find
%             $$\argmin_{s} \Big[\max_\lambda \big\{E_0(s)+\frac{\lambda}{2} (s^Ts-nv^2)\big\}\Big].$$
% One can show that the optimal setting of the Lagrange multiplier $\lambda$ is the largest eignevalue of $J$.

% For a fixed $\lambda$, define $E_\lambda(s):=E_0(s)+\frac{\lambda}{2} s^Ts.$ This is the $s$ dependent part of the Lagrangian, whose $s$-derivative has to vanish at the optimal point. Note that the gradient $\nabla_s E_\lambda(s) = - Js + \lambda s$ is zero at $s^*=(1/\lambda)Js^*$, confirming that $s^*$ is indeed an eigenvector of $J$.

% Given an initial vector $s(0) \in \mathbb{R}^n$, gradient descent $s(t+1) - s(t) = -\gamma \nabla E_\lambda(s(t))$ can be written as 
%   \begin{equation*}
%      s(t+1) = s(t) +  \gamma (J - \lambda I) s(t)=(1-\gamma \lambda)s(t) +  \gamma J s(t).
%   \end{equation*} 

% In particular, look at the setting $\gamma=1/\lambda$ where $\lambda$ is the largest eigenvalue of $J$.

% Then, we have the iteration
% \begin{equation*}
%      s(t+1) = \frac{1} {\lambda} J s(t),
%   \end{equation*} 
% which, when repeated, makes $s(t)$ converge to the eigen-subspace corresponding the largest eigenvalue of $J$ (provided $\lambda$ is the largest eigenvalue of $J$).

% To connect to the linear self-attention in Section \ref{sec:results}, we interpret the $x_i$ vectors as context tokens $i=1, \ldots, L-1$, whereas the $s$ vector is related to both the corrupted query vector $\tilde x_L$ and its denoised correction $\hat x_L$.
% Given a corrupted query token $s_\textrm{corrupted}$, interpret one step of gradient descent as

% \begin{equation}
% \begin{aligned}
% s_\textrm{corrected} & = s_\textrm{corrupted} + \gamma (J - \lambda I) s_\textrm{corrupted} \\
%                      & = s_\textrm{corrupted} - \gamma \lambda s_\textrm{corrupted} + \gamma J s_\textrm{corrupted} \\
%                      & = \frac{1}{\lambda} J s_\textrm{corrupted} \\
%                      & = \frac{1}{\lambda L} \sum_i \langle x_i, s_\textrm{corrupted} \rangle x_i  
% \end{aligned}
% \end{equation}

% where the third line uses a step size $\gamma=1/\lambda$, resulting in an update rule that closely resembles \bluebf{[TODO - matches exactly? also care $L$ vs $L-1$]} the linear attention formula Eq. (...). \bluebf{This block is repetitive with the above blocks. Need to cleanup. Can skip to last line but first say what $J$ is.}



% \section{Discussion}
% \label{sec:discussion}
% \bluebf{todo...}

% \textbf{Discussion points}
% \begin{itemize}
%   \item \textbf{Connection between associative memory and self-attention} 
  
%   \item \textbf{Initialization / ``Why scaled identity empirically"} 
%    Observation of \citet{trockman2023identity} (cited/noted in \citet{bartlett2024jmlr}) -- empirically, trained vision transformers with standard self-attention heads have $W_{KQ}$ and $W_{PV}$ nearly proportional to scaled identity matrices (i.e. $\gamma I + \epsilon$). 
%    \citet{trockman2023identity} leverage this observation to propose alternative initialization to the weight matrices. 
%    \bluebf{See what this means vs. the note about attn dim vs. token dim above}. 
% \end{itemize}

% \textbf{Limitations}: In our minimal setup, we do not consider various details common to training conventional large transformers (e.g. \cite{...}). These include:
% \begin{itemize}
%   \item Autoregressive training with causal masking. 
%   \item Positional embedding (our results do not depend on the order of tokens preceding the final query token).
%   \item Our simplification to only use $W_{KQ}$ means we are effectively using one large attention head ($n_\textrm{attn} \ge n$). When $n_\textrm{attn} < n$, which is a common practical choice reducing computational costs, $W_K^T W_Q \neq W_{KQ}$ in general.
%   \item Multiple attention heads. 
%   \item Pre-attention normalization (``Layer norm").
%   \item Post-attention MLP step. 
%   \item (currently) Multiple layers.
% \end{itemize}


% \textbf{TODO list}
% \begin{itemize}
%     \color{blue}
%     \item Gaussian mixture model (see \citet{reddy2024iclr})... dirichlet... the same idea of one corruption at end -- now softmax should be doing the job better than linear. 
%     \item ``If we add covariance structure... may need two-layer transformer or MLP".
%     \item Low-dim nonlinear manifolds (e.g. sphere - in progress now).
%     \item Documents with topics \citep{li2023transformers}. Discuss Oct. 2 a bit (one-hot encoded words, corpus scrambled, and full-rank attention $W_{KQ}$).
%     \item Q: Assoc mem angle: in what cases does the denoising improve if we take multiple steps?
%     \item Effect of distribution shift at prompt/test time (as in \citet{garg2022neurips, bartlett2024jmlr}).
%     \item Oct 2. Converge to same weights for fixed dataset from different random IC -- let's see how this solution is affected by: 
%     (A) dataset size, 
%     (B) batch size/LR (e.g. gradient flow limit),
%     (C) what if training set is just one subspace, but many noise examples (we can consider resampling the pure vectors or keeping them fixed). 
%     \item Nov. MNIST type data example (we discussed applying a permutation to distort the samples before adding gaussian noise). \href{https://arxiv.org/abs/2212.04458}{Jascha Sohl-Dickstein 2022 arxiv link}. 
% \end{itemize}
% \color{black}

% \noindent \textbf{Conferences}
% \color{blue}
% ICML late Jan. IJCAI also an option (due Jan 10/17) \href{https://ijcai24.org/important-dates/}{link}. ICLR workshops maybe February - most workshops not considered publications (quasi-peer reviewed, publish shorter versions of your work).
% \color{black}

% \section{Introduction}
% \label{sec:intro}

% % \textbf{High-level question} What problems are minimal transformer architectures capable of solving, and how do they solve them?
% % \begin{itemize}
% %   \item What ``problems" do we have in mind? - Would like to connect pattern storage/retrieval to in-context learning. 
% %   \item What do we mean by ``minimal" - What are the constraints on the architecture?
% %   \item By ``how do they solve it" - We'd like to map back to \citet{ramsauer2021iclr}. Further, how does a solution evolve over the course of training?
% % \end{itemize}


% In the recent years, the tranformer architecture \cite{Vaswani2017} has seen many successes in applications, from natural langugage processing \cite{zhang2023turing,patwardhan2023transformers}, in particular, large language models \cite{raiaan2024review}, to computer vision tasks \cite{khan2022transformers,han2022survey}. Understanding the mechanism of transormer-based networks is a major research goal at this moment. In an interesting development, \citet{bietti2024birth} discuss transformer performance in terms  of associative memories.

% The most celebrated model for associative memories in systems neurocience is the so-called Hopfield model \cite{amari1972learning, nakano1972associatron, little1974existence, Hopfield1982}. This model has a capacity for memories proportional to the number of nodes \cite{Hopfield1982,Amit1985}. In the last decade, new energy functions \cite{krotov2016hopfield, demircigil2017} were proposed for dense associative associative memories with much higher capacities. These energy functions are often referred to as modern Hopfield models. 
% \citet{ramsauer2021iclr} pointed out the similarity between one-step update rule of a certain modern Hopfield network \cite{demircigil2017} and a particular one-layer transformer map, generating interest in the statistical physics and the systems neuroscience community \cite{krotov2021large,krotov2023new,lucibello2024prl}.  However, it is not clear what specific task that particular transformer in \cite{ramsauer2021iclr} is solving, making it difficult to make further progress.

% In this paper, we propose that certain in-context denoising problems, which can be solved by a one-layer single-head transformer, provide a bridge between the world of Hopfield models and optimized transformers. In-context learning normally refers to supevised tasks where
%  a sequence of input-output pairs $\{(x_i,g(x_i))\}$ (called a prompt) are provided as an input, and the model can take a test example $(x,-)$ and convert it to something $\approx (x,g(x))$, without having to make any updates to the parameters in the model. Of course the, the model can achieve this only after having trained on sufficiently large and diverse set of supervised learning problems. We could also view this setting as a special unsupervised task, regarding $(x_i,g(x_i))$ as $z_i$ and regarding $(x,-)$ as $\tilde z$ an augmentation or a corruption of $z=(x,g(x))$. This point of view motivates our generalization of in-context learning to the denoising task, where the propmt is a sample from a distribution and the test is done on a noise-corrupted version of a new sample from the same distribution.

%  In-context learning for simple problems like linear regression \citep{garg2022neurips, bartlett2024jmlr, akyurek2023} has taught us much about the capacity of one-layer transformers. In fact, in-context learning in a pre-trained transformer, can be cast as gradient descent in forward pass \citep{vonOswald2023mordvintsev, shen2023khashabi, dai2023gptlearnicl}. Number of layers corresponds to number of iterations of gradient descent on the context-only loss (e.g. $L(\beta)= \sum_{i=1}^n (y_i - \beta^T x_i)^2$). More generally, transformers learn preconditioned gradient descent \cite{ahn2023transformers}.
 
% By setting up this problem and considering the behavior of the trained one-layer transformer, we answer some of the interesting questions. One of them concerns the memorization-generalization dilemma in denoising. A Hopfield model's success is usually measured by successful memorization where in-context learning may have to solve a completely new problem. Another question has to do with the number of iterations of the corresponding Hopfield model: why does the \citet{ramsauer2021iclr} correspondence involves only one iteration of Hopfield energy minimization and not many?

% \textbf{Organization}
% \begin{itemize}
%   \item In Section \ref{sec:results} ...
%   \item In Section \ref{sec:experiments} ...
%   \item In Section \ref{sec:discussion} ...
% \end{itemize}

% \textbf{In summary, our contributions are as follows}
% \begin{itemize}
%   \item ...
%   \item ...
%   \item ...
%   \end{itemize}

% \textbf{Literature to cite}
% \begin{itemize}

%   \item In-context learning in minimal transformers: simple problems (e.g. linear regression) \citep{garg2022neurips, bartlett2024jmlr, akyurek2023}; ``burstiness of ICL" \citep{reddy2024iclr}. Note that Garg and Reddy mention explicit or implicit use of training curricula. 
  
%   \item In-context learning in large pre-trained transformers: can cast as gradient descent in forward pass; see \citep{vonOswald2023mordvintsev} and later works \citep{shen2023khashabi, dai2023gptlearnicl}. Number of layers corresponds (somehow) to number of iterations of gradient descent on the context-only loss (e.g. $L(\beta)= \sum_{i=1}^n (y_i - \beta^T x_i)^2$).
%   See also Ahn... Sra 2023. 
  
%   \item Additional ICL references: \citep{bai2023neurips, bhattamishra2024iclr, vladymyrov2024versatileICL}. \bluebf{Rong Ge, last author on vladymyrov2024, gave talk at 2024 Simons ML meeting - Sept 26-27. Look for slides/video?}. 
  
%   \item Additional ``linear" transformer references. (Linear refers to them attention step scaling linearly with sequence length rather than quadratically). ``Performer" (which they call ``FAVOR+" -- Fast Attention Via positive Orthogonal Random features) \citep{choromanski2021performer} with connection classic Hopfield in \href{https://ml-jku.github.io/blog-post-performer/}{separate blog post}; ``fast weight programmers" \citep{schlag2021schmidhuberLinearTransformersFWP}; see also \cite{katharopoulos2020icml}. Ideas also date further back, work by Hinton is pertinent \citep{hinton1987fastweights, hinton2016fastweights}. 
  
%   \item Connection between softmax attention \bluebf{(what about linear)} and dense associative memories \citep{krotov2016hopfield, demircigil2017} first noted in \citet{ramsauer2021iclr}. 
%   This connection is also discussed in \citet{krotov2021large} which additionally considers continuous-time dynamics for the associated DAM.   
%   Explored further in subsequent works 
%   \citep{millidge2022universal, hu2023sparseModernHopfield, lucibello2024prl, alonso2024ncomm, hoover2023energytransformer}. 
%   The connection of ICL to ``gradient descent in the forward pass" (above) may relate to the observation of \citet{ramsauer2021iclr} that the (softmax) attention update is a single step of gradient descent in the associative memory retrieval -- has anyone explored this angle? 
%   Need to see which ICL works cite Ramsauer; note that \citet{vonOswald2023mordvintsev} does but only superficially.
%   Recent work by Krotov at NeurIPS24: \href{https://arxiv.org/abs/2410.24153}{https://arxiv.org/abs/2410.24153}.
  
%   \item Additional associative memory references: connection to SDM \citep{bricken2021attention}; overparameterized NN implement assoc. memory \citep{radhakrishnanBelkin2020pnas}.
% \end{itemize}

% \textbf{List of potentially related recent works}
% \begin{enumerate}

%   \item 
%   \citep{schaeffer2024bridgingassociativememoryprobabilistic}
%   \href{https://arxiv.org/abs/2402.10202}{2024 arxiv - Bridging Associative Memory and Probabilistic Modeling (under review)}. They call it ``in-context learning of energy functions" (rather than denoising). Large team, 12 people (Stanford/MIT). 
%   See also \href{https://www.youtube.com/watch?v=hK_L9Typ8PM}{talk link (youtube)}.   
%   Appears to be an extension of their \href{https://rylanschaeffer.github.io/content/research/2023_neurips_workshop_infinite_associative_memory/main.html}{2023 NeurIPS workshop / poster}.
% \end{enumerate}




% \section{Problem formulation - In-context denoising}
% \label{sec:results}
% To study the generalized denoising problems introduced below,
% we adapt the setup of \citep{garg2022neurips, akyurek2023, bartlett2024jmlr}. These and related works have investigated in-context learning of minimal sequence models by presenting elementary mathematical queries (e.g. linear regression problems) as prompts. 
% In particular, sequence models are trained to perform next-token prediction on inputs of the form $X=(x_1, g(x_1), ..., x_k, g(x_k), x_{\text{query}})$ where $g(x):\mathbb{R}^n \rightarrow \mathbb{R}$ is drawn from a predetermined function class, such as the family of linear maps $g(x)= w^T x$. Crucially, $g(x)$ is implied solely through the context and differs across prompts -- performant models are therefore said to ``learn $g(x)$ in context".  

% Misc notes
% \begin{itemize}
%   \item Canonical problem: linear regression, given dataset of form $X, y$, interpret as training pairs $\{x_i, y_i\}$ with scalar $y_i = w^T x_i + \epsilon_i$. 
%   \item Loss function is squared loss $L(y,y')=(y-y')^2$. 
%   In \citet{akyurek2023} they also consider regularized linear regression objective (ridge regression). 
% \end{itemize}

% We generalize previous approaches by making the training pairs in the sequence implicit. 
% Specifically, we do not provide the correct tokens explicitly as part of the input sequence itself. 
% Past works have either expanded the token dimension by stacking the training pairs 
% (e.g. \citet{bartlett2024jmlr} where training pairs $(x_i, g(x_i)) \in \mathbb{R}^{n} \times \mathbb{R}$ are presented as single tokens $[x_i,  g(x_i)] \in \mathbb{R}^{n+1}$). 
% or additionally doubling the sequence length as mentioned above
% (e.g. \citet{garg2022neurips, akyurek2023}).  

% \textbf{Notation} \bluebf{[Collect notation here or in Setup section]}
% \begin{itemize}
%   \item $n$ -- ambient dimension of input tokens
%   \item $x \in \mathbb{R}^{n}$ -- an input token (vector)
%   \item $X = (x_1, ..., x_{L-1}, \tilde x_L) \in \mathbb{R}^{n \times L}$ -- sequence model input consisting of $L$ tokens, where ``tilde" indicates that the final token has in some way been corrupted
%   \item $L$ -- context length of input sequence
%   \item $d$ -- dimensionality of manifold $S$ that $x_t$ are sampled from
% \end{itemize}

% \subsection{Setup}

% We consider three elementary in-context denoising tasks:
% \begin{enumerate}
%   \item Linear manifolds ($d$-dimensional subspaces)
%   \item Nonlinear manifolds ($d$-spheres)
%   \item Gaussian mixtures (clustering)
% \end{enumerate}

% Each case is characterized by a distribution $x_t \sim  p_X(x)$ from which tokens are sampled. 
% Sequences of the form $X=(x_1, x_2, ..., x_{L-1}, x_\text{query}) \in \mathbb{R}^{n \times L}$ are given as ``context" (input) to a sequence model $f(X)$. The first $L-1$ tokens are ``pure samples" from $p_X(x)$ whereas $x_\textrm{query} = x + \eta$ is a pure token that has been corrupted by isotropic Gaussian noise $\eta \sim \mathcal{N}(0, \sigma_{\eta}^2 I_n)$. The objective is to return a denoised token $\hat x_L$ that minimizes the mean squared error (MSE). 

% In the following subsection we explain the pure token distributions for each task. We then derive Bayes optimal estimators for each of the three settings. In Section \ref{sec:experiments} we use estimators as baselines to evaluate the performance of one-layer transformers $f_{\theta}(X)$ trained on finite datasets. 

% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.65\textwidth]{figs/fig1-problem-setup-linear-v2.pdf}
% \caption{
%   Problem setup for the case of denoising linear subspaces and baseline estimators (linear projections).
%   }
% \label{fig:setup}
% \end{figure*}


% \begin{figure*}[h!]
% \centering
% \includegraphics[width=0.95\textwidth]{figs/fig1-cases.pdf}
% \caption{
%   The three denoising cases considered here include instances of linear and non-linear manifolds as well as Gaussian mixtures. 
%   In each case, the context consists of a sequence of pure tokens from the true distribution along with a single query token (shown in solid red) that has been corrupted by Gaussian noise.
%   The solid green token is the target that the model is asked to predict given the prompt. 
%   In the final example, the cluster centers (squares) are shown for illustration and are not explicitly provided in the prompt. 
%   }
% \label{fig:setup-cases}
% \end{figure*}

% \subsection{Task-specific token distributions}

% Below we describe the task-specific distributions $p_X(x)$ and the process for sampling tokens $\{x_t\}$ which we use to generate prompts $X=(x_1, x_2, ..., x_{L-1}, x_\text{query})$. 

% % NOTE: since \tilde x_L = query, we could write this section emphasizing this (and have a fig 1 panel like attention Wikipedia figures) 

% The same corruption process applies to all cases. Let $\eta \sim \mathcal{N}(0, \sigma_{\eta}^2 I_n)$ and define $x_\textrm{query} = x + \eta$
% %$\tilde \rvx \equiv \rvx + \boldsymbol{\eta}$
% ; in practice we will only corrupt the last token $x_L$ of the sequence.

% \subsubsection{Case 1 - Linear manifolds}
% A given training prompt consists of pure tokens sampled from a random $d$-dimensional subspace $S$ of $\mathbb{R}^n$. 

% \begin{itemize}
%   \item Let $P$ be the projection operator to a $d$-dim subspace $S$ of $\mathbb{R}^n$. 
%   \bluebf{maybe mention how $P$ is sampled}
%   \item Let $\rvz \sim \mathcal{N}(0, \sigma_z^2 I_n)$ and define $\rvx = P \rvz$; we use this process to construct input sequences $X=(x_1, ..., x_L)$ of $L$ independent tokens.
% \end{itemize}

% We have $p_{X}(x) \sim \mathcal{N}(0, \sigma_z^2 P)$. \bluebf{[note: tricky to define like this since $P$ is not invertible]}

% \subsubsection{Case 2 - Nonlinear manifolds}
% We focus on the case of $d$-dimensional spheres of fixed radius $R$ centered at the origin in $\mathbb{R}^n$. 

% \bluebf{TODO explain p(x) ?}

% In practice, we uniformly sample points with fixed norm in $\mathbb{R}^d$ and embed them in $\mathbb{R}^n$ by concatenating zeros. We then rotate the points by selecting a random orthogonal matrix $Q \in \mathbb{R}^{n \times n}$. 

% \subsubsection{Case 3 - Gaussian mixtures (Clustering)}
% Pure tokens are sampled from a weighted mixture of isotropic Gaussians in $n$-dimensions, 
% $\{w_{\alpha}, (\mu_{\alpha}, \sigma_{\alpha}^2)\}_{\alpha=1}^p$. The density is

% $$p_{X}(x) =  
%     \sum_{\alpha } w_{\alpha} C_{\alpha}
%         e^{- \lVert x - \mu_{\alpha} \rVert ^2 / 2 \sigma_{\alpha}^2}, $$

% where $C_{\alpha} = (2 \pi \sigma_{\alpha}^2)^{-n/2}$ are normalizing constants. 

% Below we will consider the limit that the variances go to zero. In that case, the density is simply

% $$p_{X}(x) =  
%     \sum_{\alpha } w_{\alpha} \delta(x - \mu_{\alpha}). $$

% \subsection{Bayes optimal denoising baselines for each case}
% % based on notes 24-11-21

% We refer to the distribution of pure samples as $p_{X}(x)$ and corrupted samples as $p_{\tilde X}(\tilde x)$. Remarks:

% \begin{itemize}
%   \item $p_{X}(x)$ is task-dependent (the three scenarios considered here are introduced above).
  
%   \item $p_{\tilde X}(\tilde x)$ where $\tilde x = x + \eta$. 
%   For a sum of independent random variables, $Y=X_1+X_2$, their pdf is a convolution $p_Y(y)=\int p_{X_1}(x) p_{X_2}(y-x)dx$. Thus:
%     \begin{equation*}
%     \begin{aligned}
%       p_{\tilde X}(\tilde x) 
%         & = \int p_{\eta}(z) p_{X}(\tilde x - z) dz \\
%         & = C_{\eta} \int e ^{- \lVert z \rVert ^2 / 2 \sigma_{\eta}^2} p_{X}(\tilde x - z) dz
%     \end{aligned}
%     \end{equation*}

%     where $C_\eta = (2 \pi \sigma_{\eta}^2)^{-n/2}$ is a constant. E.g. in the linear denoising problem:
%     \begin{equation*}
%     \begin{aligned}
%       p_{\tilde X}(\tilde x) & = C_{\eta} C_{X} \int 
%         e ^{- \lVert z \rVert ^2 / 2 \sigma_{\eta}^2}
%         e ^{- \lVert z - \tilde X \rVert ^2 / 2 \sigma_{z}^2}
%       dz.
%     \end{aligned}
%     \end{equation*}

%   \item $p_{\tilde X \mid X}(\tilde x \mid x)$. 
%     This is simply 
%     $$p_{\eta}(\tilde x - x) = C_{\eta} e ^{- \lVert \tilde x - x \rVert ^2 / 2 \sigma_{\eta}^2}.$$

%   \item $p_{X \mid \tilde X}(x \mid \tilde x)$. By Bayes' theorem, this is

%     \begin{equation*}
%     \begin{aligned}
%       p_{X \mid \tilde X}(x \mid \tilde x) 
%         & = \frac{p_{\tilde X \mid X}(\tilde x \mid x) p_{X}(x)} { p_{\tilde X}(\tilde x) }\\
%         & = \frac
%             {e ^{- \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} p_X(x) }
%             {\int e ^{- \lVert z - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} p_X(z) dz}.
%     \end{aligned}
%     \end{equation*}

% \end{itemize}

% In this setting, we seek a function $f(\tilde X): \mathbb{R}^n \rightarrow \mathbb{R}^n$ such that $\E_{X, \tilde X} \left[ \Vert X - f(\tilde X) \rVert^2 \right]$ is minimized. Below we will drop the subscript when the expectation is over the joint distribution $p_{X, \tilde X}$. 
% \bluebf{TODO cleanup notation}.

% Claim: The objective is minimized by

% \begin{equation*}
% \begin{aligned}
%   f(\tilde X) &= \E_{X \mid \tilde X} [ X \mid \tilde X ]
%               = \int x\: p_{X \mid \tilde X}(x \mid \tilde x) dx \\
%               &= \frac{1}{p_{\tilde X}(\tilde x)} \int x\: p_{X, \tilde X}(x, \tilde x) dx.
% \end{aligned}
% \end{equation*}

% Proof: Using the law of total expectation, observe that 
% \bluebf{[matt: it was not obvious that the cross-terms cancelled when going from 1st to 2nd line, but they do]} 
% % NOTE: The same basic result is shown for scalar r.v. on Wikipedia here:
% %    https://en.wikipedia.org/wiki/Conditional_variance
% \begin{equation*}
% \begin{aligned}
%   \E \left[ \Vert X - f(\tilde X) \rVert^2 \right] 
%   & = \E_{\tilde X} \left[ 
%         \E_{X \mid \tilde X} \bigl[ \lVert X - f(\tilde X) \rVert^2 \mid \tilde X \bigr] 
%       \right] \\ 
%   & = \E_{\tilde X} \Bigl[ 
%         \E_{X\mid \tilde X} \bigl[ \lVert X - \E [X \mid \tilde X] \rVert^2 \mid \tilde X \bigr]  \\
%          & \:\:\:\:\:\:\:\:\:\:\:\: + \lVert \E [X \mid \tilde X] - f(\tilde X) \rVert^2 \Bigr] \\
%   & \ge \E_{\tilde X} \left[ 
%         \E_{X\mid \tilde X} \bigl[ \lVert X - \E [X \mid \tilde X] \rVert^2 \mid \tilde X \bigr]
%     \right] \\
%   & = \E_{\tilde X} \left[ \Tr{ \Cov (X \mid \tilde X)} \right].
% \end{aligned}
% \end{equation*}

% Note the final line is independent of $f$. This lower bound is met when $f(\tilde X) = \E [ X \mid \tilde X ]$. 

% Thus, the Bayes optimal denoiser is the posterior expectation for $X$ given $\tilde X$. The expected loss is found by computing the posterior sum of variances. 

% This quantity can be computed for both the PCA and manifold cases (given the variances and dimensionalities). In the Gaussian mixture (clustering) case, it depends on the choice of the centroids which then needs to be averaged over. 

% \emph{Linear case} \\ \\
% The PCA baseline for the linear denoising task is a special case of this result. 
% There, $X$ is an isotropic Gaussian in a restricted subspace,
% \begin{equation*}
%   p_{X \mid \tilde X}(x \mid \tilde x) = C(\tilde x) p_X(x) e ^{  
%     - \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2
%   }    
% \end{equation*}
% where $C(\tilde x)$ is a normalizing factor. The noise can be decomposed into parallel and perpendicular parts using the projection $P$ onto $S$, i.e.
% \begin{equation*}
%   \tilde X = \tilde X_{\paral} + \tilde X_{\perp}= P \tilde X + (I-P) \tilde X,
% \end{equation*}
% so that 
% \begin{equation*}
%   e^{- \lVert x - \tilde x \rVert ^2 / 2 \sigma_{\eta}^2} = 
%     e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \:\:
%     e^{- \lVert \tilde x_{\perp} \rVert ^2 / 2 \sigma_{\eta}^2}.
% \end{equation*}

% Only the first factor matters for $p_{X \mid \tilde X}(x \mid \tilde x)$ since it depends on $x$. Then, inside the subspace $S$ (dropping $\tilde X_{\perp}$),   \\
% \begin{equation*}
% \begin{aligned}
% p_X(x)e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2}
%   & \propto e^{- \lVert x \rVert ^2 / 2 \sigma_{z}^2
%          - \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \\
%   & \propto \exp 
%     \left( - \frac{\lVert x - \frac{\sigma_{z}^2}{\sigma_{z}^2 + \sigma_{\eta}^2} \tilde x_{\paral} \rVert ^2}
%                    {2 \frac{\sigma_{z}^2 \sigma_{\eta}^2}{\sigma_{z}^2 + \sigma_{\eta}^2}
%                   } 
%     \right). 
% \end{aligned}
% \end{equation*}

% Thus, $f(\tilde X) 
%     = \frac{\sigma_{z}^2}{\sigma_{z}^2 + \sigma_{\eta}^2} \tilde X_{\paral} 
%     = \frac{\sigma_{z}^2}{\sigma_{z}^2 + \sigma_{\eta}^2} P \tilde X$.

% \emph{Manifold case} \\ \\
% In the nonlinear manifold denoising problem, we focus on the case of lower dimensional spheres $S$ (e.g. the circle $S^1 \subset \mathbb{R}^2$). 
% For such manifolds, we have
% \begin{equation*}
% \begin{aligned}
% \E [ X \mid \tilde X ] 
%   & = 
%   \frac{\int e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \: x\: p_X(x) dx}
%        {\int e^{- \lVert x - \tilde x_{\paral} \rVert ^2 / 2 \sigma_{\eta}^2} \: p_X(x) dx} \\
%   & = 
%   \frac{\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{\eta}^2} \:x\, d S_x}
%        {\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{\eta}^2} \: d S_x}.
% \end{aligned}
% \end{equation*}

% Note that this prediction can be expressed by a single softmax self-attention layer \bluebf{(check and compare to trained weights)},

% \begin{equation*}
% \E [ X \mid \tilde X ] 
%  = W_{PV} X \textrm{softmax}(X^T W_{KQ} \tilde x) 
% \end{equation*}

% if we select $W_{KQ}=\frac{1}{\sigma_{\eta}^2} I$, $W_{PV}=I$, and provide sufficiently large context to approximate the integrals.

% If $S$ is a $d_x$--sphere of radius $R_x$, then the optimal predictor is again a shrunk projection of $\tilde x$ onto $S$,

% \begin{equation*}
% \begin{aligned}
%   \frac{\int_0^{\pi} e^{R_x \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{\eta}^2} \: \cos \theta \sin^{(d_x - 1)} \theta \: d\theta}
%        {\int_0^{\pi} e^{R_x \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{\eta}^2} \: \sin^{(d_x - 1)} \theta \: d\theta}
%   R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert} \\
%   = 
%   \frac{I_\frac{d_x+1}{2} \left(R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2} \right)}
%        {I_\frac{d_x-1}{2} \left(R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2} \right)} 
%   R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert},
% \end{aligned}
% \end{equation*}

% where $I_a(y)$ is a modified Bessel function and 
% $R_x \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert}$
% is the point on $S$ in the direction of $x_{\paral}$. \\
% \bluebf{(see Anirvan diagram in notes)}

% \bluebf{CITE Gradshteyn and Ryzhik 5th Ed (integral book) Sec 3.915 p517 and Sec 8.486 p981...\\
% Anirvan notes p5-8: ``more than one way of expressing ratio in terms of the modified Bessel function"
% }

% Denoting $\beta = R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2}$, we arrive at \bluebf{(check this)}:
% \begin{equation*}
% \begin{aligned}
% \E_{S}[\cos{\theta}] 
%     = \frac{-\frac{d}{\beta}I_d(\beta) + I_d^{\prime}(\beta)}{I_d(\beta)} 
%     = \frac{I_{d+1}(\beta)}{I_d(\beta)}.
% \end{aligned}
% \end{equation*}
  

% Alternative note: \bluebf{cleanup, debugging}: 
% \begin{itemize}
%   \item Modified Bessel function 
%   $$I_n(\beta) \equiv \frac{1}{{\pi}}\int_0^{\pi}  e^{\beta \cos \theta} \: \cos (n \theta) d \theta $$
%   \item Recursive identity (for $d-1$ being even)
% $$\int_0^{\pi} e^{\beta \cos \theta} \: \cos \theta \sin^{2m} \theta \: d\theta
% =...$$

% \item Check case of $d_x=1$ (circle). Ratio should match 
%     $$\E_{S}[\cos{\theta}] = \frac{I_1(\beta)}{I_0(\beta)}.$$

% \item Check case of $d_x=2$ (sphere). Ratio should match  
%     $$\E_{S}[\cos{\theta}] = \frac{1}{\beta}(\beta \coth(\beta) - 1) 
%                          \neq \frac{I_2(\beta)}{I_1(\beta)}.$$

% \end{itemize}




% \emph{Clustering case} \\ \\
% For the clustering case involving isotropic Gaussian mixtures 
% $\{w_{\alpha}, (\mu_{\alpha}, \sigma_{\alpha}^2)\}_{\alpha=1}^p$, 
% consider the limit that the variances go to zero. Then, 
% \begin{equation*}
%   p_{X \mid \tilde X}(x \mid \tilde x) = \frac
%     {\sum_{\alpha }
%          w_{\alpha} \delta (x - \mu_{\alpha}) 
%          e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}}
%     {\sum_{\alpha }
%         w_{\alpha} e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}},
% \end{equation*}
% which gives the conditional expectation 
% \begin{equation*}
%   \E [X \mid \tilde X] = \frac
%     {\sum_{\alpha } 
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: w_{\alpha} \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \: w_{\alpha}
%         }.
% \end{equation*}

% Note that when the mixtures are equal-weight and the center norms $\lVert \mu_{\alpha} \rVert $ are the same, this is the softmax solution: 
% $$\E [X \mid \tilde X] = \frac
%     {\sum_{\alpha } 
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}} 
%   = \sum_{\alpha} p_{\alpha} \mu_{\alpha} .$$
% \bluebf{TODO elaborate and connect further}

% \subsection{Analysis for each case (just linear currently)}

% \subsubsection{Analysis for linear case and connection to principle component analysis} 

% \bluebf{[A lot of this is now a bit redundant with the Bayesian generalization; cleanup]}

% Given corrupt samples $\tilde \rvx$, we seek an optimal linear estimator $\hat \rvx = V^* \tilde \rvx$ satisfying 
%   $\textrm{argmin}_{V} \, \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right]$.
%   %; $F$ indicates Frobenius norm (which we omit below).

% We first note $\lVert V \tilde \rvx - \rvx \rVert^2 = \lVert (V - I_n) P \rvz + V \boldsymbol{\eta} \rVert^2$. Furthermore, we have by definition that $\E[\rvx \rvx^T]=P \E[\rvz \rvz^T] P^T = \sigma_Z^2 P$. Thus, the sum $\frac{1}{L}\sum_{t=1}^L x_t x_t^T = \frac{1}{L} XX^T$, where $x_t$ are independent samples of $\rvx$, converges in probability to $\sigma_Z^2 P$ as $L \rightarrow \infty$. Finally, since $\rvz$ has zero mean and isotropic covariance, $\E [ \rvz^T A \rvz ] = \sigma_z^2 \Tr [ A ]$ for any matrix $A$. The same holds for $\boldsymbol{\eta}$. 

% Leveraging the above properties as well as the independence of both random variables, we have: 

% \begin{equation*}
%   \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \sigma_{\eta}^2 \Tr (V^T V) + \sigma_{z}^2 \Tr((V-I_n)^T(V-I_n)P).
% \end{equation*}

% One is free to choose a basis, so select one which gives $P = \begin{pmatrix} I_d & 0 \\ 0 & 0 \end{pmatrix}$ and denote $V = \begin{pmatrix} V_\parallel & V_1 \\ V_2 & V_\perp \end{pmatrix}$ accordingly. 
% Carrying out the multiplications, we have
% \begin{equation*}
% \begin{aligned}
%   \E \left[  & \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \\
%     &  \: \sigma_{\eta}^2 \bigl[ \Tr (V_\parallel^T V_\parallel) + \Tr (V_\perp^T V_\perp)
%     + \Tr (V_1^T V_1) + \Tr (V_2^T V_2) \bigr]   \\
%   & + \sigma_{z}^2 
%     \left[\Tr((V_\parallel - I_d)^T (V_\parallel - I_d)) + \Tr(V_1^T V_1) \right].
% \end{aligned}
% \end{equation*}

  
% Observe that setting $V_1, V_2, V_\perp$ to zero solely benefits the objective. The question remains to identify $V_\parallel$ by solving 
%   $\textrm{argmin}_{V_\parallel} \, 
%   \sigma_{\eta}^2 \lVert V_\parallel \rVert_F^2 
%     + \sigma_{z}^2 \lVert V_\parallel - I_d \rVert_F^2$. 
%   %\item 
% Extremizing this by differentiation gives 
% $V_\parallel^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} I_d$, which when combined with the aforementioned zero blocks gives
% %demonstrating that the optimal linear denoiser is the ``shrunk" projection operator,

% %$V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P$.
% \begin{equation}
%     \label{eq:shrunk_projector}
%   V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P
% \end{equation}
  
% Thus, given a sequence of independent samples $X=(x_1,...,x_{L-1}, \tilde x_L)$ from $S$ -- where the last token $\tilde x_L = x_L + \eta$ has been corrupted out of $S$ -- a good \bluebf{(standard notation? MMSE?)} linear estimator for $x_L$ is

% \begin{equation}
%     \label{eq:linear_predictor}
%     %\hat x_L = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{\sigma_z^2}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
%     \hat x_L = \frac{1}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{1}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
% \end{equation}

% The optimal linear denoiser is therefore a shrunk projection operator, tending to zero as the corruption scale $\sigma_{\eta}$ becomes large relative to the scale of pure samples $\sigma_z$.

% \bluebf{Check/show that same minima are found with gradient flow from particular or random initial conditions...} 
% $d \theta / dt = - \nabla C(\theta)$ given an initialization $\theta_0$ (or distribution of $\theta_0$) as in \citet{bartlett2024jmlr}. 

% From the above analysis, it is evident that the optimal weights for the linear self-attention transformer are scaled identity matrices \bluebf{todo: cite figure showing experimental match}: 
% \begin{equation}
%     \label{eq:linear_predictor_weights}
% W_{KQ}^*=\alpha_1 I, \: \: W_{PV}^*=\alpha_2 I
%     \:\:\:
%     \textrm{with} \:\:\:
%     \alpha_1 \alpha_2 = \frac{1}{ \sigma_{\eta}^2 + \sigma_{z}^2}.
% \end{equation}
% %$W_{KQ}^*=\alpha_1 I$, $W_{PV}^*=\alpha_2 I$ with $\alpha_1 \alpha_2 = \frac{\sigma_{z}^2}{ \sigma_{\eta}^2 + \sigma_{z}^2}$.

% Furthermore, substituting $V^*$ shows that the minimized expected error is 
% \begin{equation}
%     \label{eq:linear_predictor_errorval}
%     \E \left[ \lVert V^* \tilde \rvx - \rvx \rVert^2 \right] 
%     = d \sigma_{\eta}^2 \sigma_z^2 / (\sigma_z^2 + \sigma_{\eta}^2),
% \end{equation}
% which grows linearly with the subspace dimension $d$.
% %\bluebf{(note: need to divide by $1/n$ when comparing to torch MSE which auto scales loss by $1/n$ i.e. number of features/dimensions)}.

% \begin{itemize}

%   \item \bluebf{Note: sum should be to L-1 but in practice we go to L and include the corrupt token? Should try to subtract the contribution of last token in steps above.}
%   Note (``correction term"): find $\E[\beta XX^T] = \beta L \E[x x^T] + \beta \E[\eta \eta^T] = P \sigma_z^2 + \beta \sigma_{\eta}^2 I_n$. Thus: $\Tr [\hat P] =  d \sigma_z^2 + \frac{n}{L} \sigma_{\eta}^2$.
  
%   % Good
%   %\item \bluebf{TODO: check that Eq. \ref{eq:linear_predictor_errorval} matches our MSE after training (use as baseline)}

%   \item \bluebf{TODO: comparison to PCA and case of orthogonal corruption; code already there for it} (introduce a scalar $\gamma \in [0,1]$ which selects between orthogonal and isotropic noise).

%   \item \bluebf{TODO: generalize to affine subspaces; code already there for it.}

% \end{itemize}

% \section{In-context denoising with minimal transformers - empirical results}
% \label{sec:experiments}

% \textbf{Input:}
% Let $X = (x_1, x_2, ..., x_{L-1}, x_\text{query}) \in \mathbb{R}^{n \times L}$ be an input to a sequence model. The tokens $x_t \in \mathbb{R}^n$ are sampled from a task-specific distribution $p_X(x)$ as introduced in Section \ref{sec:results}. The final token $x_\text{query}$ is a pure token that has been corrupted by additive Gaussian noise. 
% %For instance, in Case 1 (Linear subspaces), the first $L-1$ tokens lie exactly on a subspace $S$ of $\mathbb{R}^n$ with dimension $d \ll L$, while the final token $x_\text{query}$ is a corrupted version of a pure token $x_L$ drawn from $S$. 

% \textbf{Objective:}
% Given an input sequence $X$, return the decorrupted final token $x_L$. We consider the mean-squared error loss over a collection of $M$ training pairs, $\{X^{(i)}, x_L^{(i)}\}_{i=1}^{M}$, with $x_L^{(i)}$ denoting the uncorrupted final token (the target), 
% \begin{equation}
% \label{eq:cost}
%     %\textrm{argmin}_{\theta} \: L(\theta) \:\: \textrm{where} \:\: 
%     C(\theta) = \sum_{i=1}^{M} \lVert f_{\theta}(X^{(i)}) - x_{L}^{(i)} \rVert^2,
% \end{equation}
% where $f_{\theta}(X^{(i)})$ denotes the denoised final token predicted by the model for input sequence $X^{(i)}$.
% \bluebf{Q: should we consider explicit objective that pure samples are not corrupted (i.e. include it in cost)? I.e., the first $L-1$ tokens should be invariant under $f(X)$, just the last token is updated.}

% \textbf{Minimal transformer architecture}

% Following \citet{bartlett2024jmlr}, we consider simplified one-layer transformers 
% \begin{equation}
%   F(X) = W_{PV} X\sigma(\beta X^T W_{KQ} X) \:\:\: \in \mathbb{R}^{n \times L}    
% \end{equation}

% where $\beta\equiv\frac{1}{L}$ and $W_{KQ}, W_{PV} \in \mathbb{R}^{n \times n}$ are learnable weights abbreviated by $\theta$. %See Appendix for details. 

% For the map $\sigma(\cdot):\mathbb{R}^{L \times L} \rightarrow \mathbb{R}^{L \times L}$ we consider column-wise softmax 
% (i.e. for each column $z$ of $Z$, $\sigma(z)=\frac{1}{\sum_{i=1}^n e^{z_i}}(e^{z_1}, \ldots, e^{z_n}) \in \mathbb{R}^n$) 
% and linearized attention with 
% %Softmax attention: $\sigma(Z)$ denotes column-wise softmax, i.e. for each column $z$, $\sigma(z)=\frac{1}{\sum_{i=1}^n e^{z_i}}(e^{z_1}, \ldots, e^{z_n}) \in \mathbb{R}^n$.
% %Linear attention:  $\sigma(\cdot)=I$
% $\sigma(\cdot)=I$ so that
% \begin{equation}
%    F_\textrm{LSA}(X) = W_{PV} X \beta X^T W_{KQ} X.
% \end{equation}

% %%\bluebf{Note: \citet{bartlett2024jmlr} divides both expressions by $\rho \equiv L-1$} \\

% In either case, the network prediction $f(X) \in \mathbb{R}^n$ is simply the last column $F(X)_{:, L}$ of the attention layer, $f(X) = W_{PV} X \sigma (X^T W_{KQ} \tilde x_L) = W_{PV} \sum_{t=1}^L a_{tL} x_t$:

% Softmax case: 
% $$f(X) = \frac{W_{PV} \sum_{t=1}^L \exp{(\beta x_t^T W_{KQ} \tilde x_L)} x_t}
%               {\sum_{i=1}^L \exp{(\beta x_i^T W_{KQ} \tilde x_L)}} . $$

% Linear case:  
% $$f_\textrm{LSA}(X)=W_{PV} \hat P W_{KQ} \tilde x_L$$ where $\hat P \equiv \beta \sum_{t=1}^L x_t x_t^T$. 

% \bluebf{Note:} \cite{bartlett2024jmlr} Eq. (3) has the residual term $X + f(X)$.

% Notation to incorporate above:
% \begin{itemize}
%   \item $A \equiv \sigma(X^T W_{KQ} X)  \in \mathbb{R}^{L \times L}$ -- attention matrix (provided $n_\textrm{attn} > n$).
%   \item $a_{ij} = \exp{(x_i^T W_{KQ} x_j)} / \sum_{t=1}^L \exp{(x_t^T W_{KQ} x_j)}$ -- softmax attention coefficients 
% \end{itemize}


% \subsection{Case 1 - Linear}
% \bluebf{TODO}. 
% See Figure \ref{fig:empirical-training-linear} and Figure \ref{fig:empirical-ICL}. 

% \subsection{Case 2 - Clustering}
% \bluebf{TODO}

% \subsection{Case 3 - Nonlinear manifolds}
% \bluebf{TODO}


% \begin{figure}[h]
% \centering
% \includegraphics[width=0.47\textwidth]{figs/fig2-training-baselines-diag.pdf}
% \caption{
%   Linear case. Training dynamics and learned weights (NOTE: colorbar inaccurate for initial weights on left).
%   }
% \label{fig:empirical-training-linear}
% \end{figure}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=0.95\textwidth]{figs/fig3-icl.pdf}
% \caption{
% (a) Trained linear attention network converges to Bayes optimal estimator as context length increases. 
% (b) An attention network trained to denoise subspaces of dimension $d=8$ can accurately denoise subspaces of different dimensions presented at inference time.}
% \label{fig:empirical-ICL}
% \end{figure*}

\begin{figure}[h!]
\centering
\includegraphics[width=0.49\textwidth]{fig-energy-landscape-denoising-1x2-wide.pdf}
%\includegraphics[width=0.45\textwidth]{figs/fig-energy-landscape-denoising-1x2.pdf}
%\includegraphics[width=0.45\textwidth]{figs/fig-energy-landscape-denoising.pdf}
\caption{
  Gradient descent denoising for the nonlinear manifold case (spheres) in $n=2$ with $d=1$. A context-aware dense associative memory network $\En(X_{1:L}, s)$ is constructed whose gradient corresponds to the Bayes optimal update (trained attention layer). Note that the density of sampled context tokens sculpts the valleys of the energy landscape. 
  Left: the attention step of a one-layer transformer trained on the denoising task corresponds to a single gradient descent step. 
  Right: Iterating the denoising process\textemdash as is conventional for Hopfield networks\textemdash can potentially degrade the estimate by causing it to become query-independent (e.g. converging to a distant minimum). Here $R=1, \sigma_{Z}^2=10, L=20$ and $\alpha=1, \beta= 1/\sigma_{Z}^2$. 
  }
\label{fig:energy-landscape-denoising}
\end{figure}

\section{Discussion}
\label{sec:discussion}

Motivated by the connection between attention mechanisms and dense associative memories, here we have introduced in-context denoising, a task that distills their relationship. We first analyze the general problem, deriving Bayes optimal predictors for certain restricted tasks. We identify that one-layer transformers using either softmax or linearized self-attention are expressive enough to describe these predictors. We then empirically demonstrate that standard training of attention layers from random initial weights will readily converge to ``scaled identity" weights with scales that approach the derived optima given sufficient context.
Accordingly, the rather minimal transformers studied here can perform optimal denoising of novel tasks provided at inference time via self-contained prompts. This work therefore sheds light on other in-context learning phenomena, a point we return to below. 

While practical transformers differ in various ways from the minimal models studied here, we note several key connections. 
Intriguingly, the self-attention heads of trained transformers sometimes exhibit weights $W_{KQ}$, $W_{PV}$ that resemble scaled identity matrices, i.e. $ c I + \epsilon$ with small fluctuations $\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$,
an observation noted in \citet{trockman2023identity}. This phenomenon motivated their proposal of ``mimetic" weight initialization schemes mirroring this learned structure. Relatedly, connections to associative memory concepts have been explored in other architectures \cite{smart2021iclr}, which enabled data-dependent weight initialization strategies to be identified and leveraged. 
More broadly, our study suggests that trained attention layers can readily adopt structures that facilitate context-aware associative retrieval. 
We have also noted preliminary connections between our work and other architectural features of modern transformers, namely layer normalization and residual streams, which warrant further study. 

%\matt{Anirvan 1pm note: diffusion model connection / generative model (since we're doing denoising); there is a recent Krotov ref. maybe worth tiny discussion paragraph here? E.g. https://arxiv.org/pdf/2309.16750 and https://repository.ubn.ru.nl/bitstream/handle/2066/312683/312683.pdf?sequence=1}
% papers:
% https://arxiv.org/abs/2410.24153 (Oct 2024)
% https://arxiv.org/abs/2309.17290 (orig?)
% https://arxiv.org/abs/2309.16750 (followup?)
%\ams{Edited it some more and changed the second sentence. Do not know if it is too confusing for others. It pays homages to adjacent physics crowd!}
%\ams{
%Since in-context denoising and generative modeling both involve learning about an underlying distribution, there may be interesting relationships between these two tasks. Recently, \citet{pham2024memorization} invoked spurious states of the Hopfield model as a way of understanding how one can move away from retrieving individual memorized patterns towards generalization via appropriate mixtures of multiple similar `memories'. In our work, one-step updates do not have to land in a spurious minimum, but we often operate under circumstances where there are such states (see, for example, the energy landscape in Fig. \ref{fig:energy-landscape-denoising}). More generally, analogies between energy-based associative memory and diffusion models have been emphasized by \citet{hoover2023memory}. 
%Lastly, Bayes optimal denoisers play an important role in the analysis \cite{ghio2024sampling} of a very related generative model that is based on stochastic interpolants \cite{albergo2022building}. 
%Of course, this work focuses on the case where it is possible to sample the enough tokens from the relevant distributions for certain functions to converge, generative models become important when the distribution is in a prohibitively high dimensional space making direct sampling difficult. Still, being able understand the precise the relationship between our work and different generative modeling approaches would be an interesting direction to pursue.} 
%\matt{The second sentence is maybe too detailed yet vague, and the second-last sentence I am not familiar with those refs -- up to you. I think we need to mention (1) we are in interpolation mode, we do not need full recall, which can in some cases be bad (2) general connection to diffusion models \citep{hoover2023memory} }

Since in-context denoising and generative modeling both involve learning about an underlying distribution, suggesting potential relationships between these two tasks. 
Recently, \citet{pham2024memorization} invoked spurious states of the Hopfield model as a way of understanding how one can move away from retrieving individual memorized patterns towards generalization via appropriate mixtures of multiple similar ``memories".   
In our work, one-step updates do not have to land in a spurious minimum, but we often operate under circumstances where there are such states (see, for example, the energy landscape in Fig. \ref{fig:energy-landscape-denoising}). 
More generally, analogies between energy-based associative memory and diffusion models have been emphasized by \citet{hoover2023memory}. 
Lastly, Bayes optimal denoisers play an important role in the analysis \cite{ghio2024sampling} of a very related generative model that is based on stochastic interpolants \cite{albergo2022building}. 
Of course, this work focuses on the case where it is possible to sample enough tokens from the relevant distributions for certain functions to converge, generative models become important when the distribution is in a prohibitively high dimensional space making direct sampling difficult. Nonetheless, investigating the precise relationship between our work and different generative modeling approaches would be an interesting direction to pursue.

Overall, this work refines the connection between dense associative memories and attention layers first identified in \cite{ramsauer2021iclr}. 
%\matt{maybe add comment on memorization vs. generalization, or interpolation}. 
%\matt{as a shorter alternative to having the full paragraph above, maybe we can sneak in a single sentence pointing to the diffusion models here:} \matt{Since their stimulating work, DAMs have also been connected to other key architectures for generative modeling, namely diffusion models [refs...]}. 
While we show that one energy minimization step of a particular DAM (associated with a trained attention layer) is optimal for the denoising tasks studied here, it remains an open question whether multilayer architectures with varying or tied weights could extend these results to more complex tasks by effectively performing multiple iterative steps. This aligns with recent studies on in-context learning, which have considered whether transformers with multiple layers emulate gradient descent updates on a context-specific objective \cite{vonOswald2023mordvintsev, shen2023khashabi, dai2023gptlearnicl, ahn2023transformers}. Investigating when and how multilayer attention architectures perform such gradient descent iterations in a manner that is both context-dependent and informed by a large training set represents an exciting direction for future research at the intersection of transformer mechanisms, associative memory retrieval, and in-context learning.

%\matt{Moved this block from intro following Alberto comment. maybe this is a good point to end on, as it's a logical direction for future work but not smth we need to do - we prove and show that one layer is optimal for our tasks!}

% \textbf{Limitations}: 
% In our minimal setup, we do not consider various details common to training conventional large transformers \matt{e.g. refs...}. These include:
% \begin{itemize}
%   \item Autoregressive training with causal masking. 
%   \item Positional embedding (our results do not depend on the order of tokens preceding the final query token).
%   \item Our simplification to only use $W_{KQ}$ means we are effectively using one large attention head ($n_\textrm{attn} \ge n$). When $n_\textrm{attn} < n$, which is a common practical choice reducing computational costs, $W_K^T W_Q \neq W_{KQ}$ in general.
%   \item Multiple attention heads. 
%   \item Pre-attention normalization (``Layer norm").
%   \item Post-attention MLP step. 
%   \item Residual term/stream.
%   \item Multiple layers.
% \end{itemize}

%reported in the literature \matt{refs, or say ``see below" (i.e. last para)}. 

% %\matt{[Tangential point on weight initialization?] ``Why scaled identity empirically"; could turn this into: ``while practical transformers seem very different} 
% \matt{Could turn this paragraph into: ``While practical transformers seem very different [see laundry list below], we note several contact points with our study which focuses on isolated attention layers". Then mention (a) residual stream (b) depth as gradient descent iterations (c) pre-norm observation (d) scaled identity weights } 
%    Empirically, trained vision transformers with standard self-attention heads have $W_{KQ}$ and $W_{PV}$ nearly proportional to scaled identity matrices (i.e. $\gamma I + \epsilon$), as note by \citet{trockman2023identity}.
%    %(cited/noted in \citet{bartlett2024jmlr}) -- . 
%    There, the authors leverage this observation to propose alternative initialization to the weight matrices. Relatedly, connections to associative memory concepts have been explored in other architectures \cite{smart2021iclr}, which likewise led to the identification of deterministic weight initialization strategies.
%    %relating to associative memory concepts have been considered in other works \citet{smart2021iclr} in other simple architectures. 


% ===========================================================================
%\section*{Software and Data}
%\bluebf{TODO check if we need this section (or any others) at init submission}

%\noindent \textbf{Scrap notes} 
%\begin{itemize}
  %\item \citet{bartlett2024jmlr} --  (See section E). They also use an extension of codebase from \citet{garg2022neurips}. They used Adam for the big models at least and also used a curriculum training method like \citet{garg2022neurips}. They make a note about training only on the last token vs. training on the full list of sub-prompts contained in the sequence (size $k=1,...,L$), which we don't have in our setup (since $x_1$ to $x_{L-1}$ are not corrupted). Accordingly, they disregard causal masking (as we do here).
%\end{itemize}

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
MS acknowledges M. Mézard for very useful feedback on an earlier version of this work. AS thanks D. Krotov and P. Mehta for enlightening discussions on related matters. 
%\bluebf{- People:  D. Krotov, M. Mézard... .}

%\section*{Impact Statement}
%This paper presents work whose goal is to advance the field of 
%Machine Learning. There are many potential societal consequences 
%of our work, none which we feel must be specifically highlighted here.

%\bibliography{references}
%\bibliographystyle{icml2025}

\input{main.bbl}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Notation}

%\bluebf{The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{appA}
\renewcommand{\theequation}{\ref{appA}.\arabic{equation}}
\setcounter{equation}{0}  % reset the counter


\subsection{Recurring notation} 
\label{appendix:notation}

\begin{itemize}
  \item $n$ -- ambient dimension of input tokens.
  \item $x_t \in \mathbb{R}^{n}$ -- the value of the $t$-th random input token.
  \item $E=(X_1, ..., X_{L}, \tilde X) $ -- the random variable input to the sequence model. The ``tilde" indicates that the final token has in some way been corrupted. $E$ takes values $(x_1, ..., x_{L}, \tilde x) \in \mathbb{R}^{n \times (L+1)}$.
  \item $L$ -- context length = number of uncorrupted tokens.
  \item $d$ -- dimensionality of manifold $S$ that $x_t$ are sampled from
  \item $N$ -- number of training pairs
\end{itemize} 

\subsection{Bayes posterior notation}
\label{appendix:Bayes-notation}

\begin{itemize}
  \item $p_{X}(x)$ is task-dependent (the three scenarios considered here are introduced above).
  
  \item $p_{\tilde X}(\tilde x)$ where $\tilde x = x + z$. 
  For a sum of independent random variables, $Y=X_1+X_2$, their pdf is a convolution $p_Y(y)=\int p_{X_1}(x) p_{X_2}(y-x)dx$. Thus:
    \begin{equation*}
    \begin{aligned}
      p_{\tilde X}(\tilde x) 
        & = \int p_{Z}(z) p_{X}(\tilde x - z) dz \\
        & = C_{Z} \int e ^{- \lVert z \rVert ^2 / 2 \sigma_{Z}^2} p_{X}(\tilde x - z) dz
    \end{aligned}
    \end{equation*}
   where $C_Z = (2 \pi \sigma_{Z}^2)^{-n/2}$ is a constant. 
    %E.g. in the linear denoising problem:
    % \begin{equation*}
    % \begin{aligned}
    %   p_{\tilde X}(\tilde x) & = C_{Z} C_{X} \int 
    %     e ^{- \lVert z \rVert ^2 / 2 \sigma_{Z}^2}
    %     e ^{- \lVert\tilde x -z\rVert ^2 / 2 \sigma_{0}^2}
    %   dz.
    % \end{aligned}
    % \end{equation*}

  \item $p_{\tilde X \mid X}(\tilde x \mid x)$: 
    This is simply 
    $$p_{Z}(\tilde x - x) = C_{Z} e ^{- \lVert \tilde x - x \rVert ^2 / 2 \sigma_{Z}^2}.$$
   

  \item $p_{X \mid \tilde X}(x \mid \tilde x)$: By Bayes' theorem, this is

    \begin{equation*}
    \begin{aligned}
      p_{X \mid \tilde X}(x \mid \tilde x) 
        & = \frac{p_{\tilde X \mid X}(\tilde x \mid x) p_{X}(x)} { p_{\tilde X}(\tilde x) }\\
        & = \frac
            {e ^{- \lVert \tilde x -  x \rVert ^2 / 2 \sigma_{Z}^2} p_X(x) }
            {\int e ^{- \lVert \tilde x -  x' \rVert ^2 / 2 \sigma_{Z}^2} p_X(x') dx'}.
    \end{aligned}
    \end{equation*}

% In this setting, we seek a function $f(\tilde X): \mathbb{R}^n \rightarrow \mathbb{R}^n$ such that $\E_{X, \tilde X} \left[ \Vert X - f(\tilde X) \rVert^2 \right]$ is minimized. Below we will drop the subscript when the expectation is over the joint distribution $p_{X, \tilde X}$. 
% \bluebf{TODO cleanup notation}.

% Claim: The objective is minimized by
\item Posterior mean:
\begin{equation*}
\begin{aligned}
   \E_{X \mid \tilde X} [ X \mid \tilde X ]
              &= \int x\: p_{X \mid \tilde X}(x \mid \tilde x) dx \\
              &= \frac{1}{p_{\tilde X}(\tilde x)} \int x\: p_{X, \tilde X}(x, \tilde x) dx.
\end{aligned}
\end{equation*}

\end{itemize}
\section{Bayes optimal predictors for square loss}
\label{appendix:Bayes-optimal}



\subsection{Proof of Proposition \ref{prop:performance-bound}}
\label{appendix:performance-bound}

\begin{proof}

Observe that 
% \bluebf{[matt: cross-terms cancel when going from 1st to 2nd line; can just state the result and  (a) cite the scalar version, or (b) move this mini derivation to the appendix]} 
% % NOTE: The same basic result is shown for scalar r.v. on Wikipedia here:
% %    https://en.wikipedia.org/wiki/Conditional_variance
\begin{equation*}
\begin{aligned}
  \E \left[ \Vert X - f(\tilde X) \rVert^2 \right] 
  & = \E_{\tilde X} \left[ 
        \E_{X \mid \tilde X} \bigl[ \lVert X - f(\tilde X) \rVert^2 \mid \tilde X \bigr] 
      \right] \\ 
  & = \E_{\tilde X} \Bigl[ 
        \E_{X\mid \tilde X} \bigl[ \lVert X - \E [X \mid \tilde X] \rVert^2 \mid \tilde X \bigr]  \\
         & \:\:\:\:\:\:\:\:\:\:\:\: + \lVert \E [X \mid \tilde X] - f(\tilde X) \rVert^2 \Bigr] \\
  & \ge \E_{\tilde X} \left[ 
        \E_{X\mid \tilde X} \bigl[ \lVert X - \E [X \mid \tilde X] \rVert^2 \mid \tilde X \bigr]
    \right] \\
  & = \E_{\tilde X} \left[ \Tr{ \Cov (X \mid \tilde X)} \right].
\end{aligned}
\end{equation*}

Note the final line is independent of $f$. This inequality becomes an equality when $f(\tilde X) = \E [ X \mid \tilde X ]$. 
\end{proof}
% Thus, the Bayes optimal denoiser is the posterior expectation for $X$ given $\tilde X$. The expected loss is found by computing the posterior sum of variances. 


\section{Details of Bayes optimal denoising baselines for each case}
\label{appendix:bayes-optimal-details}
% based on notes 24-11-21

\emph{The linear case} 

\subsection{Proof of Proposition \ref{prop:Bayes-optimal-linear}}
\label{appendix:Bayes-optimal-linear}
\begin{proof}
The linear denoising task is a special case of the result in Proposition \ref{prop:performance-bound}. 
Here, $X$ is an isotropic Gaussian in a restricted subspace,
\begin{equation*}
  p_{X \mid \tilde X}(x \mid \tilde x) = C(\tilde x) p_X(x) e ^{  
    -\frac{\lVert x - \tilde x \rVert ^2} {2 \sigma_{Z}^2}
  }    
\end{equation*}
where $C(\tilde x)$ is a normalizing factor. The noise can be decomposed into parallel and perpendicular parts using the projection $P$ onto $S$, i.e.
\begin{equation*}
  \tilde X = \tilde X_{\paral} + \tilde X_{\perp}= P \tilde X + (I-P) \tilde X,
\end{equation*}
so that 
\begin{equation*}
  e^{- \frac{\lVert x - \tilde x \rVert ^2} {2 \sigma_{Z}^2}} = 
    e^{- \frac{\lVert x-\tilde x_{\paral} \rVert ^2}{2 \sigma_{Z}^2}} \:\:
    e^{- \frac{\lVert \tilde x_{\perp} \rVert ^2} {2 \sigma_{Z}^2}}.
\end{equation*}

Only the first factor matters for $p_{X \mid \tilde X}(x \mid \tilde x)$ since it depends on $x$. Then, for $x\in S$, the linear subspace supporting $p_X$,  dropping the $x$ independent $\tilde x_{\perp}$ contribution,   \\
\begin{equation*}
\begin{aligned}
p_X(x)e^{- \frac{\lVert x - \tilde x_{\paral} \rVert ^2} {2 \sigma_{Z}^2}}
  & \propto e^{- \frac{\lVert x \rVert ^2}{ 2 \sigma_{0}^2}
         - \frac{\lVert x - \tilde x_{\paral} \rVert ^2 }{ 2 \sigma_{Z}^2} }\\
  & \propto \exp 
    \left( - \frac{\lVert x - \frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} \tilde x_{\paral} \rVert ^2}
                   {2 \frac{\sigma_{0}^2 \sigma_{Z}^2}{\sigma_{0}^2 + \sigma_{Z}^2}
                  } 
    \right). 
\end{aligned}
\end{equation*}

Thus, $f(\tilde X) 
    = \frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} \tilde X_{\paral} 
    = \frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} P \tilde X$.

Using $\tilde X=X+Z$, $X=PX$, and the independence of $X$ and $Z$
$$\E\Big[\lVert X-\frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} P \tilde X \rVert^2\Big] =  \E\Big[\lVert\frac{\sigma_{Z}^2}{\sigma_{0}^2 + \sigma_{Z}^2}PX\rVert^2\Big]+\E\Big[\lVert\frac{\sigma_{0}^2}{\sigma_{0}^2 + \sigma_{Z}^2} PZ \rVert^2\Big]=\frac{\sigma_Z^4d\sigma_{0}^2+\sigma_0^4d\sigma_{Z}^2}{(\sigma_{0}^2 + \sigma_{Z}^2)^2}=\frac{d\sigma_0^2\sigma_{Z}^2}{\sigma_{0}^2 + \sigma_{Z}^2}.$$
\end{proof}

\emph{The manifold case} \\ \\

\subsection{Proof of Proposition \ref{prop:Bayes-optimal-manifold}}
\label{appendix:Bayes-optimal-manifold}

\begin{proof}
In the nonlinear manifold denoising problem, we focus on the case of lower dimensional spheres $S$ (e.g. the circle $S^1 \subset \mathbb{R}^2$). 
For such manifolds, we have
\begin{equation*}
\begin{aligned}
\E [ X \mid \tilde X ] 
  & = 
  \frac{\int e^{- \frac{\lVert x - \tilde x_{\paral} \rVert ^2 }{ 2 \sigma_{Z}^2}} \: x\: p_X(x) dx}
       {\int e^{- \frac{\lVert x - \tilde x_{\paral} \rVert ^2 }{ 2 \sigma_{Z}^2}} \: p_X(x) dx} \\
  & = 
  \frac{\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \:x\, d S_x}
       {\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \: d S_x}.
\end{aligned}
\end{equation*}
We have used the fact that $\lVert x - \tilde x_{\paral} \rVert ^2=\lVert x \rVert ^2+\lVert\tilde x_{\paral} \rVert ^2-2\langle x,\tilde x_{\paral}\rangle$ and that $\lVert x \rVert$ is fixed on the sphere.

% From this formulation, two observations can be made.
% First, a finite sample estimator for this this predictor can be represented by a single softmax self-attention layer,

% \begin{equation*}
% \E [ X \mid \tilde X ] 
%  = W_{PV} X \textrm{softmax}(X^T W_{KQ} \tilde x) 
% \end{equation*}

% if we select $W_{KQ}=\frac{1}{\sigma_{\eta}^2} I$, $W_{PV}=I$, and provide sufficiently large context $X$ to approximate the integrals. Below we will see that this solution is readily identified via SGD. 
% \bluebf{(check and compare to trained weights, which seem to settle along a hyperbolic curve | what about gradient flow)}

The integrals can be evaluated directly once the parameters are specified. If $S$ is a $d$--sphere of radius $R$, then the optimal predictor is again a shrunk projection of $\tilde x$ onto $S$,

\begin{equation*}
\begin{aligned}
  \frac{\int_0^{\pi} e^{R \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{Z}^2} \: \cos \theta \sin^{(d - 1)} \theta \: d\theta}
       {\int_0^{\pi} e^{R \lVert \tilde x_{\paral} \rVert \cos \theta / \sigma_{Z}^2} \: \sin^{(d - 1)} \theta \: d\theta}
  R \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert} \\
  = 
  \frac{I_\frac{d+1}{2} \left(R \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{Z}^2} \right)}
       {I_\frac{d-1}{2} \left(R \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{Z}^2} \right)} 
  R \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert},
\end{aligned}
\end{equation*}
where we used identities involving  $I_\nu(y)$, modified Bessel function of the first kind of order $\nu$ \cite{gradstein2007zwillinger}. The vector 
$R \frac{\tilde x_{\paral}}{\lVert \tilde x_{\paral} \rVert}$
is the point on $S$ in the direction of $x_{\paral}$. 

\end{proof}

% \bluebf{(see Anirvan diagram in notes)}

% \bluebf{CITE Gradshteyn and Ryzhik 5th Ed (integral book) Sec 3.915 p517 and Sec 8.486 p981...\\
% Anirvan notes p5-8: ``more than one way of expressing ratio in terms of the modified Bessel function"
% }

% Denoting $\beta = R_x \frac{\lVert \tilde x_{\paral} \rVert}{\sigma_{\eta}^2}$, we arrive at \bluebf{(check this)}:
% \begin{equation*}
% \begin{aligned}
% \E_{S}[\cos{\theta}] 
%     = \frac{-\frac{d}{\beta}I_d(\beta) + I_d^{\prime}(\beta)}{I_d(\beta)} 
%     = \frac{I_{d+1}(\beta)}{I_d(\beta)}.
% \end{aligned}
% \end{equation*}
  

% Alternative note: \bluebf{cleanup, debugging}: 
% \begin{itemize}
%   \item Modified Bessel function 
%   $$I_n(\beta) \equiv \frac{1}{{\pi}}\int_0^{\pi}  e^{\beta \cos \theta} \: \cos (n \theta) d \theta $$
%   \item Recursive identity (for $d-1$ being even)
% $$\int_0^{\pi} e^{\beta \cos \theta} \: \cos \theta \sin^{2m} \theta \: d\theta
% =...$$

% \item Check case of $d_x=1$ (circle). Ratio should match 
%     $$\E_{S}[\cos{\theta}] = \frac{I_1(\beta)}{I_0(\beta)}.$$

% \item Check case of $d_x=2$ (sphere). Ratio should match  
%     $$\E_{S}[\cos{\theta}] = \frac{1}{\beta}(\beta \coth(\beta) - 1) 
%                          \neq \frac{I_2(\beta)}{I_1(\beta)}.$$

% \end{itemize}




\emph{The clustering case} \\ \\

\subsection{Proof of Proposition \ref{prop:bayes-case3}}
\label{appendix:bayes-case3}

\begin{proof}
For the clustering case involving isotropic Gaussian mixtures with parameters
$\{w_{a}, (\mu_{a}, \sigma_{a}^2)\}_{a=1}^p$, 
% consider the limit that the variances go to zero. Then, 
% \begin{equation*}
%   p_{X \mid \tilde X}(x \mid \tilde x) = \frac
%     {\sum_{\alpha }
%          w_{\alpha} \delta (x - \mu_{\alpha}) 
%          e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}}
%     {\sum_{\alpha }
%         w_{\alpha} e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde x \rangle / \sigma_{\eta}^2}},
% \end{equation*}
% which gives the conditional expectation 
% \begin{equation*}
%   \E [X \mid \tilde X] = \frac
%     {\sum_{\alpha } 
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: w_{\alpha} \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%         e^{- \lVert \mu_{\alpha} \rVert ^2 / 2 \sigma_{\eta}^2} e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \: w_{\alpha}
%         }.
% \end{equation*}

% Note that when the mixtures are equal-weight and the center norms $\lVert \mu_{\alpha} \rVert $ are the same, this is the softmax solution: 
% \begin{equation*}
% \begin{aligned}
% \E [X \mid \tilde X] & = \frac
%     {\sum_{\alpha } 
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}
%         \:\: \mu_{\alpha}
%         }
%     {\sum_{\alpha }
%          e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{\eta}^2}} 
%   = \sum_{\alpha} p_{\alpha} \mu_{\alpha} \\
%   & = W_{PV} X \textrm{softmax}(X^T W_{KQ} \tilde x) .
% \end{aligned}
% \end{equation*}
% where $W_{PV} = I$, $W_{KQ} = \frac{1}{\sigma_\eta^2} I$, and $X$ is a sequence of tokens uniformly sampled from the mixture. 
% \bluebf{TODO check this and elaborate + connect further}

% \bluebf{Jan 23 notes below - finite cluster variance case - does not appear to be expressible by one-layer softmax attention}

% On the other hand, for non-zero cluster variances $\sigma_{\alpha}^2$,
\begin{equation*}
\E [X \mid \tilde X] = 
  \frac
   {\int e^{-\frac{\|x - \tilde{x}\|^2}{2 \sigma_Z^2}} \sum_a \left( w_{a} C_a e^{-\frac{\|x - \mu_\alpha\|^2}{2 \sigma_a^2}}\right) 
     x \, dx
   }
   {\int e^{-\frac{\|x - \tilde{x}\|^2}{2 \sigma_Z^2}} \sum_a \left( w_{a} C_a e^{-\frac{\|x - \mu_a\|^2}{2 \sigma_a^2}} \right)
     \, dx
   },
\end{equation*}
where $C_a=(2\pi\sigma_a^2)^{-\tfrac{n}{2}}$.


We can simplify this expression by completing the square in the exponent and using the fact that the integral of a Gaussian about its mean is zero. This yields
\begin{equation*}
  \E [X \mid \tilde X] 
   = 
  \frac
   {\sum_{a} w_{a}C_a m_{a} \int \exp(-g_{a}) \,dx}
   {\sum_{a} w_{a} C_a \int \exp(-g_{a}) \,dx}
\end{equation*}

where we have introduced
\begin{equation*}
g_{a} =\frac{1}{2}\Bigl(\frac{\sigma_Z^2+\sigma_a^2}{\sigma_Z^2\sigma_a^2}\Bigr) \,\|x - m_\alpha\|^2
\, 
+ \frac{1}{2 (\sigma_Z^2 + \sigma_a^2)}\lVert \tilde x - \mu_{a}\rVert^2,
\end{equation*}
with 
%``constant" 
%   $C_{\alpha} = -\frac{\lVert \tilde x - \mu_{\alpha}\rVert}{2 (\sigma_\eta^2 + \sigma_\alpha^2)}$

% and
\begin{equation*}
m_a 
% =
% \frac{\frac{1}{\sigma_\eta^2} \,\tilde{x} + \frac{1}{\sigma_\alpha^2}\,\mu_\alpha}
%      {\frac{1}{\sigma_\eta^2} + \frac{1}{\sigma_\alpha^2}}
 = \frac{\sigma_a^2 \, \tilde{x} + \sigma_Z^2 \,\mu_a}
     {\sigma_a^2 + \sigma_Z^2}.
\end{equation*}

Doing the integrals and using the expressions for $C_a,m_a$
\begin{equation*}
  \E [X \mid \tilde X] 
   = 
  \frac
   {\sum_{a} w_{a}\big(\frac{\sigma_Z^2+\sigma_a^2} {\sigma_a^2}\big)^{n/2} \exp\big(-\frac{\lVert \tilde x - \mu_{a}\rVert^2}{2 (\sigma_Z^2 + \sigma_a^2)}\big) \big(\frac{\sigma_a^2 \, \tilde{x} + \sigma_Z^2 \,\mu_a}
     {\sigma_a^2 + \sigma_Z^2} \big)}
   {\sum_{a} w_{a} \big(\frac{\sigma_Z^2+\sigma_a^2} {\sigma_a^2}\big)^{n/2} \exp\big(-\frac{\lVert \tilde x - \mu_{a}\rVert^2}{2 (\sigma_Z^2 + \sigma_a^2)}\big)}
\end{equation*}
% By symmetry, the integral of a Gaussian about its mean is zero, so the numerator simplifies to
% \begin{equation*}
% \begin{aligned}
%   \int \exp(g_{\alpha}) x \,dx 
%    & = m_{\alpha} \int \exp(g_{\alpha}) \,dx \\
%    & = m_{\alpha} \int \exp(g_{\alpha}) \,dx.
% \end{aligned}
% \end{equation*}

In the case that the center norms $\lVert \mu_{a} \rVert$ are independent of $a$ and variances $\sigma_{a}^2=\sigma_0$, we have
\begin{equation*}
  \E [X \mid \tilde X] 
%  & = 
%  \frac{\sigma_\alpha^2}
%     {\sigma_\alpha^2 + \sigma_\eta^2} \, \tilde{x}
%    + 
%\frac{\sigma_\eta^2}
%     {\sigma_\alpha^2 + \sigma_\eta^2}
%  \frac
%   {\sum_{\alpha} w_{\alpha} \mu_{\alpha} \int \exp(g_{\alpha}) \,dx}
%   {\sum_{\alpha} w_{\alpha} \int \exp(g_{\alpha}) \,dx} \\
 = 
  \frac{\sigma_0^2}
     {\sigma_0^2 + \sigma_Z^2} \, \tilde{x}
    + 
\frac{\sigma_Z^2}
     {\sigma_0^2 + \sigma_Z^2}
  \frac
   {\sum_{a} w_{a} \mu_{a} 
            \exp\left( \frac{\langle \tilde{x}, \mu_a \rangle}{\sigma_Z^2 + \sigma_0^2} \right)
     }
   {\sum_{a} w_{a} 
            \exp\left( \frac{\langle \tilde{x}, \mu_a \rangle}{\sigma_Z^2 + \sigma_0^2} \right)
            %e^{\frac{\langle \tilde{x}, \mu_\alpha \rangle}{(\sigma_\eta^2 + \sigma_\alpha^2)}}
     }.
\end{equation*}

% Which can be expressed as a softmax operation:
% \begin{equation*}
%   \E [X \mid \tilde X] = a \tilde{x} + W_{PV} M \textrm{softmax}(M^T W_{KQ} \tilde x + \ln w),
% \end{equation*}
% where $a = \frac{\sigma_\alpha^2}{\sigma_\eta^2 + \sigma_\alpha^2}$, 
% $W_{PV} = \frac{\sigma_\eta^2}{\sigma_\eta^2 + \sigma_\alpha^2} I$, 
% $W_{KQ} = \frac{1}{\sigma_\eta^2 + \sigma_\alpha^2} I$, and $M$ is a matrix with the cluster centers as columns.

% \matt{For this case we could maybe present this finite sigma result first, then say in small $\sigma$ limit can be approximated by a one layer transformer, becoming exact as $\sigma \rightarrow 0$.}

Note that in the limit that $\sigma_{0} \rightarrow 0$ , this becomes expressible by one-layer self-attention, since one can simply replace the matrix of cluster centers $M=[\mu_1 \ldots \mu_p]$ implicit in the expression with the context $X_{1:L}$ itself, 

\begin{equation*}
  \E [X \mid \tilde X] = \frac
    {\sum_{a } w_{a}
         e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{Z}^2}
        \mu_{a}
        }
    {\sum_{a }w_a e^{\langle \mu_{\alpha}, \tilde X \rangle / \sigma_{Z}^2}
                  }.
\end{equation*}
\end{proof}

% \subsection{Alternative pedagogical derivation for the linear manifolds case}

% Given corrupt samples $\tilde \rvx$, we seek an optimal linear estimator $\hat \rvx = V^* \tilde \rvx$ satisfying 
%   $\textrm{argmin}_{V} \, \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right]$.
%   %; $F$ indicates Frobenius norm (which we omit below).

% We first note $\lVert V \tilde \rvx - \rvx \rVert^2 = \lVert (V - I_n) P \rvz + V \boldsymbol{\eta} \rVert^2$. Furthermore, we have by definition that $\E[\rvx \rvx^T]=P \E[\rvz \rvz^T] P^T = \sigma_Z^2 P$. Thus, the sum $\frac{1}{L}\sum_{t=1}^L x_t x_t^T = \frac{1}{L} XX^T$, where $x_t$ are independent samples of $\rvx$, converges in probability to $\sigma_Z^2 P$ as $L \rightarrow \infty$. Finally, since $\rvz$ has zero mean and isotropic covariance, $\E [ \rvz^T A \rvz ] = \sigma_z^2 \Tr [ A ]$ for any matrix $A$. The same holds for $\boldsymbol{\eta}$. 

% Leveraging the above properties as well as the independence of both random variables, we have: 

% \begin{equation*}
%   \E \left[ \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \sigma_{\eta}^2 \Tr (V^T V) + \sigma_{z}^2 \Tr((V-I_n)^T(V-I_n)P).
% \end{equation*}

% One is free to choose a basis, so select one which gives $P = \begin{pmatrix} I_d & 0 \\ 0 & 0 \end{pmatrix}$ and denote $V = \begin{pmatrix} V_\parallel & V_1 \\ V_2 & V_\perp \end{pmatrix}$ accordingly. 
% Carrying out the multiplications, we have
% \begin{equation*}
% \begin{aligned}
%   \E \left[  & \lVert V \tilde \rvx - \rvx \rVert^2 \right] = \\
%     &  \: \sigma_{\eta}^2 \bigl[ \Tr (V_\parallel^T V_\parallel) + \Tr (V_\perp^T V_\perp)
%     + \Tr (V_1^T V_1) + \Tr (V_2^T V_2) \bigr]   \\
%   & + \sigma_{z}^2 
%     \left[\Tr((V_\parallel - I_d)^T (V_\parallel - I_d)) + \Tr(V_1^T V_1) \right].
% \end{aligned}
% \end{equation*}

  
% Observe that setting $V_1, V_2, V_\perp$ to zero solely benefits the objective. The question remains to identify $V_\parallel$ by solving 
%   $\textrm{argmin}_{V_\parallel} \, 
%   \sigma_{\eta}^2 \lVert V_\parallel \rVert_F^2 
%     + \sigma_{z}^2 \lVert V_\parallel - I_d \rVert_F^2$. 
%   %\item 
% Extremizing this by differentiation gives 
% $V_\parallel^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} I_d$, which when combined with the aforementioned zero blocks gives
% %demonstrating that the optimal linear denoiser is the ``shrunk" projection operator,

% %$V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P$.
% \begin{equation}
%     \label{eq:shrunk_projector}
%   V^* = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} P
% \end{equation}
  
% Thus, given a sequence of independent samples $X=(x_1,...,x_{L-1}, \tilde x_L)$ from $S$ -- where the last token $\tilde x_L = x_L + \eta$ has been corrupted out of $S$ -- the optimal linear estimator for $x_L$ is

% \begin{equation}
%     \label{eq:linear_predictor}
%     %\hat x_L = \frac{\sigma_{z}^2}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{\sigma_z^2}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
%     \hat x_L = \frac{1}{ \sigma_z^2 + \sigma_{\eta}^2} \left( \frac{1}{L-1} \sum_{t=1}^{L-1} x_t x_t^T \right) \tilde x_L.
% \end{equation}

% The optimal linear denoiser is therefore a shrunk projection operator, tending to zero as the corruption scale $\sigma_{\eta}$ becomes large relative to the scale of pure samples $\sigma_z$.

% From the above analysis, it is evident that the optimal weights for the linear self-attention transformer are scaled identity matrices (Fig. \ref{fig:empirical-training-linear}): 
% \begin{equation}
%     \label{eq:linear_predictor_weights}
% W_{KQ}^*=\alpha_1 I, \: \: W_{PV}^*=\alpha_2 I
%     \:\:\:
%     \textrm{with} \:\:\:
%     \alpha_1 \alpha_2 = \frac{1}{ \sigma_{\eta}^2 + \sigma_{z}^2}.
% \end{equation}
% %$W_{KQ}^*=\alpha_1 I$, $W_{PV}^*=\alpha_2 I$ with $\alpha_1 \alpha_2 = \frac{\sigma_{z}^2}{ \sigma_{\eta}^2 + \sigma_{z}^2}$.

% Furthermore, substituting $V^*$ shows that the minimized expected error is 
% \begin{equation}
%     \label{eq:linear_predictor_errorval}
%     \E \left[ \lVert V^* \tilde \rvx - \rvx \rVert^2 \right] 
%     = d \sigma_{\eta}^2 \sigma_z^2 / (\sigma_z^2 + \sigma_{\eta}^2),
% \end{equation}
% which grows linearly with the subspace dimension $d$.
% %\bluebf{(note: need to divide by $1/n$ when comparing to torch MSE which auto scales loss by $1/n$ i.e. number of features/dimensions)}.

% \begin{itemize}

%   \item \bluebf{Note: sum should be to L-1 but in practice we go to L and include the corrupt token? Should try to subtract the contribution of last token in steps above.}
%   Note (``correction term"): find $\E[\beta XX^T] = \beta L \E[x x^T] + \beta \E[\eta \eta^T] = P \sigma_z^2 + \beta \sigma_{\eta}^2 I_n$. Thus: $\Tr [\hat P] =  d \sigma_z^2 + \frac{n}{L} \sigma_{\eta}^2$.

% \end{itemize}


\section{Notes on attention and softmax expansion} 

\subsection{Standard self-attention}
\label{appendix:self-attention-general}
Given a sequence of $L_{\text{seq}}$ input tokens $x_i \in \mathbb{R}^n$ represented as a matrix $X \in \mathbb{R}^{n \times L_{\text{seq}}}$, standard self-attention defines query, key, and value matrices
%K = W_K X$, $Q = W_Q X$, $V = W_V X$  
\begin{equation}
  K = W_K X, Q = W_Q X, V = W_V X
\end{equation}
where $W_K, W_Q \in \mathbb{R}^{n_\textrm{attn} \times n}$ and $W_V \in \mathbb{R}^{n_\textrm{out} \times n}$. 
The softmax self-attention map \cite{Vaswani2017} is then 
%$f(X)=X + V \textrm{softmax}(QK^T)$.
\begin{equation}
  \text{Attn}(X,W_V,W_K^TW_Q):=V \textrm{softmax}(K^T Q)\in\R^{n_\textrm{out}\times L_{\text{seq}}}.
\label{eq:attention-VKQ}
\end{equation}
%\bluebf{what about $W_P$ and $W_{PV}$ (note in single layer we need output dim = embed dim)}


  On merging $W_K$, $W_Q$ into $W_{KQ}=W_K^T W_Q$: 
  The simplification $W_{KQ}=W_K^T W_Q$ (made here and elsewhere) is general only when $n_\textrm{attn} \ge n$; in that case, the product $W_{KQ}$ can have rank $n$ and thus it is reasonable to work with the combined matrix. 
  On the other hand, if $n_\textrm{attn} < n$, then the rank of their product is at most $n_\textrm{attn}$ and thus there are matrices in $\mathbb{R}^{n \times n}$ that cannot be expressed as $W_K^T W_Q$. A similar point can be made about $W_{PV}$. 
  We note that while $n_\textrm{attn} < n$ may be used in practical settings, one often also uses multiple heads which when concatenated could be (roughly) viewed as a single higher-rank head. 
  %\bluebf{formalize/check this sentence if we want to keep}. 
  %\\
  %\bluebf{Should we vary dim $n_\textrm{attn}$? What do others typically choose?}

 % \bluebf{Potential ambiguity: our preferred attention layer omits the query from the context $X$, so it does not immediately fall into the standard self-attention form of Eq. (A.1). Is it fair to call it self-attention? Probably yes, just inflate the context by 1 and set appropriate rows/columns of $W_K, W_V, W_Q$ to zero.}


%\subsection{Linearized self-attention via kernel functions} 

%Since the softmax self-attention operation scales $O(L^2)$, i.e. quadratically with the number of tokens. This has led to interest in ways to reduce the cost while preserving the intent. 

%\matt{see kernel notes from \citep{schlag2021schmidhuberLinearTransformersFWP, millidge2022universal}}.



We will also use the simplest version of linear attention \cite{katharopoulos2020icml},
%\matt{maybe ``with an identity kernel"? they do not explicitly have such expression I think, they consider to first transform the key and query rows/columns with some kernel $\phi(\cdot)$. E.g. ``When the kernel $\phi(\cdot)$ is chosen to be the identity, ... recover the attention operation underlying $F_\textrm{Lin}$ in the main text:"}:

\begin{equation}
  \text{Attn}_{\text{Lin}}(X,W_V,W_K^TW_Q):=\frac{1}{L_{\text{seq}}}V (K^T Q)\in\R^{n_\textrm{out}\times L_{\text{seq}}}.
\label{eq:attention-VKQ}
\end{equation}




\subsection{Minimal transformer architecture for denoising}
\label{appendix:self-attention-denoising}

%To motivate our choice of transformer architecture, let us start by discussing the linear case. 
% To motivate our choice of architecture, let us start by discussing the linear case. 

% There we have $f_\text{opt}(\tilde X)=\tfrac{\sigma_0^2}{\sigma_0^2+\sigma_Z^2}P\tilde X$. Note that, by the law of large numbers, $\hat P=
% \tfrac{1}{\sigma_0^2L}\sum_{t=1}^LX_tX_t^T$ is a random matrix that converges in probability to the orthogonal projection $P$ as $L\to \infty$, since, for each $t$, $X_tX_t^T$ has the expectation $\sigma_0^2P$ and has a finite covariance matrix. So we could propose

% \begin{equation}
%     f(\tilde X)=\frac{\sigma_0^2}{\sigma_0^2+\sigma_Z^2}\hat P\tilde X=\frac{1}{(\sigma_0^2+\sigma_Z^2)L}\sum_{t=1}^LX_t\langle X_t,\tilde X\rangle.
% \end{equation}



 We now consider a simplified one-layer linear transformer in term of our variable $E=(X_{1:L},\tilde X)$ taking values in $\R^{n\times (L+1)}$ and start with the linear transformer which still has sufficient expressive power to capture our finite sample approximation to the Bayes optimal answer in the linear case.
Inspired by \citet{bartlett2024jmlr}, we define 
\begin{equation}
  \textrm{Attn}_\text{Lin}(E,W_{PV},W_{KQ}) :=\frac{1}{L} W_{PV}E M_\text{Lin}E^T W_{KQ} E     
\end{equation}
taking values in $\mathbb{R}^{n \times (L+1)}$. The additional aspect compared to the last subsection is the masking matrix  $M_\text{Lin}\in\R^{(L+1)\times(L+1)}$ which is of the form
\begin{equation}
M_\text{Lin}=\begin{bmatrix}
  I_L & 0_{L\times 1}\\
  0_{1\times L} & 0
  \end{bmatrix},
\end{equation}
preventing  $W_{PV}\tilde X$ from being added to the output. 

Note that this more detailed expression is equivalent to the form used in the main text. 
$$ 
\hat X = F_\textrm{Lin}(E,\theta) := \frac{1}{L} W_{PV} X_{1:L}X_{1:L}^T W_{KQ} \tilde X
$$
% where $E_{0} = E_{:,1:L}, \tilde X = E_{:, L+1}$.

With learnable weights $W_{KQ}, W_{PV} \in \mathbb{R}^{n \times n}$ abbreviated by $\theta$, we define 
\begin{equation}
  F(E,\theta):=[\textrm{Attn}_\text{Lin}(E,W_{PV},W_{KQ})]_{:,L+1}.    
\end{equation}
Note that, when $W_{PV}=\alpha I_n,W_{KQ}=\beta I_n$, and 
$\alpha \beta = \tfrac{1}{\sigma_0^2+\sigma_Z^2}$, $F(E,\theta)$ should approximate the Bayes optimal answer $f_\text{opt}(\tilde X)$ as $L\to \infty$.

Similarly, we could argue that the second two problems, the $d$-dimesional spheres and the $\sigma_0\to 0$ zero limit of the Gaussian mixtures could be addressed by the full softmax attention
\begin{equation}
  \textrm{Attn}(E,W_{PV},W_{KQ}) = W_{PV} E \textrm{softmax}(E^T W_{KQ} E+M) 
\end{equation}
taking values in $\mathbb{R}^{n \times (L+1)}$ where 
$M\in\bar\R^{(L+1)\times(L+1)}$ is a masking matrix  of the form
\begin{equation}
M=\begin{bmatrix}
  0_{L\times(L+ 1)}\\
  (-\infty) 1_{1\times L+1} 
  \end{bmatrix},
\end{equation}
once more, preventing the contribution of $\tilde X$ value to the output.  
The function $\textrm{softmax}(z):=\frac{1}{\sum_{i=1}^n e^{z_i}}(e^{z_1}, \ldots, e^{z_n})^T \in \mathbb{R}^n$ is applied column-wise. 

We then define 
\begin{equation}
  F(E,\theta):=[\textrm{Attn}(E,W_{PV},W_{KQ})]_{:,L+1},
\end{equation}

which is equivalent to the simplified form used in the main text:
$$
\hat X = F(E,\theta) := W_{PV} X_{1:L} \softmax(X_{1:L}^T W_{KQ} \tilde X).
$$
% where $E_0 = E_{:, 1:L}$. 

\subsection{Proof of Theorem \ref{theorem:convergence}}
\label{appendix:convergence}
\begin{proof}
Let the support of  $p_X$ be a subset of a sphere, centered around the origin, of radius $R$. Then the function 
\begin{equation}
\label{eq:softmax-ratio}
g(\{X_t\}_{t=1}^L,\tilde x)=\frac{\sum_{t=1}^LX_te^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}{\sum_{t=1}^Le^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}=\frac{\frac{1}{L}\sum_{t=1}^LX_te^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}{\frac{1}{L}\sum_{t=1}^Le^{\langle X_t,\tilde x\rangle/\sigma_Z^2}}.
\end{equation}
Both the numerator $\tfrac{1}{L}\sum_{t=1}^LX_te^{\langle X_t,\tilde x\rangle/\sigma_Z^2}$  and the denominator $\tfrac{1}{L}\sum_{t=1}^Le^{\langle X_t,\tilde x\rangle/\sigma_Z^2}$ are averages of independent and identically distributed bounded random variables. By the strong law of large numbers, as $L\to \infty$, the average vector in the numerator converges to almost surely to $\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \:x\, dp_X(x)$ for each component, while the average in the denominator almost surely converges $\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \: dp_X(x)$, which is positive. So, as $L\to\infty$, the ratio in Eq. \ref{eq:softmax-ratio} converges almost surely to 
 $$ \frac{\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \:x\, dp_X(x)}
       {\int e^{\langle x, \tilde x_{\paral} \rangle / \sigma_{Z}^2} \: dp_X(x)}$$
which is the Bayes optimal answer $f_\text{opt}(\tilde x)$ for all $\tilde x\in \R^n$.
\end{proof}



% Informally speaking, the softmax attention model has the capacity to subsume the linear attention model. 


% \begin{proposition}
% \begin{align}
% &\textrm{Attn}(E,\frac{1}{\beta}W_{PV},\beta W_{KQ})=\frac{1}{\beta} W_{PV}\bar X\nonumber\\
% &+ \frac{1}{L}W_{PV}\sum_{t=1}^LX_t(X_t-\bar X)^TW_{KQ}\tilde X+O(\beta),
% \end{align}
% where $\bar X=\tfrac{1}{L}\sum_{t=1}^LX_t$ is the empirical mean. 
% \end{proposition}
% See Appendix for the details of small $\beta$ expansion. Since, for case 1, $\E[X_t]=0$ and covariance of $X_t$ is finite, $E[\bar X]=0$, and $E[||\bar X||^2]=O(\tfrac{1}{L})$, allowing us to drop $\bar X$ as $L\to \infty$. If, in addition, $\beta$ is small, only the second term survives. Thus, $\textrm{Attn}(E,\frac{1}{\beta}W_{PV},\beta W_{KQ})$ starts to approximate $\textrm{Attn}_{\text{Lin}}(E,W_{PV},W_{KQ})$ when $L$ is large and $\beta$ is small \alb{why do you need $L$ and $||W_{PV}||$ large? isn't small~$W_{KQ}$ sufficient?}\ams{Does that convince you, Alberto?} \matt{I'm also confused why we need large $W_{PV}$}\alb{yes, thanks!}. We, therefore, could use the softmax model for all three cases. 


\section{Limiting behaviors of the softmax function and attention}
\label{appendix:expansion-softmax}

%\matt{Should clearly obtain $F_\textrm{Lin}(E, %\theta)$ and understand what happens to the weights} %\\
%\matt{need to align this with main text} \\
%\matt{maybe also what happens to the MHN energy function}  

\subsection*{For small argument}
%\( \beta \) (or small $W_{KQ}$)}
A Taylor expansion of the softmax function at zero gives
% \begin{equation*}
% \text{softmax}(v_i) = \frac{1}{Z} \left( 1 + v_i + \frac{1}{2} v_i^2 + O(v_i^3) \right),
% \end{equation*}
% where $Z = \sum_j \left( 1 + v_j + \frac{1}{2} v_j^2 + O(v_j^3)\right)$ is a normalizing factor.
\begin{equation*}
\text{softmax}(\beta v) = \frac{1}{Z} \left( \mathbbm{1} + \beta v  + O(\beta^2) \right),
\end{equation*}
where $Z = \sum_i \left( 1 + \beta v_i + O(\beta^2))\right)=L(1+\beta\bar v+ O(\beta^2))$ is a normalizing factor, with $\bar v=\tfrac{1}{L}\sum_iv_i$. The notation $\mathbbm{1}$ stands for a column vector of ones with the same dimension as $v$.

Thus, we have
\begin{lemma}[Small argument expansion of softmax]
\label{appendix:small-arg}
As $\beta\to 0$,
\begin{equation*}
\mathrm{softmax}(\beta v)
= \frac{1}{L \left( 1 + \beta\bar v + O(\beta^2)\right)} \left( \mathbbm{1} + \beta v +O(\beta^2)\right)
=\frac{1}{L}\left( \mathbbm{1} + \beta(v-\bar v\mathbbm{1}) + O(\beta^2)\right).
\end{equation*}
\end{lemma}

\subsection{Proof of Proposition \ref{prop:attention-limit}}
\label{appendix:attention-limit}
\begin{proof}

$$
F\Big(E,\big(\frac{1}{\epsilon}W_{PV},\epsilon W_{KQ}\big)\Big) := \frac{1}{\epsilon}W_{PV} X_{1:L} \softmax(\epsilon X_{1:L}^T W_{KQ} \tilde X).
$$

Using Lemma \ref{appendix:small-arg}, as $\epsilon\to 0$,
\begin{align}
&F\Big(E,\big(\frac{1}{\epsilon}W_{PV},\epsilon W_{KQ}\big)\Big)=\frac{1}{\epsilon}W_{PV} X_{1:L}\Bigg[\frac{1}{L}\left( \mathbbm{1}_L +\epsilon \big( X_{1:L}^T W_{KQ}\tilde X-(\frac{1}{L}\sum_t X_t^TW_{KQ}\tilde X)\mathbbm{1}_L\big) + O(\epsilon^2)\right)\Bigg]\nonumber\\
&=\frac{1}{\epsilon} W_{PV}\bar X
+ \frac{1}{L}W_{PV}\sum_{t=1}^LX_t(X_t-\bar X)^TW_{KQ}\tilde X+O(\epsilon),
\end{align}
where $\bar X=\tfrac{1}{L}\sum_{t=1}^LX_t$ is the empirical mean and the notation $\mathbbm{1}_L$ emphasizes that it is a column vector of ones with dimension $L$. 

%  the one-layer transformer (softmax attention) Eq. (\ref{eq:transformer-softmax-attn}) approximates to 



% \begin{equation*}
% \begin{aligned}
%   F(E, \theta) & \,\approx\: \frac{1}{Z} W_{PV} X_{1:L} (\vec 1 + X_{1:L}^T W_{KQ} \tilde X) \\
%                & \,\approx\: W_{PV} \langle x \rangle + F_\textrm{Lin}(X - \langle x\rangle\vec 1, \theta)
%                & \,=\:       W_{PV} \langle x \rangle + F_\textrm{Lin}(E', \theta)
% \end{aligned}
% \end{equation*}
% \ams{Will have to fix notation here!}
% \matt{challenging with our $E$}
% where $\langle X \rangle = \frac{1}{L} X_{1:L} \vec 1$ is the mean token (averaged over the context) $F_\textrm{Lin}$ is the linear self-attention layer Eq. (\ref{eq:transformer-linear-attn}), and 
% , $E'=[X_1:L - \langle X \rangle, \tilde X]$ is an augmented context matrix.
\end{proof}

% Thus the one-layer softmax attention Eq. (\ref{eq:transformer-softmax-attn})  is reasonably approximated by $F_\textrm{Lin}(E, \theta)$ (with additional contributions from the context mean, which is centered about $0$) in the $v\rightarrow 0$ limit. 

\subsection*{For large argument}
%\( \beta \) (or large $W_{KQ}$)}
As \( \beta \to \infty \), the softmax function simply selects the maximum over its inputs (as long as the the maximum is unique):
\begin{equation*}
\text{softmax}(\beta v) \approx
\begin{cases}
  1 & \text{if } i = \arg\max_j v_j, \\
  0 & \text{otherwise}.
\end{cases}
\end{equation*}
In this case, all attention weight is given to a single element, and the others are effectively ignored.

\section{MSE Loss landscape for scaled identity weights}
\label{appendix:loss-landscapes}

\begin{figure}[h!]
\centering
\includegraphics[width=0.92\textwidth]{fig-appendix-landscape-MSE.pdf}
\caption{
  Loss landscape corresponding to case 2 and case 3 of Fig. \ref{fig:empirical-training-triple}. 
  The MSE is numerically evaluated by assuming scaled identity weights $W_{KQ}=\beta I_n$ (x-axis) and $W_{PV}=\alpha I_n$ (y-axis) and scanning over a $50 \times 50$ grid. The green point corresponds to the heuristic minimizer identified from the posterior mean. In case 2 it is exact, while in case 3 it is an approximation that neglects the residual term (see Proposition \ref{prop:bayes-case3}). The orange point corresponds to the learned weights displayed in Fig. \ref{fig:empirical-training-triple}(b), while the white point corresponds to the numerically identified minimum from this 2D scan. These can fluctuate due to the finite context ($L=500$) and sampling ($N=800$ here). In both panels, it is apparent that the trained weights and the heuristic estimator co-occur in a broad valley (contour) of the loss landscape. 
  }
\label{fig:loss-landscape-MSE-2d}
\end{figure}

%\section{Notes on associative memory networks}

%\matt{here I commented out some mostly historical/tengential baggage, would be nice to include and flesh out if we had more time}

% \subsection{Classical Hopfield networks}
% \label{appHN}

% %%%\begin{comment}
% Hopfield networks are classically used to store a set of patterns as minima of a quadratic energy function, and retrieve said patterns through an energy-decreasing update rule for the recurrently connected units. Accordingly, the basins of attraction of each stored pattern provide a means to decorrupt/denoise input vectors (queries). 

% Consider a set of $N$ binary units $s_i \in \{-1, +1\}$ which interact in a pairwise manner through the associated Hamiltonian $\displaystyle H(\vs)=-\tfrac{1}{2}\vs^T \mJ \vs$. 
% The couplings $\mJ$ may be used to encode a set of $p<N$ \emph{pattern} (or \emph{memory}) vectors $\{ \boldsymbol{\xi}_{\mu} \}_{\mu=1}^p$ through a one-shot encoding rule $\mJ(\{ \boldsymbol{\xi}_{\mu} \} )$. Such rules can be concisely expressed in terms of the $N \times p$ pattern matrix $\boldsymbol{\xi} = [\boldsymbol{\xi}_1 \cdots \boldsymbol{\xi}_p]$. 
% For orthogonal or nearly orthogonal patterns, the Hebbian rule $J_\textrm{hebb}=\frac{1}{N} \boldsymbol{\xi} \boldsymbol{\xi}^T$ is commonly used \citep{Hopfield1982, Amit1989}.
% In the large $N$ limit with $p$ randomly sampled patterns, this model has a critical capacity $\alpha \equiv p/N$ -- which reflects the number of patterns that can be reliably retrieved for a given number of state variables $N$ -- of $\alpha_c \approx 0.14$ \citep{Amit1985}.
% If the patterns are not orthogonal but still linearly independent, other storage rules like the projection rule 
% $J_\textrm{proj}=\boldsymbol{\xi} (\boldsymbol{\xi}^T\boldsymbol{\xi})^{-1} \boldsymbol{\xi}^T$
% \citep{Personnaz1986} can be used to store up to $p=N$ patterns.

% Memory storage and retrieval in Hopfield networks has been studied extensively from a statistical mechanics perspective (see e.g. \citet{Amit1985, Kanter1987}). These and other works have established that the number of patterns that can be reliably stored in such models is fundamentally limited, scaling at best linearly with the number $N$ of recurrently connected units $s_i$ (i.e. features).

% \bluebf{mention: m(s)2 overlaps notation here for next section simplicity}

% \bluebf{mention: update rule / recurrent dynamics}

% \bluebf{mention: spurious patterns}

% \subsection{Dense associative memories (``Modern Hopfield networks")}
% \label{appDAM}

% Building on the classical Hopfield networks described above, there exist modern generalizations with improved capacity $\alpha \equiv p/N$. 
% These have been called dense associative memory networks \cite{krotov2016hopfield} (as well as ``Modern Hopfield networks"). 
% % In the case that the patterns $\{ \boldsymbol{\xi}_{\mu} \}$ are orthogonal, 
% The key is to replace the quadratic energy function Eq. (\ref{eq:1}) 
% -- which can be written as a sum of pattern overlaps $H(\mathbf{s})=-\sum_{\mu=1}^p (\boldsymbol{\xi}_{\mu}^T \mathbf{s})^2$ 
% -- with higher-order nonlinearities, 
% e.g. $H(\mathbf{s})=-\sum_{\mu=1}^p (\boldsymbol{\xi}_{\mu}^T \mathbf{s})^3$ 
% or 
% $H(\mathbf{s})=-\sum_{\mu=1}^p \exp{( \boldsymbol{\xi}_{\mu}^T \mathbf{s} )}$.
% This results in sharper similarity cutoffs for comparing query vectors $\mathbf{s}$ to patterns in the memory bank 
% $\{ \boldsymbol{\xi}_{\mu} \}$,
% thereby enhancing how the capacity scales with the dimensionality of the state space. 

\section{Additional comments on the mapping from attention to associative memory models}
\label{appendix:sec-mapping-attn-assocmem}

\subsection{Linear attention and traditional Hopfield model}
\label{appendix:linear-attention-trad-Hopfield}
We have considered a trained network with linear attention, relating the query $\tilde X$ and the estimate of the target $\hat X$, of the form 
\begin{equation}
\hat X=f(\tilde X):=\frac{\gamma}{L}\sum_{t=1}^LX_t\langle X_t,\tilde X\rangle
\label{linear-net}
\end{equation}
with $\gamma=\tfrac{1}{\sigma_0^2+\sigma_Z^2}$. 
%and of the form
% \begin{equation}
% \hat X=f(\tilde X):=\frac {\sum_{t=1}^LX_te^{\beta\langle X_t,\tilde X\rangle}}{\sum_{t=1}^Le^{\beta\langle X_t,\tilde X\rangle}}
% \label{softmax-net}
% \end{equation}
% for the two other cases (with $\beta=\tfrac{1}{\sigma_Z^2}$). We now demonstrate that these relations could be thought of as one-step gradient descent (with specific step sizes) of energy models related to DAM networks, also known as Hopfield models.
 
% Consider energy functions of the form
% \begin{equation}
%     \En(s) = \frac{\lambda}{2} \|s\|^{2} - \phi(s)
% \label{eq:Energy}
% \end{equation}
% where $s\in\R^n$. The differentiable function $\phi$ would be chosen later and would depend upon $\{X_t\}_{t=1}^L$, a dependence that is suppressed in the notation \alb{and~$\phi$ will be chosen later?}\ams{Incorporated in the text}. A one-step gradient descent update with step size $\gamma$ gives
% \begin{align}
%     s^{\text{new}} &=s^{\text{old}}-\gamma\nabla \En(s^{\text{old}})\nonumber\\
%     &=(1-\gamma\lambda)s^{\text{old}} +\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD}
% \end{align}
% Choosing  $\gamma\lambda=1$, we get
% \begin{equation}
%     s^{\text{new}} =\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD-fixed-step}
% \end{equation}

With 
\begin{equation}
    \En(X_{1:L},s) := \frac{1}{2\gamma} \|s\|^{2} - \frac{1}{2L}s^T(\sum_{t=1}^LX_tX_t^T)s
\label{eq:spherical-Hopfield-appendix}
\end{equation}
gradient descent iteration $s(t+1) =  s(t) - \gamma \;\nabla_s \En \bigl(X_{1:L}, s(t)\bigr)$ gives us $$s(t+1) =\frac{\gamma}{L}\sum_tX_t\langle X_t,s(t)\rangle$$ 
making the one-step iteration our denoising operation.

We will call this energy function the Naive Spherical Hopfield model for the following reason.
For random memory patterns $X_{1:L}$, and the query denoting Ising spins $s\in\{-1,1\}^n$, the so-called Hopfield energy is 
\begin{equation}
    \En_\text{Hopfield}(X_{1:L},s) := - \frac{1}{2L}s^T(\sum_{t=1}^LX_tX_t^T)s.
\label{eq:Ising-Hopfield}
\end{equation}
We could relax the Ising nature of the spins by letting $s\in\R^n$, with a constraint $||s||^2=n$. This is the spherical model \cite{fischer1993spin} since the spin vector $s$ lives on a sphere. If we minimize this energy the optimal $s$ would be aligned with the dominant eigenvector of the matrix  $\tfrac{1}{L}(\sum_{t=1}^LX_tX_t^T)$ \cite{fischer1993spin}, and the model will not have a retrieval phase (see \citet{bolle2003spherical} for a similar model that does). A soft-constrained variant can also be found in Section 3.3, Model C of  \citet{krotov2021large}.

We could reformulate the optimization problem of minimizing the Hopfield energy, subject to $||s||^2=R^2$, as
$$\argmin_{s\in\R^n} \Big[\max_\lambda \big\{ - \frac{1}{2L}s^T(\sum_{t=1}^LX_tX_t^T)s+\lambda (s^Ts-R^2)\big\}\Big].$$
The $s$-dependent part of the Lagrangian, with $\lambda$ replaced by $\tfrac{1}{2\gamma}$ gives us the energy function in Eq. \ref{eq:spherical-Hopfield-appendix} which we have called the Naive Spherical Hopfield model. 

\begin{equation}
    \En(X_{1:L},s) := \frac{1}{2\gamma} \|s\|^{2} - \frac{1}{2L}s^T(\sum_{t=1}^LX_tX_t^T)s=\frac{1}{2} s^T\Big[(\sigma_0^2+\sigma_Z^2) I_n-\frac{1}{L}(\sum_{t=1}^LX_tX_t^T)\Big ]s.
\label{eq:Naive-spherical-Hopfield-appendix}
\end{equation}


For $L$ much larger than $n$, $\tfrac{1}{L}\sum_{t=1}^LX_tX_t^T\approx \sigma_0^2 P$, so its eigenvalues are either 0 or are very close to $\sigma_0^2$. Hence, for large $L$ and $\sigma_Z>0$, this quadratic function is very likely to be positive definite.  One-step gradient descent brings $s$ down to the $d$-dimensional linear subspace $S$ spanned by the patterns, but repeated gradient descent steps would take $s$ towards zero.


\subsection{Remarks on the softmax attention case (mapping to dense associative memory networks)}

%\matt{TODO appendix: there is a symmetry condition on the weights that is required for the dynamics to be conservative. Anirvan you may have written such a constraint on a whiteboard... can you check if this matches and is not gibberish?}
%As noted in \cite{ramsauer2021iclr}, the mapping discussed in the main text can be generalized further. 

Regarding the mapping discussed in the main text, we note that there is a symmetry condition on the weights $W_{KQ}, W_{PV}$ that is necessary for the softmax update to be interpreted as a gradient descent (i.e. a conservative flow). 
In general, a flow $ds/dt = f(s)$ is conservative if it can be written as the gradient of a potential, i.e. 
$f(s)=\nabla_s V(s)$ for some $V$. For this to hold, the Jacobian of the dynamics $J_f(s)=\nabla_s f$ must be symmetric. 

Let $z(s)=X^T W_{KQ} \,s$ and $g(s) = \softmax(z(s))$. 
Then the Jacobian of the softmax layer presented in the main text is
\begin{equation}
  J(s) = 
  W_{PV} X \frac{\partial g} {\partial s} = W_{PV} X \left( \text{diag}(g) - g g^T \right) X^T W_{KQ}.
\end{equation}

Observe that $Y = X \left( \text{diag}(g) - g g^T \right) X^T$ is symmetric (keeping in mind that $g(s)$ depends on $W_{KQ}$). The Jacobian symmetry requirement $J=J^T$ therefore places a type of adjoint constraint on feasible $W_{KQ}, W_{PV}$:
\begin{equation}
  W_{PV} \,Y \,W_{KQ}^T = W_{KQ} \,Y \, W_{PV}^T.
\end{equation}

It is clear that this condition holds for the scaled identity attention weights discussed in the main text. Potentially, it could allow for more general weights that might arise from non-isotropic denoising tasks to be cast as gradient descent updates.

%\matt{Not sure if we make note on residual terms here or in main text/discussion:} 
The mapping discussed in the main text involves discrete gradient descent steps, Eq. (\ref{eq:DAM-GD-update}). In general, this update rule retains a ``residual" term in $s(t)$ if we choose a different descent step size $\gamma \neq \alpha$. Thus, taking $K$ recurrent updates could be viewed as the depthwise propagation of query updates through a $K$-layer architecture if one were to use tied weights. Analogous residual streams are commonly utilized in more elaborate transformer architectures to help propagate information to downstream attention heads.



%We refer the reader to the Appendix of \cite{ramsauer2021iclr} for many detailed properties pertaining to the softmax update rule. 

% \subsection{Remarks on the linear attention case - connection to ``Naive Spherical Hopfield model"}

% \matt{TODO: as a bridge to the ``linear" sections below, consider Taylor expansion of the softmax update / LSE energy function (look at Jacobian as in Ramsauer2021 appendix)}

% Consider the problem of finding the ground state of a `spherical' Hopfield model. Let $$E_0(s)=-\frac{1}{2}s^T J s=-\frac{1}{2}\sum_{ab} s_a J_{ab} s_b,$$ with $$J=\sum_ix_ix_i^T$$ for some prescribed set of vectors $\{x_i\} \in \mathbb{R}^n$. In components, we have the more familiar $J_{ab} = \frac{1}{L}\sum_{i=1}^L x_{ia} x_{ib}$. We want to find $$\argmin_{s} E_0(s)$$ subject to the constraint $$s^Ts=nv^2,$$ $v$ being a constant. In the associative memory literature, the $\{x_i\}$ are seen as patterns or memories that have been encoded in the couplings $J_{ab}$. 
% The model is ``spherical" in the sense that the norm of $s$ is fixed, so $s$ lives on a sphere.

% The solution to the problem is obvious: $s$ has to belong to the eigenvector subspace corresponding to the largest eigenvalue of $J$.

% We can reformulate this constrained optimization problem using a Lagrange multiplier: find
%             $$\argmin_{s} \Big[\max_\lambda \big\{E_0(s)+\frac{\lambda}{2} (s^Ts-nv^2)\big\}\Big].$$
% One can show that the optimal setting of the Lagrange multiplier $\lambda$ is the largest eignevalue of $J$.

% For a fixed $\lambda$, define $E_\lambda(s):=E_0(s)+\frac{\lambda}{2} s^Ts.$ This is the $s$ dependent part of the Lagrangian, whose $s$-derivative has to vanish at the optimal point. Note that the gradient $\nabla_s E_\lambda(s) = - Js + \lambda s$ is zero at $s^*=(1/\lambda)Js^*$, confirming that $s^*$ is indeed an eigenvector of $J$.

% Given an initial vector $s(0) \in \mathbb{R}^n$, gradient descent $s(t+1) - s(t) = -\gamma \nabla E_\lambda(s(t))$ can be written as 
%   \begin{equation*}
%      s(t+1) = s(t) +  \gamma (J - \lambda I) s(t)=(1-\gamma \lambda)s(t) +  \gamma J s(t).
%   \end{equation*} 

% In particular, look at the setting $\gamma=1/\lambda$ where $\lambda$ is the largest eigenvalue of $J$.

% Then, we have the iteration
% \begin{equation*}
%      s(t+1) = \frac{1} {\lambda} J s(t),
%   \end{equation*} 
% which, when repeated, makes $s(t)$ converge to the eigen-subspace corresponding the largest eigenvalue of $J$ (provided $\lambda$ is the largest eigenvalue of $J$).

% To connect to the linear self-attention in Section \ref{sec:results}, we interpret the $x_i$ vectors as context tokens $i=1, \ldots, L-1$, whereas the $s$ vector is related to both the corrupted query vector $\tilde x_L$ and its denoised correction $\hat x_L$.
% Given a corrupted query token $s_\textrm{corrupted}$, interpret one step of gradient descent as

% \begin{equation}
% \begin{aligned}
% s_\textrm{corrected} & = s_\textrm{corrupted} + \gamma (J - \lambda I) s_\textrm{corrupted} \\
%                      & = s_\textrm{corrupted} - \gamma \lambda s_\textrm{corrupted} + \gamma J s_\textrm{corrupted} \\
%                      & = \frac{1}{\lambda} J s_\textrm{corrupted} \\
%                      & = \frac{1}{\lambda L} \sum_i \langle x_i, s_\textrm{corrupted} \rangle x_i  
% \end{aligned}
% \end{equation}

% where the third line uses a step size $\gamma=1/\lambda$, resulting in an update rule that closely resembles the linear attention formula Eq. (...). 

% \begin{itemize}

%    \item \matt{Note: \citep{krotov2021large} refers to something a bit different as ``spherical memory" in Sec. 3.3, Model C. Also, the constrain is also not `hard' here, the vectors aren't forced to lie on sphere; I guess if we wanted to encourage a radius-1 spherical constraint, the second energy term could instead be like $+\lambda (\sum_a s_a^2 - 1)^2$.}
   
% \end{itemize}



% \textbf{[OLD] Connecting ``Spherical Hopfield model" to linear self-attention}

% \begin{itemize}
%   \item Consider the spherical model: $E(s)=-\frac{1}{2}\sum_{ab} s_a J_{ab} s_b + \frac{\lambda}{2} \sum_a s_a^2$, with $J_{ab} = \frac{1}{L}\sum_{i=1}^L x_{ia} x_{ia}$ for some prescribed set of vectors $\{x_i\}$ in $\mathbb{R}^n$. In the associative memory literature, the $\{x_i\}$ are seen as patterns or memories that have been encoded in the couplings $J_{ab}$. 
%   The model is ``spherical" in the sense that the norm of $s$ is restrained.
  
%   \item Note that the gradient $\nabla_s E_\lambda(s) = - Js + \lambda s$ is zero at $s^*=(1/\lambda)Js^*$. Given an initial vector $s(0) \in \mathbb{R}^n$, gradient descent $s_{t+1} - s_t = -\gamma \nabla E_\lambda(s_t)$ can be written as 
%   \begin{equation*}
%      s(t+1) = s(t) +  \gamma (J - \lambda I) s(t).
%   \end{equation*} 
   
% \end{itemize}

% This analogy mirrors that of \citet{ramsauer2021iclr}, which connects softmax self-attention map and a dense associative memory network with an exponential kernel. 

%%%\bluebf{Connection to self-attention, maybe connect classic to performer REF}.

% ===========================================================================

% \subsection{Section 4 layout option 2}
% We have considered trained networks, relating the query $\tilde X$ and the estimate of the target $\hat X$, of the form 
% \begin{equation}
% \hat X=f(\tilde X):=\gamma\sum_{t=1}^LX_t\langle X_t,\tilde X\rangle
% \label{linear-net}
% \end{equation}
% for the first case (with $\gamma=\tfrac{1}{(\sigma_0^2+\sigma_Z^2)L}$) and of the form
% \begin{equation}
% \hat X=f(\tilde X):=\frac {\sum_{t=1}^LX_te^{\beta\langle X_t,\tilde X\rangle}}{\sum_{t=1}^Le^{\beta\langle X_t,\tilde X\rangle}}
% \label{softmax-net}
% \end{equation}
% for the two other cases (with $\beta=\tfrac{1}{\sigma_Z^2}$). We now demonstrate that these relations could be thought of as one-step gradient descent (with specific step sizes) of energy models related to DAM networks, also known as Hopfield models.
 
% Consider energy functions of the form
% \begin{equation}
%     \En(s) = \frac{\lambda}{2} \|s\|^{2} - \phi(s)
% \label{eq:Energy}
% \end{equation}
% where $s\in\R^n$. The differentiable function $\phi$ would be chosen later and would depend upon $\{X_t\}_{t=1}^L$, a dependence that is suppressed in the notation \alb{and~$\phi$ will be chosen later?}\ams{Incorporated in the text}. A one-step gradient descent update with step size $\gamma$ gives
% \begin{align}
%     s^{\text{new}} &=s^{\text{old}}-\gamma\nabla \En(s^{\text{old}})\nonumber\\
%     &=(1-\gamma\lambda)s^{\text{old}} +\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD}
% \end{align}
% Choosing  $\gamma\lambda=1$, we get
% \begin{equation}
%     s^{\text{new}} =\gamma\nabla\phi(s^{\text{old}})
% \label{eq:GD-fixed-step}
% \end{equation}

% Now note that choosing $\phi(s):=\tfrac{1}{2}\sum_{t=1}^L(\langle X_t,s\rangle)^2$ or 
% \begin{equation}
%     \En(s) := \frac{1}{2\gamma} \|s\|^{2} - \frac{1}{2}s^T(\sum_{t=1}^LX_tX_t^T)s
% \label{eq:spherical-Hopfield}
% \end{equation}
% gives us $s^{\text{new}} =\gamma\sum_tX_t\langle X_t,s^{\text{old}}\rangle$ which is Eq. (\ref{linear-net}), once we identify the query $\tilde X$ to be $s^{\text{old}}$ and the predicted target $\hat X$ to be $s^{\text{new}}$. We will call this energy function the Naive Spherical Hopfield model which does not have a retrieval phase \cite{fischer1993spin} (see \citet{bolle2003spherical} for similar model that does) \alb{refs?}\ams{Refs added!}. Also, See Appendix.

% On the other hand, choosing $\phi(s):=\tfrac{1}{\beta}\sum_{t=1}^Le^{\beta \langle X_t,s\rangle}$ or
% \begin{equation}
%     \En(s) := \frac{1}{2} \|s\|^{2} - {\beta}\sum_{t=1}^Le^{\beta \langle X_t,s\rangle}
% \label{eq:LSE-Hopfield}
% \end{equation}
% with the step size $\gamma=1$ gives us Eq. (\ref{softmax-net}), the same identifications for the query and the target as before. The energy in Eq. (\ref{eq:LSE-Hopfield}) is the log-sum-exp function-based energy defined by \citet{ramsauer2021iclr}, starting with work of \citet{demircigil2017}.

% \section{Scrap to address or move/incorporate elsewhere}

% \noindent \textbf{Misc.}
% Computing gradients of the cost with respect to the weights $\theta = (W_{KQ}$, $W_{PV})$ in the linear case $\hat x_L = f_\textrm{LSA}(X)$. 


% \bluebf{Check/show that same minima are found with gradient flow from particular or random initial conditions...} 
% $d \theta / dt = - \nabla l(\theta)$ given an initialization $\theta_0$ (or distribution of $\theta_0$) as in \citet{bartlett2024jmlr}. 

% The cost can be written as $C(W_{KQ}, W_{PV}) = \sum_\textrm{data} v^T v$ where $v = x_L - f_\textrm{LSA}(X)$. Carrying out the derivatives for gradient flow $\dot W_{\alpha} = - \nabla_{W_{\alpha}} C(\theta)$, we have for a dataset of $M$ examples $(X^{(i)}, x_L^{(i)})$:

% \begin{itemize}
%   \item  $\dot W_{KQ} = -2\frac{1}{M}\sum_\textrm{data} \tilde x_L \tilde x_L^T W_{KQ}^T W_{PV} \beta^2 (XX^T)^2 W_{PV}^T - \tilde x_L x_L^T W_{PV} \beta X X^T$.
%   \item  $\dot W_{PV} = -2 \frac{1}{M}\sum_\textrm{data} \beta X X^T W_{KQ} \tilde x_L (- v^T)$
% \end{itemize}

% If we select a subspace $S$ (i.e. freeze $X$) and omit the contribution of the final corrupted token to the sum $XX^T$, then the above reduces to a sum over the random corruption $\eta$:
% \begin{itemize}
%   \item  $\dot W_{KQ} = -2 \E_{\eta} [\tilde x_L \tilde x_L^T] W_{KQ}^T W_{PV} \beta^2 (XX^T)^2 W_{PV}^T 
%                         -2 \E_{\eta} [\tilde x_L x_L^T ] W_{PV} \beta X X^T = ...$  
%   \item  $\dot W_{PV} = -2 \beta X X^T W_{KQ} \E_{\eta} [\tilde x_L (- v^T)]$
% \end{itemize}

%\noindent \textbf{Misc. Remarks}
%\begin{itemize}
%
%  \item For $f_\textrm{LSA}$ and finite context $L$, the projection operator will be inexact, so we expect our estimate to not lie exactly on $S$ -- what happens if we iterate the denoising procedure? 
%  Relatedly, what happens if $\tilde x_L$ already lies on $S$ (i.e. zero noise) -- no issues?
%\end{itemize}

%\noindent \textbf{Links (blogs/videos)}
%\begin{itemize}
%  \item 
%  \href{https://ml-jku.github.io/hopfield-layers}{2021, blog post} for ``HN is all you need"
%  \item 
%  \href{https://ml-jku.github.io/blog-post-performer/}{2020, blog post A - attention x hopfield}
%  \item 
%  \href{https://rylanschaeffer.github.io/blog_posts/2022-09-08-Universal-Hopfield-Networks.html}{2022, blog post R. Schaeffer}
%  \item 
%  \href{https://www.beren.io/2024-03-03-Linear-Attention-as-Iterated-Hopfield-Networks/}{2024, blog post B - attention x hopfield}
%  \item 
%  talk: \href{https://youtu.be/pC4zRb_5noQ?t=446}{youtube 2022 Anthropic} (has ICL q\&A, versus: ``conditioning on the prompt")
%  \item 
%  talk: \href{https://www.youtube.com/watch?v=DiJsg93zQDc}{youtube 2023} from \cite{garg2022neurips} last author (Gregory Valiant). Mentions the curriculum training. 
%\end{itemize}
  
\end{document}
