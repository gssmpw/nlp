% auto-ignore
\section{Introduction} \label{sec:intro}

%这篇论文的关键：需要找到一个VFL在图片分类的一个场景是什么？
%现在的描述不具有说服力。

%纵向联邦学习大背景
%提出在纵向联邦学习需要持续学习的概念。
%解决的关键科学问题：实现
%本文所解决的是特征增强和类增强的纵向持续学习。
%特征增强：意味着增加被动方数量，如果在现有训练的模型的基础上，实现持续学习，增强模型的性能，能够结合已经训练好的模型，而不需要重新训练。
%对应的是第一个时间段参与训练的是客户端1，2,3---model1
%然而第二个时间段参与训练的是客户端4,5----model2
%如何在第二个时间段结束后模型拥有客户端1,2,3,4,5知识

%第一段指明纵向联邦学习的优势是什么？最后一句点明当前对于动态任务的需求
% 近年来，纵向联邦学习（Vertical Federated Learning, VFL）因其能够在保持数据隐私的前提下跨机构协作训练模型而备受关注。在VFL框架下，多个数据持有者可以利用不同特征空间的数据进行联合学习，而无需共享原始数据，这在医疗、金融及安全等领域尤为重要。通过这种方式，VFL在分类任务中展示出显著优势，例如在图像分类任务中，不同数据源提供的互补属性可以提升识别的精度和鲁棒性。然而，现实世界的数据是动态变化的，模型也需要适应不断变化的任务，这对VFL模型提出了持续学习的需求。因此，为应对这种动态任务，VFL需要具备持续学习的能力。

% However, when a new task arises, clients are not able to access data from previous (previous) tasks due to privacy concerns and can only update their local model with data from the new task. This often leads to a significant decrease in performance on previous tasks, which is known as catastrophic forgetting

%在一些常见中，样本特征是随着时间动态增加的，我们称为纵向联邦特征持续学习(VFFCL)
%Federated Feature-Continual Learning(FFCL)

%解决的问题有两个
%1.先前特征灾难性遗忘
%2.先前类的灾难性遗忘
%纵向联邦学习是一种分布式机器学习范式，目标是使用分布式共享样本特征协同训练一个全局模型。
%纵向联邦学习场景，定义

%第一段指出持续学习的相关研究。
% VFL作为联邦学习的一个具体场景，在跨领域场景中有着广泛的应用。例如，如图 1 所示，购物中心可以与视频平台和银行合作，训练一个全局模型来预测共享用户的购物意图。通常，VFL 的参与者包括一个主动客户端和多个被动客户端。每个客户端使用本地模型将原始样本特征转换为特征嵌入。然后将它们发送到活动客户端并通过全局模型进行聚合以进行预测。此外，还采用了值得信赖的协调员来确保安全通信和样本对齐[8,21,62,104,118,125,132,133]。此设置有助于跨域协作，且不会损害数据隐私。最近，VFL 得到了广泛的探索，并在各个领域显示出有希望的结果，包括金融 [31, 117, 205]、推荐系统 [12, 105, 166, 166, 191, 193] 和医疗保健 [68, 69, 143, 148、177]等[53、63、64、95、108、110、129、156、201]。
%第一段引出纵向联邦学习的动态持续学习的重要性。
%第二段持续学习为解决上述问题提供了技术路线，当前的研究主要集中实现联邦持续学习解决类增强和数据异构问题。采用的方法有三种。VFCL纵向联邦持续学习的重要性。
% {
% \color{red} Vertical Federated Learning (VFL) \cite{liu2024vertical,huang2023vertical} is a distributed machine-learning paradigm with extensive application in cross-domain scenarios. \cite{wen2023survey}.
% Vertical Federated Learning (VFL) \cite{liu2024vertical,huang2023vertical} is a distributed machine-learning framework that is often applied in domains with feature-federation distributions \cite{wen2023survey}. 

% Vertical Federated Learning (VFL) \cite{liu2024vertical,huang2023vertical} is a distributed machine-learning paradigm that is widely applied in various cross-domain scenarios, enabling collaborative model training while ensuring data privacy across multiple institutions \cite{wen2023survey}.
% For example, VFL leverages various complementary features from multiple clients to enhance recognition precision \cite{hu2023federated}.
% } 
% 通常，VFL的参与者包括一个主动方和多个被动方。每个被动方使用底层模型将部分样本特征转化为embedding。然后将它们发送给主动方并通过全局模型进行聚合来预测。
% VFL \cite{castiglia2022compressed,wang2023bdvfl} typically involves one active party and multiple passive parties.
% Each passive party employs a bottom model to generate embeddings from selected sample features. 
% These embeddings are then sent to the active party and aggregated using a global model to facilitate prediction.
%现有纵向联邦学习方法的假设性较强，通常假设所有客户端拥有样本包括类和特征应该是提前知道的并且永远不可更改。
%在现实世界的应用数据具有动态性的性能，各个客户端的训练数据收集随着时间增加而不断增加，也就是完整数据是随着持续任务到来的。
% 例如，在金融领域，随着时间（也称为任务）的增加，参与信贷风控评估的银行和互联网拥有的训练数据量均在不断的增多。
% 然而，现实世界的数据是动态变化的，模型也需要适应不断变化的任务，这对VFL模型提出了持续学习的需求。因此，为应对这种动态任务，VFL需要具备持续学习的能力。
%然而，当模型训练新任务时，传统的VFL将遭受灾难性遗忘。具体这里的灾难性遗忘包括类灾难性遗忘和特征灾难性遗忘，如图1所示。
% Existing VFL methods typically make strong assumptions, generally presuming that all clients possess sample categories and features that are known in advance and remain unchanged at the beginning of training.     

%VFL的一句话介绍，例子是非常经典的，有需求，要有参考文献，白皮书或国际报告，公司有科研方向。不同的银行不能共享原始数据，疾病分类，联合所有医院的数据。疾病多个。可分享的数据需求。具体化。
% VFL不一样的特点是什么，价值是什么？
% 在VFL有类增量和特征增量的情况。一句话说清楚的例子，后来再定义特征和类增强学习。两类需求说明，不可见的训练。需求要非常确定，通过例子说明VFCL。
%如何进行特征和类增强同时有需求，同时解决这两种持续学习是关键的。


%第一段要点名问题需求是重要的

% 纵向联邦学习是一种特征联合的隐私保护机器学习范式，确保客户端的数据不出域。例如，在信用卡营销领域，纵向联邦学习通过协同用户的互联网平台(称为被动方)的消费行为特征和银行（也称为主动方）的金融属性特征训练信用卡营销模型，提高其性能。
%在实际的信用卡营销中，随着时间（也称为任务）的增长，银行和互联网平台对齐的用户数据会不断增多，也有可能会出现新的信用卡类型。例如，从图1所示，Task1任务中训练样本只有信用卡类别为“Type1和Type2”的用户数据，随着时间的增长，Task2任务中的训练样本增加了新的信用卡类别为“Type3”的用户数据。那么，Task2任务训练结束后，信用卡营销模型应该具备预测“Type1、type2和type3”的能力（类持续学习）。
%此外，经过数个月，互联网平台可以统计出每个用户的月消费次数，采用新增加的消费次数特征X5优化Task2得到的信用卡营销模型，提升信用卡营销模型的性能是必要的（特征持续学习）如图1中的Task3。
% 因此，实现具有类持续学习和特征持续学习的纵向联邦学习是当前应用场景所必需的。


% 在xin信用卡营销是银行最前端的金融业务，传统的单边营销的方式会导致互联网平台缺乏信用卡客户的金融属性特征，银行信用卡缺乏客户在具体消费场景中的行为数据。互联网平台无法准确筛取银行想要营销的对象，银行也无法根据用户消费行为有针对性的进行交叉营销。
% 
%在实际的信用卡营销中，随着时间（也称为任务）的增长，银行和互联网平台对齐的用户数据会不断增多，也有可能会出现新的信用卡类型。例如，从图1所示，Task1任务中训练样本只有信用卡类别为“Type1和Type2”的用户数据，随着时间的增长，Task2任务中的训练样本增加了新的信用卡类别为“Type3”的用户数据。那么，Task2任务训练结束后，信用卡营销模型应该具备预测“Type1、type2和type3”的能力（类持续学习）。
%此外，经过数个月，互联网平台可以统计出每个用户的月消费次数，采用新增加的消费特征优化信用卡营销模型，提升信用卡营销模型的性能是必要的（特征持续学习）。
% 因此，实现具有类持续学习和特征持续学习的纵向联邦学习是当前应用场景所必需的。
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/Scheme/intro8.pdf}
    \caption{Illustration of the classes and features incremental learning in VFL. Task2 adds a new class ``Type III" for credit card marketing, and Task3 adds a new feature ``X5". The model performance in Task1, Task2, and Task3 increases in sequence. 
    % \yj{Zoom up the words.} solve
    %Task2增加了信用卡营销的新类Type III,Task3中增加了新特征X5。其中Task1，Task2和Task3任务中的模型性能是依次递增的。
    % In class incremental learning, training on task 2 leads to forgetting the classes "dog" and "horse" previously learned in task 1. Similarly, in feature-incremental learning, after advancing to task 2, the model forgets features $f1$ and $f2$ acquired during task 1. This illustrates the challenge of catastrophic forgetting where new learning can disrupt previously stored knowledge. \yj{The figure is not clear to claim the forgetting issues.}
    }
    \label{fig: intro}
\end{figure}
% 纵向联邦学习是一种隐私保护的机器学习范式，确保客户端数据不外泄。在信用卡营销中，它结合互联网平台（被动方）的消费特征与银行（主动方）的金融特征来训练营销模型，从而提升性能。
% 随着时间推移，银行与互联网平台对齐的用户数据不断增加，可能出现新的信用卡类型。例如，在Task1中，训练样本仅包含“Type1”和“Type2”用户数据，而在Task2中新增了“Type3”用户数据。因此，完成Task2后，模型应能预测“Type1”、“Type2”和“Type3”的能力（类持续学习）。
% 此外，经过数月，互联网平台可以统计每个用户的月消费次数，并利用新增的消费特征X5来优化Task2得到的信用卡营销模型（特征持续学习），如图1中的Task3所示。因此，实现具备类持续学习和特征持续学习的纵向联邦学习是当前应用场景的必要条件。

%介绍一下纵向联邦学习的具体是什么？
%介绍一个具体的特征，数据的维度，任务类别也在变化。产生了类别和特征持续变化的问题，拿一个场景举例子。图表里表示出来abc。拿这个例子来说明是什么。数据出来。只要讲清楚，增加了一个什么类，增加什么的特征。任务的必要性。解决类和特征的难的点是什么？目标是什么？核心的技术问题是什么？同时保证。。。不遗忘的问题。


\textit{Vertical Federated Learning} (VFL) is a type of \textit{Federated Learning} (FL) that provides multi-party collaborative computing in which datasets of different parties have overlapping samples without overlapping feature spaces \cite{liu2024vertical,castiglia2022compressed,wang2023bdvfl}, being explored in various privacy-sensitive application scenarios, e.g., financial services \cite{liu2023vertical} and healthcare \cite{sakib2024explainable}. 
%在现实应用场景中，用户拥有的本地数据随着时间不断的增多，纵向联邦学习应该支持类持续学习和特征持续学习来增强训练模型的性能. 
In real-world applications, as users' local data increase, achieving \textit{Vertical Federated Continuous Learning} (VFCL) is an expected development direction, which are generally facilitated by the integration of \textit{Class Incremental Learning} (CIL) and \textit{Feature Incremental Learning} (FIL).
%VFL is expected to facilitate both \textit{Class Incremental Learning} (CIL) and \textit{Feature Incremental Learning} (FIL), to enhance model performance.
As shown in Figure \ref{fig: intro}(a)(b), take the credit card scenario for example \cite{author2024},
%VFL has been widely employed in various privacy-sensitive applications such as financial services, smart healthcare, and recommender systems.
%\textit{Vertical Federated Learning} (VFL) \cite{liu2024vertical,castiglia2022compressed,wang2023bdvfl} is a privacy-preserving machine learning paradigm that ensures the confidentiality of client data.
% 
% \yj{(1)Add an sentence to explain the ample-aligned feature fed-3
% eration mechanism of VLP, and its important applications. (2) Briefly introduce the issues of class and feature increase. Later, introducing the example to explain the issues. Take the credit card marketing scenario for example. As shown in Figure 1,...} 
% in the context of credit cards \cite{liu2023vertical}, for example, VFL can be used for integrating the consumption features from the internet platform (the passive party without labels) with financial features from the bank (the active party with labels) to train a marketing model, thereby achieving a collaborative computing without disclosing raw data from all participants \cite{author2024}.
% Due to different service offerings, the user data associated with the credit card will also be varied. 
% For example, 
%in Task 1, 
the training sample only contains user data for ``Type I" and ``Type II" credit cards in Task 1, while Task 2 further includes user data for ``Type III" credit card. 
Therefore, after completing Task 2, the model should be able to predict all three types, Type I-III.
This presents a case of 
% \textit{Class Incremental Learning} 
CIL, i.e., the ability of the model to learn new classes over time \cite{lebichot2024assessment,ma2022continual,casado2023ensemble}.
Another case is that the Internet platform can utilize the new consumption feature ``X5" deriving from tracking the monthly consumption frequency of each user, so that the credit card marketing model derived from Task 2 can be optimized. 
This presents a case of
% \textit{Features Incremental Learning} 
FIL that involves incorporating new features to enhance model performance~\cite{ni2024feature}, refer to Task 3 in Figure \ref{fig: intro}(c). 
%Therefore, it is observable that implementing \textit{Vertical Federated Continuous Learning} (VFCL) can broaden the scope of FL applications due to its integration of both CIL and FIL. 

% hu2018novel,

%In the context of credit card marketing \cite{liu2023vertical}, it integrates the consumption features from the internet platform (the passive party without labels) with the financial features from the bank (the active party with labels) to train a marketing model, thereby enhancing its performance \cite{author2024}.
%As time progresses, the user data aligned between banks and internet platforms continues to grow, and new types of credit cards may emerge. 
%For example, in Task 1, the training samples only include user data for ``Type I" and ``Type II" credit cards, while Task 2 introduces user data for the new ``Type III" credit card. 
%Consequently, after completing Task 2, the model should be able to predict all three types: ``Type I," ``Type II," and ``Type III" (class incremental learning, which refers to the model's ability to learn from new classes over time) \cite{lebichot2024assessment,ma2022continual,casado2023ensemble}.
% Furthermore, after several months, the internet platform can track the monthly consumption frequency of each user and employ the newly introduced consumption feature ``X5" to optimize the credit card marketing model derived from Task 2 (features incremental learning \cite{hu2018novel,ni2024feature}, which involves incorporating new features to enhance model performance), as illustrated in Task 3 of Fig. \ref{fig: intro}. 
% Therefore, it is essential to implement \textit{Vertical Federated Continuous Learning} (VFCL), encompassing both class incremental learning and feature incremental learning, which is the real-world application scenario.
% Therefore, it is essential to implement \textit{Vertical Federated Continuous Learning} (VFCL), which integrates both class-incremental and feature-incremental learning to broaden its scope of applications.










%为了解决这个问题，现有的方法主要集中是。。。横向联邦场景下的类持续学习。
%一两句解决现有的方法。一句话讲出来为什么不能应用到纵向联邦学习中，每一类都不能迁移的原理是什么？现有的VFL的持续方法不能直接迁移。存在的挑战是什么？基于现有的方法存在的挑战，需要解决的挑战。为什么不能做迁移

%特征增强的是什么？是否有特征增强的现有研究。其他的方向。找到类似的别的方法，不适用于我们的场景。
%特征持续学习存在的挑战。
%同时解决类持续学习和特征持续的挑战。从哪几个方面不能解决，难度是比较

%特征增强的难点在哪里，需要具有什么样的能力，能力和关系是什么？类别和特征持续学习所需要的。为什么需要原始数据，现有的方式为什么做不到。概念第一次出现需要解释一下，什么是被动方模型，底层模型。
%底层模型和顶层模型都需要。被动存在的挑战，前面的持续要讲清楚（第一段的时候需要讲清楚）。后面的挑战是从哪个维度来讲的，对于主动方有哪个问题，对于被动方有哪些问题。哪个维度的挑战，分维度来说明。

%实现纵向联邦学习的类增强学习的关键是解决旧任务类灾难性遗忘问题。现有研究者探索了很多策略解决持续学习中的灾难性遗忘问题，包括。
%针对类持续学习现有的研究，以及存在的挑战
% A major challenge in implementing class-incremental learning within VFCL is to avoid class-catastrophic forgetting in previous tasks. 


However, existing VFCL supportive technologies still encounter a variety of technical challenges. 
To be specific, on one hand, most existing methods fail to address class \textit{Catastrophic Forgetting} (CF) of previous task knowledge in the local model of the passive party in VFL. 
Previous studies in CIL have mostly tried following strategies, including regularization, {\em Dynamic Network Expansion} (DNE), and replay. 
Among them, regularization methods adjust the learning algorithm to limit changes to key weights, preserving essential knowledge \cite{chen2021overcoming,yu2024overcoming}.
Both regularization and DNE methods adjust or expand parts of the model to mitigate catastrophic forgetting of previous classes \cite{luo2023gradma}.
However, the passive party only has a local model and lacks the full model (local and server models) to extract local data embeddings in the VFL scenario. 
In addition, the replay technique mitigate class catastrophic forgetting by reconstructing previous task datasets using class labels \cite{li2024towards}, but each passive party only has partial feature sets without class labels in VFL, making this technique ineffective for addressing class catastrophic forgetting.


% % Several approaches have been proposed to mitigate this challenge in \textit{Continual Learning} (CL), including
% Researchers have developed strategies to tackle class-incremental learning of the \textit{Continual Learning} (CL), including regularization, dynamic network expansion, and replay. 
% \textit{Regularization methods} \cite{chen2021overcoming,yu2024overcoming} adjust the learning algorithm to limit changes to crucial weights, preserving essential knowledge. 
% %正则化方法和Dynamic network expansion两种方法分别是固定更新或增加完整模型的部分参数来防止遗忘先前类知识。
% %然而，在纵向联邦学习中，被动方只拥有本地模型而没有完整模型(指的是本地模型和服务器模型)用于提取本地数据的embedding，无法计算本地模型的重要性参数。因此，正则化方法和动态网络扩展的方法无法解决被动方的本地模型对于旧任务知识的灾难性遗忘问题。
% Both \textit{regularization methods} \cite{chen2021overcoming,yu2024overcoming} and \textit{dynamic network expansion} \cite{luo2023gradma} methods adjust or expand parts of the model to mitigate catastrophic forgetting of previous classes.
% {\color{red}
% However, the passive party only has a local model and lacks the full model (local and server models) to extract local data embeddings in the VFL scenario. 
% Therefore, these methods cannot address class catastrophic forgetting of previous task knowledge in the passive party’s local model.
% }
%现有的方法解决不了什么问题，所以解决不了纵向联邦学习。
% 通过学习算法控制对完整模型中的部分重要参数更新来防止遗忘先前知识。
% 
% Replay techniques \cite{li2024towards} mitigate class catastrophic forgetting by reconstructing previous task datasets using class labels. 
% {\color{red}
% However, in VFL, each passive party only has partial feature sets without class labels, making replay techniques ineffective for addressing class catastrophic forgetting.
% % However, in VFL, each passive party only has partial feature sets without class labels, making replay techniques ineffective for addressing class catastrophic forgetting. 
% % Therefore, existing methods for class catastrophic forgetting in CL cannot be directly applied to VFCL.
% Therefore, existing methods developed to tackle class catastrophic forgetting in CL are not directly applicable to mitigating this issue in VFL.
%}
% \textit{Replay techniques} \cite{li2024towards} reintroduce previous data or create synthetic samples to reinforce previous tasks without affecting new tasks. 
%重放技术通过根据旧任务的类标签来增强或合成完整的旧任务数据集来解决类灾难性遗忘问题。
%然而，在纵向联邦学习中，各个被动方只拥有样本的部分特征而不拥有类标签，replay techniques无法解决纵向联邦学习中类灾难性问题。
%因此，现有适用于解决持续学习的类灾难性遗忘方法无法直接用于解决纵向联邦学习的类灾难性遗忘问题。
% \textit{Dynamic network expansion} \cite{luo2023gradma} allows networks to grow by adding new elements while keeping the core structure for existing tasks.
% However, these approaches are primarily designed for HFL, where clients possess similar features but distinct samples, allowing for techniques like data synchronization and replay. 
% The existing methods necessitate that participants possess complete models and independent data sets.
% However, in VFL, participants only control partial model parameters and subsets of training data features, which prevents them from independently conducting model training. 
% {\color{Why}}
% Therefore, these constraints make it unfeasible to directly apply these methods to address the significant challenges of feature and class catastrophic forgetting in VFL. 


% 实现VFCL是一件非容易的事需克服以下关键性挑战。
% 首先，由于数据隐私和有限的存储，参与方不能存储先前任务的原始数据，这就意味着我们仅仅从当前任务的数据来推断和增强先前任务的知识。然而，每个被动方仅仅拥有部分的
%其次，

%8410016: A Novel Feature Incremental Learning Method for Sensor-Based Activity Recognition
% 识别日常生活活动是健康监测和老年护理的重要研究课题。然而，大多数现有的活动识别模型仅适用于静态和预定义的传感器配置。使现有的活动识别模型能够适应动态环境中新传感器的出现是一项重大挑战。在本文中，我们提出了一种新颖的特征增量学习方法，即特征增量随机森林 (FIRF)，以使用少量新出现特征的数据来提高现有模型的性能。它由两个重要组件组成 1) 基于互信息的多样性生成策略 (MIDGS) 和 2) 特征增量树生长机制 (FITGM)。MIDGS 增强了随机森林的内部多样性，而 FITGM 提高了单个决策树的准确性。为了评估 FIRF 的性能，我们对三个众所周知的活动识别公共数据集进行了广泛的实验。实验结果表明，与其他最先进的方法相比，FIRF 的准确性和效率明显更高。它有可能允许在不断变化的环境中动态利用新传感器。

%ni2024feature: Feature incremental learning with causality
% 随着数据采集方式的不断发展，特征呈现增量式逐渐积累的趋势。由于特征空间的扩展，训练数据集和测试数据集之间出现未知偏差的情况变得更加普遍，这被称为“未知数据选择偏差”，属于非独立同分布（non-i.i.d.）样本学习场景。在这种情况下，传统方法（假设数据独立同分布）性能会严重下降。如何设计一种算法来解决特征增量场景中的数据选择偏差问题是非常关键但很少研究的课题。本文提出了一种基于因果关系的特征增量分类算法。首先，我们将因果学习中的混杂变量平衡算法嵌入到预测建模中，并利用带有平衡正则项的逻辑回归算法作为基准。然后，为了满足特征增量的特殊要求，我们设计了一个新的正则化项，该正则化项保持当前阶段和上一阶段之间回归系数的一致性，保持旧特征与标签之间的关联性。最后，我们提出了多重平衡逻辑回归模型（MBRLR），通过联合优化平衡正则化项和带权重的逻辑回归模型，来处理多重特征集。我们还提供了理论结果，证明我们提出的算法能够进行精确且稳定的预测。此外，数值实验结果也证明了我们提出的MBRLR算法优于其他方法。

%10227560: SAFC: Incremental Learning for Simultaneous Augmentation of Feature and Class
% 随着许多动态环境应用中新数据收集方式的出现，样本逐渐聚集在累积的特征空间中。随着新类型特征的加入，可能会导致类别数量的增加。例如，在活动识别中，在热身时使用旧特征，我们可以区分不同的热身练习。随着从新添加的传感器获得的新属性的积累，我们可以更好地区分新出现的正式练习。这种同时增强特征和类别的学习至关重要，但很少被研究，特别是当具有完整观察的标记样本有限时。在本文中，我们通过提出一种新的增量学习方法来解决这个问题，该方法以两阶段的方式同时增强特征和类别 (SAFC)。为了保证在先前数据上训练的模型的可重用性，我们在当前模型中添加了一个正则化器，它可以为训练新的分类器提供可靠的先验。我们还给出了关于泛化界限的理论分析，可以验证模型继承的效率。在解决了一次性问题之后，我们还将其扩展到多次。实验结果证明了我们方法的有效性及其在活动识别应用中的有效性。



%VFL面临的独特的特征持续学习所存在的挑战
%实现具有特征增强的VFCL的关键性挑战是避免旧任务特征的灾难性遗忘。
%现有研究
%已有相关研究提出了相关方法来解决特征增强学习。
%FIRF \cite{8410016} 采用增加新特征对应的树模型的相关结点来提供决策树的准确性。
%然而, FIRF仅适用于基于树模型的简单数据结构类型的特征增强，而不适用于基于神经网络模型VFL的复杂数据结构的特征增强。
% MBRLR \cite{ni2024feature} 和SAFC \cite{10227560} 采用在整个模型中添加一个正则化器避免旧特征的遗忘。
%然而，由于VFL的模型是被划分为本地模型和服务器模型，被动方根据服务器模型传输的反向梯度进行本地模型更新，当主动方采用正则化方法避免旧特征遗忘时，被动方只能收到当前任务的梯度值，而不能获得先前任务的梯度值，底层模型将丢失旧任务的知识，无法实现特征增强学习。


On the other hand, the local model loses feature knowledge from previous tasks, causing a feature incrementality issue. 
%developing an effective solution to addressing feature incrementality has an urgent.
Prior studies have explored FIL in the activity recognition tasks \cite{8410016,ni2024feature}. 
For example, FIRF \cite{8410016} is a typical method that incorporates nodes corresponding to newly introduced features to improve decision tree accuracy, but this method is limited to feature augmentation in simple data structures based on tree models and cannot be applied to complex data structures, e.g., neural network models, in VFL.
Some other work tried regularization across the entire model to prevent forgetting of previous features \cite{ni2024feature,10227560}. 
However, models in VFL generally is divided into local and server models.
Passive parties update local models in terms of reverse gradients received from the server, so that passive parties only receive gradients for the current task and are unable to access gradients from previous task when the active party applies regularization to prevent forgetting of previous features.
%Thus, the local model loses feature knowledge from previous tasks, implying a feature incrementality issue. 


% {\color{red}
% However, in VFL, where the model is partitioned into local and server models, passive parties update their local models based on reverse gradients received from the server. 
% When the active party applies regularization to prevent forgetting of previous features, passive parties only receive gradients for the current task and are unable to access gradients from previous tasks. 
% The local model loses feature knowledge from previous tasks.
% % Thus, it is essential to develop a novel approach in VFCL to effectively tackle feature incrementality.
% Thus, developing a novel approach in VFCL to effectively address feature incrementality is essential.
% }



% Existing research \cite{8410016,ni2024feature} has explored methods for feature incremental learning in the activity recognition task.
% FIRF \cite{8410016} improves decision tree accuracy by incorporating nodes corresponding to newly introduced features.
% However, FIRF is limited to feature augmentation in simple data structures based on tree models and cannot be applied to the more complex data structures found in VFL based on neural network models.
% MBLRR \cite{ni2024feature} and SAFC \cite{10227560} introduce regularization across the entire model to prevent forgetting of previous features.
% {\color{red}
% However, in VFL, where the model is partitioned into local and server models, passive parties update their local models based on reverse gradients received from the server. 
% When the active party applies regularization to prevent forgetting of previous features, passive parties only receive gradients for the current task and are unable to access gradients from previous tasks. 
% The local model loses feature knowledge from previous tasks.
% % Thus, it is essential to develop a novel approach in VFCL to effectively tackle feature incrementality.
% Thus, developing a novel approach in VFCL to effectively address feature incrementality is essential.
% }
% 在VFL中，主动方在训练时需要聚合所有被动方的本地embedding，当新特征任务到来时
% When the active party employs regularization or data replay to tackle catastrophic forgetting, passive parties only receive gradients for the current task, lacking access to gradients from previous tasks. 
% 只使用于简单的数据结构如表格化数据,不适用复杂的数据结构。
% 包括为了解决特征增强学习问题Several approaches have been proposed to mitigate this challenge

% To address the aforementioned challenges, we propose a new VFCL framework, named Vertical Federated Continual Learning via Prototype Knowledge Fusion (VFCL).



To address the aforementioned challenges, we propose a novel VFCL framework, named \underline{V}ertical Federated Continual \underline{L}earning via \underline{E}volving Pro\underline{t}otype Kn\underline{o}wledge (V-LETO), to achieve the enhanced learning for both local and server models, including class and feature augmentation. 
To addressing the loss of prior knowledge in the global model, we propose a evolving prototype knowledge method to transfer knowledge from previous tasks. Additionally, the forgetting of prior knowledge in the local model is mitigated by constraining its updates.
%我们构建了一个演化的原型知识实现旧任务知识的转移来解决全局模型对先前知识的遗忘问题。此外，我们通过限制被动方本地模型更新来解决本地模型对先前知识遗忘问题。
% We employ a regularization scheme in terms of prototype knowledge from previous tasks to update the server model, thereby mitigating the loss of knowledge from prior tasks. 
%To address the aforementioned challenges, we propose a novel VFCL framework, named Vertical Federated Continual Learning via Prototype Knowledge Fusion (VFCL).
%为了同时实现本地模型和服务器模型增强学习(类增强和特征增强)，我们采用基于旧任务的原型知识的正则化方法更新服务器模型，避免减少旧任务知识的遗忘。
%此外，我们通过固定对于旧任务重要的本地模型参数来减少旧任务知识的遗忘。
%提出了一种同时具备，随着时间推移的文档性。只将核心内容，怎么做到做不到的东西。
%三个模块，是如何构建的。为了构建一个具有。。。能力的Prototype，我们提出了一个。模块是怎么构建的
%为了同时支持类别和特征的遗忘。如何支持特征和类别的遗忘。利用原型同时训练顶层模型和底层模型。先将一个问题，再讲一下如何解决方法。。。。。具体是什么动能。
%To enable enhanced learning for both the local and server models (including class and feature augmentation), we employ a regularization method based on prototype knowledge from previous tasks to update the server model, thereby mitigating the loss of knowledge from prior tasks.
% Additionally, we reduce the forgetting of knowledge from previous tasks by freezing the local model parameters that are crucial for these tasks. \yj{Express in the way of key technique. `Freezing' seems weak.}
% Furthermore, we mitigate the forgetting of knowledge from previous tasks by restricting the updates to key local model parameters, which are critical for maintaining this knowledge.
% \yj{What's the origin of the model name?}. \yj{Summarize the basic mechanism of the proposed method before introducing details.}
%具体地，VFCL包括三个模块分别是特征原型聚合、类原型增强和模型优化。
Specifically, VFCL consists of three modules: 
% The \textit{Prototype Generation} 
% process enables the effective transfer of previous knowledge across tasks, enhancing overall model performance.
% is used to generate class prototypes and transfer previous task knowledge to prevent its forgetting. 
% \yj{Avoid using `is used to'.}
To address the issue of the passive party lacking complete features for prototype construction, we propose a \textit{prototype generation} module to preserve prior task knowledge. 
The server aggregates global embeddings, derived from the passive party's local embeddings and labels, to build class prototypes.
We also propose a \textit{prototype evolving module} to mitigate catastrophic forgetting of prior knowledge, which evolves prototypes that integrate both previous and current task knowledge.
Meanwhile, we propose a \textit{model optimization module} to optimize both global and local models.
%为了解决被动方仅拥有部分特征而无法构建原型的问题，我们提出了一种prototype generation module用于保存先前任务知识。该模块中服务器通过聚合被动方的本地localembedding得到的全局embedding和标签中构建每个类的原型。
%此外，我们提出了一种prototype evolving module构建旧任务知识来解决旧任务知识灾难性遗忘问题。该模型基于先前任务原型和当前任务原型演化具有先前任务知识和当前任务知识的原型。
%最后，我们提出了一种模型优化模块用于优化全局模型和本地模型。
% 为了构建具有先前任务知识的原型，我们提出了一个prototype gengeration module
%原型生成用于生成类原型，并传输旧任务知识来避免其遗忘。
% 保存旧任务知识
% (1)Feature Prototype Aggregation过程实现任务之间特征知识转移，提高训练模型的性能。
% (2)Class Prototype Augmenting实现任务之间类知识转移，减少灾难性遗忘。
% (3)Model Optimization采用新旧知识更新顶层模型和旧任务重要参数固定更新底层模型，减少灾难性遗忘，提供训练模型的性能。
% \textit{Prototype Augmentation} facilitates the retention of previous task knowledge across tasks, helping to mitigate catastrophic forgetting.
% The \textit{Model Optimization} strategy integrates new and prior knowledge into the top model while fixing critical parameters in the bottom model essential to previous tasks, realizing the class and feature incremental learning. 
% \yj{The core technique is prototype knowledge fusion. The introduction of each module should focus on how to tackle the forgetting issues.}
% Moreover, extensive and comprehensive experiments have shown that our method improves accuracy by 32.27\% compared to other baseline methods in the CIFAR 10 datasets and class incremental learning, highlighting its effectiveness. \yj{Experimental results are preferred in the contributions.}


% 针对以上问题，本文提出了一种新的纵向联邦持续学习（Vertical Federated Continual Learning, VFCL）框架，通过结合参数隔离和选择性知识保留的混合策略，有效解决灾难性遗忘问题。我们的方案采用模块化更新的方式，避免不同任务间的干扰，同时确保各方的数据隐私和跨机构的协作性不受影响。在此基础上，VFCL框架通过不影响已有知识的前提下实现增量更新，为联邦学习环境中的图像分类提供了一种更加灵活和抗遗忘的解决方案，从而满足了动态任务的需求。

This paper's contributions are as follows:
% 1) We propose a new VFCL paradigm, Prototype Knowledge Fusion-based Vertical Federated Continuous Learning, where previous tasks can be traced and optimized to address the repetitive task sequence in a VFL system. 
% To the best of our knowledge, this is the first attempt in the literature to study and explore VFCL.
% 据我们所知，我们第一次在纵向联邦学习中尝试同时实现了具有类增强学习和特征增强学习的纵向持续学习方法，扩展了VFL的应用场景。
% is used to generate class prototypes and transfer previous task knowledge to prevent its forgetting.
%我们提出了一种新颖的VFCL方法来解决纵向联邦学习鲜有研究但是被许多现实应用领域所需的问题。据我们所知，我们可能是第一尝试在纵向联邦学习中同时实现类增强学习和特征增强学习。
(1) We propose a novel VFCL method to address an underexplored issue in VFL, which is critical in many real-world applications. To the best of our knowledge, this may be the first to attempt the simultaneous implementation of CIL and FIL in the VFL.
% \yj{The framework/schema to simutanously solve the two forgetting issues.}
% 原型生成策略获取并保存先前任务知识，用于转移先前知识到当前任务中。原型增强策略确保了本地模型和服务器模型能够学习到先前任务知识。模型优化策略确保了服务器模型和本地模型避免先前任务知识的遗忘
%优势，泛化的能力，领域有推荐，方向领域有哪些贡献。相比其他持续学习的方法，通用的方法等。
% We design an ST-Net for cross-city traffic prediction under FL. To overcome spatial heterogeneity, we decouple the ST-Net and share space-independent traffic patterns with the server. Meanwhile, we propose an adaptive layer-wise model interpolation method to alleviate the effect of temporal heterogeneity.
% 我们设计了一种V-LETO用于实现VFL中的CIL和FIL。为了克服旧任务知识灾难性遗忘，我们提出了一种基于演化原型的旧任务知识和当前任务知识的全局模型优化方法。与此同时，我们提出了一种限制本地模型更新的方法来减少本地模型对旧任务知识的灾难性遗忘。
(2) We propose V-LETO as a framework for implementing CIL and FIL within VFL. To address catastrophic forgetting of prior task knowledge, we propose a global model optimization method based on evolving prototypes, which combines both prior and current task knowledge. Additionally, we propose a method that constrains local model updates, mitigating the catastrophic forgetting of previous task knowledge in the local model.
% The prototype generation strategy captures and preserves knowledge from previous tasks, facilitating its transfer to the current task. The prototype enhancement strategy ensures that both the local and server models effectively learn from the knowledge of previous tasks. 
% The model optimization strategy prevents the forgetting of knowledge in both the global and local models. \yj{summarize the key contribution first, like `evolving prototype-based ... method'.}
% In VFL, we defined and utilized task prototypes to transfer knowledge from previous tasks, enabling both class- and feature-level continual learning. Additionally, by preserving key parameters in the bottom model essential for previous tasks, we mitigate knowledge forgetting, thereby enhancing model performance. \yj{Prototype knowledge fusion strategy to guarantee ... knowledge of previous tasks, avoiding the requirement of data replay and ... .}
% 3) \Extensive experiments on our constructed benchmark demonstrate the superiority of VFCL over other baselines.
%在什么数据集上做了哪些实验，效率，可扩展性，之外的能力。实验结果如何证明到什么结果
(3) 
% We further demonstrate the advantages of our proposed method across various aspects of datasets. \yj{Delete this sentence. Directly describe the experimental results by concrete ability aspects and experimental results.} The experimental results clearly demonstrate that our method outperforms competing approaches in almost all cases. 
% 我们在四个数据集中进行了广泛的实验，验证了V-LETO的CIL和FIL在多个代表性最先进方法中的优越表现，与基线方法相比，对于CIL和FIL任务，我们的方法模型的性能分别提高了10.39\%和35.15\%。此外，我们可视化了演化原型，消融实验和超参数分析了本方法的高性能。
% 我们在四个真实世界数据集上进行了广泛的实验，验证了V-LETO在多个代表性最先进方法中的优越表现。与表现最佳的基线方法相比，它能够分别减少1.9%和0.8%的平均MAE和RMSE。
We conduct extensive experiments on four datasets to evaluate the performance of the V-LETO, demonstrating its superiority over several state-of-the-art methods.
Our method outperforms the baseline method by 10.39\% and 35.15\% for CIL and FIL tasks, respectively. 
We visualized the evolving prototypes and conducted ablation and hyperparameter analysis to further evaluate our method.
% We conduct extensive experiments on four real-world datasets to verify that V-LETO achieves superior performance over several representative state of-the-art methods. 
% It is capable of reducing average MAE and RMSE by 1.9\% and 0.8\% respectively compared to the best-performing baseline.

% \yj{Show method effectiveness by experimental details from multiple views.}


% Tackling catastrophic forgetting in VFL is non-trivial and requires overcoming the following key challenges.
% %首先，纵向联邦学习中的主动方需要联合所有被动方的本地embedding预测和训练模型。当特征增强的任务是顺序到来时，由于隐私被动方的旧任务的数据将不可得，那么主动方将无法联合所有特征进行模型训练。
% %然而，
% %其次，被动方根据顶层模型传输的反向梯度进行底层模型更新，当主动方采用正则化或者数据重放解决类灾难性遗忘问题时，被动方只能收到当前任务的梯度值，而不能获得先前任务的梯度值，因此底层模型将丢失旧任务的知识，无法实现类灾难性问题。
% Firstly,  the active party needs aggregate local embeddings from all passive parties to train and make predictions with the model. 
% When feature-enhancement tasks are processed sequentially, the unavailability of private previous task data from passive parties prevents the active party from utilizing all features for training. 
% \yj{The expression is confusing. Is it the challenge of feature forgetting?} 
% Secondly, \textcolor{green}{passive parties} update their \textcolor{green}{bottom models} based on gradients backpropagated from the top model. 
% When the active party employs regularization or data replay to tackle catastrophic forgetting, passive parties only receive gradients for the current task, lacking access to gradients from previous tasks. 
% As a result, the bottom model loses information from previous tasks, hindering the effective resolution of catastrophic forgetting issues.
% \yj{Which model needs to solve forgetting issues, the bottom model or the top model?} 
% Thus, it is essential to develop a novel approach in VFCL to effectively tackle feature catastrophic forgetting.



% \yj{The techniques are quite different in solving the two forgetting issues. When claiming the weakness of related works, the paper should introduce the views of corresponding issues, instead of a general introduction.}
% %上述方法需要参与方拥有完整的模型和独立的样本。
% %然而，纵向联邦学习的参与方仅仅拥有部分的模型参数和部分样本，无法独立的完成模型训练，无法直接采样上述方法解决纵向联邦学习中面临的特征和类灾难性遗忘问题。


% % \yj{The explanation is not clear. Why do different clients have similar features? Why does it support data synchronization and replay? Are these solutions all for class-level HFCL?} 
% % In contrast, VFL involves participants with complementary but non-overlapping feature spaces, making data replay infeasible as it would require sharing raw data—an action that directly violates the core privacy principles of VFL. 
% % Additionally, VFL's unique structure introduces complexities in parameter synchronization and update coordination,  making direct application of HFL-based continual learning methods unsuitable in practice.
% % \yj{Need more clear explanation. What's the unique structure? Why does it involve complexities in parameter synchronization and update coordination? What's the relation between the above issue and the application of HFL-based continual learning methods?}
% % Furthermore, there remains a significant research gap regarding continual learning strategies specifically tailored to VFL. 
% % \yj{What's gap?}
% % Although catastrophic forgetting is especially problematic in VFL settings, current literature lacks dedicated solutions to address this challenge within the VFL framework.
% % \yj{Is there any solutions for VFCL?}










% {\color{red}
% Vertical Federated Learning (VFL) \cite{liu2024vertical,castiglia2022compressed,wang2023bdvfl} is a distributed machine learning framework widely used in domains characterized by feature-federation distributions \cite{wen2023survey},
% such as healthcare \cite{huang2023vertical}, finance \cite{liu2023vertical}, and others \cite{hashemi2021vertical}.
% % Although VFL offers significant advantages and has been widely adopted, it encounters a critical challenge: the dynamic nature of training data. 
% %虽然纵向联邦学习具有较大的优势得到了广泛的应用，然而，VFL仍面临着一个挑战那就是缺乏训练数据的动态性。在实际的应用场景中，各个客户端的训练数据通常是动态的收集新的数据，也就是随着时间的增长不断的会增加新的训练数据。例如，在医疗系统的医院通常定期的收集病人的记录；金融领域的银行同样也是定期的收集用户的存款记录
% % 因此，如何适用于动态环境的纵向联邦持续学习进而提供模型性能变得至关重要。
% Although VFL demonstrates significant advantages and has been widely adopted, it faces challenges due to the dynamic nature of training data.
% In practical applications, the training data for each client is subject to frequent changes as new data is continuously collected over time \cite{yang2024federated}.
% For example, hospitals in the healthcare sector regularly gather patient records, while banks in the financial sector routinely collect user deposit information \cite{guo2024dynamic}.
% Therefore, it is essential to develop vertical federated continual learning methods that can effectively adapt to these dynamic environments to improve model performance.
% \yj{There is no need to introduce related work here. This paragraph should focus on the importance of the problems by clearly introducing the application scenarios and the non-trivial challenges.} 
% }


% %第四段：将我们的方法，什么的缩写要描述清楚，核心的技术思想是什么？方法是什么，解决了上面了两个挑战，描述的过程中有哪些关键词带出来的。
% % 用了。。。。方法解决了。。。。问题；用来。。。。方法解决。。。。问题
% % 每个方法覆盖上面的挑战，字面上能理解方法大概能解决上述问题。
% %整体的问题是如何实现的，如何汇聚数据，汇聚哪些数据，考虑的是。。。问题。针对。。。问题来设计方法。
% %增强的：有哪些问题，有哪些能力，增强的方法。问题先行。问题的衔接将上下的模块之间的关系。模态内-模态间（互补没有冗余）
% %模块之间的关系、核心解决的问题、模块是如何做的。整体有哪些能力

% %实验结果放到contribute。
% %第一个太抽象了。做了什么贡献，如何体现除了的。基本的方法，如何做到了贡献。不是重复原理，是如何做到持续学习的贡献，是否能扩展。提高到更高的意义，按照本方法的意义。框架是什么样的框架，不是具体的实现方法。框架保持。。。服务器如何设计的原型，高效和优势。能力清楚。
% %第二个框架的核心技术，具体是什么方法，具有哪些能力。解决了什么问题
% %实验结果，多少种方法比较，与baseline相比可量化的指标，具有。。。可解释性，其他维度的能力也需要添加一下。不同的场景下，哪些类别做的实验。概括，能力写出来。






% %图需要重新画，符合现实应用需求，现在感觉是伪需求


% % In practical scenarios, the data distribution is unbalanced
% % or even long-tailed, and the client dynamically receives new
% % data. For instance, smartphones generate data throughout the
% % day, and hospitals in the healthcare system periodically collect patient records在现实世界应用中，
% % 在现实世界的应用数据具有动态性的性能，各个客户端的训练数据收集随着时间增加而不断增加，也就是完整数据是随着持续任务到来的。
% % 例如，在金融领域，随着时间（也称为任务）的增加，参与信贷风控评估的银行和互联网拥有的训练数据量均在不断的增多。
% % 然而，现实世界的数据是动态变化的，模型也需要适应不断变化的任务，这对VFL模型提出了持续学习的需求。

% {\color{red}
% In real-world applications, \yj{such as...[refs],} data is dynamic \yj{How does the data dynamically change?}; the training data collected by each client continuously increases over time, indicating that complete datasets accumulate progressively as ongoing tasks are performed. 
% For instance, the volume of training data available to banks and internet companies involved in credit risk assessment steadily grows over time in the financial scenarios. 
% \yj{The example should be solid by given refs and clear by explaining the two continuous issues.}.
% However, traditional VFL methods are susceptible to catastrophic forgetting when models are trained on new tasks, as illustrated in Fig. \ref{fig: intro}.
% \yj{What's the exact new problem tackled in this paper? To solve the two forgetting issues simultaneously?} 
% }
% % This phenomenon can be categorized into class catastrophic forgetting and feature catastrophic forgetting, as illustrated in Figure 1.

% % In real-world applications, participant data in VFL may come continuously, making it essential to fully leverage all available data from the current task to optimize model performance.
% % However, due to data privacy restrictions on previous tasks, minimizing the loss on new tasks can inadvertently increase the loss on previous tasks, resulting in class and feature catastrophic forgetting.
% %在现实的应用场景中，VFL的参与方的数据可能是持续到来的，联合当前任务所有数据实现高性能纵向联邦学习是至关重要的。

% % 纵向联邦持续学习主要分为两类，分别是类别持续学习和特征持续学习
% %在类增强学习中，task2训练后，会遗忘task1中拥有的类“dog, horse”
% %在特征增强学习中，task2训练后，会遗忘task1中拥有的特征f1和f2
% %纵向联邦学习中参与方数据持续到来主要有两种类型类增强学习和特征增强学习，如图1所示。类增强学习指的是随着任务的增加，各个参与方本地拥有的样本数在逐渐增加。例如，在task1时，各个参与方拥有样本“cat，dog，horse”，在task2时，各个参与方增加了样本“bird，ship”，那么参与方在taks2时能够识别所有的样本是“cat，dog，horse，bird，ship”至关重要的。特征增强学习指的是随着任务的增加，被动方拥有的特征在逐渐增加。例如，在task1时，各个参与方只拥有样本“cat，dog，horse”的特征“f1，f2”。在task2时，各个参与方增加了样本“cat，dog，horse”的特征“f3，f4”，那么参与方在task2时能够使用样本当前所有的特征提升模型训练是至关重要的。
% % 然而，由于参与方旧任务的原始数据具有不可访问性，我们在最小化新任务的损失可能会增加旧任务的损失，进而导致类灾难性遗忘和特征灾难性遗忘。

% {\color{red}
% VFCL is primarily divided into two categories: class-continual learning \cite{ma2022continual,casado2023ensemble}, which aims to learn new classes of data, and feature-continual learning, which focuses on integrating new features into the existing data structure, as illustrated in Fig. \ref{fig: intro}.
% % In VFL, there are two primary incremental learning paradigms: class-incremental learning \cite{ma2022continual,casado2023ensemble} and feature-incremental learning, as illustrated in Fig. \ref{fig: intro}.

% In class-incremental learning, the number of samples each participant hpreviouss locally increases over successive tasks. 
% For example, in task 1, each participant might hprevious samples like “cat,” “dog,” and “horse.” By task 2, additional samples such as “bird” and “ship” are introduced, making it essential that participants are capable of recognizing all samples, including “cat,” “dog,” “horse,” “bird,” and “ship.”
% Similarly, feature-incremental learning involves progressive feature enrichment for the passive party.
% For example, in task 1, each participant has access only to features “f1” and “f2” for samples like “cat,” “dog,” and “horse.” By task 2, new features “f3” and “f4” become available, ensuring that participants leverage all available features for effective model training.
% However, due to data privacy restrictions on previous tasks, minimizing the loss on new tasks can inadvertently increase the loss on previous tasks, resulting in class and feature catastrophic forgetting.
% }


% %上述方法需要参与方拥有完整的模型和独立的样本。
% %然而，纵向联邦学习的参与方仅仅拥有部分的模型参数和部分样本，无法独立的完成模型训练，无法直接采样上述方法解决纵向联邦学习中面临的特征和类灾难性遗忘问题。


% % \yj{The explanation is not clear. Why do different clients have similar features? Why does it support data synchronization and replay? Are these solutions all for class-level HFCL?} 
% % In contrast, VFL involves participants with complementary but non-overlapping feature spaces, making data replay infeasible as it would require sharing raw data—an action that directly violates the core privacy principles of VFL. 
% % Additionally, VFL's unique structure introduces complexities in parameter synchronization and update coordination,  making direct application of HFL-based continual learning methods unsuitable in practice.
% % \yj{Need more clear explanation. What's the unique structure? Why does it involve complexities in parameter synchronization and update coordination? What's the relation between the above issue and the application of HFL-based continual learning methods?}
% % Furthermore, there remains a significant research gap regarding continual learning strategies specifically tailored to VFL. 
% % \yj{What's gap?}
% % Although catastrophic forgetting is especially problematic in VFL settings, current literature lacks dedicated solutions to address this challenge within the VFL framework.
% % \yj{Is there any solutions for VFCL?}

% % 实现VFCL是一件非容易的事需克服以下关键性挑战。
% % 首先，由于数据隐私和有限的存储，参与方不能存储先前任务的原始数据，这就意味着我们仅仅从当前任务的数据来推断和增强先前任务的知识。然而，每个被动方仅仅拥有部分的
% %其次，

% % Therefore, a novel vertical federation continual learning (VFCL) is needed to cope with the heterogeneous situation.

% %在现实的应用场景中，
% % In this scenario, clients share the same samples (shared/aligned samples) but have different local features. The goal of VFL is to train a global model capable of making predictions using the distributed features of shared samples.
% % VFL is to train a global model capable of making predictions using the distributed features of shared samples
% % FL aims to collaboratively train models across participants without exposing raw data. 
% % Based on the distributed way of data, FL can be primarily categorized into Horizontal Federated Learning (HFL) \cite{huang2022fairness}, Vertical Federated Learning (VFL) \cite{liu2024vertical, castiglia2022compressed, huang2023vertical}, and Federated Transfer Learning (FTL) \cite{saha2021federated}. 
% % As a specific FL scenario, VFL finds extensive application in cross-domain scenarios.
% % For example, VFL leverages various complementary attributes from multiple clients to enhance recognition precision in image classification for the classification tasks \cite{hu2023federated}. 
% % \yj{No need to introduce FL and HFL. What's VFL? What's the application of VFL?}
% % Vertical Federated Learning (VFL) \cite{liu2024vertical, castiglia2022compressed} has recently emerged as a state-of-the-art paradigm for collaborative learning across institutions that share data on the same individuals but with different features. 
% % This approach allows organizations to train models jointly without compromising data privacy, a critical requirement in fields such as healthcare, finance, and security.
% % VFL effectively combines distinct feature sets from multiple data hpreviousers, which can improve model accuracy and robustness, particularly in classification tasks. 
% % For instance, VFL leverages various complementary attributes from different sources to enhance recognition precision in image classification.
% %现有的VFL研究通常假设训练的是静态任务，即每个客户端拥有的训练数据是固定的。
% % Existing VFL research \cite{liu2024vertical, castiglia2022compressed, hu2023federated} generally assumes a static training task, with each client’s data remaining fixed throughout the process.
% % However, as real-world data are dynamic and constantly evolving, there is an increasing demand for VFL models that can adapt to new tasks and data over time without retraining from scratch. 
% % \yj{No need to emphasize existing works. Here we need to express two points: (1) What's the real-world scenarios for feature-level VFCL and class-level VFCL, respectively? (2) What's the core challenges (catastrophic forgetting) for the above two VFCL problems ? It's better to refer to the title figure for clear explanation.}
% % However, the evolving nature of real-world data necessitates VFL models that can incrementally adapt to new tasks without full retraining.
% % However, a variety of challenges still exist in VFL training process, 
% % However, VFL still faces some new security concerns \cite{he2023backdoor} as the server cannot access clients' training data.
% % A variety of attacks have been conducted on VFL,
% % e.g., label inference attacks \cite{fu2022label}, data inference attacks \cite{luo2021feature}, and data poison attacks \cite{zhang2022fldetector}.


% %第二段点名实现持续纵向联邦学习会有灾难性遗忘的挑战
% % 为了实现持续学习，VFL模型需要能够处理新增的任务或数据。然而，在实现这一目标的过程中，灾难性遗忘（Catastrophic Forgetting）成为一项不可忽视的挑战。在传统机器学习中，灾难性遗忘通常指模型在连续学习新任务时会丧失之前任务的信息。
% %传统的持续学习采用样本重放的思想来减弱先前任务知识的遗忘问题。
% % 在VFL的情境下，这一问题更为突出，因为隐私保护要求使得各参与方无法直接访问先前的完整数据，进而导致旧知识容易被遗忘。
% % 因此，如何让VFL模型在适应新数据时仍然保留已学知识，是实现动态任务处理的关键难题。

% %持续学习为解决上述问题
% % Continual Learning (CL) 
% %实现动态数据联邦持续学习的关键是解决catastrophic forgetting问题。
% % To support learning, VFL models need to incorporate continual learning capabilities to handle newly arriving tasks or data while retaining the previous tasks.
% % Addressing the issue of catastrophic forgetting \cite{yu2024overcoming} is essential for enabling federated continual learning with dynamic data.
% % % However, a major obstacle to achieving this goal is catastrophic forgetting \cite{yu2024overcoming}.
% % Catastrophic forgetting occurs when a model trained sequentially on new tasks forgets information learned from earlier tasks. 
% % In the context of VFL, this issue becomes even more pronounced, as privacy constraints prevent participants from accessing previously trained data, further exacerbating the loss of previously acquired knowledge. 
% % Thus, ensuring that VFL models can retain past information while adapting to new data—without direct access to historical data—is essential for enabling VFL to address dynamic task demands. 
% % \yj{This paragraph is unnecessary.}



% %第三段据我们了解，现在解决灾难性遗忘的方法有哪些，但是不适用于纵向联邦学习
% % 在传统联邦学习和机器学习中，许多方法被提出以应对灾难性遗忘问题，包括正则化、回放（Replay）以及动态网络扩展等策略。正则化方法通过对权重施加约束，防止在新任务训练时对旧任务至关重要的权重发生显著变化，而回放方法通过引入旧任务的数据或合成数据，帮助模型强化早期学习内容。然而，这些方法大多设计为横向联邦学习（Horizontal Federated Learning, HFL）的应用，HFL的不同客户端拥有相同特征但不同样本，因而可以使用分布式数据同步和数据回放等技术。而在纵向联邦学习（VFL）中，由于参与方之间的特征空间不同且互不重叠，无法直接应用回放方法，因为VFL框架下各参与方的数据特征互为补充，共享原始数据将直接违反隐私保护原则。此外，VFL的特性决定了参数调整和同步机制更加复杂，简单地将HFL的方法迁移至VFL在实际应用中存在明显的局限性。



% %第四段提出纵向联邦学习解决灾难性遗忘所存在的挑战是什么
% % 解决灾难性遗忘在VFL中面临独特挑战，主要表现在三个方面。首先，在引入新任务时，模型需要能够在不依赖于历史数据的前提下更新自身，以确保数据隐私。其次，不同于传统持续学习中模型的逐步更新，VFL中的各参与方必须同步更新模型，这增加了管理分布式数据源模型一致性的复杂性。此外，由于VFL在不同特征空间上运行，确保新增任务不会破坏先前学习的特征关联也成为实现有效持续学习的一大难题。因此，VFL的灾难性遗忘问题比传统场景更具挑战性，亟需创新的解决方案。
% %实现纵向持续联邦学习面临下面
% % Achieving TFCL is non-trivial and requires overcoming the following key challenges. First, the client cannot store all the previous task data due to the limited storage, which means that the trained model in the current state is the only resource we can utilize to infer and augment the previous features. Considering the poor interpretability of neural networks, it is challenging to accurately trace and optimize the corresponding data features. Second, the task repeatability in different clients may be irregular, which suggests that we cannot directly federate them because in a certain timeline, the feature distribution of repetitive tasks among clients might be highly heterogeneous. Therefore, a novel federation scheme is needed to cope with the heterogeneous situation.



% % \begin{enumerate}
% %     \item We propose a new VFCL paradigm, Prototype Knowledge Fusion-based Vertical Federated Continuous Learning, where previous tasks can be traced and optimized to address the repetitive task sequence in a VFL system. To the best of our knowledge, this is the first attempt in the literature to study and explore VFCL.
% %     \item 
% %     \item Extensive experiments on our constructed benchmark demonstrate the superiority of VFCL over other baselines.
% % \end{enumerate}
% % (1) We propose a new VFCL paradigm, Prototype Knowledge Fusion-based Vertical Federated Continuous Learning, where previous tasks can be traced and optimized to address the repetitive task sequence in a VFL system. To the best of our knowledge, this is the first attempt in the literature to study and explore VFCL.
% % (2) In VFL, we defined task prototypes and utilized them to transfer knowledge from previous tasks, enabling both class-level and feature-level continual learning. Additionally, by preserving key parameters in the bottom model that are essential for previous tasks, we mitigate knowledge forgetting, thereby enhancing model performance.
% % % (2)我们在纵向联邦学习中定义了类原型，并采用原型实现旧任务知识的转移，实现纵向联邦学习中的类持续学习和特征持续学习。此外，我们通过固定对于旧任务重要的底层模型参数，降低底层模型对旧任务知识的遗忘，提高模型性能。
% % (3) Extensive experiments on our constructed benchmark demonstrate the superiority of VFCL over other baselines.