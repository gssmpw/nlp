% auto-ignore
% \begin{abstract} 
%纵向联邦学习作为一种样本对齐特征联合的隐私保护机器学习范式受到广泛关注。在实际场景中，参与VFL训练的样本(eg. 特征或者类别)随着时间推移而不断的增加。然而，模型在学习新任务时往往会忘记先前任务中学习到的知识，导致先前知识灾难性遗忘问题。据我们了解，目前尚未有关于纵向持续学习的相关研究。
%为了解决这个问题，我们提出了VFCL(Vertical Federated Continual Learning via Prototype Knowledge Fusion)，采用定义原型表示实现时间先前特征和类知识转移。具体来说，VFCL由三个组件组成：（1）特征原型聚合实现先前特征知识转移。（2）类原型增强实现先前类知识转移。（3）采用新任务和先前任务知识优化模型，旨在提高模型的学习能力。大量的实验结果表明，我们的方法在减轻先前认为知识的灾难性遗忘方面表现良好。
% Vertical Federated Learning has attracted extensive attention as a privacy-preserving machine learning paradigm for sample-aligned feature federation.
\textit{Vertical Federated Learning} (VFL) has garnered significant attention as a privacy-preserving machine learning framework for sample-aligned feature federation.
%然而，传统的VFL未考虑现实应用场景中类持续学习和特征持续学习，面临着先前任务知识灾难性遗忘挑战。
However, traditional VFL approaches do not address the challenges of class and feature continual learning, resulting in catastrophic forgetting of knowledge from previous tasks.
% In real-world scenarios, the number of samples (e.g., features or classes) involved in VFL training increases over time. 
% {\color{red} Example,}
%举个例子说明，现实哪些应用场景中会持续不断到来新的任务。
%上面写的是现实应用场景，下面写的是任务，前后不一致。
% However, when learning a new task, the model often forgets the knowledge learned in the previous task, resulting in the catastrophic forgetting of the previous task knowledge. 
% To the best of our knowledge, there is no relevant research on vertical continuous learning. 
% \yj{Don't emphasize `no relevant work'. It's important to summarize the core technical challenges for feature and class continual learning in VFL.}
% To solve the above challenge, we first proposed a novel vertical federated continual learning method, named \underline{V}ertical Federated Continual \underline{L}earning via \underline{E}volving Pro\underline{t}otype Kn\underline{o}wledge (V-LETO), which employs an evolving prototype to realize the transfer of previous task knowledge.
To address the above challenge, we propose a novel vertical federated continual learning method, named \underline{V}ertical Federated Continual \underline{L}earning via \underline{E}volving Pro\underline{t}otype Kn\underline{o}wledge (V-LETO), which primarily facilitates the transfer of knowledge from previous tasks through the evolution of prototypes.
% {\color{red}
%关键思想是主动方通过使用当前知识和旧知识更新顶层模型，使得顶层模型拥有先前知识和当前知识的能力。此外，通过固定对于旧任务重要的底层模型参数，降低被动方拥有的底层模型对旧任务知识的遗忘，从而提高模型性能。
%添加原型学习和对比学习的思想。采用原型学习来学习表示。
Specifically, we propose an evolving prototype knowledge method, enabling the global model to retain both previous and current task knowledge. 
Furthermore, we introduce a model optimization technique that mitigates the forgetting of previous task knowledge by restricting updates to specific parameters of the local model, thereby enhancing overall performance.
% 我们提出了一种纵向联邦学习中演化原型知识方法并实现全局模型拥有先前任务知识和当前任务知识的能力。此外，我们提出了一种模型优化方法，此方法通过限制本地模型部分参数更新来降低对先前任务知识遗忘，提供模型性能。
% The core idea involves updating the top model by integrating knowledge from current and previous tasks, allowing the top model to retain a comprehensive understanding across tasks. 
% Additionally, we fix critical parameters in the bottom model that are essential for retaining previous knowledge of previous tasks, thereby enhancing overall model performance.
% }
% Feature prototype aggregation to realize the transfer of prior feature knowledge.
% Class prototype enhancement to transfer prior class knowledge. 
% Our method can improve its learning ability by using new tasks and previous task knowledge. 
% \yj{It is not necessary to give the algorithm details. Here we need to explain the basic mechanism of VFCL on how to realize feature transfer in feature continual learning and knowledge transfer in feature continual learning.} 
% Extensive experimental results show that our method performs well in mitigating the catastrophic forgetting of prior knowledge.
Extensive experiments conducted in both CIL and FIL settings demonstrate that our method, V-LETO, outperforms the other state-of-the-art methods. 
For example, our method outperforms the state-of-the-art method by 10.39\% and 35.15\% for CIL and FIL tasks, respectively.
Our code is available at \url{https://anonymous.4open.science/r/V-LETO-0108/README.md}.
% \end{abstract}