% auto-ignore

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/Scheme/method7.pdf}
    \caption{
    The framework overview of V-LETO. V-LETO consists two entities: \textit{the active party} with partial features and labels, and \textit{the passive party} with partial features.
    V-LETO consists of three modules:
    \textit{Prototype Generation} module use local labels in conjunction with global embeddings to generate class-specific prototypes. \textit{Prototype Evolving} module uses the outputs from PG module and evolves the prototype knowledge from previous tasks for both CIL and FIL.
    \textit{Model Optimization} module optimizes the model by integrating knowledge from both previous and current tasks.
    %我们方法的架构说明,包括两个实体，分别是具有部分特征和样本标签的主动方，只拥有部分特征的被动方。V-LETO包括三个模块：Prototype Generation module用于生成被动方
    % 
%  
    }
    % \yj{It's not clear to show the key contributions.}
    \label{fig: model}
\end{figure*}

\section{Problem Definition} \label{sec: problemformulation}
% \subsection{Vertical Federated Feature CL}
% \subsection{Vertical Federated Class CL}
% 让我们考虑一个联邦学习设置，其中 N 个客户端解决分类问题，旨在将每个数据样本（即所考虑设置中的图像）分配给可能的类 c ∈ C 之一。我们假设使用通用深度学习模型 M = D ◦ E，由编码器 E 和解码器 D 组成，与大多数图像分类架构一样。
%让我们考虑一个场景的纵向联邦学习设置，其中包括1个主动方和N-1个被动方协同解决图像分类问题。由于在VFL中，每个被动方拥有样本的部分特征，因此图像分类问题简化为通过所有被动方的特征联合判断数据样本所属的类别。





%Let us consider
Consider a typical VFL setting \cite{romanini2021pyvertical,liu2024vertical}, assume that there exists one active party $l_1$ collaborating with $K-1$ passive parties to tackle an image classification problem.
For a given training task $t$, each passive party $l_k$ holds a subset of features $x_{ik}^t$ of the aligned sample data $i$ and the local model $ \mathcal{B}_k^t$. 
%Meanwhile, 
The active party possesses the label $Y_{i}^{t}$ of sample data $i$ and the server model $\mathcal{T}^t$. 
%Furthermore, 
We assume that the active party $l_1$ and all passive parties $l_k$ have aligned training dataset samples as existing works that are obtained from the privacy set intersection \cite{luo2021feature,zhang2022adaptive}.
The objective of VFL is to collaboratively train the model between the active and passive parties to minimize the loss function, defined by Equation (\ref{eq:1}).
%which can be defined as follows.
%对于一个训练任务t，每个被动方l_k拥有对齐样本i的部分特征x_{ik}^t和底层模型\\mathcal{B}_k^t，主动方拥有样本i的标签Y_{i}^{t}和顶层模型\mathcal{T}^t。
%纵向联邦学习的目标是主动方协同被动方训练模型最小化损失函数L(), which is formulated by Equation (\ref{eq: 1}).
% This work aims to facilitate passive parties' involvement in the active party's local model training while maintaining all participants' data locally for privacy protection.
% This work sets the active party's local model as the VFL's global model. 
% The training problem of VFL is formulated by Equation (\ref{eq:1}).
%
%
\begin{equation} \label{eq:1}
\resizebox{0.42\textwidth}{!}{$
    \min\ell(\mathcal{T}^t; \{\mathcal{B}_k^t\}_{k = 2}^K; D) \triangleq \frac{1}{N} \sum_{i = 1}^{N} \mathcal{L} \left(\mathcal{T}^t(\{E_{ik}^t\}_{k = 2}^K); Y_{i}^{t}\right)$},
\end{equation} 
%
%
%In Equation (\ref{eq: 1}), 
where $D = (X^t, Y^{t})$ denotes training datasets; $E_{ik}^{t}$ denotes the local embedding of $l_k$th passive party; $E_{ik}^{t} = \mathcal{B}_k^t(x_i^t)$;
$X = \{\mathbf{x}_{i}\}_{i = 1}^{N}$ and $\mathbf{x}_{k} = \bigcup_{i = 1}^{N} \{x_{ik}^t\}$; 
$N$ denotes the total number of data samples; 
$K$ represents the total number of clients participating in training; 
$\{x_{ik}\}$ denotes some local features owned by the $l_k$th client; 
$\mathcal{L}$ denotes the loss function. 

%Next, we define VFCL which aims to train a model via iterative communication between the active and passive parties on a series of tasks $\{t_1, t_2, \dots, t_{T}\}$. 

VFCL trains a model via iterative communications between active and passive parties on a series of tasks $\{p_t | t\in 1, 2, \dots, T\}$, in which data from previous tasks ($p_1$-$p_{t-1}$ tasks) will become unavailable for training when a new task (the $p_t$ task) arrives.
A typical goal of VFCL is to train a model that minimizes the loss across both current tasks ($p_t$) and previous tasks ($p_1, \dots, p_{t-1}$).
That is to say, the objective is to minimize losses on all local tasks up to task $p_t$ through iterative active-passive communication. 
%下一步，我们定义纵向联邦持续学习，旨在主动方协同训练模型在一系列任务到达时。
%It is worth noting that 
%When the $t$th task arrives, data from previous tasks will become unavailable for training. 
% We define the global model parameters obtained from the previous task as $t−1$, and the new task as $Tt = SK k=1 T k t$, where $Tkt$ contains newly collected data at each client.
% The goal is to train a goal model with a minimized loss on the new task $t_{T}$ as well as old tasks $\{t_1; t_2; \dots; t_{T}\}$.
%The goal is to train a model that minimizes the loss across both new task $t$ and old tasks $\{t_1, t_2, \dots\}$ previously learned tasks.
%目标是训练一个最小损失的模型在新任务和旧任务。
%The optimization objective is achieved by minimizing the losses on all local tasks up to time $t$ through iterative active-passive communication. 
% The global model parameters can be obtained as follows:


% 先介绍一下纵向联邦学习的概念以及设计目标

%下一段介绍纵向联邦学习持续学习的概念和设计目标
% In VFL, each passive party possesses a subset of the features of each data sample. Consequently, the image classification task is transformed into a joint effort, where the class of a data sample is determined based on the integrated features contributed by all passive parties.

% Let us consider a federated learning setting where N clients tackle a classification problem aiming at assigning each data sample (i.e., an image in the considered setting) to one of the possible classes c ∈ C. We assume to use a generic deep learning model M = D ◦ E, composed of an encoder E followed by a decoder D as most architectures for image classification.
% In the considered setting each client at each time step has access to a different subset of the data: let C(t) k ⊂C denote the set of classes (i.e., the task) available for client k at round t. We denote with θ(t) = {ξ(t), ψ(t)} the parameters of the encoder and decoder models at round t. We assume that the client has access to the same data stream for a certain time interval of variable length, then moves to a new set of data, and so on. We can thus define an ordered set of boundaries Tk = {Tk,i}Rk i=1, such that each client changes task only at the boundaries location, i.e.,
% C (t) k = C (Tk,i) k ∀t : Tk,i−1 < t ≤ Tk,i. Rk denotes the total number of data streams for client k.
% Each client has a local stream dataset D = {D(Tk,i) k }T i=1, where Dt k = {X, Y} corresponds to the image-label pair. Note how the available data change at each boundary Tk,i as depicted in Fig. 2. Each client performs local training on its sequence of tasks {C(Tk,i) k }Rk i=1 in an asynchronous way w.r.t the others
% At each communication round t, a subset of clients Kt ⊂ N is randomly selected. Every client k ∈ Kt downloads the global model (i.e., the model weights θ(t−1)), performs training on his local data D(t) k , and sends the updated weights θ(t) k to the server, which will perform aggregation and produce the new global model θ(t).

%In FL, a central server and K clients cooperate to train a model on task T through R rounds of communication. The optimization objective can be written as follows:
% where T k refers to a collection of nk training samples at k-th client and na is the sum of all nk
% Here we define Continual Federated Learning (CFL), which aims to train a global model via iterative communication between the server and clients on a series of tasks {T1; T2; T3; · · · } accumulated at the client-side.
% When the t-th task arrives, data of previous tasks will become unavailable for training. We define the global model parameters obtained from the previous task as t−1, and the new task as Tt = SK k=1 T k t , where T k t contains newly collected data at each client.
% The goal is to train a global model with a minimized loss on the new task Tt as well as old tasks {T1; · · · ; Tt−1}. The optimization objective is achieved by minimizing the losses of K clients on all local tasks up to time t through iterative server-client communication. The global model parameters can be obtained as follows:

% Generally, the standard FCL process is composed of the following key steps: (1) The client side copes with the streaming tasks with a specially designed loss for typical continual learning [20, 36]; (2) At a timestep, the continually learned models in each client are uploaded to a central server, where different model aggregation methods are implemented to generate an improved global model. We iterate this pipeline many rounds until model convergence. Compared to traditional federated learning, FCL assumes that the overall system runs in a dynamic data environment, which is more challenging to achieve effective federation.
% 论文[8, 43]试图解决FCL中广泛出现的客户端干扰和灾难性遗忘问题，而不考虑任务序列中的数据可重复性。
% Motivated by the poor performance of current FCL solutions in the repetitive task environment, we attempt to introduce a new paradigm, Traceable Federated Continual Learning, to achieve effective FCL under the repetitive data stream. Specifically, our data setting differs previous ones from two aspects: (1) For a certain time, the incoming task has ever appeared in the previous task sequence; (2) Besides the task repeatability, the incoming task in different clients can also be various.
% Formally, assuming there are N local clients, each of which contains a task sequence Si t = {si1, si2, ..., sin} at the time t. Here i denotes the ith client and n is the number of the task sequence. If Si t−k = Si t, we believe a repetitive task appears at the time t and should be traced back to the time t − k for reprocessing. Based on the symbols, we define the goal of Traceable Federated Continual Learning as follows.
% n the following sections, we will describe a series of designed techniques in detail for accomplishing our objective.
% Definition 3.1. (Traceable Federated Continual Learning (TFCL)). Suppose the current model Mcur has learned P = {p1, p2, ..., pq} tasks, which are reflected by corresponding features F = {f1, f2, ..., fq}. Here q is the number of previous tasks. When the incoming task pj ∈ P , the goal of TFCL is to accurately trace and connect the corresponding data features fj to pj for further processing, and then selectively federate the various clients for an improved model Mnew.

%在纵向联邦学习中存在的持续学习的问题是什么？



\section{Methodology} \label{sec: methodology}



% \subsection{Overview}

% The two modules will be operated many times until we achieve desirable performance. 
% In the remainder of the section, we describe in detail our approach for implementing each module.
%上述模块将执行多次来获得我们渴望的性能
%在下面的subsection中，我们将详细描述每个模块的实现。
% Iterate through the aforementioned modules until a model with improved performance is achieved.
% The overview of FedProK is depicted in Fig. 2 and summarized in Algorithm 1. FedProK comprises two novel components: a client-side feature translation procedure to achieve temporal knowledge transfer and a server-side prototypical knowledge fusion mechanism to achieve spatial knowledge transfer. During the local update phase, clients train on their private data, compute the prototype for the new classes, and perform feature translation to mitigate forgetting of the previous classes. The prototype lists, as well as the weights of local networks, are uploaded to the server. During the global aggregation phase, the server needs to perform a standard FedAvg to aggregate the model parameters and a prototypical aggregation to construct a global knowledge base. Then, the class-wise prototype list is distributed back to the clients. Since the knowledge base contains the global prototypical list of previous classes without reserving or globally generating any exemplars, FedProK can effectively transfer knowledge across clients and among tasks without compromising data privacy.

%我们提出一种新颖的VFCL方法，具体地，采用参数划分和知识蒸馏实现样本的类增强持续学习，特征原型知识聚合实现特征持续学习。
%我们提出一种新颖的VFCL方法。首先，我们根据被动方的本地embedding定义了类原型并采用原型知识聚合实现新任务特征和旧任务特征融合。此外，VFCL方法采用原型增强构建新任务中未出现的原型解决时序灾难性遗忘问题。最后，采用原型知识蒸馏的方法优化被动方底层模型和顶层模型。
%主要包括三个部分：
%1. 原型融合--解决特征持续学习
%2. 原型增强--解决类遗忘问题（生成之前任务存在，但是当前任务不存在的类原型，用于增强学习）。
%3. 在模型训练的过程中，使用两个损失函数，一个是交叉熵损失函数，一个是原型损失函数（对应的是类增强学习）
% We propose a novel method of VFL integrating continuous learning, named VFCL. 
% Specifically, VFCL utilizes parameter partitioning and knowledge distillation to enable class-enhanced CL of sample data, while feature prototype knowledge aggregation facilitates CL of feature representations.


%We propose a \underline{P}rototype Know\underline{l}edge F\underline{u}sion-based Ve\underline{r}tical F\underline{e}derated Continuous \underline{L}earning (VFCL) scheme.
% We propose a novel personalized federated learning framework to transfer the traffic knowledge from the data-rich source cities to the data-scarce target city. As shown in Figure 2, the framework consists of two stages. Stage I: Local Spatio-Temporal Neural Network (ST-Net). We design
% an ST-Net to learn global shared traffic knowledge and local
% personalized spatio-temporal features for the cross-city traffic prediction task. Each city is seen as a client and deploys
% an ST-Net to train on the local traffic data. Stage II: Personalized Federated Learning. We decouple the components
% within ST-Net and selectively share space-independent traffic patterns to overcome spatial heterogeneity. Meanwhile, to
% alleviate temporal heterogeneity, we propose adaptive layerwise aggregation (model interpolation) to balance global generalization and local personalization.
% Class prototypes are constructed based on the passive party's local embedding and the active party’s labels, and prototype knowledge aggregation is used to fuse features from new and old tasks in our scheme. 
% To address the issue of class catastrophic forgetting, we use current prototypes to generate some prototypes that do not appear in the new task.
% A prototype knowledge distillation scheme is proposed for model optimization. 
% There are three major modules in our scheme, including {\em Prototype Generation} (PG), {\em Prototype Evolving} (PE), and {\em Model Optimization} (MO), as shown in Fig. \ref{fig:model}.

%To address catastrophic forgetting of previous task knowledge, we propose a novel VFCL framework, named Evolving Prototype Knowledge (V-LETO).
As shown in Figure \ref{fig: model}, V-LETO consists of three major modules, including {\em Prototype Generation} (PG), {\em Prototype Evolving} (PE), and {\em Model Optimization} (MO).
% PG module constructs the prototype for each class based on local labels and the global embedding. 
% These prototypes encompass the complete knowledge of each class, providing the foundation for VFCL.
% Then, PE module
% PE module 基于PG模块构建的原型实现CIL和FIL的先前任务原型知识evolving并构建旧任务知识信息。
%MO module基于PE构建旧任务知识信息和当前任务知识信息优化模型，提供模型的性能。指的注意的是，为了防止本地模型遗忘先前任务知识，在本地模型更新时，我们通过固定对旧任务知识重要的本地模型参数来减少对先前任务知识的遗忘。
% The PG module generates prototypes for each class based on local labels and global embeddings. 
% These prototypes encapsulate the full knowledge of each class, forming the foundation for VFCL.
Specifically, PG module use local labels in conjunction with global embeddings to generate class-specific prototypes, which  effectively capture and represent the complete knowledge associated with each class. 
PE module uses the outputs from PG module and evolves the prototype knowledge from previous tasks for both CIL and FIL.
%The PG module generates class-specific prototypes by utilizing local labels in conjunction with global embeddings. 
%These prototypes effectively capture and represent the complete knowledge associated with each class, thereby establishing a solid foundation for VFCL.
% These prototypes encapsulate the full knowledge of each class, forming the foundation for VFCL.
%The PE module then evolves the prototype knowledge from previous tasks for both CIL and FIL, leveraging the PG module while retaining the knowledge from earlier tasks.
Finally, MO module optimizes the model by integrating knowledge from both previous and current tasks. %, thereby enhancing its performance.
To mitigate the forgetting of prior task knowledge, we lock the local model parameters essential for preserving knowledge from earlier tasks during updates, thus minimizing previous task knowledge loss.
We execute the above modules in sequence and repeat multiple times until the overall model performance is improved.
%In the subsequent subsections, we provide comprehensive descriptions of the implementation details for each module.
% Detailed descriptions of each module's implementation are given in the following subsections.
% Thus, the active party optimizes the server model by using the joint loss, and the passive party optimizes the local model by applying a method of fixing key model parameters. % It uses prototype augmenting to obtain previous pseudo-class prototypes that have not appeared in the current task.
%     %对于类增强持续学习，被动方分别使用第t-1时刻任务和第t时刻任务的底层模型获得当前类的原型。然后，根据计算当前类原型的平均距离估计得到先前类原型。
%     %此外，我们通过高斯分布构造出先前类的embedding值，并在模型更新的时同时考虑先前类和当前类所计算的loss，CPA有效的解决先前类灾难性遗忘问题。
%     For CIL, the passive party uses the local models of the $t-1$th task and $t$th task to obtain the prototype of the current class. 
%     %Then,
%     The previous class prototype is estimated by calculating the average distance of the current class prototype. 
%     Moreover, we construct the embedding values of the previous class following a Gaussian distribution.
%     For FIL, the PA module obtains a new prototype of incremental features via aggregating local embeddings of incremental features. 
%     A new global prototype is constructed by integrating new and older prototypes from previous tasks.
%     % The loss from both the previous class and the current class is considered by this module when updating the model. 
%     Thus, this module addresses the issue of catastrophic forgetting of the previous task knowledge.
% The active party constructs the prototype for each class based on local labels and the global embedding. 
% These prototypes encompass the complete knowledge of each class, providing the foundation for CL.
%As shown in Fig. \ref{fig:model}, the overall process of our scheme mainly consists of three modules, namely, {\em Prototype Generation} (PG), {\em Prototype Augmenting} (PA), and {\em Model Optimization} (MO).

%We propose a novel method of VFL integrating continuous learning, named VFCL.
% Specifically, we define class prototypes based on the local embedding of the passive party and use prototype knowledge aggregation to achieve the fusion of new task features and old task features. 
%Specifically, we construct class prototypes based on the passive party's local embedding and use prototype knowledge aggregation to fuse features from new and old tasks. 
%In addition, the VFCL method uses current prototypes to generate some prototypes that do not appear in the new task to solve the problem of class catastrophic forgetting. 
%
%Finally, we use the prototype knowledge distillation method to optimize the model.
% 最后，我们使用
%The overall process of our proposed method VFCL includes three modules, as shown in Fig. 
% \ref{fig: overview}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{}
%     \caption{Caption}
%     \label{fig: overview}
% \end{figure}



%\begin{itemize}
%    \item %\textit{Prototype Generation (PG)}:
    %用来解决特征增强的灾难性遗忘问题。
    %Feature Prototype Aggregation（FPA）模块利用特征原型聚合来获得任务的全局原型。当增量特征的本地embedding到来时，FPA模块会对增量特征的本地embedding聚类得到增量特征的新原型。然后，整合新原型和旧任务获得的原型得到全局原型。全局原型保留了旧特征和新特征的信息，在新特征到来时仍能保留旧特征的信息，有效的解决了特征增强的灾难性遗忘问题。
    %将这个模块修改为原型构建。
%     PG Module:
% This module is designed to obtain a global task prototype from aggregating feature prototypes. 
% For feature-continual learning, the FPA module obtains a new prototype of incremental features via aggregating local embeddings of incremental features. 
% A new global prototype is constructed by integrating new and older prototypes from previous tasks.
% That is to say, the new global prototype stores both old and new features. 
% The catastrophic forgetting issue of feature enhancement is addressed by this module since older features remain when new features are embedded. 
%PG模块利用被动方的本地embedding值来获得全局embedding值，全局embedding值中包含某个类的所有知识。主动方根据本地标签值和全局embedding值构建每个类原型。类原型中包含了每个类的所有知识，为实现持续学习提供了条件。
% {\bf PG Module:}
% It utilizes local embeddings from passive parties to obtain the global embedding, which contains the knowledge of all classes. 
% The active party constructs the prototype for each class based on local labels and the global embedding. 
% These prototypes encompass the complete knowledge of each class, providing the foundation for CL.
% 模块利用特征原型聚合来获得任务的全局原型。当增量特征的本地embedding到来时，FPA模块会对增量特征的本地embedding聚类得到增量特征的新原型。然后，整合新原型和旧任务获得的原型得到全局原型。全局原型保留了旧特征和新特征的信息，在新特征到来时仍能保留旧特征的信息，有效的解决了特征增强的灾难性遗忘问题。

    
%    This module uses a feature prototype to obtain the aggregate prototype of the task. 
%    For feature-continual learning, the FPA module clusters the local embedding of the new feature to obtain a new prototype. 
    % (refer to the new feature belonging to the new task)
 %   Then, the new and previous prototypes obtained from the previous task are integrated to obtain the aggregate prototype. 
    % The global prototype preserves information from both previous and new features, allowing it to retain prior feature information as new features are introduced. 
%    The aggregate prototype retains information from previous features as new features are introduced, effectively mitigating the catastrophic forgetting problem of old tasks features.


    %原型增强：类增强和特征增强
%    \item %\textit{Prototype Augmenting (PA)}:
    %被动方如何进行本地模型训练，能够实现类增强训练，也就是随着时间的迁移，能够提取新类的前提下依旧能够提取旧类。这里面临的关键问题是，主动方拥有标签，被动方无法根据标签来判断当前的类是新类还是旧类。或者有一种可能是，在训练之前，主动方通过标签判断当前的样本是新类还是旧类，将其信息与被动方同步。
% {\bf PA Module:}
%     It uses prototype augmenting to obtain previous pseudo-class prototypes that have not appeared in the current task.
%     %对于类增强持续学习，被动方分别使用第t-1时刻任务和第t时刻任务的底层模型获得当前类的原型。然后，根据计算当前类原型的平均距离估计得到先前类原型。
%     %此外，我们通过高斯分布构造出先前类的embedding值，并在模型更新的时同时考虑先前类和当前类所计算的loss，CPA有效的解决先前类灾难性遗忘问题。
%     For CIL, the passive party uses the local models of the $t-1$th task and $t$th task to obtain the prototype of the current class. 
%     %Then,
%     The previous class prototype is estimated by calculating the average distance of the current class prototype. 
%     Moreover, we construct the embedding values of the previous class following a Gaussian distribution.
%     For FIL, the PA module obtains a new prototype of incremental features via aggregating local embeddings of incremental features. 
%     A new global prototype is constructed by integrating new and older prototypes from previous tasks.
%     % The loss from both the previous class and the current class is considered by this module when updating the model. 
%     Thus, this module addresses the issue of catastrophic forgetting of the previous task knowledge.
  %  The CPA module effectively solves the problem of catastrophic forgetting of the previous class.
% 利用特征原型聚合来获得任务的全局原型。当增量特征的本地embedding到来时，FPA模块会对增量特征的本地embedding聚类得到增量特征的新原型。然后，整合新原型和旧任务获得的原型得到全局原型。全局原型保留了旧特征和新特征的信息，在新特征到来时仍能保留旧特征的信息，有效的解决了特征增强的灾难性遗忘问题
  
  %模型优化：底层模型分为类增强和特征增强
%    \item %\textit{Model Optimization (MO)}:
    %这个模块主要是使用聚合后的特征原型、当前类原型和预估类原型训练模型，旨在提高模型的学习能力。主动方分别使用完整的当前原型知识和预估先前原型知识获得交叉熵损失和增强损失。然后主动方使用联合损失优化顶层模型，被动方采用重要模型参数固定的方法优化底层模型。
    %
%     {\bf MO Module:}
% It is designed to strengthen the learning ability of the model, which involves the aggregated feature prototype, current class prototype, and estimated class prototype.
% The active party obtains the cross-entropy loss and enhancement loss by adopting the complete current prototype knowledge and estimated previous prototype knowledge, respectively. 
% Thus, the active party optimizes the server model by using the joint loss, and the passive party optimizes the local model by applying a method of fixing key model parameters. 
    
%    This module mainly uses the aggregated feature prototype, current class prototype, and estimated class prototype to train the model, aiming to improve the learning ability of the model. The active side uses the complete current prototype knowledge and the estimated previous prototype knowledge to obtain the cross entropy loss and enhancement loss respectively. 
%    Then the active party uses the joint loss to optimize the server model, and the passive party uses the method of fixing important model parameters to optimize the local model.
    % This module mainly trains the model using the aggregated feature prototype, current class prototype, and estimated class prototype, aiming to improve its learning ability. 
    % The active party uses the complete current prototype knowledge and the estimated previous prototype knowledge to obtain the cross entropy loss and enhancement loss respectively. 
    % Moreover, the participating party uses cross-entropy loss and enhancement loss to optimize the model and alleviate the catastrophic forgetting problem of old tasks.
%\end{itemize}




%Repeat the modules as mentioned above until we achieve improved model performance. 
%In the following subsection, we describe the implementation of each module in detail.


%注释的算法
% \begin{algorithm}[!t]
% \caption{VFCL}
% \label{alg: VFCL}
% \textbf{Input:} local dataset $\{x_1, \dots, x_K\}$, local model $\mathcal{B}$, server model $\mathcal{T}$, global epoch $R$, number of the tasks $T$ \\
% \textbf{Output:} final model $\mathcal{B}$, $\mathcal{T}$
% \begin{algorithmic}[1]
% \FOR{$t = 1$ to $T$}
% \FOR{$r = 1$ to $R$}
%    % \STATE Compute task number $t = r / \left\lfloor (R / T) \right\rfloor + 1$
%     \FOR{each passive party $k$}
%         \STATE Compute local embedding $E_{k}^t = \mathcal{B}_k^t(x_{k})$
%         \STATE Send $E_{k}^t$ to active party
%         \IF{$t > 1$}
%             \STATE Compute local embedding $E_{k}^{t-1} = \mathcal{B}_k^{t-1}(x_{k})$
%             \STATE Send $E_{k}^{t-1}$ to active party
%         \ENDIF
%     \ENDFOR
%     \STATE \textbf{\textit{The Active Party}}
%     \STATE Compute aggregation embedding $E_k^t$ via Equation (\ref{eq: E_global})
%     \STATE Compute prototype $\mu_{c}^t$ by Equation (\ref{eq: proto})
%     \STATE Compute loss value $\mathcal{L}_{CE} = \mathcal{L}(\mathcal{T}^t(E),Y^t)$
%     \IF{$t > 1$}
%         \STATE Compute aggregate prototype $\bar{\mu_{c}^t}$ by Equation (\ref{eq: global_proto})
%         \STATE Construct pseudo prototype $\hat{\mu_{p}^t}$
%     \ENDIF
%     \STATE Active party computes the loss $\mathcal{L}_F$, $\mathcal{L}_A$ 
%     \STATE Compute training loss $\mathcal{L}$ according Equation (\ref{eq: L})
%     \STATE Active party update server model $\mathcal{T}$
%     \STATE Active party computes and sends $\frac{\partial \mathcal{L}}{\partial E_k}$ to passive parties
%     \FOR{each passive party $k$}
%     \STATE Compute important model parameters of the prior task, refer to Equations (\ref{eq: thre} and \ref{eq: AF})
%     \STATE Update local model based on the Equation (\ref{eq: local_update})
%     \STATE Save local model $\mathcal{B}_k$ in local
%     \ENDFOR
% \ENDFOR
% \ENDFOR
% \RETURN local model $\mathcal{B} = \{\mathcal{B}_k\}_{k=1}^K$, server model $\mathcal{T}$ 
% \end{algorithmic}
% \end{algorithm}




\subsection{Prototype Generation}
A prototype is generated by a PG module, which addresses the issue caused by fact that passive participants only have partial sample features and lack labels in VFCL, making prototype construction dramatically difficult. 
In this work, we consider the prototype a prototype belonging to a certain class.
The mechanism of PG module is that an active party aggregates local embeddings from all passive parties to obtain global embeddings, so that the knowledge of all classes are encompassed. 
We use global embeddings to generate prototypes for each class for obtaining the corresponding class prototypes. 
%为了解决VFCL中被动方仅仅拥有样本部分特征且无标签而无法构建原型的问题，
% To solve the catastrophic forgetting of previous task knowledge, we construct a prototype in the active party to realize the transfer of knowledge of temporal features. 
% we
% In particular, note that the prototype refers to a prototype belonging to a certain class in this work.
%It is worth noting that the feature prototype in this work refers to the prototype belonging to a certain class.
% The basic idea of the PA module is to realize the transfer of previous features by constructing feature prototypes, denoted as $\mu$.
%In addition, 
% We obtain the global feature prototype by aggregating the prototype of the new feature and the prototype of the previous feature.
% However, a passive party's local embeddings only represent partial sample knowledge instead of class prototypes because the passive party only keeps partial features of samples.
% In our scheme, we propose a prototype construction scheme for V-LETO. 
%To address the issue in VFCL where passive participants only have partial sample features and lack labels, making prototype construction impossible, we propose a PG module to generate a prototype for V-LETO.
% we construct a prototype in the active party to realize the transfer of knowledge of temporal features. 
%In particular, note that the prototype refers to a prototype belonging to a certain class in this work.
%
%
%The mechanism of the PG module is that an active party aggregates local embeddings from all passive parties to obtain global embeddings, encompassing the knowledge of all classes. 
%Thus, we use global embeddings to generate prototypes for each class, so that the corresponding class prototypes can be obtained. 
%we generate prototypes for each class using global embedding to derive the corresponding class prototypes.
%PA模块的主要思想是主动方通过获得所有被动方的本地embedding并聚合得到全局embedding，其中全局embedding包含一个类的所有知识。因此，我们通过全局embedding来生成原型获得每个类原型。
% The operating principle of the proposed scheme is obtaining all feature representations of the sample and adopting an aggregation on those feature representations for gaining class prototypes. 
Specifically, an active party firstly obtains all passive parties' local embeddings $E_{ik}^{t}$, where $i$ denotes the sample, $k$ denotes the passive party $l_k$, and $t$ denotes the $t$th task.  
An active party, for instance, obtains all feature embeddings $E_{i}^{t}$ of the sample $i$ in the $t$th task by aggregating local embeddings $E_{ik}^{t}$ from passive parties, expressed by $E_{i}^t = \sum_{k = 2}^{K} E_{ik}^{t}$.
%expresse by Equation (\ref{eq: E_global}).
%We all know the passive party's local embeddings can only represent partial sample knowledge and cannot represent class prototypes since they contain only a subset of sample features. 
%Thus, we introduce a vertical federated learning class prototype construction method. 
%The main idea is to represent all sample features and then aggregate them to derive class prototypes.
% The main idea is to obtain a complete representation of the sample features and then utilize an aggregation approach to derive the class prototypes.
% 具体地，首先，主动方获得所有被动方的本地embedding E_{ik}^{t}，其中i表示样本，k表示被动方l_k，t表示第t个任务。其次，主动方聚合得到样本i在第t个任务的所有特征表示E_{i}^{t}如公式1所示。
% Specifically, first, the active party obtains the local embedding $E_{ik}^{t}$ of all passive parties, where $i$ represents the sample, $k$ represents the passive party $l_k$, and $t$ represents the $t$th task.
%To be more precise, the active party first obtains the local embedding $E_{ik}^{t}$ of all passive parties, where $i$ denotes the sample, $k$ represents the passive party $l_k$, and $t$ represents the $t$th tasks.
% Secondly, the active party aggregates all feature representations $E_{i}^{t}$ of sample $i$ in the $t$th task as shown in Equation \ref{eq: E_global}.
%Then, the active party obtains all feature embedding $E_{i}^{t}$ of sample $i$ in the $t$th task by aggregates local embedding $E_{ik}^{t}$ from the passive parties, as shown in Equation (\ref{eq: E_global}).
%
% \begin{equation} \label{eq: E_global}
%     E_{i}^t = \sum_{k = 2}^{K} E_{ik}^{t}.
% \end{equation}
%
%此外，主动方拥有每个样本i的标签信息Y_i^t，主动方就能知道当前样本所述的类别$c$。因此主动方聚合$E_{i}^t$来获得属于每类的原型$\mu_c^t$如下所示,并得到当前任务的原型列表$\mathcal(L)^t$.
In addition, an active party has the label information $Y_i^t$ for each sample $i$ and can identify the sample's class $c$. 
Consequently, the active party aggregates $E_{i}^t$ to obtain the prototypes for each class $c$, as shown in Equation (\ref{eq: proto}). 
% The client $k$ computes the local prototype $\mu$ for each class and reserves it in the local prototypical list $\mathrm{L}^t$.
\begin{equation} \label{eq: proto}
    \mu_{c}^{t}=\frac{1}{\left|D_{c}\right|} \sum_{c=1}^{\left|D_{c}\right|} E_{i,c}^t,
\end{equation}
%其中|D_{c}|表示属于类c的样本数量。通过公式3，我们可以得到第t个任务中类别c的原型$\mu_{c}^t$，所有类别的原型组成当前的原型列表$\mathcal(L)^t$.
where $|D_{c}|$ represents the number of samples of class $c$. 
We obtain the class $c$ prototype $\mu_{c}^t$ in the $t$th task by Equation (\ref{eq: proto}).

\subsection{Prototype Evolving}
PE module is designed to reduce the catastrophic forgetting of previous task knowledge, 
The module facilitates the evolution of prototypes and stores the prototypes in an evolving global prototype list for task-level knowledge transfer.
% we propose a PE module to achieve inter-task knowledge transfer.
% we propose a PE module to evolve prototypes generated by the PG module, enabling knowledge transfer across tasks.
%我们基于PG模块生成的原型提出了一个PE模块来演化原型并保存在evolving global prototype列表中用于实现任务间知识转移。
%we propose a PE module to facilitate the evolution of prototypes and store them in an evolving global prototype list for task-level knowledge transfer.
There are two modules in PE, which are \textit{Class Evolving} (CE) and \textit{Feature Evolving} (FE) modules.
% CPA的主要思想是先前任务和当前任务在新类数据的学习能力的差异来构建旧类伪原型。
%
% The main idea of PA is to construct a pseudo-prototype of the old class based on the difference in learning ability between the previous task and the current task on new class data.
%具体地，CPA主要包括两个模块：分别是\textit{Knowledge distance}用于估计当前任务和先前任务的差距。\textit{Prototype augmenting}主要根据distance估计旧类原型实现原型增强。
% Specifically, PA mainly consists of two modules. 
% % \textit{Feature Augmenting} is used to estimate the gap between the current task and the previous task; 
% % \textit{Class Augmenting} mainly implements prototype enhancement in accordance with the distance estimation of the old class prototype.
% \textbf{\textit{Class augmenting}} synthesizes class prototypes from previous tasks to enable cross-task class knowledge transfer.
% \textbf{\textit{Feature augmenting}} aggregates the global prototype with the current task's prototype to achieve cross-task feature knowledge transfer.

%\textbf{\textit{Class Augmenting}}.
%我们通过拟合当前类中未出现的先前类$p$来解决先前类灾难性遗忘,其中$p \notin \mathcal{C}^t$, but $p \in \mathcal{L}^g$。
\noindent\textbf{Class Evolving Module.} CE mainly implements prototype evolving in accordance with the distance estimation of the old class prototype.
We address catastrophic forgetting of previous task classes by fitting a previous class $p$ that does not appear in the current task, i.e., $p \notin \mathcal{C}^t$, but $p \in \mathcal{L}^g$.
%我们通过Equation \ref{eq: dis}近似得到先前任务和当前任务学习的能力差
%然后，我们根据距离差来近似计算获得伪先前类的原型，如下式所示。
We estimate the difference in learning ability between the previous task and current task by Equation (\ref{eq: dis}) and compute the prototype of the pseudo-prior class knowledge augmenting according to the distance difference.
%我们在原型表$\mathcal{L}$中旧类p的$\mu_{p}^g$添加类间距离得到近似旧类$\hat{\mu_{p}}$,如下式所示。
We add the inter-class distance to the prototype ($\mu_{p}^g$) of the previous class $p$ in the prototype $\mathcal{L}$ to obtain the approximate previous class $\hat{\mu_{p}}$, shown in Equation (\ref{eq: old_task}).
% as shown in the following equation.
% %对于旧类$p$，我们通过得到当前任务中近似
% new class $c \in \mathcal{C}^t$  
%我们通过下式计算得到
\begin{equation} \label{eq: old_task}
   \hat{\mu_{p}^t} = \mu_{p}^g + \gamma\frac{1}{|\mathcal{C}^t|} \sum_{c \in \mathcal{C}^t} \operatorname{dis}\left(\mu_{c}^{t-1}, \mu_{c}^{t}\right),
\end{equation}
where $\gamma$ denotes the hyper-parameters used as weighting factors for the knowledge distance, $\mu_{c}^{t-1}$ and $\mu_{c}^{t}$ of the new class $c$ at the $(t-1)$th task and the $t$th task, respectively.
%特征增强主要是通过聚合全局原型与当前任务的原型来实现任务间的特征知识传递
%类增强主要是通过伪造先前任务的类原型实现任务间的类知识传递。
% %我们根据公式2和公式3分别得到第t-1个任务和第t个任务时新类c的原型$\mu_{c}^{t-1}$和$\mu_{c}^{t}$。
$\operatorname{dis}$ denote the pair-wise relation with cosine similarity by Equation (\ref{eq: dis}).
% % 表示当前类$
% According to Eqs. (\ref{eq: E_global}) and (\ref{eq: proto}), we obtain the prototypes $\mu_{c}^{t-1}$ and $\mu_{c}^{t}$ of the new class $c$ at the $(t-1)$th task and the $t$th task, respectively.
% Then, we compute the pair-wise relation with cosine similarity 
\begin{equation} \label{eq: dis}
   \operatorname{dis}\left(\mu_{c}^{t-1}, \mu_{c}^{t}\right)=\frac{\mu_{c}^{t-1} \cdot \mu_{c}^{t}}{\left\|\mu_{c}^{t-1}\right\|_{2} \times\left\|\mu_{c}^{t}\right\|_{2}}.
\end{equation}

%\textbf{\textit{Feature Augmentation}}.
%针对特征增强任务t，当前类c的原型\mu_{c}^t只包含当前的样本特征知识，而不包括先前特征知识。纵向联邦学习通常使用所有的特征知识来训练模型。
%这一部分存在的问题，在训练的过程中，采用原型会导致所有的数据趋于一致，所以模型训练的准确率比较高100%，测试准确率反而比较低。
\noindent\textbf{Feature Evolving Module.} FE aggregates global prototype with current task's prototype to achieve cross-task feature knowledge transfer.
For an FIL task $t$, current class $c$'s prototype ($\mu_{c}^t$) only contains current sample feature knowledge not previou knowledge, even though VFL require all feature knowledge for model training. 
%In our scheme, 
We save the class prototype list $\mathcal{L}^g$ of previous tasks during the training process so that the feature knowledge of previous tasks can be obtained from $\mathcal{L}^g$.
A prototype aggregation mechanism is developed to weight current and previous task knowledge, as expressed by Equation (\ref{eq: global_proto}).
%The mathematical expression of the weighted prototype aggregation process is given by Equation (\ref{eq: global_proto}).
%For feature enhancement task $t$, the prototype $\mu_{c}^t$ of the current class $c$ only contains the current sample feature knowledge, but not the previous feature knowledge. 
%VFL usually uses all feature knowledge to train the model.
% 因此我们设计了原型聚合机制，权重聚合当前特征知识原型和先前特征知识原型。权重原型聚合的过程如公式 \ref{eq: global_proto}所示。
%幸运的是，我们在训练过程中保存了先前任务的类原型列表$\mathcal{L}^g$,并通过$\mathcal{L}^g$可以获得先前任务的特征知识。
%Fortunately, we save the class prototype list $\mathcal{L}^g$ of the previous task during the training process and can obtain the feature knowledge of the previous task through $\mathcal{L}^g$.
%Therefore, we designed a prototype aggregation mechanism to weigh the current and previous feature knowledge prototypes. 
%The weighted prototype aggregation process is defined as the Equation (\ref{eq: global_proto}).
\begin{equation} \label{eq: global_proto}
    \bar{\mu_{c}^t}=\left\{\begin{array}{ll}
\mu_{c}^t, & c \notin \mathcal{L}^g \\
\beta \mu_{c}^t + (1-\beta) \mu_{c}^{g}, & \text {otherwise},
\end{array}\right.
\end{equation}
where $\beta$ denotes the hyper-parameters used as weighting factors for the respective prototype.
We evolve a global prototype list $\mathcal{L}^g$ using $\bar{\mu_{c}^t}$.
%具体地，如果类别c属于global prototype list $\mathcal{L}^g$，那么我们使用$\mu_{c}$替换$\mu_{c}^g$。如果类别c不属于$\mathcal{L}^g$，那么我们在$\mathcal{L}^g$添加类别c及原型$\mu_{c}$
%In particular, 
When class $c$ belongs to global prototype list $\mathcal{L}^g$, we exchange $\bar{\mu_{c}^t}$ for $\mu_{c}^g$ (e.g. $\mu_{c}^{g} = \mu_{c}^t$); otherwise, we insert class $c$ and prototype $\bar{\mu_{c}^t}$ into $\mathcal{L}^g$.
We reserve the global prototype list $\mathcal{L}^g$ in local memory.
N$\mathcal{L}^g$ contains all classes $c$ of the current task with corresponding prototypes. 
%It is worth noting that $\mathcal{L}^g$ contains all of the classes $c$ of the current task along with the corresponding prototypes. 
% $\bar{\mu_{c}^t}$.



\begin{table*}[!t]
\centering  
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{clccccc|cccc}
\toprule
\multirow{3}{*}{Datasets}                  & \multirow{3}{*}{Methods} & \multicolumn{9}{c}{Models Testing Accuracy (\%)}\\ 
\cmidrule{3-11}  
& &\multicolumn{5}{c|}{Class Incremental Learning}& \multicolumn{4}{c}{Feature Incremental Learning} \\ 
\cmidrule{3-11}
& & Task1 & Task2 & Task3 & Task4 & AVG & Task1 & Task2($\color{blue}{\uparrow}$) & Task3(${\color{blue}\uparrow}$) & Task4(${\color{blue}\uparrow}$) \\ \midrule
% \rowcolor{gray!20}
% \cellcolor{white}
\multirow{6}{*}{MNIST}
& Standalone                  &  99.15    &  94.82   & 96.66 & 93.00  & 95.90& \textbf{32.11}    & 78.28   & 79.33 &  34.82 \\
& Pyvertical
% \cite{romanini2021pyvertical}
& \textbf{99.21} &  61.37   &  45.42 & 36.63 &  60.65&  32.01  & 68.61(${\color{blue}\uparrow 36.60}$)  & 77.54(${\color{blue}\uparrow 8.93}$) &  67.87(${\color{blue}\downarrow 9.67}$)\\
&Pass
% \cite{zhu2021prototype}
+ VFL             & 72.69     & 74.57   & 69.73 & 68.02 & 71.25 & 21.97   &52.50(${\color{blue}\uparrow30.52}$)  &55.83(${\color{blue}\uparrow3.33}$)  &63.27(${\color{blue}\uparrow7.44}$) \\
& FedSpace + VFL            & 74.70    & 76.50   & 70.80 & 67.99 & 72.50 & 21.98    & 53.77 (${\color{blue}\uparrow31.79}$)   & 67.24 (${\color{blue}\uparrow13.47}$) &  32.30 (${\color{blue}\downarrow34.94}$)\\
&FedProK
% \cite{gao2024fedprok} 
+ VFL           & 74.69      & 75.52   & 75.72 & 69.95 & 73.97 &21.04   & 21.95(${\color{blue}\uparrow0.91}$) & 45.02(${\color{blue}\uparrow23.07}$) & 44.37(${\color{blue}\downarrow0.65}$) \\
& \textbf{V-LETO (Our)}                & 99.09      & \textbf{94.52} & \textbf{84.14}  &  \textbf{71.04} & \textbf{87.19}  & 27.67      & \textbf{93.33}(${\color{blue}\uparrow65.66}$) & \textbf{95.61}(${\color{blue}\uparrow2.83}$)  &  \textbf{96.79}(${\color{blue}\uparrow1.18}$) \\ \midrule 
% \rowcolor{gray!20} \cellcolor{white} 
\multirow{6}{*}{FMNIST}
 & Standalone                  &   95.54      &  86.96    &  81.83  & 90.66 & 88.00 & 60.32     & 73.55  &74.01 & 61.63\\
& Pyvertical 
% \cite{romanini2021pyvertical}
&  \textbf{96.15}&  57.10    & 43.01  & 37.43 &  58.42 & 60.60    & 64.57(${\color{blue}\uparrow 3.97}$)  & 68.21(${\color{blue}\uparrow 3.64}$) & 67.91(${\color{blue}\downarrow 0.30}$) \\
&Pass 
% \cite{zhu2021prototype} 
+ VFL             & 65.66     & 65.56   & 61.24 & 58.48 & 63.48 &  50.11  & 62.54(${\color{blue}\uparrow12.43}$)  & 67.38(${\color{blue}\uparrow4.84}$) 
& 66.30(${\color{blue}\uparrow1.08}$) \\
&FedSpace+ VFL 
% \cite{shenaj2023asynchronous} 
 & 67.66     & 67.58   & 63.26 & 61.42 & 64.98 & 50.34  &59.07(${\color{blue}\uparrow8.73}$)  & 58.55(${\color{blue}\downarrow0.52}$) & 40.52(${\color{blue}\downarrow18.03}$)
\\
&FedProK 
% \cite{gao2024fedprok}
+ VFL          & 74.69      & 65.52   & 63.72 & 59.95 & 73.97& 50.67  &53.13(${\color{blue}\uparrow2.46}$)  & 21.63(${\color{blue}\downarrow31.5}$) &26.18(${\color{blue}\downarrow4.55}$) \\
& \textbf{V-LETO (Our)}    & 94.38       &  \textbf{85.71} &   \textbf{64.04}  &  \textbf{69.13} & \textbf{76.14} & \textbf{60.93 }     &  \textbf{88.68}(${\color{blue}\uparrow27.78}$) &\textbf{95.06}(${\color{blue}\uparrow6.38}$)    & \textbf{97.39}(${\color{blue}\uparrow2.33}$)  \\ \midrule
% \rowcolor{gray!20} \cellcolor{white} 
\multirow{6}{*}{CIFAR10} & Standalone                  & 78.73     & 55.41   & 44.55 & 55.66 &  58.58 &\textbf{53.40}   &59.72  &60.90  &52.43  \\
& Pyvertical          &  69.66 &  25.92   & 20.37&  24.49 &  35.11 &52.19   &60.43(${\color{blue}\uparrow8.24}$)  &61.82(${\color{blue}\uparrow1.39}$)  & 63.09 (${\color{blue}\uparrow1.27}$)\\
&Pass + VFL             & 51.43     & 45.96  & 31.95 & 28.71& 39.51 &50.88   &67.67(${\color{blue}\uparrow16.79}$)  &71.69(${\color{blue}\downarrow4.02}$)  & 65.36(${\color{blue}\downarrow6.33}$)\\
& FedSpace+ VFL            & 51.45     & 46.06   & 31.99 & 28.81  & 39.57 & 50.40  &51.43(${\color{blue}\uparrow1.30}$)  & 58.78(${\color{blue}\uparrow7.35}$) &49.72(${\color{blue}\downarrow9.06}$) \\
&FedProK + VFL          & 51.45    & 44.30   & 32.18 & 30.51 & 39.61 &50.21   &46.64(${\color{blue}\downarrow3.57}$)  & 54.10(${\color{blue}\uparrow7.46}$) &47.24(${\color{blue}\downarrow6.86}$) \\
& \textbf{V-LETO} (Our)           & \textbf{74.04}      & \textbf{54.29} & \textbf{42.86}  & \textbf{33.86 } &  \textbf{52.26} & 53.31 & \textbf{85.12}(${\color{blue}\uparrow31.81}$)  & \textbf{87.81}(${\color{blue}\uparrow2.69}$) &  \textbf{88.34}(${\color{blue}\uparrow0.53}$) \\
% f folder save/V-LETO_Two/Feature/cifar10 exist
\midrule
% \rowcolor{gray!20} \cellcolor{white} 
\multirow{6}{*}{CINIC10} & Standalone  & 80.08    & 60.00 &  68.35 & 84.37  & 71.20& 43.61& 50.86   &  50.84   &44.07 \\
& Pyvertical  &  \textbf{79.84} & 38.35    & 36.20 &33.75 & 47.53  &43.36    & 51.68(${\color{blue}\uparrow8.32}$) &53.95(${\color{blue}\uparrow2.27}$)  & 51.60(${\color{blue}\downarrow2.35}$)    \\
&Pass + VFL     &58.59       & 47.08    &31.05  &30.75   & 41.86 & 36.35 &-  &-  &-  \\
& FedSpace + VFL  &61.32       &44.58     &28.98  &28.50   &40.35  & \textbf{46.57}  &  - & -  & -\\
&FedProK + VFL  &78.90       & 41.66    &29.92   &26.72   &44.30 & 44.86    & 42.89(${\color{blue}\downarrow1.97}$)   & 41.55(${\color{blue}\downarrow1.34}$)  &39.51(${\color{blue}\downarrow2.04}$)  \\
& \textbf{V-LETO (Our)}           & 79.62 & \textbf{54.50}  &\textbf{40.65} & \textbf{34.37} &\textbf{52.28} & 41.75  & \textbf{82.93}(${\color{blue}\uparrow40.81}$)&  \textbf{83.44}\textbf{(${\color{blue}\uparrow0.51}$)}&\textbf{84.97}\textbf{(${\color{blue}\uparrow1.35}$)} \\
\bottomrule
\end{tabular}
\caption{Comparison of V-LETO methods with baseline methods on four datasets and the number of class inclement learning tasks and feature inclement learning tasks.
In CIL, the bold text represents the highest test accuracy, excluding the Standalone method. ``AVG'' represents the average test accuracy across the four tasks. 
In FIL, ``$\uparrow$'' indicates the improvement in model accuracy from the previous task to the current task. ``-'' denotes that the model has not converged due to the complexity of the CINIC10 dataset.
} 
%在CIL中，标黑的表示除了Standalone以外，最优的测试准确率。在FIL中，$\uparrow$表示后一个任务相比于前一个任务模型提升的准确率。
%AVG表示四个任务的平均测试准确率
\label{table: V-LETO}
\end{table*}


\subsection{Model Optimization}
% $\lambda$
% a vector of augmented prototypes ˆe of the same length of the batch size is constructed (for each element ˆe[n] its class c = Y[n] is randomly selected among the ones not present in the current stage).
%为了优化模型参数, 
%where LCE is the standard supervised cross-entropy loss used for classification on D(t) k , Lp is the prototype-based loss and finally Lr corresponds to our contrastive representation loss. The λp and λr hyper-parameters are used as weighting factors for the respective losses.
% a vector of augmented prototypes ˆe of the same length of the batch size is constructed (for each element ˆe[n] its class c = Y[n] is randomly selected among the ones not present in the current stage). The loss Lp is thus computed as the cross entropy between the prediction computed from the augmented prototypes and their labels:
% 构建一个与批大小长度相同的增强原型 ˆe 向量（对于每个元素 ˆe[n]，其类 c = Y[n] 是从当前阶段不存在的类中随机选择的）。因此，损失 Lp 被计算为从增强原型计算出的预测与其标签之间的交叉熵
%为了减少旧知识的灾难性遗忘，我们需要使用新旧任务的知识训练优化模型，提高模型的性能。对于t时刻的新任务，我们采用新任务的数据获得的embedding中得到交叉熵损失函数\mathcal{L}_{CE},如下所示。
% To reduce the catastrophic forgetting of old knowledge, we need to use the knowledge of new and old tasks to optimize the server model and local model, aiming to improve the model's performance. 
%To mitigate the catastrophic forgetting of previous knowledge, it is necessary to optimize the global model and local models by leveraging the knowledge of both previous and current task knowledge.
This module is designed to optimize the global model and local models by using the knowledge of both previous and current task knowledge.
Two components include \textit{Gobal Model Optimization} (GMO) and \textit{Local Model Optimization} (LMO).

%This module mainly comprises two components: \textit{Gobal Model Optimization} (GMO) utilizing the regularization method and \textit{Local Model Optimization} (LMO) employing the fixed parameter of the previous task.
%本文中的模型优化包括两部分，分别是采用正则化方法优化顶层模型，采用参数固定的方法优化底层模型。

\noindent\textbf{Global Model Optimization.}
For the current task $t$, we use the global embedding obtained from the current task data to obtain the cross-entropy loss function $\mathcal{L}_{CE} = \mathcal{T}^t(E_i^t, Y_i^t)$.
% , as follows the Equation (\ref{eq: LCE}).
% \begin{equation}\label{eq: LCE}
%  \mathcal{L}_{CE} = \mathcal{T}^t(E_i^t, Y_i^t).
% \end{equation}
%此外，对于特征持续和类持续学习，我们基于增强特征原型$\bar{\mu_{c}^t}$和增强类原型$\hat{\mu_{p}^t}$构建一个与批次大小相同的向量$\textbf{\bar{\mu_{c}^t}}$和$\hat{\mu_{p}^t}$。然后我们根据构建的原型向量计算预测值与其标签计算得到损失\mathcal{L}_{F}和\mathcal{L}_A。其中
%In addition, 
For CIL and FIL tasks, we generate vectors $\bar{\mu_{c}^t}$ and $\hat{\mu_{p}^t}$ with the same batch size based on the enhanced class prototype $\hat{\mu_{p}^t}$ and the enhanced feature prototype $\bar{\mu_{c}^t}$. 
Then, we calculate the prediction value and its label based on the constructed prototype vector to obtain the loss $\mathcal{L}_A$ and $\mathcal{L}_{F}$, from $\mathcal{L}_A = \sum_{n} \mathcal{L}(\mathcal{T}^t(\hat{\mu_{p}^t}[n]), Y_p[n])$ and $\mathcal{L}_{F} = \mathcal{L}(\mathcal{T}^t(\bar{\mu_{c}^t}[n], Y_c[n]))$.
% \begin{equation} \label{eq: LF}
% \mathcal{L}_{F} = \mathcal{L}(\mathcal{T}^t(\bar{U_{c}^t}[n],Y_c[n]))
% \end{equation}
% \begin{equation} \label{eq: LA}
%     \mathcal{L}_A = \sum_{n} \mathcal{L}(\mathcal{T}^t(\hat{U_{p}^t}[n]), Y_p[n])
% \end{equation}
%最后，我们根据下面的损失函数训练优化模型参数。
We train and optimize model parameters for obtaining the following loss, refer to Equation (\ref{eq: L}).
\begin{equation} \label{eq: L}
  \mathcal{L} = \lambda_{CE} \mathcal{L}_{CE} + \lambda_A \mathcal{L}_A + \lambda_F \mathcal{L}_{F} ,
\end{equation}
where $\lambda_F$ and $\lambda_A$ hyperparameters are weighting factors for the respective losses. 
%我们采用损失值和梯度下降法反向传播更新顶层模型参数。
We use the loss $\mathcal{L}$ and stochastic gradient descent (SGD) \cite{malinovskiy2020local} to backpropagate and update the server model parameters.
%然后将损失值发送给被动方
The server sent the loss $\mathcal{L}$ to the passive party.




\noindent\textbf{Local Model Optimization.}
%被动方根据主动方发送的损失值$\mathcal{L}$来计算得到其梯度值为$\frac{\partial \mathcal{L}}{\partial E_k}$来更新底层模型。
The passive party computes the gradient value \(\textit{g} = \frac{\partial \mathcal{L}}{\partial E_k}\) and updates the local model using the loss value \(\mathcal{L}\) provided by the active party.
However, the gradient \( g \) contains only the knowledge of the current task and lacks information from previous tasks. To enable the passive party to retain prior knowledge during model training, we have adopted a method that involves fixing the parameters critical to previous tasks, thereby preserving those model parameters that are particularly significant to the prior tasks.
%然而, 梯度g中只有当前任务的知识，而不拥有先前任务的知识。
%为了使被动方在模型训练时保留旧知识信息，我们采用旧任务重要参数固定的方法来保留对于旧任务相对重要的模型参数。
%特殊地，我们通过计算模型的Fisher Information Matrix来衡量每个参数对旧任务的重要性。
Specifically, we estimate the importance of each parameter to the previous tasks by calculating the Fisher Information Matrix (FIM) \cite{yang2023dynamic}, expressed by Equation (\ref{eq: AF}).

\begin{equation} \label{eq: AF}
    \mathcal{F}_{ki} \approx \frac{1}{N} \sum_{i=1}^{N}\left(\nabla_{\theta_{k}} \mathcal{L}\left(x_{i}, y_{i}\right)\right)^{2},
\end{equation}
where $\mathcal{F}_{ki}$ denote the FIM of $\mathcal{B}$ with the previous tasks, $k$ denote the $k$th passive party, $i$ denote the number of samples.

In addition, we set a threshold $\kappa$ by calculating the mean and standard deviation of FIM to select model parameters.
\begin{equation} \label{eq: thre}
    \kappa = \frac{1}{N} \sum_{i=1}^N \mathcal{F}_{ki} - \delta \cdot \sqrt{\frac{1}{N} \sum_{i=1}^N (\mathcal{F}_{ki} - \frac{1}{N} \sum_{i=1}^N \mathcal{F}_{ki})^2},
\end{equation}
where $\delta = k_0 + \alpha \cdot \log(t + 1)$ is a hyperparameter; a larger \(\delta\) value increases the emphasis on previous tasks.

We select the relatively important model parameters for previous tasks based on a defined threshold $\kappa$. 
The important parameters are then fixed to prevent interference from new tasks, thus preserving the foundational model’s retention of knowledge from prior tasks and mitigating forgetting. 
\begin{equation} \label{eq: local_update}
    \mathcal{B}_{ki}^t=\left\{\begin{array}{ll}
\mathcal{B}_{ki'}^{t-1}, & \mathcal{F}_{ki} \ge \kappa \\
\text{Update}~\mathcal{B}_{ki}^{t-1}, & \text {otherwise}.
\end{array}\right.
\end{equation}
% where $i$ denotes $i$th local model parameter. 
% where $i$ represents the local model owned by the $i$th passive party.    
















































% \subsection{Prototype Augmenting}
% %%%完成%%%%%%%%%
% %针对特征增强，第一步我们将所有的本地embedding聚合得到global embedding，
% %我们将得到的global embedding作为原型聚合得到一个类的原型
% %将原型保持在ipfs文件中。
% %得到现有的原型，然后基于现有的原型和之前的原型获得包含所有特征的原型。
% %将所有特征的原型进行下一步顶层模型训练
% %原型中会包含所有的特征，先前的特征和新的特征。进行下一步的模型训练。
% %2号完成
% % 为了解决数据异构性问题，我们在服务器上构建了一个具有原型知识融合机制的知识库，以实现客户端之间的空间知识传递。在每一轮r及其对应的任务t中，客户端k为每个类c∈Ct k计算本地原型，并将本地原型列表Pk上传到服务器。
% %为了解决先前特征遗忘问题，我们在主动方构建了特征原型聚合来实现时序特征知识的传递。值得注意的是，本文的特征原型指的是属于某一个类的原型。
% %FPA模块的主要思想是通过构建特征原型来实现先前特征的转移。此外，我们通过聚合新特征的原型和先前特征的原型来获得全局特征原型。全局特征原型中包含了先前特征和新特征的知识信息，因此可以有效的解决特征增强时面临的灾难性遗忘问题。
% To solve catastrophic forgetting of the previous feature, we constructed feature prototype aggregation on the active party to realize the transfer of temporal feature knowledge. 
% In this work, the feature prototype refers to a prototype belonging to a certain class.
% %It is worth noting that the feature prototype in this work refers to the prototype belonging to a certain class.

% The basic idea of the PA module is to realize the transfer of previous features by building feature prototypes, denoted as $\mu$.
% %In addition, 
% We obtain the global feature prototype by aggregating the prototype of the new feature and the prototype of the previous feature. 
% % The global feature prototype contains the knowledge information of the previous feature and the new feature, so it can effectively solve the catastrophic forgetting problem faced during feature enhancement.
% %Specifically, 
% To be specific, FPA module comprises two key components, i.e., \textit{Prototype Construction} for achieving all local embedding transfer and \textit{Prototype Aggregation} for achieving spatial knowledge transfer.
% %Below, we provide a detailed introduction to these two components.

% % \subsubsection{}
% \textit{Prototype Construction}.
% %由于被动方只拥有样本的部分特征，因此被动方的本地embedding只能表示样本的部分知识，无法表示样本的类原型。本节我们提出了一种在纵向联邦学习中类原型的构造方法。主要的思想是我们通过获得样本的所有特征表示，然后采用聚合的方法来得到类原型。
% % Since the passive party only possesses a subset of the features of the samples, the local embeddings of the passive party can only capture partial knowledge of the samples and cannot represent the class prototypes.
% In general, a passive party's local embeddings only represent partial sample knowledge instead of class prototypes because the passive party only keeps partial features of samples.
% In our scheme, we propose a VFL class prototype construction scheme. 
% The operating principle of the proposed scheme is obtaining all feature representations of the sample and adopting an aggregation on those feature representations for gaining class prototypes. 
% Specifically speaking, an active party firstly obtains all passive parties' local embeddings $E_{ik}^{t}$, where $i$ denotes the sample, $k$ denotes the passive party $l_k$, and $t$ denotes the $t$th task.  
% Thus, the active party, for example, obtains all feature embeddings $E_{i}^{t}$ of the sample $i$ in the $t$th task by aggregating local embeddings $E_{ik}^{t}$ from the passive parties, represented by Equation (\ref{eq: E_global}).
% %We all know the passive party's local embeddings can only represent partial sample knowledge and cannot represent class prototypes since they contain only a subset of sample features. 
% %Thus, we introduce a vertical federated learning class prototype construction method. 
% %The main idea is to represent all sample features and then aggregate them to derive class prototypes.
% % The main idea is to obtain a complete representation of the sample features and then utilize an aggregation approach to derive the class prototypes.
% % 具体地，首先，主动方获得所有被动方的本地embedding E_{ik}^{t}，其中i表示样本，k表示被动方l_k，t表示第t个任务。其次，主动方聚合得到样本i在第t个任务的所有特征表示E_{i}^{t}如公式1所示。
% % Specifically, first, the active party obtains the local embedding $E_{ik}^{t}$ of all passive parties, where $i$ represents the sample, $k$ represents the passive party $l_k$, and $t$ represents the $t$th task.
% %To be more precise, the active party first obtains the local embedding $E_{ik}^{t}$ of all passive parties, where $i$ denotes the sample, $k$ represents the passive party $l_k$, and $t$ represents the $t$th tasks.
% % Secondly, the active party aggregates all feature representations $E_{i}^{t}$ of sample $i$ in the $t$th task as shown in Equation \ref{eq: E_global}.
% %Then, the active party obtains all feature embedding $E_{i}^{t}$ of sample $i$ in the $t$th task by aggregates local embedding $E_{ik}^{t}$ from the passive parties, as shown in Equation (\ref{eq: E_global}).

% \begin{equation} \label{eq: E_global}
%     E_{i}^t = \sum_{k = 2}^{K} E_{ik}^{t}.
% \end{equation}

% %此外，主动方拥有每个样本i的标签信息Y_i^t，主动方就能知道当前样本所述的类别$c$。因此主动方聚合$E_{i}^t$来获得属于每类的原型$\mu_c^t$如下所示,并得到当前任务的原型列表$\mathcal(L)^t$.
% Furthermore, the active party has the label information $Y_i^t$ for each sample $i$ and can identify the sample's class $c$. 
% Consequently, the active party aggregates $E_{i}^t$ to obtain the prototypes for each class $c$, as shown below. 
% % The client $k$ computes the local prototype $\mu$ for each class and reserves it in the local prototypical list $\mathrm{L}^t$.
% \begin{equation} \label{eq: proto}
%     \mu_{c}^{t}=\frac{1}{\left|D_{c}\right|} \sum_{c=1}^{\left|D_{c}\right|} E_{i,c}^t,
% \end{equation}
% %其中|D_{c}|表示属于类c的样本数量。通过公式3，我们可以得到第t个任务中类别c的原型$\mu_{c}^t$，所有类别的原型组成当前的原型列表$\mathcal(L)^t$.
% where $|D_{c}|$ represents the number of samples of class $c$. 
% We obtain the class $c$ prototype $\mu_{c}^t$ in the $t$th task by Equation (\ref{eq: proto}).
% % and reserves it in the current prototypical list $\mathcal{L}^t$.
% % It is worth noting that $\mathcal{L}^t$ contains all of the classes $c$ of the current task along with the corresponding prototypes $\mu_{c}^t$.

% %值得注意的是，$\mathcal{L}^t$中保存了当前任务的所有类c和对应的原型\mu_{c}^t.

% \textit{Feature Augmentation}.
% %针对特征增强任务t，当前类c的原型\mu_{c}^t只包含当前的样本特征知识，而不包括先前特征知识。纵向联邦学习通常使用所有的特征知识来训练模型。
% For the feature enhancement task $t$, the current class $c$'s prototype ($\mu_{c}^t$) only contains current sample feature knowledge rather than previous feature knowledge. 
% VFL generally adopts all feature knowledge for model training. 
% In our scheme, we save the class prototype list $\mathcal{L}^g$ of previous tasks during the training process so that the feature knowledge of previous tasks can be obtained from $\mathcal{L}^g$.
% Thus, we develop a prototype aggregation mechanism to weight current and previous feature knowledge prototypes. 
% The mathematical expression of the weighted prototype aggregation process is given by Equation (\ref{eq: global_proto}).
% %For feature enhancement task $t$, the prototype $\mu_{c}^t$ of the current class $c$ only contains the current sample feature knowledge, but not the previous feature knowledge. 
% %VFL usually uses all feature knowledge to train the model.
% % 因此我们设计了原型聚合机制，权重聚合当前特征知识原型和先前特征知识原型。权重原型聚合的过程如公式 \ref{eq: global_proto}所示。
% %幸运的是，我们在训练过程中保存了先前任务的类原型列表$\mathcal{L}^g$,并通过$\mathcal{L}^g$可以获得先前任务的特征知识。
% %Fortunately, we save the class prototype list $\mathcal{L}^g$ of the previous task during the training process and can obtain the feature knowledge of the previous task through $\mathcal{L}^g$.
% %Therefore, we designed a prototype aggregation mechanism to weigh the current and previous feature knowledge prototypes. 
% %The weighted prototype aggregation process is defined as the Equation (\ref{eq: global_proto}).
% \begin{equation} \label{eq: global_proto}
%     \bar{\mu_{c}^t}=\left\{\begin{array}{ll}
% \mu_{c}^t, & c \notin \mathcal{L}^g \\
% \beta \mu_{c}^t + (1-\beta) \mu_{c}^{g}, & \text {otherwise},
% \end{array}\right.
% \end{equation}
% where $\beta$ denotes the hyper-parameters used as weighting factors for the respective prototype.
% Furthermore, we update the global prototype list $\mathcal{L}^g$ using $\bar{\mu_{c}^t}$.
% %具体地，如果类别c属于global prototype list $\mathcal{L}^g$，那么我们使用$\mu_{c}$替换$\mu_{c}^g$。如果类别c不属于$\mathcal{L}^g$，那么我们在$\mathcal{L}^g$添加类别c及原型$\mu_{c}$
% In specially, if the class $c$ belongs to the global prototype list $\mathcal{L}^g$, we exchange $\bar{\mu_{c}^t}$ for $\mu_{c}^g$ (e.g. $\mu_{c}^{g} = \mu_{c}^t$), otherwise, we insert the class $c$ and prototype $\bar{\mu_{c}^t}$ into $\mathcal{L}^g$.
% We reserve the global prototype list $\mathcal{L}^g$ into the local memory.
% It is worth noting that $\mathcal{L}^g$ contains all of the classes $c$ of the current task along with the corresponding prototypes $\bar{\mu_{c}^t}$.

% % 此外，如果当前数据集包括先前的类，则应更新服务器上的知识库Pt G，以纠正先前数据的偏差并减轻概念漂移的影响。因此，我们设计了原型知识融合机制，水平聚合来自不同客户的异构原型知识，并沿时间线垂直融合先前和新的知识。融合过程定义为

% % 本地类集在客户端之间重叠，定义为 Ct i ∩ Ct j ̸= ∅(i, j ∈ K)。在这种情况下，上传的本地原型列表Pi和Pj可能包含同一类c的不同原型μt i,c ̸= μt j,c。

% \subsection{Class Prototype Augmentation}
% %这一部分描述本地embedding生成过程。在已有的模型上 
% %为了减少类知识的灾难性遗忘，我们使用类原型增强机制实现任务间类知识转移。
% To reduce the catastrophic forgetting of class knowledge, we use a class prototype augmenting mechanism (CPA) to achieve inter-task class knowledge transfer.
% % CPA的主要思想是先前任务和当前任务在新类数据的学习能力的差异来构建旧类伪原型。
% The main idea of CPA is to construct a pseudo-prototype of the old class based on the difference in learning ability between the previous task and the current task on new class data.
% %具体地，CPA主要包括两个模块：分别是\textit{Knowledge distance}用于估计当前任务和先前任务的差距。\textit{Prototype augmenting}主要根据distance估计旧类原型实现原型增强。
% Specifically, CPA mainly consists of two modules. 
% \textit{Knowledge Distance} is used to estimate the gap between the current task and the previous task; \textit{Prototype Augmenting} mainly implements prototype enhancement in accordance with the distance estimation of the old class prototype.

% \textit{Knowledge Distance}.
% %需要考虑一下这一部分需要如何的表示
% %为了获得旧类的原型解决类灾难性遗忘，我们提出一种知识距离度量的方法来评估先前任务和当前任务的差距。
% %这一部分需要重新思考，是否需要
% To obtain the prototype of the old class and solve the catastrophic forgetting of the class, we propose a knowledge distance measurement method to evaluate the gap between the previous task and the current task.
% % 具体地，被动方k需要本地保存第t-1个任务时的底层模型$\mathcal(B)^{t-1}$。
% Specifically, each passive party needs to locally save the local model $\mathcal{B}_k^{t-1}$ at the $t-1$th task.
% % 当属于新类$c$的样本$x_(c,k)^t$到来时，各个被动方分别使用第t-个任务的底层模型和第t个任务的底层模型获得本地嵌入值$E_k^{t-1}$ 和$E_k^{t}$。
% When the data sample $x_{c,k}^t$ belonging to the new class $c$ arrives, each passive party uses the local model of the $t$th task and the local model of the $t$th task to obtain local embedding values $E_k^{t-1}$ and $E_k^{t}$, respectively,
% % 其中k表示第k个被动方, $E_k^{t-1} = \mathcal(B)^{t-1}(x_(c,k)^t)$ 和$E_k^{t} = \mathcal(B)^{t}(x_(c,k)^t)$ 
% where $k$ represents the kth passive party, $E_k^{t-1} = \mathcal{B}_k^{t-1}(x_{c,k}^t)$ and $E_k^{t} = \mathcal{B}^{t}(x_{c,k}^t)$.


% \textit{Prototype Augmenting}.
% %我们通过拟合当前类中未出现的先前类$p$来解决先前类灾难性遗忘,其中$p \notin \mathcal{C}^t$, but $p \in \mathcal{L}^g$。
% We address catastrophic forgetting of previous classes by fitting a previous class $p$ that does not appear in the current class, where $p \notin \mathcal{C}^t$, but $p \in \mathcal{L}^g$.
% %我们通过Equation \ref{eq: dis}近似得到先前任务和当前任务学习的能力差
% %然后，我们根据距离差来近似计算获得伪先前类的原型，如下式所示。
% We approximate the difference in learning ability between the previous task and the current task by Equation (\ref{eq: dis}).
% Then, we approximate the prototype of the pseudo-prior class knowledge augmenting based on the distance difference.
% %我们在原型表$\mathcal{L}$中旧类p的$\mu_{p}^g$添加类间距离得到近似旧类$\hat{\mu_{p}}$,如下式所示。
% We add the inter-class distance to the $\mu_{p}^g$ of the old class $p$ in the prototype $\mathcal{L}$ to obtain the approximate old class $\hat{\mu_{p}}$, as shown in the following Equation (\ref{eq: old_task}).
% % as shown in the following equation.
% % %对于旧类$p$，我们通过得到当前任务中近似
% % new class $c \in \mathcal{C}^t$  
% %我们通过下式计算得到

% \begin{equation} \label{eq: old_task}
%    \hat{\mu_{p}^t} = \mu_{p}^g + \gamma\frac{1}{|\mathcal{C}^t|} \sum_{c \in \mathcal{C}^t} \operatorname{dis}\left(\mu_{c}^{t-1}, \mu_{c}^{t}\right),
% \end{equation}
% where $\gamma$ denotes the hyper-parameters used as weighting factors for the knowledge distance.


% \subsection{Model Optimization}
% % $\lambda$
% % a vector of augmented prototypes ˆe of the same length of the batch size is constructed (for each element ˆe[n] its class c = Y[n] is randomly selected among the ones not present in the current stage).
% %为了优化模型参数, 
% %where LCE is the standard supervised cross-entropy loss used for classification on D(t) k , Lp is the prototype-based loss and finally Lr corresponds to our contrastive representation loss. The λp and λr hyper-parameters are used as weighting factors for the respective losses.
% % a vector of augmented prototypes ˆe of the same length of the batch size is constructed (for each element ˆe[n] its class c = Y[n] is randomly selected among the ones not present in the current stage). The loss Lp is thus computed as the cross entropy between the prediction computed from the augmented prototypes and their labels:
% % 构建一个与批大小长度相同的增强原型 ˆe 向量（对于每个元素 ˆe[n]，其类 c = Y[n] 是从当前阶段不存在的类中随机选择的）。因此，损失 Lp 被计算为从增强原型计算出的预测与其标签之间的交叉熵
% %为了减少旧知识的灾难性遗忘，我们需要使用新旧任务的知识训练优化模型，提高模型的性能。对于t时刻的新任务，我们采用新任务的数据获得的embedding中得到交叉熵损失函数\mathcal{L}_{CE},如下所示。
% % To reduce the catastrophic forgetting of old knowledge, we need to use the knowledge of new and old tasks to optimize the server model and local model, aiming to improve the model's performance. 
% To mitigate the catastrophic forgetting of old knowledge, it is necessary to optimize the server and local models by leveraging the knowledge of both new and old task knowledge.
% This module mainly comprises of two components, \textit{server model optimization} utilizing the regularization method and \textit{local model optimization} employing the fixed parameter of the previous task.
% %本文中的模型优化包括两部分，分别是采用正则化方法优化顶层模型，采用参数固定的方法优化底层模型。

% \textit{server Model Optimization.}
% For the new task $t$, we use the embedding obtained from the new task data to obtain the cross-entropy loss function $\mathcal{L}_{CE}$, as follows the Equation (\ref{eq: LCE}).

% \begin{equation}\label{eq: LCE}
%  \mathcal{L}_{CE} = \mathcal{T}^t(E_i^t, Y_i^t).
% \end{equation}

% %此外，对于特征持续和类持续学习，我们基于增强特征原型$\bar{\mu_{c}^t}$和增强类原型$\hat{\mu_{p}^t}$构建一个与批次大小相同的向量$\textbf{\bar{\mu_{c}^t}}$和$\hat{\mu_{p}^t}$。然后我们根据构建的原型向量计算预测值与其标签计算得到损失\mathcal{L}_{F}和\mathcal{L}_A。其中
% In addition, for feature continual and class continual learning, we generate a vector $\bar{U_{c}^t}$ and $\hat{U_{p}^t}$ with the same batch size based on the enhanced feature prototype $\bar{\mu_{c}^t}$ and the enhanced class prototype $\hat{\mu_{p}^t}$. 
% Then we calculate the prediction value and its label based on the constructed prototype vector to obtain the loss $\mathcal{L}_{F}$ and $\mathcal{L}_A$, where $\mathcal{L}_{F} = \mathcal{L}(\mathcal{T}^t(\bar{U_{c}^t}[n], Y_c[n]))$ and $\mathcal{L}_A = \sum_{n} \mathcal{L}(\mathcal{T}^t(\hat{U_{p}^t}[n]), Y_p[n])$.
% % \begin{equation} \label{eq: LF}
% % \mathcal{L}_{F} = \mathcal{L}(\mathcal{T}^t(\bar{U_{c}^t}[n],Y_c[n]))
% % \end{equation}
% % \begin{equation} \label{eq: LA}
% %     \mathcal{L}_A = \sum_{n} \mathcal{L}(\mathcal{T}^t(\hat{U_{p}^t}[n]), Y_p[n])
% % \end{equation}
% %最后，我们根据下面的损失函数训练优化模型参数。
% Finally, we train and optimize the model parameters according to the following loss.
% \begin{equation} \label{eq: L}
%   \mathcal{L} = \mathcal{L}_{CE} + \lambda_F \mathcal{L}_{F} + \lambda_A \mathcal{L}_A,
% \end{equation}
% where $\lambda_F$ and $\lambda_A$ hyperparameters are used as weighting factors for the respective losses. 
% %我们采用损失值和梯度下降法反向传播更新顶层模型参数。
% We use the loss $\mathcal{L}$ and stochastic gradient descent (SGD) \cite{malinovskiy2020local} to backpropagate and update the server model parameters.
% %然后将损失值发送给被动方
% The server sent the loss $\mathcal{L}$ to the passive party.




% \textit{local Model Optimization.}
% %被动方根据主动方发送的损失值$\mathcal{L}$来计算得到其梯度值为$\frac{\partial \mathcal{L}}{\partial E_k}$来更新底层模型。
% The passive party computes the gradient value \(\textit{g} = \frac{\partial \mathcal{L}}{\partial E_k}\) and updates the local model using the loss value \(\mathcal{L}\) provided by the active party.
% However, the gradient \( g \) contains only the knowledge of the current task and lacks information from previous tasks. To enable the passive party to retain prior knowledge during model training, we have adopted a method that involves fixing the parameters critical to previous tasks, thereby preserving those model parameters that are particularly significant to the previous tasks.
% %然而, 梯度g中只有当前任务的知识，而不拥有先前任务的知识。
% %为了使被动方在模型训练时保留旧知识信息，我们采用旧任务重要参数固定的方法来保留对于旧任务相对重要的模型参数。
% %特殊地，我们通过计算模型的Fisher Information Matrix来衡量每个参数对旧任务的重要性。
% Specifically, we estimate the importance of each parameter to the previous tasks by calculating the Fisher Information Matrix (FIM) \cite{yang2023dynamic} as the Equation (\ref{eq: AF}).

% \begin{equation} \label{eq: AF}
%     \mathcal{F}_{ki} \approx \frac{1}{N} \sum_{i=1}^{N}\left(\nabla_{\theta_{k}} \mathcal{L}\left(x_{i}, y_{i}\right)\right)^{2},
% \end{equation}
% where $\mathcal{F}_{ki}$ denote the FIM of $\mathcal{B}$ with the previous tasks, $k$ denote the $k$th passive party, $i$ denote the number of sample.

% Additionally, we obtain a threshold $\kappa$ by calculating the mean and standard deviation of the FIM, which is then used to select the important model parameters.
% \begin{equation} \label{eq: thre}
%     \kappa = \frac{1}{N} \sum_{i=1}^N \mathcal{F}_{ki} - \delta \cdot \sqrt{\frac{1}{N} \sum_{i=1}^N (\mathcal{F}_{ki} - \frac{1}{N} \sum_{i=1}^N \mathcal{F}_{ki})^2},
% \end{equation}
% where $\delta = k_0 + \alpha \cdot \log(t + 1)$ is a hyperparameter; a larger \(\delta\) value increases the emphasis on previous tasks.

% We select the relatively important model parameters for previous tasks based on a defined threshold $\kappa$. 
% The important parameters are then fixed to prevent interference from new tasks, thus preserving the foundational model’s retention of knowledge from prior tasks and mitigating forgetting. 
% \begin{equation} \label{eq: local_update}
%     \mathcal{B}_{ik}^t=\left\{\begin{array}{ll}
% \mathcal{B}_{ik}^{t-1}, & \mathcal{F}_{ki} \ge \kappa \\
% \text{Update} \mathcal{B}_{ki}^{t-1}, & \text {otherwise},
% \end{array}\right.
% \end{equation}
% % where $i$ denotes $i$th local model parameter. 
% where $i$ represents the local model owned by the $i$th passive party.

% %最后，我们根据阈值选择出对旧任务相对重要的模型参数，并固定重要的模型参数使其不受新任务的影响，进而保证底层模型不遗忘旧任务的知识。
% %FIM的均值和方差分别为

% %此外，我们通过计算Fisher信息的均值和标准差来获得阈值进而筛选重要的模型参数。
% % 通常计算Fisher Information Matrix (FIM) \cite{Dynamic personalized federated learning with adaptive differential privacy} Fisher信息矩阵的对角线元素来衡量每个参数对旧任务的重要性。根据 Fisher 信息矩阵的定义，其对角元素\mathcal{F}可以近似为每个参数在对数似然上的梯度平方的期望.
% % The Fisher Information Matrix (FIM) is typically calculated by considering only its diagonal elements, which estimate the importance of each parameter with the previous task \cite{yang2023dynamic}.
% % According to the definition of the FIM, its diagonal elements, $\mathcal{F}$ can be approximated as the expected value of the squared gradient of each parameter for the log-likelihood.
% % \begin{equation} \label{eq: TF}
% %     \mathcal{F}_{k} \approx \mathbb{E}\left[\left(\nabla_{\theta_{k}} \log p(y \mid x ; \theta)\right)^{2}\right]
% % \end{equation}

% %然而，在实际应用场景中，我们无法确切知道真实的数据分布。因此，我们通常使用训练样本的梯度平方来近似这个期望。也就是说，对于模型的每一个参数的Fisher信息值可以近似为下式。
% % However, we do not have precise knowledge of the true data distribution in practical scenarios. 
% % Therefore, we typically approximate this expectation using the squared gradients of the training samples. 
% % The fisher information value for each parameter in the model can be approximated as the Equation (\ref{eq: AF}).
% % \begin{equation} \label{eq: AF}
% %     \mathcal{F}_{ki} \approx \frac{1}{N} \sum_{i=1}^{N}\left(\nabla_{\theta_{k}} \mathcal{L}\left(x_{i}, y_{i}\right)\right)^{2}
% % \end{equation}

% %




