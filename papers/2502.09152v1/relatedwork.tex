
% auto-ignore
\section{Related Works} \label{sec: relatedwork}
%\subsection{Continual Learning}
% Continual Learning (CL) aims to address catastrophic forgetting of the previous knowledge after training on the new task \cite{wuerkaixi2024accurate, ma2022continual}.
% \noindent\textbf{Continual Learning} (CL). CL aims to solve the issue of catastrophic forgetting, where a model loses previous knowledge after training for new tasks.
% % \cite{wuerkaixi2024accurate,ma2022continual,wang2024comprehensive}
% % Existing CL research can be categorized into four methods.
% Existing CL research can be divided into three primary categories.
% % (1) \textit{Parameter regularization.} 
% %Data reply的主要思想是通过存储旧任务的原始数据集来防止灾难性遗忘。
% %例如，The work\cite{maracani2021recall}提出重放缓冲区存储原始数据来同时解决了灾难性遗忘和噪声标签问题
% %然而，直接存储原始数据不仅会造成隐私泄露。
% % \cite{maracani2021recall,kim2021continual}
% (1) \textit{Data reply}. 
% The primary idea of data replay is to prevent catastrophic forgetting by storing the original datasets of previous tasks. 
% For example, the work by Maracani et al. \cite{maracani2021recall} proposes a replay buffer that stores original data to simultaneously address catastrophic forgetting and noisy labels. 
% However, directly storing original data raises privacy concerns.
% (2) \textit{Parameter separation} \cite{wang2024traceable}. 
% %参数分离通过将客户端的本地模型划分为不同的子集处理不同的任务，解决灾难性遗忘问题。
% %例如，在联邦学习的场景中，wang \cite{wang2024traceable}通过将整个模型分解为一系列标记的子模型，精准的定位重复任务优化每个客户端的任务，提高模型性能。
% %然而，划分后的子模型需要占用较大的存储空间，造成较大的训练开销，模型训练效率低下。
% Parameter separation overcomes catastrophic forgetting by dividing a client’s local model into separate subsets, with each model subset handling a specific task. 
% For example, TagFed \cite{wang2024traceable} decomposes the model into labeled sub-models, enabling precise task optimization and improved performance. 
% However, the parameter separation method requires substantial storage for the sub-models subsets, leading to increased training costs and reduced efficiency.
% (3) \textit{Knowledge distillation} \cite{li2023variational,shenaj2023asynchronous}.
% %基于知识蒸馏的方法主要是通过惩罚重要参数或者添加目标损失来限制更新过程进而保留旧模型中学习的知识。
% %例如，FedProK \cite{gao2024fedprok} and AFCL \cite{shenaj2023asynchronous} 均是通过原型知识蒸馏近似保留旧任务的原型，进而解决灾难性遗忘问题。
% %然而，随着域变化较大时，根据新任务数据近似蒸馏旧任务方法会造成性能的显著下降。
% Knowledge distillation-based methods aim to retain knowledge from the previous model by penalizing critical parameters or adding target losses by constraining the update process.
% For example, FedProK \cite{gao2024fedprok} and AFCL \cite{shenaj2023asynchronous} use prototype knowledge distillation to approximate the retention of previous task prototypes, thereby addressing catastrophic forgetting.
% However, when domain shifts are significant, approximating old tasks using new task data for distillation can substantially drop performance.
% In our work, we are devoted to extending traditional CL under the vertical federation scenario and propose a new paradigm, namely vertical federated continual learning for image classification, to address catastrophic forgetting when training for new classification tasks.
% %持续学习旨在解决先前知识的灾难性遗忘问题(catastrophic forgetting)。\cite{wuerkaixi2024accurate, ma2022continual}
% %多种持续学习的方法提出来解决灾难性遗忘和实现任务间的知识转移。现有的CL方法主要分为四类：regularization，数据重放、parameter separation、and knowledge distillation。

% %现有的CL方法主要分为三类：data replay数据重放--存储旧任务的原始数据来防止灾难性遗忘，然而，直接接触到原始数据会造成数据隐私泄露
% %Parameter separation--参数分离通过使用本地模型参数的不同子集来处理每个任务，客服灾难性遗忘问题
% %\cite{wang2024traceable}
% %随着新任务的到来，分离方法将导致参数无限增加，这在 FL 中很快就会变得不可接受。
% %基于正则化的方法通过惩罚重要参数的更新或添加知识蒸馏目标函数损失来限制更新过程从旧模型中学习知识。然而，其重要性很难准确评估。一些基于蒸馏的方法根据新的任务数据进行蒸馏，但当域变化很大时，其效率会显着下降。其他利用未标记的外部数据的方法解决了上述困难。 CFeD采用公共数据集的知识蒸馏，并提出客户端划分机制，以减少时间和计算成本。
% %知识蒸馏

% %原型的持续学习是针对当前未出现的特征采用原型增强来补全
% %假设所有的样本标签均是存在的。
% %对比学习是通过将相同类的距离越来越近，不同类之间的距离越来越远。
% % Asynchronous Federated Continual Learning
% %FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer
% %这两篇文章均是采用原型增强技术来实现持续学习
% %Traceable Federated Continual Learning
% %采用参数分离的方式，对于每个任务采用部分的模型参数来表示。

% %\subsection{Federated Continual Learning}
% %最近相关研究考虑将持续学习引入到联邦学习并提出了一种新的范式Federated continual Learning(FCL). FCL旨在融合多客户端异构数据知识并在学习新任务的同时保留先前任务的知识。
% \noindent\textbf{Federated Continual Learning} (FCL). 
%Recent studies propose a novel paradigm called Federated Continual Learning (FCL) \cite{yang2024federated}, which aims to learn heterogeneous data knowledge from multiple clients and retain knowledge acquired from previous tasks while learning new tasks.
Our work is closely related to \textit{Federated Continual Learning} (FCL) methods that can be classified into two categories. 
%, which aim to address the CF problem. 
% These methods can be broadly categorized into three types.
%These methods can be classified into two categories.

\noindent\textbf{Parameter decomposition-based FCL.}
% \textbf{}
Most existing methods retain previous task knowledge while adopting the model to new tasks \cite{yang2024federated}. 
For example, TagFed \cite{wang2024traceable} decomposes the entire model into a series of labeled independent models to optimize each client’s task; FedWeIT \cite{yoon2021federated} mitigates temporal catastrophic forgetting by dividing the global model parameters into sparse task-specific parameters through parameter sparsification; Cross-FCL \cite{zhang2022cross} utilizes a parameter-decomposition-based FCL model that retains previously acquired knowledge while learning new tasks. 
We find that most methods highly rely on complete client models, so that it brings the communication overhead issue and makes them unsuitable for VFCL. 

%Most parameter decomposition-based FCL methods  \cite{wang2024traceable,zhang2022cross,yoon2021federated} aim to retain prior task knowledge while adapting the model to new tasks.
%For example, TagFed \cite{wang2024traceable} decomposes the entire model into a series of labeled independent models to optimize each client’s task and improve the performance of the training model.
%FedWeIT \cite{yoon2021federated} mitigates temporal catastrophic forgetting by dividing the global model parameters into sparse task-specific parameters through parameter sparsification. 
%尽管他们能够有效的解决FCL面临的灾难性遗忘问题，但是他们依赖于客户端拥有完整的模型并且需要额外的通信开销，不适用于VFCL的场景。在本文中，我们将借鉴上述方法的思想用于解决VFCL中被动方拥有本地模型所面临的灾难性遗忘问题
% While these methods effectively tackle catastrophic forgetting in FCL, they rely on complete client models and incur additional communication overhead, making them unsuitable for VFCL. 
%These methods address CF in FCL but rely on complete client models and introduce communication overhead, making them unsuitable for VFCL.
%This paper adapts their principles to address catastrophic forgetting in the passive party's local model in VFCL.


\noindent\textbf{Prototype fusion-based FCL.}
Most existing methods primarily rely on clients locally constructing class prototypes, with the server facilitating inter-class knowledge transfer based on these prototypes. 
Knowledge distillation \cite{ma2022continual} is used to propagate the enhanced prototypes from the server across clients.
For example, Pass \cite{zhu2021prototype} is a prototype-based technique designed for enhancing prototypes within the feature space to maintain decision boundaries for prior tasks. 
FedSpace \cite{shenaj2023asynchronous} argued that prototype distillation, representation loss, fractal pre-training, and modified aggregation could be used for improving global model performance. 
FedProK \cite{gao2024fedprok} has explored prototype features as a knowledge representation mechanism and utilized prototype fusion for spatial-temporal knowledge transfer.
FedCBC \cite{yu2024overcoming} employ the global model as a teacher model to perform knowledge distillation on the local model to improve model learning across clients.
% FedCBC \cite{yu2024overcoming} transfers prior knowledge from the global model to multiple local models and employs knowledge fusion techniques to improve learning across clients.
%同时解决先前任务灾难性遗忘问题和客户端异构问题simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients
%Most prototype fusion-based FCL methods 
% 现有的大多数基于原型融合的FCL方法是通过客户端在本地构建类原型，服务器根据类原型实现类间知识传递，最后客户端根据服务器的增强原型采用知识蒸馏实现客户端间知识传递。Existing research proposes knowledge transfer and prototype fusion methods.
%\cite{zhu2021prototype,shenaj2023asynchronous,gao2024fedprok} primarily relies on clients locally constructing class prototypes, with the server facilitating inter-class knowledge transfer based on these prototypes. 
%Knowledge distillation \cite{ma2022continual} is then used to propagate the enhanced prototypes from the server across clients.
%Specifically, Pass \cite{zhu2021prototype} proposes a prototype-based technique that preserves knowledge from previous task classes, enhancing prototypes within the feature space to maintain decision boundaries for prior tasks.
% enhancing prototypes within the feature space to maintain decision boundaries for prior tasks.
% FedCBC \cite{yu2024overcoming} transfers prior knowledge from the global model to multiple local models and employs knowledge fusion techniques to improve learning across clients.
% we employ the global model as a teacher model to perform knowledge distillation on the local model.
%FedSpace \cite{shenaj2023asynchronous} improves global model performance by utilizing prototype distillation, representation loss, fractal pre-training, and modified aggregation strategies.
% fractal pre-training, and modified aggregation strategies.
% FedProK \cite{gao2024fedprok} employs prototype features as a knowledge representation mechanism and utilizes prototype fusion for spatiotemporal knowledge transfer.
% knowledge representation mechanism and utilizes prototype fusion for spatiotemporal knowledge transfer.
However, the above existing prototype fusion-based methods assume that each client has access to complete sample labels and models, suitable only for Horizontal FCL.
% Most parameter decomposition-based FCL methods propose local parameter decomposition techniques, which aim to retain prior task knowledge while adapting the model to new tasks.



Our investigation finds that the catastrophic forgetting issue in VFCL has been rarely addressed by prior work. 
The technical difficulties derive from the fact that  clients only have partial features and models, and lack access to labels (except the active party), hindering to generate prototypes and update local models independently.
In this paper we focus on this issue, i.e., solving the issue of class and feature catastrophic forgetting in VFCL.

% 尽管上述方法有效的解决了FCL的灾难性遗忘问题，但是失败的同时解决VFCL灾难性遗忘问题，as clients only have partial features and models, with no access to labels (except for the active party), preventing independent prototype generation and model updates.
%在本文中，我们聚焦于解决VFCL面临的类灾难性遗忘问题和特征灾难性遗忘问题。
% Therefore, although the aforementioned methods effectively address the CF problem in FCL, they fail to solve the catastrophic forgetting issue in VFCL. 
% % This is due to the fact that clients only have partial features and models, without access to labels (except for the active party), which prevents them from independently generating prototypes and updating local models.
% This is because clients only have partial features and models, and lack access to labels (except the active party), hindering their ability to generate prototypes and update local models independently.
% In this paper, we focus on solving the class and feature catastrophic forgetting problems encountered in VFCL.
% This paper adapts their principles to address catastrophic forgetting in the passive party's local model in VFCL.
% Although these methods support a 
% fail to address catastrophic forgetting in VFCL, as clients only have partial features and models, with no access to labels (except for the active party), preventing independent prototype generation and model updates.
% To the best of our knowledge, there are rarely studies about VFCL, with catastrophic forgetting remaining a significant challenge in VFL.

% Our work is closely related to 
% %我们的工作与现有联邦持续学习方法是紧密相关的。这些方法用于解决灾难性遗忘问题主要分为下面的三类。
% %用于解决类灾难性遗忘的
% {\em Federated Continual Learning} (FCL) is a technical path to learn heterogeneous data knowledge from multiple clients and retain knowledge acquired from previous tasks while learning current tasks \cite{yang2024federated}.
% %联邦持续学习
% % {\color{red}
% %按照数据划分和训练方式不同，现有的联邦持续学习研究主要分为横向联邦持续学习和纵向联邦持续学习。
% Specifically, FCL mainly faces \textit{Temporal Catastrophic Forgetting} (TCF) and \textit{Spatial Catastrophic Forgetting} (SCF) challenges. 
% %具体地，FCL主要面临Temporal Catastrophic Forgetting(TCF)和\textit{Spatial Catastrophic Forgetting} (SCF)
% %按照数据划分和训练方式不同，FCL分为HFCL和VFCL两种类型，现有研究主要集中于解决HFCL中的灾难性遗忘问题。
% %为了解决HFCL中客户端的本地模型的关键参数在新任务上更新变化而导致先前任务的性能较差的问题(Temporal catastrophic forgetting)，现有工作\cite{wang2024traceable,zhang2022cross,yoon2021federated}提出了本地参数分解方法优化本地模型在更新新任务时仍能保留先前任务的知识 。
% %然而，由于本地参数分解专注于解决客户端本地模型持续学习新任务的同时保留先前任务的知识，现有的本地参数分解方法无法有效的解决在本地数据异构的情况下全局模型性能低于本地模型的挑战，即Spatial catastrophic forgetting (SCF)问题。
% % 为了解决SCF问题，相关研究\cite{ma2022continual}提出了知识蒸馏的方法来为不同的客户端分配不同的学习目标，缩短全局模型和本地模型之间的距离，进而在客户端数据异构的情况下提升全局模型性能。
% % 然而，上述知识蒸馏的方法未考虑全局模型和本地模型在学习新任务时会遗忘先前任务知识，无法有效解决TCF问题。
% FCL is typically categorized into two types: \textit{Horizontal Federated Continual Learning} (HFCL) and VFCL, with much of the research focusing on addressing catastrophic forgetting in HFCL.
% % \textit{Temporal Catastrophic Forgetting} (TCF) arises when updates to key local model parameters result in degraded performance on prior tasks. 
% To mitigate TCF of HFCL, existing methods \cite{wang2024traceable,zhang2022cross,yoon2021federated} propose local parameter decomposition techniques, which aim to retain prior task knowledge while adapting the model to new tasks.
% % However, while local parameter decomposition primarily addresses TCF by preserving prior task knowledge in the local model during new task learning, it does not tackle the SCF challenge. 
% % SCF occurs when the global model performs poorly compared to local models due to heterogeneous client data.
% However, existing local parameter decomposition methods address TCF by preserving prior task knowledge in the local model but do not resolve SCF, which occurs when the global model underperforms due to heterogeneous client data.
% % To address SCF of HFCL, knowledge distillation methods have been proposed in some studies \cite{ma2022continual}, which assign distinct learning targets to each client, reducing the performance gap between the global and local models, thus improving the global model’s accuracy under heterogeneous data conditions.
% To address SCF in HFCL, some studies \cite{ma2022continual} propose knowledge distillation, assigning distinct learning targets to each client to reduce the performance gap between global and local models, thereby improving global accuracy under heterogeneous data.
% % However, these methods do not account for the potential for both global and local models to forget prior task knowledge during new task learning, ultimately failing to effectively address the TCF issue.
% % However, the above methods fail to account for the forgetting of prior task knowledge in both the global and local models, thus inadequately addressing the TCF issue.
% However, these methods do not address the loss of prior task knowledge in both global and local models, thus inadequately solving the TCF issue.

% %为了同时解决HFCL面临的TCF和SCF问题，现有研究提出了知识转移和原型融合方法。
% % 具体地，
% % Pass \cite{zhu2021prototype} 提出了一种类原型保存先前任务旧类的知识和在特征空间中采用原型增强来维护先前任务的决策边界。
% % FedCBC \cite{yu2024overcoming} 采用先前知识从全局模型转移到多个本地模型和选择知识融合技术。
% % FedSpace \cite{shenaj2023asynchronous}采用基于原型蒸馏、表示损失、分形预训练和修改后的聚合策略来提高全局模型的性能。
% % FedProK \cite{gao2024fedprok} 利用原型特征作为知识表现和原型融合来实现时空知识转移。
% %上述方法假设每个客户端拥有完整的样本标签和模型，仅适应于HFCL。
% % 然而，在VFL中，各个客户端仅拥有部分的样本特征和部分模型，除了主动方以为其他参与方不拥有标签值，VFL的客户端无法独立的生成原型和更新本地模型。因此上述方法失败的同时解决VFCL所面临的灾难性遗忘问题。

% % To address the TCF and SCF challenges in HFCL, existing research has proposed methods based on knowledge transfer and prototype fusion. 
% % {\color{red}
% To simultaneously address the TCF and SCF challenges in HFCL, existing research proposes knowledge transfer and prototype fusion methods.
% Specifically, Pass \cite{zhu2021prototype} proposes a prototype-based technique that preserves knowledge from previous task classes, enhancing prototypes within the feature space to maintain decision boundaries for prior tasks.
% %
% % FedCBC \cite{yu2024overcoming} transfers prior knowledge from the global model to multiple local models and employs knowledge fusion techniques to improve learning across clients.
% FedSpace \cite{shenaj2023asynchronous} improves global model performance by utilizing prototype distillation, representation loss, fractal pre-training, and modified aggregation strategies.
% % 
% FedProK \cite{gao2024fedprok} employs prototype features as a knowledge representation mechanism and utilizes prototype fusion for spatiotemporal knowledge transfer.
% %
% These methods assume that each client has access to complete sample labels and models, making them suitable only for HFCL.
% % However, in VFCL, each client only has access to partial sample features and models, with all participants (except for the active party) lacking access to label values. 
% % Consequently, clients in VFCL cannot independently generate prototypes or update their local models. As a result, the aforementioned methods do not effectively resolve the catastrophic forgetting problem in VFCL.
% However, the aforementioned methods fail to address catastrophic forgetting in VFCL, as clients only have partial features and models, with no access to labels (except for the active party), preventing independent prototype generation and model updates.
% To the best of our knowledge, there are rarely studies about VFCL, with catastrophic forgetting remaining a significant challenge in VFL.
% }

% 为了解决SCF问题，
% 此外，在FCL中，当各个客户端的本地数据是异构情况下，服务器聚合得到的全局模型的表现可能比本地模型差，这就导致了Spatial catastrophic forgetting (SCF)。
% \textit{(2) Spatial catastrophic forgetting (SCF)}. 
%SCF指的是在客户端的本地拥有异构数据的情况下，服务器聚合的全局模型在本地测试集上的表现可能比本地模型差。现有的研究主要采用知识蒸馏的技术来解决SCF问题。CFedD \cite{ma2022continual} 采用知识蒸馏技术为不同的客户端分配不同的学习目标,缩写全局模型和局部模型之间的距离。

% SCF refers to a scenario where the globally aggregated model underperforms on local test sets compared to locally trained models, particularly in the context of heterogeneous data across clients.

% 例如，Cross-FCL \cite{zhang2022cross}采用参数分解的FCL模型在参与新任务训练时仍能保留过去学到的知识。FedWeIT \cite{yoon2021federated} 采用了参数稀疏化技术划分了全局模型参数和稀疏特定任务参数来解决时序灾难性遗忘问题。TagFed \cite{wang2024traceable} 通过将整个模型分解为一系列标记的子模型来优化每个客户端任务进而解决空间灾难性问题。
% Existing FCL methods basically can be classified into three categories in terms of target problems. 
%根据所解决的问题不同，现有的FCL主要分为三类。
%Based on the different problems, existing FCL approaches can be classified into three categories.
% (1) Temporal catastrophic forgetting (TCF)。TCF主要指的是在连续任务上学习时单个模型的关键参数发生变化，导致之前任务的性能较差。为了解决TCF问题，现有研究主要采用参数稀疏化来解决TCF问题。例如，Cross-FCL \cite{zhang2022cross}采用参数分解的FCL模型在参与新任务训练时仍能保留过去学到的知识。FedWeIT \cite{yoon2021federated} 采用了参数稀疏化技术划分了全局模型参数和稀疏特定任务参数来解决时序灾难性遗忘问题。TagFed \cite{wang2024traceable} 通过将整个模型分解为一系列标记的子模型来优化每个客户端任务进而解决空间灾难性问题。
% \textit{(1) Temporal Catastrophic Forgetting (TCF)}. 
% (1) Temporal catastrophic forgetting (TCF)。TCF主要指的是在连续任务上学习时单个模型的关键参数发生变化，导致之前任务的性能较差。为了解决TCF问题，现有研究主要采用参数稀疏化来解决TCF问题。例如，Cross-FCL \cite{zhang2022cross}采用参数分解的FCL模型在参与新任务训练时仍能保留过去学到的知识。FedWeIT \cite{yoon2021federated} 采用了参数稀疏化技术划分了全局模型参数和稀疏特定任务参数来解决时序灾难性遗忘问题。TagFed \cite{wang2024traceable} 通过将整个模型分解为一系列标记的子模型来优化每个客户端任务进而解决空间灾难性问题。
% TCF refers to the deterioration in performance on previous tasks when a single model's critical parameters change during the learning of consecutive tasks. 
% The first issue is the \textit{Temporal Catastrophic Forgetting} (TCF) that is mostly explored by parameter sparsification-based techniques \cite{yoon2021federated,zhang2022cross,wang2024traceable}. 
% %To address the \textit{Temporal Catastrophic Forgetting} (TCF) challenge, existing studies \cite{yoon2021federated,zhang2022cross,wang2024traceable} primarily employ parameter sparsification techniques. 
% For instance, FedWeIT \cite{yoon2021federated} mitigates temporal catastrophic forgetting by dividing the global model parameters into sparse task-specific parameters through parameter sparsification. 
% Similarly, Cross-FCL \cite{zhang2022cross} utilizes a parameter-decomposition-based FCL model that retains previously acquired knowledge while learning new tasks. 
% TagFed \cite{wang2024traceable} decomposes the entire model into a series of labeled sub-models to optimize each client’s task and improve the performance of the training model.



% The next issue is the \textit{Spatial Catastrophic Forgetting} (SCF) \cite{ma2022continual,shenaj2023asynchronous}.
% %To tackle the \textit{Spatial Catastrophic Forgetting} (SCF) issue, existing methods \cite{ma2022continual,shenaj2023asynchronous} primarily utilize knowledge distillation techniques. 
% For example, CFedD \cite{ma2022continual} employs knowledge distillation to assign distinct learning objectives to different clients, thereby minimizing the divergence between the global and local models. 
% FedSpace \cite{shenaj2023asynchronous} integrates prototype distillation, representation loss, fractal pre-training, and a modified aggregation strategy to improve global model performance.
% % \textit{(3) Spatial-Temporal catastrophic forgetting (STCF)}.
% %STCF指的是在FL训练的过程时需要同时解决空间和时序带来的灾难性遗忘问题。
% %现有研究主要采用知识转移和原型融合的技术来解决STCF。
% % FedCBC \cite{yu2024overcoming} 采用先前知识从全局模型转移到多个本地模型和选择知识融合技术。
% % FedProK \cite{gao2024fedprok} 利用原型特征作为知识表现和原型融合来实现时空知识转移。
% % STCF refers to addressing problems with both TCF and SCF simultaneously during FL training.
% Furthermore, to address both TCF and SCF challenges in FL simultaneously, existing research \cite{yu2024overcoming,gao2024fedprok,zhu2021prototype} primarily adopts techniques such as knowledge transfer and prototype fusion to mitigate \textit{Spatial-Temporal catastrophic forgetting} (STCF).
% For example, FedCBC \cite{yu2024overcoming} transfers prior knowledge from the global model to multiple local models and employs knowledge fusion techniques. 
% Similarly, FedProK \cite{gao2024fedprok} utilizes prototype features as a representation of knowledge and implements prototype fusion to improve model performance.
% %纵向联邦学习在训练的过程只有主动方
% %然而，据我们所知，目前鲜有纵向持续联邦学习的相关研究解决空间灾难性遗忘问题
% %现有关于FCL的研究主要集中在横向联邦学习。由于现有的FCL通常依赖于全局模型的联合优化而无法适用于以特征联合的纵向持续联邦学习中。
% Existing FCL research primarily focuses on \textit{Horizontal Federated Learning} (HFL) \cite{ye2023heterogeneous}. 
% However, current FCL approaches often rely on global model optimization, making them incompatible with the feature-partitioned nature of VFL \cite{liu2024vertical}.
% To the best of our knowledge, there are rarely studies about VCL, with spatial-temporal catastrophic forgetting remaining a significant challenge in VFL.
% However, in the real-world scenario, VFL is worth investigating.
%
% However, to the best of our knowledge, there are few studies on VCFL that address the  of spatial catastrophic forgetting.

% To the best of our knowledge, existing research in VFL and CL remains limited, particularly in effectively mitigating the challenge of spatial-temporal catastrophic forgetting.
