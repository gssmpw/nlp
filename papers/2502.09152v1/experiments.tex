
% auto-ignore




\section{Experiments} \label{sec: exper}







\noindent\textbf{Datasets.}
%我们依据已有的纵向联邦学习方案评估数据集，在四种数据集中评估了我们的模型性能，四种数据集分别是
% To evaluate the performance of our method,
We construct one using four datasets widely used for the VFL  \cite{das2021multi,wang2024unified}, which are MNIST \cite{baldominos2019survey}, FashionMNIST \cite{xiao2017fashion}, CIFAR-10 \cite{abouelnaga2016cifar} and CINIC10 \cite{NEURIPS2021_2f2b2656}
% \textbf{(1) MNIST} \cite{baldominos2019survey} comprises images of handwritten digits, including the numbers 0 to 9.
% \textbf{(2) FashionMNIST} \cite{xiao2017fashion} is an image classification dataset similar to MNIST, but it contains grayscale images of fashion items. 
% \textbf{(3) CIFAR-10} \cite{abouelnaga2016cifar} consists of color images divided into 10 distinct object categories, with 50,000 training samples and 10,000 testing samples, measuring 32x32 pixels each.
% \textbf{(4) CINIC10} \cite{NEURIPS2021_2f2b2656} is an extended version of the CIFAR10 dataset, created by augmenting it with ImageNet images.
% \textbf{(5) Adult datasets}
% ISIC 2019
% \textbf{(4) CIFAR-100} \cite{zheng2024comparative} consists of 60,000 color images (32×32 pixels), divided into 100 classes. Each class contains 600 images, with 500 used for training and 100 for testing.
We partitioned image dataset features vertically into $N$ parties for $N$ participants in VFL, the same as existing work \cite{qiu2024integer}.
% wang2023tvfl,fu2022label
% For MNIST and FashionMNIST datasets, we utilized a CNN-split model with two convolutional layers followed by two fully connected layers.
We adopted a deeper CNN model with three convolutional layers and three fully connected layers for all the datasets.









\noindent\textbf{Baselines.}
%据我们所知，针对纵向联邦持续学习的研究很少，大多数基线实现的是横向联邦持续学习。
%横向持续学习主要解决的是类持续学习，而不涉及特征持续学习。
%因此，针对类增强持续学习，我们选择用于横向联邦持续学习迁移到纵向联邦学习作为基线方法，包括Standalone, Pyvertical\cite{romanini2021pyvertical},Pass\cite{}+VFL, FedSpace+VFL, FedProK+VFL。它们是经典的纵向联邦学习方法和联邦持续学习设计的最先进的方法。
%此外，针对特征增强学习，我们选择Pyvertical，不同任务单独模型训练和全部特征模型训练作为基线方法来比较我们方法的有效性。我们在附件中提供了基线的详细描述。
To the best of our knowledge, most existing schemes focused on the HFCL and very rare work has addressed VFCL, such that the evaluation methods were varied due to different training mechanisms for these two types of FCL. 
% Since HFCL primarily addresses CIL rather than involving FIL,
% We adopted a few the state-of-the art methods that were originally designed for HFCL to evaluate V-LETO.
We modified state-of-the-art methods designed for HFCL to adapt them for VFCL, using these modified methods as baselines to evaluate the performance of our proposed approach, V-LETO.
% We modified state-of-the-art methods designed for HFCL to adapt them for VFCL, using these modified methods as baselines to evaluate the performance of our proposed approach, V-LETO.
% 我们改进了现有最先进的适用于解决HFCL的方法使其适用于VFCL来并baseline进而评估我们的方法V-LETO。
% The implemented baselines in our evaluations included Standalone, PyVertical \cite{romanini2021pyvertical}, Pass \cite{zhu2021prototype} + VFL, FedSpace \cite{shenaj2023asynchronous} + VFL, and FedProK \cite{gao2024fedprok} + VFL, which were deemed representative VFL and FCL methods. 
The baselines implemented in our evaluations included Standalone, PyVertical \cite{romanini2021pyvertical}, Pass \cite{zhu2021prototype} + VFL, FedSpace \cite{shenaj2023asynchronous} + VFL, and FedProK \cite{gao2024fedprok} + VFL, all of which were selected as representative VFL methods and state-of-the-art HFCL methods.
%为了公平清晰的评估本方案的性能结果，我们与基线方法评估了本方法的CIL和FIL的性能。
To ensure a fair and clear evaluation of our method, we compared the CIL and FIL performance of V-LETO with those of the baseline methods.
% In addition, to evaluate feature-enhanced continual learning, we compared our scheme with PyVertical \cite{romanini2021pyvertical}. 
Additionally, we employed independent model training for distinct tasks and a fall-feature model training as the baseline method to evaluate the effectiveness of our approach. 
% Details of these baselines were provided in supplementary materials.







\noindent\textbf{Implementation Details.}
%我们使用pytorch在python中实现所有的实验，服务器使用的是有1个 NVIDIA GeForce RTX 3090, CUDA Version的是12.5。此外，我们设置总共的任务数为4，每个任务具有一部分新增的类或者特征。我们设置被动方的数量为4，每个被动方拥有一部分的特征。我们使用SGD作为模型训练的优化器，学习率设置为1e-3。超参数\beta，\lambda_F，\lambda_A均设置为0.5，k_0设置为30 \alpha设置为5。指的注意是是，为了公平的比较各方法的性能，所有的实验均在相同的超参数下进行。
% Our evaluations were implemented in Python on PyTorch and the server equipped an NVIDIA GeForce RTX 3090 GPU with CUDA Version 12.5. 
Evaluations were performed in Python on PyTorch, using a server with an NVIDIA GeForce RTX 3090 GPU and CUDA 12.5.
The number of total tasks was configured to 4 and each task contains a set of new classes or features; the number of passive participants was configured to 4 and each participant holds a subset of features. 
We adopted a \textit{Stochastic Gradient Descent} (SGD) as the optimizer of modal training with a learning rate of 1e-3.
The hyperparameters $\beta$, $\lambda_F$, and $\lambda_A$ were all set to 0.5; $k_0$ was set to 15; $\alpha$ was set to 3. 
% To achieve a fair comparison, all experiments were conducted by adopting the same hyper-parameters for evaluating distinct methods' performance. 
To achieve a fair comparison, all experiments were conducted under the same configuration to evaluate the performance of different methods.


% All experiments were implemented in Python using PyTorch, and conducted on a server equipped with an NVIDIA GeForce RTX 3090 GPU and CUDA Version 12.5. 
% The total number of tasks was set to 4, with each task containing a set of new classes or features. We configured 4 passive participants, each possessing a subset of features. Stochastic Gradient Descent (SGD) was used as the optimizer for model training, with a learning rate of 1e-3. The hyperparameters $\beta$, $\lambda_F$, and $\lambda_A$ were all set to 0.5, while $k_0$ was set to 30 and $\alpha$ to 5. 
% It is important to note that, for a fair comparison of the performance of different methods, all experiments were conducted under the same experiment settings.
\begin{figure*}[!t] 
	\begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_class_1.pdf}}
		\centerline{(a) Task1}
	\end{minipage}
    \begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_class_2.pdf}}
		\centerline{(b) Task2}
	\end{minipage}
	\begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_class_3.pdf}}
		\centerline{(c) Task3}
	\end{minipage}
    \begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_Class_4.pdf}}
		\centerline{(d) Task4}
	\end{minipage}

    \begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_Feature_0.pdf}}
		\centerline{(e) Task1}
	\end{minipage}
    \begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_Feature_2.pdf}}
		\centerline{(f) Task2}
	\end{minipage}
	\begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_Feature_1.pdf}}
		\centerline{(g) Task3}
	\end{minipage}
    \begin{minipage}{0.24\linewidth}
		\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Prototype/F_Feature_3.pdf}}
		\centerline{(h) Task4}
	\end{minipage}
\caption{t-SNE visualization of the global embedding and prototypes produced by CIL (a)(b)(c)(d) and FIL (e)(f)(g)(h) in V-LETO on FMNIST datasets.
Colored circles represent global embeddings, and black squares represent prototypes of samples from different classes.}
	\label{Fig: Vis_CIL}
\end{figure*}


\begin{table*}[!t]
\centering
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{ccc|c|c|ccc|cccc|ccccc}
\toprule
% Datasets & & & & \multicolumn{5}{c|}{FMNIST}& \multicolumn{5}{c}{CINIC10} \\
% \midrule
\multirow{2}{*}{$\mathcal{L}_{CE}$} & \multirow{2}{*}{$\mathcal{L}_{A}$} & \multirow{2}{*}{LMO}&Training task& task1& \multicolumn{3}{c|} {task2} & \multicolumn{4}{c|}{task3} & \multicolumn{5}{c}{task4}\\
% \cmidrule{4-17}
& & & Testing task & task1 & task1 & task2 & task(12) & task1 & task2 & task3 & task(123)& task1 & task2 & task3 & task4 & task(1234)\\
\midrule
$\times$ & $\checkmark$ & $\checkmark$ &V-LETO w/o $\mathcal{L}_{CE}$ &  93.82 & 63.54&13.59&41.87 & 93.20&61.95&16.01&57.50 & 78.59&75.53&39.92&0.00&47.57 \\
$\checkmark$ & $\times$ & $\checkmark$ &V-LETO w/o $\mathcal{L}_{A}$ &  93.82 &41.64&85.62&57.96 & 0.00&42.26&85.40&44.21 & 0.00&0.00&46.40&91.87&36.25 \\
$\checkmark$ & $\checkmark$ & $\times$ &V-LETO w/o LMO & 93.90  & 43.12&91.32&62.61 & 0.00&47.42&92.74&42.03 & 0.00&0.00&47.65&95.93&37.96 \\
$\checkmark$ & $\checkmark$ & $\checkmark$ &V-LETO (Ours)     & \textbf{94.38}  & 83.73&83.92&\textbf{85.71} & 48.82&62.18&81.17&\textbf{64.04} & 46.50&45.70&64.37&86.87&\textbf{69.13} \\
% $\checkmark$ & $\checkmark$ & $\checkmark$ &V-LETO (Ours)     &94.38  &83.73 &83.22 & 84.94 &46.86  &60.72 &81.52 &63.87 &40.84 &37.77 &63.44 &89.63 & 59.56\\
\bottomrule
\end{tabular}
}
\caption{Ablation study of CIL in V-LETO on FMNIST datasets,   ``w/o'' denotes ``without''. We evaluate the test accuracy of the current training task, previous tasks, and all current tasks. For example, when training task 2, we evaluate the previous task tesk1, the current task tesk, and all current tasks task(12), where task(12) means using the data of task1 and task2 to evaluate the performance of the current model.}
\label{table: ablation-class}
\end{table*}
% 我们评估了当前训练任务，先前任务和当前所有任务的测试准确率。例如，当前训练任务task2时，我们评估了先前任务tesk1、当前任务tesk和当前所有任务task(12),其中，task(12)表示使用task1和task2的数据评估当前模型的性能

\begin{table}[!t]
\centering
\setlength{\tabcolsep}{1.3pt}
\begin{tabular}{ccc|c|ccccc}
\toprule
 $\mathcal{L}_{CE}$ & $\mathcal{L}_{F}$ & LMO & \textbf{Settings} & task1 & task2 & task3 & task4 \\
\midrule
$\times$ & $\checkmark$ & $\checkmark$ & V-LETO w/o $\mathcal{L}_{CE}$ & 60.90 & 79.99 & 89.97 & 90.01 \\
$\checkmark$ & $\times$ & $\checkmark$ & V-LETO w/o $\mathcal{L}_{F}$ & 61.01 & 74.83 & 71.94 & 60.70 \\
$\checkmark$ & $\checkmark$ & $\times$ &V-LETO w/o LMO &  60.17 & 60.26 & 64.32 & 43.10 \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & V-LETO (Ours)     &  \textbf{60.93} & \textbf{88.68}& \textbf{95.06} & \textbf{97.39} \\
\bottomrule
\end{tabular}
\caption{Ablation study of FIL task in the FMNIST datasets.}
\label{table: ablation-feature}
\end{table}











\subsection{Comparison with State-of-the-Art Methods}
\noindent\textbf{Class Incremental Learning.}
%我们与构造的基线方法比较了我们方法的类持续学习的性能。我们从实验结果如表\ref{table: V-LETO}可知,我们的方法具有优越的模型性能。其中Standalone表示每个任务单独训练获得的准确率，不存在持续学习的过程，因此Standalone的模型的准确率是较高的。随着训练任务的持续增多，由于类遗忘问题，因此我们可以看到所有基线方法的模型训练的准确率在逐渐降低。当训练完4个任务后，我们方法的模型准确率具有优越的提升。例如在cifar10数据集上，我们的方法较最优的基线方法的模型准确率提升了10.39%。此外，平均准确率提升了32.27\%。
%因此，我们的方法可以有效的降低类别遗忘对模型性能造成的影响。
%We evaluated the CIL performance of our approach against various baseline methods. 
As shown in Table \ref{table: V-LETO}, our approach demonstrates superior model performance in CIL.
The Standalone method represents the accuracy achieved by training each task separately, without applying continual learning, thus yielding relatively high accuracy. 
However, as the number of tasks increases, baseline models gradually decline in accuracy due to class forgetting. 
After training on four tasks, our approach performs substantial accuracy gains. 
For example, on the CIFAR10 dataset, V-LETO outperforms the best baseline by 10.39\%. 
Additionally, the average accuracy improvement reaches 32.27\%, effectively mitigating the impact of class forgetting on model performance.




\noindent\textbf{Feature Incremental Learning.}
%表\ref{table: V-LETO}展示了我们方案与基线方法在FIL任务中的模型测试准确率，其中箭头表示当前任务比前一个任务模型所提升的准确率。指的注意的是，随着FIL任务的增加，基线方法的模型准确率不会呈现持续增加的情况，而我们的方法的模型准确率在不断增加，例如在FMNIST数据集下，与基线方案相比，我们的方法在最后一个任务task4时模型测试准确率提升了43.87\%。这就表明，随着FIL任务的增加，基线方法存在特征遗忘的情况，即无法利用先前的任务的特征来优化模型参数。我们的方法可以有效的使用先前的任务特征优化模型参数，提升模型性能。
Table \ref{table: V-LETO} presents comparisons of model test accuracy in FIL tasks. 
Arrows represent the accuracy improvement from the previous task to the current task
%of our method and the baseline methods in the FIL task, where the arrows represent the accuracy improvement from the previous task to the current task. 
It depicts that V-LETO continuously improve model accuracy when the number of FIL tasks increases, greatly superior to baseline methods. 
%Notably, as the number of FIL tasks increases, the baseline methods fail to show consistent improvements in model accuracy, while our method continues to improve. 
For example, V-LETO achieves a 35.15\% improvement in model test accuracy in task4 on CIFAR10 dataset compared to the State-of-Art methods.
It implies that the State-of-Art methods suffer from feature forgetting and can hardly use features from previous tasks, while V-LETO has been evidenced that it can effectively utilize features from previous tasks to optimize model parameters and improve performance.
% to optimize model parameters. %, when the number of FIL tasks increases. 
%With the increasing number of FIL tasks, the State-of-Art methods suffer from feature forgetting and are unable to use features from previous tasks to optimize model parameters. 
%In contrast, the results evidence that V-LETO effectively utilizes features from previous tasks to optimize model parameters and improve performance.
% For example, in the FMNIST dataset, our method achieves a 43.87\% improvement in model test accuracy in task 4 compared to the baseline method.
% As the number of FIL tasks increases, the baseline methods suffer from feature forgetting and cannot leverage the features from previous tasks to optimize model parameters
% For example, in the FMNIST dataset, our method achieves a 43.87\% improvement in model test accuracy in task 4 compared to the baseline method. 
% This suggests that as the number of FIL tasks increases, the baseline methods suffer from feature forgetting, meaning they are incapable of utilizing the features from previous tasks to optimize model parameters.
%Our method effectively leverages the features from previous tasks to optimize model parameters and enhance performance.







\begin{table*}[!t]
\centering
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{0.1pt}
\begin{tabular}{cc|c|c|ccc|cccc|ccccc}
\toprule
% Datasets & & & & \multicolumn{5}{c|}{FMNIST}& \multicolumn{5}{c}{CINIC10} \\
% \midrule
\multirow{2}{*}{$\lambda_{CE}$} & \multirow{2}{*}{$\lambda_{A}$} &Training task & task1 & \multicolumn{3}{c|} {task2} & \multicolumn{4}{c|}{task3} & \multicolumn{5}{c}{task4}\\
& & Testing task & task1 & task1(${\color{blue}\uparrow}$) & task2(${\color{blue}\downarrow}$) & task(12) & task1(${\color{blue}\uparrow}$) & task2(${\color{blue}\uparrow}$) & task3(${\color{blue}\downarrow}$) & task(123)& task1(${\color{blue}\uparrow}$) & task2(${\color{blue}\uparrow}$) & task3(${\color{blue}\uparrow}$) & task4(${\color{blue}\downarrow}$) & task(1234)\\
\midrule
$0.8$ & $0.2$ &V-LETO&94.41  & 84.37&84.78& 84.92&40.10 &59.07& 84.90&62.67 & 37.98&31.05&59.80&91.59&57.18 \\
$0.7$ & $0.3$ &V-LETO&  94.37 & 83.51&84.35&85.19 &42.21 &59.51&84.10&62.80
 &38.89&33.05&60.64&91.05&58.14 \\
% $0.5$ & $0.5$ &V-LETO (Ours)& 94.38  &83.73 &83.22 & 84.94 &46.86  &60.72 &81.52 &63.87 &40.84 &37.77 &63.44 &89.63 & 59.56 
% \\
$0.5$ & $0.5$ &V-LETO&94.38  & 83.73&83.92&85.71 & 48.82&62.18&81.17&64.04 & 46.50&45.70&64.37&86.87&69.13\\
$0.3$ & $0.7$ &V-LETO & 94.41 &84.39 &81.84& 84.10&58.71 &62.19 &75.06 &66.37 &45.70 &43.22&62.13 &85.27 & 61.03 
 \\
$0.2$ & $0.8$ &V-LETO& 94.41 &84.77 &80.43 &83.39 &71.45 &63.07 &68.65 &69.86 &51.02 &47.58 &63.59 &81.50 &62.18 
\\
\bottomrule
\end{tabular}
}
\caption{The impact of hyperparameters $\lambda_{CE}$ and $\lambda_{A}$ on CIL model performance in the FMNIST datasets}
\label{table: hyper_CIL1}
\end{table*}



\begin{table*}[!t]
\centering
\setlength{\tabcolsep}{2.8pt}
\begin{tabular}{cc|c|c|ccc|cccc|ccccc}
\toprule
% Datasets & & & & \multicolumn{5}{c|}{FMNIST}& \multicolumn{5}{c}{CINIC10} \\
% \midrule
\multirow{2}{*}{$k_0$} & \multirow{2}{*}{$\alpha$} &Training task& task1 & \multicolumn{3}{c|} {task2} & \multicolumn{4}{c|}{task3} & \multicolumn{5}{c}{task4}\\
& & Testing task & task1 & task1 & task2 & task(12) & task1 & task2 & task3 & task(123) & task1 & task2 & task3 & task4 & task(1234)\\
\midrule
$5$ & $1$ &V-LETO &94.39   & 83.12 &83.98 &84.96 & 47.11 &61.07 &81.52 &63.79 & 41.13 &38.07 &62.99 &89.61 &59.77 
  \\
$10$ & $2$ &V-LETO &94.43 &83.35 &83.82 &84.89 &47.36 &61.40 &81.21 &63.88 &41.02 &38.26 &63.30 &89.43 & 59.85 
 \\
% $15$ & $3$ &V-LETO (Ours)& 94.39 &83.59 &83.28 &84.96 &47.13  &61.00 &81.21 & 63.71 &40.84 &37.75 &62.77 &89.51 & 59.59 \\
$15$ & $3$ &V-LETO &94.38  & 83.73 & 83.92 & 85.71 & 48.82&62.18&81.17&64.04 & 46.50&45.70&64.37&86.87&69.13\\
$20$ & $4$ &V-LETO &94.39 &83.52 & 83.18 & 84.87 &46.97 &60.86 &81.09 &63.71 &40.88 &37.91 &62.81 &89.57 &59.68 
  \\
$25$ & $5$ &V-LETO & 94.39 &83.54  &83.20&84.90 &46.93 &60.82 &81.15 &63.75 &40.88 &37.97 &63.05 &89.82 &59.76   \\
\bottomrule
\end{tabular}
\caption{The impact of hyperparameters $k_0$ and $\alpha$ on CIL model performance in the FMNIST datasets}
\label{table: hyper_CIL2}
\end{table*}
\subsection{Effectiveness of Prototype Evolving}

%特征增强的持续学习 
% 我们通过t-SNE\ref{cai2022theoretical}可视化了FMNIST测试集的样本。在Figure \cite{Fig: Vis_CIL} 和Figure \cite{Fig: Vis_FIL}分别展示了CIL和FIL不同任务测试集global embedding的可视化结果，其中不同颜色的小点表示不同类别的global embedding，对应颜色黑色边框的大点表示该类别的原型。从图Figure \cite{Fig: Vis_CIL}中，我们可以看出，随着CIL任务的增多，每个任务包含了当前任务的globalembedding和根据先前任务类原型生成的global embedding。这就表明V-LETO可以同时保持当前任务和先前任务的性能。从Figure \cite{Fig: Vis_FIL}(a)-(d)中，我们可以看出，随着FIL任务的增多，也就是从任务1到任务4，特征数量逐渐增多，同一类别的样本在同一区域聚集得更多，这就意味模型的性能在逐渐变好。因此，V-LETO可以有效的解决类灾难性遗忘和特征灾难性遗忘问题并高效的实现CIL和FIL任务。
We apply t-SNE \cite{cai2022theoretical} to visualize the samples in the FMNIST dataset. 
Figures \ref{Fig: Vis_CIL}(a)-(d) show visualizations of the global embeddings for different tasks in CIL and FIL, where small colored points represent the global embeddings of various classes, and the large points with black borders represent the corresponding class prototypes. 
From Figure \ref{Fig: Vis_CIL}, we observe that as the number of CIL tasks increases, each task includes both the global embeddings of the current task and those generated based on the class prototypes of previous tasks. 
This indicates that V-LETO can maintain the performance of both current and prior tasks simultaneously. 
% From Figures \ref{Fig: Vis_CIL} (e)-(h), we observe that as the number of FIL tasks increases (i.e., from task 1 to task 4), the number of features increases, and samples of the same class are more tightly clustered in the same region, suggesting improved model performance. 
Figures \ref{Fig: Vis_CIL} (e)-(h) show that as FIL tasks increase (from task 1 to task 4), the number of features grows, and samples of the same class become more tightly clustered, indicating improved model performance.
Therefore, V-LETO effectively addresses both class and feature catastrophic forgetting while efficiently managing both CIL and FIL tasks.

\begin{figure}[!t] 
	\begin{minipage}{0.48\linewidth}
    \centerline{\includegraphics[width=\textwidth]{Figures/Exper/Feature/feature_test_accuracy_0.2_2.pdf}}
		\centerline{(a) (0.8,0.2)}
	\end{minipage}
    \begin{minipage}{0.48\linewidth}
    \centerline{\includegraphics[width=\textwidth]{Figures/Exper/Feature/feature_test_accuracy_0.3_2.pdf}}
		\centerline{(b) (0.7,0.3)}
	\end{minipage}
    
    \begin{minipage}{0.48\linewidth}
\centerline{\includegraphics[width=\textwidth]{Figures/Exper/Feature/feature_test_accuracy_0.5_2.pdf}}
		\centerline{(c) (0.5,0.5)}
	\end{minipage}
	\begin{minipage}{0.48\linewidth}
    \centerline{\includegraphics[width=\textwidth]{Figures/Exper/Feature/feature_test_accuracy_0.8_2.pdf}}
		\centerline{(d) (0.2 0.8)}
	\end{minipage}
	\caption{The impact of hyperparameters ($\lambda_{CE}$, $\lambda_{F}$) on FIL task in the FMNIST datasets}
	\label{Fig: hyperparameter}
\end{figure}


\subsection{Ablation Study}
% We also conduct ablation studies to verify the effectiveness of
% our framework designs.
%我们组织了消融实验验证了我们方法中的各个模块的效果，实验结果如Table \ref{table: ablation-class}和Table \ref{table: ablation-feature}所示。对于类增强学习in V-LETO, 我们从Table \ref{table: ablation-class}可以看出V-LETO w/o $\mathcal{L}_{CE}$中当前训练任务的性能显著下降，这就表明$\mathcal{L}_{CE}$成功的学习到当前的任务的学习。V-LETO w/o $\mathcal{L}_{A}$和V-LETO w/o $LMO$中先前任务的性能显著下降，这就表明$\mathcal{L}_{A}$和$LMO$成功的减轻了类灾难性遗忘问题并通过类原型演变实现类持续学习。
% 对于特征持续学习in V-LETO。我们从Table \ref{table: ablation-feature}中可以看出V-LETO w/o $\mathcal{L}_{F}$和V-LETO w/o $LMO$并不会随着特征任务的增多而提升模型的性能，这就表明$\mathcal{L}_{F}$和$LMO$可以实现先前任务的特征知识转移而提供模型性能。$\mathcal{L}_{CE}$，$\mathcal{L}_{F}$和$LMO$三个模块同时实现在V-LETO的持续学习。
%We conducted ablation experiments to verify the effectiveness of each module in our method, with the experimental results presented in Table \ref{table: ablation-class} and Table \ref{table: ablation-feature}. 
For CIL in V-LETO, Table \ref{table: ablation-class} depicts that the performance of the current training task significantly decreases in V-LETO w/o $\mathcal{L}_{CE}$, indicating that $\mathcal{L}_{CE}$ successfully facilitates learning for the current task. 
The performance of previous tasks drops significantly in V-LETO w/o $\mathcal{L}_{A}$ and V-LETO w/o LMO, indicating that $\mathcal{L}_{A}$ and LMO effectively mitigate the class catastrophic forgetting problem and enable CIL through class prototype evolution.
In addition, for FIL in V-LETO, Table \ref{table: ablation-feature} depicts that V-LETO w/o $\mathcal{L}_{F}$ and V-LETO w/o LMO do not exhibit improved model performance as feature tasks increase, suggesting that $\mathcal{L}_{F}$ and LMO facilitate feature knowledge transfer from previous tasks, thereby enhancing model performance. 

%For CIL in V-LETO, we observe from Table \ref{table: ablation-class} that the performance of the current training task significantly decreases in V-LETO w/o $\mathcal{L}_{CE}$, indicating that $\mathcal{L}_{CE}$ successfully facilitates learning for the current task. 
%The performance of previous tasks drops significantly in V-LETO w/o $\mathcal{L}_{A}$ and V-LETO w/o LMO, demonstrating that $\mathcal{L}_{A}$ and LMO effectively mitigate the class catastrophic forgetting problem and enable CIL through class prototype evolution.
%For FIL in V-LETO, we observe from Table \ref{table: ablation-feature} that V-LETO w/o $\mathcal{L}_{F}$ and V-LETO w/o LMO do not exhibit improved model performance as feature tasks increase, suggesting that $\mathcal{L}_{F}$ and LMO facilitate feature knowledge transfer from previous tasks, thereby enhancing model performance. 
% The three modules—$\mathcal{L}{CE}$, $\mathcal{L}{F}$, and $LMO$—together enable FIL in V-LETO.


\subsection{Hyper-parameter Analysis}
%我们评估了V-LETO中核心的超参数，包括在影响当前任务更新$\lambda_{CE}$，防止先前任务的遗忘的$\lambda_{A}，\lambda_{P}$以及影响本地模型更新的$k_0$和$\alpha$.V-LETO的超参数分析结果如表\cite{table: hyper_CIL2},表\cite{table: hyper_CIL2}和图\cite{fig: hypermeter}所示。从表\cite{table: hyper_CIL2}中，我们可以看出来，随着$\lambda_{CE}$逐渐减少，$\lambda_{A}$逐渐增加，先前任务的准确率不断增加，当前任务的准确率逐渐减少。此外我们从表\cite{table: hyper_CIL2}可以看出，随着$k_0$和$\alpha$的逐渐增加，本地模型中旧任务的重要参数阈值不断增加，当$k_0=15$和$\alpha=5$时模型的性能较好。从图\cite{fig: hypermeter}中，我们可以看出当$\lambda_{CE}=0.5$和$\lambda_{B}=0.5$时FIL中各个任务的性能较好。因此，优化超参数有利于提高模型的性能。
% We evaluated the core hyperparameters in V-LETO, which include the update rate for the current task, $\lambda_{CE}$, parameters for mitigating catastrophic forgetting in previous tasks, $\lambda_{A}$ and $\lambda_{P}$, and hyperparameters affecting local model updates, such as $k_0$ and $\alpha$. 
We evaluated the core hyperparameters in V-LETO, including the current task update rate ($\lambda_{CE}$), catastrophic forgetting mitigation parameters ($\lambda_{A}$, $\lambda_{F}$), and local model update parameters ($k_0$, $\alpha$).
The results of the hyperparameter analysis are presented in Tables \ref{table: hyper_CIL1}, \ref{table: hyper_CIL2} and Figure \ref{Fig: hyperparameter}.
Table \ref{table: hyper_CIL1} depicts that the accuracy of previous tasks continuously increases while the accuracy of the current task declines, as $\lambda_{CE}$ decreases and $\lambda_{A}$ increases.
Table \ref{table: hyper_CIL2} shows that the importance of old task parameters in the local model increases as $k_0$ and $\alpha$ increase, with optimal model performance achieved when $k_0 = 15$ and $\alpha = 5$. 
Figure \ref{Fig: hyperparameter} exhibits that FIL task model performance is optimized when $\lambda_{CE} = 0.5$ and $\lambda_{F} = 0.5$.
%From Table \ref{table: hyper_CIL1}, we observe that as $\lambda_{CE}$ decreases and $\lambda_{A}$ increases, the accuracy of previous tasks improves continuously, while the accuracy of the current task declines.
%From Figure \ref{Fig: hyperparameter}, we observe that FIL task model performance is optimized when $\lambda_{CE} = 0.5$ and $\lambda_{F} = 0.5$.
%Additionally, Table \ref{table: hyper_CIL2} showns that the importance of old task parameters in the local model increases as $k_0$ and $\alpha$ increase, with optimal model performance achieved when $k_0 = 15$ and $\alpha = 5$. 
These findings demonstrate that hyperparameter optimization is crucial for achieving optimal model performance.
% These findings highlight the importance of hyperparameter optimization for improving model performance.
 % 任务的模型性能呈现不断增加后减少的现象，。

% 因此，选择合适的超参数$\lambda_{CE}$和$\lambda_{A}$来优化先前任务和当前任务的准确率至关重要。
