% auto-ignore
\section{Conclusion}
%在本文中，我们提出了一种新颖的纵向持续学习方法(PLUREL),通过在任务间传递原型知识来减少灾难性遗忘问题，提高模型的性能。
%我们设计了纵向联邦学习中的类原型表示，并采用全局原型表实现任务间的知识传递。
%针对特征增强学习，我们设计了特征聚合实现先前任务特征与当前任务特征协同训练模型，提高模型的性能。
%针对类增强学习，我们提出了一种原型增强模型实现旧任务知识的转移。此外，我们在被动方本地模型更新时采取固定对旧任务重要的模型参数减少新任务损失对旧任务知识的影响，降低灾难性遗忘问题，提高模型的性能。
In this paper, we propose a novel method, Vertical Federated Continual Learning via Evolving Prototype Knowledge, which mitigates the issue of catastrophic forgetting and enhances model performance by evolving prototype knowledge across tasks. 
We propose a prototype generation method within VFL and leverage a global prototype table to enable knowledge transfer between tasks. 
%为了解决先前任务知识灾难性遗忘问题，我们基于PG模块获得原型提出了一种演化原型module方法，该方法基于先前任务原型知识和当前任务原型知识构建全局原型知识进而优化全局模型。
%此外，我们提出了一种限制本地模型参数更新方法来解决本地模型对先前任务知识的灾难性遗忘问题。
%我们进行了丰富的实验证明了本方法V-LETO的性能优于当前最优的方法。
%未来的工作，我们将探索增量数据不对齐的纵向联邦持续学习方法。
To address catastrophic forgetting of prior task knowledge, we propose an evolving prototype module based on the PG module. This module integrates prototype knowledge from both previous and current tasks to construct global prototypes, thereby optimizing the global model.
Additionally, we propose a MO module that restricts updates to specific parameters of the local model to mitigate catastrophic forgetting of prior task knowledge.
Extensive experiments demonstrate that our method, V-LETO, outperforms current state-of-the-art methods.
% In future work, we plan to explore VFCL methods for handling misaligned incremental data.
% For the FIL task, we design a feature aggregation approach that facilitates collaborative training between the features of previous and current tasks, thereby improving model performance. 
% To address knowledge transfer across classes, we propose a prototype-enhanced model that enables the transfer of knowledge from older tasks. 
% Additionally, during local model updates on the passive party's side, we fix the model parameters that are critical for retaining old task knowledge. This helps reduce the impact of new task loss on old task knowledge, mitigating catastrophic forgetting and improving overall model performance.
% Experiments on our simulated benchmark demonstrate the effectiveness and efficiency of V-LETO.

