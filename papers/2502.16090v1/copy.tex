\begin{abstract}
This document provides a basic paper template and submission guidelines.
Abstracts must be a single paragraph, ideally between 4--6 sentences long.
Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

\section{Electronic Submission}
\label{submission}

Submission to ICML 2025 will be entirely electronic, via a web site
(not email). Information about the submission process and \LaTeX\ templates
are available on the conference web site at:
\begin{center}
\textbf{\texttt{http://icml.cc/}}
\end{center}

The guidelines below will be enforced for initial submissions and
camera-ready copies. Here is a brief summary:
\begin{itemize}
\item Submissions must be in PDF\@. 
\item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
\item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
\item \textbf{Do not include author information or acknowledgements} in your
    initial submission.
\item Your paper should be in \textbf{10 point Times font}.
\item Make sure your PDF file only uses Type-1 fonts.
\item Place figure captions \emph{under} the figure (and omit titles from inside
    the graphic file itself). Place table captions \emph{over} the table.
\item References must include page numbers whenever possible and be as complete
    as possible. Place multiple citations in chronological order.
\item Do not alter the style template; in particular, do not compress the paper
    format by reducing the vertical spaces.
\item Keep your abstract brief and self-contained, one paragraph and roughly
    4--6 sentences. Gross violations will require correction at the
    camera-ready phase. The title should have content words capitalized.
\end{itemize}

\subsection{Submitting Papers}

\textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
author information may appear on the title page or in the paper
itself. \cref{author info} gives further details.

\medskip

Authors must provide their manuscripts in \textbf{PDF} format.
Furthermore, please make sure that files contain only embedded Type-1 fonts
(e.g.,~using the program \texttt{pdffonts} in linux or using
File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
might come from graphics files imported into the document.

Authors using \textbf{Word} must convert their document to PDF\@. Most
of the latest versions of Word have the facility to do this
automatically. Submissions will not be accepted in Word format or any
format other than PDF\@. Really. We're not joking. Don't send Word.

Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
Those using \texttt{latex} and \texttt{dvips} may need the following
two commands:

{\footnotesize
\begin{verbatim}
dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
ps2pdf paper.ps
\end{verbatim}}
It is a zero following the ``-G'', which tells dvips to use
the config.pdf file. Newer \TeX\ distributions don't always need this
option.

Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
results. This program avoids the Type-3 font problem, and supports more
advanced features in the \texttt{microtype} package.

\textbf{Graphics files} should be a reasonable size, and included from
an appropriate format. Use vector formats (.eps/.pdf) for plots,
lossless bitmap formats (.png) for raster graphics with sharp lines, and
jpeg for photo-like images.

The style file uses the \texttt{hyperref} package to make clickable
links in documents. If this causes problems for you, add
\texttt{nohyperref} as one of the options to the \texttt{icml2025}
usepackage statement.


\subsection{Submitting Final Camera-Ready Copy}

The final versions of papers accepted for publication should follow the
same format and naming convention as initial submissions, except that
author information (names and affiliations) should be given. See
\cref{final author} for formatting instructions.

The footnote, ``Preliminary work. Under review by the International
Conference on Machine Learning (ICML). Do not distribute.'' must be
modified to ``\textit{Proceedings of the
$\mathit{42}^{nd}$ International Conference on Machine Learning},
Vancouver, Canada, PMLR 267, 2025.
Copyright 2025 by the author(s).''

For those using the \textbf{\LaTeX} style file, this change (and others) is
handled automatically by simply changing
$\mathtt{\backslash usepackage\{icml2025\}}$ to
$$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
Authors using \textbf{Word} must edit the
footnote on the first page of the document themselves.

Camera-ready copies should have the title of the paper as running head
on each page except the first one. The running title consists of a
single line centered above a horizontal rule which is $1$~point thick.
The running head should be centered, bold and in $9$~point type. The
rule should be $10$~points above the main text. For those using the
\textbf{\LaTeX} style file, the original title is automatically set as running
head using the \texttt{fancyhdr} package which is included in the ICML
2025 style file package. In case that the original title exceeds the
size restrictions, a shorter form can be supplied by using

\verb|\icmltitlerunning{...}|

just before $\mathtt{\backslash begin\{document\}}$.
Authors using \textbf{Word} must edit the header of the document themselves.

\section{Format of the Paper}

All submissions must follow the specified format.

\subsection{Dimensions}




The text of the paper should be formatted in two columns, with an
overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
between the columns. The left margin should be 0.75~inches and the top
margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
whether you print on US letter or A4 paper, but all final versions
must be produced for US letter size.
Do not write anything on the margins.

The paper body should be set in 10~point type with a vertical spacing
of 11~points. Please use Times typeface throughout the text.

\subsection{Title}

The paper title should be set in 14~point bold type and centered
between two horizontal rules that are 1~point thick, with 1.0~inch
between the top rule and the top edge of the page. Capitalize the
first letter of content words and put the rest of the title in lower
case.

\subsection{Author Information for Submission}
\label{author info}

ICML uses double-blind review, so author information must not appear. If
you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
\verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
will not be printed unless \texttt{accepted} is passed as an argument to the
style file.
Submissions that include the author information will not
be reviewed.

\subsubsection{Self-Citations}

If you are citing published papers for which you are an author, refer
to yourself in the third person. In particular, do not use phrases
that reveal your identity (e.g., ``in previous work \cite{langley00}, we
have shown \ldots'').

Do not anonymize citations in the reference section. The only exception are manuscripts that are
not yet published (e.g., under submission). If you choose to refer to
such unpublished manuscripts \cite{anonymous}, anonymized copies have
to be submitted
as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
paper should be self contained and should contain sufficient detail
for the reviewers to evaluate the work. In particular, reviewers are
not required to look at the Supplementary Material when writing their
review (they are not required to look at more than the first $8$ pages of the submitted document).

\subsubsection{Camera-Ready Author Information}
\label{final author}

If a paper is accepted, a final camera-ready copy must be prepared.
%
For camera-ready papers, author information should start 0.3~inches below the
bottom rule surrounding the title. The authors' names should appear in 10~point
bold type, in a row, separated by white space, and centered. Author names should
not be broken across lines. Unbolded superscripted numbers, starting 1, should
be used to refer to affiliations.

Affiliations should be numbered in the order of appearance. A single footnote
block of text should be used to list all the affiliations. (Academic
affiliations should list Department, University, City, State/Region, Country.
Similarly for industrial affiliations.)

Each distinct affiliations should be listed once. If an author has multiple
affiliations, multiple superscripts should be placed after the name, separated
by thin spaces. If the authors would like to highlight equal contribution by
multiple first authors, those authors should have an asterisk placed after their
name in superscript, and the term ``\textsuperscript{*}Equal contribution"
should be placed in the footnote block ahead of the list of affiliations. A
list of corresponding authors and their emails (in the format Full Name
\textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
Ideally only one or two names should be listed.

A sample file with author names is included in the ICML2025 style file
package. Turn on the \texttt{[accepted]} option to the stylefile to
see the names rendered. All of the guidelines above are implemented
by the \LaTeX\ style file.

\subsection{Abstract}

The paper abstract should begin in the left column, 0.4~inches below the final
address. The heading `Abstract' should be centered, bold, and in 11~point type.
The abstract body should use 10~point type, with a vertical spacing of
11~points, and should be indented 0.25~inches more than normal on left-hand and
right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
sentences. Gross violations will require correction at the camera-ready phase.

\subsection{Partitioning the Text}

You should organize your paper into sections and paragraphs to help
readers place a structure on the material and understand its
contributions.

\subsubsection{Sections and Subsections}

Section headings should be numbered, flush left, and set in 11~pt bold
type with the content words capitalized. Leave 0.25~inches of space
before the heading and 0.15~inches after the heading.

Similarly, subsection headings should be numbered, flush left, and set
in 10~pt bold type with the content words capitalized. Leave
0.2~inches of space before the heading and 0.13~inches afterward.

Finally, subsubsection headings should be numbered, flush left, and
set in 10~pt small caps with the content words capitalized. Leave
0.18~inches of space before the heading and 0.1~inches after the
heading.

Please use no more than three levels of headings.

\subsubsection{Paragraphs and Footnotes}

Within each section or subsection, you should further partition the
paper into paragraphs. Do not indent the first line of a given
paragraph, but insert a blank line between succeeding ones.

You can use footnotes\footnote{Footnotes
should be complete sentences.} to provide readers with additional
information about a topic without interrupting the flow of the paper.
Indicate footnotes with a number in the text where the point is most
relevant. Place the footnote in 9~point type at the bottom of the
column in which it appears. Precede the first footnote in a column
with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
appear in each column, in the same order as they appear in the text,
but spread them across columns and pages if possible.}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
\caption{Historical locations and number of accepted papers for International
Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
produced, the number of accepted papers for ICML 2008 was unknown and instead
estimated.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Figures}

You may want to include figures in the paper to illustrate
your approach and results. Such artwork should be centered,
legible, and separated from the text. Lines should be dark and at
least 0.5~points thick for purposes of reproduction, and text should
not appear on a gray background.

Label all distinct components of each figure. If the figure takes the
form of a graph, then give a name for each axis and include a legend
that briefly describes each curve. Do not include a title inside the
figure; instead, the caption should serve this function.

Number figures sequentially, placing the figure number and caption
\emph{after} the graphics, with at least 0.1~inches of space before
the caption and 0.1~inches after it, as in
\cref{icml-historical}. The figure caption should be set in
9~point type and centered unless it runs two or more lines, in which
case it should be flush left. You may float figures to the top or
bottom of a column, and you may set wide figures across both columns
(use the environment \texttt{figure*} in \LaTeX). Always place
two-column figures at the top or bottom of the page.

\subsection{Algorithms}

If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
environments to format pseudocode. These require
the corresponding stylefiles, algorithm.sty and
algorithmic.sty, which are supplied with this package.
\cref{alg:example} shows an example.

\begin{algorithm}[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Tables}

You may also want to include tables that summarize material. Like
figures, these should be centered, legible, and numbered consecutively.
However, place the title \emph{above} the table with at least
0.1~inches of space before the title and the same after it, as in
\cref{sample-table}. The table title should be set in 9~point
type and centered unless it runs two or more lines, in which case it
should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\begin{table}[t]
\caption{Classification accuracies for naive Bayes and flexible
Bayes on various data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & Naive & Flexible & Better? \\
\midrule
Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Tables contain textual material, whereas figures contain graphical material.
Specify the contents of each row and column in the table's topmost
row. Again, you may float tables to a column's top or bottom, and set
wide tables across both columns. Place two-column tables at the
top or bottom of the page.

\subsection{Theorems and such}
The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
\begin{definition}
\label{def:inj}
A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
\end{definition}
Using \cref{def:inj} we immediate get the following result:
\begin{proposition}
If $f$ is injective mapping a set $X$ to another set $Y$, 
the cardinality of $Y$ is at least as large as that of $X$
\end{proposition}
\begin{proof} 
Left as an exercise to the reader. 
\end{proof}
\cref{lem:usefullemma} stated next will prove to be useful.
\begin{lemma}
\label{lem:usefullemma}
For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
\end{lemma}
\begin{theorem}
\label{thm:bigtheorem}
If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
\end{theorem}
An easy corollary of \cref{thm:bigtheorem} is the following:
\begin{corollary}
If $f:X\to Y$ is bijective, 
the cardinality of $X$ is at least as large as that of $Y$.
\end{corollary}
\begin{assumption}
The set $X$ is finite.
\label{ass:xfinite}
\end{assumption}
\begin{remark}
According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
\end{remark}
%restatable

\subsection{Citations and References}

Please use APA reference format regardless of your formatter
or word processor. If you rely on the \LaTeX\/ bibliographic
facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
included in the style-file package to obtain this format.

Citations within the text should include the authors' last names and
year. If the authors' names are included in the sentence, place only
the year in parentheses, for example when referencing Arthur Samuel's
pioneering work \yrcite{Samuel59}. Otherwise place the entire
reference in parentheses with the authors and year separated by a
comma \cite{Samuel59}. List multiple references separated by
semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
construct only for citations with three or more authors or after
listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

Authors should cite their own work in the third person
in the initial version of their paper submitted for blind review.
Please refer to \cref{author info} for detailed instructions on how to
cite your own papers.

Use an unnumbered first-level section heading for the references, and use a
hanging indent style, with the first line of the reference flush against the
left margin and subsequent lines indented by 10 points. The references at the
end of this document give examples for journal articles \cite{Samuel59},
conference publications \cite{langley00}, book chapters \cite{Newell81}, books
\cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
\cite{mitchell80}, and dissertations \cite{kearns89}.

Alphabetize references by the surnames of the first authors, with
single author entries preceding multiple author entries. Order
references for the same authors by year of publication, with the
earliest first. Make sure that each reference includes all relevant
information (e.g., page numbers).

Please put some effort into making references complete, presentable, and
consistent, e.g. use the actual current name of authors.
If using bibtex, please protect capital letters of names and
abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
in your .bib file.

\section*{Accessibility}
Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

\section*{Software and Data}

If a paper is accepted, we strongly encourage the publication of software and data with the
camera-ready version of the paper whenever appropriate. This can be
done by including a URL in the camera-ready copy. However, \textbf{do not}
include URLs that reveal your institution or identity in your
submission for review. Instead, provide an anonymous URL or upload
the material as ``Supplementary Material'' into the OpenReview reviewing
system. Note that reviewers are not required to look at this material
when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of
the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
usually should) include acknowledgements.  Such acknowledgements
should be placed at the end of the section, in an unnumbered section
that does not count towards the paper page limit. Typically, this will 
include thanks to reviewers who gave useful comments, to colleagues 
who contributed to the ideas, and to funding agencies and corporate 
sponsors that provided financial support.

\section*{Impact Statement}

Authors are \textbf{required} to include a statement of the potential 
broader impact of their work, including its ethical aspects and future 
societal consequences. This statement should be in an unnumbered 
section at the end of the paper (co-located with Acknowledgements -- 
the two may appear in either order, but both must be before References), 
and does not count toward the paper page limit. In many cases, where 
the ethical impacts and expected societal implications are those that 
are well established when advancing the field of Machine Learning, 
substantial discussion is not required, and a simple statement such 
as the following will suffice:

``This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.''

The above statement can be used verbatim in such cases, but we 
encourage authors to think about whether there is content which does 
warrant further discussion, as this statement will be apparent if the 
paper is later flagged for ethics review.





% \begin{equation} \label{message_set}
%     \mathcal{M}_t  = \{(\mathcal{I}_0, \mathcal{S}_0),...,(\mathcal{I}_t, \mathcal{S}_t)\} = \{(\mathcal{I}_i, \mathcal{S}_i)\}|_{i=0}^{t}
% \end{equation}


% \begin{figure}[t!]
% \centering

% \includegraphics[width=.9\linewidth]{figs/em.pdf} \\
% \vspace{1mm}


% % \includegraphics[height=31mm]{figs/bar_lmm_training_samples_transposed.pdf}
% % \includegraphics[height=31mm]{figs/architecture.png} \\

% \caption{The Relationship Between Episodic Memory and Long-Term Memory}
% \label{fig:em}
% % \vspace{-3mm}
% \end{figure}



% \begin{table}[t!]
%   \centering
%   \scalebox{0.88}{
%     \begin{subtable}{\linewidth} 
%       \centering 
%       \begin{tabular}{p{2.2cm} p{5.5cm} }
%         \toprule
%         \multicolumn{2}{l}{\bf Visual input example, Multitask Balancing Problem:}  \\
%         \midrule
%         &  \includegraphics[height=3cm]{figs/extreme_ironing.jpg} \\
%         User & Is this unusual? Please explain in detail. \\
%         InstructBLIP & yes \\
%         \bottomrule
%       \end{tabular}
%       \caption{Example of InstructBLIP~\cite{dai2023instructblip} (Vicuna-13B) having difficulty balancing between short- and long-form answers.}
%       \label{subtab:instructblip_difficulty}  
%     \end{subtable} 
%   }

%   \vspace{3mm}

%   \scalebox{0.88}{
%     \begin{subtable}{\linewidth} 
%       \centering
%       \begin{tabular}{p{2.7cm} p{5.5cm} }
%         \toprule
%         \multicolumn{2}{l}{\bf Visual input example, Different Format Prompts:}  \\
%         \midrule
%         Normal prompt & What is the color of the shirt that the man is wearing? \\
%         Response & The man is wearing a yellow shirt. \\
%         \midrule
%         Ambiguous prompt & Q: What is the color of the shirt that the man is wearing? A: \\
%         Response & The man is wearing a yellow shirt. \\
%         \midrule
%         Formatting prompt & What is the color of the shirt that the man is wearing? \textbf{Answer the question using a single word or phrase.} \\
%         Response & Yellow. \\
%         \bottomrule
%       \end{tabular}
%       \caption{Comparison of how different prompts regularize the output format. The results are obtained zero-shot directly after LLaVA undergoes the 1-stage vision-language alignment pretraining, without the 2-stage visual instruction tuning.}  
%       \label{subtab:format_prompts_diff} 
%     \end{subtable} 
%   }

%   \caption{Visual input example to illustrate the challenge of (a) multitask balancing and (b) different format prompts. The same image input is used.}
%   \label{tab:response_format_example}  
% \end{table}


% \begin{table}[t!]
%   \centering
%   \begin{subtable}{\linewidth} 
%     \centering 
%     \scalebox{0.88}{
%       \begin{tabular}{p{2.2cm} p{6.2cm} }
%         \toprule
%         \multicolumn{2}{l}{\bf Visual input example, Multitask Balancing Problem:}  \\
%         \midrule
%         &  \includegraphics[height=3cm]{figs/extreme_ironing.jpg} \\
%         User & Is this unusual? Please explain in detail. \\
%         InstructBLIP & yes \\
%         \bottomrule
%       \end{tabular}
%     }
%     \caption{Example of InstructBLIP~\cite{dai2023instructblip} (Vicuna-13B) having difficulty balancing between short- and long-form answers.}
%     \label{subtab:instructblip_difficulty}  
%   \end{subtable} 

%   \vspace{3mm}
  
%   \begin{subtable}{\linewidth} 
%     \centering
%     \scalebox{0.88}{
%       \begin{tabular}{p{2.7cm} p{5.7cm} }
%         \toprule
%         \multicolumn{2}{l}{\bf Visual input example, Different Format Prompts:}  \\
%         \midrule
%         Normal prompt & What is the color of the shirt that the man is wearing? \\
%         Response & The man is wearing a yellow shirt. \\
%         \midrule
%         Ambiguous prompt & Q: What is the color of the shirt that the man is wearing? A: \\
%         Response & The man is wearing a yellow shirt. \\
%         \midrule
%         Formatting prompt & What is the color of the shirt that the man is wearing? \textbf{Answer the question using a single word or phrase.} \\
%         Response & Yellow. \\
%         \bottomrule
%       \end{tabular}
%     }
%     \caption{Comparison of how different prompts regularize the output format. The results are obtained zero-shot directly after LLaVA undergoes the first-stage vision-language alignment pretraining, without the second-stage visual instruction tuning.}  
%     \label{subtab:format_prompts_diff} 
%   \end{subtable} 
  
%   \caption{Visual input example to illustrate the challenge of (a) multitask balancing and (b) different format prompts. The same image input is used.}
%   \label{tab:response_format_example}  
% \end{table}


% \begin{table}[t!]
%   \begin{minipage}{0.99\linewidth}
% \centering
% \begin{subtable}{1.0\textwidth} 
% \centering 
% \scalebox{0.88}{
% \begin{tabular}{p{2.2cm} p{5.5cm} }
% \toprule
%  \multicolumn{2}{l}{\bf Visual input example, Multitask Balancing Problem:}  \\
% \midrule
% &  \includegraphics[height=3cm]{figs/extreme_ironing.jpg} \\
% User & Is this unusual? Please explain in detail. \\
% InstructBLIP & yes \\
% \bottomrule
% \end{tabular}

% } 
% \caption{Example of InstructBLIP~\cite{dai2023instructblip} (Vicuna-13B) having difficulty balancing between short- and long-form answers.}  
%   \end{subtable} 
%   \label{tab:instructblip_difficulty}  


% \begin{subtable}{1.0\textwidth} 
% \scalebox{0.88}{
% \begin{tabular}{p{2.7cm} p{5.5cm} }
% \toprule
%  \multicolumn{2}{l}{\bf Visual input example, Different Format Prompts:}  \\
% \midrule
% % &  \includegraphics[height=3cm]{figs/extreme_ironing.jpg} \\

% Normal prompt & What is the color of the shirt that the man is wearing? \\
% Response & The man is wearing a yellow shirt.
% \\
% \midrule
% Ambiguous prompt & Q: What is the color of the shirt that the man is wearing? A: \\
% Response & The man is wearing a yellow shirt. \\
% \midrule
% Formatting prompt & What is the color of the shirt that the man is wearing? \textbf{Answer the question using a single word or phrase.} \\
% Response & Yellow. \\
% \bottomrule
% \end{tabular}
% }
% \caption{Comparison of how different prompts regularize the output format. The results are obtained zero-shot directly after LLaVA undergoes the 1-stage vision-language alignment pretraining, without the 2-stage visual instruction tuning.}  
% \end{subtable} 
%   \end{minipage}
  
% \captionof{table}{Visual input example to illustrate the challenge of (a) multitask balancing and (b) different format prompts. The same image input is used.}
% \label{tab:response_format_example}  
% \end{table}




% \begin{table}[t!]
%   \begin{minipage}{0.99\linewidth}
% \centering
% \scalebox{0.88}{
% \begin{tabular}{p{2.7cm} p{5.5cm} }
% \toprule
%  \multicolumn{2}{l}{\bf Visual input example, Different Format Prompts:}  \\
% \midrule
% &  \includegraphics[height=3cm]{figs/extreme_ironing.jpg} \\

% Normal prompt & What is the color of the shirt that the man is wearing? \\
% Response & The man is wearing a yellow shirt.
% \\
% \midrule
% Ambiguous prompt & Q: What is the color of the shirt that the man is wearing? A: \\
% Response & The man is wearing a yellow shirt. \\
% \midrule
% Formatting prompt & What is the color of the shirt that the man is wearing? \textbf{Answer the question using a single word or phrase.} \\
% Response & Yellow. \\
% \bottomrule
% \end{tabular}
% }
% \captionof{table}{Comparison of how different prompts regularize the output format. The results are obtained zero-shot directly after LLaVA undergoes the first-stage vision-language alignment pretraining, without any visual instruction tuning.}
% \label{tab:response_format_example}  
%   \end{minipage}
% \end{table}


% \subsection{Balanced Multitask Learning}

% \subsection{Response Format Prompting}

% We find that the inability~\cite{chen2023visual} to balance between short- and long-form VQA for approaches like InstructBLIP~\cite{dai2023instructblip}, which leverages instruction following data that includes both natural responses and short-answers, is mainly due to the following reasons.
% First, \emph{ambiguous prompts on the response format}. For example, \emph{Q: \{Question\} A: \{Answer\}}. Such prompts do not clearly indicate the desired output format, and can overfit an LLM behaviorally to short-form answers even for natural visual conversations.
% Second, \emph{not finetuning the LLM}. The first issue is worsened by InstructBLIP only finetuning the Qformer for instruction-tuning. It requires the Qformer's visual output tokens to control the length of the LLM's output to be either long-form or short-form, as in prefix tuning~\cite{li2021prefixtuning}, but Qformer may lack the capability of properly doing so, due to its limited capacity compared with LLMs like LLaMA.
% % See Table~\ref{tab:response_format_example} for a qualitative example.

% %To address this, 
% Thus, to enable LLaVA to better handle short-form answers while addressing the issues of InstructBLIP, we propose to use a single response formatting prompt that clearly indicates the output format. It is appended at the end of VQA questions when promoting short answers: \emph{Answer the question using a single word or phrase}.
% We find that when the LLM is \emph{finetuned} with such prompts, LLaVA is able to properly adjust the output format according to the user's instructions (see Table~\ref{subtab:format_prompts_diff}), and does not require additional processing of the VQA answers using ChatGPT~\cite{chen2023visual}, which further enables scaling to various data sources. As shown in Table~\ref{tab:scaling_ablation}, by merely including VQAv2~\cite{goyal2017vqav2} in training, LLaVA's performance on MME significantly improves (1323.8 \textit{vs} 809.6) and outperforms InstructBLIP by 111 points.


% \begin{figure*}[t!]
% \centering

% \includegraphics[width=.8\linewidth]{figs/high_res_arch_v2.pdf} \\

% \caption{\textbf{\newshortname{}-HD.} Scaling \newshortname{} to higher resolutions by splitting the image into grids and encoding them independently. This allows the model to scale to any resolution, without performing positional embedding interpolation for ViTs. We additionally concatenate the feature of a downsampled image to provide the LLM with a global context.}
% \label{fig:high_res_arch}
% % \vspace{-1mm}
% \end{figure*}

% \begin{table}[t!]
% \centering
% \scalebox{0.82}{
% \begin{tabular}{l l p{6mm}p{7mm} | p{6mm}  p{8mm} c}
% \toprule
% \multicolumn{2}{l}{Method} & LLM & Res. & GQA & MME & MM-Vet \\
% \midrule
% \multicolumn{2}{l}{InstructBLIP} & 14B & 224 & 49.5 & 1212.8 & 25.6 \\
% \midrule
% \multicolumn{7}{l}{\it Only using a subset of InstructBLIP training data} \\
% 0 & \textbf{LLaVA} & 7B & 224 & -- & 809.6 & 25.5 \\
% \rowcolor{lightblue} 1 & +VQA-v2 & 7B & 224 & 47.0 & 1197.0 & 27.7 \\
% \rowcolor{lightblue} 2 &  +Format prompt & 7B & 224 & 46.8 & 1323.8 & 26.3 \\
% \rowcolor{lightpink} 3 & +MLP VL connector & 7B & 224 & 47.3 & 1355.2 & 27.8 \\
% \rowcolor{lightblue} 4 & +OKVQA/OCR & 7B & 224 & 50.0 & 1377.6 & 29.6 \\
% \midrule
% \multicolumn{7}{l}{\it Additional scaling} \\
% \rowcolor{lightblue} 5 &  +Region-level VQA & 7B & 224 & 50.3 & 1426.5 & 30.8 \\
% \rowcolor{lightorange} 6 &  +Scale up resolution & 7B & 336 & 51.4 & 1450 & 30.3 \\
% \rowcolor{lightblue} 7 &  +GQA & 7B & 336 & 62.0$^*$ & 1469.2 & 30.7 \\
% \rowcolor{lightblue} 8 &  +ShareGPT & 7B & 336 & 62.0$^*$ & 1510.7 & 31.1 \\
% \rowcolor{lightpink} 9 &  +Scale up LLM & 13B & 336 & \textbf{63.3}$^*$ & \textbf{1531.3} & \textbf{36.1} \\
% \bottomrule
% \end{tabular}
% }
% \caption{
% \textbf{Scaling results} on \colorrect{lightblue} data, \colorrect{lightpink} model, and \colorrect{lightorange} resolution. We choose to conduct experiments on GQA~\cite{hudson2019gqa}, MME~\cite{fu2023mme}, and MM-Vet~\cite{yu2023mmvet} to examine the representative capabilities of VQA with short answers, VQA with output formatting, and natural visual conversations, respectively.  $^*$Training images of GQA were observed during training.
% }
% \label{tab:scaling_ablation}
% \end{table}

% % \subsection{Probing a Large Multimodal Model}
% \subsection{Scaling the Data and Model}
% % \subsection{Scaling LLaVA}
% \label{sec:scale_model_data}

% \paragraph{MLP vision-language connector.}
% Inspired by the improved performance in self-supervised learning by changing from a linear projection to an MLP~\cite{chen2020simple,mocov2}, we find that improving the vision-language connector's representation power with a two-layer MLP can improve LLaVA's multimodal capabilities, compared with the original linear projection.

% \paragraph{Academic task oriented data.}
% We further include additional academic-task-oriented VQA datasets for VQA, OCR, and region-level perception, to enhance the model's capabilities in various ways, as shown in Table~\ref{tab:scaling_ablation}. 
% We first include four additional datasets that are used in InstructBLIP: open-knowledge VQA (OKVQA~\cite{okvqa}, A-OKVQA~\cite{schwenk2022okvqa}) and OCR (OCRVQA~\cite{mishra2019ocrvqa}, TextCaps~\cite{sidorov2020textcaps}). A-OKVQA is converted to multiple choice questions and a specific response formatting prompt is used: \emph{Answer with the option's letter from the given choices directly}. With only a subset of the datasets InstructBLIP uses, LLaVA already surpasses it on all three tasks in Table~\ref{tab:scaling_ablation}, suggesting LLaVA's effective design. Furthermore, we find further adding region-level VQA datasets (Visual Genome~\cite{krishna2017visual}, RefCOCO~\cite{kazemzadeh2014referitgame,mao2016generation}) improves the model's capability of localizing fine-grained visual details.

% \paragraph{Additional scaling.}
% We further scale up the input image resolution to 336$^2$ to allow the LLM to clearly ``see'' the details of images, by swapping the vision encoder to CLIP-ViT-L-336px (the highest resolution available for CLIP). In addition, we add the GQA dataset as an additional visual knowledge source. We also incorporate ShareGPT~\cite{sharegpt} data and scale up the LLM to 13B as in \cite{bai2023qwen,lu2023empirical,chen2023shikra}. Results on MM-Vet shows the most significant improvement when scaling the LLM to 13B, suggesting the importance of the base LLM's capability for visual conversations. 

% \paragraph{\newshortname{}.}
% We denote this final model with all the modifications as \newshortname{} (the last two rows in Table~\ref{tab:scaling_ablation}), which achieves an impressive performance that significantly outperforms the original LLaVA~\cite{llava}.

% \paragraph{Computational cost.}
% For \newshortname{}, we use the same pretraining dataset, and keep the training iterations and batch size roughly the same for instruction tuning as LLaVA~\cite{llava}. Due to the increased image input resolution to 336$^2$, the training of \newshortname{} is $\sim$2$\times$ as long as LLaVA: $\sim$6 hours of pretraining and $\sim$20 hours of visual instruction tuning, using 8$\times$ A100s.

%  of LCS-558K\footnote{LCS-558K: a subset of $\sim$558K image-text pairs from LAION-CC-SBU with BLIP captions, as used in LLaVA-Lightning series.}






% \begin{table*}[t!]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{l l c c c | c c c c c }
% \toprule
% % Method & LLM & Res. & PT & IT & VQA-v2 & GQA & VisWiz & SciQA-IMG & TextVQA \\
% \multirow{2}{*}{Method} & \multirow{2}{*}{LLM} & Image & \multicolumn{2}{c|}{Sample Size} & VQAv2 & GQA & VisWiz & SciQA- & TextVQA \\
%  & & Size & Pretrain & Finetune & \cite{goyal2017vqav2} & \cite{hudson2019gqa} & \cite{gurari2018vizwiz} & IMG~\cite{lu2022learn} & \cite{singh2019textvqa} \\
% \midrule
% BLIP-2~\cite{li2023blip} & Vicuna-13B & 224$^2$ & 129M & - & 65.0 & 41 & 19.6 & 61 & 42.5 \\
% InstructBLIP~\cite{dai2023instructblip} & Vicuna-7B & 224$^2$ & 129M & 1.2M & -- & 49.2 & 34.5 & 60.5 & 50.1 \\
% InstructBLIP~\cite{dai2023instructblip} & Vicuna-13B & 224$^2$ & 129M & 1.2M & -- & 49.5 & 33.4 & 63.1 & 50.7 \\
% Shikra~\cite{chen2023shikra} & Vicuna-13B & 224$^2$ & 600K & 5.5M & 77.4$^*$ & -- & -- & -- & -- \\
% IDEFICS-9B~\cite{idefics} & LLaMA-7B & 224$^2$ & 353M & 1M & 50.9 & 38.4 & 35.5 & -- & 25.9 \\
% IDEFICS-80B~\cite{idefics} & LLaMA-65B & 224$^2$ & 353M & 1M & 60.0 & 45.2 & 36.0 & -- & 30.9 \\
% Qwen-VL~\cite{bai2023qwen} & Qwen-7B & 448$^2$ & 1.4B$^\dagger$ & 50M$^\dagger$ & \underline{78.8}$^*$ & 59.3$^*$ & 35.2 & 67.1 & \textbf{63.8}$^*$ \\
% Qwen-VL-Chat~\cite{bai2023qwen} & Qwen-7B & 448$^2$ & 1.4B$^*$ & 50M$^\dagger$ & 78.2$^*$ & 57.5$^*$ & 38.9 & 68.2 & \underline{61.5}$^*$ \\
% \midrule
% \rowcolor{Gray}
% \textbf{\newshortname{}} & Vicuna-7B & 336$^2$ & \textbf{558K} & \textbf{665K} & \underline{78.5}$^*$ & \underline{62.0}$^*$ & \underline{50.0} & 66.8 & 58.2 \\
% \rowcolor{Gray}
% \textbf{\newshortname{}} & Vicuna-13B & 336$^2$ & \textbf{558K} & \textbf{665K} & \textbf{80.0}$^*$ & \textbf{63.3}$^*$ & \textbf{53.6} & \textbf{71.6} & \underline{61.3} \\
% \rowcolor{Gray}
% \textbf{\newshortname{}-HD} & Vicuna-13B & 448$^2$ & \textbf{558K} & \textbf{665K} & \textbf{81.8}$^*$ & \textbf{64.7}$^*$ & \textbf{57.5} & \underline{71.0} & \underline{62.5} \\
% \midrule
% \multicolumn{5}{l|}{\color{gray} Specialist SOTA: PaLI-X-55B~\cite{chen2023pali}} & \color{gray}86.1$^*$ & \color{gray}72.1$^*$ & \color{gray}70.9$^*$ & \color{gray}-- & \color{gray}71.4$^*$ \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-1mm}
% \caption{\textbf{Comparison with SoTA methods on academic-task-oriented datasets.} \newshortname{} achieves the best performance on 4/5 benchmarks, and ranks the second on the other. $^*$The training images/annotations of the datasets are observed during training. $^\dagger$Includes in-house data that is not publicly accessible.}
% % \vspace{-2mm}
% \label{tab:results}
% \end{table*}


% \begin{table*}[t!]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{l | c c c | c | c c | c c c | c | c }
% \toprule
% \multirow{2}{*}{Method} & \multicolumn{3}{c|}{POPE~\cite{li2023pope}} & MME & \multicolumn{2}{c|}{MMBench~\cite{liu2023mmbench}} & \multicolumn{3}{c|}{SEED-Bench~\cite{li2023seed}} & LLaVA- & MM-Vet \\
%  & rand & pop & adv & \cite{fu2023mme} & en & cn & all & img & vid & Wild~\cite{llava} & \cite{yu2023mmvet}\\
% \midrule
% BLIP2-14B~\cite{li2023blip} & \textbf{89.6} & 85.5 & 80.9 & 1293.8 & -- & -- & 46.4 & 49.7 & 36.7 & 38.1 & 22.4 \\
% InstructBLIP-8B~\cite{dai2023instructblip} & -- & -- & -- & -- & 36 & 23.7 & 53.4 & 58.8 & 38.1 & 60.9 & 26.2 \\
% InstructBLIP-14B~\cite{dai2023instructblip} & \underline{87.7} & 77 & 72 & 1212.8 & -- & -- & -- & -- & -- & 58.2 & 25.6 \\
% Shikra-13B~\cite{chen2023shikra} & -- & -- & -- & -- & 58.8 & -- & -- & -- & -- & -- & -- \\
% IDEFICS-9B~\cite{idefics} & -- & -- & -- & -- & 48.2 & 25.2 & -- & 44.5 & -- & -- & -- \\
% IDEFICS-80B~\cite{idefics} & -- & -- & -- & -- & 54.5 & 38.1 & -- & 53.2 & -- & -- & -- \\
% Qwen-VL~\cite{bai2023qwen} & -- & -- & -- & -- & 38.2 & 7.4 & 56.3 & 62.3 & 39.1 & -- & -- \\
% Qwen-VL-Chat~\cite{bai2023qwen} & -- & -- & -- & 1487.5 & 60.6 & 56.7 & \underline{58.2} & 65.4 & 37.8 & -- & -- \\
% \midrule
% \shortname{}-7B~\cite{llava} & 76.3 & 72.2 & 70.1 & 809.6 & 38.7 & 36.4 & 33.5 & 37.0 & 23.8 & 62.8 & 25.5 \\
% \rowcolor{Gray}
% \textbf{\newshortname{}-7B} & 87.3 & \underline{86.1} & \underline{84.2} & \underline{1510.7} & \underline{64.3} & 58.3 & \underline{58.6} & \underline{66.1} & 37.3 & 65.4 & \underline{31.1} \\
% \rowcolor{Gray}
% \textbf{\newshortname{}-13B} & 87.1 &  \textbf{86.2} & \textbf{84.5} & \textbf{1531.3} & \textbf{67.7} & \textbf{63.6} & \textbf{61.6} & \textbf{68.2} & \textbf{42.7} & \textbf{72.5} & \textbf{36.1} \\
% \rowcolor{Gray}
% \textbf{\newshortname{}-13B-HD} & 87.5 & \textbf{86.4} & \textbf{85.0} & 1500.1 & \textbf{68.8} & \underline{61.9} & \textbf{62.6} & \textbf{70.1} & \underline{41.3} & \underline{72.0} & \textbf{39.4} \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-1mm}
% \caption{\textbf{Comparison with SoTA methods on benchmarks for instruction-following LMMs.} \newshortname{} achieves the best overall performance.}
% \label{tab:results}
% \end{table*}





% \subsection{Scaling to Higher Resolutions}

% In Sec.~\ref{sec:scale_model_data}, we observe the advantage that scaling up the input image resolution improves the model's capabilities. However, the image resolution of the existing open source CLIP vision encoders is limited to 336$^2$, preventing the support of higher resolution images by simply replacing the vision encoder as we did in Sec.~\ref{sec:scale_model_data}. In this section, we present an early exploration of scaling the LMM to higher resolutions, while maintaining the data efficiency of \newshortname{}.

% When using ViT~\cite{dosovitskiy2020image} as the vision encoder, to scale up the resolution, previous approaches mostly choose to perform positional embedding interpolation~\cite{bai2023qwen,li2023blip} and adapt the ViT backbone to the new resolution during finetuning. However, this usually requires the model to be finetuned on a large-scale image-text paired dataset~\cite{bai2023qwen,li2023blip}, and limits the resolution of the image to a fixed size that the LMM can accept during inference.

% Instead, as shown in Fig.~\ref{fig:high_res_arch}, we overcome this by dividing the image into smaller image patches of the resolution that the vision encoder is originally trained for, and encode them independently. After obtaining the feature maps of individual patches, we then combine them into a single large feature map of the target resolution, and feed that into the LLM. To provide the LLM with the global context and to reduce the artifact of the split-encode-merge operation, we additionally concatenate the feature of a downsampled image to the merged feature map.
% This allows us to scale the input to any arbitrary resolution and maintain the data efficiency of \newshortname{}. We call this resulting model \newshortname{}-HD.
