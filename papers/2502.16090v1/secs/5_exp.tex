\section{Experiment}
\label{sec:experiment}
\subsection{Experimental Setups}

\paragraph{Selected LLMs.} We evaluate a series of LLMs on EM-Test, including the current state-of-the-art open-source and closed-source models. Particularly, we select LLAMA3-8b \cite{dubey2024llama}, ChatGLM3-6B \cite{glm2024chatglm} for open-source models, and for closed-source models, we employ GPT-3.5-turbo \cite{openai2023gpt4}, GPT-4 \cite{openai2023gpt4}, ChatGLM3-trubo \cite{glm2024chatglm}.

\paragraph{Implementation Details.}
In MADGF, the LLM serving as the agent is Qwen2-72B-Instruct \cite{yang2024qwen2}, which is a high-performance open-source LLM. We use chatglm3-6B \cite{glm2024chatglm} as the base model for Echo, and implement it with full fine-tuning.

\paragraph{Evaluation Methods and Metrics.}

In the quantitative analysis, we first collect the responses of LLMs at the test points, then ask human annotators to score these responses on a scale of 1 to 10. Additionally, we use the widely adopted Sentence Transformer model, all-MiniLM-L6-v2\footnote{\tiny \url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}, to encode the LLMs' responses and the reference standard outputs provided by the test set, obtaining \(\mathcal{E}_{\text{LLM}}\) and \(\mathcal{E}_{\text{Standard}}\). We then calculate the cosine similarity \(\mathcal{S}\) using the Equation \ref{equ:simi}.

\begin{equation} \label{equ:simi}
    \mathcal{S}  = cos\_sim(\mathcal{E}_{\text{LLM}},\mathcal{E}_{\text{Standard}}) \times 100
\end{equation}

We consider using the Pearson correlation coefficient, denoted by $ \mathcal{R} $, to measure the correlation between the human score and the similarity metric. It is calculated using Equation \ref{equ:pr}.

% We consider using the Pearson correlation coefficient, denoted by $ \mathcal{R} $, to measure the correlation between the human score and the similarity metric. The Pearson correlation coefficient quantifies the strength and direction of the linear relationship between two variables. It is calculated using Equation \ref{equ:pr}.

% The Pearson correlation coefficient, denoted by $ \mathcal{R} $, is calculated using Equation \ref{equ:pr}.

\begin{equation} \label{equ:pr}
% \[
\mathcal{R} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}
% \]
\end{equation}
where:
\begin{itemize}
    \item \( x_i \) and \( y_i \) are individual sample points indexed with \( i \),
    \item \( \bar{x} \) and \( \bar{y} \) are the mean values of \( x \) and \( y \) respectively,
    \item \( n \) is the number of sample points.
\end{itemize}

If the \(\mathcal{R}\) value between two datasets is greater than 0.8, it is considered to be highly positively correlated \cite{cohen2013statistical}.

% 定量实验
\subsection{Quantitative Analysis}

\paragraph{Overall Performance}

\input{tabs/exp1_human}

\input{tabs/exp1_simi}


% We present the results of LLMs on human scores and similarity metrics in Table \ref{table:exp1_human} and Table \ref{table:exp1_simi}, respectively. Our experimental analysis is provided from the following aspects: Performance of several LLMs, Human Score and Similarity Metrics, Comparisons Across Easy and Hard Levels, Comparisons Across Different Time Spans, and Consistency Between Human Score and Similarity Metric.
We present the results of LLMs on human scores and similarity metrics in Table \ref{table:exp1_human} and Table \ref{table:exp1_simi}, respectively. Our experimental analysis is provided from the following aspects: performance of several LLMs, human score and similarity metrics, comparisons across easy and hard levels, comparisons across different time spans, and consistency between human score and similarity metric.

\begin{figure*}[h]
\centering

\includegraphics[width=.9\linewidth]{figure/exp-demo1.pdf} \\

\caption{Examples of complex episodic memory capability in the Echo.}
\label{fig:exp-demo1}

\end{figure*}

\textit{Performance of Several LLMs.}
It can be observed that our Echo model achieved the best performance in both Human Score and Similarity Metric. Specifically, it scored 6.7 and 5.9 in the easy and hard levels of Human Score, respectively, and 84.0 and 74.5 in the Similarity Metric. Over different time spans, Echo nearly obtained the best or second-best scores across all metrics, except for the easy level "One day" and "Several Decades" in Human Score. These results indicate that the Echo model excels in EM capability.
In contrast, among all models, the open-source ChatGLM3-6B performed the worst overall. As the base model of Echo, this indirectly demonstrates the effectiveness of the EM-Train data generated using the MADGF framework in enhancing a model's EM capability.
Moreover, GPT-4 also showed excellent comprehensive performance, achieving second-best scores in the easy level of Human Score and the hard level of Similarity Metric. In the hard level of Human Score, GPT-4 (5.4) narrowly trailed behind LLAMA3-8B (5.5), which had the second-best performance. In the easy level of Similarity Metric, GPT-4 (72.3) slightly lagged behind LLAMA3-8B (74.8), which also had the second-best performance.


\textit{Comparisons across Easy and Hard Levels.}
Most LLMs perform well on easy-level problems, but their performance drops at the hard level. Specifically, we found that all LLMs performed better overall at the Easy Level compared to the Hard Level. For instance, in Table \ref{table:exp1_human}, GPT-3.5-turbo scored 5.2 at the Easy Level, but only 4.0 at the Hard Level. Additionally, we observed that the average score for all models at the Hard Level in Table \ref{table:exp1_human} was 4.5, which is 0.5 lower than the Easy Level (5.0). Similar results were seen in Table \ref{table:exp1_simi}.

\textit{Comparisons across Different Time Spans.}
% Models exhibit different performances across various time spans. On the easy time span, we found that models perform better on "several decades" questions. This is because the mean values for "several decades" are the highest for all models according to both metrics.
Models exhibit different performances across various time spans. Due to the inconsistency in model performance between the human score and similarity metric, we first consider the common performance under both metrics, then focus on the analysis based primarily on the human score. 
On the easy time span, we found that models perform better on "several decades" questions. This is because the mean values for "several decades" are the highest for all models according to both metrics (6.2 for human score and 82.8 for similarity metric). Meanwhile, we observed that models perform poorly on certain time spans (few days, few months, few years), which may be attributed to the difficulty models have in understanding these temporal concepts.

\begin{figure*}[h]
\centering

\includegraphics[width=.9\linewidth]{figure/exp-demo2.pdf} \\

\caption{Examples of episodic memory ability without temporal information in the Echo.}
\label{fig:exp-demo2}
% \vspace{-1mm}
\end{figure*}

\textit{Consistency Between Human Scores and Similarity Metrics.}
We consider calculating the Pearson correlation coefficient \( \mathcal{R} \) for two metrics to observe their correlation. The overall results of the Human Score (i.e., 2.7, 5.6, ..., 6.7) and the Similarity Metrics (i.e., 57.0, 70.2, ..., 84.0) are used to compute similarity at the Easy Level, and similarly for the Hard Level. Using Equation \ref{equ:pr}, we obtained Pearson correlation coefficients \( \mathcal{R} \) of 0.935 for the Easy Level and 0.842 for the Hard Level, both greater than 0.8. Therefore, the results of the two metrics can be considered highly positively correlated. Given that the Human Score requires expensive manual evaluation, Similarity Metrics can be considered as a cost-effective alternative for subsequent model evaluations.
Additionally, we found inconsistencies in the performance evaluation of models by Human Score and Similarity Metrics across different time spans. We calculated the Pearson correlation coefficient \( \mathcal{R} \) between the mean values of the two metrics over different time spans, resulting in coefficients of 0.429 (moderate correlation) and 0.003 (very weak correlation) for the Easy Level and Hard Level, respectively.
Our hypothesis is that the EM-Test may have insufficient data points for each time span, leading to inadequate statistical samples. This insufficiency results in inconsistent outcomes between Human Scores and Similarity Metrics.
\vspace{-2mm}

\paragraph{Performance in Episodic Memory Without Temporal Information}

We tested the EM capability of the models without considering time information, as shown in Table \ref{table:exp2}. We found that the models perform similarly whether or not time information is considered. Our Echo model and GPT-4 still performed well, achieving the first and second highest scores, respectively; while ChatGLM3-6B continued to perform the worst. These results indicate that the dataset EM-Train, obtained using our MADGF framework, can effectively improve the EM capability of models even when time information is not considered. In addition, we provide extended experiments regarding the model's temporal awareness and reasoning capability in the appendix for further analysis.

\input{tabs/exp2}


% 定性实验
\subsection{Qualitative Analysis}

\paragraph{Analysis of Complex Episodic Memory Ability} 
We conducted experiments on the Echo model using real-life scenarios that require complex Episodic Memory abilities, as shown in Figure \ref{fig:exp-demo1}. Some dialogues unrelated to episodic memory have been omitted using vertical ellipses. These dialogues are intended to increase the challenge of improving the model's Episodic Memory ability in long texts. Additionally, some content details unrelated to episodic memory skills have also been omitted. To test the model's performance over longer time spans, all time information was manually provided.

The test dialogue on the left side of Figure \ref{fig:exp-demo1} demonstrates that the model can accurately recall recent conversation content and timing, indicating its ability to understand time and associate it with events, which is a sign of Episodic Memory capability. In the test questions on the right side, the model shows even stronger Episodic Memory ability by recalling what human characters ate on a specific day from lengthy historical records, and understanding and judging whether conversations took place during a certain period.


\paragraph{Analysis of Episodic Memory Ability Without Temporal Information}

We tested Echo using questions that do not require considering time information for responses, as shown in Figure \ref{fig:exp-demo2}. From the test dialogue, it is clear that Echo can accurately recall the human character's favorite band and food, and provide relevant information even after multiple rounds of dialogue. Additionally, in the final round of test questions, Echo did not confuse any content that we had not actually told it, avoiding the hallucination issue. This problem often occurs when conversing with other LLMs.


