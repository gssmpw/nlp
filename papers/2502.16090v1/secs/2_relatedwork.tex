\section{Related Work}
\label{sec:background}

\paragraph{Methods for Enhancing Long-Term Memory Capabilities}
% Some work has been proposed to enhance the long-text memory capabilities of large models, primarily through two approaches: improving the model itself and introducing external storage. Methods to improve the model typically involve modifying positional encodings \cite{kazemnejad2024impact}, enhancing the representational capacity of attention mechanisms \cite{katharopoulos2020transformers,ding2023longnet}, and optimizing computational methods. These techniques help the model converge more easily in long-text tasks and aid in mitigating the forgetting problem. However, such methods do not effectively enhance the model's episodic memory capabilities.

Some methods have been proposed to enhance the long-term memory capabilities of large models, such as MemoryBank \cite{zhong2024memorybank}, H-EMV \cite{barmann2024episodic}, EM-LLM \cite{fountas2024human}, MemGPT \cite{packer2023memgpt}, MS \cite{gao2024memory}, and CHATDB \cite{hu2023chatdb}. These methods use external storage to retain historical information and design various operations to help LLMs utilize information.

MemoryBank \cite{zhong2024memorybank} introduces a novel memory mechanism specifically designed for LLM. This mechanism processes historical conversation to extract summary information and user portrait. When a user poses a question, the mechanism retrieves relevant information based on similarity and combines it with the summary information and user portrait, to form a Meta Prompt that assists the model in generating responses.
EM-LLM \cite{fountas2024human} adopts a similar method by incorporating key information into preceding prompts. This method effectively handles nearly unlimited context lengths while maintaining high computational efficiency.
MemGPT \cite{packer2023memgpt} enables LLMs to perform tasks beyond the current context limits by simulating extended virtual memory through paging between physical memory and disk storage, akin to how operating systems manage memory to extend LLM context.
MS \cite{gao2024memory}, H-EMV \cite{barmann2024episodic}, and CHATDB \cite{hu2023chatdb} introduce distinct data structures designed for the storage of historical information: namely, a memory-sharing framework, a tree-based storage structure, and a specialized database, respectively. Each of these architectures facilitates the retrieval of pertinent historical data to support the response generation.

These methods require various operations on external storage that can be time-consuming. Moreover, they primarily focus on retrieving a copy of the data, rather than implementing the constructive nature of episodic memory \cite{sprott1933remembering,schacter2012constructive}, failing to enhance the model's inherent ability to process episodic memory.
% However, these methods face two issues: 1. Various operations on external storage can be time-consuming. 2. Episodic memory is thought to be constructive, meaning recall is the (re)construction of a past experience rather than the retrieval of a copy \cite{sprott1933remembering,schacter2012constructive}.



\paragraph{Methods for Data Generation utilizing LLM}
Manually annotated data is expensive, so many methods \cite{xu2023wizardlm,luo2023wizardmath,zhao2024wildchat,wang2022self,ding2023enhancing,li2023camel} have been proposed to automate data generation utilizing LLMs. Besides obtaining data through user interactions on online platforms using ChatGPT, like WILDCHAT \cite{zhao2024wildchat}, Self-Instruct \cite{wang2022self} was one of the first to propose generating instructions, inputs, and outputs using LLMs to build instruction fine-tuning data. To increase the diversity of instructions, WizardLM \cite{xu2023wizardlm} introduced an evolutionary instruction approach starting from a small set of seed instructions to generate more complex and diverse instruction. Further, WizardMath \cite{luo2023wizardmath} incorporated a reward model to select better instruction data from multiple outputs, collecting higher-quality generated data. Additionally, some methods \cite{ding2023enhancing,li2023camel} propose having LLMs play the roles of both AI assistant and user to collect data, which allows for the collection of multi-turn dialogues. UltraChat \cite{ding2023enhancing} uses this approach to extract instruction data covering various tasks, such as Questions about the World and Creation and Generation. In contrast, CAMEL \cite{li2023camel} focuses on generating instruction data for specific tasks, such as "Develop a trading bot for the stock market."

These LLM-based data generation methods primarily focus on extracting high-quality instruction fine-tuning data grounded in semantic memory from LLMs. In contrast, our MADGF mainly aims to simulate real-life scenarios to generate dialogue content rich in episodic memory.

% These methods primarily focus on extracting high-quality instruction fine-tuning data from LLMs. Unlike these methods, our data generation approach, MADGF, primarily aims to simulate real-life scenarios to generate dialogue content rich in episodic memory.

% In MADGF, we design three key elements: characters, scenarios, and environments. This design allows for free-form character interactions while controlling the direction of the dialogue content and enhancing the model's episodic memory capabilities during the conversation process.








