% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{amsmath} 
\usepackage{amssymb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{R-LoRA: Random Initialization of Multi-Head LoRA\\ for Multi-Task Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Jinda Liu \\
% %  School of Artificial Intelligence /Jilin University \\
%   \texttt{liujd9922@mails.jlu.edu.cn} \\
%   Yuan Wu* \\
%   \texttt{yuanwu@jlu.edu.cn} \\}
% }
\author{
        Jinda Liu$^{1}$ \quad  Yi Chang$^{1,2,3}$ \quad Yuan Wu$^{1}$\footnotemark[1] \\
        $^{1}$School of Artificial Intelligence, Jilin University \\
        $^{2}$Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China \\
        $^{3}$International Center of Future Science, Jilin University\\
        liujd9922@mails.jlu.edu.cn, yichang@jlu.edu.cn, yuanwu@jlu.edu.cn \\
}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors}

\begin{abstract}
Fine-tuning large language models (LLMs) is prohibitively expensive in terms of computational and memory costs. Low-rank Adaptation (LoRA), as one of the most popular parameter-efficient fine-tuning (PEFT) methods, offers a cost-effective alternative by approximating the model changes $\Delta W \in \mathbb{R}^{m \times n}$ through the product of down-projection matrix $A \in \mathbb{R}^{m \times r}$ and head matrix $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$. In real-world scenarios, LLMs are fine-tuned on data from multiple domains to perform tasks across various fields, embodying multi-task learning (MTL). LoRA often underperforms in such complex scenarios. To enhance LoRA's capability in multi-task learning, we propose R-LoRA, which incorporates Multi-Head Randomization. Multi-Head Randomization diversifies the head matrices through Multi-Head Random Initialization and Multi-Head Dropout, enabling more efficient learning of task-specific features while maintaining shared knowledge representation. Extensive experiments demonstrate that R-LoRA is better at capturing task-specific knowledge, thereby improving performance in multi-task scenarios.
The code is available at \url{https://github.com/jinda-liu/R-LoRA}.
\end{abstract}

\begin{figure*}[t]
\centering
  \includegraphics[width=0.8\linewidth]{framework.pdf}
  \caption {Training architecture comparison. (a) Full parameter fine-tuning; (b) Vanilla LoRA; (c) Multi-Adapter architecture; (d) Multi-Head/Asymmetric architecture.}
  \label{architecture}
\end{figure*}

\section{Introduction}

In recent years, large language models (LLMs) have manifested unprecedentedly superior performance in various natural language processing (NLP) tasks \cite{brown2020language,zhao2023survey,chang2024survey}. Due to its impressive capabilities in language understanding and generation, LLMs have gained extensive interest from both academia and industry. Despite their high generalizability, LLMs still require fine-tuning for specific domains or updating the knowledge base~\cite{agiza2024mtlora,xin2024beyond}. 

Supervised fine-tuning (SFT) is crucial for aligning large language models (LLMs) with human instructions, which trains the model with a small yet high-quality set of labeled data~\cite{hu2021lora,xia2024rethinking}. The vast number of parameters in LLMs poses significant challenges regarding computational efficiency and memory consumption during full fine-tuning (FT), which updates all parameters. 

To address the issue of hardware requirements for LLM adaptation, a solution called parameter efficient fine-tuning (PEFT) has been proposed~\cite{han2024parameter}. PEFT methods reduce VRAM usage of cached optimizer states by only optimizing a fraction of model parameters while keeping the rest frozen. Various PEFT methods, such as prefix-tuning\cite{li2021prefix}, p-tuning\cite{liu2024gpt}, \(IA^{3}\)\cite{liu2022few}  and Low-rank adaption(LoRA)\cite{hu2021lora}, have been widely studied. Among these methods, LoRA has emerged as the mainstream alternative to full parameter fine-tuning. Instead of updating the original parameter matrix directly, LoRA approximates the updated parameters using the product of two smaller matrices. During inference, the output obtained from the original parameter matrix is combined with the output from the updated parameter matrices. However, LoRA does not perform well in multi-task scenarios, particularly in dealing with complex datasets.

Recent LoRA variants have improved multi-task learning by employing multiple LoRA adapters, including Multi-LoRA~\cite{wang2023multilora}, LoRA-MoE~\cite{dou2023loramoe}, and MoeLoRA~\cite{liu2024moemeetsllmsparameter}. We refer to this extended framework as the Multi-Adapter LoRA architecture, which consists of multiple down-projection matrices (A) and their corresponding head matrices (B), enabling task-specific adaptation through diverse parameter sets. Notably, LoRA-MoE and MoeLoRA further enhance this architecture by introducing a Mixture of Experts (MoE) mechanism to aggregate adapter outputs.
\citet{tian2024hydraloraasymmetricloraarchitecture} observes that in the Multi-Adapter LoRA architecture, the parameters of the down-projection matrices A are relatively consistent, while the differences between the head matrices B are more pronounced, which aids in capturing task-specific knowledge. To leverage this property, HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} is proposed to feature an asymmetric architecture with one shared down-projection matrix A and multiple task-specific head matrices B. Additionally, HydraLoRA also employs an MoE mechanism to aggregate the outputs of the head matrices. This design achieves a good balance between training performance and parameter efficiency. The mathematical formalization of HydraLoRA is detailed in Section~\ref{multi-head}. In this work, we propose R-LoRA, which adopts HydraLoRA's asymmetric architecture, explicitly defining it as a Multi-Head structure, and introduce Multi-Head randomization to improve LLMs' performance on multi-task learning. Figure~\ref{architecture} illustrates the differences among the aforementioned structures.

However, in the Multi-Head architecture, the parameter similarity among head matrices remains high, hindering task-specific knowledge learning and slowing convergence speed. This is due to zero initialization of head matrices B, leading to similar update directions. To address this, we use multi-head randomization in R-LoRA, combining random initialization and multi-head dropout to diversify starting points and inputs, thereby improving task-specific learning. This approach enables LLMs to better learn task-specific knowledge by breaking the symmetry of initial parameters and diversifying optimization trajectories. Extensive experiments demonstrate the effectiveness of our proposed method. R-LoRA achieves significant improvements in multi-task scenarios while delivering modest gains in single-task contexts, showcasing its adaptability across a variety of tasks.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{cs_hyd.pdf}
  \caption {Cosine similarity among head matrices. "Overall mean" represents the average similarity across all layers.}
  \label{cosin}
\end{figure*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.32\linewidth]{tsne_down_proj.pdf}
  \includegraphics[width=0.32\linewidth]{tsne_gate_proj.pdf}
  \includegraphics[width=0.32\linewidth]{tsne_up_proj.pdf}
  \caption {T-SNE analysis of head matrices}
  \label{tsne}
\end{figure*}

\section{Related Works}

\subsection{LoRA}
Current LLMs generally follow a decoder-only structure, characterized by a series of blocks, each comprising two key components with residual connections: a multi-head self-attention (MHA) layer and a feed-forward network (FFN)~\cite{vaswani2017attention}. These layers involve using dense learnable matrices. 

There is a need to adapt LLMs for specific tasks or domains with limited resources. To achieve this, low-rank adaptation (LoRA)~\cite{hu2021lora}, inspired by the concept of low intrinsic dimensionality in LLMs, decomposes the weight gradient $\Delta \mathbf{W}$ into low-rank matrices, thereby reducing the number of trainable parameters. Specifically, for a dense weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, LoRA employs two low-rank matrices, $\mathbf{B} \in \mathbb{R}^{m \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times n}$, to approximate the accumulated gradient updates $\Delta \mathbf{W}$. The rank $r$ is chosen to be much smaller than the minimum of $d$ and $k$, effectively decreasing the number of trainable parameters. Consequently, the resulting weight matrix is expressed as $\mathbf{W} + \mathbf{B}\mathbf{A}$, and the output $h$ for an input $x$ through this updated weight matrix is formulated as:

\begin{equation}
    h = (\mathbf{W} + \Delta \mathbf{W}) x = \mathbf{W} x + \mathbf{B} \mathbf{A} x
    \label{eq:lora_output}
\end{equation}
Normally matrix B is initialized with zeroes and matrix A is initialized with Kaiming Uniform~\cite{he2015delving} to ensure that the initial outputs are consistent with the pre-trained model, thereby avoiding the introduction of random disturbances.

Following LoRA, AdaLoRA~\cite{zhang2023adalora} dynamically learns the rank size needed for LoRA in each layer of the model. DeltaLoRA~\cite{zi2023delta} updates the original weights of the model using parameters from adapter layers, enhancing LoRA’s representational capacity. LoSparse~\cite{li2023losparse} incorporates LoRA to prevent pruning from eliminating too many expressive neurons. DoRA~\cite{liu2024dora} introduces a magnitude component to learn the scale of $\Delta W$ while utilizing the original AB as a direction component of $\Delta W$. PiSSA~\cite{meng2025pissa} and LoRA-GA~\cite{wang2024loragalowrankadaptationgradient} have improved the convergence speed and performance of LoRA by refining its initialization method. Their approaches focus on optimizing the initial parameter settings, which enhances the training dynamics and leads to more efficient and stable convergence. 

\subsection{Multi-Head architecture}
\label{multi-head}
%MTL-LoRA~\cite{yang2024mtlloralowrankadaptationmultitask} and HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} are among the first methods to introduce the multi-head architecture in LoRA, which is characterized by a central shared down-projection matrix A and several distinct head matrices B. 
MTL-LoRA~\cite{yang2024mtlloralowrankadaptationmultitask} and HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} are pioneering methods that introduce the multi-head architecture into LoRA. This architecture is characterized by a central shared down-projection matrix A and multiple distinct head matrices B, enabling efficient and flexible adaptation across diverse tasks. 
As shown in Figure~\ref{architecture}, this architecture differentiates task-specific information while effectively capturing shared knowledge across various tasks. The Multi-Head architecture can be formulated as:

\begin{equation}
    W + \Delta W = W + \sum_{i=1}^{N} \omega_i \cdot B_i A
    \label{eq:multi-head}
\end{equation}

In HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}, the weights $w_i$ are computed through the routing matrix $W_r$ and the softmax function. It can be formulated as:
\begin{equation}
    \omega = Softmax(W_{r} x)
    \label{eq:multi-head}
\end{equation}
Normal routing matrix is initialized with Kaiming Uniform~\cite{he2015delving}. R-LoRA retains the same architecture as HydraLoRA, ensuring consistency in the routing mechanism and weight computation.

\subsection{Dropout}
Dropout is a widely used technique to prevent overfitting in deep networks by randomly deactivating units during training \cite{srivastava2014dropout}. This process samples from an exponential number of thinned networks, reducing unit co-adaptation and enhancing noise robustness. At test time, the full network is utilized, benefiting from the ensemble effect of the thinned networks. In our work, we adapt dropout to a novel context within the multi-head structure of R-LoRA. Specifically, we employ dropout to differentiate the inputs of the head matrices, ensuring that each head learns distinct and complementary representations.

\section{Motivation}
\label{Motivation}
In this section, we analyze the parameter similarity between different head matrices in the Multi-Head LoRA architecture. To achieve our objectives, we focus on HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} and use cosine similarity and the T-SNE method to observe the parameters of the head matrices. We fine-tune Qwen2.5-3B-Base \cite{qwen2.5} with HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} on five different tasks: Paraphrase Detection (QQP), Natural Language Inference (QNLI)~\cite{wang2018glue}, Commonsense Reasoning (SIQA)~\cite{sap2019socialiqa}, Physical Commonsense Reasoning (PIQA)~\cite{bisk2020piqa}, and Math (GSM8K)~\cite{cobbe2021training}. First, we flatten the head matrices into vectors and then calculate the cosine similarity between the vectors to obtain a similarity matrix. The average value of the matrix is regarded as the similarity of the head matrix corresponding to the parameter matrix. Additionally, we perform T-SNE analysis on all the head matrices. 

As shown in Figure~\ref{cosin} and Figure~\ref{tsne}, the average similarity between different head matrices still reaches around 80\%. With such a high similarity, the knowledge learned between different head matrices is also quite similar, which hinders the learning of task-specific knowledge. To the best of our knowledge, this is due to the zero initialization of the head matrices and the shared A matrix. After receiving the outputs from shared down-projection matrix A, the outputs of the head matrices are highly similar in the early stages of training, leading to highly similar update directions during gradient updates. The differences in the updates of the head matrices arise solely from the Kaiming uniform initialization~\cite{he2015delving} of the router matrix, which is insufficient.\\ \textbf{Reserach Question 1:} \textit{Is there a simple yet effective approach to differentiate head matrices such that they capture distinct task-specific knowledge?}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{architecture.pdf}
  \caption{Overview of the R-LoRA.}
  \label{R-LoRA}
\end{figure}

\begin{table*}
  \centering
  \begin{tabular}{lcc}
    \hline
    \textbf{Method} & \textbf{A Initialization} & \textbf{B Initialization} \\
    \hline
    LoRA     & $U\left( -\sqrt{\frac{3}{d_{in}}}, \sqrt{\frac{3}{d_{in}}} \right)$ & $0$ \\
    HydraLoRA & $U\left( -\frac{1}{d_{in}}, \frac{1}{d_{in}} \right)$ & $0$ \\
    R-LoRA    & $\frac{\sqrt[4]{d_{out}}}{\sqrt{\gamma}} \cdot N\left( 0, \frac{1}{d_{in}} \right)$ & $\frac{\sqrt[4]{d_{out}}}{\sqrt{\gamma}} \cdot N\left( 0, \frac{1}{d_{out}} \right)$ \\
    \hline
  \end{tabular}
  \caption{Comparison of initialization.}
  \label{initialization}
\end{table*}

\section{Method}
In this work, we propose R-LoRA, which leverages multi-head randomization to assist the model in learning distinct knowledge. Multi-head randomization consists of two components: multi-head dropout and random initialization. An overview of R-LoRA is illustrated in Figure~\ref{R-LoRA}\\ \textbf{Reserach Objective: } \textit{To exploit randomization to differentiate the head matrices, thereby facilitating the convergence of their parameters to distinct regions and enhancing the diversity among the head matrices.}

\subsection{Multi-Head Dropout}
Multi-Head LoRA architecture is characterized by a shared down-projection matrix A and several distinct head matrices B. In HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}, the head matrices receive the same output from the shared matrix A. According to ~\cite{hayou2024lora+} and~\cite{tian2024hydraloraasymmetricloraarchitecture}, the down-projection matrix A and the head matrix B in LoRA play distinct roles. We hypothesize that the down-projection matrix A is more inclined to learn task-agnostic knowledge, capturing general features applicable across tasks, while the head matrices tend to specialize in task-specific knowledge, enabling the model to differentiate and adapt to the unique requirements of individual tasks. This division of roles enhances the model's ability to balance generalization and specialization in multi-task learning scenarios. We propose employing multi-head dropout to differentiate the outputs of down-projection matrix A, thereby ensuring that the head matrices produce distinct outputs. The framework of Multi-Head dropout and R-LoRA is shown in Figure~\ref{R-LoRA}. Our architecture is similar to HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}, but it introduces multi-head dropout. The input, after being processed by the down-projection matrix A, obtains a task-agnostic representation. Multi-head dropout diversifies this representation, enabling the model to learn task-specific knowledge from multiple perspectives, enhancing both generalization and task adaptability.

\subsection{Multi-Head Random Initialization}
The zero initialization of the head matrices results in identical starting points for the different head matrices during training, causing them to converge to similar positions. As shown in Table~\ref{initialization}, we utilize non-zero initialization for the head matrices to provide them with distinct starting points during training, thereby encouraging them to converge to different positions. However, multi-head dropout introduces greater variance to the inputs of the head matrices. While this variance aids the model in learning diverse knowledge, it may also lead to scaling instability. Inspired by \cite{he2015delving} and \cite{wang2024loragalowrankadaptationgradient}, we introduce a coefficient $\frac{\sqrt[4]{d_{out}}}{\sqrt{\gamma}}$ or $\frac{\sqrt[4]{d_{in}}}{\sqrt{\gamma}}$ during initialization to the matrix in order to ensure scale stability. The $\gamma$ is a hyperparameter, and we follow the setting of \cite{wang2024loragalowrankadaptationgradient} by setting it to 64. When the head matrices are initialized with non-zero values, the $\Delta \mathbf{W}$ is no longer zero. To ensure that the initial outputs are approximately consistent with the pre-trained model and reduce introducing disturbances, we subtract the initial $\Delta \mathbf{W}$ from the original parameter matrix $\mathbf{W}$. It can be formulated as:

\begin{equation}
    W = W - \frac{1}{N}\sum_{i=1}^{N} B_i \cdot A
    \label{eq:multi-head}
\end{equation}



\begin{table*}[ht]
  \centering
  \begin{tabular}{l c c c c c c c c c c c}
    \hline
    \textbf{Schemes} & \textbf{General} & \textbf{Medical} & \textbf{Law} & \textbf{Code}  & \textbf{Math}  & \textbf{Avg} & \textbf{\%Param} & \textbf{\#A} & \textbf{\#B} \\
    \hline
    Base*  & 38.88 & 35.98 & 33.51 & 20.34 & 10.38 & 27.82 & - & - & -  \\
    Full* & 49.91 & 46.78 & 46.08  & 32.93 & 25.70 & 40.28 & 100 & - & -  \\
    \hline
    Prompt Tuning*  & 39.91 & 37.59 & 35.02  & 21.55 & 13.18 & 29.45 & 0.001 & - & -  \\
    P-Tuning*  & 41.11 & 39.81 & 36.72  & 21.13 & 15.56 & 30.87 & 0.193 & - & -  \\
    Prefix Tuning*  & 41.78 & 40.28 & 36.54  & 22.56 & 16.89 & 31.61 & 0.077 & - & -  \\
     \(IA^{3}\)*  & 40.45 & 37.12 & 35.25  & 23.17 & 13.98 & 29.99 & 0.009 & - & - \\
    \hline
    LoRA$(r=8)$  & 43.44 & 41.18 & 37.95 & 22.82 & 18.72 & 32.82 & 0.062 & 1 & 1 \\
    AdaLoRA*$(r=8)$  & 44.32 & 42.83 & 39.36 & 23.78 & 19.51 & 33.96 & 0.093 & 1 & 1 \\
    LoRA$(r=16)$  & 45.12 & 43.22 & 40.24 & 25.22 & 20.14 & 34.79 & 0.124 & 1 & 1 \\
    HydraLoRA$(r=8)$  & 47.12 & 45.28 & 43.28 & 27.43 & 22.27 & 37.08 & 0.124 & 1 & 3 \\
    \hline
    R-LoRA$(r=8)$  & 47.79 & 45.31 & 43.22 & 27.27 & 22.12 & \textbf{37.13} & 0.124 & 1 & 3 \\
    \hline
  \end{tabular}
  \caption{Comparison of different training schemes on single task. * indicates results from \cite{tian2024hydraloraasymmetricloraarchitecture}}
  \label{single}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{l | cccccc}
    \hline
    \textbf{Metrics} & \textbf{Base} & \textbf{LoRA} & \textbf{LoRAHub*}& \textbf{LoRA MoE*}& \textbf{HydraLoRA}& \textbf{R-LoRA}\\
    \hline
    7B     & 31.6 & 37.2 & 39.7 & 40.3 & 41.8 & \textbf{42.3} \\
    13B & 38.4 & 40.9 & 41.9 & 43.7 & 44.7 & \textbf{45.4} \\
    A/B for training  & - & 1/1 & 48/48 & 48/48 & 1/10 & 1/10 \\
    A/B for inference & - & 1/1 & 20/20 & 48/48 & 1/10 & 1/10 \\
    \% Param & - & 0.062 & 1.240 & 2.976 & 0.341 & 0.341 \\
    \hline
  \end{tabular}
  \caption{Comparison of different training schemes on multi task. * indicates results from \cite{tian2024hydraloraasymmetricloraarchitecture}}
  \label{multi}
\end{table*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{cs_mt.pdf}
  \caption {Cosine similarity among head matrices in R-LoRA. "Overall mean" represents the average similarity across all layers.}
  \label{cosin_mt}
\end{figure*}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{gradient_norm.pdf}
  \caption{Gradient norms of HydraLoRA and R-LoRA.}
  \label{Gradient}
\end{figure}

\section{Experiment}
In this section, we validate the superiority of R-LoRA across various models and settings. First, we followed the settings of \cite{tian2024hydraloraasymmetricloraarchitecture} and conducted experiments on the LLaMA-2 model~\cite{touvron2023llama}, evaluating both single-task and multi-task scenarios. Subsequently, we tested the performance of R-LoRA under different multi-task settings on the new Qwen2.5~\cite{qwen2.5}. The model sizes range from 0.5B to 13B. Through an extensive ablation study, we demonstrate the effectiveness of the multi-head randomization in R-LoRA.
\subsection{Experiment Setting}

\textbf{Model:} In the single-task setting, we use LLaMA2-7B, while in the multi-task setting, we additionally incorporated LLaMA2-13B. In the ablation study, we use Qwen2.5-0.5B and Qwen2.5-3B models.

\noindent\textbf{Dataset \& Benchmarks:} 

\noindent\textbf{Single-task:}  
\begin{itemize}
    \item \textbf{General:} We fine-tune the model using the general instruction tuning dataset Databricks-Dolly-15k~\cite{conover2023free}, which focuses on generic language capabilities. The performance is then evaluated using the MMLU benchmark~\cite{hendrycks2020measuring}.
    \item \textbf{Medical:} We fine-tune the model using GenMedGPT and the Clinic-10k dataset from ChatDoctor~\cite{li2023chatdoctor}, targeting medical applications. The model’s performance on medical tasks is assessed using MMLU.
    \item \textbf{Law:} Fine-tuning is conducted using two legal instruction datasets, Lawyer-Instruct~\cite{alignment_lab_ai_lawyer_instruct} and US-Terms~\cite{chalkidis2023lexfiles}, and the model is evaluated on legal tasks using MMLU.
    \item \textbf{Code:} We fine-tune the model using the CodeAlpaca~\cite{codealpaca} for code generation tasks, and the evaluation is conducted using the HumanEval~\cite{chen2021evaluating}.
    \item \textbf{Math:} The model is fine-tuned on the training set of GSM8K~\cite{cobbe2021training} to enhance its mathematical reasoning capabilities and is evaluated on the corresponding test set
\end{itemize}

\noindent\textbf{Multi-task:}  
We fine-tune the model using a subset of the Flanv2 dataset~\cite{brown2020language} that includes tasks from both Natural Language Understanding (NLU) and Natural Language Generation (NLG), grouped into 10 distinct task clusters. The model’s performance is evaluated using the Big-Bench Hard (BBH) benchmark. More Details of the datasets will be provided in the appendix~\ref{appendix}.

\noindent\textbf{Baseline:}
First, we compare R-LoRA against various PEFT methods on single datasets: 1) Full fine-tuning; 2) Prompt Tuning~\cite{lester2021power}; 3) P-Tuning~\cite{liu2024gpt}; 4) Prefix Tuning~\cite{li2021prefix}; 5)  \(IA^{3}\)~\cite{liu2022few}; 6) AdaLoRA~\cite{zhang2023adalora}; 7) HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}. Second, we extend the comparison by evaluating R-LoRA against other weighted averaging methods across multiple datasets: 1) Lorahub~\cite{huang2023lorahub}, which utilizes black-box optimization to learn weights for 20 randomly selected LoRAs for new tasks, applying weighted averaging without the need for gradient calculations; 2) LoRA MoE~\cite{liu2024moemeetsllmsparameter}, which combines lightweight experts (LoRA) with a Mixture of Experts (MoE) architecture for high efficiency, enabling generalization to new tasks without prior knowledge; 3) HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}, which employs Multi-Head structure in conjunction with MoE to achieve a balance between parameter efficiency and training effectiveness.


\subsection{Performance}
\subsubsection{Performance of R-LoRA on Single Task}
As shown in Table~\ref{single}, in the single-task setting, where the knowledge and text format of the data are relatively homogeneous, multi-head randomization does not yield significant performance improvements. Nevertheless, R-LoRA achieves performance on par with HydraLoRA, demonstrating that the multi-head randomization mechanism preserves learning effectiveness while maintaining stability for single-task scenarios. This highlights R-LoRA's robustness and adaptability, even in settings where its full potential may not be fully utilized.

\subsubsection{Performance of R-LoRA on Multi-Tasks}
The evaluation across diverse tasks, as shown in Table~\ref{multi}, demonstrates that R-LoRA, building upon the foundation of HydraLoRA, consistently outperforms all other schemes. By introducing multi-head dropout and random initialization for the head matrices, R-LoRA further enhances the model's stability and adaptability. The performance gains of R-LoRA, rooted in these multi-head randomization techniques, surpass those of both conventional PEFT methodologies and HydraLoRA.


\begin{table*}
  \centering
  \begin{tabular}{l | cccccc}
    \hline
    \textbf{Schemes} & \textbf{Task1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg}\\
    \hline
    HydraLoRA & 90.97 & 80.30 & 77.20 & 65.80 & 49.20 & 72.69 \\
    +MT       & 91.20 & 80.80 & 77.50 & 66.20 & \textbf{49.40}& 73.02  \\
    +Init     & 91.40 & 81.20 & 77.10 & 66.10 & 49.10 & 72.98  \\
    R-LoRA    & \textbf{91.74} & \textbf{81.50 }& \textbf{77.60} & \textbf{67.80} & 49.30 & \textbf{73.59} \\
    \hline
  \end{tabular}
  \caption{Results of Ablation Studies on Qwen2.5-0.5B with Different Schemes Across Various Tasks. The table compares the performance of HydraLoRA, +MT (HydraLoRA with Multi-Head Dropout), +Init (Multi-Head Random Initialization), and R-LoRA across five tasks.}
  \label{5-task}
\end{table*}

\begin{table*}[ht]
  \centering
  \begin{tabular}{l | ccccccccc}
    \toprule
    % \cline{2-6}
    \textbf{Schemes} & \textbf{Task1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{Avg}\\
    \midrule
    \multicolumn{6}{l}{\textit{Qwen2.5-0.5B}}\\
    \midrule
    HydraLoRA & 87.5 & 77 & 73.5 & 63.5 & 52.5 & 52 & 54.5 & 52.5 & 64.13\\
    +MT       & 88 & 78.5 & 74.5 & 66.5 & 54.75 & 53.5 & 58.5 & 52.5 & 66.22\\
    +Init     & 87.5 & 79.75 & 75.75 & 65.75 & 53 & 58 & 56 & 55 & 66.31\\
    R-LoRA    & 89 & 82.5 & 76.5 & 66.5 & 55 & 57 & 55.5 & 56.5 & \textbf{67.31}\\
    \midrule
    \multicolumn{6}{l}{\textit{Qwen2.5-3B}}\\
    \midrule
    HydraLoRA & 95.5 & 84 & 83.5 & 83.5 & 71.25 & 72 & 83.5 & 88 & 82.66\\
    +MT       & 96 & 83.5 & 83.5 & 84.5 & 71.75 & 72 & 83.5 & 88 & 82.84\\
    +Init     & 96.5 & 84 & 83.5 & 84.75 & 71.5 & 73 & 85 & 88.5 & 83.34\\
    R-LoRA    & 96.5 & 83.5 & 85 & 86.5 & 72.75 & 73.5 & 86 & 89 & \textbf{84.09}\\
    \bottomrule
  \end{tabular}
  \caption{Results of Ablation Studies on Qwen2.5-0.5B and Qwen2.5-3B Models with Different Schemes Across Various Tasks. The table compares the performance of HydraLoRA, +MT (HydraLoRA with Multi-Head Dropout), +Init (Multi-Head Random Initialization), and R-LoRA across eight tasks.}
  \label{8-task}
\end{table*}

\subsection{Parameter Analysis}
\textbf{Reserach Question2:} \textit{Does multi-head randomization effectively enhance the acquisition of diverse knowledge across the head matrices?}

In this section, we analyze the parameter differences among the head matrices in R-LoRA. The methodology and experimental setup align with those described in Section~\ref{Motivation}. As shown in Figure~\ref{cosin_mt}, the parameter similarity between head matrices in R-LoRA is reduced to below 70\%. This significant decrease indicates that multi-head randomization effectively enhances the model's capacity to learn task-specific knowledge, thereby mitigating redundant learning and increasing the diversity of acquired knowledge across tasks. T-SNE analysis will be shown in appendix~\ref{more result} 


\subsection{Training Process}
\textbf{Reserach Question3:} \textit{Does multi-head randomization impact the stability of the training process?}

As illustrated in Figure~\ref{Gradient}, R-LoRA benefits from multi-head randomization, exhibiting significantly larger gradient norms in the early stages of training compared to HydraLoRA. This drives the head matrices to converge to distinct regions, enhancing the model's ability to capture diverse representations and improving overall performance. Furthermore, R-LoRA demonstrates greater training stability than HydraLoRA, as evidenced by its more stable gradient norms throughout the training process. This stability enables the model to effectively acquire diverse knowledge without compromising training efficiency.



\subsection{Ablation Study}
In this section, we empirically validate the effectiveness of R-LoRA's multi-head randomization components through extensive experiments. Ablation studies were conducted on two models, Qwen2.5-0.5B and Qwen2.5-3B, under two task settings: 5-task and 8-task configurations. For the 5-task setting, models were fine-tuned on 5 datasets spanning two categories (NLU and commonsense reasoning). For the 8-task setting, models were trained on 11 datasets across 8 categories, with all models evaluated on their respective test sets. The experimental results are presented in Table~\ref{5-task} and Table~\ref{8-task}. Details of the datasets will be provided in the appendix~\ref{ablation}. 

Experimental results demonstrate that the two key components of multi-head randomization—random initialization and dropout—are pivotal for enhancing the model's adaptability across tasks. Random initialization assigns unique weights to each head matrix, enabling the capture of task-specific patterns and reducing the risk of head convergence. Dropout diversifies inputs to the head matrices, fostering distinct learning pathways and reducing redundancy. Together, these components improve task-specific feature capture while ensuring robustness in multi-task learning.

\section{Conclusion}
In this work, we first investigated the multi-head structure of LoRA and analyzed the parameters of the head matrices, revealing that they remain highly similar. To address this, we proposed R-LoRA, which introduces multi-head randomization—a simple yet effective approach—to enable the model to learn knowledge from different tasks, thereby enhancing its performance in multi-task scenarios. This method not only improves the model's generalization capabilities but also supports its adaptability across diverse tasks. Extensive experiments have validated the superiority of R-LoRA. Parameter analysis demonstrates that multi-head randomization effectively differentiates the head matrices, enabling them to learn knowledge from distinct tasks. This capability significantly enhances the model's performance in multi-task scenarios, confirming the effectiveness of the proposed approach.

\section{Limitation}
Despite the promising results of R-LoRA, several limitations should be acknowledged. While empirical evidence supports the effectiveness of multi-head randomization, a rigorous theoretical analysis of its underlying mechanisms remains absent. Additionally, multi-head random initialization does not ensure consistency with the pre-trained model's outputs, potentially introducing random disturbances. Future work could explore data-driven initialization as a promising approach to enhance the learning of task-specific knowledge by the head matrices, a direction we intend to pursue further.

%\section{Acknowledgment}
%We acknowledge the use of AI tools for language polishing and proofreading in the preparation of this manuscript.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

% \section{Appendix}
\section{Datasets}
\label{appendix}
\subsection{Single-task}
\begin{enumerate}
    \item \textbf{General}: We fine-tune with the general instruction tuning dataset \texttt{databricks-dolly-15k} for generic language capability and evaluate with MMLU.
    
    \item \textbf{Medical}: We fine-tune with \texttt{GenMedGPT} and \texttt{clinic-10k} from \texttt{ChatDoctor} for medicine applications and evaluate medical tasks in MMLU including three related tasks: "clinical knowledge", "professional medicine", and "college medicine".
    
    \item \textbf{Law}: We fine-tune with two legal instruction tuning datasets \texttt{Lawyer-Instruct} and \texttt{US-Terms} then evaluate with law tasks in MMLU including two related tasks: "professional law" and "international law".
    
    \item \textbf{Math}: We fine-tune with the training split of \texttt{GSM8K} for mathematical reasoning and evaluate with the test set of \texttt{GSM8K}.
    
    \item \textbf{Code}: We fine-tune with \texttt{CodeAlpaca} for code generation and evaluate with \texttt{HumanEval}.
\end{enumerate}

\subsection{Multi-task}
For complex mixed multi-task/domain, we select a portion of the \texttt{Flanv2} datasets covering Natural Language Understanding (NLU) and Natural Language Generation (NLG), which can be grouped into 10 distinct task clusters. Then we evaluate it with the Big-Bench Hard (BBH) benchmark.

We summarize the details of the used datasets as follows:

\begin{enumerate}
    \item \textbf{Struct-to-Text Conversion}: This task evaluates the capability to generate natural language descriptions from structured data inputs. We use the following datasets: (1) CommonGen; (2) DART; (3) E2ENLG; (4) WebNLG
    
    \item \textbf{Translation}: Translation involves converting text from one language to another, maintaining the original meaning and nuances. We use the following datasets: (1) En-Fr from WMT'14; (2) En-De, En-Tr, En-Ru, En-Fi, En-Ro from WMT'16; (3) En-Es from Paracrawl.
    
    \item \textbf{Commonsense Reasoning}: This involves assessing the ability to apply physical or scientific principles alongside common sense in reasoning tasks. We use the following datasets: (1) COPA; (2) HellaSwag; (3) PiQA; (4) StoryCloze.
    
    \item \textbf{Sentiment Analysis}: A fundamental task in natural language processing (NLP) that determines the sentiment polarity (positive or negative) of a given text. We use the following datasets: (1) IMDB; (2) Sentiment140; (3) SST-2; (4) Yelp.
    
    \item \textbf{Paraphrase Detection}: This task requires models to ascertain whether two sentences convey the same meaning, indicating semantic equivalence. We use the following datasets: (1) MRPC; (2) QQP; (3) Paws Wiki.
    
    \item \textbf{Coreference Resolution}: Involves identifying instances within a text that refer to the same entity, demonstrating an understanding of textual context. We use the following datasets: (1) DPR; (2) WSC273.
    
    \item \textbf{Reading Comprehension}: Assesses the capability to derive answers to questions from a provided text containing relevant information. We use the following datasets: (1) BoolQ; (2) DROP; (3) MultiRC; (4) OBQA; (5) SQuADv1; (6) SQuADv2.
    
    \item \textbf{Reading Comprehension with Commonsense}: Merges traditional reading comprehension skills with commonsense reasoning, requiring understanding beyond the explicit text. We use the following datasets: (1) CosmosQA; (2) ReCoRD.

    
    \item \textbf{Natural Language Inference}: Focuses on deducing the relationship between two sentences, determining if the second sentence logically follows from, contradicts, or is unrelated to the first sentence. We use the following datasets: (1) ANLI; (2) CB; (3) MNLI; (4) QNLI; (5) SNLI; (6) WNLI; (7) RTE.
    
    \item \textbf{Closed-Book Question Answering}: This task challenges models to answer questions about general knowledge without direct access to external information sources. We use the following datasets: (1) ARC; (2) NQ; (3) TriviaQA.

\end{enumerate}

\subsection{Ablation Study}
\label{ablation}
Due to limited computational resources, we selected a subset of the dataset for training and testing.
\textbf{Five tasks}:
    \begin{itemize}
        \item Task 1: Sentiment Analysis (SST2)
        \item Task 2: Paraphrase Detection (QQP)
        \item Task 3: Natural Language Inference (QNLI)
        \item Task 4: Physical Commonsense Reasoning (PiQA)
        \item Task 5: Commonsense Reasoning (SiQA)
    \end{itemize}

\textbf{Eight tasks}:
    \begin{itemize}
        \item Task 1: Sentiment Analysis (SST2)
        \item Task 2: Paraphrase Detection (QQP)
        \item Task 3: Natural Language Inference (MNLI + QNLI)
        \item Task 4: Reading Comprehension (BoolQ + OBQA)
        \item Task 5: Commonsense Reasoning (PiQA + SiQA)
        \item Task 6: Reading Comprehension with Commonsense (CosmosQA)
        \item Task 7: Coreference Resolution (SiQA)
        \item Task 8: Closed-Book Question Answering (ARC)
    \end{itemize}
    
\section{Baselines}

\begin{enumerate}

    \item \textbf{Prompt Tuning}: This method adds task-specific prompts to the input. These prompt parameters are updated independently while the pretrained model parameters remain frozen.

    \item \textbf{P-Tuning}: This method incorporates trainable prompt embeddings into the input, optimized by a prompt encoder to automatically discover effective prompts, removing the need for manual design. Prompt tokens can be placed anywhere in the input sequence, and anchor tokens are introduced to enhance performance.

    \item \textbf{Prefix Tuning}: This method prefixes a series of task-specific vectors to the input sequence. These prefix parameters can be learned while keeping the pretrained model frozen. The prefix parameters are inserted into all layers of the model.

    \item \textbf{\(IA^{3}\)}: This method enhances efficiency by infusing learned vectors into transformer architectures, drastically reducing the number of trainable parameters.

    \item \textbf{AdaLoRA}: Unlike LoRA, which distributes parameters evenly across all modules, AdaLoRA optimizes the number of trainable parameters assigned to weight matrices and layers. More parameters are allocated to important weight matrices and layers, while less important ones receive fewer parameters.

    \item \textbf{LoraHub} randomly aggregates 20 LoRAs for new downstream tasks. It employs a black-box optimization technique to determine the weight of each LoRA, eliminating the need for gradient calculations of the large model. This involves parameter-level weighted averaging.
    
    \item \textbf{LoRA MoE}. A collection of $n$ parameterized experts, denoted as $E_1, \ldots, E_n$, is orchestrated by a router network $R$. $E_i=B_iA_i$. Router network features a dense layer with adjustable weights $W_R$ from $\mathbb{R}^{d_m \times n}$. A softmax function then processes an intermediate token representation $x$, yielding gating scores $s_1, \ldots, s_n$ that determine the weighted contribution of each expert's output:
\begin{equation}
    s_i = R(x)_i = \text{softmax}(Top(W_R^T x, K))
\end{equation}
Subsequently, the overall output $y$ is synthesized by aggregating the Top-K experts' outputs, each modulated by its respective gating score:
\begin{equation}
    y = \sum_{i=1}^n s_i \cdot E_i(x) \quad (\text{MoE})
\end{equation}
This results in a dynamic allocation of the model's capacity, enabling specialized processing by experts as directed by the router's gating mechanism.

    \item \textbf{HydraLoRA} uses a shared matrix $A$ and multiple matrices $B_1, \ldots, B_n$. The shared matrix $A$ is used to project the input vector $x$ into a lower-dimensional space, while each matrix $B_i$ is used to modulate the output of the corresponding expert $E_i$. The overall output $y$ is synthesized by aggregating the experts' outputs, each modulated by its respective gating score:
\begin{equation}
    y = \sum_{i=1}^n s_i \cdot (B_i \cdot A \cdot x)
    \tag{7}
\end{equation}
This approach allows for efficient parameterization and specialization of the model's capacity, leveraging the shared matrix $A$ for common transformations and the individual matrices $B_i$ for task-specific adjustments.

\end{enumerate}

\section{More Results}
\label{more result}
The T-SNE analysis of R-LoRA has been shown in Figure~\ref{tsne_mt_down},~\ref{tsne_mt_up},~\ref{tsne_mt_gate}
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{tsne_mt_down_proj.pdf}
  \caption{T-SNE analysis of head matrices(down\_proj) in R-LoRA.}
  \label{tsne_mt_down}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{tsne_mt_up_proj.pdf}
  \caption{T-SNE analysis of head matrices(up\_proj) in R-LoRA.}
  \label{tsne_mt_up}
\end{figure}\begin{figure}[t]
  \includegraphics[width=\columnwidth]{tsne_mt_gate_proj.pdf}
  \caption{T-SNE analysis of head matrices(gate\_proj) in R-LoRA.}
  \label{tsne_mt_gate}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{train_loss.pdf}
  \caption{Training loss curves of HydraLoRA and R-LoRA. The loss of M-LoRA remains lower throughout the entire training process..}
  \label{loss}
\end{figure}

\end{document}
