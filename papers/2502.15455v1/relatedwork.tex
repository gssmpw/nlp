\section{Related Works}
\subsection{LoRA}
Current LLMs generally follow a decoder-only structure, characterized by a series of blocks, each comprising two key components with residual connections: a multi-head self-attention (MHA) layer and a feed-forward network (FFN)~\cite{vaswani2017attention}. These layers involve using dense learnable matrices. 

There is a need to adapt LLMs for specific tasks or domains with limited resources. To achieve this, low-rank adaptation (LoRA)~\cite{hu2021lora}, inspired by the concept of low intrinsic dimensionality in LLMs, decomposes the weight gradient $\Delta \mathbf{W}$ into low-rank matrices, thereby reducing the number of trainable parameters. Specifically, for a dense weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, LoRA employs two low-rank matrices, $\mathbf{B} \in \mathbb{R}^{m \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times n}$, to approximate the accumulated gradient updates $\Delta \mathbf{W}$. The rank $r$ is chosen to be much smaller than the minimum of $d$ and $k$, effectively decreasing the number of trainable parameters. Consequently, the resulting weight matrix is expressed as $\mathbf{W} + \mathbf{B}\mathbf{A}$, and the output $h$ for an input $x$ through this updated weight matrix is formulated as:

\begin{equation}
    h = (\mathbf{W} + \Delta \mathbf{W}) x = \mathbf{W} x + \mathbf{B} \mathbf{A} x
    \label{eq:lora_output}
\end{equation}
Normally matrix B is initialized with zeroes and matrix A is initialized with Kaiming Uniform~\cite{he2015delving} to ensure that the initial outputs are consistent with the pre-trained model, thereby avoiding the introduction of random disturbances.

Following LoRA, AdaLoRA~\cite{zhang2023adalora} dynamically learns the rank size needed for LoRA in each layer of the model. DeltaLoRA~\cite{zi2023delta} updates the original weights of the model using parameters from adapter layers, enhancing LoRAâ€™s representational capacity. LoSparse~\cite{li2023losparse} incorporates LoRA to prevent pruning from eliminating too many expressive neurons. DoRA~\cite{liu2024dora} introduces a magnitude component to learn the scale of $\Delta W$ while utilizing the original AB as a direction component of $\Delta W$. PiSSA~\cite{meng2025pissa} and LoRA-GA~\cite{wang2024loragalowrankadaptationgradient} have improved the convergence speed and performance of LoRA by refining its initialization method. Their approaches focus on optimizing the initial parameter settings, which enhances the training dynamics and leads to more efficient and stable convergence. 

\subsection{Multi-Head architecture}
\label{multi-head}
%MTL-LoRA~\cite{yang2024mtlloralowrankadaptationmultitask} and HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} are among the first methods to introduce the multi-head architecture in LoRA, which is characterized by a central shared down-projection matrix A and several distinct head matrices B. 
MTL-LoRA~\cite{yang2024mtlloralowrankadaptationmultitask} and HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture} are pioneering methods that introduce the multi-head architecture into LoRA. This architecture is characterized by a central shared down-projection matrix A and multiple distinct head matrices B, enabling efficient and flexible adaptation across diverse tasks. 
As shown in Figure~\ref{architecture}, this architecture differentiates task-specific information while effectively capturing shared knowledge across various tasks. The Multi-Head architecture can be formulated as:

\begin{equation}
    W + \Delta W = W + \sum_{i=1}^{N} \omega_i \cdot B_i A
    \label{eq:multi-head}
\end{equation}

In HydraLoRA~\cite{tian2024hydraloraasymmetricloraarchitecture}, the weights $w_i$ are computed through the routing matrix $W_r$ and the softmax function. It can be formulated as:
\begin{equation}
    \omega = Softmax(W_{r} x)
    \label{eq:multi-head}
\end{equation}
Normal routing matrix is initialized with Kaiming Uniform~\cite{he2015delving}. R-LoRA retains the same architecture as HydraLoRA, ensuring consistency in the routing mechanism and weight computation.

\subsection{Dropout}
Dropout is a widely used technique to prevent overfitting in deep networks by randomly deactivating units during training \cite{srivastava2014dropout}. This process samples from an exponential number of thinned networks, reducing unit co-adaptation and enhancing noise robustness. At test time, the full network is utilized, benefiting from the ensemble effect of the thinned networks. In our work, we adapt dropout to a novel context within the multi-head structure of R-LoRA. Specifically, we employ dropout to differentiate the inputs of the head matrices, ensuring that each head learns distinct and complementary representations.