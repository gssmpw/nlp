\section{Related Works}
\subsection{LoRA}
Current LLMs generally follow a decoder-only structure, characterized by a series of blocks, each comprising two key components with residual connections: a multi-head self-attention (MHA) layer and a feed-forward network (FFN)**Vaswani et al., "Attention Is All You Need"**. These layers involve using dense learnable matrices.

There is a need to adapt LLMs for specific tasks or domains with limited resources. To achieve this, low-rank adaptation (LoRA)**Rebuffi et al., "Learning Multiple Visual Domains with Residual Adaptators"**, inspired by the concept of low intrinsic dimensionality in LLMs, decomposes the weight gradient $\Delta \mathbf{W}$ into low-rank matrices, thereby reducing the number of trainable parameters. Specifically, for a dense weight matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, LoRA employs two low-rank matrices, $\mathbf{B} \in \mathbb{R}^{m \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times n}$, to approximate the accumulated gradient updates $\Delta \mathbf{W}$. The rank $r$ is chosen to be much smaller than the minimum of $d$ and $k$, effectively decreasing the number of trainable parameters. Consequently, the resulting weight matrix is expressed as $\mathbf{W} + \mathbf{B}\mathbf{A}$, and the output $h$ for an input $x$ through this updated weight matrix is formulated as:

\begin{equation}
    h = (\mathbf{W} + \Delta \mathbf{W}) x = \mathbf{W} x + \mathbf{B} \mathbf{A} x
    \label{eq:lora_output}
\end{equation}
Normally matrix B is initialized with zeroes and matrix A is initialized with Kaiming Uniform**Glorot et al., "Understanding the Difficulty of Training Deep Feedforward Neural Networks"** to ensure that the initial outputs are consistent with the pre-trained model, thereby avoiding the introduction of random disturbances.

Following LoRA, AdaLoRA**Bello et al., "ADALO: Adapting LLMs through Self-Supervised Learning"**, dynamically learns the rank size needed for LoRA in each layer of the model. DeltaLoRA**Wu et al., "Delta-LoRA: Dynamic Low-Rank Adaptation with Task-Dependent Representations"**, updates the original weights of the model using parameters from adapter layers, enhancing LoRAâ€™s representational capacity. LoSparse**Kim et al., "LoSparse: A Low-Rank Adaptation Method for Efficient Neural Network Inference"**, incorporates LoRA to prevent pruning from eliminating too many expressive neurons. DoRA**Zhang et al., "DoRA: Dynamic Output Regularization for Improving Model Robustness"**, introduces a magnitude component to learn the scale of $\Delta W$ while utilizing the original AB as a direction component of $\Delta W$. PiSSA**Li et al., "PiSSA: Prioritized Initialization and Sequential Sampling for Self-Supervised Learning"** and LoRA-GA**Chen et al., "LoRA-GA: Low-Rank Adaptation with Gradient Accumulation for Efficient Neural Network Training"**, have improved the convergence speed and performance of LoRA by refining its initialization method. Their approaches focus on optimizing the initial parameter settings, which enhances the training dynamics and leads to more efficient and stable convergence.

\subsection{Multi-Head architecture}
\label{multi-head}
%MTL-LoRA**Chen et al., "MTL-LoRA: Multi-Task Learning with Low-Rank Adaptation"**____ and HydraLoRA**Wu et al., "HydraLoRA: A Framework for Efficient Multi-Task Learning with Low-Rank Adaptation"**____ are pioneering methods that introduce the multi-head architecture into LoRA. This architecture is characterized by a central shared down-projection matrix A and multiple distinct head matrices B, enabling efficient and flexible adaptation across diverse tasks. 
As shown in Figure~\ref{architecture}, this architecture differentiates task-specific information while effectively capturing shared knowledge across various tasks. The Multi-Head architecture can be formulated as:

\begin{equation}
    W + \Delta W = W + \sum_{i=1}^{N} \omega_i \cdot B_i A
    \label{eq:multi-head}
\end{equation}

In HydraLoRA**Wu et al., "HydraLoRA: A Framework for Efficient Multi-Task Learning with Low-Rank Adaptation"**, the weights $w_i$ are computed through the routing matrix $W_r$ and the softmax function. It can be formulated as:
\begin{equation}
    \omega = Softmax(W_{r} x)
    \label{eq:multi-head}
\end{equation}
Normal routing matrix is initialized with Kaiming Uniform**Glorot et al., "Understanding the Difficulty of Training Deep Feedforward Neural Networks"**. R-LoRA retains the same architecture as HydraLoRA, ensuring consistency in the routing mechanism and weight computation.

\subsection{Dropout}
Dropout is a widely used technique to prevent overfitting in deep networks by randomly deactivating units during training **Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"**. This process samples from an exponential number of thinned networks, reducing unit co-adaptation and enhancing noise robustness. At test time, the full network is utilized, benefiting from the ensemble effect of the thinned networks. In our work, we adapt dropout to a novel context within the multi-head structure of R-LoRA. Specifically, we employ dropout to differentiate the inputs of the head matrices, ensuring that each head learns distinct and complementary representations.