\section{Introduction}
\Ac{IS} is one of the most important activities in our daily life and work. In the past few decades, search engines have become the main way people access information by locating relevant documents from the Web \cite{schutze2008introduction}. In recent years, the rapid development of \ac{LLM} has opened new opportunities to improve \ac{IS}, shifting from ranking relevant documents to producing reliable answers \cite{zhao2023survey}. 
However, generating answers directly with \ac{LLM} presents challenges, including missing real-time information, insufficient domain knowledge, and the risk of hallucinate claims, resulting in unreliable responses in real-world scenarios \cite{huang2024survey, tonmoy2024comprehensive, feng2023trends}.

%To address these challenges, researchers have proposed \ac{RAG}, which combines the strength of search systems and \acp{LLM} to improve the quality of results \cite{gao2023retrieval}.
To this end, \ac{RAG} has emerged as a promising solution by combining the strength of search systems and \acp{LLM} to improve the quality of results \cite{gao2023retrieval, zhao2024retrieval, li2024structrag}.
On one hand, \ac{RAG} leverages search to process external large corpus, enhancing access to real-time information. On the other hand, it utilizes \ac{LLM} to reason and generate text, improving the accuracy of the answer.
To better integrate the retriever and the generator, the researchers further developed various \ac{RAG} frameworks to improve the accuracy of answers, such as Self-RAG~\cite{asai2023self}, ActiveRAG~\cite{xu2024activerag}, CoRAG~\cite{wang2025chain}, etc.
In addition to improving accuracy, some studies also try to improve source attribution of the result, thus improving the reliability of results \cite{zhou2024trustworthiness, guan2025deeprag, friel2024ragbench}, such as InstructRAG~\cite{wei2024instructrag}, LongCite~\cite{zhang2024longcite}, SelfCite~\cite{chuang2025selfcite}, etc.
%For example, \citet{asai2023self} proposes Self-RAG to adaptively retrieve, generate, and critique through self-reflection to improve the factual accuracy of generated results.
%\citet{xu2024activerag} introduces ActiveRAG, a multi-agent framework that empowers \acp{LLM} to actively engage with and learn from retrieved evidence. 
%\citet{wang2025chain} introduces the chain-of-retrieval augmented generation (CoRAG) by fine-tuning \acp{LLM} to conduct iterative retrieval and reasoning for complex queries.
%In addition to improving accuracy, some studies also try to improve source attribution of the result, thus improving the reliability of results \cite{zhou2024trustworthiness}. For example, \citet{xia2024ground} improves retrieval-augmented \acp{LLM} with interleaved reference-claim generation to produce answer with fine-grained citations. \citet{zhang2024longcite} introduces LongCite to enable \acp{LLM} to generate fine-grained sentence-level citations in long-context question answering.
However, most of these works focus on improving specific aspects of the \ac{RAG} framework, while real-world applications require systematic enhancements across all components.
 
In addition to the above advances, researchers have created various open-source systems \cite{Liu_LlamaIndex_2022, guo2024lightrag, jin2024flashrag, zhang2024raglab} to support the development and practical application of \ac{RAG}.
For example, Langchain~\footnote{https://github.com/langchain-ai/langchain} is the most widely used \ac{RAG} framework, providing modular components for integrating \acp{LLM} with external data sources.
LLamaIndex~\footnote{https://github.com/run-llama/llama\_index} serves as a data framework designed to efficiently construct \ac{RAG} applications by streamlined data ingestion, indexing and querying processes.
LightRAG~\footnote{https://github.com/HKUDS/LightRAG} introduces a dual-level retrieval mechanism by incorporating the graph structure into the text indexing and retrieval. 
Nevertheless, these frameworks focus mainly on modularizing the process and simplifying the implementation of \ac{RAG} systems, further efforts are needed to improve the attribution of RAG-generated content. 

\begin{figure}[!t]
\centering
\includegraphics[scale=0.43]{./figures/architecture.png}
\caption{An Overview of the System Architecture.}
\label{fig:architecture}
\end{figure}

In this demo, we introduce a novel \ac{RAG} system, named TrustRAG, to improve the accuracy and attribution of the result in a more comprehensive way. The overall architecture of the system consists of two main components: 
1) \textbf{the TrustRAG library}: an easy to use \ac{RAG} library which improves three major components of the RAG framework, including semantic-enhanced indexing, utility-enhanced retrieval, and attribution-enhanced generation; 
2) \textbf{the TrustRAG studio}: a user-friendly and interactive Web interface which enables users to browse, configure, experience, and create RAG applications.
%With the TrustRAG system, researchers can: 1) quickly build a reliable \ac{RAG} application with simple configuration using their own proprietary data sources, 2) fully controlled configuration of the indexing, retrieval, and generation pipeline,  ..., 3) apply ....
Additionally, we provide an example application for \textit{Excerpt-Based Question Answering} (ExQA) built on TrustRAG. The ExQA is characterized by answers typically presented as a list, where each entry is derived from one or more documents and must be traceable to the original text. 
Our work makes the following key contributions:
\begin{itemize}
    \item \textbf{The TrustRAG Studio for No-Code \ac{RAG} Application Development}: The studio enables users to create their own \ac{RAG} applications without writing any code. It incorporates \textit{corpus management} and \textit{conversation management}--with corpus management, users can upload their own documents, configure chunking parameters,  and apply semantic-enhanced indexing, while conversation management enables them to adjust retrieval parameters, choose generation models, and engage in interactive Q\&A based on their corpus.
    \item \textbf{The TrustRAG Library for Low-Code \ac{RAG} Experimentation}: The library enables low-code experimentation with RAG by offering a comprehensive pipeline that covers indexing, retrieval, and generation, featuring over 20 modular components. Additionally, it incorporates semantic-enhanced indexing, utility-enhanced retrieval, and citation-enhanced generation, allowing users to flexibly combine these modules to build trustworthy RAG systems.
    \item \textbf{An Example Application for Excerpt-Based Question Answering}: The ExQA is a typical information seeking scenario that focuses on fine-grained information extraction from structured corpora. It aggregates and distills attribute-specific information from multiple sources to produce a final answer. The answers must be both clearly structured and fully traceable, with each entry linked to its corresponding text spans in the original documents. Common applications include Q\&A on legal documents, policies and regulations, and product manuals.
    \item \textbf{An open-source implementation} that lowers the barrier for researchers and developers to apply TrustRAG on the client side. We provide comprehensive documents to help users use TrustRAG to implement on-device \ac{RAG} system across different Web environments. The TrustRAG studio and library are publicly accessible at \url{https://huggingface.co/spaces/golaxy/TrustRAG} and \url{https://github.com/gomate-community/TrustRAG}.
\end{itemize}
