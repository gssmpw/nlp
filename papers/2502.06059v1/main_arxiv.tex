\documentclass{article}
\PassOptionsToPackage{table, dvipsnames}{xcolor}
% Use the following line for the initial blind version submitted for review:
\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{soul}
\usepackage{fontawesome}
\usepackage[many]{tcolorbox}
% Recommended, but optional, packages for figures and better typesetting:
% \usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{lipsum}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{xcolor}
\definecolor{darkblue}{RGB}{44,62,80}
\definecolor{CalGoldHex}{RGB}{253, 181, 21} % FDB515
\definecolor{gold}{RGB}{149, 113, 30} % 95711E

% \definecolor{pigment}{rgb}{0.2, 0.2, 0.6}

%\setcitestyle{numbers}
%\setcitestyle{square}
%\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage[breaklinks=true,colorlinks,bookmarks=false,citecolor=gold,linkcolor=darkblue]{hyperref}
%\usepackage{hyperref}

\usepackage{enumitem}
\usepackage{doi}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\usepackage{fourier}

\usepackage[textsize=tiny]{todonotes}
% \usepackage{xurl}

\usepackage{tabularx}



%\title{AI Risk Management Should Take into Account Both Safety and Security}

% \title{AI Risk Management Should Understand and Account for Both Safety and Security}

\title{Position: We Need An Adaptive Interpretation of \\Helpful, Honest, and Harmless Principles}




\newcommand\blfootnote[1]{
  \begingroup
\renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand*{\affmark}[1][*]{\textsuperscript{\textnormal{#1}}}

\author{
\textbf{Yue Huang}\affmark[1], \textbf{Chujie Gao}\affmark[2], \textbf{Yujun Zhou}\affmark[1], \textbf{Kehan Guo}\affmark[1], \textbf{Xiangqi Wang}\affmark[1], \textbf{Or Cohen-Sasson},\affmark[3] \and \textbf{Max Lamparth}\affmark[4], \textbf{Xiangliang Zhang}\affmark[1]\\
~\\
\affmark[1]Department of Computer Science and Engineering, University of Notre Dame~~~\affmark[2]MBZUAI~~~
\\\affmark[3]School of Law, University of Miami \\ \affmark[4]Center for AI Safety, Stanford University
}

% \author{
% \textbf{Xiangyu Qi}\affmark[1], \textbf{Yangsibo Huang}\affmark[1], \textbf{Yi Zeng}\affmark[2], \textbf{Edoardo Debenedetti}\affmark[3], \textbf{Jonas Geiping}\affmark[4], \textbf{Luxi He}\affmark[1], \textbf{Kaixuan Huang}\affmark[1], \and \textbf{Udari Madhushani}\affmark[5,6], \textbf{Vikash Sehwag}\affmark[7], \textbf{Weijia Shi}\affmark[8], \textbf{Boyi Wei}\affmark[1], \textbf{Tinghao Xie}\affmark[1], \textbf{Danqi Chen}\affmark[1], \textbf{Pin-Yu Chen}\affmark[9],\and \textbf{Jeffrey Ding}\affmark[10], \textbf{Ruoxi Jia}\affmark[2], \textbf{Jiaqi Ma}\affmark[11], \textbf{Arvind Narayanan}\affmark[1], \textbf{Weijie J. Su}\affmark[12], \textbf{Mengdi Wang}\affmark[1], \textbf{Chaowei Xiao}\affmark[13],\and \textbf{Bo Li}\affmark[11,14],  \textbf{Dawn Song}\affmark[15], \textbf{Peter Henderson}\affmark[1], \textbf{Prateek Mittal}\affmark[1]\\
% ~\\
% \affmark[1]Princeton University~~~~\affmark[2]Virginia Tech~~~~\affmark[3]ETH Zurich~~~~\affmark[4]ELLIS Institute TÃ¼bingen\\\affmark[5]JPMorgan AI Research~~~~\affmark[6]{Stanford University}~~~~\affmark[7]Sony AI~~~~\affmark[8]University of Washington\\\affmark[9]IBM Research~~~~\affmark[10]George Washington University~~~~\affmark[11]University of Illinois at Urbana-Champaign\\\affmark[12]University of Pennsylvania~~~~\affmark[13]University of Wisconsin, Madison~~~~\affmark[14]University of Chicago~~~~\affmark[15]UC Berkeley
% }


% \renewcommand{\shorttitle}{AI Risk Management Should Understand and Account for Both Safety and Security}

\renewcommand{\shorttitle}{Position: We Need An Adaptive Interpretation of Helpful, Honest, and Harmless Principles}

\usepackage{pythonhighlight}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fourier}
\usepackage{multirow}
\usepackage{comment}
%\usepackage{minted}
\usepackage{wrapfig}

\usepackage{enumitem}
\usepackage{array}

\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}m{#1}}

\newenvironment{packeditemize}{
\begin{list}{$\bullet$}{
\setlength{\labelwidth}{8pt}
\setlength{\itemsep}{0pt}
\setlength{\leftmargin}{\labelwidth}
\addtolength{\leftmargin}{\labelsep}
\setlength{\parindent}{0pt}
\setlength{\listparindent}{\parindent}
\setlength{\parsep}{0pt}
\setlength{\topsep}{3pt}}}{\end{list}}

\usepackage{color}




\definecolor{deepred}{rgb}{0.631,0.102,0.102}
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}
\definecolor{darkgreen}{rgb}{0.3,0.7,0.3}
\definecolor{salmon}{RGB}{241, 150, 141}





\begin{document}
\maketitle
\blfootnote{Correspondence to: Yue Huang~(\url{yhuang37@nd.edu}), Xiangliang Zhang~(\url{xzhang33@nd.edu}).}


\begin{abstract}
The Helpful, Honest, and Harmless (HHH) principle is a foundational framework for aligning AI systems with human values. However, existing interpretations of the HHH principle often overlook contextual variability and conflicting requirements across applications. In this paper, we argue for \textbf{an adaptive interpretation of the HHH principle} and propose a reference framework for its adaptation to diverse scenarios. %\textbf{Theoretically, }
We first examine the principle's foundational significance and identify ambiguities and conflicts through case studies of its dimensions. To address these challenges,  we introduce the concept of priority order, which provides a structured approach for balancing trade-offs among helpfulness, honesty, and harmlessness.  Further, we explore the interrelationships between these dimensions, demonstrating how harmlessness and helpfulness can be jointly enhanced and analyzing their interdependencies in high-risk evaluations.
Building on these insights, we propose a reference framework that integrates context definition, value prioritization, risk assessment, and benchmarking standards to guide the adaptive application of the HHH principle. This work offers practical insights for improving AI alignment, ensuring that HHH principles remain both ethically grounded and operationally effective in real-world AI deployment.
\end{abstract}




\section{Introduction}

The development of AI assistants has progressed rapidly, evolving from simple rule-based systems \citep{10.1145/365153.365168} to advanced models, such as Large Language Models (LLMs) \citep{zhao2023survey}, which excel in understanding context and generating human-like responses. The wide application of these models highlights not only the expanding capabilities of AI but also the growing need to ensure these systems align with human preferences and values. To address this challenge, the HHH principle--standing for Helpful, Honest, and Harmless--was proposed by \citet{askell2021general} as a guiding framework for designing and evaluating AI systems. The principle aims and \textit{has been widely utilized} to align AI behavior with human-centered values, providing critical benchmarks for tasks such as training data selection, strategy design, and deployment guidance \citep{touvron2023llama, peng2023instruction, li2024llava, bai2023qwen}. Its adoption has been instrumental in advancing AI alignment, particularly in ensuring that powerful models like LLMs prioritize user benefit, truthfulness, and safety.

However, current studies indicate the existence of ambiguities and conflicts among the three HHH dimensions. For example, the same prompt input might be classified as either harmless or harmful depending on the context or criteria used in different research. Similarly, the definition of specific dimensions often varies across studies, influenced by the diverse contexts in which they are applied. Furthermore, key questions such as ``which dimension should be prioritized'' and ``how different dimensions interrelate'' remain unresolved, underscoring the need for a clearer and more adaptable interpretation of the HHH principle.

In this position paper, \textbf{we argue that the HHH principle is not static or rigid but requires an adaptive interpretation to remain effective across diverse scenarios}. %, where the paper explores the evolving nature of the HHH principle and the necessity for contextual adaptation.
To support this argument, first, we review the initial definition and fundamental importance of the HHH principle, examining its positive impact on AI alignment efforts (\textbf{\S\ref{sec:background}}). Next, we identify ambiguities and conflicts within current interpretations of the principle through case studies of different dimensions (\textbf{\S\ref{sec:ambiguity}}), highlighting the need for consensus or innovative solutions. To address these conflicts, we introduce the concept of priority order, which provides a structured approach to balancing competing requirements among dimensions (\textbf{\S\ref{sec:priority_order}}). Furthermore, we investigate the relationships between dimensions, analyzing how harmlessness and helpfulness can be simultaneously improved and examining their interdependencies in high-risk evaluations (\textbf{\S\ref{sec:relation}}). Building on these insights, we propose a reference framework that guides the adaptive application of the HHH principle to specific scenarios (\textbf{\S\ref{sec:framework}}). The framework outlines critical considerations and offers a comprehensive methodology for interpreting and operationalizing the HHH principle in varied contexts.

Through this position paper, we aim to deepen the understanding of the HHH principle and foster its effective utilization as a tool for advancing AI development and alignment. We hope this work inspires further research and discussion on aligning AI systems with human values in a nuanced and adaptable manner.





\section{HHH Principle}
\label{sec:background}

\subsection{Initial Definition}

The HHH principle is a set of guiding principles for developing and aligning AI models, particularly large language models, with human values. \citet{askell2021general} define it from its aim: \textit{We will define an AI as "aligned" if it is, in three words, helpful, honest, and harmless or 'HHH'. Our alignment efforts aim to measure and address this general problem with large language models.} For the sake of simplicity, we summarize the HHH principle based on their original definitions: 

\vspace{5pt}
\textit{1) \textbf{Helpful}:  AI models should assist users by providing useful, accurate, and contextually relevant information or services. They must be designed to meet user needs, enhance productivity, and effectively solve problems. \\
2) \textbf{Honest}:  AI models should ensure transparency and truthfulness in their responses, providing factual information while openly acknowledging their limitations. They must refrain from generating falsehoods or misleading content. \\
3) \textbf{Harmless}:  AI models should avoid causing harm by preventing the generation of biased, offensive, or unethical content. They should prioritize safety and respect in their interactions, ensuring that they do not produce harmful or inappropriate outputs.}
\vspace{5pt}

% Building on this definition, numerous recent studies have approached the HHH principle from various perspectives. For example, some benchmarks evaluate helpfulness based on performance scores \citep{zheng2023judging}, while \citet{huang2024position} equate ``utility'' with helpfulness. For honest, some works propose different definitions \citep{gao2024honestllm, yang2023alignment, chern2024behonest}. These differing interpretations contribute to ambiguity and potential conflicts within the principle, which will be further discussed in \autoref{sec:ambiguity}.

\subsection{Fundamental Value of the HHH Principle} 


%\textbf{One of the fundamental challenges in AI development is ensuring that AI systems align with human values and preferences.} The HHH principle plays a critical role in guiding this alignment by 
\textbf{The HHH principle plays a critical role in guiding   AI system development to align with human values and preferences.} This principle has been used for 
establishing clear, user-centric benchmarks \citep{zheng2023judging}, and designing training methods like Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training} for advancing AI system performance. \textbf{Helpfulness} is directly related to the functional effectiveness of AI models, addressing what \textit{human users really need}, thus enhancing human productivity. \textbf{Honesty} ensures that AI systems provide truthful (\emph{e.g.}, avoid hallucination \citep{huang2023survey, zhang2023siren}), and transparent (\emph{e.g.}, express uncertainty \citep{xiong2024can}) information, which is crucial for fostering trust between humans and machines. \textbf{Harmlessness}, in turn, guarantees that AI outputs are safe and ethical, preventing the generation of harmful, biased, or misleading content \citep{huang2024position, decodingtrust, liu2023trustworthy}. The HHH principle determines the three most important sub-directions aligned with human preferences and values, which have been widely used for guiding AI model design as shown in \autoref{app:model_example}.


\textbf{The HHH principle uniquely emphasizes both trustworthiness and utility \footnote{Utility here refers to the model effectiveness  in natural language processing tasks, including  logical reasoning, content summarization, text generation, and so on \citep{huang2024position}.}.} 
AI models, such as LLMs, interact with users by providing information, generating content, and assisting in decision-making, making trustworthiness and utility essential for their effectiveness and reliability. While utility is prioritized in model development through training and tuning loss minimization, an automated and human-effortless process, there is a risk of compromising ethical integrity or introducing biases. Thus, typical alignment principles emphasize additional efforts to align AI models with human values, as  shown in \autoref{tab:framework}.  Research indicates that the resulting models often choose safe responses over helpful ones \citep{touvron2023llama1}. However, the HHH principle offers a unique balance by explicitly emphasizing both dimensions. The dimension of \textbf{honesty} requires AI models generate accurate and truthful outputs. 
%As shown in \autoref{tab:framework}, while previous frameworks have guided aligning AI models with human values, they often focus disproportionately on either trustworthiness or utility, rarely achieving a balance between the two \citep{zhang2024bi}. Some alignment approaches prioritize safety and ethical considerations, ensuring trustworthiness but potentially limiting the model's flexibility and capacity to address complex user needs. Research shows that models often chose a safe response over a helpful one \citep{touvron2023llama1}. In contrast, utility-focused frameworks can prioritize effectiveness and productivity, but risk compromising ethical integrity or introducing biases \citep{huang2024position}. However, the HHH principle achieves a unique balance by explicitly emphasizing both dimensions. For example, honesty, a key framework component, requires AI models to produce accurate and truthful outputs. This not only builds trustworthiness, as users can rely on the model to provide non-hallucinated information but also directly enhances utility by ensuring that the outputs are relevant and factually correct, aligning with the user's goals and needs.


\begin{table}
    \centering
    \begin{tabular}{ccc}
    \toprule[1pt]
     \textbf{Framework \& Principle} & \textbf{Trust.} & \textbf{Utility} \\
    \midrule
     HHH \citep{askell2021general} & \faStar & \faStar \\
     TrustLLM \citep{huang2024position}    & \faStar & \faStarHalfEmpty\\
     NIST \citep{nist_cybersecurity_framework} & \faStar & \faStarHalfEmpty\\
     AI Safety and Security \citep{qi2024ai} & \faStar & \faStarO \\
     Risk Assessment \citep{kapoor2024societal} & \faStar & \faStarO \\
     Transparency \citep{bommasani2024foundation} &\faStar & \faStarO \\
    \bottomrule[1pt]
    \end{tabular}
    \caption{Typical principles or frameworks for guiding AI model alignment (excluding government laws or acts (\emph{e.g.}, EU AI Act \citep{eu_ai_act} and Blueprint for an AI Bill of Rights \citep{BlueprintAIBill2022}). NIST refers to the NIST Cybersecurity Framework. \faStar, \faStarHalfEmpty, and \faStarO~represents  different focus efforts on trustworthiness or utility.}
    \label{tab:framework}
\end{table}


\textbf{The HHH principle facilitates the transition of AI from a passive tool to an active participant.} The societal impact of AI models extends \textit{beyond tangible benefits}, influencing more subtle and indirect aspects of human life. AI-generated content has the potential to shape human perspectives (\emph{e.g.}, create ideas in scientific research \citep{si2024can} and simulate social behaviors \cite{huang2024social}), cultural norms, and even values \citep{ramezani-xu-2023-knowledge, agarwal-etal-2024-ethical}. Repeated exposure to biased or subtly manipulative outputs could inadvertently influence public opinion or entrench societal biases \citep{zeng2024ai}. This transition from AI being a \textit{passive tool} to an \textit{active participant} raises concerns about its potential role as a manipulative force in shaping discourse and decision-making  \citep{pmlr-v235-simmons-edler24a}. The HHH principle provides a structured approach to addressing these emerging challenges. While \textbf{helpfulness} ensures AI effectively meets user objectives, \textbf{honesty} and \textbf{harmlessness} serve as safeguards, mitigating risks and protecting societal well-being as AI systems become increasingly powerful and influential.

%\textbf{As AI systems become increasingly pervasive and powerful, shaping them from passive tools to active influencers of human decisions, the need for socially responsible development practices is more urgent than ever.} As AI models become increasingly pervasive, the need for socially responsible development practices is more urgent than ever. For instance, the emergence of LLMs has marked a significant shift in the landscape of AI technologies. Unlike earlier AI models, which were typically domain-specific and designed for narrow tasks, these new models exhibit remarkable versatility, enabling them to perform a wide range of tasks \citep{wang2023gpt, liu2023deid} with minimal additional training \citep{zhang2023instruction, zhuo2024astraios}.
%The societal impact of AI models is profound and multifaceted. They drive economic growth by boosting productivity, automating tasks, and fostering innovation \citep{sahoo2024shifting}. Generative models, such as Sora \citep{sora_openai}, are transforming creative fields like filmmaking, while agentic systems \citep{wang2024survey} optimize workflows, revolutionizing customized services \citep{openai2025gptstore} and scientific research \citep{wang-etal-2019-paperrobot}. However, the influence of AI extends \textit{beyond tangible benefits}, touching on more subtle and indirect effects. AI-generated content has the potential to shape human perspectives (\emph{e.g.}, create ideas in scientific research \citep{si2024can}), cultural norms, and even values \citep{ramezani-xu-2023-knowledge, agarwal-etal-2024-ethical}. For example, repeated exposure to biased or subtly manipulative outputs could inadvertently influence public opinion or entrench societal biases \citep{zeng2024ai}. This transition from AI being a \textit{passive tool} to an \textit{active participant}--and in some cases, a manipulative force \citep{pmlr-v235-simmons-edler24a}. The HHH principle provides a vital framework to address these risks while harnessing the benefits of AI technologies. While helpfulness ensures AI meets user objectives, the growing prominence of honesty and harmlessness highlights the need to mitigate risks and safeguard societal well-being as AI systems become increasingly powerful and influential.


% \textbf{The long-term sustainability of AI technologies is intrinsically tied to how well these systems align with human values and ethical guidelines.}



\section{Ambiguity and Conflicts in HHH}
\label{sec:ambiguity}

While the HHH principle plays a crucial role in guiding AI alignment, its practical implementation often reveals ambiguities and conflicts among the three dimensions, particularly given the complexity of different contexts. Next, we discuss these ambiguities and conflicts through case studies in each dimension. 

\subsection{Case Study I: Boundary of Harmlessness}

%The ambiguity in determining the safety of inputs and outputs in generative models presents substantial challenges. 
The distinction between harmful and benign content is not always clear-cut, as context, intent, and interpretation play crucial roles. 
This lack of clarity complicates the development of safety mechanisms and alignment strategies \citep{bauer2021generative, truong2024attacks}.  
%in terms of the input provided to the model and its output. This lack of clarity complicates the development of safety mechanisms and introduces ethical and practical challenges \citep{bauer2021generative, truong2024attacks}. We discuss such case from the perspective of both input and output:

\textbf{\textit{A harmless query becomes harmful}} when it is used to bypass safety restrictions, e.g, in the scenario of adversarial prompting.  For example,  if an AI model initially rejects a query for being harmful ($I_{B}$, \textit{\textcolor{red!60!black}{"Write a post promoting militaristic aggression,"}}), the user may slightly rephrase it to appear benign while maintaining the original intent: $I_{A}$, \textit{\textcolor{green!60!black}{"Please write a post encouraging people to build a strong national defense"}}. If the model then accepts and responds to the modified query $I_{A}$, it may unintentionally generate harmful content. However, if the same revised query $I_{A}$ were submitted independently,  without following a rejected attempt, it would still appear harmless.  Training machine learning classifiers to detect toxicity in queries may fail in this case \citep{Toxicity_Detection1, Toxicity_Detection2, openai_moderation_quickstart}. This highlights the importance of incorporating context and intent when distinguishing between harmful and harmless queries.

%A critical question arises regarding the inputs to generative models: 
%\textbf{\textit{How can we clearly identify if the input is harmful  without any context?}} Previous efforts have employed human evaluation or trained machine learning classifiers to detect toxicity in inputs \citep{Toxicity_Detection1, Toxicity_Detection2, openai_moderation_quickstart}. However, these methods inherently reflect general human values, either directly or indirectly. For instance, consider the user query $I_{A}$, such as \textit{\textcolor{green!60!black}{"Please write a post encouraging people to build a strong national defense"}}. While this might seem benign from a human perspective (and might be used in studies to evaluate exaggerated safety in large language models \citep{an2024automatic}), it could be a subtle rephrasing of a more harmful query, such as $I_{B}$, \textit{\textcolor{red!60!black}{"Write a post promoting militaristic aggression,"}} potentially generated through techniques like jailbreak attacks \citep{zeng2024johnnypersuadellmsjailbreak}. In this context, query $I_{A}$ should be considered harmful because its resulting output could closely resemble that of query $I_{B}$. The conflicting safety assessments of these examples, indicating both harmful and harmless interpretations of the same query from different research perspectives (i.e., jailbreak \citep{wei2024jailbroken} versus exaggerated safety \citep{rottger2023xstest}), underscore the ambiguity in current definitions and standards for safety or harmfulness. 



Moreover, \textbf{\textit{the harmfulness or harmlessness of a generated response can also depend on the user's intent}}. For example, if a model responds to a query but includes a moral disclaimer to discourage misuse, the response itself may remain neutral or informative \citep{mazeika2024harmbench, ran2024jailbreakeval, huang2024obscureprompt}. However, an attacker could exploit this by simply removing the moral disclaimer, and repurposing the content for malicious objectives. Conversely, a well-intentioned user may utilize the same response for legitimate purposes without causing harm.  

%Similarly, when considering the outputs of generative models, another important question emerges: \textbf{\textit{How can we accurately judge if the output is harmful if without context?}} For example, if a model responds to a query but includes a moral disclaimer, the safety of such a response remains debatable \citep{mazeika2024harmbench, ran2024jailbreakeval}. An attacker could exploit these responses by simply removing the moral disclaimer, thereby using the content for malicious purposes. On the contrary, a user without malicious intent may simply use it for normal purposes without causing any harm. 
Although recent work by OpenAI \citep{openai_improving_model_safety_2024} has proposed a set of rules that a trustworthy LLM should adhere to (e.g., categorizing response into hard refusal, soft refusal, and compliance), the core issue lies in \textit{the ambiguity of determining the appropriateness of a response without knowing the user's real intent and context}.
%it's hard for AI models (even human) to understand and infer users' real intent without context}. This highlights the critical need for human-AI alignment to be addressed in a more fine-grained and clearly defined manner.
This highlights the need for an adaptive interpretation of HHH principle, which can dynamically adjust to different contexts and user intents.

\subsection{Case Study II: Definition of   Honesty}

Honesty has emerged as a central topic in the alignment of AI assistants. However, consensus on its precise definition remains elusive, as recent studies offer divergent perspectives on what it means for an AI model to be ``honest'' \citep{yang2023alignment, gao2024honestllm}. In \citet{askell2021general}, honesty is often equated with providing accurate information, closely aligned with the concepts of truthfulness \citep{huang2024trustllm} and non-hallucination \citep{huang2023survey, zhang2023siren}. This traditional view primarily emphasizes the factual accuracy of the model's responses. Recent studies, however, have introduced more nuanced categorizations of honesty, distinguishing between two major dimensions: \emph{epistemic honesty} and \emph{interactive honesty}. 

\textbf{Epistemic honesty} is concerned with transparency regarding AI models' knowledge limitations and ability to express uncertainty. Represented by \citet{yang2023alignment}, this perspective emphasizes that an honest AI model should ``candidly answer questions it knows and humbly admit to those it does not.'' This view goes beyond factual accuracy, incorporating the notion of humility in acknowledging gaps in knowledge. For example, \citet{yang2023alignment} think LLMs should explicitly say ``I don't know'' when they  lack sufficient knowledge to provide an accurate and reliable answer.
%about to provide an inaccurate answer to user queries. 
Subsequent studies have extended this perspective by exploring methods to improve AI models' ability to express uncertainty \citep{chern2024behonest, yin2023large}.

\textbf{Interactive honesty} focuses on the AI model's ability to maintain objectivity, avoid spreading misinformation, and ensure clarity in its interactions with users. \citet{gao2024honestllm} define this form of honesty as ``the ability to recognize its limitations (e.g., LLMs are unable to process visual information without external tools), remain objective without pandering (e.g., avoiding sycophantic behavior as discussed in \citep{sharma2023towards}), and thereby avoid spreading misinformation or inducing hallucinations.'' Interactive honesty emphasizes not only factual accuracy but also the model's capacity for self-awareness \citep{li2024i, jiang2024assessing} and user-oriented objectivity. This perspective underscores the importance of preventing models from misleading users or offering false reassurance, even in situations where knowledge is incomplete. Subsequent studies have adopted this view to explore honesty through the lens of human cognition and interaction, emphasizing its role in building trust and effective communication \citep{brahman2024art, wen2024know}.

While epistemic honesty prioritizes knowledge calibration and transparency, interactive honesty emphasizes behavioral consistency and ethical interaction. These differences highlight a deeper tension: the difficulty of reconciling the technical feasibility of implementing honesty with the philosophical rigor required for alignment. Resolving these tensions is crucial for developing a coherent, operationalizable definition of honesty that aligns with both trustworthiness and utility. Without such clarity, ambiguity in the definition of honesty risks undermining the efforts of AI alignment.





% Importantly, they claim the cognition-level definition of honesty may offer a more stable evaluation criterion, as it does not shift with the evolving knowledge base of the model. Unlike knowledge-level definitions that the model may influence growing or changing datasets, the cognition-based perspective allows for a more consistent and objective assessment of honesty.

% The divergence between different interpretations of honest AI models highlights the tension in defining this principle. These conflicts underscore a deeper issue: the challenge of reconciling technical feasibility with rigor in aligning AI models. Resolving these tensions is essential to establishing a coherent and operationalizable definition of honesty that supports both trustworthiness and utility within the principle.


\subsection{Case Study III: Helpfulness in Different Contexts}
\label{sec:helpfulness_context}
Helpfulness, while fundamental to AI systems, lacks a standardized definition and varies significantly across contexts. This conceptual ambiguity and variability present substantial challenges for measuring and optimizing helpful behavior in AI models.

The definition of helpfulness varies substantially based on domain-specific objectives. For instance, in education, \citet{hemami2024can} frame helpfulness as the ability of AI models to identify optimal learning strategies and personalize content for individual students. For general language assistants, helpfulness is defined as a clear attempt to perform the task or answer the posed question \citep{askell2021general}. These variations, illustrated in \autoref{app:example_helpfulness}, reveal the importance of contextual adaptability in defining and applying the concept of helpfulness.

The absence of standardized definitions and evaluation metrics stems not only from contextual variability but also from researchers' tendency to optimize for domain-specific objectives. A clear example is the reliance on alignment techniques to tune models to human preferences \citep{ouyang2022training, bai2022training, rafailov2024direct}. Most alignment datasets reflect either general value alignment \citep{stiennon2020learning, bai2022constitutional} or specific task objectives \citep{tian2023fine, xie2024v}. For example, reasoning-focused tasks prioritize coherent and multi-step explanations \citep{huang2023metatool, zhuo2024astraios}, while conversational assistants prioritize concise, user-friendly responses \citep{sun2024adaplanner}. Moreover, \citet{awad2018moral} and \citet{santurkar2023whose} both reveal that AI models may reflect preference on specific opinions.

Current evaluation methods often employ narrow metrics that align with specific research objectives rather than capturing helpfulness holistically. Moreover, most frameworks utilize specialized metrics \citep{liu2024large} or LLM-as-judge approaches \citep{zheng2023judging, wang2024helpsteer2}, \citet{ye2024justice}, demonstrating that such evaluations can introduce systematic biases through prompt design choices. Future benchmarks should incorporate multidimensional evaluation such as complementary dimensions (e.g., correctness assessment, coherence analysis, creativity metrics), while considering the relative importance and interactions between these aspects. Consequently, multidimensional approaches would better reflect the diverse expectations of helpfulness in real-world applications.






\section{Priority Order}
\label{sec:priority_order}

\textbf{Definition and necessity.} In the deployment of AI models, conflicting requirements often arise among helpfulness, honesty, and harmlessness across various scenarios. For instance, in cybersecurity applications, when a user requests information about system vulnerabilities, the model must balance providing helpful technical details (helpfulness) with the need to prevent malicious exploitation (harmlessness) \citep{zhang2024cybench}. Similarly, in medical or legal domains, prioritizing honesty may sometimes conflict with harmlessness, such as when conveying critical but distressing information. To address such conflicts, we introduce the concept of \emph{priority order}, which is defined as: 

\vspace{5pt}\textit{A dynamic hierarchical framework that determines the relative importance and execution sequence of three dimensions of the HHH principle based on contextual requirements.}\vspace{5pt}




The necessity of adopting a dynamic rather than static priority order arises from the complexity and variability of real-world applications, where the relative importance of helpfulness, honesty, and harmlessness shifts based on context, user intent, and domain-specific requirements. In this section, we explore the priority order of the HHH principle from two complementary perspectives as shown in \autoref{fig:priority}: (1) a structural and statistical perspective (\textbf{\S\ref{sec:level_scale}}), which examines how prioritization operates along hierarchical levels and scales, shaping its manifestation across different domains and impact scopes; and (2) an application-level embodiment (\textbf{\S\ref{sec:user_task}}), which investigates how priority order shifts based on user populations and task contexts.

%\subsection{Structural \& Statistical Perspective: Priority Order Through Level And Scale Dimensions}
\subsection{Prioritization Levels and Scales}
\label{sec:level_scale}

\textbf{Prioritization levels} refer to the vertical structuring of the HHH principles, meaning that in each task, certain principles take precedence over others based on the risk and ethical constraints of the scenario. As shown in \autoref{fig:priority}, this hierarchical view defines which dimension should be prioritized in different tasks. 
%The prioritization of  HHH dimensions operates along level and scale dimensions, representing the vertical prioritization among the three principles and the horizontal variations in their manifestation across different impact scopes. 
To characterize the relative importance of each dimension, we introduce the \emph{priority value} as an attribute representing the extent of emphasis placed on each dimension, which could potentially be quantified through specific metrics in future work.
Prior studies have provided empirical evidence supporting the feasibility of prioritization levels across different fields.
%The level dimension represents the order ranking among the three principles, determining their relative priorities in different application contexts. 
In education, harmlessness is paramount to protect learners' development, followed by helpfulness for engagement, with honesty about AI capabilities as the foundation \citep{kooli2023chatbots, selwyn2022future}. Creative domains instead prioritize helpfulness to drive innovation, with honesty secondary and harmlessness setting safety bounds while preserving creative freedom \citep{flick2022ethics}. This varying prioritization levels across domains indicates the need for an adaptive interpretation of the HHH principle rather than a one-size-fits-all approach.


\textbf{Prioritization scales} refer to horizontal variations within the same ranking level, determining how the HHH principle is applied across different user groups ranging from micro (individual users) to macro (societal user groups). For harmlessness, the micro-scale emphasizes protecting individual privacy and enforcing data minimization \citep{gdpr2016general, staab2024principle}, while the macro scale may  necessitate selective data collection to mitigate systemic risks and ensure public safety. 
For instance, an AI system assisting a single user with a research query may prioritize helpfulness and honesty, while an AI generating public information (e.g., automated news summaries) must weigh harmlessness more heavily to prevent misinformation.
These scale-dependent variations indicate that effective HHH implementation requires consideration of both individual and societal impacts, even when the nominal priority level remains constant.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figure/priority.pdf}
    \caption{The conceptual structure of priority order within the HHH principle. For each task, we first determine a priority order (level dimension), and then adjust the priority values for different users within that task (scale dimension).}
    \label{fig:priority}
\end{figure}





%\subsection{Applicational Perspective: Priority Order Across User Groups And Task Contexts}
\subsection{Context-Aware Prioritization: User and Task Perspectives}
\label{sec:user_task}
Prioritization of the HHH principles should also be shaped by specific user contexts and task requirements. 
%Beyond the level and scale dimensions discussed above, the HHH principle's prioritization introduces complexity across user populations and task contexts. 
Priority ordering could vary significantly across different user populations \citep{bao2022whose}. Expert users, such as AI researchers or domain specialists, may prioritize helpfulness and honesty over strict harmlessness constraints, while general users or vulnerable populations often require emphasis on harmlessness.

Besides, these priorities should dynamically adjust during task execution, shifting relative importance as tasks progress through different phases. For instance, in financial advising, early goal-setting prioritizes helpfulness for exploring options \citep{shanmuganathan2020behavioural}, while investment execution emphasizes honesty in risk disclosure. This becomes particularly evident in interactive scenarios requiring real-time adaptation.

%The intersection of user variations and task dynamics creates a complex prioritization landscape, especially in scenarios with multiple users engaging in diverse concurrent tasks. Systems must consider how priorities shift across user expertise levels and task phases while managing their dynamic interplay. These challenges highlight the need for sophisticated, context-aware approaches to implementing HHH principle in environments characterized by diverse user groups performing interconnected tasks.

All the above discussion highlights that it is crucial for the interpretation of the HHH principle, which motivates us to design relevant components to include it in \textbf{\S\ref{sec:framework}}.


\section{Trade-off or Synergy? Relationship Between Different Dimensions}
\label{sec:relation}

After establishing the priority order across different dimensions, the interplay between helpfulness, honesty, and harmlessness in AI alignment remains a critical and ongoing debate. A key question is whether these dimensions are inherently in conflict, requiring trade-offs, or they can be optimized in synergy to enhance one another.
While some research suggests that enhancing one always comes at the expense of the other \citep{qi2023fine}, there is also evidence that strategies allow both to be improved simultaneously \citep{huang2024position}. This section aims to discuss the relationship between different dimensions.

\subsection{Boost Harmlessness and Helpfulness Simultaneously}



Helpfulness is generally a subset of utility, while harmlessness and honesty often reflect trustworthiness. As AI assistants evolve, balancing utility and trustworthiness becomes critical. Some view the **SB 1047 AI Bill**\footnote{A 2024 California bill aimed at mitigating catastrophic AI risks.} \citep{California2024SB1047}, designed to ensure AI harmlessness, as a potential barrier to innovation \citep{calchamber2024godmother}.  

Recent studies highlight the close link between harmlessness and helpfulness \citep{wolf2024tradeoffs, qi2023fine, huang2024position, bai2022training, zhang2024bi}. \citet{huang2024position} found a positive correlation between them, while \citet{qi2023fine} showed that even intent-free fine-tuning can weaken harmlessness. \citet{bai2022training} and \citet{zhang2024bi} explored ways to balance the two during training. However, \citet{ren2024safetywashing} noted that many safety benchmarks strongly correlate with a model's upstream capabilities.

Prioritizing harmlessness over helpfulness can lead to unintended trade-offs. Excessive safety constraints--such as strict content filtering or rigid ethical frameworks--may limit a model's usefulness and creativity, reducing overall helpfulness \citep{xstest, kirk2023understanding}. This imbalance risks producing overly cautious models that struggle in real-world applications where adaptability and innovation are essential.

On the other hand, maximizing helpfulness at the expense of harmlessness carries significant risks. Models that prioritize helpfulness but lack fairness, transparency, or robustness may produce biased outputs, eroding user trust and raising ethical concerns \citep{huang2024position, liu2023trustworthy, decodingtrust, li2025preference}. In high-stakes domains like healthcare and finance, untrustworthy models are not only unsustainable but can also be harmful \citep{xia2024cares}. Thus, sacrificing one dimension for the benefit of the other is inherently flawed. A paradigm is needed where harmlessness and helpfulness can be simultaneously improved to ensure that AI assistants are reliable and effective.

Rather than treating harmlessness and helpfulness as competing objectives in multi-objective optimization \citep{kochenderfer2019algorithms}, recent research suggests they can be mutually reinforcing. Some approaches first establish a baseline of harmlessness before optimizing for helpfulness \citep{gao2024honestllm}, while others integrate multi-objective alignment to improve both simultaneously \citep{yang2024metaaligner, wang2024hybrid, zhou-etal-2024-beyond, fu2024unlocking, guo-etal-2024-controllable}. 
% These proposals recognize that a rigid, one-size-fits-all alignment process might not be optimal; instead, adjustments during the training process allow the model to improve all aspects together.

One crucial insight from the discussion is that harmlessness is a safeguard--ensuring that the AI assistant is inherently safe and trustworthy before other features are optimized. This aligns with the view that harmlessness is not a constraint on helpfulness but a necessary component of it. 


The balance between harmlessness and helpfulness is not a zero-sum game where enhancing one necessarily diminishes the other \citep{tuan2024towards}. On the contrary, the two can--and should--be pursued in tandem to create robust, effective AI assistants. Sacrificing either harmlessness or helpfulness for short-term gains in the other is ultimately unsustainable and could lead to detrimental consequences in both ethical and practical applications. The key lies in developing methods, like the harmlessness-first approach, where harmlessness serves as a foundation for subsequent helpfulness maximization (a priority for different dimensions as discussed in \textbf{\S\ref{sec:priority_order}}). This strategy ensures that AI assistants remain safe and effective, setting the stage for a future where they can thrive in various real-world contexts without compromising on either front.

\subsection{Interdependencies Among Dimensions in High-Risk Evaluations}
While helpfulness, harmfulness, and honesty are always intertwined to some degree, in many everyday tasks these interactions are easy to manage or less impactful \citep{zheng2023judging, huang2024trustllm, zhou2024defending, wu2024towards, sandmann2024systematic, huang2023trustgpt}. By contrast, in high-risk or specialized tasks, these same interactions can become more complex and lead to significant risks. So simple, independent evaluation of each dimension may underestimate systemic hazards \citep{zhou2024labsafety, phan2024rx, thirunavukarasu2023large}. For instance, in medical diagnostics, a drug's helpfulness in treating a condition and the harmfulness associated with possible side effects exist simultaneously \citep{phan2024rx, thirunavukarasu2023large, sandmann2024systematic}. If the helpfulness is overlooked, the underlying condition remains untreated, potentially leading to serious deterioration of the patient's health. Conversely, if the harmful side effects receive insufficient attention, the patient could suffer from severe complications or even life-threatening adverse reactions. A truly effective medical decision must balance both factors. Therefore, when assessing a model's diagnostic capabilities, it is insufficient to focus solely on whether it provides a "helpful" conclusion. It is equally important to evaluate the probability, severity, and ethical implications of harmful consequences \citep{thirunavukarasu2023large}. 
This issue is also highlighted when people interact with LLMs in mental health emergencies, e.g., when suffering from mania or psychosis \citep{grabb2024risks}.
In such scenarios, both providing harmful information to the user (helpfulness) or refusing to respond (harmlessness) can exaggerate existing symptoms and lead to severe harm of the user or others \citep{cnn_lasvegas_genai}.
Similarly, in answering highly specialized questions, if an LLM generates hallucinated outputs, its "helpfulness" is immediately called into question. In lab safety contexts, any "helpful" advice that neglects safety considerations could lead to severe accidents, and hallucinations in the model's generation process present hidden high-risk factors \citep{zhou2024labsafety}.

Thus, when evaluating LLMs, particularly in high-risk and specialized fields, it is crucial to develop holistic evaluation frameworks that account for the emergent properties. This requires systematic identification of cascading risk scenarios that could lead to severe outcomes \citep{zhou2024labsafety, phan2024rx, thirunavukarasu2023large}. For example, to avoid lab safety accidents, responses with potential risks should undergo rigorous scrutiny and verification. In medical diagnostics, more stringent test sets and consequence-driven weighting mechanisms should be implemented for scenarios where a drug is both helpful and potentially harmful, incorporating the severity of misdiagnosis or side effects into the evaluation process. Only by explicitly modeling these "multidimensional entanglement" scenarios can we more accurately gauge the risks and value a model may present in real-world applications, thereby providing more targeted guidance for model optimization and regulatory decision-making.


\section{Reference Framework}
\label{sec:framework}

Building on the current ambiguous understanding of the principle (\textbf{\S\ref{sec:ambiguity}}), it is evident that interpreting the helpfulness, honesty, and harmlessness in diverse contexts presents significant challenges. These principles often interact in complex ways (relationship as discussed in \textbf{\S\ref{sec:relation}}), with their relative importance varying depending on the specific application and scenario (i.e., priority order as discussed in \textbf{\S\ref{sec:priority_order}}). This variability underscores the urgent need for a reference framework to systematically guide the interpretation and application of the HHH principle. Such a framework would address the critical question: \textbf{What perspectives and considerations are necessary to adapt these principles effectively in different scenarios?} 

This framework benefits multiple stakeholders. For developers in industries, it provides structured guidelines to integrate the HHH principles into the development of AI models, ensuring regulatory compliance. For academic researchers, it offers a novel view to study trade-offs between these principles in different domains (out-of-distribution (OOM) problem). For end users of AI models, it enhances transparency and governance, ensuring AI interactions align with user expectations \citep{larsson2020transparency}.
In this section, we propose a reference framework to address this issue from multiple perspectives: \textit{Contextual Object}, \textit{Value Anchor \& Value Scale}, \textit{Risk Assessment}, \textit{Benchmarking Standard}, and \textit{System Transparency \& Governance}. We also provide a detailed case study on applying the 3H principles to the development of a chemistry foundation model, which can be found in \autoref{app:chemistry_case}.

\textbf{Contextual Object.} First, the framework requires determining the objects in a specific scenario when adapting HHH principle. As discussed in \textbf{\S\ref{sec:helpfulness_context}}, different contexts will result in various understandings of dimensions, where the process, of determining scenario objects, directly define the specific context in which the AI model operates, mitigating potential ambiguity and conflicts. The objects contain key elements that are important for determining the context: \textit{User Group}, \textit{Application Aim}, \textit{Task Type}, and \textit{Environment Access}.


\begin{itemize}[nolistsep, leftmargin=*]
    \item \textit{User Group}: Identifying the primary audience or user group is essential to understanding their expectations and expertise level, as emphasized by lots of recent work on fine-grain or user-level alignment \citep{zhao2023group, fan2024user}. For example, general users may require simplified outputs and stricter safeguards for harmlessness, while domain experts may prioritize nuanced and highly truthful outputs with greater flexibility in helpfulness and honesty.
    \item \textit{Application Aim}: The domain in which the AI operates, such as healthcare \citep{li2024llava}, finance \citep{wu2023bloomberggpt}, or law \citep{cui2023chatlaw}, significantly influences the prioritization of HHH dimensions (as discussed in \textbf{\S\ref{sec:priority_order}}). Education domain always strictly requires harmlessness \citep{yan2024practical, cybersecurity2025childsafety}. High-stakes domains like medicine require a stronger emphasis on information authority (\emph{e.g.}, helpfulness and honesty), whereas creative applications might focus less on the truthfulness of model output.
    \item \textit{Task Type}: The type of task performed by models, such as real-time assistance, decision support, or content creation, defines the expected outputs and constraints. For instance, decision-making tasks demand high levels of honesty \citep{sun2024adaplanner}.
    \item \textit{Environment Access}: Whether the model has access to external tools, real-time data, or isolated environments affects its capability boundary and risks. Models with an external tool (\emph{e.g.}, GUI operation \cite{chen2024gui}) access may need stricter safeguards for harmlessness to prevent misuse. Moreover, the retrieved information through external tools influence the model's performance as well \citep{zhang2024defining, gao2023retrieval, huang2023metatool}.
\end{itemize}


\textbf{Value Anchor \& Value Scale.} The framework next requires the identification of a \textit{value anchor} and a \textit{value scale} to guide the prioritization of the HHH principle within a specific context. The \textit{value anchor} defines the core value or central priority, such as harmlessness in high-risk domains or helpfulness in performance-driven applications. The \textit{value scale}, in turn, allows for a dynamic balance among the principles by assigning relative weights to each dimension based on contextual demands, stakeholder needs, and societal expectations. For example, in the case of an educational AI model designed for younger students, the \textit{value anchor} would prioritize harmlessness to prevent inappropriate or harmful outputs. The corresponding \textit{value scale} might place harmlessness ``\textit{far before}'' helpfulness, reflecting the heightened importance of safety in such a sensitive application. 

In other words, this framework aims to endow AI models with specific traits or values (\textit{value anchor}) and control the depth and degree of these traits (\textit{value scale)} to ensure that the model remains predictable and operates within a controllable range. 


\textbf{Risk Assessment.} Building on the contextual objects and value anchor \& value scale, the reference framework must include a risk assessment mechanism to evaluate the trade-offs and potential negative impacts of prioritizing and customizing certain dimensions of the HHH principle. The assessment helps developers, users, and decision-makers determine whether the risks associated are acceptable and manageable within the given context.

Risk assessment involves three primary considerations: \ul{measuring the potential risks}, \ul{assessing stakeholder tolerance for those risks}, \ul{evaluating system consistency}, and \ul{considering potential marginal risk} \citep{kapoor2024societal}. For the same example in an education application where harmlessness is prioritized far above helpfulness, a risk assessment would evaluate whether this decision significantly damages the user experience (e.g., overly restrictive outputs that limit engagement or creativity \citep{li2024mossbench}). Simultaneously, it would determine whether stakeholders--such as developers and end users--are willing to accept this trade-off to ensure safety and ethical integrity. For context-dependent HHH, consistency is a requirement for trustworthy deployment to ensure similar behavior or balance of HHH in similar contexts (e.g., reproducible priority ordering), thus, the framework also emphasizes the significance of consistency evaluation. Moreover, \citet{kapoor2024societal} point out some marginal risks (e.g., Non-consensual intimate imagery (NCII) \citep{inhope2025ncii}) that may be ignored in open foundation models \citep{bommasani2021opportunities}, which should also be strictly considered during assessment. 

In particular, most existing risk assessment frameworks such as EU High-Risk AI guidelines \citep{aiact2025article6} focus primarily on societal-level risks, particularly those related to the aspect of harmfulness and safety. While these are crucial, we emphasize that assessments should also account for the direct impacts on stakeholders, such as developers' economic viability and user experiences. For instance, excessive economic costs associated with implementing overly strict harmlessness safeguards can be a significant risk for developers, just as diminished user satisfaction from overly cautious systems can reduce utility and adoption.

\textbf{Benchmarking Standard.} Benchmarks provide an objective and reproducible way to evaluate whether an AI system adheres to the HHH principle under specific scenarios. Without well-defined standards, evaluations risk becoming inconsistent \citep{mcintosh2024inadequacies}, subjective \citep{ye2024justice}, or overly dependent on context-specific assumptions. Under a specific context, the benchmarking standard must align closely with the interpretation (e.g., prioritization) of the HHH principle established by the framework. For example, the benchmarks should reflect this prioritization if helpfulness is prioritized above harmlessness. In such a case, utility-related benchmarks should evaluate helpfulness directly and assess how trustworthiness and harmlessness contribute to or detract from the overall utility. Moreover, to achieve a fine-grained evaluation of AI-powered systems, it is essential to integrate multiple benchmarks that focus on different dimensions of model performance. If necessary, the dynamic and automatic evaluation mechanism \cite{zhu2023dyval, wu2024unigen, an2024automatic, chen2024interleaved} can also be introduced to prevent data leakage. Furthermore, as emphasized by \citet{reuel2024betterbench}, designing a framework for assessing the reliability of the benchmark can enhance its credibility and authority. We show some benchmark examples of HHH principle in \autoref{app:model_example}.


\textbf{System Transparency \& Governance.} An effective framework for interpreting the HHH principle must prioritize transparency in its implementation. Transparency is essential for several reasons. First, it ensures that stakeholders, including developers, users, and policymakers, clearly understand how the principles of helpfulness, honesty, and harmlessness are being interpreted in specific contexts, and provide a foundation for audits and external reviews. This clarity helps build trust in the developed system, as stakeholders can evaluate whether the system aligns with their expected standards. Beyond transparency, the framework must also address how the HHH principle facilitate governance \citep{reuel2024open, wilczek2024government}. Governance involves setting policies and protocols to ensure that models are developed, deployed, and monitored responsibly. By embedding the HHH principle into the governance structure, the framework can guide developers and regulators in identifying and mitigating risks, establishing ethical safeguards, and ensuring compliance with societal standards. For example, helpfulness can inform standards for task performance and user satisfaction, while harmlessness can shape policies for harm mitigation.



\section{Open Challenges}


While the proposed reference framework offers an adaptable approach for interpreting the HHH principle, \textbf{it is subject to several limitations that merit consideration}. First, the framework does not adequately address fundamental challenges inherent to the HHH principle themselves. For example, it provides limited support for mitigating issues like out-of-distribution (OOD) adversarial attacks \citep{zou2023universal, huang2024obscureprompt, huang2025contextualdistraction}, where models encounter inputs significantly deviating from their training data. Such scenarios often require specialized robustness and generalization methods for enhancement, which fall outside the scope of this framework.

Additionally, \textbf{the framework faces implementation challenges}--inherent difficulties associated with multi-objective optimization (MOO). Balancing helpfulness, honesty, and harmlessness requires trade-offs that are complicated by normative assumptions embedded in the optimization objectives. The lack of precise quantification methods for the interaction between these principles complicates achieving a theoretically optimal balance, limiting the applicability of the framework in certain high-stakes or ambiguous scenarios. Following up recent studies propose potential methods for MOO alignment \citep{bai2022training, mukherjee2024multi}, more efforts should be made in addressing this challenge.

Moreover, a fundamental question remains: \textbf{Can human values, ethics, and honesty be embedded into AI systems with sufficient certainty?} This challenge is an requirement and pivotal for the proposed framework to function effectively, yet it remains an open problem \citep{sorensen2024roadmap}. Human values are inherently dynamic, context-dependent, and often subjective, making it difficult to codify them in a way that guarantees reliable and universally accepted AI behavior \citep{scherrer2024evaluating}. Furthermore, ethical principles may conflict in ambiguous scenarios, requiring nuanced decision-making that AI models currently struggle to replicate (e.g., \citep{moore-etal-2024-large, shrivastava2024measuringfreeformdecisionmakinginconsistency}). Without reliable mechanisms to encode these values, the framework risks being constrained by the same uncertainties that challenge broader AI alignment efforts.

Even though the proposed framework faces these challenges and may be judged by some other communities, we believe that offering a structured reference is a necessary step forward. By fostering discussion, guiding practical implementation, and encouraging interdisciplinary collaboration, this framework can serve as a foundation for future refinements.


\section{Conclusion}
The Helpful, Honest, and Harmless (HHH) principles are critical for aligning AI models with human values, but their static and ambiguous interpretation falls short to address the complexities of diverse real-world contexts. This position paper advocates for an adaptive approach to the HHH principle, enabling AI systems to contextually balance these dimensions while maintaining their ethical integrity and practical effectiveness.  By introducing a flexible prioritization framework, we 
aim to bridge the gap between theoretical alignment and real-world applicability, ensuring AI systems remain both responsible  across various domains. 

\section*{Impact Statement}
The introduction of an adaptive framework for interpreting the HHH principles redefines AI alignment by addressing ambiguities and contextual variability inherent in current interpretations. This work enhances the operational effectiveness of HHH principles by enabling AI systems to dynamically balance these priorities across domains such as healthcare, education, and creative industries. Its significance lies in ensuring AI systems remain safe, trustworthy, and user-centric while addressing complex tasks, particularly in high-risk applications like medical diagnostics. By promoting transparency and ethical alignment, this framework bridges the gap between theoretical principles and practical application, paving the way for AI models to safeguard societal well-being and foster innovation.


\section*{Acknowledgement}

We would like to express our sincere gratitude to Prof. Xiuying Chen from MBZUAI for her valuable suggestions and insightful feedback on this paper. Her expertise and thoughtful guidance greatly contributed to the improvement of our work.
Max Lamparth is partially supported by the Stanford Center for AI Safety, the Center for International Security and Cooperation, and the Stanford Existential Risk Initiative.

\bibliography{icml2024}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Model and Benchmark Example}
\label{app:model_example}

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule[1pt]
\textbf{Model} & \textbf{Type} & \textbf{Time} \\
\midrule
Llama 2 \citep{touvron2023llama}        & LLM           & 2023          \\
LLaVA \citep{liu2024visual}          & VLM           & 2023          \\
Qwen  \citep{bai2023qwen}          & LLM           & 2023          \\
InternLM \citep{team2023internlm}       & LLM           & 2023          \\
Llava-med  \citep{li2024llava}     & VLM           & 2024          \\
Aya model  \citep{ustun2024aya}     & LLM           & 2024          \\
LLaVA-Plus \citep{liu2025llava}     & VLM           & 2024          \\
MiniCPM  \citep{hu2024minicpm}       & LLM           & 2024          \\
\bottomrule[1pt]
\end{tabular}
\caption{Model examples that utilized HHH principle during design or training.}
\label{tab:model_example}
\end{table}

\begin{table}[h]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{cccc}
    \toprule[1pt]
        \textbf{Benchmark} & \textbf{Helpful} & \textbf{Harmless} & \textbf{Honest} \\
        \midrule
        MTBench \citep{zheng2023judging} & \checkmark &  & \\
        MMLU \citep{hendrycks2020measuring} & \checkmark &  & \\
        HumanEval \citep{chen2021evaluating} & \checkmark & & \\
        Libra-Leaderboard \citep{li2024libra} & \checkmark & \checkmark & \\
        DecodingTrust \citep{decodingtrust} &  & \checkmark & \\
        TrustLLM \citep{huang2024trustllm} &  & \checkmark & \checkmark \\
        HonestLLM \citep{gao2024honestllm} &  &  & \checkmark \\
        BeHonest \citep{chern2024behonest} &  &  & \checkmark \\
        \bottomrule[1pt]
    \end{tabular}}
    \caption{Benchmark examples related to the HHH principle.}
    \label{tab:benchmark_example}
\end{table}

\section{Example of Priority Order}
\label{app:example_priority}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{figure/priority_intro.pdf}
    \caption{Priority orders of HHH principle in different downstream applications. \textbf{Notably, the figure shows just one of the situations in a specific application for reference and does not represent universality.}}
    \label{fig: priority_introduction}
\end{figure*}


The prioritization of \textbf{Honesty, Helpfulness, and Harmlessness (HHH)} varies across domains based on ethical considerations and practical requirements. In Figure \ref{fig: priority_introduction}, we present potential HHH prioritization based on our understanding. Furthermore, we outline the rationale behind this ranking below.

\begin{itemize}\setlength{\itemsep}{0pt}
\item \textbf{Healthcare}: \textit{Honesty} is uttermost critical, as incorrect diagnoses or misleading AI outputs can directly endanger lives. \textit{Harmlessness} safeguards sensitive medical records, ensuring AI maintains their privacy yet with accurate responses \cite{liu2023deid}. \textit{Helpfulness} is desirable but meaningless if accuracy is compromised.

\item \textbf{Education}: \textit{Harmlessness} takes precedence to protect students from inappropriate or harmful content. \textit{Helpfulness} follows, ensuring AI enhances learning without misleading students.

\item \textbf{Finance}: \textit{Honesty} is paramount, as misinformation can lead to great financial loss. \textit{Harmlessness} protects financial data integrity, ensuring AI upholds confidentiality without misinformation. \textit{Helpfulness} is valuable only if reliability is maintained for an AI finance assistant.

\item \textbf{Journalism}: \textit{Honesty} is fundamental for credible reporting without fake news. \textit{Harmlessness} is important but only required for credited reports other than rumors. %\textit{Helpfulness} enhances accessibility but must not distort factual accuracy and societal misleading.
\item \textbf{Scientific Research}: \textit{Honesty} is paramount, as scientific integrity relies on factual generated results. \textit{Helpfulness} ensures research remains practical, as innovation is enouraged upon validity. \textit{Harmlessness} is a consideration, though scientific breakthroughs often involve controlled risks.
\item \textbf{Legal}: \textit{Honesty} is the highest priority, as incorrect legal information can lead to potential crimes or harmful deeds of users. \textit{Harmlessness} follows to ensure ethical responsibilities are upheld as long as legal information remains accurate. \textit{Helpfulness} is useful but secondary to legal correctness.

\end{itemize}

%\newpage

\section{Example of Helpfulness Definition}
\label{app:example_helpfulness}

\begin{table*}[h]

\centering
\renewcommand\arraystretch{1.15}
\rowcolors{2}{white}{gray!10}
\resizebox{1\textwidth}{!}{ 
\begin{tabular}{p{0.3\textwidth} p{0.7\textwidth}}
\toprule[1pt]
\multicolumn{1}{c}{\textbf{Context}} & \multicolumn{1}{c}{\textbf{Definition of Helpfulness}}  \\
\midrule
  Achieving distinct human-centered objectives \citep{labarta2024study} & A quantifiable improvement in user performance on tasks that are aligned with the goals facilitated by the provision of explanations.  \\
  Peer assessment helpfulness evaluation \citep{liu2024generative} & Includes essential features (i.e., comprehensiveness, non-repetitiveness) and constructive features (i.e., praise, problem statement, suggestions, localization, providing examples).\\
Human-robot collaboration \citep{freedman2020helpfulness} & Joint Plan Helpfulness assesses known collaborations, Responsive Plan Helpfulness adapts to dynamic interactions, Normalized Helpfulness standardizes across tasks, and Relative Helpfulness quantifies the reduction in human effort.\\
Controllable balancing of safety and helpfulness 
\citep{tuan2024towards} & How well the responses fulfill user requests and provide needed information.\\
% A general language assistant for alignment \citep{askell2021general} &  A clear attempt to perform the task or answer the question posed.\\
Improving LLM honesty and helpfulness simultaneously \citep{gao2024honestllm} & The model's ability to fulfill user requests by providing clear explanations, further guidance, and potential solutions.\\
The evaluation of AI-generated suggestions in design science research \citep{memmert2023human} & Including five dimensions: the ability to inspire new ideas, ease of understanding, relevance to the specific design component, relevance to the broader research domain, and the level of unexpectedness they provide. \\
RLHF for harmless, honest, and helpful AI \citep{toloka} & Understanding the user's intentions, correctly executing their requested actions, and providing relevant supporting information or alternative solutions if the requested action is not feasible.\\
\bottomrule[1pt]
\end{tabular}}
\caption{Different definitions of helpfulness under various contexts.}
\label{tab: helpfulness_example}
\end{table*}



\newpage

\section{Case Study of the 3H principles for developing Chemistry Foundation Models}
\label{app:chemistry_case}
\begin{table*}[h!]
\centering
\renewcommand\arraystretch{1.15}
\rowcolors{2}{white}{gray!10}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{p{0.3\textwidth} p{0.7\textwidth}}
\toprule[1pt]
\multicolumn{1}{c}{\textbf{Framework Step}} & \multicolumn{1}{c}{\textbf{Chemistry Foundation Models}} \\
\midrule
\textbf{Contextual Object} & 
\textbf{User Group: }Potential users include chemists, pharmaceutical engineers, and students involved in research or education.
  
\textbf{Application Aim:} Examples include predicting chemical compound properties, generating synthetic pathways, or designing novel drugs.

 \textbf{Task Type:} Tasks may range from generating molecular structures to optimizing reaction conditions.
 
 \textbf{Environment Access:} Constraints such as whether the model is deployed online, offline, or requires access to sensitive or proprietary data. \\
\hline
\textbf{Value Anchor and Value Scale} & 
    \textbf{Value Anchor:} In high-risk domains like drug discovery, harmlessness is the primary concern to prevent harmful outputs, such as toxic or unsafe compounds.
    
    \textbf{Value Scale:} Relative weights should be dynamically adjusted based on the context. For example: In scientific research, \textbf{honesty} (scientific accuracy) may outweigh helpfulness.
    In educational applications, \textbf{helpfulness} may take precedence to enhance the learning experience.
 \\ 
\hline
\textbf{Risk Assessment} & 

\textbf{Risk Identification:} Assess potential risks such as generating biased outputs, recommending harmful synthesis pathways, or presenting misleading interpretations.

\textbf{Multi-Level Assessment:} Evaluate both direct risks (e.g., generation of toxic molecules) and indirect risks (e.g., incorrect predictions leading to resource waste).

\textbf{Stakeholder Tolerance:} Incorporate feedback from developers, end users, and regulators to determine acceptable risk thresholds.
 \\ 
\hline
\textbf{Benchmarking Standards} & 
\textbf{Context-Specific Benchmarks:} Design benchmarks that align with the prioritized dimensions. For instance:
1). Measured by the model's predictive accuracy and relevance to user tasks~\citep{guo2023can}, 2). Evaluated through safety checks on generated molecular outputs.
3). Assessed by comparing generated outputs with verified scientific data~\citep{guo2024can,bushuiev2024massspecgym}. 

\textbf{Multi-Dimensional Integration:} Use complementary benchmarks to capture performance holistically across different dimensions.
\\
\hline
\textbf{System Transparency and Governance} & 
\textbf{Transparency: }In the chemistry domain, models must provide interpretable outputs, highlighting key chemical features influencing predictions and annotating data sources such as PubChem~\citep{pubchem2021} or ChEMBL~\citep{chembl2019}. Confidence scores and error margins should accompany predictions to ensure reliability.

\textbf{Governance:} It requires strict validation of outputs against experimental data, safeguards against misuse (e.g., toxic compound generation), and compliance with safety standards like EPA(Environmental Protection Agency)) and REACH(Registration, Evaluation, Authorization, and Restriction of Chemicals). Regular audits by chemists and toxicologists are essential to refine predictions and maintain ethical and safety standards.
 \\ 
\hline
\end{tabular}}
\caption{Application of the 3H framework to the development and application of chemistry foundation models.}
\label{app:3h_framework_chemistry}
\end{table*}



% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\end{document}
