\section{Related Work}
\label{sec:rel}


This section briefs relevant efforts from three perspectives.

\begin{figure*}
	\centering
        \vspace{-1ex}
	\includegraphics[width=\linewidth]{framework_new.pdf}
	\vspace{-4ex}
	\caption{The framework of \ours. It is mainly constituted of the id role, the superego role, and the ego role. Here we expect that the discussion of the three roles will  enlighten LLMs to reason better.}
 \label{fig:framework}
 \vspace{-3ex}
\end{figure*}


\subsection{Fine-tuning \llm}
Methods in this approach treat \llm as a supervisor to obtain feedback for a provided mistake to fine-tune \llm.
\sta~\cite{DBLP:conf/nips/ZelikmanWMG22} emphasizes the rationales leading to the correct results. With the gold annotations, an iterative generation strategy is employed to obtain the ideal reasoning path for each question to fine-tune \llm.
\lema~\cite{DBLP:journals/corr/abs-2310-20689} focuses on inaccurate reasoning paths, and employs GPT-4 to generate details, reasons, and final answer for them. These samples then perform as annotations for fine-tuning \llm.
Similar to \lema, \gwfs~\cite{DBLP:journals/corr/abs-2310-10477} directly guides \llm to generate harmful responses and informs \llm to evaluate its output with specific critique. This mistake analysis data is then used for model fine-tuning. 
\srethink~\cite{DBLP:conf/acl/TongLWWTS24}pre-defines some error classes to provide \llm with typical correction examples to prepare fine-tuning samples.
\salam~\cite{DBLP:conf/emnlp/WangL23} designs an assistant LLM to interact with the main LLM and leverages the resulting conversations to fine-tune assistant LLM, thereby enhancing the supervisor's flexibility compared with aforementioned approaches.
The utilization of domain fine-tuning can enhance the performance of \llm; however, a new training phase encounters delayed response issue. 
Additionally, a substantial amount of computational powers are required to execute the learning process. 

\subsection{Leveraging Tools}
The main idea in this line is to leverage the feedback to verify \llm' outputs.
\tran~\cite{DBLP:conf/emnlp/YangLL23} accumulates rules from incorrect cases summarized by \llm to form a rule base. When encountering new queries, it extracts relevant rules as external clues for the reasoning process. 
\ve~\cite{DBLP:conf/acl/ZhaoLJQB23} transforms the attention to leverage external knowledge, e.g., Wikipedia and Google. The information performs as the facts to verify the results and supports the re-answer procedure.
To put the thoughts deeply, {ReAct}~\cite{DBLP:conf/iclr/YaoZYDSN023} proposes an interaction framework to enforce \llm to execute the thought-action-observation circle based on goolge engine. In each phrase, the model will make a feedback-dependent decision and prepare for the next state.
{Reflexion}~\cite{DBLP:conf/nips/ShinnCGNY23} tries reinforcement learning for reflection to further explore decision-making capability.
External assistance can help users to obtain immediate results; however, the performance is heavily reliant on the quality of knowledge source or effective tools. 
Once the assistance is inaccessible, the essential components of these methods are compromised.

\subsection{Multi-agent Debate}
Several agents are defined to invoke discussion within the same topic.
LM vs LM~\cite{DBLP:conf/emnlp/CohenHGG23} facilitates a multi-turn conversation between the claim-generating LM and an examining LM, which introduces questions to discover inconsistencies.
Multiagent Debate~\cite{DBLP:journals/corr/abs-2305-14325} makes multiple agents propose and debate their individual responses and reasoning processes over multiple rounds to reach a common final answer.
MAD~\cite{DBLP:journals/corr/abs-2305-19118} facilitates the expression of arguments among multiple agents in a "tit for tat" manner, while a judge oversees the debate process to obtain a final solution.
Self-Contrast~\cite{DBLP:conf/acl/ZhangSWPWZ024} employs diverse agents     to offer distinct perspectives on a query. Subsequently, a new agent compares and summarizes discrepancies to generate the final answer. 
The multi-agent debate framework effectively ensures timeliness and independence, but the omission of agents' self-denial results in perpetual resource competition.
\ours tries to tackle the identified challenges via the initial attempts, the regular guidance, and detailed executable steps from tailored roles.