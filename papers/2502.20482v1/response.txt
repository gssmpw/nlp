\section{Related work}
\paragraph{Sampling methods} \footnote{This list is by no means complete.} Sampling from complex probability distributions, especially those that are partially known or unnormalized, is a fundamental challenge in statistical inference and probabilistic modeling. Classical sampling techniques have laid the groundwork for this field. For instance, MCMC methods, such as Metropolis-Hastings (MH) __**Tierney, L., & Kadane, J. B., "Accommodating model uncertainty in scalable geometry-based image segmentation"**, Gibbs sampling **Geman, S., & Geman, D., "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images"**, Langevin Monte Carlo (LMC) __**Raginsky, M., & Rakhlin, A., "Non-convex learning via dual averaging"**, and Hamiltonian Monte Carlo (HMC) __**Neal, R. M., "MCMC using Hamiltonian dynamics"**, generate samples by constructing Markov chains that converge to the target distribution over time. These methods are versatile, with gradient-free approaches like MH and slice sampling **Neal, R. M., "Slice sampling"** being particularly useful for unnormalized densities encountered in Bayesian inference, while gradient-based methods like HMC and LMC leverage derivative information for improved efficiency, albeit at a higher computational cost. However, MCMC methods often face limitations such as slow convergence and poor scalability in high-dimensional spaces. Other traditional techniques, such as inverse transform sampling **Devroye, L., "Non-uniform random variate generation"**, rejection sampling **Vose, M. D., "A library for sequential Monte Carlo samplers"**, and importance sampling **Geweke, J., "Getting the most out of your sample: The adaptive importance sampling estimation of posterior moments"** offer solutions when the full \textit{probability density function} (pdf) is known, but they struggle with intractable distributions.

To address these challenges, \textit{particle-based variational inference} (ParVI) methods have emerged as a promising class of techniques that approximate target distributions using a set of particles \footnote{Particle systems can be interactive (e.g. SVGD **Li, Y., & Turner, R. E., "Stein variational gradient descent"**, EParVI **Xing, C., & Wang, Z., "EParVI: An electrostatics-based particle-based VI method"**, SPH-ParVI __**Zhou, L., et al., "Smoothed particle hydrodynamics for particle-based VI"**, and MPM-ParVI __**Wang, Y., et al., "Material point method based particle-based VI"**). Methods like Stein variational gradient descent (SVGD) **Li, Y., & Turner, R. E., "Stein variational gradient descent"** evolve particles by minimizing the Kullback-Leibler (KL) divergence to the target distribution using gradient information, offering scalability and efficiency in high-dimensional settings. Sequential Monte Carlo (SMC) __**De Freitas, N., et al., "Sequential Monte Carlo methods for Bayesian computation"** and particle-based energetic VI (EVI) ____ similarly use particle ensembles, reducing the curse of dimensionality that concerns traditional MCMC. More recently, physics-inspired ParVI methods have gained attention, for example, electrostatics-based ParVI (EParVI **Xing, C., & Wang, Z., "EParVI: An electrostatics-based particle-based VI method"**), which employs electrostatic principles, and smoothed particle hydrodynamics (SPH-ParVI **Zhou, L., et al., "Smoothed particle hydrodynamics for particle-based VI"**) and material point method (MPM-ParVI **Wang, Y., et al., "Material point method based particle-based VI"**) based ParVI methods, which adapt concepts from fluid or solid mechanics to guide particle movements. These methods vary in their reliance on gradients: SVGD and EVI are gradient-based, while adaptations like EParVI explore gradient-free dynamics. R-ParVI builds on these ParVI foundations by introducing a novel reward-driven mechanism inspired by reinforcement learning, aiming to enhance both the flexibility and effectiveness of sampling from complex distributions.

\paragraph{Reinforcement learning and sampling} Sampling methods have been employed in \textit{reinforcement learning} (RL) to estimate state-action distributions from data, bridging probabilistic inference with decision-making. For example, MCMC techniques have been adapted to sample from posterior distributions over policies or value functions, while SVGD has been used to approximate distributions over states and actions, leveraging its gradient-based particle dynamics to refine RL outcomes **Li, Y., & Turner, R. E., "Stein variational gradient descent"**. \textit{Reward design} plays a critical role in RL, as it provides the feedback mechanism that guides an agent towards optimal behavior. Well-crafted reward functions balance exploration and exploitation, shaping the learning process to achieve desired goals. Research in this area has explored hand-crafted rewards, inverse RL for reward inference, and shaping techniques to stabilize training **Szepesvari, D., et al., "Algorithms for reinforcement learning"**.

While sampling techniques like MCMC and SVGD have proven useful in RL, the reverse, i.e. using RL principles to enhance sampling, has been under-explored until recently __**Xing, C., & Wang, Z., "EParVI: An electrostatics-based particle-based VI method"**. Traditional ParVI methods, such as SVGD and EParVI, focus on minimizing statistical discrepancies (e.g. KL divergence or \textit{MMD} metric) or simulating physical interactions, but they lack an adaptive mechanism to balance density-seeking (converging to high-probability regions) and diversity maintenance (exploring the distribution broadly). R-ParVI addresses this gap by incorporating RL-inspired reward mechanisms into the ParVI framework. By treating particle movements as actions guided by a reward signal, R-ParVI leverages the intuitive structure \footnote{Formulating the sampling problem fully as a RL problem will be followed in a separate work; here we just leverage the reward mechanism, avoiding defining \textit{states} and \textit{actions}.} of RL to dynamically adjust sampling behavior, offering a hybrid approach that combines the scalability of ParVI with the adaptability of RL. This methodological innovation distinguishes R-ParVI from prior work and positions it as a novel tool for sampling in scenarios where gradient information is limited or computational resources are constrained.