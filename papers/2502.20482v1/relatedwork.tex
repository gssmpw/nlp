\section{Related work}
\paragraph{Sampling methods} \footnote{This list is by no means complete.} Sampling from complex probability distributions, especially those that are partially known or unnormalized, is a fundamental challenge in statistical inference and probabilistic modeling. Classical sampling techniques have laid the groundwork for this field. For instance, MCMC methods, such as Metropolis-Hastings (MH) \cite{Metropolis1953}, Gibbs sampling \cite{Geman1984}, Langevin Monte Carlo (LMC) \cite{Roberts1996}, and Hamiltonian Monte Carlo (HMC) \cite{Duane1987}, generate samples by constructing Markov chains that converge to the target distribution over time. These methods are versatile, with gradient-free approaches like MH and slice sampling \cite{Neal2003} being particularly useful for unnormalized densities encountered in Bayesian inference, while gradient-based methods like HMC and LMC leverage derivative information for improved efficiency, albeit at a higher computational cost. However, MCMC methods often face limitations such as slow convergence and poor scalability in high-dimensional spaces. Other traditional techniques, such as inverse transform sampling \cite{Devroye1986}, rejection sampling \cite{vonNeumann1951}, and importance sampling \cite{Kahn1949}, offer solutions when the full \textit{probability density function} (pdf) is known, but they struggle with intractable distributions.

To address these challenges, \textit{particle-based variational inference} (ParVI) methods have emerged as a promising class of techniques that approximate target distributions using a set of particles \footnote{Particle systems can be interactive (e.g. SVGD \cite{Liu2016SVGD}, EParVI \cite{huang2024EParVI}, SPH-ParVI \cite{huang2024SPHParVI}, MPM-ParVI \cite{huang2024MPMParVI}) or non-interactive as in this work.}. Methods like Stein variational gradient descent (SVGD \cite{Liu2016SVGD}) evolve particles by minimizing the Kullback-Leibler (KL) divergence to the target distribution using gradient information, offering scalability and efficiency in high-dimensional settings. Sequential Monte Carlo (SMC \cite{Doucet2001}) and particle-based energetic VI (EVI) \cite{Wang2021EVI} similarly use particle ensembles, reducing the curse of dimensionality that concerns traditional MCMC. More recently, physics-inspired ParVI methods have gained attention, for example, electrostatics-based ParVI (EParVI \cite{huang2024EParVI}), which employs electrostatic principles, and smoothed particle hydrodynamics (SPH-ParVI \cite{huang2024SPHParVI}) and material point method (MPM-ParVI \cite{huang2024MPMParVI}) based ParVI methods, which adapt concepts from fluid or solid mechanics to guide particle movements. These methods vary in their reliance on gradients: SVGD and EVI are gradient-based, while adaptations like EParVI explore gradient-free dynamics. R-ParVI builds on these ParVI foundations by introducing a novel reward-driven mechanism inspired by reinforcement learning, aiming to enhance both the flexibility and effectiveness of sampling from complex distributions.

\paragraph{Reinforcement learning and sampling} Sampling methods have been employed in \textit{reinforcement learning} (RL) to estimate state-action distributions from data, bridging probabilistic inference with decision-making. For example, MCMC techniques have been adapted to sample from posterior distributions over policies or value functions, while SVGD has been used to approximate distributions over states and actions, leveraging its gradient-based particle dynamics to refine RL outcomes \cite{Chung2020,Depthfirstlearning2019}. \textit{Reward design} plays a critical role in RL, as it provides the feedback mechanism that guides an agent towards optimal behavior. Well-crafted reward functions balance exploration and exploitation, shaping the learning process to achieve desired goals. Research in this area has explored hand-crafted rewards, inverse RL for reward inference, and shaping techniques to stabilize training \cite{Devidze2021,Ng2000,Ma2024}.

While sampling techniques like MCMC and SVGD have proven useful in RL, the reverse, i.e. using RL principles to enhance sampling, has been under-explored until recently \cite{Chung2020}. Traditional ParVI methods, such as SVGD and EParVI, focus on minimizing statistical discrepancies (e.g. KL divergence or \textit{MMD} metric) or simulating physical interactions, but they lack an adaptive mechanism to balance density-seeking (converging to high-probability regions) and diversity maintenance (exploring the distribution broadly). R-ParVI addresses this gap by incorporating RL-inspired reward mechanisms into the ParVI framework. By treating particle movements as actions guided by a reward signal, R-ParVI leverages the intuitive structure \footnote{Formulating the sampling problem fully as a RL problem will be followed in a separate work; here we just leverage the reward mechanism, avoiding defining \textit{states} and \textit{actions}.} of RL to dynamically adjust sampling behavior, offering a hybrid approach that combines the scalability of ParVI with the adaptability of RL. This methodological innovation distinguishes R-ParVI from prior work and positions it as a novel tool for sampling in scenarios where gradient information is limited or computational resources are constrained.