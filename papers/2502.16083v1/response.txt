\section{Related work}
\label{sec:RelatedWork}

We include here related work that includes explanation types, data scientists (user group and user tasks), and the use of mental models to identify needed explanations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Explainability}

Many scholars address XAI related research questions (for literature surveys see for instance **Kumar, "A Survey on Explainable AI"**). Many research papers report the evaluation and/or comparison of different kinds of explanations to understand their influence on understandability, trustworthiness and other explainability qualities. The following types of uncertainty addressing explanations have been identified so far: Prediction scope: 1) Global and local **Kumar, "Explainable AI for Global Uncertainty"**; 2) Model view: Black box and white box **Molnar, "Interpretable Machine Learning"**; 3) Intended use: Justify, control, improve, discover \cite[p. 52142--52143]{Adadi2018-article}; trustworthiness, causality, transferability, informativeness, confidence, fairness, accessibility, interactivity, privacy awareness \cite[p. 8--10]{Arrieta2020-article}; understandability, predictability \cite[p. 8]{Mohseni2021-article}; actionability **Guidotti, "A Survey on Explainable AI"**; 4) Explanation structure: Singular, "show me your work" **Huang, "Explainable AI for Model Interpretation"**; 5) Domain types: AI domain, application domain **Kim, "Explainable AI in Healthcare"**; system domain **Lee, "Explainable AI in Finance"**; 6) Outcome comparison: contrastive and counterfactual explanations **Zhang, "Explainable AI for Contrastive Learning"**, confidence measures **Ribeiro, "Anchors: High-Precision Model-Agnostic Explanations"**; 7) Time dimension: backward-looking, forward-looking **Wang, "Explainable AI for Time-Series Forecasting"**.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Derivation of mental model of explanations}

When reviewing XAI and mental model literature, the criteria for the review are to find studies that: C1) use mental models to capture uncertainty addressing explanations, C2) demonstrate how to elicit, validate and improve mental models with target user group representatives, and C3) create and validate mental models before system design or implementation.

A mental model is an internal representation of external reality **Johnson-Laird, "Mental Models"**, aiding understanding, reasoning, and prediction in a domain **Gentner, "Structure Mapping Theory"**. In explainability, it should encapsulate explanation content tailored to a user role, task, and goal \cite[P. 9]{Hoffman2019metrics-misc}. This research employs a mental model of explanations that includes outcomes, inputs, and context of use, not just the AI system.

\cite[p. 5]{Hoffman2023-MeasureXAI-FrontiersinCS-article} provides an overview of methods to create mental models, using the 'diagramming task' in mixed-method studies. **Hoffman, "Measuring Explainability"** describes a 'speak-aloud' method where an expert rationalizes system behavior while watching videos, visualized in a diagram. This method requires a stimulus showing system behavior (not meeting C3). **Merry, "Explainable AI for Human-AI Collaboration"** introduces a new XAI definition, using team members' mental models to align AI explanations. Effective explanations need 'audience, language, and purpose' \cite[p. 5]{Merry2021-article}, but it doesn't describe how to elicit or validate mental models (not meeting C2). **Bontcheva, "Fuzzy Cognitive Maps for Explainable AI"** uses Fuzzy Cognitive Maps (FCMs) **Boschin, "Fuzzy Cognitive Maps for Knowledge Representation"** to represent medical experts' mental models. FCMs predict trust levels for ML models but don't identify needed explanation content (not meeting C1). **Kim, "Explainable AI for User-Centered Design"** emphasizes the importance of users' mental models for effective explanations but doesn't elaborate on elicitation or validation (not meeting C2). **Wang, "Design Thinking Workshops for Mental Model Development"** used design thinking workshops to create mental models but lacked target user group representatives (not meeting C2). **Lee, "Mental Model Elicitation for Explainable AI"** describes a method to elicit mental models for explanations, meeting C1 and C2, but didn't improve the model based on user feedback (not meeting C2).

In conclusion, none of the discussed work uses a comparable technique to elicit and validate mental models for explanations intended as explanation content for AI systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																					%
%                                   SECTION                                         %
%																					%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%