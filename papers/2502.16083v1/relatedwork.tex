\section{Related work}
\label{sec:RelatedWork}

We include here related work that includes explanation types, data scientists (user group and user tasks), and the use of mental models to identify needed explanations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Explainability}

Many scholars address XAI related research questions (for literature surveys see for instance \cite{Adadi2018-article,Angelov2021-article,Dosilovic2018-inproceedings,Guidotti2019-article,Hu2021-proceedings,Islam2022-article,Mueller2019-article,Vilone2020-article}). Many research papers report the evaluation and/or comparison of different kinds of explanations to understand their influence on understandability, trustworthiness and other explainability qualities. The following types of uncertainty addressing explanations have been identified so far: Prediction scope: 1) Global and local \cite{Arrieta2020-article,Adadi2018-article,Hoffman2018-article}; 2) Model view: Black box and white box \cite{Rudin2019-article}; 3) Intended use: Justify, control, improve, discover \cite[p. 52142--52143]{Adadi2018-article}; trustworthiness, causality, transferability, informativeness, confidence, fairness, accessibility, interactivity, privacy awareness \cite[p. 8--10]{Arrieta2020-article}; understandability, predictability \cite[p. 8]{Mohseni2021-article}; actionability \cite{Degen2023-inproceedings}; 4) Explanation structure: Singular, "show me your work" \cite{Degen2023-inproceedings}; 5) Domain types: AI domain, application domain \cite{Degen2023-inproceedings}, system domain \cite{Degen2024-inproceedings}; 6) Outcome comparison: contrastive and counterfactual explanations \cite{Miller2019ExplanationIA,Stepin2021-contrastive_counterfactual_explanations-article}, confidence measures \cite{Waa2020_ConfidenceLevel_2020,Zhang2020_Confidence_2020}; 7) Time dimension: backward-looking, forward-looking \cite{Degen2024-inproceedings}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Derivation of mental model of explanations}

%When looking through XAI and mental model literature, the criteria for the literature review is to find other studies that have C1) the mental model has the intended used of capturing uncertainty addressing explanations, C2) demonstrated how to elicit, validate, and improve a mental model for explanations with representatives of the target user group, C3) the mental model is created and validated prior to system design or implementation, and  

When reviewing XAI and mental model literature, the criteria for the review are to find studies that: C1) use mental models to capture uncertainty addressing explanations, C2) demonstrate how to elicit, validate, and improve mental models with target user group representatives, and C3) create and validate mental models before system design or implementation.

A mental model is an internal representation of external reality \cite{Johnson-Laird1986-book}, aiding understanding, reasoning, and prediction in a domain \cite{Gentner2001-incollection}. In explainability, it should encapsulate explanation content tailored to a user role, task, and goal \cite[P. 9]{Hoffman2019metrics-misc}. This research employs a mental model of explanations that includes outcomes, inputs, and context of use, not just the AI system.

\cite[p. 5]{Hoffman2023-MeasureXAI-FrontiersinCS-article} provides an overview of methods to create mental models, using the 'diagramming task' in mixed-method studies. \cite{Garcia2018-XAI-MentalModel-inproceedings} describes a 'speak-aloud' method where an expert rationalizes system behavior while watching videos, visualized in a diagram. This method requires a stimulus showing system behavior (not meeting C3). \cite{Merry2021-article} introduces a new XAI definition, using team members' mental models to align AI explanations. Effective explanations need 'audience, language, and purpose' \cite[p. 5]{Merry2021-article}, but it doesn't describe how to elicit or validate mental models (not meeting C2). \cite{Onari2023-XAI-MentalModel-arxiv-article} uses Fuzzy Cognitive Maps (FCM) \cite{Kosko1986-FuzzyCognitiveMaps-IJMMS-article} to represent medical experts' mental models. FCMs predict trust levels for ML models but don't identify needed explanation content (not meeting C1). \cite{Rutjes2019-XAI-MentalModel-CHI-inproceedings} emphasizes the importance of users' mental models for effective explanations but doesn't elaborate on elicitation or validation (not meeting C2). \cite{Sheridan2023-MentalModel-InProceedings} used design thinking workshops to create mental models but lacked target user group representatives (not meeting C2). \cite{Degen2023-inproceedings,Degen2024-inproceedings} describes a method to elicit mental models for explanations, meeting C1 and C2, but didn't improve the model based on user feedback (not meeting C2).

In conclusion, none of the discussed work uses a comparable technique to elicit and validate mental models for explanations intended as explanation content for AI systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																					%
%                                   SECTION                                         %
%																					%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%