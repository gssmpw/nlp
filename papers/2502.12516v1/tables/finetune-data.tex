\begin{table}
    \centering
    \begin{tabularx}{\linewidth}{Xcccc}
        \hline
        \textbf{Format} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{Acc} \\ 
        \hline
        5 Most-FE & 0.605 & 0.699 & 0.649 & 0.480 \\ 
        5 Diverse & 0.648 & \underline{0.708} & 0.677 & 0.511 \\ 
        5 Random & \underline{0.717} & 0.675 & \underline{0.696} & \underline{0.533} \\ 
        \hline
        Full Dataset & \textbf{0.774} & \textbf{0.762} & \textbf{0.768} & \textbf{0.624} \\ 
        \hline
    \end{tabularx}
    \caption{Performance comparison of GPT-4o-mini fine-tuned on different partitions of the dataset.}
    \label{tab:finetune-data-subsample}
\end{table}

% Diverse - Get as many FEs in as few samples possible, then randomly choose remaining up to 5.
% Most-FE - Get samples with most FEs, up to 5
% Random - Get 5 random samples

% Instead of all of this on gpt-4o mini, what if we just do qwen-3b
% - make sure this is done on the dev set
% - how does this scale? 1 example, 5 examples, 10 examples, etc.