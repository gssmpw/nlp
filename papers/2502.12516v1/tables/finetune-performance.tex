% \begin{table}
%     \centering
%     \begin{tabular}{lcccc}
%         \hline
%         \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{Acc} \\ 
%         \hline
%         Llama 3.2-3B    & 0.717 & 0.691 & 0.704 & 0.543 \\
%         Llama 3.2-8B    & 0.736 & 0.711 & 0.724 & 0.567 \\
%         Llama 3.3-70B   & 0.748 & 0.738 & 0.743 & 0.591 \\
%         \hline
%         Qwen 2.5-0.5B   & 0.716 & 0.682 & 0.699 & 0.537 \\
%         Qwen 2.5-1.5B   & 0.748 & 0.719 & 0.733 & 0.579 \\
%         Qwen 2.5-3B     & 0.765 & 0.740 & 0.752 & 0.603 \\
%         Qwen 2.5-7B     & 0.769 & 0.754 & 0.762 & 0.615 \\
%         Qwen 2.5-14B    & 0.782 & 0.772 & 0.777 & 0.635 \\
%         % Qwen 2.5-32B    & 0.792 & 0.787 & 0.789 & 0.652 \\
%         Qwen 2.5-32B    & 0.792 & 0.787 & 0.789 & 0.652 \\
%         Qwen 2.5-72B    & 0.798 & 0.790 & 0.794 & 0.658 \\
%         \hline
%         Phi-4 (14B)     & 0.793 & 0.777 & 0.785 & 0.646 \\
%         \hline
%         GPT-4o-mini     & 0.774 & 0.762 & 0.768 & 0.624 \\
%         GPT-4o          & 0.xxx & 0.xxx & 0.xxx & 0.xxx \\
%         \hline
%     \end{tabular}
%     \caption{Performance comparison of different models fine-tuned using JSON-exist format.}
%     \label{tab:finetune_performance}
% \end{table}

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccc}
        \hline
        \textbf{Model} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{Acc} \\ 
        \hline
        Qwen 2.5-0.5B   & 0.716 & 0.682 & 0.699 & 0.537 \\
        Llama 3.2-3B    & 0.717 & 0.691 & 0.704 & 0.543 \\
        Llama 3.2-8B    & 0.736 & 0.711 & 0.724 & 0.567 \\
        Qwen 2.5-1.5B   & 0.748 & 0.719 & 0.733 & 0.579 \\
        Qwen 2.5-3B     & 0.765 & 0.740 & 0.752 & 0.603 \\
        % Qwen 2.5-7B-cand     & 0.757 & 0.666 & 0.708 & 0.548 \\~~~
        % Phi-4-cand     & 0.798 & 0.717 & 0.756 & 0.607 \\~~~
        Qwen 2.5-7B     & {0.769} & {0.754} & {0.762} & {0.615} \\
        GPT-4o-mini     & 0.774 & 0.762 & 0.768 & 0.624 \\
        Qwen 2.5-14B    & 0.782 & 0.772 & 0.777 & 0.635 \\
        Phi-4 (14B)     & \underline{0.793} & \underline{0.777} & \underline{0.785} & \underline{0.646} \\
        \hline
        Llama 3.3-70B   & 0.748 & 0.738 & 0.743 & 0.591 \\
        Qwen 2.5-32B    & 0.792 & {0.787} & {0.789} & {0.652} \\
        % Calme-3.2       & 0.791 & 0.790 & 0.790 & 0.653 \\
        Qwen 2.5-72B    & \textbf{0.798} & \textbf{0.790} & \textbf{0.794} & \textbf{0.658} \\
        % GPT-4o          & 0.xxx & 0.xxx & 0.xxx & 0.xxx \\
        \hline
        \citet{lin-etal-2021-graph}     & -     & -     & 0.721 & - \\ 
        AGED                            & 0.750 & 0.752 & 0.751 & - \\ 
        KAF-SPA                         & \underline{0.760} & 0.743 & 0.751 & - \\ 
        \citet{Ai_Tu_2024}              & 0.756 & \underline{0.753} & \underline{0.755} & - \\ 
        \hline
    \end{tabular}}
    \caption{Performance of different models fine-tuned using JSON-exist. Models are ordered by F1 score and separated into size buckets 0-14B and 14B+.}
    \label{tab:finetune_performance}
\end{table}
