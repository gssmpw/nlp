\section{Methodology}
\input{tables/fe-representation}

\subsection{Input Representation Design}
\label{sec:input-representation}
Previous research has shown that large language models are sensitive to input formatting~\cite{Sclar2023QuantifyingLM} and that different representations can result in different model performance~\cite{tam-etal-2024-speak,textsql-eval-gao-2024,exploring-marcos-2024}. To study these effects on frame-semantics, we systematically evaluated multiple input-output representation formats to determine their impact on frame element extraction performance.

For all input formats, we wrap the target word or phrase in double asterisks, as shown in Table~\ref{tab:fe-representation}, to explicitly mark the token that evokes the frame. This marking helps focus the model's attention on the relevant part of the sentence when making frame element predictions, ensuring that the model identifies frame elements for the correct target.

We developed and tested four distinct representation formats. The Markdown format offers a simple, human-readable approach where frame elements are represented as a markdown list. Each list item contains a frame element name paired with its corresponding text span from the sentence. This format only includes frame elements that the model predicts are present in the input. The XML Tags format provides a structured approach that uses XML-style tags to wrap frame elements within the sentence text. The tag names correspond to frame element names, providing both semantic labeling and precise positional information without requiring additional processing.

We also developed two JSON-based formats. The JSON-Existing format uses frame element names as keys and their corresponding text spans from the sentence as values. Similar to the Markdown format, this only includes predicted frame elements. The JSON-Complete format provides an exhaustive representation different from previous representations that includes all possible frame elements as keys, with empty strings as values for elements not found in the sentence. This format was designed to test whether explicitly presenting all possible frame elements might improve model performance. Examples of each representation format are provided in Table~\ref{tab:fe-representation}, illustrating how they encode the same semantic information in different ways.

\subsection{Model Selection and Implementation}
To ensure a comprehensive evaluation across the current LLM landscape, we selected models varying in size, architecture, and accessibility. Our selection criteria focused on three key dimensions. In terms of model scale, we included models ranging from 0.5B to 78B parameters, categorizing them into small-scale (0-14B parameters) and large-scale (14B+ parameters) groups to analyze the impact of model size on performance. For architecture diversity, we selected top-performing models from the HuggingFace LLM leaderboard, with particular focus on Qwen 2.5 and Llama 3.2, which have shown strong performance on various tasks.

We included both open-source models (Qwen 2.5, Llama 3, and Phi-4) and closed-source systems (GPT-4o and GPT-4o-mini) to compare performance across different levels of model accessibility. For the open-source models, we implemented fine-tuning using LoRA~\cite{hu2021loralowrankadaptationlarge}. We used $r=16$ for all models except Llama 3.3 and Qwen 2.5 (72B) where we used $r=32$, according to best practices. This approach allowed us to optimize model performance while maintaining reasonable computational requirements.

\subsection{Evaluation}
Our evaluation framework was designed to comprehensively assess model performance across different scenarios and conditions. We began by testing each representation's effectiveness using controlled experiments with GPT-4o-mini. Model performance was evaluated using standard metrics including precision, recall, F1 score, and accuracy with exact match criteria.

To understand data requirements and efficiency, we analyzed performance with varying amounts of training data to understand data efficiency and saturation points. We also conducted extensive testing of model performance on unseen frames, unseen frame elements, and out-of-domain samples. Finally, we analyzed the distribution of argument extraction performance for each frame to gain a granular understanding. This evaluation framework enables us to systematically evaluate LLMs' capabilities in frame-semantic parsing while providing insights into the impact of different design choices and implementation strategies.


% We use common representations from our knowledge, including Markdown, XML tags, and two types of JSON representations. Examples of each of these representations can be found in Table~\ref{tab:fe-representation}. 

% The \textit{Markdown} representation is a very simple representation which represents a typical approach for instructing an LLM. The output is expected as a markdown list which contain a frame element name and its corresponding value within the sentence. In this representation, only frame elements which the model predicts exist will be included in the output. 

% The \textit{XML Tags} representation contains XML tags wrapped around the frame elements in the sentence. The names of these XML tags are determined by the frame element name. One additional benefit this approach provides is a positional understanding of the frame elements without additional post-processing. 

% The JSON representations are similar to the Markdown outputs, but in JSON format instead, where keys are the names of frame elements and values are the substring for those frame elements in the sentence. The \textit{JSON-Complete} representation, unlike other representations, includes all frame elements from the input frame as keys. For the frame elements which do not appear in the sentence, they are left blank. We include this option to identify whether alleviating the need to recall the frame elements from the input may improve the performance.

% \subsection{Models}

% We explore several state-of-the-art large language models for our evaluation. We particularly focus on diversity in a few key areas, namely, model size, model architecture, and model availability. For model size, we explore sizes ranging from 0.5B up to 78B, loosely categorizing them into small- (0-14B parameters) and large-scale models (14B+ parameters). We selected top-performing model families from the HuggingFace LLM leaderboard and found that Qwen 2.5 and Llama 3.2 are very common among top-performing systems. Finally, we also explored closed-source systems like GPT-4o and 4o-mini.

% We fine-tuned the open-source models (Qwen 2.5, Llama 3, and Phi-4) using LoRA~\cite{hu2021loralowrankadaptationlarge}. The details of our implementation are reported in Appendix~\ref{app:reproducibility}. 
