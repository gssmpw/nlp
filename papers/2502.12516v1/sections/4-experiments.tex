\section{Experiments}
In this section, we thoroughly evaluate the performance of LLMs on frame-semantic parsing through several experiments designed to address three primary research questions: RQ1) How does the representation of FEs impact performance? RQ2) How does model architecture and scale impact performance? RQ3) Are LLMs better on out-of-domain/unseen samples than previous non-LLM methods?

\subsection{Dataset}
We utilize the FrameNet 1.7~\cite{FramenetExtended} dataset for our primary experiments. FrameNet provides detailed definitions of semantic frames and their elements, including partially-annotated exemplar sentences for each frame and a corpus of fully-annotated sentences (referred to as "full-text annotations"). We use the full-text annotations for model training due to their complete coverage of frames and frame elements.

Following established conventions from \citet{opensesame} and \citet{das-smith-2011-semi}, we use standard train/test splits of non-overlapping documents. The training split consists of 3,353 sentences which evoke 19,391 frames with 34,219 frame elements, while the test split contains 1,247 sentences evoking 6,714 frames and 11,302 frame elements. For out-of-domain evaluation, we use the YAGS dataset, which contains 2,093 test sentences evoking 364 frames with 4,162 frame elements.

\input{tables/fe-repr-metrics}
\subsection{Frame Element Representations (RQ1)}
To answer RQ1, we evaluate various FE representation approaches using in-context learning with GPT-4o-mini. Our experiments (Table~\ref{tab:representation_performance}) reveal that JSON-Existing achieves superior performance with significant margins in precision (+4\%), F1 score (+3.4\%), and accuracy (+1.9\%). While JSON-Complete showed higher recall (+4.4\%), we attribute this to the simplified cognitive load of outputting all possible frame elements rather than selecting relevant ones. This comprehensive approach leads to more complete but less precise predictions. Notably, XML tag representation performed significantly worse, showing a 12.9 percentage point reduction in F1 score compared to JSON-exist, likely due to the difficulty of the representation. This also suggests that FrameNet's annotations may not be included in common pre-training data as they are originally in XML format. 

We found that using JSON-Existing results in the highest precision, F1 score, and accuracy, by significant margins (+4\%, +3.4\%, and +1.9\%, respectively). Interestingly, JSON-Complete had a higher recall (+4.4\%). We believe this is due to a reduced cognitive load given by instructing the LLM to output each frame element instead of just the ones which exist in the sentence. This likely reducing the chance of missing particular frame elements, resulting in higher recall. We also found that XML tags performed the worst by a large margin, likely due to the added positional requirements introduced. This indicates that FrameNet is likely not included in the pretraining corpus of LLMs as its native annotations are in XML format. 

\subsection{Generating LLM Instructions}
To validate our instruction creation process, we conducted a comparative study using instructions generated by GPT-4o. The automated approach included all frame-specific information and examples to allow flexibility in prompt generation. Despite being similar to our manual instructions (ROUGE-1/L score: 0.59/0.36), the automated instructions resulted in significantly lower performance (F1 score: 0.225 vs. 0.471). ~\todo{should we clarify that we use the diff prompts but the same llm to get these numbers?} We found that this was primarily due to the LLM predicting frame elements that do not exist, leading us to proceed with manually-crafted instructions for subsequent experiments. 

\lstset{
     basicstyle=\footnotesize\scriptsize,
    backgroundcolor=\color{gray!10}, % Light gray background
    linewidth=\columnwidth,
    breaklines=true,
    frame=single, % Adds a frame around the listing
    rulecolor=\color{black}, % Color of the frame
    showstringspaces=false, % Don't show spaces in strings
    numbers=left, % Line numbers on the left
    numberstyle=\tiny\color{gray}, % Style of the line numbers
    xleftmargin=0em, % Indent from left margin
    numbers=none, % Remove line numbers if not desired
    moredelim=**[is][\color{gray!70}]{@}{@} % Distinguish lines
}
\begin{lstlisting}[caption={Sample prompt used for zero-shot evaluation.}, label=lst:prompt]
### Task:
You are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. 

### Frame Information:
Frame Name: Awareness
Frame Definition: A Cognizer has a piece of Content in their model of the world. ... [omitted for brevity] ...
Examples:
  - Your boss is aware of your commitment. -> {"Cognizer": "Your boss", ...}
  ... [omitted] ...

Frame Elements:
Cognizer (Core): The Cognizer is the person whose awareness of phenomena is at question. 
  - Your boss is **aware** of your commitment. -> {"Cognizer": "Your boss"}
  ... [omitted] ...

Explanation (Extra-Thematic): The reason why or how it came to be that the Cognizer has awareness of the Topic or Content.

### Notes:
- Return the tagged sentence in a ```json ``` code block.
- Texts must not overlap.

\end{lstlisting}

\lstset{
    basicstyle=\small\scriptsize,
    backgroundcolor=\color{gray!10}, % Light gray background
    linewidth=\columnwidth,
    breaklines=true,
    frame=single, % Adds a frame around the listing
    rulecolor=\color{black}, % Color of the frame
    showstringspaces=false, % Don't show spaces in strings
    numbers=left, % Line numbers on the left
    numberstyle=\tiny\color{gray}, % Style of the line numbers
    xleftmargin=0em, % Indent from left margin
    numbers=none, % Remove line numbers if not desired
    moredelim=**[is][\color{gray!70}]{@}{@} % Distinguish lines
}
\begin{lstlisting}[caption={Sample input for fine-tuning.}, label=lst:prompt2]
{
    "role": "system",
    "content": "### Task:
    You are given a sentence and a frame with its associated frame elements and sometimes examples. Your task is to label the frame elements in the sentence using JSON. Keys should only be one of the defined frame elements. Do not make up your own frame elements, and do not remove or change the input in any way. Identify the frame elements based on the highlighted target word. 
    
    ### Notes:
    - Return the tagged sentence in a ```json ``` code block.
    - Texts must not overlap."
},
{
    "role": "user",
    "content": "### Frame Information
    Frame Name: Law
    Frame Definition: A Law regulates activities or states of affairs within a Jurisdiction, dictating ... [omitted for brevity] ...

    Frame Elements:
    Law (Core): This FE identifies the rule designed to guide ... [omitted]
    ... [omitted]
    
    ### Input:
    Since the early 1990s , China has improved its export controls , including the promulgation of **regulations** on nuclear and nuclear dual - use exports and has pledged to halt exports of nuclear technology to un - safeguarded facilities."
},
{
    "role": "assistant",
    "content": "### Output: 
    ```json{'Law': 'regulations', 'Forbidden': 'on nuclear and nuclear dual - use exports'}```"
}
\end{lstlisting}

\input{tables/incontext_performance}

\subsection{Model Selection and Evaluation (RQ2)}
\label{sec:model-selection}
We evaluated the in-context learning performance with GPT-4o, GPT-4o-mini, and Deepseek V3 using the prompt in Listing~\ref{lst:prompt}. The results of these models are shown in Table~\ref{tab:incontext_performance}. These experiments included several exemplar sentences defined in each frame. Since these in-context learning methods use exemplar data, we include previous works which have used exemplar sentences.  
Additionally, we benchmark these LLM-based approaches against state-of-the-art systems, including KID~\cite{zheng-etal-2022-double}, AGED~\cite{aged2023}, and \citet{Ai_Tu_2024}.

\input{tables/finetune-performance}

For fine-tuning, we experimented with Llama 3.2 (3B, 8B), Llama 3.3 (70B), Qwen 2.5 (0.5B-72B), Phi-4 (14B), and GPT-4o-mini\footnote{Due to high training and inference costs, we did not fine-tune GPT-4o.}, as detailed in Table~\ref{tab:finetune_performance}. These models were fine-tuned exclusively on the full-text annotations without exemplar sentences. Consequently, we exclude methods that rely on exemplars for fine-tuning. Our fine-tuning prompt is shown in Listing~\ref{lst:prompt2}.

To assess the impact of instruction tuning, we compared the base and instruction-tuned variants of Qwen 2.5-7B. The instruction-tuned version performed significantly worse (0.703 vs. 0.768 F1 score), leading us to prioritize base models where available in subsequent experiments.

Our results show that Qwen 2.5 consistently outperforms Llama 3 across all model sizes. Most fine-tuned LLMs surpass previous state-of-the-art approaches, with Qwen 2.5 (3B) notably outperforming the much larger Llama 3.3 (70B). Among smaller-scale models, Phi-4 achieved the best performance, while at the larger scale, Qwen 2.5 (72B) outperformed all competitors, including the smaller models. Notably, these two LLMs surpassed the previous best-performing system~\citet{Ai_Tu_2024} by +3.0\% and +3.9\% F1 score, respectively.

\input{tables/finetune-data}
% \vspace{0.2cm}
\subsection{Dataset Analysis}
% To better understand the characteristics and challenges of our dataset, we conducted a series of analyses focused on data quality, distribution, and its impact on model performance. This section details our investigation into fine-tune data subsampling and its effects on training efficiency and effectiveness.

\paragraph{Fine-tune Data Subsampling}
To reduce the costs associated with the high token count of the full training dataset, we first investigated whether strategic subsampling could reduce training overhead and cost while maintaining performance. We evaluated three distinct approaches: selecting up to five samples with the highest number of Frame Elements (5 Most-FE), randomly selecting up to five samples (5 Random), and selecting up to five samples that maximize Frame Element diversity (5 Diverse). Each of these approaches utilize approximately 15\% of the original training dataset. 

The results of this experiment, presented in Table~\ref{tab:finetune-data-subsample}, revealed an interesting trade-off. While the diversity-focused and FE-rich sampling strategies achieved higher recall, they resulted in lower F1 scores and precision compared to random sampling. This suggests that these targeted approaches enhanced the model's ability to identify a broader range of Frame Elements, but at the expense of precision on commonly occurring FEs. Because each of these approaches still fell significantly short of the full dataset's performance, we continue subsequent experiments with the entire dataset.

\input{figures/subsets_f1_phi4}
\input{tables/data-saturation}
\paragraph{Data Saturation Analysis}
We also examined the relationship between training data volume and LLM performance through systematic experimentation with different dataset sizes during fine-tuning. We conducted this analysis using Phi-4, selected for its combination of strong performance and smaller model size. Each smaller subset is fully contained within larger ones to ensure consistency. 

The results of this analysis are presented visually in Figure~\ref{fig:subsets-f1} and in detail in Table~\ref{tab:saturation}. The performance trajectory shows distinct phases: a period of steady improvement from 1\% to 25\% of the dataset, followed by a transition to more modest gains beyond the 50\% mark. While the rate of average performance improvement diminishes after utilizing 50\% of the data, we observed two notable effects when using the complete dataset: a reduction in the inter-quartile range and enhanced performance on previously challenging frames. This suggests that additional training data continues to contribute to model robustness, even after average performance metrics begin to plateau.

\input{tables/unseen-ood}
\subsection{Unseen and Out-of-domain Data (RQ3)}
\paragraph{Unseen Sample Evaluation}
We evaluate the ability of LLMs to identify frame elements on unseen and out-of-domain data in Table~\ref{tab:unseen}. We separate unseen data into two categories, Unseen (Frame) and Unseen (FEs). These categories correspond to test samples whose frames and frame elements are not seen in the training set, respectively. For this experiment we use a fine-tuned Phi-4 LLM for the same reason stated above.

% We find that on unseen frames, where the entire frame is unseen in the training set, there is a reduction in performance of -7.7\% F1 score. However, on unseen FEs, we see a much larger reduction of -27.0\%. This indicates that the model gains sufficient information during training to solve FEs which are common across many frames. Meanwhile, because unseen FEs are likely to be more rare or very specific to a particular frame, the model lacks sufficient understanding to recognize them. 

Our analysis reveals a notable performance disparity between the two categories of unseen data. On unseen frames, where the entire frame is unseen in the training set, we observe a reduction in performance of -7.7\% F1 score compared to the overall performance. This relatively modest degradation suggests that the model has developed a robust general understanding of frame semantics that transfers reasonably well to new frames.

However, on unseen frame elements (FEs), we observe a substantially larger performance drop of -27.0\% F1 score. This significant degradation indicates a fundamental challenge in generalizing to entirely new frame elements. The disparity between these two scenarios provides valuable insights into the model's learning dynamics: the model appears to develop strong transferable knowledge about common frame elements that appear across multiple frames, enabling it to maintain reasonable performance even when encountering new frames that use familiar elements.

The stark performance difference with unseen FEs can be attributed to a few factors. First, unseen FEs are often highly specific to particular frames and may represent more nuanced or specialized semantic roles. Second, these elements typically have fewer analogous examples in the training data, limiting the model's ability to learn generalizable patterns. Third, the contextual cues for identifying these specialized FEs may be more subtle or require domain-specific knowledge that the model hasn't adequately acquired during training.


\input{tables/yags-metrics}
\paragraph{Out-of-domain Evaluation}
We also evaluate the performance of Phi-4 on out-of-domain samples using the YAGS dataset (Table~\ref{tab:yags}). We include an in-context learning GPT-4o implementation as a baseline along with SEMAFOR~\cite{das-etal-2014-frame}. SEMAFOR is one of the first frame-semantic parsing systems, and the only other previous work which was evaluated on the YAGS dataset; however, it is often outperformed by modern approaches. We found that both LLM implementations performed quite poorly, with GPT-4o achieving an F1 score of 0.387 and Phi-4 achieving 0.533. Both of these are lower than SEMAFOR's performance. This indicates an area where LLMs struggle significantly more than previous approaches. 

We also perform an assessment of the errors of these models to understand their cause. One observation we made is that there are many FEs in the YAGS dataset which are not defined in FrameNet. Another observation is that the sentences in YAGS tend to use poor grammar and often use slang. Additionally, we found that Phi-4's predictions were often more aligned with our human judgments than the original annotations, hinting at a possibility of data quality issues (further discussed in Appendix~\ref{app:yags-bad}). These factors, along with the new topics of discussion in the sentences are likely what leads to the poor performance on YAGS. 

\input{tables/benchmark-correlation}
% \vspace{0.3cm}
\subsection{Benchmark Correlation Analysis}
Finally, we aim to understand what makes particular LLMs better than others on frame-semantic parsing. To do this, we analyze the correlation between our performance metrics and several common benchmarks for each LLM, where available. For this experiment, we focus on the IFEVal~\cite{zhou2023instructionfollowingevaluationlargelanguage}, BBH~\cite{suzgun2022challengingbigbenchtaskschainofthought}, GPQA~\cite{rein2023gpqagraduatelevelgoogleproofqa}, MUSR~\cite{sprague2024musrtestinglimitschainofthought}, and MMLU~\cite{hendrycks2021measuringmassivemultitasklanguage} benchmarks. We compute partial correlations between each benchmark and the F1 score on argument identification, accounting for model size as a confounding variable. Table~\ref{tab:benchmark-correlation} shows the correlation for each benchmark.

Our results indicate that MUSR exhibits the strongest positive correlation with frame-semantic parsing performance. Given that MUSR is designed to assess multistep reasoning, this suggests that models excelling in structured reasoning tasks also tend to perform well in frame-semantic parsing. Similarly, BBH and MMLU-PRO show strong positive correlations, aligning with their emphasis on complex reasoning and broad knowledge across multiple disciplines.

Interestingly, we observe a negative correlation with IFEval, which evaluates instruction-following capabilities using verifiable constraints. This suggests a potential trade-off between strict adherence to instructions and general problem-solving ability. This aligns with our earlier findings (Section~\ref{sec:model-selection}) that instruction-tuned models underperform their base versions on frame-semantic parsing. One possible explanation is that instruction-tuned models prioritize following explicit directives over deep semantic understanding.

\input{tables/candidate-frame}
% \vspace{0.3cm}
\subsection{Frame Identification}
Previous work~\cite{devasier-etal-2024-robust} explored the possibility of filtering candidate targets produced by matching potential lexical units using a frame identification model. To build upon this idea towards a single-step frame-semantic parsing method, we explore the potential of frame elements being used to perform frame identification. In this approach, no ground-truth frame inputs are given. This also removes the bias from the model assuming the input always has at least one frame element. 

We compared this method with state-of-the-art approaches not using exemplar sentences, including KGFI~\cite{su-etal-2021-knowledge}, CoFFTEA~\cite{an-etal-2023-coarse}, and KAF-SPA~\cite{zhang2023knowledge}. We used Phi-4 for this experiment for the same reason as previous experiments. We found that directly using the model performed poorly, likely due to bias in the model's training using ground-truth frames, i.e., each input contains the given frame. To address this, we fine-tuned Phi-4 using candidates ($\text{Phi-4}_{cand}$) from the training set produced by \citet{devasier-etal-2024-robust} and achieved very strong performance. 

Sometimes frame elements are predicted for multiple candidate frames. When this happens, we randomly select one of the frames to be used as the prediction. Other options were explored, such as selecting the one with the most frame elements, only selecting the first frame, or utilizing GPT-4o as a tie-breaker, but none of these were effective. 

This method showed strong performance, particularly on ambiguous targets--targets with more than one possible frame-- where it achieved an accuracy of 0.862, higher than any previous approach. If we apply lexicon filtering~\cite{su-etal-2021-knowledge} on unambiguous targets, as is common among previous approaches, the overall accuracy is further increased to 89.4\%.

