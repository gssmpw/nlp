\section{Introduction}
Frame-semantic parsing~\cite{gildea-jurafsky-2002-automatic} is a fundamental task in natural language understanding that involves identifying semantic frames~\cite{baker-etal-1998-berkeley-framenet} and their associated elements within a sentence. This process is typically divided into three sub-tasks: target identification (detecting words that evoke frames, e.g., began in Figure~\ref{fig:fsp-example}), frame identification (determining the specific frame evoked, i.e., Activity\_start), and argument identification (extracting frame elements, i.e., Time, Agent, and Activity).

Traditional approaches to frame-semantic parsing have found success with supervised classification models~\cite{chakma2024semantic}. However, the potential of large language models (LLMs) for this task remains largely unexplored. Recent work~\cite{Su2024} has applied in-context learning with LLMs but found their performance to be significantly weaker, raising concerns about their ability to accurately extract frame-semantic arguments.

\input{figures/fsp-example}

In this work, we conduct a comprehensive study on the effectiveness of LLMs for frame-semantic argument identification, evaluating key factors that may influence performance, including input representation formats, model architecture and scale, and generalization to unseen and out-of-domain samples. Our experiments span a diverse range of state-of-the-art LLMs, from 0.5B-parameter models to 78B-parameter models, including both open-source models (e.g., Qwen 2.5~\cite{qwen2025qwen25technicalreport}, Llama 3) and closed-source models (e.g., GPT-4o).

Recent work~\cite{devasier-etal-2024-robust} has explored unifying target identification and frame identification by applying frame identification models to candidate targets. We also expand on this idea with a novel method for unifying frame identification and argument identification by leveraging predicted frame elements of candidate frames. 

Our findings reveal several important insights. First, we confirm that LLMs struggle in zero-shot and few-shot settings, reinforcing prior concerns about their reliability for frame-semantic parsing. Second, we demonstrate that the choice of input representation significantly impacts model performance, with JSON-based formats showing superior results. Surprisingly, we found that while model scale generally correlates with better performance, smaller models like Qwen 2.5 (3B) outperform the much larger Llama 3.3 (70B). Finally, our proposed frame identification method using predicted frame elements achieves competitive performance, particularly for ambiguous targets (i.e., words that can evoke multiple frames), where it surpasses previous state-of-the-art approaches by +1.2\%.

To summarize, this work makes the following contributions.
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
    \item We conduct a systematic evaluation\footnote{All of our training and evaluation code is available at \url{https://anonymous.4open.science/r/llm-fsp-831F}} of different input/output representations for frame-semantic parsing with generative LLMs.
    \item We produce comprehensive benchmarks of different LLM architectures at varying scales, resulting in a +3.9\% F1 score improvement over the previous best argument identification model. 
    \item We developed a novel frame identification approach leveraging predicted frame elements on candidate frames which achieves state-of-the-art performance on ambiguous targets.
\end{itemize}



% Our key contributions are:
% \begin{itemize}
% \item A systematic evaluation of different input representations for frame-semantic parsing with LLMs, demonstrating the superiority of JSON-based formats and providing insights for future implementations
% \item Comprehensive benchmarking of various model architectures and scales, revealing the relationship between model size and performance while identifying efficient smaller-scale alternatives
% \item A novel frame identification approach leveraging predicted frame elements that achieves state-of-the-art performance on ambiguous cases, demonstrating the value of integrating frame element information
% \item Detailed analysis of LLMs' generalization capabilities for unseen frames and frame elements, providing insights into their robustness and applicability in real-world scenarios
% \end{itemize}



% Frame-semantic parsing, a critical task in natural language understanding, involves the automatic extraction of semantic frames~ and their elements from text. This task is foundational for applications such as information extraction, question answering, and machine translation, as it provides a structured representation of the semantic roles and relationships within a sentence. Despite its importance, frame-semantic parsing remains a challenging problem, particularly in identifying frame elements (FEs) with high precision and recall. Traditional approaches have relied heavily on supervised learning with annotated datasets, such as FrameNet~\cite{}, and have predominantly used classification-based methods for argument identification~\cite{chakma2024semantic}. These methods, often built on BERT-like encoder models, treat the task as either token/segment classification~\cite{Su2024, aged2023, zheng-etal-2022-double, Bastianelli2020EncodingSC, opensesame, lin-etal-2021-graph} or span identification~\cite{Ai_Tu_2024, devasier-etal-2024-claimlens, aged2023, chen-etal-2021-joint}. However, such approaches are limited by their reliance on large amounts of annotated data and their inability to generalize to unseen frames or domains.



% Recent advances in large language models (LLMs) have sparked interest in their potential for frame-semantic parsing, particularly in zero-shot, few-shot, and fine-tuned settings. LLMs, with their vast pretraining on diverse corpora, offer the promise of leveraging implicit knowledge to perform complex semantic tasks without extensive task-specific supervision. However, initial explorations have revealed significant limitations. For instance, \citet{Su2024} demonstrated that even state-of-the-art LLMs like Llama 2 perform poorly on zero-shot and few-shot frame-semantic parsing, suggesting that frame-semantic knowledge may not be sufficiently captured during pretraining. 

% In this paper, we conduct a comprehensive evaluation of LLMs for frame-semantic parsing, focusing on their ability to identify semantic frame elements under varying conditions. We explore multiple input representations for frame and frame element annotations, including XML-tagged strings, JSON representations, and natural-text bulleted lists. Our experiments span a range of LLMs, both open-source and proprietary, of varying sizes, to assess their scalability and generalizability.

% Our findings reveal that LLMs perform poorly in zero-shot and few-shot settings, likely due to the scarcity of frame-semantic parsing data in their pretraining corpora. However, fine-tuned models exhibit significant improvements, with GPT-4o-mini outperforming the previous best non-LLM system~\cite{Ai_Tu_2024}. This suggests that while LLMs require task-specific adaptation to excel at frame-semantic parsing, their potential for achieving state-of-the-art performance is substantial.

% The key contributions of this work are as follows:
% \begin{itemize}
% \item We provide the first systematic evaluation of LLMs for frame-semantic parsing using in-context learning, and fine-tuned settings.
% \item We introduce novel input representations for frame and frame element annotations, demonstrating their impact on model performance.
% \item We identify the limitations of LLMs in zero-shot and few-shot settings and propose fine-tuning as a viable solution for achieving competitive results.
% \item We release our fine-tuned models and experimental framework to facilitate further research in this area.
% \end{itemize}

% Our work not only advances the understanding of LLMs' capabilities in structured semantic tasks but also provides practical insights for leveraging these models in real-world applications.





% Motivate need for frame-semantics

% Explain background of FE extraction

% In this paper we perform detailed evaluations the capability of large language models on identifying semantic frame elements. In particular, we explore LLM approaches in zero-shot, few-shot, and instruction fine-tuned settings. 

% As an input to the LLM, we provide the frame name and definition, frame element names and their definitions. For few-shot approaches, we also include up to 5 training samples and all exemplar sentences. 

% We explore multiple approaches of representing FE annotations, namely as XML-tagged strings, JSON representations of existing/complete frame elements, and as a natural-text bulleted list. 

% We experiment with commonly used LLMs of varying sizes, both open-source and proprietary.

% We found that LLMs perform very poorly on zero-shot and few-shot approaches. We hypothesize that this is likely because frame-semantic parsing data is not easily accessible and likely is not included in the pretraining data. 

% Finetuned models perform significantly better, with GPT-4o-mini outperforming the previous-best non-LLM system~\cite{Ai_Tu_2024}.

% Results

% Findings

% Contributions