% \vspace{0.2cm}
\section{Conclusion}
% In this study, we conducted a comprehensive evaluation of large language models (LLMs) on argument identification. Our experiments reveal several key insights: JSON-based representations significantly improve extraction performance, model fine-tuning with LoRA can enable smaller models to achieve competitive accuracy, and model. Additionally, our novel approach to frame identification, leveraging predicted frame elements of candidate frames, demonstrated state-of-the-art performance, particularly on ambiguous cases. These findings highlight the potential of LLMs to generalize beyond seen frames and frame elements, offering a promising avenue for more robust semantic parsing.

% Despite these advancements, several challenges remain. Our results indicate that LLMs still struggle with out-of-domain generalization, as seen in the YAGS dataset evaluation, where performance lagged behind traditional approaches. Additionally, our benchmark correlation analysis suggests that instruction-tuning can sometimes hinder frame-semantic performance, indicating a trade-off between general instruction-following abilities and domain-specific structured reasoning. Future research is needed explore integrated methods that better utilize relationships between different frame-semantic parsing subtasks, as well as techniques for enhancing robustness in cross-domain settings. 

This work presents a comprehensive evaluation of large language models for frame-semantic parsing, with a particular focus on argument identification. Our systematic analysis reveals several important insights about the capabilities and limitations of LLMs in this domain. While LLMs demonstrate poor performance in zero-shot and few-shot settings, fine-tuned models achieve state-of-the-art results, with Qwen 2.5 (72B) surpassing previous approaches by a significant margin (+3.9\% F1 score).

Our investigation into input representations demonstrates that LLMs are sensitive to specific formats, with JSON-based formats achieving superior performance compared to alternatives. Our correlation analysis between frame-semantic parsing performance and common LLM benchmarks reveals that models excelling in multistep reasoning (as measured by MUSR) tend to perform better at argument identification, while instruction-following capabilities (measured by IFEval) show a negative correlation.

However, our results also highlight significant challenges. The substantial performance degradation on unseen frame elements (-27.0\% F1 score) and out-of-domain data indicates that current LLM approaches, despite their improvements over previous methods, still struggle with generalization. This limitation suggests that frame-semantic knowledge may not be sufficiently encoded, and that additional strategies may be needed to enhance model robustness across diverse contexts.

Our novel approach to frame identification using predicted frame elements shows promise, particularly for ambiguous targets, where it achieves state-of-the-art performance. This suggests that integrating frame element predictions into the frame identification process could be a valuable direction for future research.

% These findings have important implications for both theoretical understanding and practical applications of frame-semantic parsing. While our results demonstrate the potential of LLMs in this domain, they also highlight the need for continued research into improving generalization capabilities and handling out-of-domain scenarios. 


% Limitations of FSP
% - Need to expand FSP to work across sentences
% - Abstract/infer frame elements from context
% - Could FSP be a helpful pretraining strategy to improve other NLP tasks?