\section{Theoretical Proof}
\subsection{Analysis of Effectiveness of Self-consistency and Coverage Design}
\label{append:proof-self}
\subsubsection{Analysis of Effectiveness of Self-consistency}
First, according to the series resistor assumption of CoT, the total resistance of series resistors can be expressed as:
\begin{equation}
    R_{\text{all}} = R_0 + R_{\text{CoT}}.
\end{equation}
Building upon the illustration in Figure~\ref{fig:cot-strategy}c, we hypothesize that self-consistency, achieved through multiple parallel reasoning processes and subsequent result aggregation by majority voting, can be represented as an aggregation resistor integrated alongside several parallel CoT resistors. In this framework, the effective resistance of the reasoning process transitions from  $R_{\text{CoT}}$ to $\frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}}$, accompanied with a majority-voting resistor $R_S$ to determine the final outcome. Consequently, the total resistance associated with self-consistency, denoted as $R^{'}_{\text{all}}$, is expressed as:
\begin{equation}
    R^{'}_{\text{all}} = R_0 + R_S + \frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}},\label{eq:r-all-self}
\end{equation}
where $n$ is the number of samples, \(R_{\text{CoT}}^i\) denotes the resistance corresponding to the \(i-\)th sample and $R_S$ accounts for the resistor due to self-consistency integration.
This framework allows for rapid computation of the output power:
\begin{equation}
    P^{\text{self}}_{\text{out}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(R_0+R_S + \frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}})^2}.
\end{equation}
As the number of samples increases, the curves in Figure~\ref{fig:cot-strategy}c reveal that the real performance curve is almost exactly consistent with the theoretical range.
Furthermore, assume that \(\{R_{\text{CoT}}^i\}_{i=1}^\infty\) is bounded thus we can easily prove that the overall resistance has a lower bound:
\begin{equation}
    \lim_{n\rightarrow +\infty}R'_{\text{all}} = R_0 + R_S + \lim_{n\rightarrow +\infty}\frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}} = R_0 + R_S.
\end{equation}
The corresponding performance bound is:
\begin{equation}
    \hat{P}^{\text{self}}_{\text{out}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(R_0+R_S)^2}
\end{equation}
As shown in Figure~\ref{fig:cot-strategy}c, model performance aligns with this theoretical upper bound as sample size increases.



\subsubsection{Analysis of Effectiveness of Coverage Design}
Similarly, we hypothesize that coverage can be considered correct if at least one of multiple parallel reasoning is accurate. Under this assumption, the integration difficulty in self-consistency verification approaches zero. Analogously, this can be modeled as a zero-value combined resistance in parallel with multiple CoT resistors, where the coverage resistance of self-consistency $\hat{R}_{\text{all}}$ is given by:
\begin{equation}
    \hat{R}_{\text{all}} = R_0 + R_S + \frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}}= R_0 + \frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}}
\end{equation}
where $n$ denotes the number of samples. Using this, the theoretical output power can be similarly calculated as:
\begin{equation}
    P^{\text{cover}}_{\text{out}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(\frac{1}{\sum_n\frac{1}{R_{\text{CoT}}}} + R_0)^2},\label{eq:p-cover}
\end{equation}
which are much more smaller than $P^{\text{self}}_{\text{out}}$ in the same $n$ sample size with $R_S \gg 0$. 
As the number of samples increases, the curve in Figure~\ref{fig:cot-strategy}c demonstrates that the real performance aligns closely with the theoretical range. Building upon the same bounded assumption of \(\{R_{\text{CoT}}^i\}_{i=1}^\infty\), this setup also defines a resistance lower bound:
\begin{equation}
    \lim_{n\rightarrow +\infty}\hat{R}_{\text{all}} = R_0 + \lim_{n\rightarrow +\infty}\frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}} = R_0
\end{equation}
and the corresponding lower-bound performance:
\begin{equation}
    \hat{P}^{\text{cover}}_{\text{out}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2}{R_0}.
\end{equation}
As shown in Figure~\ref{fig:cot-strategy}c, the model's performance gradually converges toward this upper bound and much larger than the performance of self-consistency, demonstrating consistency with the theoretical predictions.

\paragraph{NOTE}
According to the theoretical formulation in Eq.~\ref{eq:p-cover}, in the reasoning phase where the resistance is significantly greater than \( R_0 \), the overall trend shows that Coverage is approximately proportional to the number of samples. Meanwhile, certain Inference Scaling Laws indicate that Coverage and the logarithm of the number of samples exhibit an approximately proportional relationship. These two observations are not contradictory, as our model assumes relatively independent sampling. However, we have observed that when the number of samples exceeds 100, an increasing proportion of the samples become duplicates rather than independent samples. Consequently, the probability of generating new samples follows the trend where \( n \) samples are expected to yield approximately \( \log n \) distinct reasoning samples.

\subsection{Analysis of Effectiveness of Fine-grained Self-consistency}\label{append:proof-fine}
According to Eq.~\ref{eq:r-all-self}, the original circuit can be considered as a large parallel reasoning and verification process and the total resistor is as follows:
\begin{equation}
    R'_{\text{all}} = R_0 + R_S + \frac{1}{\sum_{i=1}^n\frac{1}{R_{\text{CoT}}^i}}.
\end{equation}
As shown in Figure~\ref{fig:cot-optimization}a, this process is decomposed into multiple sequential, smaller-scale parallel reasoning and verifications. This refinement transforms \textit{a single large parallel reasoning resistance with significant verification resistance} into \textit{a sequence of smaller parallel reasoning resistance and smaller verification resistance}. Formally, this is expressed as:
\begin{equation}
    R^{\text{fine}}_{\text{all}} = R_0 + \sum_j R_j^S + \sum_j\frac{1}{\sum_{i=1}^n\frac{1}{R_j^{i}}},
\end{equation}
where $n$ denotes the sampling size, $ R_j^S $ represents the small and fine-grained verification resistance for $j$-th step, and $ R_j^{i} $ corresponds to smaller reasoning resistance for $j$-th step for $i$-th sampling. The total resistance can be broken down into resistance of sub-step resistor as $ R^i_{\text{CoT}} = \sum_j R_j^{i} $. 

We aim to demonstrate that our approach indeed optimizes the total resistance, i.e., $ R^{\text{fine}}_{\text{all}} < R'_{\text{all}} $. To prove this, we need to show that $ R^{\text{fine}}_{\text{all}} - R'_{\text{all}} < 0 $. The proof is as follows:
\begin{align}
    R^{\text{fine}}_{\text{all}} - R'_{\text{all}} &= \sum_j R_j^S - R_S + \frac{R_{\text{CoT}}}{n} - \sum_j\frac{R_j^{i}}{n}\\
    &= \sum_j R_j^S - R_S.
\end{align}
It is evident that verifying correctness at the individual step level is simpler than verifying the correctness of the entire chain, so $ \sum_i R_i^S < R_S $. Therefore, $ R^{\text{fine}}_{\text{all}} - R'_{\text{all}} < 0 $, indicating that the total resistance of the circuit has been optimized.


\subsection{Analysis of Effectiveness of Chain-of-Self-consistency}\label{append:proof-chain}
Furthermore, we optimize the total circuit resistance from a different perspective, specifically by improving the resistance of validation resistor through self-consistency. In this approach, the validation resistor is decomposed into a parallel combination of multiple validation resistors and a meta-validation resistor with resistance $R_{\text{meta}}$, which verifies the correctness of the validation process. The original equation is thus reformulated as:
\begin{align*}
R^{\text{CoV}}_{\text{all}} &= R_0 +\frac{1}{\sum_{j=1}^k\sum_{i=1}^n \frac{1}{R_{\text{S}}}} + \sum_{j=1}^k\left(R_{\text{meta}}\right) + \frac{R_{\text{CoT}}}{n}\\
&= R_0 + \frac{R_{\text{S}}}{nk} + k R_{\text{meta}} + \frac{R_{\text{CoT}}}{n}.
\end{align*}
Clearly, as the sample size $n$ and the meta-validation steps $k$ increase, $\frac{R_{\text{S}}}{nk}$ decreases, leading to reduced difficulty in meta-validation. This significantly optimizes the total resistance and improves output power. Figure~\ref{fig:cot-optimization}b illustrates this performance enhancement.

However, as the number of samples requiring validation grows, the model's accuracy first increases and then decreases. Specifically, for sufficiently large validation steps, the resistance satisfies:
\begin{equation}
    \lim_{nk \to +\infty} R^{\text{CoV}}_{\text{all}} = R_0 + k R_{\text{meta}}.
\end{equation}
With excessive validation steps, $k$ becomes too large, resulting in $k R_{\text{meta}} > R_S$. Consequently, $R^{\text{CoV}}_{\text{all}}$ exceeds its optimized value, leading to a performance trend characterized by initial improvement followed by decline.

\section{Proof of Impact of Resistor Optimization}\label{append:proof-resistor}
To compute the derivatives of $ P_{\text{model}} $ with respect to $ R_{\text{CoT}} $, we start with the given expression:
\begin{equation}
P_{\text{model}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(R_{\text{CoT}} + R_0)^2}.
\end{equation}
According to Eq.~\ref{eq:resistor}, let $ R_{\text{CoT}} = R_1 + R_2 $, where $R_1 \gg R_2$, if $R_1$ is improved to $\frac{R_1}{k}$, the power can be calculated as:
\begin{equation}
    P^1_{\text{model}} = \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(\frac{R_1}{k}+R_2 + R_0)^2}
\end{equation}
The corresponding power improvement is:
\begin{align}
    \Delta P^1_{\text{model}} &=P^1_{\text{model}}-P_{\text{model}} \\
    &= \frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(R_1+R_2 + R_0)^2}-\frac{\left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0}{(\frac{R_1}{k}+R_2 + R_0)^2}\\
    &= \left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0 (\frac{1}{(R_1+R_2 + R_0)^2}-\frac{1}{(\frac{R_1}{k}+R_2 + R_0)^2})
\end{align}

Similarly, if $R_2$ is improved to the original $\frac{R_2}{k}$, the power improvement can be calculated as:
\begin{equation}
    \Delta P^2_{\text{model}} = \left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0 (\frac{1}{(R_1+R_2 + R_0)^2}-\frac{1}{(R_1+\frac{R_2}{k} + R_0)^2})
\end{equation}
To determine which improvement has a greater effect, consider:
\begin{equation}
    \Delta P^1_{\text{model}} - \Delta P^2_{\text{model}} = \left(\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}\right)^2 R_0 (\frac{1}{(\frac{R_1}{k} +R_2 + R_0)^2}-\frac{1}{(R_1+\frac{R_2}{k} + R_0)^2})
\end{equation}
Define: 
\begin{equation}
A = \frac{R_1}{k} + R_2 + R_0, \quad B = R_1 + \frac{R_2}{k} + R_0
\end{equation}
The fractional difference simplifies as: 
\begin{equation}
\frac{1}{A^2} - \frac{1}{B^2} = \frac{B^2 - A^2}{A^2 B^2}
\end{equation}
We need to prove that for \( R_1 \gg R_2 \) whether $B^2 - A^2 $ larger or smaller than zero. Now, let's calculate \( B^2 - A^2 \):
\begin{equation}
B^2 - A^2 = (B - A)(B + A)
\end{equation}
where: 
\begin{align}
B - A &= \left(R_1 + \frac{R_2}{k} + R_0\right) - \left(\frac{R_1}{k} + R_2 + R_0\right)\\
&= R_1 - \frac{R_1}{k} + \frac{R_2}{k} - R_2\\
&= R_1\left(1 - \frac{1}{k}\right) + R_2\left(\frac{1}{k} - 1\right)
\end{align}

and:
\begin{align}
B + A &= \left(R_1 + \frac{R_2}{k} + R_0\right) + \left(\frac{R_1}{k} + R_2 + R_0\right)\\
&= R_1 + \frac{R_1}{k} + R_2 + \frac{R_2}{k} + 2R_0
\end{align}

Analysis of the Sign of \( B^2 - A^2 \):
\begin{equation}
B^2 - A^2 = (B - A)(B + A)
\end{equation}
We only need to examine the sign of \( B - A \), since \( B + A > 0 \) holds true at all times.since \( R_1 \gg R_2 \), and \( 1 - \frac{1}{k} > 0 \) for \( n > 1 \), we have:
\begin{align}
R_1\left(1 - \frac{1}{k}\right) - R_2\left(1 - \frac{1}{k}\right) &>0\\
(R_1-R_2)\left(1 - \frac{1}{k}\right)&> 0\\
B^2 - A^2 &> 0\\
\Delta P^1_{\text{model}} - \Delta P^2_{\text{model}} &> 0
\end{align}
Thus, $\Delta P^1_{\text{model}} < \Delta P^2_{\text{model}}$ the difference is negative, and the proof is complete. Therefore, The greater the original difficulty of the task corresponding to the capability, the greater the impact on the final performance.