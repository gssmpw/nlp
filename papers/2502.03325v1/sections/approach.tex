
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/p-acc-analysis.pdf}
    \caption{
        \textbf{The Experimental Results of Theoretical Model Verification on \modelname{}.}\\
        \textbf{a,}
        An analysis of \modelname{} incorporating the CoT approach reveals a scatter plot illustrating the relationship between model accuracy and theoretical power with 0.8843 Spearman coefficient.
        \textbf{b.}
        Six similarity metrics are employed to evaluate whether context demonstrations can be interpreted as a semantic magnetic field. The results indicate that the projection length achieving the highest coefficient of 0.9106 between practical accuracy and theoretical power.
        \textbf{c,}
        The integrated \modelname{} framework combines ICL and CoT. Across three retrieval strategies for ICL demonstrations, a consistent linear correlation is observed.
        \textbf{d,}
        Experiments on various LLMs under different power supply voltages, representing their capacities, reveal a strong linear correlation between theoretical power estimates and observed accuracy.
        \textbf{e,}
        Broader analyses across diverse tasks, including multi-hop knowledge reasoning, medical probing, and robotic planning, demonstrated robustness.\vspace{-8pt}
    }
    \label{fig:verification}
    
\end{figure}

\section{Theoretical Model Verification}

\subsection{CoT Meets Ohm's Law}
First, we verify whether CoT adheres to Ohm's law. Since the electric current $I_{model}$ in Eq.~\ref{eq:ohm} is not directly observable, we instead examine whether $P_{out}$ is linearly related to true accuracy, thereby indirectly validating CoT Ohm's law.
Specifically, to control variables, we remove the ICL and let the model perform zero-shot reasoning step-by-step following the instructions, which satisfies:
$
    P_{\text{out}} = \frac{\mathcal{E}_{\text{model}}^2 R_0}{(R_{\text{CoT}} + R_0)^2}.
$
The strong Spearman correlation coefficient between theoretical power ($P_{\text{out}}$) and observed accuracy (Acc), shown in Figure~\ref{fig:verification}a, is 0.8843, with a $p$-value below 0.01. This strong correlation suggests difficulty of CoT reasoning actually exhibits resistor-like behavior, making it a reliable predictor of model performance.

\subsection{ICL Meets Faraday's Law}
We conceptualize contextual demonstrations in ICL as a semantic magnetic field governed by Faraday's Law during reasoning. Under this framework, semantic influence and proximity are quantified by projection length, as illustrated in the lower right of Figure~\ref{fig:verification}b. To validate this hypothesis, we evaluate various metrics for contextual proximity between demonstrations and user requests. Our analysis (Figure~\ref{fig:verification}b) shows that the Spearman correlation between the calculated power $P_{\text{model}}$ in Eq.~\ref{eq:power} and observed spearman coefficient and R$^2$ merely for projection length exceeds 0.85. This indicates that the geometric alignment of the demonstrations with the task request is a strong predictor of model performance, which further prove the existance of Faraday's Law of ICL.

\subsection{ICL and CoT Satisfy a Unified \modelname{}}
In an unprecedented integration, we unify ICL and CoT within a single \modelname{}, offering a comprehensive framework to explain their interaction and collective impact on model performance. We rigorously test this unified model using three distinct retrieval strategies for ICL demonstrations, all of which yields a consistent linear distribution, as illustrated in Figure~\ref{fig:verification}c. This consistent alignment, with a Spearman correlation coefficient of 0.8979 and a $p$-value approaching zero between calculated power in Eq.~\ref{eq:power} and practical accuracy, provides strong empirical evidence for the robustness of our theoretical model. These results confirm that both ICL and CoT not only can be calculated independently but are inherently compatible within the same theoretical structure, offering a powerful new way to describe and predict the behavior of LLM systems.
\vspace{-3pt}
\subsection{Extension Experiments on Various LLMs and Tasks.}\vspace{-3pt}
To demonstrate the generality of our theoretical \modelname{}, we conduct extension experiments across LLMs configured with varying power supply voltages to represent different capacities. Figure~\ref{fig:verification}d reveals a consistent linear correlation over 0.8348 between theoretical output power estimates and empirical accuracy, confirming the \modelname{}'s robustness in predicting performance. Beyond traditional mathematical reasoning tasks, we test \modelname{} on diverse real-world problems, including multi-hop knowledge reasoning~\citep{yang-etal-2018-hotpotqa}, medical probing~\citep{cheng2024adapting}, and robotic planning~\citep{valmeekam2023planbench}. As shown in Figure~\ref{fig:verification}e, \modelname{} achieves Spearman coefficients exceeding 0.86 and R$^2$ exceeding 0.77 across all tasks, underscoring its versatility in both cognitive and practical applications.\vspace{-3pt}

\section{Explanation based on \modelname{}}\vspace{-3pt}
In this section, we mainly pay attention to how to use \modelname{} to explain the effectiveness of existing prompting methods.
\subsection{Explanation for ICL Strategies}
\paragraph{Enhanced embeddings facilitate accurate semantic magnetic field strength estimation and retrieval.}
Recent advancements in embedding models have shown that improved text representations directly enhance LLM's efficacy in the retrieval process~\citep{liu2024incontext}.
This improvement is tied to the precise estimation of semantic magnetic field strength. As shown in Figure~\ref{fig:icl-exploration}a, we employ three models—BERT~\cite{devlin-etal-2019-bert}, RoBERTa~\cite{liu2019roberta}, and BGE~\cite{chen2024bge}—to represent and retrieve demonstrations in ICL.
Among these, BGE achieves the highest degree of alignment, with a Spearman correlation coefficient of 0.9106 between theoretical power and empirical accuracy, significantly surpassing the other models. This result highlights superior embedding ability attributed to the accurate semantic magnetic field estimation.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figure/different-embedder.pdf}
    \caption{
        \textbf{The Experimental Explanation for Existing ICL Strategies.}\\
        \textbf{a,}
        The relationship between theoretical power and actual accuracy for various encoders is assessed. BGE achieves the highest Spearman correlation coefficient of 0.9106.
        \textbf{b,}
        A comparison of the Vanilla Encoder and the Meta-Recognition Representation reveals the latter’s ability to enhance the linear correlation between theoretical power and practical accuracy, from 0.9130 to 0.9337.
        \textbf{c,} The distribution of magnetic field strengths in generated demonstrations (with CDF representing cumulative distribution function and CCDF representing complementary cumulative distribution function). Further, we demonstrates the effectiveness of different Synthetic-CoT with degree of different degree of semantic field strength.
        \textbf{d,} The effectiveness of Synthetic-CoT is analyzed across three conditions: Positive Synthetic-CoT (demonstrations with positive extra voltage from ICL based on positive magnetic fields), Zero-shot CoT (no demonstrations), and Negative Synthetic-CoT (demonstrations with negative extra voltage from ICL based on negative magnetic fields).
        The left scatter plot displays the performance distribution of these conditions on the power-accuracy landscape. The right bar chart highlights factors contributing to Synthetic-CoT's optimization of extra voltage provided by ICL.
    }
    \label{fig:icl-exploration}
\end{figure}


\paragraph{Meta Recognition Enhance Magnetic Field Strength Estimation Modeling.}
The study of \citet{didolkar2024metacognitive} introduces Meta-Recognition, a method leveraging GPT-4~\cite{achiam2023gpt} to decompose complex problems into modular sub-capabilities for ICL representation, improving LLMs' performance.
We attribute its effectiveness to the enhanced sample representation modeling, which enhances ICL voltage and subsequently boosts output power and accuracy.
As illustrated in Figure~\ref{fig:icl-exploration}b, Meta-Recognition significantly increases the correlation between theoretical power and practical accuracy, improving from 0.9130 to 9337. It underscores that representation based on sub-capability decomposition effectively enhances the precision of magnetic field strength modeling.

\paragraph{Synthetic-CoT predominantly generates samples with positive magnetic fields, enhancing model performance.}
Synthetic-CoT, a widely used approach in ICL, enables LLMs to autonomously generate ICL demonstrations, thereby improving performance~\citep{shao2023synthetic}. As illustrated in Figure~\ref{fig:icl-exploration}d, all ICL demonstrations produced by Synthetic-CoT exhibit positive semantic magnetic field contributions, which are strongly correlated with enhanced model accuracy. This effect is likely due to the positive voltage induced by the magnetic field, which optimizes circuit functionality. 
As shown in Figure~\ref{fig:icl-exploration}d, the larger ICL magnetic field strength corresponds to the larger power and the higher accuracy.
To further investigate the impact of magnetic field polarity on model performance, we analyzed demonstrations with negative magnetic field. Including such examples led to a 21.47\% reduction in accuracy and a substantial decline in theoretical power output (Figure~\ref{fig:icl-exploration}d). These findings underscore the critical role of positive magnetic field demonstrations in the superior performance achieved by Synthetic-CoT.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/cot-strategy.pdf}
    \caption{
        \textbf{The Experimental Explanation for Existing CoT Strategies.}\\
        \textbf{a,}
        The distributions of accuracy, power, and resistance for Zero-shot without CoT and Zero-shot with CoT are presented. The average resistance for Zero-shot without CoT is 10.93, significantly higher than the 1.50 observed for Zero-shot with CoT.
        \textbf{b,}
        Mechanistic analysis of Tool Usage (TU) and Program-of-Thought (PoT) within the \modelname{}. Initially, TU, modeled on the original CoT, had non-zero computational resistance (\(R_C\)), limiting power-accuracy correlation. Assuming \(R_C = 0\) achieves a 10\% improvement in the Spearman coefficient.
        PoT, representing TU with structured code, further enhanced performance by keeping \(R_C=0\) and reducing planning resistance \(R_P\), achieving another 10\% improvement in power-accuracy correlation.
        \textbf{c,}
        A circuit-based rationale to explain how to utilize self-consistency and coverage to enhance performance. Self-consistency reduces total resistance by processing multiple reasoning resistors in parallel (sampling $n$ solutions and selecting the output with the highest consensus). In contrast, coverage minimizes resistance by increasing sample size with negligible resistance of voting resistor (sampling $n$ instances and passing the task if any succeeds).
    }
    \label{fig:cot-strategy}
\end{figure*}


\subsection{Explanation for CoT Strategies}
\paragraph{Direct Answering Introduces Greater Logical Resistor Compared to CoT}
Recent studies suggest that generating direct answers without intermediary reasoning steps increases cognitive load on the model~\citep{feng2024towards,kojima2022large}. We hypothesize that the absence of explicit reasoning pathways introduces significant resistance of ``logical resistor,'' leading to weaker performance compared to Zero-shot CoT. To test this, we analyzed performance and power correlations for direct answering and Zero-shot CoT on a power-accuracy graph, finding a strong linear correlation of 0.9131 ($p$-value $< 0.01$). The average resistance for direct answering is calculated as 10.93, substantially higher than the 1.50 observed for Zero-shot CoT. This discrepancy corresponds to a performance reduction of over 30\% for direct answering.
These findings indicate that the lack of a CoT framework imposes additional greater logical resistor, hindering the model’s reasoning capabilities and resulting in diminished performance.

\paragraph{Tool Usage optimizes computational resistor, and PoT further reduces planning resistor, improving model performance.}
\citet{chen2024unlocking} propose that Program of Thought (PoT) and Tool Usage (TU) enhance computational reasoning capabilities to a theoretical maximum, effectively minimizing reasoning complexity to zero, akin to eliminating the resistance to zero. Inspired by this, we investigate the correlation between CoT and TU models under this "zero-resistance" assumption.
Our analysis revealed that modeling TU as a circuit with zero resistance of computational resistor increased the Power-accuracy Spearman correlation by over 10\% compared to the original model, as illustrated in Figure~\ref{fig:cot-strategy}b, which  validates our hypothesis.

Furthermore, we hypothesize that PoT, leveraging programming languages, reduces planning resistance. As shown in Figure~\ref{fig:cot-strategy}b, modeling PoT as a circuit with lower planning resistance achieved a correlation score exceeding 0.9423 between theoretical power and accuracy—significantly higher than TU. This finding highlights the superior effectiveness of PoT in improving model performance.

\paragraph{Self-consistency and Inference Scaling Law via Parallel Resistor in Circuits}
Self-consistency, as demonstrated by \citet{wang2023selfconsistency}, utilizes multiple reasoning paths simultaneously, and determines final decision through majority voting, enhancing performance robustness and accuracy. Here, we interpret its effectiveness using a circuit model, which introduce parallelization of original resistors ($R_{\text{CoT}}$), significantly reducing the total system resistance. As shown in Figure~\ref{fig:cot-strategy}c, the practical accuracy of the self-consistency strategy aligns closely with the theoretical output power curve, validating the correctness of our proposed model.
Recent findings on the Inference Scaling Law~\cite{wu2024inference,openai2024o1} further demonstrate that increasing the number of parallel samples, $n$, substantially improves accuracy. As $n$ grows, the resistance value of CoT resistor approaches zero, optimizing the reasoning process. Further, when coverage introduces an absolutely correct answer verification process with zero resistance of verification resistor, exceeding the gains achieved by self-consistency alone. The alignment between theoretical predictions and empirical results, also evident in Figure~\ref{fig:cot-strategy}c, reinforces the validity of this model. These results highlight the central role of self-consistency and inference scaling in enhancing the performance of circuits.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/icl-strategy.pdf}
    \caption{
        \textbf{The Experimental Exploration for ICL Strategies.}\\
        \textbf{a,}
        The relationship between the introduction of demonstrations and model performance. Using Reverse ICL + CoT, which introduces demonstrations with negative magnetic fields, impairs the model’s reasoning ability and results in performance falling below that of Zero-shot CoT. In contrast, Retrieval-ICL + CoT, incorporating demonstrations with positive magnetic fields, significantly improves performance.
        \textbf{b,}
        The upper part of the figure contrasts Diversity-oriented Static ICL (DS-ICL), where demonstrations are retrieved for the whole task, and Similarity-oriented Dynamic ICL (SD-ICL), where demonstrations are retrieved for each request. In SD-ICL, demonstrations are more aligned with test requests, creating a stronger semantic magnetic field.
        The lower part highlights Diversity-oriented Dynamic ICL (DD-ICL), where increasing sampling range enhances the diversity but decreases the magnetic field strength of retrieved demonstrations, resulting in a more diverse but weaker strength, leading to less accuracy.
        \textbf{c,}
        The left figure illustrates the demo-optimization process, a methodology designed to refine the demonstrations used in ICL. The right figure shows that demo-optimization outperforms traditional prompt optimization by improving theoretical power.
    }
    \label{fig:icl-strategy}
\end{figure}
\section{Exploration based on \modelname{}}
In this section, we primarily focus on how to use \modelname{} to explore phenomena that have never been well explained and more novel prompting methods.
\label{sec:exploration}
\subsection{ICL Exploration}



\paragraph{When Few-shot CoT Performs Worse than Zero-shot CoT?}
Despite the growing importance of ICL and CoT, a critical question remains: under what conditions is ICL essential, and when does zero-shot CoT suffice?
To investigate this, we systematically examine the interaction between these two strategies. As shown in Figure~\ref{fig:icl-strategy}a, our findings reveal a phenomenon we term ``reverse ICL," where demonstrations associated with a negative semantic magnetic field significantly impair the output power of circuits, thereby decreasing the model’s reasoning ability and resulting in worse performance compared to zero-shot CoT. This suggests that not all demonstrations contribute positively to the learning process. In contrast, demonstrations with a positive semantic magnetic field lead to an enhancement in performance. These results highlight the critical importance of selecting demonstrations based on their associated semantic magnetic field, which plays a crucial role in determining the success of ICL.


\paragraph{The influence of diversity in ICL retrieval: a static vs. dynamic perspective}
In ICL retrieval, two paradigms are commonly studied: static ICL, where the set of demonstrations remains fixed during testing, and dynamic ICL, where demonstrations are selected in real-time based on the specific test sample. Previous studies~\citep{li-qiu-2023-finding} reveal that semantic diversity in static ICL enhances performance by exposing the model to a broader range of contextual cues. Conversely, in dynamic ICL, diversity does not provide the same benefits and may even hinder performance~\cite{qin2024what}. This raises the question: \textit{Why does diversity yield such contrasting outcomes in these retrieval modes?}
To this end, we hypothesize that the benefits of diversity in static ICL can be likened to a magnetic field. Diverse demonstrations create a robust and expansive field, offering the model access to varied directional signals as test samples change. This broad-spectrum magnetic field stabilizes the model’s decision-making process, improving its generalizability and overall performance. As illustrated in Figure~\ref{fig:icl-strategy}b, static ICL benefits from this diversity, which amplifies the model's predictive power compared to random selection strategies.
In dynamic ICL retrieval, however, the focus shifts toward selecting demonstrations closely aligned with the test sample. This approach optimizes the magnetic field for stronger, more targeted signals in the relevant direction. In this context, diversity becomes minimally critical; the alignment and intensity of retrieved demonstrations take precedence. As shown lower panel in Figure~\ref{fig:icl-strategy}b, diverse but less aligned demonstrations enable less accuracy.

\paragraph{Demo-Optimization: a novel approach to enhance demonstration efficacy in ICL}

Recent advancements in prompt-based learning leverage feedback from outputs as model gradients, enabling large models to autonomously refine their prompts for more efficient learning~\citep{yuksekgonul2024textgrad}. Expanding on this foundation, we propose demo-optimization, a methodology tailored to optimize the demonstrations used in ICL. Demo-optimization focuses on balancing the relative effectiveness between user requests and demonstrations (See Method for implementation details).
As illustrated in Figure~\ref{fig:icl-strategy}c, demo-optimization significantly outperforms traditional prompt optimization approaches, which primarily refine instructions. This method not only improves accuracy but also theoretically strengthens reasoning capabilities. Moreover, it addresses the persistent underfitting problem in prior ICL methods, offering a more precise and scalable strategy for enhancing demonstration-driven learning.




\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/cot-optimization.pdf}
    \caption{
        \textbf{Experimental Evaluation of Chain-of-Thought Strategies and Power-Optimized Performance on Complex Tasks}\\
        \textbf{a,}
        Overview of the fine-grained self-consistency strategy in circuits. By decomposing parallel CoT resistors into smaller units, this approach effectively reduces overall circuit resistance value.
        \textbf{b,}
        Overview of the Chain-of-Verfication (CoV) strategy in circuits. In addition to optimizing parallel CoT resistances, this method incorporates the concept of voting resistors in self-consistency to further reduce overall circuit resistance. The CoV strategy achieves this by sampling multiple voting results and validating them to finalize the decision.
        \textbf{c,}
        Leveraging stronger magnetic fields and reducing logical resistance significantly enhances circuit performance. The power-enhanced model achieved notable results in the IOI (left figure) and the IMO (right figure).
        \textbf{d,}
        The methodology for incorporating sufficient theoretical power into manual ratings during idea generation scenarios. This approach demonstrates a significant improvement over the original solution by enhancing the accuracy and reliability of the evaluation process.
    }
    \label{fig:cot-optimization}
\end{figure}
\subsection{CoT Exploration}
\paragraph{Optimize the same proportion of capabilities. The greater the original difficulty of the task corresponding to the capability, the greater the impact on the final performance.}
We can prove that optimizing a resistor with higher resistance to its original value $\frac{1}{k}$ results in a greater reduction in overall resistance of reasoning resistor \( R_{\text{CoT}} \), yielding a more significant performance improvement compared to optimizing a lower resistance (See in Theoretical Proof).
As shown in the meta-analysis in Appendix~\ref{sec:method}, when solving questions that demand advanced planning, o1~\cite{openai2024o1}'s reasoning method, which emphasizes extensive exploration, significantly enhances the most challenging reasoning components. This improvement ultimately boosts overall performance on complex reasoning tasks. However, for tasks where the hardest component requires extensive domain-specific knowledge, insufficient optimization of such knowledge limits performance gains in those domains.

\paragraph{Fine-grained Self-consistency achieves better performance}
Easily, we can prove that, with a stable supply voltage, we can split a self-consistency ensemble into more fine-grained step-by-step self-consistency by conducting majority voting verification for each step, so that each resistance value of verification resistor can be smaller, and we can obtain larger output power (see Theoretical Proof). Indeed, as the number of self-consistency steps increases, the theoretical power value gradually increases. As shown in Figure~\ref{fig:cot-optimization}a, the corresponding performance also gradually increases. At the same time, the corresponding upper bound and lower bound also satisfy the original self-consistency and coverage. This not only proves the effectiveness of our proposed fine-grained self-consistency method but also proves the practical usability of the theoretical model.



\paragraph{Chain-of-Verification enhances model performance with parallelized verification resistors}
In conventional self-consistency frameworks, verification is typically conducted through series resistors. We propose an innovative approach, referred to as the ``chain-of-verification,'' which involves two key looped stages: first, verifying the correctness of individual outputs in parallel; and second, merging these verified results and subsequently meta-validating the correct to confirm the overall result of the merged outputs. It can enhance model performance by reducing the overall verification resistance. As depicted in Figure~\ref{fig:cot-optimization}b, parallelization initially boosts performance by distributing verification tasks and lowering verification resistance.
However, this improvement has limits. As parallelization increases, the overall resistors for parallel verifiers begins to rise. Beyond a critical threshold, this combined resistor surpasses the original self-consistency resistor, reversing the performance gains. As depicted in Figure~\ref{fig:cot-optimization}b, this also demonstrates excessive chain-of-verification introduces diminishing returns. Thus, the chain-of-verification framework underscores the importance of balancing parallelization and resistor to optimize model verification efficiency.


\section{Best Practice}
To evaluate the impact of sufficient power on LLM performance, we analyzed its effects across three domains: the International Olympiad in Informatics (IOI), the International Mathematical Olympiad (IMO), and Natural Language Processing (NLP) idea generation using the AI-Scientist framework~\cite{lu2024aiscientist}, by ensembling strategies outlined in Exploration Section.
Our findings, shown in Figure~\ref{fig:cot-optimization}c, demonstrate that augmenting the o1-preview model with enhanced power results in significant improvements.
The power-enhanced o1-preview outperformed AlphaCode~\citep{doi:10.1126/science.abq1158} and other state-of-the-art models on the IOI-level test set (code\_contests~\citep{doi:10.1126/science.abq1158}). Crucially, this boost consistently surpassed configurations lacking sufficient power, underscoring its pivotal role in model efficacy.
In competitive contexts, such as IOI 2024 and IMO 2024, the powered o1-preview outperformed nearly 80\% of global participants, earning silver and bronze medals. This highlights power as a decisive factor in high-stakes scenarios where traditional optimizations often fall short.
Beyond correctness-focused tasks, as shown in Figure~\ref{fig:cot-optimization}d, sufficient power also enhanced performance in creative challenges. For instance, in NLP-related idea-generation, it facilitates the better generation of novel and impactful research ideas (over 1 point of overall score improvement on average), expanding the potential of LLMs in both objective and subjective tasks.

\section{Conclusion \& Discussion}
The Electronic Circuit Model (\modelname{}) offers a unified framework for understanding and optimizing the ICL and CoT mechanisms, marking a significant advancement in LLM research. This framework elucidates model behavior while providing a quantifiable approach to enhance LLM performance. Our experiments demonstrate that \modelname{} effectively captures model intricacies, serving as a reliable tool for analyzing and improving AI capabilities. Moreover, applying power-based optimization derived from \modelname{} to advanced reasoning challenges, such as the International Olympiad in Informatics and the International Mathematical Olympiad, yielded superior results compared to leading competitors. These outcomes validate the theoretical foundation of \modelname{} and highlight its transformative potential in improving AI reasoning and learning efficiency, with broad implications for next-generation intelligent systems.
\section{Code Availability}
The code and relevant data are available at \url{https://github.com/LightChen233/ECM}.

\bibliographystyle{plainnat}
\bibliography{custom}
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\section{Method Implementation Details}
\label{sec:method}
\subsection{Theoretical Model}

\paragraph{Faraday's Law of In-Context Learning}
In-context learning (ICL) can be likened to the behavior of a semantic magnetic field generated by a sub-power source, as described by Faraday's Law. The positive direction of the semantic magnetic field is defined by the query semantic vector $\overrightarrow{S}_q$.
Given $N$ context samples represented by semantic vectors $\overrightarrow{S}_i$, the total semantic magnetic field strength $\Phi^B_0$ experienced by the model is the projection of these vectors onto $\overrightarrow{S}_q$. Mathematically:
\begin{equation}
    \Phi^B_0 = \sum^N_{i=1} \cos \theta_{qi} \cdot \left\lvert \overrightarrow{S}_i \right\rvert = \sum^N_{i=1} \frac{\overrightarrow{S}_q \cdot \overrightarrow{S}_i}{\left\lvert \overrightarrow{S}_q \right\rvert}\label{eq:phy-b}
\end{equation}
where $\cos \theta_{qi}$ represents the angle between $\overrightarrow{S}_q$ and $\overrightarrow{S}_i$.

As new context samples are processed, the semantic field strength $\Phi^B_0$ is hypothesized to decay linearly over time:
\begin{equation}
    \Phi^B(t) = -\lambda \Phi^B_0 t,
\end{equation}
where \(\lambda\) is the decay constant representing the rate at which the model forgets or absorbs information.
According to Faraday’s Law of electromagnetic induction, the rate of change in the semantic magnetic field induces an electromotive force, which drives effective ICL. With a time step \(\Delta t\) for each reasoning step, the induced EMF, \(\mathcal{E}_{\text{ICL}}\), is:
\begin{equation}
    \mathcal{E}_{\text{ICL}} = -\frac{\Delta \Phi^B(t)}{\Delta t} = \lambda \Phi^B_0.
\end{equation}
Substituting \(\Phi^B_0\) into this equation yields:
\begin{equation}
    \mathcal{E}_{\text{ICL}} = \lambda \sum^N_{i=1} \frac{\overrightarrow{S}_q \cdot \overrightarrow{S}_i}{\left\lvert \overrightarrow{S}_q \right\rvert}.
\end{equation}

In conclusion, this analogy offers a structured framework to analyze how information flows during ICL. By extending the dynamics of ICL through the principles of electromagnetic induction, we provide a mathematical model to explain how models incorporate and gradually forget information as they process contextual examples.

\paragraph{Ohm's Law of Chain-of-Thought}
In this framework, each sub-difficulty of a model introduces a distinct level of difficulty, analogous to resistors in series.
Formally, following \citet{chen2024unlocking}, the ease with which a model can solve a particular task is represented by its reasoning boundary. Let \( \mathcal{B}_{\text{CoT}} \) denote the overall reasoning boundary of a Chain-of-Thought process, and \( \mathcal{B}_{i} \) denote the reasoning boundary of the \(i\)-th sub-capability. The overall reasoning boundary can be expressed by the harmonic sum:
\begin{equation}
\mathcal{B}_{\text{CoT}} = \frac{1}{\sum^K_{i=1} \frac{1}{\mathcal{B}_{i}}}, \label{eq:granularity}
\end{equation}
where $K$ denotes the number of sub-capabilities.
Reasoning resistor, $R$, quantifies the difficulty of a reasoning process and is mathematically defined as the reciprocal of the reasoning boundary. A higher boundary corresponds to an easier problem and lower resistor. Specifically, we define the sub-difficulty of the $i$-th sub-task as:
\begin{equation}
    R_i = \frac{1}{\mathcal{B}_{i}}.
\end{equation}
Substituting this into Equation~\ref{eq:granularity}, the total reasoning resistor for the CoT process is expressed as:
\begin{equation}
    R_{\text{CoT}} = \sum^K_{i=1} R_i, \label{eq:resistor}
\end{equation}
where $R_{\text{CoT}}$ represents the combined difficulty, analogous to total resistor in a series circuit.
Overall, the CoT reasoning framework is further formalized by the Ohm's law analogy:
\begin{equation}
    I_{model} = \frac{\mathcal{E}_{\text{model}} + \mathcal{E}_{\text{ICL}}}{R_{\text{CoT}} + R_0},
    \label{eq:ohm2}
\end{equation}
where $R_0$ denotes a output resistor with static resistance value. This modular formulation effectively quantifies cognitive task complexity through combined reasoning resistor.


\subsection{Genral Experimental Settings}
All experiments, unless stated otherwise, are conducted on GPT-3.5-Turbo. Following \citet{wei2022chain}, CoT experiments select 5 manually constructed demonstrations for all multi-step reasoning tasks. \textit{Top-p} is selected from $\{0.95, 1\}$, and \textit{Temperature} is selected in range \([0, 1]\) for robustness evaluation.
Experiments are primarily conducted on the \textsc{BigGSM} dataset~\citep{chen2024unlocking}, a benchmark for mathematical reasoning. 
In ICL, demonstrations are dynamically retrieved using a 5-shot approach from the GSM8K development dataset~\citep{cobbe2021training} by default. To formalize the demonstration selection process, we retrieve the semantic magnetic field strength \(\Phi^B_0\) by default, as defined in Eq.~\ref{eq:phy-b}. All demonstration representation methods employ the meta-recognition~\cite{didolkar2024metacognitive} framework by default: GPT-4o~\cite{achiam2023gpt} extracts sub-capability tags, and Roberta~\cite{liu2019roberta} concatenates and represents these tags in the hidden space. In addition, only the magnetic field strength $\Phi^B_0$ is calculated and the resistor value $R_{\text{CoT}}$ directly adopts the fitting values from \citet{chen2024unlocking}. In addition, other parameters are fitted values using the verification set. Different models fit different voltages $\mathcal{E}_{model}$. The whole experiment shares a $R_0$ value. Different demonstration representation methods have different $\lambda$. All experiments involving variance were sampled 3 times at different temperatures.

\subsection{Model Verification Experiment Details}

\paragraph{CoT Ohm's Law Verification}
To validate the Ohm's Law assumption in the CoT approach, we conduct the following experiment. To eliminate potential interference from ICL and ensure effective stimulation of CoT capabilities, we append the phrase ``Let's think step-by-step!" to the user query. This adjustment aims to stimulate and isolate the CoT mechanism and prevent ICL-related influences.
We then perform sampling based on power levels from the \textsc{BigGSM} dataset, using 30-unit intervals to map calculated power to accuracy\footnote{The absolute power values are meaningless due to inconsistencies in semantic space scaling across encoding methods. However, their relative values within a method correlate positively with real accuracy.}. To reduce random errors and ensure sufficient sampling points, we retain data points with at least 10 samples. Accuracy is calculated for each sampling interval to analyze the relationship between power levels and model performance, enabling an effective evaluation of the correlation between sampling power and accuracy. 

\paragraph{ICL Faraday's Law Verification}
To validate the semantic magnetic field hypothesis in ICL, we observe that without CoT reasoning, performance on complex mathematical tasks often declines to near-zero levels, thereby cannot easily observe their relevance. Thus, this experiment incorporates both CoT reasoning and ICL. While the CoT reasoning format remains constant, we focus on varying the similarity measures used for sample retrieval through ICL.
The central adjustment involves applying different similarity measures for sample retrieval, directly affecting the model's performance on complex tasks. These measures also help calculate the additional ``voltage" introduced by semantic magnetic fields, quantifying their influence on performance. All demonstration representation methods employ BGE~\cite{chen2024bge} to represent the request in the hidden space.

\paragraph{Unified circuit mechanism verification}
To further validate the effectiveness of our unified framework for modeling ICL and CoT, we conduct experiments integrating both paradigms into different retrieval settings. These experiments test the robustness and rationality of the proposed theoretical framework. Specifically, we examine three retrieval methods: random retrieval, forward magnetic field retrieval, and reverse magnetic field retrieval.
In the random retrieval setup, samples are selected randomly from the development set as ICL demonstrations for each test sample. The Retrieval-ICL selects samples with maximized magnetic field intensity in the corresponding test sample's direction. Conversely, the Reverse-ICL focuses on samples with minimal (even negative) magnetic field intensity, enabling exploration of contrasting ICL examples.
This comprehensive validation highlights the robustness of our framework across diverse retrieval strategies and empirically demonstrates its generalizability in both ICL and CoT scenarios.

\paragraph{Extension Experiment Details on Various Models and Tasks}
To further enhance the applicability of \modelname{}, we conducted experiments across a series of models, where each model was designed to fit specific baseline voltage values, $\Phi^B_0$, and compute the corresponding power output. Subsequently, all models were consolidated onto a single graph, enabling us to analyze the correlation between output power and relative accuracy. As illustrated in Figure x, the results reveal clear and detailed correlations across the models. However, when accuracy exceeds 90\%, the scarcity of samples requiring sufficient power in this range, likely due to limitations in the dataset, leads to poor fitting in this region.

Furthermore, our experiments on HotpotQA and Medical Probing closely adhered to the experimental settings established by \citet{chen2024unlocking}. However, since Medical Probing includes rationale annotations limited to background knowledge rather than chain reasoning, we supplemented portions of the reasoning process to facilitate a more precise correlation analysis. Additionally, the data for robotic planning was directly extracted from the running logs provided by \citet{valmeekam2023planbench}.

\subsection{Model Explanation Experiments}
\subsubsection{ICL Explanation}
\paragraph{Embedder Comparison}
To evaluate how different embedders influence model performance, we replace the sample encoder with BERT~\cite{devlin-etal-2019-bert}, RoBERTa~\cite{liu2019roberta}, and BGE~\cite{chen2024bge}. These embeddings are used for sample retrieval in ICL to minimize potential biases that may arise if a single embedder disproportionately emphasizes specific data features. For instance, an embedder might favor samples aligned with its encoding patterns, leading to distortions in the relationship between retrieved representations and model performance. By employing diverse embedders, we aim to mitigate such biases and promote more accurate theoretical modeling for ICL.



\paragraph{Synthetic-CoT Implementation} 
This section examines the implementation of \textit{Synthetic-CoT} within the \modelname{} framework to assess its impact on reasoning performance. Synthetic-CoT autonomously generates in-context demonstrations with chain-of-thought reasoning paths. We hypothesize that these demonstrations align the system’s magnetic field, reducing logical resistor and enhancing overall circuit power output.
To validate this, we generate a series of demonstrations with two categories by \textit{Synthetic-CoT}, including:
\begin{itemize}[leftmargin=4ex]
    \item \textbf{Positive Magnetic Field Demonstrations}: In-context demonstrations generated by \textit{Synthetic-CoT} are positively aligned with the magnetic polarity of user queries direction.
    \item \textbf{Negative Magnetic Field Demonstrations}: In-context demonstrations generated by \textit{Synthetic-CoT} are with opposing magnetic polarity of user queries direction.
\end{itemize}
We analyze the magnetic field contributions of these categories, focusing on their differing effects. Subsequently, we integrate the computed power and measured accuracy to further investigate their operational mechanisms.

\subsubsection{CoT Explanation}
\paragraph{Direct answer output}  
We analyze the effect of \textit{direct answer output} on reasoning performance through the \modelname{} framework. This approach bypasses intermediary reasoning steps by appending the directive, ``Let’s directly output the result without additional reasoning content,'' which generates significant logical resistor within the system. To quantify this, we gradually improve the resistance value of reasoning resistor $R_{\text{CoT}}$, which meets largest Spearman correlations in the validation set, and employ the $R_{\text{CoT}}$ to the test set.

\paragraph{Tool-Usage and PoT}  
We further investigate how \textit{Tool-Usage} and \textit{Program-of-Thought (PoT)} mitigate specific resistors in the reasoning process, focusing on their impact under the \modelname{} framework on computational and planning resistors:
\begin{itemize}[leftmargin=4ex]
    \item \textbf{Tool-Usage}: Tool-Usage aims to reduce \textit{computational resistor} by offloading complex calculations to external tools. Modeled as a zero-resistor computational circuit in our experiments, Tool-Usage enhances the Power-Accuracy Spearman correlation.
    \item \textbf{Program-of-Thought (PoT)}: PoT targets \textit{planning resistor} by employing structured programming logic to enable clearer and more efficient reasoning. This advantage stems from PoT's superior ability to reduce planning resistor, facilitating finer-grained logical alignment.\footnote{All implementation prompts come from \citet{chen2024unlocking}}
\end{itemize} 
These findings confirm that while Tool-Usage primarily addresses single-step computational resistor, PoT excels in optimizing overall planning resistor. Both methods significantly enhance performance within the \modelname{} framework, highlighting their complementary contributions to reducing resistor and improving reasoning capabilities.

\paragraph{Self-consistency and Coverage Design} 
This experiment evaluated the self-consistency strategy proposed by \citet{wang2023selfconsistency}, which employs multiple parallel reasoning paths and a majority-vote mechanism for the final decision. This strategy is hypothesized to enhance performance by reducing the system's resistance to diverse inputs, analogous to parallel resistors in a circuit, where total resistor decreases compared to series connections. By applying this analogy, we demonstrate how parallel reasoning can lower cognitive load, improving accuracy and efficiency in decision-making.

To examine the influence of inference scaling on model performance, we investigated the relationship between the number of parallel samples, \( k \), and the metric like Accuracy or Pass@k, as described in the Inference Scaling Law. Specifically, Pass@k measures the accuracy of Coverage method which obtains a correct result when at least one sampled prediction is accurate. We modeled the reasoning process as a resistive circuit, where each sample represents a parallel resistor. As \( k \) increases, the resistance of reasoning resistor approaches zero, reflecting a refined reasoning process and enhanced accuracy. Further, Coverage methods introduce zero verification resistance introduce much less overall resistance values, larger output power, and higher related performance. The experiment systematically varied \( k \) from 1 to 100 in increments of 5, recording the accuracy and Pass@k metric under each condition\footnote{We remove those generated samples are totally the same to keep the independent sampling.}. We track individual reasoning path resistance, comparing these values with observed accuracy to validate the circuit analogy. The effectiveness of Self-consistency and Coverage methods can be mathematically proved based on the \modelname{} detailed in Theoretical Proof.
\subsection{Model Exploration Experiments}

\subsubsection{ICL Exploration}
\paragraph{Diversity}
This study examines the differing impacts of semantic diversity on static and dynamic ICL retrieval. We map all samples into the hidden space using RoBERTa. In static ICL, we employ a fixed set of demonstrations with the highest semantic diversity across the task, hypothesizing that this diversity enhances model performance by providing a wider range of contextual cues. In contrast, dynamic ICL selects demonstrations in real-time based on the test sample’s characteristics, prioritizing semantically similar examples that align closely with the input.
To test these, we evaluate both retrieval strategies. For static ICL, we compare models exposed to high-diversity and random demonstration sets, measuring performance using accuracy. For dynamic ICL, we contrast random selection with retrieval strategies emphasizing semantic alignment. We hypothesize that semantic diversity improves static ICL by stabilizing decision-making, while in dynamic ICL, retrieving highly relevant demonstrations yields better accuracy. In addition, in order to prove that diversity is ineffective in the setting of dynamic ICL, we sort the semantic magnetic field strength and select as diverse samples as possible among the top-$k$ samples as ICL demonstrations.


\paragraph{Demo-Optimization}
This experiment introduces demo-optimization, a novel method for optimizing demonstrations in ICL. The approach dynamically refines demonstrations based on feedback from the LLM's outputs, which serve as gradients for improvement. Specifically, initial ICL demonstrations in prompting are selected, with both development dataset for optimization and optimization prompt configurations prepared. A feedback loop guides adjustments to the demonstrations, improving their clarity, relevance, and alignment with prompt requirements. Demonstrations are iteratively refined, and evaluated for contextual alignment with user requests in the development dataset. The method is compared against static and whole-prompt-optimized baselines, with performance assessed using task accuracy, output consistency, and learning efficiency.

\subsubsection{CoT Exploration} 
\paragraph{CoT Optimization Proportion}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.54\textwidth]{figure/meta-analysis.pdf}
    \caption{Meta-analysis for the model improvements on different tasks. The data comes from \citet{wu2024comparative,valmeekam2024llms} and \citet{openai2024o1}. We collect evaluation data related to plan and knowledge based on \citet{openai2024o1}. In addition, we remove data points with final results exceeding 90\%, because these data have exceeded human levels, resulting in potential labeling bias and test set labeling errors.}\label{fig:meta-analysis}
\end{figure}

To evaluate the improvement in reasoning resistance \( R_i \) to the target level of \( \frac{1}{k} \), which depends heavily on the task's reasoning demands, we conducted a meta-analysis of experimental results from diverse datasets. These datasets are divided into two main categories: (1) tasks requiring advanced planning and logical reasoning, such as mathematical derivations and multi-hop question answering, and (2) tasks reliant on extensive domain-specific knowledge, such as chemistry and physical question answering.
As shown in Figure~\ref{fig:meta-analysis}, our meta-analysis demonstrates that tasks requiring advanced planning gain the most from enhanced exploratory reasoning, significantly improving challenging components (e.g., a 2.8-fold optimization in planning capability) and overall task performance (e.g., a 3.6-fold optimization on AIME, a benchmark emphasizing long-term planning). In contrast, tasks relying heavily on domain-specific knowledge exhibit limited improvement without targeted optimization in those areas. This underscores the importance of domain-aware reasoning strategies. For example, tasks involving complex reasoning and knowledge integration, such as GPQA (chemistry, physics) and HotpotQA, show performance improvements directly proportional to advancements in the related knowledge domains (approximately 1:1 ratio). The effectiveness and the underlying reasons for such phenomena can be mathematically analyzed and proved based on the \modelname{} detailed in Theoretical Proof.


\paragraph{Fine-grained Self-consistency}

In this experiment, we examine how decomposing a self-consistency ensemble into smaller, incremental majority-voting steps reduces reasoning resistor in circuits, compared to the conventional single-step approach. The circuit operates at a stable supply voltage of 5 units, and reasoning resistance value and output power are evaluated as the number of steps increases. Each majority-voting step simplifies logical complexity and reduces overall resistance by incorporating less verification resistance in the decision-making process. Our findings demonstrate a consistent decline in reasoning resistance alongside a rise in output power, confirming that finer-grained majority-voting processes improve both efficiency and power output in circuits. The effectiveness of this method can be mathematically proved based on the \modelname{} detailed in Theoretical Proof.

\paragraph{Chain-of-Verification}
Traditional self-consistency frameworks rely on a sequential verification model, where each step depends on the completion of the previous one. This structure often incurs high computational costs, as subsequent tasks cannot begin until earlier ones are resolved. To overcome this limitation, we introduce the ``chain-of-verification (CoV)," a novel strategy that parallelizes verification tasks into independent components. Each component operates concurrently, validating distinct parts of the system without interdependence. This parallel approach reduces overall computational resistance by distributing the workload across multiple verifiers, thereby enabling more effective validation. As illustrated in Figure~\ref{fig:cot-optimization}b, parallelization not only accelerates the verification process but also enhances model performance by mitigating bottlenecks. This improvement is particularly pronounced in large-scale systems, where sequential verification would otherwise impede efficiency. The effectiveness of CoV can also be mathematically proved based on the \modelname{} detailed in Theoretical Proof.

\subsection{Best Practice}
In practice, we integrate all proposed strategies into the O1-preview model to address IOI, IMO problem-solving, and idea-generation tasks for much less resistance value of  planning resistor.

\paragraph{IOI Task}
For the IOI task, we employ the following methods: First, using O1-preview, we maximize the optimization of the resistance value of its ``planning resistor." Additionally, five high-intensity semantic field problems from the code\_contest training dataset are retrieved as demonstrations, boosting the voltage supplied by additional ICL fields. Second, we apply the Coverage technique, sampling 100 instances and leveraging both provided and model-generated test cases in parallel, effectively reducing resistance in chain-of-thought reasoning. This process, executed in parallel, effectively reduces the resistance in the chain-of-thought reasoning. Furthermore, we incorporate the CoV concept to iteratively validate the final outputs, thereby optimizing the final performance. Additionally, the CoV framework is employed to iteratively validate outputs and optimize overall performance. To prevent data leakage from competition datasets in C++, competition code is generated in Python. Manual adjustments ensure compatibility between Python and C++ input/output handling, preserving the core logic for fair comparative testing.

\paragraph{IMO Task}
A similar approach is adopted for the IMO task. O1-preview is optimized for planning resistor, and five high-intensity semantic field problems from past IMO datasets are retrieved as demonstrations to enhance ICL field voltage. The Coverage technique is again employed, sampling 100 instances in parallel to reduce reasoning resistance. We introduce the fine-grained self-consistency approach within the CoV framework, enabling iterative validation of outputs and achieving notable performance gains. Additionally, we only evaluate answers and do not consider process scores.

\paragraph{Idea-Generation Task}
We utilize the AI Scientist framework~\citep{lu2024aiscientist} to generate over 10 ideas per domain and draft academic papers, limited to the abstract and introduction sections to address ethical concerns by avoiding experimental claims. A 16-member review panel, including 2 Superior reviewers (6+ years of experience), 8 Senior reviewers (4+ years), and 6 Junior reviewers (2+ years), ensures reliability. Each paper is reviewed by at least four reviewers, including two Senior or Superior reviewers, following NeurIPS guidelines. High-quality examples with strong semantic fields, sourced from past ICLR, NeurIPS, and CoLM openreview data, are used to guide generation. The Coverage technique, sampling of 100 instances, and iterative validation through the CoV framework further refine the process.


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/explanation.pdf}
    \caption{Neuron activation heatmaps for the most activated attention layer under three reasoning strategies: (a) Zero-shot CoT, (b) ICL with Semantic Magnetic Field Retrieval, and (c) Program of Thought (PoT).}\label{fig:explanation}
\end{figure}
\subsection{Internal Mechanism underlying the \modelname{}}
To investigate the internal mechanisms underlying \modelname{}, we conduct experiments  in the LLaMA3-8B~\cite{dubey2024llama} focusing on three strategies: Zero-shot CoT, ICL with Semantic Magnetic Field Retrieval, and PoT. By analyzing neuron activation values and frequencies—particularly in the most activated attention layer—we aim to elucidate the relationship between reasoning strategies and neural dynamics. These findings support the theoretical framework of \modelname{}.
The experiments use the BigGSM dataset, a standard benchmark for mathematical reasoning tasks. For ICL, demonstrations are dynamically retrieved using the BGE model in a 3-shot configuration.
Figure~\ref{fig:explanation} shows neuron activation heatmaps for the most activated attention layer under each reasoning strategy. For Zero-shot CoT, the heatmap reveals sparse activations, with limited regions of intense activity, highlighting the difficulty of reasoning without contextual support. Conversely, ICL and PoT display more distributed and elevated activation patterns. These suggest that the underlying reasoning mechanisms engage a broader spectrum of neurons, with comparable activation dynamics reflecting similar reasoning power across these strategies.