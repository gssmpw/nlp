\documentclass{article}


\usepackage[nonatbib,final]{neurips_2024}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage[numbers]{natbib}
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{xcolor}         
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{adjustbox} 
\usepackage{multirow}
\usepackage{paralist}
\usepackage{CJKutf8}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage[textsize=tiny]{todonotes}
\usepackage{arydshln}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
\definecolor{newgreen}{rgb}{0.7, 0.9, 0.7}
\definecolor{newblue}{rgb}{0.85, 0.85, 0.9}

\newcommand{\modelname}{\texttt{ECM}}
\definecolor{tkcolor}{RGB}{224,223,255}
\definecolor{shallowred}{RGB}{242,204,208}
\newtcolorbox{takeaways}[1][]{
	width=\columnwidth,
	colback = tkcolor, 
	colframe = tkcolor, 
	boxsep=0pt,left=10pt,right=10pt,top=5pt,bottom=5pt,
	fontupper=\linespread{0.9}\selectfont,
	title=#1}

\newtcolorbox{limitation}[1][]{
	width=\columnwidth,
	colback = shallowred, 
	colframe = shallowred, 
	boxsep=0pt,left=10pt,right=10pt,top=5pt,bottom=5pt,
	fontupper=\linespread{0.9}\selectfont,
	title=#1}
\definecolor{shallowgray}{RGB}{237,240,246}
\newtcolorbox{prompt}[1][]{
	width=\columnwidth,
	colback = shallowgray, 
	colframe = shallowgray, 
	boxsep=0pt,left=10pt,right=10pt,top=5pt,bottom=5pt,
	fontupper=\linespread{0.9}\selectfont,
	title=#1}
\usepackage{hyperref}       
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\jq}[1]{\textcolor{orange}{#1}} 

\title{ECM: A Unified Electronic Circuit Model for Explaining the Emergence of In-Context Learning and Chain-of-Thought in Large Language Model}


\author{
	Qiguang Chen$^{\dagger}$ \quad Libo Qin$^{\ddagger}$\thanks{Corresponding Author} \quad Jinhao Liu$^{\dagger}$ \quad Dengyun Peng$^{\dagger}$ \quad Jiaqi Wang$^\diamondsuit$ \\
	\textbf{Mengkang Hu$^\clubsuit$ \quad Zhi Chen$^\spadesuit$ \quad Wanxiang Che$^{\dagger}$\footnotemark[1] \quad Ting Liu$^{\dagger}$} \\
	$^{\dagger}$ Research Center for Social Computing and Information Retrieval\\
	$^\dagger$ Harbin Institute of Technology\\
	$^{\ddagger}$ School of Computer Science and Engineering, Central South University \\
	$^\diamondsuit$ The Chinese University of Hong Kong \\
	$^\clubsuit$ The University of Hong Kong \\
	$^\spadesuit$ ByteDance Seed (China) \\
	\texttt{\{qgchen,car\}@ir.hit.edu.cn},  \texttt{lbqin@csu.edu.cn} \\
}


\begin{document}
	
	
	\maketitle
	
	
	\begin{abstract}
		Recent advancements in large language models (LLMs) have led to significant successes across various applications, where the most noticeable is to a series of emerging capabilities, particularly in the areas of In-Context Learning (ICL) and Chain-of-Thought (CoT). To better understand and control model performance, many studies have begun investigating the underlying causes of these phenomena and their impact on task outcomes. However, existing explanatory frameworks predominantly focus on isolating and explaining ICL and CoT independently, leading to an incomplete understanding of their combined influence on model performance. 
    To address this gap, we propose the Electronic Circuit Model (\modelname{}), which provides a foundation for developing scalable, learnable policies and improving the management of AI-generated content. Specifically, \modelname{} conceptualizes model behavior as an electronic circuit: ICL is represented as semantic magnetic field to providing an additional voltage following Faraday's Law, while CoT is modeled as series resistors to constrain the model output performance following Ohm's Law. Experimental results demonstrate that the \modelname{} effectively predicts and explains LLM performance across a variety of prompting strategies. Furthermore, we apply \modelname{} to advanced reasoning strategy optimization on a series of tasks, such as the International Olympiad in Informatics (IOI) and the International Mathematical Olympiad (IMO), achieving competitive performance that surpasses nearly 80\% of top human competitors.

    \textbf{Key Words:} Electronic Circuit Model, Large Language Model, In-Context Learning, Chain-of-Thought
	\end{abstract}
	
	\input{sections/introduction}
	\input{sections/framework}
	\input{sections/approach}
	
	\input{sections/appendix}

\end{document}