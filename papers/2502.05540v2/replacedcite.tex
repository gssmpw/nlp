\section{Related Work}
% Incremental learning, also known as continual learning, emphasizes progressively acquiring new knowledge without forgetting past information. This method is classified into task-incremental, class-incremental, and domain-incremental learning issues. Among these, class-incremental learning is regarded the most applicable yet the most difficult, and is also the primary focus of this paper.

Incremental learning, or continual learning, progressively learn new knowledge while retaining previous information. It is categorized into task-incremental, class-incremental, and domain-incremental challenges. The most challenging class-incremental learning is the primary focus of this paper.

\subsection{Incremental Learning for Classification}
Most influential incremental learning studies have focused on classification tasks. 
These methods can be categorized into regularization-based, structure-based, and replay-based methods. 
Some regularization-based methods enforce the stability of logits____ or intermediate features____ to preserve the learned knowledge, while others apply restrictions on the weight of the model____ or on gradients during optimization____. 
Structure-based methods are dedicated to learning specific parameters for different tasks, with a dynamically expanding architecture____ or grouped parameters in a static model____. 
For replay-based methods, they can be divided into experience replay____ and generative replay methods____, depending on the examples stored in a buffer or generated with a model. 
Recently, incremental learning based on foundation models such as CLIP____ has also attracted attention. 
Research works such as L2P____, O-LoRA____, and VPT-NSP\textsuperscript{2}____ attempt to learn continuously based on the parameter-efficient transfer learning technique____ have achieved superior performance.

\begin{figure*}[t]
    \centering
    \begin{minipage}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/RPNRecall/rpn_iou_recall_1.pdf}
        \vspace{-0.8cm} % 减小间距
        \caption*{(a) RPN's recall on $\mathcal{D}_1^{test}$.}
        \vspace{-0.8cm} % 减小间距
        \label{fig:sub1}
    \end{minipage}
    \hspace{-0.7cm}
    \begin{minipage}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/RPNRecall/rpn_iou_recall_2.pdf}
        \vspace{-0.8cm} % 减小间距
        \caption*{(b) RPN's recall on $\mathcal{D}_2^{test}$.}
        \vspace{-0.8cm} % 减小间距
        \label{fig:sub2}
    \end{minipage}
    \hspace{-0.7cm}
    \begin{minipage}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/RPNRecall/rpn_iou_recall_3.pdf}
        \vspace{-0.8cm} % 减小间距
        \caption*{(c) RPN's recall on $\mathcal{D}_3^{test}$.}
        \vspace{-0.8cm} % 减小间距
        \label{fig:sub3}
    \end{minipage}
    \hspace{-0.7cm}
    \begin{minipage}[t]{0.27\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/RPNRecall/rpn_iou_recall_4.pdf}
        \vspace{-0.8cm} % 减小间距
        \caption*{(d) RPN's recall on $\mathcal{D}_4^{test}$.}
        \vspace{-0.8cm} % 减小间距
        \label{fig:sub4}
    \end{minipage}
    \caption{Recall-Objectness curve of RPN's prediction. IoU threshold is set to 0.5. 
    {\color{blue}Blue}: ${\cal M}_j$ has been trained with training images of $\mathcal{D}_i$ in earlier stages. 
    {\color{green}Green}: ${\cal M}_j$ is just fine-tuned on $\mathcal{D}_i$.
    {\color{red}Red}: ${\cal M}_j$ has not seen the training set of $\mathcal{D}_i$ before. 
    {\color{gray}Gray}: ${\cal M}_{joint}$ is trained jointly on all training images of $\cal D$. 
    %For example, in (a), {\color{green}${\cal M}_1$} is freshly fine-tuned on $\mathcal{D}_1$, while {\color{blue}${\cal M}_2$} to {\color{blue}${\cal M}_4$} have seen $\mathcal{D}_1$ in previous training stage. In (d), {\color{red}${\cal M}_1$} to {\color{red}${\cal M}_3$} have not seen $\mathcal{D}_4$ before.
    }
    \label{fig:rpnrecall}
\end{figure*}

\subsection{Incremental Learning for Object Detection}
Incremental object detection presents unique challenges compared with the classification task. 
IOD are required to locate and classify the visual objects in images. 
It also faces a distinctive missing annotation problem where potential instances not belonging to the classes of the current learning stage are labeled as background. 
Most existing IOD works can be summarized into two categories. 
One is knowledge distillation based methods____. BPF____ bridges past and future with pseudo-labeling and potential object estimation to align models across stages, ensuring a consistent optimization direction. 
MMA____ consolidates the background and all old classes into one entity to minimize the conflict between optimization objects between previous and current tasks. 
The other is to preserve knowledge through a replay of previous data stored in images____, instances____, or features____. 
ABR____ replayed foreground objects from previous tasks stored in a buffer to reinforce the learned knowledge.  RODEO____ stored compressed representations in a fixed-capacity memory buffer to incrementally perform object detection in a streaming fashion.
Unlike existing methods, we delve into analyzing where the forgetting originated for the two-stage incremental object detector.
Then we tailor a method specifically designed to combat the crux forgetting module, \textit{i.e.} RoI Head classifier, by replaying RoI features from previously seen tasks to preserve the classification performance.