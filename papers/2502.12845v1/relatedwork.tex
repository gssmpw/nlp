\section{Related Work}
\subsection{Molecular Design with Machine Learning}
Numerous advanced models for molecular design span GA, BO, MOO, MCMC, RL, and DL methodologies. \textbf{Deep Learning (DL)} leverages neural networks in various molecular design models. Differentiable Scaffolding Tree (DST)~\cite{fu2021dst} with GNNs, Junction Tree Variational Autoencoders (JTVAE)~\cite{jin2018jtvae}, and VJTNN+GAN~\cite{VJTNNGan} combine generative and adversarial architectures to generate molecules. MOOD~\cite{mood} utilizes Diffusion models to address out-of-distribution generation. Recent developments in Generative Pre-trained Transformers (GPT) led Bagal et al. to train MolGPT~\cite{2021molgpt} on next-token prediction, while Fang et al. pre-trained MOLGEN~\cite{2024molgen} on molecule reconstruction tasks, achieving cross-domain applicability. Although DL methods offer powerful capabilities in capturing complex molecular structures and enabling cross-domain applicability such as DST, JTVAE, and MolGPT, they often underperform in MOO scenario. Latent Space Optimization (LSO)~\cite{abeer2024lso} has further advanced multi-objective molecular design, but only for deep generative models. 

\textbf{Reinforcement Learning (RL)} combined with DL iteratively refines molecules by learning from feedback, often based on property scores. REINVENT~\cite{olivecrona2017reinvent} applies RL to train an RNN to generate molecules meeting multiple goals, while RationaleRL~\cite{rationaleRL} uses a Graph Neural Network (GNN) to generate molecules by building interpretable substructures, or ``rationales''. Based on REINVENT, Shin et al. proposed a novel divide-and-conquer approach called DyMol~\cite{dymol} to train the model for multiple objectives and achieve SOTA results. Kim et al. also achieve SOTA performance by integrating genetic algorithms into GFlowNets~\cite{genetic-gfn}.

In addition to DP methods, classical probabilistic models and optimization methods also achieve SOTA performance in many cases, such as in PMO~\cite{pmo}. A notable example of \textbf{Genetic Algorithms (GA)} is GB-GA~\cite{gbga}, commonly used as a baseline, where molecular structures are modified in graph form during mating operation. AkshatKumar et al.~\cite{gadt} introduced a neural network discriminator to enhance diversity, surpassing GB-GA in maximizing penalized-logP~\cite{gomez2018plogp}. Later, Tripp et al.~\cite{tripp2021gb-bo} employed a Tanimoto kernel in a Gaussian Process in GB-GA, outperforming GB-GA. It uses SELFIES~\cite{krenn2020selfies}, a 100\% valid molecular representation system; however, Gao et al.~\cite{pmo} later showed there are no obvious shortcomings of SMILES compared to SELFIES. MLPS~\cite{mlps} combines \textbf{MOO} with BO and an encoder-decoder network to efficiently locate global Pareto-optimal solutions, while Verhellen et al. introduced a graph-based MOO~\cite{verhellen2022graph} for molecular optimization. Furthermore, MARS~\cite{mars} uses \textbf{Markov Chain Monte Carlo (MCMC)} to explore chemical spaces probabilistically to identify molecules with desirable properties. Similarly, MolSearch~\cite{sun2022molsearch} utilizes Monte Carlo tree search for multi-objective molecular discovery. However, GA, BO, MOO, and MCMC methods are independent of domain knowledge, which is highly beneficial in molecular design but challenging to incorporate into such algorithms.


\subsection{Multi-Objective Optimization and Genetic Algorithm with LLM}
Recently, Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) benchmarks~\cite{brown2020gpt,ai4science2023impact}, sparking interest in their application as optimization operators in MOO to address the challenges of high-dimensional search spaces and to incorporate domain knowledge~\cite{2024EALLMsurvey}. For instance, OPRO~\cite{opro} and LMEA~\cite{lmea} employ LLMs as crossover and mutation operators within GA, using prompts that include parent values from the current population, with LMEA further exploring the balance of exploitation and exploration by adjusting the temperature parameter. Furthermore, Wang et al.~\cite{2024ConstrainedMOLLM} investigated constrained MOO with prompt engineering, demonstrating promising alignment results. Other studies have highlighted the effectiveness and efficiency of LLMs in GA compared to standalone LLMs and traditional MOO algorithms, proposing well-structured pipelines~\cite{liu2023algorithm,liu2024evolution,liu2024large,huang2024exploring,brahmachary2024llmleo}. However, research on LLMs with MOO is still nascent, with results largely limited to preliminary findings in numerical optimizations and planning problems.

\subsection{Molecular Design with LLM}
LLMs with pre-trained domain knowledge are increasingly popular for accelerating drug discovery and materials design~\cite{ai4science2023impact}. In particular, ChemCrow~\cite{2024chemcrow} uses LLMs as agents capable of reasoning, planning, and selecting appropriate external tools to iteratively refine candidates in chemical tasks. LICO~\cite{2024lico} improves molecule generation through in-context learning by pretraining the model with separate embedding and prediction layers, while Moayedpour et al.~\cite{2024many} extend this approach to multi-objective setups, and MolReGPT~\cite{li2024molReGPT} advances few-shot learning for molecular optimization. MOLLEO~\cite{molleo} applies GA combined with LLMs for molecular design, aligning with our framework, but differing significantly in details. MOLLEO's results as well as research in prompts remain preliminary, lacking extensive multi-objective experiments and failing to consider the impact of varying initial populations.