\section{Related Work}
\subsection{Molecular Design with Machine Learning}
Numerous advanced models for molecular design span GA, BO, MOO, MCMC, RL, and DL methodologies. \textbf{Deep Learning (DL)} leverages neural networks in various molecular design models. Differentiable Scaffolding Tree (DST)____ with GNNs, Junction Tree Variational Autoencoders (JTVAE)____, and VJTNN+GAN____ combine generative and adversarial architectures to generate molecules. MOOD____ utilizes Diffusion models to address out-of-distribution generation. Recent developments in Generative Pre-trained Transformers (GPT) led Bagal et al. to train MolGPT____ on next-token prediction, while Fang et al. pre-trained MOLGEN____ on molecule reconstruction tasks, achieving cross-domain applicability. Although DL methods offer powerful capabilities in capturing complex molecular structures and enabling cross-domain applicability such as DST, JTVAE, and MolGPT, they often underperform in MOO scenario. Latent Space Optimization (LSO)____ has further advanced multi-objective molecular design, but only for deep generative models. 

\textbf{Reinforcement Learning (RL)} combined with DL iteratively refines molecules by learning from feedback, often based on property scores. REINVENT____ applies RL to train an RNN to generate molecules meeting multiple goals, while RationaleRL____ uses a Graph Neural Network (GNN) to generate molecules by building interpretable substructures, or ``rationales''. Based on REINVENT, Shin et al. proposed a novel divide-and-conquer approach called DyMol____ to train the model for multiple objectives and achieve SOTA results. Kim et al. also achieve SOTA performance by integrating genetic algorithms into GFlowNets____.

In addition to DP methods, classical probabilistic models and optimization methods also achieve SOTA performance in many cases, such as in PMO____. A notable example of \textbf{Genetic Algorithms (GA)} is GB-GA____, commonly used as a baseline, where molecular structures are modified in graph form during mating operation. AkshatKumar et al.____ introduced a neural network discriminator to enhance diversity, surpassing GB-GA in maximizing penalized-logP____. Later, Tripp et al.____ employed a Tanimoto kernel in a Gaussian Process in GB-GA, outperforming GB-GA. It uses SELFIES____, a 100\% valid molecular representation system; however, Gao et al.____ later showed there are no obvious shortcomings of SMILES compared to SELFIES. MLPS____ combines \textbf{MOO} with BO and an encoder-decoder network to efficiently locate global Pareto-optimal solutions, while Verhellen et al. introduced a graph-based MOO____ for molecular optimization. Furthermore, MARS____ uses \textbf{Markov Chain Monte Carlo (MCMC)} to explore chemical spaces probabilistically to identify molecules with desirable properties. Similarly, MolSearch____ utilizes Monte Carlo tree search for multi-objective molecular discovery. However, GA, BO, MOO, and MCMC methods are independent of domain knowledge, which is highly beneficial in molecular design but challenging to incorporate into such algorithms.


\subsection{Multi-Objective Optimization and Genetic Algorithm with LLM}
Recently, Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) benchmarks____, sparking interest in their application as optimization operators in MOO to address the challenges of high-dimensional search spaces and to incorporate domain knowledge____. For instance, OPRO____ and LMEA____ employ LLMs as crossover and mutation operators within GA, using prompts that include parent values from the current population, with LMEA further exploring the balance of exploitation and exploration by adjusting the temperature parameter. Furthermore, Wang et al.____ investigated constrained MOO with prompt engineering, demonstrating promising alignment results. Other studies have highlighted the effectiveness and efficiency of LLMs in GA compared to standalone LLMs and traditional MOO algorithms, proposing well-structured pipelines____. However, research on LLMs with MOO is still nascent, with results largely limited to preliminary findings in numerical optimizations and planning problems.

\subsection{Molecular Design with LLM}
LLMs with pre-trained domain knowledge are increasingly popular for accelerating drug discovery and materials design____. In particular, ChemCrow____ uses LLMs as agents capable of reasoning, planning, and selecting appropriate external tools to iteratively refine candidates in chemical tasks. LICO____ improves molecule generation through in-context learning by pretraining the model with separate embedding and prediction layers, while Moayedpour et al.____ extend this approach to multi-objective setups, and MolReGPT____ advances few-shot learning for molecular optimization. MOLLEO____ applies GA combined with LLMs for molecular design, aligning with our framework, but differing significantly in details. MOLLEO's results as well as research in prompts remain preliminary, lacking extensive multi-objective experiments and failing to consider the impact of varying initial populations.