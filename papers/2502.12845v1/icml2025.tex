%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{booktabs} % for professional tables
\usepackage{array}
\usepackage[table]{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts}

\begin{document}

\twocolumn[
\icmltitle{MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nian Ran}{yyy}
\icmlauthor{Yue Wang}{comp}
\icmlauthor{Richard Allmendinger}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of Manchester, Manchester, United Kingdom}
\icmlaffiliation{comp}{Independent Researcher, Beijing, China}

\icmlcorrespondingauthor{Nian Ran}{r992988188@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  %如果被接收要去icml2025.sty uncomment line 549 leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Molecular design plays a critical role in advancing fields such as drug discovery, materials science, and chemical engineering. This work introduces the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel framework that combines domain-specific knowledge with the adaptability of Large Language Models to optimize molecular properties across multiple objectives. Leveraging in-context learning and multi-objective optimization, MOLLM achieves superior efficiency, innovation, and performance, significantly surpassing state-of-the-art (SOTA) methods. Recognizing the substantial impact of initial populations on evolutionary algorithms, we categorize them into three types: best initial, worst initial, and random initial, to ensure the initial molecules are the same for each method across experiments. Our results demonstrate that MOLLM consistently outperforms SOTA models in all of our experiments. We also provide extensive ablation studies to evaluate the superiority of our components.
\end{abstract}

\section{Introduction}
Molecular design is fundamental in fields such as drug discovery, materials science, and chemical engineering. In these areas, the ability to design novel molecules with targeted properties, including stability, reactivity, or bioactivity, can drive significant advancements, from the development of new pharmaceuticals to the creation of sustainable, innovative materials. Traditionally, molecular design has relied on trial-and-error experimentation and repeated synthesis, which is resource intensive, time-consuming, and ultimately inefficient. During the past few decades, with rapid advances in computational power, various machine learning techniques~\cite{2019deepmolgensurvey,du2022molgensurvey} have been introduced not only to accelerate this process but also to enable the discovery of novel, more potent molecules. Methods include Bayesian Optimization (BO)~\cite{tripp2021gb-bo}, Multi-Objective Optimization (MOO)~\cite{mlps,choi2023rebadd,verhellen2022graph}, Markov Chain Monte Carlo (MCMC)~\cite{mars,sun2022molsearch}, Genetic Algorithms (GA)~\cite{gbga,gadt,lmea,brahmachary2024llmleo,molleo,assmoea}, Reinforcement Learning (RL)~\cite{olivecrona2017reinvent,rationaleRL,fu2022rga}, and Deep Learning (DL) models~\cite{VJTNNGan,2021molgpt,mood,2024molgen}.

Although these methods have yielded excellent results, most of them lack the integration of expert knowledge during runtime, despite the crucial role of professional feedback and search direction in molecular design. Large Language Models (LLMs), typically based on transformer architectures~\cite{vaswani2017transformer}, are pre-trained on extensive high-quality data, including books and academic papers, enabling them to capture domain-specific expertise. They have demonstrated significant potential in scientific discovery, particularly in molecular understanding and the generation of novel molecular candidates, as exemplified by models like GPT-4~\cite{ai4science2023impact}. Recent studies highlight the advantages of in-context learning~\cite{2024lico} and iterative evolutionary approaches~\cite{molleo} in enhancing LLM effectiveness. However, research in this area remains nascent, with only preliminary findings and a lack of systematic investigation.

Furthermore, despite significant progress in training large neural networks to understand chemistry and molecular structures with domain knowledge, these models often require additional parameters and retraining, particularly for MOO, as seen in MolGPT~\cite{2021molgpt} and LICO~\cite{2024lico}. In contrast, MOLLEO~\cite{molleo} leverages domain knowledge from pre-trained large language models without additional training but still relies on GB-GA within its framework.

In practice, most molecular design tasks optimizes multiple objectives, yet existing methods often ignore this aspect. For example, GB-BO~\cite{tripp2021gb-bo}, JTVAE~\cite{jin2018jtvae}, and MolGen~\cite{2024molgen} provide limited multi-objective capabilities. MolGPT requires specific training for different objectives, restricting its flexibility, while MolGen focuses primarily on target molecular discovery and employs only single-objective optimization.

Finally, the formulation of MOO using GA has often lacked rigor in previous studies. First, oracle calls should be restricted to ensure fair comparisons and practical applicability, since the evaluation of certain molecular properties requires costly experiments or specifically trained models, as noted in the Practical Molecular Optimization benchmark~\cite{pmo}. Additionally, the initial population significantly impacts final performance under a fixed number of oracle calls, yet this factor has been largely overlooked in methods that incorporate genetic algorithms, such as MARS, MOLLEO, and GB-GA.

To address these gaps and enhance multi-objective molecular design, we propose Multi-Objective Large Language Model (MOLLM), a LLM-based framework that integrates MOO, in-context learning and prompt engineering. Our model is mainly consisted of a mating module to generate parent molecules for in-context learning, a prompt template to integrates all information and instructions to maximally leverage the knowledge in LLM, a experience pool, and a selection module that contains both Pareto front selection and fitness value selection. The results show that our model demonstrates SOTA performance on different objectives, especially in multi-objective cases and when the number of objectives become larger. Our key contributions are:
\begin{itemize} 
\item We carefully design the in-context learning and prompt engineering mechanism in our model to fully leverage the domain knowledge pre-trained in LLMs. This is seamlessly integrated into MOO framework, achieving SOTA performance in both optimization quality and efficiency. Our framework requires no additional training for specific objectives while capitalizing on domain expertise, reasoning capabilities, and is adaptable to various LLMs. Unlike MOLLEO, we employ LLMs for all mating operations, ensuring that the framework is entirely LLM-driven.
\item Recognizing the critical influence of initial population selection in genetic algorithm-based methods, we evaluate models using three types of initial populations: the worst, random, and best molecules from the ZINC250K dataset. Our results show that MOLLM outperforms all kinds of SOTA models built on GA, BO, MCMC, LLM, RL and DL in our experiments, particularly in maximizing the sum of absolute property values in multi-objective settings. In addition, extensive ablation studies validate the effectiveness of our approach and design choices.
\end{itemize}

\section{Related Work}
\subsection{Molecular Design with Machine Learning}
Numerous advanced models for molecular design span GA, BO, MOO, MCMC, RL, and DL methodologies. \textbf{Deep Learning (DL)} leverages neural networks in various molecular design models. Differentiable Scaffolding Tree (DST)~\cite{fu2021dst} with GNNs, Junction Tree Variational Autoencoders (JTVAE)~\cite{jin2018jtvae}, and VJTNN+GAN~\cite{VJTNNGan} combine generative and adversarial architectures to generate molecules. MOOD~\cite{mood} utilizes Diffusion models to address out-of-distribution generation. Recent developments in Generative Pre-trained Transformers (GPT) led Bagal et al. to train MolGPT~\cite{2021molgpt} on next-token prediction, while Fang et al. pre-trained MOLGEN~\cite{2024molgen} on molecule reconstruction tasks, achieving cross-domain applicability. Although DL methods offer powerful capabilities in capturing complex molecular structures and enabling cross-domain applicability such as DST, JTVAE, and MolGPT, they often underperform in MOO scenario. Latent Space Optimization (LSO)~\cite{abeer2024lso} has further advanced multi-objective molecular design, but only for deep generative models. 

\textbf{Reinforcement Learning (RL)} combined with DL iteratively refines molecules by learning from feedback, often based on property scores. REINVENT~\cite{olivecrona2017reinvent} applies RL to train an RNN to generate molecules meeting multiple goals, while RationaleRL~\cite{rationaleRL} uses a Graph Neural Network (GNN) to generate molecules by building interpretable substructures, or ``rationales''. Based on REINVENT, Shin et al. proposed a novel divide-and-conquer approach called DyMol~\cite{dymol} to train the model for multiple objectives and achieve SOTA results. Kim et al. also achieve SOTA performance by integrating genetic algorithms into GFlowNets~\cite{genetic-gfn}.

In addition to DP methods, classical probabilistic models and optimization methods also achieve SOTA performance in many cases, such as in PMO~\cite{pmo}. A notable example of \textbf{Genetic Algorithms (GA)} is GB-GA~\cite{gbga}, commonly used as a baseline, where molecular structures are modified in graph form during mating operation. AkshatKumar et al.~\cite{gadt} introduced a neural network discriminator to enhance diversity, surpassing GB-GA in maximizing penalized-logP~\cite{gomez2018plogp}. Later, Tripp et al.~\cite{tripp2021gb-bo} employed a Tanimoto kernel in a Gaussian Process in GB-GA, outperforming GB-GA. It uses SELFIES~\cite{krenn2020selfies}, a 100\% valid molecular representation system; however, Gao et al.~\cite{pmo} later showed there are no obvious shortcomings of SMILES compared to SELFIES. MLPS~\cite{mlps} combines \textbf{MOO} with BO and an encoder-decoder network to efficiently locate global Pareto-optimal solutions, while Verhellen et al. introduced a graph-based MOO~\cite{verhellen2022graph} for molecular optimization. Furthermore, MARS~\cite{mars} uses \textbf{Markov Chain Monte Carlo (MCMC)} to explore chemical spaces probabilistically to identify molecules with desirable properties. Similarly, MolSearch~\cite{sun2022molsearch} utilizes Monte Carlo tree search for multi-objective molecular discovery. However, GA, BO, MOO, and MCMC methods are independent of domain knowledge, which is highly beneficial in molecular design but challenging to incorporate into such algorithms.


\subsection{Multi-Objective Optimization and Genetic Algorithm with LLM}
Recently, Large Language Models (LLMs) have demonstrated remarkable performance across various Natural Language Processing (NLP) benchmarks~\cite{brown2020gpt,ai4science2023impact}, sparking interest in their application as optimization operators in MOO to address the challenges of high-dimensional search spaces and to incorporate domain knowledge~\cite{2024EALLMsurvey}. For instance, OPRO~\cite{opro} and LMEA~\cite{lmea} employ LLMs as crossover and mutation operators within GA, using prompts that include parent values from the current population, with LMEA further exploring the balance of exploitation and exploration by adjusting the temperature parameter. Furthermore, Wang et al.~\cite{2024ConstrainedMOLLM} investigated constrained MOO with prompt engineering, demonstrating promising alignment results. Other studies have highlighted the effectiveness and efficiency of LLMs in GA compared to standalone LLMs and traditional MOO algorithms, proposing well-structured pipelines~\cite{liu2023algorithm,liu2024evolution,liu2024large,huang2024exploring,brahmachary2024llmleo}. However, research on LLMs with MOO is still nascent, with results largely limited to preliminary findings in numerical optimizations and planning problems.

\subsection{Molecular Design with LLM}
LLMs with pre-trained domain knowledge are increasingly popular for accelerating drug discovery and materials design~\cite{ai4science2023impact}. In particular, ChemCrow~\cite{2024chemcrow} uses LLMs as agents capable of reasoning, planning, and selecting appropriate external tools to iteratively refine candidates in chemical tasks. LICO~\cite{2024lico} improves molecule generation through in-context learning by pretraining the model with separate embedding and prediction layers, while Moayedpour et al.~\cite{2024many} extend this approach to multi-objective setups, and MolReGPT~\cite{li2024molReGPT} advances few-shot learning for molecular optimization. MOLLEO~\cite{molleo} applies GA combined with LLMs for molecular design, aligning with our framework, but differing significantly in details. MOLLEO's results as well as research in prompts remain preliminary, lacking extensive multi-objective experiments and failing to consider the impact of varying initial populations.

\section{Methodology}
The core ideas of MOLLM is that molecular design should leverage prior domain knowledge embedded in SOTA LLMs rather than training models from scratch, disregarding expert feedback during optimization, or relying on external algorithms such as GB-GA as operators. Therefore, we propose utilizing LLMs exclusively for both crossover and mutation operations in our model. The reason of two operations is to balance exploitation and exploration. While LLMs may not always generate molecules that perfectly consider the trade-offs of objectives, we incorporate Pareto front selection within (MOO) to ensure that molecules selected for the next generation better account for all objectives while maintaining structural diversity. Empirical experiments demonstrate that well-formulated prompts and in-context learning significantly enhance the utilization of LLM knowledge and the information encoded in parent molecules. Thus, we carefully design a prompt template comprising five key components, making it adaptable to any LLM.

\subsection{MOLLM Overview}
\begin{figure*}[h]
\centering
\includegraphics[width=1\textwidth]{images/mollm_overview.png}
\caption{The overall pipeline of initial MOLLM.}
\label{fig:mollm}
\end{figure*}
Figure~\ref{fig:mollm} presents the complete MOLLM optimization pipeline. The task involves unconstrained molecular optimization, where, given a set of objectives, the model is initialized with molecules selected from the ZINC250K dataset. It then iterates through mating, prompt generation, scoring, experience updating, and next-generation selection.
\textbf{Initialization:} For an optimization problem with one or multiple objectives, we initialize with $N_i$ molecules, either randomly selected or chosen based on the best or worst objective values from the ZINC250K dataset. In our setting, $N_i=100$. The ZINC dataset~\cite{irwin2012zinc} is selected as it is widely used for population initialization in molecular optimization studies~\cite{molleo, gbga, fu2022rga}. ZINC250K comprises approximately 250,000 curated drug-like molecules from the ZINC database, providing key properties such as chemical structures, logP, QED, and SA, making it well-suited for drug discovery and molecular optimization tasks.

\textbf{Mating}: This step involves prompting the LLM to generate new candidate molecules that are expected to improve on the parent molecules given. First, the parent molecules are randomly selected from the current population with probabilities $P_c$ and $P_m$ for crossover and mutation, respectively. Each crossover involves two parents, while mutation involves a single parent. These selected parents are then formatted into a flexible prompt template. Our \textbf{prompt template} consists of five key components: multi-objective requirements, objective descriptions, parent objective values (for in-context learning), output instructions, and past experience, if applicable. The whole framework of our model and an example is shown in Figure~\ref{fig:mollm}. In this setup, the model receives structured input specifying primary objectives, descriptions of molecular modifications (in SMILES format) that may increase or decrease property values, and parent molecules represented by their SMILES structures, objective values, and an aggregated objective score as an overall performance indicator. The output instructions specify that only molecular structures should be generated, omitting explanations to significantly reduce runtime and query costs without affecting performance. We employ crossover and mutation to balance exploitation and exploration. Although LLMs perform well in crossover due to their straightforward nature, they struggle with mutation, as its prompt is highly similar to crossover. To address this, we provide a list of common molecular mutation operations in the instructions to improve exploration. Finally, after generating offspring, we identify the best and worst molecules among them and query the LLM to update its experience based on these molecules and experience in the last iteration. This iterative refinement allows the experience to evolve dynamically, transitioning from general suggestions to more detailed and actionable guidance over time.


\textbf{Multi-objective optimization:} At this stage, we typically have $N$ parent molecules from the previous generation and $N$ offspring from the current generation, assuming all molecules are valid, where 
$N$ denotes the population size. These molecules are then combined and subjected to either Pareto front selection or F-value selection, where F represents the sum of normalized objective values, to determine the top $N$ candidates for the next generation. The selection operation is executed with equal probability (50\% each) for both methods. This hybrid selection strategy balances exploration and exploitation: F-value selection allows the model to focus on the current optimal solutions, while Pareto front selection promotes diversity in the next generation, reducing the risk of premature convergence to a local optimum.

\begin{algorithm}[tb]
\caption{MOLLM framework}
\label{alg:mollm}
\begin{algorithmic}
    \STATE {\bfseries Input:} initial population $\mathbb{M}_0$, population size $N$, fitness function $F$, probability of adding experience $P_{exp}$, probability of crossover $P_{c}$ and probability of mutation $P_{m}$. 
    \STATE {\bfseries Initialize:} $t \gets 0$.
    \FOR{$m\in \mathbb{M}_0$}
    \STATE Compute $F(m)$
    \ENDFOR
    \WHILE{$t<=$ oracle\_budget}
        \STATE parent\_pairs $\leftarrow$  Random\_Sample($\mathbb{M}_0$,$P_{c}$,$P_{m}$)
        \STATE prompts $\leftarrow$ Prompt\_Module(parent\_pairs)
        \IF{a random probability $p$ is less than $P_{exp}$}
            \STATE prompts $\leftarrow$ prompt + experience
        \ENDIF
        \STATE offspring $\leftarrow$ \textbf{Parallel\_Query}(prompts)
        \FOR{$m\in $ offspring}
        \STATE Compute $F(m)$
        \ENDFOR
        \IF{a random probability $p$ is less than $P_{exp}$}
            \STATE Update\_Experience\_Pool()
        \ENDIF
        \STATE $\mathbb{M}_{t}\leftarrow\mathbb{M}_{t-1} \cup $ offspring
        \IF{single\_objective or a random probability $p$ is less than 0.5}
            \STATE $\mathbb{M}_{t} \leftarrow $ F\_Value\_Selection($\mathbb{M}_{t}$,$N$)
        \ELSE
            \STATE $\mathbb{M}_{t} \leftarrow $ Pareto\_Frontier\_Selection($\mathbb{M}_{t}$,$N$)
        \ENDIF
    \ENDWHILE
    \STATE {\bfseries return} $\mathbb{M}_{t}$
\end{algorithmic}
\end{algorithm}

\section{Experiment}

\subsection{Task}
The initial population plays a critical role in determining the final outcomes of genetic-based algorithms under a fixed computational budget. However, most prior studies have overlooked its significance. In practical applications, researchers often initialize the search with the best available molecules rather than selecting them entirely at random. To ensure a comprehensive and fair evaluation of our model, we conduct experiments on three distinct initialization scenarios: the top 100, bottom 100, and randomly sampled molecules from the ZINC 250K dataset, using their F-values as indicators. The best-initialization scenario assesses the model’s upper performance limit, the random initialization reflects common real-world usage, and the worst-initialization scenario presents a more challenging optimization task. We adhere to the PMO benchmark and operate within a budget of 5,000 oracle calls.
For molecular property optimization, we focus on the following objectives: QED (drug-likeness), SA (synthetic accessibility), LogP (octanol-water partition coefficient), DRD2 (dopamine receptor D2 affinity), LogS (log of solubility), reduction potential, JNK3 (c-Jun N-terminal Kinase 3), and GSK3$\beta$ (Glycogen Synthase Kinase 3 Beta). In addition to these well-defined objectives, we also include BBBP (Blood-Brain Barrier Permeability), a more complex and less predictable property influenced by multiple biological factors.

\subsection{Metrics}
To fully evaluate the performance in many aspects, we use several metrics. The most important goal is maximizing the sum of normalized property values, denoted as F value, representing the absolute improvement that accountsnts for all the objectives. On top of that, we use uniqueness, validity, diversity and efficiency to full evaluate the ability of model to propose molecules. However, these additional metrics need to be considered in conjunction with the F-value, as it is less meaningful of other metrics if they have relatively low F values. 
\begin{itemize}
    \item \textbf{Top 1 F \& Mean Top 10 F}: F (fitness) is the sum of the normalized objective values, which gives the direct representation of the strength of a molecule~\cite{molleo}. The weight in our experiment to each objective is the same.
    \begin{equation}
        \max_{m \in M}F(m) = \sum_{i=1}^k w_if_i(m)
    \end{equation}
    where $m$ is a molecule in SMILES form, k is the number of objectives, $w_i$ and $f_i$ is the weight and normalized objective value. If an objective is to be minimized, it will be transformed by $1-f_i(m)$. We give an equal weight to each objective.
    \item \textbf{Uniqueness}: the fraction of valid generated molecules that are unique. A low uniqueness highlights repetitive molecule generation and a low level of distribution learning by the model~\cite{2021molgpt}, while a high uniqueness value means that the model effectively explores novel molecules, the equation is blow: 
    \begin{equation}
    U = 1 - \frac{\mathbb{M}_{\text{rep}}}{\mathbb{M}_{\text{all}}}
    \label{eq:uniqueness}
    \end{equation}
    where \( \mathbb{M}_{\text{rep}} \) is the number of repeated molecules, and \( \mathbb{M}_{\text{all}} \) is the total number of molecules proposed in history.
    \item \textbf{Validity}: the fraction of molecules generated that are valid, it measures how well the model has learned the SMILES grammar and the valency of atoms~\cite{2021molgpt}. The equation of validity is below:
    \begin{equation}
    V = \frac{\mathbb{M}_{\text{val}}}{\mathbb{M}_{\text{all}}}
    \label{eq:uniqueness}
    \end{equation}
    where \( \mathbb{M}_{\text{rep}} \) is the number of valid molecules.
    \item \textbf{Structural Diversity}: Structural diversity reflects the chemical diversity of the Pareto set and is computed by taking the average pairwise Tanimoto distance between Morgan fingerprints of molecules in the set~\cite{benhenda2017diversity}. The equation of computing a set of molecules is:
    \begin{equation}
        D(A) = \frac{1}{|A|^2} \sum_{(x,y)\in A\times A} T_d(x,y)
    \end{equation}
    where $A$ is the set of molecules and $T_d$ is the tonimoto distance. 
    \item \textbf{Efficiency}: Efficiency is compared by the running time in hours, as well as LLM calls if application. It is a important metric when using LLM for inference, because querying LLM incurs high computational costs. 
\end{itemize}

\subsection{Baselines}
To demonstrate the superiority and for fair comparison extensively, we choose SOTA models from a series of algorithms including GA, BO, MCMC, RL, DL and LLM-based method as our baselines. These algorithms are GB-GA, GB-BO, JT-VAE, MARS, REINVENT, MOLLEO, and recently proposed DyMol and Genetic-GFN which have achieved SOTA performance. More details and hyperparemeters of each baseline are provided in Appendix~\ref{apd:baseline}. For a fair comparison, we use Chatgpt 4o for both MOLLM and MOLLEO. We use the default hyperparameters for GB-GA, JT-VAE, GB-BO, MARS, REINVENT defined in PMO benchmark \cite{pmo}. In terms of MOLLEO, DyMol and Genetic-GFN, we also use the default hyperparameters defined in their codes and papers. For fair comparison, the normalized objectives are applied for all methods, which also includes the correct optimizing direction. 


\subsection{Main Experiment Results}
\label{sec:results}

\setlength{\tabcolsep}{1pt}
\begin{table*}[h]
\centering
\small
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{2.5cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.7cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{2.2cm} >{\centering\arraybackslash}m{2.2cm}}
\toprule
Metric & GB-GA & JT-VAE & GB-BO & MARS & REINVENT & MOLLEO & DyMol & Genetic-GFN & MOLLM(ours) \\   \midrule
\multicolumn{10}{c}{\textbf{(Worst initial)}} \\
Top1 F     & 4.048 & 3.817 & 3.665 & 3.907 & - & 4.096 & - & - & \cellcolor{lime}\textbf{4.187}\\
Top10 F    & 4.019 & 3.782 & 3.637 & 3.853 & - & 4.044 & - & - & \cellcolor{lime}\textbf{4.152}\\
Uniqueness & 0.786 & 1.000 & 1.000 & 0.488 & - & 0.672 & - & - & 0.937 \\
Validity   & 1.000 & 1.000 & 1.000 & 1.000 & - & 0.930 & - & - & 0.915 \\
Diversity  & 0.583 & 0.847 & 1.000 & 0.826 & - & 0.656 & - & - & 0.556 \\ \midrule
\multicolumn{10}{c}{\textbf{(Random initial)}} \\
Top1 F     & 3.941 & 3.923 & 4.015 & 3.924 & 4.092 & 4.098 & 4.232 & 4.157 & \cellcolor{lime}\textbf{4.276} \\
Top10 F    & 3.926 & 3.851 & 3.937 & 3.875 & 4.023 & 4.065 & 4.164 & 4.087 & \cellcolor{lime}\textbf{4.245}\\
Uniqueness & 0.821 & 0.956 & 1.000 & 0.477 & 0.690 & 0.575 & 0.986 & 0.349 & 0.949 \\
Validity   & 1.000 & 1.000 & 1.000 & 0.999 & 0.979 & 0.938 & 1.000 & 0.998 & 0.900 \\ 
Diversity  & 0.623 & 0.778 & 0.717 & 0.819 & 0.640 & 0.570 & 0.581 & 0.653 & 0.529 \\ \midrule
\multicolumn{10}{c}{\textbf{(Best initial)}} \\
Top1 F     & 4.583 & 4.329 & 4.582 & 4.420 & - & \cellcolor{lime}\textbf{4.699} & - & - & \cellcolor{lime}\textbf{4.699}\\
Top10 F    & 4.582 & 4.132 & 4.472 & 4.181 & - & 4.564 & - & - & \cellcolor{lime}\textbf{4.628} \\
Uniqueness & 0.729 & 1.000 & 1.000 & 0.432 & - & 0.678 & - & - & 0.942 \\
Validity   & 1.000 & 1.000 & 1.000 & 0.999 & - & 0.913 & - & - & 0.790 \\
Diversity  & 0.424 & 0.792 & 0.630 & 0.788 & - & 0.600 & - & - & 0.491 \\
\bottomrule
\end{tabular}
\caption{Unconstrained molecular design results, objectives: QED$\uparrow$ + SA$\downarrow$ + DRD2$\downarrow$ + GSK3$\beta\downarrow$ + JNK3$\uparrow$}
\label{tab:main_results}
\end{sc}
\end{table*}

\setlength{\tabcolsep}{1pt}
\begin{table*}[h]
\centering
\scriptsize
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{1.8cm} | >{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm} |>{\centering\arraybackslash}m{1.2cm}}
\toprule
 & \multicolumn{2}{c|}{\textbf{1 Objective}} & \multicolumn{2}{c|}{\textbf{2 Objectives}} & \multicolumn{2}{c|}{\textbf{3 Objectives}} & \multicolumn{2}{c|}{\textbf{4 Objectives}} & \multicolumn{2}{c|}{\textbf{5 Objectives}} & \multicolumn{2}{c}{\textbf{6 Objectives}} \\ \midrule  
 
Metric & \multicolumn{2}{c|}{MOLLM MOLLEO} & \multicolumn{2}{c|}{MOLLM MOLLEO} & \multicolumn{2}{c|}{MOLLM MOLLEO} & \multicolumn{2}{c|}{MOLLM MOLLEO} & \multicolumn{2}{c|}{MOLLM MOLLEO} & \multicolumn{2}{c}{MOLLM MOLLEO}  \\  \midrule
Top1 F & \multicolumn{2}{c|}{\textbf{0.948} \hspace{0.2cm} 0.941} & \multicolumn{2}{c|}{\textbf{1.901} \hspace{0.2cm} 1.887} & \multicolumn{2}{c|}{\textbf{2.901} \hspace{0.2cm} 2.891} & \multicolumn{2}{c|}{\textbf{3.901} \hspace{0.2cm} 3.890} & \multicolumn{2}{c|}{\textbf{4.276} \hspace{0.2cm} 4.098} & \multicolumn{2}{c}{\textbf{5.183} \hspace{0.2cm} 4.964} \\
Top10 F & \multicolumn{2}{c|}{\textbf{0.948} \hspace{0.2cm} 0.936} & \multicolumn{2}{c|}{\textbf{1.901} \hspace{0.2cm} 1.882} & \multicolumn{2}{c|}{\textbf{2.901} \hspace{0.2cm} 2.886} & \multicolumn{2}{c|}{\textbf{3.901} \hspace{0.2cm} 3.887} & \multicolumn{2}{c|}{\textbf{4.245} \hspace{0.2cm} 4.065} & \multicolumn{2}{c}{\textbf{5.164} \hspace{0.2cm} 4.948} \\
Uniqueness & \multicolumn{2}{c|}{\textbf{0.929} \hspace{0.2cm} 0.150} & \multicolumn{2}{c|}{\textbf{0.666} \hspace{0.2cm} 0.231} & \multicolumn{2}{c|}{\textbf{0.778} \hspace{0.2cm} 0.273} & \multicolumn{2}{c|}{\textbf{0.807} \hspace{0.2cm} 0.387} & \multicolumn{2}{c|}{\textbf{0.949} \hspace{0.2cm} 0.575} & \multicolumn{2}{c}{\textbf{0.957} \hspace{0.2cm} 0.591}\\
Validity & \multicolumn{2}{c|}{\textbf{0.796} \hspace{0.2cm} 0.159} & \multicolumn{2}{c|}{\textbf{0.962} \hspace{0.2cm} 0.552} & \multicolumn{2}{c|}{\textbf{0.946} \hspace{0.2cm} 0.803} & \multicolumn{2}{c|}{\textbf{0.946} \hspace{0.2cm} 0.783} & \multicolumn{2}{c|}{0.900 \hspace{0.2cm} \textbf{0.938}} & \multicolumn{2}{c}{0.890 \hspace{0.2cm} \textbf{0.926}}\\
Diversity & \multicolumn{2}{c|}{0.538 \hspace{0.2cm} \textbf{0.865}} & \multicolumn{2}{c|}{0.450 \hspace{0.2cm} \textbf{0.646}} & \multicolumn{2}{c|}{0.510 \hspace{0.2cm} \textbf{0.627}} & \multicolumn{2}{c|}{0.375 \hspace{0.2cm} \textbf{0.614}} & \multicolumn{2}{c|}{0.529 \hspace{0.2cm} \textbf{0.573}} & \multicolumn{2}{c}{0.529 \hspace{0.2cm} \textbf{0.611}}\\
\bottomrule
\end{tabular}
\caption{Unconstrained molecular design results with 1 to 6 objectives. The sixth objective is BBBP.}
\label{tab:multi-objective}
\end{sc}
\end{table*}
Following the experimental settings of MOLLEO~\cite{molleo}, we first conduct experiments to optimize five molecular properties simultaneously using molecules sampled from the ZINC 250K dataset. Among these objectives, three are minimized: SA, DRD2, and GSK3$\beta$, while two are maximized: QED and JNK3. Each model is run with five different random seeds, and the final results are reported as the average over these runs. Since the initial population for REINVENT, DyMol, and Genetic-GFN cannot be explicitly set, these models are only evaluated in the randomly initialized scenario. The key evaluation metrics are top-1 fitness and average top-10 fitness, both of which directly reflect the sum of the normalized property values. To enhance clarity, the highest values in each metric are highlighted in Table~\ref{tab:main_results}. Our model demonstrates a significant improvement over other SOTA models across all three initialization cases, with a clear performance gap compared to the second-best approach. Notably, in both the worst-initialization and random-initialization scenarios, the mean top-10 F-value exceeds the top-1 F-value of the second-best model, highlighting the superior performance and convergence capabilities of our approach.

In the best-initialization scenario, while the top-1 fitness of MOLLEO matches that of MOLLM, the mean top-10 fitness of MOLLM is noticeably higher than both the top-1 and mean top-10 fitness of all other models. Furthermore, our model maintains a uniqueness rate above 90\%, whereas MOLLEO, despite being another LLM-based method, exhibits significantly lower uniqueness. This underscores the strong capability of MOLLM in effectively exploring the chemical space. The validity of generated molecules is also comparable to other models. Although our model exhibits relatively lower diversity among the top-100 molecules, we observe that models with higher diversity often achieve lower top fitness values. This suggests that direct comparisons of diversity may be less meaningful in this context but highlight a potential direction for future improvements. Across all three initialization settings, MOLLM consistently maintains higher diversity while achieving superior fitness values, demonstrating its robustness in molecular optimization.



\section{Ablation Study}
In addition to the SOTA results from our main experiments involving the optimization of five objectives, we conduct further experiments with one to six objectives to assess the efficacy of MOLLM across varying optimization complexities and less predictable properties. Following this, we present an analysis of an interesting finding related to the experience pool utilized in our algorithm. Finally, we evaluate the impact of hyperparameters and demonstrate the effectiveness of our proposed components.  
\subsection{More Objectives}  
To further investigate MOLLM’s performance across different objective configurations, we conduct experiments using random initialization across scenarios with one to six objectives. The specific objective combinations are as follows:
\begin{enumerate}
    \item QED$\uparrow$ 
    \item QED$\uparrow$ + SA$\downarrow$ 
    \item QED$\uparrow$ + SA$\downarrow$ + DRD2$\downarrow$ 
    \item QED$\uparrow$ + SA$\downarrow$ + DRD2$\downarrow$ + GSK3$\beta\downarrow$
    \item QED$\uparrow$ + SA$\downarrow$ + DRD2$\downarrow$ + GSK3$\beta\downarrow$ + JNK3$\uparrow$
    \item QED$\uparrow$ + SA$\downarrow$ + DRD2$\downarrow$ + GSK3$\beta\downarrow$ + JNK3$\uparrow$ + BBBP$\uparrow$
\end{enumerate}
As the number of objectives increases, the performance gap between MOLLM and MOLLEO widens, particularly when optimizing more than four objectives, highlighting the superior capability of MOLLM in handling MOO. Additionally, MOLLM consistently achieves higher uniqueness and competitive validity compared to MOLLEO, while in MOLLEO these metrics tend to degrade significantly when optimizing fewer objectives. The consistently high uniqueness across all cases underscores the stability and effectiveness of MOLLM in optimization tasks with varying numbers of objectives. To further assess the robustness of MOLLM, we introduce BBBP (Blood-Brain Barrier Permeability) as a sixth objective, as it is a more complex and less predictable property with limited domain knowledge. Notably, despite the increased difficulty, MOLLM successfully generates a top 100 molecule set where all molecules are Blood-Brain Barrier Permeable (BBB+), demonstrating its strong adaptability and effectiveness in optimizing challenging molecular properties. 


\begin{table}[h]
\centering
\scriptsize
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.5cm}}
\toprule
Method & LLM calls & Runing Time (hours) \\ \midrule
MOLLEO & 8517 & 7.32 \\
MOLLM & 2908 & 0.52 \\
\bottomrule
\end{tabular}
\caption{Running time of MOLLEO and MOLLM}
\label{tab:efficiency}
\end{sc}
\end{table}
Apart from that, without early stopping, MOLLM only uses nearly $\frac{1}{3}$ LLM calls compared to MOLLEO, more than even 14x faster than MOLLEO in run time to achieve significantly better results, as shown in Table~\ref{tab:efficiency}. 

\subsection{Experience Pool}
\begin{table}[h]
\centering
\scriptsize
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} }
\toprule
$P_{exp}$& Top1 F & Top10 F & Uniqueness & Validity & Diversity \\ \midrule
0.0 & \textbf{4.187} & \textbf{4.152} & 0.937 & 0.915 & \textbf{0.556} \\
0.1 & 4.175 & 4.163 & 0.935 & \textbf{0.917} & 0.548 \\
0.3 & 4.154 & 4.124 & 0.961 & 0.903 & 0.544 \\
0.5 & 4.168 & 4.144 & \textbf{0.978} & 0.898 & 0.554 \\
\bottomrule
\end{tabular}
\caption{Experiments of adding experience.}
\label{tab:exp}
\end{sc}
\end{table}
Inspired by ExpeL~\cite{zhao2023expel}, we incorporate an experience pool into our algorithm to enhance molecular optimization. The experience pool consists of two key components: (1) knowledge gained from generating better and structurally similar molecules, and (2) insights for avoiding suboptimal molecules. These are achieved by summarizing information from the top 10 and bottom 10 molecules, respectively, in each iteration. The worst 10 molecules are selected using a sliding window approach with a stride of 10. Specifically, if in the previous iteration, the worst molecules were extracted from the bottom 10, the next iteration extracts molecules ranked from the bottom 20 to the bottom 10. This mechanism ensures that the experience pool continuously evolves, integrating knowledge from both current and past iterations.

While the concept of experience pools aligns with human decision-making—shifting from general heuristics to more concrete optimization strategies—we observe a performance decline when incorporating experience into our model. As shown in Table~\ref{tab:exp}, where $P_{exp}$ represents the probability of integrating experience into the prompt, the best performance is achieved when $P_{exp}=0.0$, indicating that excluding experience leads to superior optimization and greater molecular diversity. We attribute this phenomenon to the nature of optima distribution in the molecular space. Since local optima tend to be large and widely separated, the experience pool may cause the model to focus excessively on a given local optimum, thereby hindering exploration of alternative high-quality solutions. Consequently, to maximize optimization performance, we temporarily exclude the experience pool from our experiments.

\subsection{Hyperparameters}
\begin{table}[h]
\centering
\scriptsize
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} }
\toprule
Method & Top1 F & Top10 F & Uniqueness & Validity & Diversity \\ \midrule
Without MO Selection & 3.830 & 3.791 & \textbf{0.999} & 0.816 & \textbf{0.842} \\
With MO Selection & \textbf{4.187} & \textbf{4.152} & 0.961 & \textbf{0.915} & 0.556 \\
\bottomrule
\end{tabular}
\caption{Experiments of using MO.}
\label{tab:selection}
\end{sc}
\end{table}
To validate the effectiveness of the key components in MOLLM, we conduct a series of ablation studies. In MOLLM, Pareto front selection and F-value selection are applied with equal probability in each iteration. The importance of this design is demonstrated in Table~\ref{tab:selection}, where performance significantly deteriorates when multi-objective selection is removed. Furthermore, if an objective is included in the prompt but is not explicitly considered in MO selection, the performance of MOLLM declines substantially. This highlights the critical role of MO selection in ensuring effective optimization across multiple objectives.

\begin{table}[b]
\centering
\scriptsize
\begin{sc}
\begin{tabular}{>{\centering\arraybackslash}m{1.7cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.5cm} >{\centering\arraybackslash}m{1.2cm} >{\centering\arraybackslash}m{1.2cm} }
\toprule
Method & Top1 F & Top10 F & Uniqueness & Validity & Diversity \\ \midrule
GPT-4o direct propose & 3.974 & 3.955 & 0.955 & 0.864 & 0.644 \\
MOLLM (Llama3-8B) & 3.988 & 3.900 & 0.986 & 0.482 & 0.749 \\
1 offspring each call & 4.068 & 3.980 & 0.969 & 0.942 & 0.575 \\
3 offspring each call & 4.208 & 4.114 & 0.970 & 0.831 & 0.592 \\
2 offspring each call & \textbf{4.276} & \textbf{4.245} & 0.949 & 0.900 & 0.529 \\
\bottomrule
\end{tabular}
\caption{Experiments of effects of hyperparameters.}
\label{tab:llm}
\end{sc}
\end{table}
In Table~\ref{tab:llm}, the MOLLM with ``2 offspring each LLM call'' is used in our official version. Compared to 5000 molecules directly proposed by GPT-4o, MOLLM makes a significant improvement to it, illustrating the effectiveness of our framework. Even with Llama3-8B~\cite{grattafiori2024llama3} as our backbone, which is much inferior to GPT-4o, its performance is also comparable to other models in Table~\ref{tab:main_results}. We make the LLM to generate two offsprings in both crossover and mutation for each LLM call. This design significantly reduces the number of LLM calls needed and achieves better performance, compared to one offspring each call which is used by MOLLEO and three offspring each call.  

\section{Conclusion}
In this work, we introduce MOLLM, a novel framework that integrates MOO, GA, and LLMs with in-context learning and prompt engineering for molecular design. MOLLM requires no additional training, relying exclusively on LLMs as genetic operators, and achieves SOTA performance in unconstrained molecular optimization. Through rigorous framework design, empirical evaluations, and ablation studies, we demonstrate its effectiveness and efficiency. MOLLM significantly reduces computational costs while outperforming other LLM-based approaches and other SOTA methods. This efficiency is particularly advantageous for practical applications, where molecular property evaluations often involve costly biological and pharmaceutical testing, and LLM inference imposes a high computational overhead. Our results show that MOLLM maintains robust performance across various objective settings and remains superior when optimizing multiple objectives, including less predictable properties such as BBBP. Furthermore, MOLLM is adaptable to different LLM architectures, facilitated by a carefully designed prompt template that fully utilizes LLM knowledge. Future research may focus on enhancing molecular diversity and refining the experience pool mechanism to further improve optimization performance.

\section*{Impact Statement}
The development of MOLLM introduces a novel approach to multi-objective molecular design by integrating LLMs as genetic operators. This work has the potential to advance computational drug discovery, materials science, and chemical engineering by significantly improving the efficiency and effectiveness of molecular optimization.

From an ethical perspective, MOLLM does not generate molecules directly aimed at harmful applications, such as toxic or hazardous compounds. However, as with any generative model in molecular design, dual-use concerns may arise, necessitating responsible usage and safeguards to ensure ethical deployment. Researchers and practitioners leveraging MOLLM should carefully consider biosecurity implications, regulatory frameworks, and best practices in molecular design.

On a societal level, the framework reduces the reliance on resource-intensive molecular synthesis and experimental testing, potentially accelerating drug discovery and enabling more cost-effective pharmaceutical and material innovations. Additionally, MOLLM’s adaptability to different LLM architectures ensures that future advancements in AI models can further enhance molecular design without requiring retraining or additional computational resources.

Future work should focus on improving molecular diversity, refining the experience pool mechanism, and ensuring ethical guidelines are upheld in real-world applications. This work aligns with the broader goal of advancing machine learning for scientific discovery, contributing to AI-driven molecular design with potential long-term benefits in healthcare, sustainability, and materials innovation.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

%\bibliography{example_paper}
\input{icml2025.bbl}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{Baseline Details}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
