\documentclass{elsarticle}

%\pagestyle{empty}


\usepackage{amsmath, amssymb}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{verbatim}
\usepackage{graphicx}


\def\n{\noindent}
\def\s{\vspace{0.3cm}}
\def\ns{\vspace{-0.2cm}}
\def\bop{\noindent {\bf Proof.}$ \;$ }
\def\eop{\hfill $\Box$ \vspace{0.3 true cm}}

\def\supp{{\rm supp}}
\def\diam{{\rm diam}}
\def\conv{{\rm conv}}
%\def\span{{\rm span}}
\def\card{{\rm card}}
\def\sinc{{\rm sinc}}
\def\cl{{\rm cl}}
\def\vol{{\rm vol}}
\def\interior{{\rm int}}
\def\sign{{\rm sign}}

\def\ve{\varepsilon}
\def\vt{\vartheta}

\def\var{{\bf var}}



\def\P{{\rm P}}
\def\Q{{\rm Q}}
\def\E{{\rm E}}
\def\cA{{\mathcal A}}
\def\cO{{\mathcal O}}
\def\cP{{\mathcal P}}
\def\cQ{{\mathcal Q}}
\def\cE{{\mathcal E}}
\def\cW{{\mathcal W}}
\def\cC{{\mathcal C}}
\def\cF{{\mathcal F}}
\def\cG{{\mathcal G}}
\def\cH{{\mathcal H}}
\def\cK{{\mathcal K}}
\def\cL{{\mathcal L}}
\def\cB{{\mathcal B}}
\def\cI{{\mathcal I}}
\def\cS{{\mathcal S}}
\def\cP{{\mathcal P}}
\def\cX{{\mathcal X}}
\def\cY{{\mathcal Y}}

%
\def\bR{{\mathbb R}}
\def\bN{{\mathbb N}}
\def\bZ{{\mathbb Z}}

\def\dsst{\displaystyle}

\def\n{\noindent}

\def\bop{\noindent {\bf Proof.\ }}
%\def\eop{\hfill $\Box$}


\def\cR{{\mathbb R}}
\def\Re{\cR}

\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bv{{\bf v}}
\def\ba{{\bf a}}
\def\be{{\bf e}}
\def\bw{{\bf w}}
\def\bb{{\bf b}}
\def\bu{{\bf u}}
\def\bc{{\bf c}}
\def\bz{{\bf z}}
\def\vt{\vartheta}
\def\ve{\varepsilon}
\def\sign{{\rm sign}}
\def\bE{{\bf E}}

\def\bk{{\bf k}}
\def\dist{{\rm dist}}
\def\conv{{\rm conv}}
\def\span{{\rm span}}
\def\cl{{\rm cl}}
\def\card{{\rm card}}
\def\vol{{\rm vol}}
\def\var{{\rm var}}
\def\sgn{\mbox{\rm sgn}}
\def\udef{:=}

\def\bop{\noindent {\bf Proof.}$ \;$ }
\def\eop{\hfill $\Box$ \vspace{0.5cm}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}



\begin{document}

\begin{frontmatter}

\title{Networks with Finite VC Dimension: Pro and Contra}

\author[VK]{V\v{e}ra K\r{u}rkov\'{a}}

\address[VK]{Institute of Computer Science of the Czech Academy of Sciences \\
Pod Vod\'{a}renskou v\v{e}\v{z}\'\i,  2 - 18207 Prague, Czech Republic} \ead[VK]{vera@cs.cas.cz}

\author[MS]{Marcello Sanguineti}

\address[MS]{DIBRIS \\ University of Genova \\ Via Opera Pia, 13 - 16145 Genova, Italy}
\ead[MS]{marcello.sanguineti@unige.it}

\date{}

\maketitle



\begin{abstract}
Approximation and learning of classifiers of large data sets by neural networks in terms of high-dimensional geometry and statistical learning theory are investigated. The influence of the VC dimension of sets of input-output functions of networks on approximation capabilities is compared with its influence on consistency in learning from samples of data. It is shown that, whereas finite VC dimension is desirable for uniform convergence of empirical errors, it may not be desirable for approximation of functions drawn from a
probability distribution modeling the likelihood that they occur in a given type of application.
Based on the concentration-of-measure properties of high dimensional geometry, it is proven that both errors in approximation and empirical errors behave almost deterministically for networks implementing sets of input-output functions with finite VC dimensions in processing large data sets.
Practical limitations of the universal approximation property, the trade-offs between the accuracy of approximation and consistency in learning from data,
and the influence of depth of networks with ReLU units on their accuracy and consistency are discussed.
\end{abstract}
\begin{keyword} Approximation by neural networks;  Accuracy and consistency; Random classifiers; Probabilistic bounds on approximation errors;  VC dimension; Growth function; Concentration of measure; Method of bounded differences.
\end{keyword}

\end{frontmatter}


\section{Introduction}

The capability to compute large classes of functions with sufficient accuracy (the so-called ``universal approximation property'') has been proven for many types of networks, assuming their unlimited complexity. More refined analysis has been focused on trade-offs between constraints on classes of functions to be approximated and bounds on various measures of network complexity. They have been studied for classes of multivariable functions defined by bounds on suitable norms, such as Sobolev norms (see \cite{yazh20} and references therein) and variational norms tailored to computational units (see, e.g., \cite{ba93},\cite{vk12}).

In contrast to constraints on sets of functions to be approximated, which are defined by norms, we consider constraints defined in terms of probability distributions. As the number of all classifiers on a data set increases exponentially with its size, most are likely irrelevant for tasks for which a neural network model is searched. So, we assume that functions to be computed are characterized by probabilities representing the likelihood that they occur in a given type of task. To assess the suitability of classes of networks for such tasks, we investigate the accuracy of computation and consistency in learning from data samples in terms of probability distributions of approximation and empirical error functionals.

We study the properties of classifiers as vectors in spaces of dimensions equal to the sizes of data sets. As such sets are typically large, we employ properties of high-dimensional geometry and probability, particularly the concentration of measure \cite{misc86,le01,dupa09}. It implies almost deterministic behavior of values of sufficiently smooth functions of random variables around their mean values. We show that both approximation and empirical errors satisfy a smoothness condition that can be seen as a coordinate-wise Lipschitz property, which allows us to use the McDiarmid Bound \cite{mc89}. Combining concentration bounds with upper bounds on the growth of sizes of sets of I/O functions with sizes of their domains, we derive lower and upper probabilistic bounds on accuracy. We show that the polynomial growth of sets of input-output functions holding for networks with finite VC dimension is ``too small'' to compensate for exponentially decreasing sizes of sets of classifiers, and thus, both approximation and empirical errors are tightly concentrated.
In particular, when there is no prior knowledge available, one has to assume that the probability of occurrence of functions is uniform, and in this case, the errors concentrate around a large value. So, although the universal approximation property holds, almost all functions have too large errors in approximation by networks with finite VC dimensions.

We compare the concentration of errors in approximation with classical results from statistical learning theory. In both cases, the finiteness of the VC dimension plays a crucial role, but with different consequences. For empirical errors, it implies their uniform convergence. In contrast, for approximation errors, it implies their concentration around values that can be large - unless a network is selected so that some of its input-output functions highly correlate with the probability distribution. We also discuss the trade-offs between accuracy in approximation and consistency in learning from data.
Some preliminary results were presented in the conference proceedings \cite{vk23}.

The paper is organized as follows. Section \ref{sec:prel} introduces basic concepts and notations. Section \ref{sec:prob} presents a probabilistic approach to function approximation and verifies smoothness conditions that guarantee the concentration of  approximation and empirical errors. In Section \ref{sec:conc}, our main results on accuracy and consistency are derived. Section \ref{sec:VC} combines probabilistic bounds with estimates of growth functions of deep ReLU networks and derives consequences for the suitability of networks for tasks described by probability distributions.
Finally, Section \ref{sec:disc} summarizes and discusses the paper's main findings.



 \section{Preliminaries}
 \label{sec:prel}

We use the standard notation: $\bN_{+}$ denotes the set of {\em positive integers}, $(\bR^m, \| . \|_2)$  the $m$-dimensional Euclidean space with the $l_2$-norm.
The {\em distance of a function $f$} in a normed linear space $(\cX, \|.\|_{\cX})$ from its subset $\cH$ is denoted
$$\| f - \cH\|_{\cX} := \inf_{h \in \cH} \|f - h\|_{\cX}.$$
By $f^{\circ} := \frac{f}{\| f \|_{\cX}}$  we denote the normalized function. For $U \subseteq \bR^d$, we denote
$$\cB(U) : = \{ f : U \to \{-1,1\} \}.$$
The set $\cB(U)$ models binary classifiers on $U$. We investigate
approximation and learning of classifiers on {\em finite data sets}.
So we consider functions from $\cB(X)$  where $$X= \{x_1, \dots, x_m\} \subset \bR^d.$$ \n The set $\cB(X)$ can be identified with the {\em Hamming cube} $\{-1,1\}^m$ and its elements with $m$-dimensional vectors. For a set $\cH \subset \cB(\bR^d)$ we denote
$$\cH(X):= \cH_{|_X} = \{ h|_{X}: X \to \{-1,1\} \, | \, h \in \cH\} \subset \cB(X)\,.$$

{\em Feedforward neural networks} compute parameterized families of input-output (I/O) functions.  They are  determined by network architectures described by directed acyclic graphs $\cG$, where nodes represent computational units and edges connections between them. In practical applications, network inputs belong to large finite sets, so we investigate networks with finite sets of inputs $X =\{x_1, \dots, x_m\}  \subset \bR^d$ representing input data in the form of vectors of features. We focus on networks with single binary-valued outputs from $\{-1,1\}$.
Sets of I/O functions of such networks are subsets of $\cB(X)$ formed by vectors having $l_2$-norms equal to $\sqrt{m}$.

The parameters of computational units and of connections between them are optimized during learning.
A {\em loss function} $\ell: X \times Y \to \bR_+$  measures the difference between the desired value $y \in Y$ and the output $h(x)$ that is computed  by an I/O function $h$ on the input $x$.

In {\em multilayer networks}, units are arranged in layers, network inputs are viewed as the layer $0$, output as the layer $L+1$,  the layers $l=1, \dots, L$ are called {\em hidden layers}, and $L$ is called the {\em network depths}.
A biologically inspired computational unit called {\em perceptron}  applies  a fixed {\em activation function}  $\psi: \bR \to \bR$ to an affine functions with varying parameters. A perceptron computes functions of the form
$$\psi(v \cdot . +b): \bR^d \to \bR,$$
\n where $v \in \bR^d$ is called a {\em weight vector}, $b \in \bR$ a {\em bias}, and $v \cdot x = \sum_{i=1}^d v_i x_i$ is the {\em scalar product} of the weight vector with input vector $x$ (weighted sum of inputs). Originally, perceprons were endowed with sigmoidal activation functions representing hard or soft threshold, such as the {\em Heaviside} function defined as  $\theta(t) = 0$ for $t\ \le 0$ and $\theta(t) =1$ for $t \geq 1$. Currently, the most widespread computational units are {\em rectified linear units} (ReLU) which are perceptrons with the activation function $\rho(t) = \max(0,t)$.


\section{Probabilistic bounds on approximation and empirical errors}
\label{sec:prob}

We study approximation and learning of binary-valued functions on finite sets of data $X = \{x_1, \dots, x_m\}$ that are constrained by a probability distribution modeling the likelihood that a function might occur in a given type of applications. We assume that there is a {\em probability distribution} ${\P}$ on
$$\cB(X) := \{ f: X \to \{-1,1\} \}\simeq \{-1,1\}^m$$
 \n  and functions to be computed by neural networks are drawn according to $\P$. We consider the case when $\P$ can be expressed as the product
$$\P(f) := \prod_{i=1}^m \P_i (f(x_i)) =  \prod_{i=1}^m \P(f(x_i)|x_i),$$
\n which implies that the random variables $f(x_1), \dots, f(x_m)$ are {\em independent}. Unless specified otherwise, we do not assume that they are identically distributed.

To model learning from samples of input-output pairs of data, we assume that there is a probability distribution $\P_X$ on $X$. and that the training samples
$$S := ( \, (x_{i_1},y_{i_1}), \dots, (x_{i_n}, y_{i_n}) \, ) \in (X \times \{-1,1\})^n,$$
\n where $n \leq m$, are selected according to the probability

$${\bar \P_n}(S) := \prod_{j=1}^n \P_X(x_{i_j})\P(y_{i_j}| x_{i_j}).$$

\n For any $h \in \cH(X)$,  let
$\eta_{h}: \cB(X) \to \bR_+$ denote the {\em square of the $l_2$-distance of $h^{\circ}$ from normalized functions from $\cB(X)$}

$$\eta_{h}(f) := \| f^o - h^o \|^2_2 = \frac{1}{m} \| f -h\|^2_2.$$

\n For a loss function $\ell: X \times \{-1,1\} \to \bR_+$  and $n \in\bN_+$, let
  $\cE_{\ell,h,n}: (X \times \{-1,1\})^n \to \bR_+$ denotes the {\em empirical error functional}
defined for a sample $S = ( (x_{i_1},y_{i_1}), \dots, (x_{i_n}, y_{i_n}) )$ as
 $$\cE_{\ell,h,n}(S) := \frac{1}{n} \sum_{j=1}^n \ell \bigl ( y_{i_j},h(x_{i_j}) \bigr ).$$


\n We explore the functional $\eta_{h}$ as a function of random variables $f(x_1), \dots, f(x_m)$ drawn from $\{-1,1\}$ according to ${\P}$ and the functional $\cE_{\ell,n,h}$  as a function  of random variables $(x_{i_1}, y_{i_1}), \dots, (x_{i_n}, y_{i_n})$ drawn from $(X \times \{-1,1\})$ according to ${\bar P}_n$.

We employ a concentration inequality  that holds for functions of independent random variables satisfying a smoothness assumption that can be seen as a coordinate-wise Lipschitz property.
A function
$\phi: A_1 \times \ldots \times A_m \to \bR$ satisfies the {\em bounded differences (BD) condition
with the parameter vector} $c= (c_1, \dots, c_m)$ if for all $i=1, \dots, m$ and all vectors $a, a' \in  A_1 \times \ldots \times A_m$ that differ only in the $i$-th coordinate,
 \begin{equation}
 | \phi(a) - \phi(a')| \leq c_i.
\label{eq:BD}
\end{equation}

\n Note that with sufficiently large parameters, the BD condition holds for every bounded function. The following theorem by McDiarmid \cite[p.70]{dupa09},\cite{mc89} shows that when the coefficients are small enough, then the BD condition implies concentration of function values around their mean value.

\begin{theorem}[McDiarmid Bound]
Let $A_1, \dots, A_m \subset \bR^d$, $\phi: \prod_{i=1}^m A_i  \to \bR$ satisfies the bounded differences condition with the vector of parameters $c:= (c_1, \ldots, c_m)$,  $\cP = \prod_{i=1}^m \cP_i$ a probability on $\prod_{i=1}^m A_i$, and $z_1, \ldots, z_m$ be independent random variables with values in ranges $A_1, \ldots, A_m$, drawn according to $\cP_i$, resp. Then for every $\lambda>0$,

\begin{equation}
\cP\Bigl [ \, \Bigl | \phi(z_1, \ldots, z_m) - \E(\phi)\Bigr | > \lambda \Bigr ]  \leq e^{-2\lambda^2/\| c \|_2^2}\,.
\end{equation}
\label{th:MD}
\end{theorem}


\n When a function $\phi$ satisfies the BD condition with $\|c\|_2^2 \leq \frac{s}{m}$ for some $s$ independent on $m$, then Theorem \ref{th:MD} implies

\begin{equation}
\cP\Bigl [ \, \Bigl | \phi(z_1, \ldots, z_m) - \E(\phi)\Bigr | > \lambda \Bigr ]  \leq e^{-2\lambda^2m/s}\,.
\label{eq:MD}
\end{equation}

\n So for a fixed $\lambda$, with $m$ increasing the probability that values of $\phi$ are close to their mean value $\E(\phi)$ converges quickly to $1$.
To apply the McDiarmid bound (\ref{eq:MD}) to the functionals $\eta_{h}$  and $\cE_{\ell,h,n}$, we estimate parameters for which  they satisfy the BD condition.

\begin{proposition}
Let $X = \{x_1, \dots, x_m\} \subset \bR^d$,  $h \in \cB(X)$, $\ell: \{-1,1\} \times \{-1,1\} \to [0,b]$ be a loss function.

 (i) $\eta_{h}: \cB(X) \to \bR_+$ satisfies the BD condition with the parameter vector $c$ with all $c_i= \frac{4}{m}$;

(ii) $\cE_{\ell,n,h}: (X \times \{-1,1\})^n \to \bR_+$ satisfies the BD condition with the parameter vector $c$ with all $c_i=\frac{2b}{n}$.
\label{pr:BD}
\end{proposition}

\bop
Without loss of generality, we assume that in both cases the coordinate to be changed is the first one.

(i) Let $f(x_1) = y_1$ and $f'(x_1)= {y_1}'$ and $f(x_i) = f'(x_i) = y_i$ , $i=2, \dots, m$. Then we have\\
$\frac{1}{m}  \Bigl |
\sum_{i=1}^m \bigl ( f(x_i) - h(x_i) \bigr )^2  -
\sum_{i=1}^m \bigl ( f'(x_i) - h(x_i) \bigr )^2
\Bigr | =
\frac{1}{m} \Bigl |
\bigl (y_1 - h(x_1)\bigr )^2 - \bigl ({y_1}' - h(x_1) \bigr )^2   \Bigr |  \leq \frac{4}{m}$.

(ii)
$ \frac{1}{n} \Bigl |\,  \ell(y_1, h(x_1)) + \sum_{i=2}^n  \ell(y_i,h(x_i)) - \ell(y'_1, h(x'_1)) - \sum_{i=2}^n  \ell(y_i,h(x_i)) \, \Bigr | =$\\
$ \frac{1}{n} \Bigl | \ell(y_1, h(x_1)) - \ell(y'_1, h(x'_1)) \Bigr |  \leq \frac{2b}{n}.$
\eop

\n Proposition \ref{pr:BD} shows that for every $h \in \cB(X)$, the functional $\eta_h$ satisfies the BD condition with $\| c\|^2_2= \frac{16}{m}$ and the functional $\cE_{\ell,h,n}$ with $\| c\|^2_2= \frac{4b^2}{n}$.
So by (\ref{eq:MD}), for a sufficiently large $m$ and $n$, resp. both functionals behave almost deterministically.



\section{Accuracy and consistency}
\label{sec:conc}

 To derive estimates of accuracy of neural-network approximation of functions constrained by probability distributions, we combine  the concentration results from the previous section with estimates of sizes of their sets of I/O functions.

On infinite domains ($\bR^d$ or its open subsets), I/O functions of neural networks of many types determine network parameters {\em uniquely up to permutations and sign flips} (see, e.g., \cite{su92,also93,vkka94,vkka96}).
On finite domains, typically function representations by neural networks  are highly redundant. In particular, in the case of networks with binary outputs, infinitely many parameterizations generate only finitely many functions.
Sizes of their sets of I/O functions can be studied using concepts from the statistical learning theory introduced by Vapnik and Chervonenkis \cite{vach71}. For a family of binary-valued functions $\cH$
on any $U \subset \bR^d$,  the {\em growth function} $\Pi_{\cH}(m): \bN_{+} \to \bN_{+}$ is defined  as
$$ \Pi_{\cH}(m) := \max_{V \subset U, \card V =m} \card \cH _{| \, V}.$$

We show that the growth function $\Pi_{\cH}$ plays a critical role in probabilistic bounds on the accuracy of approximation by $\cH$. We derive the bounds by  applying the McDiarmid Bound (\ref{eq:MD}) to $\eta_h$. For each $h \in \cH(X)$, let
$$\mu_h= \E(\eta_h)$$
\n be the mean value of $\eta_h$ with respect to $\P$ and
$$\mu_{\cH(X)}= \min_{h \in \cH(X)} \mu_h.$$



\begin{theorem}
Let $\cH \subset \cB(\bR^d)$, $X=\{x_1, \dots, x_m\} \subset \bR^d$, $\P$ be a product probability distribution on $\cB(X)$ and $\cH(X) \subset \cB(X)$. Then for every $\lambda>0$\\
$$ (i)  \quad \P \Bigl [ \mu_{\cH(X)} -\lambda \leq \|f^{\circ} - \cH(X)^{\circ} \|^2_2 \Bigr ] > 1 - \Pi_{\cH}(m) \, e^{-\frac{m\lambda^2}{8}};$$
$$ (ii)  \quad \P \Bigl [ \|f^{\circ} - \cH^{\circ} \|^2_2 \leq \mu_{\cH(X)} + \lambda \Bigr ] > 1 - e^{-\frac{m\lambda^2}{8}}  ;$$
\label{th:l2H}
\end{theorem}

\bop
Combining Theorem \ref{th:MD} with Proposition \ref{pr:BD} we obtain for every $h \in \cH$,
\begin{equation}
\P \Bigl [ \mu_h -\lambda\leq  \| f^{\circ} - h^{\circ} \|^2_2
\leq  \mu_h + \lambda \Bigr ] > 1- e^{-\frac{m\lambda^2}{8}}.
\label{eq:h}
\end{equation}
Hence $$\P \Bigl [ ( \forall h \in \cH(X))\, \bigl ( \mu_h -\lambda\leq  \| f^{\circ} - h^{\circ} \|^2_2 \bigr ) \Bigr ]
> 1- \card \cH(X) \, e^{-\frac{m\lambda^2}{2}} \geq 1 - \Pi_{\cH}(m) \, e^{-\frac{m\lambda^2}{2}}.$$
\n As $\mu_{\cH(X)} \leq \mu_h$ for all $h \in \cH(X)$,  (i) holds.
As $\cH(X)$ is finite, for every $f \in \cB(X)$ there exists  $h_f \in \cH(X)$ for which $\| f - \cH(X)\|_2 = \| f - h_f\|$.
So (ii) follows.
\eop


To apply the McDiarmid Bound also to the empirical error functional $\cE_{\ell,h}$, we assume that the probabilities $\P_1, \dots, \P_m$ on $\{-1,1\}$ are not only independent but also identically distributed. We set $\P_0 = \P_i$ for all $i=1, \dots, m$, and denote
$$\cE_{\ell}(h) = \E(\ell(y,h(x))$$
\n the mean value of the functional $\ell(y, h(x))$ with respect to the probability $${\bar \P} =\P_X(x)\P_0(x|y).$$

\begin{theorem}
Let $X=\{x_1, \dots, x_m\} \subset \bR^d$, $\P$ be a probability distribution on $\cB(X)$ such that  $\P= \prod_{i=1}^m \P_i$ where $\P_i$ are i.i.d.,
$\cH \subset \cB(\bR^d)$, and $\ell: \{-1,1\}  \to [0,b]$ be a loss function. Then for every $h \in \cH(X)$, every positive integer $n \leq m$, every random sample $S=(x_{i_1}, y_{i_1}), \dots, (x_{i_n}, y_{i_n})$, and every $\lambda >0$
$${\bar \P_n} \bigl [ \, \bigl ( \forall h \in \cH(X) \bigr ) \left | \, \cE_{\ell,n,h}(S) - \cE_{\ell}(h) \, \bigr  |  \leq \lambda \, \right ]\,  > 1-  \Pi_{\cH}(n) e^{-\frac{n \lambda^2}{2b^4}}.$$
\label{th:empH}
\end{theorem}

\bop
Let $U_n \subset X$ with $\card \, U_n =n$ and
$S = ( \, (x_{i_1},y_{i_1}), \dots, (x_{i_n}, y_{i_n}) \, )$ be a sample such that $\{x_{i_1}, \dots, x_{i_n}\}=U_n$. As the random variables $(x_{i_j},y_{i_j})$ are i.i.d., for every $h\in \cB(U_n)$,
we have
$$\E \bigl (  \cE_{\ell, h,n}(h) \bigr ) = \E \bigl( \frac{1}{n}
\sum_{j=1}^n \ell(y_{j_i}, h(x_{j_i})) \, \bigr )  =
 \E \bigl (\ell(y,h(x) \bigr ) = \cE_{\ell}(h),$$
 \n where the mean value is considered with respect to $\P_X(x)\P_0(y|x)$.
By Theorem \ref{th:MD},
$$ \P_0^n \bigl [ \,  \left | \, \cE_{\ell,n,h}(S) - \cE_{\ell}(h) \, \bigr  |  > \lambda \, \right ]\, \leq e^{-\frac{n \lambda^2}{2b^4}}.$$

\n Thus
$$ \P_0^n \bigl [ \, (\forall h \in \cH)  \left | \, \cE_{\ell,n,h}(S) - \cE_{\ell}(h) \, \bigr  |  > \lambda \, \right ]\,  \leq\Pi_{\cH}(m) e^{-\frac{n \lambda^2}{2b^4}}.$$
\n As ${\bar \P_n} (S)=   \prod_{j=1}^n \P_X(x_{i_j}) \bigl ( \P_0(y_{i_j} \bigr )^n$, we have
$$ {\bar \P}_n \bigl [ \, (\forall h \in \cH)  \left | \, \cE_{\ell,n,h}(S) - \cE_{\ell}(h) \, \bigr  |  > \lambda \, \right ]\,  \leq \Pi_{\cH}(m) e^{-\frac{n \lambda^2}{2b^4}}.$$
\eop


\n For $\lambda = m^{-1/4}$, Theorems \ref{th:l2H} and \ref{th:empH} imply
 \begin{equation}
 \P \Bigl [ \mu_{\cH(X)} - m^{-1/4} \leq \|f^{\circ} - \cH(X)^{\circ} \|^2_2 \Bigr ] > 1 - \Pi_{\cH}(m) \, e^{-\frac{m^{1/2}}{8}}
\label{eq:l2Hi}
\end{equation}

 \begin{equation}
 \P \Bigl [ \|f^{\circ} - \cH(X)^{\circ} \|^2_2 \leq \mu_{\cH(X)} + m^{-1/4} \Bigr ] > 1 - e^{-\frac{m^{1/2}}{8}}.
\label{eq:l2Hii}
\end{equation}


\begin{equation}
{\bar \P}_n \bigl [ \bigl ( \forall h \in \cH(X) \bigr ) \,  \left | \, \cE_{\ell,n,h}(S) - \cE_{\ell}(h) \, \bigr  |  \leq n^{-1/4} \, \right ]\,  > 1-  \Pi_{\cH}(n) e^{-\frac{n^{1/2}}{2b^4}}.
\label{eq:empH}
\end{equation}



So, Theorems \ref{th:l2H} and \ref{th:empH} show that the size of the set $\cH(X)$ influences the probabilities that errors in approximation by $\cH(X)$ are close to their mean value and that the empirical errors $\cE_{\ell,h,n}$ converge to the theoretical error $\cE_{\ell}(h)$ uniformly for all $h \in \cH(X)$. When the grows function $\Pi_{\cH}(m)$ does not outweigh $e^{-\frac{m{\lambda}^2}{8}}$, then errors in approximation of almost all functions behave almost deterministically, they concentrate around their mean value. When $\Pi_{\cH}(m)$ does not outweigh  $e^{-\frac{n{\lambda}^2}{2b^4}}$,
then empirical errors $\cE_{\ell, n,h}$ converge to the theoretical error $\cE_{\ell}(h)$ for all $h \in \cH(X)$.



The convergence of empirical errors following from Theorem \ref{th:empH} is the well-known result from statistical learning theory on the consistency of learning from samples of data \cite{vach71}. Here, we include the case of empirical error for comparison with approximation error. Both follow properties of high-dimensional geometry, but their influence on the suitability of networks for tasks characterized by $\P$ can differ.
While consistency in learning from data is a desirable property of a class of neural networks, the concentration of approximation errors close to $\mu_{\cH(X)}$ need not be desirable. When  $\mu_{\cH(X)}$ is not sufficiently small, then almost all functions chosen according to $\P$ cannot be approximated with a desired accuracy by I/O functions of networks with finite VC dimension. They
have squared normalized $l_2$-errors at least $\mu_{\cH(X)} - \lambda$. On the other hand, when  $\mu_{\cH(X)}$ is sufficiently small, then by the upper bound from Theorem \ref{th:l2H}(ii) almost all functions have errors at most $\mu_{\cH(X)} +\lambda$ in approximated by merely one function $h^{*}$ (any function for which the minimum $\mu_{\cH(X)}= \mu_{h^*}$.




\section{Concentration of errors on networks with finite VC dimension}
\label{sec:VC}


Theorems from the previous section imply that both concentration of approximation errors and convergence of empirical errors hold for networks with sets of I/O functions growing with $m$ polynomially. The essential property of the growth function of any set $\cH$, which was proven by Vapnik and Chervonenkis \cite{vach71}, is that it is either equal to $2^m$ for all $m$ or there exists a breaking point, called the {\em VC dimension}, from which it is bounded by a polynomial.  We denote
$$d_{V}(\cH) = \max_{m \in \bN} \Pi_{\cH}(m) = 2^m,$$
\n if such maximum exists, otherwise $d_{V}(\cH) = \infty$.
Vapnik and Chervonenkis \cite[Theorem 1]{vach71} proved that for $m>d_V$
\begin{equation}
\Pi_{\cH}(m) \leq m^{d_V +1}.
\label{eq:VC}
\end{equation}

\n A similar (slightly better) result obtained independently by Shelah \cite{sh72} and Sauer \cite{sa72}
states that for all $m>d_V$
\begin{equation}
\Pi_{\cH}(m) \leq \sum_{i=0}^{d_V} \binom{m}{i} \leq \left ( \frac{em}{d_V} \right )^{d_V} = \left ( \frac{e}{d_V} \right )^{d_V}\, m^{d_V}.
\label{eq:SaSh}
\end{equation}


\n So, any set with a finite VC dimension has a growth function that is bounded by a polynomial. Theorem \ref{th:l2H} implies that errors in approximation by networks with sets of I/O functions with finite VC dimension concentrate around their mean values. The smaller $\Pi_{\cH}(m)$, the larger probability that an accuracy of approximation of a random function is close to $\mu_{\cH(X)}$.

Estimates of sizes of sets of binary-valued functions generated on $m$-point subsets of $\bR^d$ have been studied since the 19th century. Schl\"{a}fli \cite{sc01,sc50} derived an upper bound on the number of {\em linearly separable dichotomies} of $m$ points in $\bR^d$. In neural network terminology, this number expresses the maximal number of functions computable by a single perceptron with the Heaviside activation on $m$-point subsets of $\bR^d$. Function-counting theorems estimating the numbers of dichotomies separated by some
non-linear surfaces (such as hyperspheres and hypercones) were derived by Cover \cite{co65}.

Bartlett et al. \cite{baal98,baal19} investigated the VC dimension of deep networks with perceptrons with piecewise linear activation functions. Inspection of \cite[Theorem 7 and Remark 9]{baal19} shows that for $\cH$ denoting the set of I/O functions of networks with ReLU perceptrons, $L$ layers, total number of parameters $W$,
$$\Pi_{\cH}(m) \leq \bigl( 4eLm \bigr )^{LW}.$$
\n So, the growth function of the set of I/O functions of deep ReLU networks depends on the size of the domain $m$ polynomially, with the degree equal to  the product of the number of layers $L$ and the total number of parameters $W$. It is reasonable to assume that both  $L$ and $W$ are considerably smaller than $m$ (networks with the same number of parameters of layers as the size of data to be processes would not be efficient). Thus for sufficiently large $m$, errors in approximation of random functions by the currently popular deep ReLU networks behave almost deterministically.

Without prior knowledge, we must assume that all functions in $\cB(X)$ have the same likelihood that they occur in a task, so we have to assume that $\P$ is uniform. Because of symmetry, $\E(\eta_h)= 2$  for all $h \in \cB(X)$ and so $\mu_{\cH(X)}=2$ for any set $\cH(X)$.  Networks with $\Pi_{\cH(X)}$ growing with $\card X =m$ slower than $e^{-\frac{m{\lambda}^2}{8}}$ are insufficient for a good approximation of almost all functions. The concentration of errors of all uniformly randomly chosen functions around this mean value shows the practical limitations of the universal approximation property. When the size of sets of I/O functions does not outweigh $e^{-\frac{m{\lambda}^2}{8}}$, then most functions cannot be approximated with better accuracy than $2-\lambda$. For $\lambda \leq m^{/1/4}$, almost all functions cannot be approximated with errors smaller than $2 - m^{-1/4}$.
Particularly,  networks with finite VC dimensions cannot approximate with a sufficient accuracy almost all functions from $\cB(X)$. Seen as vectors in $\{-1,1\}^m$, most functions are nearly orthogonal to any set $\cH(X)$ with $\Pi_{\cH}(m)$ polynomial.


When the likelihood of occurrence of a function in a given type of task is nonuniform, it might happen that
$\mu_{\cH(X)} = \min_{h \in \cH(X)} \E(\eta_h)$ is smaller or equal to the  desired accuracy. This implies that there exists some $h^* \in \cH(X)$, for which $\E(\eta_{h^*})$ is sufficiently small. Inspection of the proof of Theorem \ref{th:l2H} shows that approximation errors of almost all
functions are concentrated around $\E(\eta_{h^*}) = \E \| f^{\circ} - h^{*\circ}\|_2^2$. So, almost all functions chosen randomly according to $\P$ can be well approximated by the function $h^*$.


\section{Discussion}
\label{sec:disc}

We have shown that while it is well-known that the finiteness of the VC dimension of a set $\cH(X)$ of network I/O functions is desirable for learning (it guarantees uniform convergence of empirical errors), it may be a disadvantage for function approximation. The approximation accuracy by networks with sets of I/O functions with finite VC dimensions may be poor for almost all functions. It depends on the minimum $\mu_{\cH(X)}$ of the mean values of distances from all I/O functions, around which approximation errors tightly concentrate. If $\mu_{\cH(X)}$ is large, almost all functions drawn according to the probability $\P$, which characterizes the likelihood that a function occurs in a given type of task, cannot be computed accurately. In particular, when the probability distribution is uniform, almost all functions have large approximation errors. On the other hand, when $\mu_{\cH(X)}$ is small, then merely one I/O function $h^*$ with a small mean value $\E(\eta_{h^*})$ well approximates almost all functions. $\mu_{\cH(X)}$ is small when the probability is high on a small neighborhood of some input-output function of the network and elsewhere is negligible.

Proofs of our results are based on the geometry of high-dimensional spaces, which exhibits some counterintuitive properties, sometimes called the ``blessing of dimensionality'' \cite{do00,goty18,goal19,vk19}. They include exponential growth of the quasi-orthogonal dimension \cite{kavk93}, the concentration of the volume and the surface of high-dimensional spheres \cite{ba97}, and reduction of dimensionality by random projections \cite{joli84}. Other applications of the ``blessing of dimensionality'' to neurocomputing were focused on highly-varying functions \cite{vksa16}, robustness, and intrinsic dimensionality of data \cite{sual23,baal23}.

Note that classical results on uniform convergence of empirical errors assume that random variables are i.i.d., but our results on the concentration of approximation errors merely assume that the variables are independent. In real applications,  often neither identical distribution nor independence of data hold. In \cite{vksa21}, we investigated correlations of random classifiers under some special dependence conditions.




 \section*{Acknowledgments}
V.K. was partially supported by the Czech Science Foundation grant GA22-02067S, the institutional support of the Institute of Computer
Science RVO 67985807, and a Visiting Professor grant from GNAMPA-INdAM
(Gruppo Nazionale per l’Analisi Matematica, la Probabilità
e le loro Applicazioni - Instituto Nazionale di Alta Matematica). M.S. is a member of
GNAMPA-INdAM and Guest Scholar  at IMT - School for Advanced Studies, Lucca,
Italy. He was supported by the projects PDGP
DIT.AD021.104 of INM-CNR (National Research Council of Italy),
where he is Research Associate,  F/310027/02/X56 of the Italian Ministry of Enterprises and Made in Italy,  PRIN 2022S8XSMY of the Italian Ministry of Research, and PRIN PNRR “MOTUS” (CUP: D53D23017470001), funded by the European Union – Next Generation EU program.

\begin{thebibliography}{10}

\bibitem{also93}
F.~Albertini and E.~D. Sontag.
\newblock For neural networks, function determines form.
\newblock {\em Neural Networks}, 6(7):975--990, 1993.

\bibitem{ba97}
K.~Ball.
\newblock An elementary introduction to modern convex geometry.
\newblock In S.~Levy, editor, {\em Flavors of Geometry}, pages 1--58. Cambridge
  University Press, 1997.

\bibitem{ba93}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock {\em IEEE Trans. on Information Theory}, 39:930--945, 1993.

\bibitem{baal98}
P.~Bartlett, V.~Maiorov, and R.~Meir.
\newblock Almost linear {VC}-dimension bounds for piecewise polynomial
  networks.
\newblock {\em Neural Computation}, 10:2159--2173, 1998.

\bibitem{baal19}
P.~L. Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight {VC}-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock {\em Journal of Machine Learning Research}, 20:1--17, 2019.

\bibitem{baal23}
A.~Bastounis, A.~N. Gorban, A.~C. Hansen, D.~V.~Prokhorov D.~J.~Higham, O.~J.
  Sutton, I.~Y. Tyukin, and Q.~Zhou.
\newblock {\em The boundaries of verifiable accuracy, robustness, and
  generalisation in deep learning}, page 530â€“541.
\newblock Springer Nature Switzerland, 2023.

\bibitem{co65}
T.~Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applictions in pattern recognition.
\newblock {\em IEEE Trans. on Electronic Computers}, 14:326--334, 1965.

\bibitem{do00}
D.~L. Donoho.
\newblock High-dimensional data analysis: the curses and blessings of
  dimensionality.
\newblock {\em AMS Math Challenges of the 21st Century}, pages 1--32, 2000.

\bibitem{dupa09}
D.P. Dubhashi and A.~Panconesi.
\newblock {\em Concentration of Measure for the Analysis of Randomized
  Algorithms}.
\newblock Cambridge University Press, 2009.

\bibitem{goal19}
A.~N. Gorban, V.~Makarov, and I.~Tyukin.
\newblock The unreasonable effectiveness of small neural ensembles in
  high-dimensional brain.
\newblock {\em Physics of Life Review}, 9:55--58, 2019.

\bibitem{goty18}
A.~N. Gorban and I.~Tyukin.
\newblock Blessing of dimensionality: Mathematical foundations of the
  statistical physics of data.
\newblock {\em Philosophical Transactions of Royal Society A}, 376:2017--2037,
  2018.

\bibitem{joli84}
W.~B. Johnson and J.~Lindenstrauss.
\newblock Extensions of {L}ipschitz mappings into a {H}ilbert space.
\newblock {\em Contemporary Mathematics}, 26:189â€“206, 1984.

\bibitem{kavk93}
P.~C. Kainen and V.~K\r{u}rkov\'a.
\newblock Quasiorthogonal dimension of {E}uclidean spaces.
\newblock {\em Applied Mathematical Letters}, 6(3):7--10, 1993.

\bibitem{vk12}
V.~K\r{u}rkov\'{a}.
\newblock Complexity estimates based on integral transforms induced by
  computational units.
\newblock {\em Neural Networks}, 33:160--167, 2012.

\bibitem{vk19}
V.~K\r{u}rkov\'a.
\newblock Some insights from high-dimensional spheres.
\newblock {\em Phys. Life Rev.}, 29:98--100, 2019.

\bibitem{vk23}
V.~K\r{u}rkov\'a.
\newblock Approximation of binary-valued functions by networks of finite {VC}
  dimension.
\newblock In L.~Iliadis et~al., editor, {\em ICANN 2023}, volume 14254 of {\em
  LNCS}, pages 483--490. Springer, 2023.

\bibitem{vkka94}
V.~K\r{u}rkov\'{a} and P.~C. Kainen.
\newblock Functionally equivalent feedforward neural networks.
\newblock {\em Neural Computation}, 6(3):543--558, 1994.

\bibitem{vkka96}
V.~K\r{u}rkov\'{a} and P.~C. Kainen.
\newblock Singularities of finite scaling functions.
\newblock {\em Applied Math Letters}, 9(2):33--37, 1996.

\bibitem{vksa16}
V.~K\r{u}rkov\'{a} and M.~Sanguineti.
\newblock Model complexities of shallow networks representing highly varying
  functions.
\newblock {\em Neurocomputing}, 171:598--604, 2016.

\bibitem{vksa21}
V.~K\r{u}rkov\'{a} and M.~Sanguineti.
\newblock Correlations of random classifiers on large data sets.
\newblock {\em Soft Computing}, 25:12641--12648, 2021.

\bibitem{le01}
M.~Ledoux.
\newblock {\em The Concentration of Measure Phenomenon}.
\newblock AMS, Providence, 2001.

\bibitem{mc89}
C.~McDiarmid.
\newblock On the method of bounded differences.
\newblock In J.~Siemons, editor, {\em Surveys in Combinatorics}, pages
  148--188, Cambridge, 1989. Cambridge University Press.

\bibitem{misc86}
V.D. Milman and G.~Schechtman.
\newblock {\em Asymptotic theory of finite dimensional normed spaces}, volume
  1200 of {\em Lecture Notes in Mathematics}.
\newblock Springer-Verlag, 1986.

\bibitem{sa72}
N.~Sauer.
\newblock On the density of families of sets.
\newblock {\em Journal of Combinatorial Theory}, 13:145â€“147, 1972.

\bibitem{sc01}
L.~Schl\"{a}fli.
\newblock {\em Theorie der vielfachen {K}ontinuit\"{a}t}.
\newblock Z\"{u}rcher \& Furrer, Z\"{u}rich, 1901.

\bibitem{sc50}
L.~Schl\"{a}fli.
\newblock {\em Gesamelte Mathematische Abhandlungen}.
\newblock Birkh\"{a}user, Basel, 1950.

\bibitem{sh72}
S.~Shelah.
\newblock A combinatorial problem; stability and order for models and theories
  in infinitary languages.
\newblock {\em Pacific Journal of Mathematics}, 41:247â€“261, 1972.

\bibitem{su92}
H.~J. Sussman.
\newblock Uniqueness of the weights for minimal feedforward nets with a given
  input-output map.
\newblock {\em Neural Networks}, 5(4):589--593, 1992.

\bibitem{sual23}
O.~J. Sutton, Q.~Zhou, A.~N. Gorban, and I.~Y. Tyukin.
\newblock {\em Relative intrinsic dimensionality is intrinsic toÂ learning},
  pages 516--529.
\newblock Springer Nature Switzerland, 2023.

\bibitem{vach71}
V.~N. Vapnik and A.~Y. Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock {\em Doklady Akademii Nauk}, 16(2):781, 1971.

\bibitem{yazh20}
D.~Yarotsky and A.Zhevnerchuk.
\newblock The phase diagram of approximation rates for deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  30:13005--13015, 2020.

\end{thebibliography}

\end{document}

\bibliographystyle{plain}
\bibliography{vcdim}

\end{document}

