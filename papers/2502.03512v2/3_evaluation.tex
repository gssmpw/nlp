






\section{Empirical Evaluation}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{img/DPO_Axioms_Impact.pdf}
    \caption{
        \textit{Impact of Training DPO with Individual Axioms on Others: A Comparative Evaluation.} 
        The plots illustrate the impact of training DPO to optimize a single axiom on the other alignment objectives. Each subplot corresponds to one axiom, with percentage changes in performance (relative to baseline) shown for all other objectives. For example, training on \emph{Artistic Freedom} improves it by 40\%, but causes notable declines in \emph{Cultural Sensitivity} (-30\%) and \emph{Verifiability} (-35\%), while improving \emph{Faithfulness to Prompt} (+22\%) and \emph{Originality} (+25\%). These results underscore the inherent trade-offs of single-axiom optimization and motivate the need for holistic alignment approaches like CAO.
    }
    \label{fig:dpo_axiom_impact}
\end{figure*}

\begin{figure*}[ht!]
\centering
\includegraphics[width=\textwidth]{img/DPO_vs_DPO_CAO_Impact.pdf}
\caption{Comparison of Alignment Impacts: The plot illustrates the effect of training with DPO versus CAO across six axioms: Artistic Freedom, Faithfulness to Prompt, Emotional Impact, Originality, Cultural Sensitivity, and Verifiability. While DPO exhibits uncontrolled variations in the impacts, leading to undesirable tradeoffs (e.g., +40\% Artistic Freedom but -30\% Cultural Sensitivity), CAO achieves a more balanced alignment with controlled tradeoffs (e.g., +10\% Artistic Freedom and +44\% Cultural Sensitivity). This demonstrates CAO's ability to harmonize competing axioms effectively.}
\label{fig:DPO_vs_DPO_CAO_Impact}
\end{figure*}


\textls[-10]{\textbf{Evaluation Setup and Insights}: Our evaluation examines the limitations of optimizing Directed Preference Optimization (DPO) models on individual alignment objectives. Specifically, we trained six models, each focusing on one axiom: \emph{Artistic Freedom}, \emph{Faithfulness to Prompt}, \emph{Emotional Impact}, \emph{Originality}, \emph{Cultural Sensitivity}, and \emph{Verifiability}. The impact of this single-axiom optimization on the other five objectives was measured in terms of percentage changes compared to a baseline.}

\subsection*{\textls[-10]{Impact of Training DPO with Individual Axioms}}

\begin{itemize}
    \item \textbf{Artistic Freedom}: Training for \emph{Artistic Freedom} resulted in a 40\% improvement, but at the expense of reduced \emph{Cultural Sensitivity} (-30\%) and \emph{Verifiability} (-35\%). \emph{Faithfulness to Prompt} and \emph{Originality} improved by 22\% and 25\%, respectively.
    \item \textbf{Faithfulness to Prompt}: Optimizing for \emph{Faithfulness to Prompt} led to a 40\% improvement but reduced \emph{Artistic Freedom} (-10\%) while marginally improving \emph{Originality} (+10\%) and \emph{Emotional Impact} (+5\%).
    \item \textbf{Emotional Impact}: Training on \emph{Emotional Impact} increased it by 40\%, but resulted in a 20\% decline in \emph{Faithfulness to Prompt} and a 10\% decline in \emph{Cultural Sensitivity}. \emph{Artistic Freedom} increased slightly (+15\%).
    \item \textbf{Originality}: Prioritizing \emph{Originality} improved it by 40\%, but reduced \emph{Cultural Sensitivity} (-25\%) and \emph{Verifiability} (-15\%).
    \item \textbf{Cultural Sensitivity}: Optimizing \emph{Cultural Sensitivity} led to a 40\% improvement, but reduced \emph{Verifiability} (-30\%) and \emph{Originality} (-20\%). \emph{Artistic Freedom} dropped by 15\%.
    \item \textbf{Verifiability}: Training for \emph{Verifiability} resulted in a 40\% improvement but came at the expense of \emph{Originality} (-25\%) and \emph{Cultural Sensitivity} (-30\%). \emph{Faithfulness to Prompt} and \emph{Emotional Impact} saw minor declines of 10\% and 15\%.
\end{itemize}




\textbf{Key Insights:} 
Empirical findings elucidate the inherent limitations of single-axiom DPO training, where optimization bias disrupts inter-axiom equilibria, thereby affirming the necessity of multi-objective strategies such as CAO for holistic alignment. This motivates the need for our proposed CAO, which harmonizes trade-offs across all alignment objectives.

For a detailed discussion of the optimization landscape differences between DPO and CAO, including comparative visualizations of error surfaces, refer to \cref{sec:appendix_error_surface_analysis}. The computational complexity and overhead introduced by the CAO framework, along with strategies to mitigate these challenges, are elaborated in \cref{sec:appendix_complexity_analysis}. Additionally, future avenues for reducing the computational burden of global synergy terms are explored in \cref{sec:appendix_synergy_overhead_reduction}. For an overview of the key hyperparameters, optimization strategies, and architectural configurations used in this work, see \cref{sec:appendix:hyperparams}.








\section{Generalization vs. Overfitting: Effect of Alignment}
\label{sec:HTSR_generalization}

The \textit{Weighted Alpha} metric \cite{martin2021predicting} offers a novel way to assess generalization and overfitting in LLMs without requiring training or test data. Rooted in Heavy-Tailed Self-Regularization (HT-SR) theory, it analyzes the eigenvalue distribution of weight matrices, modeling the Empirical Spectral Density (ESD) as a power-law \(\rho(\lambda) \propto \lambda^{-\alpha}\). Smaller \(\alpha\) values indicate stronger self-regularization and better generalization, while larger \(\alpha\) values signal overfitting. The Weighted Alpha \(\hat{\alpha}\) is computed as:
$\hat{\alpha} = \frac{1}{L} \sum_{l=1}^L \alpha_l \log \lambda_{\max,l}$,
where \(\alpha_l\) and \(\lambda_{\max,l}\) are the power-law exponent and largest eigenvalue of the \(l\)-th layer, respectively. This formulation highlights layers with larger eigenvalues, providing a practical metric to diagnose generalization and overfitting tendencies. Results reported in \cref{fig:htsr_generalization_main}.



\subsubsection*{Research Questions and Key Insights}
\begin{enumerate}
    \item \textbf{\ul{RQ1}: Do aligned T2I models lose generalizability and become overfitted?}  
    Alignment procedures introduce a marginal increase in overfitting, as evidenced by a generalization error drift of \(|\Delta \mathcal{E}_{\text{gen}}| \leq 0.1\), remaining within an acceptable range of \(\pm 10\%\).

    \item \textbf{\ul{RQ2}: Between DPO and CPO, which offers better generalizability?}  
    CAO is only marginally less generalized compared to DPO, demonstrating a minor increase in the generalization gap. However, CAO achieves superior alignment by addressing six complex and contradictory axioms, such as faithfulness, artistic freedom, and cultural sensitivity, which DPO alone cannot comprehensively balance. This trade-off between generalizability and alignment complexity highlights CAO's ability to maintain robust prompt adherence while handling nuanced alignment challenges effectively.
\end{enumerate}




