

% \newpage
\appendix
\section{Appendix}
\label{sec:appendix}

The Appendix serves as a comprehensive supplement to the main content, offering detailed technical justifications, theoretical insights, and experimental evidence that could not be included in the main body due to space constraints. Its purpose is to enhance the clarity, reproducibility, and transparency of the research. This material provides readers with deeper insights into the methodology, empirical results, and theoretical contributions of YinYangAlign. The appendix is organized into the following sections:

\begin{itemize}
    \item \textbf{Annotation Process and Dataset Details:} 
    Detailed explanation of the annotation pipeline, dataset filtering criteria, inter-annotator agreement, and dataset composition. cf \cref{sec:appendix:dataset}.

    \item \textbf{DPO: Contradictory Alignment Optimization (CAO):} 
    Mathematical formulations and explanations of local axiom preferences, global synergy preference, and axiom-specific regularization. cf \cref{sec:appendix:dpo-cao}.

    \item \textbf{Key Hyperparameters, Optimization Strategies, and Architecture Choices:} 
    Descriptions of model hyperparameters, training protocols, and architectural configurations. cf \cref{sec:appendix:hyperparams}.

    \item \textbf{Ablation Studies on Regularization Coefficients (\( \tau_a \)) and Combined Impact of Synergy Weights (\( \omega_a \)) and Regularization Coefficients (\( \tau_a \)):} 
    Analysis of the effects of regularization coefficients and synergy weights on alignment performance and stability. cf \cref{sec:appendix:ablation_regularization_synergy}.

    \item \textbf{Gradient Calculation of DPO-CAO:} 
    Detailed derivations of gradients for DPO-CAO, highlighting the role of synergy weights and regularization terms. cf \cref{sec:appendix_gradient}.

    \item \textbf{Details on the Synergy Jacobian \(\mathbf{J}_{\mathcal{S}}\):} 
    Discussion on the synergy Jacobian's role in regulating gradient interactions among contradictory objectives. cf \cref{sec:appendix_synergy_jacobian}.

    \item \textbf{Why Wasserstein Distance and Sinkhorn Regularization?} 
    Theoretical justifications for choosing these methods, emphasizing their advantages in distributional similarity and computational efficiency. cf \cref{sec:appendix_wasserstein_Sinkhorn}.

    \item \textbf{Comparative Error Surface Analysis for DPO and DPO-CAO:} 
    Visualizations and insights into the differences in optimization landscapes between DPO and DPO-CAO. cf \cref{sec:appendix_error_surface_analysis}.

    \item \textbf{Complexity Analysis and Computational Overhead of DPO-CAO:} 
    Detailed breakdown of the computational cost of DPO-CAO compared to vanilla DPO, with proposed strategies for reducing overhead. cf \cref{sec:appendix_complexity_analysis}.

    \item \textbf{Future Directions for Reducing Global Synergy Overhead:} 
    Discussion of potential methods to mitigate the computational burden introduced by global synergy terms. cf \cref{sec:appendix_synergy_overhead_reduction}.

    \item \textbf{Details on Axiom-Specific Loss Function Design:} 
    Mathematical formulations and theoretical justifications for each axiom-specific loss function, including Faithfulness to Prompt, Artistic Freedom, Emotional Impact, Neutrality, Cultural Sensitivity, Verifiability, and Originality. cf \cref{sec:appendix_axiom_specific_loss}.
\end{itemize}

We encourage readers to refer to the appendix to gain a deeper understanding of the methodologies and findings presented in the main paper.

\section{Annotation Process and Dataset Details}
\label{sec:appendix:dataset}

To construct the YinYang dataset, we carefully selected diverse datasets tailored to each of the six alignment axioms. Specifically:

\begin{itemize}
    \item For the first three axioms—\textbf{Faithfulness to Prompt vs. Artistic Freedom}, \textbf{Emotional Impact vs. Neutrality}, and \textbf{Visual Realism vs. Artistic Freedom}—we utilized the \textbf{MS COCO dataset}~\cite{lin2014microsoft}.
    \item For \textbf{Originality vs. Referentiality}, we leveraged \textbf{Google's Conceptual Captions dataset}~\cite{sharma2018conceptual}.
    \item For \textbf{Verifiability vs. Artistic Freedom}, we selected the \textbf{FACTIFY 3M dataset}~\cite{chakraborty-etal-2023-factify3m}.
    \item For \textbf{Cultural Sensitivity vs. Artistic Freedom}, we employed the \textbf{Facebook Hate Meme Challenge}~\cite{DBLP:journals/corr/abs-2005-04790} and \textbf{Memotion datasets}~\cite{sharma-etal-2020-semeval}, carefully filtering for culturally sensitive data points.
\end{itemize}




\begin{table*}[t!]
\centering
\begin{adjustbox}{max width=\textwidth}
\ttfamily
\begin{tabular}{|p{4cm}|p{12cm}|}
\hline
\textbf{Axiom} & \textbf{Instructions} \\
\hline
Faithfulness to Prompt vs. Artistic Freedom &
Given the textual description (prompt) and an image, evaluate the alignment of the image. \newline
1. Faithfulness to Prompt: Evaluate how well the image adheres to the user's prompt. \newline
2. Artistic Freedom: Assess if the image introduces creative or artistic elements that deviate from, enhance, or reinterpret the original prompt. \newline
3. Identify if artistic freedom significantly compromises faithfulness to the prompt. \newline
\textbf{Output Format:} Faithfulness Score (1-5), Artistic Freedom Score (1-5), Observations (Text). \\
\hline

Emotional Impact vs. Neutrality &
Given the textual description (prompt) and an image, evaluate the alignment of the image. \newline
1. Emotional Impact: Evaluate whether the image conveys specific emotions as implied by the prompt. \newline
2. Neutrality: Assess if the image avoids strong emotional biases and maintains an impartial tone. \newline
3. Identify if the emotional intensity compromises the neutrality required by the prompt. \newline
\textbf{Output Format:} Emotional Impact Score (1-5), Neutrality Score (1-5), Observations (Text). \\
\hline

Visual Realism vs. Artistic Freedom &
Given the textual description (prompt) and an image, evaluate the alignment of the image. \newline
1. Visual Realism: Evaluate how accurately the image replicates real-world visuals, including details, textures, and proportions. \newline
2. Artistic Freedom: Assess if the image introduces artistic or creative elements that deviate from strict realism. \newline
3. Identify if artistic freedom compromises the visual realism implied or required by the prompt. \newline
\textbf{Output Format:} Realism Score (1-5), Artistic Freedom Score (1-5), Observations (Text). \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Instructions for evaluating alignment across six key axioms in Text-to-Image generation, designed for GPT-4.}
\label{tab:axiom_instructions}
\end{table*}


Here are the steps we follow in our annotations process. 

\begin{enumerate}[label=\arabic*., leftmargin=1.5em]

    \item \textbf{Dataset Consolidation:}  
    Collect all captions/prompts and original images from the mentioned datasets to ensure diversity and coverage of the six alignment axioms.

    \item \textbf{Image Generation:}  
    For each prompt, generate 10 images using \textbf{MidJourney 6.0}. This ensures sufficient variation in artistic and realistic interpretations of the same prompt.

    \item \textbf{Preliminary Annotation by Vision-Language Models (VLMs):}
    \begin{itemize}
        \item Annotate all generated images using two VLMs: GPT-4 and LLaVA. See \cref{tab:axiom_instructions}.
        \item Evaluate each image for the six alignment axioms (e.g., Emotional Impact, Visual Realism).  
        \item Retain images where both VLMs give a high score (\(\geq 3\)) for a specific axiom. For example, if both models assign a high score for Emotional Impact, the image is retained for further processing.  
        \item Discard images that fail to achieve a high score from either VLM for any axiom, as well as those where none of the VLMs provide a high score.
        \item For Originality vs. Referentiality, Verifiability vs. Artistic Freedom, and Cultural Sensitivity vs. Artistic Freedom we used automatic methods as discussed in the \cref{sec:axiom_loss}.
        
        \item After this filtering process, approximately \textbf{50K images} remain where \textbf{GPT-4 and LLaVA} agree on a specific axiom.  
    \end{itemize}

    \item \textbf{Human Annotation Process:}  
    \begin{itemize}
        \item Engage \textbf{10 human annotators} for manual evaluation. Each annotator is assigned \textbf{5,500 images} to ensure comprehensive coverage of the dataset.  
        \item Include a \textbf{500-image overlap} between adjacent annotators to calculate inter-annotator agreement and ensure consistency and reliability in the annotations.  
    \end{itemize}

    \item \textbf{Further Filtering During Human Annotation:}  
    \begin{itemize}
        \item Discard approximately \textbf{10K images} during the manual annotation process due to quality issues, such as:  
        \begin{itemize}
            \item Distorted image generation (e.g., unrealistic artifacts).  
            \item Improper color rendering or other significant quality issues.  
        \end{itemize}
    \end{itemize}

    \item \textbf{Final Dataset:}  
    \begin{itemize}
        \item The final \textbf{YinYang dataset} consists of \textbf{40K high-quality datapoints}, carefully selected and annotated for the six alignment axioms.  
        \item This dataset will be released for research purposes, enabling studies in Text-to-Image alignment and related areas.  
    \end{itemize}

\end{enumerate}

This selection ensures a comprehensive and contextually relevant evaluation across all alignment objectives. \cref{tab:yintang_annotation_detailed_examples} presents several detailed examples to enhance the authors' understanding.




\clearpage
\input{examples_data}







\section{DPO: Contradictory Alignment Optimization (CAO)}
\label{sec:appendix:dpo-cao}

Contradictory Alignment Optimization (CAO) is proposed to address the inherent trade-offs in aligning Text-to-Image (T2I) models across six contradictory objectives. These objectives include, for example, \textit{Faithfulness to Prompt vs. Artistic Freedom} or \textit{Emotional Impact vs. Neutrality}. CAO builds upon the Directed Preference Optimization (DPO) framework~\cite{rafailov2024directpreferenceoptimizationlanguage} and introduces a synergy-based approach to unify conflicting alignment goals using multi-objective optimization and Pareto efficiency principles~\cite{miettinen1999nonlinear}. The CAO framework introduces the following key components:

\begin{enumerate}
    \item \textbf{Local Axiom-Wise Loss Functions:}  
    Each alignment axiom (e.g., \textit{Faithfulness to Prompt vs. Artistic Freedom}) is assigned a specific loss function that balances two competing sub-objectives:
    \[
    f_a(I) = \alpha_a L_p(I) + (1 - \alpha_a)L_q(I),
    \]
    where:
    \begin{itemize}
        \item $L_p(I)$ and $L_q(I)$ represent the sub-objectives within an axiom. For example, in \textit{Faithfulness to Prompt vs. Artistic Freedom}, $L_p$ may measure semantic alignment to the prompt, while $L_q$ quantifies stylistic deviation.
        \item $\alpha_a \in [0, 1]$ is a mixing parameter controlling the trade-off for axiom $a$. Larger $\alpha_a$ prioritizes $L_p$, whereas smaller $\alpha_a$ favors $L_q$.
    \end{itemize}

    \item \textbf{Global Synergy Aggregator:}  
    To reconcile multiple axioms, a global synergy function $S(I)$ aggregates the local losses:
    \[
    S(I) = \sum_{a=1}^A \omega_a f_a(I),
    \]
    where:
    \begin{itemize}
        \item $A$ is the total number of axioms (e.g., $A=6$ for the YinYang framework).  
        \item $\omega_a$ represents the priority or weight assigned to each axiom $a$. This parameter allows practitioners to emphasize certain objectives over others depending on the application.
    \end{itemize}

    \item \textbf{Pareto Frontiers:}  
    By varying the weights $\omega_a$, CAO explores Pareto frontiers, which represent sets of non-dominated solutions where improvement in one axiom necessitates a trade-off in another~\cite{deb2001multi}. For example, increasing \textit{Artistic Freedom} may reduce \textit{Faithfulness to Prompt}, but Pareto efficiency ensures that these trade-offs are optimized globally.
\end{enumerate}

\subsection{Unified CAO Loss Function}
The CAO framework integrates both local axiom-wise preferences and global synergy into a single optimization objective, building on the DPO loss formulation~\cite{rafailov2024directpreferenceoptimizationlanguage}:
\[
L_{\text{CAO}} = -\sum_{a=1}^A \sum_{(i, j)} \log(P^a_{ij}) - \lambda \sum_{(i, j)} \log(P^S_{ij}),
\]
where:
\begin{itemize}
    \item $P^a_{ij}$ is the Bradley-Terry preference probability for axiom $a$:
    \[
    P^a_{ij} = \frac{\exp(f_a(I_i))}{\exp(f_a(I_i)) + \exp(f_a(I_j))},
    \]
    ensuring pairwise alignment under axiom $a$.  
    \item $P^S_{ij}$ is the global synergy preference probability:
    \[
    P^S_{ij} = \frac{\exp(S(I_i))}{\exp(S(I_i)) + \exp(S(I_j))}.
    \]
    \item $\lambda$ is a scaling factor controlling the relative importance of local and global preferences. Extended equation is reported in \ref{box:dpo-cao}.
\end{itemize}

\subsection{Axiom-Specific Regularization}
To stabilize optimization and avoid overfitting, CAO incorporates regularization terms for each axiom:
\[
L_{\text{DPO-CAO}} = \sum_{a=1}^A \left[ f_a(I) + \tau_a R_a \right],
\]
where:
\begin{itemize}
    \item $\tau_a$ controls the influence of the regularizer $R_a$ for axiom $a$.  
    \item Common regularizers include Wasserstein Distance~\cite{villani2008optimal} to enforce smoothness in feature space and Sinkhorn regularization~\cite{cuturi2013sinkhorn} for computational efficiency in high-dimensional scenarios.
\end{itemize}




\newpage
\onecolumn

% \centering
\begin{center} % Ensures the box is horizontally centered
\raisebox{0px}[450pt][0pt]{% Adjust the vertical position
% \centering
\begin{adjustbox}{center,angle=90,max width=0.6\textwidth,max totalheight=1.8\textheight}
% \centering
\begin{tcolorbox}[
  enhanced,
  width=3.1\textwidth,
  height=0.85\textheight,
  boxrule=1pt,
  sharp corners,
  colback=white,
  colframe=black,
  center title,
  title={Objective Function for Dual-Phase Optimization with Context-Aware Objectives (CAO)\label{box:dpo-cao}}
]
\centering % Center the content inside the box
\begin{align*}
L_{\text{DPO-CAO}} 
\\ &= - \log \left(
    \frac{\exp(f_{\text{faithArtistic}}(I_i))}
         {\exp(f_{\text{faithArtistic}}(I_i)) + \exp(f_{\text{faithArtistic}}(I_j))}
\right) \\
&\quad - \log \left(
    \frac{\exp(f_{\text{emotionNeutrality}}(I_i))}
         {\exp(f_{\text{emotionNeutrality}}(I_i)) + \exp(f_{\text{emotionNeutrality}}(I_j))}
\right) \\
&\quad - \log \left(
    \frac{\exp(f_{\text{visualStyle}}(I_i))}
         {\exp(f_{\text{visualStyle}}(I_i)) + \exp(f_{\text{visualStyle}}(I_j))}
\right) \\
&\quad - \log \left(
    \frac{\exp(f_{\text{originalityReferentiality}}(I_i))}
         {\exp(f_{\text{originalityReferentiality}}(I_i)) + \exp(f_{\text{originalityReferentiality}}(I_j))}
\right) \\
&\quad - \log \left(
    \frac{\exp(f_{\text{verifiabilityCreative}}(I_i))}
         {\exp(f_{\text{verifiabilityCreative}}(I_i)) + \exp(f_{\text{verifiabilityCreative}}(I_j))}
\right) \\
&\quad - \log \left(
    \frac{\exp(f_{\text{culturalArtistic}}(I_i))}
         {\exp(f_{\text{culturalArtistic}}(I_i)) + \exp(f_{\text{culturalArtistic}}(I_j))}
\right) \\
&\quad - \lambda \log \Biggl(
  \frac{%
    \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_i)
         + \omega_2 f_{\text{emotionNeutrality}}(I_i)
         + \omega_3 f_{\text{visualStyle}}(I_i)
         + \omega_4 f_{\text{originalityReferentiality}}(I_i)
         + \omega_5 f_{\text{verifiabilityCreative}}(I_i)
         + \omega_6 f_{\text{culturalArtistic}}(I_i)\bigr)
  }{%
    \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_i)
         + \omega_2 f_{\text{emotionNeutrality}}(I_i)
         + \omega_3 f_{\text{visualStyle}}(I_i)
         + \omega_4 f_{\text{originalityReferentiality}}(I_i)
         + \omega_5 f_{\text{verifiabilityCreative}}(I_i)
         + \omega_6 f_{\text{culturalArtistic}}(I_i)\bigr)
    +
    \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_j)
         + \omega_2 f_{\text{emotionNeutrality}}(I_j)
         + \omega_3 f_{\text{visualStyle}}(I_j)
         + \omega_4 f_{\text{originalityReferentiality}}(I_j)
         + \omega_5 f_{\text{verifiabilityCreative}}(I_j)
         + \omega_6 f_{\text{culturalArtistic}}(I_j)\bigr)
  }
\Biggr) \\
&\quad + \tau_1 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{faith}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{faith}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
&\quad + \tau_2 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{emotion}}(x)\,Q_{\text{neutrality}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{emotion}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{neutrality}}(y)\,\mathrm{d}y} \\
&\quad + \tau_3 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{realism}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{realism}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
&\quad + \tau_4 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{originality}}(x)\,Q_{\text{referentiality}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{originality}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{referentiality}}(y)\,\mathrm{d}y} \\
&\quad + \tau_5 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{verifiability}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{verifiability}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
&\quad + \tau_6 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
                             P_{\text{cultural}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
       {\int_{\mathcal{X}} P_{\text{cultural}}(x)\,\mathrm{d}x
        \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y}.
\end{align*}
\end{tcolorbox}
\label{box:dpo-cao}
\vspace{-10mm}
\end{adjustbox}
}% End of \raisebox
\end{center}
\clearpage
\newpage
\twocolumn

\subsection{Mathematical Benefits of CAO}
\begin{itemize}
    \item \textbf{Local Interpretability:}  
    Each axiom retains independent interpretability through its loss function, enabling targeted diagnostics for specific trade-offs.
    \item \textbf{Global Consistency:}  
    The synergy-based loss ensures that all axioms are optimized harmoniously, avoiding scenarios where one axiom dominates others.
    \item \textbf{Pareto-Aware Control:}  
    By systematically varying $\omega_a$, CAO provides insights into trade-offs across objectives, ensuring efficient exploration of Pareto frontiers~\cite{deb2001multi}.
    \item \textbf{Computational Scalability:}  
    Leveraging Sinkhorn regularization reduces the computational burden, making CAO applicable to large-scale T2I models.
\end{itemize}




\section{Key Hyperparameters, Optimization Strategies, and Architecture Choices}
\label{sec:appendix:hyperparams}

This section provides details on the key hyperparameters, optimization strategies, and architectural configurations used in training T2I models with the DPO-CAO frameworks.

\subsection{Hyperparameters for Training}
\begin{itemize}
    \item \textbf{Learning Rate:}  
    - For both DPO and DPO-CAO, we use an initial learning rate of \(1 \times 10^{-4}\), with a cosine decay schedule~\cite{loshchilov2016sgdr} applied over the training epochs.  
    - Separate learning rates are employed for the image encoder and text decoder to account for modality-specific training dynamics.  

    \item \textbf{Batch Size:}  
    - A batch size of 256 is used for stable optimization, balancing memory requirements and gradient variance.  
    - For larger datasets, gradient accumulation is employed to mimic an effective batch size of 1024.  

    \item \textbf{Mixing Parameter (\(\alpha_a\)):}  
    - The mixing parameter \(\alpha_a\) governs the trade-off between the two competing sub-objectives for each axiom \(a\). For example, in \textit{Faithfulness to Prompt vs. Artistic Freedom}, \(\alpha_a\) balances the semantic alignment loss (\(L_p\)) and the stylistic deviation loss (\(L_q\)):
    \[
    f_a(I) = \alpha_a L_p(I) + (1 - \alpha_a) L_q(I),
    \]
    where \(\alpha_a \in [0, 1]\). A higher \(\alpha_a\) gives more importance to \(L_p\) (semantic alignment), while a lower \(\alpha_a\) favors \(L_q\) (stylistic deviation).

    - Initially, \(\alpha_a\) is set to 0.5, assigning equal weights to both sub-objectives, ensuring no bias during the early stages of training:
    \[
    \alpha_a^{(0)} = 0.5.
    \]

    - As training progresses, \(\alpha_a\) is dynamically adjusted based on the relative magnitudes of \(L_p\) and \(L_q\). For instance:
        \begin{itemize}
            \item If \(L_p \ll L_q\), indicating that semantic alignment is well-optimized while stylistic deviation is not, \(\alpha_a\) is decreased:
            \[
            \alpha_a^{(t+1)} = \alpha_a^{(t)} - \eta \frac{\partial L_q}{\partial \alpha_a},
            \]
            where \(\eta\) is the learning rate for \(\alpha_a\).  
            \item Conversely, if \(L_q \ll L_p\), \(\alpha_a\) is increased to give higher priority to semantic alignment:
            \[
            \alpha_a^{(t+1)} = \alpha_a^{(t)} + \eta \frac{\partial L_p}{\partial \alpha_a}.
            \]
        \end{itemize}
    - This dynamic adjustment ensures that neither sub-objective is neglected, maintaining balanced optimization across the axiom.

\item \textbf{Weighting Coefficients (\(\omega_a\)):}  
    - The weighting coefficients \(\omega_a\) determine the relative importance of each axiom \(a\) in the global synergy function. Initially, all axioms are assigned equal weights:
    \[
    \omega_a^{(0)} = \frac{1}{A}, \quad \forall a \in \{1, 2, \dots, A\},
    \]
    where \(A\) is the total number of axioms.  

    - During training, \(\omega_a\) is fine-tuned based on validation metrics to prioritize certain axioms for specific applications. The global synergy function is defined as:
    \[
    S(I) = \sum_{a=1}^A \omega_a f_a(I),
    \]
    where \(f_a(I)\) is the axiom-specific loss.  

    - Fine-tuning \(\omega_a\) is performed iteratively by monitoring the validation loss for each axiom:
        \begin{itemize}
            \item If validation metrics for axiom \(a\) show underperformance (e.g., high loss), \(\omega_a\) is increased:
            \[
            \omega_a^{(t+1)} = \omega_a^{(t)} + \eta_\omega \frac{\partial f_a}{\partial \omega_a},
            \]
            where \(\eta_\omega\) is the learning rate for \(\omega_a\).  
            \item Conversely, if axiom \(a\) is overemphasized, \(\omega_a\) is decreased:
            \[
            \omega_a^{(t+1)} = \omega_a^{(t)} - \eta_\omega \frac{\partial f_a}{\partial \omega_a}.
            \]
        \end{itemize}

    - This iterative adjustment ensures the global synergy function achieves balanced trade-offs across all axioms, catering to specific application requirements.


\item \textbf{Regularization Coefficients (\(\tau_a\)):}  
    - Regularization coefficients \(\tau_a\) are introduced to stabilize training and prevent overfitting, especially for high-dimensional sub-objectives. The overall loss function for each axiom \(a\) is regularized as:
    \[
    L_a(I) = f_a(I) + \tau_a R_a(I),
    \]
    where:
    \begin{itemize}
        \item \(f_a(I)\) is the axiom-specific loss (e.g., a weighted combination of sub-objectives such as \(L_p\) and \(L_q\)).
        \item \(R_a(I)\) is the regularization term (e.g., \(L_2\)-norm, Wasserstein distance, or Sinkhorn divergence~\cite{cuturi2013sinkhorn}).  
        \item \(\tau_a > 0\) determines the influence of the regularization term on the total loss.  
    \end{itemize}

    - The regularization coefficients \(\tau_a\) are initialized uniformly across all axioms:
    \[
    \tau_a^{(0)} = \tau_{\text{init}}, \quad \forall a \in \{1, 2, \dots, A\}.
    \]

    - During training, \(\tau_a\) is fine-tuned based on validation performance using hyperparameter sweeps. The updated \(\tau_a\) is adjusted as:
    \[
    \tau_a^{(t+1)} = \tau_a^{(t)} - \eta_\tau \frac{\partial L_{\text{val}}}{\partial \tau_a},
    \]
    where:
    \begin{itemize}
        \item \(L_{\text{val}}\) is the validation loss observed for axiom \(a\).
        \item \(\eta_\tau\) is the learning rate for \(\tau_a\).  
    \end{itemize}

    - Specific tuning of \(\tau_a\) depends on the complexity of the axiom:
        \begin{itemize}
            \item For simpler objectives (e.g., \textit{Faithfulness to Prompt vs. Artistic Freedom}), smaller \(\tau_a\) values are used to avoid underfitting:
            \[
            \tau_a = \tau_{\text{min}}, \quad \text{where } \tau_{\text{min}} \approx 1 \times 10^{-4}.
            \]
            \item For more complex objectives (e.g., \textit{Cultural Sensitivity vs. Artistic Freedom}), larger \(\tau_a\) values are applied to improve robustness:
            \[
            \tau_a = \tau_{\text{max}}, \quad \text{where } \tau_{\text{max}} \approx 1 \times 10^{-2}.
            \]
        \end{itemize}

    - This regularization framework ensures that:
        \begin{itemize}
            \item High-dimensional objectives are smoothed through \(R_a(I)\), reducing sensitivity to noisy gradients.  
            \item The model maintains generalizability across all alignment axioms while optimizing specific alignment goals.  
        \end{itemize}
\end{itemize}

\subsection{Optimization Strategies}
\begin{itemize}
    \item \textbf{Optimizer:}  
    - We use the AdamW optimizer~\cite{loshchilov2017decoupled} with weight decay set to \(1 \times 10^{-2}\).  

    \item \textbf{Gradient Clipping:}  
    - To prevent exploding gradients, gradient clipping is applied with a maximum norm of 1.0.  

    \item \textbf{Loss Scaling:}  
    - Loss scaling is applied to balance the contributions of local axiom-wise losses and the global synergy loss. The scaling factor \(\lambda\) is set to 0.7 based on validation performance.  

    \item \textbf{Pareto Front Exploration:}  
    - To identify optimal trade-offs, Pareto front exploration is conducted by varying synergy weights \(\omega_a\) in the range [0.1, 0.9].  
    - We use scalarization techniques~\cite{deb2001multi} to ensure efficient exploration and selection of Pareto-optimal solutions. 

    

    \begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{img/synergy_weight.png}
    \caption{Weight-Objective Heatmap: Visualizing the impact of varying synergy weights (\(\omega_a\)) on alignment scores across multiple axioms. Each row corresponds to a specific synergy weight, while each column represents an alignment axiom. Lighter colors indicate better alignment, while darker colors reveal areas for improvement.}
    \label{fig:weight_objective_heatmap}
    \end{figure}

    The Weight-Objective Heatmap (see Figure~\ref{fig:weight_objective_heatmap}) is a visual representation of how varying synergy weights (\(\omega_a\)) influences the alignment of a Text-to-Image (T2I) model across multiple axioms. Each row corresponds to a specific synergy weight configuration (\(\omega_a\)), while each column represents an alignment axiom (e.g., Faithfulness to Prompt, Artistic Freedom). The values in each cell indicate the model's objective score for a specific axiom under the corresponding weight configuration. Higher scores (lighter colors) represent better alignment with the axiom, while lower scores (darker colors) suggest areas needing improvement. The plot is constructed by evaluating the model’s performance across a range of weights (\(\omega_a \in [0.1, 0.9]\)) for each axiom, with the scores obtained from validation metrics.

    To interpret the heatmap, examine the rows to identify synergy weight configurations that yield consistent high scores across multiple axioms, indicating balanced trade-offs. Conversely, columns reveal the sensitivity of individual axioms to changes in weights. For example, an axiom with varying scores across rows is more sensitive to weight adjustments, while consistently high scores in a column suggest robustness to weight changes. The highlighted row (red border) indicates the synergy weight configuration that achieves the best overall balance, making it a strong candidate for Pareto-optimal alignment.

    The heatmap’s implication lies in its ability to guide optimization and model refinement. By visualizing trade-offs and sensitivities, it helps practitioners select weights that balance competing objectives, identify challenging axioms needing additional regularization, and prioritize configurations aligned with specific application needs. This tool provides an actionable framework for exploring Pareto-optimal solutions in multi-objective optimization for T2I models.

\end{itemize}

\subsection{Architecture Choices}
\begin{itemize}
    \item \textbf{Image Encoder:}  
    - A pre-trained Vision Transformer (ViT-L/14)~\cite{dosovitskiy2020image} is used as the image encoder, fine-tuned during training for improved alignment with text prompts.  

    \item \textbf{Text Encoder:}  
    - The text encoder is based on a pre-trained T5-Large~\cite{raffel2020exploring} model, leveraging its ability to capture nuanced semantics in natural language prompts.  

    \item \textbf{Synergy Aggregator:}  
    - The synergy function is implemented as a fully connected network with three hidden layers, each containing 512 units and ReLU activation.  
    - Dropout~\cite{srivastava2014dropout} with a probability of 0.2 is applied to prevent overfitting.  

    \item \textbf{Loss Module:}  
    - Both local axiom-wise losses (\(L_p, L_q\)) and the global synergy loss (\(S(I)\)) are implemented with efficient Sinkhorn iterations for computational efficiency~\cite{cuturi2013sinkhorn}.  
\end{itemize}

\subsection{Training Pipeline}
\begin{enumerate}
    \item Pre-train the T2I model using standard cross-entropy loss on the training dataset to initialize the image and text encoders.  
    \item Fine-tune the model with the CAO objective:
    \begin{itemize}
        \item Use local axiom-wise losses (\(f_a(I)\)) to ensure alignment for each axiom.  
        \item Aggregate losses with the synergy function (\(S(I)\)) for global optimization.  
    \end{itemize}
    \item Monitor alignment metrics (e.g., faithfulness scores, emotional impact) on a validation set and adjust hyperparameters (e.g., \(\alpha_a, \omega_a\)) to ensure balanced performance.  
    \item Use early stopping based on the validation loss to prevent overfitting.  
\end{enumerate}

\subsection{Computational Resources}
\begin{itemize}
    \item Training is conducted on NVIDIA A100 GPUs with 40 GB memory. A full training run (including hyperparameter tuning) requires approximately 72 hours.  
    \item Mixed precision training is employed to accelerate computation and reduce memory usage.  
\end{itemize}

\subsection{Key Observations}
\begin{itemize}
    \item Dynamic adjustment of \(\alpha_a\) and \(\omega_a\) significantly improves trade-offs between contradictory objectives.  
    \item Regularization and gradient clipping stabilize the training process, especially in high-dimensional spaces.  
    \item The synergy aggregator effectively balances local and global objectives, resulting in robust alignment across all six axioms.  
\end{itemize}



% \begin{tcolorbox}[
%   width=0.8\textwidth,   % Set the box to 80% of the text width
%   colback=blue!10,       % Light blue background
%   colframe=blue!80!black,% Darker blue frame
%   boxrule=1pt,           % Frame thickness
%   sharp corners          % Square corners
% ]

% \begin{align*}
% L_{\text{DPO-CAO}} 
% \\ &= - \log \left(
%     \frac{\exp(f_{\text{faithArtistic}}(I_i))}
%          {\exp(f_{\text{faithArtistic}}(I_i)) + \exp(f_{\text{faithArtistic}}(I_j))}
% \right) \\
% &\quad - \log \left(
%     \frac{\exp(f_{\text{emotionNeutrality}}(I_i))}
%          {\exp(f_{\text{emotionNeutrality}}(I_i)) + \exp(f_{\text{emotionNeutrality}}(I_j))}
% \right) \\
% &\quad - \log \left(
%     \frac{\exp(f_{\text{visualStyle}}(I_i))}
%          {\exp(f_{\text{visualStyle}}(I_i)) + \exp(f_{\text{visualStyle}}(I_j))}
% \right) \\
% &\quad - \log \left(
%     \frac{\exp(f_{\text{originalityReferentiality}}(I_i))}
%          {\exp(f_{\text{originalityReferentiality}}(I_i)) + \exp(f_{\text{originalityReferentiality}}(I_j))}
% \right) \\
% &\quad - \log \left(
%     \frac{\exp(f_{\text{verifiabilityCreative}}(I_i))}
%          {\exp(f_{\text{verifiabilityCreative}}(I_i)) + \exp(f_{\text{verifiabilityCreative}}(I_j))}
% \right) \\
% &\quad - \log \left(
%     \frac{\exp(f_{\text{culturalArtistic}}(I_i))}
%          {\exp(f_{\text{culturalArtistic}}(I_i)) + \exp(f_{\text{culturalArtistic}}(I_j))}
% \right) \\
% &\quad - \lambda \log \Biggl(
%   \frac{%
%     \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_i)
%          + \omega_2 f_{\text{emotionNeutrality}}(I_i)
%          + \omega_3 f_{\text{visualStyle}}(I_i)
%          + \omega_4 f_{\text{originalityReferentiality}}(I_i)
%          + \omega_5 f_{\text{verifiabilityCreative}}(I_i)
%          + \omega_6 f_{\text{culturalArtistic}}(I_i)\bigr)
%   }{%
%     \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_i)
%          + \omega_2 f_{\text{emotionNeutrality}}(I_i)
%          + \omega_3 f_{\text{visualStyle}}(I_i)
%          + \omega_4 f_{\text{originalityReferentiality}}(I_i)
%          + \omega_5 f_{\text{verifiabilityCreative}}(I_i)
%          + \omega_6 f_{\text{culturalArtistic}}(I_i)\bigr)
%     +
%     \exp\bigl(\omega_1 f_{\text{faithArtistic}}(I_j)
%          + \omega_2 f_{\text{emotionNeutrality}}(I_j)
%          + \omega_3 f_{\text{visualStyle}}(I_j)
%          + \omega_4 f_{\text{originalityReferentiality}}(I_j)
%          + \omega_5 f_{\text{verifiabilityCreative}}(I_j)
%          + \omega_6 f_{\text{culturalArtistic}}(I_j)\bigr)
%   }
% \Biggr) \\
% &\quad + \tau_1 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{faith}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{faith}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
% &\quad + \tau_2 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{emotion}}(x)\,Q_{\text{neutrality}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{emotion}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{neutrality}}(y)\,\mathrm{d}y} \\
% &\quad + \tau_3 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{realism}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{realism}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
% &\quad + \tau_4 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{originality}}(x)\,Q_{\text{referentiality}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{originality}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{referentiality}}(y)\,\mathrm{d}y} \\
% &\quad + \tau_5 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{verifiability}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{verifiability}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y} \\
% &\quad + \tau_6 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\|\,
%                              P_{\text{cultural}}(x)\,Q_{\text{artistic}}(y)\,\mathrm{d}x\,\mathrm{d}y}
%        {\int_{\mathcal{X}} P_{\text{cultural}}(x)\,\mathrm{d}x
%         \;\cdot\; \int_{\mathcal{X}} Q_{\text{artistic}}(y)\,\mathrm{d}y}.
% \end{align*}
% \end{tcolorbox}

%\clearpage

% \begin{aligned}
% L_{\text{DPO-CAO}} = 
% & - \log \left( \frac{\exp(f_{\text{faithArtistic}}(I_i))}{\exp(f_{\text{faithArtistic}}(I_i)) + \exp(f_{\text{faithArtistic}}(I_j))} \right) \\
% & - \log \left( \frac{\exp(f_{\text{emotionNeutrality}}(I_i))}{\exp(f_{\text{emotionNeutrality}}(I_i)) + \exp(f_{\text{emotionNeutrality}}(I_j))} \right) \\
% & - \log \left( \frac{\exp(f_{\text{visualStyle}}(I_i))}{\exp(f_{\text{visualStyle}}(I_i)) + \exp(f_{\text{visualStyle}}(I_j))} \right) \\
% & - \log \left( \frac{\exp(f_{\text{originalityReferentiality}}(I_i))}{\exp(f_{\text{originalityReferentiality}}(I_i)) + \exp(f_{\text{originalityReferentiality}}(I_j))} \right) \\
% & - \log \left( \frac{\exp(f_{\text{verifiabilityCreative}}(I_i))}{\exp(f_{\text{verifiabilityCreative}}(I_i)) + \exp(f_{\text{verifiabilityCreative}}(I_j))} \right) \\
% & - \log \left( \frac{\exp(f_{\text{culturalArtistic}}(I_i))}{\exp(f_{\text{culturalArtistic}}(I_i)) + \exp(f_{\text{culturalArtistic}}(I_j))} \right) \\
% & - \lambda \log \left( 
% \frac{\exp\left(\omega_1 f_{\text{faithArtistic}}(I_i) + \omega_2 f_{\text{emotionNeutrality}}(I_i) + \omega_3 f_{\text{visualStyle}}(I_i) + \omega_4 f_{\text{originalityReferentiality}}(I_i) + \omega_5 f_{\text{verifiabilityCreative}}(I_i) + \omega_6 f_{\text{culturalArtistic}}(I_i)\right)}
% {\exp\left(\omega_1 f_{\text{faithArtistic}}(I_i) + \omega_2 f_{\text{emotionNeutrality}}(I_i) + \omega_3 f_{\text{visualStyle}}(I_i) + \omega_4 f_{\text{originalityReferentiality}}(I_i) + \omega_5 f_{\text{verifiabilityCreative}}(I_i) + \omega_6 f_{\text{culturalArtistic}}(I_i)\right) 
% + \exp\left(\omega_1 f_{\text{faithArtistic}}(I_j) + \omega_2 f_{\text{emotionNeutrality}}(I_j) + \omega_3 f_{\text{visualStyle}}(I_j) + \omega_4 f_{\text{originalityReferentiality}}(I_j) + \omega_5 f_{\text{verifiabilityCreative}}(I_j) + \omega_6 f_{\text{culturalArtistic}}(I_j)\right)}
% }
% \right) \\
% & + \tau_1 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{faith}}(x) Q_{\text{artistic}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{faith}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{artistic}}(y) \, \mathrm{d}y} \\
% & + \tau_2 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{emotion}}(x) Q_{\text{neutrality}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{emotion}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{neutrality}}(y) \, \mathrm{d}y} \\
% & + \tau_3 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{realism}}(x) Q_{\text{artistic}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{realism}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{artistic}}(y) \, \mathrm{d}y} \\
% & + \tau_4 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{originality}}(x) Q_{\text{referentiality}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{originality}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{referentiality}}(y) \, \mathrm{d}y} \\
% & + \tau_5 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{verifiability}}(x) Q_{\text{artistic}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{verifiability}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{artistic}}(y) \, \mathrm{d}y} \\
% & + \tau_6 \cdot \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_{\text{cultural}}(x) Q_{\text{artistic}}(y) \, \mathrm{d}x \, \mathrm{d}y}
% {\int_{\mathcal{X}} P_{\text{cultural}}(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_{\text{artistic}}(y) \, \mathrm{d}y}.
% \end{aligned}



%\pagebreak


\section{Ablation Studies on Regularization Coefficients (\( \tau_a \)) and Combined Impact of Synergy Weights (\( \omega_a \)) and Regularization Coefficients (\( \tau_a \))}
\label{sec:appendix:ablation_regularization_synergy}

This section presents a detailed analysis of the impact of regularization coefficients (\( \tau_a \)) and their interaction with synergy weights (\( \omega_a \)) on the alignment performance and optimization landscape of DPO-CAO. These parameters jointly influence alignment quality, stability, and computational efficiency.

\subsection{Regularization Coefficients (\( \tau_a \))}

The regularization coefficients control the influence of axiom-specific regularizers in the overall loss function. By varying \( \tau_a \), we evaluate its role in balancing alignment stability and performance.

\paragraph{Experimental Setup.}
\begin{itemize}
    \item \textbf{Baseline Configuration:} All regularization coefficients are initialized to \( \tau_a = 10^{-3} \).
    \item \textbf{Perturbation:} Individual coefficients (\( \tau_a \)) are varied across a logarithmic scale (\( 10^{-4} \) to \( 10^{-1} \)) while keeping others constant.
    \item \textbf{Metrics Evaluated:}
    \begin{itemize}
        \item \textbf{Alignment Stability:} Variance in alignment scores over epochs.
    \end{itemize}
\end{itemize}

\paragraph{Results.}
\begin{table}[ht!]
\centering
\caption{Impact of Regularization Coefficients (\( \tau_a \)) on Alignment Stability.}
\label{tab:regularization_ablation}
\begin{tabular}{|c|c|}
\hline
\textbf{\( \tau_a \)} & \textbf{Alignment Stability (Variance)} \\ \hline
\( 10^{-4} \)         & High (0.15)                            \\ 
\( 10^{-3} \)         & Low (0.05)                             \\ 
\( 10^{-2} \)         & Medium (0.10)                          \\ \hline
\end{tabular}
\end{table}

\paragraph{Insights.}
\begin{itemize}
    \item \textbf{Under-Regularization (\( \tau_a = 10^{-4} \)):} Leads to unstable gradients and high variance in alignment scores.
    \item \textbf{Optimal Regularization (\( \tau_a = 10^{-3} \)):} Balances gradient stability and alignment performance.
    \item \textbf{Over-Regularization (\( \tau_a = 10^{-2} \)):} Excessive smoothing reduces alignment performance.
\end{itemize}

\subsection{Combined Impact of Synergy Weights (\( \omega_a \)) and Regularization Coefficients (\( \tau_a \))}

The interaction between \( \omega_a \) and \( \tau_a \) is critical for achieving balanced alignment. Synergy weights prioritize specific axioms, while regularization coefficients stabilize optimization across competing objectives.

\paragraph{Experimental Setup.}
\begin{itemize}
    \item Conduct grid searches across \( \omega_a \) (\( 0.1, 1/6, 0.5 \)) and \( \tau_a \) (\( 10^{-4}, 10^{-3}, 10^{-2} \)).
    \item \textbf{Metrics Evaluated:}
    \begin{itemize}
        \item \textbf{Alignment Trade-offs:} Differences in primary and secondary objective scores.
    \end{itemize}
\end{itemize}

\paragraph{Results.}
\begin{table}[ht!]
\centering
\caption{Combined Impact of Synergy Weights (\( \omega_a \)) and Regularization Coefficients (\( \tau_a \)) on Alignment Performance.}
\label{tab:combined_ablation}
\begin{tabular}{|c|c|c|}
\hline
\textbf{\( \omega_a \)} & \textbf{\( \tau_a \)} & \textbf{Trade-off Deviation} \\ \hline
\( 1/6 \)               & \( 10^{-3} \)         & 0.05                         \\ 
\( 0.5 \)               & \( 10^{-3} \)         & 0.15                         \\ 
\( 0.1 \)               & \( 10^{-3} \)         & 0.10                         \\ 
\( 1/6 \)               & \( 10^{-4} \)         & 0.12                         \\ 
\( 1/6 \)               & \( 10^{-2} \)         & 0.08                         \\ \hline
\end{tabular}
\end{table}

\paragraph{Insights.}
\begin{itemize}
    \item \textbf{Balanced Configuration (\( \omega_a = 1/6, \tau_a = 10^{-3} \)):} Minimizes alignment trade-offs and demonstrates robust performance across all axioms.
    \item \textbf{Skewed Synergy Weights (\( \omega_a = 0.5 \)):} Prioritizes specific objectives but increases trade-off deviations.
    \item \textbf{Suboptimal Regularization (\( \tau_a = 10^{-4} \) or \( \tau_a = 10^{-2} \)):} Either destabilizes gradients or overly smooths the loss landscape, reducing overall efficiency.
\end{itemize}

\textbf{Conclusion}: The synergy weights (\( \omega_a \)) and regularization coefficients (\( \tau_a \)) play complementary roles in shaping the optimization landscape of DPO-CAO:
\begin{itemize}
    \item \textbf{Synergy Weights:} Control the prioritization of axioms and influence alignment trade-offs.
    \item \textbf{Regularization Coefficients:} Stabilize gradients and ensure consistent updates.
\end{itemize}

Balanced configurations (\( \omega_a = 1/6 \), \( \tau_a = 10^{-3} \)) consistently achieve the best trade-offs. Future work could explore adaptive mechanisms to dynamically adjust these parameters for improved scalability and alignment quality.




\section{Gradient Calculation of CAO}
\label{sec:appendix_gradient}

The DPO-CAO loss function consists of three components: \textit{Local Axiom Preferences}, \textit{Global Synergy Preference}, and \textit{Axiom-Specific Regularizers}. The gradient for each component is derived as follows:

\subsection{Local Axiom Preferences}
The local alignment loss for each axiom $a$ is given by:
\[
L_{\text{Local}} = - \sum_{(i, j) \in \mathcal{P}_{a}} \log \left( \frac{\exp(f_a(I_i))}{\exp(f_a(I_i)) + \exp(f_a(I_j))} \right),
\]
where $f_a(I)$ is the model’s output for axiom $a$. The gradient with respect to $f_a(I_i)$ is:
\[
\frac{\partial L_{\text{Local}}}{\partial f_a(I_i)} = \sum_{(i, j) \in \mathcal{P}_{a}} \left( \frac{\exp(f_a(I_i))}{\exp(f_a(I_i)) + \exp(f_a(I_j))} - 1 \right).
\]
For $f_a(I_j)$, the gradient is:
\[
\frac{\partial L_{\text{Local}}}{\partial f_a(I_j)} = \sum_{(i, j) \in \mathcal{P}_{a}} \frac{\exp(f_a(I_j))}{\exp(f_a(I_i)) + \exp(f_a(I_j))}.
\]
Finally, the gradient with respect to the model parameters $\theta$ is:
\[
\frac{\partial L_{\text{Local}}}{\partial \theta} = \sum_a \frac{\partial L_{\text{Local}}}{\partial f_a(I)} \cdot \frac{\partial f_a(I; \theta)}{\partial \theta}.
\]

\subsection{Global Synergy Preference}
The global synergy loss aggregates the axiom-specific preferences:
\[
L_{\text{Global}} = - \lambda \sum_{(i, j) \in \mathcal{P}_S} \log \left( \frac{\exp\left( \sum_a \omega_a f_a(I_i) \right)}{\exp\left( \sum_a \omega_a f_a(I_i) \right) + \exp\left( \sum_a \omega_a f_a(I_j) \right)} \right).
\]
Define $z_i = \sum_a \omega_a f_a(I_i)$ and $z_j = \sum_a \omega_a f_a(I_j)$. The gradient with respect to $z_i$ is:
\[
\frac{\partial L_{\text{Global}}}{\partial z_i} = \lambda \sum_{(i, j) \in \mathcal{P}_S} \left( \frac{\exp(z_i)}{\exp(z_i) + \exp(z_j)} - 1 \right).
\]
Using $z_i = \sum_a \omega_a f_a(I_i)$, the gradient with respect to $f_a(I_i)$ becomes:
\[
\frac{\partial L_{\text{Global}}}{\partial f_a(I_i)} = \lambda \omega_a \sum_{(i, j) \in \mathcal{P}_S} \left( \frac{\exp(z_i)}{\exp(z_i) + \exp(z_j)} - 1 \right).
\]
Finally, the gradient with respect to $\theta$ is:
\[
\frac{\partial L_{\text{Global}}}{\partial \theta} = \sum_a \frac{\partial L_{\text{Global}}}{\partial f_a(I)} \cdot \frac{\partial f_a(I; \theta)}{\partial \theta}.
\]

\subsection{Axiom-Specific Regularizers}
The regularizer for axiom $a$ is:
\[
\mathcal{R}_a = \frac{\int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_a(x) Q_a(y) \, \mathrm{d}x \, \mathrm{d}y}{\int_{\mathcal{X}} P_a(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_a(y) \, \mathrm{d}y}.
\]
The gradient with respect to $P_a(x)$ is derived using the quotient rule:
\[
\frac{\partial \mathcal{R}_a}{\partial P_a(x)} = \frac{\|x - y\| Q_a(y)}{\int_{\mathcal{X}} P_a(x) \, \mathrm{d}x \cdot \int_{\mathcal{X}} Q_a(y) \, \mathrm{d}y} - \frac{\mathcal{R}_a}{\int_{\mathcal{X}} P_a(x) \, \mathrm{d}x}.
\]
The total gradient with respect to $\theta$ is:
\[
\frac{\partial L_{\text{Regularization}}}{\partial \theta} = \sum_a \tau_a \cdot \frac{\partial \mathcal{R}_a}{\partial P_a(x)} \cdot \frac{\partial P_a(x; \theta)}{\partial \theta}.
\]

\subsection{Final Gradient}
Combining all components, the total gradient is:
\[
\frac{\partial L_{\text{DPO-CAO}}}{\partial \theta} = \frac{\partial L_{\text{Local}}}{\partial \theta} + \frac{\partial L_{\text{Global}}}{\partial \theta} + \frac{\partial L_{\text{Regularization}}}{\partial \theta}.
\]
This gradient is used to update the model parameters during training, ensuring alignment with the specified axioms and global synergy preferences.


\section{Details on the Synergy Jacobian \(\mathbf{J}_{\mathcal{S}}\)}
\label{sec:appendix_synergy_jacobian}

The synergy Jacobian \(\mathbf{J}_{\mathcal{S}}\) plays a pivotal role in the CAO framework by regulating the interactions among gradients of axiom-specific losses. This mechanism ensures that updates to one axiom’s parameters do not excessively disrupt the optimization of others, fostering a balanced alignment process.


\subsection{Definition and Mathematical Formulation}
The synergy Jacobian is defined as the matrix of partial derivatives of the synergy aggregator \(\mathcal{S}(I)\) with respect to the model parameters \(\theta\):
\[
\mathbf{J}_{\mathcal{S}} = 
\begin{bmatrix}
\frac{\partial \mathcal{S}}{\partial \theta_1} & \frac{\partial \mathcal{S}}{\partial \theta_2} & \cdots & \frac{\partial \mathcal{S}}{\partial \theta_p}
\end{bmatrix}^\top
=
\begin{bmatrix}
\frac{\partial f_1}{\partial \theta_1} & \frac{\partial f_2}{\partial \theta_1} & \cdots & \frac{\partial f_A}{\partial \theta_1} \\
\frac{\partial f_1}{\partial \theta_2} & \frac{\partial f_2}{\partial \theta_2} & \cdots & \frac{\partial f_A}{\partial \theta_2} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_1}{\partial \theta_p} & \frac{\partial f_2}{\partial \theta_p} & \cdots & \frac{\partial f_A}{\partial \theta_p}
\end{bmatrix},
\]
where:
\begin{itemize}
    \item \(f_a(I)\) is the axiom-specific loss for axiom \(a\).
    \item \(A\) is the total number of axioms.
    \item \(\theta = \{\theta_1, \theta_2, \dots, \theta_p\}\) are the model parameters.
\end{itemize}

This matrix captures how changes to model parameters \(\theta\) affect the combined synergy score \(\mathcal{S}(I)\), which aggregates all axiom-specific losses.


\subsection{Role in Gradient Interaction and Balancing}
During training, the synergy Jacobian provides a mechanism for tempering gradient updates:
\[
\Delta \theta = -\eta \cdot \mathbf{J}_{\mathcal{S}} \cdot \nabla \mathcal{S},
\]
where:
\begin{itemize}
    \item \(\eta\) is the learning rate.
    \item \(\nabla \mathcal{S} = \sum_{a=1}^A \omega_a \nabla f_a\) is the weighted sum of axiom-specific gradients.
    \item \(\mathbf{J}_{\mathcal{S}}\) modulates the step size and direction of \(\Delta \theta\), preventing dominance by any single axiom.
\end{itemize}


\subsection{Regulating Gradient Conflicts}
Gradient conflicts arise when updates for one axiom-specific loss degrade the performance of others. The synergy Jacobian resolves these conflicts by:
\begin{itemize}
    \item \textbf{Gradient Scaling:} Adjusting the magnitude of conflicting gradients based on the entries in \(\mathbf{J}_{\mathcal{S}}\).
    \item \textbf{Conflict Minimization:} Encouraging updates that align gradients across axioms by minimizing the off-diagonal terms in \(\mathbf{J}_{\mathcal{S}}\), which represent inter-axiom interactions.
    \item \textbf{Trade-off Control:} Balancing competing objectives by regularizing the Frobenius norm of \(\mathbf{J}_{\mathcal{S}}\):
    \[
    \mathcal{R}_{\text{jacobian}} = \lambda_{\text{jac}} \|\mathbf{J}_{\mathcal{S}} - \mathbf{I}\|_F^2,
    \]
    where \(\mathbf{I}\) is the identity matrix, and \(\lambda_{\text{jac}}\) controls the regularization strength.
\end{itemize}


\subsection{Numerical Stability and Implementation}
To ensure numerical stability during computation:
\begin{itemize}
    \item \textbf{Gradient Clipping:} Limit the maximum magnitude of individual entries in \(\mathbf{J}_{\mathcal{S}}\) to prevent exploding gradients.
    \item \textbf{Efficient Backpropagation:} Use automatic differentiation frameworks to compute \(\mathbf{J}_{\mathcal{S}}\) efficiently without explicitly storing the entire matrix.
    \item \textbf{Sparse Approximations:} In high-dimensional models, approximate \(\mathbf{J}_{\mathcal{S}}\) using block-diagonal structures to reduce computational overhead.
\end{itemize}

\subsection{Key Insights and Implications}
The synergy Jacobian \(\mathbf{J}_{\mathcal{S}}\) provides the following benefits:
\begin{itemize}
    \item \textbf{Improved Convergence:} By moderating gradient conflicts, it stabilizes training and accelerates convergence.
    \item \textbf{Balanced Alignment:} Ensures that no single axiom-specific objective dominates the optimization process.
    \item \textbf{Generalizability:} Encourages parameter updates that benefit multiple objectives simultaneously, leading to better generalization across diverse tasks.
\end{itemize}


\subsection{Future Directions}
While the synergy Jacobian has demonstrated its effectiveness in CAO, potential areas for further research include:
\begin{itemize}
    \item \textbf{Dynamic Weighting Mechanisms:} Incorporate adaptive strategies for weighting axiom-specific gradients based on their contributions to \(\mathcal{S}(I)\).
    \item \textbf{Low-Rank Approximations:} Explore low-rank factorization techniques to make \(\mathbf{J}_{\mathcal{S}}\) computation feasible for large-scale models.
\end{itemize}



\section{Why Wasserstein Distance and Sinkhorn Regularization?}
\label{sec:appendix_wasserstein_Sinkhorn}

The choice of Wasserstein Distance~\cite{arjovsky2017wasserstein} and Sinkhorn Regularization~\cite{cuturi2013sinkhorn} in the alignment framework is motivated by their mathematical robustness, practical scalability, and suitability for high-dimensional tasks like Text-to-Image (T2I) generation. This section elaborates on the advantages of these techniques in the context of aligning generated images with user prompts.


\subsection{Advantages of Wasserstein Distance}

The Wasserstein Distance, also known as the Earth Mover’s Distance (EMD), is a measure of the cost required to transform one probability distribution into another. Its key advantages include:

\begin{itemize}
    \item \textbf{Semantic Alignment:} Wasserstein Distance considers the underlying geometry of distributions, making it well-suited for tasks where latent spaces capture semantic relationships between prompts and images.
    \item \textbf{Handling Disjoint Supports:} Unlike divergence-based metrics (e.g., KL divergence), Wasserstein Distance remains well-defined when distributions have disjoint supports. This property is particularly useful in early training stages of T2I systems, where generated distributions may not overlap with target distributions.
    \item \textbf{Gradient Robustness:} Wasserstein Distance provides meaningful gradients even for distributions with minimal overlap, avoiding gradient vanishing issues that occur with some other metrics.
    \item \textbf{Interpretability:} The metric’s interpretation as the “minimal cost” of transforming one distribution into another aligns with intuitive notions of alignment and quality in T2I systems.
\end{itemize}


\subsection{Advantages of Sinkhorn Regularization}

While Wasserstein Distance offers significant benefits, its computation can be expensive for high-dimensional data. The \textbf{Sinkhorn Regularization} modifies the computation of Wasserstein Distance by introducing an entropic term, resulting in several practical benefits:

\begin{itemize}
    \item \textbf{Computational Efficiency:} The entropic regularization reformulates the Wasserstein computation into a differentiable optimization problem, significantly reducing computational cost from \(O(n^3)\) to \(O(n^2 \log n)\) for \(n\) data points.
    \item \textbf{Smoothness:} Sinkhorn Regularization ensures smoothness in the loss surface, leading to more stable gradients and improved convergence during training.
    \item \textbf{Scalability:} The approximate computation enabled by Sinkhorn Regularization allows alignment optimization at scale, making it suitable for real-world T2I applications with large datasets.
    \item \textbf{Numerical Stability:} By adding entropy to the transport problem, Sinkhorn Regularization mitigates numerical instabilities caused by small values or noise in probability distributions.
    \item \textbf{Flexibility:} The regularization coefficient \(\lambda\) provides a tunable parameter, allowing the trade-off between exact Wasserstein Distance and entropy-regularized divergence. This flexibility accommodates tasks of varying complexity.
\end{itemize}


\subsection{Combined Benefits for T2I Systems}

The synergy of Wasserstein Distance and Sinkhorn Regularization offers the following combined benefits for T2I alignment:

\begin{itemize}
    \item \textbf{Nuanced Semantic Alignment:} Wasserstein Distance captures subtle semantic relationships between textual prompts and generated images, ensuring high-quality alignment.
    \item \textbf{Efficient and Scalable Optimization:} Sinkhorn Regularization enables the use of Wasserstein-based metrics in large-scale training, overcoming the computational bottlenecks of exact Wasserstein computation.
    \item \textbf{Robustness to Variability:} The combined approach handles variability and noise in generated images without compromising alignment quality, making it ideal for multi-axiom optimization frameworks like CAO.
\end{itemize}

\subsection{Applications and Future Directions}

Wasserstein Distance with Sinkhorn Regularization has shown significant promise in T2I alignment, and future research could explore:
\begin{itemize}
    \item \textbf{Dynamic Regularization:} Adaptive tuning of the Sinkhorn regularization coefficient \(\lambda\) during training to balance computational efficiency with alignment accuracy.
    \item \textbf{Multimodal Extensions:} Extending the framework to jointly optimize text, image, and audio embeddings using Wasserstein-based metrics.
    \item \textbf{Task-Specific Optimizations:} Developing tailored variants of Wasserstein Distance for specific domains, such as cultural sensitivity or emotional impact.
\end{itemize}







\section{Comparative Error Surface Analysis for DPO and CAO}
\label{sec:appendix_error_surface_analysis}

In this section, we present a detailed analysis of the error surfaces for Vanilla DPO and CAO to illustrate the impact of introducing axiom-specific losses and synergy terms in the optimization process.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{img/DPO_vs_DPO_CAO_Error_Surface_600dpi.pdf}
    \caption{Error Surfaces for Vanilla DPO (Left) and CAO (Right). The smooth surface of DPO contrasts with the oscillatory patterns in CAO, reflecting the increased complexity due to multi-objective optimization.}
    \label{fig:error_surfaces}
\end{figure*}


\subsection{Error Surface Visualization}
The plots in Figure~\ref{fig:error_surfaces} showcase the error surfaces of DPO and CAO, modeled using synthetic data. These surfaces provide an intuitive understanding of the optimization landscapes.

\begin{itemize}
    \item \textbf{Vanilla DPO (Left Plot):}
    \begin{itemize}
        \item The error surface is smooth and convex, reflecting the simplicity of the optimization objective.
        \item It represents a single loss function consisting of the contrastive loss and a regularization term (e.g., KL divergence).
        \item This smoothness facilitates faster convergence, as the gradients are consistent and straightforward to follow.
    \end{itemize}

    \item \textbf{DPO-CAO (Right Plot):}
    \begin{itemize}
        \item The error surface is characterized by oscillatory patterns, introduced by axiom-specific losses and the global synergy term.
        \item These peaks and valleys highlight the trade-offs between contradictory alignment objectives, such as Faithfulness to Prompt vs. Artistic Freedom or Emotional Impact vs. Neutrality.
        \item The oscillations also reflect the interactions between local axiom preferences and the synergy aggregator, making the optimization process more complex.
    \end{itemize}
\end{itemize}

\subsection{Interpretation of the Error Surfaces}
\begin{itemize}
    \item \textbf{Vanilla DPO:} The smooth surface demonstrates a simpler optimization landscape, suitable for single-objective alignment tasks.
    \item \textbf{DPO-CAO:} The oscillatory nature illustrates the challenges of multi-objective optimization. These oscillations:
    \begin{itemize}
        \item Indicate regions where specific axioms dominate or interact strongly with others.
        \item Highlight the need for careful tuning of synergy weights (\(\omega_a\)) and regularization coefficients (\(\tau_a\)).
    \end{itemize}
\end{itemize}

\subsection{Implications}
\begin{itemize}
    \item \textbf{Optimization Complexity:} The increased oscillations in DPO-CAO suggest a higher computational overhead, as gradient steps must navigate more complex regions.
    \item \textbf{Alignment Trade-offs:} The peaks and valleys provide insights into how competing objectives can influence model behavior, requiring systematic exploration of Pareto-optimal solutions.
    \item \textbf{Guidance for Future Research:} The visualization motivates the need for lightweight synergy models or adaptive axiom prioritization to reduce computational overhead while maintaining alignment quality.
\end{itemize}



This comparative analysis demonstrates the trade-offs and challenges inherent in transitioning from single-objective to multi-objective optimization frameworks like DPO-CAO. Future research should explore methods to balance complexity with practical efficiency.




\section{Complexity Analysis and Computational Overhead of DPO-CAO}
\label{sec:appendix_complexity_analysis}

The CAO loss function introduces significant computational overhead compared to vanilla DPO due to the integration of multiple objectives (axioms), synergy weights, and axiom-specific regularization terms. While DPO-CAO optimizes six contradictory alignments simultaneously, practical use cases may only require focusing on one or two axioms. Below, we analyze the computational complexity of each component.

\subsection{Components of DPO-CAO}
The DPO-CAO loss function is composed of three main components:
\[
L_{\text{DPO-CAO}} = L_{\text{Local}} + L_{\text{Global}} + \sum_{a=1}^6 \tau_a \cdot \mathcal{R}_a
\]

\subsubsection{Local Alignment Loss}
The local alignment loss for each axiom \(a\) is defined as:
\[
L_{\text{Local}} = - \sum_{a=1}^6 \sum_{(i, j) \in \mathcal{P}_a} \log \left( \frac{\exp(f_a(I_i))}{\exp(f_a(I_i)) + \exp(f_a(I_j))} \right)
\]
where \(\mathcal{P}_a\) represents the set of pairwise comparisons for axiom \(a\).

\textbf{Complexity:} For \(n\) samples and \(m = 6\) axioms, the complexity is:
\[
O(m \cdot |\mathcal{P}_a|) = O(m \cdot n^2)
\]

\subsubsection{Global Synergy Loss}
The global synergy loss ensures consistency across multiple axioms:
\[
L_{\text{Global}} = - \lambda \sum_{(i, j) \in \mathcal{P}_S} \log \left( \frac{\exp\left(\sum_{a=1}^6 \omega_a f_a(I_i)\right)}{\exp\left(\sum_{a=1}^6 \omega_a f_a(I_i)\right) + \exp\left(\sum_{a=1}^6 \omega_a f_a(I_j)\right)} \right)
\]
where \(\omega_a\) is the synergy weight for axiom \(a\).

\textbf{Complexity:} For \(n\) samples and \(m = 6\) axioms:
\[
O(|\mathcal{P}_S| \cdot m) = O(n^2 \cdot m)
\]

\subsubsection{Axiom-Specific Regularizers}
The regularizer for each axiom \(a\) stabilizes optimization:
\[
\mathcal{R}_a = \int_{\mathcal{X}} \int_{\mathcal{X}} \|x - y\| P_a(x) Q_a(y) \, \mathrm{d}x \, \mathrm{d}y
\]
where \(P_a(x)\) and \(Q_a(y)\) are distributions for axiom \(a\).

\textbf{Complexity:} Computing pairwise distances between samples in \(d\)-dimensional feature space has a complexity of:
\[
O(n^2 \cdot d)
\]

\subsection{Total Complexity}
The total computational complexity for CAO is the sum of the complexities for local alignment, global synergy, and regularization:
\[
O(m \cdot n^2) + O(n^2 \cdot m) + O(n^2 \cdot d \cdot m) = O(n^2 \cdot m \cdot (1 + d))
\]
where \(n\) is the number of samples, \(m = 6\) is the number of axioms, and \(d\) is the feature dimensionality.

\subsection{Comparison with Vanilla DPO}

\begin{table}[ht!]
\centering
\caption{Comparison of Computational Complexity Between Vanilla DPO and CAO.}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{|p{0.3\columnwidth}|p{0.3\columnwidth}|p{0.3\columnwidth}|}
\hline
\textbf{Aspect}           & \textbf{Vanilla DPO}                  & \textbf{DPO-CAO} \\ \hline
\textbf{Pairwise Comparisons} & $O(n^2)$                              & $O(n^2 \cdot m)$ \\ \hline
\textbf{Regularization}       & $O(n \cdot d)$                        & $O(n^2 \cdot d \cdot m)$ \\ \hline
\textbf{Synergy Weights}      & Not Applicable                        & $O(n^2 \cdot m)$ \\ \hline
\textbf{Total Complexity}     & $O(n^2)$                              & $O(n^2 \cdot m \cdot (1 + d))$ \\ \hline
\end{tabular}
\end{adjustbox}
\label{tab:complexity_comparison}
\end{table}

\section{Future Directions for Reducing Global Synergy Overhead}
\label{sec:appendix_synergy_overhead_reduction}

The global synergy term in CAO introduces significant computational overhead due to its reliance on weighted aggregations across multiple axioms and pairwise comparisons. While we have not empirically tested the following strategies, they provide theoretical avenues to reduce this overhead. These approaches could be explored in future research to make CAO more scalable and efficient.

\subsection{Simplified Synergy Functions}
One possible extension is to replace the current weighted summation of axiom-specific scores:
\[
f_{\text{synergy}}(I) = \sum_{a=1}^m \omega_a f_a(I)
\]
with simpler aggregation functions:
\begin{itemize}
    \item \textbf{Max Aggregation:} Use the maximum score among all axioms:
    \[
    f_{\text{synergy}}(I) = \max_{a} f_a(I).
    \]
    \item \textbf{Mean Aggregation:} Compute the average score across axioms:
    \[
    f_{\text{synergy}}(I) = \frac{1}{m} \sum_{a=1}^m f_a(I).
    \]
\end{itemize}
These simplifications eliminate the need for weighted combinations and reduce the computational complexity from \(O(m)\) to \(O(1)\) per sample.

\subsection{Sparse Synergy Weights}
Instead of assigning non-zero weights \(\omega_a\) to all axioms, enforcing sparsity could reduce computational overhead. This can be achieved through:
\begin{itemize}
    \item \textbf{\(L_1\)-Regularization:} Apply regularization to drive some weights to zero:
    \[
    \mathcal{L}_{\text{regularization}} = \lambda \sum_{a=1}^m |\omega_a|.
    \]
    \item \textbf{Group Sparsity:} Suppress all weights associated with certain axioms or groups of axioms:
 \[
\mathcal{L}_{\text{regularization}} = \lambda \|\mathbf{\omega}_{\text{group}}\|_2.
\]

\end{itemize}
Sparse weights focus computation on high-impact axioms, reducing unnecessary overhead.

\subsection{Precomputed Synergy Scores}
Synergy scores can be precomputed for groups of similar samples to avoid redundant calculations during training:
\begin{itemize}
    \item \textbf{Clustering-Based Precomputation:} Cluster samples in feature space and compute a single synergy score for each cluster representative.
    \item \textbf{Embedding-Based Approximation:} Use a lightweight neural network to predict synergy scores:
    \[
    f_{\text{synergy}}(I) = \text{NN}(I).
    \]
\end{itemize}
These techniques shift computation from runtime to preprocessing, improving efficiency.

\subsection{Adaptive Axiom Selection}
Instead of using all axioms for synergy computation, adaptive strategies can dynamically select the most relevant ones:
\begin{itemize}
    \item \textbf{Dynamic Weight Adjustment:} Adjust \(\omega_a\) during training based on gradient magnitudes:
    \[
    \omega_a \propto \frac{\partial L}{\partial f_a(I)}.
    \]
    \item \textbf{Task-Specific Reduction:} Predefine a subset of axioms relevant to specific tasks, eliminating unnecessary terms.
\end{itemize}

\subsection{Approximation Techniques for Synergy Weights}
Approximation methods can reduce the cost of computing synergy weights:
\begin{itemize}
    \item \textbf{Low-Rank Approximation:} Decompose the weight matrix \(\mathbf{\omega}\) into low-rank components:
    \[
    \mathbf{\omega} \approx \mathbf{U} \mathbf{V}^T.
    \]
    \item \textbf{Probabilistic Sampling:} Randomly sample a subset of axioms for each iteration:
    \[
    f_{\text{synergy}}(I) = \sum_{a \in \text{sampled}} \omega_a f_a(I).
    \]
\end{itemize}

\subsection{Neural Approximations for Synergy}
A small neural network could replace the explicit computation of synergy scores:
\[
f_{\text{synergy}}(I) = \text{NN}(f_1(I), f_2(I), \dots, f_m(I)).
\]
This approach reduces computational redundancy by sharing representations across axioms.

\subsection{Future Exploration}
The above strategies represent theoretical extensions to reduce the computational overhead of the global synergy term in DPO-CAO. While these methods have not been empirically tested, they hold promise for improving the scalability and efficiency of the framework. We aim to explore some or all of these approaches in future research to validate their effectiveness.





\section{Details on Axiom-Specific Loss Function Design}
\label{sec:appendix_axiom_specific_loss}

Designing loss functions for each alignment axiom is a critical component of the CAO framework. Each axiom-specific loss is tailored to capture the nuanced trade-offs inherent in T2I generation tasks, such as balancing creative freedom with prompt fidelity or maintaining cultural sensitivity without compromising artistic expression. This section provides detailed mathematical formulations, practical insights, and design considerations for each loss function, ensuring that they align with the broader goals of the CAO framework. By leveraging state-of-the-art models, robust metrics, and adaptive weighting strategies, these loss functions offer a modular and extensible foundation for multi-axiom alignment. The following subsections delve into the specifics of each loss function, highlighting their role in addressing the challenges posed by their corresponding axioms.


\subsection{Artistic Freedom: \(\mathcal{L}_{\text{artistic}}\)}

The \emph{Artistic Freedom Loss} (\(\mathcal{L}_{\text{artistic}}\)) quantifies the creative enhancements applied to a generated image \(I_{\text{gen}}\) relative to a \emph{baseline} image \(I_{\text{base}}\). It integrates three core components: \textbf{Style Difference}, \textbf{Content Abstraction}, and \textbf{Content Difference}, each addressing distinct aspects of artistic freedom.

\subsubsection{1. Style Difference}
The Style Difference term measures stylistic deviation between \(I_{\text{gen}}\) and \(I_{\text{base}}\). Using VGG-based Gram features~\cite{gatys2016neural, johnson2016perceptual}, it is defined as:
\[
\text{StyleDiff} = \| S(I_{\text{gen}}) - S(I_{\text{base}}) \|_2^2,
\]
where \(S(\cdot)\) represents the Gram matrix of feature maps extracted from a pre-trained style network.

\paragraph{Gram Matrix:} Given feature maps \(\mathbf{F} \in \mathbb{R}^{C \times HW}\), where \(C\) is the number of channels, and \(H, W\) are the spatial dimensions, the Gram matrix \(G \in \mathbb{R}^{C \times C}\) is:
\[
G_{ij} = \sum_{k} F_{ik} F_{jk}.
\]
The style loss is computed as:
\[
\text{StyleDiff} = \sum_{l} \| G^l(I_{\text{gen}}) - G^l(I_{\text{base}}) \|_F^2,
\]
where \(l\) indexes the layers, and \(\|\cdot\|_F\) denotes the Frobenius norm.

\subsubsection{2. Content Abstraction}
Content Abstraction evaluates how abstractly \(I_{\text{gen}}\) interprets the textual prompt \(P\). It is computed as:
\[
\text{ContentAbs} = 1 - \cos\bigl(E(P), E(I_{\text{gen}})\bigr),
\]
where \(E(\cdot)\) is a multimodal embedding model such as CLIP~\cite{radford2021learning}. The cosine similarity measures alignment between \(P\) and \(I_{\text{gen}}\), with higher \(\text{ContentAbs}\) values indicating greater abstraction.

\subsubsection{3. Content Difference}
Content Difference ensures fidelity to \(I_{\text{base}}\), defined as:
\[
\text{ContentDiff} = 1 - \cos\bigl(E(I_{\text{gen}}), E(I_{\text{base}})\bigr).
\]
This term acts as a mild regularizer, balancing creative freedom with adherence to the baseline.

\subsubsection{Composite Loss Function}
The overall Artistic Freedom Loss combines these components:
\[
\mathcal{L}_{\text{artistic}} = \alpha \cdot \text{StyleDiff} + \beta \cdot \text{ContentAbs} + \gamma \cdot \text{ContentDiff},
\]
where \(\alpha, \beta, \gamma\) are tunable hyperparameters. By default, \(\alpha = 0.5\), \(\beta = 0.3\), and \(\gamma = 0.2\).

\subsubsection{Gradient Analysis}
The gradients of \(\mathcal{L}_{\text{artistic}}\) guide optimization:
\begin{itemize}
    \item \textbf{Gradient of \(\text{StyleDiff}\):}
    \[
    \frac{\partial \text{StyleDiff}}{\partial I_{\text{gen}}} = \sum_{l} \frac{\partial \| G^l(I_{\text{gen}}) - G^l(I_{\text{base}}) \|_F^2}{\partial I_{\text{gen}}}.
    \]
    \item \textbf{Gradient of \(\text{ContentAbs}\):}
    \[
    \frac{\partial \text{ContentAbs}}{\partial I_{\text{gen}}} = - \frac{\partial}{\partial I_{\text{gen}}} \frac{\langle E(P), E(I_{\text{gen}}) \rangle}{\|E(P)\| \cdot \|E(I_{\text{gen}})\|}.
    \]
    \item \textbf{Gradient of \(\text{ContentDiff}\):}
    \[
    \frac{\partial \text{ContentDiff}}{\partial I_{\text{gen}}} = - \frac{\partial}{\partial I_{\text{gen}}} \frac{\langle E(I_{\text{gen}}), E(I_{\text{base}}) \rangle}{\|E(I_{\text{gen}})\| \cdot \|E(I_{\text{base}})\|}.
    \]
\end{itemize}

\subsubsection{Theoretical Properties}
\begin{itemize}
    \item \textbf{Convexity:} Each component is non-negative, ensuring bounded loss.
    \item \textbf{Flexibility:} The weights \(\alpha, \beta, \gamma\) enable task-specific tuning.
    \item \textbf{Interpretability:} Each term directly corresponds to an intuitive notion of artistic freedom.
\end{itemize}

\subsubsection{Future Directions}
To enhance \(\mathcal{L}_{\text{artistic}}\), future work could:
\begin{itemize}
    \item Explore adaptive weighting schemes for \(\alpha, \beta, \gamma\).
    \item Integrate domain-specific style features to better capture artistic nuances.
    \item Validate the loss function across diverse artistic domains such as abstract art, photography, and conceptual design.
\end{itemize}


\subsection{Faithfulness to Prompt: \(\mathcal{L}_{\text{faith}}\)}

Faithfulness to the prompt is a cornerstone of T2I alignment, ensuring that the generated image adheres to the semantic and visual details specified by the user. To evaluate faithfulness, we leverage a semantic alignment metric based on the \textbf{Sinkhorn-VAE Wasserstein Distance}, a robust measure of distributional similarity that has gained traction in generative modeling for its interpretability and computational efficiency~\cite{arjovsky2017wasserstein, tolstikhin2018wasserstein}.

\subsubsection{Mathematical Formulation}
The Faithfulness Loss is defined as:
\[
\mathcal{L}_{\text{faith}} = -W_d^\lambda(P(Z_{\text{prompt}}), Q(Z_{\text{image}})),
\]
where:
\begin{itemize}
    \item \(P(Z_{\text{prompt}})\) is the latent distribution of the textual prompt extracted using a Variational Autoencoder (VAE).
    \item \(Q(Z_{\text{image}})\) is the latent distribution of the generated image obtained from the same VAE.
    \item \(W_d^\lambda\) represents the \textbf{Sinkhorn-regularized Wasserstein Distance}, defined as:
    \[
    W_d^\lambda(P, Q) = \min_{\pi \in \Pi(P, Q)} \int_{\mathcal{X} \times \mathcal{X}} \|x - y\|^d \,\pi(x, y) \, \mathrm{d}x \, \mathrm{d}y + \lambda \, \mathcal{R}(\pi),
    \]
    where:
    \begin{itemize}
        \item \(\Pi(P, Q)\) is the set of all joint probability distributions with marginals \(P\) and \(Q\).
        \item \(\|x - y\|^d\) is the cost function measuring the distance between latent points \(x\) and \(y\).
        \item \(\mathcal{R}(\pi)\) is the Sinkhorn regularizer:
        \[
        \mathcal{R}(\pi) = \int_{\mathcal{X} \times \mathcal{X}} \pi(x, y) \, \log(\pi(x, y)) \, \mathrm{d}x \, \mathrm{d}y,
        \]
        which ensures smooth and computationally efficient optimization~\cite{cuturi2013sinkhorn}.
    \end{itemize}
\end{itemize}

\subsubsection{Latent Representations}
The latent distributions \(P(Z_{\text{prompt}})\) and \(Q(Z_{\text{image}})\) are modeled using a shared Variational Autoencoder (VAE):
\[
Z_{\text{prompt}}, Z_{\text{image}} \sim \mathcal{N}(\mu, \sigma^2),
\]
where:
\begin{itemize}
    \item \(\mu\) and \(\sigma^2\) are the mean and variance of the respective latent embeddings, learned through the encoder.
    \item The shared latent space ensures compatibility between textual and visual representations, aligning semantic content across modalities.
\end{itemize}

\subsubsection{Properties of Faithfulness Loss}
\begin{itemize}
    \item \textbf{Semantic Depth:} By aligning latent distributions, the loss captures nuanced semantic relationships between the prompt and the generated image, beyond simple token matching.
    \item \textbf{Robustness:} The Sinkhorn regularizer (\(\lambda \, \mathcal{R}(\pi)\)) ensures smooth optimization and accommodates minor creative deviations without heavily penalizing them.
    \item \textbf{Scalability:} The Sinkhorn-regularized Wasserstein Distance is computationally efficient, making it suitable for large-scale applications.
\end{itemize}

\subsubsection{Gradient Analysis}
The gradient of \(\mathcal{L}_{\text{faith}}\) with respect to the generated image \(I_{\text{gen}}\) is computed as:
\[
\frac{\partial \mathcal{L}_{\text{faith}}}{\partial I_{\text{gen}}} = - \frac{\partial W_d^\lambda(P(Z_{\text{prompt}}), Q(Z_{\text{image}}))}{\partial Q(Z_{\text{image}})} \cdot \frac{\partial Q(Z_{\text{image}})}{\partial I_{\text{gen}}}.
\]
Breaking this down:
\begin{itemize}
    \item \(\frac{\partial W_d^\lambda}{\partial Q(Z_{\text{image}})}\) computes the gradient of the Wasserstein Distance with respect to the latent distribution.
    \item \(\frac{\partial Q(Z_{\text{image}})}{\partial I_{\text{gen}}}\) propagates the gradient from the latent space back to the pixel space.
\end{itemize}

\subsubsection{Implementation Details}
To compute \(\mathcal{L}_{\text{faith}}\) in practice:
\begin{itemize}
    \item Use a pretrained VAE to encode both the prompt and image into a shared latent space.
    \item Employ Sinkhorn iterations to efficiently optimize the Wasserstein Distance, following the algorithm proposed in~\cite{cuturi2013sinkhorn}.
    \item Set \(\lambda\) empirically to balance computational cost and alignment accuracy. Typical values range from \(0.01\) to \(0.1\).
\end{itemize}

\subsubsection{Future Directions}
Potential extensions to \(\mathcal{L}_{\text{faith}}\) include:
\begin{itemize}
    \item Incorporating multimodal transformers to jointly encode text and image embeddings for better semantic alignment.
    \item Exploring alternative regularizers (e.g., entropic or gradient regularization) for improved robustness.
    \item Testing the loss on diverse datasets, including abstract or ambiguous prompts, to evaluate generalization.
\end{itemize}


\subsection{Emotional Impact Score (EIS): \(\mathcal{L}_{\text{emotion}}\)}

The \textit{Emotional Impact Score} (EIS) quantifies the emotional intensity conveyed by generated images. It measures the strength and dominance of emotions such as happiness, sadness, anger, or fear, ensuring that T2I models can evoke the intended emotional response based on user prompts. This metric is particularly important for domains like marketing, storytelling, or psychological studies where emotional resonance plays a key role.

\subsubsection{Mathematical Definition of EIS}
EIS is computed as the average emotional intensity across a batch of generated images:
\[
\text{EIS} = \frac{1}{M} \sum_{i=1}^M \text{EmotionIntensity}(\text{img}_i),
\]
where:
\begin{itemize}
    \item \(M\): Total number of images in the batch.
    \item \(\text{EmotionIntensity}(\text{img}_i)\): The scalar intensity of the dominant emotion in the image \(\text{img}_i\), computed using pretrained emotion detection models (e.g., DeepEmotion~\cite{abidin2018deepemotion}).
\end{itemize}

\paragraph{Emotion Detection Models:}
Pretrained emotion detection models, such as DeepEmotion, rely on convolutional neural networks trained on datasets labeled with basic emotions (e.g., happiness, sadness, anger, fear). The emotion intensity score is normalized to range between 0 and 1, where 1 indicates maximum emotional intensity.


\subsubsection{Neutrality Score (N)}
While EIS captures the strength of the dominant emotion, the \textit{Neutrality Score} (N) quantifies the absence of emotional dominance, representing emotional balance or impartiality. This metric is useful in cases where emotionally neutral outputs are desired, such as in educational or scientific content.

\[
N = 1 - \max(\text{EmotionIntensity}),
\]
where:
\begin{itemize}
    \item \(\max(\text{EmotionIntensity})\): The intensity of the most dominant emotion detected in the image.
\end{itemize}

\paragraph{Interpretation of Neutrality Score:}
\begin{itemize}
    \item \(N \approx 1\): The image is emotionally neutral, with no strongly dominant emotion.
    \item \(N \approx 0\): The image strongly reflects a specific emotion, indicating high emotional dominance.
\end{itemize}


\subsubsection{Combined Metric: Tradeoff Between Emotional Impact and Neutrality}
To evaluate the tradeoff between Emotional Impact and Neutrality, a combined metric, \(T_{\text{EMN}}\), is defined as:
\[
T_{\text{EMN}} = \alpha \cdot \text{EIS} + \beta \cdot N,
\]
where:
\begin{itemize}
    \item \(\alpha\): Weight assigned to Emotional Impact.
    \item \(\beta\): Weight assigned to Neutrality.
    \item \(\alpha + \beta = 1\): Ensures a balanced contribution of both terms, with default values \(\alpha = 0.3\) and \(\beta = 0.7\), chosen empirically.
\end{itemize}

\paragraph{Interpretation of \(T_{\text{EMN}}\):}
\begin{itemize}
    \item Higher \(T_{\text{EMN}}\) values indicate images that either evoke strong emotional responses or maintain emotional neutrality, depending on the weights \(\alpha\) and \(\beta\).
    \item Adjusting \(\alpha\) and \(\beta\) allows for task-specific prioritization, such as favoring emotional impact (\(\alpha > \beta\)) or neutrality (\(\beta > \alpha\)).
\end{itemize}

\subsubsection{Gradient Analysis}
The gradients of \(\mathcal{L}_{\text{emotion}}\) are essential for optimizing Emotional Impact in T2I systems. For a single image \(\text{img}_i\), the gradient with respect to the generated image is:
\[
\frac{\partial \text{EmotionIntensity}(\text{img}_i)}{\partial \text{img}_i},
\]
computed using backpropagation through the pretrained emotion detection model. Similarly, for Neutrality Score \(N\), the gradient is:
\[
\frac{\partial N}{\partial \text{img}_i} = - \frac{\partial \max(\text{EmotionIntensity})}{\partial \text{img}_i}.
\]

\subsubsection{Implementation Details}
To compute EIS and \(T_{\text{EMN}}\) in practice:
\begin{itemize}
    \item Use pretrained emotion detection models like DeepEmotion~\cite{abidin2018deepemotion} or similar models fine-tuned for specific emotion datasets.
    \item Normalize emotion intensity values to ensure consistent scaling across different images and batches.
    \item Tune \(\alpha\) and \(\beta\) based on application requirements, such as creative tasks (\(\alpha > \beta\)) or neutral designs (\(\beta > \alpha\)).
\end{itemize}

\subsubsection{Future Directions}
To enhance Emotional Impact and Neutrality evaluation, future research could explore:
\begin{itemize}
    \item \textbf{Multimodal Emotion Models:} Integrate multimodal models that jointly analyze textual prompts and visual outputs to better align emotional tones.
    \item \textbf{Context-Aware Neutrality:} Develop context-aware neutrality metrics to differentiate between intended neutrality (e.g., instructional content) and unintended neutrality (e.g., lack of emotion due to poor generation).
    \item \textbf{Fine-Grained Emotions:} Extend emotion detection to capture fine-grained emotions (e.g., nostalgia, hope) for more nuanced evaluations.
\end{itemize}


\subsection{Originality vs. Referentiality: \(\mathcal{L}_{\text{originality}}\) \& \(\mathcal{L}_{\text{referentiality}}\)}

To evaluate the trade-off between originality and referentiality in a generated image \(I_{\text{gen}}\), we propose a framework leveraging pretrained CLIP models for dynamic reference retrieval and stylistic analysis. The originality metric (\(\mathcal{L}_{\text{originality}}\)) quantifies divergence from reference styles, while the referentiality metric (\(\mathcal{L}_{\text{referentiality}}\)) measures adherence to stylistic norms.


\subsubsection{Mathematical Definition}
The combined loss function is expressed as:
\[
f_{\text{originality\_referentiality}}(I_{\text{gen}}) = \frac{1}{K} \sum_{k=1}^K \Bigl[ 1 - \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr) \Bigr],
\]
where:
\begin{itemize}
    \item \(E_{\text{CLIP}}(\cdot)\): Embedding function of the pretrained CLIP model, mapping images to a joint visual-textual embedding space~\cite{radford2021learning}.
    \item \(S_{\text{retr},k}\): The \(k\)-th reference image retrieved from a curated database using CLIP Retrieval~\cite{clip-retrieval-2023}.
    \item \(K\): Number of top-matching reference images.
\end{itemize}


\paragraph{Decomposition of Loss Terms}
The loss can be separated into two components:
\begin{itemize}
    \item \textbf{Originality Loss:}
    \[
    \mathcal{L}_{\text{originality}} = \frac{1}{K} \sum_{k=1}^K \bigl[1 - \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr)\bigr],
    \]
    which quantifies the stylistic divergence from reference images. Higher values indicate more originality.
    \item \textbf{Referentiality Loss:}
    \[
    \mathcal{L}_{\text{referentiality}} = \frac{1}{K} \sum_{k=1}^K \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr),
    \]
    which evaluates adherence to stylistic norms. Higher values reflect stronger referential alignment.
\end{itemize}


\subsubsection{Reference Image Retrieval with CLIP}
Dynamic reference selection is a crucial step in evaluating originality and referentiality. The retrieval process involves the following steps:
\begin{enumerate}
    \item \textbf{Embedding Computation:} Compute the CLIP embedding of the generated image:
    \[
    E_{\text{CLIP}}(I_{\text{gen}}) \in \mathbb{R}^d,
    \]
    where \(d\) is the dimensionality of the CLIP embedding space.
    \item \textbf{Database Query:} Compare \(E_{\text{CLIP}}(I_{\text{gen}})\) against precomputed embeddings of reference images in a database. The similarity metric is cosine similarity:
    \[
    \text{Sim}(I_{\text{gen}}, S_{\text{retr},k}) = \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr).
    \]
    \item \textbf{Top-\(K\) Selection:} Retrieve the top-\(K\) reference images with the highest similarity scores:
    \[
    S_{\text{retr},k} = \arg\max_{S \in \text{Database}} \text{Sim}(I_{\text{gen}}, S).
    \]
\end{enumerate}


\subsubsection{Reference Databases}
We leverage large-scale artistic datasets to ensure diverse and meaningful reference styles:
\begin{itemize}
    \item \textbf{WikiArt:} A dataset containing over 81,000 images across 27 art styles, including impressionism, surrealism, cubism, and more~\cite{saleh2015large}.
    \item \textbf{BAM (Behance Artistic Media):} A large-scale dataset of over 2.5 million high-resolution images curated from professional portfolios, encompassing diverse artistic styles~\cite{wilber2017bam}.
\end{itemize}
These datasets provide the stylistic variety necessary for evaluating originality and referentiality comprehensively.


\subsubsection{Trade-off Between Originality and Referentiality}
The inherent trade-off between originality and referentiality can be controlled by weighting their contributions. We define a combined metric:
\[
T_{\text{OR}} = \alpha \cdot \mathcal{L}_{\text{originality}} + \beta \cdot \mathcal{L}_{\text{referentiality}},
\]
where:
\begin{itemize}
    \item \(\alpha, \beta\): Weights controlling the emphasis on originality (\(\alpha\)) versus referentiality (\(\beta\)).
    \item \(\alpha + \beta = 1\): Ensures balanced contributions.
    \item Default values: \(\alpha = 0.6\), \(\beta = 0.4\), prioritizing originality for most creative tasks.
\end{itemize}


\subsubsection{Gradient Analysis}
The gradients of \(T_{\text{OR}}\) with respect to \(I_{\text{gen}}\) guide optimization:
\[
\frac{\partial T_{\text{OR}}}{\partial I_{\text{gen}}} = \alpha \cdot \frac{\partial \mathcal{L}_{\text{originality}}}{\partial I_{\text{gen}}} + \beta \cdot \frac{\partial \mathcal{L}_{\text{referentiality}}}{\partial I_{\text{gen}}}.
\]
For each component:
\begin{itemize}
    \item Gradient of \(\mathcal{L}_{\text{originality}}\):
    \[
    \frac{\partial \mathcal{L}_{\text{originality}}}{\partial I_{\text{gen}}} = - \frac{1}{K} \sum_{k=1}^K \frac{\partial \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr)}{\partial I_{\text{gen}}}.
    \]
    \item Gradient of \(\mathcal{L}_{\text{referentiality}}\):
    \[
    \frac{\partial \mathcal{L}_{\text{referentiality}}}{\partial I_{\text{gen}}} = \frac{1}{K} \sum_{k=1}^K \frac{\partial \cos\bigl(E_{\text{CLIP}}(I_{\text{gen}}), E_{\text{CLIP}}(S_{\text{retr},k})\bigr)}{\partial I_{\text{gen}}}.
    \]
\end{itemize}


\subsubsection{Future Directions}
To improve the evaluation of originality and referentiality, future work could explore:
\begin{itemize}
    \item \textbf{Dynamic Weighting:} Develop adaptive mechanisms to adjust \(\alpha\) and \(\beta\) based on user-defined objectives.
    \item \textbf{Fine-Grained Styles:} Incorporate additional style-specific metrics to evaluate subcategories (e.g., brushstroke style, color palette).
    \item \textbf{Diverse Databases:} Expand the reference databases to include non-traditional and contemporary art styles for broader applicability.
\end{itemize}


\subsection{Cultural Sensitivity: \(\mathcal{L}_{\text{cultural}}\)}
\label{subsec:cultural_sensitivity}

Evaluating cultural sensitivity in T2I systems presents unique challenges due to the vast diversity of cultural contexts and the lack of standardized pre-trained cultural classifiers. To address this, we propose a novel metric called \textbf{Simulated Cultural Context Matching (SCCM)}, which dynamically generates culturally specific sub-prompts using Large Language Models (LLMs) and evaluates their alignment with T2I-generated images. This approach provides a flexible and extensible framework for cultural evaluation.


\subsubsection{Mathematical Formulation of SCCM}
The SCCM score evaluates the alignment between the generated image and a set of dynamically generated cultural sub-prompts. The metric comprises the following steps:

\paragraph{1. Embedding Generation}
\begin{enumerate}
    \item \textbf{Prompt Embedding:} For each LLM-generated cultural sub-prompt \(P_i\), compute embeddings using a multimodal model (e.g., CLIP):
    \[
    \{E(P_1), E(P_2), \dots, E(P_k)\},
    \]
    where \(k\) is the total number of sub-prompts.
    \item \textbf{Image Embedding:} Embed the T2I-generated image \(I_{\text{gen}}\) using the same model:
    \[
    E(I_{\text{gen}}).
    \]
\end{enumerate}

\paragraph{2. Prompt-Image Similarity}
Calculate the semantic similarity between each sub-prompt \(P_i\) and the generated image \(I_{\text{gen}}\) using cosine similarity:
\[
\text{sim}(E(P_i), E(I_{\text{gen}})) = \frac{E(P_i) \cdot E(I_{\text{gen}})}{\|E(P_i)\| \|E(I_{\text{gen}})\|}.
\]

\paragraph{3. Sub-Prompt Aggregation}
Aggregate the similarity scores across all \(k\) sub-prompts to compute the raw SCCM score:
\[
\text{SCCM}_{\text{raw}} = \frac{1}{k} \sum_{i=1}^k \text{sim}(E(P_i), E(I_{\text{gen}})).
\]

\paragraph{4. Normalization}
Normalize \(\text{SCCM}_{\text{raw}}\) to the range \([0, 1]\) for consistent evaluation:
\[
\text{SCCM}_{\text{final}} = \frac{\text{SCCM}_{\text{raw}} - \text{SCCM}_{\text{min}}}{\text{SCCM}_{\text{max}} - \text{SCCM}_{\text{min}}}.
\]
Here:
\begin{itemize}
    \item \(\text{SCCM}_{\text{min}}\) and \(\text{SCCM}_{\text{max}}\) are predefined minimum and maximum similarity scores based on a validation dataset of culturally diverse images and prompts.
    \item Normalization ensures that scores are comparable across different datasets and cultural contexts.
\end{itemize}



\subsubsection{Example Computation of SCCM}
\paragraph{User Prompt:}
\emph{“Generate an image of a Japanese garden during spring.”}

\paragraph{Step 1: Sub-Prompt Generation}
Using an LLM, generate culturally specific sub-prompts:
\begin{itemize}
    \item \(P_1\): \emph{“A traditional Japanese garden with a koi pond and a wooden bridge.”}
    \item \(P_2\): \emph{“Cherry blossoms blooming in spring with traditional Japanese stone lanterns.”}
    \item \(P_3\): \emph{“A Zen rock garden with raked gravel patterns.”}
\end{itemize}

\paragraph{Step 2: Embedding and Similarity Calculation}
Compute cosine similarities:
\[
\text{sim}(E(P_1), E(I_{\text{gen}})) = 0.85, \; \text{sim}(E(P_2), E(I_{\text{gen}})) = 0.80, \; \text{sim}(E(P_3), E(I_{\text{gen}})) = 0.75.
\]

\paragraph{Step 3: Raw Aggregated Score}
Aggregate the similarity scores:
\[
\text{SCCM}_{\text{raw}} = \frac{0.85 + 0.80 + 0.75}{3} = 0.80.
\]

\paragraph{Step 4: Final Normalized Score}
Normalize using \(\text{SCCM}_{\text{min}} = 0.70\) and \(\text{SCCM}_{\text{max}} = 0.90\):
\[
\text{SCCM}_{\text{final}} = \frac{0.80 - 0.70}{0.90 - 0.70} = 0.50.
\]

\subsubsection{Gradient Analysis}
The gradients of the Cultural Sensitivity Loss \(\mathcal{L}_{\text{cultural}}\) guide optimization by adjusting the generated image \(I_{\text{gen}}\) to better align with culturally sensitive contexts. The loss is defined as:
\[
\mathcal{L}_{\text{cultural}} = 1 - \text{SCCM}_{\text{final}}.
\]

The gradient with respect to the generated image \(I_{\text{gen}}\) is:
\[
\frac{\partial \mathcal{L}_{\text{cultural}}}{\partial I_{\text{gen}}} = - \frac{\partial \text{SCCM}_{\text{final}}}{\partial I_{\text{gen}}}.
\]

Breaking this down:
\[
\frac{\partial \text{SCCM}_{\text{final}}}{\partial I_{\text{gen}}} = \frac{1}{k (\text{SCCM}_{\text{max}} - \text{SCCM}_{\text{min}})} \sum_{i=1}^k \frac{\partial \text{sim}(E(P_i), E(I_{\text{gen}}))}{\partial I_{\text{gen}}}.
\]

For each sub-prompt \(P_i\), the gradient of the cosine similarity is:
\[
\frac{\partial \text{sim}(E(P_i), E(I_{\text{gen}}))}{\partial I_{\text{gen}}} = \frac{1}{\|E(P_i)\| \|E(I_{\text{gen}})\|} \left( E(P_i) - \text{sim}(E(P_i), E(I_{\text{gen}})) \cdot E(I_{\text{gen}}) \right) \cdot \frac{\partial E(I_{\text{gen}})}{\partial I_{\text{gen}}}.
\]

Key components:
\begin{itemize}
    \item \(\frac{\partial E(I_{\text{gen}})}{\partial I_{\text{gen}}}\): Gradient propagation through the CLIP embedding model.
    \item \(\text{sim}(E(P_i), E(I_{\text{gen}}))\): Ensures semantic alignment between the image and the cultural sub-prompts.
\end{itemize}

\subsubsection{Challenges and Future Directions}
While SCCM offers a novel approach to evaluating cultural sensitivity, there are limitations and opportunities for improvement:
\begin{itemize}
    \item \textbf{Cultural Nuance Representation:} For some nuanced cases generating sub-prompts that accurately reflect nuanced cultural elements requires further fine-tuning of LLMs.
\end{itemize}


\subsection{Verifiability Loss: \(\mathcal{L}_{\text{verifiability}}\)}

The \emph{verifiability loss} quantifies the alignment of a generated image \(I_{\text{gen}}\) with real-world references by comparing it to the top-\(K\) images retrieved from Google Image Search. This ensures that the generated content maintains authenticity, factual consistency, and visual realism by leveraging external real-world data.


\subsubsection{Mathematical Formulation}
The verifiability loss is computed as:
\[
\mathcal{L}_{\text{verifiability}}
=
1
-
\frac{1}{K}
\sum_{k=1}^{K}
\cos\Bigl(
E(I_{\text{gen}}),\,
E(I_{\text{search},k})
\Bigr),
\]
where:
\begin{itemize}
    \item \(I_{\text{gen}}\): The generated image.
    \item \(I_{\text{search},k}\): The \(k\)-th image retrieved from Google Image Search.
    \item \(E(\cdot)\): A pretrained embedding extraction model (e.g., DINO ViT~\cite{caron2021emerging}) that captures semantic and visual features.
    \item \(K\): The number of top-retrieved images used for comparison.
\end{itemize}

Here, \(\cos(\cdot, \cdot)\) represents cosine similarity, defined as:
\[
\cos\Bigl(E(I_{\text{gen}}), E(I_{\text{search},k})\Bigr) = \frac{E(I_{\text{gen}}) \cdot E(I_{\text{search},k})}{\|E(I_{\text{gen}})\| \|E(I_{\text{search},k})\|}.
\]


\subsubsection{Workflow for Computing \(\mathcal{L}_{\text{verifiability}}\)}
\paragraph{Step 1: Image Retrieval}
The generated image \(I_{\text{gen}}\) is submitted to Google Image Search using its embedding or pixel data as a query. The search retrieves \(K\) visually and semantically similar images:
\[
\{I_{\text{search},1}, I_{\text{search},2}, \dots, I_{\text{search},K}\}.
\]

\paragraph{Step 2: Embedding Extraction}
Using a pretrained embedding model \(E(\cdot)\) (e.g., DINO ViT), compute embeddings for:
\begin{itemize}
    \item The generated image \(E(I_{\text{gen}})\).
    \item Each retrieved reference image \(E(I_{\text{search},k})\), for \(k = 1, 2, \dots, K\).
\end{itemize}

\paragraph{Step 3: Similarity Calculation}
Calculate cosine similarity for each retrieved image:
\[
\text{sim}_{k} = \cos\Bigl(E(I_{\text{gen}}), E(I_{\text{search},k})\Bigr), \quad \forall k \in \{1, \dots, K\}.
\]

\paragraph{Step 4: Averaging and Loss Computation}
Aggregate the similarity scores across all \(K\) retrieved images to compute the verifiability loss:
\[
\mathcal{L}_{\text{verifiability}}
=
1 - \frac{1}{K} \sum_{k=1}^K \text{sim}_{k}.
\]


\subsubsection{Gradient Analysis}
The gradient of \(\mathcal{L}_{\text{verifiability}}\) with respect to the generated image \(I_{\text{gen}}\) guides optimization toward better alignment with real-world references. The gradient is computed as:
\[
\frac{\partial \mathcal{L}_{\text{verifiability}}}{\partial I_{\text{gen}}}
=
- \frac{1}{K} \sum_{k=1}^K
\frac{\partial \cos\Bigl(E(I_{\text{gen}}), E(I_{\text{search},k})\Bigr)}{\partial I_{\text{gen}}}.
\]

Breaking down the cosine similarity gradient:
\[
\frac{\partial \cos\Bigl(E(I_{\text{gen}}), E(I_{\text{search},k})\Bigr)}{\partial I_{\text{gen}}} =
\frac{1}{\|E(I_{\text{gen}})\| \|E(I_{\text{search},k})\|}
\Bigl(E(I_{\text{search},k}) - \text{sim}_{k} \cdot E(I_{\text{gen}})\Bigr)
\cdot \frac{\partial E(I_{\text{gen}})}{\partial I_{\text{gen}}}.
\]


\subsubsection{Key Insights and Advantages}
\begin{itemize}
    \item \textbf{Robust Authenticity Check:} By comparing the generated image to real-world references, verifiability loss ensures that the output aligns with authentic and visually consistent content.
    \item \textbf{Applicability:} This loss is particularly valuable in domains such as journalism, education, and scientific visualization, where factual consistency is crucial.
    \item \textbf{Dynamic Adaptability:} The use of external data (Google Image Search) allows the loss to adapt dynamically to diverse prompts and contexts.
\end{itemize}


\subsubsection{Challenges and Limitations}
\begin{itemize}
    \item \textbf{Search Dependency:} The quality and relevance of retrieved images depend on the search engine’s indexing and ranking algorithms, which may introduce bias or inconsistencies.
    \item \textbf{Computational Overhead:} Retrieving and embedding multiple reference images increases computational cost.
    \item \textbf{Domain-Specific Limitations:} In specialized domains (e.g., medical imaging), publicly available reference images may not provide sufficient alignment for evaluation.
\end{itemize}


\subsubsection{Future Directions}
To enhance \(\mathcal{L}_{\text{verifiability}}\), future research could explore:
\begin{itemize}
    \item \textbf{Domain-Specific Reference Databases:} Replace or complement Google Image Search with curated datasets tailored to specific applications (e.g., PubMed for medical images).
    \item \textbf{Efficient Embedding Models:} Optimize embedding extraction by using lightweight or domain-specific models for faster computation.
    \item \textbf{Adaptive Retrieval Mechanisms:} Develop algorithms that dynamically refine queries to improve the relevance of retrieved reference images.
\end{itemize}




\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|l|X|l|X|}
        \hline
        \textbf{Hyperparameter} & \textbf{Purpose} & \textbf{Recommended Range} & \textbf{Best Practices} \\
        \hline
        \(\lambda\): Synergy Weighting Factor & Balances local axiom-specific losses and global synergy preferences. & \(0.1 \leq \lambda \leq 1.0\) & Start with \(\lambda = 0.5\). Increase for strong global coherence or decrease for local dominance. \\
        \hline
        \(\tau_a\): Axiom-Specific Regularization & Controls regularization strength for each axiom. & \(0.01 \leq \tau_a \leq 0.1\) & Use uniform \(\tau_a = 0.05\). Adjust for specific tasks: lower for high-dimensional models. \\
        \hline
        \(\omega_a\): Synergy Jacobian Weights & Assigns relative importance to axiom synergies during optimization. & \(0.1 \leq \omega_a \leq 1.0\) & Start with uniform \(\omega_a = 1.0\). Prioritize conflicting axioms with higher weights. \\
        \hline
        Learning Rate (\(\eta\)) & Controls the step size during optimization. & \(10^{-4} \leq \eta \leq 10^{-2}\) & Start with \(\eta = 10^{-3}\). Use smaller values for unstable loss landscapes, larger for smoother ones. \\
        \hline
    \end{tabularx}
    \caption{Best practices and ranges for selecting hyperparameters.}
    \label{tab:hyperparameter_selection}
\end{table*}


\section{Hyperparameter Selection}
This section provides guidance on selecting hyperparameters introduced in our framework. We detail two approaches: (1) best practices with recommended ranges and (2) automated hyperparameter tuning techniques.

\subsection{Best Practices and Ranges}
The following table outlines the key hyperparameters, their purposes, recommended ranges, and best practices for manual selection:





\subsection{Automated Hyperparameter Tuning}
For scenarios requiring automated selection of hyperparameters, the following techniques are recommended:
\begin{itemize}
    \item \textbf{Grid Search:} Searches exhaustively over predefined ranges. Suitable for small parameter spaces or abundant computational resources.
    \item \textbf{Random Search:} Samples hyperparameters randomly from specified distributions. Efficient for high-dimensional spaces.
    \item \textbf{Bayesian Optimization:} Models the objective function and explores promising regions of the hyperparameter space. Ideal for complex loss surfaces and expensive evaluations.
    \item \textbf{Population-Based Training (PBT):} Combines hyperparameter tuning and training, dynamically updating hyperparameters during optimization. Effective for dynamic tasks.
\end{itemize}

To optimize performance, a practical workflow might begin with best-practice values followed by grid or random search for coarse tuning, and then Bayesian optimization or PBT for fine-tuning.


\section{Scalability}
Scalability is a cornerstone of the practical deployment of the proposed YinYangAlign framework, particularly for addressing the complexity of Text-to-Image (T2I) alignment tasks. This section explores computational, memory, and data scalability while addressing high-resolution generation. References to best practices and state-of-the-art techniques are included to strengthen the discussion.

\subsection{Computational Scalability}
The computational demands of the framework arise from evaluating synergy preferences, regularization terms, and multi-objective optimization.

\begin{itemize}
    \item \textbf{Loss Function Evaluation:}
    The term \(-\lambda \sum_{(i,j)} \log\bigl(P_{ij}^{\mathcal{S}}\bigr)\) introduces a quadratic computational overhead (\(O(N^2)\)).
    \begin{itemize}
        \item \textbf{Sparse Sampling:} Approximate pairwise evaluations by sampling a subset of interactions~\cite{johnson2019billion}.
        \item \textbf{Mini-batch Strategies:} Limit pairwise evaluations to within mini-batches, reducing memory and computational costs.
        \item \textbf{Kernel Approximation:} Use techniques like Nyström approximation for computationally efficient kernel evaluation~\cite{williams2001using}.
    \end{itemize}
    
    \item \textbf{Axiom-Specific Regularization:}
    Jacobian evaluations for \(\sum_{a=1}^A \tau_a \mathcal{R}_a\) incur computational overhead.
    \begin{itemize}
        \item Apply low-rank approximations or iterative solvers for matrix computations~\cite{saad2003iterative}.
        \item Precompute reusable gradients to accelerate axiom-specific regularization.
    \end{itemize}
    
    \item \textbf{Distributed Optimization:}
    \begin{itemize}
        \item \textbf{Multi-GPU Scaling:} Leverage distributed frameworks like Horovod (\url{https://horovod.ai}) or PyTorch Distributed (\url{https://pytorch.org/tutorials/intermediate/ddp_tutorial.html}) to parallelize computations.
        \item \textbf{Mixed Precision Training:} Use tools like NVIDIA Apex (\url{https://github.com/NVIDIA/apex}) to reduce memory usage and improve training speed.
    \end{itemize}
\end{itemize}

\subsection{Memory Scalability}
Memory efficiency is crucial for managing high-dimensional embeddings and large-scale data.

\begin{itemize}
    \item \textbf{High-Dimensional Embedding Management:}
    Synergy evaluations require large embedding matrices.
    \begin{itemize}
        \item Apply dimensionality reduction techniques like PCA or t-SNE~\cite{maaten2008visualizing} to compress embeddings.
        \item Implement online embedding computation, discarding embeddings after usage.
    \end{itemize}

    \item \textbf{Efficient Checkpointing:}
    Store only essential intermediate states for backpropagation, recomputing others as needed.
    Use gradient checkpointing libraries, such as Checkmate (\url{https://github.com/stanford-futuredata/checkmate}) for efficient training.

    \item \textbf{Dynamic Batch Sizing:}
    Adjust batch sizes based on available memory. Combine with data prefetching and asynchronous data loading for seamless memory management.
\end{itemize}

\subsection{Data Scalability}
Scaling to large datasets requires optimizing preprocessing, storage, and loading mechanisms.

\begin{itemize}
    \item \textbf{Sharding and Distributed Data Loading:}
    Partition datasets into shards and distribute them across nodes for parallel processing. Use frameworks like Apache Parquet (\url{https://parquet.apache.org}) for optimized storage and access.

    \item \textbf{Streaming:}
    Stream data in chunks during training to minimize memory usage. Libraries like TensorFlow Datasets (\url{https://www.tensorflow.org/datasets}) or PyTorch DataLoader (\url{https://pytorch.org/docs/stable/data.html}) can facilitate streaming.

    \item \textbf{Handling Imbalanced Datasets:}
    Apply oversampling or weighted losses to ensure balanced contributions across axioms~\cite{buda2018systematic}.
\end{itemize}

\subsection{High-Resolution Image Scalability}
High-resolution image generation increases both computational and memory demands.

\begin{itemize}
    \item \textbf{Hierarchical Optimization:}
    Use a multi-resolution strategy, optimizing at lower resolutions first and refining at higher resolutions. Progressive growing techniques, as used in GANs~\cite{karras2017progressive}, can reduce the computational burden early on.

    \item \textbf{Patch-Based Processing:}
    Divide high-resolution images into overlapping patches, process them independently, and aggregate results. Ensure patch consistency using overlap-tile strategies~\cite{ronneberger2015u}.

    \item \textbf{Distributed Rendering:}
    Parallelize rendering across GPUs or compute nodes using task scheduling frameworks like Ray (\url{https://www.ray.io}).
\end{itemize}

