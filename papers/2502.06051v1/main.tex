\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{smile}
\usepackage{subfigure}
% \usepackage{algorithm}
% \usepackage{algpseudocode}

\usepackage{tablefootnote}

\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=blue
            ]{hyperref}
\usepackage{enumitem}
% \usepackage[usenames]{color}
\usepackage{mathtools}

% \usepackage[x11names]{xcolor}
% \definecolor{LightCyan}{rgb}{0.5, 0.65, 1}

\usepackage{colortbl}
\definecolor{LightCyan}{rgb}{0.8, 0.9, 1}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\rE}{\mathbb E}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\KL}{\mathsf{KL}}
\newcommand{\regret}{{\mathrm{Regret}}}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}
\newcommand{\GL}{\mathrm{GL}}
\newcommand{\E}{\rE}
\newcommand{\minus}{\scalebox{0.75}[1.0]{$-$}}
\newcommand{\hz}[1]{\textcolor{green}{[Heyang] #1}}
\newcommand{\qz}[1]{\textcolor{red}{[Qingyue] #1}}
\newcommand{\todoq}[2][]{\todo[size=\scriptsize,color=orange!20!white,#1]{Quanquan: #2}}
\newcommand{\qingyue}[2][]{\todo[size=\scriptsize,color=red!20!white,#1]{Qingyue: #2}}

\ifdefined\final
\usepackage[disable]{todonotes}
\else
\usepackage[textsize=tiny]{todonotes}
\fi
%\setlength{\marginparwidth}{1in}
% \algnewcommand{\LeftComment}[1]{\(\triangleright\) {\color{LightCyan}{#1}}}
\allowdisplaybreaks


\title{\huge Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual Bandits under Single-Policy Concentrability}

% \author
% {
%     Qingyue Zhao \thanks{Equal contribution} \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt zhaoqy24@cs.ucla.edu}}
%     ~and~
%     Kaixuan Ji\footnotemark[1] \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt kaixuanji@cs.ucla.edu}} 
%     ~and~
%     Heyang Zhao\footnotemark[1] \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt hyzhao@cs.ucla.edu}} 
%     ~and~
%     Tong Zhang \thanks{University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; e-mail: {\tt tongzhang@tongzhang-ml.org}}
%     ~and~
%     Quanquan Gu \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt qgu@cs.ucla.edu}}
% }

\author{
    Qingyue Zhao\thanks{Equal contribution} \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt zhaoqy24@cs.ucla.edu}}
    ~~
    Kaixuan Ji\footnotemark[1] \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt kaixuanji@cs.ucla.edu}} 
    ~~
    Heyang Zhao\footnotemark[1] \thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt hyzhao@cs.ucla.edu}} 
    ~~
    Tong Zhang\thanks{University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; e-mail: {\tt tongzhang@tongzhang-ml.org}}
    ~~
    Quanquan Gu\thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt qgu@cs.ucla.edu}}
}
\date{}

\newcommand{\piref}{\pi^{\mathsf{ref}}}
\newcommand{\kl}[2]{\ensuremath{{\mathsf{KL}}\left(#1\|#2\right)}}
\newcommand{\tv}[2]{\ensuremath{{\mathsf{TV}}\left(#1\|#2\right)}}
\newcommand{\confbandit}{\cE}
\newcommand{\confduelbandit}{\cE}

\newcommand{\fls}{{\bar{f}}} % f estimated via least square
\newcommand{\fps}{{\hat{f}}} % f estimated via least square + pessimism

\newcommand{\fml}{\check{f}} % f estimated by MLE of the BTL model, currently this notation is not used

\newcommand{\fvi}{\hat{f}} % f estimated via LSVI
\newcommand{\fvip}{\tilde{f}} % f estimated via LSVI + pessimism

\def \fdiv {\mathtt{f}}

\def \algcb {\text{KL-PCB}}
\def \algcdb {\text{KL-PCDB}}

\def \algmdp {\text{KLR-PLSVI}}
\def \algmdpwo {\text{KLR-LSVI}}


\usepackage{soul}

\begin{document}

\maketitle

\begin{abstract}
    KL-regularized policy optimization has become a workhorse in learning-based decision making, while its theoretical understanding is still very limited. Although recent progress has been made towards settling the sample complexity of KL-regularized contextual bandits, existing sample complexity bounds are either $\tilde{O}(\epsilon^{-2})$ under single-policy concentrability or $\tilde{O}(\epsilon^{-1})$ under all-policy concentrability. In this paper, we propose the \emph{first} algorithm with $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for offline contextual bandits. Our algorithm is designed for general function approximation and based on the principle of \emph{pessimism in the face of uncertainty}. The core of our proof leverages the strong convexity of the KL regularization, and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. The near-optimality of our algorithm is demonstrated by an $\tilde{\Omega}(\epsilon^{-1})$ lower bound. Furthermore, we extend our algorithm to contextual dueling bandits and achieve a similar nearly optimal sample complexity.
\end{abstract}

\section{Introduction}\label{sec:intro}

% Regularization has been a theoretically convincing \citep{howard1972risk,sutton2018reinforcement,lattimore2020bandit} and empirically influential \citep{williams1992simple,ziebart2008maximum,schulman2017proximal,haarnoja2018soft,rafailov2023direct} idea in reinforcement learning (RL), especially when the environmental feedback is imperfect; for example, direct cumulative reward maximization is potentially error-prone when the reward is adversarially chosen or corrupted \citep{cai2020provably,neu2020efficient,liu2024corruption}, various safety constraints are critical \citep{achiam2017constrained,efroni2020exploration}, or only a perturbation of an existing policy instead of an exhaustive exploitation is needed \citep{ouyang2022training,gao2023scaling}. When confronting defective or even adversarial environments, randomization is often vital for agents \citep{auer2002nonstochastic,lykouris2018stochastic,hajiesmaili2020adversarial}, specifically, regularizers eliciting specific formats of randomness from the learned policy in addition to reward optimization are helpful in these cases. In practice, almost each of these desirable regularizers, except hard-coded constraints \citep{brunke2022safe}, reduces to an efficiently computable divergence between randomized policies (probability kernels).
% Conceptually, divergence-regularized objectives largely take the form of
% \begin{align}\label{eq:conceptual}
%     \max_{\pi } \EE_{\pi}[\mathrm{rewards}] - \mathrm{divergence}(\pi\|\piref).
% \end{align}\qingyue{intro temporarily relies on this conceptual formulation. TODO: shave it off.}


%Due to the data-hungry and instable training nature of reinforcement learning, KL regularization has been widely used in online decision making, which forces the learned policy within the neighborhood of the reference policy measured by Kullbackâ€“Leibler (KL) divergence. %the latter of which can be given either from previous iterations of the current algorithm execution or externally.
Due to the data-hungry and instable nature of reinforcement learning (RL), divergences that are \emph{straightforward to estimate via Monte Carlo methods} or \emph{amenable to constrained optimization} stand out from numerous candidates \citep{renyi1961measures,muller1997integral,basseville2013divergence} as regularizers; the former family of which is typically $f$-divergence  because any of them is an expectation under one policy (thus easy for on-policy estimation) \citep{levine2018reinforcement,levine2020offline}, and the latter are naturally cases of Bregman divergence \citep{bregman1967relaxation} induced by strongly convex functions because they are suitable for scalable optimization algorithms over the probability simplex by design \citep{lattimore2020bandit,lattimore2024bandit}. 
In particular, \emph{Kullback-Leibler (KL) divergence} is the \emph{only} one at the intersection of Bregman divergence and $f$-divergence \citep[Theorem~5]{jiao2014information}, indicating that it is uniquely favorable among common choices from both computational and statistical aspects. Therefore, \emph{the KL-regularized RL objective is arguably the most popular one in practice}:
% \begin{align}\label{eq:obj-teaser}
%     J(\pi) = \EE_{\pi}[r(s,a)] - \frac{1}{\eta}\KL({\pi}\|{\piref}),
% \end{align}
\begin{align}\label{eq:obj-teaser}
    J(\pi) = \EE_{\pi}[r] - \frac{1}{\eta}\KL({\pi}\|{\piref}),
\end{align}
where $r$ is a reward function, $\pi$ is the policy to be optimized,  $\piref$ is a reference policy, $\KL({\pi}\|{\piref})$ is the reverse KL divergence, and $\eta>0$ is the inverse temperature.
% \begin{align}\label{eq:obj-teaser}
%     J(\pi) = \EE_{\pi}[ r ] - \KL({\pi}\|{\piref}).
% \end{align}
 When $\piref$ is chosen to be the uniform distribution, the objective function reduces to the (Shannon) entropy-regularized RL objective that encourages diverse actions and enhances robustness \citep{williams1992simple,ziebart2008maximum,ziebart2010modeling,levine2013guided,fox2015taming,levine2016end,haarnoja2018soft,richemond2024offline,liu2024enhancing}. 
The KL-regularized objective has also been widely used in the RL fine-tuning of large language models (LLMs) \citep{ouyang2022training,rafailov2023direct,rafailov2024r}, where $\piref$ is a pretrained LLM. 

%Extensive empirical successes corroborate the crucial role of this type of .

%given $\piref$ with decent language generation abilities can alleviate reward hacking when the reward model $r$ in \Cref{eq:obj-teaser} is partially flawed in langauge modeling

%For online policy optimization (PO) \citep{schulman2015trust,schulman2017proximal} or more broadly on-policy preference-based RL \citep{chen2024self,rosset2024direct,wu2024self}, a popular choice of $\piref$ to rein performance improvement is the policy obtained in the previous iteration. Synergies between these KL (and other divergence-based) regularizers have also been promising \citep{kozuno2022kl,chang2024dataset,huang2024correcting}.

There has also been a surge of interest in understanding the principle behind KL-regularized RL. \citet{ahmed2019understanding,liu2019regularization} studied by ablation the effect of entropy regularization on the stability and policy improvement in policy optimization, the regret of which has been rigorously settled under the classic online mirror descent framework \citep{cai2020provably,he2022near,ji2023horizon}. \citet{neu2017unified}
unified popular KL-regularized policy optimization algorithms under a convex optimization framework, but the interplay of which with data were left untouched. A series of  work \citep{geist2019theory,vieillard2020leverage,kozuno2022kl} then analyzed the sample complexity of algorithms using KL w.r.t. the previous iteration or/and entropy regularizers with improved dependence on the effective horizon in discounted Markov decision processes (MDPs). However, the metric of suboptimality in all these studies is still with respect to the unregularized reward maximization objective, and its optimal sample complexity is $O(\epsilon^{-2})$ for finding the $\epsilon$-optimal policy.

Several recent studies \citep{xiong2024iterative,xie2024exploratory,zhao2024sharp} switched the focus to suboptimality guarantee with respect to the regularized objective in \eqref{eq:obj-teaser}. In particular, \citet{xiong2024iterative} proposed an Offline GSHF algorithm via the principle of \emph{pessimism in the face of uncertainty}, and proved $O(\epsilon^{-2})$ sample complexity under single-policy concentrability (See \Cref{sec:cb:setup} for detailed definitions of concentrability.). On the other hand, the sharp analysis in \citet{zhao2024sharp} yields the optimal sample complexity $O(\epsilon^{-1})$, but requires all-policy concentrability \citep[Definition~2.6]{zhao2024sharp}, i.e., the data-generating policy $\piref$ is required to cover the entire function class for all possible policies. \citet{xie2024exploratory} studied token-level Markov decision processes (MDPs) and proposed a KL-regularized RL algorithm named XPO, which achieves $O(\epsilon^{-2})$ sample complexity under all-policy concentrability. In fact, even for offline contextual bandits \citep[Chapter~3]{foster2023foundations} with $\iid$ data, which is among the simplest setting for offline decision making, existing sample complexity under KL-regularization are still primitive. 
%since its risk decomposition \citep[Lemma~3.1]{xie2024exploratory} did not take advantage of the strong concavity of $J(\cdot)$
Thus, a natural question arises:
\begin{center}
\emph{Is the $\tilde{O}(\epsilon^{-1})$ sample complexity w.r.t. \Cref{eq:obj-teaser} achievable for offline KL-regularized contextual bandits under \textbf{single-policy concentrability}?}
\end{center}
We answer this question affirmatively by proposing a novel yet minimalist approach based on the principle of pessimism \citep{jin2021pessimism}. Our algorithm can deal with general reward function which belongs to a reward function class with finite covering entropy. The optimal sample complexity is also due to our proof technique, which leverages the strong convexity of KL regularization and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class.

\newcolumntype{g}{>{\columncolor{LightCyan}}c}
\begin{table}[t!]
\caption{Comparison of sample complexity for finding $\epsilon$-optimal policy for \textbf{offline} contextual bandits. $\polylog(\epsilon^{-1})$ and the dependence of other parameters are omitted here. C(D)Bs stands for contextual (dueling) bandits. The last column indicates whether the bounds hold under single-policy concentrability.}\label{tab:teaser}
\vspace*{0.1in}
\centering
\begin{tabular}{gggg}
\toprule
 \rowcolor{white} {Reference}\tablefootnote{The four existing upper bounds are adapted from the implicit form in \citet[Theorem~3.1]{xiong2024iterative} and \citet[Theorem~3.3 and Theorem~4.4.]{zhao2024sharp}. See \Cref{sec:exist-result} for details.}  & {CBs} &  {CDBs} & \textbf{Single}? \\
\midrule
\rowcolor{white}  \citet{xiong2024iterative} & $\tilde{O}(\epsilon^{-2})$ & $\tilde{O}(\epsilon^{-2})$ & \cmark \\
\rowcolor{white}  \citet{zhao2024sharp} & $\tilde{O}(\epsilon^{-1})$  & $\tilde{O}(\epsilon^{-1})$  & \xmark \\
   \textbf{This work} & {$\tilde{O}(\epsilon^{-1})$} & {$\tilde{O}(\epsilon^{-1})$} & \textbf{\cmark} \\
 \rowcolor{white}  Lower bounds & $\tilde{\Omega}(\epsilon^{-1})$ & $\tilde{\Omega}(\epsilon^{-1})$ & \cmark\tablefootnote{The lower bounds in terms of $\epsilon$ are the same under both single- and all-policy concentrability, which are detailed in \Cref{rmk:global-lowerbound-bandit,rmk:global-lowerbound-dueling} respectively.} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Contributions}

\begin{itemize}
    \item We propose a new algorithm dubbed \algcb{} and develop a novel proof technique that simultaneously utilizes the curvature of KL-regularized objectives and integrates pessimism with a novel covariance-based argument, for deriving its $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability. %Both techniques do \textbf{not} hinge on any delicate algorithmic tweaks.
    \item To demonstrate the versatility of our techniques beyond absolute reward feedback, we also extend our results to the offline contextual dueling bandit (CDB) setting, and provide a tight sample complexity analysis.
    \item The matching lower bounds (up to $\polylog$ factors) in both settings suggest that our sample complexity results are nearly optimal, which renders a complete picture of the sample complexity for KL-regularized offline contextual under single policy concentrability.
\end{itemize}

For the ease of comparison, we summarize the relevant sample complexity results along with the corresponding coverage assumptions in \Cref{tab:teaser}.
 


\subsection{Additional Related Work}\label{sec:add-related-works}

We review two additional lines of theoretical progress that are closely related to our algorithm design and analysis. 

\paragraph{Pessimism in Offline RL} 
The principle of pessimism has been underpinning offline RL for both the tabular \citep{rashidinejad2021bridging} and function approximation \citep{jin2021pessimism} settings 
under the name of lower confidence bound (LCB). For contextual bandits, it is behind the adaptively optimal sample complexity analysis \citep{li2022pessimism}.
\citet{shi2022pessimistic} proposed a LCB-based model-free algorithm for tabular RL with near-optimal guarantee.
\citet{jin2021pessimism,xiong2022nearly,di2023pessimistic} utilized LCB in conjunction with the classic least-square value iteration paradigm to derive $\tilde{O}(\epsilon^{-2})$ sample complexity results for model-free RL with function approximation. 
The line of work from \citet{rashidinejad2021bridging,xie2021policy} to \citet{li2024settling} settled the sample complexity
of tabular model-based RL via pessimistic estimators exploiting the variance information.
It is also possible to leverage the idea of pessimism to design model-based algorithms under general function approximation that are at least statistically efficient \citep{xie2021bellman,uehara2021pessimistic,wang2024model}.

However, in terms of risk decomposition, to the best of our knowledge, none of these pessimism-based analyses really goes beyond the performance difference lemma \citep[Lemma~13]{foster2023foundations} or simulation lemma \citep[Lemma~23]{foster2023foundations};
both of which are not able to capture the strong concavity of KL-regularized objectives even in the bandit setting.
The algorithmic idea of using pessimistic least-square estimators under general function approximation in \citet{jin2021pessimism,di2023pessimistic} is similar to ours, but their suboptimality gap is bounded by the sum of  bonuses, the direct adaptation of which to our objective cannot lead to the desired sample complexity.

\paragraph{Offline Contextual Dueling Bandits}

CDBs \citep{dudik2015contextual} is the contextual extension of dueling bandits
in classic literature of online learning from pairwise comparisons \citep{yue2012k,zoghi2014relative}.
Since the empirical breakthrough of preference-based RL fine-tuning of LLMs \citep{ouyang2022training}, the theory of offline CDBs has received more attention 
under linear \citep{zhu2023principled,xiong2024iterative} and general \citep{zhan2022offline,zhao2024sharp,song2024importance,huang2024correcting} function approximation. Preference models without stochastic transitivity \citep{munos2023nash,ye2024theoretical,wu2024self,zhang2024general} are beyond the scope of this work, viz., our preference labels are assumed to follow the Bradley-Terry Model \citep{bradley1952rank}.

% \citep{wu2024self,zhang2024general}


\paragraph{Notation} Boldfaced lower case (resp. upper case) letters are reserved for vectors (resp. matrices). The sets $\cS$ and $\cA$ are assumed to be countable throughout this paper to avoid measure-theoretic arguments.
% $[N] \coloneqq \{1, \cdots, N\}$ for any positive integer $N$. 
For nonnegative sequences $\{x_n\}$ and $\{y_n\}$, we write $x_n = O(y_n)$ if $\limsup_{n\to\infty}{x_n}/{y_n} < \infty$, and write $y_n = \Omega(x_n)$ if $x_n = O(y_n)$. We further employ $\tilde{O}(\cdot)$ and $\tilde{\Omega}(\cdot)$ to hide $\polylog$ factors. 
Given a positive definite $\bSigma \in \RR^{d\times d}$ and $\xb\in \RR^d$, we denote the vector's Euclidean norm by $\|\xb\|_2$ and define $\|\xb\|_{\bSigma}=\sqrt{\xb^\top\bSigma\xb}$.
For coutable $\cX$ and $\cY$, we denote 
% the family of probability measures on $\cX$ by $\Delta(\cX)$ and 
the family of probability kernels from $\cX$ to $\cY$ by $\Delta(\cY|\cX)$. For a pair of probability measures $P \ll Q$ on the same space, the KL divergence from $P$ to $Q$ is $\kl{P}{Q} \coloneqq \int \log({\ud P}/{\ud Q})\ud P$, and their total variation (TV) distance is $\tv{P}{Q} \coloneqq 0.5\int|\ud P - \ud Q|$.  For $f:\cX \to \RR$, its infinity norm is denoted by $\norm{f}_{\infty} \coloneqq \sup_{x\in\cX}|f(x)|$.

\section{KL-Regularized Contextual Bandits}\label{sec:cb}

In this section, we propose a pessimism-based algorithm, dubbed PCB-KL, for offline KL-regularized contextual bandits. In the following subsections, we also showcase our key technical novelty which couples the pessimism of our reward estimator to the non-trivial curvature property of KL regularization.

\subsection{Problem Setup}\label{sec:cb:setup}

We consider contextual bandit, which is denoted by a tuple $(\cS, \cA, r, \piref)$. Specifically, $\cS$ is the context space, $\cA$ is the action space and $r: \cS \times \cA \to [0, 1]$ is the reward function. In the offline setting, the agent only has access to an $\iid$ dataset $\cD = \{(s_i, a_i, r_i)\}_{i=1}^n$. Here $s_i \in \cS$ is the state sampled from a distribution $\rho$, $a_i \in \cA$ is the action taken from a \emph{behavior policy}, and $r_i$ is the observed reward given by $r_i = r(s_i, a_i) + \varepsilon_i$, where $\varepsilon_t$ is $1$-sub-Gaussian~\citep[Definition~5.2]{lattimore2020bandit}.
% At each round $t$, the agent observes a context $s_t \in \cS$ generated from a distribution $\rho$ and take action $a_t$ according to some behavior policy denoted as $\piref$ and observe the reward $r_t = r(s_t, a_t) + \varepsilon_t$, where $\varepsilon_t$ is $1$-sub-Gaussian~\citep[Definition~5.2]{lattimore2020bandit}. In the offline setting, the agent only has access to an $\iid$ dataset $\cD = \{(s_i, a_i, r_i)\}_{i=1}^n$ generated from \todoq{from where?}.
In this work, we consider the KL-regularized objective
\begin{align}
    J(\pi) \coloneqq\EE_{(s,a) \sim \rho \times \pi} \bigg[r(s,a) - \eta^{-1} \log \frac{\pi(a|s)}{\piref(a|s)}\bigg], \label{eq:kl-objective-bandit}
\end{align}
where $\piref$ is a known reference policy and the ``inverse temperature'' $\eta$ controls the intensity of regularization. For simplicity, we assume that $\piref$ is also the behavior policy that generates the dataset $\cD$. {This type of ``behavior regularization'' has been also been studied in \citet{zhan2022offline}}. The unique optimal policy $\pi^* \coloneqq \argmax_{\pi \in \Delta(\cA | \cS)} J(\pi)$ is in the following closed-form (See, e.g., \citealt[Proposition~7.16]{zhang2023ltbook})
\begin{align}\label{eq:opt-exp}
    \pi^*(\cdot|s) \propto \exp\big(\eta \cdot r(s, \cdot)\big), \forall s\in\cS.
\end{align}
A policy $\pi$ is said to be $\epsilon$-optimal if $J(\pi^*) - J(\pi) \leq \epsilon$ and the goal of the agent is to find one such policy based on $\cD$. 
To ensure $\epsilon$-optimality is achievable, we assume that $r$ lies in a known function class $\cF \subset (\cS \times \cA \to [0,1])$, from which the agent obtains her estimator $\hat{f}$. 
% \qingyue{TODO: make the dependency clear by moving the $\cF$ in \Cref{assume:general-function-approx} here and ...}

% \paragraph{General Function Approximation.} There is some known function class $\cF: \cS \times \cA \to [0,1]$ such that $\exists \ f^* \in \cF$ such that $f^*=r$. 

\begin{assumption}[General Function Approximation]\label{assume:general-function-approx}
    There exists a known function class $\cF: \cS \times \cA \to [0,1]$ such that $\exists \ f^* \in \cF$ such that $f^*=r$. 
\end{assumption}

We also need a standard condition to control the complexity of $\cF$ through the notion of covering number \citep[Definition~5.1]{wainwright2019high}.

\begin{definition}[{$\epsilon$-net and covering number}]
Given a function class $\cG \subset (\cS \times \cA \to \RR)$, a finite set $\cG(\epsilon) \subset \cG$ is an $\epsilon$-net
of $\cG$ w.r.t. $\|\cdot\|_\infty$, if for any $g \in \cG$, there exists $g' \in \cG(\epsilon)$ such that $\| g - g'\|_\infty \leq \epsilon$. The $\epsilon$-covering number is the smallest cardinality $\cN_{\cG}(\epsilon)$ of such $\cG(\epsilon)$.
\end{definition}

\begin{assumption}\label{assume:poly-covering}
For any $\epsilon_c > 0$, the $\epsilon_c$-covering number $\cN_{\cF}(\epsilon_c)$ of $\cF$ is $\poly(\epsilon_c^{-1})$.
\end{assumption}

\begin{remark}
When $\cF$ is the class of linear functions of dimension $d$ and radius $R$, the covering number of $\cF$ is given by $\cN_{\cF} = O((1+R\epsilon^{-1})^d)$ \citep[Lemma~D.6]{jin2020provably}, indicating that the \Cref{assume:poly-covering} on $\cN_{\cF}$ is mild.
\end{remark}




\paragraph{Concentrability} The data quality of $\cD$ collected by $\piref$ is typically characterized by \emph{concentrability} in offline RL \citep{farahmand2010error,chen2019information,jiang2024offline}. In the traditional definition based on density ratio (See, e.g., \citealt[Table~1]{zhan2022offline}), a strong concentrability condition intuitively corresponds to a large $\text{supp}(\piref)$, which reduces the agent's difficulty of learning since (with high probability) $\hat{\pi}$ can hardly underfit on many state-action pairs. The two extreme cases of concentrability coefficients, all- and single-policy concentrability, intuitively correspond to (1) $\text{supp}(\piref)$ covers all possible inputs; and (2) $\text{supp}(\piref)$ only subsumes $\text{supp}(\pi^*)$. Our definition of concentrability, however, is different from the density ratio-based one, because we take the function class $\cF$ into account. In detail, we first introduce $D^2$-divergence as follows.
% At a high level, the behavior policy might not be able to visit all possible state-action pair $(s,a) \in \cS \times \cA$ frequent enough and therefore it is important to make sure that our estimation on the state-action pairs appears in offline dataset can be generalized to those state-action pairs that are scarce in the offline dataset but frequently visited by the estimated policy. 
% These generalization ability is typically characterized by concentrability. 
\begin{definition}\label{def:bandit:D-sq}
Given a function class $\cF \subset ( \cS \times \cA \to \mathbb{R})$ and a fixed policy $\pi$, define the $D^2$-divergence $D^2_{\cF}((s,a);\pi)$ as
\begin{align*}
    \sup_{ f, g \in \cF} \frac{\big( f(s, a) - g(s, a)\big)^2}{\E_{(s',a') \sim \rho \times \pi}[( f(s', a') - g(s', a'))^2]}.
\end{align*}
\end{definition}
The ``Eluder dimension''-type \Cref{def:bandit:D-sq} is directly inspired by \citet{di2023pessimistic,zhao2024sharp}, the intuition behind which is that given $(s,a) \in \cS \times \cA$, a small $D^2$-divergence indicates that for two functions $f$ and $g$, if they are close under the behavior policy $\pi$, then they will also be close on such pair $(s,a)$. Therefore, the $D^2$-divergence
quantifies how well the estimation on dataset collected by the behavior policy $\pi$ can be generalized to a specific state-action pair. We are now ready to define the two notions of  concentrability conditions.
\begin{assumption}[All-Policy Concentrability]\label{assume:all-coverage-bandit}
    Given a reference policy $\piref$, there exists $D > 0$ such that $D^2 = \sup_{(s,a) \in \cS \times \cA} D^2_{\cF}((s,a); \piref) $.
\end{assumption}
\Cref{assume:all-coverage-bandit} indicates that the errors on any state-action pairs can be bounded by the error on the samples from $\rho \times \pi$ up to a factor $D$. 
\begin{assumption}[Single-Policy Concentrability]\label{assume:single-coverage-bandit}
    Given a reference policy $\piref$, there exists $D_{\pi^*} > 0$ such that $D_{\pi^*}^2 = \E_{(s,a) \sim \rho \times \pi^*}[D^2_{\cF}((s,a); \piref)]$.
\end{assumption}
Single-policy concentrability indicates that the errors on the distributions of state-action pairs $\rho \times \pi$ can be bounded by the error on the samples from  $\rho \times \pi$ up to some constant. The single-policy concentrability assumption is strictly weaker than the all-policy concentrability in Assumption~\ref{assume:all-coverage-bandit}.

\subsection{Review of Existing Results}\label{sec:exist-result}

In this subsection, we discuss the direct adaptation of previous results to our setting and demonstrate that their results cannot imply an $\tilde{O}(\epsilon^{-1})$ sample complexity without all-policy concentrability. Specifically, \citet{xiong2024iterative} obtained a performance gap upper bound under linear function approximation
\begin{align*}
    J(\pi^*) - J(\pi) \leq \big\| \EE_{\rho \times \pi^*}[\bphi(s,a)] - \bnu \big\|_{\bSigma_{\text{off}}^{-1}} \eqqcolon \text{RHS},
\end{align*}
where $\bnu$ is the reference vector, $\bphi(s,a) \in \RR^d$ is the feature map, and $\Sigma_{\text{off}} = \sum_{i=1}^n \bphi(s_i, a_i)\bphi(s_i, a_i)^\top$ is the sample covariance matrix. However, we can show that $\text{RHS}$ can be bounded from \emph{below} by
\begin{align*}
    % & \big\| \EE_{(s,a) \sim \rho \times \pi^*}[\bphi(s,a)] - \bnu \big\|_{\bSigma_{\text{off}}^{-1}} \\
    \big\| \EE_{(s,a) \sim \rho \times \pi^*}[\bphi(s,a)] - \bnu \big\| \sqrt{\lambda_{\min}(\bSigma_{\text{off}}^{-1})} 
    & = \big\| \EE_{(s,a) \sim \rho \times \pi^*}[\bphi(s,a)] - \bnu \big\| \lambda_{\max}(\bSigma_{\text{off}})^{-1/2}  \\
    & \geq \big\| \EE_{(s,a) \sim \rho \times \pi^*}[\bphi(s,a)] - \bnu \big\| \text{tr}(\bSigma_{\text{off}})^{-1/2} \\
    & = \big\| \EE_{(s,a) \sim \rho \times \pi^*}[\bphi(s,a)] - \bnu \big\| \bigg(\sum_{i=1}^n \|\bphi(s_i, a_i)\|_2^2 \bigg)^{-1/2} \\
& = \Omega(n^{-1/2}),
\end{align*}
where $\lambda_{\min}$ and $\lambda_{\max}$ is the minimum and maximum eigenvalue of a matrix, the first inequality holds due to the fact that $\xb^\top \bSigma \xb \geq \|\xb\|_2^2 \lambda_{\min}(\bSigma)$ and the second inequality holds due to $\lambda_{\max}(\bSigma) \leq \text{tr}(\bSigma)$.
\citet{zhao2024sharp} proposed a two-stage learning algorithm and obtained an $\tilde{O}(\epsilon^{-1})$ sample complexity for online KL-regularized bandits. The algorithm can be adopted to offline learning by removing the second stage\footnote{This can be done by setting the $n$ in their paper to $0$.} and treat the samples from first stage as the offline dataset. An analogous analysis gives a sample complexity of $\tilde{O}(D^2\epsilon^{-1})$, where $D^2$ is the all-policy concentrability.
% Please refer to Table~\ref{tab:teaser} for a summarization of previous results. 



\subsection{Algorithm}

In this subsection, we present an offline bandit algorithm, $\algcb$, for KL-regularized contextual bandits in \Cref{algorithm:bandit-pess}. %achieving $\tilde{O}(\epsilon^{-1})$ sample complexity with single policy concentrability. 
$\algcb$ first leverages least-square estimator to find a function $\fls \in \cF$ that minimizes its risk on the offline dataset. In~\citet{zhao2024sharp}, such $\fls$ is directly applied to construct the estimated policy. 
% \todok{Add why this not work}
In contrast, we construct a pessimistic estimator of $f^*$ following the well-known pessimism principle in offline RL \citep{jin2021pessimism}.

Specifically, we consider the bonus term defined as
% substract a penalty term $\Gamma_n$ from the least-square estimator $\fls$ to tackle the uncertainty. Specifically, we set
\begin{align}
    \Gamma_n(s,a)= \beta  D_{\cF}\big((s,a),\piref\big), \forall (s,a) \in \cS \times \cA; \label{eq:bonus-bandit}
\end{align}
where
\begin{align}
    \beta = \sqrt{128\log\big(2\cN_{\cF}(\epsilon_c)/\delta\big)/3n + 18 \epsilon_c}. \label{eq:conf-radis-bandit}
\end{align}
We then obtain our pessimistic estimation $\fps$ by setting $\fps = \fls - \Gamma_n$, which is less than $f^*$ with high probability.
After obtaining the pessimistic estimation, $\algcb$ output the policy $\hat{\pi}$, which maximizes the estimated objective
\begin{align*}
    \hat{J}(\pi) =\EE_{(s,a) \sim \rho \times \pi} \bigg[\fps(s,a) - \eta^{-1} \log \frac{\pi(a|s)}{\piref(a|s)}\bigg],
\end{align*}
the maximizer of which is the counterpart of \Cref{eq:opt-exp}, i.e.,
\begin{align*}
    \hat \pi(a|s) \propto \piref(a|s) \exp \big(\eta \cdot \fps(s,a)\big).
\end{align*}


\begin{algorithm}[t]
	\caption{Offline KL-Regularized Pessimistic Contextual Bandit (\algcb)}\label{algorithm:bandit-pess}
	\begin{algorithmic}[1]
	\REQUIRE regularization $\eta$, reference policy $\piref$, function class $\cF$, offline dataset $\cD = \{(s_i,a_i,r_i)\}_{i=1}^n$

\STATE Compute the least square estimation of reward function\label{line:bandit-lsq}
    \begin{align*}
        \fls \in \argmin_{f \in \cF} \sum_{(s_i,a_i, r_i) \in \cD} \big(f(s_i, a_i) - r_i\big)^2
    \end{align*}
    \STATE Let $\fps \leftarrow \fls - \Gamma_n$, where $\Gamma_n$ is the bonus term in \Cref{eq:bonus-bandit} \label{line:bandit:pess}
    \ENSURE $\hat \pi(a|s) \propto \piref(a|s) \exp \big(\eta \cdot \fps(s,a)\big)$
\end{algorithmic}
\end{algorithm}




\subsection{Theoretical Results}

The sample complexity for KL-regularized contextual bandits is settled in this subsection. We first give the upper bound of $\algcb$.

\begin{theorem}\label{thm:bandit}
Under Assumption~\ref{assume:single-coverage-bandit}, for sufficiently small $\epsilon \in (0, 1)$, if we set $\Gamma_n$ as in~\eqref{eq:bonus-bandit}, then $n = \tilde{O}(\eta D_{\pi^*}^2\epsilon^{-1})$ suffices to guarantee the output policy $\hat \pi$ of Algorithm~\ref{algorithm:bandit-pess} to be $\epsilon$-optimal with probability at least $1 - \delta$.
% \footnote{The burn-in cost of our $m$ might scales linearly with $\eta^2 \big( D_{\pi^*, \mathrm{semi}}^2 \big)^2$}
\end{theorem}

\begin{remark}
Previously, \citet{zhao2024sharp} achieved an $\tilde{O}(\epsilon^{-1})$ sample complexity under Assumption~\ref{assume:all-coverage-bandit}. As a comparison, $\algcb$ achieves the same $\tilde{O}(\epsilon^{-1})$ sample complexity but only requiring Assumption~\ref{assume:single-coverage-bandit}, which is weaker than Assumption~\ref{assume:all-coverage-bandit}
\end{remark}
We also provide the sample complexity lower bound of KL-regularized contextual bandits in the following theorem.

\begin{theorem}\label{thm:lowerbound-bandit}
    For any $\epsilon \in (0, 1), \eta > 0$, and algorithm \textsf{Alg}, there is a KL-regularized contextual bandit instance with $O(1)$ single-policy concentrability such that \textsf{Alg} requires at least $\Omega\big(\min\{{\eta \log \cN_{\cF}(\epsilon)}/{\epsilon}, {\log \cN_{\cF}(\epsilon)}/{\epsilon^2}\}\big)$ samples to return an $\epsilon$-optimal policy.
\end{theorem}

\begin{remark}\label{rmk:global-lowerbound-bandit}
Theorem~\ref{thm:lowerbound-bandit} is concluded by plugging $D^2 \geq D^2_{\pi^*}$ to Theorem~3.1 of \citet{zhao2024sharp} and thus the sample complexity lower bound also holds for the KL-regularized contextual bandit instance with $O(1)$ all-policy concentrability.
\end{remark}

\begin{remark}
Theorem~\ref{thm:lowerbound-bandit} shows that when $\epsilon$ is sufficiently small, any algorithm for offline KL-regularized contextual bandits requires at least $\tilde{\Omega}(\epsilon^{-1})$ samples to output an $\epsilon$-optimal policy. This sample complexity lower bound matches the sample complexity upper bound in Theorem~\ref{thm:bandit}, indicating that $\algcb$ is nearly optimal.
\end{remark}



\subsection{Proof of Theorem~\ref{thm:bandit}}

In this section, we finish the proof of \Cref{thm:bandit}. At a high level, if we consider the regularized objective \Cref{eq:obj-teaser} multi-arm bandits, then $P\mapsto \kl{P}{Q}$ is $1$-strongly convex w.r.t. $\tv{\cdot}{\cdot}$ \citep[Exercise~I.37]{polyanskiy2024information}, and thus $J(\pi)$ is strongly concave. Therefore, $J(\pi^*) - J(\hat{\pi})$ is possible to be of the order $[\tv{\pi^*}{\hat{\pi}}]^2 \approx \tilde{O}(n^{-1})$, pretending that $\pi^*$ is the unconstrained maximizer. This intuition guides our analysis for contextual bandits.
% which is unfolded in this section.

We begin the proof with the definition of the event $\confbandit(\delta)$ given $\delta>0$ as
\begin{align}\label{eq:confbandit}
    \confbandit(\delta) \coloneqq \bigg\{ \sup_{(s,a) \in \cS \times \cA} \Big[ \big| \fls - f^*\big| - \Gamma_n \Big] (s,a) \leq 0 \bigg\},
\end{align}
where $\Gamma_n$ is defined in~\eqref{eq:bonus-bandit}. Event $\confbandit(\delta)$ holds indicates that the least square estimation $\fls$ obtained in Line~\ref{line:bandit-lsq} of Algorithm~\ref{algorithm:bandit-pess} does not deviate too much from the true function $f^*$. More specifically, we have the following lemma, whose proof are deferred to  Please refer to Appendix~\ref{app:proof-banditconf}.

\begin{lemma}\label{lem:bandit:conf}
For all $\delta > 0$, $\confbandit(\delta)$ holds with probability at least $1-\delta$.
\end{lemma}

The following covariance-type observation is the first pivot.
\begin{lemma}\label{lem:non-positive-cov}
    If a bounded random variable $X \leq 0$ almost surely, then $\EE[X^3] - \EE[X^2]\EE[X] \leq 0$.
\end{lemma}
\begin{remark}
    While this lemma is elementary, to the best of our knowledge, we are the first to isolate this structure from our non-standard analysis of offline RL, from which the sharp upper bound is derived.
\end{remark}
\begin{proof}[Proof of Lemma~\ref{lem:non-positive-cov}]
    We define $Y = -X$. Then it suffices to show that the covariance between $Y$ and $Y^2$ is
\begin{align*}
    \Cov(Y, Y^2) &= \EE[Y^3] - \EE[Y^2]\EE[Y]  \\ 
    & \geq \big(\EE[Y^2]\big)^{3/2} - \EE[Y^2]\EE[Y] \\
    & =  \big(\EE[Y^2]\big)\big( \sqrt{\EE[Y^2]} - \EE[Y] \big) \\
    & \geq 0,
\end{align*}
where both inequalities follow from Jensen's inequality.
\end{proof}

We further define the following quantities. For all $\gamma \in [0,1]$, we define
\begin{align*}
    f_\gamma \coloneqq \gamma \fps + (1 - \gamma)f^*,
\end{align*}
and based on which we further denote
\begin{align}
% & Z_f(\cdot) \coloneqq \sum_{a\in \cA} \piref(a|\cdot)\exp\big(\eta f(\cdot,a) \big), \forall f\in\cF; \label{eq:bandit:partition}\\
% & f_\gamma \coloneqq \gamma \fps + (1 - \gamma)f^*, \forall \ \gamma \in [0,1]; \\
& \pi_{\gamma}(\cdot|s) \propto \piref(\cdot|s)\exp\big( \eta  f_\gamma(s, \cdot)\big), \forall s \in \cS; \notag \\
& G(\gamma) \coloneqq \EE_{\rho \times \pi_{\gamma}}\Big[ \big(\fps - {f^*}\big)^2(s,a) \Big]. \notag
\end{align}
The key to our analysis is the monotonicity of the function $G(\gamma)$ in $\gamma$, which is formally stated in the following lemma.

\begin{lemma}\label{lem:expansion-to-endpoint}
On event $\confbandit$, $0 \in \argmax_{\gamma \in [0, 1]} G(\gamma)$.
\end{lemma}
% \qingyue{TODO: draw the connection between this derivative with the derivation of policy gradient, since the process of calculating $G'(\gamma)$ is straight but tedious, a ``freshman'' reviewer is expected to spend several minutes verifying the correctness of the first equality in the following equation block.}
\begin{proof} For simplicity, we use $\triangle(s,a)$ to denote $\big(\fps - {f^*})(s,a)$ in \emph{this} proof.
Then we know that $\triangle(s,a) \leq 0$ for all $(s,a) \in \cS \times \cA$ on event $\confbandit$. 
The most direct way to prove is to take derivative of $G$ with respect to $\gamma$,  which corresponds to the policy gradient~\citep{sutton1999policy} of $\pi_\gamma$ and thus implying a favorable structure. A direct calculation yields that
\begin{align*}
    % & \\
    G'(\gamma) & = \EE_{\rho \times \pi_\gamma}\big[\nabla_{\gamma}\log \pi_\gamma(a|s)\triangle(s,a)^2 \big] \\
    &  = \eta \EE_{\rho}\EE_{a\sim \pi_{\gamma}}\big[ \triangle^2(s,a) \big( \triangle(s,a) - \EE_{a'\sim \pi_{\gamma}}[\triangle(s,a')] \big) \big] \\
    & = \eta \EE_{\rho}\Big[ \EE_{\pi_{\gamma}} \big[ \triangle^3(s,a) \big] - \EE_{\pi_{\gamma}}\big[ \triangle^2(s,a) \big]\EE_{\pi_{\gamma}}\big[ \triangle(s,a) \big] \Big] \\
    &  \leq 0,
\end{align*}
where $\EE_{\rho}$ is the shorthand of $\EE_{s \sim \rho}$, $\EE_{\pi_{\gamma}}$ is the shorthand of $\EE_{a \sim \pi_{\gamma}}$, the first equation is derived from standard policy gradient and the inequality holds conditioned on the event $\confbandit(\delta)$ due to Lemma~\ref{lem:non-positive-cov}.
\end{proof}

Finally, we need the following lemma to bound the performance difference between two policy w.r.t. the KL-regularized objective $J$. For any given $f: \cS \times \cA \to \RR$, we define $\pi_{f}(\cdot|s) \propto \exp\big(\eta \cdot f(s, \cdot)\big), \forall s \in \cS$ to facilitate presentation;
then $\pi^* = \pi_{f^*}$ by definition. Please refer to Appendix~\ref{app:proof-taylor-expansion} for the proof of the lemma.
\begin{lemma}\label{lem:taylor-expansion}
% Let $J$ be the objective defined in~\eqref{eq:kl-objective-bandit} and 
Given $f:\cS \times \cA \to \RR$,
% Let $\pi_f$ be the policy such that $\pi_f(a|s) \propto \piref(a|s) \exp\big(\eta f(s,a)\big)$, 
% then 
there exist $\gamma \in [0,1]$ such that for $f_{\gamma} = \gamma f + (1-\gamma) f^*$ and $\pi_{\gamma} \coloneqq \pi_{f_\gamma}$,
% $ \propto \exp\big(\eta f_{\gamma}(s,a)\big)$,
\begin{align*}
    J(\pi^*) - J(\pi_f) \leq \eta \EE_{(s,a) \sim \rho\times \pi_{\gamma}} \big[\big(f^* - f\big)^2(s,a) \big].
\end{align*}
\end{lemma}


Now we are ready to prove Theorem~\ref{thm:bandit}.
\begin{proof}[Proof of Theorem~\ref{thm:bandit}]

Following the proof of \citet[Theorem~3.3]{zhao2024sharp}, we know that there exists $\bar{\gamma} \in [0, 1]$ such that
\begin{align}
    J(\pi^*) - J(\hat{\pi}) \leq \eta G(\bar{\gamma}) \leq \eta G(0),
\end{align}
%     \begin{align}
%     & J(\pi^*) - J(\hat{\pi}) \notag \\
%     & \quad = \frac{1}{\eta}\E_\rho\bigg[ \log \frac{Z_{f^*}(s)}{Z_{\fps}(s)} \bigg] + \EE_{\rho\times \hat{\pi}}\Big[ \big(\fps - {f^*}\big)(s,a) \Big] \notag\\
%     & \quad \leq G(\bar{\gamma}),
% \end{align}
where the first inequality holds due to Lemma~\ref{lem:taylor-expansion} and the second inequality holds due to the event $\confbandit$ and Lemma~\ref{lem:expansion-to-endpoint}. The term $G(0)$ can be further bounded by
\begin{align}
     G(0) & = \eta \EE_{(s,a) \sim \rho\times \pi^*}\Big[ \big(\fps - {f^*}\big)^2(s,a) \Big] \notag \\
    &\leq 4 \eta \EE_{(s,a) \sim \rho \times \pi^*} [\Gamma_n^2(s,a)] \notag \\
    & = 4\eta \beta^2 \EE_{(s,a) \sim \rho\times \pi^*}\big[D^2_{\cF}((s,a); \piref)\big] \notag \\
    &= \tilde{O}(\eta D_{\pi^*}^2 n^{-1}),
\end{align}
where the second inequality holds conditioned on $\confbandit(\delta)$ because of Lemma~\ref{lem:expansion-to-endpoint}, and the last inequality follows from the definition of $\confbandit(\delta)$ together with Line~\ref{line:bandit:pess}. By Lemma~\ref{lem:bandit:conf}, we know that event $\confbandit$ holds with probability at least $1- \delta$, which finishes the proof.
\end{proof}




% \subsubsection{Proof of Theorem~\ref{thm:bandit}}


% \begin{lemma}\label{lem:bandit:conf}
% Let $\Gamma_n(s,a)= \beta \sqrt{ D_{\cF}^2((s,a),\piref)}$, where $\beta^2 = 128\log(2\cN_{\cF}(\epsilon_c)/\delta)/3n + 18 \epsilon_c =\tilde{O}(n^{-1})$. Define
% \begin{align}\label{eq:confbandit}
%     \confbandit(\delta) \coloneqq \Big\{ \big| \fls - f^*\big|(s,a) \leq \Gamma_n(s,a), \forall (s,a) \in \cS \times \cA \Big\}.
% \end{align}
% Then $\PP(\confbandit(\delta)) \geq 1 - \delta$.
% \end{lemma}



% Besides this, we provide the following lemma.




% \subsection{Proof of Theorem~\ref{thm:bandit}}

% We define 
% % \begin{align}
% %     Z_f(\cdot) &\coloneqq \sum_{a\in \cA} \piref(a|\cdot)\exp\big(\eta f(\cdot,a) \big), \forall f\in\cF; \label{eq:bandit:partition}\\
% %      f_\gamma &\coloneqq \gamma \fps + (1 - \gamma)f^*, \pi_{\gamma}(\cdot|\cdot) \propto \piref(\cdot|\cdot)\exp\big( \eta f_\gamma(\cdot, \cdot)\big); \\
% %      \textcolor{red}{G(\gamma)} &\coloneqq \EE_{\rho \times \pi_{\gamma}}\Big[ \big(\fps - {f^*}\big)^2(s,a) \Big].
% % \end{align}

% We have the following lemma for the defined quantity.



% \begin{lemma}\label{lem:D-square-bound}
% For any $\alpha \in [0, 1]$ and sufficient large $m$, define $f_\alpha= \alpha \fls + (1 - \alpha)f^*$ and $\pi_{\alpha}(\cdot|\cdot) \propto \piref(\cdot|\cdot)\exp\big( \eta f_\alpha(\cdot, \cdot)\big)$.\footnote{Recall that $\fls$ is defined in Line~\ref{line:dueling-bandit:pess}.}
% % Under Assumption~\ref{assume:single-coverage-bandit} and conditioned on $\confbandit(\delta)$,
% % \begin{align}\label{eq:bandit:D-sq-lower}
% % \E_{(s,a) \sim \rho \times \pi_{a}}[ D^2_{\cF}((s,a); \piref)] -
% %     \E_{(s,a) \sim \rho \times \pi^*}[D^2_{\cF}((s,a); \piref)] \gtrsim -\sqrt{\beta}C_{\pi^*}^3.
% % \end{align}
% Under Assumption~\ref{assume:single-coverage-bandit} and conditioned on $\confbandit(\delta)$,
% \begin{align}\label{eq:bandit:D-sq-upper}
% \E_{(s,a) \sim \rho \times \pi_\alpha}[ D^2_{\cF}((s,a); \piref)] -
%     \E_{(s,a) \sim \rho \times \pi^*}[D^2_{\cF}((s,a); \piref)] \leq  D_{\pi^*}^2.
% \end{align}
% \end{lemma}
% \begin{proof}[Proof of Lemma~\ref{lem:D-square-bound}]

% Conditioned on $\confbandit(\delta)$, $\forall (s,a) \in \cS \times \cA$,
% \begin{align}\label{eq:a-pess}
%     f_\alpha(s,a) = \alpha \fls(s,a) + (1 - \alpha)f^*(s,a) \leq \alpha f^*(s,a) + (1 - \alpha)f^*(s,a) \leq f^*(s,a).
% \end{align}
% Therefore, we know that the left-hand-side of~\eqref{eq:bandit:D-sq-upper} can be bounded as follows.
% \begin{align}
%     \text{LHS} &= \sum_{s,a}\rho(s)(\pi_\alpha - \pi^*)(a|s) D^2_{\cF}((s,a); \piref) \notag\\
%     &= \E_{s\sim\rho}\sum_{a} \frac{\piref(a|s)D^2_{\cF}((s,a); \piref)}{Z_{f_\alpha}(s) Z_{f^*}(s)} \Big[ Z_{f^*}(s)\exp\big( \eta f_\alpha(s,a) \big) - Z_{f_\alpha}(s)\exp\big( \eta f^*(s,a) \big) \Big]  \notag\\
%     &\leq \E_{s\sim\rho}\sum_{a} \frac{\piref(a|s)D^2_{\cF}((s,a); \piref)}{Z_{f_\alpha}(s) Z_{f^*}(s)} \exp\big( \eta f^*(s,a) \big)\big[ Z_{f^*}(s) - Z_{f_\alpha}(s)\big] \notag \\
%     & = \E_{s\sim\rho} \Bigg(\frac{1}{Z_{f_\alpha}(s)}\big[ Z_{f^*}(s) - Z_{f_\alpha}(s)\big]\Bigg)\sum_{a} \frac{\piref(a|s) \exp\big( \eta f^*(s,a) \big)}{Z_{f^*}(s)} D^2_{\cF}((s,a); \piref) 
% \end{align}
% where the inequality holds due to $f_\alpha \leq f^*$ and therefore $Z_{f_\alpha}(s) \leq Z_{f^*}(s)$. It remains to bound the first term.
% \begin{align}
%     \text{LHS} &= \E_{s\sim\rho}\sum_{a} \frac{ D^2_{\cF}((s,a); \piref)}{Z_{f_\alpha}(s)  } \pi^*(a|s) \times \notag\\
%     &\quad \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big)\Big( 1 - \exp\big(\eta [f_\alpha - f^*](s,a') \big) \Big) \notag\\
%     &\leq  \E_{s\sim\rho}\sum_{a} \frac{ D^2_{\cF}((s,a); \piref)}{Z_{f_\alpha}(s)  } \pi^*(a|s) \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big) \eta [f^* - f_\alpha](s,a') \\
%     &\leq \E_{s\sim\rho}\sum_{a} \frac{ D^2_{\cF}((s,a); \piref)}{Z_{f_\alpha}(s)  } \pi^*(a|s) \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big) 2\eta b_m(s,a')  \\
%     &= 2\eta \sqrt{\beta} \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big)  \sqrt{D^2_{\cF}((s,a); \piref)} }{\sum_{a''\in \cA} \piref(a''|s)\exp\big(\eta f_{\alpha}(s,a'') \big)  }  \\
%     &\leq 2\eta \sqrt{\beta} \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big)  \sqrt{D^2_{\cF}((s,a); \piref)} }{\sum_{a''\in \cA} \piref(a''|s)\exp\big(\eta [{f}^* - 2b_m](s,a'') \big)  } \\
%     % &\leq 2\eta \sqrt{\beta} \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \sum_{a'}\piref(a'|s)\exp\big( \eta f^*(s,a') \big)  \sqrt{D^2_{\cF}((s,a); \piref)} }{\sum_{a''\in \cA} \piref(a''|s)\exp\big(\eta {f}^*(s,a'')\big) (1 - 2b_m(s,a''))   }
%     &=   \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \EE_{a' \sim \pi^*(\cdot|s)} \big[2  \eta b_m(s,a') \big] }{\EE_{a'' \sim \pi^*(\cdot|s)} \Big[ \exp\big(-2 \eta b_m(s,a'')\big) \Big] } \\
%     &\leq \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \EE_{a' \sim \pi^*(\cdot|s)} \big[2  \eta b_m(s,a') \big] }{ \exp\big(- \EE_{a'' \sim \pi^*(\cdot|s)}[ 2 \eta b_m(s,a'') ]\big)  } \\
%     &\leq \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) \frac{ \EE_{a' \sim \pi^*(\cdot|s)} \big[2  \eta b_m(s,a') \big] }{ 1 - \EE_{a'' \sim \pi^*(\cdot|s)}[ 2 \eta b_m(s,a'') ] } \\
%     &\leq \E_{(s,a)\sim\rho \times \pi^*} D^2_{\cF}((s,a); \piref) = D_{\pi^*}^2.
% \end{align}
% Here, the last inequality follows from the fact that, $\EE_{a' \sim \pi^*(\cdot|s)} \big[2  \eta b_m(s,a') \big] \leq 0.5$ for sufficiently large $m$ as long as Assumption~\ref{assumption:semi} holds. 

% \end{proof}



% \hz{Adding details regarding discussion on 12/30}\qz{This is for optimism, but pessimism  works for $\gamma = 0$}

% \begin{align*} 22
% J(\pi^*) - J(\hat \pi) &= \frac{1}{\eta} \EE_\rho \biggl[
% \log \frac{Z_{f^*}(s)}{Z_{\fls}(s)}\biggr] + \EE_{\rho\times \hat{\pi}}\Big[ \big(\fls - {f^*}\big)(s,a) \Big] \\&\le \gamma \cdot \EE_{\rho \times \pi_{\gamma}}\Big[ \big(\fls - {f^*}\big)^2(s,a) \Big]
% \end{align*}

% \begin{align*} 
%     &\frac{\partial \bigl(f(x, a) - f^*(x, a)\bigr) \cdot \pi_{f}^\eta(a|x)}{\partial f(x, a)} \\&= \pi_{f}^\eta(a|x) + \eta \bigl(f(x, a) -f^*(x, a)\bigr) \cdot \Bigl(\frac{\pi_0(a|x) \cdot \exp\bigl(\eta f(x, a)\bigr)}{Z_f^\eta(x)} - \frac{\bigl[\pi_0(a|x) \cdot \exp\bigl(\eta \cdot f(x, a)\bigr)\bigr]^2}{[Z_f^\eta(x)]^2}\Bigr) \le 0.
% \end{align*}

% \clearpage


% \subsection{Optimality of \algcb}\label{subsec:opt-cb}




%\section{Technical Overview}
%\citet{zhao2024sharp}


\section{KL-Regularized Contextual Dueling Bandits}\label{sec:cdb}

In this section, we extend our algorithm to KL-regularized dueling bandit setting, where the learner receives preference feedback instead of absolute signals. We formally introduce setting and extended algorithm. Except the objective, the core setting follows \citet{zhu2023principled,zhan2023provable}. Our objective follows \citet{xiong2024iterative,zhao2024sharp}.

\subsection{Problem Setup}\label{sec:duel-setup}

 We still\footnote{We overload some notations in \Cref{sec:cb} by their dueling counterparts for notational simplicity.} consider contextual bandits $(\cS, \cA, r, \piref)$ where $\cS$ is the state space, $\cA$ is the action space and $r: \cS \times \cA \to [0, 1]$ is the reward function. But only relative preference feedback is available, viz., we have an $\iid$ offline dataset $\cD = \{(s_i,a^1_i, a^2_i, y_i)\}_{i=1}^n$, where $s_i \in \cS$ is generated from distribution $\rho$ and $a^1_i, a^2_i \sim \piref$. The binary preference label $y_i=1$ indicates $a_i^1$ is preferred over $a_i^2$ and $0$ for the opposite. Given a state $s$ and two corresponding action $a^1$ and $a^2$, let $y$ be the preference label such that $y=1$ indicates $a^1 \succ a^2$ and $y=0$ for $a^2 \succ a^1$. In this work we consider the Bradley-Terry Model, where $\mathbb{P}[y=1|s,a^1, a^2] = \sigma(r(s_i, a^1_i) - r(s_i, a^2_i))$, where $\sigma(x) = (1+e^{-x})^{-1}$ is the link function.
The objective here identical to~\eqref{eq:kl-objective-bandit} 
% and we restate it here for completeness.
% \begin{align*}
%     J(\pi) \coloneqq\EE_{(s,a) \sim \rho \times \pi} \bigg[r(s,a) - \eta^{-1} \log \frac{\pi(a|s)}{\piref(a|s)}\bigg].
% \end{align*}
and thus the optimal policy $\pi^*$ also follows the closed form in~\eqref{eq:opt-exp}. Our goal is still to find an $\epsilon$-optimal policy. To control the complexity of the function class $\cF$, we assume that Assumption~\ref{assume:general-function-approx} still holds here.

% We make the same assumption to control the complexity of the function class $\cF$ and restate it here for completeness.

% \paragraph{General Function Approximation.} There is some known function class $\cF$ such that $\exists \ f^* \in \cF$ such that $f^*=r$. We assuem $f(s,a) \in [0, 1]$ for any $(s,a) \in \cS \times \cA$ and $f \in \cF$.

\paragraph{Concentrability} 
Analogous to Section~\ref{sec:cb}, we need our estimation from offline dataset generalizable to the state-action pairs visited by our obtained policy, and therefore requires similar $D^2$-divergence and corresponding policy coverage conditions. However, in dueling bandit, we are unable to observe and estimate the absolute value of rewards. Therefore, intuitively, the best estimation $f$ we can achieve is that for any state $s$ and actions $a^1, a^2$, our estimated $f(s,a^1) - f(s,a^2) \approx r(s,a^1) - r(s,a^2)$. This implies that there exists some mapping $b: \cS \to [-1, 1]$ such that $f(s,a) - b(s) \approx r(s,a)$ on the offline data. Consequently, it is natural to define the the following modified $D^2$-divergence for characterizing how the error $f(s,a) - b(s) - r(s,a)$ generalize to unseen samples.

% \begin{definition}[{\citealt[Definition~2.6]{zhao2024sharp}}]\label{def:dueling-bandit:D-sq}
\begin{definition}\label{def:dueling-bandit:D-sq}
Given a class of functions $\cF \subset (\cS \times \cA \to \mathbb{R})$ and some policy $\pi$, let $\cB = (\cS \to [-1, 1])$ be the function class, define the $D^2$-divergence $D^2_{\cF}((s,a);\pi)$ as
\begin{align*}
    \sup_{ f, g \in \cF} \inf_{b \in \cB} \frac{\big( f(s, a) - g(s, a) - b(s)\big)^2}{\rE_{s\sim \rho}\Var_{a' \sim \pi(\cdot | s')}[f(s', a') - g(s', a')]}.
\end{align*}
\end{definition}

% \begin{remark}
A similar definition has been introduced in \citet[Definition~2.6]{zhao2024sharp}, which underpins the following two assumptions characterizing the coverage ability of $\piref$. 
% \end{remark}

Equipped with the overloaded $D^2$-divergence definition, we are now able to define all-policy concentrability and single concentrability similar as in Section~\ref{sec:cb}.

\begin{assumption}[All-Policy Concentrability]\label{assume:all-coverage-dueling}
    Given a reference policy $\piref$, there exists $D > 0$ such that $D^2 = \sup_{(s,a) \in \cS \times \cA} D^2_{\cF}((s,a); \piref) $.
\end{assumption}

\begin{assumption}[Single-Policy Concentrability]\label{assume:single-coverage-dueling}
    Given a reference policy $\piref$, there exists $D_{\pi^*} > 0$ such that $D_{\pi^*}^2 = \E_{(s,a) \sim \rho \times \pi^*}[D^2_{\cF}((s,a); \piref)]$.
\end{assumption}

\begin{remark}
    Similar single-policy concentrability assumptions have appeared in previous work in offline contextual dueling bandits. Specifically, \citet{huang2024correcting} defined an $L_\infty$-concentrability coefficient $\cC_{\infty}^{\pi^*} = \sup_{(s, a) \in \cS \times \cA} \big(\frac{\pi^*(a|s)}{\piref(a|s)}\big)$. \citet{song2024importance} introduced a stronger assumption that $\cC_{\infty}^{\pi}$ is bounded for all the policy $\pi$ in the KL-ball surrounding $\pi^*$. In our Definition \ref{def:dueling-bandit:D-sq}, instead of simply considering the ratio of probability measure, we introduce a more refined \emph{eluder}-type uncertainty term which also relies on the structure of the model class. As a result, $D_{\pi^*}$ can be small even if $L_\infty$-concentrability coefficient of $\pi^*$ is extremely large as long as the reward model generalizes well after training on the offline dataset.
    There has also been similar notions of partial coverage for the analysis of model-based RL \citep{uehara2021pessimistic,wang2024model}, which fused the information of the transition kernel class to the density-ratio-based formulation.
\end{remark}

\subsection{Algorithm}

We elucidate $\algcdb$ for offline KL-regularized contextual dueling bandits, whose pseudocode is summarized in \Cref{algorithm:dueling-bandit}. $\algcdb$ first estimate the ground truth function $f^*$ on offline dataset with maximum likelihood estimator (MLE) to estimate a function $\fls \in \cF$. After that, analogous to Algorithm~\ref{algorithm:bandit-pess}, we adopt the principle of pessimism in the face of uncertainty. Specifically, we define the penalty term
\begin{align}
    \Gamma_n(s,a)= \beta \sqrt{ D_{\cF}^2((s,a),\piref)}, \label{eq:bonus-dueling-bandit}
\end{align}
where
\begin{align}
    \beta^2 = 128\log(2\cN_{\cF}(\epsilon_c)/\delta)/3n + 18 \epsilon_c =\tilde{O}(n^{-1}) \label{eq:conf-radis-dueling-bandit}
\end{align}
and then subtract it from the MLE $\fls$ to obtain a pessimistic estimator $\fps$.
$\algcb$ then output the policy $\hat{\pi}$, maximizing the estimated objective
\begin{align*}
    \hat{J}(\pi) =\EE_{(s,a) \sim \rho \times \pi} \bigg[\fps(s,a) - \eta^{-1} \log \frac{\pi(a|s)}{\piref(a|s)}\bigg],
\end{align*}
the maximizer of which is in closed form as the counterpart of \Cref{eq:opt-exp}.
\begin{align*}
    \hat \pi(a|s) \propto \piref(a|s) \exp \big(\eta \cdot \fps(s,a)\big).
\end{align*}

\begin{algorithm*}[t]
	\caption{Offline KL-Regularized Pessimistic Contextual Dueling Bandit (\algcdb)}\label{algorithm:dueling-bandit}
	\begin{algorithmic}[1]
	\REQUIRE regularization $\eta$, reference policy $\piref$, function class $\cF$, offline dataset $\cD = \{(s_i,a^1_i, a^2_i, y_i)\}_{i=1}^n$
\STATE Compute the maximum likelihood estimator of the reward function
    \begin{align*}
        \fls = \argmin_{f \in \cF} \sum_{i=1}^n \Big[ y_i \log \sigma \Big(\big[f(s_i, a^1_i) - f(s_i, a^2_i)\big]\Big) + (1-y_i) \log \sigma \Big(\big[f(s_i, a^2_i) - f(s_i, a^1_i)\big]\Big) \Big]
    \end{align*}\label{line:dueling-mle}
    \STATE Let $\fps(s, a) = \fls(s, a) - \Gamma_n(s,a)$, where $\Gamma_n(s, a)$ is the bonus term in \Cref{eq:bonus-dueling-bandit} \label{line:dueling-bandit:pess}
    \ENSURE $\hat \pi(a|s) \propto \piref(a|s) \exp \big(\eta \cdot \fps(s, a)\big)$
\end{algorithmic}
\end{algorithm*}

\subsection{Main Results}

In this subsection, we provide our main results for KL-regularized contextual dueling bandits. The sample complexity upper bound of $\algcdb$ is presented in the following theorem.

\begin{theorem}\label{thm:main-dueling-bandit}
Under Assumption~\ref{assume:single-coverage-dueling}, if we set $\Gamma_n$ according to Lemma~\ref{eq:bonus-dueling-bandit}, then for sufficiently small $\epsilon \in (0, 1)$, with probability at least $1 - \delta$, $n = \tilde{O}(\eta D_{\pi^*}^2\epsilon^{-1})$ is sufficient to guarantee the output policy $\hat \pi$ of Algorithm~\ref{algorithm:dueling-bandit} to be $\epsilon$-optimal.
\end{theorem}

\begin{remark}
\citet{zhao2024sharp} achieved an $\tilde{O}(\epsilon^{-1})$ sample complexity under Assumption~\ref{assume:all-coverage-dueling}. Comparing to \citet{zhao2024sharp}, $\algcdb$ achieves the same $\tilde{O}(\epsilon^{-1})$ sample complexity but only requiring Assumption~\ref{assume:single-coverage-dueling}, which is weaker than Assumption~\ref{assume:all-coverage-dueling}.
\end{remark}

The following theorem provides the sample complexity lower bound for KL-regularized dueling contextual bandits.

\begin{theorem}\label{thm:lowerbound-dueling}
    For any $\epsilon \in (0, 1)$, $\eta > 0$, and offline RL algorithm \textsf{Alg}, there is a KL-regularized contextual dueling bandit instance with $O(1)$ single-policy concentrability such that \textsf{Alg} requires at least $\Omega\big(\min\{{\eta \log \cN_{\cF}(\epsilon)}/{\epsilon}, {\log \cN_{\cF}(\epsilon)}/{\epsilon^2}\}\big)$ samples to return an $\epsilon$-optimal policy.
\end{theorem}

\begin{proof}
See Appendix~\ref{app:proof-thm-lower-bandit}.
\end{proof}

\begin{remark}\label{rmk:global-lowerbound-dueling}
Theorem~\ref{thm:lowerbound-dueling} can be concluded by plugging $D^2 \geq D^2_{\pi^*}$ into \citet[Theorem~4.3]{zhao2024sharp}. This implies that the sample complexity lower bound in Theorem~\ref{thm:lowerbound-dueling} also holds for KL-regularized dueling contextual bandit with all-policy concentrability.
\end{remark}

\begin{remark}
Theorem~\ref{thm:lowerbound-dueling} shows that when $\epsilon$ is sufficiently small, any algorithm for offline KL-regularized contextual dueling bandit requires at least $\tilde{\Omega}(\epsilon^{-1})$ samples to output a $\epsilon$-optimal policy. This sample complexity lower bound matches the sample complexity upper bound in Theorem~\ref{thm:main-dueling-bandit}, indicating that $\algcdb$ is nearly optimal.
\end{remark}


\subsection{Proof of Theorem~\ref{thm:main-dueling-bandit}}

The proof follows the proof in Section~\ref{sec:cb}. At the beginning, we first define the event $\confduelbandit(\delta)$ given $\delta>0$ as
\begin{align}\label{eq:conf-dueling-bandit}
    \confduelbandit(\delta) \coloneqq \Big\{ \exists \ b: \cS \to [-1,1], \forall (s,a) \in \cS \times \cA, 
   \big| \fls(s,a) - b(s) - f^*(s,a)\big| \leq \Gamma_n(s,a) \Big\}.
\end{align}
Here, $\Gamma_n$ is defined in~\eqref{eq:bonus-dueling-bandit}. We abuse the notation and define $b(\cdot)$ as
\begin{align}
    b = \argmin_{\cB} \sup_{(s,a) \in \cS \times \cA} \Phi_b(s,a) - \Gamma_n(s,a), \label{eq:def-bias-dueling}
\end{align}
where $\Phi_b(s,a) = \big| \fls(s,a) - b(s) - f^*(s,a)\big|$ and when $\confduelbandit$ holds, for all $(s,a) \in \cS \times \cA$, we have $\Phi_b(s,a) \leq \Gamma_n(s,a)$
This indicates that the least square estimation $\fls$ obtained in Line~\ref{line:dueling-mle} of Algorithm~\ref{algorithm:dueling-bandit}, after adjusted by some bias function $b$, is close to the true function $f^*$. The following lemma shows that this event holds with high probability. Please refer to Appendix~\ref{app:proof-duelingbandit-conf} for the proof.

\begin{lemma}\label{lem:dueling-bandit:conf}
For any $\delta > 0$, $\PP(\confbandit(\delta)) \geq 1-\delta$.
\end{lemma}

We overload the following quantities. For any $\gamma \in [0,1]$ and $(s,a) \in \cS \times \cA$, we define
\begin{align*}
    f_\gamma(s,a) \coloneqq \gamma (\fps(s,a) - b(s)) + (1 - \gamma)f^*(s,a).
\end{align*}
Furthermore, we introduce the following quantities
\begin{align}
% & f_\gamma(s,a) \coloneqq \gamma (\fps(s,a) - b(s)) + (1 - \gamma)f^*(s,a), \notag \\
% & \quad \forall \ \gamma \in [0,1], \ (s,a) \in \cS\times \cA, \notag \\
& \pi_{\gamma}(\cdot|\cdot) = \pi_{f_\gamma}(\cdot|\cdot) \propto \piref(\cdot|\cdot)\exp\big( \eta  f_\gamma(\cdot, \cdot)\big), \notag \\
& G(\gamma) \coloneqq \EE_{\rho \times \pi_{\gamma}} \big[ \big(\fps(s,a) - b(s) - {f^*}(s,a)\big)^2 \big], \notag
\end{align}
where $b(\cdot)$ is defined in~\eqref{eq:def-bias-dueling}. We still have the monotonicity of the function $G(\gamma)$, which is characterized by the following lemma.

\begin{lemma}\label{lem:expansion-to-endpoint-dueling}
On event $\confduelbandit(\delta)$, $0 \in \argmax_{\gamma \in [0, 1]} G(\gamma)$.
\end{lemma}

\begin{proof} For simplicity, we use $\triangle(s,a)$ to denote $\fps(s,a) - b(s) - {f^*}(s,a)$ in \emph{this} proof.
Then on event $\confduelbandit(\delta)$, we know that $\triangle(s,a) \leq 0$ for all $(s,a) \in \cS \times \cA$. 
% \qingyue{TODO: draw the connection between this derivative with the derivation of policy gradient, since the process of calculating $G'(\gamma)$ is straight but tedious, a ``freshman'' reviewer is expected to spend several minutes verifying the correctness of the first equality in the following equation block.}
Taking derivatives of $G$ w.r.t., $\gamma$ directly, we conclude that for all $\gamma \in [0, 1]$,
\begin{align*}
    G'(\gamma) & = \eta \EE_{\rho}\EE_{a\sim \pi_{\gamma}}\big[ \triangle^2(s,a) \big( \triangle(s,a) - \EE_{a'\sim \pi_{\gamma}}[\triangle(s,a')] \big) \big] \\
    & = \eta \EE_{\rho}\Big[ \EE_{\pi_{\gamma}} \big[ \triangle^3(s,a) \big] - \EE_{\pi_{\gamma}}\big[ \triangle^2(s,a) \big]\EE_{\pi_{\gamma}}\big[ \triangle(s,a) \big] \Big] \\
    &  \leq 0,
\end{align*}
where $\EE_{\rho}$ is the shorthand of $\EE_{s \sim \rho}$, $\EE_{\pi_{\gamma}}$ is the shorthand of $\EE_{a \sim \pi_{\gamma}}$ and the inequality holds conditioned on the event $\confduelbandit(\delta)$ due to Lemma~\ref{lem:non-positive-cov}.
\end{proof}

Finally, we have the proposition that adding some bias term $b: \cS \to \RR$ does not affect the resulting policy.

\begin{proposition} \label{prop:dueling-add-bias}
Let $b: \cS \to \RR$ be some bias function, then for all $f \in \cF$ we have $J(\pi_f) = J(\pi_{f - b})$, where $(f-b)(s,a)=f(s,a) - b(s)$.
\end{proposition}

Please see Appendix~\ref{app:proof-add-bias} for the proof of Proposition~\ref{prop:dueling-add-bias}. Now we are ready to prove Theorem~\ref{thm:main-dueling-bandit}.

\begin{proof}[Proof of Theorem~\ref{thm:main-dueling-bandit}]
We proceed the proof under the event $\confduelbandit(\delta)$. By Proposition~\ref{prop:dueling-add-bias}, we know that
% Let $Z_f(\cdot)$ be defined as in~\eqref{eq:bandit:partition}, we know that
\begin{align*}
    J(\pi^*) - J(\hat{\pi}) & = J(\pi^*) - J(\pi_{\fps}) \\
    & = J(\pi^*) - J(\pi_{\fps-b}).
\end{align*}
% We define $H_s(f) = \log Z_f(s) - \eta\sum_{a \in \cA} \pi_{f}(a|s)(f(s,a) - f^*(s,a))$. Then we know that
% \begin{align*}
%     J(\pi^*) - J(\hat{\pi}) = \frac{1}{\eta}\E_{s \sim \rho}\big[ H_s(f^*) - H_s(\fps)\big].
% \end{align*}
% Given $s$ and any $b(s)$, we also have
% \begin{align*}
%     H_s(\fps) & = \log Z_f(s) - \eta\sum_{a \in \cA} \pi_{\fps}(a|s)(\fps(s,a) - f^*(s,a)) \notag \\
%     &= \log \sum_{a \in \cA} \piref(a|s) \exp(\eta(\fps(s,a) - b(s))) \\
%     & \quad - \eta\sum_{a \in \cA} \pi_{f}(a|s)(\fps(s,a) - f^*(s,a) - b(s)) \\
%     &= H_s(\fps - b).
% \end{align*}
% Consequently, we know that
% \begin{align}
%     J(\pi^*) - J(\hat{\pi}) & =  \frac{1}{\eta}\E_{s \sim \rho}\big[ H_s(f^*) - H_s(\fps)\big] \notag \\
%     & = \frac{1}{\eta}\E_{s \sim \rho}\big[ H_s(f^*) - H_s(\fps - b)\big] \notag \\ 
%     & = J(\pi^*) - J(\pi_{\fps-b}) \label{eq:dueling-policy-w-bias}
% \end{align}
% We now define the following quantities
% \begin{align*}
%     G(\gamma) & \coloneqq \eta^{-1}\EE_{\rho \times \pi_{\gamma}}\big[ \big(\fps(s,a) - b(s) - {f^*}(s,a)\big)^2 \big], \\
%     f_\gamma &= \gamma(\fps - b) + (1-\gamma)f^*, \quad \forall \gamma \in [0,1].
% \end{align*}
Consequently, there exist some $\gamma \in [0,1]$ and $b: \cS \to [-1,1]$ such that
\begin{align}
    J(\pi^*) - J(\hat{\pi}) & = J(\pi^*) - J(\pi_{\fps-b}) \notag \\
    & \leq \eta \EE_{\rho \times \pi_{\gamma}} \big[ \big(\fps(s,a) - b(s) - {f^*}(s,a)\big)^2 \big] \notag \\
    & = \eta G(\gamma), \label{eq:dueling-taylor-expansion}
\end{align}
where the inequality holds due to Lemma~\ref{lem:taylor-expansion}. 
Under event $\confduelbandit(\delta)$, we know that $\fps(s,a) - b(s) \leq {f^*}(s,a)$. Together with Lemma~\ref{lem:expansion-to-endpoint-dueling}, we obtain $G(\gamma) \leq G(0)$. Therefore, we know that 
\begin{align}
J(\pi^*) - J(\hat{\pi}) & \leq G(0) \\
&= \eta \EE_{\rho \times \pi^*}\Big[ \big(\fps(s,a) - b(s) - {f^*}(s,a)\big)^2 \Big] \notag \\
&\leq 4 \eta \EE_{\rho\times \pi^*} \Gamma_n^2(s,a) \notag\\
& = 4\eta \beta^2 \EE_{\rho\times \pi^*}\big[D^2_{\cF}((s,a); \piref) \big]\notag \\
&= \tilde{O}(\eta D_{\pi^*}^2 n^{-1}), \label{eq:final-rate}
% &\leq { 4 \EE_{\rho \times \pi_{\gamma}} b_m^2(s,a) = 4\beta \EE_{\rho \times \pi_{\gamma}} D^2_{\cF}((s,a); \piref)}\\
% &\leq {8\beta D_{\pi^*}^2 \lesssim D_{\pi^*}^2 m^{-1},}
\end{align}
where the inequality holds due to the definition of $\confduelbandit(\delta)$. Plugging~\eqref{eq:final-rate} into~\eqref{eq:dueling-taylor-expansion}, we know that $J(\pi^*) - J(\hat{\pi})$ has upper bound $\tilde{O}(D_{\pi^*}^2 n^{-1})$. By Lemma~\ref{lem:dueling-bandit:conf}, event $\confduelbandit$ with probability at least $1-\delta$, which concludes the proof.
\end{proof}



% \subsection{Optimality of \algcdb}\label{subsec:opt-cdb}


\section{Conclusions and Future Work}
% \qingyue{Empirically, it is hard to separate our results from known ones via simulations/experiments because the contribution is weakening the concentrability assumption.}
In this work, we settled the sample complexity of KL-regularized offline contextual bandits. We first propose $\algcb$, which follows the principle of pessimism for reward estimation and yields the optimal policy accordingly. 
Our analysis leverages the curvature of KL-regularized objectives and integrates pessimism with a newly identified covariance-based observation, which in turn enables a neat refinement of a mean-value-type argument the its extreme point, showing that $\algcb$ achieves a nearly optimal sample complexity under single-policy concentrability. The pair of novel techniques are decoupled from tricky algorithmic tweaks, and thus might be of independent interest.
We then extended $\algcb$ to offline KL-regularized contextual dueling bandit problem and introduced $\algcdb$, the analysis of which manifests the near-optimality of $\algcdb$ under single-policy concentration. 
Currently, our focus is solely on the reverse-KL-regularized objective. It would be an interesting future direction to study more general regularization techniques like $f$-divergence \citep{csiszar1967information} regularization, which includes both forward and reverse KL regularizations.
% Currently, our focus is solely on the reverse KL regularization. We believe it would be a direction to extend the problem of contextual bandit with more general regularization techniques like $f$-divergence \citep{csiszar1967information} regularization.



% \clearpage

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}




\appendix

\section{Missing Proofs from \Cref{sec:cb}}

\subsection{Proof of \Cref{lem:bandit:conf}}\label{app:proof-banditconf}


We first provide the following lemmas.

\begin{lemma}[{\citealt[Lemma~C.1]{zhao2024sharp}}]\label{lem:concen-behavior-bandit}
For any policy $\pi$ and state-action pairs $\{(s_i, a_i)\}_{i=1}^m$ generated i.i.d. from $\rho \times \pi$, and $\epsilon_c < 1$, with probability at least $1-\delta$, for any $f_1$ and $f_2$ we have
\begin{align*}
    \E_{\rho \times \pi}\big[\big(f_1(s,a) - f_2(s,a)\big)^2 \big] \leq \frac{2}{n}\sum_{i=1}^n \big(f_1(s_i,a_i) - f_2(s_i,a_i)\big)^2 + \frac{32}{3n}\log(2\cN_{\cF}(\epsilon_c)/\delta) + 10 \epsilon_c,
\end{align*}
where $\cN_{\cF}(\epsilon_c)$ is the $\epsilon_c$-covering number of $\cF$.
\end{lemma}

\begin{lemma}[{\citealt[Lemma~C.2]{zhao2024sharp}}]\label{lem:concen-reward-bandit}
For arbitrary policy $\pi$ and dataset $\{(s_i, a_i, r_i)\}_{i=1}^m$ generated i.i.d., from the product of $\pi$, $\rho$ and the Bradley-Terry Model; let $\fls$ be the least square estimator of $f^*$, then for any $0< \epsilon_c < 1$ and $\delta > 0$, with probability at least $1-\delta$ we have
\begin{align*}
    \sum_{i=1}^n \big(\fls(s_i,a_i) - f^*(s_i,a_i)\big)^2 \leq 16\log (a\cN_{\cF}(\epsilon_c)/\delta) + 4n\epsilon_c.
\end{align*}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:bandit:conf}]
We have the following inequality
\begin{align}\label{eq:bandit-bonus-d2}
    \big(\fls(s,a) - f^*(s,a)\big)^2 &= \frac{\big(\fls(s,a) - f^*(s,a)\big)^2}{\E_{\piref}\big[\big(\fls(s,a) - f^*(s,a)\big)^2 \big]}\E_{\piref}\big[\big(\fls(s,a) - f^*(s,a)\big)^2 \big] \notag \\
    & \leq  \sup_{f_1, f_2 \in \cF} \frac{\big(f_1(s,a) - f_2(s,a)\big)^2}{\E_{\piref}\big[\big(f_1(s,a) - f_2(s,a)\big)^2 \big]}\E_{\piref}\big[\big(\fls(s,a) - f^*(s,a)\big)^2 \big] \notag \\
    & = D_{\cF}^2((s,a),\piref)\E_{\piref}\big[\big(\fls(s,a) - f^*(s,a)\big)^2 \big],
\end{align}
where the inequality holds by taking supremum to $f_1, f_2 \in \cF$.  Now we have
\begin{align}\label{eq:bandit-bonus-concen}
    \E_{\piref}\big[\big(\fls(s,a) - f^*(s,a)\big)^2 \big] &\leq \frac{2}{n}\sum_{i=1}^n \big(\fls(s_i,a_i) - f^*(s_i,a_i)\big)^2 + \frac{32}{3n}\log(2\cN_{\cF}(\epsilon_c)/\delta) + 10 \epsilon_c \notag \\
    & \leq \frac{2}{n}\big[16\log (\cN_{\cF}(\epsilon_c)/\delta) + 4n\epsilon_c\big] + \frac{32}{3n}\log(2\cN_{\cF}(\epsilon_c)/\delta) + 10 \epsilon_c \notag \\
    & = \frac{128}{3n}\log(2\cN_{\cF}(\epsilon_c)/\delta) + 18 \epsilon_c,
\end{align}
% \todoq{missing $D^2$ in the proof.}
where the first inequality holds due to Lemma~\ref{lem:concen-behavior-bandit} and second holds due to Lemma~\ref{lem:concen-reward-bandit}. Plugging~\eqref{eq:bandit-bonus-concen} into~\eqref{eq:bandit-bonus-d2} and setting $\epsilon_c = O(n^{-1})$ complete the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:taylor-expansion}}\label{app:proof-taylor-expansion}
This proof is extracted from the proof of \citet[Theorem~3.3]{zhao2024sharp} and we present it here for completeness. By definition of our objective in~\eqref{eq:kl-objective-bandit}, we have
\begin{align*} 
& J(\pi^*) - J(\pi_{f}) \\
& \quad = \EE_{(s, a) \sim \rho \times \pi^*} \bigg[f^*(s,a) - \eta^{-1}\log \frac{\pi^*(a|s)}{\piref(a|s)}\bigg] 
- \EE_{(s, a) \sim \rho \times\pi_{f}} \bigg[f^*(s, a) - \frac{1}{\eta}\log \frac{\pi_{f}(a|s)}{\piref(a|s)}\bigg] \\
& \quad = \frac{1}{\eta} \EE_{(s, a) \sim \rho \times\pi^*} \bigg[\log \frac{\piref(a|s) \cdot \exp\bigl(\eta f^*( s, a)\bigr)}{\pi^*(a|s)}\bigg] 
- \frac{1}{\eta} \EE_{(s, a) \sim  \rho \times\pi_{f}} \bigg[\log \frac{\piref(a|s) \cdot \exp\bigl( \eta f^*( s, a)\bigr)}{\pi_{f}(a|s)}\bigg]   \\
& \quad = \frac{1}{\eta} \rE_{s \sim \rho} \big[\log Z_{f^*}(s)\big] - \frac{1}{\eta} \rE_{s \sim \rho} \big[\log Z_{f}(s)\big] - \rE_{s \sim \rho} \bigg[\sum_{a \in \cA} \pi_{f}(a|s) \cdot \big(f^*( s, a) -f( s, a)\big)\bigg],
\end{align*}
where for all $f \in \cF$ we define $Z_f(\cdot)$ as follows,
\begin{align*}
    Z_f(\cdot) \coloneqq \sum_{a\in \cA} \piref(a|\cdot)\exp\big(\eta f(\cdot,a) \big).
\end{align*}
We further denote $\Delta(s, a) = f(s, a) - f^*( s, a)$ and $H_s(f) = \log Z_f(s) - \eta \sum_{a \in \cA} \pi_{f}(a|s) \cdot \Delta(s, a)$. It worth noticing that $\eta^{-1}\EE_{s\sim \rho}[H_s(f^*) - H_s(f)] = J(\pi^*) - J(\pi_f)$. Now we take derivative of $H$ with respect to $\Delta(s,a)$,
\begin{align*} 
    \frac{\partial H_s(f)}{\partial \Delta(s, a)}  &= \frac{\partial}{\partial \Delta(s,a)}\bigg[\log Z_{f}(s) - \eta \sum_{a \in \cA} \pi_{f}(a|s) \cdot \Delta(s,a)\bigg] \\
    &= \frac{1}{Z_f(s)} \cdot \piref(a|s) \exp\bigl(\eta \cdot f(s,a)\bigr) \cdot \eta - \eta \cdot \pi_f(a|s) \\
    &\quad - \eta^2 \cdot \Delta(s,a) \cdot \frac{\piref(a|s) \cdot \exp\bigl(\eta \cdot f(s,a)\bigr)}{Z_f(s)} + \eta^2 \cdot \Delta(s,a) \cdot \frac{\bigl[\piref(a|s) \cdot \exp\bigl(\eta \cdot f(s,a)\bigr)\bigr]^2}{[Z_f(s)]^2} \\
    &\quad + \eta \sum_{a' \in \cA \backslash \{a\}} \frac{\piref(a'|x) \cdot \exp\bigl(\eta \cdot f(s,a')\bigr)}{Z_f(s)} \cdot \eta \cdot \Delta(s,a') \cdot \frac{\piref(a|s) \cdot \exp\bigl(\eta \cdot f(s,a)\bigr)}{Z_f(s)}
    \\&= -\eta^2 \pi_f(a|s) \Delta(s,a) + \eta^2 [\pi_f(a|s)]^2 \cdot \Delta(s,a) + \eta^2 \sum_{a' \in \cA \backslash \{a\}} \pi_f(a'|x) \pi_f(a|s) \Delta(s,a'). 
\end{align*}
Therefore, by mean value theorem, there exists $\gamma \in [0,1]$ and $f_{\gamma} = \gamma f + (1 - \gamma) f^*$ such that
\begin{align*}
    H_s(f) - H_s(f^*) & = -\eta^2 \gamma \sum_{a \in \cA} \pi_{f_\gamma}(a|s) \Delta(s,a)^2 + \gamma \eta^2\sum_{a_1 \in \cA} \sum_{a_2 \in \cA} \pi_{f_\gamma}(a_1|x)\pi_{f_\gamma}(a_2|x) \Delta(s,a_1)\Delta(s,a_2) \\
    & = - \eta^2 \gamma \EE_{a \sim \pi_{f_\gamma}}\big[\big(f^*(s,a) - f(s,a)\big)^2\big] + \gamma\eta^2 \Big(\EE_{a \sim \pi_{f_\gamma}}\big[\big(f^*(s,a) - f(s,a)\big)\big]\Big)^2 \\
    & \geq - \eta^2  \EE_{a \sim \pi_{f_\gamma}}\big[\big(f^*(s,a) - f(s,a)\big)^2\big],
\end{align*}
where the inequality holds by omitting the second term and $\gamma \leq 1$. Now taking expectation over $\rho$, we have
\begin{align*}
    J(\pi^*) - J(\pi_f) &= \eta^{-1}\EE_{s\sim \rho}[H_s(f^*) - H_s(f)] \\
    & \leq \eta \EE_{(s,a) \sim \rho \time \pi_{f_\gamma}}\big[\big(f^*(s,a) - f(s,a)\big)^2\big],
\end{align*}
which concludes the proof.
% \begin{align*} 
%     &\rE_{s \sim \rho}[J(R(\hat\theta, \cdot, \cdot)) - J(R(\theta_*, \cdot, \cdot))] = \frac{1}{\eta}\rE_{s \sim \rho} \biggl[ -\eta^2 \sum_{a \in \cA}\pi_f(a|s) \cdot \gamma \cdot \bigl(R(\hat\theta, s,a) - R(\theta_*, s,a)\bigr)^2\biggr] \\&\quad + \frac{1}{\eta} \rE_{s \sim \rho} \biggl[\gamma \eta^2 \sum_{a_1 \in \cA} \sum_{a_2 \in \cA} \pi_f(a_1|x)\pi_f(a_2|x)\bigl(R(\hat\theta, s,a_1) - R(\theta_*, s,a_1)\bigr)\bigl(R(\hat\theta, s,a_2) - R(\theta_*, s,a_2)\bigr) \biggr]
%     \\&\ge -\eta \cdot \EE_{\pi_f} \bigl[\bigl(R(\hat\theta, s,a) - R(\theta_*, s,a)\bigr)^2\bigr]
% \end{align*}


\subsection{Proof of Theorem~\ref{thm:lowerbound-bandit}}\label{app:proof-thm-lower-bandit}

\begin{proof}[Proof of Theorem~\ref{thm:lowerbound-bandit}]
% Notice that \Cref{assume:all-coverage-bandit,assume:single-coverage-bandit} imply
% $D^2 \geq D^2_{\pi^*}$, which enables us to adapt \citet[Theorem~3.1]{zhao2024sharp} to our scenario.
We consider the bandit instance $\cK$ and corresponding function class $\cF$ constructed in the proof of \citet[Theorem~3.1]{zhao2024sharp}. Since the constructed $\piref(\cdot|s)$ in $\cK$ is a uniform distribution for any $s\in\cS$, the all-policy concentrability coefficient $D^2$ of $\cK$ defined in \Cref{assume:all-coverage-bandit} satisfies $D^2 = O(1)$, which indicates that the single-policy concentrability coefficient $D_{\pi^*}^2$ defined in \Cref{assume:single-coverage-bandit} satisfies $D_{\pi^*}^2 \leq D^2= O(1)$. Thus, the proof of \citet[Theorem~3.1]{zhao2024sharp} implies that any \textsf{Alg} requires at least $\Omega\big(\min\{{\eta \log \cN_{\cF}(\epsilon)}/{\epsilon}, {\log \cN_{\cF}(\epsilon)}/{\epsilon^2}\}\big)$ to obtain an $\epsilon$-optimal policy.
\end{proof}





\section{Missing Proofs from \Cref{sec:cdb}}

\subsection{Proof of Lemma~\ref{lem:dueling-bandit:conf}} \label{app:proof-duelingbandit-conf}

The following on-poicy concentration bound is the prerequisite.

\begin{lemma}[{\citealt[Lemma~D.4]{zhao2024sharp}}]\label{lem:concentration-dueling}
Consider a offline dataset $\{(s_i,a^1_i, a^2_i, y_i)\}_{i=1}^n$ generated from the product of the context distribution $\rho \in \Delta(\cS)$, policy $\pi \in \Delta(\cA|\cS)$, and the Bradley-Terry Model defined in \Cref{sec:duel-setup}. Suppose $\fls$ is the result of MLE estimation of Algorithm~\ref{algorithm:dueling-bandit}, and we further define $b(s) = \EE_{a\sim \pi(\cdot|s)}\big[\fls(s,a) - f^*(s,a)\big]$, then with probability at least $1 - 2 \delta$, we have
\begin{align*}
    \EE_{s,a \sim \rho\times \pi}\big[\big(\fls(s, a) - f^*(s,a) - b(s)\big)^2\big] \leq O\bigg(\frac{1}{n}\log(\cN_{\cF}(\epsilon_c)/\delta) + \epsilon_c\bigg).
\end{align*}
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lem:dueling-bandit:conf}]

From Lemma \ref{lem:concentration-dueling}, we have that with probability at least $1 - \delta$, it holds that \begin{align}
    \EE_{s'\sim \rho}\Var_{a' \sim \piref(\cdot |s')} \big[\fls(s', a') - f^*(s',a')\big] \leq O\bigg(\frac{1}{n}\log(\cN_{\cF}(\epsilon_c)/\delta) + \epsilon_c\bigg). \label{eqn:dueling-conf:1}
\end{align}

It further holds true that for some $b: \cS \to \RR$\begin{align}
D_\cF^2((s, a), \piref)) \cdot \EE_{s\sim \rho}\Var_{a \sim \piref(\cdot |s)} \big[\fls(s, a) - f^*(s,a)\big] \geq \big(\fls(s,a) - b(s) - f^*(s,a)\big)^2. \label{eqn:dueling-conf:2}
\end{align}
Substituting \eqref{eqn:dueling-conf:1} into \eqref{eqn:dueling-conf:2}, we have
\begin{align*}\label{eq:dueling-bandit-bonus-d2}
    & \inf_{b} \big(\fls(s,a) - b(s) - f^*(s,a)\big)^2 \\
    & \quad = \inf_{b} \frac{\big(\fls(s,a) - b(s) - f^*(s,a)\big)^2}{\EE_{s'\sim \rho}\Var_{a' \sim \piref(\cdot |s')} \big[\fls(s', a') - f^*(s',a')\big]}\EE_{s'\sim \rho}\Var_{a' \sim \piref(\cdot |s')} \big[\fls(s', a') - f^*(s',a')\big] \notag \\
    & \quad \leq D_{\cF}^2((s,a),\piref)\E_{\piref}\big[\big(\fls(s,a) - b(s) - f^*(s,a)\big)^2 \big] \\
    & \quad \leq D_{\cF}^2((s,a),\piref) O\bigg(\frac{1}{n}\log(\cN_{\cF}(\epsilon_c)/\delta) + \epsilon_c\bigg),
\end{align*}
where the first inequality holds due to the definition of $D_{\cF}^2((s,a),\piref)$ and the last inequality holds due to Lemma~\ref{lem:concentration-dueling}.
\end{proof}

\subsection{Proof of Proposition~\ref{prop:dueling-add-bias}}\label{app:proof-add-bias}

\begin{proof}[Proof of Proposition~\ref{prop:dueling-add-bias}]
For any fixed state $s \in \cS$, we have for any $a \in \cA$ that,
\begin{align*}
    \pi_{f}(a|s) & = \frac{\piref(a|s)\exp\big(\eta f(s,a)\big)}{ \sum_{a'\in \cA} \piref(a'|s)\exp\big(\eta f(s,a') \big)} \\
    & = \frac{\piref(a|s)\exp\big(\eta f(s,a)\big) \exp\big(\minus \eta b(s)\big)}{ \sum_{a'\in \cA} \piref(a'|s)\exp\big(\eta f(s,a') \big) \exp\big(\minus \eta b(s)\big)} \\
    & = \frac{\piref(a|s)\exp\big(\eta [f(s,a) - b(s)] \big)}{ \sum_{a'\in \cA} \piref(a'|s)\exp\big(\eta [f(s,a') - b(s)] \big) } \\
    & = \pi_{f-b}(a|s),
\end{align*}
which indicates that $\pi_f = \pi_{f-b}$. This immediately leads to  $J(\pi_f) = J(\pi_{f - b})$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:lowerbound-dueling}}
\label{app:proof-thm-lower-dueling}

\begin{proof}
Notice that \Cref{assume:all-coverage-dueling,assume:single-coverage-dueling} imply
$D^2 \geq D^2_{\pi^*}$, which enables us to adapt \citet[Theorem~4.3]{zhao2024sharp} to our scenario. In particular, we consider the bandit instance $\cK$ and the corresponding function class $\cF$ constructed in \citet[Theorem~4.3]{zhao2024sharp}. By Theorem~4.3 of \citet{zhao2024sharp}, we know that the all-policy concentrability coefficient $D^2$ of $\cK$ satisify $D^2 = O(1)$, which indicates that its single-policy concentrability coefficient $D_{\pi^*} = O(1)$. Theorem~4.3 of \citet{zhao2024sharp} further indicates that 
\textsf{Alg} requires at least $\Omega\big(\min\{{\eta \log \cN_{\cF}(\epsilon)}/{\epsilon}, {\log \cN_{\cF}(\epsilon)}/{\epsilon^2}\}\big)$ to obtain an $\epsilon$-optimal policy, which finishes the proof.
\end{proof}

\bibliographystyle{ims}
\bibliography{ref}
\end{document}
