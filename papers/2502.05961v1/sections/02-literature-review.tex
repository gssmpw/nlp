\section{{Literature Review}}
\label{sec:lit-review}
We identify how bias manifests in ML systems, particularly in T2I generative models, as well as corresponding efforts within the ML discipline to mitigate these biases. We then turn to popular bias mitigation strategies in ML. Shifting to social computing literature, we trace the role of datasets as infrastructures in encoding and perpetuating biases. Finally, we identify how participatory approaches have been applied in ML to mitigate biases in datasets. We point to existing work in CSCW that documents the double-edged nature of participatory approaches: participation can be burdensome.  Our paper builds upon this foundation to demonstrate how achieving the benefits of participation (e.g., alignment and meaningful shifts in power) requires attending to the invisible labour of building infrastructures for participation.

\subsection{Bias in machine learning}
ML has many applications in technology used in the modern world, such as search engines, image captioning, and content generation. ML models, including those responsible for content generation, produce biased outputs~\cite{agarwal2021evaluating_bias,berg2022prompt_bias, luccioni2024stable_bias, caliskan2017semantics_bias, hovy2016social_bias}. The origins of biased outputs can often be traced back to biases reflecting societal structures—such as cultural norms, historical inequalities, and dominant power dynamics—that establish the normative values guiding dataset curation, model development, and evaluation practices~\cite{Blodgett2021_bias, caliskan2017semantics_bias, birhane2021misogyny, hall2024visogender_bias, WinogenderRudinger2018_bias}.
Many biases in ML manifest in the real world, with real-world implications. For example, many non-white Uber drivers report being locked out of their apps and unable to work when the facial recognition systems used by Uber to conduct random ``identity checks'' fail to recognise their faces~\cite{watkins2023face}. Model failures can occur due to insufficient evaluation before release and/or these real-world conditions being difficult to produce in an evaluation setting~\cite{dev2021genderexclusiveharms_bias,birhaneAIAuditingBroken2024,dengUnderstandingPracticesChallenges2023}.  

While there are many ways to define bias in ML (e.g.,~\cite{weidinger2021ethical, weidinger2023sociotechnical, mehrabi2021survey, katzman2023taxonomising, shelby2023sociotechnical}), in this work we are particularly concerned with the representational and quality of service harms as defined in~\cite{weidinger2021ethical, weidinger2023sociotechnical} and~\cite{shankar2017allocational, de2019doescvworkallocational}, respectively. Representational harms are evident in how people, groups, and their heritage are presented and perceived. Quality of service harms typically manifest downstream in a real-world circumstance (for example, someone prompting a T2I model at home) and stem from a physical manifestation of representational harm. For example, people with visible disabilities experience representational harms when T2I models are unable to create accurate and dignified representations of their physical bodies~\cite{mack2024they}. 

Previous studies have focused on bias at the axes of gender and occupation~\cite{berg2022prompt_bias, hall2024visogender_bias,WinogenderRudinger2018_bias}, race and gender~\cite{currie2024genderethnicity, west2024fieldgenderethnicity}, and the medical domain~\cite{gisselbaek2024beyondmedical, wiegand2024demographicmedical}. In our work, we follow calls by~\citet{weidinger2023sociotechnical} to expand the scope of bias investigations to other intersectionalities, such as the representation of cultural objects from diverse regions. The \textsc{WWD} project was therefore initiated in response to this call to investigate and mitigate ML biases in diverse contexts such as culture. Extensive work has been done to understand the scope in which cultural biases exist, and even to what ``culture'' can refer.~\citet{adilazuarda2024culturesurvey} unpack definitions of culture used in the extensive literature related to bias investigation in large language models. In this work, we use the term ``culture'' to mean ``cultural heritage'' as defined in~\cite{adilazuarda2024culturesurvey, blake2000defining}. We intentionally chose food as a lens into culture because food is a salient cultural artefact. Food is shaped by a region's history, geography, and even religious symbolism. Intangible cultural heritage is considered a ``mainspring of cultural diversity'' and the social practices and rituals around food are so deeply intertwined with culture that they are recognised as ``intangible cultural heritage'' under UNESCO's Convention for the Safeguarding of the Intangible Cultural Heritage.\footnote{https://ich.unesco.org/en/convention. This is the primary international legal instrument in the field.} 

In this paper, we are primarily interested in T2I models, a type of GenAI model that generates novel images based on a user prompt. GenAI is a term used to describe ML models that produce audio~\cite{borsos2023audiolm, kreuk2022audiogen}, visual~\cite{hu2024instructimagen, reed2016generativedalle}, and/or written~\cite{tonja2024inkubalm, achiam2023gpt, team2023gemini} content. This type of AI is contrasted with methods of image classification~\cite{deng2009imagenet, lin2014microsoftcoco}, prediction models (e.g., weather forecasting~\cite{lam2023weatherforecasting}), and reinforcement learning~\cite{sutton2018reinforcement} because of its ability to \textit{generate new content} based on learned associations from training data. We focus particularly on T2I models and their implications in contributing to cultural erasure through stereotyping, misaligned values, and inaccurate outputs.

\subsection{Mitigating bias in machine learning through crowdsourcing representative datasets}
Social computing scholars have identified the role of datasets in encoding bias that ML systems then reproduce~\cite{gebru2021datasheets,buolamwiniGenderShades,sweeney2013discrimination,scheuermanDatasetsHavePolitics2021,scheuermanProductsPositionalityHow2024,kapaniaHuntSnarkAnnotator2023,sambasivan2021everyone}. In recent years, fairness in ML researchers have made significant efforts to construct datasets that are more representative using crowdsourcing methods~\cite{singh2024aya_dataset,romero2024cvqa}. Crowdsourcing knowledge tends to take on two forms: a top-down model where raters and annotators are contracted through a centralised body~\cite{miceli2020between}, and a bottom-up model that directly engages community members in the data collection. 

Top-down models of crowdsourcing to build more representative datasets include work such as~\cite{bhutani2024seegull, jha2024visage,rojas2022the}, which pursued diverse and representative samples from the Majority World.\footnote{The term ``Majority World'' is a deviation from the more commonly used literary term ``Global South.'' We have chosen to use ``Majority World'' to highlight that the majority of the populations with which we engage come from these regions.} The ORBIT computer vision dataset~\cite{massiceti2021orbit} demonstrates how top-down models can engage in the ethical sourcing of data contributors, as it involves annotators from Enlabeler (Pty) Ltd,\footnote{https://www.enlabeler.com/} a company that empowers its employees by offering technical skill development alongside their data annotation work. While top-down data collection can be efficient and more easily implemented at scale, it excludes the many perspectives of people who do not work as data annotators~\cite{geiger2020garbage}. Moreover, top-down data collection efforts typically dictate to data contributors what an acceptable submission looks like, without room for bottom-up feedback~\cite{posada2022dispotif}. Contributors in top-down models can be flagged for fraudulent behaviour if the company overseeing data collection suspects that multiple people share an account, a common practice in households where there may only be one computer available~\cite{posada2022coloniality,jones2021refugees}. In contrast, bottom-up and community-based data collection broadens the range of people who can contribute and can empower more people not only to share cultural artefacts, knowledge, and expertise but also to help shape the parameters of their own participation~\cite{denton2021whose,delgado2023participatory,birhanePowerPeopleOpportunities2022}. We characterise bottom-up, community-based crowdsourcing as efforts that allow for any community member to engage (e.g., they do not need to be formally employed as a  data crowd worker), make space for community contributors to actively shape parts of the research process (e.g., they may take part in decisions about what kinds of data should be collected), and tend to---in large part---be volunteer-based.  

Community-based crowdsourcing has great potential to reach people in the community and to gather local knowledge. For example, the AYA dataset from Cohere's research lab~\cite{singh2024aya_dataset} presents a framework for large-scale community data collection. Their collaborators spanned 119 countries and were able to create a multilingual text dataset comprising 513 million instances across 114 languages. All their systems have been open-sourced. In 2020,~\citet{orife2020masakhaneoriginal} introduced Masakhane, which is an ongoing open-source, continent-wide, online research effort for machine translation for African languages. Masakhane has also consistently demonstrated ethical participatory research design for machine translation and natural language processing in Africa~\cite{nekoto2020masakhane, adelani2022masakhaner,ogundepo2023afriqamasakhane}. While efforts such as AYA and Masakhane make significant strides in producing datasets with diverse community-based input, both research efforts foreground the dataset as the primary artefact of interest. In our contribution, we aim to build upon the precedent set by bottom-up crowdsourcing efforts by calling attention to the processes and complexities---such as designing accessible infrastructure to support data collection and working with communities to build trust and rapport---throughout the data collection process by which these types of datasets are created. 

\subsection{Infrastructures for dataset construction}
Infrastructure is the foundation underpinning any large-scale system; it is the underlying substrate that is embedded in action, tools, and built environments and takes on significance in relation to organised practices~\cite{star1999ethnography,star201620,rice2006handbook}. Infrastructure can take the form of datasets~\cite{bowker2000sorting} that classify information and are subsequently used to create and reinforce categorisations. For example, FairFace, a large-scale labelled image dataset, is an infrastructure that has been used to determine what counts as a face in an image---but, crucially, relies on perceived gender and race labels~\cite{karkkainen2021fairface}. Datasets, like all infrastructures, are socio-technical; information does not naturally fall into classifications~\cite{bowker1994information,bowker2000sorting,star1999ethnography}. Instead, data must be produced by humans who make subjective decisions about what deserves to be counted and how it should be classified~\cite{d2024counting,denton2021whose}. This necessarily means that datasets are value-laden: they are imbued with the values of their designers and the larger social contexts in which they were created~\cite{rice2006handbook,star1999ethnography,scheuermanDatasetsHavePolitics2021,d2023data}. 

However, the value-laden nature of datasets often remains unseen until breakdowns---disruptions between expected and actual outcomes---occur~\cite{luccioni2023bugs, birhane2024into}. Critical computing scholars have demonstrated the value-laden nature of datasets for ML by leveraging \textit{infrastructural inversion} as an analytical tool to trace breakdowns back to incongruences between designers and users~\cite{scheuermanDatasetsHavePolitics2021,scheuermanHumanDataDataset2023,buolamwiniGenderShades}. For example, the Gender Shades study leverages infrastructural inversion to surface the social values and power dynamics underlying image datasets by tracing the poor performance of facial recognition systems on Black women's faces back to image datasets that lacked adequate and accurate representation of marginalised groups~\cite{buolamwiniGenderShades}. The lack of adequate and accurate representation of marginalised groups in these datasets reflects a lack of value for these identities---they were overlooked in dataset creation. 

Accounting for diverse values in dataset creation requires labour~\cite{scheuermanDatasetsHavePolitics2021,sambasivan2021everyone}. As~\citet{scheuermanDatasetsHavePolitics2021} points out, the data work that goes into developing a dataset is often ``silenced'', which results in a lack of incentive structures to center, support, and document the labour that goes into dataset creation. In our paper, we leverage infrastructure studies to call attention to the invisible labour that underpins the creation of large-scale systems and the subsequent implications for the \textit{kinds} of values that then get embedded into the resulting system. 

\subsection{The pitfalls and potentials of participation}
Recognising the limitations of top-down designed datasets, ML researchers have begun a \textit{participatory turn in AI} in an effort to construct datasets that represent a wider range of values and identities~\cite{delgado2023participatory,birhanePowerPeopleOpportunities2022,ParticipationScaleTensions,corbett2023power,suresh2024participation}. Participation is thought to enable better alignment of system performance with end-user expectations, as impacted communities would be involved in shaping the values of the ML systems~\cite{delgado2023participatory}. In other words, communities would be empowered as co-designers of the ML systems that impact them. However, the majority of participatory AI efforts fall short of their promises to support meaningful participation among stakeholders in the design process~\cite{ParticipationScaleTensions}. Researchers argue that inadequate attention is paid to shifting power away from systems designers, who are often the primary beneficiaries of improved model performance, and towards participants who could benefit by ensuring models accurately represent them and encode their values~\cite{birhanePowerPeopleOpportunities2022}. While participatory AI is a relatively nascent subfield within the ML community, participatory design is a research tradition that has existed for decades in social computing~\cite{asaro2000transforming,bodker2022can}. In our contribution, we draw upon lessons from participatory design in CSCW to demonstrate how these methods can be applied to achieve meaningful power shifts between researchers and communities. 

CSCW has a long and rich history of leveraging community-based participatory research (CBPR) methods to center socially marginalised groups' epistemologies in the design of technology~\cite{le2015strangers,sum2023translation,bratteteig2016unpacking,harrington2019deconstructing,bannon2018reimagining,pierre2021getting}. CBPR is used as a method to mitigate the power imbalances between researchers and communities by empowering community members to shape the trajectory and design of research projects~\cite{reasonSAGEHandbookAction2008}. For example,~\citet{asaro2000transforming} documents how, in light of impending technological changes in the workplace, workers themselves were invited to participate in the design of those technologies and thus had the power to shape the impacts of these technologies on their well-being.~\citet{harrington2019deconstructing} considers the role of research setting on community access, and therefore the accessibility of participation for targeted groups. In CBPR, practitioners strive to overcome their status as outsiders by building trust through affective and moral connections with community members~\cite{le2015strangers,mcmillan1986sense}. Taken together, CBPR literature overwhelmingly stresses the need for researchers to become imbricated with the communities which they are studying---through building these relationships, they become invested in the outcomes of the research from an insider, community member perspective. 

While participatory methods have the potential to give communities a greater say over the design of technology, many of these efforts fall short in meaningfully shifting power from the researchers to the researched~\cite{birhane2021misogyny,ojewaleAIAccountabilityInfrastructure2024,ParticipationScaleTensions}.  Participation takes work on behalf of communities.~\citet{pierre2021getting} documents how communities assume the ``epistemic burden'' of producing data about their lived experiences in participatory research efforts~\cite{pierre2021getting}. Finally, some scholars have critiqued the tendency of efforts to include socially marginalised groups as inherently ``othering''~\cite{epstein2008rise}. In our contribution, we draw upon the rich tradition of CBPR in CSCW literature to apply methods to build datasets more equitably. In particular, we attend to how research infrastructures can support the work of constructing the field site, building relationships with community members, and making room for meaningful community participation in the research design process~\cite{le2015strangers}. 
