\section{Discussion}
\label{sec:discussion}
\textsc{WWD} is a socio-technical infrastructure that supports the collection of cultural data, in the form of food, in a bottom-up community-led manner. Community members' needs and experiences actively shaped the architecture of WWD. Our data collection platform was constructed to be compatible with the established digital infrastructure and cultural norms of the communities we worked with. The types of data we collected (e.g., the attributes for each dish) were informed by community members who identified what was important to capture about a dish to accurately represent how the dish is prepared and consumed in their culture. 

Building the \textsc{WWD System} in a bottom-up, community-led manner required an immense amount of labour. Data did not simply flood in once the system architecture was built. Core Organisers and Community Ambassadors engaged in \textit{data work}---a socio-technical process through which data about local cuisines was produced. As many social computing scholars have noted, data work is often overlooked despite its essential role in shaping the epistemology of a dataset and consequently the downstream performance of ML systems~\cite{sambasivan2021everyone,ismailEngagingSolidarityData2018,mollerWhoDoesWork2020,scheuermanProductsPositionalityHow2024}. 

In our discussion, we surface the tensions that occurred during data work. These breakdowns in the data work process help us to reveal deeper structural issues in the AI/ML production pipeline that confound bottom-up, community-led approaches to dataset construction. Communities facing representational harms~\cite{weidinger2021ethical} and disparities in quality of service~\cite{shankar2017allocational, de2019doescvworkallocational} face a catch-22 when participating in efforts to improve dataset coverage: They can shoulder the burden of participation or be excluded from model ontology. New technologies, particularly GenAI tools, have been proposed as a way for communities to preserve their culture representation by participating in efforts to contribute data to model training~\cite{heritage7030070}. However, participation does not necessarily entail improved outcomes for communities~\cite{birhanePowerPeopleOpportunities2022}. We point to a difference in ethical frameworks between communities on the African continent with whom we worked and those of large tech companies that build and control GenAI technologies to illuminate why the promises of participation often fall short. 


\subsection{Tensions in data collection}
 
Our results show that there were tensions around data collection. Specifically, issues surrounding image provenance, the accuracy of information about a dish, and the benefits of participation arose throughout the data collection process. These issues reveal deeper structural problems with the AI/ML pipeline. 
\subsubsection{Establishing a clean bill of data provenance}
Recent efforts in participatory ML research attempt to safeguard the labour and intellectual property of community data contributors by creating dataset licenses restricting the use of the community's dataset, which assign ownership and terms of access and use to these datasets~\cite{birhanePowerPeopleOpportunities2022,longpre2024largelicence}. However, for the license to be effective, the data must have a clean bill of provenance. \textsc{World Wide Dishes} was built to be an open-source dataset with a Creative Commons license that could be used for model evaluation. As a result, the WWD dataset had to fulfil strict requirements for data quality, including ensuring that the dataset creators had a right to the images contained within the dataset.  In other words, the creators of \textsc{WWD} must then be able to claim rightful use and ownership over all the images collected as part of the project. However, many of the images that Contributors submitted during the data collection phase were taken from the Internet and lacked proper licensing. As a result, Community Ambassadors had to engage in extensive consultation and discussion with Contributors to ensure they understood the importance of data provenance in their submissions. Contributions, where the origins of the submitted were unclear, had to be deleted, erasing bits of cultural knowledge from our dataset. Ensuring a clean bill of data provenance was time-intensive and not easily scalable. It was difficult to enforce image upload guidelines in a volunteer effort, resulting in a smaller, less representative, dataset than we would have liked. However, the rigorous process of ensuring a clean bill of data provenance for each submission enabled us to, in good faith, release our dataset as an open-source project. 

The standard of open-sourcing datasets and applying Creative Commons license, while understandable, places massive burdens on small, community-led projects such as \textsc{WWD} to ensure clean bills of data provenance. To be clear, we are not arguing against open-source datasets or Creative Commons licenses, but rather are demonstrating the need to build infrastructures that support and fund the labour needed to verify that data for their projects can be used. As mentioned in~\cref{background}, understanding cultural nuance on a fine-grained, regional scale requires extensive (and non-extractive) consultation with community members who have the capacity to share local expertise. As such, non-exploitative and non-extractive community consultation is an important step in verifying the validity and veracity of cultural information. 

\subsubsection{Verifying cultural information}

Collecting accurate and representative cultural data is exceptionally difficult. Cultures are not bounded by government borders and/or other manufactured systems, but rather extend across larger regions and are often the product of intercultural exchanges~\cite{gupta2008beyondculture}. This makes determining the veracity of a data point in \textsc{WWD} almost impossible without extensive consultation with a community member with local expertise. In \textsc{WWD}, we sought to include as many Contributors as possible to collect a granular representation of cultural data. We accessed Contributors through our community ambassadors who had established relationships and trust with the folks they asked to contribute. Inclusion, however, can be a slippery slope~\cite{epstein2008rise,benjamin2016informed}. 

ML researchers continue to pursue the construction of ever more representative datasets in the name of improving model performance for \textit{everyone}~\cite{luccioni2021everyone, radford2018improvingeveryone}. Often, ML researchers have trouble accessing ``hard-to-reach'' populations, such as the communities we worked with to build \textsc{WWD}. Many recent projects have attempted to solicit engagement from ``hard-to-reach'' populations~\cite{kirk2024prism,ramaswamy2023geodegeographicallydiverseevaluation,singh2024aya_dataset}, yet none of these projects interrogate why these populations might be hard for researchers to access. Drawing on Benjamin's work~\cite{benjamin2016informed}, \textbf{we urge ML researchers to consider how research institutions and industry laboratories may engender distrust within communities that have endured centuries of extractive practices by actors from the Global North.} It is essential that researchers not only endeavour to make participation accessible to members of ``hard-to-reach'' communities but also work towards establishing themselves as trustworthy partners in the research process, in the same way that~\citet{singh2024aya_dataset} do this. 

\subsubsection{Explaining the benefits of participation}

Community Ambassadors wrestled with explaining the benefits of participation in \textsc{WWD} to potential Contributors. Participation was not financially compensated. The research team chose not to make use of professional data centre workers\footnote{Professional data centre workers are those people employed in a centralised manner to perform data collection tasks. Their livelihood is, therefore, connected to the requirement to engage in data contribution, which does not align with \textsc{WWD} goals. Additionally, even had we wished to use data centre workers, we lacked the resources to which a large technology company might have access, such as the ability to engage a business outsourcing company (e.g., Enlabler) to recruit and pay data workers.} because the nature of the data collection process argued for prioritising organic engagement through social networks to collect perspectives from people who do not, and have not, typically contributed to Internet datasets from around the world. We purposely chose a data collection method that would enable the use of social networks and allow us to reach participants other than those employed in a data worker centre, such as older generations and those across a wide socioeconomic range. We also wanted to empower participants to involve their families in the process. 

Although the research team would have preferred to individually compensate each Contributor, because \textsc{WWD} relies on a decentralised, global-scale data collection method, and, crucially, as of the time of data collection, normative standards and infrastructure do not exist to support such a decentralised payment process to effectively \textbf{pay data contributors}, we were \textit{unable} to pay them. The research team explored many possible avenues for paying participants but each time came up against prohibitively expensive and logistically insurmountable barriers. For example, money transfer services such as PayPal~\cite{paypal_countries_2023} and Wise~\cite{wise_usage_2023} were unavailable in many of the regions where \textsc{WWD} operates. These types of services also require that payment recipients have access to digital banking services, which many within our target communities do not. In addition, some of our Core Organisers, who are from the African continent and utilise digital banking services, provided anecdotal evidence of times when their transactions were flagged for seemingly no other reason than their nationality. Infrastructures to support financial remuneration for research participants in the Majority World are simply not commensurate with the many calls from Western researchers to engage participants in these parts of the world. \textbf{Researchers must therefore build the infrastructures to enable equitable participation with communities}; in particular, researchers should investigate how to address breakdowns in participant compensation infrastructures. Other similarly decentralised efforts have remunerated contributors with material items (e.g., sweaters and small gifts)~\cite{singh2024aya_dataset}. Still other researchers point to the limitations of financial compensation for participants and urge researchers to consider what kinds of remuneration would be useful given the context of their research site~\cite{hodge2020relational}.

Despite the lack of extrinsic, financial incentives, the Contributors did exhibit some intrinsic motivation. Contributors shared many different reasons for having participated, such as wanting to make a difference in GenAI outputs, supporting a friend, or contributing to a mission and team they believed in. The majority of data contributions came from the African continent. The authors have speculated why this might be, and have wondered if there is a common focus uniting these Contributors: a central philosophy of ``familyhood'' and unity. This is known by different terms across the continent, including djema’a (Arabic), ubuntu (Zulu), ujamaa (KiSwahili), umuntu (Chichewa), and unhu (Shona). Community Ambassadors also suspected that their positionality as members of the communities from which they were soliciting data contributions further strengthened sentiments of unity among participants who saw \textsc{WWD} as an extension of the growing ``By Africans, for Africans'' movement in ML~\cite{birhane_2024_for_africans}. 

Whilst we can only speculate about why participants engaged in the data contribution process, the authors recognise the responsibility they were given to respect and honour these Contributors and to avoid extractive and exploitative practices. 

\subsection{Participate or be excluded: A catch-22}


Cultural erasure and lack of representation are rooted in deep systemic issues that date back centuries. GenAI, especially T2I models, play an increasingly prominent role in shaping the media ecosystem. However, relying on these models to ``fix'' centuries of intentional cultural erasure overlooks the deeper systemic issues that will likely constrain the efficacy of these technocentric solutions. During the data collecting for \textsc{WWD}, Community Ambassadors often found themselves rationalising the uncompensated nature of data contributions by demonstrating that existing T2I models perform poorly when creating images of local dishes so Contributors should provide accurate data to teach the model what the dish should look like. Regardless, many Contributors, Community Ambassadors recalled, were eager to participate in an African-researcher-led ML effort. 

Through the reflection process, Community Ambassadors shared conflicting feelings about tapping into the shared philosophy of familyhood and unity that they suspected motivated Contributors' participation. On the one hand, local communities were engaging in the dataset creation process through the lens of \textit{Ubuntu} (broadly translated as ``I am because we are'')---an ethical framework that emphasises dignity, reciprocity, and the common good~\cite{ewuoso2019core}. In contrast, the models that would subsequently be trained by these datasets are developed in Western contexts and imbued with utilitarian ethics---a framework that emphasises the best for the greatest number of people~\cite{selbstFairnessAbstractionSociotechnical2019,west2004introduction}. These two distinct, yet interrelated elements of the ML pipeline---dataset production and model development---are therefore produced not only in distinct geographic regions~\cite{sambasivan2021everyone,scheuermanDatasetsHavePolitics2021} but also, in our case, in two distinct ethical frameworks. Contributors who engaged with us out of a sense of \textit{Ubuntu} are unlikely to see their values recognised and preserved in the actual functioning of the downstream T2I model that is optimising for fundamentally different well-being criteria.  

Participation in dataset construction is not a guaranteed way to achieve representational justice in T2I models. Thus, proposing to communities that to avoid being excluded from the future of media representation, they should participate in dataset development, is misleading. This false choice obfuscates (1) the deeper systematic issues that dictate whose culture gets preserved and represented and (2) the disjunction between the value system under which participants may contribute data and that of the models that are then trained on this data. 

Future research efforts should examine how to bring the ethical frameworks of dataset creation and model development into alignment by prioritising local, community ownership over AI. 

