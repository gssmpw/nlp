\section{Introduction}\label{introduction}

Generative AI (GenAI) models are increasingly ubiquitous in tools and products that have the potential to shape societal representation---how society values, perceives, and shares knowledge about certain objects~\cite{gillespie2024generative,luccioni2023stablebiasanalyzingsocietal}---through content generation. Fair, accurate, and diverse representation is important to avoid all forms of bias. Societal representation is important to counteract stereotyping, misrepresentation, and the resultant quality of service harms to which these can contribute. Text-to-Image (T2I) models, a common form of GenAI, play an increasingly important role in shaping the media ecosystem~\cite{Mim2024images,qadri2024}. These models are being integrated into creative design~\cite{nouraei2024creativedesign, kalving2024aidesign}, user experience design~\cite{li2024UXD}, and content production software (such as PowerPoint Designer and Canva) to facilitate the process of generating images based on text prompts~\cite{MicrosoftImagePromptsDallE, CanvaLeonardoAI}. However, T2I models have documented shortcomings when it comes to creating images of people and objects from socially marginalised groups~\cite{Mim2024images,qadri2024}. These failures can be traced back in part to the web-scraped training datasets used in model development~\cite{birhane2021large,schuhmann2022laion}. 

 Scraping the Internet for training data is an inherently limited approach for developing fair and robust GenAI systems, as the content available on the Internet is heavily skewed towards the Global North and reflects existing social, political, and economic power structures~\cite{luccioni2024stable_bias,hongevaldatarace23}.  Additionally, the tools used to scaffold the scraping process and filter results into a usable format, such as CLIP \cite{radford2021learning}, tend to systematically remove content from under-represented groups, leading to the further narrowing of dataset ontology~\cite{hong2024s}. Further, these models tend to \textit{fail} in filtering out harmful content such as stereotypes, racist and ethnic slurs, and other extremely problematic content~\cite{birhane2021misogyny}. The relative (and manufactured) scarcity of data representing the experiences of socially marginalised groups results in poor GenAI model performance: these groups are often misrepresented or not represented at all in generated images~\cite{mack2024they,qadri2024,dasProvenanceAberrationsImage2024}. These values are, in turn, baked into the encodings of the models during training, and are then reflected in the machine learning (ML) models' output~\cite{birhane2022values}. 
 
 ML researchers attribute poor model performance in this context to the ``long-tailed'' nature of data~\cite{massiceti2021orbit}. The term ``long-tailed'' refers to the relative scarcity of data that attempts to capture the experience of socially marginalised groups, as compared to the overall distribution of training data. To counteract these limitations, ML researchers are actively investing in efforts to engage socially marginalised groups in contributing to datasets used to train large models~\cite{singh2024aya_dataset,kirk2024prism,ramaswamy2023geodegeographicallydiverseevaluation}. 
 
While significant strides are being made towards building more representative datasets alongside socially marginalised groups~\cite{singh2024aya_dataset,massiceti2021orbit}, the human and socio-technical processes of dataset curation are rarely documented in ML literature. In our paper, we use frameworks developed in computer-supported cooperative work (CSCW) literature to reflexively analyse our process of co-curating \textsc{World Wide Dishes (WWD)}, a large-scale image-text dataset of dishes from local cuisines compiled through participatory crowdsourcing.

We take inspiration from CSCW scholarship that investigates the labour underpinning large-scale systems and shift the focus from the dataset as the final product to the \textit{data work} involved in its creation~\cite{sambasivan2021everyone,scheuermanDatasetsHavePolitics2021}. We use infrastructuring as an analytical lens to draw attention to the labour involved in the participatory collection and categorisation of cultural data. We show how participatory ML places labour and infrastructure demands on (1) local communities, putting them in a sometimes unfair bind: perform the burdensome data work or have representation excluded from model ontology and on (2) ML researchers/developers: make the significant infrastructure investments needed for participation and local engagement to be translated into model ontologies.

We document the components of the data work involved in building the \textsc{WWD} dataset. We first present the technical system we built to facilitate the data collection in~\Cref{sec:system}. In~\Cref{sec:rollout}, we present our reflections on the process of deploying our data collection system with community members to answer our research question: \textit{How can researchers collect cultural data using a bottom-up, community-led approach?} Our reflections are based on field notes and post-mortems created by the \textsc{WWD} research team throughout and following the development of the \textsc{WWD} dataset. In our discussion in~\Cref{sec:discussion}, we reflect on how tensions that emerged during the data work were manifestations of deeper structural issues within the AI/ML development pipeline. We then critically assess the choice with which marginalised communities are often presented: participate in data collection efforts to improve how GenAI systems depict them, or have their representation excluded from model ontology.~\Cref{sec:limitations} presents suggestions for how future research efforts can build the infrastructure to support data work.

\textbf{Through our case study, we demonstrate that participation and local engagement require infrastructure}. This infrastructure does not just appear. It must be invested in and created. We build upon existing efforts in fair ML by leveraging a participatory approach to data collection for GenAI systems that engage community members as active architects of the dataset. We extend existing critiques of participatory methods that identify the burdens of participation~\cite{pierre2021getting,birhanePowerPeopleOpportunities2022} by demonstrating how these burdens play out empirically. Our work contributes to the CSCW literature by (1) providing empirical evidence of how the burdens of participation are shifted onto communities when building datasets that accurately encode localised knowledge and (2) surfacing the infrastructures that are required to translate participants' knowledge into datasets and subsequently into models. 

