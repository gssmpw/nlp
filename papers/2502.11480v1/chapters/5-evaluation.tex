\section{Experiments}
\label{sec:exp}
%In this section, we evaluate BOMS with the baseline methods on multiple offline RL benchmark tasks. 
%Specifically, we would like to answer the following three questions: (Q1) Does BOMS outperform the existing model selection schemes? (Q2) Does BOMS effectively capture the relationship between models via the proposed kernel? (Q3) Is BOMS generic enough in the sense that it can be integrated with various off-the-shelf offline model-based RL methods to achieve favorable results?
%(Q1) How does BOMS's performance measure up against other baselines? (2) Does BOMS effectively capture the relationship between models via the kernel function? (3) Can BOMS be combined with other offline model-based methods to achieve similar results?

\vspace{-1mm}    
\subsection{Experimental Setup}
\vspace{-1mm}
\textbf{Evaluation domains.} We evaluate BOMS on various benchmark RL tasks as follows:
\begin{itemize}[leftmargin=*]
    \item \textit{Locomotion tasks in MuJoCo}: We consider walker2d, hopper, and halfcheetah with 3 different types of datasets from D4RL \citep{fu2020d4rl} (namely, medium, medium-replay, medium-expert) generated from different behavior policies. These tasks are popular choices for offline RL evaluation due to the complex legged locomotion and challenges related to stability and control. Each dataset contains $10^6$ sampled transitions by default.
    \item \textit{Robot arm manipulation in Adroit and Meta-World}: We take the Adroit-pen task from D4RL, which involves the control of a 24-DoF simulated Shadow Hand robot tasked with placing the pen in a certain direction, with three different datasets (cloned, expert, mixed) generated from different behavior policies. The first two datasets are directly from D4RL, and the pen-mixed is a custom hybrid dataset with 50-50 split between the cloned and expert datasets. Each dataset contains $5\times 10^5$ transitions from the environment. Moreover, we take the door-open task in Meta-World \citep{yu2019meta}, which is goal-oriented and involves opening a door with a revolving joint under randomized door positions. Since the default Meta-World does not provide an offline dataset, we follow a data collection process similar to D4RL to obtain the offline medium-expert dataset.
    %\item \pch{\textit{Meta-World}: We take the door-open task Since the default Meta-World does not provide an offline dataset, we follow a data collection process similar to D4RL to obtain the offline medium-expert dataset. That is, the dataset is generated by mixing one million samples collected from the medium policy and one million samples collected from the expert policy.} 
    %The policies are trained online using SAC, with the expert policy trained until converged and the medium policy partially trained.
\end{itemize}
%choose 3 different tasks simulated from the gym-MuJoCo, including walker2d, hopper, and halfcheetah with 3 different kinds of datasets (medium, medium-replay, medium-expert) generated from different behavior policies of each task from D4RL. These tasks are common choices for the performance evaluation of the RL methods due to the complex legged locomotion and challenges related to stability and control. Each dataset contains 1 million samples of environmental information. We also implement BOMS in the pen task with three different datasets (cloned, expert, mixed) of the Adroit environment, which aims to control the robotic hand to place the pen in a certain direction. The first three datasets are from D4RL, and the pen-mixed is a custom hybrid dataset with 50-50 split between cloned and expert datasets.
\vspace{-3mm}

\textbf{Candidate dynamics models.} Regarding training the candidate dynamics models, we follow the implementation steps outlined in MOPO \citep{yu2020mopo}. 
%(i) Dynamics models: 
Specifically, we employ model ensembles by aggregating multiple independently trained neural network models to generate the transition function in the form of Gaussian distributions. 
%(ii) Policies: We leverage SAC \citep{haarnoja2018soft} on the uncertainty-penalized MDP constructed by MOPO. 
{To acquire the candidate model set $\cM$, we construct a collection of 150 dynamics models for each task. Specifically, we obtain 50 models under each of the 3 distinct random seeds for each MuJoCo and Meta-World task, and we obtain 10 models under each of the 15 distinct seeds for the Adroit task.
For the calculation of model distance required by the GP covariance, BOMS randomly samples 1000 states from the offline dataset, obtains actions from the learned policy, and compares the predictions generated from different models.}

\textbf{Model selection process and inference regret.} In the model selection phase, we set the total number of BO iterations $T=20$. At each BO iteration, the evaluation of a policy is based on the observed Monte-Carlo returns averaged over 5 trajectories for MuJoCo and 20 trajectories for Adroit (note that this difference arises since the maximum trajectory lengths in MuJoCo and Adroit are 1000 and 100, respectively). For each selected dynamics model, we train the corresponding policy by employing SAC \citep{haarnoja2018soft} on the uncertainty-penalized MDP constructed by MOPO. For a more reliable performance evaluation, we conduct the entire model selection process for 10 trials and report the mean and standard deviation of the \textit{inference regret} defined as $\cR(t):=J^* - J^{\pi_{\text{out},t}}_{M^*}$, where ${J^*}$ denotes the true optimal total expected return among all model candidates in $\cM$ and $J^{\pi_{\text{out},t}}_{M^*}$ denotes the true total expected return of the policy output by the model selection algorithm at the end of $t$-th iteration. 

\vspace{1mm}
\noindent\textbf{Remark on the non-monotonicity of inference regret:} The inference regret defined above is a standard performance metric in the BO literature \citep{wang2017max,hvarfner2022joint,li2020multi}. Note that the inference regret $\mathcal{R}(t)$ is \textit{not} necessarily monotonically decreasing with $t$ since the policy $\pi_{\text{out},t}$ output by the model selection algorithm is not necessarily the best among those observed so far (due to the randomness in Monte-Carlo estimates).
%we want to identify the optimal dynamics model in 20 selection epochs. For the performance evaluation, we conduct the entire model selection process for 10 trials and get the mean and standard deviation of the simple regret ${= R^* - R_t}$ to measure the behaviors of different methods. ${R^*}$ represents the optimal return across all model candidates, while ${R_t}$ corresponds to the average observed returns from 5 environment trajectories in the case of MuJoCo environments and 20 trajectories for Adroit environments. This discrepancy arises because the maximum rollout steps allowed in MuJoCo is 1000, while it's 100 for the pen environment.


\vspace{-1mm}

\subsection{Results and Discussions}
%In this subsection, we formally answer the three questions.
\vspace{-1mm}

\noindent\textbf{{Does BOMS outperform the existing model selection schemes?}}
To answer this, we compare BOMS with three baseline methods, including \textit{Validation}, \textit{OPE}, and \textit{Random Selection}. Validation is the default model selection scheme in MOPO, which chooses the model with the lowest validation loss in the model training process. Another baseline is OPE, which selects the dynamics model with the highest OPE value. As suggested by \citep{tang2021model}, we employ Fitted Q-Evaluation \citep{le2019batch} as our OPE method, which is a value-based algorithm that directly learns the Q-function of the policy. Random Selection selects uniformly at random the models to apply in the environment and outputs the model with the highest empirical average return at the end. 
\input{chapters/table-main}

%To answer this, we evaluate BOMS performance and compare our experiment results with 3 baselines, including \textbf{MOPO}, \textbf{OPE}, and \textbf{Random Selection}. MOPO only chooses the model with the lowest validation loss in the model training process and does not apply any additional model selection conditions. Another baseline is OPE, which selects the dynamic model with the highest OPE value. We employ Fitted Q-Evaluation as our OPE method, which is a value-based algorithm that directly learns the Q-function of the policy. Last, Random Selection indicates uniformly selecting the model candidates to apply in the real environment and choosing the model with the highest average return in the end. 
\begin{comment}
Figure~\ref{fig:baselines} shows the regret performance of BOMS and the baselines. We can observe that BOMS indeed outperforms the baseline methods on almost all the MuJoCo and Adroit tasks.  
While OPE and MOPO can achieve low regrets on a few tasks, they are clearly not reliable enough for effective model selection. 
On the other hand, Random Selection generally improves the regret performance as the selection epoch increases, but its progress appears much slower than BOMS.
For further evaluation, we proceed to understand how much performance can be improved solely through a small amount of online interaction. 
\end{comment}
Tables~\ref{tab:mopo_regrets}-\ref{tab:door_open_regrets} present the normalized regret scores of vanilla MOPO, OPE, and BOMS at the $5{\text{th}}$, $10{\text{th}}$, $20{\text{th}}$ iterations. The normalized regrets fall within the range of 0 to 100, where 0 indicates the regret of the optimal dynamics model in the task, and 100 indicates the return is 0. The normalized regrets indicate that under BOMS, it only takes about 5 selection iterations, which corresponds to only $1\%$-$2.5\%$ of training data, to significantly improve over vanilla MOPO and OPE in almost all tasks. \pch{Figure~\ref{fig:baselines} shows the regret curves. BOMS significantly outperforms Random Selection, and this further corroborates the benefit of using GP and the proposed model-induced kernel in capturing the similarity among the candidate dynamics models. Due to the page limit, more regret curves are in Figure~\ref{fig:main_appendix} in Appendix~\ref{app:exp}.}
%\pch{This demonstrates the superior sample efficiency of BOMS. Due to the page limit, the detailed regret curves are provided in Figure~\ref{fig:baselines} in Appendix \ref{app:exp}.} 
%Notably, BOMS can achieve significant decrease in regret with only 10 BO iterations, which corresponds to only no more than 2\% and 5\% of training data in Adroit and MuJoCo, respectively.
%By the end of 20 epochs, our method gets lower regret performance in 11 of 13 tasks and gets significant improvements compared to MOPO and OPE. 
%Notably, BOMS can achieve significant decrease in regret with only 10 BO iterations, which corresponds to only no more than 2\% and 5\% of training data in Adroit and MuJoCo, respectively.
%From the evaluation table shown above, it's surprising that only a minimal amount of selections and limited environmental interactions are needed to enhance existing offline RL algorithms significantly.

%For further evaluation, we want to understand how much performance can be improved solely through a small amount of online interaction in different selection epochs. Table~\ref{tab:mopo_regrets} shows the normalized regret scores of the $5^{th}$, $10^{th}$, $15^{th}$, $20^{th}$ epoch, MOPO and OPE. The regret values after normalization fall within the range of 0 to 100, where 0 indicates the regret of the optimal dynamic model in the task, and 100 indicates the return is 0. The normalized regrets table shows that it only takes 10 selection epochs for BOMS to exceed MOPO and the OPE in most tasks. By the end of 20 epochs, our method gets lower regret performance in 11 of 13 tasks and gets significant improvements compared to MOPO and OPE. From the evaluation table shown above, it's surprising that only a minimal amount of selections and limited environmental interactions are needed to enhance existing offline RL algorithms significantly.
\input{chapters/fig-regret-main}


\vspace{1mm}
\noindent\textbf{{Does BOMS effectively capture the relationship between models via the model-induced kernel?}}
%\subsection{Comparison With Alternative Model Distances}
To validate the proposed model-induced kernel, we further construct four other heuristic approaches for a comparison, namely \textit{Weight Bias}, \textit{Model-based Policy}, \textit{Model-free Policy}, and \textit{Exploratory Policy}. Weight Bias calculates the discrepancy between the weights and biases of the model neural networks as the model distance. The other three approaches generate rollouts with different policies (instead of the policy $\pi_t$ used in BOMS). Specifically, the Model-based Policy refers to the SAC policy trained with a pessimistic MDP, the Model-free Policy refers to the SAC policy learned from the pre-collected offline dataset, and the Exploratory Policy simply samples actions randomly. 
Figure~\ref{fig:ablations_selected} shows the evaluation results of BOMS under the proposed model distance and the above heuristics. We observe that BOMS indeed enjoys better regrets under the proposed model distance than other alternatives across almost all tasks. More regret curves are provided in Figure~\ref{fig:ablations} in Appendix \ref{app:exp}.
%We can see that BOMS achieves greater stability in the magnitude of regret reduction across most tasks. 



\begin{comment}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.475\textwidth]{figures/traj_diff_new.png}
    \caption{The illustration of the trajectory disparities of dynamics models.}
    \label{fig:traj_diff}
\end{figure}
\end{comment}
%BOMS randomly samples 1000 states from the fixed dataset and obtains actions from the learned policy, then compares the predictions generated from different models as the measurement of dynamics model distance. 
%To ensure our kernel captures the correlation between dynamics models effectively, we construct four other approaches to quantify the model distance: \textbf{Weight Bias}, \textbf{Model-based Policy}, \textbf{Model-free Policy}, and \textbf{Exploratory Policy}. The first is Weight Bias, which calculates the discrepancy between the weight and bias of the model neural network as the model distance. The other three approaches use different policies to generate the model trajectories and measure their disparities to represent the model distance. Their distinction is that the Model-based Policy is built upon the SAC trained with a pessimistic MDP, while the Model-free Policy relies on the SAC obtained from the pre-collected offline dataset, and the Exploratory Policy generates trajectories by executing actions randomly. 
%Figure~\ref{fig:ablations} shows the evaluation results of alternative model distances comparison. Similar to the findings from the baseline evaluations, BOMS still gets better performances compared with other methods of calculating model distance. We can see that BOMS achieves greater stability in the magnitude of regret reduction across most tasks. 

On the other hand, recall from (\ref{eq:distance}) that by default BOMS takes only one-step predictions in measuring the model distance. One natural question to ask is whether \textit{multi-step predictions} generated by the learned models would help in the kernel design. To answer this, for the sake of comparison, we extend the model distance in (\ref{eq:distance}) to the multi-step version that takes the differences in states and rewards of the multi-step rollouts into account. Let $\ell$ denote the rollout length. From Figure~\ref{fig:ablations_traj_len}, we can see that BOMS under $\ell=1$ achieves the best regret performance, while the multi-step version under $\ell=5$, $20$ exhibit slower progress.
We hypothesize that this phenomenon is attributed to the inherent compounding modeling errors widely observed in MBRL.
Indeed, as also described in the celebrated work of MBPO \citep{janner2019trust}, longer trajectory rollouts typically lead to more severe compounding errors in MBRL. 
In summary, this supports the model distance based on one-step predictions as in (\ref{eq:distance}).


%Therefore, to reduce the impact of compounding modeling inaccuracies, we set $\ell$ = 5 and 20, with 200 and 50 trajectories respectively. 
%Regarding the experimental setup for the ablation study mentioned earlier, we set the maximum length of a trajectory as 100 and get 10 trajectories for model distance estimation, and a notable thing is that BOMS equals the one-step Model-based Policy. 
%To delve deeper into the impact of varying trajectory lengths $\ell$ on the selection result, we aim to compare BOMS and the Model-based Policy under different trajectory lengths $\ell$. 




%Based on the practical evaluation findings and the theoretical backing provided in Proposition ~\ref{prop:model_dis}, we have effectively addressed the question (2). We introduce an improved approach for capturing inter-model correlations, resulting in more favorable outcomes in the context of model selection. 


\input{chapters/fig-regret-ablation}

\vspace{1mm}
\noindent\textbf{{Is BOMS generic such that it can be integrated with various offline MBRL methods?}}
% \subsection{Is BOMS generic such that it can be integrated with various offline MBRL methods?}
%The experimental results displayed above are obtained using MOPO to generate dynamics model sets and policies. 
We demonstrate the compatibility of BOMS with other offline MBRL methods by integrating BOMS with RAMBO \citep{rigter2022rambo}, another recent offline MBRL method.
%as the offline model-based RL structure. 
Table~\ref{tab:rambo_regrets} shows the normalized regrets of BOMS-augmented RAMBO and the vanilla RAMBO. 
%Due to the page limit, the standard deviations are reported in Appendix. 
Similarly, BOMS-RAMBO can greatly enhance RAMBO within 10-20 iterations. This further validates the wide applicability of BOMS with offline MBRL in general. 
%Therefore, we can infer that even when different methods are employed, BOMS retains its model selection capability with limited online interactions.



\renewcommand{\arraystretch}{0.9}
\begin{table}[!h]
\caption{Normalized regrets under vanilla RAMBO and BOMS-augmented RAMBO. }
\label{tab:rambo_regrets}
\begin{adjustbox}{center}
\scalebox{0.75}{\begin{tabular}{c|l|c|c|c|c}
    \toprule
        \multicolumn{2}{c|}{\multirow{2}{*}{Tasks}} &
        \multicolumn{3}{c|}{BOMS-Augmented RAMBO} &
        \multirow{2}{*}{RAMBO} \\
        \cline{3-5}
        \multicolumn{2}{c|}{\multirow{2}{*}{}}&{$T=5$} & {$T=10$} & {$T=20$} & \\
    \midrule
        & med & 79.3 $\pm$ 10.9 & 70.2 $\pm$ 24.7 & \textbf{37.7 $\pm$ 38.1} & 73.6\\
        walker2d & med-r & 93.0 $\pm$ 2.6\phantom{0} & 93.0 $\pm$ 2.5\phantom{0} & \textbf{92.7 $\pm$ 2.5\phantom{0}} & 99.3 \\
        & med-e & 53.3 $\pm$ 10.6 & 43.0 $\pm$ 15.6 & \textbf{38.9 $\pm$ 17.2} & 65.4\\
    \bottomrule
\end{tabular}}
\end{adjustbox}
\end{table}

\noindent\textbf{How does the choice of $\alpha$ in (\ref{eq:distance}) impact model selection?} Table~\ref{tab:rescale_reward} shows the results of BOMS with $\alpha \in \{0.1, 1, 10\}$. We observe that different values of $\alpha$ influence the selection of the models, though the influence is not significant. This also shows that $\alpha = 1$ is generally a good choice.


\input{chapters/table-alpha}

%\subsection{Extra Experiments}
\begin{comment}
\subsection{Normalized Regrets of BOMS With Different $\alpha$}
The proposed model distance is a combination of the states and the rewards so applying affine transformations on the states and the rewards will influence the kernel. Therefore, to strike a balance between the discrepancy in states and rewards, we introduce a hyperparameter $\alpha$ in Equation (\ref{eq:distance}).
%and get
%\begin{equation}
%    d(M_t, M):= \displaystyle\mathop{\mathbb{E}}\bigg[\lVert s_1'-s_2'\rVert+\alpha\big\lvert r_t(s,a)-r(s,a)\big\rvert\bigg],
%\end{equation}
%where the expectation is taken over $s\sim \cD_{\text{off}}$, $a \sim \pi_t(\cdot\rvert s)$, $s_1'\sim P_t(\cdot\rvert s,a)$, and $s_2'\sim P(\cdot\rvert s,a)$.
Table~\ref{tab:rescale_reward} shows the results of BOMS with $\alpha \in \{0.1, 1, 10\}$. We can see that different values of $\alpha$ indeed influence the selection of the models, though the influence is not significant. And it also shows that $\alpha = 1$ is generally a good chioce.
To make the kernel truly invariant to transformations, one possibility is to consider reward and state normalization.
\end{comment}


\begin{comment}
The performance of BOMS is not limited to the tasks in D4RL. To show this, we also compare BOMS and the baseline methods on the "door-open" task in Meta-World \citep{yu2019meta}, which is a task solving problem.
Since the default Meta-World does not provide an offline dataset, we follow a data collection process similar to D4RL to obtain the offline medium-expert dataset. That is, the dataset is generated by mixing 1 million samples collected from the medium policy and 1 million samples collected from the expert policy. The policies are trained online using SAC, with the expert policy trained until converged and the medium policy partially trained.
Table~\ref{tab:door_open_regrets} shows the normalized regret results of BOMS and the baseline methods. It is clear that BOMS significantly improve over the vanilla MOPO. Additionally, BOMS can select a better model and have a chance to solve the task, while the vanilla MOPO can not solve the task.
\end{comment}

