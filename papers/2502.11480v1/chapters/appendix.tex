%\appendices

% \newpage
\section{Proof of Proposition \ref{prop:model_dis}}
\label{app:proof}

% \begin{table*}[!t]
% \caption{The normalized regrets of vanilla RAMBO and BOMS-augmented RAMBO. }
% \label{tab:rambo_regrets_appendix}
% \begin{adjustbox}{center}
% \begin{tabular}{cl|c|c|c|c|c}
%     \toprule
%         \multicolumn{2}{c|}{\multirow{2}{*}{Environments}} &
%         \multicolumn{4}{c|}{BOMS-Augmented RAMBO} &
%         \multirow{2}{*}{RAMBO} \\
%         \cline{3-6}
%         & &{$T=5$} & {$T=10$} & {$T=15$} & {$T=20$} & \\
%     \midrule
%         \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{walker2d}}} 
%         & medium & 79.33$\pm$10.90 & 70.25$\pm$24.79 & 37.72$\pm$38.08 & \textbf{37.72$\pm$38.08} & 73.66\\
%         & medium-replay & 93.00$\pm$2.61 & 93.02$\pm$2.59 & 92.74$\pm$2.59 & \textbf{92.74$\pm$2.59} & 99.36 \\
%         & medium-expert & 53.34$\pm$10.68 & 43.04$\pm$15.68 &  43.04$\pm$15.68 & \textbf{38.93$\pm$17.20} & 65.42\\
%     \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table*}

Before proving Proposition \ref{prop:model_dis}, we first introduce a useful property usually known as the \textit{simulation lemma} (\eg as indicated in Lemma A.1 in \citep{vemula2023virtues}).
\begin{lemma}[Simulation Lemma]
\label{lemma:sim}
    For any policy $\pi$, any initial state distribution $\omega$, and any pair of dynamics models $M', M''$, we have
    \begin{align}
        J^{\pi}_{M'}(\omega)-J^{\pi}_{M''}(\omega)
        &=\mathbb{E}_{s\sim \omega}[V^{\pi}_{M'}(s)-V^{\pi}_{M''}(s)]\\
        & = \frac{\gamma}{1-\gamma}\mathbb{E}_{s\sim\omega, a\sim\pi}\left [\mathbb{E}_{s'\sim M'(s,a)}[V^{\pi}_{M''}(s')]-\mathbb{E}_{s''\sim M''(s,a)}[V^{\pi}_{M''}(s'')]\right].
    \end{align}
\end{lemma}

For convenience, we restate the proposition as follows.
\difprop*
\begin{comment}
For convenience, we restate the proposition as follows.
\begin{prop}
%\label{prop:model_dis}
Given two policies $\widetilde{\pi}_1$ and $\widetilde{\pi}_2$ which are the optimal policies learned on models $\widetilde{M}_1=(P_1,r_1)$ and $\widetilde{M}_2=(P_2,r_2)$, respectively. Let ${L}$ be the Lipschitz constant of the value function. Then, we have 
\begin{align*}
    &J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega) \le \Big( J^{\widetilde{\pi}_1}_{M^*}(\omega)-J^{\pi_\beta}_{M^*}(\omega) \Big)+2\lambda \epsilon(\pi_\beta) \\
    &+\frac{1}{1-\gamma} {\mathbb{E}}_{\scriptstyle s\sim\omega, a\sim\widetilde{\pi}_1, \atop \scriptstyle s'_1\sim {P_1}(\cdot\rvert s,a), s'_2\sim {P_2}(\cdot \rvert s,a)} \Big[ \gamma L\norm\big{s'_1 - s'_2}_1 + \big| r_1(s,a)-r_2(s,a)\big| \Big],
\end{align*}
where $\epsilon(\pi_\beta):= \mathbb{E}_{(s,a)\sim \rho^{\pi_\beta}}[u(s,a)]$ is the modeling error under behavior policy $\pi_\beta$.
\end{prop}    
\end{comment}

\begin{proof}
To begin with, we have
\begin{align}
    J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega) 
    &= J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_1}_{\widetilde{M_1}}(\omega) + J^{\widetilde{\pi}_2}_{\widetilde{M_2}}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega) +
     J^{\widetilde{\pi}_1}_{\widetilde{M_1}}(\omega) \underbrace{- J^{\widetilde{\pi}_2}_{\widetilde{M_2}}(\omega) + J^{\widetilde{\pi}_1}_{\widetilde{M}_2}(\omega)}_{\leq 0} - J^{\widetilde{\pi}_1}_{\widetilde{M}_2}(\omega) \\
    &\le \underbrace{\left(J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_1}_{\widetilde{M}_1}(\omega)\right)}_{(A_1)}+ \underbrace{\left(J^{\widetilde{\pi}_2}_{\widetilde{M}_2}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega)\right)}_{(A_2)}
    + \underbrace{\left(J^{\widetilde{\pi}_1}_{\widetilde{M}_1}(\omega) - J^{\widetilde{\pi}_1}_{\widetilde{M}_2}(\omega)\right)}_{(A_3)},
\end{align}
where the first equality follows from the fact that $\widetilde{\pi}_1$ and $\widetilde{\pi}_2$ are optimal with respect to $\widetilde{M}_1$ and $\widetilde{M}_2$, respectively.
For the first and second terms on the RHS, we consult the theoretical results of MOPO. As $\widetilde{M}_1$ and $\widetilde{M}_2$ are uncertainty-penalized MDPs, one can verify that for any policy $\pi$,
\begin{align}
    J^{{\pi}}_{\widetilde{M}_1}(\omega) &\leq J^{{\pi}}_{M^*}(\omega),\\
    J^{{\pi}}_{\widetilde{M}_2}(\omega) &\leq J^{{\pi}}_{M^*}(\omega),
\end{align}
which have been shown in Equation (7) in MOPO \citep{yu2020mopo}.
Recall that $\widetilde{M}_1$ and $\widetilde{M}_2$ are the estimated dynamics models with the uncertainty penalty, and let $\widehat{M}_1$ be non-penalized version of $\widetilde{M}_1$. Then, we have 
\begin{align}
    (A_1)+(A_2) &=\left(J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_1}_{\widetilde{M}_1}(\omega) \right)+ \left(J^{\widetilde{\pi}_2}_{\widetilde{M}_2}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega)\right) \\
    &= J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{{\pi}_\beta}_{M^*}(\omega) + \underbrace{J^{{\pi}_\beta}_{\widetilde{M}_1}(\omega) -J^{\widetilde{\pi}_1}_{\widetilde{M}_1}(\omega)}_{\leq 0} + J^{{\pi}_\beta}_{M^*}(\omega) - J^{{\pi}_\beta}_{\widetilde{M}_1}(\omega) + {J^{\widetilde{\pi}_2}_{\widetilde{M}_2}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega)} \\ 
    &\le J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\pi_\beta}_{M^*}(\omega) + {J^{\pi_\beta}_{M^*}(\omega) - J^{{\pi}_\beta}_{\widetilde{M}_1}(\omega)} + \underbrace{J^{\widetilde{\pi}_2}_{\widetilde{M}_2}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega)}_{\leq 0} \\
    &\le J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\pi_\beta}_{M^*}(\omega) + J^{\pi_\beta}_{M^*}(\omega) - J^{{\pi}_\beta}_{\widehat{M_1}}(\omega) + J^{{\pi}_\beta}_{\widehat{M_1}}(\omega) - J^{{\pi}_\beta}_{\widetilde{M}_1}(\omega) \\
    &\le J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\pi_\beta}_{M^*}(\omega) + \left| J^{\pi_\beta}_{M^*}(\omega) - J^{{\pi}_\beta}_{\widehat{M_1}}(\omega) \right| +\lambda \epsilon_u (\pi_\beta) \\
    &\le \Big( J^{\widetilde{\pi}_1}_{M^*}(\omega)-J^{\pi_\beta}_{M^*}(\omega) \Big) + 2 \lambda \epsilon_u (\pi_\beta),
\end{align}
where the second last inequality holds by the definition of $\epsilon_u(\pi_\beta)$, and the last inequality holds by Lemma 4.1 in \citep{yu2020mopo}.

Next, to deal with the term $(A_3)$, we leverage the assumption that ${V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s)}$ is Lipschitz-continuous in state with a Lipschtize constant $L$. Take MuJoCo environments as an example, the state spaces of these tasks are continuous and composed of the positions of body parts and their corresponding velocities, and the reward functions are also calculated based on the body locations and velocities. In this scenario,  the value function is Lipschitz-continuous in states. 
Besides the assumption mentioned above, we also utilize the \textit{simulation lemma} in Lemma \ref{lemma:sim}. For notational convenience, below we let $\mathbb{E}$ be a shorthand for $\displaystyle {\mathbb{E}}_{\scriptstyle s\sim\omega, a\sim\widetilde{\pi}_1, \atop \scriptstyle s'_1\sim \widetilde{M_1}(\cdot\rvert s,a), s'_2\sim \widetilde{M_2}(\cdot \rvert s,a)}$ throughout this proof. Then, an upper bound of $(A_3)$ is provided as follows:
\begin{align}
    J^{\widetilde{\pi}_1}_{\widetilde{M}_1}(\omega) - J^{\widetilde{\pi}_1}_{\widetilde{M}_2}(\omega)
    &= \displaystyle\mathop{\mathbb{E}}_{s\sim\omega} 
    {\left[ V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s) - V^{\widetilde{\pi}_1}_{\widetilde{M}_2}(s) \right]} \\
    &=\displaystyle\mathop{\mathbb{E}}_{s\sim\omega, a\sim\widetilde{\pi}_1} \Big[\Big(r_1(s,a)+ \gamma\displaystyle\mathop{\mathbb{E}}_{s'_1\sim \widetilde{M}_1(s,a)}[V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_1)]\Big) - \Big( r_2(s,a)+\gamma \displaystyle\mathop{\mathbb{E}}_{s'_2\sim \widetilde{M}_2(s,a)}[V^{\widetilde{\pi}_1}_{\widetilde{M}_2}(s'_2)] \Big)\Big] \\
    &= \mathop{\mathds{E}} \Big[\gamma \Big( V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_1)-V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_2)+V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_2)-V^{\widetilde{\pi}_1}_{\widetilde{M}_2}(s'_2) \Big) + \Big(r_1(s,a)-r_2(s,a)\Big) \Big]\\
    &= \frac{1}{1-\gamma} \displaystyle\mathop{\mathds{E}} {\left[ \gamma \Big(V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_1) - V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_2) \Big) + \Big(r_1(s,a)-r_2(s,a)\Big)\right]}\\
    & \tag*{\text{(by Lemma \ref{lemma:sim})}} \\
    &\le \frac{1}{1-\gamma}  \displaystyle\mathop{\mathds{E}} {\left[\gamma\left| V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_1) - V^{\widetilde{\pi}_1}_{\widetilde{M}_1}(s'_2)\right| + \Big| r_1(s,a)-r_2(s,a)\Big| \right]} \\
    &\le \frac{1}{1-\gamma}  \displaystyle\mathop{\mathds{E}} \Big[ \gamma L\norm\big{ s'_1 - s'_2}_1 + \big| r_1(s,a)-r_2(s,a)\big| \Big]
\end{align}

By combining all the upper bounds of ($A_1$)-($A_3$), we complete the proof of Proposition~\ref{prop:model_dis}.
\end{proof}

\newpage
\section{Experimental Configurations}
\label{app:config}


\noindent\textbf{Detailed configuration of Table~\ref{table:motivating}:} 
As a motivating experiment, we evaluate the validation-based and OPE-based model selection schemes on MOPO to highlight their limitations.
Regarding training the dynamics models and policies, we follow
the implementation steps outlined in MOPO: (i) Regarding the
dynamics models, we use model ensembles by aggregating multiple
independently trained neural network models to generate the
transition function in the form of Gaussian distributions. (ii) Regarding the policy policy, we use the popular SAC algorithm on the uncertainty-penalized MDP constructed by MOPO. To acquire the
candidate model set $\cM$, we create a collection of 150 dynamics models for each task (specifically, we obtain 50 models under each
of the 3 distinct random seeds for each MuJoCo locomotion task).
Regarding the OPE-based approach, we employ Fitted Q-Evaluation \citep{le2019batch} as our OPE method, which is a value-based algorithm that directly learns the Q-function of the policy.

\noindent\textbf{Hyperparameters of the experiments in Section~\ref{sec:exp}:}
Here we provide the hyperparameters utilized in our experiments. For the offline model-based RL setup, we directly adopt the default configuration of MOPO, including the hyperparameters in the dynamics model and policy training.
Regarding the calculation of model distance, we employ 1000 data points as input in BOMS. To maintain a comparable dataset size, we utilize 10 trajectories with a maximum of 100 rollout steps to assess model distance for Model-based Policy and Exploration Policy methods in the study.
For the online interaction with the environment, different environments lead to different budget settings. In MuJoCo tasks, we employ 5 trajectory samples for BO updates as the maximum rollout length is set to be 1000.
On the other hand, in the case of the Pen task in the Adroit environment, where the maximum number of steps is restricted to 100, we increase the number of trajectories to 20 (i.e., 2000 sample transitions) for the BO updates to get a comparable number of samples.

{\textbf{Computing infrastructure:} All our experiments are conducted on machines with Intel Xeon Gold 6154 3.0GHz CPU, 90GB DDR4-2666 RDIMM memory, and NVIDIA Tesla V100 SXM2 GPUs.}

\section{Additional Related Work}
\label{app:related}
%\subsection{Offline RL}
The existing offline RL methods address the distribution shift problem from either a model-free or a model-based perspective:
%Offline RL offers a solution to address problems by utilizing pre-collected datasets for policy learning, making it a more effective and practical approach compared to online RL. However, an inevitable challenge in offline RL is the presence of the distribution shift, where the learned policy encounters the states in the environment that differ from those in the training dataset.

\vspace{1mm}
\noindent {\textbf{Offline model-free RL:} To tackle the distribution shift issue, several prior offline model-free RL approaches encode conservatism into the learned policies to prevent the learner from venturing into the out-of-distribution (OOD) areas. Specifically, conservatism can be implemented by either by either restricting the learned policy to stay aligned with the behavior policy \citep{fujimoto2019off, wu2019behavior, kumar2019stabilizing, nair2020awac, fujimoto2021minimalist, wang2022diffusion}, adding regularization to the value functions \citep{kumar2020conservative, kostrikov2021offline, bai2022pessimistic}, or applying penalties based on the uncertainty measures to construct conservative action value estimations \citep{an2021uncertainty, wu2021uncertainty, bai2022pessimistic}. Note that the above list is by no means exhaustive and is mainly meant to provide an overview of the main categories of offline model-free RL. Notably, among the above works, one idea in common is that these conservative model-free methods encourage the learned policy to align its actions with the patterns observed in the offline dataset.}
%or imposing constraints onto the policy \citep{fujimoto2019off, fujimoto2021minimalist, nair2020awac}.

%These methods involve restricting the learned policy to stay aligned with the behavior policy \citep{fujimoto2019off, wu2019behavior, kumar2019stabilizing, nair2020awac, fujimoto2021minimalist, wang2022diffusion}, incorporating regularization into value function learning for OOD state-action pairs \citep{kumar2020conservative, kostrikov2021offline, xu2022constraints}, and adding the penalties based on uncertainty quantification to obtain conservative action value estimations (\citep{bai2022pessimistic, an2021uncertainty, wu2021uncertainty}). One thing in common is that these conservative model-free methods encourage the learned policy to align its actions with the patterns observed in the offline dataset. 
\vspace{1mm}
\noindent {\textbf{Offline model-based RL:} On the other hand, offline model-based RL \citep{janner2019trust, lu2021revisiting, argenson2020model, matsushima2020deployment} encode conservatism into the learned dynamics models based on the pre-collected dataset and thereafter optimize the policy. 
%Many model-based methods also aim to mitigate the distribution shift problem through the conservative concept. 
To begin with, MOPO \citep{yu2020mopo} and MOReL \citep{kidambi2020morel} are two seminal conservative offline model-based methods, which construct pessimistic MDPs by giving penalty to those OOD state-action pairs based on different uncertainty estimations.
Specifically, MOPO chooses the standard deviation of the transition distribution as the soft reward penalty, and MOReL introduces the ``halt state" in MDPs and imposes a fixed penalty when the total variation distance between the learned transition and the true transition exceeds a certain threshold.
Subsequently, \citep{matsushima2020deployment} proposes BREMEN, which  approaches the distributional shift problem through behavior-regularized model ensemble.
Moreover, given that uncertainty estimates with complex models can be unreliable, COMBO \citep{yu2021combo} addresses the distribution shift by directly regularizing the value function on out-of-support state-action tuples generated under the learned model, without requiring an explicit uncertainty estimation in the planning process. 
More recently, model-based RL with conservatism has been addressed from an adversarial learning perspective. For example, \citep{rigter2022rambo} proposes RAMBO-RL, which directly learns the models in an adversarial manner by enforcing conservatism through a minimax optimization problem over the policy and model spaces. Moreover, \citep{bhardwaj2023adversarial} proposes ARMOR, which adopts the concept of relative pessimism through a minimax problem over policies and models with respect to some reference policy. In this way, ARMOR can improve upon an arbitrary reference policy regardless of the quality of the offline dataset. 
}

{Despite the recent progress on offline model-based RL, it remains largely unexplored how to achieve effective model selection. The proposed BOMS serves as a practical solution that can complement the existing offline model-based RL literature.}
