%\section{A Bayesian Optimization Framework for Model Selection}
\section{Methodology}
\label{sec:boms}
\vspace{-1mm}
%In this section, we formally present the proposed BO approach for model selection. We start from the overall BOMS framework and proceed to introduce the proposed kernel and the practical implementation of BOMS.
%In this section, we formally present the proposed BOMS framework and introduce the proposed kernel and the implementation.
%The existing offline model-based RL approaches often opt for training policies using the dynamics model with the lowest validation loss achieved through supervised learning. Nevertheless, it is worth noting that the validation dataset typically have limited coverage, potentially making it insufficient to comprehensively evaluate the performance of the dynamics model across the entire state-action space in the context of model training. 
%To select a proper dynamics model, one straightforward method is to use OPE and compare the estimated total reward of the policy learned from each dynamics model. However, as mentioned in Section \ref{sec:intro}, OPE suffers from limited accuracy due to the distribution shift problem and can arrive at a rather sub-optimal policy. Motivated by these concerns, we propose Bayesian Optimization for Model Selection, an offline model-based RL algorithm that incorporates Bayesian Optimization and the limited environment interaction budget for dynamics model selection in offline RL.} 

%\subsection{Method Structure}
\subsection{Recasting Model Selection as BO}
\label{sec:boms:framework}
\vspace{-1mm}
We start by connecting the active model selection problem to BO. Specifically, below we interpret each component of model selection in the language of BO.
\vspace{-1mm}
\begin{itemize}[leftmargin=*]
    \item \textit{Input domain $\cX$}: Recall that offline model-based RL first uses the offline dataset $\cD_{\text{off}}$ to generate a collection of ${N}$ dynamics models denoted by $\cM$ ${= \{M^{(1)}, M^{(2)}, ..., M^{(N)}\}}$, which serve as the candidate set for model selection. We view this candidate set $\cM$ as the input domain in BO, \ie $\cM\equiv \cX$.
    %\vspace{-1mm}
    \item \textit{Black-box objective function}: The goal of model selection is to choose a dynamics model from $\cM$ such that the policy learned downstream enjoys high expected return. Let $\pi^{(i)}$ be the policy learned under $M^{(i)}\in \cM$, and let $J_{M^*}^{\pi^{(i)}}$ be the true total expected return achieved by $\pi^{(i)}$. Accordingly, model selection can be viewed as black-box maximization with an unknown objective function $f \equiv J^{\pi}_{M^*}$ defined on $\cM$.
    %\vspace{-1mm}
    \item \textit{Sampling procedure}: In the context of model selection, each sample corresponds to evaluating the policy learned from a model $M'\in \cM$. We let $M_t$ and $J^{\pi_t}_{M^*}$ denote the model selected and the corresponding policy performance at the $t$-th iteration, respectively. Let $\cH_t:=\{(M_i,J^{\pi_i}_{M^*})\}_{i=1}^{t}$ denote the sampling history up to the $t$-th iteration. To determine the $(t+1)$-th sample, we take any off-the-shelf AF, compute $\Psi(M'; \cH_t)$ for each model $M'\in \cM$, and then select the one with the largest AF value, \ie $M_{t+1}\in \arg\max_{M'\in \cM} \Psi(M'; \cH_t)$. 
    In our experiments, we use the celebrated GP-UCB \citep{srinivas2012information} as the AF.
    Then, upon sampling, we learn a policy $\pi_{t+1}$ based on the chosen dynamics model $M_{t+1}$ (\eg by applying SAC), and the function value $J^{\pi_{t+1}}_{M^*}$ can be obtained and included in the history. In practice, we use the Monte-Carlo estimates $R_{t+1}$ an an approximation of the true $J^{\pi_{t+1}}_{M^*}$.
    %\vspace{-1mm}
    \item \textit{GP function prior}:  As in standard BO, here we impose a GP function prior that implicitly captures the structural properties of the objective function. Notably, GP serves as a surrogate model for facilitating probabilistic inference on the unknown function values. Specifically, under a GP prior, we assume that for any subset of models $\cM^\dagger \subseteq \cM$, their function values follow a multivariate normal distribution with mean and covariance characterized by a mean function $m:\cM\rightarrow \mathbb{R}$ and a covariance function $k:\cM\times \cM \rightarrow \mathbb{R}$. As in the standard BO literature, we simply take $m$ as a zero function. Recall that $R_{t}$ denotes the Monte-Carlo estimate of the true $J^{\pi_{t}}_{M^*}$. Let $\bm{R}_t$ denote the vector of all $R_i$, for $i\in \{1,\cdots,t\}$. For convenience, we let $\cM_t$ denote the set of models selected in the first $t$ iterations. 
    For any pair of model subset $\cM',\cM''$, define $\bm{K}(\cM',\cM'')$ to be a $\lvert \cM'\rvert \times \lvert \cM''\rvert$ covariance matrix, where the entries are the covariances $k(M',M'')$ of $M'\in \cM'$ and $M''\in \cM''$.
    Then, given the history $\cH_t$, the posterior distribution of $J^{\pi}_{M^*}$ of each model $M\in \cM$ follows a normal distribution $\mathcal{N}(\mu_t(M),\sigma^2_t(M))$, where
    \begin{align}
    \mu_t(M) &= {\bm{K}(M, \cM_t)} \bm{K}(\cM_t,\cM_t)^{-1} \bm{R}_t,   \\
    \begin{split}    
    \sigma^2_t(M) &= \bm{K}(M, M)\\
        \quad -  &{\bm{K}(M,\cM_t)} \bm{K}(\cM_t,\cM_t)^{-1} \bm{K}(\cM_t, M).
    \end{split}
    \end{align}
    \item \textit{GP kernels for the covariance function}: To specify the covariance function $k(\cdot,\cdot)$ defined above, we adopt the common practice in BO and use a kernel function. For each pair of models $M',M''\in \cM$, under some pre-configured distance $d:\cM\times\cM\rightarrow [0,\infty)$, we use the popular Radial Basis Function (RBF) kernel \citep{williams2006gaussian}
    \begin{equation}
    k(M', M'') := \exp(-\frac{d(M', M'')^\text{2}}{\text{2}\ell^\text{2}}),\label{eq:RBF}
    \end{equation}
    where $\ell$ is the kernel lengthscale and ${d(M', M'')}$ is the model distance between $M'$ and $M''$.
    The overall procedure of BOMS is summarized in Algorithm \ref{algo:boms}. 
    %between the currently selected model ${M_t}$ and other dynamics model candidates ${M_j}$, ${1 \le j \le N}$. 
    %\item \textit{Acquisition function}: To determine the next dynamics model ${M_{t+\text{1}}}$ to evaluate in the environment, we need an acquisition function that quantifies the potential of an input point to improve the current best solution. The acquisition function balances exploration and exploitation based on the GP model's uncertainty. In our approach, we leverage UCB as the acquisition function and choose the dynamics model with the highest UCB value, emphasizing both high expected value (exploitation) and high uncertainty (exploration) in the estimation: $M_{t+\text{1}} = \arg\max_m (\mu_t(m)+\kappa \: \sigma_t(m))$, where ${\kappa}$ is the hyperparameter that controls the balance between exploration and exploitation. 
    %Specifically, BO considers prior information of the function ${f}$ and refines the prior using samples drawn from ${f}$. This process results in a posterior distribution that provides a more accurate approximation of ${f}$. We typically use a Gaussian process (GP) as the surrogate function to approximate ${f}$, which provides a probabilistic estimation of the objective function's behavior. Since we suppose the similar dynamics models generate similar values, it is reasonable to let the collected dynamics models $\cM$ be candidate points and the corresponding empirical return set $\cR$ ${= \{R_1, R_2, ..., R_N\}}$ be the objective function values, where $R= \sum_{n=1}^{\infty} r_n$. With the posterior mean ${\mu}$ and variance ${\sigma^2}$, we can compute the probability distributions of the GP as follows:
%where ${k}$ is the vector of kernel values between the new model point ${M_t}$ and all the training model points $\cM$, and K is the covariance matrix given by the kernel function applied to all possible pairs of candidate points. The kernel function K defines the shape and characteristics of the GP model and is used to capture the underlying relationships between data points. Formally, the kernel function in our approach is the radial basis function (RBF) and can be denoted as the following function:
\end{itemize}
%In our approach, we first assume that similar dynamic models have similar model values. We use the fixed dataset $\cD$ to generate ${N}$ dynamics models as the candidates for the model selection, and the collected model set can be denoted as $\cM$ ${= \{M_1, M_2, ..., M_N\}}$. In each selection epoch ${t}$, we choose one dynamics model ${M_t}$ with the highest score from the acquisition function based on BO, and then train the optimal policy ${\pi_t}$ of the selected model ${M_t}$ for the environmental interaction and gain the empirical returns ${R_t}$ from the limited trajectories. By the assumption above, we can update GP and choose the next promising model by taking into account the recently collected empirical returns and the kernel function. At the end of the selection phase, the dynamics model with the highest empirical return is our final dynamics model selection result. The pseudo-code of BOMS is demonstrated in Algorithm~\ref{algo:boms}.

\begin{algorithm}[ht]
\caption{BOMS}\label{algo:boms}
\begin{algorithmic}
\Require True environment $M^*$, candidate model set $\cM$, and total number of selection iterations $T$.
\For{$t \gets 1$ to $T$}
    \If{$t=1$} 
        \State Randomly choose a model $M_1$ from $\cM$.
    \Else
        \State \multiline{Update the posterior $\mathcal{N}(\mu_t(M),\sigma^2_t(M))$ of each dynamics model $M\in \cM$.}
        %\State \multiline{Fit GP model with new empirical average return ${R_t}$ and K.}
        \State 
            %\multiline{Select the next model $M_t\in \cM$ by using an acquisition function $\Psi$ as ${M_{t}}\in \arg\max_{M\in \cM}\Psi(M;\cH_t)$}
            \multiline{Select ${M_{t}}\in \arg\max_{M\in \cM}\Psi(M;\cH_t)$.}
    \EndIf
    \State 
        \multiline{Learn a policy ${\pi_{t}}$ on the selected model ${M_{t}}$.}
    \State 
        \multiline{Evaluate $\pi_t$ by an estimate of empirical total reward $R_t$ based on online interactions with $M^*$.}
        %\multiline{Using ${\pi_{t}}$ to interact with the \textbf{env} and get the ${R_{t}}$ of limited trajectories.}
    \State 
        \multiline{Calculate the model distance $d(M_{t},M')$ between ${M_{t}}$ and all other models $M'\in \cM$.}
\EndFor

\State Return the model ${M}_{t^*}$ with $t^*:=\arg\max_{1\leq t\leq T}R_t$.
\end{algorithmic}
\end{algorithm}

\vspace{-1mm}
\noindent{\textbf{Remarks on realism of obtaining a candidate model set $\cM$:} We can naturally obtain $\cM$ by collecting the dynamics models learned during training under any offline MBRL method, without any additional training overhead. For example, in Section \ref{sec:exp}, we follow the model training procedure of MOPO and take the last 50 models as candidate models. This also induces a fair comparison between MOPO and BOMS as they both see the same group of dynamics models.} 
\vspace{-1mm}
%Subsequently, we train the optimal policy ${\pi_{t+\text{1}}}$ of the dynamics model ${M_{t+\text{1}}}$ selected from BO and apply the trained policy in the environment to get the empirical return of the limited trajectories. Then, ${R_{t+\text{1}}}$ is used to update ${\mu}$ and ${\sigma^\text{2}}$ of the GP for the next model selection epoch.


%; initial states ${\omega}$;

%\subsection{Theoretical Insights for Kernel Design}
%\label{sec:boms:kernel}

\subsection{Model-Induced Kernels for BOMS}
\label{sec:boms:kernel}
To enable BOMS, one major challenge is to measure the distance $d(M',M'')$ between the dynamics models required by the kernel function. However, in offline MBRL, it remains unknown how to characterize the distance between dynamics models in a way that nicely reflects the smoothness in the total reward. 
\vspace{-1mm}

\noindent{\textbf{Theoretical insights for BOMS kernel design:} To tackle the above challenge, we provide useful theoretical results that can motivate the subsequent design of a distance measure for offline MBRL. For convenience, as all the MDPs considered in this paper share the same $\cS, \cA, \omega$, and $\gamma$, in the sequel, we use a two-tuple $(P,r)$ to denote a model $M$, with a slight abuse of notation. Recall that $\pi_\beta$ denotes the behavior policy of the offline dataset. Here we use MOPO as an example and consider the penalized MDPs  $\widetilde{M}=({P},\widetilde{r})$, where $\widetilde{r}$ is the reward function penalized by the uncertainty estimator $u(s,a)$ and denoted as $\widetilde{r}(s,a) = r(s,a) - \lambda u(s,a)$ by MOPO, as described in Section \ref{sec:prelim}. 
To establish the following proposition, we make one regularity assumption that the value functions of interest are Lipschitz continuous in state, \ie there exist $L>0$ such that $\lvert V^{\pi}_{M}(s)-V^{\pi}_{M}(s)\rvert \leq L\cdot \lVert s-s'\lVert$, for all $s,s'\in \cS$, and ${L}$ is the Lipschitz constant.
We state Proposition~\ref{prop:model_dis} below, and the proof  is in Appendix \ref{app:proof}.}
\begin{restatable}{prop}{difprop}
%\begin{prop}
\label{prop:model_dis}
Given two policies $\widetilde{\pi}_1$ and $\widetilde{\pi}_2$ which are the optimal policies learned on models $\widetilde{M}_1=(P_1,r_1)$ and $\widetilde{M}_2=(P_2,r_2)$, respectively. Then, we have 
\begin{align*}
    &J^{\widetilde{\pi}_1}_{M^*}(\omega) - J^{\widetilde{\pi}_2}_{M^*}(\omega) \le \Big( J^{\widetilde{\pi}_1}_{M^*}(\omega)-J^{\pi_\beta}_{M^*}(\omega) \Big)+2\lambda \epsilon(\pi_\beta) \\
    &+\frac{1}{1-\gamma} {\mathbb{E}} \Big[ \gamma L\norm\big{s'_1 - s'_2}_1 + \big| r_1(s,a)-r_2(s,a)\big| \Big],
\end{align*}
where the expectation is taken over $s\sim\omega$, $a\sim\widetilde{\pi}_1$, $s'_1\sim {P_1}(\cdot\rvert s,a)$, and $s'_2\sim {P_2}(\cdot \rvert s,a)$ and $\epsilon(\pi_\beta):= \mathbb{E}_{(s,a)\sim \rho^{\pi_\beta}}[u(s,a)]$ is the modeling error under behavior policy $\pi_\beta$.
%\end{prop}
\end{restatable}

\noindent\textbf{Insights offered by Proposition \ref{prop:model_dis}}: Notably, Proposition~\ref{prop:model_dis} shows that the policy performance gap of the learned models evaluated in the true environment is bounded by three main factors: 
(i) The first term is the expected discounted return difference between a learned policy and the behavior policy $\pi_\beta$. This term can be considered fixed since it does not change when we measure the distances between the selected model ${M}_t$ and other candidate models.
(ii) The second term is the modeling error along trajectories generated by $\pi_\beta$. Under MOPO, $\epsilon_u(\pi_\beta)$ should be quite small as $u(s,a)$ mainly estimates the discrepancy between the learned transition $\hat{P}$ and the true transition ${P}$, where $\hat{P}$ is learned from the dataset yielded from $\pi_\beta$. Therefore, for state-action pairs from $\rho^{\pi_\beta}$, $\hat{P}$ is close to the true transition ${P}$, and thus $\epsilon_u(\pi_\beta)$ should be relatively small. 
(iii) The third term is the next-step predictions difference between two dynamics models given $\widetilde{\pi}_1$. 
Moreover, note that the first two terms depend on the behavior policy, which is completely out of control by us, and the third term is the only term that reflects the discrepancy between $\widetilde{M_1}$ and $\widetilde{M}_2$. As a result, only the third term matters in measuring of the performance gap between dynamics models in BOMS.

\noindent\textbf{Model-induced kernels:} Built on the BOMS framework in Section \ref{sec:boms:framework} and the theoretical grounding provided above, we are ready to present the kernel used in BOMS. Recall from (\ref{eq:RBF}) that we use an RBF kernel in the design of the covariance matrix. To leverage Proposition \ref{prop:model_dis} in the context of BOMS, we take ${\pi}_1$ as the policy learned from the selected model $M_t=(P_t,r_t)$ at the $t$-th iteration, and ${\pi}_2$ be the policy from another candidate model $M=(P,r)$ in $\cM$. Then, motivated by Proposition \ref{prop:model_dis}, we design the model distance
\begin{equation}
    d(M_t, M):= \displaystyle\mathop{\mathbb{E}}\bigg[\lVert s_1'-s_2'\rVert+\alpha \big\lvert r_t(s,a)-r(s,a)\big\rvert\bigg],\label{eq:distance}
\end{equation} 
where the expectation is taken over $s\sim \cD_{\text{off}}$, $a \sim \pi_t(\cdot\rvert s)$, $s_1'\sim P_t(\cdot\rvert s,a)$, and $s_2'\sim P(\cdot\rvert s,a)$, and $\alpha$ is a parameter that balances the discrepancies in states and rewards. In the experiments, we find that $\alpha=1$ is generally a good choice.
%Inspired by Lemma mentioned above, we define ${d(M_t, M_j)}$ as the Euclidean distance between the next-step predictions of models. 
In practice, we randomly sample a batch of states from the fixed dataset $\cD_{\text{off}}$ to obtain empirical estimates of the above distance $d(M_t, M)$. Additionally, since the covariance function $k(\cdot,\cdot)$ in BO is typically symmetric, we further impose a condition that the model distance $d$ enjoys symmetry, \ie ${d(M_t, M_j)}$ is equal to ${d(M_j, M_t)}$.

\vspace{1mm}
\noindent\textbf{Complexity of the proposed kernel:} Under BOMS with the proposed kernel and $N$ candidate models, the calculation of the covariance terms at the $t$-th iteration takes $O(t^2+tN)$, which is the same as standard BO. Hence, the proposed kernel does not introduce additional computational overhead.

\begin{comment}
    \begin{prop}
\label{prop:model_dis}
Given two policies ${\pi}_i$ and ${\pi}_j$ which are learned from models ${M_i}$ and ${M_j}$ respectively. Let ${L}$ be the Lipschitz constant and $\epsilon_u(\pi)= \displaystyle\mathop{\mathbb{E}}_{ (s,a)\sim\rho^\pi} \left[u(s,a)\right]$, where $\rho^\pi$ is the discounted state-action probability distribution under policy $\pi$, and let $\mathop{\mathbb{E}}$ be a shorthand for $\displaystyle \mathop{\mathbb{E}}_{\scriptstyle s\sim\omega, a\sim{\pi}_i, \atop \scriptstyle s'_1\sim \widetilde{M_i}(s,a), s'_2\sim \widetilde{M_j}(s,a)}$,  we have: 
\begin{align*}
    &J^{{\pi}_i}_{M^*}(\omega) - J^{{\pi}_j}_{M^*}(\omega) \le  \\
    &\frac{1}{1-\gamma} \mathop{\mathds{E}} \Big[ \gamma L\norm\big{s'_1 - s'_2}_1 + \big| r_i(s,a)-r_j(s,a)\big| \Big] + 2\lambda \epsilon_u(\pi_\beta) \\
    & + \Big( J^{{\pi}_i}_{M^*}(\omega)-J^{\pi_\beta}_{M^*}(\omega) \Big)
\end{align*}
\end{prop}
\end{comment}

%By referencing the theoretical sections in MOPO and the work from \citep{vemula2023virtues}, 
%there are not many works that discuss the definition of the distance between different dynamics models. By referencing the theoretical sections in MOPO and the work from \citep{vemula2023virtues}, we state Proposition~\ref{prop:model_dis} to give our model distance method the theoretical support.


\begin{comment}
\subsection{Practical Implementation}
For the practical implementation part, we use MOPO as our offline model-based RL framework. Each dynamics model $m$ corresponds to the penalized MDP $\widetilde{M}= \{ S,A,\widetilde{r},\widehat{P}, \omega, \gamma \}$ with different transition function $\widehat{P}$ respectively, where $\widetilde{r}$ is the reward function penalized by the uncertainty estimator $u(s,a)$ and denoted as $\widetilde{r} = r - \lambda u(s,a)$ by MOPO. 

%Having an appropriate method to measure the model distance plays an important part in the BO kernel function. However, in the field of offline model-based RL, there are not many works that discuss the definition of the distance between different dynamics models. By referencing the theoretical sections in MOPO and the work from \citep{vemula2023virtues}, we state Proposition~\ref{prop:model_dis} to give our model distance method the theoretical support.

In BO model selection stage, assume ${\pi}_i$ be the policy learned from the selected model $M_t$ at time $t$, and ${\pi}_j$ be the policy from another candidate. For the second term, MOPO indicates that $\epsilon_u(\pi_\beta)$ should be quite small. $u(s,a)$ mainly estimate the transition disparity between the learned transition $\hat{P}$ and the true transition ${P}$, which $\hat{P}$ trained from the dataset yielded from $\pi_\beta$. Therefore, for state-action pairs from $\rho^\pi$, $\hat{P}$ should be close to the true transition ${P}$, and thus $\epsilon_u(\pi_\beta)$ should be relatively small. The third term can be considered constant since the expected return difference between ${\pi}_i$ and behavior policy does not change when we measure the different distances between the selected model ${M}_t$ and other models. As a result, only the first term matters in the measurement of the performance gap between the two dynamics models in our BO selection phase.
\begin{gather}
    d(M_t, M_j) = \displaystyle\mathop{\mathbb{E}}_{s\sim D, a \sim \pi_t} \: d(M_t(s,a), M_j(s,a))
\end{gather}
Inspired by Lemma mentioned above, we define ${d(M_t, M_j)}$ as the Euclidean distance between the next-step predictions of models. For the practical implementation, we randomly sample the $I$ states from the fixed dataset $\cD$ and gain actions from learned policy ${\pi_t}$ as the inputs of dynamics models. Additionally, since the covariance matrix K in BO should be symmetric, we further give a reasonable definition which the model distance ${d(M_t, M_j)}$ is equal to ${d(M_j, M_t)}$ for the currently selected model ${M_t}$ and other dynamics model candidates ${M_j}$, which makes the covariance matrix be identical to the transpose of the original matrix.

The time complexity of BOMS with $n$ candidatesâ€™ is mainly composed of GP ($O(n^3)$) and the model distance measurement ($I$ model outputs * $n$ candidates).
    
\end{comment}

