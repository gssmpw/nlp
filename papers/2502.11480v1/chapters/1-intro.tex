\section{Introduction}
\label{sec:intro}
%Reinforcement learning (RL) has recently drawn significant attention due to its remarkable ability to train autonomous agents for complex sequential decision-making across a variety of real-world domains, such as robotics \cite{kalashnikov2018scalable, ibarz2021train}, networked systems \cite{chu2020multi, qu2020scalable}, healthcare \cite{yu2021reinforcement, yom2017encouraging}, and recommender systems \cite{afsar2022reinforcement, zhao2021dear}. In conventional online RL, an agent learns by collecting information about the transition dynamics and the reward function through direct online interactions with the environment, either under the learned policy or an exploratory behavior policy. Accordingly, one fundamental issue of online RL is that in many real-world applications, online interactions could be rather impractical as data collection is either costly (\eg robotics and recommender systems under the risk of damaging machines or losing customers) or even dangerous (\eg healthcare \cite{nambiar2023deep}). To address this challenge, the formulation of \textit{offline RL} has been adopted to learn a well-performing policy based solely on the pre-collected training datasets \cite{levine2020offline}. This characteristic underpins the great potential of offline RL in real-world applications where online interaction is impractical or unsafe.

%Reinforcement learning (RL) has drawn substantial attention in training autonomous agents in complex sequential decision-making across various domains like robotics \cite{kalashnikov2018scalable, ibarz2021train}, networked systems \cite{chu2020multi, qu2020scalable}, healthcare \cite{yu2021reinforcement, yom2017encouraging}, and recommender systems \cite{afsar2022reinforcement, zhao2021dear}. 
In conventional online reinforcement learning (RL), agents learn through direct online interactions, posing challenges in real-world scenarios where online data collection is either costly (\eg robotics \citep{kalashnikov2018scalable, ibarz2021train} and recommender systems \citep{afsar2022reinforcement, zhao2021dear} under the risk of damaging machines or losing customers) or even dangerous (\eg healthcare \citep{yu2021reinforcement, yom2017encouraging}). \textit{Offline RL} addresses this challenge by learning from pre-collected datasets, showcasing its potential in applications where online interaction is infeasible or unsafe.
To enable offline RL, existing methods mostly focus on addressing the fundamental \textit{distribution shift} issue and avoid visiting the out-of-distribution state-action pairs by encoding conservatism into the policy \citep{levine2020offline,kumar2020conservative}.
%, which results from the mismatch between the state-action distribution of the offline dataset and that the agent encounters during actual deployment . 
%Accordingly, offline RL methods avoid visiting those out-of-distribution (OOD) state-action pairs by encoding conservatism into the learned policy.

As one major category of offline RL, offline \textit{model-based} RL (MBRL) implements such conservatism through one or an ensemble of dynamics models, which is learned from the offline dataset and later modified towards conservatism, either by uncertainty estimation \citep{yu2020mopo, kidambi2020morel,yu2021combo,rashidinejad2021bridging,sun2023model} or adversarial training \citep{rigter2022rambo,bhardwaj2023adversarial}. 
The modified dynamics models facilitate the policy learning by offering additional imaginary rollouts, which could achieve better data efficiency and potentially better generalization to those states not present in the offline dataset. Moreover, several offline MBRL approaches \citep{rigter2022rambo,sun2023model,bhardwaj2023adversarial} have achieved competitive results on the D4RL benchmark tasks \citep{fu2020d4rl}.
%(i) In offline \textit{model-free} methods, such conservatism can be directly implemented by either adding regularization or penalty to the value functions \cite{kumar2020conservative, kostrikov2021offline, bai2022pessimistic} or imposing constraints onto the policy \cite{fujimoto2019off, fujimoto2021minimalist, nair2020awac}. 
%(ii) By contrast, 
%\pch{We need to emphasize the importance of offline model-based RL}


Despite the progress, one fundamental and yet underexplored issue in offline MBRL lies in the \textit{selection of the learned dynamics models}. Specifically, in the phase of learning the transition dynamics, one needs to determine either a stopping criterion or which model(s) in the candidate model set to choose and introduce conservatism to for the downstream policy learning. Notably, the model selection problem is challenging in offline RL for two reasons: (i) \textit{Distribution shift}: Due to the inherent distribution shift, the validation of either the dynamics models or the resulting policies could be quite inaccurate; (ii) \textit{Sensitivity of neural function approximators}: To capture the complex transition dynamics, offline MBRL typically uses neural networks as parameterized function approximators, but the neural networks are widely known to be highly sensitive to the small variations in the weight parameters \citep{jacot2018neural,tsai2021formalizing, liu2017fault}. 
As a result, it has been widely observed that the learning curves of offline MBRL can be fairly unstable throughout training, as pointed out by \citep{lu2021revisiting}.%The instability of the resulting dynamics model could adversely affect the performance of the policy learned downstream.
\vspace{-1mm}

\noindent\textbf{Limitations of the existing model selection schemes:} The existing offline MBRL methods typically rely on one of the following model selection schemes: 

\vspace{-1mm}
\begin{itemize}[leftmargin=*]
    \item \textit{Validation as in supervised learning}: As the learning of dynamics models could be viewed as an instance of supervised learning, model selection could be done based on a validation loss, \eg mean squared error with respect to a validation dataset \citep{janner2019trust,yu2020mopo}. However, as the validation dataset only covers a small portion of the state and action spaces, the distribution shift issue still exists and thereby leads to poor generalization.
    \vspace{-1mm}
    \item \textit{Off-policy evaluation}: Another commonly-used approach is off-policy evaluation (OPE) \citep{paine2020hyperparameter,voloshin2019empirical}, where given multiple candidate dynamics models, one would first learn the corresponding policy based on each individual model, and then proceed to choose the policy with the highest OPE score. However, it is known that the accuracy of OPE is limited by the coverage of the offline dataset and therefore could still suffer from the distribution shift \citep{tang2021model}. 
\end{itemize}
\vspace{-1mm}
To further illustrate the limitations of the validation-based and OPE-based model selection schemes, we take the model selection problem of MOPO \citep{yu2020mopo}, a popular offline model-based RL algorithm, as a motivating example and empirically evaluate these two model selection schemes on multiple MuJoCo locomotion tasks of the D4RL benchmark datasets \citep{fu2020d4rl}. Please refer to Appendix \ref{app:config} for the experimental configuration. 
Table~\ref{table:motivating} shows that the total rewards achieved by the two model selection schemes can be much worse than that of the best choice (among 150 candidate models obtained via MOPO \citep{yu2020mopo}) and thereby demonstrates the significant sub-optimality of the existing model selection schemes.
%highlights the issue of directly using validation and OPE in model selection for offline model-based RL. 
%\textcolor{orange}{Since the pure offline RL method is afflicted by the distribution shift, perhaps adding a bit of online interaction in offline RL can improve the issue.} 

\vspace{-1mm}
\noindent\textbf{Active model selection:} \todo{Despite the difficulties mentioned above, in various practical applications, a small amount of online data collection is usually allowed to ensure reliable deployment \citep{lee2022offline, zhao2022adaptive, konyushova2021active}, such as robotics (testing of control software on real hardware), recommender systems (the standard A/B testing), and transportation networks (testing of the traffic signal control policies on demonstration sites). In this paper, we propose a new problem setting -- \textit{Active Model Selection}, where the goal is to reliably select models with the help of a small budget of online interactions (\eg $5\%$ of training data). Specifically, we answer the following important and practical research question:}

\vspace{1mm}
\textit{How to achieve effective model selection with only a small budget of online interaction for offline MBRL?}
\vspace{1mm}

\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{figures/revised_return_comp_.png}
    \caption{A comparison of the normalized return achieved by the validation-based and OPE-based model selection methods on multiple MuJoCo locomotion tasks. ``Validation'' indicates the dynamics model with the lowest validation loss among those in the candidate model set. ``OPE" chooses the model with the highest OPE score.}
    \label{fig:return_comp}
\end{figure}
\end{comment}
\begin{table}[!t]
\centering
%\caption{Total rewards achieved by the policies learned from the dynamics models chosen by validation-based and OPE-based model selection schemes on multiple D4RL benchmark tasks. The performance of the best model is also reported to demonstrate the sub-optimality of the existing schemes.}
\caption{Total rewards achieved by MOPO~\citep{yu2020mopo} under validation-based and OPE-based model selection on D4RL. The performance of the best model is also reported to highlight the sub-optimality of the existing schemes.}
%\caption{A comparison of the normalized return achieved by the validation-based and OPE-based model selection methods on multiple MuJoCo locomotion tasks. ``Validation'' indicates the dynamics model with the lowest validation loss among those in the candidate model set. ``OPE" chooses the model with the highest OPE score.}
\scalebox{0.85}{\begin{tabular}{cccc}
\hline
           & hopper & walker2d & halfcheetah \\ 
           & medium & medium & medium \\ \hline
Best model & 2073 {\small$\pm 822$}   & 4002 {\small$\pm 238$}  & 6607 {\small$\pm 295$}       \\
Validation & \phantom{0}821 {\small$\pm 440$}   & \phantom{00}39 {\small$\pm 61\phantom{0}$}     & 5978 {\small$\pm 140$}      \\
OPE        & \phantom{0}495 {\small$\pm 265$}      & 3618 {\small$\pm 241$}    & 4355 {\small$\pm 581$}     \\ \hline
\end{tabular}}
\label{table:motivating}
\end{table}

\vspace{-1mm}
\noindent\textbf{Bayesian optimization for model selection:} In this paper, we propose to address \textit{Active Model Selection} for offline MBRL from a Bayesian optimization (BO) perspective. Our method aims to identify the best among the learned dynamics models with only a small budget of online interaction for downstream policy learning. In BO, the goal is to optimize an expensive-to-evaluate black-box objective function by iteratively sampling a small number of candidate points from the candidate input domain. Inspired by \citep{konyushova2021active}, we propose to recast model selection in offline MBRL as a BO problem (termed BOMS throughout this paper), where the candidates are the dynamics models learned during training and the objective function is the total expected return of the policy that corresponds to each dynamics model. BOMS proceeds iteratively via the following two steps: In each iteration $t$, (i) we first learn a surrogate function with the help of a Gaussian process (GP) and determine the next model to evaluate (denoted by $M_t$) based on the posterior under GP. (ii) We then evaluate the policy obtained under $M_t$ with a small number of online trajectories. 
Figure \ref{fig:BOMS} provides a high-level illustration of the BOMS framework.

\vspace{1mm}
\noindent\textbf{Model-induced kernels for BOMS:} In BOMS, one fundamental challenge is to determine the similarity of dynamics models (analogous to the Euclidean distances between domain points in standard BO). To enable BOMS, we propose a novel 
\textit{model-induced kernel function} that captures the similarity between dynamics models, and the proposed kernel is both computationally efficient and theoretically-grounded by the derived sub-optimality lower bound. 
Somewhat surprisingly, through extensive experiments, we show that significant improvement can be achieved through active model selection of BOMS with only a relatively small number of online interactions (\eg around 20 to 50 trajectories for various locomotion and robot manipulation tasks).

% We utilize Bayesian Optimization to select a dynamics model through the limited online interaction budget
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/BOMS.png}
    \caption{An illustration of the BOMS framework.}
    \label{fig:BOMS}
\end{figure}
%After several selection epochs, the dynamics model with the highest limited budget expected return is our optimal selection result. 
%In our method, we first learn a Gaussian process (GP) that serves as the surrogate function for Bayesian Optimization. Additionally, we define and calculate the distance between different dynamics models to help GP capture the relationship between dynamics models and the expected returns. 
%Based on the GP estimation, we select the next model to have the online policy evaluation and get the expected return with a limited trajectory budget for updating the GP model. After several selection epochs, the dynamics model with the highest limited budget expected return is our optimal selection result. The primary contributions of this work include providing a framework for offline model-based RL with a small budget for online interaction and identifying the distance between dynamics models. 

%The main contributions of this paper can be summarized as follows:
\begin{comment}
\noindent{\textbf{Contributions:}} We summarize our main contributions as follows.
\begin{itemize}[leftmargin=*]
\item We identify the critical problem of model selection in offline MBRL and pinpoint the inherent issues of the existing validation and OPE methods.
\item We introduce the active model selection problem and propose BOMS, which is a general-purpose active model selection framework for improving the performance of any off-the-shelf offline MBRL method with only a small budget of online interaction. Moreover, we propose a theoretically-grounded kernel function that enables computationally-efficient BOMS.
\item Through extensive experiments and an ablation study on multiple Offline RL benchmark tasks, we demonstrate that BOMS could significantly enhance the performance of offline MBRL algorithms with a small amount of online interaction.
\end{itemize}
    
\end{comment}