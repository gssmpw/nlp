\section{Introduction}
%Reinforcement learning (RL) has recently drawn significant attention due to its remarkable ability to train autonomous agents for complex sequential decision-making across a variety of real-world domains, such as robotics \cite{kalashnikov2018scalable, ibarz2021train}, networked systems \cite{chu2020multi, qu2020scalable}, healthcare \cite{yu2021reinforcement, yom2017encouraging}, and recommender systems \cite{afsar2022reinforcement, zhao2021dear}. In conventional online RL, an agent learns by collecting information about the transition dynamics and the reward function through direct online interactions with the environment, either under the learned policy or an exploratory behavior policy. Accordingly, one fundamental issue of online RL is that in many real-world applications, online interactions could be rather impractical as data collection is either costly (e.g., robotics and recommender systems under the risk of damaging machines or losing customers) or even dangerous (e.g., healthcare \cite{nambiar2023deep}). To address this challenge, the formulation of \textit{offline RL} has been adopted to learn a well-performing policy based solely on the pre-collected training datasets \cite{levine2020offline}. This characteristic underpins the great potential of offline RL in real-world applications where online interaction is impractical or unsafe.

Reinforcement learning (RL) has drawn substantial attention for training autonomous agents in complex sequential decision-making across various domains like robotics \cite{kalashnikov2018scalable, ibarz2021train}, networked systems \cite{chu2020multi, qu2020scalable}, healthcare \cite{yu2021reinforcement, yom2017encouraging}, and recommender systems \cite{afsar2022reinforcement, zhao2021dear}. In conventional online RL, agents learn through direct online interactions, posing challenges in real-world scenarios where online data collection is either costly (e.g., robotics and recommender systems under the risk of damaging machines or losing customers) or even dangerous (e.g., healthcare). \textit{Offline RL} addresses this challenge by learning from pre-collected datasets, showcasing its potential in applications where online interaction is infeasible or unsafe.

One fundamental challenge of offline RL is the \textit{distribution shift} issue that results from the mismatch between the state-action distribution of the offline dataset and the distribution that the agent encounters during actual deployment \cite{kumar2019stabilizing,levine2020offline,kumar2020conservative}. 
Accordingly, one important aspect of offline RL is to encode conservatism into the learned policy such that the agent could largely avoid visiting those out-of-distribution (OOD) state-action pairs: (i) In the family of offline model-free methods, such conservatism can be directly implemented based on the offline dataset, by either adding regularization or penalty to the value functions \cite{kumar2020conservative, kostrikov2021offline, bai2022pessimistic} or imposing constraints onto the policy \cite{fujimoto2019off, fujimoto2021minimalist, nair2020awac}. (ii) On the other hand, offline \textit{model-based} methods implement such conservatism with the help of a learned dynamics model, which is learned from the offline dataset and later modified by either uncertainty estimation \cite{yu2020mopo, kidambi2020morel} or adversarial training \cite{rigter2022rambo}. Accordingly, the modified dynamics models could facilitate the policy learning by offering additional synthetic data, which enjoys better data efficiency and potentially better generalization to OOD regions. Moreover, several offline model-based RL approaches \cite{rigter2022rambo,bhardwaj2023adversarial} have been shown to achieve competitive results on the well-known D4RL benchmark dataset \cite{fu2020d4rl}.
%\pch{We need to emphasize the importance of offline model-based RL}
\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/BOMS.png}
    \caption{The illustration of BOMS. We utilize Bayesian Optimization to select a dynamics model through the limited online interaction budget.}
    \label{fig:BOMS}
\end{figure}

Due to the distribution shift, one salient challenge in offline RL is model selection, 
Despite the above progress, one major challenge in offline model-based RL lies in the model selection of the learned dynamics models. Specifically, in the phase of learning the transition dynamics, one needs to determine either a stopping criterion or which model to introduce conservatism to subsequently. Moreover, given that modern model-based RL typically uses neural networks as function approximators for capturing complex transition dynamics, the issue of model selection could be even more challenging as the neural networks are highly sensitive to the small variations of the learned parameters \cite{jacot2018neural}. The instability of the resulting dynamics model could adversely affect the performance of the policy learned downstream.

The existing offline model-based methods typically rely on one of the following two model selection methods: (i) As the learning of dynamics models could be viewed as an instance of supervised learning, model selection could be done based on a validation loss, e.g., mean squared error with respect to a validation dataset \cite{janner2019trust, yu2020mopo}. However, as the validation dataset only covers a small portion of the state and action spaces, the distribution shift issue could also exist and lead to poor generalization.
(ii) Another commonly used approach is to leverage the off-policy evaluation (OPE) techniques \cite{paine2020hyperparameter, voloshin2019empirical}. Specifically, one would first pick multiple candidate dynamics models and learn the corresponding policy based on each individual model and then proceed to choose the policy with the highest OPE score. However, it is known that OPE is limited to the offline dataset and could also suffer from the distribution shift that leads to inaccurate estimation of model performance \cite{tang2021model}. 

As a motivating experiment, Figure~\ref{fig:return_comp} highlights the issue of directly using validation and OPE in model selection for offline model-based RL. \textcolor{orange}{Since the pure offline RL method is afflicted by the distribution shift, perhaps adding a bit of online interaction in offline RL can improve the issue.} In various practical applications, prior to deployment, a small amount of online data collection is allowed to ensure reliable deployment \cite{lee2022offline, zhao2022adaptive, konyushova2021active}, such as robotics (testing of control software on real hardware), recommender systems (the standard A/B testing), and transportation networks (testing of the traffic signal control policies on demonstration sites). 
Therefore, one important research question remains to be answered: \textit{How to improve model selection with only a small budget of online interaction for offline model-based RL?}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/returns_comp.png}
    \caption{The normalized return comparison of different model selection methods. We show the performance discrepancies of three dynamics model selection methods in different tasks. Validation indicates the dynamics model with the lowest validation loss in the model training stage. OPE method chooses the model with the highest OPE value. Optimal denotes the optimal dynamics model with the highest true return.}
    \label{fig:return_comp}
\end{figure}
In this work, we propose to answer the above research question from a Bayesian optimization (BO) perspective. Our method aims to identify the best among the learned dynamics models with only a small budget of online interaction for downstream policy learning. In BO, the goal is to optimize an expensive-to-evaluate black-box objective function by iteratively sampling a small number of candidate points from the candidate input domain. Inspired by \cite{konyushova2021active}, we propose to recast model selection in offline model-based RL as a BO problem (termed BOMS throughout this paper), where the candidates are the dynamics models learned during training, the objective function is the total expected return of the policy that corresponds to each dynamics model. BOMS proceeds iteratively via the following two steps: In each iteration $t$, (i) we first learn a surrogate function with the help of Gaussian process (GP) and determine the next model to evaluate (denoted by $M_t$) based on the posterior under GP. (ii) We then evaluate the policy obtained under $M_t$ with a small number of online trajectories. Moreover, to enable BOMS, we propose a novel kernel function for offline model-based RL, which is computationally efficient and well-motivated by the derived sub-optimality lower bound. In our method, we first learn a Gaussian process (GP) that serves as the surrogate function for Bayesian Optimization. Additionally, we define and calculate the distance between different dynamics models to help GP capture the relationship between dynamics models and the expected returns. Based on GP estimation, we select the next model to have the online policy evaluation and get the expected return with a limited trajectory budget for updating the GP model. After several selection epochs, the dynamics model with the highest limited budget expected return is our optimal selection result. The primary contributions of this work include providing a framework for offline model-based RL with a small budget for online interaction and identifying the distance between dynamics models. We summarize the main contributions of this paper as follows:
\begin{itemize}
\item We identify the critical problem of model selection in offline model-based RL and the inherent issues of simple validation and OPE.
\item We propose BOMS, which is a general-purpose model selection framework for improving the performance of any off-the-shelf offline model-based RL method with a small budget of online interaction. Moreover, we propose a theoretically-grounded kernel function that enables efficient BOMS.
\item Through extensive experiments and an ablation study, we demonstrate that BOMS could significantly enhance the performance of offline model-based RL with a limited amount of online policy evaluation.
\end{itemize}
