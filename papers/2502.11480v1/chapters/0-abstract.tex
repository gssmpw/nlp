\begin{abstract}
Offline model-based reinforcement learning (RL) serves as a competitive framework that can learn well-performing control policies  solely from offline data with the help of learned dynamics models. However, regardinoffline model-based RL conventionally relies on either validation or off-policy evaluation, but overfitting may occur and fail to generalize the unseen states in the environment
%Offline reinforcement learning (RL) is a method that trains agents using pre-collected datasets from the environment. This offline approach enhances the efficiency of dataset utilization and offers a safe and promising way to learn policies for real-world problems. Among various offline RL algorithms, offline model-based RL stands out by generating a convenient and powerful forward dynamics model from the fixed dataset to facilitate policy learning. However, conventional offline model-based RL methods often choose the model with the lowest validation loss to predict the dynamical system transition, but overfitting may occur and fail to generalize the unseen states in the environment. Moreover, the stability of the dynamics model in offline model-based RL is a critical concern since it is quite sensitive and can be easily affected by the quality of the training dataset and the dynamics training process. Obtaining a well-trained dynamics model does not guarantee the complete transitions of the environment. We propose a framework for the dynamics model selection in offline model-based RL to address this issue. Our work BOMS leverages a limited online dataset to measure the distance between models and employs Bayesian Optimization to perform the model selection. Moreover, we introduce a method to estimate the distance between dynamics models to help a Gaussian process capture the relationship between dynamics models and expected returns. Our experimental results show that under a limited online interaction budget, our method produces more accurate dynamics models with improved performance compared to other baselines and ablations.
\end{abstract}

\begin{IEEEkeywords}
Offline model-based reinforcement learning, Bayesian optimization, Dynamics model selection
\end{IEEEkeywords}