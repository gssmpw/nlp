\section{Additional Experimental Results}
\label{app:exp}
\subsection{Comparison of Model Selection Methods}
Figure~\ref{fig:main_appendix} shows the regret performance of different model selection methods, highlighting the effectiveness of BOMS compared to baseline approaches.
\input{chapters/figure_appendix_main}

\subsection{Comparison of Various Designs of Model Distance for BOMS in Inference Regret}
{Figure~\ref{fig:ablations} shows the regret performance of BOMS under various designs of model distance. We can observe that BOMS with  the proposed distance defined in Equation (\ref{eq:distance}) generally achieves the lowest inference regret across all the tasks. This further corroborates the theoretically-grounded design suggested by Proposition \ref{prop:model_dis}.}
%\subsection{Comparison of BOMS and Baseline Methods in Inference Regret}
%\pch{Figure~\ref{fig:baselines} shows the regret performance of BOMS and the baselines. We can observe that BOMS indeed outperforms the baseline methods on almost all the MuJoCo and Adroit tasks.  While OPE and MOPO can achieve low regrets on a few tasks, they are clearly not reliable enough for effective model selection. On the other hand, Random Selection generally improves the regret performance as the selection epoch increases, but its progress appears much slower than BOMS.}
%\input{chapters/fig-regret-main}



\input{chapters/figure-regret-model}
\newpage

\subsection{Normalized Rewards Under BOMS and Other Offline RL Methods on D4RL }
Table~\ref{tab:mopo_rewards} shows the comparison of BOMS and other offline RL methods in terms of normalized rewards. Specifically, we include 4 popular benchmark methods, namely COMBO \citep{yu2021combo}, IQL \citep{kostrikov2021implicitq}, TD3+BC \citep{fujimoto2021minimalist}, and TT \citep{janner2021offline}, for comparison. We follow the default procedure provided by D4RL \citep{fu2020d4rl} to compute the normalized rewards: (i) A
normalized score of 0 corresponds to the average returns of a uniformly random policy; (ii) A normalized score of 100 corresponds to the average returns of a domain-specific expert policy. 
Notably, despite that the vanilla MOPO (with validation-based model selection) itself is not particularly strong, MOPO augmented by the model selection scheme of BOMS can outperform other benchmark offline RL methods on various offline RL tasks. This further showcases the practical values of the proposed BOMS in enhancing offline MBRL.


\renewcommand{\arraystretch}{0.95}
\begin{table*}[!ht]
\caption{Normalized rewards of BOMS and the offline RL benchmark methods. The best performance of each row is highlighted in bold.}
\label{tab:mopo_rewards}
\begin{adjustbox}{center}
\scalebox{0.75}{\begin{tabular}{c|l|c|c|c|c|c|c|c|c|c|c}
    \toprule
        \multicolumn{2}{c|}{\multirow{2}{*}{Tasks}} &
        \multicolumn{4}{c|}{BOMS} &
        \multirow{2}{*}{MOPO} &
        \multirow{2}{*}{OPE} &
        \multirow{2}{*}{COMBO} &
        \multirow{2}{*}{IQL} &
        \multirow{2}{*}{TD3+BC} &
        \multirow{2}{*}{TT} \\
        \cline{3-6}
        \multicolumn{2}{c|}{\multirow{2}{*}{}}&{$T=5$} & {$T=10$} & {$T=15$} & {$T=20$} & & & & & &\\
    \midrule
        & med & 84.98$\pm$0.59\phantom{0} & 85.34$\pm$0.13\phantom{0} & 85.60$\pm$0.00 & \textbf{85.60$\pm$0.00} & -0.02 & 80.72 & 63.76 & 62.64 & 66.96 & 65.04\\
        walker2d & med-r & 20.85$\pm$12.29 & 38.91$\pm$18.58 & \phantom{0}45.59$\pm$17.18 & \textbf{\phantom{0}57.68$\pm$12.19} & 15.87 & 3.96 & 45.89 & 25.56 & 28.29 & 27.46\\
        & med-e & 7.81$\pm$4.52 & 13.13$\pm$12.39 & \phantom{0}24.02$\pm$23.97 & \phantom{0}24.40$\pm$23.64 & 0.07 & 76.66 & \textbf{116.18} & 111.82 & 112.33 & 92.83\\
    \midrule
        & med & 21.32$\pm$3.21\phantom{0} & 23.12$\pm$0.75\phantom{0} & 23.00$\pm$0.84 & 23.17$\pm$0.79 & 10.26 & 6.74 & 88.82 & 89.68 & 80.27 & \textbf{91.15}\\
        hopper & med-r & 96.74$\pm$1.37\phantom{0} & 97.09$\pm$2.68\phantom{0} & 97.05$\pm$2.69 & 97.05$\pm$2.69 & \textbf{99.20} & 42.02 & 70.07 & 38.21 & 24.80 & 40.08\\
        & med-e & 36.37$\pm$12.43 & 42.05$\pm$12.47 & \phantom{0}50.31$\pm$13.13 & \phantom{0}53.07$\pm$12.15 & 44.30 & 24.85 & 98.79 & 85.77 & 91.81 & \textbf{99.26}\\
    \midrule
        & med & 56.27$\pm$0.22\phantom{0} & 56.37$\pm$0.24\phantom{0} & 56.52$\pm$0.19 & \textbf{56.52$\pm$0.19} & 54.55 & 37.39 & 53.7 & 47.66 & 48.52 & 44.4\\
        halfcheetah & med-r & 56.65$\pm$0.86\phantom{0} & 56.78$\pm$0.69\phantom{0} & 56.86$\pm$0.74 & \textbf{56.86$\pm$0.74} & 54.66 & 56.24 & 54.66 & 44.94 & 45.33 & 44.85\\
        & med-e & 103.24$\pm$3.75\phantom{00} & 104.63$\pm$0.56\phantom{00} & \phantom{0}104.87$\pm$0.74 \phantom{0}& \textbf{104.87$\pm$0.74\phantom{0}} & 75.57 & 88.44 & 89.9 & 59.18 & 61.81 & 29.05\\
    \midrule
        & cloned & 5.69$\pm$4.01 & 6.91$\pm$6.33 & \phantom{0}8.95$\pm$6.22 & \textbf{\phantom{0}9.43$\pm$6.86} & 6.89 & 3.70 & - & - & - & -\\
        pen & mixed & 38.98$\pm$8.76\phantom{0} & 42.49$\pm$6.04\phantom{0} & 44.81$\pm$5.55 & \textbf{44.85$\pm$5.55} & 28.66 & 17.45 & - & - & - & -\\
        & expert & 36.54$\pm$7.58\phantom{0} & 40.19$\pm$12.65 & 45.06$\pm$9.59 & \textbf{49.09$\pm$8.88} & 47.58 & 18.67 & - & - & - & -\\
    \bottomrule
\end{tabular}}
\end{adjustbox}
\end{table*}

\newpage

\subsection{Reproduction of MOPO}
Table~\ref{tab:mopo_impl_comp} shows the normalized rewards of our MOPO implementation and those of the original MOPO reported in \citep{yu2020mopo}.
Since our experiments are mainly based on MOPO, we would like to clarify that we have indeed tried our best on tuning MOPO properly, and hence our reproduced MOPO has similar or even better performance than the original MOPO results on halfcheetah and hopper. Additionally, although our MOPO perform on walker2d, after model selection with a small online interaction budget, our MOPO can still enjoy good results and even outperform other SOTA offline RL as shown in Table~\ref{tab:mopo_rewards}.

\renewcommand{\arraystretch}{0.95}
\begin{table*}[!htbp]
\caption{Normalized rewards of our reproduced MOPO and the original MOPO.}
\label{tab:mopo_impl_comp}
\begin{adjustbox}{center}
\scalebox{0.85}{\begin{tabular}{c|l|c|c}
    \toprule
        \multicolumn{2}{c|}{Tasks} &
        {Our MOPO} &
        {Original MOPO} \\
    \midrule
        & med & -0.1 & \textbf{17.8}\\
        walker2d & med-r & 15.9 & \textbf{39.0}\\
        & med-e & 0.1 & \textbf{44.6}\\
    \midrule
        & med & 10.3 & \textbf{28.0}\\
        hopper & med-r & \textbf{99.2} & 67.5\\
        & med-e & \textbf{44.3} & 23.7\\
    \midrule
        & med & \textbf{54.6} & 42.3\\
        halfcheetah & med-r &  \textbf{54.7} & 53.1\\
        & med-e & \textbf{75.6} & 63.3\\
    \midrule
        & cloned & 6.9 & -\\
        pen & mixed & 28.7 & -\\
        & expert & 47.6 & -\\
    \bottomrule
\end{tabular}}
\end{adjustbox}
\end{table*}

\begin{comment}
    \renewcommand{\arraystretch}{0.95}
\begin{table*}[!htbp]
\caption{Normalized rewards of our reproduced MOPO and the original MOPO.}
\label{tab:mopo_impl_comp}
\begin{adjustbox}{center}
\begin{tabular}{cl|c|c}
    \toprule
        \multicolumn{2}{c|}{\multirow{2}{*}{Environments}} &
        {Our} &
        {Original} \\
        & & {MOPO} & {MOPO}\\
    \midrule
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{walker2d}}} 
        & med & -0.02 & \textbf{17.8$\pm$19.3}\\
        & med-r & 15.87 & \textbf{39.0$\pm$9.6}\\
        & med-e & 0.07 & \textbf{44.6$\pm$12.9}\\
    \midrule
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{hopper}}}
        & med & 10.26 & \textbf{28.0$\pm$12.4}\\
        & med-r & \textbf{99.20} & 67.5$\pm$24.7\\
        & med-e & \textbf{44.30} & 23.7$\pm$6.0\\
    \midrule
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{cheetah}}}
        & med & \textbf{54.55} & 42.3$\pm$1.6\\
        & med-r &  \textbf{54.66} & 53.1$\pm$2.0\\
        & med-e & \textbf{75.57} & 63.3$\pm$38.0\\
    \midrule
        \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{pen}}} 
        & cloned & 6.89 & -\\
        & mixed & 28.66 & -\\
        & expert & 47.58 & -\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}
\end{comment}
\begin{comment}
\subsection{Regrets and Success Rates of the Door-Open Task in Meta-World}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/door_open.png}
    \caption{Comparison of BOMS and the baselines on the door-open task. Left: Inference regrets; Right: Success rates.}
    \label{fig:door_open}
\end{figure*}
\end{comment}