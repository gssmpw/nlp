\section{Preliminaries}
\label{sec:prelim}
%\subsection{Markov Decision Processes}
\textbf{Markov decision processes}. We consider a Markov decision processes (MDP) specified by a tuple $M:=(\cS, \cA, r, P, \omega, \gamma)$, where $\cS$ and $\cA$ denote the state space and action space, $r: \cS\times \cA \rightarrow [-r_{\max}, r_{\max}]$ is the reward function, $\omega$ is the initial state distribution, $P(s'\rvert s,a)$ denotes the transition kernel, and $\gamma\in [0,1)$ is the discount factor. Throughout this paper, we use $M^*$ to denote the MDP with the true transition dynamics and true reward function of the environment. We use $\pi$ to denote a stationary policy, which specifies the action distribution at each state. Let $\Pi$ denote the set of all stationary policies. 
For any $\pi \in \Pi$, the discounted state-action visitation distribution $\rho^\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is defined as that for each $(s,a)$,
\begin{equation}
   \rho^\pi(s,a):= (1-\gamma) \pi(a\rvert s) \sum_{t=0}^{\infty} \gamma^t P(s_t=s\rvert \pi).  
\end{equation}
Define the value function of a policy $\pi$ under $M^*$ as
\begin{equation}
V^\pi_{M^*}(s):= \displaystyle\mathop{\mathbb{E}}_{\substack{a_t\sim\pi(\cdot\rvert s_t)\\ s_{t+1}\sim P(\cdot\rvert s_t,a_t)}}\left[\sum_{t=0}^{\infty} \gamma^t r(s_t,a_t) \Big\rvert s_0=s \right].
\end{equation}
The performance of a policy $\pi$ is evaluated by its total expected discounted return, which is defined as
\begin{equation}
J^\pi_{M^*}(\omega):= \displaystyle\mathop{\mathbb{E}}_{s\sim\omega}\left[V^\pi(s)_M^*\right].
\end{equation}
The goal of RL is to learn an optimal policy $\pi^*$ that maximizes the total expected discounted return, \ie
\begin{equation}
  \pi^* \in \arg\max_{\pi\in \Pi} J^\pi_{M^*}(\omega).
\end{equation}


%\subsection{Offline Model-Based RL}
%\label{sec:prelim:offline}
%In standard online RL, the learner searches for $\pi^*$ by directly applying control actions to collect information about the environment dynamics and accrue rewards. 
%By contrast, 
\textbf{Offline model-based RL.} In {offline MBRL} methods, the learner is required to learn a well-performing policy based solely on a fixed dataset denoted by $\cD_{\text{off}}:=\{(s_i,a_i,r_i,s_i')\}_{i=1}^{N}$, which is collected from the environment modeled by $M^*$ under some \textit{unknown} behavior policy $\pi_\beta$, without any online interaction.
For ease of notation, we also let $\hat{M}:=(\cS, \cA, \hat{r}, \hat{P}, \omega, \gamma)$ denote the MDP induced by the estimated dynamics model $\hat{P}$ and the estimated reward function $\hat{r}$.

For didactic purposes, we use the popular MOPO algorithm \citep{yu2020mopo} as an example to describe the common procedure of offline model-based methods. MOPO consists of three main subroutines as follows.

\begin{itemize}[leftmargin=*]
    \item \textit{Learn the dynamics models (along with some model selection scheme)}: MOPO learns the estimated $\hat{P}$ and $\hat{r}$ via supervised learning (\eg minimizing squared error) on the offline dataset $\cD_{\text{off}}$. Regarding model selection, MOPO uses a validation-based method that induces a stopping rule based on the validation loss.
    \item \textit{Construct an uncertainty-penalized MDP}: MOPO then construct a penalized MDP $\widetilde{M}:=(\cS, \cA, \widetilde{r}, \hat{P}, \omega, \gamma)$, where $\widetilde{r}$ is the penalized reward function defined as $\widetilde{r}(s,a):=\hat{r}(s,a)-\lambda u(s,a)$ with $u(s,a)$ being an uncertainty measure and $\lambda$ being the penalty weight. 
    \item \textit{Run an RL algorithm on uncertainty-penalized MDPs}: MOPO then learns a policy by running an off-the-shelf RL algorithm (\eg Soft Actor Critic \citep{haarnoja2018soft}) on the penalized MDP.
\end{itemize}
%As described in Section \ref{sec:intro}, to tackle the distribution shift issue in offline RL, model-based approaches learn the underlying model $\hat{P}$ based on $\cD$ and then encodes pessimism for downstream policy learning, either by uncertainty estimation \citep{yu2020mopo, kidambi2020morel,yu2021combo,rashidinejad2021bridging,sun2023model} or adversarial training \citep{rigter2022rambo,bhardwaj2023adversarial}.
%with regularization \citep{kumar2020conservative, kostrikov2021offline, xu2022constraints} or conservatism \citep{yu2020mopo,bai2022pessimistic, an2021uncertainty, wu2021uncertainty}. 


%\vspace{1mm}
%\noindent\textbf{Model-Based Offline Policy Optimization (MOPO):} 
%For didactic purposes, we use the popular MOPO algorithm \citep{yu2020mopo} as an example to describe the common procedure of offline model-based methods. 


%one major approach is to handle the OOD issue from the model-based perspective, which learns the underlying dynamics model $\hat{P}$ based on $\cD$ to obtain uncertainty estimates for downstream policy learning, possibly with regularization \citep{kumar2020conservative, kostrikov2021offline, xu2022constraints} or conservatism \citep{bai2022pessimistic, an2021uncertainty, wu2021uncertainty}. For ease of notation, we let $\hat{M}:=(\cS, \cA, r, \hat{P}, \omega, \gamma)$ denote the MDP induced by the estimated dynamics model $\hat{P}$.
%one fundamental challenge in offline RL is the out-of-distribution issue, where the learner fails to make a proper decision in those state-action pairs not present in the offline dataset. 
%To tackle the challenge of offline RL, 



%\subsection{Bayesian Optimization}
%\label{sec:prelim:bo}
\textbf{Bayesian optimization.} 
As a popular framework of black-box optimization, BO finds a maximum point ${x^*}$ of an unknown and expensive-to-evaluate objective function $f(\cdot): \mathbb{R}^d \rightarrow \mathbb{R}$ through iterative sampling by incorporating two critical components:
\vspace{-1mm}
%\citep{snoek2012practical, shahriari2015taking, snoek2015scalable} 
%Black-box optimization is one classic problem formulation that involves finding a minimum or maximum point ${x^*}$ of an unknown and expensive-to-evaluate objective function $f(\cdot): \mathbb{R}^d \rightarrow \mathbb{R}$ through iterative sampling, without the knowledge about the gradient or higher-order derivatives \citep{snoek2012practical, shahriari2015taking, snoek2015scalable}. As a popular framework of black-box optimization, Bayesian optimization (BO) serves as a sample-efficient method by incorporating two critical components:
\begin{itemize}[leftmargin=*]
\item \textit{Gaussian process (GP)}: To facilitate the inference of unknown black-box functions, BO leverages a GP function prior that characterizes the correlation among the function values and thereby implicitly captures the underlying smoothness of the functions. Specifically, a GP prior can be fully characterized by its mean function $m(\cdot): \mathcal{X}\rightarrow \mathbb{R}$ and the kernel function $k(\cdot,\cdot): \mathcal{X}\times \mathcal{X} \rightarrow \mathbb{R}$ that determines the covariance between the function values at any pair of points in the domain $\mathcal{X}$ \citep{schulz2018tutorial}. As a result, in each iteration $t$, given the first $t$ samples $\mathcal{H}_t:=\{(x_i,y_i)\}_{i=1}^{t}$, the posterior distribution of each $x$ in the domain remains a Gaussian distribution denoted as $\mathcal{N}(\mu_t(x),\sigma^2_t(x))$, where $\mu_t(x):=\mathbb{E}[f(x)\rvert \mathcal{H}_t]$ and $\sigma_t(x):=\sqrt{\mathbb{V}[f(x)\rvert \mathcal{H}_t]}$ could be derived exactly.
Notably, this is one major benefit of using a GP prior in black-box optimization.
\vspace{-1mm}
\item \textit{Acquisition functions}: In general, the sampling process of BO could be viewed as solving a sequential decision making problem. However, applying any standard dynamic-programming-like method to determine the sampling could be impractical due to the curse of dimensionality \citep{wang2016bayesian}. To enable efficient sampling, BO typically employs an acquisition function (AF) denoted by $\Psi(x; \cF_t)$, which provides a ranking of the candidate points $x\in \cX$ in a myopic manner based on the posterior means and variances given the history $\cF_t$. As a result, an AF can induce a tractable index-type sampling policy. Existing AFs have been developed from various perspectives, such as the improvement-based methods (\eg Expected Improvement \citep{jones1998efficient}), optimism in the face of uncertainty (\eg, GP-UCB \citep{srinivas2009gaussian}), information-theoretic approaches (\eg MES \citep{wang2017max}), and the learning-based methods (\eg MetaBO \citep{volpp2019meta} and FSAF \citep{hsieh2021reinforced}).
\end{itemize}
