\section{Related Work}

%applying penalties to the value function when evaluated on state-action pairs outside of the fixed dataset distribution. It takes into account both the pre-collected dataset and the rollout dataset yielded from the dynamics model to determine the penalty.
%Additionally, there are two classic conservative model-based methods, MOPO \citep{yu2020mopo} and MOReL \citep{kidambi2020morel}. These two methods construct pessimistic MDPs and give the penalty to OOD state-action pairs based on different uncertainty estimations. 
%MOPO chooses the standard deviation of the transition distribution as the soft reward penalty, which allows the learned policy to have more flexibility in choosing actions. On the other hand, MOReL introduces the Halt state in MDP and gives a fixed penalty when the total variation distance between the learned transition and the true transition exceeds a certain threshold. By taking the conservative approaches, these model-based works prevent the learned policy from venturing into the states that are different from the training dataset.

%\subsection{Online Interaction Integration for Offline RL}
%\subsection{Improving Offline RL by Online Interactions}
%To address this, several recent attempts \citep{konyushova2021active,zhao2022adaptive, kostrikov2021implicitq, niu2022trust, nakamoto2023cal} have proposed to incorporate policies learned from offline RL with a small budget of online interactions:

One notable challenge of pure offline RL is that the absence of online interaction with the environment can severely limit the performance due to limited coverage of the fixed dataset. 
%Moreover, when deploying the learned policy in the actual environment, valuable real-world information may be overlooked and miss opportunities for algorithm improvement.
%, enabling more effective fine-tuning processes.
Indeed, the need for a small budget of online interactions has been studied and justified in the offline RL literature:
%To address this, two main categories of methods have been proposed to incorporate policies learned from offline RL with a small budget of online interactions:

\textbf{Offline RL with policy selection}: Among these works, \citep{konyushova2021active} is the most relevant to ours in terms of problem formulation. Specifically, given a collection of candidate policies learned by any offline RL algorithm, \citep{konyushova2021active} proposes a setting called active offline policy selection (AOPS), which applies BO to actively choose a well-performing policy in the candidate policy set. Despite this high-level resemblance, AOPS does \textit{not} address the selection of dynamics models and is not readily applicable in active model selection. More specifically, the direct application of AOPS to our problem (\ie active model selection for offline MBRL) would \textit{require training policies for all the candidate dynamics models} at the first place, and this unavoidably leads to enormous computational overhead. Therefore, we view \citep{konyushova2021active} as an orthogonal direction to ours.

\textbf{Offline-to-online RL}: As a promising research direction, Offline-to-online RL (O2O) is meant to improve the policies learned by offline RL through fine-tuning on a small amount of online interaction.
To begin with, \citep{lee2022offline} introduces an innovative O2O approach that involves fine-tuning ensemble agents by combining offline and online transitions with a balanced replay scheme.
In a similar vein, \citep{agarwal2022reincarnating} presents Reincarnating RL (RRL), which aims to mitigate the data inefficiency of deep RL. Traditional RL algorithms typically start from scratch without leveraging any prior knowledge, resulting in computational and sample inefficiencies in practice. RRL reuses existing logged data or learned policies as an initialization for further real-world training to avoid redundant computations and improve the scalability.
More recently, the O2O problem has been studied from various perspectives, such as data mixing \citep{zheng2023adaptive,ball2023efficient}, policy fine-tuning \citep{xie2021policy,uchendu2023jump,zhang2023policy}, value-based fine-tuning \citep{zhang2024perspective}, and reward-based fine-tuning \citep{nair2023learning}. In contrast to the above works, BOMS utilizes online interactions to enhance the model selection for offline MBRL.

\pch{Moreover, due to the page limit, a review of the offline MBRL methods is provided in Appendix~\ref{app:related}.}
%. They establish a balanced replay scheme by storing the online and offline samples in the prioritized buffer and assigning these samples different priorities based on density online-offline density ratios. Then, they train a pessimistic ensemble Q-function to mitigate the state-action distribution shift. 
%In a similar vein, \citep{konyushova2021active} proposes a method that utilizes OPE estimation and limited online interactions to evaluate policies for policy selection using Bayesian Optimization. The presence of the distribution shift can lead to inaccurate off-policy evaluation (OPE), resulting in the selection of suboptimal policies in offline RL. By incorporating additional information from the environment, this method helps identify the optimal policy in offline RL. 
%In a similar vein, Another offline-to-online work is Reincarnating RL (RRL) \citep{agarwal2022reincarnating}, which aims to mitigate the inefficiency of deep RL. Traditional RL algorithms typically start from scratch without leveraging any prior knowledge, resulting in computational and sample inefficiencies in practice. RRL reuses existing logged data or learned policies as a starting point for further real-world training to avoid redundant computations and improve the scalability of complex real-world problems.

%By pre-training the agent or model offline, reasonable policies can be tested prior to deployment and ensure more reliable and robust performance. The offline-to-online approach not only strengthens the existing policies but also mitigates the risks associated with online interaction.