\section{Introduction}
%Introduce the following topics:
%\begin{itemize}
 %   \item Low rank approximation using inexact matvecs. Used in Hutch++. 
  %  \item Smarter methods to compute low rank approximations if matvecs are approximated using Krylov subspace methods. 
   % \item We analyze two things:
    %\begin{enumerate}
     %   \item Low rank approximation and Hutch++ with inexact matvecs;
      %  \item Krylov aware low rank approximation. Don't throw away information in
       % \begin{equation*}
            %\mathcal{K}_{q}(\bm{A},\bm{\Omega}) = \range\left(\begin{bmatrix} \bm{\Omega}& \bm{A}\bm{\Omega} & \cdots & \bm{A}^{q-1} \bm{\Omega} \end{bmatrix}\right).
        %\end{equation*}
    %\end{enumerate}
%\end{itemize}

Computing and approximating matrix functions is an important task in many application areas, such as differential equations~\cite{havel1994matrix,highamscaleandsquare}, network analysis~\cite{estrada2000characterization,estradahigham}, and machine learning \cite{DongErikssonNickisch:2017,PleissJankowiakEriksson:2020}. For a symmetric matrix $\bm{A} \in \mathbb{R}^{n \times n}$ with eigenvalue decomposition 
\begin{equation*}
    \bm{A} = \bm{U} \bm{\Lambda} \bm{U}^\T, \quad \bm{\Lambda} = \diag(\lambda_1,\ldots,\lambda_n),
\end{equation*}
the matrix function is defined as 
\begin{equation*}
    f(\bm{A}) = \bm{U} f(\bm{\Lambda}) \bm{U}^\T, \quad f(\bm{\Lambda}) = \diag(f(\lambda_1),\ldots,f(\lambda_n)),
\end{equation*}
where $f$ is a function defined on the eigenvalues of $\bm{A}$. Explicitly forming $f(\bm{A})$ generally costs $O(n^3)$ operations, which becomes prohibitively expensive for large $n$. 

However, instead of computing $f(\bm{A})$ exactly, in many cases we only need an approximation to $f(\bm{A})$, such as a low-rank approximation. For example, recent work has shown that low-rank approximations can be used to accurately estimate the trace or diagonal entries of $f(\bm{A})$ \cite{baston2022stochastic,epperly2023xtrace,jiang2021optimal,hpp, ahpp}. Furthermore, when $f(\bm{A})$ admits an accurate low-rank approximation $\bm{B}$, one can accurately and cheaply approximate matrix-vector products (matvecs) with $f(\bm{A})$ using the approximation
\begin{equation*}
    \bm{B} \bm{Z} \approx f(\bm{A}) \bm{Z}.
\end{equation*}

Popular algorithms to compute cheap, yet accurate, low-rank approximations of matrices include the randomized SVD, randomized subspace iteration, and randomized block Krylov iteration. For an overview and analysis of these algorithms, we turn readers to \cite{rsvd,MM15,tropp2023randomized}. These methods can be referred to as \textit{matrix-free}, since they do not require explicit access to the matrix; they only require implicit access via matvecs. Furthermore, the dominant computational cost of these algorithms is usually the number of matvecs with the matrix that we want to approximate. 

When the matrix we wish to approximate is a matrix-function, $f(\bm{A})$, we do not have access to exact matvecs. However, we can typically compute approximate matvecs through some black-box method, such as the (block) Lanczos method, or another Krylov subspace method \cite{blocklanczos,guttel,functionsofmatrices,lanczosfunctionshandbook}. 
These methods are \emph{themselves} matrix-free, only requiring matvecs with $\bm{A}$ to approximation matvecs with $f(\bm{A})$. 

To summarize, the standard approach for efficiently computing a low-rank approximation to a matrix function, $f(\bm{A})$, combines two black-box methods: an outer-loop like the randomized SVD that requires matvecs with $f(\bm{A})$, and an inner-loop that computes each of these matvecs using multiple matvecs with $\bm{A}$ 
%
A broad theme of this paper and related work is that computational benefits can often be obtained by \emph{opening up} these black-boxes. I.e., we can get away with \emph{fewer} total matvecs with $\bm{A}$ to obtain a low-rank approximation to $f(\bm{A})$ if we shed the simple inner/outerloop approach.
Such an approach was taken in \cite{chen_hallman_23} and \cite{persson_kressner_23,funnystrom2}, where it is shown that algorithms like the randomized SVD and Nystr\"om approximation for low-rank approximation, and  Hutch++ for trace estimation, can benefit from such introspection. 

% \sout{Algorithms such as the randomized SVD, randomized subspace iteration, and randomized block Krylov iteration are widely used in practice and enjoy strong theoretical gurantees. For an overview and analysis of these algorithms, we turn readers to \cite{rsvd, MM15,tropp2023randomized}. 
% These methods can be referred to as \textit{matrix free}, since they do not require explicit access to the matrix; they only require implicit access via matrix-vector products (matvecs). Furthermore, the dominant computational cost of these algorithms is the number of matvecs with the matrix that we want to approximate.

% However, when the matrix at hand is a matrix-function we do not have access to exact matrix-vector products, only approximate matrix-vector products 
% For an overview and analysis of these algorithms, we turn readers to \cite{rsvd, MM15,tropp2023randomized}. 
% These methods can be referred to as \textit{matrix free}, since they do not require explicit access to the matrix; they only require implicit access via matrix-vector products (matvecs). Furthermore, the dominant computational cost of these algorithms is the number of matvecs with the matrix that we want to approximate.
% \David{Check if this is okay? Here...}
% In many cases we cannot compute exact matvecs. One such example is when the matrix at hand is a matrix function. For a symmetric matrix $\bm{A} \in \mathbb{R}^{n \times n}$ with eigenvalue decomposition }
% \begin{equation*}
%     \bm{A} = \bm{U} \bm{\Lambda} \bm{U}^\T, \quad \bm{\Lambda} = \diag(\lambda_1,\ldots,\lambda_n),
% \end{equation*}
% \sout{the matrix function is defined as} 
% \begin{equation*}
%     f(\bm{A}) = \bm{U} f(\bm{\Lambda}) \bm{U}^\T, \quad f(\bm{\Lambda}) = \diag(f(\lambda_1),\ldots,f(\lambda_n)),
% \end{equation*}
% \sout{where $f$ is a function defined on the eigenvalues of $\bm{A}$. Computing low-rank approximations of matrix functions is important in many applications. For example, recent work has shown that low-rank approximations can be used to accurately estimate the trace or diagonal entries of $f(\bm{A})$ \cite{baston2022stochastic,jiang2021optimal,hpp, ahpp}. Furthermore, when $f(\bm{A})$ admits an accurate low-rank approximation $\bm{B}$, one can accurately and cheaply approximate matvecs with $f(\bm{A})$ using the approximation}
% \begin{equation*}
%     \bm{B} \bm{Z} \approx f(\bm{A}) \bm{Z}.
% \end{equation*}
% \sout{\textit{Exactly} computing matvecs with $f(\bm{A})$ generally costs $O(n^3)$ operations, since we first would have to explicitly form the matrix function. 
% Hence, we need to resort to approximate methods. 
% {\color{blue}What does exact mean, because even eigensolvers are iterative. so then one could Lanczos is as exact for a single matvec.} \David{Up to $\epsilon_{mach}$. I am assuming that eigensolvers, explicit methods to compute matrix functions, etc are exact. In our analysis we never take into account that the computation of $f(\bm{T}_q)$ is not exact anyway. }
% When matrix-free methods are applied to a matrix function $f(\bm{A})$ it is common to assume that matvecs with $f(\bm{A})$ are implemented using some black-box method; e.g. with a Krylov subspace method (KSM) like the (block) Lanczos method \cite{blocklanczos,guttel,functionsofmatrices}, which may require many matvecs with $\bm{A}$. Hence, when applying, for example, the randomized SVD to matrix functions the measure of computational cost should not be the number of matvecs with $f(\bm{A})$, but rather the number of matvecs with $\bm{A}$. \David{... until here} }


Concretely, our work builds on a recent paper of
Chen and Hallman \cite{chen_hallman_23}, which develops so-called ``Krylov-aware'' variant of the Hutch++ and A-Hutch++ algorithms for trace estimation \cite{hpp,ahpp} .
The Hutch++ algorithm uses the randomized SVD to compute a low-rank approximation, which in turn is used reduce the variance of a stochastic estimator for the trace of $f(\bm{A})$ \cite{cortinoviskressner,ubarusaad,ubaru2017applications}. 
Chen and Hallman develop a more efficient method to compute this low-rank approximation for matrix functions.
% that would typically be applied with a black-box Krylov method.
Numerical experiments suggest that their method is significantly more efficient than naively implementing the randomized SVD on $f(\bm{A})$ with approximate matvecs obtained in a black-box way from a Krylov subspace method. 
% However, a crisp theoretical analysis is missing. \Chris{revist: can we be more concrete about what is missing? "crisp" feels vague to me}
Our main contribution is to strengthen our theoretical understanding of the method from \cite{chen_hallman_23} by providing strong and intuitive error bounds that justify its excellent empirical performance. %\David{I would remove this last part. We don't quantify how much better it is. } \sout{We show that this method is better than a naive implementation of the randomized SVD applied to $f(\bm{A})$, and we provide a bound that quantifies how much better it is.}

\subsection{Notation}
%\Chris{ADd definition of $\|\bm{X}\|_2$ and possible $\|\bm{X}\|_F$}
Let $f: \mathbb{R} \mapsto \mathbb{R}$ be a function and let $\bm{A}$ be a symmetric matrix with eigendecomposition
\begin{align}\label{eq:A}
    \begin{split}
    \bm{A} &= \bm{U} \bm{\Lambda} \bm{U}^\T = \begin{bmatrix} \bm{U}_k & \bm{U}_{n \setminus k} \end{bmatrix} \begin{bmatrix} \bm{\Lambda}_k & \\ & \bm{\Lambda}_{n \setminus k} \end{bmatrix} \begin{bmatrix} \bm{U}_k^\T \\ \bm{U}_{n\setminus k}^\T \end{bmatrix}, \\
     \bm{\Lambda}_k &= \diag(\lambda_1,\lambda_2,\cdots,\lambda_k),\\
     \bm{\Lambda}_{n \setminus k} &= \diag(\lambda_{k+1},\lambda_{k+2},\cdots,\lambda_n),
     \end{split}
\end{align}
where the eigenvalues are ordered so that 
\begin{equation*}
|f(\lambda_1)| \geq |f(\lambda_2)| \geq \ldots \geq |f(\lambda_n)|.
\end{equation*}
For a matrix $\bm{B}$ we denote by $\|\bm{B}\|_\F$ and $\|\bm{B}\|_2$ the \emph{Frobenius norm} and \emph{spectral norm}, respectively. 
For a sketching matrix $\bm{\Omega} \in \mathbb{R}^{n \times \ell}$, where $\ell \geq k$, we define
\begin{equation}\label{eq:omega_partition}
    \bm{\Omega}_k := \bm{U}_k^\T \bm{\Omega}, \quad \bm{\Omega}_{n\setminus k} := \bm{U}_{n\setminus k}^\T \bm{\Omega}
\end{equation}
For any matrix $\bm{B} \in \mathbb{R}^{m \times n}$, the submatrix consisting of rows $r_1$ through $r_2$ and columns $c_1$ through $c_2$ is denoted by $\bm{B}_{r_1:r_2,c_1:c_2}$. We write $\bm{B}_{:,c_1:c_2}$ and $\bm{B}_{r_1:r_2,:}$ to denote $\bm{B}_{1:m,c_1:c_2}$ and $\bm{B}_{r_1:r_2,1:n}$ respectively. We write $\llbracket \bm{B} \rrbracket_k$ to denote the best rank $k$ approximation of $\bm{B}$ obtained by truncating the SVD. By $\mathbb{P}_s$ we denote the set of all polynomials whose degree is at most $s$. %We write $\bm{E}_i = \bm{e}_i \otimes \bm{I}$, where $\bm{e}_i$ is the $i^{\text{th}}$ standard basis vector, and the sizes of $\bm{e}_i$ and $\bm{I}$ can be inferred from the context. 
The Krylov subspace $\mathcal{K}_s(\bm{A},\bm{\Omega})$ is defined as
\begin{equation*}
    \mathcal{K}_{s}(\bm{A},\bm{\Omega}) := \range\left(\begin{bmatrix} \bm{\Omega} & \bm{A} \bm{\Omega} & \cdots & \bm{A}^{s-1} \bm{\Omega} \end{bmatrix}\right).
\end{equation*}
%\Chris{Should $d_s$ also have $\ell$ in the subscript?} \David{Hmm, maybe. I will go through the paper and change it}
We denote $d_{s,\ell} := \dim(\mathcal{K}_s(\bm{A},\bm{\Omega}))$. For a function $f$ defined on a set $I$ we write
\begin{equation*}
    \|f\|_{L^{\infty}(I)} := \sup\limits_{x \in I}|f(x)|.
\end{equation*}