\section{Krylov-aware low-rank approximation}

We now describe and motivate the algorithm that we will analyse. Inspired by \cite{chen_hallman_23}, we will call it \textit{Krylov-aware low-rank approximation}. 
%We now motivate and describe the Krylov-aware low-rank approximation algorithm studied in this paper. 
We begin with outlining the block-Lanczos algorithm to approximate matvecs with matrix functions. Next, we present how one would naively implement the randomized SVD on $f(\bm{A})$ using the block-Lanczos method to approximate matvecs. Finally, we present the alternative Krylov-aware algorithm and why this method allows us to gain efficiencies. 

\subsection{The block-Lanczos algorithm}\label{section:block_lanczos}

Given an $n\times \ell$ matrix $\bm{\Omega}$, the block-Lanczos algorithm (\cref{alg:block_lanczos}) can be used to iteratively obtain an orthonormal (graded) basis for $\mathcal{K}_{s}(\bm{A},\bm{\Omega})$. 
In particular, using $s$ block-matrix vector products with $\bm{A}$, the algorithm produces a basis $\bm{Q}_s$ and a block-tridiagonal matrix $\bm{T}_s$ 
\begin{equation}\label{eqn:QTdef}
\bm{Q}_s = 
    \begin{bmatrix} \bm{V}_0 & \cdots & \bm{V}_{s-1} \end{bmatrix}
,\quad
    \bm{T}_s = \bm{Q}_s^\T \bm{A}\bm{Q}_s = 
    \operatorname{tridiag}
    \left(\hspace{-.75em} \begin{array}{c}
        \begin{array}{cccc} \bm{R}_1^\T  & \cdots & \bm{R}_{s-1}^\T \end{array} \\
            \begin{array}{ccccc} \bm{M}_1 & \cdots & \cdots & \bm{M}_{s} \end{array} \\
        \begin{array}{cccc} \bm{R}_1 & \cdots & \bm{R}_{s-1}  \end{array} 
    \end{array} \hspace{-.75em}\right),
    % \begin{bmatrix}
    %     \bm{M}_1 & \bm{R}_1^\T \\
    %     \bm{R}_1 & \ddots & \ddots \\
    %     & \ddots & \ddots & \bm{R}_{q-1}^\T \\
    %     & & \bm{R}_{q-1} & \bm{M}_{q} 
    % \end{bmatrix}
\end{equation}
%\David{I removed the three-term recurrence. We never use it} %These are related by the block-three-term recurrence\begin{equation}\label{eqn:block_krylov}
    %\bm{A} \bm{Q}_s = \bm{Q}_s \bm{T}_s + \bm{V}_s \bm{R}_s \bm{E}_s^\T,
%\end{equation}
where $\bm{R}_0$ is also output by the algorithm and is given by the relation $\bm{\Omega} = \bm{V}_0 \bm{R}_0$. 
\begin{remark}
The block sizes of $\bm{Q}_s$ and $\bm{T}_s$ will never be larger than $\ell$, but may be smaller if the Krylov subspaces $\mathcal{K}_{i}(\bm{A},\bm{\Omega})$ obtained along the way have dimension less than $i\ell$. 
In particular, if $\mathcal{K}_{i}(\bm{A},\bm{\Omega}) = \mathcal{K}_{i-1}(\bm{A},\bm{\Omega})$, then the blocks $\bm{V}_j, \bm{R}_{j-1}, \bm{M}_{j}$ will be empty for all $j \geq i-1$.
The algorithm can terminate at this point, but for analysis it will be useful to imagine that the algorithm is continued and zero-width blocks are produced until it terminates at iteration $s$.
\end{remark}

\begin{algorithm}
\caption{Block-Lanczos Algorithm}
\label{alg:block_lanczos}
\textbf{input:} Symmetric $\bm{A} \in \mathbb{R}^{n \times n}$. Matrix $\bm{\Omega} \in \mathbb{R}^{n \times \ell}$. Number of iterations $s$.\\
\textbf{output:} Orthonormal basis $\bm{Q}_s$ for $\mathcal{K}_{s}(\bm{A}, \bm{\Omega})$,
%\begin{bmatrix} \bm{Q}_1 & \bm{Q}_2 \end{bmatrix}$ for $\mathcal{K}_{q}(\bm{A}, \bm{Z})$, where $\bm{Q}_1$ is orthonormal basis for $\mathcal{K}_{s}(\bm{A}, \bm{Z})$, 
and block tridiagonal $\bm{T}_s$.
\begin{algorithmic}[1]
    \State Compute an orthonormal basis $\bm{V}_0$ for $\range(\bm{\Omega})$ and $\bm{R}_0 = \bm{V}_0^T \bm{\Omega}$.
    \For{$i=1,\ldots, s$}
    \State $\bm{Y} = \bm{A} \bm{V}_{i-1} - \bm{V}_{i-2} \bm{R}_{i-1}^\T$ \Comment{$\bm{Y} = \bm{A} \bm{V}_{i-1}$ if $i=1$}
    \State $\bm{M}_i = \bm{V}_{i-1}^\T \bm{Y}$ 
    % \Chris{Is there a reason to have steps 4 and 5 seperate?} \David{We need to define $\bm{M}_i$ to define $\bm{T}_s$}
    \State $\bm{Y} = \bm{Y} - \bm{V}_{i-1}\bm{M}_i$
    \State $\bm{Y} = \bm{Y} - \sum_{j=0}^{i-1} \bm{V}_j\bm{V}_j^\T \bm{Y}$ \Comment{reorthogonalize (repeat as needed)}
    \State Compute an orthonormal basis $\bm{V}_i$ for $\range(\bm{Y})$ and $\bm{R}_i = \bm{V}_i^T \bm{Y}$. \label{alg:line:qr}
    \EndFor
    \State \Return $\bm{Q}_s$ and $\bm{T}_s$ as in \cref{eqn:QTdef}
\end{algorithmic}
\end{algorithm}
% \Chris{Should we leave reothogonalization out of Algorithm 1? Is it necessary? Or maybe say "optionaly reothogonalize" then add a comment that doing so is only necessary in finite precision?} \David{Yes, will do}

% block-three term recurrence:
% \begin{equation}
%     \bm{A} \overline{\bm{Q}}_i 
%     = \overline{\bm{Q}}_{i-1} \bm{R}_{i}^\T + \overline{\bm{Q}}_i \bm{M}_i + \overline{\bm{Q}}_{i+1} \bm{R}_{i+1}
% \end{equation}

The block Lanczos algorithm can be used to approximate matvecs and quadratic forms with $f(\bm{A})$ using the approximations
\begin{align}\label{eq:lanczos_approximation}
    \bm{Q}_s f(\bm{T}_s)_{:,1:\ell}\bm{R}_0 &\approx f(\bm{A}) \bm{\Omega},
    \\
    \label{eq:lanczos_approximation_qf}
    \bm{R}_0^\T f(\bm{T}_s)_{1:\ell,1:\ell}\bm{R}_0 &\approx \bm{\Omega}^\T f(\bm{A}) \bm{\Omega}. 
\end{align}
If $f$ is a low-degree polynomial, then the approximations \cref{eq:lanczos_approximation,eq:lanczos_approximation_qf} are exact.\footnote{\eqref{eq:lanczos_approximation} and \eqref{eq:lanczos_approximation_qf} are written out under the assumption that $\bm{\Omega}$ has rank $\ell$. If $\rank(\bm{\Omega}) = r <\ell$, then the index set $1:\ell$ should be replaced with $1:r$ in both \eqref{eq:lanczos_approximation} and \eqref{eq:lanczos_approximation_qf}.} 
% \footnote{In fact, a stronger result for multipolynomials holds, see \cite[Theorem 2.7]{frommer_lund_szyld_20}. \Chris{I looked at this paper and didn't see the term ``multipolynomial'' used. I haven't heard it before. Can we remove or just say something more vague?} \David{Perhaps change the word "multipolynomial" to "matrix polynomial", which they use in \cite{frommer_lund_szyld_20}}}
%
\begin{lemma}[{\cite[Lemma 2.1]{chen_hallman_23}}]\label{lemma:block_lanczos_exact}
The approximation \cref{eq:lanczos_approximation} is exact if $f\in\mathbb{P}_{s-1}$, and the approximation \cref{eq:lanczos_approximation_qf} is exact if $f\in \mathbb{P}_{2s-1}$.
\end{lemma}
% \begin{proof}
%     To prove the first result, it suffices to consider $f(x) = x^{j}$ for $j\leq s-1$.
%     Note that $\range(\bm{A}^j \bm{\Omega})\subset \range(\bm{Q}_s)$.
%     Thus, using \eqref{eqn:block_krylov} and $\bm{Q}_s^\T \bm{A} \bm{Q}_s = \bm{T}_s$ we have
%     \[
%     \bm{A}^{j}\bm{\Omega}
%     = \bm{Q}_s\bm{Q}_s^\T \bm{A}^j \bm{\Omega}
%     = \bm{Q}_s\bm{Q}_s^\T \bm{A} \bm{Q}_s \cdots \bm{Q}_s^\T \bm{A}\bm{Q}_s\bm{Q}_s^\T \bm{\Omega}
%     = \bm{Q}_s (\bm{T}_s^j)_{:,\ell} \bm{R}_0.
%     \]
%     The second result follows from the first and the fact that $\bm{Q}_s^\T \bm{A} \bm{Q}_s = \bm{T}_s$. 
% \end{proof}
It follows that \cref{eq:lanczos_approximation,eq:lanczos_approximation_qf} are good approximations if $f$ is well approximated by polynomials.\footnote{We note that for block-size $\ell>1$, the Krylov subspace is not equivalent to $\cup\{\range(p(\bm{A}) \bm{\Omega}) : p \in \mathbb{P}_{s-1}\}$, and bounds based on best approximation may be pessimistic due to this fact.
In fact, deriving stronger bounds is an active area of research; see e.g. \cite{chen_greenbaum_musco_musco,frommer_schweitzer_guttel,frommer_lund_szyld_20,frommer_schweitzer,frommer_simoncini, hochbruck_lubich}. 
% However, in this work we will stick with this simple and well-known bound.
}
In particular, one can obtain bounds in terms of the best polynomial approximation to $f$ on $[\lambda_{\min},\lambda_{\max}]$ \cite{Saad:1992,OrecchiaSachdevaVishnoi:2012,Amsel:2024}.


%\tyler{There used to be a lemma here, but it didn't seem like we used it anywhere but instead use \cref{lemma:2_times_polynomial_approx}.}
% This can be formalized as the following (well-known) lemma. 
% \begin{lemma}\label{thm:lanczos_exact}
% Let $\bm{Q}_s$ and $\bm{T}_s$ be as in \Cref{alg:block_lanczos} and let $\lambda_{\min}$ and $\lambda_{\max}$ be the smallest and largest eigenvalue of $\bm{A}$ respectively. Then, 
% \begin{equation}\label{eqn:pAv_exact}
%     \|f(\bm{A}) \bm{\Omega} - \bm{Q}_s f(\bm{T}_s)_{:,1:\ell}\bm{R}_0\|_2 \leq 2 \|\bm{\Omega}\|_2 \min\limits_{p \in \mathbb{P}_{s-1}}\|f-p\|_{L^{\infty}([\lambda_{\min},\lambda_{\max}])}
% \end{equation}
% and
% \begin{equation}\label{eqn:vpAv_exact}
%     \|\bm{\Omega}^\T f(\bm{A}) \bm{\Omega} - \bm{R}_0^\T f(\bm{T}_s)_{1:\ell,1:\ell}\bm{R}_0\|_2 \leq 2 \|\bm{\Omega}\|_2^2 \min\limits_{p \in \mathbb{P}_{2s-1}}\|f-p\|_{L^{\infty}([\lambda_{\min},\lambda_{\max}])}.
% \end{equation}
% Moreover, if $\bm{V}_s$ is empty (i.e. because the Krylov subspace has become invariant under $\bm{A}$), then in fact \cref{eqn:pAv_exact,eqn:vpAv_exact} hold for polynomials of arbitrary degree.
% \end{lemma}
% In particular, one can obtain bounds in terms of the best polynomial approximation to $f$ on $[\lambda_{\min},\lambda_{\max}]$.
% For block-size $\ell>1$, the Krylov subspace is not equivalent to $\cup\{\range(p(\bm{A}) \bm{\Omega}) : p \in \mathbb{P}_{s-1}\}$, and bounds based on best approxiamtion may be pessimistic due to this fact.
% In fact, deriving stronger bounds is an active area of research; see e.g. \cite{chen_greenbaum_musco_musco,frommer_schweitzer_guttel,frommer_lund_szyld_20,frommer_schweitzer,frommer_simoncini, hochbruck_lubich}. However, in this work we will stick with this simple and well known bound. 



% The key observation which allows the algorithms in this paper to be implemented efficiently is that the Krylov subspace of a Krylov subsapce is a Krylov susbspace. 
% \begin{lemma}\label{lemma:krylov_nested}
%     \Chris{I think it's unclear what it means to write "in the setting of Algortihm 1"? Maybe we should just say "let $Q_1$ and $Z$ be as definied in algorithm 1 or similar". Should we be using $\Omega$ here instead of $\bm{Z}$?} 
%     \tyler{I started a reworded version above.}
%     Assume that $\mathcal{K}_{s+r}(\bm{A},\bm{Z})$ has dimension $\ell (s+r)$. Then, in the setting of \Cref{alg:block_lanczos} we have
%     \begin{equation*}
%         \mathcal{K}_{s+r}(\bm{A}, \bm{Z}) = \mathcal{K}_{r+1}(\bm{A}, \bm{Q}_1),
%     \end{equation*}
%     and running \Cref{alg:block_lanczos} with starting block $\bm{Q}_1$ for $r+1$ iterations and  running \Cref{alg:block_lanczos} with starting block $\bm{Z}$ for $s+r$ iterations yields the same result.
% \end{lemma}

\subsection{The randomized SVD for matrix functions}
The randomized SVD \cite{rsvd} is a simple and efficient algorithm to compute low-rank approximations of matrices that admit accurate low-rank approximations. The basic idea behind the randomized SVD is that if $\bm{\Omega}$ is a standard Gaussian random matrix, i.e. the entries in $\bm{\Omega}$ are independent identically distributed $N(0,1)$ random variables, then $\range(\bm{B} \bm{\Omega})$ should be reasonable approximation to the range of $\bm{B}$'s top singular vectors. Hence, projecting $\bm{B}$ onto $\range(\bm{B}\bm{\Omega})$ should yield a good approximation to $\bm{B}$. \Cref{alg:rsvd} implements the randomized SVD on a symmetric matrix $\bm{B}$. %\David{I added a comment about "to truncate or not to truncate?"}
The algorithm returns either the rank $\ell \geq k$ approximation $\bm{W}\bm{X} \bm{W}^\T$ or the rank $k$ approximation $\bm{W}\llbracket\bm{X}\rrbracket_k \bm{W}^\T$, depending on the needs of the user.

\begin{algorithm}
\caption{Randomized SVD}
\label{alg:rsvd}
\textbf{input:} Symmetric $\bm{B} \in \mathbb{R}^{n \times n}$. Rank $k$. Oversampling parameter $\ell -k$. \\
\textbf{output:} Low-rank approximation to $\bm{B}$: $\bm{W} \bm{X} \bm{W}^{\T}$ or $\bm{W} \llbracket \bm{X} \rrbracket_k \bm{W}^{\T}$
\begin{algorithmic}[1]
    \State Sample a standard Gaussian $n \times \ell $ matrix $\bm{\Omega}$.
    \State Compute $\bm{K} = \bm{B} \bm{\Omega}$.\label{line:K_rsvd}
    \State Compute an orthonormal basis $\bm{W}$ for $\range(\bm{K})$.\label{line:V}
    \State Compute $\bm{X} = \bm{W}^{\T} \bm{B} \bm{W}$. \label{line:X_rsvd}
    \State \Return $\bm{W} \bm{X} \bm{W}^{\T}$ or $\bm{W} \llbracket \bm{X} \rrbracket_k \bm{W}^{\T}$
\end{algorithmic}
\end{algorithm}

Typically, the dominant cost of \Cref{alg:rsvd} is that computation of matvecs $\bm{B}$. We require $2\ell$ such matvecs: $\ell$ in \cref{line:K_rsvd} with $\bm{\Omega}$ and $\ell$ in \cref{line:X_rsvd} with $\bm{W}$. When \Cref{alg:rsvd} is applied to a matrix function $\bm{B} = f(\bm{A})$ these matvecs cannot be performed exactly, but need to be approximated using, for example, the block Lanczos method discussed in the previous section.
\Cref{alg:rsvd_matfun} implements the randomized SVD applied to $f(\bm{A})$ with approximate matvecs using the block Lanczos method. 
The cost is now $(s+r)\ell$ matvecs with $\bm{A}$, where $s$ and $r$ should be set sufficiently large so that the approximations \cref{eq:lanczos_approximation,eq:lanczos_approximation_qf} are accurate.

\begin{algorithm}
\caption{Randomized SVD on a matrix function $f(\bm{A})$}
\label{alg:rsvd_matfun}
\textbf{input:} Symmetric matrix $\bm{A} \in \mathbb{R}^{n \times n}$. Rank $k$. Oversampling parameter $\ell -k$. Matrix function $f: \mathbb{R} \to \mathbb{R}$. Accuracy parameters $s$ and $r$. \\
\textbf{output:} Low-rank approximation to $f(\bm{A})$: $\bm{W}\bm{X} \bm{W}^\T$ or $\bm{W}\llbracket \bm{X}\rrbracket_k \bm{W}^\T$
\begin{algorithmic}[1]
    \State Sample a standard Gaussian $n \times \ell $ matrix $\bm{\Omega}$.
    \State Run \Cref{alg:block_lanczos} for $s$ iterations to obtain an orthonormal basis $\bm{Q}_s$ for $\mathcal{K}_s(\bm{A},\bm{\Omega})$, a block tridiagonal matrix $\bm{T}_s$ and an upper triangular matrix $\bm{R}_0$.
    \State Compute the approximation $\bm{K} = \bm{Q}_s f(\bm{T}_s)_{:,1:\ell} \bm{R}_0 \approx f(\bm{A}) \bm{\Omega}$.
    \State Compute an orthonormal basis $\bm{W}$ for $\range(\bm{K})$. 
    \State Run \Cref{alg:block_lanczos} for $r$ iterations with starting block $\bm{W}$ to obtain the block tridiagonal matrix $\widetilde{\bm{T}}_r$. 
    \State Compute the approximation $\bm{X} = f(\widetilde{\bm{T}}_r)_{1:\ell,1:\ell} \approx \bm{W}^\T f(\bm{A}) \bm{W}$.\label{line:X}
    \State \Return $\bm{W} \bm{X} \bm{W}^{\T}$ or $\bm{W} \llbracket \bm{X} \rrbracket_k \bm{W}^{\T}$
\end{algorithmic}
\end{algorithm}

\subsection{Krylov-aware low-rank approximation}
A key observation in \cite{chen_hallman_23} was that $\range(\bm{W}) \subseteq \range(\bm{Q}_s)$, where $\bm{W}$ and $\bm{Q}_s$ are as in \Cref{alg:rsvd_matfun}. Therefore, by \cite[Lemma 3.3]{funnystrom2} one has
\begin{align*}
    \|f(\bm{A}) - \bm{Q}_s \bm{Q}_s^{\T} f(\bm{A})\bm{Q}_s \bm{Q}_s^{\T}\|_\F &\leq \|f(\bm{A}) - \bm{W} \bm{W}^{\T} f(\bm{A})\bm{W} \bm{W}^{\T}\|_\F,\\
    \|f(\bm{A}) - \bm{Q}_s \llbracket \bm{Q}_s^{\T} f(\bm{A})\bm{Q}_s \rrbracket_k \bm{Q}_s^{\T}\|_\F &\leq \|f(\bm{A}) - \bm{W} \llbracket \bm{W}^{\T} f(\bm{A})\bm{W} \rrbracket_k \bm{W}^{\T}\|_\F.
\end{align*}
Hence, assuming that the quadratic form $\bm{Q}_s^\T f(\bm{A}) \bm{Q}_s$ can be computed accurately, the naive implementation of the randomized SVD outlined in \Cref{alg:rsvd_matfun} will yield a worse error than using $\bm{Q}_s\bm{Q}_s^\T f(\bm{A}) \bm{Q}_s \bm{Q}_s^\T$ as an approximation to $f(\bm{A})$.%, and $\bm{Q}_s$ is a basis that is already computed. 

Since $\bm{Q}_s$ could have as many as $s\ell$ columns, an apparent downside to this approach is that approximating $f(\bm{A})\bm{Q}_s$ might require $rs\ell$ matvecs with $\bm{A}$ if we use $r$ iterations of the block Lanczos method (\Cref{alg:block_lanczos}) to approximately compute each matvec with $f(\bm{A}$.
The key observation which allows Krylov-aware algorithms to be implemented efficiently is the following: 
\begin{lemma}[{\cite[Section 3]{chen_hallman_23}}]\label{lemma:krylovkrylov}
    Suppose that $\bm{Q}_s$ is the output of \Cref{alg:block_lanczos} with starting block $\bm{\Omega}$ and $s$ iterations. Then, running $r+1$ iterations of \Cref{alg:block_lanczos} with starting block $\bm{Q}_s$ yields the same output as running $s+r$ iterations of \Cref{alg:block_lanczos} with starting block $\bm{\Omega}$. 
    %Suppose $\bm{Q}_s$ is a basis for $\mathcal{K}_s(\bm{A},\bm{\Omega})$.
    \end{lemma}
The insight behind \Cref{lemma:krylovkrylov} is that, since $\bm{Q}_s$ is a basis for the Krylov subspace $\mathcal{K}_s(\bm{A},\bm{\Omega})$, we have $\mathcal{K}_{r+1}(\bm{A},\bm{Q}_s) = \mathcal{K}_{s+r}(\bm{A},\bm{\Omega})$. This fact can be verified by explicitly writing out $\mathcal{K}_{r+1}(\bm{A},\bm{Q}_s)$:
\begin{align*}
    \mathcal{K}_{r+1}(\bm{A},\bm{Q}_s) &= \range\big( \big[ \bm{Q}_s \,\bm{A}\bm{Q}_s \, \cdots \, \bm{A}^r\bm{Q}_s \big]\big)\\
    &=  \range\big( \big[\bm{\Omega} \,\hspace{5pt}\bm{A}\bm{\Omega} \, \hspace{5pt}\cdots\hspace{5pt} \, \bm{A}^r\bm{\Omega}\\% First line
    &\hspace{2.2cm} \bm{A} \bm{\Omega} \,\hspace{5pt} \bm{A}^2\bm{\Omega} \, \cdots \, \bm{A}^{r+1} \bm{\Omega}\\
    & \hspace{3.5cm} \ddots\\
    & \hspace{3.5cm} \bm{A}^r \bm{\Omega} \,\hspace{5pt}\bm{A}^{r+1} \bm{\Omega} \,\hspace{5pt} \cdots \, \bm{A}^{s+r-1} \bm{\Omega} \big] \big) = \mathcal{K}_{s+r}(\bm{A},\bm{\Omega}). 
\end{align*}
In addition to the work of Chen and Hallman, this observation was recently used to analyze ``small-block" randomized Krylov methods for low-rank approximation \cite{MeyerMuscoMusco:2024} and a randomized variant of the block conjugate gradient algorithm \cite{BCGPreconditioning}.
    
%     Then,
%     \[
%     \mathcal{K}_{s+r}(\bm{A},\bm{\Omega}) 
%     = \mathcal{K}_{r+1}(\bm{A},\bm{Q}_s).
%     \]
% % Moreover, \cref{alg:line:qr} in \cref{alg:block_lanczos} can be implemented in such a way the output of \cref{alg:block_lanczos} on $\bm{A},\bm{\Omega}$ run for $q=s+r$ iteration is identical to the output of \cref{alg:block_lanczos} run on $\bm{A},\bm{Q}_s$ for $r+1$ iterations.
% \end{lemma}
% \begin{proof}
% Without loss of generality, we can take $\bm{Q}_s = [\bm{\Omega}~\bm{A}\bm{\Omega}~\cdots~\bm{A}^{s-1}\bm{\Omega}]$.
% Then,
% \begin{align*}
%     \mathcal{K}_{r+1}(\bm{A},\bm{Q}_s)
%     &= \range\left(\begin{bmatrix} \bm{Q}_s& \bm{A}\bm{Q}_s & \cdots & \bm{A}^{r} \bm{Q}_s \end{bmatrix}\right)
%     \\&= \range\left(\left[\begin{array}{cccc}
%     \bm{\Omega}& \bm{A}\bm{\Omega} & \cdots & \bm{A}^{s-1} \bm{\Omega} 
%     \end{array}\right.\right. 
%     \\&\hspace{6em}\begin{array}{cccc}
%     \bm{A}\bm{\Omega}& \bm{A}^2\bm{\Omega} & \cdots & \bm{A}^{s} \bm{\Omega} \end{array}
%     \\&\hspace{7em}
%     \left.\left.\begin{array}{cccc}
%     \bm{A}^{r}\bm{\Omega}& \bm{A}^{r+1}\bm{\Omega} & \cdots & \bm{A}^{s+r-1} \bm{\Omega} \end{array}\right]\right)
%     \\&=
%     \range\left(\begin{bmatrix} \bm{\Omega}& \bm{A}\bm{\Omega} & \cdots & \bm{A}^{s+r-1} \bm{\Omega}\end{bmatrix}\right)
%     = \mathcal{K}_{s+r}(\bm{A},\bm{\Omega}).
%     \qedhere
% \end{align*}
% \end{proof}
% \noindent
Notably, \Cref{lemma:krylovkrylov} enables us to approximate $\bm{Q}_s^\T f(\bm{A})\bm{Q}_s$ with just $r\ell$  additional matvecs with $\bm{A}$, even though $\bm{Q}_s$ has $s\ell$ columns! Hence, approximately computing $\bm{Q}_s^\T f(\bm{A}) \bm{Q}_s$ is essentially no more expensive, in terms of the number of matvecs with $\bm{A}$, than approximating $\bm{W}^\T f(\bm{A}) \bm{W}$, as done in \Cref{alg:rsvd_matfun}~\cref{line:X}. As a consequence of \Cref{lemma:krylovkrylov}, we have the following result. 
\begin{lemma}\label{lemma:2_times_polynomial_approx}
Let $\lambda_{\max}$ and $\lambda_{\min}$ denote the largest and smallest eigenvalue of $\bm{A}$. Let $q = s+r$ and let $\bm{T}_q$ and $\bm{Q}_s$ be computed using \Cref{alg:block_lanczos}. Then, 
\begin{align*}
    &\|\bm{Q}_s^\T f(\bm{A})\bm{Q}_s - f(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}}\|_\F \leq 2\sqrt{\ell s} \inf\limits_{p \in \mathbb{P}_{2r+1}}\|f(x)-p(x)\|_{L^{\infty}([\lambda_{\min},\lambda_{\max}])}.
\end{align*}
\end{lemma}
\begin{proof}
%By \cite[Lemma 3.1]{chen_hallman_23} we know that for any polynomial $p \in \mathbb{P}_{2r+1}$ we have $\bm{Q}_s^\T p(\bm{A}) \bm{Q}_{s} = p(\bm{T}_q)_{1:d_s,1:d_s}$. 
By \Cref{lemma:block_lanczos_exact,lemma:krylovkrylov} we know that for any polynomial $p \in \mathbb{P}_{2r+1}$ we have $\bm{Q}_s^\T p(\bm{A}) \bm{Q}_{s} = p(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}}$.
Therefore, since $\|\bm{Q}_s\|_\F \leq \sqrt{\ell s}$ and $\|\bm{Q}_s\|_2 \leq 1$ we have
\begin{align*}
    \hspace{5em}&\hspace{-5em}\|\bm{Q}_s^\T f(\bm{A})\bm{Q}_s - f(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}}\|_\F
    \\&= \|\bm{Q}_s^\T f(\bm{A}) \bm{Q}_s - p(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}} + p(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}} - f(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}} \|_\F 
    \\&\leq \|\bm{Q}_s^\T f(\bm{A}) \bm{Q}_s - \bm{Q}_s^\T p(\bm{A}) \bm{Q}_s \|_\F +  \|(p(\bm{T}_q) - f(\bm{T}_q) )_{1:d_{s,\ell},1:d_{s,\ell}} \|_\F 
    \\&\leq \sqrt{\ell s} \left( \|f(\bm{A}) - p(\bm{A}) \|_2 +  \| p(\bm{T}_q)  - f(\bm{T}_q) \|_2\right)
    \\&\leq 2 \sqrt{\ell s} \|f(x) - p(x)\|_{L^{\infty}([\lambda_{\min},\lambda_{\max}])},
\end{align*}
where the last inequality is due to the fact that the spectrum of $\bm{T}_q$ is contained in $[\lambda_{\min},\lambda_{\max}]$.
Optimizing over $p\in\mathbb{P}_{2r+1}$ gives the result.
%Applying \cref{thm:lanczos_exact} with $\bm{Q}_s$ as the starting block and running block-Lanczos for $r+1$ iterations we have that, for any $p\in\mathbb{P}_{2r+1}$, $\bm{Q}_s^\T p(\bm{A}) \bm{Q}_s = p(\bm{T})_{1:d_s,1:d_s}$.
%Therefore,
%\begin{align*}
    %\varepsilon_r
    %&= \|\bm{Q}_1^\T f(\bm{A}) \bm{Q}_1 - \bm{Q}_1^\T p(\bm{T}) \bm{Q}_1 + p(\bm{T})_{1:d_s,1:d_s} - f(\bm{T})_{1:d_s,1:d_s} \|_\F 
    %\\&\leq \|\bm{Q}_1^\T f(\bm{A}) \bm{Q}_1 - \bm{Q}_1^\T p(\bm{A}) \bm{Q}_1 \|_\F +  \|(p(\bm{T}) - f(\bm{T}) )_{1:d_s,1:d_s} \|_\F 
    %\\&\leq \sqrt{d_s} \left( \|f(\bm{A}) - p(\bm{A}) \|_2 +  \| p(\bm{T}) \bm{E}_1 - f(\bm{T}) \|_2\right)
    %\\&\leq 2 \sqrt{d_s} \|f(x) - p(x)\|_{L^{\infty}([\lambda_{\min},\lambda_{\max}])},
%\end{align*}
%where the last inequality is due to the fact that the spectrum of $\bm{T}$ is contained in $[\lambda_{\min},\lambda_{\max}]$.
%Optimizing over $p\in\mathbb{P}_{2r-1}$ gives the result.
\end{proof}

We can now present the Krylov-aware low-rank approximation algorithm; see \cref{alg:krylow}. The total number of matvecs with $\bm{A}$ is $(s+r)\ell$, the same as \cref{alg:rsvd_matfun}. 
However, as noted above, \cref{alg:krylow} (approximately) projects $f(\bm{A})$ onto a higher dimensional subbspace, ideally obtaining a better approximation.
%The complete algorithm is shown in \cref{alg:krylov_aware_polynomial}, and the total number of matrix-vector products used is $(s+r-1)\ell$, the same as \cref{alg:low-rank_polynomial}. 

%However, the above mentioned fact assumes that one can compute $\bm{Q}_q^T f(\bm{A}) \bm{Q}_q$ accurately. Since $\bm{Q}_q$ has more columns than $\bm{W}$, the potential benefit from using $\bm{Q}_q$ instead of $\bm{W}$ might be lost by the fact that approximating $\bm{Q}_q^\T f(\bm{A})\bm{Q}_q$ is more expensive than $\bm{W}^\T f(\bm{A})\bm{W}$. However, due to the special structure of $\bm{Q}_q$ one can show that approximating $\bm{Q}_q^\T f(\bm{A})\bm{Q}_q$ essentially comes at the same cost as approximating $\bm{W}^\T f(\bm{A})\bm{W}$, even if $\bm{Q}_q$ has more columns than $\bm{W}$; see \cite[Lemma 3.1]{chen_hallman_23}.

\begin{algorithm}
\caption{Krylov-aware low-rank approximation}
\label{alg:krylow}
\textbf{input:} Symmetric $\bm{A} \in \mathbb{R}^{n \times n}$. Rank $k$. Oversampling parameter $\ell -k$. Matrix function $f: \mathbb{R} \to \mathbb{R}$. Number of iterations $q = s + r$.\\
\textbf{output:} Low-rank approximation to $f(\bm{A})$: $\bm{Q}_s \bm{X} \bm{Q}_s^\T$ or $\bm{Q}_s \llbracket \bm{X} \rrbracket_k  \bm{Q}_s^\T$%$\bm{Q}_s \llbracket f(\bm{T}_q)_{1:d_s,1:d_s} \rrbracket_k  \bm{Q}_s^\T$. 
\begin{algorithmic}[1]
    \State Sample a standard Gaussian $n \times \ell $ matrix $\bm{\Omega}$.
    \State Run \cref{alg:block_lanczos} for $q=s+r$ iterations to obtain an orthonormal basis $\bm{Q}_s$ for $\mathcal{K}_{s}(\bm{A},\bm{\Omega})$ and a block tridiagonal matrix $\bm{T}_q$.
    \State Compute $\bm{X} = f(\bm{T}_q)_{1:d_{s,\ell},1:d_{s,\ell}}$. \Comment{$\approx \bm{Q}_s^\T f(\bm{A}) \bm{Q}_s$} 
    %\State Compute the eigenvalue decomposition of $\bm{X} = \bm{V} \bm{D} \bm{V}^\T$, where the eigenvalues in $\bm{D}$ are ordered in descending absolute magnitude. 
    \State \textbf{return} $\textsf{\textup{ALG}}(s,r;f) = \bm{Q}_s \bm{X}\bm{Q}_s^\T$ or $\textsf{\textup{ALG}}_k(s,r;f) = \bm{Q}_s \llbracket \bm{X} \rrbracket_k  \bm{Q}_s^\T$.
\end{algorithmic}
\end{algorithm}
We conclude by noting that the function $f$ in \cref{alg:krylow} does not need to be fixed; one can compute a low-rank approximation for many different functions $f$ at minimal additional cost. 