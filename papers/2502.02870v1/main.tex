%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% Our own defined packages and commands
\input{Format/packages}
\input{Format/newcmnds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}




% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Uncertainty Quantification with the Empirical Neural Tangent Kernel}

\begin{document}

\twocolumn[
\icmltitle{Uncertainty Quantification with the Empirical Neural Tangent Kernel}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Joseph Wilson}{uq}
\icmlauthor{Chris van der Heide}{melbelec}
\icmlauthor{Liam Hodgkinson}{melb}
\icmlauthor{Fred Roosta}{uq,cires}
\end{icmlauthorlist}

\icmlaffiliation{uq}{
School of Mathematics and Physics, University of Queensland, Australia}
\icmlaffiliation{melb}{
School of Mathematics and Statistics, University of Melbourne, Australia}
\icmlaffiliation{melbelec}{
Department of Electrical and Electronic Engineering, University of Melbourne, Australia}
\icmlaffiliation{cires}{ARC Training Centre for Information Resilience (CIRES),
Brisbane, Australia}

\icmlcorrespondingauthor{Joseph Wilson}{joseph.wilson1@uqconnect.edu.au}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Bayesian Deep Learning, Neural Tangent Kernel, Uncertainty Quantification, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
While neural networks have demonstrated impressive performance across various tasks, accurately quantifying uncertainty in their predictions is essential to ensure their trustworthiness and enable widespread adoption in critical systems. Several Bayesian uncertainty quantification (UQ) methods exist that are either cheap or reliable, but not both. 
We propose a post-hoc, sampling-based uncertainty quantification (UQ) method for overparameterized networks at the end of training. Our approach constructs efficient and meaningful deep ensembles by employing a (stochastic) gradient-descent sampling process on appropriately linearized networks. We demonstrate that our method effectively approximates the posterior of a Gaussian Process using the empirical Neural Tangent Kernel. Through a series of numerical experiments, we show that our method not only outperforms competing approaches in computational efficiency--often reducing costs by multiple factors--but also maintains state-of-the-art performance across a variety of UQ metrics for both regression and classification tasks.
\end{abstract}


\section{Introduction}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/UncertaintyComparison.pdf}
    \vskip -4mm
    \caption{Comparison of various Bayesian UQ methods (see \cref{sec:background}) on a 1-layer MLP, trained on the data (red) lying on $y = x^3$ (black), with Gaussian noise added. The methods' mean predictors (blue) $\pm 3\sigma$ (green) are shown, where $\sigma^2$ is the variance estimated via each method. Clearly, VI, SWAG and LA all perform poorly on this task, while DE performs better, and LLA and NUQLS perform well.}
    \label{fig:toy_regression}\vskip -4mm
\end{figure*}

Neural networks (NN) achieve impressive performance on a wide array of tasks, in areas such as speech recognition \citep{speech_2019_review,speech_cnn,new_speech_dnn}, image classification \citep{lenet5,krizhevsky2012imagenet,resnet}, computer vision \citep{YOLO,YOLO9000}, and language processing \citep{attention,ray2023chatgpt,bert_google}, often significantly exceeding human performance. While the promising predictive and generative performance of modern NNs is evident, accurately quantifying uncertainty in their predictions remains an important and active research frontier \citep{uncertainty_review}. Models are often over-confident in predictions on out-of-distribution (OoD) inputs \citep{calibrate_temp_scaling}, and sensitive to distribution-shift \citep{ford2019adversarial}. By quantifying a model's uncertainty, we can determine when it fails to provide well-calibrated predictions, indicating the need for additional training (possibly on more diverse data) or even human intervention. This is vital for deploying NNs in critical applications like diagnostic medicine and autonomous machine control \citep{uncertain_health_engineering}. 

An array of uncertainty quantification methods for NNs exist, each with benefits and drawbacks. Frequentist statistical methods, such as conformal prediction \citep{vovk2005algorithmic,papadopoulos2002inductive,lei2014distribution}, create prediction intervals through parameter estimation and probability distributions on the data. While currently considered state-of-the-art, the main drawback is that conformal prediction is data-hungry, requiring a large hold-out set.  In contrast, parametric Bayesian methods model the uncertainty in the parameters, providing conditional predictive probabilities for each function output. 

Unfortunately, largely due to the curse of dimensionality, existing Bayesian methods are very expensive to compute, and provide poor approximations to the predictive distribution \citep{folgoc2021mc}. This results in a suite of methods that require excessive approximations to scale to large problems \citep{daxberger2021laplace}, often at the cost of theoretical underpinnings, or necessitating modifications to the network itself \citep{he2020bayesian}. 

Several works \cite{ beyondTheNorms, dadalto2023data, granese2021doctor} employ the feature-space representations of the network to quantify the uncertainty of test predictions, through kernel densities \cite{kotelevskii2022nonparametric}, Gaussian Discriminant analysis \cite{mukhoti2023deep}, non-constant mapping functions \cite{tagasovska2019single}, etc. The interpretation of uncertainty in these works is related to risk of misclassification; these methods generally perform very well at detecting OoD points.While both conformal predictions and the feature-space works just mentioned are important, we limit our scope to Bayesian methods in this paper. Though Bayesian methods can suffer from prior misspecification \cite{masegosa2020learning} and computational burdens, the predictive and posterior distributions of a model are very natural frameworks for quantifying the uncertainty and spread of possible values of the model.

Gaussian Processes (GPs) \citep{williams2006gaussian} are important tools in Bayesian machine learning (ML) that can directly capture the epistemic (model) uncertainty of predictions, and arise as large-width limits of NNs \citep{neal96a}. However, na\"ive GP training scales cubically in the number of training datapoints, necessitating approximations for modern applications.  Neural Tangent Kernels (NTKs) \citep{ntk_jacot} describe the functional evolution of a NN under gradient flow, and naturally arise in the analysis of model quality \citep{hodgkinson2023interpolating}. Due to their deep connection to NNs, these covariance functions are enticing as potential tools for UQ of their corresponding NNs. 

Motivated by this, we present a UQ method for a trained, over-parameterized NN model, wherein we approximate the predictive distribution through an ensemble of linearized models, trained using (stochastic) gradient-descent. For certain loss functions, this ensemble samples from the posterior of a GP with an empirical NTK.

\vspace{1mm}
\noindent
\textbf{Contributions.} Our contributions are as follows:
% \vspace{-3mm}
\begin{enumerate}
    \item We present a Monte-Carlo sampling based UQ method to approximate the predictive distribution of NNs, called Neural Uncertainty Quantification by Linearized Sampling (NUQLS). Our method is lightweight, post-hoc, numerically stable, and embarrassingly parallel. 
    \item Under certain assumptions, we show that NUQLS converges to the predictive distribution of a GP with an empirical NTK kernel, providing a novel perspective on the connection between NNs, GPs, and the NTK.
    \item On various ML problems, we show that NUQLS performs as well as or better than leading UQ methods, is less computationally expensive than deep ensemsble, and scales to large image classification tasks.
\end{enumerate}

\paragraph{Notation.} \label{sec:notation}
Throughout the paper, we denote scalars, vectors, and matrices as lower-case, bold lower-case, and bold upper-case letters, e.g., $c$, $\btheta$ and $\km$, respectively. For two vectors $ \vv \in \real^p $ and $ \ww \in \real^p $, their Euclidean inner product is denoted as $\dotprod{\vv,\ww} = \vv^{\T}\ww $.
We primarily consider supervised learning, which involves a function $\ff:\real^d\times\real^p\to\real^c$, assumed sufficiently smooth in its parameters $\btheta \in \real^p$, a training dataset $\sD = \{\sX,\sY\} = \{ \xx_i, \yy_i \}_{i=1}^n \subset  \real^d \times \real^c$, and a loss function $\ell: \real^{c} \times \real^{c} \to [0,\infty)$. 

The process of training amounts to finding a solution, $\widehat{\btheta}$, to the optimization problem $\min_{\btheta} \sum_{i=1}^n \ell(\ff(\xx_i, \btheta),\yy_i) + \mathcal{R}(\btheta)$, where $\mathcal{R}(\btheta)$ is a regulariser. For a kernel function $\km:\real^{d}\times \real^{d} \to \real^{c \times c}$, we define $\km_{\sX,\sX} \in \real^{nc \times nc}$ where the $(i,j)^{\text{th}}$ (block) element is  $\km(\xx_i,\xx_j) \in \real^{c \times c}$. Additionally, we define $\km_{\sX,\xx} \defeq \begin{bmatrix}
   \km(\xx_{1},\xx) &  \ldots & \km(\xx_{n},\xx) 
\end{bmatrix}^{\T} \in \real^{nc \times c}$ with $\km^{\T}_{\sX,\xx} = \km_{\xx,\sX}$. For a matrix $\JJ$, its Moore-Penrose pseudo-inverse is denoted by $\JJ^{\dagger}$.

\section{Background}
\label{sec:background}

\paragraph{Bayesian Framework.}
Parametric Bayesian methods admit access to a distribution over predictions $\ff(\btheta,\xx^{\star})$ for unseen test points $\xx^{\star}$, 
through the posterior distribution $p(\btheta | \mathcal{D}) \propto p(\btheta) p(\sD | \btheta)$, where $p(\btheta)$ is the prior, and $p(\sD | \btheta)$ is the likelihood function evaluated on the training data. By integrating these parameter values weighted by how well they explain the training data, a distribution over predictions $p(\yy^{\star} | \xx^{\star}, \sD) = \int p(\yy^{\star} | \ff(\btheta,\xx^{\star})) p(\btheta | \mathcal{D}) d\btheta$ is obtained. Both the posterior and the predictive distributions are computationally intractable in all but the simplest cases. In our setting, their calculation involves very high-dimensional integrals, which we can approximate through a Monte Carlo (MC) approximation $p(\yy^{\star} | \xx^{\star}, \sD) \approx 1 / S \sum_s p(\yy^{\star} | \ff(\btheta_s,\xx^{\star}))$ for  $\btheta_s \sim q(\btheta)$, where $q(\btheta)$ is an approximation to the posterior distribution. 
%
The effectiveness of classical Markov Chain Monte Carlo (MCMC) methods in posterior sampling diminishes in this setting due to the curse of dimensionality, limiting the tractable techniques available with theoretical guarantees. This limitation necessitates coarser approximations for estimating the posterior $q(\btheta)$, leading to the emergence of the following Bayesian methods for posterior approximation.


Proposed in \citep{gal2016dropout}, \textit{Monte Carlo Dropout} (MC-Dropout) takes a trained NN, and uses the \textit{dropout} regularization technique at test time to sample $S$ sub-networks, $\left \{ \ff(\btheta_s, \xx) \right \}_{s=1}^S$ as an MC approximation of the predictive distribution. 
MC-Dropout is an inexpensive method, yet it is unlikely to converge to the true posterior, and is erroneously multi-modal \citep{folgoc2021mc}. 

In \textit{Variational Inference} (VI)  \citep{hinton1993keeping,graves2011practical}, a tractable family of approximating distributions for $p(\btheta | \sD)$ is chosen, denoted by $q_{\bpsi}(\btheta)$, and parameterized by $\bpsi$. 
The optimal distribution in this family is obtained by finding $\bpsi$ that minimizes the Kullback-Leibler divergence between $q_{\bpsi}(\btheta)$ and $p(\btheta | \sD)$. 
To be computationally viable, mean field and low-rank covariance structures are often required for $q_{\bpsi}(\btheta)$. 

The \textit{Laplace Approximation} (LA) \citep{mackay1992bayesian,ritter2018scalable} is a tool from classical statistics which approximates the posterior distribution by a Gaussian centered at the maximum a posteriori solution (MAP) with normalized inverse Fisher information covariance. 
This is justified by the Bernstein-von Mises Theorem \citep[pp. 140--146]{asymptotic_statistics}, which guarantees that the posterior converges to this distribution in the large-data limit, for well-specified regular models. 
%
However, NNs are often over-parameterized, and the regime where $n \to \infty$ with fixed $p$ is no longer valid or a reasonable reflection of modern deep learning models \citep{de2021quantitative}.

These limitations are acknowledged but seldom discussed by the Bayesian Deep Learning (BDL) community, which tends to view this as an additional layer of approximation rather than a modelling error, leading to the development of synonymous LA-inspired methods. However, we will show that such methods typically perform poorly compared to deep ensembles, which are often excluded from comparisons. 

A generalized Gauss-Newton (GGN) approximation to the Hessian ensures a positive-definite Fisher approximation \citep{khan2020approximate}. Moreover, to scale, the LA requires reduction to a subset of parameters or further approximations of the covariance structure, such as only forming the posterior over parameters in the last dense layer of a NN (last-layer), or using Kronecker-Factored Approximate Curvature (KFAC) \citep{martens2015optimizing}.

We can evaluate the posterior and predictive distribution in the LA using the linearization of $\ff(\xx,\btheta)$ around the MAP solution. This approach is known as the \textit{Linearized Laplace Approximation} (LLA) and typically delivers better performance than  LA \citep{immer2021improving}. Recent work \citep{antoran2022sampling, ortega2023variational} has enabled LLA to become more scalable for larger models and datasets.

\textit{Deep Ensembles} (DE) \citep{lakshminarayanan2017simple} are comprised of $S$ networks that are independently trained on the same training data, with different initializations, leading to a collection of parameters $\{\btheta_s; s = 1\ldots,S\}$. 
At test time, our predictive distribution becomes $p(\yy | \xx, \sD) \approx 1/S \sum_{s=1}^S p(\yy | \ff( \btheta_s,\xx))$.  
Despite their simple construction, DEs are able to obtain samples from different modes of the posterior, and are often considered state-of-the-art for BDL \citep{de_bayesian}. However, due to the often large cost of training performant neural networks, deep ensembles of reasonable size can be undesirably expensive to obtain.

\textit{Stochastic Weight Averaging Gaussian} (SWAG) \citep{maddox2019simple} takes a trained network and undergoes further epochs of SGD training to generate a collection of parameter samples. A Gaussian distribution with sample mean and a low-rank approximation of the sample covariance is then used to approximate a posterior mode. 


\paragraph{Gaussian Processes.}
A GP is a stochastic process that is defined by a mean and a kernel function. A GP models the output of a random function $\ff: \real^{d} \to \real^{c}$, at a finite collection of points $\xx$, as being jointly Gaussian distributed. 
Conditioning on training data $\sD$, it generates a posterior predictive distribution $p(\ff(\xx_{\star}) | \sD)$ at a test point $\xx_{\star}$. For example, in regression settings where $c = 1$, with the mean and kernel functions $\mu : \real^d \to \real$ and $\kappa : \real^d \times \real^d \to \real$, as well as the observations $y \sim \sN(f(\xx),\sigma^2)$, there is a closed form expression for the predictive distribution, $p(f(\xx_{\star}) | \sD) \sim \sN \big(\bmu(\xx_\star), \bsigma(\xx_\star)\big)$, where
\begin{align*}
    \bmu(\xx_\star) &= \kv_{\xx_{\star}, \sX} \big[\km_{\sX, \sX} + \sigma^2 \eye \big]^{-1} \big(\yy - \bmu(\sX) \big) + \bmu(\xx_{\star})\\
    \bsigma(\xx_\star) &= \kappa(\xx_{\star}, \xx_{\star}) - \kv_{\xx_{\star}, \sX} \big [ \km_{\sX, \sX} + \sigma^2 \eye \big ]^{-1} \kv_{\sX, \xx_{\star}}
\end{align*}
for $\yy \defeq [ y_1, \dots, y_n ]^{\T}$ and $\bmu(\sX) \defeq [\mu(\xx_1), \dots, \mu(\xx_n) ]^{\T}$.

GPs can yield impressive predictive results when a suitable kernel is chosen \citep{rasmussen1997evaluation}. However, forming the kernel and solving linear systems makes GP computations intractable for large datasets. Approximations such as sparse variational inference \citep{titsias2009variational}, Nystr\"{o}m methods \citep{martinsson2020randomized}, and other subspace approximations \cite{gpytorch} can alleviate the computational burden; however, these approximations often result in a significant decline in predictive performance.

\paragraph{Neural Tangent Kernel.}
Under continuous time gradient flow, it can be shown that a NN output $\ff(\cdot,\btheta):\real^{d} \to \real^{c}$ undergoes kernel gradient descent,  namely $\partial_t \ff(\xx,\btheta_{t}) = - \sum_{i=1}^{n} \km_{\btheta_{t}}(\xx, \xx_{i}) \nabla_{\ff} \ell(\ff(\xx_i,\btheta_{t}),\yy_i)$, where  
\begin{align}
    \label{eq:ntk}
    \km_{\btheta}(\xx, \yy) \defeq \dotprod{\frac{\partial \ff}{\partial \btheta}(\xx,\btheta), \frac{\partial \ff}{\partial \btheta}(\yy,\btheta)} \in \real^{c \times c},
\end{align}
is the \textit{empirical NTK} \citep{ntk_jacot}. As the width of a network increases, the empirical NTK converges (in probability) to a deterministic limit, sometimes referred to as the analytic NTK, that is independent of the network's parameters. \citet{lee2019wide} showed that in this limit, the network acts according to its NTK linearization during gradient descent (GD) training. This parameter independence results in a loss of feature learning in the limiting regime \citep{yang2021feature}. However, for finite-width NNs, \citet{fort2020deep} empirically showed that the empirical NTK becomes ``data-dependent'' during  training. Since we focus exclusively on the finite-width regime, we refer to the empirical NTK simply as the NTK.

\section{NUQLS} \label{sec:NUQLS}
We now present \textbf{N}eural \textbf{U}ncertainty \textbf{Q}uantification by \textbf{L}inearized \textbf{S}ampling (NUQLS), our post-hoc sampling method for quantifying the uncertainty of a trained NN. We begin by presenting the motivation and a high-level overview of our method. Subsequently, we provide theoretical justification, demonstrating that, under specific conditions, the NUQLS samples represent draws from the approximate posterior of the neural network, which is equivalent to a Gaussian process defined by the NTK. 

\subsection{Motivation and High-level Overview}
NNs are often over-parameterized, resulting in non-uniqueness of interpolating solutions, with sub-manifolds of parameter space able to perfectly predict the training data \citep{hodgkinson2023interpolating}. 
%
To generate a distribution over predictions, we adopt a Bayesian framework, where the uncertainty in a neural network's prediction can be interpreted as the spread of possible values the network might produce for a new test point, conditioned on the training data. 
%
To quantify this uncertainty, we can evaluate the test point on other ``nearby'' models with high posterior probability and analyze their range of predictions. To identify such models, we propose using the linearized approximation of the original network around its trained parameters as a simpler yet expressive surrogate. This approach can retain, to a great degree, the rich feature representation of the original network while enabling tractable exploration of the posterior distribution. In the same spirit as DE, in the overparameterized setting, we can fit this linear model to the original training data, using (stochastic) gradient descent with different initializations, resulting in an ensemble of linear predictors. Not only does this ensemble explain the training data well, but it also provides a practical way to estimate predictive uncertainty. 

More precisely, let $\widehat{\btheta}$ be a set of parameters obtained after training the original NN. Linearizing $\ff$ around $\widehat{\btheta}$ gives
\begin{align}
    \label{eq:linearized_one_f}
    \ff(\btheta, \xx) &\approx \widetilde{\ff}(\btheta, \xx) \defeq \ff(\widehat{\btheta}, \xx) + \JJ(\widehat{\btheta}, \xx) (\btheta - \widehat{\btheta}),
\end{align}
where $\JJ(., \xx) = [\partial \ff(.,\xx)/\partial \btheta]^{\T} \in \real^{c \times p}$ is the Jacobian of $\ff$. 
Using the linear approximation \cref{eq:linearized_one_f}, we consider
\begin{align}
\label{eq:gen_loss_lin}
    \min_{\btheta} \; & \sum_{i=1}^{n} \ell(\widetilde{\ff}(\btheta,\xx_{i}) , \yy_{i}).
\end{align}
In overparameterized settings, \cref{eq:gen_loss_lin} may have infinitely many solutions. To identify these solutions and create our ensemble of linear predictors, we employ (stochastic) gradient descent, initialized at zero-mean isotropic Gaussian perturbations of the trained parameter, $\widehat{\btheta}$. The pseudo-code for this algorithm is provided in \cref{alg:ntk_uq_linear}. 

Note that while the training cost for a linearised network is only slightly higher per epoch compared to standard NN training, each network  in the NUQLS ensemble is initialized in a neighborhood of a local minimum of the original NN. As a result, NUQLS often requires significantly fewer epochs to converge, leading to an order-of-magnitude computational speedup relative to DE (see \cref{table:uci_reg,table:img_class,table:uci_reg_all} for wall-clock time comparisons.).
%Optimizing the implementation for speed and number of realizations is left for future work. 


\begin{algorithm}[t]
\caption{NUQLS}\label{alg:ntk_uq_linear}
\begin{algorithmic}
\STATE \textbf{Input:} Number of realizations, $S$,  NN weights, $\widehat{\btheta}$.
\FOR{$s=1$ : $S$}
\STATE $\btheta_{0,s} \leftarrow \widehat{\btheta} + \zz_0$, where $\zz_0 \sim \mathcal{N}(\zero,\gamma^{2}\eye)$
\STATE $\btheta^{\star}_{s} \leftarrow$ Run (stochastic) gradient descent from $\btheta_{0,s}$ to (approximately) solve \cref{eq:gen_loss_lin} and obtain $\btheta^{\star}_{s}$ 
\ENDFOR
\RETURN $\{\widetilde{\ff}(\btheta^{\star}_{s},.)\}_{s=1}^S$
\end{algorithmic}
\end{algorithm}
For a given test point $\xxs$, the mean prediction and the associated uncertainty can be computed using $\{\widetilde{\ff}(\btheta^{\star}_{s},\xx^{\star})\}_{s=1}^S$. 


\subsection{Theoretical Analysis} \label{sec:GP_connection}
We now establish the key property of \cref{alg:ntk_uq_linear}: \textit{under mild conditions, NUQLS generates samples from the approximate posterior of the neural network, which in many cases corresponds to a Gaussian process defined by the NTK}.  Proofs are provided in \cref{sec:appendix:proofs}.


Suppose $\btheta^{\ddagger}$ is any solution to \cref{eq:gen_loss_lin}. Using $\btheta^{\ddagger}$, one can construct a family of solutions to \cref{eq:gen_loss_lin} as 
\begin{align}
    \label{eq:gen_theta}
    \btheta^{\star}_{\zz} = \btheta^{\ddagger} + \left(\eye - \JJ_{\sX}^{\dagger}\JJ_{\sX}\right) \zz, \quad \forall \zz \in \real^{p},
\end{align}
where $\widehat{\btheta}$ is the parameters of the trained NN and $\JJ_{\sX} = [ \JJ^{\T}(\widehat{\btheta}, \xx_{1})  \;\ldots\;  \JJ^{\T}(\widehat{\btheta}, \xx_{n}) ]^{\T} \in \real^{nc \times p}$. Note that the second term in \cref{eq:gen_theta} consists of all vectors in the null space of $\JJ_{\sX}$. Since any such $\btheta^{\ddagger}$ can be decomposed as the direct sum of components in the null space of $\JJ_{\sX}$ and its orthogonal complement, the family of solutions in \cref{eq:gen_theta} depends on the choice of $\btheta^{\ddagger}$.
However, under certain assumptions, we can ensure that the representation \cref{eq:gen_theta} is uniquely determined, i.e., $\btheta^{\ddagger}$ can be taken as the unique solution to \cref{eq:gen_loss_lin} that is orthogonal to the null space of $\JJ_{\sX}$. More precisely, we can show that, under these assumptions on the loss,  $\btheta^{\ddagger}$ in \cref{eq:gen_theta} can be taken as the unique solution to  
\begin{align}
\label{eq:ell_range}
\min_{\btheta} \; & \sum_{i=1}^{n} \ell(\widetilde{\ff}(\btheta,\xx_{i}) , \yy_{i}), \;\; \text{s.t.} \;\; \btheta \in \range\left(\JJ_{\sX}^{\T}\right).
\end{align}
\begin{lemma}
    \label{lemma:unique_sol}
    Suppose the loss, $\ell(\,\cdot\,,\yy)$, is either:
\begin{itemize}
    \item strongly convex in its first argument, or
    \item strictly convex in its first argument, and a solution to \cref{eq:gen_loss_lin} exists.
\end{itemize}
    The  problem \cref{eq:ell_range} admits a unique solution. 
\end{lemma}
As it turns out, any solution of the form \cref{eq:gen_theta} can be efficiently obtained using (stochastic) gradient descent. 
\begin{theorem}
\label{theorem:gd_genloss}
    Consider the optimization problem \cref{eq:gen_loss_lin} and assume $\JJ_{\sX}$ is full row-rank. 
    \begin{itemize}
        \item (\textbf{Gradient Descent}) Suppose $\ell(\ff,\yy)$ is strictly convex with locally Lipschitz continuous gradient, both with respect to $\ff$, and the problem \cref{eq:gen_loss_lin} admits a solution. Gradient descent, initialized at $\zz$ and with appropriate learning rate, converges to  \cref{eq:gen_theta}.
        \item (\textbf{Stochastic Gradient Descent}) Suppose $\ell(\ff,\yy)$ is strongly convex with Lipschitz continuous gradient, both with respect to $\ff$, and any solution to the problem \cref{eq:gen_loss_lin} is interpolating. Stochastic gradient descent, initialized at $\zz$ and with small enough learning rate, converges to  \cref{eq:gen_theta} with probability one.
    \end{itemize}
\end{theorem}
We note that the local smoothness requirement in the first part of \cref{theorem:gd_genloss} is a relatively mild assumption; for example, it holds if we simply assume that $\ell$ is twice continuously differentiable. Also, the full-row rank assumption on $\JJ_{\sX}$ in the second part of \cref{theorem:gd_genloss} is reasonable for highly over-parameterized networks; e.g., see \cite{liu2022loss}. 

Now, suppose $\JJ_{\sX}$ is full row-rank and the assumption of \cref{lemma:unique_sol} holds, ensuring the existence of the unique solution $\btheta^{\ddagger}$ to \cref{eq:ell_range}. Noting $\range\left(\JJ_{\sX}^{\T}\right) = \range\left(\JJ_{\sX}^{\dagger}\right)$, we can write \cref{eq:gen_theta} as
\begin{align*}
   \btheta^{\star}_{\zz} = &\JJ_{\sX}^{\T} \km_{\sX,\sX}^{-1} \ww + \left(\eye - \JJ_{\sX}^{\T} \km_{\sX,\sX}^{-1} \JJ_{\sX}\right) \zz, \quad \forall \zz \in \real^{p}
\end{align*}
where $\ww$ is some vector for which $\btheta^{\ddagger} = \JJ_{\sX}^{\dagger} \ww$, and $\km_{\sX,\sX} \defeq \JJ_{\sX} \JJ^{\T}_{\sX} = \km_{\widehat{\btheta}}(\sX,\sX) \in \real^{nc \times nc}$ is the Gram matrix of the empirical NTK \cref{eq:ntk} on the training data $\sX$. 
%
Setting $ \zz = \widehat{\btheta} -  \zz_{0} $ for some $\zz_0$, we get
\begin{align*}
    \btheta^{\star} &= \JJ_{\sX}^{\T} \km_{\sX,\sX}^{-1} \ww +\left(\eye - \JJ_{\sX}^{\T} \km_{\sX,\sX}^{-1} \JJ_{\sX}\right) (\widehat{\btheta} -  \zz_{0}),
\end{align*}
for which
\begin{align}
    \label{eq:f_pred}
    &\widetilde{\ff}(\btheta^{\star},\xx) = \ff(\widehat{\btheta},\xx) + \km_{\xx,\sX} \km_{\sX,\sX}^{-1} \left( \ww - \JJ_{\sX} \widehat{\btheta}\right) \nonumber \\
    &+ \left( \km_{\xx,\sX} \km_{\sX,\sX}^{-1} \JJ_{\sX}  -  \JJ(\widehat{\btheta},\xx)\right) \zz_{0}, 
\end{align} 
where $\km_{\xx,\sX}  \defeq \km_{\widehat{\btheta}}(\xx,\sX) \in \real^{c \times nc}$. Taking $\zz_{0}$ to be a random variable, we form an ensemble of predictors $\{ \widetilde{\ff}(\btheta^{\star},\xx) \}_{\zz}$, where each $\btheta^{\star}$ is formed from the projection of a random $\zz$ onto $\text{Null}(\JJ_{\sX})$. We require $\zz$'s distribution to be symmetric, isotropic, and centered at %the parameter values we are linearising around, i.e 
$\widehat{\btheta}$, as we do not know \textit{a priori} which directions contain more information. We choose the maximum entropy distribution for a given mean and variance, which is Gaussian\footnote{Heavier-tailed distributions matching the above criteria, e.g. logistic distributions, \emph{may} improve results.}. Hence, we let $\zz_0 \sim \mathcal{N}(\zero,\gamma^{2}\eye)$, for some hyper-parameter $\gamma \in \real$. The expectation and variance of \cref{eq:f_pred} are $\Ex \big( \widetilde{\ff}(\btheta^{\star}, \xx) \big) = \bmu(\widehat{\btheta},\xx), \text{Var}\big ( \widetilde{\ff}(\btheta^{\star}, \xx) \big) = \bsigma^{2}(\widehat{\btheta},\xx)$, where
\begin{align}
    \mathbf{\bmu}(\widehat{\btheta},\xx) &= \km_{\xx,\sX}^{\T} \km_{\sX,\sX}^{-1} \JJ_{\sX} ( \btheta^{\ddagger} - \widehat{\btheta}) + \ff(\widehat{\btheta},\xx), \label{eq:mu_classification} \\
    \mathbf{\bsigma}^{2}(\widehat{\btheta},\xx) &=  \big( \km_{\xx,\xx} - \km_{\xx,\sX}^{\T} \km_{\sX,\sX}^{-1} \km_{\xx,\sX} \big)  \gamma^{2}. \label{eq:sigma2_classification}
\end{align}
The distribution of the predictor $\ff(\btheta^{\star} ,\xx)$ is then
\begin{align}
    \label{eq:normal_gen}
    \ff(\btheta^{\star} ,\xx) \overset{\text{approx}}{\sim} \sN\big( \bmu(\widehat{\btheta},\xx), \bsigma^{2}(\widehat{\btheta},\xx) \big).
\end{align}

\begin{remark}[Connections to GP: Regression]\label{sec:regression_gp}
For scalar-valued $f:\real^{p} \times \real^{d} \to \real$ with quadratic loss $\ell(f(\btheta,\xx), y) = (f(\btheta,\xx)- y)^{2}$, we can explicitly write $\btheta^{\ddagger}= \JJ_{\sX}^{\dagger} ( \yy - \ff(\widehat{\btheta},\sX) + \JJ_{\sX}\widehat{\btheta})$, where $\ff(\btheta,\sX) \defeq \big[f(\btheta,\xx_{1}),~f(\btheta,\xx_{2}),~\hdots,~f(\btheta,\xx_{n}) \big ]^{\T}$ and $\yy \defeq [ y_1, \dots, y_n ]^{\T}$.
%
For $\zz_0 \sim \mathcal{N}(\zero,\gamma^{2}\eye)$, we thus have  $f(\btheta^{\star} ,\xx) \overset{\text{approx}}{\sim} \sN\big( \mu(\widehat{\btheta},\xx), \sigma^{2}(\widehat{\btheta},\xx) \big)$
with
\begin{align*}
    \mu(\widehat{\btheta},\xx) &= \kv_{\xx,\sX} \km_{\sX,\sX}^{-1} ( \yy - \ff(\widehat{\btheta},\sX)) + f(\widehat{\btheta},\xx), \\
    \sigma^{2}(\widehat{\btheta},\xx) &=  \big( \kappa(\xx,\xx) - \kv_{\xx,\sX} \km_{\sX,\sX}^{-1} \kv_{\xx,\sX} \big)  \gamma^{2}.
\end{align*}
By the full-rank assumption on the Jacobian, this amounts to the conditional distribution of the following normal distribution, conditioned on interpolation $ \ff(\btheta^{\star}, \sX)  = \yy $,
\begin{align}
    \label{eq:GP}
    \hspace{-3mm}\begin{bmatrix}
        \ff(\btheta^{\star}, \sX) \\ 
        f(\btheta^{\star}, \xx)
    \end{bmatrix} \!&\sim \sN \Bigg(\!\!\begin{bmatrix}
        \ff(\widehat{\btheta},\sX) \\ 
        f(\widehat{\btheta},\xx)
    \end{bmatrix}\!, 
    \gamma^{2}\! \begin{bmatrix}
        \km_{\sX,\sX} & \kv_{\xx,\sX} \\
        \kv_{\xx,\sX}^{\T} & \kappa_{\xx,\xx}
    \end{bmatrix}\!\!\Bigg).\!\!
\end{align}

Therefore, $f(\btheta^{\star},\xx)$ follows a GP with an NTK kernel. Conditioning on $\ff(\btheta^{\star}, \sX)  = \yy $ is reasonable, since by construction $\ff(\btheta^{\star} ,\sX) \approx  \widetilde{\ff}(\btheta^{\star},\sX)$, and under the full-rank assumption of $\JJ_{\sX}$, we have $\widetilde{\ff}(\btheta^{\star},\sX) = \yy$.
\end{remark}

\begin{remark}[Connections to GP: General Loss]
Beyond quadratic loss, for a general loss function satisfying the assumptions of \cref{lemma:unique_sol}, a clear GP posterior interpretation like that in \cref{eq:GP} may not exist. Nevertheless, we can still derive related insights.
If $\widehat{\btheta}$ is an interpolating solution from initial training, which is common for modern NNs, then as long as $\JJ_{\sX}$ is full row-rank, solving \cref{eq:gen_loss_lin} is equivalent to finding $\btheta \in \real^{p}$ such that $\widetilde{\ff}(\btheta, \sX) = \ff(\widehat{\btheta},\sX)$, i.e., find $\btheta \in \real^{p}$ such that $\JJ_{\sX} (\btheta - \widehat{\btheta}) = \zero$. So we obtain \eqref{eq:normal_gen} with 
$\bmu(\widehat{\btheta},\xx) = \ff(\widehat{\btheta},\xx)$ and $
\bsigma^{2}(\widehat{\btheta},\xx) =  \big( \km_{\xx,\xx} - \km_{\xx,\sX}^{\T} \km_{\sX,\sX}^{-1} \km_{\xx,\sX} \big)  \gamma^{2}$ as the conditional distribution of \cref{eq:GP}, conditioned on the event  $\ff(\btheta^{\star},\sX)=\ff(\widehat{\btheta},\sX)$, i.e.,  interpolation  with $\ell(\ff(\btheta^{\star},\xx_{i}),\yy_i) = 0$ for $i=1,\ldots,n$. 
\end{remark}


\paragraph{The Punchline.} Drawing samples from the posterior \cref{eq:normal_gen} by explicitly calculating \cref{eq:mu_classification,eq:sigma2_classification} can be intractable in large-scale settings. Moreover, it can be numerically unstable due to the highly ill-conditioned nature of the NTK matrix\footnote{Recall that the condition number of $\km_{\sX,\sX}$ is the square of that of $\JJ_{\sX}$.}. However, by combining the above derivations with \cref{theorem:gd_genloss}, we arrive at the key property of \cref{alg:ntk_uq_linear}: it enables efficient sampling from the posterior \cref{eq:normal_gen}. 
\begin{corollary}[Key Property of NUQLS]
    With the assumptions of \cref{theorem:gd_genloss}, the samples generated by \cref{alg:ntk_uq_linear} represent draws from the predictive distribution in \cref{eq:normal_gen}.
\end{corollary}
Hence, we approximate \cref{eq:mu_classification,eq:sigma2_classification} by computing the sample mean and covariance of $\{\widetilde{\ff}(\btheta^{\star}_{s},\xx)\}_{s=1}^{S}$ obtained from \cref{alg:ntk_uq_linear}. By the law of large numbers, the quality of these approximations improves as $S \to \infty$. 


\begin{remark}
Loss functions that do not satisfy the assumption of \cref{lemma:unique_sol}, such as the cross-entropy loss, may fail to yield a unique representation of \cref{eq:gen_theta}, so the above posterior analysis does not apply. However, our experiments demonstrate that \cref{alg:ntk_uq_linear} can still generate samples that effectively capture the posterior variance (see \cref{sec:img_class_unc}) and posterior mean (see \cref{sec:img_class_pred}). In cases where \cref{eq:gen_loss_lin} lacks a solution, \cref{alg:ntk_uq_linear} can still be executed by terminating the iterations of (stochastic) GD early. Investigating the distribution of the resulting ensemble and its connection to an explicit posterior remains a potential direction for future research.  
\end{remark}

\section{Related Works}
NUQLS shares notable similarities with, and exhibits key differences from, several prior works that we briefly allude to here; please see \cref{sec:appendix:related_work} for more discussions. 

\noindent
\textbf{LLA Framework.} Like the popular LLA framework \cite{khan2020approximate, foong2019between, immer2021improving, daxberger2021laplace}, NUQLS relies on linearizing the network, but it diverges in its approach to posterior approximation. While LLA requires sampling from a parameter distribution and often imposes a prior to handle degeneracy in overparameterized settings, NUQLS directly approximates the predictive distribution without artificial priors, avoiding biases and additional hyperparameters. This results in a noise-free GP with an NTK kernel, making NUQLS more suitable for interpolating regimes \cite{hodgkinson2023interpolating}. Additionally, NUQLS captures output covariances in classification tasks, offering a more comprehensive predictive distribution than LLA, which produces independent GPs for each output. NUQLS also provides flexibility in regression tasks by allowing post-hoc variance scaling (see $\gamma$ in \cref{eq:normal_gen}), enabling efficient hyperparameter tuning without retraining (see \cref{sec:appendix:regression_tuning} for an explanation of this method). 

Among the LLA-inspired methods, Sampling-LLA in \citet{antoran2022sampling} bears the closest resemblance to NUQLS. However, in addition to
the fundamental differences between NUQLS and LLA-inspired methods mentioned above, Sampling-LLA has several 
distinctions from NUQLS. Above all, unlike Sampling-LLA, NUQLS avoids the need for regularization and random subproblems. Instead, NUQLS leverages the true training data and random initialization to naturally capture uncertainty in overparameterized settings. Another related approach is Variational LLA \citep{ortega2023variational}, which uses a variational sparse GP with an NTK to approximate the LLA predictive distribution. Variational LLA has been shown to outperform Sampling-LLA in terms of both computational efficiency and uncertainty quantification performance.

\noindent
\textbf{Ensemble Framework.} NUQLS also differs from ensemble-based methods. Unlike Bayesian Deep Ensembles (BDE) in \cite{he2020bayesian}, which modifies infinitely wide networks to derive a GP predictive distribution, NUQLS demonstrates that finite-width, unmodified linearized networks inherently sample from a GP with an NTK kernel.  Similarly, the \textit{local ensembles} method \cite{madras2019detecting}, which perturbs the NN along small-curvature directions to create nearly loss-invariant ensembles, differs from NUQLS in that it relies on second-order information and includes directions with small but non-zero curvature. NUQLS, by contrast, constructs ensembles with exactly the same loss level from using only first-order information, providing a more precise representation of uncertainty. Low-rank approximations used in local ensembles may also poorly capture the true curvature structure, further distinguishing the two approaches.


\begin{table*}[t]
\centering
\caption{Comparing performance of NUQLS, DE, LLA and SWAG on UCI regression tasks. NUQLS performs as well as or better than all other methods, while showing a speed up over other methods; this speed up increases with the size of the datasets. LLA-K denotes LLA with a KFAC covariance structure.}
\label{table:uci_reg}
\vskip 2mm
\begin{tabular}{cccccc}
\hline
\textbf{Dataset}  & \textbf{Method}       & \textbf{RMSE $\downarrow$}                 & \textbf{NLL $\downarrow$}                   & \textbf{ECE $\downarrow$}                  & \textbf{Time}(s)         \\ \hline
\textbf{Energy}   & NUQLS        & \bentry{0.047}{0.006} & \bentry{-2.400}{0.209} & \bentry{0.002}{0.002} & $\mathbf{0.151}$        \\
         & DE           & \entry{0.218}{0.032} & \bentry{-1.651}{0.783}  & \bentry{0.004}{0.002} & $102.244$        \\
         & LLA          & \bentry{0.048}{0.006} & \bentry{-2.475}{0.128} & \bentry{0.004}{0.004} & $0.269$        \\ 
         & SWAG         & \bentry{0.058}{0.015} & \entry{-1.950}{0.158} & \entry{0.080}{0.011} & $37.084$        \\ \hline
\textbf{Kin8nm}   & NUQLS        & \bentry{0.252}{0.005} & \entry{-0.796}{0.025} & \bentry{0.000}{0.000} & $\mathbf{0.264}$        \\
         & DE           & \bentry{0.252}{0.006} & \bentry{-0.914}{0.028}  & \entry{0.002}{0.001} & $73.967$        \\
         & LLA          & \bentry{0.260}{0.010} & \entry{-0.783}{0.054} & \bentry{0.001}{0.001} & $11.966$        \\ 
         & SWAG         & \entry{0.457}{0.149} & \entry{-0.006}{0.295} & \entry{0.054}{0.012} & $150.263$        \\ \hline
\textbf{Protein}  & NUQLS        & \bentry{0.623}{0.005} & \bentry{0.209}{0.047} & \bentry{0.002}{0.000} & $\mathbf{1.356}$        \\
         & DE           & \entry{0.741}{0.052} & \bentry{0.203}{0.203}  & \bentry{0.011}{0.020} & $1014.827$        \\
         & LLA-K          & \entry{0.640}{0.007} & \entry{0.458}{0.071} & \bentry{0.002}{0.000} & $9.506$        \\ 
         & SWAG         & \entry{0.730}{0.044} & \bentry{0.187}{0.080} & \bentry{0.002}{0.002} & $468.972$        \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/violin_plot_uncertainty.pdf}
    \vskip -3mm
    \caption{Violin plot of VMSP, for correctly predicted ID test points, incorrectly predicted ID test points, and OoD test points. Median is shown, with violin width depicting density. Low variance is expected for ID correct points, and large variance for ID incorrect and OoD points. Here FMNIST denotes FashionMNIST, MC denotes MC-Dropout, and BASE denotes Baseline.}
    \label{fig:resnet_variance}
    \vskip -3mm
\end{figure*}


\begin{table*}[t]
    \centering
    \caption{Image classification predictive performance, using LeNet5 on MNIST and FashionMNIST (FMNIST). Experiment was run $5$ times with different random MAP initialisations to get standard deviation on metrics.}
    \label{table:img_class}
    \vskip 2mm
    \begin{tabular}{cccccllc}
        \hline
        \textbf{Datasets}   & \textbf{Method}       & \textbf{NLL $\downarrow$} & \textbf{ACC $\uparrow$} & \textbf{ECE $\downarrow$}   & \textbf{OOD-AUC $\uparrow$}   & \textbf{AUC-ROC $\uparrow$}   & \textbf{Time (s)} \\ \hline
                    & MAP                   & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         & \entry{0.008}{0.001}      & \entry{0.888}{0.008}          & \entry{0.886}{0.008} 
                    & $257$         \\
                    & NUQLS                 & \entry{0.035}{0.002}     & \entry{0.989}{0.001}         &  \bentry{0.003}{0.000}    & \bentry{0.930}{0.026}         & \bentry{0.928}{0.026} 
                    & $106$         \\
\textbf{MNIST}      & DE                    & \entry{0.034}{0.004}      & \bentry{0.991}{0.000}        &  \entry{0.011}{0.004}     & \bentry{0.932}{0.009}          & \bentry{0.928}{0.009} 
                    & $2845$        \\
                    & MC-Dropout            & \entry{0.044}{0.002}      & \entry{0.989}{0.000}         &  \entry{0.017}{0.01}      & \entry{0.873}{0.032}          & \entry{0.871}{0.031} 
                    & $533$         \\
                    & SWAG                  & \bentry{0.029}{0.003}     & \bentry{0.991}{0.000}        &  \bentry{0.004}{0.002}     & \entry{0.902}{0.008}          & \entry{0.900}{0.008} 
                    & $489$         \\
                    & LLA*                   & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         &  \entry{0.008}{0.001}     &  \entry{0.888}{0.008}         & \entry{0.886}{0.008} 
                    & $45$          \\
                    & VaLLA                 & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         &  \entry{0.008}{0.001}     &  \entry{0.889}{0.008}         & \entry{0.886}{0.008} 
                    & $1583$        \\ \hline 
                    
                    & MAP                   & \entry{0.298}{0.007}      & \entry{0.891}{0.3}         & \bentry{0.006}{0.001}      & \entry{0.840}{0.022}          & \entry{0.804}{0.021} 
                    & $158$         \\
                    & NUQLS                 & \entry{0.302}{0.006}     & \entry{0.891}{0.002}         &  \bentry{0.005}{0.002}    & \bentry{0.904}{0.007}         & \bentry{0.870}{0.006} 
                    & $89$         \\
\textbf{FMNIST}     & DE                    & \entry{0.288}{0.002}      & \entry{0.896}{0.001}        &  \entry{0.013}{0.001}     & \entry{0.876}{0.003}          & \entry{0.836}{0.003} 
                    & $1587$        \\
                    & MC-Dropout            & \entry{0.306}{0.007}      & \entry{0.892}{0.003}         &  \entry{0.026}{0.002}      & \entry{0.856}{0.021}          & \entry{0.813}{0.019} 
                    & $291$         \\
                    & SWAG                  & \bentry{0.283}{0.005}     & \bentry{0.899}{0.003}        &  \entry{0.018}{0.002}     & \entry{0.817}{0.023}          & \entry{0.783}{0.022} 
                    & $264$         \\
                    & LLA*                   & \entry{0.298}{0.007}      & \entry{0.891}{0.003}         &  \bentry{0.006}{0.001}     &  \entry{0.841}{0.022}         & \entry{0.805}{0.021} 
                    & $26$          \\
                    & VaLLA                 & \entry{0.298}{0.007}      & \entry{0.891}{0.003}         &  \entry{0.007}{0.001}     &  \entry{0.841}{0.022}         & \entry{0.805}{0.021} 
                    & $1583$        \\ \hline 
    \end{tabular}
    \vskip -4mm
\end{table*}

\section{Experiments}
We now compare the performance of our method with alternatives on various regression and classification tasks. Implementation details are given in \cref{sec:implimentation}.
% The PyTorch implementation of our experiments is available \href{https://anonymous.4open.science/r/nuqls}{here}.
 The PyTorch implementation of our experiments is available \href{https://github.com/josephwilsonmaths/nuqls}{here}.
\subsection{Toy Regression}
We compare the performance of our method on a toy regression problem, taken from \citep{hernandez2015probabilistic} and extended in \citep{park2024density}. In \cref{fig:toy_regression}, we take $20$ uniformly sampled points in the domain $x \in [-4,-2] \cup [2,4]$, and let $y = x^3 + \epsilon, ~ \epsilon \sim \sN(0,3^2)$. A small MLP was trained on these data and used for prediction. We apply VI, SWAG, LA, LLA, DE and NUQLS to the network to find a predictive mean and uncertainty. Close to the training data, i.e. in $[-4,-2] \cup [2,4]$ we expect low uncertainty; outside of this region, the uncertainty should grow with distance from the training points. VI underestimates, while SWAG and LA overestimate the uncertainty. DE grows more uncertain with distance from the training points, however both NUQLS and the LLA contain the underlying target curve within their confidence intervals. Note that deep ensembles output a heteroskedastic variance term, and were trained on a Gaussian likelihood; in comparison, the variances for LLA and NUQLS were computed post-hoc. 

\subsection{UCI Regression}\label{sec:uci_regression}

In \cref{table:uci_reg,table:uci_reg_all}, we compare NUQLS with DE, LLA and SWAG on a series of UCI regression problems. Mean squared error (MSE) and expected calibration error (ECE) respectively evaluate the predictive and UQ performance, with Gaussian negative log likelihood (NLL) evaluating both. See \citep[$\S4.11$]{nemani2023uncertainty} for an explanation of ECE. We see that NUQLS consistently has the (equal) best ECE (except for the Song dataset, where it falls short of the LLA ECE by $\mathit{0.1\%}$). It has comparable or better NLL than other methods on all datasets, and often gives an improvement on RMSE. Finally, it is the quickest method, often by a very significant margin, and it does not \textit{fail} on any datasets, like the other methods do. Note that for the two largest datasets, Protein and Song, we required approximations on the covariance structure of LLA (see \citep{daxberger2021laplace}). A detailed explanation of the hyper-parameter tuning method for NUQLS is given in \cref{sec:appendix:regression_tuning}.

\subsection{Image Classification - Uncertainty} \label{sec:img_class_unc}
In \cref{fig:resnet_variance}, we compare the UQ performance of NUQLS, DE, SWAG, LLA* (LLA with a last-layer and KFAC approximation), and MC-Dropout, on larger image classification tasks. While previous works have measured the uncertainty quantification performance through entropy, ECE, and AUC-ROC, we propose a new metric. The previous measurements are all based on calibration of the softmax output for classification. 
However, the softmax probabilities are not a notion of uncertainty, but are instead a point prediction of class probability; uncertainty should accompany a point prediction, and quantify the spread of possible values. Instead, we propose to use the variance of the maximum predicted softmax probability (VMSP), for a given test point, as a better quantifier of uncertainty. \\

\Cref{fig:resnet_variance} presents a violin plot of the VMSP for three test-groups: correctly predicted in-distribution (FashionMNIST, CIFAR-10) test points, incorrectly predicted in-distribution test points, and out-of distribution (MNIST, CIFAR-100) test points. We would expect that there should be smaller uncertainty for ID test points that have been correctly predicted, and larger uncertainty for incorrectly predicted ID and OoD test points. We compare against a completely randomized baseline method (BASE), where we sample $10$ standard normal realizations of logits, passed through a softmax. In \cref{table:resnet_variance_median_skew} in \cref{sec:img_class_appendix}, we display the corresponding median values for each method in each test group. Ideally, a method should far outperform the baseline, so we can use the median difference between a method and the baseline as a way to compare different methods. We see that NUQLS performs as well as or better than all other methods, including the SOTA method DE. Furthermore, we would like the ID correct distribution to have a positive-skew, while ID incorrect and OoD distributions should have a negative-skew. We also display the sample skew values in \cref{table:resnet_variance_median_skew}. In this metric, our method again performs as well as or better than all other methods. 


\subsection{Image Classification - Predictive} \label{sec:img_class_pred}
We now evaluate the peformance of the novel predictive mean on image classification tasks. We compare NUQLS against the MAP NN, DE, MC-Dropout, SWAG, LLA* and VaLLA, on the MNIST and FashionMNIST datasets, using the LeNet5 network. We compare test cross-entropy (NLL), test accuracy (ACC), ECE, and the AUC-ROC measurement for maximum softmax probability (AUC-ROC) and entropy (OOD-AUC) as the detector. The last two metrics evaluate a methods ability to detect out-of distribution points. We display the results in \cref{table:img_class}. We see that NUQLS performs the best in ECE, AUC-ROC and OOD-AUC, and is competitive in the other metrics, while having the second fastest wall-time.

\section{Conclusion}
We have presented NUQLS, a Bayesian, post-hoc UQ method that approximates the predictive distribution of an over-parameterized NN through GD/SGD training of linear networks, allowing scalability without sacrificing performance. Under assumptions on the loss function, this predictive distribution reduces to a GP using the NTK. We find that our method is competitive with, and often far outperforms, existing UQ methods on regression and classification tasks, whilst providing a novel connection between NNs, GPs and the NTK.
Potential future research could involve extending \cref{theorem:gd_genloss} to broader classes of loss functions and other deterministic and stochastic optimization algorithms. 

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
Fred Roosta was partially supported by the Australian Research Council through an Industrial Transformation Training Centre for Information Resilience (IC200100022).


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

% \bibliography{Bib/biblio}
% \bibliographystyle{Bib/icml2025}

\begin{thebibliography}{76}
    \providecommand{\natexlab}[1]{#1}
    \providecommand{\url}[1]{\texttt{#1}}
    \expandafter\ifx\csname urlstyle\endcsname\relax
      \providecommand{\doi}[1]{doi: #1}\else
      \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
    
    \bibitem[Abdar et~al.(2021)Abdar, Pourpanah, Hussain, Rezazadegan, Liu, Ghavamzadeh, Fieguth, Cao, Khosravi, Acharya, Makarenkov, and Nahavandi]{uncertainty_review}
    Abdar, M., Pourpanah, F., Hussain, S., Rezazadegan, D., Liu, L., Ghavamzadeh, M., Fieguth, P., Cao, X., Khosravi, A., Acharya, U.~R., Makarenkov, V., and Nahavandi, S.
    \newblock A review of uncertainty quantification in deep learning: Techniques, applications and challenges.
    \newblock \emph{Information Fusion}, 76:\penalty0 243--297, 2021.
    \newblock ISSN 1566-2535.
    \newblock \doi{https://doi.org/10.1016/j.inffus.2021.05.008}.
    
    \bibitem[Abdel-Hamid et~al.(2014)Abdel-Hamid, Mohamed, Jiang, Deng, Penn, and Yu]{speech_cnn}
    Abdel-Hamid, O., Mohamed, A.-r., Jiang, H., Deng, L., Penn, G., and Yu, D.
    \newblock Convolutional neural networks for speech recognition.
    \newblock \emph{IEEE/ACM Transactions on audio, speech, and language processing}, 22\penalty0 (10):\penalty0 1533--1545, 2014.
    
    \bibitem[Altieri et~al.(2024)Altieri, Romanelli, Pichler, Alberge, and Piantanida]{beyondTheNorms}
    Altieri, A., Romanelli, M., Pichler, G., Alberge, F., and Piantanida, P.
    \newblock Beyond the norms: Detecting prediction errors in regression models.
    \newblock \emph{Forty-first International Conference on Machine Learning}, 2024.
    
    \bibitem[Antor{\'a}n et~al.(2022)Antor{\'a}n, Padhy, Barbano, Nalisnick, Janz, and Hern{\'a}ndez-Lobato]{antoran2022sampling}
    Antor{\'a}n, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hern{\'a}ndez-Lobato, J.~M.
    \newblock Sampling-based inference for large linear models, with application to linearised laplace.
    \newblock \emph{arXiv preprint arXiv:2210.04994}, 2022.
    
    \bibitem[Bassily et~al.(2018)Bassily, Belkin, and Ma]{bassily2018exponential}
    Bassily, R., Belkin, M., and Ma, S.
    \newblock {On exponential convergence of SGD in non-convex over-parametrized learning}.
    \newblock \emph{arXiv preprint arXiv:1811.02564}, 2018.
    
    \bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and Wierstra]{bbb}
    Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
    \newblock Weight uncertainty in neural network.
    \newblock \emph{International Conference on Machine Learning}, pp.\  1613--1622, 2015.
    
    \bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
    Bubeck, S. et~al.
    \newblock Convex optimization: Algorithms and complexity.
    \newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.
    
    \bibitem[Dadalto et~al.(2023)Dadalto, Romanelli, Pichler, and Piantanida]{dadalto2023data}
    Dadalto, E., Romanelli, M., Pichler, G., and Piantanida, P.
    \newblock A data-driven measure of relative uncertainty for misclassification detection.
    \newblock \emph{arXiv preprint arXiv:2306.01710}, 2023.
    
    \bibitem[Daxberger et~al.(2021)Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{daxberger2021laplace}
    Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
    \newblock Laplace redux -- effortless bayesian deep learning.
    \newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20089--20103, 2021.
    
    \bibitem[De~Bortoli \& Desolneux(2022)De~Bortoli and Desolneux]{de2021quantitative}
    De~Bortoli, V. and Desolneux, A.
    \newblock On quantitative {L}aplace-type convergence results for some exponential probability measures with two applications.
    \newblock \emph{\emph{To appear in the} Journal of Machine Learning Research}, 2022.
    
    \bibitem[Deng et~al.(2013)Deng, Hinton, and Kingsbury]{new_speech_dnn}
    Deng, L., Hinton, G., and Kingsbury, B.
    \newblock New types of deep neural network learning for speech recognition and related applications: An overview.
    \newblock \emph{International Conference on Acoustics, Speech and Signal Processing}, pp.\  8599--8603, 2013.
    
    \bibitem[Deng et~al.(2022)Deng, Zhou, and Zhu]{deng2022accelerated}
    Deng, Z., Zhou, F., and Zhu, J.
    \newblock Accelerated linearized laplace approximation for bayesian deep learning.
    \newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 2695--2708, 2022.
    
    \bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert_google}
    Devlin, J., Chang, M., Lee, K., and Toutanova, K.
    \newblock {BERT:} pre-training of deep bidirectional transformers for language understanding.
    \newblock pp.\  4171--4186. Association for Computational Linguistics, 2019.
    \newblock \doi{10.18653/V1/N19-1423}.
    
    \bibitem[Eschenhagen et~al.(2021)Eschenhagen, Daxberger, Hennig, and Kristiadi]{eschenhagen2021mixtures}
    Eschenhagen, R., Daxberger, E., Hennig, P., and Kristiadi, A.
    \newblock Mixtures of laplace approximations for improved post-hoc uncertainty in deep learning.
    \newblock \emph{arXiv preprint arXiv:2111.03577}, 2021.
    
    \bibitem[Folgoc et~al.(2021)Folgoc, Baltatzis, Desai, Devaraj, Ellis, Manzanera, Nair, Qiu, Schnabel, and Glocker]{folgoc2021mc}
    Folgoc, L.~L., Baltatzis, V., Desai, S., Devaraj, A., Ellis, S., Manzanera, O. E.~M., Nair, A., Qiu, H., Schnabel, J., and Glocker, B.
    \newblock Is mc dropout bayesian?
    \newblock \emph{arXiv preprint arXiv:2110.04286}, 2021.
    
    \bibitem[Foong et~al.(2019)Foong, Li, Hern{\'a}ndez-Lobato, and Turner]{foong2019between}
    Foong, A.~Y., Li, Y., Hern{\'a}ndez-Lobato, J.~M., and Turner, R.~E.
    \newblock 'in-between' uncertainty in bayesian neural networks.
    \newblock \emph{arXiv preprint arXiv:1906.11537}, 2019.
    
    \bibitem[Ford et~al.(2019)Ford, Gilmer, Carlini, and Cubuk]{ford2019adversarial}
    Ford, N., Gilmer, J., Carlini, N., and Cubuk, D.
    \newblock Adversarial examples are a natural consequence of test error in noise.
    \newblock \emph{International Conference on Machine Learning}, 97, 2019.
    
    \bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and Ganguli]{fort2020deep}
    Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli, S.
    \newblock Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel.
    \newblock \emph{Advances in Neural Information Processing Systems}, 34, 2020.
    
    \bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
    Gal, Y. and Ghahramani, Z.
    \newblock Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
    \newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of The 33rd International Conference on Machine Learning}, volume~48 of \emph{Proceedings of Machine Learning Research}, pp.\  1050--1059, New York, New York, USA, 20--22 Jun 2016. PMLR.
    \newblock URL \url{https://proceedings.mlr.press/v48/gal16.html}.
    
    \bibitem[Gardner et~al.(2018)Gardner, Pleiss, Bindel, Weinberger, and Wilson]{gpytorch}
    Gardner, J.~R., Pleiss, G., Bindel, D., Weinberger, K.~Q., and Wilson, A.~G.
    \newblock Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.
    \newblock In \emph{Advances in Neural Information Processing Systems}, 2018.
    
    \bibitem[Garrigos \& Gower(2023)Garrigos and Gower]{garrigos2023handbook}
    Garrigos, G. and Gower, R.~M.
    \newblock Handbook of convergence theorems for (stochastic) gradient methods.
    \newblock \emph{arXiv preprint arXiv:2301.11235}, 2023.
    
    \bibitem[Granese et~al.(2021)Granese, Romanelli, Gorla, Palamidessi, and Piantanida]{granese2021doctor}
    Granese, F., Romanelli, M., Gorla, D., Palamidessi, C., and Piantanida, P.
    \newblock Doctor: A simple method for detecting misclassification errors.
    \newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5669--5681, 2021.
    
    \bibitem[Graves(2011)]{graves2011practical}
    Graves, A.
    \newblock Practical variational inference for neural networks.
    \newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.
    
    \bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{calibrate_temp_scaling}
    Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
    \newblock On calibration of modern neural networks.
    \newblock \emph{International Conference on Machine Learning}, 70:\penalty0 1321--1330, 2017.
    
    \bibitem[He et~al.(2020)He, Lakshminarayanan, and Teh]{he2020bayesian}
    He, B., Lakshminarayanan, B., and Teh, Y.~W.
    \newblock Bayesian deep ensembles via the neural tangent kernel.
    \newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1010--1022, 2020.
    
    \bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
    He, K., Zhang, X., Ren, S., and Sun, J.
    \newblock Deep residual learning for image recognition.
    \newblock \emph{Conference on Computer Vision and Pattern Recognition}, pp.\  770--778, 2016.
    
    \bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and Adams]{hernandez2015probabilistic}
    Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
    \newblock Probabilistic backpropagation for scalable learning of bayesian neural networks.
    \newblock \emph{International Conference on Machine Learning}, pp.\  1861--1869, 2015.
    
    \bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
    Hinton, G.~E. and Van~Camp, D.
    \newblock Keeping the neural networks simple by minimizing the description length of the weights.
    \newblock In \emph{Proceedings of the sixth annual conference on Computational learning theory}, pp.\  5--13, 1993.
    
    \bibitem[Hodgkinson et~al.(2023)Hodgkinson, van~der Heide, Salomone, Roosta, and Mahoney]{hodgkinson2023interpolating}
    Hodgkinson, L., van~der Heide, C., Salomone, R., Roosta, F., and Mahoney, M.~W.
    \newblock The interpolating information criterion for overparameterized models.
    \newblock \emph{arXiv preprint arXiv:2307.07785v1}, 2023.
    
    \bibitem[Hoffmann \& Elster(2021)Hoffmann and Elster]{de_bayesian}
    Hoffmann, L. and Elster, C.
    \newblock Deep ensembles from a bayesian perspective.
    \newblock \emph{arXiv preprint arXiv:2105.13283}, 2021.
    
    \bibitem[Immer et~al.(2021)Immer, Korzepa, and Bauer]{immer2021improving}
    Immer, A., Korzepa, M., and Bauer, M.
    \newblock Improving predictions of bayesian neural nets via local linearization.
    \newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  703--711. PMLR, 2021.
    
    \bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{ntk_jacot}
    Jacot, A., Gabriel, F., and Hongler, C.
    \newblock Neural tangent kernel: Convergence and generalization in neural networks.
    \newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.
    
    \bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
    Johnson, R. and Zhang, T.
    \newblock Accelerating stochastic gradient descent using predictive variance reduction.
    \newblock \emph{Advances in neural information processing systems}, 26, 2013.
    
    \bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
    Karimi, H., Nutini, J., and Schmidt, M.
    \newblock {Linear convergence of gradient and proximal-gradient methods under the Polyak-{\L}ojasiewicz condition}.
    \newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part I 16}, pp.\  795--811. Springer, 2016.
    
    \bibitem[Khan et~al.(2019)Khan, Immer, Abedi, and Korzepa]{khan2020approximate}
    Khan, M. E.~E., Immer, A., Abedi, E., and Korzepa, M.
    \newblock Approximate inference turns deep networks into gaussian processes.
    \newblock \emph{Advances in Neural Information Processing Systems}, 33, 2019.
    
    \bibitem[Kotelevskii et~al.(2022)Kotelevskii, Artemenkov, Fedyanin, Noskov, Fishkov, Shelmanov, Vazhentsev, Petiushko, and Panov]{kotelevskii2022nonparametric}
    Kotelevskii, N., Artemenkov, A., Fedyanin, K., Noskov, F., Fishkov, A., Shelmanov, A., Vazhentsev, A., Petiushko, A., and Panov, M.
    \newblock Nonparametric uncertainty quantification for single deterministic neural network.
    \newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36308--36323, 2022.
    
    \bibitem[Krishnan et~al.(2022)Krishnan, Esposito, and Subedar]{krishnan2022bayesiantorch}
    Krishnan, R., Esposito, P., and Subedar, M.
    \newblock Bayesian-torch: Bayesian neural network layers for uncertainty estimation, January 2022.
    \newblock URL \url{https://github.com/IntelLabs/bayesian-torch}.
    
    \bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{krizhevsky2012imagenet}
    Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
    \newblock Imagenet classification with deep convolutional neural networks.
    \newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.
    
    \bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{lakshminarayanan2017simple}
    Lakshminarayanan, B., Pritzel, A., and Blundell, C.
    \newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
    \newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.
    
    \bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lenet5}
    LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
    \newblock Gradient-based learning applied to document recognition.
    \newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.
    
    \bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{lee2019wide}
    Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J.
    \newblock Wide neural networks of any depth evolve as linear models under gradient descent.
    \newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
    
    \bibitem[Lei \& Wasserman(2014)Lei and Wasserman]{lei2014distribution}
    Lei, J. and Wasserman, L.
    \newblock Distribution-free prediction bands for non-parametric regression.
    \newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 76\penalty0 (1):\penalty0 71--96, 2014.
    
    \bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
    Liu, C., Zhu, L., and Belkin, M.
    \newblock Loss landscapes and optimization in over-parameterized non-linear systems and neural networks.
    \newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0 85--116, 2022.
    
    \bibitem[MacKay(1992)]{mackay1992bayesian}
    MacKay, D.~J.
    \newblock Bayesian interpolation.
    \newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 415--447, 1992.
    
    \bibitem[Maddox et~al.(2019)Maddox, Garipov, Izmailov, Vetrov, and Wilson]{maddox2019simple}
    Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., and Wilson, A.~G.
    \newblock A simple baseline for bayesian uncertainty in deep learning, 2019.
    
    \bibitem[Madras et~al.(2019)Madras, Atwood, and D'Amour]{madras2019detecting}
    Madras, D., Atwood, J., and D'Amour, A.
    \newblock Detecting underspecification with local ensembles.
    \newblock \emph{arXiv preprint arXiv:1910.09573}, 2019.
    
    \bibitem[Malitsky \& Mishchenko(2020)Malitsky and Mishchenko]{mishchenko2020adaptive}
    Malitsky, Y. and Mishchenko, K.
    \newblock Adaptive gradient descent without descent.
    \newblock In \emph{International Conference on Machine Learning}, pp.\  6702--6712. PMLR, 2020.
    
    \bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
    Martens, J. and Grosse, R.
    \newblock Optimizing neural networks with kronecker-factored approximate curvature.
    \newblock In \emph{International conference on machine learning}, pp.\  2408--2417. PMLR, 2015.
    
    \bibitem[Martinsson \& Tropp(2020)Martinsson and Tropp]{martinsson2020randomized}
    Martinsson, P.-G. and Tropp, J.~A.
    \newblock Randomized numerical linear algebra: Foundations and algorithms.
    \newblock \emph{Acta Numerica}, 29:\penalty0 403--572, 2020.
    
    \bibitem[Masegosa(2020)]{masegosa2020learning}
    Masegosa, A.
    \newblock Learning under model misspecification: Applications to variational and ensemble methods.
    \newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 5479--5491, 2020.
    
    \bibitem[Matthews et~al.(2017)Matthews, Hron, Turner, and Ghahramani]{matthews2017sample}
    Matthews, A. G. d.~G., Hron, J., Turner, R.~E., and Ghahramani, Z.
    \newblock Sample-then-optimize posterior sampling for bayesian linear models.
    \newblock \emph{NeurIPS Workshop on Advances in Approximate Bayesian Inference}, 2017.
    
    \bibitem[Meurant(2006)]{meurant2006lanczos}
    Meurant, G.
    \newblock \emph{The Lanczos and Conjugate Gradient Algorithms: From Theory to Finite Precision Computations}.
    \newblock SIAM, 2006.
    
    \bibitem[Miani et~al.(2024)Miani, Beretta, and Hauberg]{miani2024sketched}
    Miani, M., Beretta, L., and Hauberg, S.
    \newblock Sketched lanczos uncertainty score: a low-memory summary of the fisher information.
    \newblock \emph{arXiv preprint arXiv:2409.15008}, 2024.
    
    \bibitem[Mukhoti et~al.(2023)Mukhoti, Kirsch, van Amersfoort, Torr, and Gal]{mukhoti2023deep}
    Mukhoti, J., Kirsch, A., van Amersfoort, J., Torr, P.~H., and Gal, Y.
    \newblock Deep seterministic uncertainty: A new simple baseline.
    \newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  24384--24394, 2023.
    
    \bibitem[Nassif et~al.(2019)Nassif, Shahin, Attili, Azzeh, and Shaalan]{speech_2019_review}
    Nassif, A.~B., Shahin, I., Attili, I., Azzeh, M., and Shaalan, K.
    \newblock Speech recognition using deep neural networks.
    \newblock \emph{IEEE access}, 7:\penalty0 19143--19165, 2019.
    
    \bibitem[Neal(1996)]{neal96a}
    Neal, R.~M.
    \newblock \emph{Bayesian Learning for Neural Networks, Vol. 118 of Lecture Notes in Statistics}.
    \newblock Springer-Verlag, 1996.
    
    \bibitem[Nemani et~al.(2023{\natexlab{a}})Nemani, Biggio, Huan, Hu, Fink, Tran, Wang, Zhang, and Hu]{nemani2023uncertainty}
    Nemani, V., Biggio, L., Huan, X., Hu, Z., Fink, O., Tran, A., Wang, Y., Zhang, X., and Hu, C.
    \newblock Uncertainty quantification in machine learning for engineering design and health prognostics: A tutorial.
    \newblock \emph{Mechanical Systems and Signal Processing}, 205:\penalty0 110796, 2023{\natexlab{a}}.
    
    \bibitem[Nemani et~al.(2023{\natexlab{b}})Nemani, Biggio, Huan, Hu, Fink, Tran, Wang, Zhang, and Hu]{uncertain_health_engineering}
    Nemani, V., Biggio, L., Huan, X., Hu, Z., Fink, O., Tran, A., Wang, Y., Zhang, X., and Hu, C.
    \newblock Uncertainty quantification in machine learning for engineering design and health prognostics: A tutorial.
    \newblock \emph{Mechanical Systems and Signal Processing}, 205:\penalty0 110796, 2023{\natexlab{b}}.
    
    \bibitem[Ortega et~al.(2023)Ortega, Santana, and Hern{\'a}ndez-Lobato]{ortega2023variational}
    Ortega, L.~A., Santana, S.~R., and Hern{\'a}ndez-Lobato, D.
    \newblock Variational linearized laplace approximation for bayesian deep learning.
    \newblock \emph{arXiv preprint arXiv:2302.12565}, 2023.
    
    \bibitem[Papadopoulos et~al.(2002)Papadopoulos, Proedrou, Vovk, and Gammerman]{papadopoulos2002inductive}
    Papadopoulos, H., Proedrou, K., Vovk, V., and Gammerman, A.
    \newblock Inductive confidence machines for regression.
    \newblock In \emph{13th European Conference on Machine Learning}, pp.\  345--356. Springer, 2002.
    
    \bibitem[Park \& Blei(2024)Park and Blei]{park2024density}
    Park, Y. and Blei, D.
    \newblock Density uncertainty layers for reliable uncertainty estimation.
    \newblock \emph{International Conference on Artificial Intelligence and Statistics}, 238:\penalty0 163--171, 2024.
    
    \bibitem[Rasmussen(1997)]{rasmussen1997evaluation}
    Rasmussen, C.~E.
    \newblock \emph{Evaluation of Gaussian processes and other methods for non-linear regression}.
    \newblock PhD thesis, University of Toronto Toronto, Canada, 1997.
    
    \bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{williams2006gaussian}
    Rasmussen, C.~E. and Williams, C. K.~I.
    \newblock \emph{{Gaussian Processes for Machine Learning}}.
    \newblock The MIT Press, 11 2005.
    \newblock ISBN 9780262256834.
    \newblock \doi{10.7551/mitpress/3206.001.0001}.
    \newblock URL \url{https://doi.org/10.7551/mitpress/3206.001.0001}.
    
    \bibitem[Ray(2023)]{ray2023chatgpt}
    Ray, P.~P.
    \newblock Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope.
    \newblock \emph{Internet of Things and Cyber-Physical Systems}, 2023.
    
    \bibitem[Redmon \& Farhadi(2017)Redmon and Farhadi]{YOLO9000}
    Redmon, J. and Farhadi, A.
    \newblock Yolo9000: Better, faster, stronger.
    \newblock \emph{Conference on Computer Vision and Pattern Recognition}, pp.\  7263--7271, 2017.
    
    \bibitem[Redmon et~al.(2016)Redmon, Divvala, Girshick, and Farhadi]{YOLO}
    Redmon, J., Divvala, S., Girshick, R., and Farhadi, A.
    \newblock You only look once: Unified, real-time object detection.
    \newblock \emph{Conference on Computer Vision and Pattern Recognition}, pp.\  779--788, 2016.
    
    \bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
    Ritter, H., Botev, A., and Barber, D.
    \newblock A scalable {L}aplace approximation for neural networks.
    \newblock \emph{International Conference on Learning Representations}, 6, 2018.
    
    \bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{roux2012stochastic}
    Roux, N., Schmidt, M., and Bach, F.
    \newblock A stochastic gradient method with an exponential convergence \_rate for finite training sets.
    \newblock \emph{Advances in neural information processing systems}, 25, 2012.
    
    \bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and Zhang]{shalev2013stochastic}
    Shalev-Shwartz, S. and Zhang, T.
    \newblock Stochastic dual coordinate ascent methods for regularized loss minimization.
    \newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (1), 2013.
    
    \bibitem[Tagasovska \& Lopez-Paz(2019)Tagasovska and Lopez-Paz]{tagasovska2019single}
    Tagasovska, N. and Lopez-Paz, D.
    \newblock Single-model uncertainties for deep learning.
    \newblock \emph{Advances in neural information processing systems}, 32, 2019.
    
    \bibitem[Titsias(2009)]{titsias2009variational}
    Titsias, M.
    \newblock Variational learning of inducing variables in sparse gaussian processes.
    \newblock In \emph{Artificial Intelligence and Statistics}, pp.\  567--574. PMLR, 2009.
    
    \bibitem[Van~der Vaart(2000)]{asymptotic_statistics}
    Van~der Vaart, A.~W.
    \newblock \emph{Asymptotic Statistics}, volume~3.
    \newblock Cambridge University Press, 2000.
    
    \bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{attention}
    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L.~u., and Polosukhin, I.
    \newblock Attention is all you need.
    \newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.
    \newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.
    
    \bibitem[Vovk et~al.(2005)Vovk, Gammerman, and Shafer]{vovk2005algorithmic}
    Vovk, V., Gammerman, A., and Shafer, G.
    \newblock \emph{Algorithmic Learning in a Random World}, volume~29.
    \newblock Springer, 2005.
    
    \bibitem[Xie et~al.(2022)Xie, Tang, Cai, Sun, and Li]{xie2022power}
    Xie, Z., Tang, Q.-Y., Cai, Y., Sun, M., and Li, P.
    \newblock On the power-law hessian spectrums in deep learning.
    \newblock \emph{arXiv preprint arXiv:2201.13011}, 2022.
    
    \bibitem[Yang \& Hu(2021)Yang and Hu]{yang2021feature}
    Yang, G. and Hu, E.~J.
    \newblock Feature learning in infinite-width neural networks.
    \newblock \emph{International Conference on Machine Learning}, 139, 2021.
    
\end{thebibliography}
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs}
\label{sec:appendix:proofs}
\begin{proof}[Proof of \cref{lemma:unique_sol}]
    First, we note that by the assumption on $\ell$, a solution to \cref{eq:ell_range} always exists. Suppose to the contrary that \cref{eq:ell_range} has two distinct solutions $\tilde{\btheta}$ and $\widehat{\btheta}$ such that $\tilde{\btheta} \neq \widehat{\btheta}$. Since $\tilde{\btheta} \in \range([\JJ(\widehat{\btheta},\sX)]^{\T})$ and $\widehat{\btheta} \in \range([\JJ(\widehat{\btheta},\sX)]^{\T})$, i.e., $(\tilde{\btheta} - \widehat{\btheta}) \perp \Null ([\JJ(\widehat{\btheta},\sX)])$, it follows that $\JJ(\widehat{\btheta},\sX)\tilde{\btheta} \neq \JJ(\widehat{\btheta},\sX) \widehat{\btheta}$, which in particular implies $\langle \nabla \ff(\widehat{\btheta}, \xx_{i}), \tilde{\btheta}\rangle \neq \langle\nabla \ff(\widehat{\btheta}, \xx_{i}), \widehat{\btheta}\rangle$ for all $i=1,\ldots,n$. Consider $\bar{\btheta} = (\tilde{\btheta} + \widehat{\btheta})/2$. By strict convexity on $\ell$, we have
    \begin{align*}
        \sum_{i=1}^{n} \ell(\widetilde{\ff}(\bar{\btheta},\xx_{i}) , y_{i}) 
        &= \sum_{i=1}^{n} \ell\Bigg(\ff(\widehat{\btheta}, \xx_{i}) + \dotprod{\nabla \ff(\widehat{\btheta}, \xx_{i}), \frac{\tilde{\btheta} + \widehat{\btheta}}{2} - \widehat{\btheta}} , y_{i}\Bigg) \\
        &= \sum_{i=1}^{n} \ell\Bigg(\frac{\ff(\widehat{\btheta}, \xx_{i}) + \dotprod{\nabla \ff(\widehat{\btheta}, \xx_{i}), \tilde{\btheta} - \widehat{\btheta}}}{2} + \frac{\ff(\widehat{\btheta}, \xx_{i}) + \dotprod{\nabla \ff(\widehat{\btheta}, \xx_{i}), \widehat{\btheta} - \widehat{\btheta}}}{2} , y_{i}\Bigg) \\
        &< \hf \sum_{i=1}^{n} \ell\left(\widetilde{\ff}(\tilde{\btheta},\xx_{i}), y_{i}\right) + \hf \sum_{i=1}^{n} \ell\left(\widetilde{\ff}(\widehat{\btheta},\xx_{i}), y_{i}\right) \\
        &=  \sum_{i=1}^{n} \ell\left(\widetilde{\ff}(\tilde{\btheta},\xx_{i}), y_{i}\right),
    \end{align*}
    which is a contradiction.
\end{proof}

\begin{proof}[Proof of \cref{theorem:gd_genloss}]
\hfill
\begin{itemize}
    \item (Gradient Descent) Denoting
\begin{align*}
    \JJ(\widehat{\btheta},\sX)  \defeq \begin{bmatrix}
        \JJ^{\T}(\widehat{\btheta}, \xx_{1})  \;\ldots\;  \JJ^{\T}(\widehat{\btheta}, \xx_{n})
    \end{bmatrix}^{\T} \in \real^{nc \times p},
\end{align*}
we  can write 
\begin{align*}
    \zz  = \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX) \zz + \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz,
\end{align*}
where $\widehat{\btheta}$ represents the parameters around which the linear model $\widetilde{\ff}$ is defined in \eqref{eq:linearized_one_f}. The first iteration of gradient descent, initialized at $\btheta^{(0)} = \zz$, is given by
\begin{align*}
    \btheta^{(1)} &= \zz - \alpha \sum_{i=1}^{n}  \frac{\partial \widetilde{\ff}}{\partial \btheta} (\zz,\xx_{i}) \nabla \ell(\widetilde{\ff}(\zz,\xx_{i}),\yy_{i})   \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX) \zz  - \alpha \sum_{i=1}^{n}  \frac{\partial \widetilde{\ff}}{\partial \btheta} (\zz,\xx_{i}) \nabla \ell(\widetilde{\ff}(\zz,\xx_{i}),\yy_{i})   \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX) \zz  - \alpha \sum_{i=1}^{n}  \left[\JJ(\widehat{\btheta}, \xx_{i})\right]^{\T} \nabla \ell(\widetilde{\ff}(\zz,\xx_{i}),\yy_{i})   \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \vv^{(1)},
\end{align*}
where $\vv^{(1)} \in \range\left(\left[\JJ(\widehat{\btheta},\sX)\right]^{\T}\right)$.
%
The next iteration is similarly given by
\begin{align*}
    \btheta^{(2)} &= \btheta^{(1)} - \alpha \sum_{i=1}^{n} \frac{\partial \widetilde{\ff}}{\partial \btheta} (\btheta^{(1)},\xx_{i}) \nabla \ell(\widetilde{\ff}(\btheta^{(1)},\xx_{i}),\yy_{i})   \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \vv^{(1)}  - \alpha \sum_{i=1}^{n} \left[\JJ(\widehat{\btheta}, \xx_{i})\right]^{\T} \nabla \ell(\widetilde{\ff}(\btheta^{(1)},\xx_{i}),\yy_{i})   \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \vv^{(1)} + \vv^{(2)},
\end{align*}
where again $\vv^{(2)} \in \range\left(\left[\JJ(\widehat{\btheta},\sX)\right]^{\T}\right)$. Generalizing to the $n$th iteration, 
\begin{align*}
    \btheta^{(n)} &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \sum_{i=1}^n \vv^{(i)}.
\end{align*}
Hence, by the assumption on $\ell$, as long as an adaptive learning rate is chosen appropriately according to  \citet{mishchenko2020adaptive}, GD must converge to a solution of the form
\begin{align*}
    \btheta^{\star} &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \sum_{i=1}^\infty \vv^{(i)} \\
    &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \vv,
\end{align*}
where $\vv \in \range\left(\left[\JJ(\widehat{\btheta},\sX)\right]^{\T}\right)$. In particular, for $\zz = \zero$, by \cref{lemma:unique_sol}, we must have that $\vv = \btheta^{\ddagger}$,  where $\btheta^{\ddagger}$ is the solution to \cref{eq:ell_range}. Therefore, 
\begin{align*}
    \btheta^{\star} &= \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz + \btheta^{\ddagger}.
\end{align*}
\item (Stochastic Gradient Descent) Using a similar argument as above, it is easy to show that each iteration of the mini-batch SGD is of the form 
\begin{align*}
    \btheta^{(n)} \in \left(\eye - \JJ^{\dagger}(\widehat{\btheta},\sX) \JJ(\widehat{\btheta},\sX)\right) \zz +  \range\left(\left[\JJ(\widehat{\btheta},\sX)\right]^{\T}\right).
\end{align*}
Hence, it suffices to show that SGD converges almost surely. 
Defining 
\begin{align*}
    \sL(\btheta) \defeq \sum_{i = 1}^{n} \ell(\widetilde{\ff}(\btheta,\xx_{i}),\yy_{i}), \quad \text{and} \quad 
    \bgg(\btheta,\sX) \defeq \begin{bmatrix}
        \nabla \ell(\widetilde{\ff}(\btheta,\xx_{1}),\yy_{1}) \\ \vdots \\ \nabla \ell(\widetilde{\ff}(\btheta,\xx_{n}),\yy_{n})
    \end{bmatrix} \in \real^{nc},
\end{align*}
we write
\begin{align*}
    \nabla \sL(\btheta) = \sum_{i=1}^{n} \left[\JJ(\widehat{\btheta}, \xx_{1})\right]^{\T} \nabla \ell(\widetilde{\ff}(\btheta,\xx_{i}),\yy_{i}) = \left[\JJ(\widehat{\btheta},\sX)\right]^{\T} \bgg(\btheta,\sX). 
\end{align*}
Let $\btheta^{\star}$ be any solution to \cref{eq:gen_loss_lin}. The full row-rank assumption on $\JJ(\widehat{\btheta},\sX)$ implies that we must have $\bgg(\btheta^{\star},\sX) = \zero $, i.e., $\nabla \ell(\widetilde{\ff}(\btheta^{\star},\xx_{i}),\yy_{i}) = \zero$, $i=1,\ldots,n$. 
%
By the $\mu$-strong convexity assumption on $\ell(.,\yy)$ with respect to its first argument, it is easy to see that $\sL(\btheta)$ satisfies the Polyak-\L{}ojasiewicz inequality \cite{karimi2016linear} with constant $2 \mu \lambda$ where 
\begin{align*}
    \lambda \defeq \min_{i=1,\ldots,n} \sigma^{2}_{\min}(\JJ(\widehat{\btheta}, \xx_{i})),
\end{align*}
and $\sigma_{\min}(\JJ(\widehat{\btheta}, \xx_{i}))$ is the smallest non-zero singular value of $\JJ(\widehat{\btheta}, \xx_{i})$. 
%
Indeed, let $\btheta^{\star}$ be any solution to \cref{eq:gen_loss_lin}. From $\mu$-strong convexity of $\ell(.,\yy)$ with respect to its first argument, for any $\btheta$, we have
\begin{align*}
    \ell(\widetilde{\ff}(\btheta,\xx_{i}),\yy_{i})  - \ell(\widetilde{\ff}(\btheta^{\star},\xx_{i}),\yy_{i}) &\leq \frac{1}{2 \mu} \vnorm{\nabla \ell(\widetilde{\ff}(\btheta,\xx_{i}),\yy_{i})}^{2} \\
    &\leq \frac{1}{2 \mu \sigma_{\min}(\JJ(\widehat{\btheta}, \xx_{i}))} \vnorm{\left[\JJ(\widehat{\btheta}, \xx_{i})\right]^{\T} \nabla \ell(\widetilde{\ff}(\btheta,\xx_{i}),\yy_{i})}^{2},
\end{align*}
which implies
\begin{align*}
    \sL(\btheta) - \sL(\btheta^{\star}) &\leq \frac{1}{2 \mu \lambda} \vnorm{\left[\JJ(\widehat{\btheta},\sX)\right]^{\T} \bgg(\btheta,\sX)}^{2} = \frac{1}{2 \mu \lambda} \vnorm{\nabla \sL(\btheta)}^{2}. \\
\end{align*}
By the smoothness assumption on each $\ell(\widetilde{\ff}(.,\xx_{i}),\yy_{i})$ as well as the interpolating property of $\btheta^{\star}$, \citet[Theorem 1]{bassily2018exponential} implies that the mini-batch SGD with small enough step size $ \eta $ has an exponential convergence rate as
\begin{align*}
    \Ex \sL(\btheta^{(k)})  &\leq (1-\rho)^{k} \sL(\btheta^{(0)}),
\end{align*}
for some contact $0 < \rho < 1$. This in particular implies that for any $\epsilon > 0$,
\begin{align*}
    \sum_{k=1}^{\infty} \Pr(\sL(\btheta^{(k)}) > \epsilon) \leq \sum_{k=1}^{\infty} \frac{\Ex \sL(\btheta^{(k)})}{\epsilon} \leq   \frac{\sL(\btheta^{(0)})}{\epsilon} \sum_{k=1}^{\infty}(1-\rho)^{k} = \frac{\sL(\btheta^{(0)})}{\epsilon \rho} < \infty.
\end{align*}
Now, the BorelCantelli lemma  gives $\sL(\btheta^{(k)}) \to 0$, almost surely.
\end{itemize}
\end{proof}

\section{Related Works: Further Details and Discussions}
\label{sec:appendix:related_work}

NUQLS shares notable similarities with, and exhibits distinct differences from, several prior works, which are briefly discussed below. 

\subsection{Linearized Laplace Approximation (LLA) Framework.}
The popular LLA framework \cite{khan2020approximate, foong2019between, immer2021improving, daxberger2021laplace} shares a close connection with NUQLS, as both methods fundamentally rely on linearizing the network. However, a subtle yet significant distinction lies in their constructions: LLA begins by obtaining a proper distribution over the parameters and then draws parameter samples from it, while NUQLS bypasses this step and directly targets an approximation of the posterior distribution of the neural network. 

As a direct consequence of this, in overparameterized settings, where the Hessian (or its Generalized Gauss-Newton approximation) is not positive definite, the LLA framework necessitates imposing an appropriate prior over the parameters to avoid degeneracy. In sharp contrast, NUQLS directly generates samples from the predictive distribution without introducing any artificial prior, thereby avoiding potential biases that such priors might impose on the covariance structure of the outputs and eliminating the need for additional hyperparameters. Consequently, as the LLA framework corresponds to a Bayesian generalized linear model (GLM), the weight-space vs.\ function-space duality in GLMs implies that its predictive distribution corresponds to a noisy GP with an NTK kernel. On the other hand, NUQLS leads to a noise-free GP. In interpolating regimes, where the model perfectly fits the data, the noise-free setting of NUQLS appears to be more suitable \cite{hodgkinson2023interpolating}.

Another important consequence of this distinction arises in classification tasks. Due to the Laplace approximation, the LLA framework produces an independent GP for each output of the linearized model. In contrast, NUQLS captures the covariance between outputs, offering a more comprehensive representation of the predictive distribution. 

Finally, for regression tasks, NUQLS offers additional flexibility by allowing the variance to be scaled post-hoc by a factor $\gamma$. This enables efficient hyperparameter tuning on a validation set without the need for retraining the model or optimizing a marginal likelihood--a level of flexibility not available in the LLA framework.


NUQLS can be seen as an extension of the ``sample-then-optimize'' framework for posterior sampling of large, finite-width NNs \citep{matthews2017sample}. In this context, the work of \citet{antoran2022sampling}, henceforth referred to as Sampling-LLA, enables drawing samples from the posterior distribution of the LLA in a manner analogous to \cref{alg:ntk_uq_linear}. In this approach, a series of regularized least-squares regression problems are constructed, and the collection of their solutions  is shown to be distributed according to the LLA posterior. An EM algorithm is then employed for hyperparameter tuning.
%
In addition to the fundamental differences between NUQLS and LLA-inspired methods mentioned earlier, Sampling-LLA has notable distinctions from \cref{alg:ntk_uq_linear}. First, the objective functions in Sampling-LLA have non-trivial minimum values. As a result, the convergence of SGD for such problems necessitates either a diminishing learning rate \cite{bubeck2015convex}, which slows down convergence, or the adoption of variance reduction techniques \cite{roux2012stochastic,shalev2013stochastic,johnson2013accelerating}, which can introduce additional computational and memory overhead. By contrast, in overparameterized settings and under the assumptions of \cref{theorem:gd_genloss}, the optimization problem in \cref{eq:gen_loss_lin} allows interpolation. Consequently, SGD can employ a constant step size for convergence, improving optimization efficiency \cite{garrigos2023handbook}.
%
Second, the inherent properties of the LLA framework, which require a positive definite Hessian or its approximation, necessitate regularizing the least-squares term in the subproblem of Sampling-LLA. This results in a strongly convex problem with a unique solution.  Consequently, to generate a collection of solutions, Sampling-LLA constructs a random set of such subproblems, each involving fitting the linearized network to random outputs. These random outputs are sampled from a zero-mean Gaussian, with covariance given by the Hessian of the loss function, evaluated on the data. In contrast, the subproblem of NUQLS, i.e., \cref{eq:gen_loss_lin}, involves directly fitting the training data, and the ensemble of solutions is constructed as a result of random initialization of the optimization algorithm. Hence, the uncertainty captured by NUQLS arises naturally from the variance of solutions in the overparameterized regime, without the need for additional regularization or artificially constructed subproblems.

While the Sampling-LLA method enhances the scalability of LLA, the competing method Variational LLA (VaLLA) \citep{ortega2023variational} offers comparable or superior UQ performance while significantly reducing computation time. VaLLA achieves this by computing the LLA predictive distribution using a variational sparse GP with an NTK. Another competing LLA extension is Accelerated LLA (ELLA), which uses a Nystr\"{o}m approximation of the functional LLA covariance matrix \citep{deng2022accelerated}, and seems to attain similar performance to VaLLA, again at a reduced cost compared to Sampling-LLA. We compare the performance of NUQLS to Sampling-LLA, VaLLA and ELLA in \cref{sec:valla_llas_ella_comp}.

\subsection{Ensemble Framework.}
In \citet{lee2019wide}, infinitely wide neural networks were shown to follow a GP distribution; however, this GP did not correspond to a true predictive distribution. Building on this, \citet{he2020bayesian} introduced a random, untrainable function to an infinitely wide neural network, deriving a GP predictive distribution using the infinite-width NTK. An ensemble of these modified NNs was then interpreted as samples from the GP's predictive posterior. In contrast, our method demonstrates that trained, finite-width, unmodified linearized networks are inherently samples from a GP with an NTK kernel.
While their method, Bayesian Deep Ensembles (BDE), shares some conceptual similarities with ours, we omit it from our experiments for several reasons. Firstly, while it performed well in a toy regression problem in the original paper, it performed poorly on the alternate toy regression problem found in \cref{fig:toy_regression_bde} in \cref{sec:bde_toy}; this is likely due to the infinite-width NTK being a poor approximation in practice. Second, the method is computationally more expensive than DE, particularly for classification tasks, where it requires additional parameter tuning on a validation set. Since our goal is to provide a UQ method that is more computationally efficient than the state-of-the-art SOTA method DE while maintaining competitive performance, BDE is not considered a suitable candidate for comparison.


In \citet{madras2019detecting}, the authors propose a method called ``local ensembles'', which perturbs the parameters of a trained network along directions of small loss curvature to create an ensemble of \textit{nearly} loss-invariant networks. The uncertainty of the original network is then quantified as the standard deviation across predictions in the ensemble. Building on this approach, \citet{miani2024sketched}, in a method called Sketched Lanczos Uncertainty (SLU), use the GGN approximation of the Hessian of the loss to identify these directions and introduce a sketched Lanczos algorithm \citep{meurant2006lanczos} to efficiently compute them. We compare the performance of NUQLS to SLU in \cref{sec:slu_comp}.

While similar, there are key differences between local ensembles and NUQLS. Local ensembles form a subspace of networks that attain \textit{similar} loss values, allowing for directions with small but non-zero curvature, potentially encompassing more directions than those with exactly zero curvature. In contrast, when a solution to the linear optimization problem exists, NUQLS creates an ensemble of networks that all attain \textit{exactly} the same loss. Additionally, NUQLS relies on first-order information to construct this ensemble, whereas local ensembles depend on second-order information.

While the zero-curvature directions of the GGN approximation correspond to the Jacobian's null space, the local ensembles method also includes directions with small but non-zero curvature. This inclusion introduces a notable distinction between the ensembles generated by local ensembles and those formed by NUQLS. Finally, while local ensembles employ low-rank approximations to compute directions efficiently, such approximations may inadequately represent the Hessian and fail to accurately capture the true curvature structure, as highlighted in \citet{xie2022power}. 

\section{Hyper-parameter Tuning}
NUQLS contains several hyper-parameters: the number of linear networks to be trained, the number of epochs and learning rate of training, and the variance of initialisation, $\gamma$. In this section, we discuss strategies to select optimal hyper-parameters.

\subsection{Regression} \label{sec:appendix:regression_tuning}
For regression, and with a sufficiently small learning rate, NUQLS samples from the distribution given in \cref{sec:regression_gp}. We can see that the variance of the predictions scales linearly with $\gamma^2$. Hence, we use the following framework to tune $\gamma$:
\begin{enumerate}
    \item To obtain $\btheta_s$ in \cref{alg:ntk_uq_linear}, we initialise our parameters with a very small gamma, e.g. $\gamma=0.01$. This enables (stochastic) gradient descent to converge quickly with a small learning rate.
    \item We compute $\{\widetilde{\ff}(\btheta^{\star}_{s},\sX_{\text{val}})\}_{s=1}^S$, where $\sX_{\text{val}}$ is the inputs from a validation set. As per \cref{sec:regression_gp}, for each point $\xx \in \sX_{\text{val}}$, we compute the mean prediction as $\mu(x) = \text{SampleMean}\left ( \{\widetilde{\ff}(\btheta^{\star}_{s},\xx)\}_{s=1}^S \right)$ and the variance as $\sigma^2_{\gamma}(x) = \text{SampleVariance} \left (\{\gamma \widetilde{\ff}(\btheta^{\star}_{s},\sX_{\text{val}})\}_{s=1}^S \right)$ (note the scaling by $\gamma$ for only the variance). We then use these values to compute the ECE across the validation dataset $\sD_{\text{val}}$. 
    \item As coverage of a confidence interval scales linearly with the size of the given standard deviation, and our computed standard deviation scales linearly with $\gamma$, we find that the ECE is convex in $\gamma$. Hence, we employ the Ternary search method (see \cref{alg:ternary_search}) to find the value $\hat{\gamma}$ that minimizes this validation ECE.
    \item For a test point $\xx^{\star}$, our mean prediction and variance is then $\mu(\xx^{\star})$ and $\sigma^2_{\hat{\gamma}}(\xx^{\star})$.
\end{enumerate}
As can be seen in \cref{table:uci_reg} and \cref{table:uci_reg_all}, this framework means that for regression our method is incredibly fast and computes well-calibrated variance values. 

\begin{algorithm}
\caption{Ternary Search}\label{alg:ternary_search}
\begin{algorithmic}
\STATE $f$: function to minimize, $l$: left boundary of search space, $r$: right boundary of search space, $\delta$: tolerance, iter: iterations. 
\STATE $i = 0$
\WHILE{$|l - r| \geq \delta$ and $i < $iter}

    \STATE $l_{1/3} = l + (r - l)/3$
    \STATE $r_{1/3} = r - (r - l)/3$
    \IF{$f(l_{1/3}) < f(r_{1/3})$}
        \STATE $l = l_{1/3}$
    \ELSE
        \STATE $r = r_{1/3}$
    \ENDIF
\ENDWHILE
\RETURN $(l + r)/2$
\end{algorithmic}
\end{algorithm}

\subsection{Classification}
For classification, with cross-entropy loss, there is no principled connection to a GP, nor a clean calibration method as in \cref{sec:appendix:regression_tuning}. Instead, we propose some heuristics for how to choose the hyper-parameters of NUQLS. To accomplish this, we train a LeNet model on both MNIST and FashionMNIST, and run NUQLS on the trained model for a variety of learning rates, epochs and values of $\gamma$. We measure training loss of NUQLS, accuracy, ECE, AUC-ROC and OOD-AUC, as well as the training loss of the NN. We repeat this for $5$ random initialisations, and plot the average results in \cref{fig:gamma_performance_comp}. 

Looking at \cref{fig:gamma_performance_comp}, we see that for loss values under the training loss of the original NN, performance is quite similar. Furthermore, for loss values over the training loss of the NN, a higher learning rate and number of epochs seems to equate to better performance. Finally, greater loss values, up to a threshold, seem to increase the OoD detection performance (measured by AUC-ROC and OOD-AUC). Greater loss values may equate to greater deviation in the row-space components of the ensemble solutions; this extra variance may help uncertainty quantification performance, though calibration for ID points seems to decrease (as measured by ECE).

We propose the following heuristic for hyper-parameter selection:
\begin{itemize}
    \item For a set learning rate and number of epochs, choose the largest $\gamma$ such that the NUQLS training loss is less than the NN training loss.
    \item Parameter combinations such that the NUQLS training loss is \textit{moderately} greater than the NN training loss may be better for uncertainty quantification purposes, thought the predictive calibration may be worse. 
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/combined_152_gamma.pdf}
    \caption{Comparison of NUQLS performance as a function of learning rate, epochs and standard deviation of initialisation $\gamma$. We train a LeNet model on (top) MNIST and (bottom) FashionMNIST, and compute a range of metrics. The results are averaged over $5$ random seeds. The blue horizontal line corresponds to the final training loss of the LeNet model.}
    \label{fig:gamma_performance_comp}
\end{figure}

\section{Further Experimental Results}

\subsection{Comparison to SLU}\label{sec:slu_comp}
Here we compare the performance of our method against the competing SLU method. See \cref{sec:appendix:related_work} for a discussion of the differences between NUQLS and SLU. Due to the extensive experimental details given in \citep{miani2024sketched}, we run certain experiments from this work and report the performance of NUQLS against the SLU results found in \citep{miani2024sketched}. In \cref{table:slu_comp}, we compare NUQLS against SLU on OoD detection using the AUC-ROC metric, on a smaller single-layer MLP and a larger LeNet model. The MLP is trained on the MNIST dataset, while the LeNet model is trained on the FashionMNIST dataset. For MNIST, the OoD datasets are FashionMNIST, KMNIST, and a Rotated MNIST dataset, where for each experiment run, we compute the average AUC-ROC score over a range of rotation angles $(15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180)$. For FashionMNIST, the OoD datasets are MNIST, and the average Rotated FashionMNIST dataset. The AUC-ROC metric was calculated using the variance of the logits, summed over the classes for each test point, as a score. We observe that NUQLS has a better AUC-ROC value over all OoD datasets, often by a significant margin. 


\begin{table}[]
\caption{Comparing performance of NUQLS against SLU method. Metric given is the AUC-ROC, computed using the variance of the logits, summed over the classes for each test point, as a score. AUC-ROC measures ability of a method to differentiate between ID and OoD points. We see that NUQLS out-performs SLU in these experiments.}
\label{table:slu_comp}
\vskip 0.15in
\centering
\begin{tabular}{c|ccc|cc}
\textbf{Model}    & \multicolumn{3}{c|}{\textbf{MLP $p = 15k$}}                         & \multicolumn{2}{c}{\textbf{LeNet $p = 40k$}} \\
\textbf{ID Data}  & \multicolumn{3}{c|}{\textbf{MNIST vs}}                            & \multicolumn{2}{c}{\textbf{FashionMNIST}}  \\
\textbf{OoD Data} & \textbf{FashionMNIST} & \textbf{KMNIST}     & \textbf{Rotation (avg)}   & \textbf{MNIST}        & \textbf{Rotation (avg)}  \\ \hline
SLU               &  \entry{0.26}{0.02}   & \entry{0.42}{0.04}  &  \entry{0.59}{0.02}       &  \bentry{0.94}{0.01}   &  \entry{0.74}{0.03}     \\
NUQLS             &  \bentry{0.67}{0.07}   & \bentry{0.79}{0.02}  &  \bentry{0.74}{0.01}       &  \bentry{0.95}{0.02}   &  \bentry{0.91}{0.01}     
\end{tabular}
\end{table}

\subsection{Comparison to Sampling-LLA, VaLLA and ELLA}\label{sec:valla_llas_ella_comp}
We now compare NUQLS against Sampling-LLA, VaLLA and ELLA. Due to issues with convergence, performance, memory usage and package compatibility when either running source code or implementing methods from instructions given in \citep{antoran2022sampling} and \citep{ortega2023variational}, we instead compare NUQLS against the results for Sampling-LLA, VaLLA and ELLA taken verbatim from \citep[Figure 3]{ortega2023variational}. We train a $2$-layer MLP, with $200$ hidden-units in each layer, on the MNIST dataset, according to the experimental details given in \citep{ortega2023variational}. We then compare the accuracy, NLL, ECE, Brier score and the OOD-AUC metric of NUQLS against those reported for Sampling-LLA, VaLLA and ELLA. The results are shown in \cref{table:valla_llas_comp}. We display the mean and standard deviation for NUQLS; as is quoted in \citep{ortega2023variational}, the standard deviation for the other methods was below $10^{-4}$ in magnitude, and was thus omitted. We see that NUQLS outperforms Sampling-LLA, VaLLA and ELLA in accuracy, NLL, ECE, and Brier score, and is within one-sixth of a standard deviation of the leading OOD-AUC value, attained by Sampling-LLA. While we cannot directly comment on differences in computation time between our method and these LLA extensions, we were able to run the source code for VaLLA for the experiments in \cref{table:img_class}, where NUQLS was an order-of-magnitude faster than VaLLA in wall-time. In \citep[Figure 3 (right)]{ortega2023variational}, we also observe that VaLLA is an order-of-magnitude faster than both Sampling-LLA and ELLA. We also compare with \citep[Table 1]{ortega2023variational}, where a ResNet20 model is trained on CIFAR-10. The results are displayed in \cref{table:valla_llas_comp_resnet}. We see that NUQLS is competitive with all other methods in this setting. \\


\begin{table}[]
\caption{Comparing performance of NUQLS against Sampling-LLA, VaLLA and ELLA on MNIST, trained used a 2-layer MLP with $200$ hidden units in each layer, and $\tanh$ activation. Original results taken from \citep{ortega2023variational}.}
\label{table:valla_llas_comp}
\vskip 0.15in
\centering
\begin{tabular}{l|ccccc}
\hline
\textbf{Model} & \textbf{ACC}       & \textbf{NLL}          & \textbf{ECE}              & \textbf{BRIER}            & \textbf{OOD-AUC} \\ \hline
ELLA           & $97.6$             & $0.076$               & $0.008$                   & $0.036$                   & $0.905$          \\
Sampled LLA    & $97.6$             & $0.087$               & $0.026$                   & $0.040$                   & $\mathbf{0.954}$          \\
VaLLA 100      & $97.7$             & $0.076$               & $0.010$                   & $0.036$                   & $0.916$          \\
VaLLA 200      & $97.7$             & $0.075$               & $0.010$                   & $0.035$                   & $0.921$          \\
NUQLS          & \bentry{98.0}{0.1}  & \bentry{0.065}{0.003}  & \bentry{0.005}{0.001}      & \bentry{0.031}{0.001}      & \bentry{0.953}{0.006}          \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\caption{Comparing performance of NUQLS against Sampling-LLA, VaLLA and ELLA on CIFAR-10, with ResNet20. Original results taken from \citep{ortega2023variational}.}
\label{table:valla_llas_comp_resnet}
\vskip 0.15in
\centering
\begin{tabular}{l|ccccc}
\hline
\textbf{Model} & \textbf{ACC}       & \textbf{NLL}          & \textbf{ECE}  \\ \hline
ELLA           & $92.5$             & $0.233$               & $0.009$       \\
Sampled LLA    & $92.5$             & $0.231$               & $\mathbf{0.006}$  \\
VaLLA      & $\mathbf{92.6}$        & $\mathbf{0.228}$      & $0.007$           \\
NUQLS          & \entry{92.4}{0.1}  & \entry{0.236}{0.001}  & \entry{0.008}{0.001}     \\ \hline
\end{tabular}
\end{table}

\label{sec:appendix:exp}
\subsection{Toy Regression BDE}\label{sec:bde_toy}
\cref{fig:toy_regression_bde} displays the performance of BDE on the toy regression problem from \cref{fig:toy_regression}. 
\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/UncertaintyComparison_bde.pdf}
    \caption{Comparison of BDE, DE and NUQLS on the toy regression problem from \cref{fig:toy_regression}. We can see that the uncertainty of the BDE method is quite small. }
    \label{fig:toy_regression_bde}
\end{figure*}

\subsection{UCI Regression}\label{sec:uci_regression_appendix}
We present the results for select UCI regression datasets in \cref{table:uci_reg} in the main body; we present results for several more datasets in \cref{table:uci_reg_all}. 

\begin{table*}[h]
\centering
\caption{Comparing performance of NUQLS, DE, LLA and SWAG on UCI regression tasks. NUQLS performs as well as or better than all other methods, while showing a speed up over other methods; this speed up increases with the size of the datasets. Note that for the Song dataset, the LLA method uses a diagonal covariance structure due to memory constraints: this is denoted as LLA-D.}
\label{table:uci_reg_all}
\vskip 0.15in
\begin{tabular}{cccccc}
\hline
\textbf{Dataset}  & \textbf{Method}       & \textbf{RMSE $\downarrow$}                 & \textbf{NLL $\downarrow$}                   & \textbf{ECE $\downarrow$}                  & \textbf{Time}(s)         \\ \hline
\textbf{Concrete} & NUQLS        & \bentry{0.330}{0.047} & \bentry{-0.316}{0.501} & \bentry{0.003}{0.001} & $\mathbf{0.185}$        \\
         & DE           & \entry{0.379}{0.019} & \bentry{-0.574}{0.098}  & \bentry{0.002}{0.002} & $29.047$        \\
         & LLA          & \bentry{0.333}{0.050} & \bentry{-0.294}{0.479} & \bentry{0.003}{0.002} & $0.297$        \\ 
         & SWAG         & \bentry{0.334}{0.050} & \bentry{-0.562}{0.224} & \entry{0.009}{0.006} & $36.262$        \\ \hline
\textbf{Naval}    & NUQLS        & \bentry{0.049}{0.012} & \bentry{-2.546}{0.134} & \bentry{0.002}{0.002} & $\mathbf{0.295}$        \\
         & DE           & \entry{0.076}{0.006} & \entry{-1.761}{0.250}  & \entry{0.093}{0.040} & $96.570$        \\
         & LLA          & \entry{0.070}{0.022} & \entry{25.292}{17.570} & \entry{0.192}{0.029} & $129.659$        \\ 
         & SWAG         & \entry{1.130}{1.500} & \entry{0.303}{1.091} & \entry{0.084}{0.022} & $92.662$        \\ \hline
\textbf{CCPP}     & NUQLS        & \entry{0.244}{0.008} & \entry{-0.885}{0.020} & \bentry{0.000}{0.000} & $\mathbf{0.174}$        \\
         & DE           & \bentry{0.227}{0.006} & \bentry{-1.009}{0.041}  & \bentry{0.002}{0.003} & $79.791$        \\
         & LLA          & \entry{0.243}{0.007} & \entry{29.420}{4.565} & \entry{0.163}{0.008} & $32.048$        \\ 
         & SWAG         & \entry{0.252}{0.012} & \entry{-0.849}{0.038} & \bentry{0.001}{0.002} & $66.833$        \\ \hline
\textbf{Wine}     & NUQLS        & \bentry{0.789}{0.042} & \bentry{0.284}{0.066} & \bentry{0.001}{0.000} & $\mathbf{0.115}$        \\
         & DE           & \bentry{0.789}{0.041} & \bentry{0.320}{0.109}  & \bentry{0.001}{0.001} & $13.241$        \\
         & LLA          & \bentry{0.792}{0.041} & \entry{1.012}{0.182} & \entry{0.009}{0.004} & $0.340$        \\ 
         & SWAG         & \bentry{0.798}{0.038} & \bentry{0.367}{0.103} & \entry{0.005}{0.003} & $11.807$        \\ \hline
\textbf{Yacht}    & NUQLS        & \bentry{0.042}{0.013} & \bentry{-1.561}{2.319} & \bentry{0.012}{0.010} & $\mathbf{0.164}$        \\
         & DE           & \entry{0.647}{0.121} & \entry{-2.032}{0.349}  & \bentry{0.016}{0.008} & $40.132$        \\
         & LLA          & \bentry{0.043}{0.014} & \bentry{-2.733}{0.468} & \bentry{0.011}{0.006} & $0.177$        \\ 
         & SWAG         & \bentry{0.044}{0.014} & \entry{-2.565}{0.118} & \entry{0.067}{0.025} & $15.822$        \\ \hline
\textbf{Song}     & NUQLS        & \bentry{0.839}{0.014} & \entry{0.646}{0.056} & \entry{0.001}{0.000} & $\mathbf{91.673}$        \\
         & DE           & \bentry{0.846}{0.006} & \bentry{0.180}{0.013}  & \entry{0.005}{0.000} & $2562.789$        \\
         & LLA-D          & \bentry{0.851}{0.029} & \entry{0.456}{0.093} & \bentry{0.000}{0.000} & $210.429$        \\ 
         & SWAG         & \entry{0.845}{0.002} & \entry{0.680}{0.062} & \entry{0.003}{0.001} & $3274.440$        \\ \hline
\end{tabular}
\end{table*}

\subsection{eNUQLS}
In \cref{fig:resnet_variance_ensemble} we demonstrate the performance of an ensembled version of NUQLS, eNUQLS. This method is similar to a Mixture of Laplace Approximations \cite{eschenhagen2021mixtures}. We observe excellent separation of the variances between correct and incorrect/OoD test groups for eNUQLS, especially for CIFAR-10 on ResNet9. Note that there is significant cost to ensembling our method, and we provide this figure simply to illustrate performance capacity.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/violin_plot_uncertainty_eNUQLs.pdf}
    \vskip 0.0in
    \caption{Violin plot of VMSP, with an ensembled version of NUQLS, eNUQLS, included.}
    \label{fig:resnet_variance_ensemble}
\end{figure*}

\subsection{Image Classification}\label{sec:img_class_appendix}
The median and sample skew for the VMSP in \cref{fig:resnet_variance} is found in \cref{table:resnet_variance_median_skew}.

\begin{table*}[]
    \centering
    \caption{Sample median and sample skew of variance from \cref{fig:resnet_variance}. IDC = ID correct, IDIC = ID incorrect, FMNIST = FashionMNIST. The median and skew for a method is compared against a baseline method. We expect positive skew for the ID correct (IDC) variances, and a negative skew for the ID incorrect (IDIC) and OoD variances. If the median or skew for a method is worse than the corresponding baseline, then it is written in gray.}
    \label{table:resnet_variance_median_skew}
    \vskip 0.15in
    \begin{tabular}{cc|cccccc}
    \hline
    \multicolumn{2}{c|}{\textbf{Median}}     & NUQLS                 & DE                    & SWAG                  & MC                                & LLA                       & BASE \\ \hline
    \textbf{ResNet9}     & \textbf{IDC $\downarrow$}  & $2.45 \times 10^{-8}$ & $1.31 \times 10^{-5}$ & $4.64 \times 10^{-7}$ & $5.05 \times 10^{-7}$             & $9.02 \times 10^{-10}$    &  $0.020$     \\
    \textbf{FMNIST}      & \textbf{IDIC $\uparrow$}   & $0.041$               & $0.040$               & $0.048$               & $\color{gray}{0.004}$               & $\color{gray}{0.004}$       &  $0.020$     \\ 
                        & \textbf{OoD $\uparrow$}    & $0.119$               & $0.109$               & $0.075$               & $\color{gray}{0.004}$               & $\color{gray}{0.009}$       &  $0.020$     \\ \hline
    \textbf{ResNet50}    & \textbf{IDC $\downarrow$}  & $0.00$                & $8.29 \times 10^{-8}$ & $0.018$               & $1.49 \times 10^{-9}$             & $1.70 \times 10^{-7}$     & $0.019$      \\
    \textbf{CIFAR-10}    & \textbf{IDIC $\uparrow$}   & $0.178$               & $0.120$               & $0.123$               & $\color{gray}{1.32 \times 10^{-4}}$ & $\color{gray}{0.008}$       &  $0.020$ \\
                        & \textbf{OoD  $\uparrow$}   & $0.178$               & $0.106$               & $0.134$               & $\color{gray}{6.74 \times 10^{-5}}$ & $\color{gray}{0.004}$       & $0.020$      \\ \hline
    \textbf{ResNet50}    & \textbf{IDC $\downarrow$}  & $1.00 \times 10^{-8}$ & $8.73 \times 10^{-4}$ & $0.0449$              & $1.12 \times 10^{-4}$             & $3.25 \times 10^{-4}$     & $0.020$      \\
    \textbf{CIFAR-100}   & \textbf{IDIC $\uparrow$}   & $0.211$               & $0.0624$               & $0.101$              & $\color{gray}{5.93 \times 10^{-3}}$ & $0.0225$                  &  $0.020$ \\
                        & \textbf{OoD  $\uparrow$}   & $0.214$               & $0.0665$               & $0.0956$             & $\color{gray}{5.46 \times 10^{-3}}$ & $\color{gray}{0.0199}$      & $0.020$   \\ \hline
    \multicolumn{2}{c|}{} \\
    \hline
    \multicolumn{2}{c|}{\textbf{Sample Skew}} \\
    \hline
    \textbf{ResNet9}        & \textbf{IDC $\uparrow$}   & $4.35$    & $3.51$    & $3.72$        & $4.92$                & $6.46$            & $1.01$ \\
    \textbf{FMNIST}         & \textbf{IDIC $\downarrow$}& $0.935$   & $0.928$   & $0.378$       & $\color{gray}{1.60}$    & $\color{gray}{1.18}$& $1.11$   \\
                            & \textbf{OoD $\downarrow$} & $0.048$   & $0.321$   & $0.076$       & $\color{gray}{1.69}$    & $0.926$           & $1.11$ \\ \hline
    \textbf{ResNet50}       & \textbf{IDC $\uparrow$}   & $2.97$    & $2.68$    & $4.02$        & $4.55$                & $5.14$            & $1.14$\\
    \textbf{CIFAR-10}       & \textbf{IDIC $\downarrow$}& $-0.15$   & $-0.05$   & $0.66$        & $\color{gray}{2.45}$    & $0.816$           &  $1.09$ \\
                            & \textbf{OoD $\downarrow$} & $0.02$    & $-0.01$   & $0.77$        & $\color{gray}{2.72}$    & $0.96$            & $1.05$   \\ \hline
    \textbf{ResNet50}       & \textbf{IDC $\uparrow$}   & $1.08$    & $1.48$    & $\color{gray}{0.508}$       & $2.87$                & $1.8$            & $1.07$\\
    \textbf{CIFAR-100}      & \textbf{IDIC $\downarrow$}& $-1.15$   & $0.241$   & $-0.47$       & $\color{gray}{2.05}$    & $0.228$           &  $1.09$ \\
                            & \textbf{OoD $\downarrow$} & $-1.37$    & $0.269$   & $-0.233$     & $\color{gray}{1.85}$    & $0.217$            & $1.13$   \\ \hline
    \end{tabular}
\end{table*}


\section{Experiment Details} \label{sec:implimentation}
All experiments were run either on an Intel i7-12700 CPU (toy regression), or on an H100 80GB GPU (UCI regression and image classification). Where multiple experiments were run, mean and standard deviation were presented.
\subsection{Toy Regression}
We use a 1-layer MLP, with a width of $50$ and SiLU activation. For the maximum a posteriori (MAP) network, we train for $10000$ epochs, with a learning rate of $0.001$, using the Adam optimizer and the PyTorch polynomial learning rate scheduler, with parameters total$\_$iters $=$ epochs, power $=~0.5$. For DE, each network in the ensemble outputs a heteroskedastic variance, and is trained using a Gaussian NLL, with $2000$ epochs and a learning rate of $0.05$. We combine the predictions of the ensembles as per \citep{lakshminarayanan2017simple}. Both DE and NUQLS use $10$ realizations. The $\gamma$ hyper-parameter in NUQLS is set to $5$, and each linear realization is trained for $1000$ epochs with a learning rate of $0.001$, using SGD with a momentum parameter of $0.9$. In SWAG, the MAP network is trained for a further $10000$ epochs, using the same learning rate, and the covariance is formed with a rank-$10$ approximation. A prior precision of $0.1$ and $1$ is used for LLA and LA respectively, as well as the full covariance matrix. The variational inference method used is Bayes By Backprop \citep{bbb}, as deployed in the Bayesian Torch package \citep{krishnan2022bayesiantorch}. The prior parameters are ($\mu = 0, \sigma = 1$), and the posterior is initialized at ($\mu = 0, \rho = -3$). For SWAG, LLA, LA and VI, $1000$ MC sample were taken at test time. These design choices gave the best performance for this problem.

\subsection{UCI Regression} \label{sec:uci_reg_details}
\begin{table}[]
\centering
\caption{Training procedure for UCI regression results in \cref{table:uci_reg} and \cref{table:uci_reg_all}.}
\label{table:uci_reg_procedure}
\vskip 0.15in
\begin{tabular}{l|ccccccccc}
\textbf{NN}      & \textbf{Energy} & \textbf{Concrete} & \textbf{Kin8nm} & \textbf{Naval} & \textbf{CCPP} & \textbf{Wine} & \textbf{Yacht} & \textbf{Protein} & \textbf{Song} \\ \hline
Learning Rate     & $1e-2$          & $1e-2$            & $1e-2$          & $1e-2$         & $1e-2$        & $1e-2$        & $1e-2$         & $1e-2$           & $1e-2$      \\
Epochs            & $1500$          & $1000$            & $500$           & $150$          & $100$         & $100$         & $1000$         & $250$            & $50$      \\
Weight Decay      & $1e-5$          & $1e-5$            & $1e-5$          & $1e-4$         & $1e-5$        & $1e-4$        & $1e-5$         & $1e-4$           & $0$      \\
Optimizer         & Adam            & Adam              & SGD             & SGD            & Adam          & SGD           & Adam           & SGD              & SGD      \\
Scheduler         & PolyLR          & PolyLR            & None            & None           & PolyLR        & None          & PolyLR         & None             & None      \\
MLP Size          & $[150]$         & $[150]$           & $[100,100]$     & $[150,150]$    & $[100,100]$   & $[100]$       & $[100]$        & $[150,200,150]$ &$[1000,1000$      \\
                  &                &                    &                 &                &               &               &                &                  & $,500,50]$       \\
\textbf{NUQLS}    &                &                    &                 &                &               &               &                &                  &            \\ \hline
Learning Rate     & $1e-2$         & $1e-2$             & $1e-2$          & $1e-2$         & $1e-2$        & $1e-2$        & $1e-2$         & $1e-2$           & $1e-3$      \\
Epochs            & $150$          & $100$              & $50$            & $15$           & $10$          & $10$          & $100$          & $25$             & $10$      \\
\\
\textbf{DE}       &                &                    &                 &                &               &               &                &                  &            \\ \hline
Learning Rate     & $1e-3$         & $1e-3$             & $1e-2$          & $1e-3$         & $1e-2$        & $1e-2$        & $1e-2$         & $1e-2$           & $1e-2$      \\
Epochs            & $1500$         & $300$              & $100$           & $100$          & $100$         & $100$         & $1000$         & $250$            & $50$      \\
Optimizer         & Adam           & Adam               & Adam            & Adam           & Adam          & Adam          & Adam           & Adam             & Adam      \\
Scheduler         & None           & None               & Cosine          & None           & None          & None          & Cosine         & None             & None      \\
                  &                &                    &                 &                &               &               &                &                  &             \\
\textbf{Experiment}    &                &                    &                 &                &               &               &                &                  &            \\ \hline
No. experiments   & $10$            & $10$              & $10$            & $10$           & $10$          & $10$          & $10$           & $10$             & $3$      \\
\end{tabular}
\end{table}

We now provide the experimental details for the UCI regression experiments (as seen in \cref{table:uci_reg} and \cref{table:uci_reg_all}). For each dataset, we ran a number of experiments to get a mean and standard deviation for performance metrics. In each experiment, we took a random $70\%/15\%/15\%$ split of the dataset for training, testing, and validation. The training hyper-parameters for the MAP, DE and NUQLS networks, size of the MLP used, and the number of experiments conducted for each dataset can be found in \cref{table:uci_reg_procedure}.
\begin{itemize}
    \item \textbf{NN:} For the PolyLR learning rate scheduler, a PyTorch polynomial learning rate scheduler was used, with parameters total$\_$iters=$10\times$epochs, power$=0.5$. The MLP used a $\tanh$ activation, so as to have smooth gradients. MLP weights were initialized as Xavier normal, and bias as standard normal. The dataset was normalized, so that the inputs and the outputs each had zero mean and unit standard deviation.
    \item \textbf{NUQLS:} The linear networks were trained using (S)GD with Nesterov momentum parameter $0.9$. For all datasets except for Song, the full training Jacobian could be stored in memory; this made training extremely fast. For the Song dataset, we trained all linear networks in parallel, by explicitly computing the gradient using JVPs and VJPs. The number of linear networks used was $10$ across all datasets, and the $\gamma$ hyper-parameter was kept at $0.01$.
    \item \textbf{DE:} Each member of the ensemble output a separate heteroskedastic variance, and was trained to minimise the Guassian negative log likelihood. The ensemble weights were also initialized as Xavier normal, and bias as standard normal. The number of ensemble members was kept at $10$.
    \item \textbf{LLA:} LLA requires two parameters for regression: a dataset noise parameter, and a prior variance on the parameters \citep{foong2019between}. To find the noise parameter, a grid search over $10$ values on a log-scale between $1e-2$ and $1e2$ was used to find the noise that minimized the Gaussian likelihood of the validation set, with the LLA mean predictor as the Gaussian mean. The same grid search was used to find the prior variance, in order to minimize the expected calibration error (ECE) of LLA on the validation set. For the Protein dataset, a Kronecker-Factored Curvature (KFAC) covariance structure was used \citep{immer2021improving}, and for the Song dataset a diagonal covariance structure was used. For all other datasets, LLA used the full covariance structure. The predictive distribution was computed with $1000$ MC samples.
    \item \textbf{SWAG:} SGD was used, and the learning rate and number of epochs was kept the same as the NN. We used a grid-search for weight decay, over the values $[0, 0.005, 0.00005]$, to minimize the ECE on the validation set. The rank of the covariance matrix was $10$ for all datasets, and the predictive distribution used $1000$ MC samples.
\end{itemize}

\subsection{Image Classification} \label{sec:image_implimentation}
We display the training procedure in \cref{table:image_class_procedure} for both \cref{fig:resnet_variance} and \cref{table:img_class}. For MNIST and FashionMNIST, we took a $5:1$ training/validation split of the training data. For CIFAR-10, we simply used the test data as a validation set. For CIFAR-10, random horizontal crop and flip on the training images was used as regularization. 
\begin{itemize}
    \item \textbf{NN:} We chose the training procedures to provide the best MAP performance. All networks have weights initialized as Xavier uniform. For SGD, a momentum parameter of $0.9$ was used. For the Cosine Annealing learning rate scheduler, the maximum epochs was set to the training epochs.
    \item \textbf{NUQLS:} The number of samples was kept to $10$ for all datasets.
    \item \textbf{DE:} Similary, $10$ ensemble members were used for all datasets. 
    \item \textbf{MC-Dropout:} Dropout was applied to the network before the last fully connected layer. The dropout probability was set to $0.1$ (a larger probability of $0.25$ was also used, but it did not change the result). At test time, $100$ MC-samples were taken.
    \item \textbf{SWAG:} The network was trained for a further $1 \times$ training epochs, at $1e2 \times $learning rate. The covariance rank was set at $10$. At test time, $100$ MC-samples were taken.
    \item \textbf{LLA*:} We used a last-layer KFAC approximation to the covariance. The prior precision was found through a grid search over $20$ values on a log scale from $1e-2$ to $1e2$, using the probit approximation to the predictive, and a validation set. This configuration for LLA is what is recommended in \citet{daxberger2021laplace}. We used $1000$ samples, to remedy the large amount of approximations used. 
    \item \textbf{VaLLA}: We used the implimentation found in \citep{ortega2023variational}. We kept nearly all hyper-parameters the same as in the MNIST and FashionMNIST experiments in \citep{ortega2023variational}; however, we reduced the number of iterations to $5000$, due to time constraints. We were unable to run this implimentation for ResNet9 or ResNet50, due to an out-of-memory error. 
\end{itemize}

\begin{table}[]
\centering
\caption{Training procedure for image classification results in \cref{table:img_class} and \cref{fig:resnet_variance}.}
\label{table:image_class_procedure}
\vskip 0.15in
\begin{tabular}{l|cccccc}
\textbf{MAP}      & \textbf{LeNet5 MNIST}   & \textbf{Lenet5 FMNIST}    & \textbf{ResNet9 FMNIST}   & \textbf{ResNet50 CIFAR10} & \textbf{ResNet50 CIFAR100}\\ \hline
Learning Rate     & $5e-3$                  & $5e-3$                    & $1e-3$                    & $1e-2$                    & $1e-1$           \\
Epochs            & $35$                    & $35$                      & $10$                      & $200$                     & $200$            \\
Weight Decay      & $1e-4$                  & $1e-4$                    & $1e-4$                    & $5e-4$                    & $5e-4$           \\
Batch Size        & $152$                   & $152$                     & $100$                     & $128$                     & $128$            \\
Optimizer         & Adam                    & Adam                      & Adam                      & SGD                       & SGD              \\
Scheduler         & Cosine                  & Cosine                    & Cosine                    & Cosine                    & Cosine           \\
Accuracy          & $99\%$                  & $90\%$                    & $92.5\%$                  &    $92.5\%$               & $75\%$           \\
                  &                         &                           &                           &                           &                  \\
\textbf{NUQLS}    &                         &                           &                           &                           &                  \\ \hline
Learning Rate     & $1e-2$                  & $1e-2$                    & $1e-1$                    & $1e-1$                    & $1e-2$                    \\
Epochs            & $10$                    & $10$                      & $5$                       & $50$                      & $10$                 \\
Batch Size        & $152$                   & $152$                     & $50$                      & $128$                     & $128$               \\
$\gamma$          & $1$                     & $0.7$                     & $0.6$                     & $0.01$                    & $0.05$     
\end{tabular}
\end{table}

\subsection{Comparison Experiments}
\subsubsection{SLU}
For the results in \cref{table:slu_comp}, we copied the training details for \citep[Table 3.]{miani2024sketched}; we point the reader to \citep[Appendix D.1]{miani2024sketched} for the exact details. For NUQLS, we used the following hyper-parameters for both MNIST and FashionMNIST: \textbf{Epochs} $10$, \textbf{samples} $10$, \textbf{learning rate} $0.01$, \textbf{batch size} $152$, $\bm{\gamma}$ $1$.

\subsubsection{Sampling-LLA, VaLLA and ELLA}
For the results in \cref{table:valla_llas_comp}, we used the training details for \citep[Figure 3. (left)]{antoran2022sampling}; again, we point the reader to the details given in \citep[Appendix F.1]{antoran2022sampling}. For NUQLS, we used the following hyper-parameters for MNIST: \textbf{Epochs} $10$, \textbf{samples} $10$, \textbf{learning rate} $0.01$, \textbf{batch size} $152$, $\bm{\gamma}$ $0.25$. For ResNet20 the following hyper-parameters were used: \textbf{Epochs} $1$, \textbf{samples} $50$, \textbf{learning rate} $0.0001$, \textbf{batch size} $152$, $\bm{\gamma}$ $0.01$.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
