\section{Related Works}
NUQLS shares notable similarities with, and exhibits key differences from, several prior works that we briefly allude to here; please see \cref{sec:appendix:related_work} for more discussions. 

\noindent
\textbf{LLA Framework.} Like the popular LLA framework \cite{khan2020approximate, foong2019between, immer2021improving, daxberger2021laplace}, NUQLS relies on linearizing the network, but it diverges in its approach to posterior approximation. While LLA requires sampling from a parameter distribution and often imposes a prior to handle degeneracy in overparameterized settings, NUQLS directly approximates the predictive distribution without artificial priors, avoiding biases and additional hyperparameters. This results in a noise-free GP with an NTK kernel, making NUQLS more suitable for interpolating regimes \cite{hodgkinson2023interpolating}. Additionally, NUQLS captures output covariances in classification tasks, offering a more comprehensive predictive distribution than LLA, which produces independent GPs for each output. NUQLS also provides flexibility in regression tasks by allowing post-hoc variance scaling (see $\gamma$ in \cref{eq:normal_gen}), enabling efficient hyperparameter tuning without retraining (see \cref{sec:appendix:regression_tuning} for an explanation of this method). 

Among the LLA-inspired methods, Sampling-LLA in \citet{antoran2022sampling} bears the closest resemblance to NUQLS. However, in addition to
the fundamental differences between NUQLS and LLA-inspired methods mentioned above, Sampling-LLA has several 
distinctions from NUQLS. Above all, unlike Sampling-LLA, NUQLS avoids the need for regularization and random subproblems. Instead, NUQLS leverages the true training data and random initialization to naturally capture uncertainty in overparameterized settings. Another related approach is Variational LLA \citep{ortega2023variational}, which uses a variational sparse GP with an NTK to approximate the LLA predictive distribution. Variational LLA has been shown to outperform Sampling-LLA in terms of both computational efficiency and uncertainty quantification performance.

\noindent
\textbf{Ensemble Framework.} NUQLS also differs from ensemble-based methods. Unlike Bayesian Deep Ensembles (BDE) in \cite{he2020bayesian}, which modifies infinitely wide networks to derive a GP predictive distribution, NUQLS demonstrates that finite-width, unmodified linearized networks inherently sample from a GP with an NTK kernel.  Similarly, the \textit{local ensembles} method \cite{madras2019detecting}, which perturbs the NN along small-curvature directions to create nearly loss-invariant ensembles, differs from NUQLS in that it relies on second-order information and includes directions with small but non-zero curvature. NUQLS, by contrast, constructs ensembles with exactly the same loss level from using only first-order information, providing a more precise representation of uncertainty. Low-rank approximations used in local ensembles may also poorly capture the true curvature structure, further distinguishing the two approaches.


\begin{table*}[t]
\centering
\caption{Comparing performance of NUQLS, DE, LLA and SWAG on UCI regression tasks. NUQLS performs as well as or better than all other methods, while showing a speed up over other methods; this speed up increases with the size of the datasets. LLA-K denotes LLA with a KFAC covariance structure.}
\label{table:uci_reg}
\vskip 2mm
\begin{tabular}{cccccc}
\hline
\textbf{Dataset}  & \textbf{Method}       & \textbf{RMSE $\downarrow$}                 & \textbf{NLL $\downarrow$}                   & \textbf{ECE $\downarrow$}                  & \textbf{Time}(s)         \\ \hline
\textbf{Energy}   & NUQLS        & \bentry{0.047}{0.006} & \bentry{-2.400}{0.209} & \bentry{0.002}{0.002} & $\mathbf{0.151}$        \\
         & DE           & \entry{0.218}{0.032} & \bentry{-1.651}{0.783}  & \bentry{0.004}{0.002} & $102.244$        \\
         & LLA          & \bentry{0.048}{0.006} & \bentry{-2.475}{0.128} & \bentry{0.004}{0.004} & $0.269$        \\ 
         & SWAG         & \bentry{0.058}{0.015} & \entry{-1.950}{0.158} & \entry{0.080}{0.011} & $37.084$        \\ \hline
\textbf{Kin8nm}   & NUQLS        & \bentry{0.252}{0.005} & \entry{-0.796}{0.025} & \bentry{0.000}{0.000} & $\mathbf{0.264}$        \\
         & DE           & \bentry{0.252}{0.006} & \bentry{-0.914}{0.028}  & \entry{0.002}{0.001} & $73.967$        \\
         & LLA          & \bentry{0.260}{0.010} & \entry{-0.783}{0.054} & \bentry{0.001}{0.001} & $11.966$        \\ 
         & SWAG         & \entry{0.457}{0.149} & \entry{-0.006}{0.295} & \entry{0.054}{0.012} & $150.263$        \\ \hline
\textbf{Protein}  & NUQLS        & \bentry{0.623}{0.005} & \bentry{0.209}{0.047} & \bentry{0.002}{0.000} & $\mathbf{1.356}$        \\
         & DE           & \entry{0.741}{0.052} & \bentry{0.203}{0.203}  & \bentry{0.011}{0.020} & $1014.827$        \\
         & LLA-K          & \entry{0.640}{0.007} & \entry{0.458}{0.071} & \bentry{0.002}{0.000} & $9.506$        \\ 
         & SWAG         & \entry{0.730}{0.044} & \bentry{0.187}{0.080} & \bentry{0.002}{0.002} & $468.972$        \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{Graphics/violin_plot_uncertainty.pdf}
    \vskip -3mm
    \caption{Violin plot of VMSP, for correctly predicted ID test points, incorrectly predicted ID test points, and OoD test points. Median is shown, with violin width depicting density. Low variance is expected for ID correct points, and large variance for ID incorrect and OoD points. Here FMNIST denotes FashionMNIST, MC denotes MC-Dropout, and BASE denotes Baseline.}
    \label{fig:resnet_variance}
    \vskip -3mm
\end{figure*}


\begin{table*}[t]
    \centering
    \caption{Image classification predictive performance, using LeNet5 on MNIST and FashionMNIST (FMNIST). Experiment was run $5$ times with different random MAP initialisations to get standard deviation on metrics.}
    \label{table:img_class}
    \vskip 2mm
    \begin{tabular}{cccccllc}
        \hline
        \textbf{Datasets}   & \textbf{Method}       & \textbf{NLL $\downarrow$} & \textbf{ACC $\uparrow$} & \textbf{ECE $\downarrow$}   & \textbf{OOD-AUC $\uparrow$}   & \textbf{AUC-ROC $\uparrow$}   & \textbf{Time (s)} \\ \hline
                    & MAP                   & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         & \entry{0.008}{0.001}      & \entry{0.888}{0.008}          & \entry{0.886}{0.008} 
                    & $257$         \\
                    & NUQLS                 & \entry{0.035}{0.002}     & \entry{0.989}{0.001}         &  \bentry{0.003}{0.000}    & \bentry{0.930}{0.026}         & \bentry{0.928}{0.026} 
                    & $106$         \\
\textbf{MNIST}      & DE                    & \entry{0.034}{0.004}      & \bentry{0.991}{0.000}        &  \entry{0.011}{0.004}     & \bentry{0.932}{0.009}          & \bentry{0.928}{0.009} 
                    & $2845$        \\
                    & MC-Dropout            & \entry{0.044}{0.002}      & \entry{0.989}{0.000}         &  \entry{0.017}{0.01}      & \entry{0.873}{0.032}          & \entry{0.871}{0.031} 
                    & $533$         \\
                    & SWAG                  & \bentry{0.029}{0.003}     & \bentry{0.991}{0.000}        &  \bentry{0.004}{0.002}     & \entry{0.902}{0.008}          & \entry{0.900}{0.008} 
                    & $489$         \\
                    & LLA*                   & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         &  \entry{0.008}{0.001}     &  \entry{0.888}{0.008}         & \entry{0.886}{0.008} 
                    & $45$          \\
                    & VaLLA                 & \entry{0.034}{0.002}      & \bentry{0.990}{0.001}         &  \entry{0.008}{0.001}     &  \entry{0.889}{0.008}         & \entry{0.886}{0.008} 
                    & $1583$        \\ \hline 
                    
                    & MAP                   & \entry{0.298}{0.007}      & \entry{0.891}{0.3}         & \bentry{0.006}{0.001}      & \entry{0.840}{0.022}          & \entry{0.804}{0.021} 
                    & $158$         \\
                    & NUQLS                 & \entry{0.302}{0.006}     & \entry{0.891}{0.002}         &  \bentry{0.005}{0.002}    & \bentry{0.904}{0.007}         & \bentry{0.870}{0.006} 
                    & $89$         \\
\textbf{FMNIST}     & DE                    & \entry{0.288}{0.002}      & \entry{0.896}{0.001}        &  \entry{0.013}{0.001}     & \entry{0.876}{0.003}          & \entry{0.836}{0.003} 
                    & $1587$        \\
                    & MC-Dropout            & \entry{0.306}{0.007}      & \entry{0.892}{0.003}         &  \entry{0.026}{0.002}      & \entry{0.856}{0.021}          & \entry{0.813}{0.019} 
                    & $291$         \\
                    & SWAG                  & \bentry{0.283}{0.005}     & \bentry{0.899}{0.003}        &  \entry{0.018}{0.002}     & \entry{0.817}{0.023}          & \entry{0.783}{0.022} 
                    & $264$         \\
                    & LLA*                   & \entry{0.298}{0.007}      & \entry{0.891}{0.003}         &  \bentry{0.006}{0.001}     &  \entry{0.841}{0.022}         & \entry{0.805}{0.021} 
                    & $26$          \\
                    & VaLLA                 & \entry{0.298}{0.007}      & \entry{0.891}{0.003}         &  \entry{0.007}{0.001}     &  \entry{0.841}{0.022}         & \entry{0.805}{0.021} 
                    & $1583$        \\ \hline 
    \end{tabular}
    \vskip -4mm
\end{table*}