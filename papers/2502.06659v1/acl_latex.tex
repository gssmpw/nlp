% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{amssymb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Who Taught You That? \\ Identifying Teachers in Model Distillation with Syntactic Patterns}
\title{Who Taught You That? \\ Tracing Teachers in Model Distillation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{\textbf{Somin Wadhwa}$^{*}$\qquad \textbf{Chantal Shaib}$^{*}$\qquad \textbf{Silvio Amir}\qquad \textbf{Byron C. Wallace}\qquad \\ 
Northeastern University \\ 
\texttt{\{wadhwa.s, shaib.c, s.amir, b.wallace\}@northeastern.edu} 
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
% \newcommand{\cs}[1]{\textcolor{purple!40}{\textbf{Chantal: #1}}} 
% \newcommand{\sw}[1]{\textcolor{olive!90}{\textbf{Somin: #1}}} 

\begin{document}
\maketitle
\begingroup\def\thefootnote{*}\footnotetext{Equal Contribution.}\endgroup
\begin{abstract}
Model distillation---using outputs from a large teacher model to teach a small student model---is a practical means of creating efficient models for a particular task. 
We ask: Can we identify a students' teacher based on its outputs? 
Such ``footprints'' left by teacher LLMs would be interesting artifacts. 
Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service.\footnote{Consider, e.g., speculation as to if DeepSeek was at least partly distilled from ChatGPT \cite{apnews2025deepseek}.}   
%While prior work has studied attributing text to specific LLMs or distinguishing human-authored text from machine-generated text, no existing work addresses attribution in the context of model distillation. 
We consider practical task distillation targets including summarization, question answering, and instruction-following. 
We assume a finite set of candidate teacher models, which we treat as blackboxes. 
We design discriminative models that operate over lexical features. 
We find that $n$-gram similarity alone is unreliable for identifying teachers, but \emph{part-of-speech (PoS)} templates \cite{shaib-etal-2024-detection} preferred by student models mimic those of their teachers. 
%pecifically, using PoS indicator features consistently outperforms $n$-gram based models. 
\end{abstract}

\begin{figure}
\centering
   \includegraphics[scale=0.38]{figures/fig1_new.pdf}
  \caption{We introduce the problem of \emph{teacher model attribution}: Given a distilled student model (e.g., a fine-tuned GPT-2), determine which of a set of possible teacher models was distilled (here, Mistral).}
  \label{fig:the_problem}
\end{figure}


\section{Introduction}

\emph{Model distillation} \cite{bucilu«é2006model,hinton2015distilling} entails teaching a small model using outputs sampled from a larger model. 
In the LLM era, this has proven an especially practical strategy: Distillation can imbue efficient small language models with task-specific capabilities competitive with (expensive) teacher LLMs \cite{xu2024survey}. 
In this work we assess the degree to which teachers inculcate ``signatures'' into student models during distillation.  
Specifically, we ask: \textbf{Given a \textit{distilled student model} can we 
identify its \emph{teacher} from a candidate set} (Figure \ref{fig:the_problem})? 


This may have practical implications. 
Imagine a start-up company distilling a particular piece of functionality (summarization, say) from a large proprietary model and using this to power a paid service. 
This may violate terms of service, so LLM providers might be keen to identify such cases. 

%\citet{ho-etal-2023-large} recently demonstrated that small models can be taught (via fine-tuning) to solve complex tasks using \textit{chain-of-thought} style reasoning elicited from larger models. \citet{li-etal-2023-symbolic} and \citet{shridhar-etal-2023-distilling} further extended this work to show how distilling tokens from LLMs that resemble \textit{reasoning} for target labels can boost small model performance on tasks like commonsense reasoning and grade school math. 

We consider a set of tasks for which distillation has proven successful in prior work. 
For reasoning and math tasks, distillation targets often include ``reasoning'' elicited from the teacher, as this has been shown to improve student performance substantially \cite{ho-etal-2023-large,li-etal-2023-symbolic,shridhar-etal-2023-distilling,wadhwa-etal-2024-investigating}.
We also consider broader ``distillation'', namely general instruction-tuning using examples elicited from a massive LLM, as done by Alpaca \cite{alpaca}. 

One might think that simply measuring similarity between the outputs generated by a student model and candidate teachers
would suffice to identify the teacher. 
%(i.e. whose outputs give the highest similarity score), but we 
%But perhaps surprisingly, 
However, we find that basic similarity measurements over texts provide practically no useful signal in this respect. 
Another obvious strategy would be to evaluate the likelihood of student outputs under different teacher models. 
Intuitively, the teacher should prefer (assign high likelihood to) outputs generated by its pupil. 
But again we find that such teacher perplexities are insufficient to reliably pick a teacher from a candidate set. 


If shallow measures of similarity do not suffice to identify teachers, what might? 
We explore syntactic Part-of-Speech (PoS) templates \cite{shaib-etal-2024-detection} as an alternative means of identifying teachers via higher-order lexical features.
We find that these templates---which are more abstract than raw output texts---carry relatively strong signal about the teacher used in distillation.


 
\section{Problem Setup}
\label{section:setup}

% Distilling LLMs involves \textit{teaching} a small, cost-efficient student model (e.g., GPT-2) using outputs from a (much) larger teacher model (e.g., GPT-4). 
% The goal is to derive a capable but efficient model. 

% We assume we are given a student model~$m$ trained to perform a particular task with data generated by a model from a candidate set of potential teacher LLMs, \( {\mathcal{M} = \{ M_1, M_2, \dots, M_T \} }\).\footnote{Our view is that in practice there are a relatively small number of LLMs that one might plausibly use for distillation. These are the very large and often proprietary models like GPT-*, Claude variants, or perhaps a large Llama.}
% Our goal is to infer which teacher model $M_j$ was used to train the student model $m$, but we do \emph{not} assume access to the distillation training data. 

% In our experiments, we use {\tt GPT-2} \cite{radford2019language} as the student model $m$ and the set of teachers $\mathcal{M}$ = \{{\tt Llama3-8B}, {\tt Llama3-70B}, {\tt Mistral-7B}, {\tt Mixtral}, {\tt Gemma2-9B}\}; we have selected open models to ensure reproducibility. 
% We have included a range of teacher model sizes (from 8B to 70B parameters), but all are quite large compared to $m$. 

Distilling LLMs involves training a small, cost-efficient student model (e.g., GPT-2) using outputs from a much larger teacher model (e.g., GPT-4) to create an efficient yet capable model. We assume a student model $m$ trained on data from one of several possible teacher LLMs, \( {\mathcal{M} = \{ M_1, M_2, \dots, M_T \} }\).\footnote{In practice, a limited number of proprietary LLMs (e.g., GPT-*, Claude, or Llama) are typically used for distillation.} Our goal is to identify which teacher model $M_j$ trained $m$, without access to the original distillation data.

In our experiments, we use {\tt GPT-2} \cite{radford2019language} as $m$ and a teacher set $\mathcal{M}$ = \{{\tt Llama3-8B}, {\tt Llama3-70B}, {\tt Mistral-7B}, {\tt Mixtral}, {\tt Gemma2-9B}\}, selecting open models for reproducibility. The teachers range from 8B to 70B parameters, all significantly larger than $m$.



% % Based on prior work, we make two additional key assumptions here. First, we assume that the training sets \( \mathcal{T}_i \) generated by teacher models contain unique stylistic features \cite{shaib-etal-2024-detection}. Second, the student model \(m\) generates content that retains stylistic tendencies of the specific teacher used to augment its training set \cite{gudibande2024the} i.e. students can \textit{reasonably} imitate their teachers. 




\begin{figure*}
    \centering
    % \includegraphics[scale=0.35]{figures/ppl_combined.pdf}
    \includegraphics[scale=0.55]{figures/ppl_heatmap.pdf}
    \caption{Perplexity under teacher models of texts generated by different pupils on (a) Rotten-Tomatoes, (b) QuaRel, and (c) OpenBookQA. Teacher perplexity does not consistently identify the teacher.}
    \label{fig:ppl_fig}
\end{figure*}


\paragraph{Tasks and Datasets} 
We experiment with tasks where larger models have been successfully distilled, including summarization, question answering, and general instruction following. 

For summarization, we use CNN-DailyMail \cite{see-etal-2017-get}, Rotten Tomatoes \cite{leone2020rotten}, and PubMed \cite{bharti2020}. For question answering, we use OpenbookQA \cite{mihaylov-etal-2018-suit} and CommonsenseQA \cite{talmor-etal-2019-commonsenseqa}. For instruction following, we randomly sample 10K instances from Alpaca~\cite{alpaca}.\footnote{Downsampling details in Appendix \ref{appx:data}.}

Training data is generated from all five teacher models. For summarization and instruction following, teachers generate outputs directly. For question answering, following \citet{li-etal-2023-symbolic}, we generate \textit{reasoning} chains for correct labels, using both chains and labels as student targets.


\section{Teacher Attribution Methods}


Here we describe models we considered to identify a teacher model based on a set of student outputs.
These include approaches based on: (i) perplexity; (ii) similarity metrics; and (iii) syntactic patterns.

LLMs ``prefer'' (assign lower perplexities to) text that they have produced. 
This suggests that simply comparing perplexities of student-generated text under each candidate teacher may be a viable identification strategy: 
We would expect the true teacher to assign lower perplexity to outputs from their pupils. 
However, as shown in Figure \ref{fig:ppl_fig}, this is insufficient to discriminate between teacher models. 
For example, Gemma assigns much higher perplexities to summaries produced by a model distilled from its own outputs (\ref{fig:ppl_fig}, a). 
% This aligns with findings from \citet{meister-cotterell-2021-language} showing that perplexity does not fully capture the linguistic properties of model outputs.  
%Their work shows that models can achieve similar perplexity scores while differing significantly in the linguistic features they encode. 
%This motivates the exploration of alternative features, such as part-of-speech tag patterns and semantic similarity scores, to better understand the dynamics of student-teacher relationships in model distillation.

\subsection{Similarity Metrics}

\begin{table}[t]
\renewcommand*{\arraystretch}{1.5}
\small
\centering
\begin{tabular}{p{1.5cm}p{2cm}cc}
\hline
\multicolumn{1}{c}{}                                                         & \multicolumn{1}{l}{\textbf{Teacher}} & \textbf{BoW} & \textbf{BERTScore} \\ \hline
\multirow{6}{*}{\texttt{\textbf{}}OBQA}                                     & \textcolor{blue}{\textbf{Llama8B}}%~\checkmark}}       
& 0.54 & 0.65 \\
                                                                            & Llama-70B  \textcolor{red}{$\times$}            & \textbf{0.56} & \textbf{0.71} \\
                                                                            &  Mistral-7B             & 0.53 & 0.62 \\
                                                                            &   Mixtral \textcolor{red}{$\times$}             & \textbf{0.56} & 0.65 \\
                                                                            &   Gemma2-9B             & 0.51 & 0.49 \\ 
                                                                            \hline
\multirow{6}{*}{\texttt{\textbf{}}Alpaca}                               &  \textcolor{blue}{\textbf{Llama8B}}%~\checkmark}}           
& 0.26 & 0.21 \\
                                                                            &   Llama-70B             & 0.22 & 0.25 \\
                                                                            &   Mistral-7B \textcolor{red}{$\times$}           & 0.25 & \textbf{0.26} \\
                                                                            &   Mixtral  \textcolor{red}{$\times$}             & \textbf{0.27} & \textbf{0.26} \\
                                                                            &  Gemma2-9B              & 0.19 & 0.11 \\   \hline
\multirow{6}{*}{\texttt{\textbf{}}C-D}                        & \textcolor{blue}{\textbf{Llama8B~\checkmark}}                      & \textbf{0.71} & 0.67 \\
                                                                            &   Llama-70B \textcolor{red}{$\times$}            & 0.60 & \textbf{0.68} \\
                                                                            & Mistral-7B              & 0.43 & 0.49 \\
                                                                            &  Mixtral                & 0.41 & 0.48 \\
                                                                            &  Gemma2-9B              & 0.28 & 0.31 \\   \hline
\end{tabular}
\caption{Neither cosine similarities (BoW) nor BERTScores between student (GPT-2) and candidate teachers reliably reveal the true teacher (Llama8B).}
\label{tab:results_similarity}
\end{table}








One approach to matching students with their teachers is to measure the similarity between the texts that they generate. 
We would expect texts generated by a student model to resemble those produced by its teacher (as compared to other LLMs). 

% SW TODO: Move to Appendix or further up. For each task type \( \mathcal{T}_i \), we evaluate approaches to identifying teacher models assuming access to varying numbers of example outputs (from 50 to 2000). 
%This allows us to assess the accuracy with which we can identify teacher models given different amounts of data. 

% In the absence of access to pre-training data, there is a possibility of overlap in the outputs generated by different teacher models, leading to some degree of similarity between them. 

% To address this, we compute text similarity across all teacher models and flag pairs with high similarity. 
% Such overlap suggests that teachers with greater similarity may produce students whose outputs are harder to distinguish solely through text similarity measures. This highlights the importance of accounting for teacher overlap in the evaluation process (see Appendix \ref{appx} for details). 

We use BERTscore \cite{zhang2020bertscoreevaluatingtextgeneration} and cosine similarity based on a bag-of-words representations as similarity measures. 
Table \ref{tab:results_similarity}\footnote{Additional results in Appendix Figure \ref{fig:sim_fig}.} shows the similarity between student and teacher outputs, as compared with other candidate teacher models across each dataset. 
Raw similarities are unreliable indicators of teacher models.

\begin{figure}[t]
\centering
  \includegraphics[scale=0.379]{figures/sim_auc_plot.pdf}
  \caption{AUC-ROC curves for a one-vs-rest LR classifier using similarity score as the sole feature. Performance across models is close to random (AUC $\approx$ 0.49‚Äì0.53), indicating limited discriminative power.}
  \label{fig:sim_auc}
\end{figure}

This limitation is further highlighted in Figure \ref{fig:sim_auc}, which presents AUC-ROC curves for logistic regression models using similarity scores as standalone features in a one-vs-rest classification setup. The average AUC score hovers around 0.52, indicating that similarity scores alone provide little discriminatory power in distinguishing between teacher models. These findings emphasize the need for more robust approaches to accurately differentiate teacher-student relationships.




\begin{figure*}
    \centering
    \includegraphics[scale=0.205]{figures/pos_fig.pdf}
    \caption{Influence of teacher models on student outputs, highlighting the retention of Part-of-Speech (PoS) templates. The color-coded PoS sequences illustrate how students inherit structural patterns from their respective teachers, suggesting that syntactic characteristics are preserved to some extent during knowledge transfer. This pattern indicates that PoS templates can serve as a distinguishing feature in identifying which teacher model was used to train a given student.}
    \label{fig:pos_label}
\end{figure*}

\subsection{Syntactic Patterns}




% \begin{table}[]
% \begin{tabular}{@{}lrrc@{}}
% \toprule
% \multicolumn{1}{c}{Data} & \multicolumn{1}{c}{BoW} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}N-grams\\ (n\textless{}=4)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}POS \\ Templates\end{tabular}} \\ \midrule
% CNN-DM            &                         &                                                                                        &                                                                              \\
% PubMed                   & 0.54                    & 0.61                                                                                   & 0.68                                                                         \\
% R-Tomatoes          &                         &                                                                                        &                                                                              \\ \midrule
% CSQA            & 0.43                    & 0.48                                                                                   & 0.67                                                                         \\
% OpenbookQA               &                         &                                                                                        &                                                                              \\
% QuaRel                   &                         &                                                                                        &                                                                              \\ \midrule
% Alpaca                   & 0.45                    & 0.52                                                                                   & 0.48                                                                         \\ \bottomrule
% \end{tabular}
% \caption{LR performance (accuracy 5-way); baseline/random 0.20. POS templates outperform BoW and N-gram in every case except on Alpaca data. N-Gram at 2500 features. POS templates 49.}
% \label{tab:lr}
% \end{table}


% \sw{summary - cite chantal's EMNLP paper here to motivate this bit, then talk about using POS templates as features and explain what we observe. for alpaca, add note that task segragation maybe needed}
%Looking beyond word-level features, we now test whether 

We next test whether ``syntactic templates'' (PoS sequences) provide signal sufficient to distinguish between different student models, inspired by recent work showing that LLMs prefer certain sentence constructions, more so than human authors \citep{shaib-etal-2024-detection}. 
Student models may internalize such structures, providing a signature of their teacher. 

%Coupled with \citet{gudibande2024the}, we hypothesize that POS templates as features can provide useful signals about which specific teacher model was used to train a given student model. 

We extract PoS templates with the {\tt diversity} package, \cite{shaib2024standardizing}\footnote{\url{https://pypi.org/project/diversity/}} finding the 50 most common PoS patterns of length 4 across all teachers, for up to 200 test instances per model. 
This yields a set of unique PoS patterns and corresponding sentences. 
We construct a training dataset using PoS template indicators as features and teacher models as targets. 
We train a simple logistic regression classifier and evaluate its performance on test sets generated by student models.



Results are summarized in Table \ref{tab:main_res}. PoS templates consistently outperform $n$-gram (up to n=4) 
and BERT-based models in distinguishing between teachers. %, with accuracy improvements observed across most datasets. 
For instance, on  PubMed the PoS template-based classifier achieves 0.68 accuracy, compared to 0.61 with $n$-grams. 
Similarly, on CommonsenseQA data PoS templates yield 0.67 accuracy, vs. 0.48 for $n$-grams. However, on Alpaca data $n$-grams slightly outperform PoS templates (0.52 vs. 0.48), demonstrating a marginal exception to the trend. Overall, these findings highlight the robustness of syntactic patterns as features for teacher model classification, particularly when compared to simpler word-based representations.

% \subsection*{Method Efficiency}
% \cs{Pick a better subsection title; TODO -- how does this method fare when we have fewer examples to compare to? Plot cost/number of generated samples vs. detection performance}  

\begin{table*}[t]
\small
\centering
\addtolength{\tabcolsep}{5pt} 
\begin{tabular}{@{}lccccccc@{}}
\toprule
                                     & \multicolumn{1}{l}{C-D} & \multicolumn{1}{l}{P-M} & \multicolumn{1}{l}{R-T} & \multicolumn{1}{l}{CSQA} & \multicolumn{1}{l}{OBQA} & \multicolumn{1}{l}{QRe} & \multicolumn{1}{l}{Alpaca} \\ \midrule
 % BoW           & 0.45                    & 0.53                    & 0.37                    & 0.41                     & 0.36                     & 0.29                    & 0.46                       \\

BERT           & 0.46                    & 0.55                    & 0.40                    & 0.44                     & 0.38                     & 0.35                    & 0.51                       \\
                       $n$-grams (1-4)      & 0.58                    & 0.68                    & 0.44                    & 0.56                     & 0.48                     & 0.50                    & \textbf{0.56 }                      \\
                       PoS Templates & \textbf{0.60}                    & \textbf{0.71}                    & \textbf{0.54}                    & \textbf{0.69 }                    & \textbf{0.51}                     & \textbf{0.59 }                   & 0.55                       \\  \bottomrule
\end{tabular}
\addtolength{\tabcolsep}{5pt} 
\caption{Classification performance of logistic regression and BERT using different feature representations across datasets. Models perform best when using PoS template features. %, followed by N-Grams, with BoW features yielding the lowest scores. 
This trend is consistent across different datasets, suggesting that syntactic structures (captured by PoS Templates) and higher-order lexical patterns (captured by $n$-grams) provide more discriminative power compared to simple word occurrence (BoW).}
\label{tab:main_res}
\end{table*}

% \section{Related Work}

% \paragraph{Distillation with LLMs}

% Prior work \citep{hase-bansal-2022-models} has shown how smaller models can learn from explanations (i.e. rationales). 
% Unlike their larger counterparts \cite{10.5555/3600270.3602070}, smaller models are not inherently capable of generating \textit{step-by-step} thinking chains, but they can be taught to do so \cite{magister-etal-2023-teaching}. 
% Recent efforts \citep{wadhwa-etal-2023-revisiting,ho-etal-2023-large} have shown that using these rationales as additional training signal can yield significant gains for student models.
% \citet{li-etal-2023-symbolic} further explores factors that may influence the creation of a \textit{teacher} corpus when distilling for tasks like commonsense reasoning. 
% \citet{pmlr-v202-fu23d} extend this line of work by exploring the trade-offs between generalizability and CoT-generation capability of small models, emphasizing how improving quality of generated rationales affect model performance. 
% More recently, \citet{wadhwa-etal-2024-investigating} investigated training details for CoT-augmented distillation, finding, e.g., that placing rationales \emph{after} labels yields superior performance. 

% \paragraph{Origin tracing} Methods for data provenance, such as watermarking, can be plausibly used for identifying the origin of a distilled student model. \citet{li2024statistical} introduce statistical tests to determine which model generated a string. 
% Our work differs in two ways: We do not assume access to model probabilities for detecting \textit{source models}, and we focus specifically on detecting text from \textit{distilled} models in which the resulting string can be attributed to either the teacher \textit{or} student model. \citet{li2024identifying} use generation-time watermarking strategies to imbue text with signatures that can later be used to trace the origin of text produced by a downstream model. While this method is robust, it requires an important assumption that the training data of the student model comes from adequately watermarked models. 

% Most similar to our work is \citet{li2023origin}, which uses perplexity and contrastive training to detect the origin of a generated text. Our method focuses instead on exploring linguistic features as model signatures, rather than relying on perplexity.

\section{Related Work}

\paragraph{Distillation with LLMs}

Smaller models can learn from explanations (i.e., rationales) \citep{hase-bansal-2022-models} despite lacking inherent \textit{step-by-step} reasoning \cite{10.5555/3600270.3602070}. However, they can be trained to generate such chains \cite{magister-etal-2023-teaching}. Recent studies \citep{wadhwa-etal-2023-revisiting,ho-etal-2023-large} show that rationales as training signals significantly improve student models. \citet{li-etal-2023-symbolic} explore factors influencing \textit{teacher} corpus creation for commonsense reasoning, while \citet{pmlr-v202-fu23d} examine trade-offs between generalizability and CoT-generation, highlighting rationale quality as crucial for performance. More recently, \citet{wadhwa-etal-2024-investigating} found that placing rationales \emph{after} labels enhances CoT-augmented distillation. 

\paragraph{Origin Tracing} 

Data provenance techniques like watermarking can trace the origins of distilled models. \citet{li2024statistical} introduce statistical tests for identifying source models without requiring access to model probabilities. \citet{li2024identifying} adopt generation-time watermarking, though this assumes student training data is watermarked. 
Most relevant to our owrk, \citet{li2023origin} use perplexity and contrastive training for text origin detection, whereas we investigate linguistic features as model signatures.


\section{Conclusions}
We have introduced the problem of identifying the teacher model used to train a distilled student model. 
We demonstrated that standard approaches, such as measuring similarity between teacher and student outputs or using perplexity as a proxy, are insufficient for reliable attribution. 
We introduced syntactic part-of-speech (PoS) templates as higher-order linguistic features capable of capturing distinctive signals from teacher models that persist in distilled student outputs.

We find that these linguistic patterns provide comparatively strong signal about teachers across tasks and datasets. 
This does not assume access to teacher model internals or use of watermarking strategies. 
While this approach achieves accuracy well above chance (which would be 0.2 here), it leaves considerable room for improvement, and the practical implications of the accuracies we have achieved are a bit unclear. 

Nonetheless, this work lays the foundation for further studies in teacher attribution, with potential implications for understanding model behaviors, ensuring compliance with usage agreements, and enhancing the transparency of AI systems. 
Future work could explore extensions to other types of linguistic features, investigate more complex attribution scenarios, or develop methods to counteract attribution in privacy-sensitive applications.

\section*{Limitations}
This work has several important limitations.

First, the effectiveness of syntactic templates as distinguishing features is dependent on the extent to which a student model retains the linguistic patterns of its teacher. 
This retention may be affected by factors such as additional fine-tuning, data augmentation, multi-teacher distillation, or \textit{shared} footprints among different teachers that are trained on the same data, which could obscure attribution signals. Investigating how these factors impact our attribution framework is an important direction for future research.

Second, while we show that syntactic patterns provide stronger attribution signals than traditional similarity metrics, our results indicate that there is considerable room for improvement.
While the predictive accuracies we reported are well above chance, they are far from perfect. 
Future work could explore integrating multiple complementary signals, such as semantic embeddings, paraphrase consistency, or latent-space representations, to further improve attribution accuracy.

Finally, our approach assumes a closed-set identification scenario where the true teacher model is among a predefined set of candidate models. 
Extending our methods to accommodate an arbitrarily large set of candidate teacher models is therefore an open direction for future work. 

\bibliography{custom}

\clearpage
\appendix
\section*{Appendix}
\label{sec:appendix}

\section{Implementation Details}
We perform all experiments on two NVIDIA A100 GPUs. 
We use publicly available implementations of all models via the {\tt Huggingface} library \cite{wolf2020huggingfacestransformersstateoftheartnatural}. 
For all tasks, we use a learning rate of $3e^{-5}$ and a maximum input length of $1024$. 
We evaluated checkpoints every $500$ steps with early stopping. 
We used a batch size of $12$ for question answering, $2$ for summarization, and $4$ for instruction following. 
Default values were used for all other hyperparameters.


\section{Datasets}
\label{appx:data}
\paragraph{CNN-DailyMail} \cite{see-etal-2017-get} is a large-scale dataset for abstractive text summarization. The dataset consists of online news articles from CNN and the Daily Mail, paired with human-written summaries in the form of bullet points. Each article is accompanied by a summary that captures its key points concisely. The dataset contains over 300,000 news articles and summaries, making it a widely used benchmark for training and evaluating summarization models. The dataset is divided into training, validation, and test sets, with 287,227, 13,368, and 11,490 examples, respectively. 

\paragraph{SumPubMed} \cite{bharti2020} is a large-scale dataset for abstractive biomedical text summarization, derived from PubMed, a leading repository of biomedical literature. Each document in the dataset consists of a full-text research article paired with its structured abstract, enabling the development and evaluation of automatic summarization models in the biomedical domain. The dataset contains over 30,000 articles spanning a diverse range of medical and life sciences topics. 

\paragraph{Rotten Tomatoes} \cite{leone2020rotten} is a dataset of meta-reviews which which synthesize multiple input reviews and and aggregate critic perception of the film. The dataset contains information for 9,095 movies with meta-reviews constructed from 244,000 individual reviews. 

\paragraph{CommonsenseQA} \cite{talmor-etal-2019-commonsenseqa} is a multiple-choice question-answering dataset requiring commonsense knowledge. Each question has five answer choices, with only one correct. The dataset contains 12,102 questions, split into training (9,741), development (1,221), and test (1,140) sets.

\paragraph{OpenbookQA} \cite{mihaylov-etal-2018-suit} is a multiple-choice question-answering dataset that tests reasoning over elementary science facts. Each question includes four answer choices and requires applying scientific facts beyond direct recall. The dataset consists of 5,957 questions, with 4,957 for training, 500 for development, and 500 for testing.

\paragraph{QuaRel} \cite{Tafjord2018QuaRelAD} is a multiple-choice dataset focused on qualitative reasoning, requiring understanding of physical relationships such as speed, force, and heat. Each question has two answer choices and is annotated with a logical representation of the reasoning process. The dataset contains 2,737 questions, split into train (1,911), development (278), and test (548) sets.

\paragraph{Alpaca} \cite{alpaca}  is an instruction-following dataset designed to fine-tune large language models for improved task generalization. Originally containing 52,000 synthetically generated instruction-response pairs from OpenAI's text-davinci-003, this version is downsampled to 10,000 instances, excluding any that contain programming-related code for efficiency. The dataset spans diverse tasks such as reasoning, summarization, and open-ended instruction following.

\begin{figure}
\centering
   \includegraphics[scale=0.379]{figures/similarity_plot_cosine.pdf}
  \caption{Average cosine similarity of student outputs (across all student models) over Bag-of-Words features with their true teachers vs. other teachers; these features provide little in the way of signal about teachers. }
  \label{fig:sim_fig}
\end{figure}

\section{Additional Results}
\label{appx:add_res}
\begin{table*}[t]
\centering
% \small
\addtolength{\tabcolsep}{7pt} 
\begin{tabular}{@{}llcccc@{}}
\toprule
\multirow{2}{*}{Features}                                                    & Support (\# test instances) \textrightarrow  & 50   & 200  & 1000 & 2000 \\
                                                                             & Data \textdownarrow & \multicolumn{4}{c}{}      \\ \midrule
\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}BoW \end{tabular}}                & CNN-DailyMail      & 0.44 & 0.44 & 0.45 & 0.45 \\
                                                                             & SumPubMed      & 0.54 & 0.55 & 0.53 & 0.53 \\
                                                                             & Rotten Tomatoes      & 0.38 & 0.37 & 0.37 & 0.37 \\
                                                                             & CommonsenseQA     & 0.43 & 0.40 & 0.41 & 0.41 \\
                                                                             & OpenbookQA     & 0.32 & 0.35 & 0.36 & 0.38 \\
                                                                             & QuaRel      & 0.24 & 0.28 & 0.29 & 0.28 \\
                                                                             & Alpaca   & 0.45 & 0.43 & 0.46 & 0.46 \\ \hline
\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}N-grams\\(n\textless{}=4)\end{tabular}} 
                                                                             & CNN-DailyMail     & 0.51 & 0.58 & 0.58 & 0.59 \\
                                                                             & SumPubMed      & 0.61 & 0.68 & 0.68 & 0.69 \\
                                                                             & Rotten Tomatoes     & 0.40 & 0.51 & 0.44 & 0.45 \\
                                                                             & CommonsenseQA     & 0.48 & 0.55 & 0.56 & 0.56 \\
                                                                             & OpenbookQA     & 0.45 & 0.48 & 0.48 & 0.49 \\
                                                                             & QuaRel      & 0.43 & 0.47 & 0.50 & 0.50 \\
                                                                             & Alpaca   & 0.52 & 0.55 & 0.56 & 0.56 \\ 
                                                                             \hline
\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}PoS \\ Templates\end{tabular}}    & CNN-DailyMail      & 0.59 & 0.60 & 0.60 & 0.62 \\
                                                                             & SumPubMed     & 0.68 & 0.70 & 0.71 & 0.72 \\
                                                                             & Rotten Tomatoes      & 0.51 & 0.53 & 0.54 & 0.56 \\
                                                                             & CommonsenseQA     & 0.67 & 0.69 & 0.69 & 0.69 \\
                                                                             & OpenbookQA     & 0.50 & 0.51 & 0.51 & 0.51 \\
                                                                             & QuaRel      & 0.57 & 0.57 & 0.59 & 0.59 \\
                                                                             & Alpaca   & 0.48 & 0.51 & 0.55 & 0.56 \\ 
                                                                             \bottomrule
\end{tabular}
\caption{Classification accuracy of logistic regression models using bag-of-words (BoW), n-grams (n\=1-4), and PoS templates as features across varying support levels (50, 200, 1000, and 2000 test instances). PoS templates consistently outperform BoW and n-grams across most datasets, with performance improving as support increases. The Alpaca dataset presents a marginal exception, where n-grams slightly outperform PoS templates at higher support levels. These results highlight the robustness of PoS templates for distinguishing between teacher models, particularly in high-support settings. Baseline/random accuracy is 0.20..}
\label{tab:add_res}
\end{table*}

To further assess the impact of support levels on classification performance, we extend our evaluation by varying the number of test instances per dataset. Table \ref{tab:add_res} presents results for logistic regression models trained using bag-of-words (BoW), n-grams (up to n=4), and PoS templates as feature representations. We consider support levels of 50, 200, 1000, and 2000 test instances to examine the stability and effectiveness of these representations across different data availability conditions.

Across all datasets and support levels, PoS templates consistently outperform BoW and n-grams in most cases. Notably, the advantage of PoS templates becomes more pronounced as support increases. For instance, in the SumPubMed dataset, accuracy rises from 0.68 at 50 instances to 0.72 at 2000 instances, surpassing both n-grams and BoW at every level. Similarly, in the CommonsenseQA dataset, PoS templates achieve a peak accuracy of 0.69, significantly outperforming BoW (0.41) and n-grams (0.56).

The Alpaca dataset remains an exception, where n-grams achieve slightly better performance (0.56 at 2000 instances) compared to PoS templates (0.56) but exhibit higher variability at lower support levels. This suggests that while PoS templates provide strong structural signals, certain datasets may benefit from richer lexical representations captured by n-grams.

Overall, these findings reaffirm the robustness of PoS templates as a reliable classification feature. Their advantage is particularly evident as more data becomes available, reinforcing the hypothesis that syntactic patterns are distinctive signatures of teacher models. This extended analysis further supports the main section's conclusion that PoS templates offer a scalable and effective alternative to traditional word-based representations.


\end{document}
