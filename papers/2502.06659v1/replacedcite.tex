\section{Related Work}
% \paragraph{Distillation with LLMs}

% Prior work ____ has shown how smaller models can learn from explanations (i.e. rationales). 
% Unlike their larger counterparts ____, smaller models are not inherently capable of generating \textit{step-by-step} thinking chains, but they can be taught to do so ____. 
% Recent efforts ____ have shown that using these rationales as additional training signal can yield significant gains for student models.
% ____ further explores factors that may influence the creation of a \textit{teacher} corpus when distilling for tasks like commonsense reasoning. 
% ____ extend this line of work by exploring the trade-offs between generalizability and CoT-generation capability of small models, emphasizing how improving quality of generated rationales affect model performance. 
% More recently, ____ investigated training details for CoT-augmented distillation, finding, e.g., that placing rationales \emph{after} labels yields superior performance. 

% \paragraph{Origin tracing} Methods for data provenance, such as watermarking, can be plausibly used for identifying the origin of a distilled student model. ____ introduce statistical tests to determine which model generated a string. 
% Our work differs in two ways: We do not assume access to model probabilities for detecting \textit{source models}, and we focus specifically on detecting text from \textit{distilled} models in which the resulting string can be attributed to either the teacher \textit{or} student model. ____ use generation-time watermarking strategies to imbue text with signatures that can later be used to trace the origin of text produced by a downstream model. While this method is robust, it requires an important assumption that the training data of the student model comes from adequately watermarked models. 

% Most similar to our work is ____, which uses perplexity and contrastive training to detect the origin of a generated text. Our method focuses instead on exploring linguistic features as model signatures, rather than relying on perplexity.