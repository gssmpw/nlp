\section{Related Works}
\subsection{Environment-Based Methods}
Environment-based RL methods train VLMs through direct interaction with GUI environments, where rewards are explicitly provided by the environment. Several frameworks like AndroidEnv____ and DistRL____ enable agents to learn through trial-and-error interactions with real-world digital interfaces. Recent works such as DigiRL____ and AutoWebGLM____ demonstrate that environment-based RL can effectively align VLMs with complex GUI navigation tasks through autonomous exploration. However, these methods face significant limitations in sample efficiency due to the high cost of environment interactions____, particularly in real-world applications where collecting online feedback is expensive and suffers from high latency. To address this, some approaches like WebRL____ and WorldGPT____ attempt to simulate GUI environments, but they struggle with state prediction accuracy and compounding errors in long interaction sequences____. The fundamental challenge lies in the dynamic nature of real-world GUIs, where interface elements and layouts frequently change, making environment-dependent reward signals inherently unstable____.

\subsection{Environment-Free Methods}
Environment-free approaches bypass direct environment interaction by leveraging offline datasets or learned reward models. Offline RL methods____ have shown promise in fine-tuning VLMs using pre-collected interaction trajectories, as demonstrated in large-scale GUI agent training by____ and____. Alternative approaches employ reward modeling techniques____ to predict task success signals from static datasets, enabling behavior alignment without environment feedback. General-purpose VLMs like GPT-4o____ represent a distinct category of environment-free methods, leveraging their pre-trained visual-language capabilities for zero-shot GUI understanding without explicit RL training____. However, these methods face distinct challenges: offline RL suffers from distribution shift when deployed to novel GUI configurations____, while reward models struggle to adapt to interface changes that invalidate their training criteria____. Even advanced VLMs like GPT-4o exhibit limitations in handling GUI complexity, as their lack of task-specific fine-tuning leads to misinterpretation of visual elements and interface dynamics____. Recent hybrid approaches like____ attempt to combine environment-free pre-training with targeted fine-tuning, but significant gaps remain in achieving robust generalization across diverse GUI ecosystems.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/structure_cropped.pdf}
    \caption{VEM Architecture: (1) Offline dataset annotation using GPT-4o's task understanding, and VEM training via supervised regression. (2) Policy optimization through frozen VEM maximization, encouraging the policy model to explore high-value actions.}
    \label{fig:structure}
    \vspace{-3mm}
\end{figure*}

% version 2