\section{Related Works}
\subsection{Environment-Based Methods}
Environment-based RL methods train VLMs through direct interaction with GUI environments, where rewards are explicitly provided by the environment. Several frameworks like AndroidEnv~\cite{toyama2021androidenv} and DistRL~\cite{wang2024distrl} enable agents to learn through trial-and-error interactions with real-world digital interfaces. Recent works such as DigiRL~\cite{bai2024digirl} and AutoWebGLM~\cite{lai2024autowebglm} demonstrate that environment-based RL can effectively align VLMs with complex GUI navigation tasks through autonomous exploration. However, these methods face significant limitations in sample efficiency due to the high cost of environment interactions~\cite{xie2021policy,niu2022trust}, particularly in real-world applications where collecting online feedback is expensive and suffers from high latency. To address this, some approaches like WebRL~\cite{qi2024webrl} and WorldGPT~\cite{ge2024worldgpt} attempt to simulate GUI environments, but they struggle with state prediction accuracy and compounding errors in long interaction sequences~\cite{guan2023leveraging,zhang2024mme}. The fundamental challenge lies in the dynamic nature of real-world GUIs, where interface elements and layouts frequently change, making environment-dependent reward signals inherently unstable~\cite{zhang2024ufo,zhang2024android}.

\subsection{Environment-Free Methods}
Environment-free approaches bypass direct environment interaction by leveraging offline datasets or learned reward models. Offline RL methods~\cite{snell2022offline,hong2023zero} have shown promise in fine-tuning VLMs using pre-collected interaction trajectories, as demonstrated in large-scale GUI agent training by~\cite{wang2024large} and~\cite{bai2024digirl}. Alternative approaches employ reward modeling techniques~\cite{stiennon2020learning,ouyang2022training} to predict task success signals from static datasets, enabling behavior alignment without environment feedback. General-purpose VLMs like GPT-4o~\cite{GPT-4o} represent a distinct category of environment-free methods, leveraging their pre-trained visual-language capabilities for zero-shot GUI understanding without explicit RL training~\cite{yan2023gpt,zhang2023appagent}. However, these methods face distinct challenges: offline RL suffers from distribution shift when deployed to novel GUI configurations~\cite{levine2020offline}, while reward models struggle to adapt to interface changes that invalidate their training criteria~\cite{prudencio2023survey}. Even advanced VLMs like GPT-4o exhibit limitations in handling GUI complexity, as their lack of task-specific fine-tuning leads to misinterpretation of visual elements and interface dynamics~\cite{xie2024osworld,zhang2024ufo}. Recent hybrid approaches like~\cite{cheng2024seeclick} attempt to combine environment-free pre-training with targeted fine-tuning, but significant gaps remain in achieving robust generalization across diverse GUI ecosystems.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/structure_cropped.pdf}
    \caption{VEM Architecture: (1) Offline dataset annotation using GPT-4o's task understanding, and VEM training via supervised regression. (2) Policy optimization through frozen VEM maximization, encouraging the policy model to explore high-value actions.}
    \label{fig:structure}
    \vspace{-3mm}
\end{figure*}

% version 2