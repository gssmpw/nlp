\section{Related Works}
\subsection{Environment-Based Methods}
Environment-based RL methods train VLMs through direct interaction with GUI environments, where rewards are explicitly provided by the environment. Several frameworks like AndroidEnv[2], "Android Application Environment" and DistRL[3], "Distributed Reinforcement Learning for Web Interfaces" enable agents to learn through trial-and-error interactions with real-world digital interfaces. Recent works such as DigiRL[4], "Digital Interface Reinforcement Learning" and AutoWebGLM[5], "Automated Web-Based GUI Modeling" demonstrate that environment-based RL can effectively align VLMs with complex GUI navigation tasks through autonomous exploration. However, these methods face significant limitations in sample efficiency due to the high cost of environment interactions[6], particularly in real-world applications where collecting online feedback is expensive and suffers from high latency. To address this, some approaches like WebRL[7], "Web-based Reinforcement Learning for GUI Tasks" and WorldGPT[8], "World-Guided Policy Transfer for Real-World GUIs" attempt to simulate GUI environments, but they struggle with state prediction accuracy and compounding errors in long interaction sequences[9]. The fundamental challenge lies in the dynamic nature of real-world GUIs, where interface elements and layouts frequently change, making environment-dependent reward signals inherently unstable[10].

\subsection{Environment-Free Methods}
Environment-free approaches bypass direct environment interaction by leveraging offline datasets or learned reward models. Offline RL methods [11], "Offline Reinforcement Learning for GUI Tasks" have shown promise in fine-tuning VLMs using pre-collected interaction trajectories, as demonstrated in large-scale GUI agent training by [12] and [13]. Alternative approaches employ reward modeling techniques [14], "Reward Modeling for GUI Tasks: A Deep Learning Approach" to predict task success signals from static datasets, enabling behavior alignment without environment feedback. General-purpose VLMs like GPT-4o[15], "General-Purpose Visual-Language Models for Zero-Shot GUI Understanding" represent a distinct category of environment-free methods, leveraging their pre-trained visual-language capabilities for zero-shot GUI understanding without explicit RL training [16]. However, these methods face distinct challenges: offline RL suffers from distribution shift when deployed to novel GUI configurations [17], while reward models struggle to adapt to interface changes that invalidate their training criteria [18]. Even advanced VLMs like GPT-4o exhibit limitations in handling GUI complexity, as their lack of task-specific fine-tuning leads to misinterpretation of visual elements and interface dynamics [19]. Recent hybrid approaches like [20], "Hybrid Environment-Free Methods for GUI Tasks: Combining Pre-Training with Targeted Fine-Tuning" attempt to combine environment-free pre-training with targeted fine-tuning, but significant gaps remain in achieving robust generalization across diverse GUI ecosystems.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/structure_cropped.pdf}
    \caption{VEM Architecture: (1) Offline dataset annotation using GPT-4o's task understanding, and VEM training via supervised regression. (2) Policy optimization through frozen VEM maximization, encouraging the policy model to explore high-value actions.}
    \label{fig:structure}
    \vspace{-3mm}
\end{figure*}