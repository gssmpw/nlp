@misc{castells2023recommendersystemsprimer,
      title={Recommender Systems: A Primer}, 
      author={Pablo Castells and Dietmar Jannach},
      year={2023},
      eprint={2302.02579},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2302.02579}, 
}

@misc{christiano2023deepreinforcementlearninghuman,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.03741}, 
}

@inproceedings{genoutfit2024,
author = {Xu, Yiyan and Wang, Wenjie and Feng, Fuli and Ma, Yunshan and Zhang, Jizhi and He, Xiangnan},
title = {Diffusion Models for Generative Outfit Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657719},
doi = {10.1145/3626772.3657719},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1350â€“1359},
numpages = {10},
keywords = {diffusion model, generative outfit recommendation, generative recommender model},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@misc{li2023recentdevelopmentsrecommendersystems,
      title={Recent Developments in Recommender Systems: A Survey}, 
      author={Yang Li and Kangbo Liu and Ranjan Satapathy and Suhang Wang and Erik Cambria},
      year={2023},
      eprint={2306.12680},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2306.12680}, 
}

@misc{paischer2024preferencediscerningllmenhancedgenerative,
      title={Preference Discerning with LLM-Enhanced Generative Retrieval}, 
      author={Fabian Paischer and Liu Yang and Linfeng Liu and Shuai Shao and Kaveh Hassani and Jiacheng Li and Ricky Chen and Zhang Gabriel Li and Xialo Gao and Wei Shao and Xue Feng and Nima Noorshams and Sem Park and Bo Long and Hamid Eghbalzadeh},
      year={2024},
      eprint={2412.08604},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2412.08604}, 
}

@inproceedings{pradeep-etal-2023-generative,
    title = "How Does Generative Retrieval Scale to Millions of Passages?",
    author = "Pradeep, Ronak  and
      Hui, Kai  and
      Gupta, Jai  and
      Lelkes, Adam  and
      Zhuang, Honglei  and
      Lin, Jimmy  and
      Metzler, Donald  and
      Tran, Vinh",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.83/",
    doi = "10.18653/v1/2023.emnlp-main.83",
    pages = "1305--1321",
    abstract = "The emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100K in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we find that generative retrieval is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge. We believe these findings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions."
}

@misc{wang2024generativerecommendationnextgenerationrecommender,
      title={Generative Recommendation: Towards Next-generation Recommender Paradigm}, 
      author={Wenjie Wang and Xinyu Lin and Fuli Feng and Xiangnan He and Tat-Seng Chua},
      year={2024},
      eprint={2304.03516},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2304.03516}, 
}

@misc{ye2023ip-adapter,
  title={IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},
  author={Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},
  booktitle={arXiv preprint arxiv:2308.06721},
  year={2023}
}

@misc{zhang2023adding,
  title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
  author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
  booktitle={IEEE International Conference on Computer Vision (ICCV)},
  year={2023},
}

@misc{ziegler2020finetuninglanguagemodelshuman,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

