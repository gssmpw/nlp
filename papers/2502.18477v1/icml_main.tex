%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{morewrites}
\usepackage{etex}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\usepackage{dsfont}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{enumitem}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem} 
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\RequirePackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\newenvironment{red}{\color{red}}{}
\newenvironment{green}{\color{green}}{}
\newenvironment{blue}{\color{blue}}{}
\newcommand\remove{\bgroup\markoverwith{\textcolor{orange}{\rule[.5ex]{2pt}{1pt}}}\ULon}
\newcommand{\FMP}[1]{{\blue\textbf{FMP:} #1}}
\newcommand{\zx}[1]{{\red\textbf{ZX:} #1}}
\newcommand{\IK}[1]{{\textcolor{orange}{\textbf{IK:} #1}}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Recommendations Beyond Catalogs: Diffusion Models for Personalized Generation}

\begin{document}

\twocolumn[
\icmltitle{Recommendations Beyond Catalogs:\\ Diffusion Models for Personalized Generation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Gabriel Patron}{uofm}
\icmlauthor{Zhiwei Xu}{uofm}
\icmlauthor{Ishan Kapnadak}{compsci}
\icmlauthor{Felipe Maia Polo}{uofm}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{uofm}{Department of Statistics, University of Michigan, Ann Arbor, United States}
\icmlaffiliation{compsci}{Department of Computer Science and Engineering, University of Michigan, Ann Arbor, United States}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Gabriel Patron}{gapatron@umich.edu}
\icmlcorrespondingauthor{Zhiwei Xu}{xuzhiwei@umich.edu}
\icmlcorrespondingauthor{Ishan Kapnadak}{kapnadak@umich.edu}
\icmlcorrespondingauthor{Felipe Maia Polo}{felipemaiapolo@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Generative Models, Diffusion Models, Generative Recommendation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Modern recommender systems follow the guiding principle of serving the right user, the right item at the right time. One of their main limitations is that they are typically limited to items already in the catalog. We propose REcommendations BEyond CAtalogs, \texttt{REBECA}, a new class of probabilistic diffusion-based recommender systems that synthesize new items tailored to individual tastes rather than retrieve items from the catalog. \texttt{REBECA} combines efficient training in embedding space with a novel diffusion prior that only requires users' past ratings of items. We evaluate \texttt{REBECA} on real-world data and propose novel personalization metrics for generative recommender systems. Extensive experiments demonstrate that \texttt{REBECA} produces high-quality, personalized recommendations, generating images that align with users' unique preferences. Code available at the \href{https://github.com/gapatron/REBECA/tree/main}{\texttt{REBECA}} repository.
\end{abstract}



\section{Introduction}
\label{submission}
Despite the widespread awareness of generative AI, its integration into daily content consumption remains surprisingly limited. A recent Reuters survey \cite{fletcher2024generative} highlights this gap, showing that while many users are familiar with generative models, their actual adoption lags behind expectations. Meanwhile, recommendation algorithms continue to dominate online interactions, shaping how users engage with digital content and products.

Modern recommender systems excel at matching users to items but are inherently constrained by the content available in their catalogs. These systems implicitly assume that relevant, engaging items exist for all users at all times; this assumption, however, needs to be reconsidered with the rise of generative AI. \begin{figure}[H]
    \vspace{.5cm}
    \centering
    \includegraphics[width=.8\linewidth]{figs/combined_front_page_4v2.pdf}
    \caption{Visualization of a subset of real images liked by the user alongside generated images for selected users at 512Ã—512 resolution. The personalized generative model effectively captures each user's preferred content and style.}
    \label{fig:visualization_ofrebeca}
\end{figure}Instead of merely retrieving content, can we dynamically enhance recommendations by generating personalized content? Generative recommender systems offer a promising path forward, capable of creating new items or modifying existing ones to better align with user preferences. Yet, integrating generative models into recommendation pipelines efficiently remains an open challenge.

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{figs/rebeca_diagram_v2.pdf}
    \caption{\texttt{REBECA} Diagram. Image embeddings $I_e$ are sampled from the distribution conditioned on the user ID and desired rating using a trained diffusion model. The sampled embeddings are then used as input for a pretrained text-to-image model with IP-Adapter. The diagram presents images generated for user 33, who, based on independent topic modeling results, has a preference for charming buildings, flowers and curious cats.}
    \label{fig:rebeca}
\end{figure*}
While recent work has explored recommendation systems that use generative algorithms, these works often focus on generative retrieval, where models leverage generative techniques to retrieve better-matching content \cite{wang2023diffusionrecommendermodel}. Generating entirely new items for recommendation, however, is comparatively underexplored. Existing proposals rely on explicit user inputs, such as textual prompts \cite{wang2024generativerecommendationnextgenerationrecommender} to guide generation or image captions \cite{genoutfit2024} during training. However, users rarely articulate their preferences directly, and their engagement patterns (e.g., clicks, likes) contain richer, more nuanced signals. In other words, despite the effectiveness of the numerous heterogeneous features tracked by modern recommender systems in identifying users' preferences \citep{zhai2024actionsspeaklouderwords}, text remains the main mechanism for generating novel content.

In this paper, we directly explore user feedback, i.e., user ratings, for personalized generation, a novel approach in recommendation systems. We present \texttt{REBECA}, the first probabilistic framework for personalized recommendation through content generation. \texttt{REBECA} directly captures user-item interactions via user feedback (e.g., likes or dislikes), enabling prompt-free, preference-aware generation. Our approach leverages pre-trained diffusion models by incorporating a lightweight adapter that efficiently encodes user preferences, eliminating the need for extensive fine-tuning or intermediation of large language models (LLMs) for personalized prompts composition. The design of \texttt{REBECA} is guided by three core principles: (i) \emph{text-free training and inference}, removing reliance on captions and prompts to enhance flexibility and scalability in user preference modeling; (ii) \emph{computational efficiency}, allowing seamless integration with existing generative pipelines, e.g., Stable Diffusion \citep{rombach2021highresolution}, maintaining high expressiveness without costly retraining; and (iii) \emph{a unified model for all users}, eliminating the need for multiple models and leveraging shared information across users for improved generalization.  

In summary, our contributions are as follows:
\begin{enumerate}[left=-1pt]
    \item We introduce \texttt{REBECA}, the first probabilistic framework for personalized recommendation via generative modeling, incorporating an efficient adapter on top of a highly expressive pretrained image generator that removes the dependency on image captions or prompting while preserving user-specific preferences.  

    \item We propose novel evaluation methodologies to assess the degree of personalization in a generative recommender system. Specifically, we (i) integrate the concept of \emph{verifiers} with a hypothesis test to formally measure and validate personalization and (ii) employ image captioning and topic modeling \cite{top2vec-angelov-inkpen-2024-topic} to evaluate whether the generated images align with user content and stylistic preferences.
    
    \item We perform extensive evaluations, including developing baselines that we compare against \texttt{REBECA} on real data and ablation studies that validate our design choices, offering valuable insights for developing scalable generative recommender systems.  
\end{enumerate}


By bridging recommendation and generative AI, \texttt{REBECA} redefines personalization, enabling dynamic content creation tailored to user preferences without requiring explicit input.

%Introduction
%A \href{https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2024-05/Fletcher_and_Nielsen_Generative_AI_and_News_Audiences.pdf}{recent survey from Reuters} reveals that despite widespread awareness of the products of generative models, hype does not precede usage. In other words, although most of us are aware of these tools and their capabilities, people are not using them as much in their daily lives as we would believe. This gap raises questions about how we interact with content and highlights the persistent dominance of recommendation algorithms in shaping user experiences. The fundamental mechanism through which we interact and consume content remains, undisputedly, social media via its master recommendation algorithms. 

%% Why is it important
%While modern recommender systems excel at pairing users and items, recommendations are constrained by the extent of their catalogs and they are compelling insofar as relevant items exist in their registry. This oft-unstated assumption â€” there are engaging items in the catalog at all times for all users â€” merits revision in the age of generative models: \textbf{can we dynamically enhance our catalogs in response to user feedback} and complement traditional retrieval-based recommendation?. Generative recommenders can produce new content or modify existing items to better align with user interests, and our efficient design permits easy integration with pre-existing generative pipelines.

%% What has been done before
%Although there has been exciting recent progress at the intersection of recommender systems and generative models, most of it focuses on generative retrieval, i.e., . Nevertheless, generating novel items remains a comparatively underexplored intersection of these fields with

%% What needs to be done
%Current systems focus on recommending pre-existing content from a catalog of items, whereas in this work we develop methodology that generates images tailored to individual preferences efficiently.   

%Our framework falls under the umbrella of multimodality, but unlike other multimodal models, ours is the first one to our knowledge designed to incorporate user feedback that does not require prompt engineering, intermediate LLM computations \cite{wang2024generativerecommendationnextgenerationrecommender} or expensive training from scratch. Our method leverages the rich representations already present in pre-trained diffusion models \cite{rombach2021highresolution}, but instead borrows from the Recommender Systems literature by modeling User-Item interactions and posits user embeddings as input signals. We contrast traditional multi-modal generative systems to the \texttt{REBECA} paradigm for learning; although they fall within the umbrella term of multimodality, there has been little attention paid to user-system interactions other than direct user input with text or images, and hence we aim to introduce a paradigm that learns to align with user preferences and needs via more natural feedback signals. Furthermore, our method does not require expensive image captioning, typically required in the majority of adapter architectures, necessitating...

%Our main contributions can be summarized as:
%\begin{enumerate}
%\item Developing \texttt{REBECA}, the first probabilistic framework for personalized recommendation through generation. Our framework is built around one adapter that encodes user preferences and does not depend on image captioning, making it not only easy and cheap to train but also enabling personalized efficient content generation. 



%\item We perform comprehensive testing and ablation studies, analyzing the impact of user priors, embedding-space training, and computational efficiency. Based on these results, we provide actionable recommendations and best practices for designing personalized generative systems.

%\end{enumerate}

%\remove{Recommender systems often rely on explicit signals, such as user reviews or textual prompts, to infer preferences. However, users rarely articulate what they find engaging, and their actions (e.g., clicks, likes) carry richer and more nuanced information. %\cite{zhai2024actionsspeaklouderwords}. 
%Existing approaches struggle to fully leverage such implicit feedback, particularly in generative settings where content creation is driven by textual prompts that may fail to capture subtleties in user preferences.}

%\remove{Adapters offer a promising solution for augmenting model capabilities, but they face two significant limitations: (1) they often require expensive paired data, such as image captions, which are time-consuming and costly to curate; and (2) training separate adapters for individual users does not scale and fails to leverage \FMP{correlation between users}\remove{collaborative filtering}. Generative model inference further exacerbates these challenges, as it is computationally intensive and resource-demanding.}

%\remove{To address these limitations, we propose a framework guided by the following principles:}
%\begin{enumerate}
%\item \remove{\textbf{Caption-free training}: Relying on textual prompts imposes constraints on capturing nuanced user preferences, making it essential to develop methods that work without paired text data.}

%\item \remove{\textbf{One model for all users}: Instead of training individual models or adapters for each user, a unified architecture should efficiently encode user preferences for personalized generation.}

%\item \remove{\textbf{Efficiency}: The solution must minimize computational overhead, reducing both energy costs and hardware requirements, while seamlessly integrating with existing generative pipelines.}
%\end{enumerate}

%\remove{To achieve these goals, we introduce a new adaptation paradigm that imposes custom priors on adapters, enabling efficient, caption-free training. By working in embedding space and modeling user-item interactions, our approach overcomes the scalability and efficiency challenges of existing methods, while maintaining high-quality personalization.}



\section{Related Work}

% The introduction to related work
Recommender systems and generative models are two rapidly evolving fields. While traditional recommender systems rely on pre-existing catalogs, generative approaches offer the promise of creating novel, user-specific content. This section reviews prior work in these areas, positioning our method as an underexplored bridge between them.


% Talk about recommender systems
\subsection{Recommender Systems}
Modern recommender systems are often described as understanding users better than they understand themselves. This perception stems from their ability to gather and analyze vast amounts of online activity data, building detailed profiles of user interests and habits. Such systems have become highly effective at matching users to items within pre-existing catalogs \cite{castells2023recommendersystemsprimer, li2023recentdevelopmentsrecommendersystems}.  Novel-item generative recommenders can enhance the current catalog offering, not just by creating, but also by making items more suitable to an individual's preferences.

\subsection{Generative Recommenders}
% Talk about work at the intersection THAT IS NOT  in the same line as our work, despite combining them
In recent years, there has been substantial progress at the intersection of generative models and recommender systems. Research in generative retrieval \cite{zhai2024actionsspeaklouderwords} and the use of large language models (LLMs) for \textit{preference discernment} \cite{paischer2024preferencediscerningllmenhancedgenerative} has introduced new paradigms, including the term ``Generative Recommender'' \cite{zhai2024actionsspeaklouderwords}. While these developments are exciting, they are not the focus of this work because they address the problem of within-catalog recommendation with generative item retrieval. Instead, we reference them here to provide contrast and contextualize our approach.

Despite the relative novelty of generative recommenders, the field has already seen terminological overlap. One particularly relevant work is GeneRec \cite{wang2024generativerecommendationnextgenerationrecommender}, which takes initial steps in framing the problem, identifying key challenges, and outlining directions for future research. Their proposed generative recommender paradigm consists of three main components: (1) an \textit{instructor} that preprocesses user instructions and feedback, (2) a \textit{generator} that creates new items, and (3) an \textit{editor} that refines and repurposes existing catalog items. This formulation, however, proposes an LLM that mediates between the user and the AI generator, necessitating explicit and active user instructions, which they are often unwilling to provide.

A more recent approach focuses on personalized fashion recommendations \cite{genoutfit2024}, but the scope is limited: it requires fitting a new model each time a novel item class enters the catalog and thus fails to leverage the rich general-purpose representations of base text-to-image models.



\subsection{Model Alignment}

Aligning generative models' outputs with human values and preferences is a vibrant research domain whose discourse often extends beyond technical communities into the public sphere. From a technical perspective, one widely adopted approach is Reinforcement Learning with Human Feedback (RLHF) \cite{christiano2023deepreinforcementlearninghuman, ziegler2020finetuninglanguagemodelshuman}, which optimizes model behavior based on reward signals derived from human preferences. However, RLHF remains computationally expensive, requires careful reward function design, and can be prone to mode collapse.

Another promising approach is Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage}, which aligns base models with human values through contrastive preference learning. DPO assumes uniform user preferences, treating human feedback as globally consistent, which is problematic in personalized settings like recommender systems, where preferences are highly heterogeneous. Additionally, DPO still requires an implicit reward structure, meaning that careful curation of preference data is crucial to avoid biases.

In our case, where we seek to model individual user preferences dynamically, both RLHF and DPO present limitations. Rather than imposing a global reward structure, we instead condition generation on user embeddings, allowing the model to capture personalized preferences in a more flexible, data-driven manner.

\subsection{Adapters, Priors and Decoders}
% Introduce our solution techniques
While our approach addresses the problem setting of GeneRec \cite{wang2024generativerecommendationnextgenerationrecommender}, our proposal is close to the \textbf{adapter} in generative modeling. The purpose of adapters is to produce augmentation to new input conditions or fine-tuning behavior \cite{zhang2023adding, ye2023ip-adapter} of large pre-trained models, and they typically require text captions during training. We argue that text captions are not only unusual in most recommendation datasets, but they also fail to distill the nuances of individual preference, better captured in the features tracked in modern recommender systems \cite{zhai2024actionsspeaklouderwords}. Although some adapters can be successfully calibrated with Low-Rank adaptations \cite{hu2021loralowrankadaptationlarge}, they are fine-tuned  per user, which not only leads to poor scaling properties, but also does not benefit from correlations between individuals. Instead, we propose a probabilistic adaptation framework that leverages CLIP latents as \textit{custom priors} and an off-the-shelf \textit{decoder} in a principled adapter formulation. In the past, \textit{priors} and \textit{decoders} have been utilized to train full base text-to-image models \cite{pradeep-etal-2023-generative}. \texttt{REBECA} is the first used to produce adapting behavior to additional input signals.


\subsection{Challenges in Establishing Baselines} 


Generative recommenders, as a novel class of systems, currently lack established baselines or datasets combining user ratings with image captions. Traditional methods in recommender systems focus on retrieval, while generative models lack personalization mechanisms. Adapters, although promising, require paired datasets that are impractical in recommendation contexts. This lack of baselines underscores the need for innovative methodologies and benchmarks, which we aim to address in this work.



\section{Methodology}

% \subsection{Motivation}


%\subsection{Custom priors}
In this section, we present \texttt{REBECA}'s pipeline. During training, images are first mapped to a lower-dimensional embedding space using CLIP \cite{radford2021learningtransferablevisualmodels}. Then a conditional diffusion model \cite{ho2020denoisingdiffusionprobabilisticmodels} is trained on these embeddings, conditioned on user and rating information. 
Specifically, at each time step \(t\) of the diffusion process, the model derives an image embedding \(I_e^t\). This embedding, along with the time step \(t\), the user ID \(U\) and the rating \(R\), is then fed into \(\varepsilon(U, R, t, I_e^t)\), a multi-layer decoder-only transformer, to predict the noise vector. The motivation is to integrate user and rating information directly into the diffusion dynamics. Classifier-free guidance \cite{ho2022classifierfreediffusionguidance} is further incorporated into the diffusion model to improve the sampling quality of image embeddings.

During inference, the trained diffusion model is provided with a user ID and a desired rating. The diffusion model subsequently generates image embeddings, which are then passed to a pretrained text-to-image model via IP-adapter \cite{ye2023ip-adapter} to produce the final images.

\subsection{Personalized Image Embedding Generation}
Our goal is to sample images \(I\) from the personalized distribution \(p(I | U, R)\) rather than the user-agnostic marginal distribution \(p(I)\), where \(U\) represents the user and \(R\) represents the desired rating. We decompose the personalized distribution as follows:
\[
p( I | U, R ) = p(I, I_e | U, R) = p(I_e | U, R) p(I | I_e, U, R), 
\]
where \(I_e\) denotes the deterministic embedding of the image \(I\). 
To model this decomposition, we employ two stages:
\begin{itemize}[left=-.7pt]
    \item A conditional diffusion model \(\hat{p}_{\theta}( \cdot \mid U,R)\) with parameter \(\theta\) is trained to approximate \(p(I_e | U, R)\). Given user \(U\) and desired rating \(R\), we sample new image embeddings from the trained diffusion model
\[
\hat{I}_e(U, R) \sim \hat{p}_{\theta}(I_e | U, R)
\]
\item The sampled embedding is then injected into a pretrained text-to-image model, for example, model in the Stable Diffusion family \cite{podell2023sdxlimprovinglatentdiffusion, rombach2021highresolution} with IP-Adapters \cite{ye2023ip-adapter}. Optionally, conditioning text prompt \(c_t\) can be added to improve the image quality, e.g. \textit{high quality image}, \textit{vivid colors} or \textit{good lightning}. We assume that the text-to-image model with IP-adapter approximates the conditional distribution:
\[
\hat{p}_{c_t}(I | \hat{I_e}(U, R)) \approx p(I | I_e, U, R).
\]
\end{itemize}

We conduct extensive experiments to evaluate how different prompt choices \(c_t\) impact the final generated images in Section \ref{sec:exp}. 

% In summary, we train a diffusion-based generative model with a transformer backbone that operates in CLIP embedding space rather than pixel space, which significantly reduces computational overhead while maintaining high expressiveness in learned representations.
% We frame our objective as sampling from the distribution of user-aligned images with a pre-trained image generation model as its engine, thus leveraging the richness in representations and expressiveness. Mathematically, we wish to sample images $I$ from a conditional distribution $p(I| U_i, R)$, i.e,
% \[
% I \sim p(I| U_i, R),
% \]
% where $U_i$ corresponds to the user ID, and scalar $S$ is the score for image $I$ by user $i$. For any of the  several \textit{CLIP} architectures \cite{ilharco_gabriel_2021_5143773, radford2021learningtransferablevisualmodels}, every image $I$ deterministically maps to a \textit{CLIP} image embedding $I_e$. Our method leverages this observation to derive our prior:
% \begin{align}
% \label{eq:decoder-prior}
%     p(I | U_i, R) 
%     &= p(I, I_e| U_i, R) \\
%     &= p(I | I_e,  U_i, R) p(I_e | U_i, R).
% \end{align}
% \FMP{Only add numbers to equations if you are referencing them later.}

% In this work, we consider the case wherein conditional on $I_e$, $U_i$ and $S$ are independent of $I$. Our motivation is twofold; firstly, it allows us to leverage an off-the-shelf text-to-image diffusion model, and secondly, it makes \texttt{REBECA} priors model agnostic, so long as they are trained on the appropriate \textit{CLIP} image embeddings. We thus approximate the \textit{decoding} factor with a Stable Diffusion text-to-image model, and the left one with a diffusion \textit{prior} $\hat{p}(I_e | U_i, S)$ that we learn with data. Following our derivation in Equation \ref{eq:decoder-prior}, we thus have

% \[
% p(I | I_e,  U_i, R) p(I_e | U_i, R) \approx p_{c_t}(I | I_e) \hat{p}(I_e | U_i, R)
% \]

% where $p_{c_t}$ is a black-box text-to-image model enhanced with image prompting capabilities such as the Stable Diffusion family of models \cite{podell2023sdxlimprovinglatentdiffusion, rombach2021highresolution} with IP-Adapters \cite{ye2023ip-adapter} or native image embedding priors as in the second generation of the Kandinsky family of models \cite{razzhigaev2023kandinskyimprovedtexttoimagesynthesis} 
% or unCLIP \cite{ramesh2022hierarchicaltextconditionalimagegeneration}. 

% Following the notation in the literature, we denote $c_t$ as the text conditioning 'prompt', which we consider as a system setting that is user-agnostic. We use it to increase image quality by adding appropriate qualifiers e.g. \textit{high quality image}, \textit{vivid colors} or \textit{good lightning} to the `prompt' argument, but otherwise do not utilize it to steer generation towards each users' preferred topics. We provide some experiments that test the effect of different types of prompt with end-result generated images. 


% More practically, during sampling, we draw observations from the custom prior distribution $I_e$ for user $U$ with rating $R$, which we then use as an input to the diffusion model via the IP-Adapter (Fig. \ref{fig:rebeca}). This produces images aligned with the user's prior history that benefit from collaborative filtering. Notably, our formulation can be trivially extended to further conditioning signals available today to most modern recommender systems, thus enhancing interaction-based generation. 




\subsection{Model training and inference}

% Our model is a diffusion-based generative model with a transformer backbone that operates in CLIP embedding space rather than pixel space. This design significantly reduces computational overhead while maintaining high expressiveness in learned representations; training requires less than $10$ minutes to reach early stopping on a single 3090 Nvidia GPU. \zx{can be moved to appendix}

% \subsection{Formulation}
 Training our diffusion model to generate personalized image embeddings conditioned on user preferences follows a standard diffusion formulation, where a data distribution $I_e \sim p(I_e | U, R)$ undergoes a forward noising process, and the model learns to predict the reverse denoising trajectory by maximizing the lower bound of a variational Markov Chain. 

Formally, given an original unperturbed embedding $I_e^0$, we apply a noising schedule $\beta_t$ to produce noisy embeddings during the forward process:
$$q(I_e^t|I_e^0)  = \mathcal{N}(I_e^t; \alpha_t I_e^0, \sigma_t^2I ),$$
where $\alpha_t$ and $\sigma_t$ depend on the choice of noise schedule.

In the reverse process, we use a transformer $\varepsilon_{\theta}(U, R, t, I_e^t)$ with input sequence being a concatenation of user, rating, time, and image tokens to approximate $\varepsilon_t$, the noise injected in the forward process conditioned on user and rating information. The reverse process does not have an exact analytic formulation, however, for $\beta_t \rightarrow 0$, it approximates a normal distribution \cite{sohldickstein2015deepunsupervisedlearningusing}. We thus approximate the reverse process by:
$$
p_\theta(I_e^{t-1}\ |I_e^t, U, R) = \mathcal{N}(I_e^{(t-1)}; \mu_{\theta}(U, R, t, I_e^t), \sigma_t^2I),
$$
where $\mu_\theta$ is derived in relation to the noise prediction model $\varepsilon_\theta$. Finally, we optimize the model using a simplified denoising objective \cite{ho2020denoisingdiffusionprobabilisticmodels}:
$$
L_{\text{simple}}(\theta) = \mathbb{E}_{(t, \varepsilon, U, R)}[||\varepsilon - \varepsilon_{\theta}(U, R, t, I_e^t)||^2], 
$$

where $t \sim \mathrm{Unif}(\{1, \cdots, T\})$ and $\varepsilon \sim \mathcal{N}(0, I)$.

At inference time, we sample an initial Gaussian noise vector $I_e^T \sim \mathcal{N}(0, I)$ and apply iterative denoising:
$$
I_e^{(t-1)} = \mu_\theta(I_e^{t}, t, U, R) + \sigma_t \varepsilon, \varepsilon \sim \mathcal{N}(0,I).
$$
The process continues until we obtain $I_\varepsilon^0$, the final user-specific image embedding.
\subsubsection*{Classifier-Free Guidance}
We employ Classifier-Free Guidance (CFG) \cite{ho2022classifierfreediffusionguidance}, a widely used technique in diffusion-based generative models, to enhance user-conditioned image embedding generation. CFG modulates the sampling process by interpolating between the unconditional model output $\varepsilon_{\theta}(I_e^t, t)$ and the user-rating conditional prediction $\varepsilon_\theta(I_e^t, t, U, R)$:
$$
\tilde{\varepsilon}_{\theta}(I_e^t, t, U, R) = (1 + \omega)\varepsilon_{\theta}(I_e^t, t, U, R) - \omega \varepsilon_{\theta}(I_e^t, t).
$$ 
Here $\omega$ is the guidance scale that controls the trade-off between personalization and diversity. Higher values of guidance result in less diverse sets of images, but more attuned to their conditioning signals. We conduct extensive experiments to evaluate the influence of different guidance scaling in Section \ref{subsec:ablation}




\section{Experiments}\label{sec:exp}
\subsection{Dataset and Setup}
We repurpose the FLICKR-AES dataset \cite{Ren_2017_ICCV} to learn a generative model for aesthetically pleasing images based on user preferences. Initially compiled to learn models for personalized aesthetics predictions, the dataset contains a curated sample of 40,988 Creative Commons-licensed photos from FLICKR, rated for aesthetic quality on a scale of 1 to 5. For reference, annotations were collected using Amazon Mechanical Turk, with each image receiving ratings from five different workers. In total, 210 workers contributed, scoring 40,988 images for a total of $193208$ ratings. On average, each worker labeled 137 images.


% Initial setup and transformations
More formally, we define our initial data setup as consisting of triplets of users, ratings, and images $(U, R, I)$. CLIP embeddings are deterministic, and computing all of them requires a single pass of our image data through the appropriate CLIP \textit{ViT-H-14} encoder, and so we augment our data with them. We further map the ratings to binary rating, representing like and dislike: given a pair of user and its rated image, we let $R=1$ if the rating is $4$ or higher, and let $R=0$ otherwise. Finally, we filter out users with less than $100$ liked images, ensuring enough information on individual preference.


At the training stage, we fit our image embedding prior with $(U, R, I_e)$. Importantly, image captions are not part of our data, and we uniquely rely on users' historic ratings. 


\subsection{Image Generation}
We restate our objective as enhancing catalogs in accordance with users' preferences. This means that at the sampling stage, we generate image embeddings that are personalized based on the userâ€™s historical interactions.

To ensure that the sampled embeddings align with user preferences, we set $R=1$ to indicate a strong preference signal. This choice enforces the model to condition its generation process on the user embedding, thereby prioritizing images that align with previously liked content.

We follow the standard denoising process using CFG and set the guidance scale of $10.0$ (see details in Section \ref{subsec:ablation}) to balance personalization and diversity. The final generated embedding $\hat{I}_e$ represents a user-adapted image feature that is used as input to a Stable Diffusion $1.5$ model with an empty prompt (denominated $T_0$) \cite{rombach2021highresolution}. We select this model for its maturity and known properties, alongside its well-known IP-Adapter \cite{ye2023ip-adapter}.

\subsection{Baselines}

We consider three baselines, all utilizing the same decoder as \texttt{REBECA}, namely Stable Diffusion 1.5 \cite{rombach2021highresolution}. The first baseline, \(T_0\), generates images from Stable Diffusion using an empty prompt, providing insight into the modelâ€™s generic outputs. The second baseline, \(T_1\), samples images using high-quality but simple positive and negative prompts. The third baseline, \(T_2\), employs high-quality and detailed positive and negative prompts, simulating a scenario where a user can significantly influence image generation through prompt engineering. We paste the prompts we use in the following box.
\begin{tcolorbox}[colback=gray!10, colframe=gray, sharp corners, width=.48\textwidth]
\small
\begin{itemize}[left=-6pt]
    \item $T_1$ prompts:
    \begin{itemize}[left=-0pt]
        \item[(\textbf{+})] ``high quality photo''
        \item[(\textbf{--})] ``bad quality photo, letters''
    \end{itemize}
    \item $T_2$ prompts:
    \begin{itemize}[left=-0pt]
        \item[(\textbf{+})] ``Realistic image, finely detailed, with balanced composition and harmonious elements. Dynamic yet subtle tones, versatile style adaptable to diverse themes and aesthetics, prioritizing clarity and authenticity.''
        \item[(\textbf{--})] ``deformed, ugly, wrong proportion, frame, watermark, low res, bad anatomy, worst quality, low quality''
    \end{itemize}
\end{itemize}

\end{tcolorbox}


%T0: basic decoder baseline (what it generically generate). 
%T1: generic good images, content, style, diversity, quality
%T2: best possible scenario

%same off-the-shelf generator
       
       

\subsection{Metrics}


% \subsubsection*{Custom Prior}

\subsubsection*{Global evaluation}
Our objective is to generate images that not only align with user preferences but also maintain high quality. To evaluate our approach, we compute the FrÃ©chet Inception Distance (FID) \citep{heusel2017gans} and Conditional Maximum Mean Discrepancy (CMMD) \citep{jayasumana2024rethinkingfidbetterevaluation}, aggregating generated images into a shared pool. Both metrics measure the distance between real and generated image distributions, providing an overall assessment of generative model performance. We report both metrics because FID, while widely used as a traditional measure of generation quality, has been shown to inadequately capture human perception of image quality \cite{jayasumana2024rethinkingfidbetterevaluation, wiles2024revisitingtexttoimageevaluationgecko, NEURIPS2023_0bc795af}. On contrast, CMMD, imposes fewer assumptions on the data-generating process and is more sample-efficient \cite{jayasumana2024rethinkingfidbetterevaluation}.

%We include CMMD - MMD distance in CLIP space - following an increasing number of works calling into question FID, which requires strong distributional assumptions and has inconsistent results for varying sample sizes. \IK{can we cite some result that questions FID?} Furthermore, CMMD aligns better with human raters while being more sample-efficient \.


\subsubsection*{Personalization Verifier}

Given that we cannot evaluate the scores of users of the FLICKR-AES dataset on generated images, we cannot directly tell if the images generated by \texttt{REBECA} satisfy their tastes. However, we can train a predictive model that predicts the probability that a certain user $U$ will like an image $I$. Models with similar purposes are known as \emph{verifiers} and are largely used in the generative models' recent literature \citep{cobbe2021training,lightman2023let}. Our model for the probability of each user $U$ liking a certain image $I$ is inspired by matrix factorization techniques \citep{toscher2009bigchaos,koren2009matrix,ong2024routellm} and Item Response Theory (IRT) from the field of psychometrics \citep{cai2016item,chen2019joint} but also used in the evaluation of LLMs \citep{polo2024tinybenchmarks,polo2024efficient}. Concretely, we assume that our verifier $v(U,I)$ is given by
\begin{align*}
  \mathbb{P}(R=1\mid U, I)= \sigma\left(\varphi(U)^\top\psi(I)\right),
\end{align*}
where $\sigma$ is the logistic function, $\varphi:[N]\to \mathbb{R}^d$ and $\psi:\mathcal{I}\to \mathbb{R}^d$ are embedding functions that map users and images to embeddings. Here, $N$ is the number of users, $\mathcal{I}$ is the set of images, and $d$ is a hyperparameter that denotes the dimension of the model. We assume $\varphi$ is trainable and that each user is represented by an independent real vector while $\psi$ is the composition of a (trainable) linear transformation and a (fixed) embedding model that maps images to high-dimensional embeddings. In practice, the fixed embedding model we use is \textit{OpenCLIP-ViT-H-14} and the scoring model is trained by minimizing the classification cross-entropy loss. The performance of a generative model $p(I|U, R=1)$  can be measured by
\begin{equation*}
    \text{Score}(
    p(I|U,R=1))=\mathbb{E}_U\mathbb{E}_{I\sim p(I|U,R=1)}\left[v(U,I)\right],
\end{equation*}
which can be estimated by: (i) fitting the verifier on training data, obtaining $\hat{v}$ and then (ii) empirically obtaining the estimator $\widehat{\text{Score}}(p(I|U,R=1))$ by sampling some images per user. We use the verifier to compare different versions of \texttt{REBECA} and to test for its personalization capabilities.

\subsubsection*{Topic modelling}
To further assess whether the generated images align with users' preferences, we employ contextual topic modeling \cite{top2vec-angelov-inkpen-2024-topic} to analyze users' preferences from multiple perspectives. Specifically, we generate image captions for the entire FLICKR dataset using GPT-4o-mini \cite{openai2025gpt4omini}. For each image, we generate two distinct captions focusing on the content and style of that image. We then train a contextual topic model for each caption type. The topic model embeds each image into multiple contextual embeddings with \textit{sentence-transformers} \citep{reimers-2019-sentence-bert}.  These embeddings are further projected into a lower-dimensional space using UMAP \cite{umapMcInnes2018} for clustering and topic identification. Two separate topic models are trained on the content and style captions from the FLICKR dataset. \(345\) topics are identified for image content and \(255\) topics are identified for image style. Each image used for training is assigned a topic distribution, representing its alignment with identified content and style topics. When presented with a new caption, the topic models map it to the most similar content and style topics from the learned topic distributions, without requiring additional training. See Appendix \ref{appendix:topic_model} for more details on the prompt design and the learned topics.



\subsection{Results}
\subsubsection*{Global quality of generated images}
Table \ref{table:fid_cmmd} presents a comparison of different models based on FID (FrÃ©chet Inception Distance) and CMMD (Conditional Maximum Mean Discrepancy), where lower values indicate better performance and a better alignment with the target distribution of images. The results demonstrate that \texttt{REBECA} achieves the best performance, with the lowest FID \(117.77\) and CMMD \(0.68\), indicating superior image quality and better alignment with user preferences. The $T_2$ baseline shows a clear degradation in performance, having the highest FID $(145.45)$ and worst CMMD $(1.56)$, indicating a significant drop in both image realism and conditional consistency despite a well-crafted prompt. These results reinforce that \texttt{REBECA} is the most effective approach, outperforming all baselines in global image quality.
\begin{table}[t]
\caption{Comparison of FID and CMMD scores for several baselines and our method. 
Lower is better for both metrics.}
\label{table:fid_cmmd}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{FID} $\downarrow$ & \textbf{CMMD} $\downarrow$ \\
\midrule
\textbf{REBECA} & \textbf{117.77} & \textbf{0.68} \\
$T_0$ & 121.50 & 1.13 \\
$T_1$ & 131.75 & 0.88 \\
$T_2$ & 145.45 & 1.56 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsubsection*{Does \texttt{REBECA} generate personalized images?}

\paragraph{Dependence between generated images and user IDs} We start measuring and testing the degree of personalization of \texttt{REBECA} utilizing our trained verifier $\hat{v}$ and a statistical hypothesis testing. The degree of personalization can be measured by randomly assigning \texttt{REBECA} images to users and comparing the verifier scores with the case in which images are correctly assigned to the user they were created for; the difference of verifier scores (correct assigned minus randomly assigned) denotes how happier users get when the correct images are assigned to them versus when random images are assigned. To measure if the difference is statistically significant, we employ a hypothesis test measuring the degree of dependence between users $U$ and generated images $I$; in statistical terms, we test the null hypothesis $H_0:U\ind I$ using a permutation test \citep{lehmann1986testing} and a significance level $\alpha\in (0,1)$. The procedure is described in concrete terms in Algorithm \ref{alg:perf-eval}. Figure \ref{fig:rebeca-permed} depicts our results and shows that, on average, assigning the correct \texttt{REBECA} images vs randomly assigning them, increases the verifier score, on average, almost 6 standard deviations and leads to a p-value  $p<10^{-3}$ (much smaller than the standard threshold $\alpha=.05$), giving strong evidence that images and users are statistically dependent and that \texttt{REBECA} generates images tailored to each user.


\input{algo}

\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth]{figs/perm.pdf}
    \caption{Measuring and testing for \texttt{REBECA}'s personalization capabilities. Assigning the correct \texttt{REBECA} images vs randomly assigning them, increases the verifier score, on average, almost in 6 standard deviations and leads to a p-value  $p<10^{-3}$. This gives strong evidence that images and users are statistically dependent and that \texttt{REBECA} generates images tailored to users.}
    \label{fig:rebeca-permed}
\end{figure}
\paragraph{Topic Precision}


\begin{figure*}[t]
     \centering
      \includegraphics[width=.49\linewidth]{figs/verifier_scores_ablation.pdf}
      \includegraphics[width=.49\linewidth]{figs/global_cmmd_v2.pdf}
      \caption{Impact of classifier-free guidance (CFG) values on: \textit{(Left)} verifier score with increase in the (3.5, 10.0) range, followed by saturation, and \textit{(right)} CMMD distance decreasing within the same range, reaching a minimum at $\omega=10.0$, and increasing afterward.}
      \label{fig:rebeca-modscores}
\end{figure*}


For each user, we define their liked topics as the set of topics that are assigned to some image that the user liked in the FLICKR data. We use the \texttt{REBECA} model to generate \(50\) images for each user conditioning on \((U, R=1)\), implying that we expect the user to like the generated images. GPT-4o mini is then applied to caption those generated images from the content and style perspective with the same prompt as that used in FLICKR image caption generation. We assign the content and style topics for those captions and identify a generated image as a liked image in terms of content (resp. style) if its assigned content (resp. style) topics is a subset of the user's liked content (resp. style) topics. We define the content (resp. style) topic precision as the proportion of generated images that are also liked images for each user. Figure \ref{fig:rebeca-topic-acc} show the distribution of content and style topic precision for each model. We can see that for content topics, \texttt{REBECA}, without any conditioning text prompt, outperforms baseline models \(T_0\) and \(T_1\), and has comparable performance to baseline model \(T_2\), which benefits from a well-crafted text prompt as input to the text-to-image model. A possible explanation is that high-quality and visually appealing images often feature content that many users naturally like, such as cute animals or snowy mountains at sunrise. For style topics, we notice that \texttt{REBECA} outperforms all baseline models, implying that it does learn the styles the users prefer only from their like/dislike history. We also notice that model \(T_1\) and \(T_2\) are worse than model \(T_0\), which does not use any text prompt, implying that artificially adding general prompts may generate nice images, but in a style that the user does not prefer.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{figs/topic_accuracy_distribution_1by2.pdf}
      \caption{Precision of the generated images on content and style topics of the \texttt{REBECA} model and the baseline models. For content topics, \texttt{REBECA} models outperform the baselines \(T_0\) and \(T_1\) and has comparable performance to \(T_2\), and using text prompts is better than no prompt. For style topics, \texttt{REBECA} models perform slightly better than the baselines, and using text prompts is worse than no use of prompts.}
      \label{fig:rebeca-topic-acc}
\end{figure}
\subsubsection*{Ablations}\label{subsec:ablation}

We conduct additional ablation studies to gain deeper insights into \texttt{REBECA}'s behavior under different conditions.

First, we examine the effect of coupling \texttt{REBECA} with prompts \(T_1\) and \(T_2\), previously used by our baselines. As shown in Table \ref{table:fid_cmmd_ablation}, \texttt{REBECA} achieves the best performance, yielding the lowest FID (117.77) and CMMD (0.68), indicating strong generative quality and alignment with user preferences. Interestingly, incorporating prompts \(T_1\) and \(T_2\) leads to performance degradation, despite their explicit emphasis on quality. This suggests that these prompts negatively impact user alignment, causing a shift away from the real data distribution. Additionally, we assess the impact of removing user embeddings while keeping the model architecture unchanged. The NoUE variant, which excludes user embeddings, performs significantly worse, with an FID of 172.29 and CMMD of 1.20. Since all \texttt{REBECA} variants outperform NoUE, this confirms that incorporating user information is crucial for enhancing both generation quality and alignment with user preferences.

We also analyze the effect of different classifier-free guidance (CFG) values for \(\omega\) on the predicted user scores. As illustrated in Figure \ref{fig:rebeca-modscores}, while increasing \(\omega\) within the range \([3.5, 10.0]\) leads to a substantial improvement in scores, meaning that generations with stronger user-conditioning signal give better results overall. The gains saturate beyond \(\omega=10\) and CMMD distance begins to rise, suggesting diminishing returns.


%We conduct further ablation studies to better understand what is the behavior of \texttt{REBECA} under different circumstances. 

%We start exploring what happens to \texttt{REBECA} when it is coupled with prompts $T_1$ and $T_2$, previously used by our baselines. Table \ref{table:fid_cmmd_ablation} shows that \texttt{REBECA} achieves the best performance, with the lowest FID $(117.77)$ and CMMD $(0.68)$, indicating strong generative quality and user-conditioned alignment. We find that the prompt variations $T_1$ and $T_2$ worsen performance, despite their emphasis on quality. This suggests that the chosen prompts impact user alignment negatively, and produce a shift away from the real data distribution. Moreover, we test the effect of removing user embeddings on an otherwise identical architecture for our prior. Removing user embeddings (NoUE) significantly degrades performance, with the worst FID $(172.29)$ and CMMD $(1.20)$.
%Considering all \texttt{REBECA} model variations outperform NoUE, we confirm that user information is critical for improving generation quality and alignment with user preferences.

%An extra study we conduct is to understand the impact of various CFG values for $\omega$, and their impact on predicted user score. Figure \ref{fig:rebeca-modscores} shows that while they show a significant increase in the $(3.5, 10.0)$ interval, the scores saturate around afterward, and we achieve no major gains.

\begin{table}[t]
\label{table:fid_cmmd_ablation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{FID} $\downarrow$ & \textbf{CMMD} $\downarrow$ \\
\midrule
\textbf{REBECA} & \textbf{117.77} & \textbf{0.68} \\
REBECA-\(T_1\) & 124.22 & 0.78 \\
REBECA-\(T_2\) & 130.56 & 0.99 \\
REBECA-NoUE & 172.29 & 1.20\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\caption{Ablation studies of the \texttt{REBECA} model, evaluating varying prompt levels and the inclusion of user embeddings}
\vskip -0.1in
\end{table}



    





\section{Discussion}
In this paper, we introduce \texttt{REBECA}, a novel probabilistic framework for personalized recommendation beyond catalogs. Unlike traditional retrieval-based recommender systems, \texttt{REBECA} generates new items tailored to individual preferences using a diffusion-based generative model. By leveraging implicit user preference instead of explicit text prompts, our approach seamlessly integrates generative AI with recommendation pipelines.

Our experiments demonstrate that \texttt{REBECA} outperforms competing approaches that rely on textual prompts. Through topic modeling and a verifier-based evaluation, we established that our method generates images aligned with user interests, without requiring costly fine-tuning or extensive prompt engineering. Notably, we showed that introducing generic prompts can degrade personalization, highlighting the importance of direct preference modeling.
Despite its success, developing personalized generative recommenders presents key challenges, particularly the lack of suitable image datasets that pair user preferences (e.g., ratings) with visual content. Future research could focus on collecting and curating benchmark datasets tailored to this task.



The effectiveness of \texttt{REBECA} in capturing user-specific content and style preferences suggests broader applicability to other domains, such as music, video, and textual content generation. Several directions remain open for future work:
\begin{itemize}
    \item Extending \texttt{REBECA} to multimodal recommendations, incorporating text and audio to further enrich user experiences.
    \item Exploring dynamic personalization, where user preferences evolve over time, allowing the model to adapt accordingly.
    \item Developing fairness-aware generative recommenders, mitigating potential biases in the generated items.
\end{itemize}






\section*{Impact Statement}
Generative modeling, including images and videos, has significant misuse potential. It can trigger
negative consequences within the society in several ways. The primary concerns include various
types of disinformation, but also the potential to amplify stereotypes and unwanted biases. While our advancements in personalized recommendation improve user experience, they may also inadvertently strengthen usersâ€™ existing biases. Furthermore, fine-tuning such models carries the potential to align their outputs more closely with human values, but it also introduces ethical challenges that must be carefully managed. Recognizing these risks, we emphasize the need for responsible development, rigorous bias mitigation strategies, and ongoing evaluation to ensure that generative models serve society in a fair and beneficial manner.
% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\newpage

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}


\subsection{Training details}\label{appendix:training_details}



Our model is a diffusion-based generative model with a transformer backbone that operates in CLIP embedding space rather than pixel space. This design significantly reduces computational overhead while maintaining high expressiveness in learned representations. Notably, training each model requires less than 10 minutes to reach early stopping on a single NVIDIA RTX 4090 GPU.

Given the imbalance in the number of rated images per user in the FLICKR dataset, we employ strategic sampling to mitigate this issue. Specifically, in each batch, we sample an equal number of images per user to ensure balanced training.

For the transformer model within the diffusion framework, we conduct a grid search over key hyperparameters:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Number of layers:} \{2, 4, 8, 16\}
    \item \textbf{Number of heads:} \{2, 4, 8, 16, 32\}
    \item \textbf{Number of user tokens:} \{1, 2, 4, 8\}
    \item \textbf{Number of image tokens:} \{1, 2, 4, 8\}
    \item \textbf{Learning rate:} \{1e-4, 1e-3\}
\end{itemize}

Following this, we perform an additional grid search over:

\begin{itemize}[noitemsep,topsep=0pt]
    \item \textbf{Scheduler steps:} \{1000, 2000, 4000, 6000\}
    \item \textbf{Samples per user:} \{30, 50, 80, 110, 140\}
\end{itemize}

The selected model, achieving the lowest validation loss, is an 8-layer, 16-head decoder-only transformer with four user tokens, one image token, and one rating token. The optimal learning rate is 1e-4, and the best-performing configuration samples 80 images per user.

We use the AdamW optimizer \cite{loshchilov2018decoupled} with a learning rate of 1e-4 and a weight decay of 1e-5.



\subsection{Topic modeling}\label{appendix:topic_model}
We ask GPT-4o mini \cite{openai2025gpt4omini} to generate captions for each image using the following prompt:

\begin{tcolorbox}[colback=gray!10, colframe=gray, sharp corners, width=\textwidth]
\small
\textbf{Captioning prompt}

Analyze the given image and generate two captions:
1. One describing the **content** of the image (what objects, scenes, or elements are present).
2. One describing the **style** of the image (artistic elements, mood, colors, or visual style).

Provide the captions in a dictionary format where the keys are `content` and `style`.

\textbf{Example Output 1:}

\texttt{
\{"content": "A bustling city street during sunset, featuring people walking on sidewalks, cars stopped at a traffic light, and tall buildings in the background. A dog is being walked by its owner in the foreground.",
"style": "A warm and vibrant color palette with shades of orange and pink reflecting the sunset. The image has a modern, urban aesthetic with a lively and dynamic atmosphere."\}
}

\textbf{Example Output 2:}

\texttt{
\{"content": "A futuristic cityscape at night with towering skyscrapers illuminated by neon lights, flying vehicles zooming through the sky, and pedestrians walking on a glowing sidewalk.",
"style": "A cyberpunk-inspired aesthetic with vivid neon colors like electric blue, hot pink, and lime green. The mood is gritty yet energetic, evoking a futuristic, high-tech vibe."\}
}

\textbf{Example Output 3:}

\texttt{
\{"content": "A dense jungle scene with tall trees, vines hanging from branches, a small stream cutting through the forest floor, and a tiger partially hidden among the bushes.",
"style": "A richly detailed and vibrant depiction with deep greens and earthy tones. The style has a dramatic, cinematic feel with strong contrasts between light filtering through the trees and dark shadows."\}
}

\textbf{Now, provide the captions for the following image:}

\end{tcolorbox}

We trained two topic models separately for the content and style captions derived from the FLICKR dataset. Below are three content topics, each represented by their top-5 keywords:

\begin{itemize}
    \item \textbf{Graduation-related}: \textit{graduation gown, graduation gowns, graduation ceremony, celebrating graduation, graduation cap}.
    \item \textbf{Golf-related}: \textit{golf course, golf cart, green turf, turf field, green grass}.
    \item \textbf{Cat-related}: \textit{cat lounging, fur coat, domestic cat, fur trim, two cats}.
\end{itemize}

Similarly, three style topics and their top-5 keywords are shown below:

\begin{itemize}
    \item \textbf{Aesthetic Design}: \textit{decorative elements, intricate designs, visually appealing, Japanese aesthetics, visual appeal}.
    \item \textbf{Atmospheric Lighting}: \textit{atmospheric depiction, twinkling lights, glowing lights, holiday cheer, brightly colored}.
    \item \textbf{Seasonal \& Festive}: \textit{holiday cheer, autumn leaves, Halloween themed, floral patterns, floral arrangement}.
\end{itemize}

 
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
