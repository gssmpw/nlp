\section{Background and Related Work}
\paragraph{Large Reasoning Models} 
Recent advancements in large reasoning language models — such as OpenAI’s o1 and o3____ and DeepSeek-R1____ have substantially enhanced LLMs’ problem-solving capabilities by integrating structured reasoning mechanisms. For example, the OpenAI o1 model spends additional compute time to generate long chains of reasoning before producing a final answer, achieving PhD-level performance on challenging mathematical and scientific benchmarks ____. Building on this, the o3 series further refines the approach to boost performance ____. In parallel, DeepSeek-R1 pioneered a reasoning-oriented reinforcement learning training approach without supervised fine-tuning, demonstrating emergent reasoning behaviors and achieving performance comparable to o1 on math, coding, and science tasks ____. These models underscore the effectiveness of test-time self-reflection in addressing complex challenges, although significant hurdles remain in ensuring their safety and reliability.


\paragraph{Safety Benchmarking for LLMs} 
As the abilities of LLMs become stronger, various benchmarks have been proposed to evaluate the safety of LLMs in different safety categories and application domains____. 
These benchmarks evaluate whether LLMs comply with malicious queries and produce harmful content, with comprehensive categories that cover safety regulations from the government and company policies. ____ also evaluate whether the safety alignment of LLMs leads to over-sensitive to benign queries. 
More recently, there are safety evaluations for new applications of LLMs, including scenarios that are relevant to cybersecurity____, and LLM agents that make sequential decisions and receive feedback from the environments____.

\paragraph{Adversarial Attacks on LLMs} 
As LLMs become integral to real-world applications, adversaries are devising increasingly sophisticated strategies to subvert their safety mechanisms. 
One prominent tactic is prompt injection____, wherein adversaries insert additional instructions into the input text to override the model’s intended directives or trigger harmful behavior. 
Another major threat comes from jailbreak attacks, which trick LLMs into responding to queries they would typically refuse. For example, strategy-based jailbreaks leverage natural language constructs—often by presenting hypothetical scenarios—to manipulate model behavior____, while optimization jailbreaks focus on optimizing a prefix string to maximize the likelihood of generating responses to otherwise harmful queries____.
In our work, we select representative safety benchmarks and attacks to analyze the safety performance and behaviors of large reasoning models. Further, we introduce multi-faceted safety evaluation to better understand their safety risks.