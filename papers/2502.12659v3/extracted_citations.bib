@article{andriushchenko2024agentharm,
  title={Agentharm: A benchmark for measuring harmfulness of llm agents},
  author={Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and others},
  journal={arXiv preprint arXiv:2410.09024},
  year={2024}
}

@article{bhatt2024cyberseceval,
  title={Cyberseceval 2: A wide-ranging cybersecurity evaluation suite for large language models},
  author={Bhatt, Manish and Chennabasappa, Sahana and Li, Yue and Nikolaidis, Cyrus and Song, Daniel and Wan, Shengye and Ahmad, Faizan and Aschermann, Cornelius and Chen, Yaohui and Kapil, Dhaval and others},
  journal={arXiv preprint arXiv:2404.13161},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{jiang2024wildteaming,
  title={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models},
  author={Jiang, Liwei and Rao, Kavel and Han, Seungju and Ettinger, Allyson and Brahman, Faeze and Kumar, Sachin and Mireshghallah, Niloofar and Lu, Ximing and Sap, Maarten and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2406.18510},
  year={2024}
}

@article{li2024llm,
  title={Llm defenses are not robust to multi-turn human jailbreaks yet},
  author={Li, Nathaniel and Han, Ziwen and Steneker, Ian and Primack, Willow and Goodside, Riley and Zhang, Hugh and Wang, Zifan and Menghini, Cristina and Yue, Summer},
  journal={arXiv preprint arXiv:2408.15221},
  year={2024}
}

@article{li2024salad,
  title={Salad-bench: A hierarchical and comprehensive safety benchmark for large language models},
  author={Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}

@article{liao2024amplegcg,
  title={Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms},
  author={Liao, Zeyi and Sun, Huan},
  journal={arXiv preprint arXiv:2404.07921},
  year={2024}
}

@article{liu2024autodan,
  title={Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms},
  author={Liu, Xiaogeng and Li, Peiran and Suh, Edward and Vorobeychik, Yevgeniy and Mao, Zhuoqing and Jha, Somesh and McDaniel, Patrick and Sun, Huan and Li, Bo and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2410.05295},
  year={2024}
}

@misc{o1card,
  author    = {OpenAI},
  title     = {O1 System Card},
  year      = {2025},
  url       = {https://cdn.openai.com/o1-system-card-20241205.pdf}
}

@misc{o3minicard,
  author    = {OpenAI},
  title     = {O3 Mini System Card},
  year      = {2025},
  url       = {https://cdn.openai.com/o3-mini-system-card.pdf}
}

@article{rottger2023xstest,
  title={Xstest: A test suite for identifying exaggerated safety behaviours in large language models},
  author={R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  journal={arXiv preprint arXiv:2308.01263},
  year={2023}
}

@article{wan2024cyberseceval,
  title={Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models},
  author={Wan, Shengye and Nikolaidis, Cyrus and Song, Daniel and Molnar, David and Crnkovich, James and Grace, Jayson and Bhatt, Manish and Chennabasappa, Sahana and Whitman, Spencer and Ding, Stephanie and others},
  journal={arXiv preprint arXiv:2408.01605},
  year={2024}
}

@article{wang2023not,
  title={Do-not-answer: A dataset for evaluating safeguards in llms},
  author={Wang, Yuxia and Li, Haonan and Han, Xudong and Nakov, Preslav and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2308.13387},
  year={2023}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xie2024sorry,
  title={Sorry-bench: Systematically evaluating large language model safety refusal behaviors},
  author={Xie, Tinghao and Qi, Xiangyu and Zeng, Yi and Huang, Yangsibo and Sehwag, Udari Madhushani and Huang, Kaixuan and He, Luxi and Wei, Boyi and Li, Dacheng and Sheng, Ying and others},
  journal={arXiv preprint arXiv:2406.14598},
  year={2024}
}

@article{yi2023benchmarking,
  title={Benchmarking and defending against indirect prompt injection attacks on large language models},
  author={Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  journal={arXiv preprint arXiv:2312.14197},
  year={2023}
}

@article{zeng2024air,
  title={Air-bench 2024: A safety benchmark based on risk categories from regulations and policies},
  author={Zeng, Yi and Yang, Yu and Zhou, Andy and Tan, Jeffrey Ziwei and Tu, Yuheng and Mai, Yifan and Klyman, Kevin and Pan, Minzhou and Jia, Ruoxi and Song, Dawn and others},
  journal={arXiv preprint arXiv:2407.17436},
  year={2024}
}

@article{zhan2024injecagent,
  title={Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents},
  author={Zhan, Qiusi and Liang, Zhixiang and Ying, Zifan and Kang, Daniel},
  journal={arXiv preprint arXiv:2403.02691},
  year={2024}
}

@article{zhang2024agent,
  title={Agent security bench (asb): Formalizing and benchmarking attacks and defenses in llm-based agents},
  author={Zhang, Hanrong and Huang, Jingyuan and Mei, Kai and Yao, Yifei and Wang, Zhenting and Zhan, Chenlu and Wang, Hongwei and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2410.02644},
  year={2024}
}

@inproceedings{zhu2024autodan,
  title={AutoDAN: interpretable gradient-based adversarial attacks on large language models},
  author={Zhu, Sicheng and Zhang, Ruiyi and An, Bang and Wu, Gang and Barrow, Joe and Wang, Zichao and Huang, Furong and Nenkova, Ani and Sun, Tong},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

