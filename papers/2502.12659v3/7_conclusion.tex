\section{Discussion and Conclusion}
\vspace{-0.27cm}
In this paper, we present a comprehensive multi-faceted analysis of the safety of large reasoning models.  
In our analysis, we identify a significant safety gap between the DeepSeek-R1 models and the o3-mini. 
% We attribute this gap to two main factors. First, although the R1 models have undergone safety alignment training, the extent of their alignment remains insufficient, especially in certain safety categories. 
We suggest three potential directions for improvement. First, enhancing the extent of safety alignment in R1 models, as their current alignment training may be insufficient, especially in certain safety categories. 
Second, advanced safety alignment techniquesâ€”such as rule-based rewards and methods that leverage reasoning ability to enhance safety~\cite{mu2024rule,guan2412deliberative} could be explored.
Third, developing new training strategies to enhance their explicit safety reasoning, in terms of activating safety thinking and improving the precision of safety judgments.
In addition, the distilled R1 model compromises the original safety performance consistently in all the safety tests. Therefore, additional safety alignment is required after reasoning distillation. 

Moreover, we find that with stronger reasoning ability, the R1 models provide more help to the malicious queries compared with their non-reasoning counterparts.  Therefore,  their unsafe responses are more harmful.
% Therefore, their unsafe responses have a higher harmfulness level. 
This further underscores the necessity of enhancing the safety of R1 models.
Finally, within the outputs of large reasoning models, we find that the thinking process may contain hidden safety risks that are not reflected in the final answer. This presents a new challenge brought by reasoning models, which requires future work to address.

% \section{Conclusion}
% \xuandong{Before the conclusion, we should discuss the current landscape of reasoning model safety. In particular, we should analyze the findings from \citet{zaremba2025trading}, which suggest that reasoning models can exhibit improved safety performance. This raises the question: why does DeepSeek-R1 fail to meet similar safety standards?  We could have some speculations over this. 
% Additionally, future reasoning models could incorporate safety rewards as rule-based rewards \citep{mu2024rule} during reinforcement learning training to enhance their safety compliance. Finally, we should discuss some potential mitigation strategies to address safety risks within the reasoning content.}
% \xin{agreed. We would like to provide some guidelines of how to improve reasoning safety.}

\section*{Limitations}
While our study provides a comprehensive safety analysis of large reasoning models, there are still limitations.
First, our analysis highlights the safety gap between open-source large reasoning models like DeepSeek-R1 and proprietary models such as o3-mini. However, the proprietary models' full safety mechanisms remain opaque, limiting direct comparisons and insights into their superior safety performance. 
Second, our study reveals that the reasoning process in large reasoning models often contains safety risks not present in final responses. While we identify trends in unsafe reasoning outputs, our work does not propose specific mitigation strategies to refine the reasoning process.

\section*{Acknowledgment}
This project was partially sponsored by the Cisco Research Award and benefited from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.