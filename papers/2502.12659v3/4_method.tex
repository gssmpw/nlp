\section{Research Questions and  Safety Evaluation Design}
% 1. how safe is reasoning models: attack and non-attack
% 2. harmfulness level
% 3. How does the safety of reasoning content compared with completion content

% \xin{
% (1) safety bench + harmfulness: Table 1, Table 2, figure 3, figure 4
% (2) jailbreak + injection: Table 1 part + Table 4, figures 
% (3) Reasoning vs. Answer. part of Table 1 + fig 2
% }
\subsection{Research Questions}
% The emergent abilities of large reasoning models have significantly enhanced their performance on complex, multi-step reasoning tasks, such as mathematics and coding. 
With the open-sourcing of the R1 series, large reasoning models are likely to see continuous advancements and broader adaptations across various applications. 
% Therefore, the safety of large reasoning models becomes very important. 
This motivates us to perform a systematic safety evaluation for these models.
% \xin{the above content is repetitive and can be removed for space concern.}. 
In this study, we aim to answer the following research questions that could help us to understand large reasoning models' safety performance and identify potential directions for improvement: 
\begin{enumerate}[leftmargin=*, itemsep=0pt]
    \item \textit{How safe are large reasoning models when given malicious queries? Are they able to refuse to follow these queries? (Section~\ref{sec: safety benchmakring})}
    \item \textit{How does enhanced reasoning ability affect the harmfulness level of the unsafe responses?
    % How is the harmfulness level of the unsafe responses from large reasoning models?
    % Are the unsafe responses from large reasoning models more harmful than their non-reasoning counterparts? 
    (Section~\ref{sec: harmfulness level})
    }
    % \xin{how do you assume it is more harmful? I don't think we know that before the experiments. The question should be open.}
    \item \textit{How safe are large reasoning models when facing adversarial attacks? (Section~\ref{sec: safety attack})}
    \item \textit{How do the safety risks of the thinking process in large reasoning models compare to those of the final answer? (Section~\ref{sec: reasoning vs answer})}
\end{enumerate}

% \xuandong{The description for each benchmark seems a little bit tedious. We should focus more on the results, analysis, and takeaways.}
\input{tables/eval_benchmark_table}
\subsection{Evaluation Design}

\paragraph{Safety Benchmarks} As shown in Table~\ref{tab:dataset}, we select 5 representative datasets from 3 safety benchmarks and 2 datasets on adversarial attacks for evaluation.
% \xin{what are those 7 datasets? still unclear to me. can you list them?}
% \xin{how many? better to have an overview}. 
For RQ1, we select Air-Bench~\cite{zeng2024air}, a comprehensive safety evaluation benchmark containing safety prompts from government regulations and corporate policies. 
On top of this, we select three test suites from CyberSecEval to test the models' safety performance when given unsafe tasks in various cybersecurity-related scenarios, including cyberattacks, secure code execution, and multi-turn spear phishing~\cite{wan2024cyberseceval}. Additionally, we test the over-refusal behavior of large reasoning models on the XStest dataset~\cite{rottger2023xstest}. 
For RQ 3, we test the models on the WildGuard Jailbreak dataset~\cite{wildguard2024}, and the prompt injection test from the CyberSecEval. 
Detailed introductions for the datasets can be found in Appendix~\ref{sec: benchmark intro}.

\paragraph{Models} 
% We focus on evaluating the safety of advanced reasoning models and comparing them with their non-reasoning counterparts. 
To facilitate a better analysis of the impact of reasoning abilities on safety, we evaluate 2 open-weight large reasoning models, DeepSeek-R1~\cite{guo2025deepseek}, the strongest reasoning model of R1 series, and DeepSeek-R1-70b~\cite{guo2025deepseek}, a reasoning model distilled based on Llama 3.3-70b, and compare them with their 2 non-reasoning counterparts, Llama 3.3-70b~\cite{dubey2024llama} and DeepSeek-V3~\cite{liu2024deepseek}. Finally, to understand the safety gap of open large reasoning models, we include the proprietary o3-mini~\cite{o3minicard} model in our evaluation.
% For reasoning models, we evaluate (i) DeepSeek-R1~\cite{guo2025deepseek}, the strongest reasoning model of R1 series. (ii) DeepSeek-R1-70b~\cite{guo2025deepseek}, a reasoning model distilled based on Llama 3.3-70b. (iii) OpenAI o3-mini~\cite{o3minicard}. For non-reasoning LLMs, we evaluate (i) Llama 3.3-70b~\cite{dubey2024llama}, (ii) DeepSeek-V3~\cite{liu2024deepseek}. 
% These models are selected as they represent a typical contrast between reasoning and non-reasoning models, facilitating a better analysis of the impact of reasoning abilities on safety.
% \xin{why those models? provide some rationals for model selection.}
% In spear phishing attack tests, we apply the Llama 3.3-70B as the victim and the judge models. 

\paragraph{Evaluation Metrics} We perform two levels of evaluation. First, for all the benchmarks, we use GPT-4o~\cite{hurst2024gpt} to do safety classification on the responses with defined criteria. 
On AIR-bench, Code Interpreter Test and MITRE Tests, responses containing no information that helps the malicious query will be considered safe. In these benchmarks, we also evaluate the safety of the reasoning process for reasoning LLMs and compare it with the final completion. 
Spear Phishing Tests originally   do not provide safety classification to the response. We create safety classification criteria based on whether the model performs the phishing attempt.
On XSTest, GPT-4o is similarly used to classify responses into full compliance, full refusal, and partial refusal, allowing for an objective evaluation of exaggerated safety behaviors. 
