\section{Background and Related Work}

\paragraph{Large Reasoning Models} 
Recent advancements in large reasoning language models — such as OpenAI’s o1 and o3~\cite{o1card,o3minicard} and DeepSeek-R1~\cite{guo2025deepseek} have substantially enhanced LLMs’ problem-solving capabilities by integrating structured reasoning mechanisms. For example, the OpenAI o1 model spends additional compute time to generate long chains of reasoning before producing a final answer, achieving PhD-level performance on challenging mathematical and scientific benchmarks \citep{o1card}. Building on this, the o3 series further refines the approach to boost performance \citep{o3minicard}. In parallel, DeepSeek-R1 pioneered a reasoning-oriented reinforcement learning training approach without supervised fine-tuning, demonstrating emergent reasoning behaviors and achieving performance comparable to o1 on math, coding, and science tasks \cite{guo2025deepseek}. These models underscore the effectiveness of test-time self-reflection in addressing complex challenges, although significant hurdles remain in ensuring their safety and reliability.


\paragraph{Safety Benchmarking for LLMs} 
As the abilities of LLMs become stronger, various benchmarks have been proposed to evaluate the safety of LLMs in different safety categories and application domains~\cite{wang2023not,bhatt2024cyberseceval,wan2024cyberseceval,li2024salad,xie2024sorry,zeng2024air,andriushchenko2024agentharm}. 
These benchmarks evaluate whether LLMs comply with malicious queries and produce harmful content, with comprehensive categories that cover safety regulations from the government and company policies. \citet{rottger2023xstest} also evaluate whether the safety alignment of LLMs leads to over-sensitive to benign queries. 
More recently, there are safety evaluations for new applications of LLMs, including scenarios that are relevant to cybersecurity~\cite{wan2024cyberseceval,bhatt2024cyberseceval}, and LLM agents that make sequential decisions and receive feedback from the environments~\cite{andriushchenko2024agentharm}.

\paragraph{Adversarial Attacks on LLMs} 
As LLMs become integral to real-world applications, adversaries are devising increasingly sophisticated strategies to subvert their safety mechanisms. 
One prominent tactic is prompt injection~\cite{yi2023benchmarking,zhan2024injecagent,zhang2024agent}, wherein adversaries insert additional instructions into the input text to override the model’s intended directives or trigger harmful behavior. 
Another major threat comes from jailbreak attacks, which trick LLMs into responding to queries they would typically refuse. For example, strategy-based jailbreaks leverage natural language constructs—often by presenting hypothetical scenarios—to manipulate model behavior~\cite{wei2024jailbroken,jiang2024wildteaming,zhu2024autodan,li2024llm,liu2024autodan}, while optimization jailbreaks focus on optimizing a prefix string to maximize the likelihood of generating responses to otherwise harmful queries~\cite{zou2023universal,liao2024amplegcg}.
In our work, we select representative safety benchmarks and attacks to analyze the safety performance and behaviors of large reasoning models. Further, we introduce multi-faceted safety evaluation to better understand their safety risks. 




