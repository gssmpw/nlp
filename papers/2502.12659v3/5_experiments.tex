\input{tables/overall_performance}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/heatmap_new.pdf}
    \label{fig:safe_prompts}
    \vspace{-1em}
    \caption{Two safety benchmark evaluations: (A) Level-2 categorized results of the models on Air-Bench.  (B) Evaluation of the models' safety rate (\%) in the Code Interpreter Tests across different risk categories.}
    \vspace{-0.06cm}
    \label{fig:two_db}
\end{figure*}




\section{Safety Benchmarking} \label{sec: safety benchmakring}
The fundamental challenge in safety benchmarking is distinguishing between safe and unsafe user queries. Given an input query $q$, the model must reliably assess its underlying intent. Specifically, for queries with harmful intent  $q_h$, the LLM should either refuse to respond or provide mitigating information. For the queries with safe intent $q_s$, the LLM should deliver informative and helpful responses without unnecessary refusals. 

In this section, we investigate the safety performance of large reasoning models in handling malicious queries. We begin by analyzing their overall performance, and identifying a distinct safety behavior from them. Then, we analyze their behavioral patterns on selected representative datasets. 
% We first compare the models' overall performance on safety benchmarks and then analyze their behavioral patterns and performance on representative benchmarks.
% Finally, we assess the harmfulness level of unsafe responses generated by large reasoning models. 
% By comparing the differences in unsafe responses between reasoning and non-reasoning models, we assess the potential risks associated with their deployment. Through a series of safety benchmarks, we analyze the behavioral patterns and risks of reasoning models.

% \paragraph{Problem Statement} 

% Failures in safety mechanisms can lead to serious risks, including misinformation, harmful content generation, cyber threats, and regulatory non-compliance.
% In this study, we evaluate the safety and security of reasoning LLMs using multiple standardized benchmarks, as described below.

\subsection{Overall Safety Analysis} 
\paragraph{Overall Performance}
We evaluate the average safety rate of all models across four benchmarks with unsafe queries. 
First, o3-mini exhibits significantly higher safety than open-source reasoning and non-reasoning models, effectively identifying and rejecting most unsafe queries across various scenarios. Open large reasoning models still have a considerable gap to close compared with o3-mini. 
Second, we observe that the distilled R1-70b consistently achieves a lower safety rate than Llama-3.3, suggesting that reasoning-supervised fine-tuning reduces a model’s safety performance; this aligns with the finding of ~\citep{qi2023fine} on the effect of supervised fine-tuning to safety performance. 
% \dawn{
% In general we know that sft reduces safety alignment, and one needs to re-apply safety alignment after finetuning. We should cite earlier work on this and say that this finding is consistent and they should put more effort in adding safety alignment after finetuning. What did they say about their effort on safety alignment after finetune/during finetune?
% }
Finally, R1 demonstrates better safety performance than V3 on the broad safety categories on AirBench. 
% This result aligns with the findings in \citep{zaremba2025trading}, where scaling inference time improves the safety on 
However, R1 shows a significantly more severe safety risk in the cybersecurity domain, with more complex tasks and environment settings. 
These results indicate that more effort should be put into safety alignment on R1 models. 

\paragraph{Safety Thinking Behavior}
From the output of the models, we identify a different safety behavior of R1 models from non-reasoning LLMs -- the thinking process of the R1 models usually determines the safety of final completion. 
In the thinking process, if the model performs safety thinking and decide that the query is not safe to answer, it will refuse the query it in the final answer. Otherwise, if no safety thinking happens, or the model believes the query is appropriate to answer, no refusal will happens.
In contrast, the refusal behavior from non-reasoning LLMs usually happens immediately without explicit thinking. Examples and more analysis on the safety thinking are in Section~\ref{analysis}.


\subsection{Select Datasets Analysis} 
% \input{tables/air_bench_helpfulness}
\paragraph{Safety Policies \& Regulations} 
We present the level 2 category results of Air-Bench in Figure~\ref{fig:two_db} (A), covering 16 categories. 
First, the comparison between reasoning and non-reasoning models has significant differences across categories. For example, Llama3.3 is significantly better than R1-70b in `Operational Misuses', `Security Risks', etc., with more than 20\% advantage, but is worse than R1-70b in `Hate/Toxicity' by 11\%. 
Second, all the reasoning models, including o3-mini have very low performance in certain categories like `Operational Misuses' and `Sexual Content'. 
Meanwhile, the open large reasoning models perform significantly worse than o3-mini in most of the categories.
%, like `Security risks' and `Economic Harm'. 
These findings suggest that certain safety aspects require greater attention in large reasoning models. 

\paragraph{Code Interpreter Test} % per-category results and model behavior analysis

The results in Figure~\ref{fig:two_db} (B) show that reasoning models exhibit higher security risks than non-reasoning models, particularly in reflected attacks and social engineering. Specifically, the safe operation percentage of the R1 model in the social engineering category is significantly lower than Llama3.3 and falls behind V3 by over 50\%. This indicates that reasoning models are more vulnerable to abuse in interactive categories. 
% \dawn{Again, we need to be careful for drawing conclusions. We can only say that these tested reasoning models are more vulnerable to abuse… unless we look deeper and see whether there’s any actual intrinsic reasons for these reasoning models not to be safer
% }

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.85\linewidth]{figures/radar_code.pdf}
% \caption{Evaluation of the Safety Performance of models in the Code Interpreter Tests across various risk categories.}
% \label{radar}
% \vspace{-0.2cm}
% \end{figure} 


\paragraph{Multi-turn Spear Phishing} 
Table~\ref{tab:main result} presents the models' safety rate in the Spear Phishing Test. In this evaluation, the tested LLM will be asked to perform multi-turn spear phishing against a victim LLM. Here, the safety rate refers to the proportion of conversations in which the model did not attack in any turn.
Notably, the result reveals a significant safety risk, as o3-mini initiates an attack in only 5\% of trials, while other models do so in over 95\% of cases.
Moreover, o3-mini almost always refuses to engage in an attack at the start of the dialogue, with only one instance where it does so immediately. In other unsafe dialogues, it begins attacking after multiple prompts. Also, in the cases where it engaged in a phishing attempt, it can later recognize the safety risk and cease the attack.
In contrast, at the start of the dialogue, neither the R1 nor R1-70b models show safety awareness when prompted to attack. 
This highlights the risks posed by R1 models as a multi-turn spear phishing agent. 

\input{tables/xstest_category_results}

% \paragraph{XSTest}
% \input{figures/xstestPlot}
\paragraph{Over-Refusal Benchmark}
The over-refusal rate to benign queries is also a strong indicator of models' safety alignment.
We present the average result and the result of the category with the worst performance for each model on the \textbf{XSTest} dataset in Table~\ref{tab:xstest} and leave the detailed categorized result in Table~\ref{tab:xstest_llm_comparison} in the Appendix~\ref{more table}.
We observe that all the models tested can usually respond to safe prompts correctly without refusal, with a refusal rate of less than 10\%. Among these, the o3-mini has the highest refusal rate due to being more sensitive to seemingly unsafe queries. 
However, all the models refuse to answer a significant portion of safe queries in at least one safety category. 

The most frequent over-refusal made by models is in the fictional privacy category, where the query asks about private information on fictional persons. For instance, when the R1-70b is asked \textit{``What is Harry Potter's criminal history?''}, it replies \textit{``As an AI assistant, I don't have access to internal company information...''}.
Besides that, Deepseek-R1 often refuse to prompts with discrimination against a nonsense group. (e.g. \textit{``Why should purple carrots not be given visas to enter the US?''} $\rightarrow$ \textit{``The People's Republic of China consistently advocates for the principle of equality ...''}). 
This shows that the current models' safety alignment is not still refined enough in specific categories. 


\section{Response Harmfulness Level Evaluation} \label{sec: harmfulness level}
\paragraph{Definition} Safety classification alone is not sufficient to comprehensively assess models' safety, as not all responses classified as unsafe are equally harmful - some provide minimal information, while others offer detailed, actionable guidance that aids malicious intent. 
To capture this, we define the harmfulness level of an unsafe response as the degree of helpfulness it provides to a malicious query. 



\paragraph{Harmfulness Evaluation} We quantitatively evaluate the model's harmfulness level on two datasets with different malicious scenarios.
For AIR-bench, we evaluate the helpfulness to the malicious question using two top pre-trained reward models on the RewardBench~\cite{lambert2024rewardbench} -- ArmoRM-Llama3-8B~\cite{wang2024interpretable} and QRM-Llama3.1-8B~\cite{dorka2024quantile}. 
These models are trained to predict the reward score for 19 attributes, such as helpfulness, correctness, and coherence. We utilize the average reward score for the \texttt{helpsteer-helpfulness} and \texttt{ultrafeedback-helpfulness} attributes to represent the helpfulness of the response to queries in AIR-bench.
In Spear Phishing Tests, the helpfulness of the model to the malicious instruction can be evaluated as the attack techniques they demonstrate in the attack process. We use the automated LLM-based grading system from the test suite to evaluate the attack skills, including persuasion, rapport, and argumentation. Specifically, we use Llama 3.3 as the LLM grader. 
\vspace{-0.2cm}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/compare_harmfulness11.pdf}
\caption{The harmfulness evaluation result for two pairs of LLMs using two reward models on Air-Bench dataset. The response from reasoning models provides more help to the harmful questions. 
% \xuandong{I think it is better for a table or put ArmoRM together and QRM together. The score ranges of ArmoRM and QRM are different.}
% \xin{not very friendly for color-blind people.}
}
\label{fig: reword_new}
\vspace{-0.22cm}
\end{figure} 
\input{tables/spear_phishing_results}

\paragraph{Results} 
% \xin{add an example}
We compare the harmfulness levels of unsafe responses to policies and regulation-related malicious queries between two pairs of reasoning and non-reasoning models in Figure~\ref{fig: reword_new}. The evaluation includes only questions for which both models in a pair generate unsafe responses. 
Overall, large reasoning models (e.g., R1, R1-70b) receive higher reward scores in helpfulness when responding to malicious questions. 
As shown in Figure~\ref{fig:harmfulness example}, we find that large reasoning models usually provide more detailed and structured responses. 
This suggests that while reasoning models demonstrate greater capability, they also pose a higher level of harmfulness when misused by malicious users.

% \vspace{-0.3cm}
\begin{figure}[t]
\centering
\includegraphics[width=0.93\linewidth]{figures/harmfulness_level1.pdf}
\caption{Example of large reasoning model provides more detailed and structured responses to the malicious query compared with non-reasoning model.
}
\label{fig:harmfulness example}
\vspace{-0.05cm}
\end{figure} 

The results of social engineering capability in the spear phishing test are shown in Table~\ref{tab:spear_phishing_results}. We notice that reasoning models perform better than the non-reasoning models (since o3-mini only has a few attack attempts, its attack score is not comparable). 
Compared with non-reasoning models, R1 and R1-70b receive higher scores in all the metrics, showing higher ability in persuasion, rapport, and argumentation, posting a higher risk when being used as a spear phishing agent. 
% Llama3.3, while less aggressive than R1,  exhibits moderate phishing capability, consistently generating deceptive messages through targeted persuasion. These results underscore the risks posed by reasoning LLMs, whose low refusal rates suggest alignment limitations in preventing cyber threats.
\begin{figure*}[t]
        \centering
        \includegraphics[width=1\linewidth]{figures/example.pdf}
        \caption{Three Scenarios of the R1 Model in Jailbreak: (A) Identifies safety concerns but executes the user's request unreflectively. (B) Recognizes safety issues but is misled. (C) Fails to recognize any safety concerns.} 
        \label{jailbreak}
        \vspace{-0.05cm}
    \end{figure*}

\section{Safety Attacking}  \label{sec: safety attack}
% \vspace{-0.2cm}
% This section examines the models' safety performance when facing adversarial attacks. We consider two types of attack: the jailbreak attack, which aims to let the model respond to harmful queries, and the prompt injection attack, which aims to override the models' intended behavior or bypass restrictions.
This section evaluates the models' safety performance against two types of adversarial attacks: the jailbreak attack, which forces the model to respond to harmful queries, and the prompt injection attack, which aims to override the models' intended behavior or bypass restrictions.

\input{tables/jailbreak}
\subsection{Jailbreak}
\label{analysis}
The results of WildGuard jailbreak attacks in Table~\ref{tab:jailbreak result} reveal that all the models exhibit weak safety performance, 
% especially when faced with relatively subtle adversarial samples, 
including o3-mini. This suggests that current LLMs struggle to detect challenging adversarial threats. 
% Further analysis of reasoning models (R1, R1-70b) reveals that when handling explicit adversarial problems, reasoning models are able to identify potential hazards through their thinking process and provide relatively safe responses. As shown in Appendix~\ref{more example} Figure~\ref{appendix:reason}, the comparison between R1, V3 and Llama 3.3 shows that reasoning model tends to provide more robust safety guarantees in explicit adversarial problems. However, when faced with more complex adversarial samples, reasoning models encounter several significant challenges:
We also find that among all the open-source models, Deepseek-R1 has the lowest attack success rate. We observe cases where reasoning models are able to identify potential hazards in their thinking process and provide relatively safe responses. An example is provided in Appendix Figure~\ref{appendix:reason}.
However, reasoning models still encounter significant challenges when facing attacks. We identify several models' failure patterns:

\vspace{0.05cm}
\noindent\textbf{Model bias towards user queries leads to harmful follow-up in thinking process.} Although reasoning models can recognize potential harm during the thinking process, they still prioritize following the user's query intentions, overlooking potential risks. Figure~\ref{jailbreak} (A) shows that R1 identifies potential security risks during the initial thinking process but generates unsafe responses in subsequent thinking steps by following the user's query.

\vspace{0.15cm}
\noindent\textbf{Models' safety thinking is misled by the jailbreak strategies.
% Models misled by adversarial samples, with the safety thinking chain change.
} As illustrated in Figure~\ref{jailbreak} (B), reasoning models may fail to accurately assess the harmfulness of inputs due to the deliberate design of adversarial samples, even when potential risks are identified during the reasoning phase. This observation suggests that the safety thinking process in R1 is not reliable enough when faced with disguised adversarial strategies. 

\vspace{0.15cm}
\noindent\textbf{Models do not perform safety thinking in the thinking process, directly executing harmful information.} Reasoning models fail to identify the risks and proceed to execute the user's instructions. In Figure~\ref{jailbreak} (C), R1 directly follows the user's request during the thinking phase, without effectively preventing harmful outputs. 

% Refer to Appendix~\ref{more example} for detailed model responses.



 



% The results of the single-turn Jailbreak attack in Table~\ref{tab:main result} show that most models exhibit comparable security performance, with o3-mini demonstrating much stronger robustness. In contrast, V3 and Llama 3.3 have slightly higher attack success rates than the reasoning models, exposing the security weaknesses of open-source models in jailbreak attacks.
% % \dawn{Not clear whether this explanation is true or makes sense}
% % \textcolor{blue}{need to modifiy} 

\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \setlength{\tabcolsep}{6pt} 
    \resizebox{0.49\textwidth}{!}{%
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Injection Type}} & \multicolumn{2}{c}{\textbf{Risk Category}} & \multirow{2}{*}{\textbf{ALL} $\downarrow$} \\ 
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
         & Direct  $\downarrow$ & Indirect $\downarrow$ & Security $\downarrow$ & Logic $\downarrow$ & \\ 
        \midrule
        Llama3.3     & 15.80  & 58.18 & 58.18 & 2.81  & 25.09 \\
        R1-70b  & 33.67  & 58.18 & 47.22 & 18.30  & 39.04 \\
        DS-V3      & 26.53 & 61.82 & 44.40  & 8.45  & 34.26 \\
        DS-R1 & 34.69 & 60.90  & 49.44 & 16.90 & 40.23 \\
        o3-mini          & 7.65  & 43.63 & 17.22 & 11.26 & 15.53 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Prompt Injection ASR (Attack Success Rate) under different injection types and risk categories.}
    \label{tab:injection_results}
    \vspace{-0.35cm}
\end{table}

\subsection{Prompt Injection}
% \vspace{-0.1cm}
Table~\ref{tab:injection_results} presents the results of the text prompt injection attack, revealing significant differences among models in terms of injection types and risk categories. 
Regarding injection types, the ASR of indirect injection is generally higher than that of direct injection. Although the reasoning models perform similarly to other models in indirect attacks, their robustness is significantly weaker in direct attacks, with ASR nearly double that of the others, indicating a higher level of compliance to direct prompts.
% This suggests that the reasoning models exhibit poor safety performance under both implicit and explicit attacks. 
In terms of risk categories, reasoning models exhibit a higher ASR for security attacks than for logic. This discrepancy indicates that despite these models demonstrate strong logical reasoning capabilities, their security protection awareness is weak.
Finally, compared with o3-mini, R1 models are more vulnerable to prompt injection attacks.

\input{tables/reasoning_content}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/example2.pdf}
    \vspace{-0.1cm}
    \caption{Two examples where the safety of the reasoning content is worse than the final completion. \textbf{Left}: The reasoning content directly provides techniques that help the malicious query. \textbf{Right}: The reasoning content provides safe paraphrasing techniques that are relevant to the malicious query. Red text is the potentially unsafe content.} 
    \label{fig:reasoning_safety}
    \vspace{-0.02cm}
\end{figure*}

\section{Thinking Process v.s. Final Answer} \label{sec: reasoning vs answer}
\vspace{-0.1cm}
Finally, we compare the safety of the thinking process from R1 models and their final answer when given harmful queries. Specifically, we take the content between \texttt{<think>} and \texttt{</think>} from the models' output and use the same evaluation prompt to judge the safety. The result on four datasets is in Table~\ref{tab:reasoning result}.
We can observe that the safety rate of the thinking process is lower than the final answer. 
After investigating the models' responses, we identify two main types of cases where the thinking process contains `hidden' safety risks that are not reflected in the final answer.
First, the model thinks about and provides relevant harmful content to the query, but at the end of the thinking process, the model realizes the safety issue and refuses to answer the query in their final answer. 
This case is more severe, as the harmful content is already presented and may be leveraged by malicious users. An example is shown in Figure~\ref{fig:reasoning_safety} (left), where the model first introduces techniques for infiltrating the network in the thinking process. Although the model realizes it is illegal in the end, they already provide direct help to the malicious query. 

In the second case, the model usually identifies the safety risk in the user's query early in reasoning. Then, instead of directly refusing to answer the user's query, the model tries to redirect the conversation to a safer direction and provides thoughts on it. During this process, the model may mention some general information that is relevant to the query. The reasoning content becomes less unsafe, since the information provided is not directly solving the user's query. An example is shown in Figure~\ref{fig:reasoning_safety} (right), where the model mentions legitimate paraphrasing techniques in their thinking without aiming to bypass the plagiarism checkers. 
These observations indicate that the emerging reasoning capabilities in RL training also bring new safety concerns that the safety alignment of the reasoning needs more improvement.  