\section{Introduction}

% \xuandong{Shall we state in the intro that: 
% We hypothesize that the safety performance of R1 may be attributed to limited safety-specific training, and the process of fine-tuning the Llama 70B model (distilled-R1) could have inadvertently degraded its pre-existing safety alignment~\citep{qi2023fine}.  
% Reasoning models possess the potential for enhanced safety, as demonstrated in OpenAI's reports~\citep{zaremba2025trading}. However, if open-source models disclose reasoning or thinking steps, additional safety alignment is essential to mitigate the potential risks.}
% \dawn{It’d be good to say something like this in intro and abstract; and it’d be good to check what evidence we can find to support this as well}

The landscape of large language models (LLMs) is evolving with the advent of large reasoning models like OpenAI-o3~\cite{o3minicard} and DeepSeek-R1~\cite{guo2025deepseek}, which leverage reinforcement learning to enhance complex reasoning. Unlike conventional LLMs, these models ``think'' (generate a structured chain-of-thought employing specialized output formats) before producing a final response. Reasoning models have superior performance in problem-solving, coding, scientific reasoning, and multi-step logical inference. 
However, their increased capabilities, combined with the recent open-sourcing of DeepSeek-R1, amplify their potential safety risks across a broad range of applications. 
Therefore, a comprehensive safety analysis of these reasoning models is essential to identify and mitigate their associated risks.


In this work, as shown in Figure~\ref{fig:fig1}, we present a systematic and comprehensive safety assessment for these language reasoning models.
% following the previous experience in the safety evaluation of LLMs. 
Specifically, we first conduct a thorough safety evaluation by testing these reasoning language models against various established safety benchmarks, covering a broad range of safety categories from company policies and government regulations~\cite{zeng2024air}, and various application scenarios~\cite{wan2024cyberseceval}. Additionally, we assess their vulnerability to different adversarial attacks, including jailbreaking and prompt injection~\cite{jiang2024wildteaming,wan2024cyberseceval}, to analyze their robustness in real-world deployments. In these evaluations, we analyze both quantitative results and the safety behaviors of large reasoning models to gain deeper insights into their safety performance.
% introduce two new safety evaluation perspective we design for the reasoning models: 1. compare the safety of the thought process and the final completion. Compare the level of harmfulness of different models for the unsafe response.

% \begin{table}[t]
% \centering
% \setlength{\abovecaptionskip}{8pt}
% \setlength{\belowcaptionskip}{8pt}
% \caption{ The Safety Rate (\%) of reasoning LLMs on different safety benchmarks. Despite stronger abilities, both distilled and the original version of Deepseek R1 show significant safety risks on different safety domains and application scenarios. *DS is short for Deepseek.
% }
% \resizebox{0.5\textwidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% Models  & AirBench & MITRE & Code Interpreter \\
%   \midrule
%   DS-V3 & 38.8 & 14.6 & \\
% DS-R1 & 51.6 & 7.4 & \\
%  Llama-3.3 & 52.9 & 27.1 &  \\
% DS-R1-70B & 46.0 & 22.3 \\
% o3-mini & 70.1 & 80.9\\
% \bottomrule
% \end{tabular}
% }
% \label{tab:overall}
% \end{table}

% \begin{figure*}[h]
% \centering
% \vspace{-0.3cm}
% \includegraphics[width=1\linewidth,]{figures/intro.pdf}
% \caption{We perform multi-faceted safety analysis on various safety benchmarks and attacks for large reasoning models and their non-reasoning counterparts.\xin{(1)too much text in the figure. maybe delete some from Safety Benchmark and Attack, which can also help align with the right part of the figure spatially. (2) move to the first page. (3) update the Figure based on the proposed RQs.}
% }
% \label{fig:fig1}
% \vspace{-0.3cm}
% \end{figure*}


Beyond classifying the safety of final model responses, a primary contribution of this work is a multi-faceted safety analysis specific to large reasoning models.  
First, to determine whether the reasoning process itself elevates safety risks, we evaluate the safety of the model's internal reasoning steps (e.g., the content within \texttt{<think>} and \texttt{ </think>} tags in DeepSeek-R1) and compare it against the safety of the final completion. 
Second, recognizing that unsafe responses can vary in their degree of harmfulness, we hypothesize that reasoning models, due to their enhanced capabilities, may generate more harmful unsafe responses. %\dawn{We need to be careful that reasoning capability can also be used to enhance safety (see openAI’s paper \& blog), so it’s better to rephrase the hypothesis} 
Therefore, in addition to binary safety classification, we evaluate the harmfulness level of model responses using pre-trained multi-attribute reward models~\cite{wang2024interpretable,dorka2024quantile}.

Our experimental findings demonstrate that open-source reasoning models have a significant safety gap compared with the close-source o3-mini in both safety benchmarking and when facing adversarial attacks. Moreover, the distilled reasoning model exhibits consistently lower safety performance compared to their base safety-aligned model. 
Crucially, our analysis reveals that when reasoning models generate unsafe responses, these responses tend to be more harmful than those from non-reasoning models due to stronger abilities.
Finally, we find that across the majority of benchmarks tested, the content generated during the reasoning process of R1 models exhibits lower safety than their final completions, underscoring an urgent need to enhance the safety of the reasoning process itself.  
We hypothesize that the safety performance of R1 models may be attributed to non-sufficient safety-specific training, and the process of fine-tuning the Llama 3.3 (distilled-R1) could have inadvertently degraded its pre-existing safety alignment~\citep{qi2023fine}. 
Given the broad adaptability of open-source reasoning models, we advocate for stronger safety alignment to mitigate potential risks in the future.

% \dawn{What do we say about OpenAI’s paper/blog on using reasoning ability to enhance ai safety? }

