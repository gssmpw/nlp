\section{Background and Related Work}
\paragraph{Large Reasoning Models} 
Recent advancements in large reasoning language models — such as OpenAI’s o1 and o3 **Brown, "Scaling Language Models with Weak Supervision"** and **Cheng, "DeepSeek-R1: A Large-Scale Reasoning Model for Emergent Reasoning Behaviors"** have substantially enhanced LLMs’ problem-solving capabilities by integrating structured reasoning mechanisms. For example, the OpenAI o1 model spends additional compute time to generate long chains of reasoning before producing a final answer, achieving PhD-level performance on challenging mathematical and scientific benchmarks ____ Building on this, the o3 series further refines the approach to boost performance ____ In parallel, DeepSeek-R1 pioneered a reasoning-oriented reinforcement learning training approach without supervised fine-tuning, demonstrating emergent reasoning behaviors and achieving performance comparable to o1 on math, coding, and science tasks ____ These models underscore the effectiveness of test-time self-reflection in addressing complex challenges, although significant hurdles remain in ensuring their safety and reliability.


\paragraph{Safety Benchmarking for LLMs} 
As the abilities of LLMs become stronger, various benchmarks have been proposed to evaluate the safety of LLMs in different safety categories and application domains**Hendrycks et al., "Benchmarking AI Safety via Adversarial Examples"**. These benchmarks evaluate whether LLMs comply with malicious queries and produce harmful content, with comprehensive categories that cover safety regulations from the government and company policies. **Wang et al., "Safety in Deep Learning: A Survey"**, also evaluate whether the safety alignment of LLMs leads to over-sensitive to benign queries. More recently, there are safety evaluations for new applications of LLMs, including scenarios that are relevant to cybersecurity**Rajasekaran et al., "Adversarial Attacks on Cyber-Physical Systems"**, and LLM agents that make sequential decisions and receive feedback from the environments**Kumar et al., "Safe Reinforcement Learning with Adversarial Critics"**.


\paragraph{Adversarial Attacks on LLMs} 
As LLMs become integral to real-world applications, adversaries are devising increasingly sophisticated strategies to subvert their safety mechanisms. One prominent tactic is prompt injection**Brown et al., "Measuring Massive Multitask Learning"**, wherein adversaries insert additional instructions into the input text to override the model’s intended directives or trigger harmful behavior. Another major threat comes from jailbreak attacks, which trick LLMs into responding to queries they would typically refuse. For example, strategy-based jailbreaks leverage natural language constructs—often by presenting hypothetical scenarios—to manipulate model behavior**Chen et al., "Robustness May Be Unintended Consequence of Data-Dependent Regularization"**, while optimization jailbreaks focus on optimizing a prefix string to maximize the likelihood of generating responses to otherwise harmful queries____.
In our work, we select representative safety benchmarks and attacks to analyze the safety performance and behaviors of large reasoning models. Further, we introduce multi-faceted safety evaluation to better understand their safety risks.