\section{Related Work}
%\ml{A related word is Work :)} \nsf{lol}



\subsection{Knowledge Component Generation} 

Traditional methods for KC creation and tagging rely on human domain experts to identify the knowledge requirements for solving a problem~\cite{bier2014approach}, a highly time-consuming process.
%This manual process can be aided by tools (e.g., CTAT~\cite{?}) and frameworks (e.g., EAKT~\cite{?}) to help identify the problem-solving steps and associated KCs students must master. 
Recent work has proposed automated approaches for KC discovery and tagging, employing data-driven approaches including the Q-matrix method~\cite{barnes2005q}. 
In programming, \cite{hosseini2013javaparser} uses a rule-based parser to obtain ASTs with KCs identified at their lowest ontological level, ~\cite{rivers2016learning} define KCs as nodes in an AST followed by a learning curve analysis to identify KCs students struggle with the
most in Python programming, \cite{hoq2024towards} uses an AST-based neural network to identify student misconceptions, \cite{shi2024knowledge} presents a deep learning approach for KC attribution, and \cite{shi2023kc,shi2024knowledge} learn latent KCs, lacking textual descriptions, by training deep learning models on KT data enforced with priors from pedagogical theory.
%However, these latent KCs lack textual descriptions making them uninterpretable. 
Recent advances in LLMs have inspired automated approaches for descriptive KC generation for dialogues~\cite{scarlatos2024exploringknowledgetracingtutorstudent}, and problems in math~\cite{ozyurt2024automated}, and science~\cite{moore2024automated}. 
However, we're among the first approaches to present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems.

\subsection{Knowledge Tracing}
There exists a wide body of work on KT~\cite{kt} in the student modeling literature. 
The classic KT task aims to estimate a student's mastery of KCs from their responses to past problems and use these estimates to predict their future performance.
Classic Bayesian knowledge tracing methods~\cite{ktcomparepardos,yudelson} use latent binary-valued variables to represent student KC mastery.
With the widespread adoption of neural networks, multiple deep learning-based KT methods were proposed with limited interpretability since student knowledge is modeled as hidden states in these networks.
Most of these methods use long short-term memory networks~\cite{lstm} or variants~\cite{dkt,saint+}, with other variants coupling them with memory augmentation~\cite{dkvmn}, graph neural networks~\cite{gikt}, or attention networks~\cite{akt,sakt}.
KT methods have been applied to many different educational domains, including programming~\cite{hoq2023sann,codedkt,pkt}. Recent work has attempted to leverage LLMs to develop generative KT methods predicting exact student responses to programming problems~\cite{tiktoc,infooirt,okt}. However, to the best of our knowledge, we are the first to present an LLM-based KT method for programming problems that leverages the textual content of KC descriptions, modeling interpretable student mastery levels on each KC, for improved KT performance.