\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}      % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{array, booktabs, multirow, makecell}       % professional-quality tables
\usepackage{amsfonts}    
\usepackage{amsmath, mathtools, amssymb}
% blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
% \usepackage{natbib}
\usepackage[numbers]{natbib}
\usepackage{doi}
\usepackage{subcaption}
\usepackage{xcolor,colortbl}
\usepackage{listings}
\usepackage{fancyvrb}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\arraybackslash}p{#1}}
\lstset{
    belowskip=-12pt,
    aboveskip=-8pt,
    basicstyle=\linespread{0}\scriptsize\ttfamily, 
    language=Java,
    tabsize=1,
    literate={\ \ }{{\ }}1
    }
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{framed}
\usepackage{capt-of}
\definecolor{OliveGreen}{rgb}{0,0.6,0}
\definecolor{CornellRed}{rgb}{0.7, 0.11, 0.11}

\lstdefinestyle{customJava}{
    language=Java,
    moredelim=[is][\color{OliveGreen}]{\$}{\$}, % Green highlighting
    moredelim=[is][\color{CornellRed}]{@}{@}      % Red highlighting
}



\title{Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\author{{Zhangqi Duan}\\
	University of Massachusetts Amherst\\
\texttt{zduan@cs.umass.edu} \\
	%% examples of more authors
	\And
	{Nigel Fernandez} \\
	University of Massachusetts Amherst\\
\texttt{nigel@cs.umass.edu} \\ 
    \And
	{Sri Kanakadandi} \\
	North Carolina State University\\
\texttt{skanaka4@ncsu.edu} \\
    \And
	{Bita Akram} \\
	North Carolina State University\\
\texttt{bakram@ncsu.edu} \\ 
    \And
	{Andrew Lan} \\
	University of Massachusetts Amherst\\
\texttt{andrewlan@cs.umass.edu} \\ 
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Automated Knowledge Component Generation and Knowledge Tracing For Coding Problems},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms.
However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. 
We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. 
We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. 
We conduct extensive quantitative and qualitative evaluations validating the effectiveness of KCGen-KT.
On a real-world dataset of student code submissions to open-ended programming problems, KCGen-KT outperforms existing KT methods.
We investigate the learning curves of generated KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model.
We also conduct a human evaluation to show that
the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by human domain experts.
\end{abstract}


% keywords can be removed
\keywords{Computer Science Education \and Knowledge Component \and Knowledge Tracing \and Large Language Model}


\section{Introduction}
In student modeling, an important task is to map problems (or items or questions) to specific skills or concepts, referred to as knowledge components (KCs). KCs provide an invaluable resource to model student learning~\cite{bier2014approach}, estimating their mastery~\cite{kt} levels on fine-grained units of knowledge. Accurately estimating student mastery levels on KCs helps enable both 1) teacher feedback, by showing this information in teacher dashboards,
and 2) adaptive and personalized learning in online learning platforms or intelligent tutoring systems~\cite{huang2020general}, by tailoring instructions and content sequencing according to student knowledge levels. Identifying fine-grained KCs students struggle~\cite{rivers2016learning} with also enables content designers to develop targeted instructional content and practice problems for students.

KCs are typically crafted by human domain experts, who also tag problems with KCs that students need to master to solve the problem correctly. This process can be highly labor-intensive, prone to bias and errors, and may not be scalable. There exist solutions to automate parts of this process using Natural Language Processing (NLP) tools, usually employing classification algorithms~\cite{pardos2017imputing}, to tag KCs to problems, which relies on having a predefined set of KCs. Recent advances in Large Language Models (LLMs) have shown potential in developing automated approaches for KC \emph{identification} in addition to tagging, in domains such as math~\cite{ozyurt2024automated} and science~\cite{moore2024automated}. Automatically generating KCs is challenging since KCs need to satisfy various criteria including being relevant to problems, being specific enough to provide teacher and student support, being generalizable across settings, and satisfying cognitive science principles. 
%Further, problems assessing semantically equivalent KCs could have KCs generated with different wordings. Evaluation methods to assess KC quality include 

Unlike other domains, generating KCs for open-ended programming problems that are common in the domain of computer science education has unique challenges. Writing code is inherently non-linear, with complex interactions between programming concepts and skills, and requires students to construct functioning code from scratch. Moreover, a programming problem can often have multiple valid solutions using different strategies, which may cover different sets of KCs. 
Prior work~\cite{hosseini2013javaparser} uses a Java parser to convert a solution program into an Abstract Syntax Tree (AST) and reports ontological concepts at the lowest level as KCs. LLMs, with their advanced programming and reasoning abilities, are yet to be tested for automated KC generation and tagging for programming problems.

\subsection{Contributions}

In this paper, we explore using LLMs to automatically generate KCs for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as \textbf{KCGen-KT}\footnote{https://github.com/umass-ml4ed/kcgen-kt}. Our contributions are summarized as follows:
\begin{enumerate}
    \item We develop a fully automated, LLM-based pipeline for KC generation and tagging. We first compute the Abstract Syntax Tree (AST) representations of two representative solution codes in the prompt, and then prompt GPT-4o~\cite{gpt-4o}, an advanced, proprietary LLM, to identify KCs that are required for a problem. Then, to aggregate KCs across problems and de-duplicate similar ones, we cluster KCs on semantic similarity, followed by prompting GPT-4o~\cite{gpt-4o} to summarize each cluster and provide a name. Finally, we automatically tag problems with KCs according to the clustering results. Table~\ref{tab:qualitative_example} shows an example problem with the set of LLM-generated KCs. 
    \item We develop an LLM-based KT method to leverage the generated KCs for the KT task. Our method leverages the textual content of the KC descriptions to capture student mastery levels on each KC, and predict not only the overall correctness of the student code submission but also the actual code itself. 
    \item We conduct extensive quantitative and qualitative evaluations to validate the effectiveness of KCGen-KT. On the CodeWorkout dataset that contains real-world student code submissions to open-ended programming problems, we show that KCGen-KT outperforms existing KT methods specifically developed for programming problems. We also investigate the learning curves for these KCs and show that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. We also conduct a human evaluation to show that the KC tagging accuracy of our pipeline is reasonably accurate when compared to that by human experts.
\end{enumerate}


\begin{table}[t]
\vspace{-1cm}
\small
\centering
\renewcommand{\arraystretch}{1.65}
\caption{Example programming problem from the CodeWorkout dataset with a sample GPT-4o generated solution code, comparing KCs generated by our KCGen-KT framework to human-written KCs.}
\label{tab:qualitative_example}
\begin{tabular}{p{0.4\linewidth}|p{0.25\linewidth}|p{0.24\linewidth}}
\toprule
\multicolumn{3}{p{0.97\linewidth}}{Problem: Write a function in Java that implements the following logic: Write a function in Java that implements the following logic: Given 2 int values greater than 0, return whichever value is nearest to 21 without going over. Return 0 if they both go over.}\\
\midrule
Representative Solution Code & Generated KCs & Human-written KCs\\
\midrule
\multirow{-1.9}{*}{\input{student_codes/qual_example_student_code}} & Conditional Logic and Evaluation & If/Else \\
& Integer Operations and Manipulations & Math ($+-*/$) \\
& Boolean Expressions and Logic & Logic (And Not Or) \\
& Java Control Flow Structures & Logic Compare Numbers \\
\bottomrule
\end{tabular}
% \vspace{-1.5cm}
\end{table}


\section{Related Work}
%\ml{A related word is Work :)} \nsf{lol}



\subsection{Knowledge Component Generation} 

Traditional methods for KC creation and tagging rely on human domain experts to identify the knowledge requirements for solving a problem~\cite{bier2014approach}, a highly time-consuming process.
%This manual process can be aided by tools (e.g., CTAT~\cite{?}) and frameworks (e.g., EAKT~\cite{?}) to help identify the problem-solving steps and associated KCs students must master. 
Recent work has proposed automated approaches for KC discovery and tagging, employing data-driven approaches including the Q-matrix method~\cite{barnes2005q}. 
In programming, \cite{hosseini2013javaparser} uses a rule-based parser to obtain ASTs with KCs identified at their lowest ontological level, ~\cite{rivers2016learning} define KCs as nodes in an AST followed by a learning curve analysis to identify KCs students struggle with the
most in Python programming, \cite{hoq2024towards} uses an AST-based neural network to identify student misconceptions, \cite{shi2024knowledge} presents a deep learning approach for KC attribution, and \cite{shi2023kc,shi2024knowledge} learn latent KCs, lacking textual descriptions, by training deep learning models on KT data enforced with priors from pedagogical theory.
%However, these latent KCs lack textual descriptions making them uninterpretable. 
Recent advances in LLMs have inspired automated approaches for descriptive KC generation for dialogues~\cite{scarlatos2024exploringknowledgetracingtutorstudent}, and problems in math~\cite{ozyurt2024automated}, and science~\cite{moore2024automated}. 
However, we're among the first approaches to present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems.

\subsection{Knowledge Tracing}
There exists a wide body of work on KT~\cite{kt} in the student modeling literature. 
The classic KT task aims to estimate a student's mastery of KCs from their responses to past problems and use these estimates to predict their future performance.
Classic Bayesian knowledge tracing methods~\cite{ktcomparepardos,yudelson} use latent binary-valued variables to represent student KC mastery.
With the widespread adoption of neural networks, multiple deep learning-based KT methods were proposed with limited interpretability since student knowledge is modeled as hidden states in these networks.
Most of these methods use long short-term memory networks~\cite{lstm} or variants~\cite{dkt,saint+}, with other variants coupling them with memory augmentation~\cite{dkvmn}, graph neural networks~\cite{gikt}, or attention networks~\cite{akt,sakt}.
KT methods have been applied to many different educational domains, including programming~\cite{hoq2023sann,codedkt,pkt}. Recent work has attempted to leverage LLMs to develop generative KT methods predicting exact student responses to programming problems~\cite{tiktoc,infooirt,okt}. However, to the best of our knowledge, we are the first to present an LLM-based KT method for programming problems that leverages the textual content of KC descriptions, modeling interpretable student mastery levels on each KC, for improved KT performance.

\section{Methodology}

In this section, we detail our automated LLM-based approach to generate KCs for programming problems from the CodeWorkout~\cite{codeworkout} dataset, and then introduce KCGen-KT, a strong KT method leveraging the semantics of the generated KCs in modeling student learning for improved KT performance. 

\begin{figure}[t]
\centering
\vspace{-.4cm}
\includegraphics[width=\textwidth,trim={0.7cm 0 0.5cm 0},clip]{figures/kcgen_pipeline.pdf}
\caption{The four steps of our automated KC generation pipeline.} 
\label{fig:kc_gen}
\vspace{-.5cm}
\end{figure}



For KC generation, we use GPT-4o~\cite{gpt-4o}, an advanced proprietary LLM with strong reasoning and programming capabilities. We generate KCs for a programming problem following five key steps: 1) solution generation, 2) converting solution code to abstract syntax trees (ASTs), 3) generating KCs associated with each problem separately, 4) cluster KCs across all problems, and 5) summarize each cluster to obtain a description of each KC. We detail these steps below. 


\noindent \textbf{Code Solution Generation} 
A programming problem may have multiple code solutions using different strategies that may use different KCs. Therefore, we prompt GPT-4o to generate \emph{two} unique solutions for each problem, thereby enhancing coverage within the (possibly large) solution space. These solutions, in addition to the problem statement, can inform us of what programming skills are needed to solve the problem, hence identifying the associated KCs. We note that writing code is inherently non-linear, with salient relationships and interactions between programming KCs. Therefore, to emphasize these structured relationships, we convert each solution code from a linear sequence of tokens to its equivalent AST representation using the code-ast~\cite{code-ast} library. 


\noindent \textbf{Prompting Strategy for KC Generation} 
To generate KCs for a problem, we prompt GPT-4o with the problem statement and the two sample solution codes ASTs. Our approach of using ASTs instead of raw code is inspired by prior work generating KCs for math problems by including a step-by-step solution~\cite{ozyurt2024automated}; in our experiments, we also found that this approach results in more precise KC descriptions. We use a simple prompt without extensive meta or contextual information to keep our method generalizable across programming topics. 



\iffalse
\begin{figure}[h]
\centering
\begin{minipage}{\textwidth} 
\footnotesize 
\begin{BVerbatim}
<System Message>
You are an experienced computer science teacher and education expert. 
You are given a Java programming problem and two AST of sample solutions. 
Your job is to generate the generalizable knowledge components/skills 
which are required to solve the problem in phrases. 
Please follow these instructions carefully when generating the KCs:
- The knowledge components should be able to apply to other questions 
without changing problem-specific information. 
- Your final response should be a JSON object using the template: 
{`KC 1': KC_text_1, `KC 2': KC_text_2}.
<User Message>
Problem: {problem_text}
AST of first solution: {AST_text_1}
AST of second solution: {AST_text_2}
\end{BVerbatim}
\end{minipage}
\caption{Prompt for KC Generation \ml{guys, we don't have space to show this thing as a figure. you can describe what you put into the prompt in 2-3 sentences of regular text}}
\label{fig:prompt_kcgen}
\end{figure}
\fi



\noindent \textbf{Clustering KCs for Semantic Equivalence} 
Since we can only prompt GPT-4o to generate KCs for each problem separately due to its limited input context length, the resulting KCs aggregated over problems need to be post-processed. We find many KC labels that are semantically equivalent but worded differently across problems. Therefore, we align them by clustering KCs: We first compute the Sentence-BERT~\cite{reimers-2019-sentence-bert} embedding of the textual description of each KC, then use HDBSCAN~\cite{mcinnes2017hdbscan}, a clustering algorithm, to cluster embedded KCs using cosine similarity as the distance function. We can tweak parameters in HDBSCAN such as minimum cluster size to control the granularity of KCs to an extent.

\noindent \textbf{Labeling KC Clusters and Tagging Problems} 
We label each KC cluster by prompting GPT-4o to generate a single informative KC name, summarizing all KCs in the cluster. We then perform a deduplication step among this new set of KC names by prompting GPT-4o to merge and label groups of semantically similar KC names, to obtain our final set of generated KCs across problems. 
We use the mapping between the initial set of GPT-4o generated KCs to their assigned clusters, and the mapping between clusters to their final summarized KC labels, to tag the final set of KCs to each problem.

\iffalse
\begin{figure}[h]
    \centering
    \begin{minipage}{\textwidth} 
        \footnotesize 
        \begin{BVerbatim}
<System Message>
You are an experienced computer science teacher and education expert. 
Now you are given a list of knowledge components that have similar 
meaning and this list is clustered by HDBSCAN. Your job is to summarize 
a name for the cluster. 
Please follow these instructions carefully when summarizing. 
- Summarize the cluster using phrases
- Your final response should be a JSON object using the template: 
{'KC': summarized_name}.

<User>
Knowledge Component Cluster: {KC_list}
    \end{BVerbatim}
    \end{minipage}
    \caption{Prompt for Cluster Summarization}
    \label{fig:cluster_summarization}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}{\textwidth} 
        \footnotesize 
        \begin{BVerbatim}
<System Message>
You are an experienced computer science teacher and education expert. 
Now you are given a list of knowledge components clusters. There might 
exist clusters referring to the same knowledge component. Your job is 
to deduplicate the knowledge components clusters in the list. 
Please follow these instructions carefully when performing the task.
- If the knowledge component is unique, keep it the same, otherwise, 
deduplicate repeating KCs by summarizing one name for them. 
- Before giving your final response, write a short summary of which 
clusters refer to the same knowledge component.
- Return a dictionary of knowledge components where the key is the new 
knowledge component name and the value is either a list of one value 
when the knowledge component is unique or a list of multiple values 
when the knowledge components are semantically similar. 
- Your final response should be a JSON object using the template: 
{'new_KC_name_1': [KC_1], 'new_KC_name_2': [KC_2, KC_3]}.

<User>
Knowledge Component Cluster List: {KC_list}
    \end{BVerbatim}
    \end{minipage}
    \caption{Prompt for Cluster Deduplication}
    \label{fig:cluster_deduplication}
\end{figure}
\fi

\subsection{Improving Knowledge Tracing via Generated KCs}

We now detail KCGen-KT, an LLM-based KT method that explicitly leverages the semantics of KCs and explicitly models student mastery levels on each KC. 

\noindent \textbf{KT Problem Formulation}
For open-ended programming problems, we define each student response to a problem as $x_t \coloneq (p_t, \{w^i_t\}, c_t, a_t)$, where $p_t$ is the textual statement of the problem, $\{w^i_t\}$ are the KCs associated with the problem, $c_t$ is the student code submission, and $a_t$ is the correctness of the submission; in most existing KT methods, $a_t$ is treated as binary-valued (correct/incorrect). 
Therefore, our goal is to estimate a student's mastery level of each KC given their past responses, $x_0, \ldots, x_t$, and use this estimate to predict both 1) the overall binary-valued correctness $a_{t+1} \in \{0,1\}$ and 2) the open-ended code $c_{t+1}$ submitted by the student on their next attempted problem $p_{t+1}$. Following previous work~\cite{codedkt}, $a_t=1$ if the student-submitted code passes all test cases associated with the problem, and $a_t=0$ otherwise.

\begin{figure}
\vspace{-.5cm}
\includegraphics[width=\textwidth,trim={0.7cm 0 1cm 0},clip]{figures/kcgenkt_model_figure}
\vspace{-.5cm}
\caption{Overview of our KCGen-KT's model with the Llama 3 LLM as the backbone. KCGen-KT leverages KC semantics, tracking student mastery levels on each KC, to predict both correctness and the student code submission.} 
\label{fig:model}
\vspace{-.5cm}
\end{figure}



\noindent \textbf{KCGen-KT}
%\nsf{talk about the KC generation process is flexible, any KC set can be used}
KCGen-KT leverages the KCs associated with a problem in two ways: 1) by improving the problem representation using the semantic information of KCs, and 2) by improving the student representation by building an interpretable student profile modeling student mastery levels on KCs. 

Following TIKTOC~\cite{okt}, we use an open-source LLM, Llama 3~\cite{llama3}, as the backbone to predict both the overall correctness and actual open-ended student code in a token-by-token manner, in a multi-task learning approach. KCGen-KT differs from OKT~\cite{okt} by leveraging the content of the KCs, and from Code-DKT~\cite{codedkt} by using text embedding methods to embed the textual problem statement.

\noindent \textbf{Student Knowledge on KCs} 
For each student, at each timestep $t$, KCGen-KT updates the student's $64$-dimensional knowledge state vector $h \in \mathcal{R}^{64}$, through a long short-term memory (LSTM)~\cite{lstm} network as in DKT~\cite{dkt}, given by $h_{t} = \text{LSTM}(h_{t-1}, p_t, c_t)$.
This knowledge state $h_t$ is compressed into a $k$-dimensional mastery vector $m_t \in [0,1]^{k}$, where $k$ is the total number of KCs, through a linear layer with weights $W_{m}$ and bias $b_{m}$, followed by a sigmoid function to map the values of $m_t$ to be in the range of $[0,1]$, given by $m_t = \sigma(W_{m}h_t + b_m)$. Each dimension $j$ of $m_t$ denotes a student's mastery level in $[0,1]$ on the $j$th unique KC, with larger values denoting higher mastery.

\noindent \textbf{Predictions} 
To use LLMs to predict the student response to the next problem, we need to connect student KC knowledge with the textual input space of LLMs. Therefore, following previous work~\cite{fernandez-etal-2024-divert,liu2023boltforsofttokens}, we transform KC mastery levels into \emph{soft} text tokens, i.e.,

\begin{equation}
    s_t^j = m_t^j\cdot \text{emb}^{\text{true}} + (1-m_t^j)\cdot \text{emb}^\text{{false}},
\end{equation}
where $\text{emb}^{\text{true}}$ and $\text{emb}^\text{{false}}$ are the embeddings of the text tokens ``true'', and ``false'', respectively. In other words, we use student KC mastery levels $m_t^j$ to combine two hard, discrete text tokens (``true'' and ``false'') into a differentiable soft token $s_t^j$, to enable the flow of gradients during training. We pass this student knowledge information into the LLM for prediction tasks using the input format of
\texttt{KC $1$: <$w^1$>. The student's mastery level on <$w^1$> is: $s_t^{l[1]}$. $\ldots$}.

\noindent \textbf{Knowledge-Guided Response Prediction}
We construct our LLM prompt for the next response prediction by including both 1) the textual statement of the next problem and 2) student mastery levels on the KCs associated with the problem, as
%
\texttt{question: {$p_t$}. <KCs with student mastery levels>}.
%

To predict the binary-valued correctness of the next student response, we average the hidden states of the last layer of Llama 3 that correspond to only the input (knowledge-guided prompt) to obtain a representation $r$, transformed for correctness prediction using a linear transformation matrix $W_p$ and a sigmoid function, given by $\hat{a}_{t+1} = \sigma(W_p \cdot r)$. We minimize the binary cross entropy (BCE) loss (for one response):
\begin{equation} \label{eq:cploss}
    \mathcal{L}_{\text{CorrPred}} = a_{t+1} \cdot \log \hat{a}_{t+1} + (1-a_{t+1})\cdot \log (1-\hat{a}_{t+1}).
\end{equation}


To predict student code, we feed the knowledge-guided prompt into Llama 3, which generates the predicted code $\hat{c}$, in a token-by-token manner, using the loss:
\begin{equation} \label{eq:cgloss}
    \mathcal{L}_{\text{CodeGen}} = \textstyle\sum_{n=1}^N -\log P_{\theta} \left( \hat{c}^n\ \middle \vert\ p,j,\{\hat{c}^{n'}\}_{n'=1}^{n-1}\right),
\end{equation}
where N is the number of tokens in the student code. $\theta$ denotes the set of learnable parameters, which includes the underlying KT model, the linear layer with weights $W_{m}$ and bias $b_{m}$ to get student mastery levels, and the parameters of the finetuned Llama 3 LLM. 

\noindent \textbf{Promoting Interpretability}
To promote interpretability of the student KC knowledge parameters, we use a conjunctive model~\cite{inproceedings} and multiply individual student KC mastery levels to obtain an overall mastery level 
\mbox{$\hat{y}_{t+1} = \prod_{k=1}^{K} s_t^k\cdot \mathbb{I}(s_t^k)$},
%\begin{equation}
%    \hat{y}_{t+1} = \prod_{k=1}^{K} s_t^k\cdot \mathbb{I}(s_t^k),
%\end{equation}
where the indicator function $\mathbb{I}(s_t^k)$ is $1$ is the KC $s_t^k$ is associated with the problem, and $0$ otherwise. 
We then minimize the BCE loss between this overall KC mastery level for this problem and its binary-valued correctness,
\begin{equation} \label{eq:kcloss}
    \mathcal{L}_{\text{KCMastery}} = a_{t+1} \cdot \log \hat{y}_{t+1} + (1-a_{t+1})\cdot \log (1-\hat{y}_{t+1}).
\end{equation}
This loss regularizes the model to be monotonic, i.e., high knowledge on KCs corresponding to high probability of a correct response, thus promoting the interpretability of student knowledge parameters $m_t^j$.  

\noindent \textbf{Multi-task Learning Objective}
Following previous work~\cite{tiktoc} showing multiple objectives are mutually beneficial to each other, our final multi-task training objective minimizes a combination of all three losses together, with a balancing parameter $\lambda \in [0,1]$ controlling the importance of the losses,
\begin{equation}
    \mathcal{L}_{\text{KCGen-KT}} = \lambda (\mathcal{L}_{\text{CodeGen}} + \mathcal{L}_{\text{CorrPred}}) + (1-\lambda) \mathcal{L}_{\text{KCMastery}},
\end{equation}
where losses are averaged over code submissions by all students to all problems.

\section{Quantitative Evaluation: KT Performance}

We conduct extensive quantitative and qualitative evaluations to validate the effectiveness of KCGen-KT including evaluating KT performance, a learning curve analysis, and a human evaluation of KC tagging accuracy.


\noindent\textbf{Dataset Details}
%
To validate the effectiveness of KCGen-KT, we use the CodeWorkout~\cite{codeworkout} dataset, a large, publicly available real-world programming education dataset previously used in the Second CSEDM Data Challenge~\cite{csedm}. CodeWorkout contains actual open-ended code submissions from real students, collected from an introductory Java programming course, together with problem textual statements and human-written KC tags (estimated programming concepts) on each problem.
 
In total, there are $246$ students attempting $50$ problems covering various programming concepts including conditionals, and loops, among others. Following prior work \cite{codedkt}, we only analyze students' first submissions to each problem, leading to a total of $10,834$ code submissions.

\iffalse
\begin{table*}
\caption{Statistics of the CodeWorkout dataset. \ml{nonono this part should be completely cut and summarized into 2-3 sentences. the dataset is not your contribution! too much syllabusQA for :hm:}}
\label{tab:dataset_stats}
\begin{tabular}
{p{0.5\linewidth}|p{0.07\linewidth}|p{0.07\linewidth}|p{0.07\linewidth}|p{0.07\linewidth}}

\toprule

\# unique problems & \multicolumn{4}{c}{$50$}\\
\# unique students & \multicolumn{4}{c}{$246$}\\
\# student code submissions & \multicolumn{4}{c}{$10834$}\\

\midrule
Statistic & $\mu$ & $\sigma$ & Min & Max \\
\midrule
\# lines/submission & $14.9$ & $6.8$ & $1$ & $101$\\
\# tokens/problem & $53.1$ & $20.6$ & $13$ & $103$\\
\# tokens/submission & $41.0$ & $17.5$ & $6$ & $206$\\
\# submissions/student & $44.0$ & $8.2$ & $13$ & $50$\\
\# submissions/problem & $216.7$ & $10.0$ & $194$ & $233$\\

\bottomrule
\end{tabular}
\end{table*}
\fi

\noindent \textbf{Metrics}
For the binary-valued correctness prediction task, following~\cite{dkt,codedkt}, we use standard metrics such as AUC, F1 score, and accuracy.
For the student code prediction task, following~\cite{okt}, we measure the similarity between generated student code and ground-truth student code using CodeBLEU~\cite{codebleu}, a variant of the classic text similarity metric BLEU~\cite{papineni2002bleu}. This metric is customized for code and measures both syntactic and semantic similarity between two pieces of code.

\noindent \textbf{Baselines}
%
In terms of KCs, we compare our generated KCs against human-written KCs that are available in the CodeWorkout dataset. We test a version of KCGen-KT by replacing our LLM-generated KCs with human-written KCs and keeping the KT method unchanged, which we refer to as \textbf{KCGen-KT (Human-written KCs)}. 
In terms of KT methods, we adapt Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC)~\cite{tiktoc}, a recent, strong KT method for programming, as the main baseline. TIKTOC also uses Llama 3 as the backbone and a multi-task learning setup to jointly predict the exact code token-by-token and whether it passes each test case. We slightly modify it for our KT task, replacing test case prediction with overall code correctness prediction, by reducing the dimension of the prediction head from the number of test cases to one, for overall correctness prediction only. We refer to the resulting method as \textbf{TIKTOC*}. We also use \textbf{Code-DKT}, a popular KT method for programming that leverages the content of student code, to predict the overall correctness of student code submissions. As a sanity check, to estimate a lower bound of performance on our KT task thereby providing a sense of task difficulty, we include two simple baselines: \textbf{Random}, which simply predicts the overall binary-valued correctness of a student code randomly with equal probability, and \textbf{Majority}, which simply predicts the majority correctness label (incorrect) among students for each problem. 

\noindent \textbf{Experimental Setup}
For the KT method component of KCGen-KT as well as for all KT baselines, to ensure a fair comparison, we use the instruction-tuned version of Llama 3~\cite{llama3} with $8$B parameters as the base LLM and a frozen ASTNN~\cite{astnn} as the code embedding model. We load Llama 3 using the Parameter Efficient Fine-Tuning (PEFT) library from HuggingFace~\cite{wolf-etal-2020-transformers} and fine-tune it via Low-Rank Adaptation (LoRA)~\cite{hu2022lora} ($\alpha=256$, $\text{rank}=128$, $\text{dropout}=0.05$) using 8-bit quantization. We use the AdamW \cite{loshchilov2018decoupled} optimizer with a batch size of $32$ and perform a grid search to determine the optimal learning rate. In KCGen-KT, we set different learning rates for different model components: $1e-5$ for Llama 3, $5e-4$ for the LSTM model, and $1e-4$ for the $W_m$ and $b_m$ parameters. 
KCGen-KT converges within $10$ training epochs, with each epoch taking $80$ minutes on an NVIDIA L40S 48GB GPU.

\noindent \textbf{Quantitative Results}
Table~\ref{tab:kt_results} shows the average performance (and standard deviation) on our two KT tasks: binary correctness prediction and student code generation, across $5$ random train-validation-test data fold splits, for all methods. We see that the Random and Majority baselines perform poorly, which suggests that the correctness prediction KT task is inherently difficult. %, possibly arising from the fact that problems are from a complex programming domain. 
Our proposed framework, KCGen-KT with either human-written or generated KCs, outperforms other strong KT methods that do not use KCs, including TITKOC* and Code-DKT. This observation suggests that for KT methods that use LLMs as backbone, leveraging the semantic information in KC names helps improve performance on the KT task. More importantly, KCGen-KT with our generated KCs outperforms human-written KCs, by a small but consistent margin on correctness prediction. This observation shows that high-quality KC descriptions and accurate tagging are key to improving downstream KT performance. The performance gap is move evident on the code prediction task, which shows that semantically-informative KC names, as evident from Table~\ref{tab:qualitative_example}, are especially important to LLMs in generative tasks.

\begin{table}[t]
\small
\centering
\vspace{-1cm}
\caption{Performance of KCGen-KT and baselines on both correctness prediction and code prediction, for all approaches across all metrics. KCGen-KT, especially with LLM-generated KCs, outperforms other KT methods.} \label{tab:kt_results}
\scalebox{.95}{
\begin{tabular}{p{0.33\linewidth}p{0.14\linewidth}p{0.14\linewidth}p{0.15\linewidth}|p{0.16\linewidth}}
\toprule
\multirow{3}{*}{Model} & \multicolumn{3}{c}{KT Correctness Pred.} &  \multicolumn{1}{c}{Code Pred.}\\
\cmidrule{2-5}
&  AUC $\uparrow$ & F1 Score $\uparrow$ & Accuracy $\uparrow$ & CodeBLEU $\uparrow$\\
\midrule
Random &  0.499 & 0.368 & 0.506 & $-$ \\
Majority &  0.500 & 0.644 & 0.526 & $-$ \\
Code-DKT~\cite{codedkt} &  $0.766_{\pm 1.8\%} $ & $0.672_{\pm 3.3\%} $ & $0.724_{\pm 1.0\%} $ & $-$ \\
TIKTOC*~\cite{tiktoc} & $0.788_{\pm 1.3\%}$ & $0.666_{\pm 3.0\%}$ & $0.726_{\pm 1.3\%}$ & $0.507_{\pm 1.5\%}$\\
KCGen-KT(Human-written KCs) & $\underline{0.810}_{\pm 1.7\%}$ & $\underline{0.713}_{\pm 3.4\%}$ & $\underline{0.740}_{\pm 1.4\%} $ & $\underline{0.551}_{\pm 2.5\%} $\\
KCGen-KT(Generated KCs) & $\textbf{0.821}_{\pm 1.8\%}$  & $\textbf{0.726}_{\pm 3.2\%}$ & $\textbf{0.749}_{\pm 1.4\%} $ & $\textbf{0.571}_{\pm 0.7\%} $\\
\bottomrule
\end{tabular}}
\vspace{-.5cm}
\end{table}

\noindent \textbf{Qualitative Case Study}
 
Table~\ref{tab:qualitative_case_study} shows the estimated student KC mastery levels and predicted student code (whitespace and indentation altered for brevity) for a student in the test set for a problem. A low student mastery level on KC ``array looping'' results in a run time error in the predicted code of indexing the array outside of its bounds, and a low mastery level on KC ``conditional logic'' results in a logical error of the student not considering more than two adjacent elements. This example shows that informative KC descriptions generated by the LLM help KCGen-KT make more accurate student code predictions.

\section{Learning Curve Analysis}
A common method to assess the quality of KCs is examining how well they match cognitive theory; the expected pattern on the KCs should follow the \emph{power law of practice}, which states that the number of errors should decrease as the amount of practice on certain knowledge component increases~\cite{newell2013mechanisms,snoddy1926learning}. Hence, we compare the error rate across different attempts at KCs and the estimated student KC mastery levels from KCGen-KT. We caution that this comparison may not accurately capture learning curves: since every problem in our dataset is tagged with multiple KCs (over $3$ on average), the overall correctness of a student code submission may not faithfully reflect how well the student masters each KC. 

\begin{table}[t]
\small
\centering
\renewcommand{\arraystretch}{1.45}
\vspace{-.5cm}
\caption{Qualitative study showing how low estimated student mastery levels on relevant KCs correspond to specific errors in the predicted student code.}\label{tab:qualitative_case_study}
\scalebox{.97}{
\begin{tabular}{p{0.47\linewidth}|p{0.4\linewidth}|p{0.05\linewidth}}
\toprule
\multicolumn{3}{p{1\textwidth}}{Problem: Say that a ``clump'' in an array is a series of 2 or more adjacent elements of the same value. Return the number of clumps in the given array.}\\
\midrule
\multicolumn{1}{c}{Predicted Student Code Submission} & \multicolumn{1}{c}{LLM-generated KC} & \multicolumn{1}{c}{Mastery}\\
\midrule
\multirow{-1.9}{*}{\input{student_codes/qual_case_study_2}} & {
%\scriptsize 
Looping and Conditional Logic in Array} & \multicolumn{1}{c}{\textcolor{CornellRed}{40.8\%}} \\
& {
%\scriptsize 
Conditional Logic and Evaluation} & \multicolumn{1}{c}{\textcolor{CornellRed}{45.2\%}} \\
& {
%\scriptsize 
Variable Scope and Management} & \multicolumn{1}{c}{\textcolor{OliveGreen}{54.6\%}} \\
& {
%\scriptsize
Method Return Values} & \multicolumn{1}{c}{\textcolor{OliveGreen}{57.1\%}} \\
\bottomrule
\end{tabular}}
\vspace{-.5cm}
\end{table}

To plot the curves, at attempt $t$, we average the correctness over all students on the problem that represents their $t$-th attempt at the KC, and the error rate is simply the complement of this average correctness score. We also calculate the predicted error rate in a similar way, using KCGen-KT to estimate the mastery level of each student on each KC at each time step, after sorting by putting the problems each student responds to in order. 

Figure~\ref{fig:horizontal_plots} shows two representative plots among all 11 LLM-generated KCs. We found that for KCs that appear more frequently, e.g., the one in the left plot, the predicted error rate curves generally show a decreasing trend with later attempts. However, the actual error rate curves tend to fluctuate a lot and do not show any obvious trend. The reason can be two-fold: First, each problem is associated with multiple KCs, which means that we do not have ground-truth information to attribute an error, i.e., incorrect code submission, to errors on one (or more) KCs. This attribution can only be done in the KCGen-KT model, which makes the predicted KC error rate curves smooth. Second, since the order in which students attempt problems differs in the CodeWorkout dataset and some students even skip certain questions, error rates at each time step are averaged over different problems. Therefore, this average introduces additional noise, such as problem difficulty, which may further exaggerate the variation in student submission correctness. 
%
On the other hand, the right plot shows a different KC that is associated with only a small number of problems. We see that the predicted KC error trend matches the observed one, although with lower values; this observation can be explained by the fact that we use a conjunctive model for code correctness prediction. Therefore, since the probability of correct code is the product of mastering all KCs involved in a problem, each KC's error rate will be lower than that of the overall problem. 

For a more quantitative evaluation, we follow prior work \cite{Pavlik_Koedinger} and fit PFA models on each KC. Results show that the weighted $R^2$ metric using the $11$ LLM-generated KCs is $0.12$, and using the $18$ human-written KCs is $0.14$. A t-test shows that there is no statistically significant difference between the two sets of KCs in terms of PFA model fit; this result suggests that the LLM-generated KCs result in a similar level-of-fit under the PFA model, compared to human-written KCs. 

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/learning_curve_1}
        %\caption{7a}
        %\label{7a}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/learning_curve_6.png}
        %\caption{7b}
        %\label{7b}
    \end{subfigure}
    \vspace{-.2cm}
    \caption{Learning curves for two representative LLM-generated KCs. 
    }
    \label{fig:horizontal_plots}
    \vspace{-.2cm}
\end{figure}

\section{Human Evaluation}
We perform a human evaluation to test one small but concrete part of KCGen-KT, which is whether it can accurately tag programming problems with KCs. For this purpose, we simply check how well KCGen-KT's KC tags align with those provided by human experts. %tagging in comparison with human domain experts, given the same set of KCs generated by KCGen-KT.
Two authors of the paper, with experience in teaching
%and creating assessment problems in 
programming at the university level, 
serve as human annotators. 
For $10$ randomly selected programming problems, we ask them to select relevant KCs where mastery is necessary for students to correctly solve the problem from the list of $11$ KCs generated by KCGen-KT. 
We first group problems into five major topics, namely, 1) math, 2) string, 3) boolean, 4) array, and 5) functions, and then randomly select two problems from each topic. Annotators are given the problem and its two sample solution codes, along with the list of $11$ KCGen-KT generated KCs. We note that this problem tagging setup is similar to the LLM input during the KC generation step of KCGen-KT.

\noindent \textbf{Metrics}
For each problem $p$, we have a set $A$ of KCs tagged by KCGen-KT and a set $B$ of KCs tagged by human annotators. We employ Intersection over Union (\textbf{IoU})~\cite{everingham2010pascal}, also known as the Jaccard index~\cite{jaccard1901etude}, to measure the similarity between these two sets, given by
\begin{equation}
    IoU(A^p,B^p) = \frac{|A^p\cap B^p|}{|A^p\cup B^p|},
\end{equation}
and then compare the overall IoU averaged across all problems. 
We measure the \textbf{F1} score between KCs tagged by KCGen-KT and human annotators: since whether each KC is tagged for each problem is binary-valued, we can treat the tagging problem as \emph{binary classification}, with $1$ denoting a KC is tagged for a problem and $0$ otherwise. We then compare the resulting $11$-dimensional binary vector of KCGen-KT predictions against the ``ground-truth'' human annotator tags, using the F1 score metric, averaged across all problems.
We also report the inter-rater reliability (IRR) between KCs tagged by KCGen-KT and human annotators using Cohen's kappa~\cite{cohen1960coefficient}.

\noindent \textbf{Results} 
The mean IOU and F1 score between KCGen-KT-tagged KCs and human annotator-tagged KCs, averaged over the two human annotators, are $0.652$ and $0.783$. For reference, the IOU, F1 score, and Cohen's kappa IRR, between the two human annotators are $0.887$, $0.936$, and $0.842$, respectively, showing high agreement between the annotators. We can conclude that LLM-based per-problem KC tagging is reasonably accurate but not at the level of human experts yet. We also note that the high IRR between human annotators is likely due to having a fixed set of KCs to select from. Future work should study whether human experts can identify KCs missed by the LLM and develop human-AI collaboration approaches for KC identification. 

\begin{table}[t]
    \centering
    % \vspace{-.5cm}
    \caption{Human evaluation results: KC tagging by KCGen-KT is comparable in accuracy to the tagging by two programming teachers.}
    \label{tab:human_eval}
    \begin{tabular}{p{0.15\linewidth}p{0.4\linewidth}}
        \toprule
         Metric & \multicolumn{1}{c}{KC Tagging}  \\
        \midrule
        IOU &  \multicolumn{1}{c}{0.652 $\pm$ 0.096} \\
        F1 Score &  \multicolumn{1}{c}{0.783 $\pm$ 0.069}  \\
        \bottomrule
    \end{tabular}
    \vspace{-.5cm}
\end{table}

\section{Conclusions
and Future Work}
In this paper, we presented a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. 
We also developed an LLM-based knowledge tracing (KT) framework, KCGen-KT, to leverage these LLM-generated KCs. 
KCGen-KT leverages the textual content of KC descriptions to capture student mastery levels on each KC, to both predict overall correctness as well as the student code submission.
Through extensive experiments, we showed that
KCGen-KT outperforms existing state-of-the-art KT methods on real-world programming problems.
We investigated the learning curves of generated KCs and showed that LLM-generated KCs have a comparable level-of-fit to human-written KCs under the performance factor analysis (PFA) model. Further, we conducted a human evaluation assessing the tagging accuracy of KCs to problems by KCGen-KT, finding the tagging to be reasonably accurate when compared to that by human domain experts.

There are many avenues for future work. First, we can obtain LLM-generated correctness labels at a KC level for student code submissions for fine-grained student modeling. Second, we can explore using training objectives to encourage KC learning curves to follow the power law of practice~\cite{shi2023kc,snoddy1926learning}. Third, we can perform a human evaluation assessing the quality of the generated KCs on aspects such as relevance and specificity.
Fourth, KCGen-KT could incorporate fairness regularization into the training objective~\cite{Zafar:Fairness:2017} to explicitly control for fairness across students from different demographic groups.
Fifth, we can evaluate our KCGen-KT framework on problems from other domains including dialogues~\cite{scarlatos2024exploringknowledgetracingtutorstudent}, math~\cite{ozyurt2024automated}, and science~\cite{moore2024automated}.

\newpage

\bibliographystyle{plain}
\bibliography{main}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
