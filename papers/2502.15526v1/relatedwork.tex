\section{Related Work}
Single-vector retrieval models can be broadly categorized into two types: dense retrieval \cite{ance,dpr,cl-drd,rocketqav2,margin-mse,tas-b,repllama,llama2vec,echo-embed} and sparse retrieval \cite{splade,splade-mistral,splade-v2,snrm,spade}. The key distinction lies in their representations: dense retrieval encodes texts into low-dimensional vectors (i.e. 768 dim in BERT) and employs approximate nearest neighbor (ANN)~\cite{ance,faiss-gpu} search for efficient retrieval. In contrast, sparse retrieval represents texts using high-dimensional sparse vectors and relies on an inverted index~\cite{spade,inverted-index} for fast retrieval. 
 %Compared to multi-vector retrieval models~\cite{citadel,colbertv2,colbert}, such as ColBERT~\cite{colbert}, single-vector retrieval is faster, requires less index memory, and is easier to implement.
 
The most straightforward fine-tuning objective for retrieval models is supervised contrastive loss (CL) \cite{repllama,co-condenser}. By incorporating techniques such as hard negative sampling \cite{ance,adore} and retrieval-oriented continued pre-training \cite{b-prop,co-condenser}, the performance of retrieval models can be further improved. 
%However, CL faces several challenges: 1) it requires a large batch size \cite{rocketqa}, 2) it performs poorly on small models \cite{tas-b,margin-mse}, and 3) it is less robust to noisy data \cite{rocketqa}. To address these limitations, 
Knowledge distillation (KD)~\cite{colbertv2,rocketqav2,cl-drd,tas-b,margin-mse,tct-colbert,kd} is another widely used fine-tuning objective. 
%Unlike CL, KD does not require a large batch size \cite{tas-b}, making it more computationally efficient and less demanding on GPU resources. 
It has also been shown to perform comparably to or even better than CL for small-scale retrieval models, particularly on in-domain benchmarks \cite{tas-b,cl-drd}.

Scaling LLMs has consistently led to improved performance across various NLP tasks~\cite{gpt-3,chinchilla,mt-scaling-1,mt-scaling-2,emergent-llm}. This trend also extends to retrieval models. ~\citet{scale-dr} demonstrated that scaling BERT-based dense retrieval models enhances retrieval performance.
Recent works~\cite{llama2vec,echo-embed,splade-mistral,llm2vec} have further explored this trend by training dense retrieval models with larger backbones, such as LLaMA2-7B~\cite{llama2}. These models have achieved state-of-the-art performance, surpassing traditional BERT-based retrieval models.
%However, these studies have exclusively focused on contrastive loss (CL) for dense retrieval, overlooking the scaling behaviors of sparse retrieval and knowledge distillation (KD), which is our focus in the paper.
\vspace{-.25cm}