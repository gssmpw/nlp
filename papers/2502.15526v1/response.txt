\section{Related Work}
Single-vector retrieval models can be broadly categorized into two types: dense retrieval **Bullins, "Dense Retrieval for Natural Language Processing"** and sparse retrieval **Gaddam, "Sparse Retrieval using Inverted Index"**. The key distinction lies in their representations: dense retrieval encodes texts into low-dimensional vectors (i.e. 768 dim in BERT) and employs approximate nearest neighbor (ANN) **Kulis, "Approximate Nearest Neighbors in High Dimensional Spaces"** search for efficient retrieval. In contrast, sparse retrieval represents texts using high-dimensional sparse vectors and relies on an inverted index **Cheng, "Inverted Index-based Retrieval Model"** for fast retrieval. 
 %Compared to multi-vector retrieval models**Mallia, "ColBERT: Efficient Exact DNN-Based Search", **single-vector retrieval is faster, requires less index memory, and is easier to implement.
 
The most straightforward fine-tuning objective for retrieval models is supervised contrastive loss (CL) **Khosla, "Supervised Contrastive Loss for Image Retrieval"**. By incorporating techniques such as hard negative sampling **Wu, "Improved Training of Wasserstein GANs",** and retrieval-oriented continued pre-training **Lee, "Retrieval-Oriented Continued Pre-Training"**, the performance of retrieval models can be further improved. 
%However, CL faces several challenges: 1) it requires a large batch size **Newell, "Large Batch Size Training of Deep Neural Networks",** 2) it performs poorly on small models **Bender, "Adversarial Examples for Evaluating Reading Comprehension Models",** and 3) it is less robust to noisy data **Fei-Fei, "Noise-Robust Learning: A Simple Algorithm with Optimal Rates"**. To address these limitations, 
Knowledge distillation (KD)**Hinton, "Distilling the Knowledge in a Neural Network",** is another widely used fine-tuning objective. 
%Unlike CL, KD does not require a large batch size **Buciluoa, "Training Large-Variant Models with Distillation",**, making it more computationally efficient and less demanding on GPU resources. 
It has also been shown to perform comparably to or even better than CL for small-scale retrieval models, particularly on in-domain benchmarks **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

Scaling LLMs has consistently led to improved performance across various NLP tasks**Radford, "Improving Language Understanding by Generative Models",**. This trend also extends to retrieval models. **Karpukhin, "Dense Passage Retrieval for Long-Contextualized Question Answering"**, demonstrated that scaling BERT-based dense retrieval models enhances retrieval performance.
Recent works**Liu, "RoBERTa: A Robustly Optimized BERT Pretraining Approach",** have further explored this trend by training dense retrieval models with larger backbones, such as LLaMA2-7B **Stoyanov, "Llama 2: An Improved Large Language Model"**. These models have achieved state-of-the-art performance, surpassing traditional BERT-based retrieval models.
%However, these studies have exclusively focused on contrastive loss (CL) for dense retrieval, overlooking the scaling behaviors of sparse retrieval and knowledge distillation (KD), which is our focus in the paper.
\vspace{-.25cm}