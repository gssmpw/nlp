\section{Related Work}
Single-vector retrieval models can be broadly categorized into two types: dense retrieval ____ and sparse retrieval ____. The key distinction lies in their representations: dense retrieval encodes texts into low-dimensional vectors (i.e. 768 dim in BERT) and employs approximate nearest neighbor (ANN)____ search for efficient retrieval. In contrast, sparse retrieval represents texts using high-dimensional sparse vectors and relies on an inverted index____ for fast retrieval. 
 %Compared to multi-vector retrieval models____, such as ColBERT____, single-vector retrieval is faster, requires less index memory, and is easier to implement.
 
The most straightforward fine-tuning objective for retrieval models is supervised contrastive loss (CL) ____. By incorporating techniques such as hard negative sampling ____ and retrieval-oriented continued pre-training ____, the performance of retrieval models can be further improved. 
%However, CL faces several challenges: 1) it requires a large batch size ____, 2) it performs poorly on small models ____, and 3) it is less robust to noisy data ____. To address these limitations, 
Knowledge distillation (KD)____ is another widely used fine-tuning objective. 
%Unlike CL, KD does not require a large batch size ____, making it more computationally efficient and less demanding on GPU resources. 
It has also been shown to perform comparably to or even better than CL for small-scale retrieval models, particularly on in-domain benchmarks ____.

Scaling LLMs has consistently led to improved performance across various NLP tasks____. This trend also extends to retrieval models. ____ demonstrated that scaling BERT-based dense retrieval models enhances retrieval performance.
Recent works____ have further explored this trend by training dense retrieval models with larger backbones, such as LLaMA2-7B____. These models have achieved state-of-the-art performance, surpassing traditional BERT-based retrieval models.
%However, these studies have exclusively focused on contrastive loss (CL) for dense retrieval, overlooking the scaling behaviors of sparse retrieval and knowledge distillation (KD), which is our focus in the paper.
\vspace{-.25cm}