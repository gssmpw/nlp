@article{adore,
  title={Optimizing Dense Retrieval Model Training with Hard Negatives},
  author={Jingtao Zhan and Jiaxin Mao and Yiqun Liu and Jiafeng Guo and M. Zhang and Shaoping Ma},
  journal={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233289894}
}

@inproceedings{b-prop,
author = {Ma, Xinyu and Guo, Jiafeng and Zhang, Ruqing and Fan, Yixing and Li, Yingyan and Cheng, Xueqi},
title = {B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462869},
doi = {10.1145/3404835.3462869},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1513–1522},
numpages = {10},
keywords = {ad-hoc retrieval, bootstrapped pre-training, pre-trained language model},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{chinchilla,
author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
title = {Training compute-optimal large language models},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2176},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{citadel,
  title={CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval},
  author={Minghan Li and Sheng-Chieh Lin and Barlas Oğuz and Asish Ghoshal and Jimmy J. Lin and Yashar Mehdad and Wen-tau Yih and Xilun Chen},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253708231}
}

@inproceedings{cl-drd,
author = {Zeng, Hansi and Zamani, Hamed and Vinay, Vishwa},
title = {Curriculum Learning for Dense Retrieval Distillation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531791},
doi = {10.1145/3477495.3531791},
pages = {1979–1983},
numpages = {5},
keywords = {curriculum learning, dense retrieval, knowledge distillation, neural ranking models},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{co-condenser,
    title = "Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval",
    author = "Gao, Luyu  and
      Callan, Jamie",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.203/",
    doi = "10.18653/v1/2022.acl-long.203",
    pages = "2843--2853",
}

@inproceedings{colbert,
author = {Khattab, Omar and Zaharia, Matei},
title = {ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401075},
doi = {10.1145/3397271.3401075},
pages = {39–48},
numpages = {10},
keywords = {bert, deep language models, efficiency, neural ir},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{colbertv2,
    title = "{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction",
    author = "Santhanam, Keshav  and
      Khattab, Omar  and
      Saad-Falcon, Jon  and
      Potts, Christopher  and
      Zaharia, Matei",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.272/",
    doi = "10.18653/v1/2022.naacl-main.272",
    pages = "3715--3734",
    abstract = "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce ColBERTv2, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6{--}10x."
}

@inproceedings{dpr,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
}

@article{emergent-llm,
title={Emergent Abilities of Large Language Models},
author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=yzkSU5zdwD},
note={Survey Certification}
}

@misc{faiss-gpu,
      title={Billion-scale similarity search with GPUs}, 
      author={Jeff Johnson and Matthijs Douze and Hervé Jégou},
      year={2017},
      eprint={1702.08734},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1702.08734}, 
}

@misc{gpt-3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@article{inverted-index,
author = {Zobel, Justin and Moffat, Alistair},
title = {Inverted files for text search engines},
year = {2006},
issue_date = {2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/1132956.1132959},
doi = {10.1145/1132956.1132959},
abstract = {The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.},
journal = {ACM Comput. Surv.},
month = jul,
pages = {6–es},
numpages = {56},
keywords = {text retrieval, information retrieval, document database, Web search engine, Inverted file indexing}
}

@article{kd,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531},
  url={https://api.semanticscholar.org/CorpusID:7200347}
}

@article{llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and others},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}

@inproceedings{llama2vec,
    title = "{L}lama2{V}ec: Unsupervised Adaptation of Large Language Models for Dense Retrieval",
    author = "Liu, Zheng  and
      Li, Chaofan  and
      Xiao, Shitao  and
      Shao, Yingxia  and
      Lian, Defu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.191/",
    doi = "10.18653/v1/2024.acl-long.191",
    pages = "3490--3500",
}

@article{margin-mse,
  title={Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation},
  author={Sebastian Hofst{\"a}tter and Sophia Althammer and Michael Schr{\"o}der and Mete Sertkan and Allan Hanbury},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.02666},
  url={https://api.semanticscholar.org/CorpusID:222141041}
}

@inproceedings{mt-scaling-1,
title={When Scaling Meets {LLM} Finetuning: The Effect of Data, Model and Finetuning Method},
author={Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=5HCnKDeTws}
}

@inproceedings{mt-scaling-2,
title={Scaling Laws for Downstream Task Performance in Machine Translation},
author={Berivan Isik and Natalia Ponomareva and Hussein Hazimeh and Dimitris Paparas and Sergei Vassilvitskii and Sanmi Koyejo},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=vPOMTkmSiu}
}

@inproceedings{repllama,
author = {Ma, Xueguang and Wang, Liang and Yang, Nan and Wei, Furu and Lin, Jimmy},
title = {Fine-Tuning LLaMA for Multi-Stage Text Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657951},
doi = {10.1145/3626772.3657951},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2421–2425},
numpages = {5},
keywords = {dense retrieval, large language model, reranker},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{rocketqa,
    title = "{R}ocket{QA}: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Qu, Yingqi  and
      Ding, Yuchen  and
      Liu, Jing  and
      Liu, Kai  and
      Ren, Ruiyang  and
      Zhao, Wayne Xin  and
      Dong, Daxiang  and
      Wu, Hua  and
      Wang, Haifeng",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.466/",
    doi = "10.18653/v1/2021.naacl-main.466",
    pages = "5835--5847",
}

@inproceedings{rocketqav2,
    title = "{R}ocket{QA}v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
    author = "Ren, Ruiyang  and
      Qu, Yingqi  and
      Liu, Jing  and
      Zhao, Wayne Xin  and
      She, QiaoQiao  and
      Wu, Hua  and
      Wang, Haifeng  and
      Wen, Ji-Rong",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.224/",
    doi = "10.18653/v1/2021.emnlp-main.224",
    pages = "2825--2835",
    abstract = "In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other`s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at \url{https://github.com/PaddlePaddle/RocketQA}."
}

@inproceedings{scale-dr,
author = {Fang, Yan and Zhan, Jingtao and Ai, Qingyao and Mao, Jiaxin and Su, Weihang and Chen, Jia and Liu, Yiqun},
title = {Scaling Laws For Dense Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657743},
doi = {10.1145/3626772.3657743},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1339–1349},
numpages = {11},
keywords = {dense retrieval, large language models, neural scaling law},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{snrm,
author = {Zamani, Hamed and Dehghani, Mostafa and Croft, W. Bruce and Learned-Miller, Erik and Kamps, Jaap},
title = {From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271800},
doi = {10.1145/3269206.3271800},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {497–506},
numpages = {10},
keywords = {weak supervision, sparse representation, semantic matching, neural ranking models, inverted index, efficiency, document representation, ad-hoc retrieval},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{spade,
author = {Choi, Eunseong and Lee, Sunkyung and Choi, Minjin and Ko, Hyeseon and Song, Young-In and Lee, Jongwuk},
title = {SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557456},
doi = {10.1145/3511808.3557456},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {272–282},
numpages = {11},
keywords = {neural ranking, pre-trained language model, sparse representations},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{splade,
author = {Formal, Thibault and Piwowarski, Benjamin and Clinchant, St\'{e}phane},
title = {SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463098},
doi = {10.1145/3404835.3463098},
abstract = {In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2288–2292},
numpages = {5},
keywords = {indexing, neural networks, regularization, sparse representations},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{splade-mistral,
  title={Mistral-SPLADE: LLMs for better Learned Sparse Retrieval},
  author={Meet Doshi and Vishwajeet Kumar and Rudra Murthy and P Vignesh and Jaydeep Sen},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.11119},
  url={https://api.semanticscholar.org/CorpusID:271915981}
}

@inproceedings{splade-v2,
author = {Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St\'{e}phane},
title = {From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531857},
doi = {10.1145/3477495.3531857},
abstract = {Neural retrievers based on dense representations combined with Approximate Nearest Neighbors search have recently received a lot of attention, owing their success to distillation and/or better sampling of examples for training -- while still relying on the same backbone architecture. In the meantime, sparse representation learning fueled by traditional inverted indexing techniques has seen a growing interest, inheriting from desirable IR priors such as explicit lexical matching. While some architectural variants have been proposed, a lesser effort has been put in the training of such models. In this work, we build on SPLADE -- a sparse expansion-based retriever -- and show to which extent it is able to benefit from the same training improvements as dense models, by studying the effect of distillation, hard-negative mining as well as the Pre-trained Language Model initialization. We furthermore study the link between effectiveness and efficiency, on in-domain and zero-shot settings, leading to state-of-the-art results in both scenarios for sufficiently expressive models.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2353–2359},
numpages = {7},
keywords = {indexing, neural networks, regularization, sparse representations},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{tas-b,
author = {Hofst\"{a}tter, Sebastian and Lin, Sheng-Chieh and Yang, Jheng-Hong and Lin, Jimmy and Hanbury, Allan},
title = {Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462891},
doi = {10.1145/3404835.3462891},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {113–122},
numpages = {10},
keywords = {batch sampling, dense retrieval, knowledge distillation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{tct-colbert,
    title = "In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval",
    author = "Lin, Sheng-Chieh  and
      Yang, Jheng-Hong  and
      Lin, Jimmy",
    editor = "Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.17/",
    doi = "10.18653/v1/2021.repl4nlp-1.17",
    pages = "163--173",
}

