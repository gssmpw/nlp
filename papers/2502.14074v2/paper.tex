\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{bbm, cuted, array, multirow}
\usepackage{subcaption}
\usepackage{enumitem} 
\usepackage{bm}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{paper}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{colortbl}
\usepackage[most]{tcolorbox}
\usepackage[T1]{fontenc}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\graybg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox{gray!10}{#1}%
  \endgroup
}

\newcommand{\orangebg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox{orange!10}{#1}%
  \endgroup
}

\newcommand{\bluebg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox{blue!10}{#1}%
  \endgroup
}

\newcommand{\yellowbg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox[HTML]{FFF2CC}{#1}%
  \endgroup
}

\newcommand{\redbg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox[HTML]{FFCCCC}{#1}%
  \endgroup
}

\newcommand{\greenbg}[1]{%
  \begingroup\setlength{\fboxsep}{0pt}%
  \colorbox[HTML]{90EE90}{#1}%
  \endgroup
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=none,
    aboveskip=1pt,
    belowskip=1pt,
}
\lstdefinestyle{plainins}{
    backgroundcolor=\color{white},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    aboveskip=0pt,
    belowskip=0pt,
    frame=single,
    escapeinside={(*@}{@*)}
}

\lstdefinestyle{plainexam}{
    backgroundcolor=\color[HTML]{FFFCF3},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    aboveskip=0pt,
    belowskip=0pt
}

\lstset{style=mystyle}
\lstset{style=plainins}
\lstset{style=plainexam}

\icmltitlerunning{Investigating Non-Transitivity in LLM-as-a-Judge}

\begin{document}

\twocolumn[
\icmltitle{Investigating Non-Transitivity in LLM-as-a-Judge}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yi Xu}{ucl}
\icmlauthor{Laura Ruis}{ucl}
\icmlauthor{Tim Rockt\"aschel}{ucl}
\icmlauthor{Robert Kirk}{ucl,ukaisi}
\end{icmlauthorlist}

\icmlaffiliation{ucl}{AI Centre, UCL}
\icmlaffiliation{ukaisi}{UK AI Security Institute}

\icmlcorrespondingauthor{Yi Xu}{y.xu.23@ucl.ac.uk}

\icmlkeywords{Machine Learning}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %

\begin{abstract}
Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0\% $\rightarrow$ 96.4\% and 82.1\% $\rightarrow$ 86.3\% respectively). To address the computational cost of round-robin tournaments, we propose \textbf{S}wiss-\textbf{W}ise \textbf{I}terative \textbf{M}atchmaking (\textsc{Swim}) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.
\end{abstract}

\section{Introduction}
\label{introduction}
\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{imgs/winrate_matrix.pdf}
\caption{\textbf{Rankings from baseline-fixed frameworks show high sensitivity to the choice of baseline.} Each entry $(x, y)$ represents the win rate of model $m_x$ against $m_y$, where each column reflects a ranking with the column model as the baseline. Inconsistency emerges when \textcolor[HTML]{2f70b8}{{Llama-3-70B}} and \textcolor[HTML]{cb1f42}{{Claude-3-Opus}} are used as baselines. \cref{detailed_matrix} provides the detailed matrix comparing 20 models.}
  \label{fig:winrate} 
\end{figure}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1\textwidth]{imgs/overview.pdf}
  \vspace{-2mm}
\caption{(Left) Inconsistent rankings are observed in baseline-fixed frameworks based on pairwise comparisons due to non-transitivity in the judge's evaluations. Different choices of baselines can lead to varying rankings, undermining the reliability and robustness of this approach. 
(Right) We propose a round-robin tournament framework where all models are compared pairwise. The results are used to capture non-transitivity in the judge's evaluations and score models using the Bradley-Terry model. This method produces rankings that are more robust and better aligned with human evaluation.}
  \label{fig:overview}
\end{figure*}
 
The growing adoption of large language models (LLMs) as generalist systems for complex, open-ended tasks \cite{openai2024gpt4technicalreport, llama31} presents a critical challenge: the lack of a universally accepted gold-standard evaluation. 
In many cases, multiple valid responses exist for a given task, complicating the establishment of effective benchmarks. Consequently, a new paradigm for evaluating open-ended tasks focuses on quantifying the alignment of LLMs with human preferences \cite{RLHF} --- an aspect existing automatic metrics cannot adequately assess. However, human evaluation is costly and lacks scalability \cite{karpinska-etal-2021-perils}. As a result, LLM-based evaluators are now widely used to automate the process, with pairwise comparisons proving particularly effective in aligning with human ratings \cite{liusie2024llmcomparativeassessmentzeroshot, liu2024aligning, vicuna2023, alpaca_eval, lin2024wildbenchbenchmarkingllmschallenging, zheng2023judgingllmasajudgemtbenchchatbot, samvelyan2024rainbowteamingopenendedgeneration, khan2024debatingpersuasivellmsleads}.

The typical pipeline for LLM-based automatic evaluation frameworks is using pairwise comparisons between a target model and a fixed baseline model, where an oracle model serves as the judge. By calculating the relative win rate against the baseline model, such comparisons enable ranking target models. However, it is unclear whether using a fixed baseline provides consistent results. If the judge exhibits non-transitive preferences, such as favoring A over B, B over C, but C over A, the resulting rankings can become sensitive to the choice of the baseline model (\cref{fig:overview}).

In this work, we investigate the existence and impact of non-transitivity within AlpacaEval \cite{alpaca_eval}, which has been largely overlooked in previous work. AlpacaEval is a popular pairwise comparison framework that employs GPT-4-Turbo as the fixed baseline model. We introduce {Soft Non-Transitivity Deviation (SNTD)} as a metric to measure the degree of soft non-transitivity in the judge's continuous preferences and find that LLMs exhibit both hard and soft non-transitive preferences. Additionally, previous studies have demonstrated that LLMs often exhibit various biases \cite{gallegos-etal-2024-bias} such as position bias \cite{zheng2023judgingllmasajudgemtbenchchatbot, wang2023largelanguagemodelsfair, zhou2024batch}, which can lead to spurious correlations in the judge's preferences. We show that the occurrence of non-transitivity is jointly influenced by position bias and the judge model's inherent non-transitive reasoning abilities.

To address the above, we propose the use of round-robin tournaments in the pairwise comparison setting, overcoming the need for a fixed baseline model. We subsequently apply the Bradley-Terry model \cite{BT} to score models based on tournament outcomes, yielding a more consistent ranking compared to baseline-fixed ranking. To address the computational cost in the round-robin tournament, we propose \textbf{S}wiss-\textbf{W}ise \textbf{I}terative \textbf{M}atchmaking (\textsc{Swim}) tournaments to improve efficiency while preserving the robustness of model comparisons.

Our contributions are as follows: 1) \textbf{We show that LLMs exhibit non-transitive preferences when performing pairwise comparisons.} Additionally, we observe that the aggregation of instruction-level non-transitive relationships culminates in model-level non-transitivity (Figure \ref{fig:winrate}). We demonstrate that such non-transitivity makes the ranking highly sensitive to the choice of the baseline model. Changing the baseline model makes the rank order inconsistent and unstable, highlighting the importance of proposing new ranking methods. 2) \textbf{We find that while position bias significantly contributes to non-transitivity, it is not the sole cause.} Our experiments confirm that position switching outperforms random assignment in mitigating position bias for stronger judges when using continuous values for judge's preferences, with reductions ranging from 17\% to 44\%. 3) \textbf{We demonstrate that applying round-robin tournaments combined with the Bradley-Terry model reduces the impact of non-transitivity, resulting in more robust rankings.} This method also aligns better with human evaluations of model rankings in Chatbot Arena. Finally, we introduce \textsc{Swim}, an efficient method for adding models with nearly identical performance compared to naive round-robin tournaments. 

\section{Related Work}
\textbf{LLM-as-a-Judge.}
The LLM-as-a-Judge \cite{zheng2023judgingllmasajudgemtbenchchatbot} evaluation method leverages frontier models to rank responses to open-ended queries without explicit ground-truths. A common approach involves using a fixed baseline model for pairwise comparisons to assess the performance of the target model, as seen in frameworks such as VicunaEval \cite{vicuna2023}, AlpacaEval \cite{alpaca_eval}, and Arena-Hard \cite{li2024crowdsourceddatahighqualitybenchmarks}. The target models are then ranked on the basis of their win rates against the baseline. However, an implicit assumption in these frameworks is that transitivity holds in preference judgments, which has not been empirically verified. Transitivity requires that if an LLM judge prefers model $m_A$ over $m_B$ and $m_B$ over $m_C$, it must consequently prefer $m_A$ over $m_C$. Violations of transitivity can result in unstable rankings that undermine the evaluation framework's reliability (\cref{fig:overview}). To address this gap, we examine the robustness of current LLM ranking methodologies by extending the AlpacaEval framework to investigate the existence of non-transitivity, aiming to establish a more rigorous foundation for the LLM evaluation system.

\textbf{Non-Transitivity in Zero-sum Games.} Prior work has explored non-transitivity in two-player zero-sum games within multi-agent reinforcement learning. \citet{balduzzi2019openendedlearningsymmetriczerosum} characterize agent interactions through convex polytopes, using their dimensionality to decompose transitive and cyclic components. \citet{NEURIPS2020_ca172e96} demonstrate that real-world strategy spaces exhibit a spinning top distribution, where non-transitivity peaks at middling performance levels but diminishes at either lower or higher levels. Given the presence of non-transitivity, evaluating a strategy based on its performance against a single opponent does not reliably reflect its true capability. Therefore, previous achievements in complex games such as StarCraft \cite{2019alphastar} and Dota 2 \cite{openai2019dota2largescale} employ population-based self-play training and evaluate agents through tournament-style competitions against diverse opponents. Mirroring the population-based evaluation paradigm that succeeded in non-transitive games, we adopt tournament-based comparisons in LLM-as-a-Judge frameworks to mitigate ranking instability induced by non-transitivity.

\section{Methods}
\subsection{Measuring Non-Transitivity in Pairwise Comparisons}
\label{measure_transitivity}
We employ an LLM, denoted as $m_{\mathrm{J}}$, to conduct pairwise comparisons between models $m_{A}$ and $m_{B}$. The objective is to determine which of the two outputs, $o_{A}^{(i)}$ or $o_{B}^{(i)}$, better follows a given instruction $I_i$. To facilitate the comparison, each model output is assigned a unique token identifier. The antisymmetric judge function $ \phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{J}, I_i)$ evaluates pairs of outputs from models and determines the probability of favoring $o_{A}^{(i)}$ as the win rate by applying a softmax operation to the log probabilities of the corresponding model tokens. The preference of $m_{A}$ over $m_{B}$ is then quantified by taking the expected value of the judge function:
\begin{equation}
J(m_A \succ m_B \mid I_i) = \mathbb{E}\left[\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)\right].
\end{equation}

\textbf{Hard Non-Transitive Cases.}
To quantify non-transitivity among a triplet of models \( (m_A, m_B, m_C) \), we first compute the Percentage of Non-Transitive cases (PNT) over the instruction set \( \mathcal{I} \), defined as:
\begin{equation}
\mathrm{PNT} = \frac{1}{|\mathcal{I}|} \sum_{I_i \in \mathcal{I}} \mathbbm{1}_{\text{non-trans.}}(m_A, m_B, m_C \mid m_{\mathrm{J}}, I_i),
\end{equation}
where the indicator function \( \mathbbm{1}_{\text{non-trans.}} \) returns 1 when the judge's preferences violate logical transitivity, and 0 otherwise. See \cref{subsec:non_trans_condition} for the complete set of conditions.

However, this metric demonstrates a limitation in sensitivity: given $J(m_A \succ m_B \mid I) = 1$ and $J(m_B \succ m_C \mid I) = 1$, it classifies both $J(m_A \succ m_C \mid I) = 0$ and $J(m_A \succ m_C \mid I) = 0.49$ as non-transitive, despite the latter exhibiting substantially stronger transitivity tendency as it is closer to the transitive threshold. Such insensitivity to transitional values near the decision boundary undermines the metric's capacity to capture nuanced deviations from ideal transitivity. 

\textbf{Soft Transitivity Deviation.} To address this limitation, we propose Soft Non-Transitivity Deviation (SNTD) to measure the \emph{degree} of non-transitivity for a single instruction with a triplet of models, defined as:
\begin{equation}
\begin{aligned}
&\mathrm{SNTD}(m_A, m_B, m_C \,|\, I_i) = \\
& \frac{1}{3} \times \mathbb{E}\Bigg[
 \textsc{Jsd}\left(\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i) \| \hat{\phi}(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)\right) \\
& + \textsc{Jsd}\left(\phi(o_{B}^{(i)}, o_{C}^{(i)} \mid m_{\mathrm{J}}, I_i) \| \hat{\phi}(o_{B}^{(i)}, o_{C}^{(i)} \mid m_{\mathrm{J}}, I_i)\right)  \\
& + \textsc{Jsd}\left(\phi(o_{A}^{(i)}, o_{C}^{(i)} \mid m_{\mathrm{J}}, I_i) \| \hat{\phi}(o_{A}^{(i)}, o_{C}^{(i)} \mid m_{\mathrm{J}}, I_i)\right) 
\Bigg],
\end{aligned}
\label{SNTD}
\end{equation}
where the Jensen–Shannon divergence ($\textsc{Jsd}$) quantifies the discrepancy between observed win rates $\phi$ and estimated win rates $\hat{\phi}$ under transitivity assumptions, as defined below.

\textbf{Estimated Win Rate.} We denote the latent quality of the outputs from models \( m_A \), \( m_B \), and \( m_C \) on instruction \( I_i \) as \( \gamma_{A}^{(i)} \), \( \gamma_{B}^{(i)} \), and \( \gamma_{C}^{(i)} \), respectively. Given empirical observations $\phi$, Bradley-Terry model \cite{BT} estimate the quality gap as:
\begin{equation}
s_{AB}^{(i)} = \gamma_{A}^{(i)} - \gamma_{B}^{(i)} = \ln\left( \frac{\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)}{1 - \phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)} \right).
\end{equation}
Based on that, we can estimate the expected win rate $\hat{\phi}$ under transitivity between any two models from a triplet \( (m_A, m_B, m_C) \) by utilizing the observed win rates between the other two pairs as (See \cref{appendix:expected_winrate} for the derivation): 
\begin{equation}
\hat{\phi}(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i) = \frac{1}{1 + e^{s_{AC}^{(i)} - s_{BC}^{(i)}}}.
\end{equation}

\subsection{Measuring Model Performance}
In this section, we define metrics to quantify and rank model performance given a model pool $\mathcal{M}$, instruction dataset $\mathcal{I}$, and judge $m_\mathrm{J}$.

\textbf{Win Rate Against Baseline.} Through currying the judge function with a fixed baseline model $m_\mathrm{base}$,  we define the win rate against the baseline model as a rating function:
\begin{equation}
\mathcal{R}_\mathrm{base}(\cdot) = \frac{1}{|\mathcal{I}|}\sum_{I_i\in\mathcal{I}}\mathbb{E}\left[ \phi(\cdot, o_{m_\mathrm{base}}^{(i)} \mid m_{\mathrm{J}}, I_i)\right].
\end{equation}
\textbf{Bradley-Terry Coefficients.}
Given a series of pairwise comparisons, we employ the Bradley-Terry (BT) model to convert comparison outcomes into coefficients \(\beta_i \in \mathbb{R}\) that quantify the strength of model $m_i$. The optimal BT coefficients \(\hat{\bm{\beta}}\) are estimated by maximizing the likelihood:
\begin{equation}
    \hat{\bm{\beta}} = \arg\max_{\bm{\beta}} \sum_{i}\sum_{j\neq i} \left[ W_{i,j} \cdot \ln \left( \frac{1}{1 + e^{(\beta_j - \beta_i)}} \right)  \right],
\label{mle}
\end{equation}
where $W_{i,j}$ represents the number of times model $i$ wins against model $j$. Rather than using discrete labels \(\{0,1\}\) to count victories, we utilize the judge's preferences as soft labels, defining $W_{i,j} = \sum_{I_k\in\mathcal{I}} J(m_i \succ m_j \mid I_k)$, which yields more accurate estimations (See \cref{sec:hard_bt_vs_soft_bt}).

\textbf{Elo Rating.}
To establish a standardized measure of model performance, we convert Bradley-Terry coefficients to Elo ratings \cite{elo1966uscf} by setting \(\xi_{i} = 400 \log_{10} \beta_i\). Under this system, the probability of model $m_i$ winning against model $m_j$ is expressed as:
\begin{equation}
P(m_i \succ m_j) = \frac{1}{1 + 10^{(\xi_j - \xi_i)/400}}.
\label{elo_1}
\end{equation}

\subsection{Tournament-Based Ranking}
We formalize the LLM-as-a-Judge evaluation as a multi-player game framework, where evaluated language models act as players. Each player's strategy space is defined by its response generation approach under given instructions. When the judge exhibits non-transitive evaluation behavior, model assessment through fixed-opponent comparisons cannot provide reliable rankings, leading us to characterize this evaluation framework as a non-transitive game.

\textbf{Round-Robin Tournament.} 
Tournament-based competition with diverse opponents has been established as an effective approach for performance evaluation in non-transitive games \cite{openai2019dota2largescale, 2019alphastar}, as it enables robust assessment of relative capabilities while mitigating the impact of non-transitivity. Based on this insight, we propose a round-robin tournament structure where each model engages in pairwise evaluation against every other model in the pool, with evaluations conducted by judge $m_\mathrm{J}$ over instruction set $\mathcal{I}$. This method enables comprehensive model evaluation through comparisons against a diverse population of models rather than relying on a fixed perspective for assessment. We subsequently apply the BT model to comparison outcomes to assign scores, which are then converted into Elo scores for the final ranking.

\textbf{Swiss-Wise Iterative Matchmaking Tournament.}
While round-robin evaluation yields reliable rankings, it presents significant computational challenges at scale. Incorporating a new model into a leaderboard of size $M$ necessitates $(M-1)$ model-level comparisons compared to a single comparison in baseline-fixed frameworks. To address this computational bottleneck, we propose the \textbf{S}wiss-\textbf{W}ise \textbf{I}terative \textbf{M}atchmaking (\textsc{Swim}) tournament (\cref{alg:ranking}), drawing inspiration from binary search and Swiss-system tournaments. Our approach dynamically adjusts matchmaking based on Bradley-Terry coefficients, focusing comparisons near model capability boundaries in a logarithmic manner, thereby reducing the number of comparisons to $\lceil\log_2(M)\rceil$. 

\subsection{Evaluation Setup}
\begin{table*}[htbp]
\centering
\caption{We measure non-transitivity in four scenarios, evaluated by GPT-4-Turbo and GPT-3.5-Turbo. \orangebg{Orange cells} indicate maximum PNT/SNTD values (highest non-transitivity); \bluebg{blue cells} indicate minimum PNT/SNTD values (highest transitivity). When using GPT-4-Turbo as the judge, more non-transitivity can be observed as evaluated model performance becomes more similar and  the highest non-transitivity occurs when the performances of all three models are similar; however, GPT-3.5-Turbo does not exhibit this pattern.}
\vspace{2mm}
\small
\begin{tabular}{>{\centering\arraybackslash}p{1.8cm} l c c | c c}
\toprule
\multirow{2}{*}{\textbf{Scenarios}}  & \multirow{2}{*}{\hspace{2.5cm}\textbf{Models}} & \multicolumn{2}{c}{\cellcolor{gray!25}\textit{GPT-4-Turbo}}  & \multicolumn{2}{c}{\cellcolor{gray!25}\textit{GPT-3.5-Turbo}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
 &  & {PNT} & {SNTD} & {PNT} & {SNTD} \\
\midrule
\textbf{LL} & \(m_A = \texttt{gpt-4o-2024-05-13}\) & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{21.37} & \multirow{3}{*}{} \\
\( m_A \gg m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & \cellcolor{blue!10}{{\textbf{3.98}}} & \cellcolor{blue!10}{{\textbf{0.1121}}} & & \cellcolor{orange!10}{{\textbf{0.2654}}} \\
\( m_B \gg m_C \)   & \(m_C = \texttt{Mistral-7B-Instruct-v0.2}\) & & & & \\
\midrule
\textbf{LM} & \(m_A = \texttt{gpt-4o-2024-05-13}\) & \multirow{3}{*}{5.96} & \multirow{3}{*}{0.1336} & \multirow{3}{*}{22.48} & \multirow{3}{*}{} \\
\( m_A \gg m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & & & & \cellcolor{blue!10}{{\textbf{0.2586}}} \\
\( m_B \approx m_C \)   & \(m_C = \texttt{claude-3-sonnet-20240229}\) & & & & \\
\midrule
\textbf{ML} & \(m_A = \texttt{Yi-34B-Chat}\) & \multirow{3}{*}{} & \multirow{3}{*}{0.1215} & \multirow{3}{*}{} & \multirow{3}{*}{0.2625} \\
\( m_A \approx m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & \cellcolor{blue!10}{{\textbf{3.98}}} & & \cellcolor{orange!10}{{\textbf{22.86}}} & \\
\( m_B \gg m_C \)   & \(m_C = \texttt{Mistral-7B-Instruct-v0.2}\) & & & & \\
\midrule
\textbf{MM} &  \(m_A = \texttt{Qwen1.5-72B-Chat}\) & \multirow{3}{*}{} & \multirow{3}{*}{\textbf{0.1431}} & \multirow{3}{*}{} & \multirow{3}{*}{0.2629} \\
\( m_A \approx m_B \)   & \(m_B = \texttt{claude-3-sonnet-20240229}\) & \cellcolor{orange!10}{{\textbf{8.45}}}
 & \cellcolor{orange!10}{\textbf{0.1431}} & \cellcolor{blue!10}{{\textbf{20.87}}} & \\
\( m_B \approx m_C \)   & \(m_C = \texttt{gpt-4-0314}\) & & & & \\
\bottomrule
\end{tabular}
\label{4senarios}
\end{table*}

\textbf{Datasets.} We use the AlpacaEval dataset \cite{alpaca_eval}, which includes a wide variety of instruction types, such as information search tasks and coding problems. 

\textbf{Participating models.} We evaluate 20 models that appear on both the AlpacaEval and Chatbot Arena\footnote{Refer to Fully Style-Controlled Chatbot Arena Leaderboard (2024/09/15)} leaderboards (see Appendix \ref{llm_details} for details).

\textbf{Scenarios.} We denote significant performance advantages with \(\gg\) and marginal advantages with \(\approx\). Based on relative model performance, we classify model triplets $(m_A, m_B, m_C)$ into four categories:

1. Lead \& Lead (\textbf{LL}): \(m_A \gg m_B\) and \(m_B \gg m_C\).

2. Lead \& Margin (\textbf{LM}): \(m_A \gg m_B\) and \(m_B \approx m_C\).

3. Margin \& Lead (\textbf{ML}): \(m_A \approx m_B\) and \(m_B \gg m_C\).

4. Margin \& Margin (\textbf{MM}): \(m_A \approx m_B\) and \(m_B \approx m_C\).

For each scenario, we select representative model triplets based on the win rates of participating models from the AlpacaEval leaderboard (see \cref{scenarios_details} for details).

\textbf{Judge models.}
For consistency with AlpacaEval, we maintain the judge configuration and prompt templates. We examine non-transitivity in judgments using two models: GPT-4-Turbo\footnote{GPT-4-Turbo refers to \texttt{gpt-4-1106-preview} in the win rate matrix to avoid ambiguity.} and GPT-3.5-Turbo \cite{openai2024gpt4technicalreport}, both with the temperature set to 0. The detailed prompt is provided in \cref{prompt_template}.

\textbf{Position Switching.} LLMs are known to exhibit biases and inconsistencies based on the order of outputs presented in the prompt \cite{ zheng2023judgingllmasajudgemtbenchchatbot, pezeshkpour2023largelanguagemodelssensitivity, raina2024llmasajudgerobustinvestigatinguniversal}. To mitigate this bias, we employ position switching, where each comparison is evaluated with responses in both orders. The final preference score is calculated as the mean of these balanced evaluations. To reduce the impact of API randomness, we invoke the judge function twice for each order configuration.

\section{Non-Transitive Judge Preferences}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1\textwidth]{imgs/position_bias_stacked.pdf}
  \vspace{-8mm}
  \caption{\textbf{Larger performance gaps lead to more consistent preferences.} We quantify the proportion of consistent preferences of GPT-4-Turbo and GPT-3.5-Turbo across four scenarios differentiated by relative model performance, where \(\gg\) denotes substantial performance advantages and \(\approx\) indicates marginal differences.}
  \label{position_bias_two_judge}
\end{figure*}
\label{sec:non_transitive_preferences}
\begin{figure}[t]
\centering
\includegraphics[width=0.475\textwidth]{imgs/combined_heatmaps.pdf}
\vspace{-2mm}
\caption{\textbf{Non-transitivity becomes more pronounced as the model performance gap approaches the origin.} We find that both PNT and SNTD peak near the origin when GPT-4-Turbo serves as the judge.}
\label{fig:heatmap}
\end{figure}

In this section, we investigate the judge's non-transitive behaviors and analyze their underlying mechanisms.
\subsection{Increased Non-Transitivity with Similar Model}
As shown in \cref{4senarios}, non-transitivity emerges across all four scenarios when GPT-4-Turbo serves as the judge. Both PNT and SNTD generally increase as the performance gap between model pairs $(m_A, m_B)$ or $(m_B, m_C)$ narrows. Notably, while scenarios \textbf{LL} and \textbf{ML} have identical PNT scores, scenario \textbf{ML} exhibits a higher SNTD value, indicating more non-transitivity. This discrepancy highlights the limitation of the PNT—it fails to capture the continuous nature of judge preferences in assessing non-transitivity.

\textbf{Weaker Judge is More Non-Transitive.} Replicating our evaluation with GPT-3.5-Turbo as the judge reveals an intriguing pattern (Table \ref{4senarios}): both PNT and SNTD values are consistently higher than those observed with GPT-4-Turbo and remain relatively stable across all scenarios, suggesting a persistent and substantial level of non-transitivity.

Previous studies have demonstrated that GPT-4-Turbo possesses stronger reasoning capabilities and exhibits significantly less bias compared to GPT-3.5-Turbo \cite{zheng2023judgingllmasajudgemtbenchchatbot}. We hypothesize that the strong non-transitivity observed with GPT-3.5-Turbo stems from its inability to distinguish the quality differences among outputs, as it is generally considered to have weaker instruction-following abilities than most participating models \cite{chiang2024chatbotarenaopenplatform, lin2024wildbenchbenchmarkingllmschallenging, alpaca_eval, white2024livebenchchallengingcontaminationfreellm}. This inability leads to preferences driven by bias predominantly, which is empirically validated in \cref{judge_capability}.

\subsection{Aggregate Non-Transitivity}
\label{sec:aggregation}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{imgs/position_bias_combined.pdf}
  \vspace{-6mm}
\caption{Proportion of (non-)transitive instructions across all scenarios, as evaluated by GPT-4-Turbo and GPT-3.5-Turbo. When evaluating model triplets with GPT-3.5-Turbo as judge, over 96\% of instructions exhibit position bias effects. In contrast, GPT-4-Turbo demonstrates substantially higher evaluation consistency. Our analysis reveals that position switching provides more effective bias mitigation than random assignment for less position-biased judges.}
  \label{position_bias_trans}
\end{figure*}

We use \( J(m_A \succ m_B) = \frac{1}{|\mathcal{I}|} \sum_{I_i \in \mathcal{I}} J(m_A \succ m_B \mid I_i) \) to denote the averaged pairwise preference, representing the model-level win rate between \( m_A \) and \( m_B \). We subsequently perform pairwise comparisons across all models and present the win rate matrix in \cref{fig:winrate} with GPT-4-Turbo as the judge to assess whether instance-level non-transitivity extends to the model-level.

\textbf{Hard Non-Transitivity at Model Level is Mild.}
Surprisingly, we detect no instances of hard non-transitivity (e.g., $m_a \succ m_b$, $m_b \succ m_c$, and $m_a \prec m_c$) at the model level, which we partially attribute to the effectiveness of calibration and randomness mitigation techniques. When implementing a more aggressive approach—where positions are randomly assigned for each evaluation, reducing the process to a single call—we observe occurrences of hard non-transitivity (see \cref{ablate_position_switching}). Nevertheless, model-level non-transitive cases remain notably rare. We hypothesize that this scarcity stems primarily from the low proportion of non-transitive evaluations when using GPT-4-Turbo as the judge. Given the sparsity of non-transitive comparisons, their aggregated effect is likely overwhelmed by the predominance of transitive evaluations, thus preventing the emergence of observable non-transitivity at the model level.

Despite this, notable instances of soft non-transitivity remain evident, leading to inconsistent ranking as shown by an example in Figure~\ref{fig:winrate}. Specifically, while GPT-4-Turbo achieves a win rate of 0.50 against GPT-4o, and GPT-4o wins against Claude-3-Opus with a rate of 0.68, transitivity would predict a win rate of 0.68 for GPT-4-Turbo against Claude-3-Opus. However, the observed rate of 0.72 reveals a subtle violation of transitivity at the model level.

\textbf{Limitations of the Baseline-Fixed Framework.} We further quantify the sensitivity of baseline-fixed frameworks. For each participating model $m$, we apply the rating function $\mathcal{R}_m(\cdot)$ to generate rankings, resulting 20 distinct ranking lists. We find that only 20\% of models maintain consistent rank positions across all rankings. Moreover, when comparing any pair of ranking lists, only 61\% of models preserve their rank positions on average. These findings demonstrate that rankings are highly sensitive to the choice
of baseline, indicating that baseline-fixed frameworks produce inconsistent and unreliable model evaluations.

\textbf{Influence of Model Performance Difference.} We further investigate the relationship between non-transitivity and the performance gap among model triplets within all participating models. For each triplet, we define the x-axis as the win rate difference between models \(m_A\) and \(m_B\) from the AlpacaEval leaderboard and the y-axis as the difference between \(m_B\) and \(m_C\). The computed PNT and SNTD values, visualized in \cref{fig:heatmap}, demonstrate that non-transitivity intensifies as the win rate differences between both model pairs decrease. Both metrics peak near the origin, indicating that non-transitivity is most pronounced when comparing models of similar capabilities (See \cref{appendix:heatmap} for implementation details).

\subsection{Non-Transitivity
is Jointly Influenced by Position Bias and Judge's Inherent Reasoning Abilities} 
\label{judge_capability}

\textbf{Position Bias in Judge Preferences.} During the evaluation, we observe that both judges exhibit position bias. Specifically, when evaluating two models on a given instruction, we define a preference as consistent if the judge's preference maintains its relationship to 0.5 (either consistently above or below) with position switching. We report the proportion of consistent preferences in each scenario, using GPT-4-Turbo and GPT-3.5-Turbo as judges (Figure \ref{position_bias_two_judge}).

In all scenarios except \textbf{MM}, both judges show the highest preference consistency when comparing \(m_A\) and \(m_C\), attributable to the substantial performance gap. A potential explanation is that AlpacaEval may have limited discriminative ability when evaluating models with similar capabilities, meaning the presumed performance gap does not hold. Moreover, GPT-3.5-Turbo shows a markedly lower preference consistency than GPT-4-Turbo, indicating that its evaluations are primarily driven by position bias rather than comparing output qualities.

\textbf{Factors of Non-Transitivity.} We further categorize instructions into two groups: ambiguous and consistent. An instruction is considered consistent only when the preferences between \((m_A, m_B)\), \((m_B, m_C)\), and \((m_A, m_C)\) are all consistent, implying that all comparisons are not influenced by position bias. Otherwise, the instruction is categorized as ambiguous, as at least one of the comparisons is affected by position bias. We report the proportion of non-transitive cases in Figure \ref{position_bias_trans}. We find that ambiguous instruction exhibits significantly higher non-transitivity rates compared to consistent instructions, suggesting position bias is indeed a contributing factor. Furthermore, when using GPT-3.5-Turbo as the judge, the proportion of ambiguous instructions exceeds 96\%, validating that it exhibits a much stronger position bias than GPT-4-Turbo.

Interestingly, we find non-transitivity still occurs within consistent instructions, with GPT-4-Turbo serving as the judge, indicating that position bias is not the sole cause of non-transitivity. Therefore, we argue that non-transitivity arises from two primary factors. The first is the inherent reasoning capability of the model, which is non-transitive due to the judge's latent comparison criteria. When the quality of the outputs is similar, the judge may display preferences akin to a rock-paper-scissors dynamic. The second factor is the position bias, which affects the judge's preferences. These two factors interact and compound the occurrence of non-transitivity.

\textbf{Stronger Position Bias Increases Non-Transitivity.} To investigate the impact of position bias, we introduce Position Difference (PD). Given an instruction $I_i$ and a model triplet $(m_A, m_B, m_C)$, we define this measure as $\text{PD}(m_A, m_B, I_i) + \text{PD}(m_B, m_C, I_i) + \text{PD}(m_A, m_C, I_i)$, ranging from 0 to 3, where $\text{PD}(m_A, m_B, I_i)$ is defined as $\left|\mathbb{E}[\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)] - \mathbb{E}[\phi(o_{B}^{(i)}, o_{A}^{(i)} \mid m_{\mathrm{J}}, I_i)] \right|$. Using GPT-4-Turbo as the judge, we evaluate all triplet permutations and partition PD values into bins. As shown in Figure \ref{bin_plot}-Left, the proportion of non-transitive cases increases with PD, demonstrating a strong positive correlation.

\textbf{Usefulness of Position Switching.}
Instead of using position switching, we repeat the experiment by randomly assigning the positions of the outputs in the prompt (\cref{position_bias_trans}). Since all preferences in the consistent instruction are consistent, the proportion of non-transitive cases remains unchanged. However, for ambiguous instructions, we observe divergent effects: GPT-4-Turbo exhibits a significant increase in non-transitivity, while GPT-3.5 shows a slight decrease. 

The distributions of judge preference (see Appendix \ref{subsec:distribution}) show distinct evaluation patterns between judges. When mitigating GPT-3.5-Turbo's position bias through position switching, the model tends to generate more uncertain outcomes (averaged preference \(\approx\) 0.5). In contrast, GPT-4-Turbo exhibits different characteristics: while position switching occasionally introduces uncertainty, its debiased preferences generally maintain clear output distinctions. This finding suggests that position switching can reduce non-transitivity for stronger judges that are less affected by position bias, with reductions ranging from 17\% to 44\%. However, for weaker judges that are more susceptible to position bias, it may have the opposite effect.

\textbf{Prompting Strategies to Mitigate Non-transitivity.}
We explore various prompting strategies to address non-transitivity in model judgments. Our findings indicate that providing judges with a structured evaluation checklist \cite{cook2024tickingboxesgeneratedchecklists} marginally reduces non-transitive cases. Notably, while incorporating Chain-of-Thought reasoning \cite{NEURIPS2022_9d560961} reduces position bias, experimental results indicate that it increases the occurrence of non-transitive preferences. Furthermore, allowing the judge to declare ties not only amplifies position bias but also further increases the likelihood of non-transitivity. See \cref{subsec:prompting} for detailed results.

\section{Results of Tournament-Based Ranking}
\begin{table}[tbp]
\centering
\caption{Correlation comparison between the round-robin-based framework and AlpacaEval, with and without length control (LC).}
\vspace{2mm}
\renewcommand\arraystretch{1.5}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|ccc|ccc}
\hline
\multicolumn{1}{c|}{\textbf{Method}} & \multicolumn{3}{c|}{\textit{Spearman Correlation}}                                         & \multicolumn{3}{c}{\textit{Kendall Correlation}}                                   \\
                                    & \textbf{w/o. LC} & \textbf{w. LC} & \textbf{$\Delta$}                        & \textbf{w/o.LC} & \textbf{w. LC} & \textbf{$\Delta$}                       \\ \hline
\rowcolor[HTML]{EFEFEF} \textit{AlpacaEval 2.0}                        & \textit{81.4\%}         & \textit{95.0\%}           & {\color[HTML]{34A853} \textit{+13.6\%}}  & \textit{63.2\%}            & \textit{82.1\%}           & {\color[HTML]{34A853} \textit{+18.9\%}} \\
\rowcolor[HTML]{EFEFEF} 
\textbf{Round-Robin}             & \textit{85.4\%}    & \textit{96.4\%}  & {\color[HTML]{34A853} \textit{+10.0\%}} & \textit{68.4\%}   & \textit{86.3\%}  & {\color[HTML]{34A853} \textit{+17.9\%}} \\
\textbf{$\Delta$} & {\color[HTML]{34A853} \textbf{+4.0\%}}    & {\color[HTML]{34A853} \textbf{+1.4\%}}  &   & {\color[HTML]{34A853} \textbf{+5.2\%}}   & {\color[HTML]{34A853} \textbf{+4.2\%}}  &  \\ \hline
\end{tabular}%
}
\label{RRBT_cor}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.4\columnwidth} 
        \centering
        \includegraphics[width=\linewidth]{imgs/overall_proportions_per_bin.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.58\columnwidth} 
        \centering
        \includegraphics[width=1.0\linewidth]{imgs/correlation_comparison.pdf}
    \end{subfigure}
    \caption{(Left) Non-transitivity strongly correlates with position difference. (Right) \textbf{Both round-robin and \textsc{Swim} tournaments achieve nearly identical performance, consistently outperforming AlpacaEval in all cases.} We compare the performance between tournament-based ranking and AlpacaEval leaderboard across different numbers of participating models. For each model count, we randomly sample models and conduct 2000 trials, reporting the mean correlation with a 95\% confidence interval.}
    \label{bin_plot}
\end{figure}
We conduct a round-robin tournament to obtain pairwise comparisons and apply the Bradley-Terry model to compute ratings, which are then converted to Elo scores. The resulting Elo scores and rankings for all 20 evaluated models are presented in Table \ref{tab:models_scores} in the Appendix.

To assess the effectiveness of our framework, we consider the human preference ranking from the Chatbot Arena as the reference. We compute the Spearman and Kendall correlations between our round-robin-based ranking and the Chatbot Arena. We also compare these correlations with those between the AlpacaEval and the Chatbot Arena. As shown in Table \ref{RRBT_cor}, our method achieves higher correlations, with a 4\% increase in Spearman correlation and a 5.2\% increase in Kendall correlation.

\textbf{Length-Controlled Winrate.} To mitigate verbosity bias and ensure a fair comparison, we adopt the generalized linear model with the same weights as Length-Controlled AlpacaEval \cite{dubois2024lengthcontrolledalpacaevalsimpleway} to derive length-controlled preferences. Using these preferences, we compute the length-controlled Bradley-Terry coefficients, which are then converted to length-controlled Elo scores. Table \ref{RRBT_cor} shows that our length-controlled round-robin ranking further improves correlations, with a 1.4\% increase in Spearman correlation and a 4.2\% increase in Kendall correlation compared to length-controlled AlpacaEval.

\textbf{Performance of \textsc{Swim}.} We demonstrate that both round-robin-based ranking and \textsc{Swim}-based ranking outperform AlpacaEval, as shown in Figure \ref{bin_plot}-Right. We do not compare performance under length control, as the generalized linear model is an empirical approach that may be less interpretable, potentially affecting fairness.

\section{Limitations and Future Work}
Our study has several limitations. While AlpacaEval provides diverse instructions, it may not fully capture real-world open-ended tasks, necessitating validation of our method across broader domains. Additionally, extending our findings to judge models beyond GPT-4-Turbo and GPT-3.5-Turbo is an important direction for future work. Furthermore, while our benchmark relies on human rankings from Chatbot Arena, inherent human biases \cite{chen-etal-2024-humans} may introduce non-transitivity in human preferences, fundamentally limiting the achievable alignment between automated and human evaluations.

Finally, our focus on pairwise comparisons leaves open questions about non-transitivity in pointwise evaluations. While pointwise methods inherently avoid position bias caused by output ordering, converting these scores to pairwise comparison (A > B if score(A) > score(B)) may introduce new forms of non-transitivity, depending on the granularity and consistency of rating criteria. Future work should investigate whether such conversions preserve transitivity and identify conditions that modulate cyclic preferences.

\section{Conclusion}
In this paper, we comprehensively study the impact of non-transitivity in the current LLM-based framework with pairwise settings, filling a gap in this area of research. Our findings show that non-transitivity can be observed at the instruction level during judgment and is related to the reasoning capability of the judge. The aggregation of instruction-level non-transitivity further leads to model-level non-transitivity, revealing the limitations of the baseline-fixed framework, as the rankings in this setting depend on the choice of the baseline model. Our analysis also demonstrates that position bias is a key factor in non-transitivity, with systematic position switching proving more effective than random assignment in reducing non-transitivity for stronger judges. 

To address the above, we propose a baseline-free framework utilizing round-robin tournaments with Bradley-Terry model, which captures non-transitivity patterns and demonstrates better alignment with human. Recognizing the computational constraints of round-robin tournaments, which require $\mathcal{O}(nm^2)$ instruction-level comparisons for ranking $m$ models across $n$ instructions, we propose \textsc{Swim} tournaments. This approach achieves $\mathcal{O}(nm\log m )$ complexity through dynamic matching, substantially reducing computational cost while maintaining nearly identical performance.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


\bibliography{paper}
\bibliographystyle{paper}


\newpage
\appendix
\onecolumn
\section{LLM Details.}

In this section, we provide detailed information about all models participating in the ranking evaluation for our experiments.

\subsection{Participating LLMs.} 
\label{llm_details}
The experimental model set consists of 20 LLMs encompassing a range of top proprietary models, large open-source models, and small open-source models. All models are concurrently presented on the AlpacaEval leaderboard and the Fully Style-Controlled Chatbot Arena (2024/09/15). The AlpacaEval leaderboard supplies pre-generated outputs for each model on the AlpacaEval dataset, allowing us to avoid the computational costs associated with output generation and focus solely on the costs involved in the evaluation process. The Fully Style-Controlled Chatbot Arena provides human preference rankings, which we use as a reference for calculating the agreement. A detailed list of participating LLMs is presented below:

\begin{itemize}
    \item \textbf{Proprietary models} includes four OpenAI models: \texttt{gpt-4-1106-preview}, \texttt{gpt-4o-2024-05-13}, \texttt{gpt4\_0314}, \texttt{gpt-4-turbo-2024-04-09} \cite{openai2024gpt4technicalreport}; three Anthropic models: \texttt{claude-2}, \texttt{claude-3-opus-20240229}, \texttt{claude-3-sonnet-20240229} \cite{claude2, anthropic2024claude3}; two Mistral models: \texttt{mistral-large-2402}, \texttt{mistral-medium} \cite{jiang2023mistral}; one Google model: \texttt{gemini-pro} \cite{geminiteam2023gemini}; and one Yi model: \texttt{yi-large-preview} \cite{yi_large_launch_2024}.
    
    \item \textbf{Large open-source models} includes \texttt{Yi-34B-Chat} \cite{ai2024yiopenfoundationmodels}, \texttt{Llama-3.1-405B-Instruct-Turbo} \cite{llama31}, \texttt{Llama-3-70B-Instruct} \cite{llama3}, \texttt{Qwen1.5-72B-Chat} \cite{qwen1.5}, \texttt{wizardlm-70b} \cite{xu2023wizardlmempoweringlargelanguage}.
    
    \item \textbf{Small open-source models} includes \texttt{Meta-Llama-3-8B-Instruct} \cite{llama3}, \texttt{vicuna-13b} \cite{vicuna2023}, \texttt{Starling-LM-7B-alpha} \cite{zhu2024starlingb}, \texttt{Mistral-7B-Instruct-v0.2} \cite{jiang2023mistral}.
\end{itemize}

\subsection{Selection of Representative Model Triplets across Scenarios.} 
\label{scenarios_details}
For each scenario, we select representative model triplets based on the win rates of participating models (shown in parentheses) from the AlpacaEval leaderboard:
\begin{enumerate}
    \item \textbf{LL}: \textsc{GPT-4o-2024-05-13} (51.3\%) as $m_A$, \textsc{Qwen1.5-72B-Chat} (26.5\%) as $m_B$, and \textsc{Mistral-7B-Instruct-v0.2} (14.7\%) as $m_C$.

    \item \textbf{LM}: \textsc{GPT-4o-2024-05-13} (51.3\%) as $m_A$, \textsc{Qwen1.5-72B-Chat} (26.5\%) as $m_B$, and \textsc{Claude-3-Sonnet-20240229} (25.6\%) as $m_C$.

    \item \textbf{ML}: \textsc{Yi-34B-Chat} (29.7\%) as $m_A$, \textsc{Qwen1.5-72B-Chat} (26.5\%) as $m_B$, and \textsc{Mistral-7B-Instruct-v0.2} (14.7\%) as $m_C$.

    \item \textbf{MM}: \textsc{Qwen1.5-72B-Chat} (26.5\%) as $m_A$, \textsc{Claude-3-Sonnet-20240229} (25.6\%) as $m_B$, and \textsc{GPT-4-0314} (22.1\%) as $m_C$.
\end{enumerate}
\section{Non-Transitivity in Preference}
\subsection{Conditions for Non-Transitivity}
\label{subsec:non_trans_condition}
In this section, we define the conditions under which non-transitivity arises in pairwise model comparisons. Consider a triplet of models, \( (m_A, m_B, m_C) \), and the corresponding pairwise comparisons on instruction $I_i$: 
\[ J(m_A \succ m_B \mid I_i), \; J(m_B \succ m_C \mid I_i), \; J(m_A \succ m_C \mid I_i) \]
where \( J(m_x \succ m_y \mid I_i) \) denotes the preference of the judge that model \( m_x \) outperforms model \( m_y \) under instruction \( I_i \).  

Non-transitivity occurs if the results of these comparisons form any of the following patterns:  
\begin{itemize}
    \item \( m_A \succ m_B, \; m_B \succ m_C, \; m_A \sim m_C \)
    \item \( m_A \succ m_B, \; m_B \succ m_C, \; m_A \prec m_C \)
    \item \( m_A \succ m_B, \; m_B \sim m_C, \; m_A \sim m_C \)
    \item \( m_A \succ m_B, \; m_B \sim m_C, \; m_A \prec m_C \)
    \item \( m_A \sim m_B, \; m_B \succ m_C, \; m_A \sim m_C \)
    \item \( m_A \sim m_B, \; m_B \succ m_C, \; m_A \prec m_C \)
    \item \( m_A \sim m_B, \; m_B \sim m_C, \; m_A \succ m_C \)
    \item \( m_A \sim m_B, \; m_B \sim m_C, \; m_A \prec m_C \)
    \item \( m_A \sim m_B, \; m_B \prec m_C, \; m_A \succ m_C \)
    \item \( m_A \sim m_B, \; m_B \prec m_C, \; m_A \sim m_C \)
    \item \( m_A \prec m_B, \; m_B \succ m_C, \; m_A \succ m_C \)
    \item \( m_A \prec m_B, \; m_B \succ m_C, \; m_A \sim m_C \)
    \item \( m_A \prec m_B, \; m_B \prec m_C, \; m_A \succ m_C \)
    \item \( m_A \prec m_B, \; m_B \prec m_C, \; m_A \sim m_C \)
\end{itemize}
where \( \succ \) means the left side wins against the right, \( \prec \) means the left side loses to the right, and \( \sim \) represents a tie between the two sides.

\textbf{Threshold Setting.}  
In practice, given the continuous nature of probability estimates, ties where $J(m_x \succ m_y \mid I_i) = 0.5$ occur with negligible frequency. Therefore, we introduce the following thresholds to determine the outcome of pairwise comparisons:
\begin{enumerate}
    \item If \( 0.475 \leq J(m_x \succ m_y \mid I_i) \leq 0.525 \), the outcome is treated as a tie (\( \sim \)). 
    \item If \( J(m_x \succ m_y \mid I_i) > 0.525 \), the outcome is classified as a win for \( M_x \) (\( \succ \)). 
    \item If \( J(m_x \succ m_y \mid I_i) < 0.475 \), the outcome is classified as a loss for \( M_x \) (\( \prec \)). 
\end{enumerate}
Notably, even without threshold settings, the non-transitivity patterns observed across all four scenarios remain consistent with \cref{sec:non_transitive_preferences}, which is shown in \cref{appendix:proportion}.

\subsection{Results with Preferences without the Threshold of Ties}
\label{appendix:proportion}
\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{imgs/position_bias_combined_strict_tie.pdf}
\caption{Proportion of (non-)transitive instructions across all scenarios (\textbf{without the threshold of ties}), as evaluated by GPT-4-Turbo and GPT-3.5-Turbo. When evaluating model triplets with GPT-3.5-Turbo as judge, over 96\% of instructions exhibit position bias effects. In contrast, GPT-4-Turbo demonstrates substantially higher evaluation consistency. }
  \label{position_bias_trans_with_strict_tie}
\end{figure*}
\begin{table*}[htbp]
\centering
\caption{We measure non-transitivity (\textbf{without the threshold of ties}) in four scenarios, evaluated by GPT-4-Turbo and GPT-3.5-Turbo. \orangebg{Orange cells} indicate maximum PNT/SNTD values (highest non-transitivity); \bluebg{blue cells} indicate minimum PNT/SNTD values (highest transitivity). When using GPT-4-Turbo as the judge, more non-transitivity can be observed as evaluated model performance becomes more similar and  the highest non-transitivity occurs when the performances of all three models are similar; however, GPT-3.5-Turbo does not exhibit this pattern.}
\vspace{2mm}
\small
\begin{tabular}{>{\centering\arraybackslash}p{1.8cm} l c c | c c}
\toprule
\multirow{2}{*}{\textbf{Scenarios}}  & \multirow{2}{*}{\hspace{2.5cm}\textbf{Models}} & \multicolumn{2}{c}{\cellcolor{gray!25}\textit{GPT-4-Turbo}}  & \multicolumn{2}{c}{\cellcolor{gray!25}\textit{GPT-3.5-Turbo}} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
 &  & {PNT} & {SNTD} & {PNT} & {SNTD} \\
\midrule
\textbf{LL} & \(m_A = \texttt{gpt-4o-2024-05-13}\) & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{1.12} & \multirow{3}{*}{} \\
\( m_A \gg m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & \cellcolor{blue!10}{{\textbf{0.25}}} & \cellcolor{blue!10}{{\textbf{0.1121}}} & & \cellcolor{orange!10}{{\textbf{0.2654}}} \\
\( m_B \gg m_C \)   & \(m_C = \texttt{Mistral-7B-Instruct-v0.2}\) & & & & \\
\midrule
\textbf{LM} & \(m_A = \texttt{gpt-4o-2024-05-13}\) & \multirow{3}{*}{1.24} & \multirow{3}{*}{0.1336} & \multirow{3}{*}{} & \multirow{3}{*}{} \\
\( m_A \gg m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & & &\cellcolor{blue!10}{{\textbf{0.25}}} & \cellcolor{blue!10}{{\textbf{0.2586}}} \\
\( m_B \approx m_C \)   & \(m_C = \texttt{claude-3-sonnet-20240229}\) & & & & \\
\midrule
\textbf{ML} & \(m_A = \texttt{Yi-34B-Chat}\) & \multirow{3}{*}{} & \multirow{3}{*}{0.1215} & \multirow{3}{*}{} & \multirow{3}{*}{0.2625} \\
\( m_A \approx m_B \)   & \(m_B = \texttt{Qwen1.5-72B-Chat}\) & 0.99 & & {{{1.86}}} & \\
\( m_B \gg m_C \)   & \(m_C = \texttt{Mistral-7B-Instruct-v0.2}\) & & & & \\
\midrule
\textbf{MM} &  \(m_A = \texttt{Qwen1.5-72B-Chat}\) & \multirow{3}{*}{} & \multirow{3}{*}{\textbf{0.1431}} & \multirow{3}{*}{} & \multirow{3}{*}{0.2629} \\
\( m_A \approx m_B \)   & \(m_B = \texttt{claude-3-sonnet-20240229}\) & \cellcolor{orange!10}{{\textbf{2.86}}}
 & \cellcolor{orange!10}{\textbf{0.1431}} & \cellcolor{orange!10}{{\textbf{1.99}}} & \\
\( m_B \approx m_C \)   & \(m_C = \texttt{gpt-4-0314}\) & & & & \\
\bottomrule
\end{tabular}
\label{4senarios_strict_tie}
\end{table*}
We observe the same pattern from the table \ref{4senarios_strict_tie} as in the main text, which is with the threshold for ties. When GPT-4-Turbo serves as the judge, both PNT and SNTD increase as the performance gap between any pair of models, \( (m_A, m_B) \) or \( (m_B, m_C) \), decreases. In cases where all three models exhibit similar performance, such as in scenario \textbf{MM}, the incidence of non-transitivity rises significantly. We attribute this to the increased uncertainty judges face when assessing quality differences between similar outputs. When the comparisons between \( m_A \) and \( m_B \), \( m_B \) and \( m_C \), and \( m_A \) and \( m_C \) are all uncertain, non-transitivity reaches its highest level. Replicating our evaluation with GPT-3.5-Turbo as judge reveals an intriguing pattern: while the PNT remains minimal across scenarios, the consistently high SNTD values indicate substantial non-transitivity. This observation motivates us to define the tie threshold, as ties can serve as an indicator of model uncertainty.

To explain the low number of hard non-transitive cases when using GPT-3.5-Turbo as the judge with position switching in Figure \ref{4senarios_strict_tie}, we hypothesize that GPT-3.5-Turbo is also affected by other biases, such as verbosity bias \cite{saito2023verbositybiaspreferencelabeling} and token bias \cite{alzahrani-etal-2024-benchmarks}. Since GPT-3.5-Turbo struggles to accurately assess the quality of outputs, these combined biases influence the judge's preferences. As a result, even though position switching mitigates the position bias, the averaged preference is still not determined by the actual quality of the outputs but rather by other fixed biases in the prompt, leading to transitive preferences. This observation also motivates us to define the threshold, as it can be used to reduce the impact of other biases.


\subsection{Derivation of Expected Win Rate}
\label{appendix:expected_winrate}
The Bradley-Terry model \cite{BT} provides a probabilistic framework for estimating pairwise win rates based on these latent quality scores. Specifically, the probability that model \( m_A \) outperforms model \( m_B \) on instruction \( I_i \) is given by:
\begin{equation}
\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i) = \frac{1}{1 + e^{-(\gamma_{A}^{(i)} - \gamma_{B}^{(i)})}} = \sigma(s_{AB}^{(i)}),
\end{equation}
where we denote \(s_{AB}^{(i)} = \gamma_{A}^{(i)} - \gamma_{B}^{(i)}\) as the quality gap. Conversely, this quality gap can be calculated from empirical observations $\phi$ as:
\begin{equation}
\hat{\phi}(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i) = \frac{1}{1 + e^{s_{AC}^{(i)} - s_{BC}^{(i)}}},
\end{equation}
\begin{equation}
s_{AB}^{(i)} = \ln\left( \frac{\phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)}{1 - \phi(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i)} \right).
\end{equation}
Based on that, we can estimate the expected win rate $\hat{\phi}$ under transitivity between any two models from a triplet \( (m_A, m_B, m_C) \) by utilizing the observed win rates between the other two pairs. For instance, to estimate the win rate for model \(m_A\) beating model \(m_B\) on instruction \(I_i\) without direct observations, we assume that the observed win rates for the remaining pairs reflect true performance differences and compute the estimated win rate as:
\begin{equation}
\hat{\phi}(o_{A}^{(i)}, o_{B}^{(i)} \mid m_{\mathrm{J}}, I_i) = \frac{1}{1 + e^{s_{AC}^{(i)} - s_{BC}^{(i)}}}.
\end{equation}
\subsection{Heatmap Implementation}
\label{appendix:heatmap}
In this experiment, we aim to investigate the relationship between non-transitivity and the performance gap between two models being compared. From the pool of 20 models, we generate all possible tuples \((m_A, m_B, m_C)\) by computing \(P(20,3) = \frac{20!}{17!} = 6,840\) permutations. For each tuple, we calculate the number of hard non-transitive cases and the degree of soft non-transitivity. The results are visualized as a 2D heatmap, where the x-axis represents the performance gap between model \(m_A\) and model \(m_B\), measured by their win-rate difference on AlpacaEval. Similarly, the y-axis represents the win-rate difference between model \(m_B\) and model \(m_C\). A positive win-rate difference indicates that the former model performs better, whereas a negative difference suggests that the latter outperforms the former.

According to the AlpacaEval leaderboard, \texttt{yi-large-preview} achieves the highest relative win rate of 57.5\%, while \texttt{vicuna-13b} records the lowest at 5.8\%. This establishes a win rate differential range of [-51.7\%, +51.7\%], which we partition into a 35 × 35 grid. For each grid cell, we compute the mean number of PNT and SNTD across all possible model triplet permutations. We apply a Gaussian filter (\(\sigma = 1\)) to reduce noise in the resulting data, and then perform quadratic interpolation to generate the final heatmap.

\subsection{Preference Distributions of Judge}
\label{subsec:distribution}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_0.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_0_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_1.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_1_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_2.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_2_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_3.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt35_3_avg.pdf}\\[0em]
\caption{Preference distribution of GPT-3.5-Turbo across scenarios (from top to bottom: \textbf{LL}, \textbf{LM}, \textbf{LM}, \textbf{MM}). (Left) Distribution with random assignment. (Right) Distribution with position switching.}
\label{gpt35_distribution}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_0.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_0_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_1.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_1_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_2.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_2_avg.pdf}\\[0em]
    
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_3.pdf}
    \includegraphics[width=5cm,height=5cm]{imgs/distributions/distribution_gpt4_3_avg.pdf}\\[0em]
\caption{Preference distribution of GPT-4-Turbo across scenarios (from top to bottom: \textbf{LL}, \textbf{LM}, \textbf{LM}, \textbf{MM}). (Left) Distribution with random assignment. (Right) Distribution with position switching.}
\label{gpt4_distribution}
\end{figure}

All scenario assumes that \(m_A\) outperforms \(m_B\), \(m_B\) outperforms \(m_C\), and \(m_A\) outperforms \(m_C\). Consequently, we expect the judge's preference distribution to exhibit a heavy-tailed pattern concentrated around 1. In scenario \textbf{LL}, because the models differ significantly in performance, the judge should tend to select the superior output. However, under the random assignment setting, GPT-3.5-Turbo exhibits a U-shaped distribution across all scenarios (\cref{gpt35_distribution}), validating that it fails to distinguish response quality and is instead primarily driven by position bias. As a result, after applying position switching, its preference distribution changes significantly, forming a sharp peak at 0.5 while rapidly decaying away from it, leading to a large number of ties.

By contrast, GPT-4-Turbo's distributions vary across scenarios (\cref{gpt4_distribution}). In scenario \textbf{LL}, where \(m_A, m_B,\) and \(m_C\) have large performance gaps, the distribution precisely follows a heavy-tailed pattern concentrated at 1, indicating that when GPT-4-Turbo perceives a substantial quality difference, it strongly favors the superior response. In \textbf{LM} and \textbf{ML} scenarios, where one model pair has a clear performance gap while the other is closer in quality, increased uncertainty arises when evaluating the latter, causing the tail to shift towards 0. In \textbf{MM}, GPT-4-Turbo also exhibits a U-shaped distribution. However, unlike GPT-3.5-Turbo, it retains 38\% of its preferences distributed across the full range from 0 to 1, demonstrating that its preferences are guided by reasoning rather than solely by position bias. Thus, position switching smooths its preference distribution while preserving a considerable proportion of decisive judgments (non-ties), reflecting that GPT-4-Turbo still distinguishes quality differences

This also explains why position switching is least effective in Scenario \textbf{MM}, reducing non-transitivity by only 17\%.


\newpage
\section{Additional Experimental Results}
\subsection{Full Pairwise Comparison Matrix (Position Switching and Two API
Calls per Order)}
\label{detailed_matrix}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.65\textwidth]{imgs/winrate_full.pdf}
  \caption{Win rate matrix for 20 models using default settings (Position Switching and Two API Calls per Order).}
  \label{full_winrate}
\end{figure*}
\subsection{Position
Switching and Multiple API Calls Reduce the Occurrence of Non-transitivity at the Model Level.}
\label{ablate_position_switching}
We hypothesize that the absence of observed hard non-transitivity in \cref{full_winrate}
is due to the use of position switching and two API calls per order, which help ensure the consistency of judgments. To validate this hypothesis, we adopt a more aggressive approach by randomly assigning positions for each evaluation, reducing the process to a single API call to mitigate position bias. However, since the preference between each model pair for a given instruction is determined by log probability rather than a binary label (0 or 1), we argue that random assignment may not fully eliminate position bias. As a result, this setup is expected to perform worse than position switching, leading to lower judgment consistency compared to the original setting.

To reduce computational costs, the judge's new preference can be interpreted as a random sample from the four API calls made in the original experiment. In other words, in this ablation experiment, the judge's preference is equivalent to selecting one random sample from the pre-computed preferences in \cref{sec:aggregation}.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.65\textwidth]{imgs/winrate_ablation.pdf}
\caption{Win rate matrix for 20 models using ablated settings (random assignment and a single API call per order). Hard non-transitivity is observed compared to Figure \ref{fig:winrate}. For instance, \texttt{Qwen1.5-72B-Chat} outperforms \texttt{Yi-34B-Chat}, and \texttt{Yi-34B-Chat} outperforms \texttt{claude-3-opus-20240229}. However, \texttt{claude-3-opus-20240229} outperforms \texttt{Qwen1.5-72B-Chat}, highlighting the presence of non-transitive relationships among the models.}
  \label{fig:winrate_ablation}
\end{figure*}
Figure \ref{fig:winrate_ablation} presents the corresponding win rate matrix from this ablation. In contrast to Figure \ref{full_winrate}, we now observe the occurrence of a hard non-transitive case at the model level. Specifically, \(\texttt{Qwen1.5-72B-Chat}\) outperforms \(\texttt{Yi-34B-Chat}\), and \(\texttt{Yi-34B-Chat}\) outperforms \(\texttt{claude-3-opus-20240229}\). However, \(\texttt{claude-3-opus-20240229}\) outperforms \(\texttt{Qwen1.5-72B-Chat}\), thus exhibiting a clear case of non-transitivity.

To further verify that the observation of non-transitivity in the ablated setting is not merely due to randomness, we repeat this ablation experiment 50 times. We quantify the degree of soft non-transitivity in the win rate matrix in a manner similar to Equation \ref{SNTD}, but applied at the model level. Specifically, for a set of 20 models, we first compute all possible permutations of triples \((m_A, m_B, m_C)\). For each triplet, we sequentially select two pairs of models and extract their corresponding values from the win rate matrix as ground truth. We then calculate the expected win rate for the remaining model pair and measure the associated SNTD at the model level. Finally, we average the results across all permutations to assess the overall non-transitivity in the win-rate matrix.
\begin{table}[ht]
\caption{Comparison of the degree of soft non-transitivity between the original and random assignment settings. The values represent the mean SNTD, with the standard deviation reported for the random assignment setting based on 50 independent trials.}
\vspace{2mm}
\centering
\begin{tabular}{l c}
\toprule
\textbf{Experiment Setting} & \textbf{SNTD}  \\
\midrule
Position Switching and Two API Calls per Order         & \( 4.00 \times 10^{-4} \)        \\
Random Assignment and One API Call (50 times)          & \( \mathbf{(5.38 \pm 0.04) \times 10^{-4}} \) \\
\bottomrule
\end{tabular}
\label{table:non_transitivity}
\end{table}

As shown in Table \ref{table:non_transitivity}, the degree of non-transitivity in the ablated experiment is significantly higher than in the original experiment. This finding demonstrates that by employing position switching and multiple API calls, we can improve the consistency of the judge’s evaluations and thereby reduce the occurrence of non-transitivity at the model level.

\subsection{More Prompting Strategies}
\label{subsec:prompting}
We evaluate six prompting strategies in Scenario \textbf{MM} to encourage the judge to exhibit more transitive preferences from a prompting perspective (\(\approx\approx\)). The prompt templates are provided in \cref{prompt_template}.
\begin{enumerate}
    \item \textbf{Direct Comparison}: Standard binary choice comparison identical to our previous experimental setup, serving as the baseline.
    
    \item \textbf{CoT Comparison}: Requires the judge to output its reasoning through Chain-of-Thought \cite{NEURIPS2022_9d560961} before making a decision.
    
    \item \textbf{Direct Comparison with Checklist}: Provides a detailed evaluation checklist \cite{cook2024tickingboxesgeneratedchecklists} for the judgment without explicit reasoning.
    
    \item \textbf{CoT Comparison with Checklist}: Combines a detailed evaluation checklist with Chain-of-Thought reasoning before judgment.
    
    \item \textbf{CoT Comparison (Tie Allowed)}: Extends the binary choice to three options by introducing the possibility of ties, while maintaining the Chain-of-Thought reasoning process.
    
    \item \textbf{CoT Comparison with Checklist (Tie Allowed)}: Incorporates both the three-choice option and evaluation checklist while preserving Chain-of-Thought reasoning.
\end{enumerate}

\begin{table}[htbp]
\centering
\small
\caption{Comparison of different prompting strategies, judged by GPT-4-Turbo. \redbg{Red cells} indicate the lowest consistency (most affected by position bias); \greenbg{green cells} represent the highest consistency (least affected by position bias). \orangebg{Orange cells} denote the highest number of non-transitive cases (greatest non-transitivity), while \bluebg{blue cells} indicate the lowest number of non-transitive cases (greatest transitivity). The values in parentheses represent the number of non-transitive cases in consistent instructions (left) and ambiguous instructions (right).}
\vspace{2mm}
\label{tab:consistency_nontransitivity}
\begin{tabular}{l*{7}{c}}
\toprule
\textbf{Method} & \textbf{A vs B} & \textbf{B vs C} & \textbf{A vs C} & \textbf{\# of Consistent} & \textbf{\# of Non-trans.} & \textbf{\# of Non-trans.} \\
& \textbf{(consist.)} & \textbf{(consist.)} & \textbf{(consist.)} & \textbf{Instr.} & \textbf{(w. threshold)} & \textbf{(w/o. threshold)} \\
\midrule
Direct          & 473 & 496 & 476 & 217 & (1, 67) & \cellcolor{blue!10}{\textbf{(1, 22)}} \\
Direct w. Chk   & 478 & 506 & \cellcolor{red!10}{\textbf{440}} & 227 & \cellcolor{blue!10}{\textbf{(0, 64)}} & \cellcolor{blue!10}{\textbf{(0, 23)}} \\
CoT             & \cellcolor{green!10}{\textbf{572}} & \cellcolor{green!10}{\textbf{577}} & \cellcolor{green!10}{\textbf{560}} & \cellcolor{green!10}{\textbf{301}} & (1, 152) & (1, 46) \\
CoT w. Chk      & 548 & 571 & 535 & 268 & (5, 172) & (5, 47) \\
CoT w. Tie      & 474 & 496 & 493 & 210 & (5, 139) & (5, 87) \\
CoT w. Chk\&Tie & \cellcolor{red!10}{\textbf{466}} & \cellcolor{red!10}{\textbf{479}} & 456 & \cellcolor{red!10}{\textbf{181}} & \cellcolor{orange!10}{\textbf{(10, 183)}} & \cellcolor{orange!10}{\textbf{(10, 129)}} \\
\bottomrule
\end{tabular}
\end{table}
For the checklist-based method, we first use GPT-4-Turbo to generate a checklist—a set of YES/NO questions assessing different aspects of the given instruction. The corresponding prompt is provided in \cref{checklist_geneartion}.

As shown in \cref{tab:consistency_nontransitivity}, providing the judge with a checklist slightly reduces non-transitivity. This aligns with our earlier assertion that the judge’s latent comparison criteria are inherently non-transitive for closely matched models. While introducing explicit criteria helps guide the judge toward more transitive preferences, the effect remains limited, likely because the automatically generated checklists lack the granularity to capture subtle output differences.

Meanwhile, although Chain-of-Thought prompting reduces position bias and improves overall preference consistency, it increases non-transitivity for ambiguous instructions and can introduce additional non-transitive cases even in consistent instructions. Additionally, when combining CoT with a checklist, we observe more inconsistency, suggesting that CoT elicits the judge's latent reasoning criteria, which may conflict with the explicitly provided checklist. Furthermore, allowing the judge to declare ties increases non-transitivity, as the judge may opt for ties instead of identifying subtle differences between outputs.


\section{Soft Bradley-Terry Model Yields More Accurate Rankings}
\label{sec:hard_bt_vs_soft_bt}
We explored three methods for computing \( W_{i,j} \) in \cref{mle}. The first method, referred to as hard-BT, directly derives discrete win rates from the judge's continuous preferences. In this approach, if \( J(m_i \succ m_j \mid I_k) > 0.5 \), the outcome is counted as a win (1); if \( J(m_i \succ m_j \mid I_k) < 0.5 \), it is counted as a loss (0); and if \( J(m_i \succ m_j \mid I_k) = 0.5 \), it is considered a tie (0.5).  

The second method, rounded-BT, incorporates a threshold to refine the win/loss definition. Specifically, if \( J(m_i \succ m_j \mid I_k) > 0.525 \), it is considered a win (1); if \( J(m_i \succ m_j \mid I_k) < 0.475 \), it is considered a loss (0); and if \( J(m_i \succ m_j \mid I_k) \) falls within the range \([0.475, 0.525]\), it is treated as a tie (0.5).  

The final method, soft-BT, follows the formulation presented in the main text. Instead of discretizing preferences into fixed categories, it directly uses the judge's continuous preference scores to compute \( W_{i,j} \), allowing for a more nuanced representation of the relative strength between models:
\[
W_{i,j} = \sum_{I_k\in\mathcal{I}} J(m_i \succ m_j \mid I_k).
\]
We evaluate these methods by computing rankings from a round-robin tournament involving 20 models, using GPT-4-Turbo as the judge, and measuring their correlation with the Chatbot Arena rankings as metrics.
\begin{table}[ht]
\centering
\caption{Comparison between Round Robin based framework with Bradley-Terry model and AlpacaEval 2.0.}
\vspace{2mm}

\label{comparison_withBT}
\begin{tabular}{lccc}
\toprule
    &  RR + Soft-BT & RR + Hard-BT  & RR + Rounded-BT\\
\midrule
Spearman Correlation  & \textbf{85.4\%}  & 84.4\%  & 84.8\%  \\
Kendall Correlation           & \textbf{68.4\%}  & 66.3\% & 67.4\% \\
\bottomrule
\end{tabular}
\end{table}

\cref{comparison_withBT} shows that soft-BT produces the most aligned ranking, demonstrating its ability to better capture the relative strength of models from continuous preferences.

\newpage
\section{\textbf{S}wiss-\textbf{W}ise \textbf{I}terative \textbf{M}atchmaking tournaments}
\label{sec:swim}
\begin{algorithm}[h]
\caption{\textbf{S}wiss-\textbf{W}ise \textbf{I}terative \textbf{M}atchmaking (\textsc{Swim}) tournament}
\label{alg:ranking}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} $M$ unranked models, a dataset $\mathcal{I}$ and a judge model $M_J$.
   \STATE {\bfseries Output:} An ordered ranking of all $M$ models.
   \STATE $R \leftarrow$ empty set $\varnothing$ to store ranked models
   \STATE $U \leftarrow$ set of all $M$ models
   \STATE $X \leftarrow$ a random model from $U$
   \STATE $R \leftarrow R \cup \{X\}$, $U \leftarrow U \setminus \{X\}$
   \WHILE{$U \neq \varnothing$}
     \STATE $P \leftarrow$ a random model from $U$
     \STATE $U \leftarrow U \setminus \{P\}$
     \STATE $s \leftarrow |R|$, $c \leftarrow \lceil \max(\log_2(s), 1)\rceil$
     \STATE $X \leftarrow$ a random model from $R$
     \STATE $T \leftarrow R \setminus \{X\}$
     \FORALL{$I_i \in \mathcal{I}$}
       \STATE Compute $J(m_P \succ m_X \mid I_i)$
     \ENDFOR
     \STATE $\beta \leftarrow$ update BT coefficient for $R \cup \{P\}$
     \FOR{$j = 1$ to $c-1$}
       \STATE $O \leftarrow \arg\min_{O \in T}|\beta_{O}-\beta_{P}|$
       \STATE $T \leftarrow T \setminus \{O\}$
       \FORALL{$I_i \in \mathcal{I}$}
         \STATE Compute $J(m_P \succ m_O \mid I_i)$
       \ENDFOR
       \STATE $\beta \leftarrow$ update BT coefficient for $R \cup \{P\}$
     \ENDFOR
     \STATE $R \leftarrow R \cup \{P\}$
   \ENDWHILE
\end{algorithmic}
\end{algorithm}
\newpage
\section{ELO Scores}
\begin{table*}[ht]
  \centering
  \caption{Evaluation Results of LLMs in Fully Style-Controlled Chatbot Arena, Round-Robin Tournament and AlpacaEval.}
  \small
  \begin{tabular}{l c c@{\hspace{4em}}c cc}
    \toprule
    \multirow{2}{*}{\textbf{Model Names}} & \multirow{2}{*}{\textbf{FSC Arena Elo}} & \multicolumn{2}{c}{\textbf{Round-Robin + BT}} & \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6}
     &  & Elo & LC Elo & Win Rate & LC Win Rate \\
    \midrule
    \texttt{gpt-4o-2024-05-13}  & 1262 & 1325& 1227 & 51.3\%& 57.5\%\\
    \texttt{gpt-4-turbo-2024-04-09}  & 1241 & 1306& 1217 & 46.1\%& 55.0\%\\
    \texttt{gpt-4-1106-preview}  & 1234 & 1337& 1206& 50.0\%& 50.0\%\\
    \texttt{yi-large-preview}  & 1204 & 1377& 1205& 57.5\%& 51.9\%\\
    \texttt{claude-3-opus-20240229}  & 1238 & 1180& 1156& 29.1\%& 40.5\%\\
    \midrule
    \texttt{Llama-3.1-405B-Instruct-Turbo}  & 1250 & 1264& 1136& 39.1\%& 39.3\%\\
    \texttt{gpt4\_0314}  & 1200& 1137& 1117& 22.1\%& 35.3\%\\
    \texttt{claude-3-sonnet-20240229}  & 1197& 1152& 1110& 25.6\%& 34.9\%\\
    \texttt{Qwen1.5-72B-Chat}  & 1148& 1168& 1108& 26.5\%& 36.6\%\\
    \texttt{Llama-3-70B-Instruct} & 1193& 1210& 1093& 33.2\%& 34.4\%\\
    \midrule
    \texttt{mistral-large-2402} & 1158& 1110& 1090& 21.4\%& 32.7\%\\
    \texttt{claude-2} & 1144& 1043& 1060& 17.2\%& 28.2\%\\
    \texttt{mistral-medium} & 1141& 1109& 1059& 21.9\%& 28.6\%\\
    \texttt{Yi-34B-Chat} & 1100& 1169& 1026& 29.7\%& 27.2\%\\
    \texttt{gemini-pro} & 1132& 1074& 1020& 18.2\%& 24.4\%\\
    \midrule
    \texttt{Llama-3-8B-Instruct} & 1141& 1110& 988& 22.6\%& 22.9\%\\
    \texttt{wizardlm-70b} & 1106& 1036& 964& 14.4\%& 17.6\%\\
    \texttt{Mistral-7B-Instruct-v0.2} & 1067& 1019& 947& 14.7\%& 17.1\%\\
    \texttt{Starling-LM-7B-alpha} & 1083& 1021& 925& 14.2\%& 14.7\%\\
    \texttt{vicuna-13b} & 1060& 800& 800& 6.7\%& 10.5\%\\
    \bottomrule
  \end{tabular}
  \label{tab:models_scores}
\end{table*}

\begin{table}[ht]
  \centering
    \caption{Ranking of LLMs based on evaluation results from the Fully Style-Controlled Chatbot Arena, Round-Robin Tournament, and AlpacaEval. The numbers in parentheses indicate changes in model rankings after applying the length-controlled debiasing technique, where $\uparrow$ denotes an increase, $\downarrow$ denotes a decrease, and \textendash{} indicates no change in ranking.}
  \small
  \begin{tabular}{l c c@{\hspace{4em}}c cc}
  \small
    \multirow{2}{*}{\textbf{Model Names}} & \multirow{2}{*}{\textbf{FSC Arena Rank}}& \multicolumn{2}{c}{\textbf{Round-Robin + BT}} & \multicolumn{2}{c}{\textbf{AlpacaEval 2.0}} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6}
     &  & Rank& LC Rank& Rank& LC Rank\\
    \midrule
    \texttt{gpt-4o-2024-05-13}  & 1& 3& 1 (2 $\uparrow$)& 2& 1 (1 $\uparrow$)\\
    \texttt{gpt-4-turbo-2024-04-09}  & 3& 4& 2 (2 $\uparrow$)& 4& 2 (2 $\uparrow$)\\
    \texttt{gpt-4-1106-preview}  & 5& 2& 3 (1 $\downarrow$)& 3& 4 (1 $\downarrow$)\\
    \texttt{yi-large-preview}  & 6& 1& 4 (3 $\downarrow$)& 1& 3 (2 $\downarrow$)\\
    \texttt{claude-3-opus-20240229}  & 4& 7& 5 (2 $\uparrow$)& 8& 5 (3 $\uparrow$)\\
    \midrule
    \texttt{Llama-3.1-405B-Instruct-Turbo}  & 2& 5& 6 (1 $\downarrow$)& 5& 6 (1 $\downarrow$)\\
    \texttt{gpt4\_0314}  & 7& 11& 7 (4 $\uparrow$)& 12& 8 (4 $\uparrow$)\\
    \texttt{claude-3-sonnet-20240229}  & 8& 10& 8 (2 $\uparrow$)& 10& 9 (1 $\uparrow$)\\
    \texttt{Qwen1.5-72B-Chat}  & 11& 9& 9 (0 \textendash)& 9& 7 (2 $\uparrow$)\\
    \texttt{Llama-3-70B-Instruct} & 9& 6& 10 (4 $\downarrow$)& 6& 10 (4 $\downarrow$)\\
    \midrule
    \texttt{mistral-large-2402} & 10& 12& 11 (1 $\uparrow$)& 14& 11 (3 $\uparrow$)\\
    \texttt{claude-2} & 12& 16& 12 (4 $\uparrow$)& 16& 13 (3 $\uparrow$)\\
    \texttt{mistral-medium} & 13& 14& 13 (1 $\uparrow$)& 13& 12 (1 $\uparrow$)\\
    \texttt{Yi-34B-Chat} & 17& 8& 14 (6 $\downarrow$)& 7& 14 (7 $\downarrow$)\\
    \texttt{gemini-pro} & 15& 15& 15 (0 \textendash)& 15& 15 (0 \textendash)\\
    \midrule
    \texttt{Llama-3-8B-Instruct} & 14& 13& 16 (3 $\downarrow$)& 11& 16 (5 $\downarrow$)\\
    \texttt{wizardlm-70b} & 16& 17& 17 (0 \textendash)& 18& 17 (1 $\uparrow$)\\
    \texttt{Mistral-7B-Instruct-v0.2} & 19& 19& 18 (1 $\uparrow$)& 17& 18 (1 $\downarrow$)\\
    \texttt{Starling-LM-7B-alpha} & 18& 18& 19 (1 $\downarrow$)& 19& 19 (0 \textendash)\\
    \texttt{vicuna-13b} & 20& 20& 20 (0 \textendash)& 20& 20 (0 \textendash)\\
    \bottomrule
  \end{tabular}
  \label{tab:models_ranks}
\end{table}

\newpage
\section{Prompt Template.}
\subsection{Judge Prompts}
\label{prompt_template}
\textbf{Direct Comparison - Identical to AlpacaEval 2.0 \cite{alpaca_eval}}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.

## Instruction

{
    "instruction": """{instruction}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M.

## Best Model Identifier
\end{lstlisting}

\textbf{Direct Comparison with Checklist}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction and the corresponding criteria. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I will provide you with prompts given to these models and their corresponding outputs. I will also provide one specific evaluation checklist which contains a list of specific criteria that a good output should fulfill. Your task is to assess these responses to see whether they satisfy the requirements of the checklist and select the model that produces the best output from a human perspective based on the provided checklist.

## Instruction

{
    "instruction": """{instruction}""",
}

## Checklist
Here is the checklist that contains the conditions specified in the question for a good output. The more requirements an output meets, the better it is considered.

{
    checklist: """{checklist}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output based on the checklist. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M.

## Best Model Identifier
\end{lstlisting}

\textbf{CoT Comparison - Identical to AlpacaEval 2.0 \cite{alpaca_eval}}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.

## Instruction

{
    "instruction": """{instruction}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by first providing a concise explanation and then end your answer by providing the model identifier of the best output. We will use the last character of your output `output[-1]` as the name of the best model, so make sure you finish with the token of the model identifiers and nothing else: `m` or `M` (no quotes, no dots, no backticks, no new lines, ...). For example:

### Concise explanation
...some text...

### Which is best, m or M?
M

Now is your turn.

## Your answer: "Concise explanation" followed by "Which is best, m or M?"
\end{lstlisting}

\textbf{CoT Comparison (Tie Allowed)}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I'll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective. If you determine that both outputs are of equal quality or are unable to decide which one is better, you should indicate a tie by providing the identifier `D`.

## Instruction

{
    "instruction": """{instruction}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by first providing a concise explanation and then end your answer by providing the model identifier of the best output. If you determine that both outputs are of equal quality or cannot decide which one is better, indicate a tie by using the identifier `D`. We will use the last character of your output `output[-1]` as the name of the best model, so make sure you finish with the token of the model identifiers and nothing else: `m`, `M`  or `D` (no quotes, no dots, no backticks, no new lines, ...). For example:

### Concise explanation
...some text...

### Which is best, m, M or D?
M

Now is your turn.

## Your answer: "Concise explanation" followed by "Which is best, m, M or D?"
\end{lstlisting}

\textbf{CoT Comparison with Checklist}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction and the corresponding criteria. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I will provide you with prompts given to these models and their corresponding outputs. I will also provide one specific evaluation checklist which contains a list of specific criteria that a good output should fulfill. Your task is to assess these responses to see whether they satisfy the requirements of the checklist and select the model that produces the best output from a human perspective based on the provided checklist.

## Instruction

{
    "instruction": """{instruction}""",
}

## Checklist
Here is the checklist that contains the conditions specified in the question for a good output. The more requirements an output meets, the better it is considered.

{
    checklist: """{checklist}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output based on the checklist. Answer by first providing a concise explanation and then end your answer by providing the model identifier of the best output. We will use the last character of your output `output[-1]` as the name of the best model, so make sure you finish with the token of the model identifiers and nothing else: `m` or `M` (no quotes, no dots, no backticks, no new lines, ...). For example:

### Concise explanation
...some text...

### Which is best, m or M?
M

Now is your turn.

## Your answer: "Concise explanation" followed by "Which is best, m or M?"
\end{lstlisting}

\textbf{CoT Comparison with Checklist (Tie Allowed)}
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction and the corresponding criteria. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.

(*@\textbf{[User Part]}@*)
I require a leaderboard for various large language models. I will provide you with prompts given to these models and their corresponding outputs. I will also provide one specific evaluation checklist which contains a list of specific criteria that a good output should fulfill. Your task is to assess these outputs to see whether they satisfy the requirements of the checklist and select the model that produces the best output from a human perspective based on the provided checklist. If you determine that both outputs are of equal quality or are unable to decide which one is better, you should indicate a tie by providing the identifier `D`.

## Instruction

{
    "instruction": """{instruction}""",
}

## Checklist
Here is the checklist that contains the conditions specified in the question for a good output. The more requirements an output meets, the better it is considered.

{
    checklist: """{checklist}""",
}

## Model Outputs

Here are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.

{
    {
        "model_identifier": "m",
        "output": """{output_1}"""
    },
    {
        "model_identifier": "M",
        "output": """{output_2}"""
    }
}

## Task

Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output based on the checklist. Answer by first providing a concise explanation based on the checklist and then end your answer by providing the model identifier of the best output. If you determine that both outputs are of equal quality or cannot decide which one is better, indicate a tie by using the identifier `D`. We will use the last character of your output `output[-1]` as the name of the best model, so make sure you finish with the token of the model identifiers and nothing else: `m`, `M`  or `D` (no quotes, no dots, no backticks, no new lines, ...). For example:

### Concise explanation
...some text...

### Which is best, m, M or D?
M

Now is your turn.

## Your answer: "Concise explanation" followed by "Which is best, m, M or D?"
\end{lstlisting}

\subsection{Checklist Generation}
\label{checklist_geneartion}
We follow \citet{cook2024tickingboxesgeneratedchecklists}'s prompt tepmplate to generate checklists.
\begin{lstlisting}[style=plainins]
(*@\textbf{[System Part]}@*)
Please help judge an AI assistant's response to an instruction by providing an evaluation checklist. 
To write a specific evaluation checklist, you get given the following entity each time: 
INSTRUCTION: An instruction that has been given to an AI assistant.

(*@\textbf{[User Part]}@*)
## Task Details
Your task is to come up with an evaluation checklist list for a given INSTRUCTION.
This evaluation checklist should be a list of questions that ask whether or not specific criteria relevant to the INSTRUCTION were met by an AI assistant's response.
Criteria covered by your checklist could be explicitly stated in the INSTRUCTION, or be generally sensible criteria for the problem domain.
You should, however, try to be concise and not include unnecessary entries in your checklist.

Checklist questions should:
- **Be answerable by 'yes' or 'no'**, with 'yes' meaning that the response successfully met the corresponding requirement.
- **Be comprehensive, but concise**, meaning that all criteria directly relevant to the INSTRUCTION should be represented by a question, but only questions that are very clearly relevant should be included.
- **Be precise**, meaning that checklist questions should avoid vague wording and evaluate specific aspects of a response, directly using the phrasing of the INSTRUCTION where appropriate.

You should always analyse the INSTRUCTION before providing an evaluation checklist.

## Response Format
Analysis: xxx
Answer: CHECKLIST QUESTIONS (each question should appear on a new
line)

## Examples

{examples}

## Real Task

### INSTRUCTION
{message}

### Response
Please analyse the instruction and provide an answer in the correct format.
Remember that each question should be phrased such that answering with 'yes' would mean that the response **successfully** fulfilled the criteria being assessed by the question.
In most cases, your checklist should contain at least two questions, but no more than eight.
\end{lstlisting}
\end{document}
