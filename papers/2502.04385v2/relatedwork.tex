\section{Related Work}
Lidar technology plays an essential role in autonomous systems by providing high-resolution depth and spatial information. Traditional lidar data processing primarily focuses on 3D point clouds, which deliver detailed environmental data but are computationally expensive and require complex preprocessing. More recent approaches have integrated lidar data with deep learning models, such as LidarCLIP \cite{hess2024lidarclip}, which embed 3D point clouds into a shared space alongside images and text. However, these methods are constrained by the need to align point clouds with the camera's field of view, limiting their ability to fully leverage lidar’s 360-degree spatial context.

The Ouster OS1 lidar sensor \cite{OusterOS1} offers high-resolution 2D images that are spatially correlated and cover the entire 360-degree field of view. These structured images align more naturally with deep learning models, overcoming the challenges associated with processing 3D point clouds. By directly processing 2D lidar images, new possibilities arise for lidar-based tasks, such as image captioning and object detection, without the need for alignment or transformation to the camera's perspective.

Recent advances in large-scale multimodal models, such as Florence 2 \cite{Xiao_2024_CVPR}, have significantly expanded the potential applications of lidar data. Florence 2’s zero-shot capabilities enable it to perform a wide range of visual tasks—image captioning, object detection, and scene understanding—without additional training. This makes Florence 2 particularly well-suited for processing 2D lidar images, yielding more detailed and contextually relevant results compared to earlier models like CLIP \cite{CLIP}.

In contrast to LidarCLIP, which operates primarily on 3D point clouds and requires alignment with images, our approach utilizes the full 360-degree field of view provided by the Ouster OS1 sensor. By leveraging Florence 2 for zero-shot image captioning and object detection on 2D lidar images, we eliminate the need for alignment and preprocessing, yielding more accurate and insightful results for lidar-based tasks. Unlike LidarCLIP, which decodes lidar points into vectors similar to its handling of paired images \cite{hess2024lidarclip}, our approach facilitates richer, more nuanced interpretations of the data. Furthermore, future models like CLIPCap \cite{mokady2021clipcapclipprefiximage}, which combine CLIP with GPT-2 \cite{GPT2} for image captioning, often produce less informative captions. This is due to CLIP's more limited image captioning capabilities, in contrast to the more robust and contextually aware performance of Florence 2 on lidar data.

\begin{figure*}[htbp]
    \captionsetup{justification=centering, position=above}  % Set caption to appear above
    \caption{\textbf{An example of our image captioning output and object detection results using 90${^\circ}$ imagery.}}
    \centering      
    \begin{minipage}[b]{0.95\linewidth}  % Increase the width to fill more of the page
        \centering
        \includegraphics[width=\linewidth]{Figures/A6479_image_003_OD.jpg}
    \caption*{
        \texttt{\textcolor{blue}{
            \textcolor{gray}{Two cars} parked on the side of a \textcolor{green}{road}, with a \textcolor{red}{person} riding a \textcolor{orange}{bicycle} in the foreground. 
            In the background, there are \textcolor{purple}{houses}, \textcolor{brown}{trees}, and a \textcolor{cyan}{sky} with \textcolor{teal}{clouds}.
        }}
    }
    \end{minipage}
    \begin{minipage}[b]{0.95\linewidth}  % Increase the width to fill more of the page
        \centering
        \includegraphics[width=\linewidth]{Figures/A8344_image_001_OD.jpg}
    \caption*{
        \texttt{\textcolor{blue}{
            A \textcolor{red}{person} walking down a \textcolor{green}{street} in front of a \textcolor{purple}{building}, surrounded by \textcolor{brown}{trees} and \textcolor{orange}{poles}.
        }}
    }
    \end{minipage}
    
    \label{fig:ouster_results}
\end{figure*}