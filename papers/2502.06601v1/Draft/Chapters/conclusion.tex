\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}
\looseness=-1
We show that Bayesian posterior inference can be amortized for a broad class of probabilistic models and explore a variety of design decisions associated with it. In particular, we show that reverse KL is effective for learning the amortization network and has significant benefits when modeling the predictive, especially in the presence of model misspecification and generalization to out-of-domain real-world setups. It provides an exciting direction of research which could reduce the load of real-world, complex, and iterative approximations through quick and cheap inference over a trained amortized network. Even further, we believe that our approach provides a direction into learning of generalist in-context Bayesian estimator capable of estimating the posterior for novel datasets zero-shot. 

We believe that scaling our approach to more complex probabilistic models as well as leveraging existing real-world data in the training paradigm can lead to even better amortized posterior estimators. Another important future work is to combine our approach with diffusion-based~\citep{zhang2021path,vargas2023denoising} and other off-policy variational systems~\citep{bengio2021flow} for modeling more complex high dimensional distributions.