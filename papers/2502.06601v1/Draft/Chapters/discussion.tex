\vspace{-3mm}
\section{Discussion and Conclusion}
\vspace{-2mm}
We show that Bayesian posterior inference can be amortized for a broad class of probabilistic models and explore a variety of design decisions associated with it. Some key conclusions from our analysis are described below.

\textbf{Forward vs Reverse KL}. Our GMM experiments (\cref{fig:mode_switching}; \textbf{Left}) indicate that forward KL is more amenable to learning multimodal solutions compared to reverse KL in low-dimensional problems. However, the latter outperforms the former in both predictive and sample-based metrics when the parameter space is high-dimensional. Further, reverse KL methods do not require access to $(\mtheta, \gD)$ samples during training and thus show improvements in misspecification and simulation to real transfer.

\looseness=-1
\textbf{Architectural Choices}. We compare permutation invariant architectures like DeepSets and Transformers with non invariant architecture like GRU and see that GRU outperforms DeepSets even though the latter is permutation invariant. We hypothesize that this could be due to limited expressivity of DeepSets and their reliance on a fixed pooling operator. In contrast, GRUs can learn to be approximately permutation invariant through training. We further see that Transformers outperform both DeepSets and GRUs as they do not rely on fixed aggregation schemes but still respect the invariant structure of the posterior.

\looseness=-1
\textbf{Capacity of $q_\varphi$}. Increasing the capacity of $q_\varphi$ using normalizing flows substantially helps for forward KL but only marginally for the reverse KL objective. We hypothesize that because of the mode-seeking tendency of reverse KL, even with the capacity to model different modes, the algorithm latches to a single one. However, in forward KL setup without additional capacity the model overestimates the variance a lot.

We provide a rigorous comparison of different in-context posterior estimators, especially in the presence of misspecification and generalization to real-world problems. It provides an exciting direction of research which could reduce the load of real-world, complex, and iterative approximations through quick and cheap inference over a trained amortized network -- providing a direction into learning a generalist in-context Bayesian estimator. We believe that scaling our approach to more complex probabilistic models, leveraging better modeling choices for high-dimensional problems~\citep{bengio2021flow,zhang2021path,vargas2023denoising}, and training a single model for multiple probabilistic models are important directions of future work.