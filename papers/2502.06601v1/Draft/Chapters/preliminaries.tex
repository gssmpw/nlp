\input{Draft/Figures/fixed_dim}
\vspace{-3mm}
\section{Background}
\label{sec:prelim}
\vspace{-2mm}
\looseness=-1
We first cover some of the important preliminaries below.

\looseness=-1
\textbf{Bayesian Inference}. Let $\vx \in \mathbb{R}^d$ denote the outcome of an experiment observed through a set of independent and identically distributed (\textit{iid}) samples $\gD := \{\vx_1, ..., \vx_N\} \subseteq \mathbb{R}^d$. Given these observations, we are interested in either quantifying the certainty of or generating potential future observations $\vx_*$. Bayesian Inference provides a natural methodology of quantifying $p(\vx_* | \gD)$ by prescribing a space of hypotheses $\mtheta \in \mathbb{R}^k$ and a \textit{prior} belief $p(\mtheta)$ over it. These hypotheses define the \textit{likelihood} of observing an outcome, i.e., $p(\vx | \mtheta)$. The likelihood and prior are then combined through Bayes rule to infer the \textit{posterior} $p(\mtheta | \gD)$, through which the quantity of interest can then be easily expressed as 
\begin{align}
    p(\vx_* | \gD) = \int_\mtheta p(\vx_* | \mtheta) p(\mtheta | \gD) d\mtheta
\end{align}
This poses two challenges: (a) the \textit{posterior}, often a quantity of interest in itself, is not known, and (b) the integration can be intractable which is often resolved through Monte Carlo estimation
\begin{align}
    p(\vx_* | \gD) = \mathbb{E}_{\mtheta | \gD} \left[p(\vx_* | \mtheta)\right]
    \approx \frac{1}{M} \sum_{m=1}^M p(\vx_* | \mtheta^{(m)})
\end{align}
where $\mtheta^{(m)} \sim p(\mtheta | \gD)$. The quantity $p(\mtheta | \gD)$ can be obtained through an application of Bayes rule
\begin{align}
    \label{eq:bayes_rule}
    p(\mtheta | \gD) = \frac{p(\gD | \mtheta)\;p(\mtheta)}{p(\gD)} 
    = \frac{p(\mtheta)}{p(\gD)} \prod_{n=1}^N p(\vx_n | \mtheta)
\end{align}
Given the form of the \textit{likelihood} and \textit{prior}, the above distribution is often difficult to sample from, especially with the added complexity of the marginal $p(\gD) = \int_\mtheta p(\gD | \mtheta)\, p(\mtheta)$ being intractable. Additionally, the posterior itself is often of interest on its own, especially in cases where $\mtheta$ is interpretable, \eg if we model the bias of a coin based on multiple tosses. We refer the readers to~\citet{bishop2006pattern} for additional applications of Bayesian Inference.
% to supervised learning, etc.

\textbf{Approximate Bayesian Inference}. To bypass the intractability of the posterior distribution, or at least the difficulty to sample from it, approximate methods are used. 

Sampling based methods provide ways of sampling from the true posterior distribution based on easy access to an unnormalized density function, e.g. rejection sampling. More advanced methods like  MCMC construct a chain of updates $\mtheta_1, \mtheta_2, \ldots$ such that asymptotically the samples converge to samples from the true posterior. Such sampling methods rely on transition kernels $\gT(\mtheta_{t+1} | \mtheta_t)$ and often some acceptance criteria $\gA(\mtheta_{t+1}, \mtheta_t)$, a key example of which is the Metropolis-Hastings algorithm. We refer the readers to~\citep{hoffman2014no,welling2011bayesian} for a detailed analysis into different MCMC methods like Langevin and Hamiltonian Monte Carlo which rely on gradient of the log density as additional signal for better convergence.

In contrast, another class of methods approximate the true posterior with a parametric family $q_\varphi(\mtheta)$ and convert the estimation problem into the following optimization problem
\begin{align}
    \varphi^* = \arg\min_\varphi \mathbb{D}\left(p(\cdot | \gD), q_\varphi(\cdot)\right)
\end{align} 
where $\mathbb{D}$ is a notion of divergence between two distributions. Once the optimal parameters $\varphi^*$ are obtained, $q_{\varphi^*}$ can be used to substitute the true posterior wherever needed. The above optimization procedure finds a member in the family of variational distributions $\{q_\varphi\}_\varphi$ that is closest to the true posterior under $\mathbb{D}$. An example of this is Variational Inference, where the reverse KL divergence is used
\begin{align}
\label{eq:rkl}
\mathbb{D}_{\text{R-}\mathbb{KL}}\left(p(\cdot | \gD), q_\varphi(\cdot)\right) &= \mathbb{E}_{\mtheta \sim q_\varphi(\cdot)} \left[\log \frac{q_\varphi(\mtheta)}{p(\mtheta | \gD)}\right]
\end{align}
which is equivalent to optimizing the well known Evidence Lower-Bound (ELBO)~\citep{Gelman2013-bayesdata}
\begin{align}
    \varphi^* &= \arg\max_\varphi \sE_{\mtheta \sim q_\varphi(\cdot)}\left[\log \frac{p(\gD, \mtheta)}{q_\varphi(\mtheta)}\right]
\end{align}
Another example is Expected Propagation \citep[EP;][]{minka2013expectation} which relies on the forward KL divergence
\begin{align}
\label{eq:fkl}
\mathbb{D}_{\text{F-}\mathbb{KL}}\left(p(\cdot | \gD), q_\varphi(\cdot)\right) &= \mathbb{E}_{\mtheta \sim p(\cdot | \gD)} \left[\log \frac{p(\mtheta | \gD)}{q_\varphi(\mtheta)}\right]
\end{align}
Once the optimal parameters $\varphi^*$ are obtained, the \textit{posterior predictive} distribution $p(\vx_* | \gD)$ can be approximated as
\begin{align}
    p(\vx_* | \gD) \approx \mathbb{E}_{q_{\varphi^*}(\mtheta)}\left[p(\vx_* | \mtheta)\right]
\end{align}
\looseness=-1
The family of distributions $q_\varphi$ is chosen such that it is easy to sample from. Typical choices include independent multivariate Gaussian distribution (mean-field approximation) or normalizing flows~\citep{rezende2015variational,papamakarios2021normalizing,freia}.

\input{Draft/Tables/fixed_dim}
\textbf{Estimators and Amortization}. A core benefit of deep learning is its ability to generalize well. Amortized approaches leverage this ability by training conditional models to solve a family of problems in an efficient and scalable manner, as opposed to independently solving each problem. For \eg, the encoder in Variational Autoencoders~\citep[VAEs;][]{kingma2013auto,rezende2014stochastic} is tasked with estimating the posterior distribution $p(\vz | \vx_i)$ for each $\vx_i \in \gD$. Here, $p(\vz)$ is the standard normal prior and $p(\vx_i | \vz)$ is the decoder defining the trainable likelihood. Instead of separate optimization problems $q_{\varphi_i^*}(\vz)$ for each posterior $p(\vz | \vx_i)$, VAEs rely on amortization to train a shared network $q_\varphi(\vz | \vx)$, where $\varphi$ now represents the parameters of a neural network and takes $\vx$ explicitly as input, allowing zero-shot generalization to new $\vx_*$ at inference. 

Amortization plays a key role in multiple domains of machine learning, beyond VAEs. For \eg, score-based diffusion models \citep{song2020score} amortize training of a time-conditioned score model, while Neural Processes (NPs) \citep{garnelo2018conditional,garnelo2018neural} and neural posterior estimation in Simulation-Based Inference (SBI) \citep{cranmer2020sbireview} amortize dataset-conditioned VAE-styled encoders under $\mathbb{D}_{\text{R-}\mathbb{KL}}$ and $\mathbb{D}_{\text{F-}\mathbb{KL}}$ respectively. Even further, in-context learning (ICL)~\citep{von2023transformers,muller2021transformers} can also be seen as amortizing the posterior predictive distribution $p(y_* | \vx_*, \gD)$ based on context examples $\gD$ as an emergent phenomena owing to the shared modality of language tying different tasks, where $y_*$ defines the label.

\input{Draft/Tables/maximum_dim}
We refer to \Cref{appdx:related_work} for details about related work.
% , as well as its connections and differences with our proposed mechanism.