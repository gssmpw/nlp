\vspace{-3mm}
\section{Posterior Estimation from Data in Context}
\label{sec:method}
\vspace{-1mm}
As described earlier, standard in-context approaches are predominantly concerned with prediction and model the posterior predictive directly, taking $\gD$ as input. However, they can be tweaked to perform posterior estimation instead. We discuss ways to train such an in-context estimator and showcase its connections to existing amortization methods.

Given any modeling assumption defined via a probabilistic model $p(\cdot | \mtheta)$ with parameters $\mtheta$, we are interested in estimating the full Bayesian posterior over the parameters $p(\mtheta | \gD)$ after obtaining some observations $\gD$, in a manner that allows fast and scalable approximation. \Cref{eq:rkl,eq:fkl} showcase two different methodologies of performing posterior estimation, however, both the methods train a new $q_\varphi$ every time new observations $\gD$ are obtained. However, such methods can be easily amortized by leveraging in-context learning with a goal towards posterior estimation instead of prediction, i.e. training a model to approximate the posterior distribution based on in-context examples. Mathematically, this is obtained by considering an approximate density $q_\varphi(\cdot | \gD)$\footnote{We term this conditional model \emph{in-context posterior estimator}.} which is explicitly conditioned on the set of observations $\gD$ and trained over multiple such sets
\begin{align}
    \varphi^* = \arg\min_\varphi \mathbb{E}_{\gD \sim \chi} \mathbb{D}\left(p(\cdot | \gD), q_\varphi(\cdot|\gD)\right)
\end{align}
where $\chi$ denotes some distribution over observations $\gD$. 

If the measure of divergence is the forward KL $\mathbb{D}_{F\text{-}\mathbb{KL}}$, it leads to the neural posterior estimation methodology of simulation-based inference (SBI-NPE) as long as an additional constraint is satisfied, i.e. $\chi$ defines sampling from the assumed underlying model $p$, \ie
\begin{align}
    \label{eq:chi_sim}
    \chi(\gD) = \int p(\mtheta) \prod_{\vx_n \in \gD} p(\vx_n | \mtheta) d\mtheta
\end{align}
The importance of this constraint is that it leads to a simpler gradient-based optimization procedure as opposed to EP
\begin{align}
    \varphi^*_{F\text{-}\mathbb{KL}} &= \arg\min_\varphi \mathbb{E}_{\gD \sim \chi} \mathbb{E}_{\mtheta \sim p(\cdot | \gD)} \left[\log \frac{p(\mtheta | \gD)}{q_\varphi(\mtheta | \gD)}\right] \\
    \label{eq:afkl}
    &= \arg\min_\varphi \mathbb{E}_{\mtheta} \mathbb{E}_{\gD \sim p(\cdot | \mtheta)} \left[-\log q_\varphi(\mtheta | \gD)\right]
\end{align}
where we focus our attention to the change in expectations which is only possible when $\gD$ is sampled according to $p$. This removes the requirement of sampling or evaluating the true posterior, a known hurdle with EP methods. 

\begin{table*}
\input{Draft/Tables/tabular}
\end{table*}
In contrast, one could also take motivation from VAEs and NP which predominantly work under the reverse KL divergence $\mathbb{D}_{R\text{-}\mathbb{KL}}$ minimization. An amortized in-context learner in this setting can be mathematically formalized as
\begin{align}
\varphi^*_{R\text{-}\mathbb{KL}} &= \arg\min_\varphi \mathbb{E}_{\gD \sim \chi} \mathbb{E}_{\mtheta \sim q_\varphi(\cdot | \gD)} \left[\log \frac{q_\varphi(\mtheta | \gD)}{p(\mtheta | \gD)}\right] \\
\label{eq:arkl}
&= \arg\min_\varphi \mathbb{E}_{\gD \sim \chi} \mathbb{E}_{\mtheta \sim q_\varphi(\cdot | \gD)} \left[\log \frac{q_\varphi(\mtheta | \gD)}{p(\gD, \mtheta)}\right]
\end{align}
It is important to note that unlike forward KL, reverse KL provides the freedom of choosing any arbitrary $\chi$ while still maintaining ease in training, i.e. the in-context estimator can be trained on datasets that come from a different distribution than $p$. Such flexibility is important because we often want to generalize well to data whose underlying true probabilistic model is unknown, and thus we often prescribe a likelihood based on our best belief. For \eg, practitioners often model regression problems as linear even when the data might come from a nonlinear process like a Gaussian Process. Reverse KL allows us to train the in-context learner on a steady stream of data even when the likelihood is misspecified, while the forward KL objective in this case will have to be trained on simulated linear data.

The estimators defined in \Cref{eq:afkl,eq:arkl} rely on a parameterization of $q_\varphi(\cdot | \gD)$, which involves two components: a flexible parametric form of density and an architecture that can be conditioned on entire datasets. In this work, we provide an in-depth analysis into the use of a diagonal Gaussian and discrete-time normalizing flows for the former, and Gated Recurrent Units (GRU), DeepSets and Transformers for the latter. We note that the conditioning architecture should respect permutation invariance of the approximate posterior to the ordering in the observations, 
% i.e. given any permutation matrix $\Pi$
% \begin{align}
    % q_\varphi(\mtheta | \gD) = q_\varphi(\mtheta | \Pi \gD) \quad\text{as}\quad p(\mtheta | \gD) = p(\mtheta | \Pi \gD)
% \end{align}
which is respected in DeepSets and Transformers but not in GRUs. See \Cref{appdx:exchangeability} for architectural choice details.
% We defer details regarding the architectural choices to Appendix~\ref{appdx:exchangeability}. 

Finally, these training procedures naturally introduce a dependency on the dataset generating distribution $\chi$. Since we are working with a known probabilistic model, an obvious choice of $\chi$ is to treat this probabilistic model as a black-box simulator and generate samples using ancestral sampling.
% , where we fix the prior as $\gN(\cdot; 0, \text{I})$. 

In the following sections, we provide a comparative analysis between the two approaches to in-context posterior estimation, as well as the different architectural choices and parametrizations of the density $q_\varphi$. While this has been independently studied in the SBI and NP framework, a rigorous comparative study between them through the lens of predictive and sample-based metrics has been lacking. Further, we aim to understand the impact of misspecification in such in-context learners, i.e. when real data may not come from the $p$ defined by the likelihood and the prior. To study this rigorously, we further evaluate the in-context estimators on observations coming from different underlying known and unknown processes in \Cref{subsec:misspecification,subsec:tabular}.