\vspace{-2mm}
\section{Background}
\label{sec:background}
\vspace{-1mm}
\looseness=-1
% To understand our proposed method of amortizing posterior inference without the need for iterative refinement strategies, w
We first cover some of the important preliminaries as well as approaches already existing in the literature. 
% We also analyze concrete differences between our proposed work and existing prior work on SBI, NPs, and amortized GP kernel estimation. While they all look at related problems of amortized posterior estimation, the underlying methodology and goals in each are different from ours, as outlined below.

\looseness=-1
\textbf{Bayesian Inference}. Let $\vx \in \mathbb{R}^d$ denote the outcome of an experiment observed through a set of independent and identically (\textit{iid}) distributed samples $\gD = \{\vx_1, ..., \vx_N\}$. Given these observations, we are often interested in either quantifying the certainty of or generating potential future observations $\vx_*$. Bayesian Inference provides a natural methodology of quantifying $p(\vx_* | \gD)$ by prescribing a space of hypotheses $\mtheta \in \mathbb{R}^k$ and a \textit{prior} belief $p(\mtheta)$ over it. These hypotheses $\mtheta$ define the \textit{likelihood} of observing a particular outcome, i.e., $p(\vx | \mtheta)$. The likelihood and prior are then combined through Bayes rule to define the \textit{posterior} $p(\mtheta | \gD)$, through which the quantity of interest can then be easily expressed as 
\begin{align}
    p(\vx_* | \gD) = \int_\mtheta p(\vx_* | \mtheta) p(\mtheta | \gD) d\mtheta
\end{align}
However, the above expression poses two challenges: (a) the \textit{posterior} $p(\mtheta | \gD)$, which is often a quantity of interest in itself, is not known, and (b) even if known, the integration might be intractable. The intractability of the integration is often resolved through Monte Carlo estimation
\begin{align}
    p(\vx_* | \gD) &= \mathbb{E}_{\mtheta | \gD} \left[p(\vx_* | \mtheta)\right] \\
    &\approx \frac{1}{M} \sum_{m=1}^M p(\vx_* | \mtheta^{(m)})
\end{align}
where $\mtheta^{(m)} \sim p(\mtheta | \gD)$. The quantity $p(\mtheta | \gD)$ can be obtained through an application of Bayes rule
\begin{align}
    \label{eq:bayes_rule}
    p(\mtheta | \gD) &= \frac{p(\gD | \mtheta)\;p(\mtheta)}{p(\gD)} \\
    &= \frac{p(\mtheta)}{p(\gD)} \prod_{n=1}^N p(\vx_n | \mtheta)
\end{align}
Given the form of the \textit{likelihood} and the \textit{prior}, the above distribution is often difficult to sample from, especially with the added complexity of the marginal $p(\gD) = \int_\mtheta p(\gD | \mtheta)\, p(\mtheta)$ being intractable. Additionally, the posterior itself is often of interest on its own, especially in cases where $\mtheta$ is interpretable, for example, if we model the bias of a coin based on multiple tosses. We refer the readers to~\citet{bishop2006pattern} for applications of Bayesian Inference to supervised learning, etc.

\textbf{Variational Inference}. To bypass the intractability of the posterior distribution, or at least the difficulty to sample from it, VI methods approximate the true posterior $p(\mtheta | \gD)$ with a variational distribution $q_\varphi(\mtheta)$ and convert the estimation problem into the following optimization problem
\begin{align}
    \varphi^* = \arg\min_\varphi \mathbb{KL}[q_\varphi(\cdot) || p(\cdot | \gD)]
\end{align} 
which is equivalent to optimizing the Evidence Lower-Bound (ELBO)~\citep{Gelman2013-bayesdata}
\begin{align}
    \varphi^* &= \arg\max_\varphi \sE_{\mtheta \sim q_\varphi(\cdot)}\left[\log \frac{p(\gD, \mtheta)}{q_\varphi(\mtheta)}\right]
\end{align}
The above optimization procedure finds a member in the family of variational distributions $\{q_\varphi\}_\varphi$ that is closest to the true posterior under the \textit{reverse}-$KL$ divergence. Once the optimal parameters $\varphi^*$ are obtained, the \textit{posterior predictive} distribution $p(\vx_* | \gD)$ can be approximated as
\begin{align}
    p(\vx_* | \gD) \approx \mathbb{E}_{q_{\varphi^*}(\mtheta)}\left[p(\vx_* | \mtheta)\right]
\end{align}
\looseness=-1
The family of distributions $q_\varphi$ is chosen such that it is easy to sample from. Typical choices include independent multivariate Gaussian distribution (mean-field approximation) or normalizing flows~\citep{rezende2015variational,papamakarios2021normalizing,freia}.

\textbf{Amortization}. One of the most powerful capabilities of neural networks is their ability to learn and generalize to a wide variety of domains and settings provided sufficient variability during training. For example, Variational Autoencoders (VAEs) define a latent-variable model $p(\vx, \vz)$ where $\vx$ represents the observation and $\vz$ the latent variable. VI typically relies on solving a separate optimization problem $q_{\varphi_i^*}(\vz_i)$ for each posterior $p(\vz_i | \vx_i)$. The cost of learning separate variational approximations can be amortized through training of a joint network $q_\varphi(\vz | \vx)$, where $\varphi$ now represents the parameters of a neural network which takes $\vx$ explicitly as input. The VI procedure then reduces to optimizing $\varphi$, which is shared across all observations, as opposed to optimizing for separate $\varphi_i's$, in the hope that $q_\varphi(\vz_i | \vx_i) \approx q_{\varphi_i}(\vz_i)$ for any $\vx_i$. When modeling using Gaussian distributions, this distinction can be seen as $q_\varphi(\vz_i | \vx_i) := \gN(\cdot\;; \mmu_\varphi(\vx_i), \mSigma_\varphi(\vx_i))$ while $q_{\varphi_i}(\vz_i) := \gN(\cdot\;; \mmu_{\varphi_i}, \mSigma_{\varphi_i})$ (note the functional dependencies). In a similar fashion,~\citet{garnelo2018neural} amortize on datasets as explicit inputs, while score-based generative models~\citep{song2020score} amortize on timesteps. Such models are largely successful owing to the generalization capabilities of neural networks to new unseen observations as long as the encoder $q_\varphi(\vz | \vx_i)$ is trained on diverse enough observations $\vx_i'$s.

\looseness=-1
\textbf{Simulation-Based Inference}. SBI considers the problem of inferring the parameters of the simulator from observations. This is often tackled via neural posterior estimation methods (SBI-NPE) where a deep-learning based model is trained to infer the posterior by explicitly conditioning an approximate distribution $q_\varphi(\mtheta | \gD)$ on the dataset, and modeling the gap between the distributions through a {\it Forward}-KL based optimization
\begin{align}
    \label{eq:sbi}
    \arg\min_\varphi \sE_{\gD}\sK\sL \left[p(\mtheta | \gD) || q_\varphi(\mtheta | \gD)\right]
\end{align}
which often leads to mode averaging behavior that can be problematic in high dimensions. While the above objective often enjoys applications to tasks without tractable likelihood functions, it can only be used for training when the dataset $\gD$ is sampled according to the probabilistic model, and thus cannot utilize off-policy non-simulated data, hindering generalization to real-world scenarios. Precisely because of this, SBI-NPE has been leveraged in controlled scenarios like modeling inverse problems with low-dimensional non-differentiable simulators where the likelihood of an observation is not tractable; but has seen limited applicability in more general high dimensional estimation problems like the distribution over weights of a Bayesian Neural Networks. 

\textbf{Neural Processes}. NPs also leverage amortized VI in training a latent-variable system for modeling predictive problems. However, unlike our setup, they define an approximate posterior distribution only over an arbitrary latent space and learn how this latent variable impacts the likelihood through point estimation of likelihood parameters. In particular, NPs rely on the Variational-EM setup, where they perform point estimation for the parameters of the likelihood and VI for the latent variable. In contrast, we focus on a similar setting where we instead do a full VI treatment of the parameters of the likelihood function; which in some sense means that our \emph{latent variables} are now parameters of the likelihood model. Thus, our approach can be seen as a fully Bayesian Inference procedure for likelihood models, whereas NPs can be seen as latent-variable models which only provide point estimates for the parameters. 

\textbf{Gaussian Process Kernel Estimation}. A specific application of our framework is the estimation of the kernel function for Gaussian Process likelihood models~\citep{liu2020task,simpson2021kernel,bitzer2023amortized}, which leverages amortized inference for tractable likelihoods defined by GP regression setups. In contrast, we provide a more general framework for conducting amortized inference, which we test across a wide variety of domains ranging from supervised to unsupervised learning and from regression to classification tasks. Thus, our proposed approach provides a framework for parameter estimation through amortized variational inference and GP kernel estimation along these lines is a specific application of this approach.

\textbf{In-Context Learning}. ICL refers to large pre-trained models having the ability of solving new tasks at inference based solely on some context examples being provided as prompt. A natural parallel to conditioning on context examples via prompt in natural language is explicit amortization on observations for general-purpose tasks~\citep{von2023transformers,muller2021transformers}. In this sense, our approach can be seen as training a general-purpose in-context learner to perform Bayesian inference, as opposed to direct predictions. 

\textbf{Sampling from Energy}. Another line of similar works~\citep{zhang2021path,berner2022optimal,vargas2023denoising,akhound2024iterated,bengio2021flow} look at sampling proportional to a pre-specified energy function, which in our case can be seen as $p(\mtheta | \gD) \propto e^{-\mathcal{E}(\mtheta)}$, with $\mathcal{E}(\mtheta) = -\log p(\gD, \mtheta)$. However, such works currently focus on non-amortized setups and we defer exploring amortized diffusion-styled posterior models as relevant future work.

We refer the readers to Appendix~\ref{appdx:related_work} for a detailed discussion about prior work, as well as its connections and differences with our proposed mechanism.