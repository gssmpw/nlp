\vspace{-3mm}
\section{Experiments}
\label{sec:experiments}
\vspace{-1mm}
\looseness=-1
To provide a fair and comprehensive evaluation of the different estimators and modeling choices, we consider a variety of well-known probabilistic models encompassing supervised and unsupervised scenarios. In particular, we look at the problem of estimating the Bayesian posterior over the (a) mean of a Gaussian distribution (GM), (b) means of a Gaussian mixture model (GMM), (c) parameters of a (non-)linear regression model (NLR/LR), and (d) parameters of a (non-)linear classification model (NLC/LC). We refer the readers to \Cref{appdx:probabilistic_models} for particulars about the probabilistic models, including their likelihoods and priors considered.

\textbf{Baselines}. We consider dataset-specific baselines to compare different amortized in-context posterior estimators with. In particular, we use the prior (Random), perform maximum likelihood estimation using gradient-based optimization (Optimization) as well as an approximate Bayesian inference procedure through Langevin and Hamiltonian based MCMC sampling. Such baselines rely on iterative procedures and must be run independently for different datasets.

\textbf{Metrics}. We consider two different types of metrics: predictive and sample-based. For the former, we consider $L_2$ loss and accuracy as applicable, in the following manner
\begin{align}
    \mathbb{E}_{(\vx_*,y_*), \gD \sim \chi} \mathbb{E}_{\mtheta \sim q_\varphi(\cdot | \gD)}\textsc{Metric}\left(\hat{y}, y_*\right)
\end{align}
where $\hat{y}$ is the mode of the distribution $p(\cdot | \vx_*, \mtheta)$ and $\textsc{Metric}$ is $L_2$ loss or accuracy for regression and classification respectively. For unsupervised learning settings, we consider a similar $L_2$ based metric defining distance from the mean of the Gaussian or the closest mean in the GMM. For sample-based metrics, we leverage the 
\begin{align}
    \gW_2^2 &= \inf_\pi \iint \norm{\mtheta_q - \mtheta_p}^2 d\pi(q, p)
\end{align}
where $\pi$ denotes a joint distribution over $(\mtheta_q, \mtheta_p)$ with marginals $q_\varphi(\cdot | \gD)$ and $p(\cdot | \gD)$ respectively. This is called the 2-Wasserstein metric which can be computed with finite samples from each, where we use samples from MCMC as reference for $p$. We also leverage the symmetric KL divergence as a metric whenever the true posterior is available. 

We refer to \Cref{appdx:experiment,appdx:metrics,appdx:results} for details about the experiments, metrics and additional results respectively.

\vspace{-2mm}
\subsection{Zero-Shot Posterior Approximation}
\vspace{-1mm}
\label{subsec:fixed-dim}
\looseness=-1
We first test the in-context estimators' ability to succeed at novel tasks solely at inference over $q_\varphi(\cdot | \gD)$. To do so, we train the estimators on datasets being generated as $\gD_{\text{train}} \sim p$, and are then evaluated on new $\gD_{\text{test}} \sim p$. Mathematically this is equivalent to setting $\chi$ according to \cref{eq:chi_sim} where the number of observations, $|\gD|$, is varied in some range both during training and evaluation. 

\input{Draft/Figures/tabular}
\Cref{fig:fixed_dim} visualizes the amortized estimators in low-dimensional problems, showing that they learn meaningful distributions over the parameters zero-shot on new tasks. Next, we turn our attention to quantitative assessment of the different estimators on more complex, high-dimensional counterparts of the same probabilistic models. \Cref{tab:fixed_dim} shows that the in-context estimators are often comparable to optimization and MCMC baselines, with reverse KL objective combined with normalizing flows and the transformer architecture outperforming other design choices. In particular, we see that for high-dimensional multi-modal problems like NLR/NLC, forward KL approach does not fare well potentially due to its mode averaging property. Surprisingly, we also see non permutation invariant architectures like GRUs perform well, and often better than DeepSets.

\vspace{-3mm}
\subsection{Generalizing to Variable Feature Dimensions}
\vspace{-1mm}
\label{subsec:variable-dim}
So far, we only considered amortization over datasets for the same underlying likelihood model, which fixes the dimensionality of the problem. For example, a different in-context estimator has to be trained for a $2$-dimensional and $5$-dimensional Bayesian linear regression model since the dimensionality of $\mtheta$ changes. It is important to note that a deep learning-based approach leaves hopes of generalizing to new datasets of different dimensionalities since the underlying functional form of the solution remains constant across different datasets, irrespective of the number of features, and is given by the solution obtained from Equation~\ref{eq:bayes_rule}. 

Alternatively, we can see that a low-dimensional problem can just be embedded into a high-dimensional space, with the extra features and parameters set to $0$, akin to the procedure of masking unnecessary dimensions, similar to \cite{hollmann2022tabpfn}. This simple but strong insight allows us to amortize $q_\varphi$ over datasets with varying dimensionalities.

We embed all low-dimensional problems in a $100$-dimensional space by masking the unnecessary dimensions. Our experiments in \Cref{tab:variable_dim} indicate that the same in-context learner can generalize to novel datasets \textit{with a variable number of feature dimensions} zero-shot.

\vspace{-3mm}
\subsection{Model Misspecification}
\vspace{-1mm}
\label{subsec:misspecification}
\looseness=-1
The true likelihood model underlying a data-generating process is often unknown. Practitioners address this by assuming a likelihood model and fitting its parameters to best explain the data. For example, while the true model for classifying emails as spam or not is unknown, one can assume a linear model to approximate the problem. This introduces model misspecification—a mismatch between the assumed and true model.

\looseness=-1
As discussed in \Cref{sec:method}, forward KL methods train only on simulated data from the assumed model, whereas reverse KL methods can use real-world data. Consequently, forward KL approaches struggle with sim-to-real transfer because they cannot incorporate real data during training. In contrast, reverse KL methods leverage real data, leading to more robust predictions in practical settings.

\begin{table}
\input{Draft/Tables/misspecification}
\end{table}
\looseness=-1
Mathematically, let $\chi_{sim}$ from \Cref{eq:chi_sim} denote simulated data and $\chi_{real}$ the actual target data. \Cref{tab:misspecification} shows that reverse KL methods outperform forward KL when trained on $\chi_{sim}$ but tested on $\chi_{real}$. Moreover, reverse KL methods trained directly on $\chi_{real}$ generalize even better (see rows labeled “+ switched data”).
For experimental details and additional results on model misspecification, see Appendices~\ref{appdx:details_misspecification} and~\ref{appdx:results_missspecification}.

\vspace{-3mm}
\subsection{Application to Tabular Benchmarks}
\vspace{-1mm}
\label{subsec:tabular}
To evaluate the efficacy of the trained in-context estimators and their ability to generalize out-of-distribution, we test the models trained in \Cref{subsec:variable-dim} on a suite of regression and classification problems chosen from the OpenML platform. These tasks have varying number of feature dimensions and inherently have different data statistics than the ones obtained from $\chi$ during training. \Cref{tab:tabular} shows the zero-shot performance of the parameters inferred from the in-context estimators, and highlights that they perform considerably better than chance, with transformer models and reverse KL methods outperforming other modeling choices.

We also look at finetuning the inferred parameters from the in-context estimators with a maximum-a-posteriori (\textit{MAP}) objective and compare its performance with a corresponding model initialized from the prior. Our results in \Cref{fig:tabular} highlights that in-context estimators lead to much faster convergence, with reverse KL methods being superior in complex, multi-modal and nonlinear tasks.

\begin{table}
\input{Draft/Tables/posterior_metrics}
\end{table}
Finally, we look at a suite of problems that are extremely out-of-distribution from the $\chi$ used during training. In particular, we look at a suite of regression and classification tasks from the OpenML platform which consist of tasks with varying number of features. We refer the readers to Appendix~\ref{appdx:results_tabular} for results on individual datasets with different $q_\varphi$, as well as Appendix~\ref{appdx:details_tabular} for implementation details.

\input{Draft/Figures/mode_switching}
\input{Draft/Tables/W2_metric}
% \looseness=-1
% While we see clear benefits in simulated settings, we next turn our attention to real-world scenarios and consider a suite of tasks from the OpenML platform for both regression and binary classification by filtering them out from the \textit{OpenML-CTR23 - A curated tabular regression benchmarking suite}~\citep{fischer2023openmlctr23} for regression and \textit{OpenML-CC18 Curated Classification benchmark}~\citep{bischl2019openmlcc18} for classification problems (details in Appendix~\ref{appdx:datasets}). We end up with 9 regression and 13 classification datasets with varying number of features and use our amortized inference systems trained on simulated data with variable feature dimensions to predict the parameters of interest.
% 
% \looseness=-1
% After initializing from the inference model, we further train the parameters of the respective probabilistic models with maximum a-posteriori (\textit{map}) objective and compare the performance with a corresponding model initialized from the prior. 
% Even in this extreme case of domain shift, we see in Table~\ref{tab:tabular} that the amortized model provides good initializations for different real-world tasks zero-shot after training only on simulated data. Figure~\ref{fig:tabular} also shows that the amortized model surpasses the SBI-NPE baseline in terms of performance and convergence speed for both linear and nonlinear setups. While Figure~\ref{fig:tabular} only provides a normalized aggregated performance over all datasets considered for a $q_\varphi$ parameterized as a diagonal Gaussian, we refer the readers to Appendix~\ref{appdx:results_tabular} for results on individual datasets with different $q_\varphi$, as well as Appendix~\ref{appdx:details_tabular} for implementation details.
% 
\vspace{-3mm}
\subsection{Evaluating Posterior Quality}
\vspace{-1mm}
While comparing with the true posterior is hard due to its intractability, it is still available when estimating the mean of a Gaussian distribution or performing Bayesian Linear Regression. Figure~\ref{fig:mode_switching} (\textbf{Right}) shows the kernel density estimate of the samples from the true posterior, amortized forward, and reverse KL model, showing that both estimators efficiently capture the true posterior. We further quantify it through the symmetric KL divergence in Table~\ref{tab:posterior_metrics}. 

For more complex problems, we compute the squared Wasserstein metric $\mathcal{W}_2^2$ between samples from the amortized posterior and multiple chains of Langevin MCMC in \cref{tab:w2}. Our results indicate that reverse KL approaches do slightly better in high-dimensional setups, while for low-dimensional multi-modal scenarios (eg. GMM), forward KL approaches fare better. Importantly, we note that this metric only provides a crude proxy to the quality of the posterior, since MCMC methods only provide asymptotic guarantees.

% \begin{table*}
% \input{Draft/Tables/tabular}
% \vspace{-5mm}
% \end{table*}
