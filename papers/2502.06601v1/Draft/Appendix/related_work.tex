\section{Related Work}
\label{appdx:related_work}
In this section, we draw parallels of our work to various approaches that have been proposed to tackle the problem of either providing a good initialization for different tasks, performing implicit optimization to model predictive distributions for new tasks, or estimating the posterior through a different objective.

\subsection{Variational Autoencoders}
VAEs~\citep{kingma2013auto,rezende2014stochastic,rezende2015variational,kingma2019introduction} are latent variable models which model observations $\vx$ conditioned on latent variables $\vz$ through the joint distribution $p_\theta(\vx, \vz) = p_\theta(\vx | \vz) p(\vz)$ where $p(\vz)$ is generally chosen as $\mathcal{N}(\mathbf{0}, \mathbf{I})$. Training the model is done through VI where $q_\varphi(\vz)$ is obtained by explicit amortization over the data point, that is, $q_\varphi(\vz | \vx) = \gN\left(\mmu_\varphi(\vx), \mSigma_\varphi(\vx)\right)$. Training this system on a dataset $\gD$ is done by similarly optimizing the Evidence Lower-Bound, which boils down to the following optimization problem
\begin{align}
    \arg\max_{\theta, \varphi} \sE_{\vx \sim \gD}\sE_{\vz \sim q(\cdot | \vx)}\left[\log \frac{p_\theta(\vx, \vz)}{q_\varphi(\vz | \vx)}\right]
\end{align}
This objective can easily be optimized using gradient-based learning and the reparameterization trick. While typically, a diagonal Gaussian distribution is considered for $q_\varphi$, more complex distributions utilizing normalizing flows can also be used.


\subsection{Hypernetworks}
Hypernetworks are neural networks that generate weights for another neural network, used in tasks such as uncertainty quantification, zero-shot learning, etc. We refer for a comprehensive overview to~\cite{chauhan2023hyperreview}. Based on experiments on predicting the weights of a compact MLP (section \ref{sec:experiments}), our work shows similarities with studies in this area but also has significant differences. 
Regarding uncertainty quantification, hypernetworks are instrumental in creating an ensemble of models by generating multiple weight vectors for the primary network. Each model within this ensemble possesses distinct parameter configurations, enabling robust estimation of uncertainty in model predictions. This feature is precious in safety-critical domains like healthcare, where confidence in predictions is essential. Multiple weight sets can be generated through techniques like dropout within hypernetworks or sampling from a noise distribution.
The latter~\citep{krueger2017hyperbayesian} is based on a Bayesian framework where weights can be sampled using invertible network architecture, such as normalizing flows. However, while we amortize posterior inference, the weights sampled from the hypernetwork are not conditioned on information from the currently observed input data during inference time but indirectly solely on the dataset available during training, and retraining would need to be done given a new dataset. 
Departing from the Bayesian framework, \cite{sun2017hypercnn} have shown data-specific discriminative weight prediction, which aligns well with their specific objective of defending a convolutional neural network against adversarial attacks.
Combining the ability to sample a new set of weights dataset-specifically but also handling dataset exchangeability, even in the more realistic case of missing information, our work has a distinctly different focus but also can be seen as an extension to hypernetwork research. 

\subsection{In-Context Learning}
Amortized inference has close links to in-context learning (ICL), which has been gaining popularity, especially in natural language modeling. Various works show how in-context learning can be seen as performing implicit optimization based on the context examples, with some constructions showing exact equivalence with gradient descent in linear regression~\citep{von2023transformers,von2023uncovering}. Other works have shown how such systems can be seen as implicitly modeling the Bayesian posterior predictive distribution~\citep{muller2021transformers}. In a similar vein, there have been additional works aimed at directly modeling the posterior predictive distribution by providing the training data as ``context" to a Transformer model and training it based on the maximum log-likelihood principle~\citep{hollmann2022tabpfn}. While such approaches have been seeing tremendous success, they cannot be directly applied to cases where we care about and want to analyze the solution space as the solution space is only modeled implicitly, and thus, recovering it is not possible. For example, if our goal is to learn a linear regression model, an ICL model could end up learning a nonlinear model and would provide no information about the actual parameters used for prediction. As opposed to this, we obtain parameters explicitly. We thus can answer questions like the relevance of a particular feature (which corresponds to its weight in the output, and we know the weight vector explicitly). Even further, many systems grounded in physics and economics only admit a constrained solution space; for example, the movement of a human arm lies on a particular manifold, or the configuration of molecules and proteins cannot be arbitrary. Thus, performing predictions through an implicit solution space, which may violate several constraints, is not ideal. Furthermore, explicitly modeling the solution space and encoding the constraints present can be done through the prior and the parametric distribution used for modeling. 

\input{Draft/Tables/fixed/gaussian}
\input{Draft/Tables/fixed/linear}
\input{Draft/Tables/fixed/nlr_relu}
\input{Draft/Tables/fixed/nlr_tanh}
\input{Draft/Tables/fixed/nlc_relu_2cl}
\input{Draft/Tables/fixed/nlc_relu_5cl}
\input{Draft/Tables/fixed/nlc_tanh_2cl}
\input{Draft/Tables/fixed/nlc_tanh_5cl}
\subsection{Meta Learning}
Meta-learning~\citep{hospedales2022metareview} aims to equip models with the ability to quickly learn from different tasks or data sets to generalize to new tasks in resource-constrained domains. This attribute is precious in practical scenarios where obtaining large amounts of task-specific data is impractical or costly. A simple way of obtaining this is through nonparametric or similarity-based models like k-Nearest Neighbours, where no training is involved. Thus, new tasks can be solved quickly based on a few examples by computing a similarity metric with these examples~\citep{koch2015siamese,vinyals2016matching,sung2018learning}. Another way of achieving this is through optimization-based setups, which use a nested optimization procedure. An inner step learns individual tasks from a shared initialization, whereas the outer loop computes the gradient of the whole inner process and moves the initialization in a way that allows for better generalization. Here, by relying on only a few iterations in the inner loop, the outer loop has the incentive to move the initialization to a point from which solutions to multiple tasks are reachable~\citep{finn2017model}. Given the similarities between meta-learning and hierarchical Bayesian inference~\citep{grant2018recasting}, our approach can be considered as a kind of meta-learning framework; however, the line between meta-learning and Bayesian posterior inference is quite blurry as any amortized approach for the latter can be seen as a case of the former.

% \input{ICLR/Tables/fixed_dim_extras/fixed_dim_1layer}
\subsection{Neural Processes}
A notable approach in meta-learning related to our research is neural processes (NP), which excel in learning scenarios with few examples. 
NPs~\citep{garnelo2018conditional,garnelo2018neural,kim2019attentive,pakman2020neural,gordon2019convolutional} can be seen as a more flexible and powerful extension of Gaussian processes that leverage a neural network-based encoder-decoder architecture for learning to model a distribution over functions that approximate a stochastic process.
However, while we are interested in approximating the posterior distribution over the parameters, NPs are used to approximate the posterior predictive distribution to make predictions based on observed data. Similar to our setup, NPs rely on amortized VI for obtaining the predictive posterior. Still, instead of working with a known probabilistic model, they train the probabilistic model primarily for prediction-based tasks through approaches analogous to variational expectation maximization. Thus, they cannot provide an explicit posterior over the parameters, but they are suitable for tasks where only predictive posteriors are essential, such as those in supervised learning. NPs, in their most basic form, accomplish this by training for the objective:
\begin{align}
    \label{eq:np}
    \arg\max_{\mtheta, \varphi} \sE_{\gD \sim \chi}\sE_{\vz \sim q_\varphi(\cdot | \gD)}\left[\log \frac{p_\mtheta(\gD, \vz)}{q_\varphi(\vz | \gD)}\right]
\end{align}
where $\vz \in \sR^p$ is an arbitrary latent variable often uninterpretable, and the parameters of the probabilistic model $\mtheta$ do not get a Bayesian treatment. In particular, NPs are more suited to modeling datasets of the form $\gD = \{\vx_i, \vy_i\}_{i=1}^n$, where all probabilities in \Eqref{eq:np} are conditioned on the input $\vx$'s, and only the predictive over $\vy$'s is modeled, and $p_\mtheta$ is modeled as a Neural Network.

These approaches can be seen as quite related to ICL, where the exchangeable architecture backbone is switched from DeepSets to Transformers. Similar to ICL, they do not provide control over the solution space as they aim to model either the posterior predictive or an arbitrary latent space. While this leads to good predictive performance on various tasks, they cannot be freely applied to problems that pose certain constraints on the underlying probabilistic model. In such cases, estimating the actual parameters is important to enforce constraints in the parameter space as well as for interpretability, which we already discussed in the ICL section.

\subsection{Simulation-Based Inference}
In the case of simulation-based inference~\citep{cranmer2020sbireview}, when the likelihood $p(\vx|\mtheta)$ is intractable, BayesFlow~\citep{radev2020bayesflow} and similar methods~\citep{lorch2022amortized} provide a solution framework to amortize Bayesian inference of parameters in complex models. Starting from the forward KL divergence between the true and approximate posteriors, the resulting objective is to optimize for parameters of the approximate posterior distribution that maximize the posterior probability of data-generating parameters $\mtheta$ given observed data $\gD$ for all $\mtheta$ and $\gD$. Density estimation of the approximate posterior can then be done using the change-of-variables formula and a conditional invertible neural network that parameterizes the approximate posterior distribution. 
\begin{equation}
    \argmin_\varphi \sK\sL[p(\mtheta|\gD) || q_\varphi(\mtheta|\gD)] = \argmin_{\varphi=\{ \nu,\psi \}} \sE_{(\mtheta, \gD) \sim p(\mtheta, \gD)} \left[ - \log p_\vz(f_\nu(\mtheta; h_\psi(\gD))) - \log \left|\det J_{f_\nu}\right| \right]
\end{equation}
Since their goal is to learn a global estimator for the probabilistic mapping from $\gD$ to data generating $\mtheta$, the information about the observed dataset is encoded in the output of a summary network $h_\psi$. It is used as conditional input to the normalizing flow $f_\nu$. Although the likelihood function does not need to be known, the method requires access to paired observations $(\vx, \mtheta)$ for training, which is sometimes unavailable. This approach is equivalent to the \textit{Forward KL} setup in our experiments when trained with DeepSets and Normalizing Flows. Current research has also leveraged score-based generative models for SBI which can condition on a dataset by learning a score model conditional only on single observations~\citep{geffner2023compositional}.

\subsection{Amortization in Gaussian Processes}
Gaussian Processes (GPs) define a class of probabilistic models that do enjoy tractable likelihood. However, inference in such systems is slow and sensitive to the choice of kernel function that defines the covariance matrix. Similar to meta learning and neural processes, current research also focuses on estimating the kernel function in GPs by leveraging permutation invariant architectures like transformers~\citep{liu2020task,simpson2021kernel,bitzer2023amortized}. Additionally, often these approaches amortize based on point estimates and are leveraged when considering GPs for regression problems, and it is not straightforward to extend them to classification or unsupervised learning. In contrast, our approach is more general and can work for all problems that define a differentiable likelihood function. Additionally, our approach also approximates the Bayesian posterior distribution over the parameters of interest, as opposed to point estimates.

\subsection{Mode Collapse in Variational Inference}
Reverse KL based methods have been widely known to suffer from mode collapse due to the nature of the optimization objective~\citep{bishop2006pattern}, which implies that even if the approximate distribution possesses the ability to represent multiple modes, optimization is often sub-optimal and the distribution ends up covering only a small handful of them. Improving normalizing flow based methods with repulsive terms or through the lens of natural gradient optimization procedure for a mixture approximate distribution~\citep{arenz2022unified,lin2020handling} is an important topic of research, and we believe it would be quite an important future work to experimentally validate if they help in learning multi-modality in amortized posterior inference problems that are studied in this work.