\section{Metrics}
\label{appdx:metrics}
In this section, we provide details about the metrics considered for the different tasks. We generally look at two main metrics for benchmarking performance: $L_2$ loss and Accuracy. For estimating the mean of a Gaussian distribution, the $L_2$ loss is defined as
\begin{align}
    GM_{L_2} &= \mathbb{E}_{\gD \sim \chi}\mathbb{E}_{\mmu \sim q_\varphi(\cdot | \gD)}\left[\sum_{i=1}^{N_\gD} (\vx_i - \mmu)^2\right]
\end{align}
where $\gD = \{\vx_i\}_{i=1}^{N_\gD}$. Intuitively, this captures the quality of the estimation of the mean parameter by measuring how far the observations are from it. Lower value implies better estimation of the mean parameter. Similarly, for estimating the means of a Gaussian Mixture Model, we rely on a similar metric but we also find the cluster closest to the observation, which can be defined as
\begin{align}
    GMM_{L_2} &= \mathbb{E}_{\gD \sim \chi}\mathbb{E}_{\mmu_k \sim q_\varphi(\cdot | \gD)}\left[\sum_{i=1}^{N_\gD} (\vx_i - \mmu_{\text{Match}\left(\vx_i, \{\mmu_1, ... \mmu_K\}\right)})^2\right] \\
    \text{Match}(\vx, \{\mmu_1, ..., \mmu_K\} &= \arg\min_k (\vx - \mmu_k)^2
\end{align}
which intuitively captures the distance of observations from the cluster closest to them. Next, we define the metric for evaluating (non-)linear regression models as
\begin{align}
    (N-)LR_{L_2} &= \mathbb{E}_{\gD \sim \chi}\mathbb{E}_{\mtheta \sim q_\varphi(\cdot | \gD)}\left[\sum_{i=1}^{N_\gD} (y_i - \text{Mode}\left[p(y_i | \vx_i, \mtheta)\right])^2\right]
\end{align}
Finally, for the (non-)linear classification setups, we define the accuracy metric as
\begin{align}
    (N-)LC_{Accuracy} &= \mathbb{E}_{\gD \sim \chi}\mathbb{E}_{\mtheta \sim q_\varphi(\cdot | \gD)}\left[\frac{100}{N_\gD} \times \sum_{i=1}^{N_\gD} \delta(y_i, \text{Mode}\left[p(y_i | \vx_i, \mtheta)\right])\right]
\end{align}
where $\delta(a, b) = 1$ if and only if $a = b$. Thus this metric captures the accuracy of the posterior predictive distribution. Another metric that we use to test the quality of the posterior is the symmetric KL divergence, defined as
\begin{align}
    \text{Symmetric }\mathbb{KL}(p(\mtheta || \gD), q_\varphi(\mtheta | \gD)) &= \frac{1}{2}\mathbb{KL}(p(\mtheta || \gD) || q_\varphi(\mtheta | \gD)) + \frac{1}{2}\mathbb{KL}(q_\varphi(\mtheta | \gD) || p(\mtheta || \gD))
\end{align}
% Additionally, another metric in the predictive space that we use is the expected negative conditional log likelihood (CNLL), which is defined as
% \begin{align}
%     CNLL &= -\mathbb{E}_{q_\varphi(\cdot | \gD)}\left[\log p(\gD | \mtheta)\right]
% \end{align}