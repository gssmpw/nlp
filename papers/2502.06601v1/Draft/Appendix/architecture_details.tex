\section{Architecture Details}
\label{appdx:architecture}
In this section, we outline the two candidate architectures that we consider for the backbone of our amortized variational inference model. We discuss the specifics of the architectures and the hyperparameters used for our experiments.

\subsection{Transformer}
\label{subsec:transformer}
We use a transformer model~\citep{vaswani2017attention} as a permutation invariant architecture by removing positional encodings from the setup and using multiple layers of the encoder model. We append the set of observations with a [CLS] token before passing it to the model and use its output embedding to predict the parameters of the variational distribution. Since no positional encodings or causal masking is used in the whole setup, the final embedding of the [CLS] token becomes invariant to permutations in the set of observations, thereby leading to permutation invariance in the parameters of $q_\varphi$.

We use $4$ encoder layers with a $256$ dimensional attention block and $1024$ feed-forward dimensions, with $4$ heads in each attention block for our Transformer models to make the number of parameters comparative to the one of the DeepSets model.

\subsection{DeepSets}
\label{subsec:deepsets}
Another framework that can process set-based input is Deep Sets~\citep{Zaheer2017deepsets}. In our experiments, we used an embedding network that encodes the input into representation space, a mean aggregation operation, which ensures that the representation learned is invariant concerning the set ordering, and a regression network. The latter's output is either used to directly parameterize a diagonal Gaussian or as conditional input to a normalizing flow, representing a summary statistics of the set input.

For DeepSets, we use $4$ layers each in the embedding network and the regression network, with a mean aggregation function, ReLU activation functions, and $627$ hidden dimensions to make the number of parameters comparable to those in the Transformer model.

\subsection{RNN}
For the recurrent neural network setup, we use the Gated Recurrent Unit (GRU). Similar to the above setups, we use a $4$-layered GRU model with $256$ hidden dimensions. While such an architecture is not permutation invariant, by training on tasks that require such invariance could encourage learning of solution structure that respects this invariance.

\subsection{Normalizing Flows}
\label{subsec:flows}
Assuming a Gaussian posterior distribution as the approximate often leads to poor results as the true posterior distribution can be far from the Gaussian shape. To allow for more flexible posterior distributions, we use normalizing flows~\citep{kingma2018glow,kobyzev2020normalizing,papamakarios2021normalizing,rezende2015variational} for approximating $q_\varphi(\mtheta | \gD)$ conditioned on the output of the summary network $h_\psi$. Specifically, let $g_\nu: \vz \mapsto \mtheta$ be a diffeomorphism parameterized by a conditional invertible neural network (cINN) with network parameters $\nu$ such that $\mtheta = g_\nu(\vz; h_\psi(\gD))$. With the change-of-variables formula it follows that $p(\mtheta)=p(\vz)\left|\det \frac{\partial}{\partial\vz}g_\nu(\vz; h_\psi(\gD))\right|^{-1} = p(\vz)|\det J_\nu(\vz; h_\psi(\gD))|^{-1}$, where $J_\nu$ is the Jacobian matrix of $g_\nu$. Further, integration by substitution gives us $d\mtheta = |\det J_\nu(\vz; h_\psi(\gD)| d\vz$ to rewrite the objective from eq. \ref{eq:arkl} as:
\begin{align}
    &\argmin_\varphi \sE_{\gD \sim \chi} \sK\sL[q_\varphi(\mtheta|\gD) || p(\mtheta|\gD)]\\
    &= \argmin_\varphi \sE_{\gD \sim \chi} \sE_{\mtheta \sim q_\varphi(\mtheta|\gD)} \left[ \log q_\varphi(\mtheta|\gD) - \log p(\mtheta, \gD) \right]\\
    &= \argmin_{\varphi=\{\psi, \nu\}} \sE_{\gD \sim \chi} \sE_{\vz \sim p(\vz)} \left[ \log \frac{q_\nu (\vz|h_\psi(\gD))}{\left| \det J_\nu(\vz; h_\psi(\gD)) \right|} - \log p(g_\nu(\vz; h_\psi(\gD)), \gD) \right]
\end{align}
As shown in BayesFlow \citep{radev2020bayesflow}, the normalizing flow $g_\nu$ and the summary network $h_\psi$ can be trained simultaneously. The $\mathrm{AllInOneBlock}$ coupling block architecture of the FrEIA Python package \citep{Ardizzone2018freia}, which is very similar to the RNVP style coupling block \citep{Dinh2017rnvp}, is used as the basis for the cINN. $\mathrm{AllInOneBlock}$ combines the most common architectural components, such as ActNorm, permutation, and affine coupling operations.

For our experiments, $6$ coupling blocks define the normalizing flow network, each with a $1$ hidden-layered non-linear feed-forward subnetwork with ReLU non-linearity and $128$ hidden dimensions.