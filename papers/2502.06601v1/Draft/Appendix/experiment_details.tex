\section{Experimental Details}
\label{appdx:experiment}
Unless specified, we obtain a stream of datasets for all our experiments by simply sampling from the assumed probabilistic model, where the number of observations $n$ is sampled uniformly in the range $[64, 128]$. For efficient mini-batching over datasets with different cardinalities, we sample datasets with maximum cardinality $(128)$ and implement different cardinalities by masking out different numbers of observations for different datasets whenever required. 
% For all our experiments on supervised setups, we sample $\vx_i \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ for simplicity, but it is possible to explore other proposal distributions (e.g., heavy-tailed distributions) too. 
% In our Bayesian Neural Networks experiments, we considered a single-layered neural network with $\mathrm{Tanh}$ activation function and $32$ hidden dimensions. We considered the likelihood function as either a Gaussian or a categorical distribution using the logits, depending on regression and classification.

To evaluate both our proposed approach and the baselines, we compute an average of the predictive performances across $25$ different posterior samples for each of the $100$ fixed test datasets for all our experiments. 
That means for our proposed approach, we sample $25$ different parameter vectors from the approximate posterior that we obtain. For MCMC, we rely on $25$ MCMC samples, and for optimization, we train $25$ different parameter vectors where the randomness comes from initialization. 
For the optimization baseline, we perform a quick hyperparameter search over the space $\{0.01, 003, 0.001, 0.0003, 0.0001, 0.00003\}$ to pick the best learning rate that works for all of the test datasets and then use it to train for $1000$ iterations using the Adam optimizer~\citep{kingma2014adam}. For the MCMC baseline, we use the open-sourced implementation of Langevin-based MCMC sampling\footnote{\href{https://github.com/alisiahkoohi/Langevin-dynamics}{https://github.com/alisiahkoohi/Langevin-dynamics}} where we leave a chunk of the starting samples as burn-in and then start accepting samples after a regular interval (to not make them correlated). The details about the burn-in time and the regular interval for acceptance are provided in the corresponding experiments' sections below.

For our proposed approach of amortized inference, we do not consider explicit hyperparameter optimization and simply use a learning rate of $1\mathrm{e}\text{-}4$ with the Adam optimizer. For all experiments, we used linear scaling of the KL term in the training objectives as described in~\citep{higgins2017betavae}, which we refer to as warmup. Furthermore, training details for each experiment can be found below. 

\subsection{Fixed-Dim}
\label{appdx:details_fixed_dim}
In this section, we provide the experimental details relevant to reproducing the results of Section~\ref{sec:experiments}. All the models are trained with streaming data from the underlying probabilistic model, such that every iteration of training sees a new set of datasets. Training is done with a batch size of $128$, representing the number of datasets seen during one optimization step. Evaluations are done with $25$ samples and we ensure that the test datasets used for each probabilistic model are the same across all the compared methods, i.e., baselines, forward KL, and reverse KL. We train the amortized inference model and the forward KL baselines for the following different probabilistic models:

\textbf{Mean of Gaussian (GM):} We train the amortization models over $20,000$ iterations for both the $2$-dimensional as well as the $100$-dimensional setup. We use a linear warmup with $5000$ iterations over which the weight of the KL term in our proposed approach scales linearly from $0$ to $1$. We use an identity covariance matrix for the data-generating process, but it can be easily extended to the case of correlated or diagonal covariance-based Gaussian distributions.

\textbf{Gaussian Mixture Model (GMM):} We train the mixture model setup for $200,000$ iterations with $50,000$ iterations of warmup. We mainly experiment with $2$-dimensional and $5$-dimensional mixture models, with $2$ and $5$ mixture components for each setup. While we do use an identity covariance matrix for the data-generating process, again, it can be easily extended to other cases.
% For all our experiments, we compute the average over 25 different samples (either from the approximate posterior, or 25 different optimization runs, etc.) to report the downstream metrics. For the optimization baseline, we perform a quick hyperparameter search for each dataset over the space of $\{\}$

\textbf{Linear Regression (LR):} The amortization models for this setup are trained for $50,000$ iterations with $12,500$ iterations of warmup. The feature dimensions considered for this task are $1$ and $100$ dimensions, and the predictive variance $\sigma^2$ is assumed to be known and set as $0.25$.

\textbf{Nonlinear Regression (NLR):} We train the setup for $100,000$ iterations with $25,000$ iterations consisting of warmup. The feature dimensionalities considered are $1$-dimensional and $25$-dimensional, and training is done with a known predictive variance similar to the LR setup. For the probabilistic model, we consider both a $1$-layered and a $2$-layered multi-layer perceptron (MLP) network with 32 hidden units in each, and either a \textsc{relu} or \textsc{tanh} activation function.

\textbf{Linear Classification (LC):} We experiment with $2$-dimensional and $100$-dimensional setups with training done for $50,000$ iterations, out of which $12,500$ are used for warmup. Further, we train for both binary classification as well as a $5$-class classification setup.

\textbf{Nonlinear Classification (NLC):} We experiment with $2$-dimensional and $25$-dimensional setups with training done for $100,000$ iterations, out of which $2,5000$ are used for warmup. Further, we train for both binary classification as well as a $5$-class classification setup. For the probabilistic model, we consider both a $1$-layered and a $2$-layered multi-layer perceptron (MLP) network with 32 hidden units in each, and either a \textsc{relu} or \textsc{tanh} activation function.

\input{Draft/Tables/misspecification/linear-mlp}
\input{Draft/Tables/misspecification/gp}
\subsection{Variable-Dim}
\label{appdx:details_max_dim}
In this section, we provide the experimental details relevant to reproducing the results of Section~\ref{sec:experiments}. All the models are trained with streaming data from the underlying probabilistic model, such that every iteration of training sees a new set of datasets. Training is done with a batch size of $128$, representing the number of datasets seen during one optimization step. Further, we ensure that the datasets sampled resemble a uniform distribution over the feature dimensions, ranging from $1$-dimensional to the maximal dimensional setup. Evaluations are done with $25$ samples and we ensure that the test datasets used for each probabilistic model are the same across all the compared methods, i.e., baselines, forward KL, and reverse KL. We train the amortized inference model and the forward KL baselines for the following different probabilistic models:

\textbf{Mean of Gaussian (GM):} We train the amortization models over $50,000$ iterations using a linear warmup with $12,5000$ iterations over which the weight of the KL term in our proposed approach scales linearly from $0$ to $1$. We use an identity covariance matrix for the data-generating process, but it can be easily extended to the case of correlated or diagonal covariance-based Gaussian distributions. In this setup, we consider a maximum of $100$ feature dimensions.

\textbf{Gaussian Mixture Model (GMM):} We train the mixture model setup for $500,000$ iterations with $125,000$ iterations of warmup. We set the maximal feature dimensions as $5$ and experiment with $2$ and $5$ mixture components. While we do use an identity covariance matrix for the data-generating process, again, it can be easily extended to other cases.
% For all our experiments, we compute the average over 25 different samples (either from the approximate posterior, or 25 different optimization runs, etc.) to report the downstream metrics. For the optimization baseline, we perform a quick hyperparameter search for each dataset over the space of $\{\}$

\textbf{Linear Regression (LR):} The amortization models for this setup are trained for $100,000$ iterations with $25,000$ iterations of warmup. The maximal feature dimension considered for this task is $100$-dimensional, and the predictive variance $\sigma^2$ is assumed to be known and set as $0.25$.

\textbf{Nonlinear Regression (NLR):} We train the setup for $250,000$ iterations with $62,500$ iterations consisting of warmup. The maximal feature dimension considered is $100$-dimensional, and training is done with a known predictive variance similar to the LR setup. For the probabilistic model, we consider both a $1$-layered and a $2$-layered multi-layer perceptron (MLP) network with 32 hidden units in each, and either a \textsc{relu} or \textsc{tanh} activation function.

\textbf{Linear Classification (LC):} We experiment with a maximal $100$-dimensional setup with training done for $100,000$ iterations, out of which $25,000$ are used for warmup. Further, we train for both binary classification as well as a $5$-class classification setup.

\textbf{Nonlinear Classification (NLC):} We experiment with a maximal $100$-dimensional setup with training done for $250,000$ iterations, out of which $62,500$ are used for warmup. Further, we train for both binary classification as well as a $5$-class classification setup. For the probabilistic model, we consider both a $1$-layered and a $2$-layered multi-layer perceptron (MLP) network with 32 hidden units in each, and either a \textsc{relu} or \textsc{tanh} activation function.

\input{Draft/Figures/dimension_trends}
\subsection{Model Misspecification}
\label{appdx:details_misspecification}
In this section, we provide the experimental details relevant to reproducing the results of Section~\ref{sec:experiments}.
All models during this experiment are trained with streaming data from the currently used dataset-generating function $\chi$, such that every iteration of training sees a new batch of datasets. Training is done with a batch size of $128$, representing the number of datasets seen during one optimization step. Evaluation for all models is done with $10$ samples from each dataset-generator used in the respective experimental subsection and we ensure that the test datasets are the same across all compared methods, i.e., baselines, forward KL, and reverse KL.

\textbf{Linear Regression Model:} The linear regression amortization models are trained following the training setting for linear regression fixed dimensionality, that is, $50,000$ training iterations with $12,500$ iterations of warmup. The feature dimension considered for this task is $1$-dimension. The model is trained separately on datasets from three different generators $\chi$: linear regression, nonlinear regression, and Gaussian processes, and evaluated after training on test datasets from all of them.
For training with datasets from the linear regression probabilistic model, the predictive variance $\sigma^2$ is assumed to be known and set as $0.25$. 
The same variance is used for generating datasets from the nonlinear regression dataset generator with $1$ layer, $32$ hidden units, and \textsc{tanh} activation function. 
Lastly, datasets from the Gaussian process-based generator are sampled similarly, using the GPytorch library~\cite{gardner2018gpytorch}, where datasets are sampled of varying cardinality, ranging from $64$ to $128$. We use a zero-mean Gaussian Process (GP) with a unit lengthscale radial-basis function (RBF) kernel serving as the covariance matrix. Further, we use a very small noise of $\sigma^2 = 1\mathrm{e}^{-6}$ in the likelihood term of the GP.
Forward KL training in this experiment can only be done when the amortization model and the dataset-generating function are the same: when we train on datasets from the linear regression-based $\chi$. Table \ref{tab:misspec_model} provides a detailed overview of the results.


\textbf{Nonlinear Regression Models:} The nonlinear regression amortization models are trained following the training setting for nonlinear regression fixed dimensionality, that is, $100,000$ training iterations with $25,000$ iterations of warmup. Here, we consider two single-layer perceptions with 32 hidden units with a \textsc{tanh} activation function. The feature dimensionality considered is $1$ dimension.
We consider the same dataset-generating functions as in the misspecification experiment for a linear regression model above. However, the activation function used in the nonlinear regression dataset generator matches the activation function of the currently trained amortization model. In this case, forward KL training is possible in the two instances when trained on datasets from the corresponding nonlinear regression probabilistic model. A more detailed overview of the results can be found in Table \ref{tab:misspec_model} and \ref{tab:misspec_gp}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Draft/Plots/real_world/linear_regression_Vanilla.pdf}
    \caption{\textbf{Tabular Experiments $|$ Linear Regression with Diagonal Gaussian}: For every regression dataset from the OpenML platform considered, we initialize the parameters of a linear regression-based probabilistic model with the amortized inference models which were trained with a diagonal Gaussian assumption. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \label{fig:regression_linear_vanilla}
\end{figure}

\subsection{Tabular Experiments}
\label{appdx:details_tabular}
For the tabular experiments, we train the amortized inference models for (non-)linear regression (NLR/LR) as well as (non-)linear classification (NLC/LC) with $\vx \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ as opposed to $\vx \sim \gU(-\mathbf{1}, \mathbf{1})$ in the dataset generating process $\chi$, with the rest of the settings the same as \textsc{maximum-dim} experiments. For the nonlinear setups, we only consider the \textsc{relu} case as it has seen predominant success in deep learning. Further, we only consider a 1-hidden layer neural network with 32 hidden dimensions in the probabilistic model. 

After having trained the amortized inference models, both for forward and reverse KL setups, we evaluate them on real-world tabular datasets. We first collect a subset of tabular datasets from the OpenML platform as outlined in Appendix~\ref{appdx:datasets}. Then, for each dataset, we perform a 5-fold cross-validation evaluation where the dataset is chunked into $5$ bins, of which, at any time, $4$ are used for training and one for evaluation. This procedure is repeated five times so that every chunk is used for evaluation once.

For each dataset, we normalize the observations and the targets so that they have zero mean and unit standard deviation. For the classification setups, we only normalize the inputs as the targets are categorical. For both forward KL and reverse KL amortization models, we initialize the probabilistic model from samples from the amortized model and then further finetune it via dataset-specific maximum a posteriori optimization. We repeat this setup over $25$ different samples from the inference model. In contrast, for the optimization baseline, we initialize the probabilistic models' parameters from $\gN(0, I)$, which is the prior that we consider, and then train 25 such models with maximum a posteriori objective using Adam optimizer. 

While we see that the amortization models, particularly the reverse KL model, lead to much better initialization and convergence, it is important to note that the benefits vanish if we initialize using the Xavier-init initialization scheme. However, we believe that this is not a fair comparison as it means that we are considering a different prior now, while the amortized models were trained with $\gN(0, I)$ prior. We defer the readers to the section below for additional discussion and experimental results.
