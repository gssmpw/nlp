\section{Probabilistic Models}
\label{appdx:probabilistic_models}
This section details the various candidate probabilistic models used in our experiments for amortized computation of Bayesian posteriors over the parameters. Here, we explain the parameters associated with the probabilistic model over which we want to estimate the posterior and the likelihood and prior that we use for experimentation.

\textbf{Mean of Gaussian (GM):} As a proof of concept, we consider the simple setup of estimating the posterior distribution over the mean of a Gaussian distribution $p(\mmu | \gD)$ given some observed data. In this case, prior and likelihood defining the probabilistic model $p(\vx, \mtheta)$ (with $\mtheta$ being the mean $\mmu$) are given by:
\begin{align}
    p(\mmu) &= \gN\left(\mmu | \mathbf{0}, \mathbf{I}\right)\\
    p(\vx | \mmu) &= \gN\left(\vx | \mmu, \mSigma\right) 
\end{align}
and $\mSigma$ is known beforehand and defined as a unit variance matrix. 

\input{Draft/Tables/variable/gaussian}
\input{Draft/Tables/variable/linear}
\input{Draft/Tables/variable/nlr_relu}
\input{Draft/Tables/variable/nlr_tanh}
\input{Draft/Tables/variable/nlc_relu_2cl}
\input{Draft/Tables/variable/nlc_relu_5cl}
\input{Draft/Tables/variable/nlc_tanh_2cl}
\input{Draft/Tables/variable/nlc_tanh_5cl}
\textbf{Linear Regression (LR):} We then look at the problem of estimating the posterior over the weight vector for Bayesian linear regression given a dataset $p(\vw, b | \gD)$, where the underlying model $p(\gD, \mtheta)$ is given by:
\begin{align}
    p(\vw) &= \gN(\vw | \mathbf{0}, \mathbf{I})\\
    p(b) &= \gN(b | 0, 1)\\
    p(y | \vx, \vw, b) &= \gN\left(y | \vw^T\vx + b, \sigma^2\right) \, ,
\end{align}
and with $\sigma^2 = 0.25$ known beforehand. Inputs $\vx$ are generated from $p(\vx) = \gN(\mathbf{0}, I)$.


\textbf{Linear Classification (LC):}
We now consider a setting where the true posterior cannot be obtained analytically as the likelihood and prior are not conjugate. In this case, we consider the underlying probabilistic model by:
\begin{align}
    p(\mW) &= \gN\left(\mW | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mW) &= \mathrm{Categorical}\left(y  \;\vline\; \frac{1}{\tau}\;\mW\vx\right)\, ,
\end{align}
where $\tau$ is the known temperature term which is kept as $0.1$ to ensure peaky distributions, and $\vx$ is being generated from $p(\vx) = \gN(\mathbf{0}, I)$.


\textbf{Nonlinear Regression (NLR):}
Next, we tackle the more complex problem where the posterior distribution is multi-modal and obtaining multiple modes or even a single good one is challenging. For this, we consider the model as a Bayesian Neural Network (BNN) for regression with fixed hyper-parameters like the number of layers, dimensionality of the hidden layer, etc. Let the BNN denote the function $f_\mtheta$ where $\mtheta$ are the network parameters such that the estimation problem is to approximate $p(\mtheta | \gD)$. Then, for regression, we specify the probabilistic model using:
\begin{align}
    p(\mtheta) &= \gN\left(\mtheta | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mtheta) &= \gN\left(y | f_\mtheta(\vx), \sigma^2\right) \, ,
\end{align}
where $\sigma^2 = 0.25$ is a known quantity and $\vx$ being generated from $p(\vx) = \gN(\mathbf{0}, I)$.
 
\textbf{Nonlinear Classification (NLC):}
Like in Nonlinear Regression, we consider BNNs with fixed hyper-parameters for classification problems with the same estimation task of approximating $p(\mtheta | \gD)$. In this formulation, we consider the probabilistic model as:
\begin{align}
    p(\mtheta) &= \gN\left(\mtheta | \mathbf{0}, \mathbf{I}\right)\\
    p(y | \vx, \mtheta) &= \mathrm{Categorical}\left(y \;\vline\; \frac{1}{\tau}\;f_\mtheta(\vx)\right)
\end{align}
where $\tau$ is the known temperature term which is kept as $0.1$ to ensure peaky distributions, and $\vx$ is being generated from $p(\vx) = \gN(\mathbf{0}, I)$.

\textbf{Gaussian Mixture Model (GMM):}
While we have mostly looked at predictive problems, where the task is to model some predictive variable $y$ conditioned on some input $\vx$, we now look at a well-known probabilistic model for unsupervised learning, Gaussian Mixture Model (GMM), primarily used to cluster data. Consider a $K$-cluster GMM with:
\begin{align}
    p(\mmu_k) &= \gN\left(\mmu_k | \mathbf{0}, \mathbf{I}\right)\\
    p(\vx | \mmu_{1:K}) &= \sum_{k=1}^K \pi_k \gN\left(\vx | \mmu_k, \mSigma_k\right) \, .
\end{align}
 We assume $\mSigma_k$ and $\pi_k$ to be known and set $\mSigma_k$ to be an identity matrix and the mixing coefficients to be equal, $\pi_k = 1/K$, for all clusters $k$ in our experiments. 