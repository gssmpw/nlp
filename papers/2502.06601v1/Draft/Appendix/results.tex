\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Draft/Plots/real_world/nonlinear_regression_Vanilla.pdf}
    \caption{\textbf{Tabular Experiments $|$ Nonlinear Regression with Diagonal Gaussian}: For every regression dataset from the OpenML platform considered, we initialize the parameters of a nonlinear regression-based probabilistic model with the amortized inference models which were trained with a diagonal Gaussian assumption. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}
    \label{fig:regression_nonlinear_vanilla}
\end{figure}
\section{Additional Experiments}
\label{appdx:results}
In this section, we outline the additional experiments we conducted in obtaining Bayesian posteriors for the different probabilistic models for different hyperparameters and their downstream uses. We provide a comprehensive account of the results in the relevant sections below.

\subsection{Fixed-Dim}
\label{appdx:results_fixdim}
While we highlighted the results with the Gaussian mixture model and classification settings with only 2 clusters/classes, we also conducted experiments with an increased number of clusters and classes, making the problem even more challenging. Tables~\ref{tab:apdx_gaussian}-\ref{tab:apdx_nlc_5cl} shows that both forward and reverse KL methods perform reasonably, with forward KL struggling more in challenging scenarios.

Next, we also consider harder tasks based on the Bayesian Neural Network (BNN) paradigm, where we consider nonlinear regression and classification setups with different activation functions: \textsc{tanh} and \textsc{relu} for a 1-layered and 2-layered BNN. We provide the results of our experiments in Tables~\ref{tab:apdx_gaussian}-\ref{tab:apdx_nlc_5cl}. The results indicate that forward KL approaches struggle a lot in such scenarios, often achieving performance comparable to random chance. On the contrary, we see that reverse KL-based amortization leads to performances often similar to dataset-specific optimization, thereby showing the superiority of our proposed method.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Draft/Plots/real_world/nonlinear_regression_Flow.pdf}
    \caption{\textbf{Tabular Experiments $|$ Nonlinear Regression with Normalizing Flow}: For every regression dataset from the OpenML platform considered, we initialize the parameters of a nonlinear regression-based probabilistic model with the amortized inference models which were trained with a normalizing flow-based model. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}
\label{fig:regression_nonlinear_flow}
\end{figure}
\subsection{Variable-Dim}
\label{appdx:results_maxdim}
Our experiments on variable dimensional datasets can be evaluated for arbitrary feature cardinality, of which we show a few examples in Section~\ref{sec:experiments}. In this section, we provide results for additional dimensionality setups. In particular, we refer the readers to Tables~\ref{tab:variable_apdx_gaussian}-\ref{tab:variable_apdx_nlc_5cl}, which contain experimental results w.r.t different dimensionalities (e.g. 50D setup), as well as different number of clusters and classes, respectively, for the GMM and LC setup. Throughout, we see that amortization leads to reasonable performance, and in particular, we see forward KL-based amortization starting to struggle in high-dimensional setups.

Again, to make the setup more challenging, we consider the Bayesian Neural Network (BNN) setup where we consider nonlinear regression and classification with different activation functions: \textsc{tanh} and \textsc{relu} for a 1-layered and 2-layered BNN, but which can now be tested for an arbitrary number of input features. Our experiments are highlighted in Tables~\ref{tab:variable_apdx_gaussian}-\ref{tab:variable_apdx_nlc_5cl}, for 1- and 2-layered BNN, among others. In such complex multi-modal and complicated setups, forward KL often performs comparable to random chance and thus does not lead to any good approximation of the true posterior distribution. On the other hand, our proposed method indeed leads to good predictive performance, often comparable to dataset-specific optimization routines.

\subsection{Model Misspecification}
\label{appdx:results_missspecification}
As a representative of the results on model misspecification (Section \ref{sec:experiments}), we highlighted training and evaluation of the amortization models with Transformer backbone on a subset of in-distribution and OoD data-generating functions (Table \ref{tab:misspecification}) to show superiority in generalization of reverse KL trained system vs. forward KL based ones on OoD data but also to highlight that training a misspecified amortization model on OoD datasets directly with our approach results in even better posterior predictive performance.

\subsection{Tabular Experiments}
\label{appdx:results_tabular}
As a case of extreme OoD generalization, we test our amortized models trained to handle variable feature dimensions on the suite of regression and classification problems that we filtered out from the OpenML platform, as outlined in Appendix~\ref{appdx:datasets}. We consider both linear and nonlinear probabilistic models to tackle the regression and binary classification setups, which lead to predicting the parameters of a linear regression/classification model and a small nonlinear neural network based on \textsc{relu} activation function. Further, we also perform the analysis with a diagonal Gaussian assumption and a normalizing flow-based amortization model trained with both a forward and reverse KL objective. We provide the results on the regression problems in (a) linear model with diagonal Gaussian assumption (Figure~\ref{fig:regression_linear_vanilla}), (b) linear model with normalizing flow (Figure~\ref{fig:regression_linear_flow}), (c) nonlinear model with diagonal Gaussian assumption (Figure~\ref{fig:regression_nonlinear_vanilla}), and (d) nonlinear model with normalizing flow (Figure~\ref{fig:regression_nonlinear_flow}). The results of the classification problems are shown in (a) linear model with diagonal Gaussian assumption (Figure~\ref{fig:classification_linear_vanilla}), (b) linear model with normalizing flow (Figure~\ref{fig:classification_linear_flow}), (c) nonlinear model with diagonal Gaussian assumption (Figure~\ref{fig:classification_nonlinear_vanilla}), and (d) nonlinear model with normalizing flow (Figure~\ref{fig:classification_nonlinear_flow}).
Our experiments indicate that initializing with amortized models leads to better performance and training than models trained via maximum a-posteriori approach and initialized with the prior, i.e., $\gN(0, I)$. 

We do provide an additional baseline of initializing with \textsc{Xavier-init} initialization, which often leads to faster convergence; however, as we consider the prior to be a unit normal, this is an unfair baseline as we assume the weights to be initialized from a different prior. We leave the work of computing Bayesian posteriors with different priors and testing an amortized Bayesian model with \textsc{Xavier-init} prior for the future.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{Draft/Plots/real_world/linear_classification_Vanilla.pdf}
    \caption{\textbf{Tabular Experiments $|$ Linear Classification with Diagonal Gaussian}: For every classification dataset from the OpenML platform considered, we initialize the parameters of a linear classification-based probabilistic model with the amortized inference models which were trained with a diagonal Gaussian assumption. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}\label{fig:classification_linear_vanilla}
\end{figure}

\begin{figure}
    \centering    \includegraphics[width=1.\textwidth]{Draft/Plots/real_world/linear_classification_Flow.pdf}
    \caption{\textbf{Tabular Experiments $|$ Linear Classification with Normalizing Flow}: For every classification dataset from the OpenML platform considered, we initialize the parameters of a linear classification-based probabilistic model with the amortized inference models which were trained with a normalizing flow-based model. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}
    \label{fig:classification_linear_flow}
\end{figure}
In addition to those experiments, we also conducted a broader range of experiments utilizing DeepSets as the backbone, various OoD data-generating functions for training and evaluation of the reverse KL system, and an additional nonlinear regression model with \textsc{relu} activation function. For a comprehensive description of these experiments and the complete setup, please refer to Section \ref{appdx:details_misspecification}.
We considered two probabilistic models, including a linear regression model and a nonlinear regression models utilizing the \textsc{tanh} activation function. The detailed results for each model can be found in Tables \ref{tab:misspec_model} and \ref{tab:misspec_gp}.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{Draft/Plots/real_world/nonlinear_classification_Vanilla.pdf}
    \caption{\textbf{Tabular Experiments $|$ Nonlinear Classification with Diagonal Gaussian}: For every classification dataset from the OpenML platform considered, we initialize the parameters of a nonlinear classification-based probabilistic model with the amortized inference models which were trained with a diagonal Gaussian assumption. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}
    \label{fig:classification_nonlinear_vanilla}
\end{figure}
In all experiments, reverse KL outperforms forward KL trained amortization models in in-distribution performance and excels in posterior prediction on OoD datasets. 
Although the significant difference in posterior prediction performance of forward vs. reverse KL in cases where the underlying model is nonlinear was already mentioned in previous experiments, here, reverse KL-trained models also excel in evaluations of posterior prediction for the linear regression model.
Although only by a margin, in the case of approximating the posterior of the simpler linear regression model, a diagonal Gaussian-shaped posterior shows the best posterior prediction results when evaluated on OoD datasets from the nonlinear regression dataset generating function.
In almost all other experiments, the posterior prediction performance could be enhanced when we used the normalizing flow based posterior. 
A definitive conclusion cannot be drawn regarding the superiority of one backbone over the other, i.e. between DeepSets or Transformer. However, amortization models with DeepSets as the backbone tend towards better generalization regarding OoD datasets.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{Draft/Plots/real_world/nonlinear_classification_Flow.pdf}
    \caption{\textbf{Tabular Experiments $|$ Nonlinear Classification with Normalizing Flow}: For every classification dataset from the OpenML platform considered, we initialize the parameters of a linear classification-based probabilistic model with the amortized inference models which were trained with a normalizing flow-based model. The parameters are then further trained with maximum-a-posteriori (MAP) estimate with gradient descent. Reverse and Forward KL denote initialization with the correspondingly trained amortized model. Prior refers to a MAP-based optimization baseline initialized from the prior $\gN(0, I)$, whereas Xavier refers to initialization from the Xavier initialization scheme.}
    \vspace{-5mm}
    \label{fig:classification_nonlinear_flow}
\end{figure}