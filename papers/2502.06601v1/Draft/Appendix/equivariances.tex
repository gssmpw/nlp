\section{Architectures respecting Exchangeability}
\label{appdx:exchangeability}
In this section, we highlight how DeepSets and Transformer models satisfy the dataset exchangeability criteria, which is essential in modeling the posterior distribution over the parameters of any probabilistic model relying on \textit{iid} data. 

\subsection{DeepSets}
DeepSets~\citep{Zaheer2017deepsets} operate on arbitrary sets $\gX = \{x_1, ... x_N\} \subset \mathbb{R}^d$ of fixed dimensionality $d$ by first mapping each individual element $\vx_i \in \gX$ to some high-dimensional space using a nonlinear transform, which is parameterized as a multi-layered neural network with parameters $\varphi_1$
\begin{align}
    \vz_i = f_{\varphi_1}(\vx_i)
\end{align}
After having obtained this high-dimensional embedding of each element of the set, it applies an aggregation function $a(\cdot)$, which is a permutation invariant function that maps a set of elements $\gZ = \{\vz_1, ..., \vz_N\} \in \mathbb{R}^z$ to an element $\vh \in \mathbb{R}^z$,
\begin{align}
    \vh = a(\gZ)
\end{align}
Thus, the outcome does not change under permutations of $\gZ$. Finally, another nonlinear transform, parameterized by a multi-layered neural network with parameters $\varphi_2$, is applied to the outcome $\vh$ to provide the final output.
\begin{align}
    \vo = g_{\varphi_2}(\vh)
\end{align}
For our experiments, we then use the vector $\vo$ to predict the parameters of a parametric family of distributions (e.g., Gaussian or Flows) using an additional nonlinear neural network. As an example, for the Gaussian case, we consider the distribution $\gN(\cdot | \mmu, \mSigma)$, where
\begin{align}
    \mmu:= \mmu_{\varphi_3}(\vo) \quad\text{and}\quad\mSigma := \mSigma_{\varphi_4}(\vo)
\end{align}
which makes $\mmu$ implicitly a function of the original input set $\gX$. To understand why the posterior distribution modeled in this fashion does not change when the inputs are permuted, let us assume that $\Pi$ is a permutation over the elements of $\gX$. If we look at one of the parameters of the posterior distribution, e.g., $\mmu$, we can see that
\begin{align}
    \mmu(\Pi \gX) &= \mmu_{\varphi_3}\left(g_{\varphi_2}\left(a\left(\{f_{\varphi_1}(\vx_{\Pi(i)})\}_{i=1}^N\right)\right)\right) \\
    &= \mmu_{\varphi_3}\left(g_{\varphi_2}\left(a\left(\{f_{\varphi_1}(\vx_i)\}_{i=1}^N\right)\right)\right) \\
    &= \mmu(\gX)
\end{align}
which simply follows from the fact that $a(\cdot)$ is a permutation invariant operation, e.g., sum or mean. We can also provide similar reasoning for the other parameters (e.g., $\mSigma$). This shows that DeepSets can be used to model the posterior distribution over parameters of interest as it respects the exchangeability criteria (\textit{iid} observations) assumptions in the data through its permutation invariant structure.

% \input{ICLR/Tables/fixed_dim_extras/fixed_dim_2layer}
\subsection{Transformers}
Similarly, we can look at Transformers~\citep{vaswani2017attention} as candidates for respecting the exchangeability conditions in the data. In particular, we consider transformer systems without positional encodings and consider an additional [CLS] token, denoted by $\vc\in\mathbb{R}^d$, to drive the prediction. If we look at the application of a layer of transformer model, it can be broken down into two components.

\textbf{Multi-Head Attention}. Given a query vector obtained from $\vc$ and keys and values coming from our input set $\gX \subset \mathbb{R}^d$, we can model the update of the context $\vc$ as
\begin{align}
    \hat{\vc}(\gX) = \text{Softmax}\left(\vc^T \mW_Q \mW_K^T \mX^T\right) \mX \mW_V
\end{align}
where $\mW_Q \in \mathbb{R}^{d\times k}, \mW_K \in \mathbb{R}^{d\times k}, \mW_V \in \mathbb{R}^{d\times k}$ and $\mX \in \mathbb{R}^{N\times d}$ denotes a certain ordering of the elements in $\gX$. Further, $\hat{\vc}$ is the updated vector after attention, and Softmax is over the rows of $\mX$. Here, we see that if we were to apply a permutation to the elements in $\mX$, the outcome would remain the same. In particular
\begin{align}
    \hat{\vc}(\Pi \mX) &= \text{Softmax}\left(\vc^T \mW_Q \mW_K^T \mX^T \Pi^T\right) \Pi \mX \mW_V \\
    &= \text{Softmax}\left(\vc^T \mW_Q \mW_K^T \mX^T\right) \Pi^T\Pi \mX \mW_V \\
    &= \text{Softmax}\left(\vc^T \mW_Q \mW_K^T \mX^T\right) \mX \mW_V \\
    &= \hat{\vc}(\mX) 
\end{align}
which follows because Softmax is an equivariant function,  i.e., applying Softmax on a permutation of columns is equivalent to applying Softmax first and then permuting the columns correspondingly. Thus, we see that the update to the [CLS] token $\vc$ is permutation invariant. This output is then used independently as input to a multi-layered neural network with residual connections, and the entire process is repeated multiple times without weight sharing to simulate multiple layers. Since all the individual parts are permutation invariant w.r.t permutations on $\gX$, the entire setup ends up being permutation invariant. Obtaining the parameters of a parametric family of distribution for posterior estimation then follows the same recipe as DeepSets, with $\vo$ replaced by $\vc$.