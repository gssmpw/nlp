{
\begin{table}[t]
    \centering
    \small
    \def\arraystretch{1.25}
    \setlength{\tabcolsep}{2.5pt}
    \begin{tabular}{lcr | c | c | cc | cc | cc | cc}
        \toprule
         &  &  & \multicolumn{10}{c}{\textit{$CNLL$} ($\downarrow)$} \\

        & $q_\varphi$ & \textbf{Model} & \multicolumn{1}{c|}{\textbf{GM}} & \multicolumn{1}{c|}{\textbf{GMM}} & \multicolumn{2}{c|}{\textbf{LR}} & \multicolumn{2}{c|}{\textbf{NLR}} & \multicolumn{2}{c|}{\textbf{LC}} & \multicolumn{2}{c}{\textbf{NLC}} \\
        
        & & & \textit{100D} & \textit{5D 2 cl} & \textit{1D} & \textit{100D} & \textit{1D} & \textit{50D} & \textit{2D} & \textit{100D} &  \textit{2D} & \textit{50D}\\
        \midrule
\multirow{3}{*}{Baseline} & - & Random & $23096.4$\std{$18.5$} & $3442.7$\std{$29.0$} & $838.2$\std{$13.1$} & $38841.6$\std{$96.2$} & $11534.6$\std{$99.6$} & $306025.4$\std{$818.4$} & $110.8$\std{$0.4$} & $544.4$\std{$2.5$} & $271.3$\std{$1.8$} & $1476.6$\std{$10.8$} \\
& - & Optimization & $13630.0$\std{$0.0$} & $200.4$\std{$0.0$} & $69.2$\std{$0.0$} & $3449.5$\std{$10.2$} & $84.5$\std{$0.0$} & $53915.7$\std{$27.9$} & $8.4$\std{$0.0$} & $135.3$\std{$0.3$} & $9.9$\std{$0.1$} & $96.6$\std{$0.0$} \\
& - & MCMC & $13984.2$\std{$25.4$} & $362.5$\std{$3.7$} & $98.7$\std{$1.0$} & $5981.6$\std{$35.2$} & \textsc{N/A} & $56126.0$\std{$400.9$} & $18.5$\std{$0.2$} & $376.0$\std{$8.8$} & $35.0$\std{$12.4$} & $1951.4$\std{$59.0$} \\
\cmidrule{3-13}
\multirow{2}{*}{Fwd-KL} & \multirow{4}{*}{\rotatebox[origin=c]{90}{Gaussian}} & DeepSets & $15025.4$\std{$62.2$} & $1536.5$\std{$6.1$} & $71.4$\std{$0.2$} & $29346.1$\std{$218.6$} & $8341.6$\std{$21.6$} & $257704.2$\std{$968.5$} & $39.3$\std{$0.5$} & $538.0$\std{$0.3$} & $272.1$\std{$2.4$} & $1479.8$\std{$3.1$} \\
& & Transformer & $14013.0$\std{$1.1$} & $1560.1$\std{$0.9$} & $72.6$\std{$0.4$} & $12039.8$\std{$274.8$} & $8294.7$\std{$124.9$} & $252765.5$\std{$355.1$} & $41.2$\std{$1.0$} & $369.1$\std{$2.4$} & $218.7$\std{$2.1$} & $1235.1$\std{$5.1$} \\
\multirow{2}{*}{Rev-KL} & & DeepSets & $13825.3$\std{$3.8$} & $236.8$\std{$10.8$} & $72.9$\std{$0.7$} & $12697.4$\std{$94.1$} & $160.2$\std{$3.3$} & $85724.3$\std{$970.3$} & $14.5$\std{$0.2$} & $275.2$\std{$3.6$} & $31.1$\std{$1.0$} & $101.7$\std{$1.5$} \\
& & Transformer & \highlight{$13809.8$\std{$1.8$}} & $227.4$\std{$11.7$} & $71.7$\std{$0.9$} & $6130.9$\std{$249.0$} & \highlight{$145.7$\std{$3.1$}} & $53075.4$\std{$443.1$} & $14.0$\std{$0.2$} & $276.8$\std{$0.8$} & $32.7$\std{$1.0$} & $291.4$\std{$9.5$} \\
\cmidrule{3-13}
\multirow{2}{*}{Fwd-KL} & \multirow{4}{*}{\rotatebox[origin=c]{90}{Flow}} & DeepSets & $15064.4$\std{$67.3$} & $723.8$\std{$286.4$} & $71.6$\std{$0.5$} & $30237.0$\std{$344.1$} & $7063.0$\std{$165.3$} & $212380.1$\std{$889.6$} & $24.3$\std{$0.5$} & $537.4$\std{$2.3$} & $270.6$\std{$4.6$} & $1497.8$\std{$7.2$} \\
& & Transformer & $13986.4$\std{$4.1$} & $265.7$\std{$29.6$} & $72.2$\std{$0.6$} & $11416.0$\std{$783.6$} & $6455.1$\std{$190.4$} & $204521.5$\std{$1589.8$} & $25.4$\std{$0.5$} & $318.5$\std{$1.5$} & $189.9$\std{$4.3$} & $1027.9$\std{$1.0$} \\
\multirow{2}{*}{Rev-KL} & & DeepSets & $13836.0$\std{$15.4$} & $244.9$\std{$7.6$} & $72.4$\std{$0.4$} & $14961.8$\std{$279.6$} & $165.1$\std{$11.8$} & $91362.3$\std{$12146.9$} & $13.5$\std{$0.1$} & \highlight{$187.5$\std{$1.9$}} & $31.4$\std{$0.1$} & \highlight{$79.7$\std{$0.9$}} \\
& & Transformer & $13815.5$\std{$3.1$} & \highlight{$221.6$\std{$3.8$}} & \highlight{$71.0$\std{$0.0$}} & \highlight{$6100.9$\std{$105.1$}} & $172.7$\std{$8.7$} & \highlight{$47972.8$\std{$316.8$}} & \highlight{$13.0$\std{$0.1$}} & $222.3$\std{$0.6$} & \highlight{$28.4$\std{$0.5$}} & $143.6$\std{$2.8$} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5mm}
    \caption{\textbf{Variable-Dimension Posterior Prediction:} Experimental results for posterior inference on variable dimensional datasets evaluated on estimating the (a) mean of a Gaussian (GM), (b) means of Gaussian mixture model (GMM), (c) parameters for (non-)linear regression (NLR/LR), and (d) parameters for (non-)linear classification (NLC/LC). We consider different backbone architectures and parametric distributions $q_\varphi$, and use dataset-specific Bayesian and point estimates as baselines. CNLL refers to the negative of the expected conditional log likelihood.}
    \vspace{-4.5mm}
    \label{tab:maximum_dim}
\end{table}
}
