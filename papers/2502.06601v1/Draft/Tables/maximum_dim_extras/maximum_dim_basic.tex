\begin{table}[t]
    \centering
    \small
    \def\arraystretch{1.25}
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lcr | c | ccc | c | cccc }
        \toprule
         &  &  & \multicolumn{5}{c|}{\textit{$L_2$ Loss} ($\downarrow$)} & \multicolumn{4}{c}{\textit{Accuracy} ($\uparrow$)}\\

        & $q_\varphi$ & \textbf{Model} & \textbf{GM} & \multicolumn{3}{c|}{\textbf{GMM}} & \textbf{LR} & \multicolumn{4}{c}{\textbf{LC}} \\
        
        & & & \textit{50D} & \textit{2D-2cl} & \textit{2D-5cl} & \textit{5D-5cl} & \textit{50D} & \textit{2D-5cl} & \textit{50D-2cl} & \textit{50D-5cl} & \textit{100D-5cl} \\
        \midrule
\multirow{3}{*}{Baseline} & - & Random & $153.45$\std{$0.0$} & $2.24$\std{$0.0$} & $0.66$\std{$0.0$} & $1.63$\std{$0.0$} & $102.04$\std{$0.2$} & $19.97$\std{$0.5$} & $50.02$\std{$0.2$} & $20.02$\std{$0.1$} & $19.96$\std{$0.1$} \\
& - & Optimization & $50.51$\std{$0.0$} & $0.18$\std{$0.0$} & $0.12$\std{$0.0$} & $0.33$\std{$0.0$} & $0.76$\std{$0.0$} & $92.22$\std{$0.0$} & $79.74$\std{$0.0$} & $52.17$\std{$0.0$} & $42.58$\std{$0.0$} \\
& - & MCMC & $55.13$\std{$0.4$} & $0.34$\std{$0.0$} & $0.18$\std{$0.0$} & $0.49$\std{$0.0$} & $8.10$\std{$0.1$} & $82.39$\std{$0.3$} & $71.30$\std{$0.5$} & $38.01$\std{$0.4$} & $31.15$\std{$0.9$} \\
\cmidrule{3-12}
\multirow{2}{*}{Fwd-KL} &\multirow{4}{*}{\rotatebox[origin=c]{90}{Gaussian}} & DeepSets & $52.04$\std{$0.1$} & $1.30$\std{$0.0$} & $0.49$\std{$0.0$} & $1.21$\std{$0.0$} & $39.63$\std{$0.5$} & $20.00$\std{$0.4$} & $51.77$\std{$0.4$} & $19.95$\std{$0.0$} & $19.99$\std{$0.1$} \\
& & Transformer & $51.49$\std{$0.1$} & $1.31$\std{$0.0$} & $0.49$\std{$0.0$} & $1.21$\std{$0.0$} & $2.71$\std{$0.4$} & $63.39$\std{$1.2$} & $69.86$\std{$0.1$} & $40.38$\std{$0.2$} & $26.90$\std{$0.1$} \\
\multirow{2}{*}{Rev-KL} & & DeepSets & $51.25$\std{$0.0$} & $0.21$\std{$0.0$} & $0.14$\std{$0.0$} & $0.39$\std{$0.0$} & $20.64$\std{$0.3$} & $85.50$\std{$0.2$} & $69.81$\std{$0.1$} & $36.22$\std{$0.5$} & $27.16$\std{$0.3$} \\
& & Transformer & \highlight{$51.15$\std{$0.0$}} & \highlight{$0.20$\std{$0.0$}} & \highlight{$0.13$\std{$0.0$}} & \highlight{$0.32$\std{$0.0$}} & $2.99$\std{$0.6$} & $84.39$\std{$0.1$} & $75.66$\std{$0.1$} & $45.66$\std{$0.1$} & $32.75$\std{$0.2$} \\
\cmidrule{3-12}
\multirow{2}{*}{Fwd-KL} &\multirow{4}{*}{\rotatebox[origin=c]{90}{Flow}} & DeepSets & $52.25$\std{$0.5$} & $0.33$\std{$0.1$} & $0.20$\std{$0.0$} & $0.50$\std{$0.0$} & $40.15$\std{$0.2$} & $20.00$\std{$0.1$} & $51.18$\std{$0.1$} & $19.92$\std{$0.1$} & $20.00$\std{$0.0$} \\
& & Transformer & $51.45$\std{$0.2$} & $0.40$\std{$0.1$} & $0.25$\std{$0.0$} & $0.55$\std{$0.0$} & \highlight{$2.30$\std{$0.1$}} & $75.73$\std{$1.1$} & $74.19$\std{$0.1$} & $41.56$\std{$0.2$} & $27.09$\std{$0.1$} \\
\multirow{2}{*}{Rev-KL} & & DeepSets & $51.25$\std{$0.0$} & $0.21$\std{$0.0$} & $0.14$\std{$0.0$} & $0.38$\std{$0.0$} & $20.46$\std{$0.2$} & \highlight{$86.43$\std{$0.1$}} & $69.68$\std{$0.5$} & $25.75$\std{$0.3$} & $22.37$\std{$0.2$} \\
& & Transformer & $51.17$\std{$0.0$} & \highlight{$0.20$\std{$0.0$}} & \highlight{$0.13$\std{$0.0$}} & $0.33$\std{$0.0$} & $2.44$\std{$0.0$} & $86.37$\std{$0.2$} & \highlight{$76.71$\std{$0.1$}} & \highlight{$46.06$\std{$0.2$}} & \highlight{$32.98$\std{$0.1$}} \\
\bottomrule
    \end{tabular}
    \caption{\textbf{Variable-Dim Posterior Prediction:} Experimental results for posterior inference on variable dimensional datasets evaluated on estimating the (a) mean of a Gaussian distribution, (b) means of Gaussian mixture model (GMM), (c) parameters for linear regression (LR), and (d) parameters for linear classification (LC) for additional probabilistic model setups (eg. multi-class). We consider different backbone architectures and parametric distributions $q_\varphi$, and use dataset-specific bayesian and point estimates as baselines. $L_2$ Loss and Accuracy refer to the expected posterior-predictive $L_2$ loss and accuracy respectively. Here, cl refers to the number of clusters for GMM and number of classes for LC.}
    \label{tab:maximum_dim_basic}
\end{table}