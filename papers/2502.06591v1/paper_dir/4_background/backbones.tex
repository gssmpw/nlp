\subsection{Deep-learning Time-series Architectures}
The core module of the TTN is the localization network, $f_{\mathrm{loc}}$, which predicts the transformation parameters $\btheta$.
While in~\cite{Shapira:NIPS:2019:DTAN} we have used a simple Temporal Convolutional Neural Network (TCN), here we explore several other 
architectures.

Similar to Computer Vision and Natural Language Processing, the field of Time Series Classification (TSC) also saw 
a recent surge in DL-based classifiers. 
Traditionally, Recurrent Neural Network (RNN)~\cite{Husken:neuro:2003:recurrent,Mikolov:inter:2010:recurrent} were the 
go-to models for this task, as their time-dependent representation allowed them to capture temporal dependencies within signals.
However, leveraging the expressive power of deeper and more recent architectures allowed TCNs to outperform RNNs while offering 
a more efficient training procedure. Fawaz et al., (2018)~\cite{fawaz:2018:deep} provided an extensive review of such architectures
for the TSC task. Their findings pointed to two architectures: Fully-Convolutional Networks (FCN)~\cite{wang:2017:time} and a 1D variant of the
now quintessential Residual Network (ResNet)~\cite{He:ECCV:2016:resnet}.

Another TCN-based architecture is InceptionTime~\cite{Ismail:2020:inceptiontime}. It is a 1D variant of the Inception Network-v4~\cite{Szegedy:AAAI:2017:inception} and is composed of
 an ensemble of Inception modules.
Arguing that frequency information is lost in current TSC models, a wavelet-based neural network structure called multilevel Wavelet
Decomposition Network (mWDN) was proposed~\cite{wang:SIGKDD:multilevel:2018}. It preserves
the advantage of multilevel discrete wavelet decomposition in frequency learning while still enabling the fine-tuning of learnable parameters via 
 back-propagation. The model takes all or partial mWDN decomposed sub-series
  different frequencies as input features and updates its parameters globally for a downstream classification or a forecasting task. As different 
 features at different frequencies are used, the authors coined the integration of mWDN features and a deep-classifier, $\psi(\cdot)$, as 
 \emph{Residual Classification Flow (RCF)}. Here, we utilize the mWDN framework for times-series alignment, which could be thought as 
 \emph{Residual Alignment Flow (RAF)}. In our experiments, $\psi(\cdot)$ is an \emph{InceptionTime} module.
 
Given these recent advancements in DL for TSC, we explore the effect of the aforementioned architectures as 
the ``backbones" of the localization networks in the TTN. In~\autoref{Sec:Results} we provide an evaluation of TCN, 
RNN-FCN, mWDN, and, InceptionTime for time-series joint alignment and averaging under the DTAN framework.
