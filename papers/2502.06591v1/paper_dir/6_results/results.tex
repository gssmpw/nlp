%%%%% FIGURE %%%%%
\subimport{./paper_dir}{6_results/fig_JA_results}
%%%%%%%%%%%%%%%%%%

\section{Experiments and Results}\label{Sec:Results}
The evaluation of our approach was conducted on both synthetic and real-world data, using the popular \emph{UCR time-series classification archive} benchmark which contains 128 datasets.~\autoref{fig:JA:results} depicts JA results on some of the UCR datasets.
The rest of the section is structured as follows.
First, we evaluate the effect of predicting non-stationary velocity fields via RDTAN in~\autoref{Sec:Results:sub:RDTAN}. 
In~\autoref{Sec:Results:sub:NCC} we compare DTAN to state-of-the-art time-series JA and averaging methods on the UCR archive in a series of experiments. 
In~\autoref{Sec:Results:sub:timming} we show the computational benefits of using DTAN compared with DTW-based approaches. 
In~\autoref{Sec:Results:sub:MTDTAN} we provide new details regarding the effect of $f_{\mathrm{loc}}(\cdot)$ and multi-task learning on JA.
Finally,~\autoref{Sec:Results:sub:PCA} details how DTAN improves PCA. 


%%%%% FIGURE %%%%%
\subimport{./paper_dir}{6_results/fig_synth}
%%%%%%%%%%%%%%%%%%
\subsection{Recurrent DTANs}\label{Sec:Results:sub:RDTAN}

\autoref{fig:fig_recurrent} displays the JA of synthetic data using RDTAN.
We generated the synthetic data by perturbing four synthetic signals with random warps obtained via cumulative distribution functions sampled from a Dirichlet-distribution prior, as detailed in~\cite{Shapira:NIPS:2019:DTAN}. 
%We avoided using random CPAB warps to highlight DTAN's effectiveness in unwarping diverse transformation families.
The top row presents the original latent sequences in red, the second row the perturbed synthetic signals in gray and their average in blue, and each of the subsequent three rows illustrates the alignment achieved in successive iterations of RDTAN. All four classes (columns) are aligned using the same single model. Consistent with our earlier discussion, the latent average and the latent warps are unknown during both training and testing, yet DTAN successfully recovers the latent signals via the successful JA. 

RDTAN's performance was assessed using the same recurrence number as in training. In certain cases, RDTAN benefits from applying extra warps beyond the training count. This improvement is feasible because the learned parameters are shared across all warps, akin to standard RNNs.~\autoref{fig:rnn:compare} demonstrates the impact of increasing the number of applied warps (up to 16) on the CBF dataset (which is a part of the UCR archive~\cite{Dau:2019:ucr}), particularly in terms of NCC accuracy (defined below). The findings reveal that (a) DTAN, without recurrences, struggles with additional ones as it lacks relevant training, and (b) RDTAN either maintains robustness (RDTAN2) or shows enhanced performance (RDTAN3/4) with an increased number of recurrences.
\subimport{./paper_dir}{6_results/fig_rnn_comparison}

%
%
%
\subsection{Nearest Centroid Classification (NCC)}\label{Sec:Results:sub:NCC}
The most updated version~\cite{Dau:2019:ucr} of the UCR archive has 128  datasets with inter-dataset variability in the number of samples, signal length, application domain, and the number of classes. Eleven of those datasets also present intra-dataset variability of the signal length; such datasets are referred to as variable-length (VL) datasets.
In all of the experiments, we used the train/test splits provided by the archive. 
To quantify performances we used, as is customary, 
the NCC accuracy. This performance index is viewed as an evaluation metric for measuring how well each centroid describes its class members (and thus, implicitly, also measures the JA quality). 
The NCC framework has 2 steps: 1) compute the centroid, $\mu_k$, for each class $k$ of the \emph{train} set; 2) label each \emph{test} sample by the class of its closest centroid. As we explain below,~\autoref{table:ncc}, which summarizes the NCC results, is divided into several parts. The full results, together with many illustrative figures, train/test comparison, and additional evaluations, appear in our Supplemental Material \textbf{(SupMat)}.

In all of our DTAN experiments, training was done via the Adam optimizer~\cite{Kingma:arxiv:2014:Adam} for 1500 epochs, batch size of 64, $N_p$ (the number of subintervals in the partition of $\Omega$) was 16, and the scaling-and-squaring parameter (used by \texttt{DIFW}) was 8. These values were previously reported to yield the highest number of \emph{Wins} in~\cite{Martinez:ICML:2022:closed}. 
While in~\cite{Shapira:NIPS:2019:DTAN} we have used RDTAN, in~\citet{Martinez:ICML:2022:closed} the authors stacked TCNs sequentially. In this study, we fixed the number of recurrences to 4 as we did not find it necessary to stack InceptionTime models.
The PyTorch \texttt{TSAI} implementation of the InceptionTime was taken from~\cite{Ignacio:tsai}.
For DTW, DBA, and SoftDTW we used the \texttt{tslearn} package~\cite{tavenard:2017:tslearn}. 
\subsubsection{Part 1: 84 datasets -- allowing an extensive HP search (previously-reported results).}
%%%%% TABLE %%%%%
\subimport{./tables/}{table_results_icml.tex}
%%%%%%%%%%%%%%%%%
An older version~\cite{Chen:UCR:Archive:2015} of the UCR archive had only 85 datasets (a subset of the 128 mentioned above). Several previous works reported results on only 84 datasets out of those 85, possibly due to the size of the largest dataset.
Part 1 of~\autoref{table:ncc} contains the results, on those 84 datasets, obtained by several key methods, as reported by their authors, as well as those obtained by a simple Euclidean averaging (\ie, a no-alignment baseline). The methods are DBA, SoftDTW, DTAN$_{\mathrm{libcpab}}$, ResNet-TW, and  DTAN$_{\mathrm{DIFW}}$. 
The regularization-free DBA requires no HP configurations. 
The SoftDTW methods have one HP for controlling the smoothness. Their results, reported in~\cite{Blondel:2021:differentiable}, were obtained by those authors using cross-validation. 
The other works~\cite{Shapira:NIPS:2019:DTAN,huang:2021:residual,Martinez:ICML:2022:closed}
reported only their best results
across different configurations. 
In~\cite{Shapira:NIPS:2019:DTAN} we have evaluated  DTAN$_\mathrm{libcpab}$ using 12 different configurations per dataset
(4 configurations for $(\lambda_{\sigma}, \lambda_{\mathrm{smooth}})$ and 3 different numbers of recurrences). In~\cite{huang:2021:residual}, ResNet-TW used the same regularization configurations
as in~\cite{Shapira:NIPS:2019:DTAN}, but also tested varying numbers of ResNet blocks (4 to 8) per dataset.
\citet{Martinez:ICML:2022:closed} evaluated  DTAN$_{\mathrm{DIFW}}$ using 96 different configurations 
(various options of $\lambda_{\sigma}, \lambda_{\mathrm{smooth}}, N_p, \#$stacked TCNs, boundary conditions, and the scaling-and-squaring parameter) per dataset. 
We note that: 1) tuning $N_p$ and the boundary conditions is another form of tweaking
the regularization; 2) as stated in (the supplemental material of)~\cite{Martinez:ICML:2022:closed}, their reported results were chosen among those 96 configurations, per dataset, based on the best performance on the test set.

\subsubsection{Part 2: Regularization vs regularization-free DTAN}
Part 1 of~\autoref{table:ncc} suggests that increasing the number of tried HP configurations translates to better performance due to the large variability across the UCR datasets. 
However, the compact summary in Part 1 of~\autoref{table:ncc} also hides an inconvenient truth:
there is no \emph{one-size-fits-all} configuration. For example, ~\citet{Martinez:ICML:2022:closed} produced the best performance but this is largely due to an expensive search over a large number of HP configurations.
In fact, inspecting the full results of either DTAN$_{\mathrm{libcpab}}$, ResNet-TW, or DTAN$_{\mathrm{DIFW}}$, reveals that 
the optimal choice of HP varies across the datasets and affects results drastically.  

To demonstrate this crucial point, we ran a new set of experiments. 
We picked the HP configuration 
that according to~\cite{Martinez:ICML:2022:closed}
achieved the highest number of wins among their 96 configurations.  
Next, using that configuration we ran,  on those 84 datasets, exactly the same DTAN but with 3 different losses:
1) WCSS plus the smoothness regularization ($\lambda_{\sigma}$ and $\lambda_{\mathrm{smooth}}$, 0.001 and 0.1, respectively);
2) our proposed $\Lcal_{\mathrm{ICAE}}$;
3) our proposed $\Lcal_{\mathrm{ICAE-triplet}}$.
In the last 2 cases, which are regularization-free, 
the values of $\lambda_{\sigma}$ and $\lambda_{\mathrm{smooth}}$ from that configuration were ignored. 
In all 3 cases, we used DTAN$_{\mathrm{DIFW}}$
with the same InceptionTime backbone~\cite{Ignacio:tsai} (in all 3 cases this gave better results than using a TCN). 
To account for random initializations and the stochastic nature of 
DL training,
in each of the 3 cases we performed 5 runs on each dataset and report both the median and best results;
see part 2 in~\autoref{table:ncc}.
The results illustrate the merits of our regularization-free approach:
a single HP configuration for the regularization, even the one stated as the best, does not properly fit the entirety of the UCR datasets. 
In contrast, dropping the regularization term and using our $\Lcal_{\mathrm{ICAE}}$ increases performance by a large margin, which is only further increased when utilizing $\Lcal_{\mathrm{ICAE-triplet}}$, 
which increases separability between class centroids (a feat current DTW-based methods are incapable of) and achieves SOTA results. 

\subsubsection{Part 3 \& 4: Using a single HP configuration in all of the 128 datasets.}
To produce the results in part 3 of~\autoref{table:ncc}, we again repeated the procedure from part 2, 
except that 1) we added another case where the loss is only WCSS with no regularization,
and 2) the results, on 117 datasets, also take into account additional fixed-length datasets that were added
in the newer UCR archive. 
The results in, and conclusions from,  Part 3 are consistent with Part 2. WCSS did slightly better than WCSS+Reg, probably since even though it distorts the signals, it makes it a bit easier (than in the WCSS+Reg case) to differentiate between classes. In any case, our losses outperform both of these methods. \textbf{Part 4} extends the results of Part 3 by adding, for the DTANs with our proposed losses, the 11 VL datasets (for a total of 128). 

%%%%% TABLE %%%%%
\subimport{./}{tables/table_timing.tex}
%%%%%%%%%%%%%%%%%

\subsection{Computation-time Comparison}\label{Sec:Results:sub:timming}
A key advantage of learning-based approaches is fast inference on new data. 
We performed several timing comparisons
between DBA, SoftDTW (whose HP, $\gamma\in\{0.01,0.1,1\}$, must be searched in each dataset), and DTAN, trained with the proposed $\Lcal_{\mathrm{ICAE}}$. We used a machine with 12 CPU cores, 32Gb RAM, and an RTX 3090 GPU card. We chose a subset of the UCR archive, spanning different lengths and sample sizes, and compared the time it took to compute the centroids on the entire train set. Since DBA and SoftDTW are optimization-based we provide timing for two approaches: (1) barycenter computation time of a new batch ($N=30$, average of 5 runs) and (2) computing DTW/SoftDTW between the batch and its barycenter (which, after warping, can be averaged again). For DTAN, this is just the inference time. \autoref{tab:timing} presents the result. On training data, for small datasets (in terms of $n, N$), SoftDTW/DBA is faster than DTAN, but this trend is reversed for the large ones. \textbf{SoftDTW and DBA run out of memory} on the largest dataset (\texttt{HandOutlines}). During inference, using DTAN is \emph{orders of magnitude faster} (x$10$--x$10^{4}$) than recomputing barycenters, and, on the larger datasets, is x$10$ faster than computing DTW/SoftDTW. 

\subsection{Multi-task Learning and Backbone Comparison}~\label{Sec:Results:sub:MTDTAN}
%%%%%%% FIGURE %%%%%%%%
\subimport{./figures/}{backbones/backbones_bar_plot.tex}
\subimport{./paper_dir}{6_results/fig_tsne}
%\subimport{./paper_dir}{6_results/fig_rnn_to_mean}
%%%%%%%%%%%%%%%%%%%%%%%
In this section, evaluation was performed on 113 (out of 128) datasets of the updated UCR archive~\cite{Dau:2019:ucr} (\ie, using all datasets,
omitting the ones containing VL and/or too short for the max-pooling operators in some of the architectures). Again, we used the provided train/test splits given by the authors
 of the archive and used 20\% of the train set as validation for choosing the best epoch.
%\RED{Additionally, we have used the closed-formed CPAB gradient introduced by~\cite{Martinez:ICML:2022:closed}.}
The experiments in the previous sections provided an in-depth evaluation of DTAN, given a fixed $f_{\mathrm{loc}}$, across different numbers of recurrences, HP values, and objective functions. In this section, we fix
$\lambda_{\sigma}=0.1, \lambda_{\mathrm{smooth}}=0.5$, and focus on how the choice of $f_{\mathrm{loc}}$ affects DTAN's performance and the effect of multitask learning. 
To this end, we chose the following %TSC 
architectures:
\begin{enumerate}
    \item \textbf{TCN}: Temporal Convolutional Network, 
    identical to the CNN from previous sections, but with an
    adaptive average pooling operator before the penultimate layer to maintain a fixed number of parameters \wrt the input's length.
    \item \textbf{RNN-FCN:} A Recurrent Neural Network (RNN) with a hidden layer of size=100 followed by a Fully-Convolutional layer (FCN)
    with 3 blocks of [128, 256, 128] and a kernel size of [7, 5, 3] respectively.
    \item \textbf{InceptionTime}~\cite{Ismail:2020:inceptiontime}: composed of 5 Inception blocks (identical to the one used in previous sections). 
    \item \textbf{mWDN}~\cite{wang:SIGKDD:multilevel:2018}: composed of 3 Wavelet Blocks and an \emph{InceptionTime} module for the 
    \emph{Residual Alignment Flow} (RAC) framework. 
\end{enumerate}
 We have used the \emph{tsai} PyTorch implementation for RNN-FCN, mWDN, and InceptionTime~\cite{Ignacio:tsai}. 

\textbf{Results:} 
\autoref{fig:barplot} shows the average NCC test accuracy of DTAN and MT-DTAN for the different architectures on 113 datasets of the 
UCR archive~\cite{Dau:2019:ucr}.  The overall 
best performance is achieved by MT-DTAN coupled with the \emph{InceptionTime} architecture. 
This is consistent with the results presented in~\cite{Ismail:2020:inceptiontime}, where the authors show that 
\emph{InceptionTime} produced the best performance for TSC. It is therefore unsurprising that 
it presented the largest performance gain when trained in the multi-task framework, as it was specifically designed
for classification. We also note that the \emph{mWDN} architecture provides the best performance for DTAN. Overall, the results indicate the importance of the choice of $f_{\mathrm{loc}}$ when it comes to time-series JA and deep TSC architectures are a good choice for this task.

Additionally, \autoref{fig:tsne} presents the t-SNE visualization of the \textit{FacesUCR} dataset
 original data, learned embeddings, and the aligned data (using the \emph{InceptionTime} architecture).
Both DTAN and MT-DTAN alignment are sufficient for the t-SNE algorithm to provide 
adequate clustering in a 2-dimensional projection. The same cannot be said for the embeddings of the \emph{InceptionTime} model, 
as DTAN is unable to provide good separation in the latent space. In comparison, MT-DTAN can provide a good 
separation between classes for both the aligned data and its embeddings. This helps to shed light on the 
performance gains achieved by MT-DTAN compared with the original model. 

\subsection{Principal Components Analysis}~\label{Sec:Results:sub:PCA}
As discussed in~\autoref{Sec:Introduction}, time-series data pose particular issues when it comes to 
dimensionality reduction by, \eg, PCA.
While the relation between time-series data and PCA has been researched in the context of functional data analysis 
(\eg functional-PCA~\cite{dauxois:1982:asymptotic, ramsay:1991:some}) or neural computation~\cite{williams:2020:twpca}, we focus on the effect of JA
on the traditional PCA algorithm.
In particular, after JA has been learned, PCA can 
be applied to the aligned time series to produce misalignment-robust principal components (PCs).
Given a set of observations and the predicted warping parameters by DTAN, $(u_i,\btheta_i)_{i=1}^N$ respectively, 
we apply PCA on the warped data.
Given the Singular Value Decomposition (SVD) of $(v_i)_{i=1}^N$, 
\begin{align}
    (v_i)_{i=1}^N = \bP\Lambda \bQ^T= \sum_{i}^{N}\sqrt{s_i}p_{i,j}q_i\, ,
\end{align}
one can perform data reconstruction in its original domain, given the first $k$ PCs: 
\begin{align}  
    \tilde{v}_j=\sum_{i}^{k}\sqrt{s_i}p_{i,j}q_i \\
    \tilde{u}_j=\tilde{v}_j\circ {T^{-\btheta_j}}
\end{align} 

To evaluate the effect of DTAN JA on dimensionality reduction we provide results on the Trace dataset as a case study.~\autoref{fig:pca} (top) shows the cumulative explained variance by 
the first 10 PCs on both the original and aligned data. Since DTAN is set to minimize the within-class variance
by reducing temporal variability, fewer PCs are required to explain the overall variance of the entire set \wrt the original data. This is also reflected in~\autoref{fig:pca} middle-to-bottom panels, where 1) the first three PCs are presented and 2) 
the reconstruction, performed by projecting the aligned data onto the first 6 PCs. 
\textbf{The first PC of the aligned data already explains $95.7\%$ of the variance while the first three PCs of the original 
data combined explain only $88.9\%$.}
The bottom panels demonstrate that, in contrast to the misaligned original data, 6 PCs adequately reconstruct the aligned data with high fidelity and also serve as a denoising procedure, suggesting that PCA and similar dimensionality reduction methods could be enhanced by DTAN.
