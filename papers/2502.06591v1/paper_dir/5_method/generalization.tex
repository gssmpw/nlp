Once the model is trained, a signal $u$ (regardless whether it is a training or a test signal) is aligned as follows. 
 First set $\btheta=f_{\mathrm{loc}}(u)$; \ie, a forward pass of the net (an operation which is, as is usually the case in DL, simple and very fast). Next, obtain the aligned signal, $v$, by warping $u$ by $T^\btheta$; \ie, set $v=u\circ T^\btheta$. 
 Especially useful and elegant is the fact that, in the multi-class case, the same single net aligns 
 each new test signal, without knowing the label of the latter. This is in sharp contrast
 to other joint-alignment methods (\eg, those based on DBA, SoftDTW, atlases, \etc.)
 that require knowing the label of the to-be-aligned signal. 
