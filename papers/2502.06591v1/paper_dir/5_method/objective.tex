%%%%%% FIGURE %%%%%
\subimport{./}{5_method/fig_icae_example.tex}
%%%%%%%%%%%%%%%%%%%
Let $u_i$ denote an input signal, let 
$\btheta_i=\btheta_i(u_i,\bw)=
f_{\mathrm{loc}}(u_i,\bw)$
denote the corresponding output of the localization net $f_{\mathrm{loc}}(\cdot,\bw)$
of weights $\bw$,
and let $v_i$, the warped signal, 
denote the result of warping $u_i$ by $T^{\btheta_i}\in\Tcal$;
\ie, $v_i = v_i(u_i,\bw) = u_i \circ T^{\btheta_i}$.
%
Consider first the case where all the $u_i$'s belong to the same class.
As the variance of the observed $(u_i)_{i=1}^N$ is (at least partially) explained by the latent warps, $(T^{\btheta_i})_{i=1}^N$, 
we seek to minimize the empirical variance of the collection of the warped signals, $(v_i)_{i=1}^N$.
In other words, our data term in this setting is
\begin{align}
\label{eqn:alinment_loss}
 \Lcal_{\mathrm{data}} 
 &\triangleq \tfrac{1}{N} \sum\nolimits_{i=1}^{N} \ellTwoNorm{v_i - \tfrac{1}{N}\sum\nolimits_{j=1}^{N}v_j}^{2} \nonumber \\
 &= \tfrac{1}{N} \sum\nolimits_{i=1}^{N} \ellTwoNorm{u_i \circ T^{\btheta_i} - \tfrac{1}{N}\sum\nolimits_{j=1}^N u_j \circ T^{\btheta_j}}^{2} 
 \nonumber \\  &
 = \tfrac{1}{N} \sum\nolimits_{i=1}^{N} \ellTwoNorm{u_i \circ T^{\btheta_i} - \mu}^{2} \nonumber \\
%  \quad v_i(u_i;\bw ) = u_i \circ T^{\btheta_i(u_i,\bw)}
 %\, ,
\end{align}
where  $\ellTwoNorm{\cdot}$ is the $\ell_2$ norm and $\mu$ is the post-alignment average signal.
Note this setting is unsupervised. 

In the multi-class case, $\Lcal_{\mathrm{data}}$ is the sum of the within-class variances, often
called the within-class sum of squares (WCSS):
\begin{align}
\Lcal_{\mathrm{data}} &\triangleq \sum_{k=1}^K 
 \frac{1}{N_k}\sum_{i:y_i=k} \bigg\|u_i \circ T^{\btheta_i} -   \mu_k \bigg\|_{\ell _2}^{2}
\end{align}
where $K$ is the number of classes, $y_i$ takes values in $\set{1,\ldots,K}$
and is the class label associated with $u_i$ (namely: $y_i=j$ if and only if $u_i$ belongs to class $j$), and $N_k=
|\set{i:y_i=j}|$ is the number of examples in class $j$.
In this setting, the learning is partially (or weakly-)supervised: 
the labels, $(y_i)_{i=1}^N$ are known during the learning (but not during the test)
 while the within-class alignment remains unsupervised as in the single-class case. 
The same single network is responsible for aligning 
 each of the classes; \ie, $\bw$ does not vary with $k$. 

Of importance is the fact that, unfortunately, it is possible to reduce $\Lcal_{\mathrm{data}}$ (even to zero!) by severely distorting
the signals such that most of the inter-signal variability concentrates on a small region of the domain and this issue only worsens due to interpolation artifacts.
We now present two complementary methods to avoid this issue:

\textbf{Approach I: Regularizing the predicted warps} by adding: 
 \begin{align}
\label{eqn:loss:reg}
 \Lcal_{\mathrm{reg}}\triangleq  
  \sum\nolimits_{i=1}^N(\btheta_i^T \bSigma_{\mathrm{CPA}}^{-1}\btheta_i)
\end{align}
 where $\bSigma_{\mathrm{CPA}}$ is the CPA covariance matrix
  (proposed by Freifeld \etal~\cite{Freifeld:ICCV:2015:CPAB,Freifeld:PAMI:2017:CPAB}) associated with a
  zero-mean Gaussian smoothness prior over the CPA field.
 Akin to the standard formulation in, \eg, Gaussian processes~\cite{Rasmussen:Book:2004:GP},
$\bSigma_{\mathrm{CPA}}$ has two parameters: $\lambda_{\mathrm{\sigma}}$, which controls the overall variance, and $\lambda_{\mathrm{smooth}}$,
which controls the smoothness of the field. A small $\lambda_{\mathrm{\sigma}}$ favors small warps (\ie, close to the identity) and vice
versa; similarly, 
the larger $\lambda_{\mathrm{smooth}}$ is, the more it favors
CPA velocity fields that are almost purely affine and vice versa, as could be seen in~\autoref{fig:prior}.

The prior also gives another way, an alternative to changing the resolution
of $\Omega$, to control the amount of expressiveness of the warps.
 %
The JA objective function, to be minimized \wrt $\bw$, is
\begin{equation}
  \label{eqn:loss:full}
  \Lcal_{\mathrm{JA}} = \Lcal_{\mathrm{data}} + \Lcal_{\mathrm{reg}}
\end{equation}
which corresponds to~\autoref{eq:JA}, where $D(\cdot)$ is the euclidean distance and $\Rcal(T;\lambda)$ is 
the smoothness prior over the CPA field with HP $\lambda=(\lambda_{\mathrm{\sigma})}, \lambda_{\mathrm{smooth}}$.

However, \emph{optimal regularization is dataset-specific}. For example, penalizing deformations that are too large might
not be ideal in many cases. Likewise, with a temporal smoothness prior, it is hard to determine the ``right" amount of smoothness.  \autoref{fig:reg:ecg} illustrates the critical role of regularization on the  barycenter computation using DBA, SoftDTW, and DTAN.
 Improper values of $\gamma$ (for SoftDTW) or $\lambda_{\sigma},\lambda_{\mathrm{smooth}}$ (for DTAN) usually result in unrealistic warps 
 or overly restrict the warps (\eg, a \emph{strong} prior for DTAN).
This leads us to the second approach, one we have recently introduced in~\cite{Shapira:ICML:2023:RFDTAN}, which is regularization-free.

\textbf{Approach II: Enforcing inverse-consistency} between the post-alignment average sequence and the original samples.
Specifically, we propose a new loss that is minimized when the average sequence is both a minimizer of the variance \emph{and} consistent with its class.  Concretely, we propose the Inverse Consistency Averaging Error loss (ICAE), defined as:
\begin{align}\label{eq:icae}
\Lcal_{\mathrm{ICAE}}&\triangleq \sum_{k=1}^K 
 \tfrac{1}{N_K}\sum_{i:y_i=k}\bigg\| \mu_k\circ T^{-\btheta_i}- u_i\bigg\|\, . 
\end{align}
$\Lcal_{\mathrm{ICAE}}$ measures how well the average signal, $ \mu_k$, fits each signal $u_i$ in its class using the inverse warp $T^{-\btheta_i}$. It does so by first aligning all of the signals in class $k$ using the predicted warps,
then computing their average $ \mu_k$, and finally warping $ \mu_k$ back toward each $u_i$ using $T^{-\btheta_i}$, 
thereby ensuring consistency between them. See~\autoref{fig:icae:example} for an illustration of these two steps.

\emph{A key insight is that ~\autoref{eq:icae} strongly discourages trivial solutions or unrealistic warps as this would result in a poor estimate of $ \mu_k$, which in turn would yield a high discrepancy between it and the original signals.} In other words, the loss favors 
realistic deformations without the need to add a regularization term. This can be seen in~\autoref{fig:training}, where we show the training procedure for the \emph{BeetleFly} dataset for both approaches: WCSS (without regularization) and the ICAE. Minimizing the WCSS distorts the signals to the point that they are no longer recognizable. In contrast, the results under the ICAE retain the key features of the data without enforcing any regularization. 
The full training procedure is described in~\autoref{Alg:training}.
%
%%%%%% FIGURE %%%%%
\subimport{./}{5_method/fig_training.tex}
%%%%%%%%%%%%%%%%%%%
%
\subsection{Variable-Length Joint Alignment}\label{Subsec:Method:Variable:Length}
Our proposed $\Lcal_{\mathrm{ICAE}}$ also allows for the JA and averaging of variable-length sequences without having to use a specialized loss function or tweak the boundary conditions on $T^\btheta$ (as mentioned in~\cite{Shapira:NIPS:2019:DTAN, Martinez:ICML:2022:closed} as a hypothetical possibility). Instead, our formulation 
(as well as our code) handles both fixed and variable-length data. It does so in the following manner. First, 
the post-alignment average signal is produced by dividing, at each time step, the sum of the relevant values
by the number of non-missing values.
That is, for each time step $t$ along the duration of the mean signal $\mu$, we compute:
\begin{align}\label{eq:variable:len}
     \mu[t]=\frac{1}{N_{\mathrm{valid}}}\sum_{i:(u_i\circ T^{\btheta_i})[t]\neq \mathrm{null}}^N(u_i\circ T^{\btheta_i})[t]
\end{align}
where $N_{\mathrm{valid}}$ is the number of signals whose domain includes a point mapped to $t$. 
Then, when $ \mu$ is warped backward, ~\autoref{eq:icae} is computed with no modifications. See, \eg,~\autoref{fig:var:len}. 
From an implementation standpoint, we note that any \texttt{null} value in either the input and/or loss would break the computational graph. To avoid \texttt{for-loops} and compute back-propagation in batches, 
it is computationally effective to first pad all samples with zeros (\wrt the longest signal) and create an indicator mask for missing values. The mask is also warped by $T^\btheta$ in~\autoref{eq:variable:len}.

\subsection{Inverse Consistent Centroids Triplet Loss}\label{Sec:Method:Subsec:Triplet}
While $\Lcal_{\mathrm{ICAE}}$ implies consistency, it is agnostic about the separation between different classes.
That said, while metrics such as DTW are completely data-driven, DTAN is learning-based, and can be utilized to learn 
task-driven representations. As such, we introduce the centroid triplet loss into our framework to encourage inter-class separation. Traditionally, \eg in classification tasks, a triplet loss is defined over a triplet ($u^{a}_i, u^p_i, u^n_i$) of an anchor, a positive, and a negative examples, respectively. As our task is intra-class JA and computing class averages (also known as centroids), adopting a centroid-based triplet loss is more adequate here~\cite{doras:2020:prototypical}. We define the \textit{Inverse Consistent Centroids Triplet Loss}  over the triplet ($u^{a}_i,  \mu^p_i,  \mu^n_i$) as 
\begin{align}\label{eq:triplet}
\begin{split}
    &\Lcal_{\mathrm{ICAE-triplet}}(u^{a}_i,  \mu^p,  \mu^n)\triangleq \\ 
    &\max(0, \|u^a_i -  \mu^p\circ T^{- \btheta_i}\|_{\ell _2}^{2} - \|u^a_i -  \mu^n\circ T^{- \btheta_i}\|_{\ell _2}^{2} + \alpha)
\end{split}
\end{align}

where $ \mu^p,  \mu^n$ are \emph{the} positive and \emph{a} negative class centroids, respectively, and $\alpha$ is the margin between them ($\alpha=1$ in all our experiments and is dataset-independent). As both $ \mu^p$ and $ \mu^n$ are compared via an inverse warp, $\Lcal_{\mathrm{ICAE-triplet}}$ does not break the consistency between samples and their mean. The $\Lcal_{\mathrm{ICAE-triplet}}$ is used in tandem with $\Lcal_{\mathrm{ICAE}}$. 
%%%%%% FIGURE %%%%%
\subimport{./}{5_method/fig_var_len}
%%%%%%%%%%%%%%%%%%%