In addition to the aforementioned objective functions, this work introduces DTAN to the notion of multitask learning. 
Inspired by the recent success of multitask learning in the context of time-series averaging~\cite{terefe:ICTAI:2020:time}, 
we propose to incorporate a classification objective as a second task in the DTAN framework.
As stated in~\cite{terefe:ICTAI:2020:time}, the classification objective is set to mitigate the chances 
of overlapping means between classes and serves as a complementary approach to $\Lcal_{\mathrm{ICAE-triplet}}$. Thus, to increase separability between classes, we propose to add a
cross-entropy term to \autoref{eqn:loss:full}:
\begin{equation}
    \label{eqn:ce}
    \Lcal_{\mathrm{ce}} \triangleq -\sum_{i=1}^{N} y_i \log \tilde{y}_i \, .
\end{equation}
where $y_i$ are the true class labels and $\tilde{y}_i$ are the predicted ones.
In terms of architecture, we attach a fully-connected layer with a SoftMax activation to the penultimate layer (\ie the embedding) of $f_{\mathrm{loc}}(\cdot)$.
Given a penultimate layer of $dim=M$, the additional classification head only adds $M\times K$ parameters to the final model. 
The classification framework is supervised \wrt the class labels, 
but still unsupervised \wrt the time-series alignment.

To control the trade-off between joint alignment and classification/separability, we introduce a hyperparameter $\lambda_{ce}$,
which is set to 1 by default. Thus, the multitask loss function is defined as:

\begin{align}
    \Lcal_{\mathrm{multi}} \triangleq 
    \Lcal_{data} + \Lcal_{reg}+ \lambda_{ce} \Lcal_{\mathrm{ce}}
\end{align}

In the case of RDTAN (to be discussed in \autoref{Subsec:Method:RNN}), the classification head is 
used only at the last recurrence of RDTAN.
