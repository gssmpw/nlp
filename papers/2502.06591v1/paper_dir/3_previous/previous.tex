\subimport{./}{tables/table_methods}

\section{Related Work}\label{Sec:previous}
\textbf{Dynamic Time Warping (DTW)} is a popular distance measure (or discrepancy) between a time-series pair~\cite{Sakoe:ICA:1971:DTW1,Sakoe:ASSP:1971:DTW2}. Given two signals of lengths $n$ and $m$, DTW computes
the best discrete alignment path in the $n\times m$ pairwise distance matrix. While its complexity is $O(nm)$, enforcing certain constraints on DTW results in a linear complexity. 
However, generalizing DTW from the pairwise case to the JA of multiple signals is prohibitively expensive
since the complexity of finding the optimal discrete alignment between $N$ signals of length $n$ is $O(n^N)$.
To overcome this limitation, several JA methods, working under the DTW geometry, were proposed.
The DTW-Barycenter Averaging (DBA)~\cite{Petitjean:2011:global,Petitjean:2014:dynamic} employs an Expectation-Maximization (EM) approach 
to refine a signal that minimizes the sum of DTW distances from the data; \ie, 
it alternates between finding  $\mu$ (while fixing $(T_i)_{i=1}^N$),
\begin{align}\label{eq:ts:average}
    \mu = \argmin{u}\sum\limits_{i=1}^N D(u, u_i\circ T_i) \, ,
\end{align}
and finding  discretely-defined $(T_i)_{i=1}^N$ (while fixing $\mu$),
\begin{align}\label{eq:loss:and:prior}
    (T_i^*)_{i=1}^N = \argmin{(T_i)_{i=1}^N\in\Tcal} \sum\limits_{i=1}^N 
     D(\mu, u_i\circ T_i) \, .
\end{align}

%
  SoftDTW~\cite{cuturi:2017:soft}, a soft-minimum variant of DTW, extends DBA. 
  Instead of using EM, SoftDBA computes $\mu$ via gradient-based optimization. SoftDTW has one HP, $\gamma$, that controls the smoothness of the alignment ($\gamma=0$
  gives the original DTW score). 
  SoftDTW-divergence~\cite{Blondel:2021:differentiable} modifies SoftDTW to a proper positive-definite divergence. Both of these optimization-based methods \emph{do not learn} how to find the JA of \emph{new} data; \ie, 
  when new signals arrive, they must be run from scratch in order to achieve JA of the new ensemble. While it is possible to align the new data to the previously-found $\mu$ in a pairwise manner, this leads to inferior results (see~\autoref{Sec:Results}). 
  Additionally, the time/memory complexity of SoftDTW is $O(mn)$. SoftDTW-div
  has an even worse complexity for a large $n$ or $m$; \eg, results on \texttt{HandOutlines} (the largest UCR dataset in terms of $n\times N$) %: $2079\times 1000$)
  were not reported by~\cite{Blondel:2021:differentiable}, and when we tried to run SoftDTW (using \texttt{tslearn}~\cite{tavenard:2017:tslearn}) on it, 
  it failed due to memory limitations. 

Other methods include the Global Alignment Kernel (GAK)~\cite{cuturi:2011:gak} on which SoftDTW is based, DTW with Global Invariances
 which generalizes DTW/SoftDTW to both time and space~\cite{Vayer:2020:time}, and 
Neural Time Warping %(NTW)
that relaxes the original problem
to a continuous optimization using a neural net (albeit limited in the number of signals it can jointly align)~\cite{kawano:ICASSP:2020:neural}. 

\textbf{Spaces of Diffeomorphisms} are often used for modeling warping paths between sequences;  \eg,~\cite{Srivastava:2010:shape,srivastava:2011:registration} proposed differomoprhisms based on the square-root velocity function (SRVF) representation. 
However, the employment of diffeomorphisms in DL used to be hindered by the associated expensive computations and/or approximation/discretization schemes. For example, this is why diffeomorphisms could not initially be used effectively within a
Spatial Transformer Net (STN)~\cite{Jaderberg:NIPS:2015:spatial} since training the latter requires a large number of evaluations of both $x\mapsto T^\btheta(x)$ and $x\mapsto \nabla_\btheta T^\btheta(x)$ (where $\btheta$ 
parameterizes the chosen diffeomorphism family), and these quantities are computed at multiple values of $x$.   
 This has changed, however, with the emergence of new methods~\cite{Skafte:CVPR:2018:DDTN,Balakrishnan:CVPR:2018:UnsupervisedDeformable}. 
In particular, \cite{Skafte:CVPR:2018:DDTN} built on the CPAB diffeomorphisms (see below) 
to propose the first diffeomorphic STNs.

%

\textbf{CPAB Diffeomorphisms~\cite{Freifeld:ICCV:2015:CPAB,Freifeld:PAMI:2017:CPAB}}. 
The name CPAB, short for CPA-Based, stems from the fact that these parametric diffeomorphisms are based on Continuous Piecewise-Affine (CPA) velocity fields. Of note, in 1D, the CPAB warp, $x\mapsto T^\btheta(x)$, has a closed form~\cite{Freifeld:ICCV:2015:CPAB}. 
The expressiveness and efficiency of the CPAB warps make them an invaluable tool in DL (see, \eg, ~\cite{Hauberg:AISTATS:2016:DA,Skafte:CVPR:2018:DDTN,Skafte:NIPS:2019:explicit,Shapira:NIPS:2019:DTAN,kaufman:icip:2021:cyclic,Shacht:2021:single,Schwobel:2022:UAI:pstn,Martinez:ICML:2022:closed, Neifar:2022l:everaging,Kryeem:ICCV:2023:personalized,Wang:AAAI:2024:Animation,kryeem:CVIU:2025:action,Chelly:ECCV:2024:ditac,Mantri:NIPS:2024:DIGRAF}) and thus this work uses them too.  
However, our method is not limited to this choice of $\Tcal$.

%
A \textbf{Temporal Transformer Net (TTN)} is the 1D variant of the STN, where the latter is a DL module which, given a transformation family, predicts and applies a transformation to its input for a downstream task. 
\citet{Lohit:CVPR:2019:temporal} use TTNs with discretized diffeomorphisms for learning rate-invariant discriminative warps. 
The SRVF framework was integrated into TTNs to either predict DTW-based warping functions~\cite{nunez:CVPRW:2020:deep}, learn a generative model over the distribution of SRVF warps~\cite{Nunez:2021:srvfnet}, and time-series JA~\cite{Chen:2021:srvfregnet}. However, computations in these nonparametric warps do not scale well with the signal length.

The \textbf{Diffeomorphic Temporal Alignment Net (DTAN)}~\cite{Shapira:NIPS:2019:DTAN}
is a diffeomorphic TTN that, using the parametric and highly-expressive CPAB warps, offers an effective learning-based solution for JA and averaging.~\citet{Shapira:NIPS:2019:DTAN} based their DTAN implementation on \texttt{libcpab}~\cite{Detlefsen:2018:libcpab}.  
Recently,~\citet{Martinez:ICML:2022:closed} released another CPAB library, Diffeomorphic Fast Warping (\texttt{DIFW}), 
which, while being similar to \texttt{libcpab} (and is, in fact, based on it), is even faster, largely due to the smart discovery of a closed-form gradient~\cite{Martinez:ICML:2022:closed} for CPAB warps. 
Together with some other changes and an extensive HP tuning on the test data, 
this let them propose
a DTAN implementation with SOTA results in terms of Nearest Centroid Classification (NCC) accuracy, a standard metric for time-series averaging.  
 Henceforth will refer to the DTAN implementations from~\cite{Shapira:NIPS:2019:DTAN} and~\cite{Martinez:ICML:2022:closed}
 as DTAN$_{\mathrm{libcpab}}$ and DTAN$_{\mathrm{DIFW}}$, respectively. 
Lastly, ResNet-TW~\cite{huang:2021:residual} also predicts CPAB warps albeit via the Large Deformation Diffeomorphic Metric Mapping 
framework~\cite{Beg:IJCV:2005}.

%
\textbf{Warp Regularization.} As is typical with diffeomorphisms, 
CPAB warps too are usually regularized. % to avoid unfavorable solutions. 
In particular, the three works above~\cite{Shapira:NIPS:2019:DTAN,huang:2021:residual,Martinez:ICML:2022:closed},
who all use the \textbf{within-class-sum-of-squares (WCSS) loss}, 
 also use the following regularization from~\cite{Freifeld:ICCV:2015:CPAB}, 
$\Rcal(T^{\btheta_i};\lambda) = \btheta_i^{\top}\bSigma^{-1}_{\mathrm{CPA}}\btheta_i$. 
The matrix $\bSigma_{\mathrm{CPA}}$ is the covariance of a
zero-mean Gaussian smoothness prior over CPA velocity fields
and has two HPs: $\lambda_{\bSigma}$, which controls the overall variance, and
$\lambda_{\mathrm{smooth}}$, which controls the smoothness of the fields. Additionally, all these three methods predict a varying number of warps (denoted by $N_{\mathrm{warps}}$), such that their composition yields the final warp.

We conclude the section with ~\autoref{table:methods} that summarizes the differences between several JA/averaging methods and ours.