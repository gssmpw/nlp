\section{Introduction}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{plots/fig1.jpg}
  \caption{\textbf{Summary of contributions.} \textbf{(A) Outline.}
  We propose quadratic word embedding models as a solvable language model and find its exact dynamical solutions under gradient flow from small initialization. Our experiments exhibit excellent agreement with theory.
  \textbf{(B) Empirical signatures.} The singular values (amber curves) of quadratic word embeddings grow sequentially, with the top modes learned first. With sufficiently small initialization, these learning steps become evident in the loss dynamics, showing stepwise decreases. These dynamics are enabled by a rapid alignment between the top singular directions of the model and the target, occurring \textit{before} the loss noticeably decreases. See \cref{fig:z-appdx-phenomena} for further discussion. We rescale time by $\tau$, the predicted timescale for realizing the first direction.
  \textbf{(C) Theory-experiment match.} We plot optimization trajectories of a \wem\ under different subsampling hyperparameters. We solve for the full dynamics in one case and solve for the final embeddings in all cases. We overlay the empirical dynamics and the theoretical prediction in a 2D subspace of the full model space. The target is inaccessible due to the rank constraint imposed by the $d$-dimensional embeddings, which we qualitatively depict as a hyperbolic boundary.
  \textbf{(D) Sequential learning of interpretable concepts.} We project the embeddings onto the 1st and 4th singular vectors at different training times. At $t\approx\tau$, the first singular mode is realized and the embeddings approximately span a 1D subspace. The embeddings proceed to expand stepwise into subspaces of increasing dimension until the rank constraint is saturated. The singular directions correspond to interpretable concepts. The final panel schematically depicts the emergence of analogy structure: when the effective rank of the embeddings is sufficiently large, the analogy's embeddings approximately form a parallelogram.
  }
\label{fig:fig1}
\end{figure*}

Large language models (LLMs) achieve impressive performance on complex reasoning tasks despite the relative simplicity of their pretraining task: predicting the next word (or token) from a preceding context.
To better understand the behavior of LLMs, we require a scientific theory that a) quantifies how LLMs model the empirical next-token distribution, and b) explains why successfully modeling this distribution is concomitant with the ability to construct internal models of the world \citep{li2023emergent} and succeed on reasoning tasks \citep{huang2022towards,wei2022chain}.
However, serious obstacles remain in developing such a theory: the architectures are sophisticated, the optimization is highly nonconvex, and the data is poorly characterized.

To make progress, we turn to simple models that admit theoretical analysis while capturing phenomena of interest.
What key properties of LLMs should be reflected in our simple model?
We suggest the following criteria.
First, the model should learn an empirical token co-occurrence distribution using a self-supervised algorithm.
Second, it should learn internal representations that have task-relevant inner product structure.
Finally, it should succeed on downstream tasks that are distinct from the pretraining task.

Word embedding algorithms have all these ingredients.
One example is \wtv\ with negative sampling \citep{mikolov2013distributed}, a contrastive self-supervised algorithm that learns to model the probability of finding two given words co-occurring in natural text using a shallow linear network.
Despite its simplicity, the resulting models succeed on a variety of semantic understanding tasks.
One striking ability exhibited by word embeddings is analogy completion: most famously, $\vec{\mathrm{man}} - \vec{\mathrm{woman}} \approx \vec{\mathrm{king}} - \vec{\mathrm{queen}}$, where $\vec{\mathrm{man}}$ is the embedding for the word ``man'' and so on.
Importantly, this ability is not explicitly promoted by the optimization objective; instead, it emerges from the embeddings' ability to model the co-occurrence distribution.

It is an ambitious goal to develop quantitative theory that connects LLM optimization dynamics and corpus statistics to the ability to solve complex reasoning tasks.
We take a step in this direction by studying a simpler setting, where similar questions remain unresolved.
What are the learning dynamics of word embedding models, given in terms of the statistical structure of natural language distributions?
How does analogical reasoning emerge from these dynamics?
How does the model size dictate which tasks are learned?
We aim to provide some answers to these questions.

\subsection{Contributions.}

We introduce \textit{quadratic word embedding models} (\wem s), a broad class of contrastive self-supervised algorithms that are simple enough to be amenable to theoretical analysis, yet nearly match the performance of \wtv\ 
on standard analogy completion benchmarks.
We show that \wem\ loss functions can be seen as quadratic approximations of well-known contrastive losses around the origin.
We thus initialize these models near the origin and train using SGD.

We then prove that \wem\ gradient descent dynamics are equivalent to those of supervised matrix factorization with a square loss (\cref{prop:qwem}).
The target matrix contains the empirical co-occurrence statistics of natural language.
Using this equivalence, we obtain analytic solutions for the final embeddings of a representative \wem\ in terms of the target matrix (\cref{thm:anisotropic}).
When the algorithm subsamples frequent words so that the effective unigram distribution is uniform, we obtain a closed form solution for the full training dynamics (\cref{thm:sigmoidal}), revealing that the embeddings' singular value dynamics are sigmoidal and sequential.
We show that practical implementations of \wem s trained on WikiText exhibit excellent agreement with our theoretical results (\cref{fig:fig1}C, \cref{fig:fig2}), and that the top singular vectors encode interpretable concepts.

Finally, we use our theoretical results to investigate the effect of model size and training time on the downstream analogy completion task.
This is motivated by the empirical observation that a model's accuracy on different analogy subtasks (e.g., masculine-feminine or country-nationality analogies) abruptly transitions from zero to nonzero at some subtask-dependent critical model size.
% This is the analog of the observation that scaling up LLMs can unexpectedly boost performance on certain downstream tasks when the performance is measured using non-smooth metrics such as top-1 accuracy \citep{wei2022emergent, schaeffer2024emergent}.
From our theoretical framework, we derive an estimator for this critical model size.
Numerical simulations demonstrate that our estimator is reliable.
Additionally, our theoretical results provide a mechanistic description of how the latent representations develop the geometric structure necessary for analogical reasoning.
See \cref{sec:analogies}.

\clearpage
