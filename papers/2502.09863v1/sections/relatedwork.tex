\section{Related work}
% \label{sec:relatedwork}

\textbf{Word embeddings.} Early research in natural language processing studied the task of assigning semantic vectors to words \citep{bengio2000neural, almeida2019word}. One algorithm, \wtv\ skip-gram with negative sampling (SGNS), is widely used for its simplicity, quick training time, and performance \citep{mikolov2013distributed, levy2015improving}. Notably, it employs a self-supervised contrastive loss.
This algorithm and many of its variants (e.g., \cite{pennington2014glove}) were later shown to implicitly or explicitly factorize a target matrix to produce their embeddings \citep{levy2014neural}. However, since the word embeddings are underparameterized, the model must converge to some low-rank approximation of the target \citep{arora2016latent}, leaving open the question of \textit{which} low-rank factorization is learned.
Our results provide the answer in a closely related setting. We solve for the final word embeddings directly in terms of quantities characterizing the data distribution and commonly used hyperparameters.
\textbf{Contrastive learning.} Contrastive self-supervised learning has seen widespread success in domains including language \citep{mikolov2013distributed,oord2018representation,clark2020electra} and vision \citep{oord2018representation,bachman2019learning,chen2020simple}.
Contrastive learning trains models to embed semantically similar inputs close together and dissimilar inputs far apart in the model's latent space by drawing input pairs from positive (correlated) and negative (uncorrelated) distributions.
Previous works attempting to explain the success of contrastive learning typically rely on assumptions on the two input distributions \citep{saunshi2019theoretical,wang2020understanding,haochen2021provable} or relate the contrastive loss function to notions of likelihood or mutual information \citep{gutmann2010noise,mikolov2013distributed,oord2018representation,bachman2019learning}. In contrast, our results require no such assumptions, and we show that obtaining performant embeddings does not require explicitly maximizing information-theoretic quantities. We corroborate the observation that contrastive learning exhibits low-rank bias in some settings \citep{jing2021understanding,simon2023stepwise}.

\textbf{Matrix factorization.} The training dynamics of matrix factorization models, word embedding models, and deep linear networks are all deeply interrelated due to a shared underlying mathematical structure. For two-layer linear feedforward networks trained on a supervised learning task with whitened inputs and weights initialized to be aligned with the target, the singular values of the model undergo sigmoidal dynamics, with each singular direction being learned independently with a distinct learning timescale \citep{saxe2014exact,saxe2019mathematical,gidel2019implicit,atanasov2022neural,domine2023exact}. We find that quadratic word embedding models with strong subsampling undergo the same dynamics despite having no labelled supervised task.

\newpage
Although our model is underparameterized, its dynamics are well-described by the greedy rank-minimizing behavior exhibited by overparameterized matrix factorization models trained from small initialization \citep{gunasekar2017implicit,li2020towards,gidel2019implicit,arora2018optimization,arora2019implicit,li2018algorithmic}.
These works formally assume some special structure in the initial weights; however, there is extensive empirical evidence that models trained from \textit{arbitrary} small initialization also exhibit this low-rank bias. In particular, \cite{gissin2019implicit,li2020towards,jacot2021saddle,simon2023stepwise} showed that learning occurs incrementally and sequentially in matrix factorization; if the initialization is small enough, the model greedily learns approximations of increasing rank. 
Compared to these works, which concern supervised setups where direct observations of the target matrix are available, we study self-supervised contrastive learning, where the target is learned implicitly. This directly expands the scope of matrix factorization theory to setups that are much more common in modern practice.
% We also accommodate the subsampling hyperparameters that are commonly used for word embedding tasks.
We also provide stronger empirical evidence that these results apply to arbitrary small initializations.

The implicit bias towards low rank directly contrasts the well-studied neural tangent kernel training regime, which is accessed when the initialization scale is order unity \citep{jacot2018neural,chizat2019lazy,woodworth2020kernel,jacot2021saddle}. In this regime, function-space dynamics and generalization performance can be characterized exactly \citep{lee2019wide,bordelon2020spectrum,simon2023eigenlearning}. When wide nonlinear networks have small initialization scale, they learn nontrivial features and exhibit improved scaling laws  \citep{yang2021tensor,vyas2023empirical,karkada2024lazy,atanasov2024optimization}. Our work naturally extends these ideas to the self-supervised setting.

\textbf{Linear representation hypothesis.} The ability of SGNS to complete analogies through vector addition suggests that interpretable concepts are encoded in linear subspaces of the latent space. This hypothesis motivates modern research areas, including representation learning \citep{jiang2024origins,park2023linear,wang2024concept}, mechanistic interpretability \citep{li2023transformers,nanda2023emergent,lee2024mechanistic}, and LLM alignment \citep{lauscher2020general,li2024inference,zou2023representation}. These studies share a common theme: leveraging interpretable linear subspaces either to uncover the model's internal mechanisms or to engineer solutions for mitigating undesired behavior. To make these efforts more precise, it is important to develop a quantitative understanding of these linear representations in simple models. Our results give closed-form solutions for the top singular vectors of the latent embeddings in terms of corpus statistics. Furthermore, we use our dynamical solutions to predict the onset of the linear structures required for analogy completion.

\clearpage