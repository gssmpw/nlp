\section{Conclusion}

We introduced quadratic word embedding models, a simple class of models that approximate known self-supervised algorithms and capture representation learning in language modeling tasks. We solved their learning dynamics and final embeddings in a variety of practically-relevant settings and found excellent agreement with practical implementations. Using our analytical results, we shed light on the effect of model scale on downstream task performance. We leave the study of scaling laws, learning curves, deeper architectures, and applications to other tasks and domains to future work.

\textbf{Author contributions.} DK developed the analytical results, ran all experiments, and wrote the manuscript with input from all authors. JS proposed the initial line of investigation and provided insight at key points in the analysis. YB and MRD helped shape research objectives and gave feedback and oversight throughout the project's execution.

