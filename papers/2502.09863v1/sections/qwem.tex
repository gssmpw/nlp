\section{Quadratic Word Embedding Models}

\begin{definition}
    Let $\M\in\R^{V\times V}$ be a parameterized matrix.
    Choose any scalar constants $a,b,c,d$ satisfying $ac\geq 0$ and $a+c > 0$, and define the polynomials $\ell^{+}(x)\defn ax^2-bx$ and $\ell^{-}(x)\defn cx^2-dx$.
    A \textit{quadratic word embedding model} (\wem) is any $\M$ obtained by minimizing the following self-supervised contrastive loss by gradient descent\footnote{We sometimes also refer to the algorithm itself as \wem.}:
    \begin{equation}
        \loss(\M) = \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[\ell^{+}(\M_{ij})\bigg] 
         + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\!\bigg[\ell^{-}(\M_{ij})\bigg].
         \label{eq:qwem_loss}
    \end{equation}
\end{definition}

We typically parameterize the model $\M\defn \T\mW\mW$, where the embeddings $\mW$ are trainable parameters.
Though it may seem restrictive to require that $\ell^+$ and $\ell^-$ are quadratic polynomials, many contrastive learning algorithms can be converted into \wem s via Taylor approximation. We will soon study two such examples.

\begin{proposition}
    \label{prop:qwem}
    Let $\M$ be a \wem\ defined with constants $a,b,c,d$.
    Define $\mG_{ij} \defn a P_{ij}+c P_i P_j$ 
    and
    \begin{equation}
        \Mstar_{ij} \defn \frac{b P_{ij}+d P_i P_j}{2 \mG_{ij}}.
    \end{equation}
    Then the gradient descent dynamics of $\M$ are identical to those given by the supervised square loss
    \begin{equation}
        \loss_\mathrm{sq}(\M) = \sum_{i,j}\mG_{ij}(\M_{ij}-\Mstar_{ij})^2.
        \label{eq:qwem_sq_loss}
    \end{equation}
    If $\M$ is unconstrained, $\Mstar$ is the unique global minimizer.
\end{proposition}
\textit{Proof.} Algebraic manipulation reveals that \cref{eq:qwem_loss} and \cref{eq:qwem_sq_loss} are equal up to an additive constant. The uniqueness of the minimum follows from strong convexity.

\cref{prop:qwem} states that training a \wem\ is equivalent to supervised learning with a target that contains the corpus statistics. We will soon exploit this equivalence to solve for the training dynamics of word embedding algorithms.

\clearpage

\cref{eq:qwem_sq_loss} reveals that our problem is equivalent to weighted matrix factorization \citep{srebro2003weighted}.
If the elements of $\M$ were the trainable parameters, the model would directly converge to $\Mstar$ regardless of the choice of $\mG$.
In contrast, here the rank constraint excludes $\Mstar$ from the feasible region and makes optimization non-convex.
As a result, the final embeddings depend on the optimization trajectory induced by the particular $\mG$. Since $\mG$ is sensitive to the subsampling rates, this provides an explanation for the empirical observation by \cite{mikolov2013distributed} that subsampling affects the quality of the final embeddings.

\subsection{Case 1: Taylor approximation of SimCLR loss}

Proofs of the main results in this section are provided in \cref{appdx:proofs}.

\begin{corollary}
    The self-supervised contrastive loss
    \begin{equation}
        \lossxe(\M) = \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[{-\M_{ij}}\bigg]
        + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\!\bigg[{\half\M_{ij}^2 +\M_{ij}}\bigg]
        \label{eq:qwem_xe}
    \end{equation}
    has a unique global minimum at
    \begin{equation}
        \Mstarxeij = \frac{P_{ij}-P_i P_j}{P_i P_j},
    \end{equation}
    and is equivalent (under gradient descent) to
    \begin{equation}
        \loss(\M) = \half\sum_{ij} P_i P_j \left(\M_{ij}-\Mstarxeij\right)^2.
    \end{equation}
    \label{cor:qwem_xe}
\end{corollary}
This follows from setting $a=0$, $c=1$, and $b=-d=1$ in \cref{prop:qwem}. In \cref{appdx:relation}, we show that $\lossxe$ is a Taylor approximation to the ``normalized temperature-scaled cross entropy'' loss used in SimCLR \citep{chen2020simple}, and that $\Mstarxe$ coarsely approximates the SGNS minimizer. Since in this case $\mG_{ij}=P_i P_j$ has rank 1, we can fruitfully study the resulting learning dynamics. Contrast this with the general case where $\mG$ is full-rank; there, we cannot obtain exact solutions since weighted matrix factorization with arbitrary non-negative weights is known to be NP-hard \citep{gillis2011low}.

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{plots/fig2.jpg}
  \caption{\textbf{Empirical validation of theoretical results.} See \cref{appdx:expts} for experimental details. \textbf{(A) Training a \wem\ on $\lossxe$ from small random initialization.} We set the subsampling hyperparameters $\Psi_i=P_i^{-1}$ and plot the singular values of $\mW$ learning 130M tokens of Wikipedia. Left column: the true learning dynamics are nearly indistinguishable from the prediction in \cref{thm:sigmoidal}, which even resolves constant factors. The dashed curve is the theory's prediction for the characteristic time $\tau_k$ for realizing the $k^\mathrm{th}$ mode. We rescale time by $\tau\defn\tau_1$. Second column: with small initialization, the alignment of the top-$k$ singular subspace occurs well before the realization of the singular values, leaving an observable signature: an early spike in analogy completion accuracy. This rapid alignment explains why \cref{thm:sigmoidal} applies despite random initialization. (Note: the middle plot is simply the top-left plot in log-log scale.) \textbf{(B) Training a \wem\ on $\losssym$.} Same setup, different loss. We see approximate quantitative agreement with the prediction obtained by replacing $\mG$ in \cref{eq:qwem_sym_mse} with its rank-1 approximation and applying \cref{thm:sigmoidal}. \textbf{(C) Effects of subsampling.} We validate \cref{thm:anisotropic} by training five \wem s: $\Psi_i=P_i^{-f}$ for $f\in\{0, 0.25, 0.5, 0.75, 1\}$. We find that each converged \wem\ is closest in Frobenius norm to the predicted model with $f'=f$, compared to the predictions for $f'\neq f$. \textbf{(D) Analogy completion performance vs. other algorithms.}
  We compare \wem s (trained on $\lossxe$ and $\losssym$), an SVD factorization of the constructed \wem\ target $\Mstarsym$, SVD factorizations of classical methods (pointwise mutual information matrices, see \cref{appdx:relation}), and \wtv\ SGNS. We find that \wem s perform well despite doing no hyperparameter search. All models have the same model capacity, $d=200$.
  }
  \label{fig:fig2}
\end{figure*}

The central variables of our theory are the singular value decompositions of both the model and the target. Note that since both the pretraining task and downstream tasks depend only on the inner products between embeddings, there is no privileged basis in embedding space, and $\mW$ has a full internal rotational symmetry in its left singular vectors. Thus without loss of generality we work with the model and target eigendecompositions, $\M(t)=\mV(t)\L(t)\T\mV(t)$ and $\Mstarxe=\mV^\star\L^\star\T{\mV^\star}$. 
Note that $\L$ contains the variances of the embeddings along their principal directions. We use $\lambda_k$ to denote $\L_{kk}$ and likewise for $\lambda^\star_k$.

\newpage

We first consider the training dynamics that result from setting the subsampling rate $\Psi_i^{-1} = P_i$. Recall the variable $Z\defn(\sum_k\Psi_kP_k)^2/\sum_{k\ell} \Psi_k \Psi_\ell P_{k\ell}=1+\epsilon$ for some $\epsilon$. Note that if $\epsilon=0$ then $\Mstarxe$ is invariant to subsampling. We empirically measure $\epsilon$ to be negligible ($\abs{\epsilon}<10^{-3}$).

\begin{restatable}{theorem}{sigmoidal}
    \label{thm:sigmoidal}
    Set $\Psi_i = P_i^{-1}$ for all $i$. Define the eigenbasis overlap matrix $\O(t)\defn \T{\mV^\star}\mV(t)$. If $Z=1$, $\lambda^\star_d > 0$, and $\O_{[:d,:d]}(0)=\mI_d$, then optimizing $\mW$ with gradient flow under \cref{eq:qwem_xe} yields the following solution:    
    \begin{align}
        \mV_{[:,:d]}(t) &= \mV_{[:,:d]}^\star\\
        \lambda_k(t) &= \frac{\lambda_k(0) \; \lambda_k^\star \; e^{\eta\lambda_k^\star t}}{\lambda_k^\star + \lambda_k(0)\left(e^{\eta\lambda_k^\star t} -1\right)},
    \end{align}
    where $\eta\defn 4/V^2$. Up to an arbitrary orthogonal rotation of the embeddings, the final embeddings are given by
    \begin{equation}
        \mW(t\to\infty) = {\Lstar}_{[:d,:d]}^\half \T{\mV_{[:,:d]}^\star}.
    \end{equation}
\end{restatable}
We see that the dynamics are decoupled in the target eigenbasis, and the embedding variance along the $k^\mathrm{th}$ principal direction undergoes sigmoidal dynamics from $\lambda_k(0)$ to $\lambda^\star_k$ in a characteristic time
$\eta\tau_k=(1/\lambda^\star_k)\ln(\lambda^\star_k / \lambda_k(0))$. These dynamics have been discovered in a variety of other tasks and learning setups \citep{saxe2014exact,gidel2019implicit,atanasov2022neural,simon2023stepwise}. By establishing that self-supervised \wem s are equivalent to supervised algorithms in \cref{prop:qwem}, our results add self-supervised word embedding models to the list.

The positivity of the top $d$ eigenvalues of the target is a weak assumption and is typically easily satisfied in practice (see \cref{appdx:expts-model}). In contrast, it is restrictive to require that $\mV$ and $\mV^\star$ are perfectly aligned at initialization. Nonetheless, if we initialize the embedding weights i.i.d. Gaussian with variance $\sigma^2/d$, and train in the \textit{small initialization} setting where $\sigma^2\ll 1$, the training dynamics are empirically very well described by \cref{thm:sigmoidal}. 
See panel B of \cref{fig:fig1} and panel A of \cref{fig:fig2} for empirical confirmation.

This remarkable agreement is due to a dynamical \textit{silent alignment}: for all $k\leq d$, $\mV_{[:,:k]}$ quickly aligns with $\mV^\star_{[:,:k]}$ while $\lambda_k$ remains near initialization. Therefore the alignment assumption is quickly near-satisfied and \cref{thm:sigmoidal} approximately holds. Exact characterization of these alignment dynamics is known in simple cases \citep{atanasov2022neural, domine2023exact}.
In \cref{appdx:derivations-QR} we provide a theoretical argument for the broad applicability of \cref{thm:sigmoidal}.

This result resolves the unexplained observation by \cite{simon2023stepwise} that vision models trained using SimCLR exhibit stepwise learning dynamics. When the initialization scale is small, the objective function is well-described by its quadratic Taylor approximation near the origin, which we have just shown exhibits sigmoidal learning dynamics.
\newpage

We now consider arbitrary subsampling rates.

\begin{restatable}{theorem}{anisotropic}
    \label{thm:anisotropic}
    For any choice of $\{\Psi_i\}_i$, define the matrix $\mP_{ij}\defn \delta_{ij} \Psi_i P_i/\sum_k \Psi_k P_k$. If $Z=1$, then the embeddings that minimize \cref{eq:qwem_xe} are given by
    \begin{equation}
        \mW = \mathrm{top}_d({\Lstar}^\half \T{\mV^\star} \mP^\half) \mP^{-\half}
        \label{eq:qwem_anisotropic_sol_W}
    \end{equation}
    up to an arbitrary orthogonal rotation of the embeddings.
\end{restatable}
Note that due to the non-convexity, \cref{thm:anisotropic} does not guarantee convergence to the global minimizer. However, \cite{srebro2003weighted} find that gradient descent reliably finds the global minimizer for natural learning problems. We confirm this empirically in \cref{fig:fig1} panel C, where the five trajectories correspond to setting $\Psi_i=P_i^{-f}$ with $f\in\{1, 0.75, 0.5, 0.25, 0\}$, and \cref{fig:fig2} panel C.

Together, \cref{thm:sigmoidal,thm:anisotropic} suggest that self-supervised models trained from small initialization are inherently greedy spectral methods. In the word embedding task, the principal components of the embeddings enjoy a one-to-one correspondence with the eigenvectors of the target statistics, and each component is realized independently and sequentially with a timescale controlled by the target eigenvalue (see \cref{fig:fig1} panel D).

\cref{eq:qwem_anisotropic_sol_W} concretizes the intuition that subsampling enables embedding algorithms to allocate less fitting power to words with large subsampling rates \citep{mikolov2013distributed}.
In particular, since by the Eckart-Young theorem $\mathrm{top}_d(\mA)$ yields the rank-$d$ matrix closest to $\mA$ in Frobenius norm, \cref{eq:qwem_anisotropic_sol_W} reveals precisely how the model prioritizes accurately resolving the embeddings with large $\Psi_iP_i$. Note that subsampling is mathematically similar to the practice of downweighting low-quality text sources in LLM training, in the sense that both practices aim to skew the training distribution to mitigate the dominance of uninformative or noisy data. In this light, our results may provide a new lens for analyzing data curation pipelines in LLM training.

\subsection{Case 2: Taylor approximation of SGNS loss}

\begin{corollary}
    The self-supervised contrastive loss
    \begin{equation}
        \losssym(\M) = \!\!\!\!\! \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[\frac{\M_{ij}^2}{4} -\M_{ij}\bigg]
        + \!\! \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\!\bigg[{\frac{\M_{ij}^2}{4} +\M_{ij}}\bigg]
        \label{eq:qwem_sym}
    \end{equation}
    has a unique global minimum at
    \begin{equation}
        \Mstarsymij = \frac{P_{ij}-P_i P_j}{\half(P_{ij} + P_i P_j)},
        % \label{eq:mstar_sym}
    \end{equation}
    and is equivalent (under gradient descent) to
    \begin{equation}
        \loss_\mathrm{sym,sq}(\M) = \half\sum_{ij} \frac{P_{ij} + P_i P_j}{2} \left(\M_{ij}-\Mstarsymij\right)^2.
    \label{eq:qwem_sym_mse}
    \end{equation}
    % \label{cor:qwem_sym}
\end{corollary}

Since the weighting coefficient $\mG$ is full-rank, \cref{eq:qwem_sym_mse} has no known closed-form minimizer. However, we may approximate the minimizer by replacing the coefficient with the best rank-1 approximation of $\mG$. We use strong subsampling to obtain an approximation for the dynamics. The approximation is qualitatively correct (see \cref{fig:fig2}), and we use it for our analysis of analogical reasoning in \cref{sec:analogies}.

In \cref{appdx:relation}, we show that $\losssym$ is the quadratic Taylor approximation to the contrastive loss used in skip-gram with negative sampling.
In addition, the minimizer $\Mstar_{\mathrm{sym}}$ is an approximation of the pointwise mutual information (PMI) matrix, which minimizes the SGNS loss. In \cref{fig:fig2} panel D, we show that models trained with $\losssym$ outperform $\lossxe$ models and approach the performance of SGNS.
Note that the comparison between \wem s and SGNS is slightly unfair: we ran SGNS with known optimal hyperparameters \citep{levy2015improving} and its full suite of engineering tricks, whereas we trained \wem s with no hyperparameter search.

Note that both \wem\ algorithms learn to model statistical fluctuations from some baseline: $\Mstarxe$ is the relative deviation of the joint statistics from the i.i.d. baseline, and $\Mstarsym$ is the symmetrized version of the same quantity.
We observe that both \wem\ algorithms match or outperform the information-theoretic measures, suggesting that SGNS succeeds \textit{despite} targeting the PMI matrix, not because of it. In practice, then, it may be unnecessary or even suboptimal to target information-theoretic measures.

The exact solutions reveal that the target eigenbasis $\mV^\star$ is the ``natural'' basis of the learning dynamics. We can now investigate whether this basis is interpretable to humans. To do this, we note that the right singular vectors reside in $\R^V$, the vocabulary space whose coordinate vectors are the one-hot embeddings of the words. Therefore, to interpret a given eigenvector, we can simply read off the words on which it has the greatest projection, since these words are most strongly aligned with its direction. Across all models considered, we find that the top eigenvectors correspond to intuitive concepts. For example, for $\Mstarsym$, the top words of singular direction 1 are related to Hollywood (bobby, johnny, songwriter, jimmy, actress, starring); singular direction 5 is related to science (science, mathematics, physics, academic, psychology, faculty, institute, research); singular direction 16 is related to criminal evidence (photographs, documents, jury, summary, victims, description, trial); and so on. Our results suggest that these concepts constitute the fundamental linear representations learned by the model.

\newpage
