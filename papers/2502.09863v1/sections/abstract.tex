\begin{abstract}

The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus.
As a simpler surrogate for representation learning in language modeling, we study a class of solvable contrastive self-supervised algorithms which we term \textit{quadratic word embedding models}.
These models resemble the \wtv\ algorithm and perform similarly on downstream tasks.
Our main contributions are analytical solutions for both the training dynamics (under certain hyperparameter choices) and the final word embeddings, given in terms of only the corpus statistics.
Our solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated.
Training on WikiText, we find that the top subspaces represent interpretable concepts.
Finally, we use our dynamical theory to predict how and when models acquire the ability to complete analogies.

% to better understand -> emphasize that this is a solvable model of language modeling
% perform competitively -> similar performance
% we study a class of models we term qwem
% use these solutions to show -> solution reveal
% DRAFT 4
% The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus.
% To better understand how these models internally encode semantic structure, we study the learning dynamics of \textit{quadratic word embedding models}, a broad class of self-supervised algorithms that resemble known embeddings such as \wtv\ and perform well on downstream analogy tasks.
% Our main contribution is to analytically solve for both the model's full training dynamics and the converged word embeddings in terms of only the corpus statistics and algorithmic hyperparameters.
% We then use these solutions to show that these embedding models learn concepts one at a time, incrementing the effective rank of the embeddings until the model capacity is saturated.
% Finally, we empirically demonstrate that these concepts are interpretable and that our dynamical theory can be used to explain how and when models acquire the ability to complete analogies.

% DRAFT 3 
% The remarkable success of attention-based large language models relies on inner products between internal representations in the model's embedding spaces.
% As a step towards understanding how such models learn semantic structure in their latent spaces, we study the learning dynamics of a broad class of self-supervised word embedding models.
% These models' performance on downstream tasks is competitive with well-known embeddings such as word2vec.
% Our main contribution is to exactly solve for the training dynamics and the final word embeddings in terms of only the corpus statistics and the relevant algorithmic hyperparameters.
% We then use these solutions to show that word embedding models learn concepts one by one, each incrementing the effective rank of the embeddings until the model capacity is saturated.
% Finally, we empirically demonstrate that these concepts are interpretable and that our dynamical theory can be used to explain how and when models acquire the ability to complete analogies.

% DRAFT 2 
% The remarkable success of large language models relies on computations involving inner products between tokens in a model's internal embedding spaces.
% As a step towards understanding how such models encode semantic structure in their latent spaces, we study the training dynamics of a broad class of self-supervised word embedding models.
% These models' performance on downstream tasks is competitive with well-known embeddings (e.g., word2vec).
% Our main contribution is to exactly solve for the training dynamics and the final word embeddings in terms of the corpus statistics, with no additional assumptions on the data distribution.
% We then use these solutions to show that word embedding models learn concepts one by one, each incrementing the effective rank of the embeddings until the model capacity is saturated.
% Finally, we empirically demonstrate that these concepts are interpretable and that our dynamical theory can be used to explain how and when models acquire the ability to complete analogies.

% DRAFT 1 
% The fundamental computational primitives of attention-based language modeling are inner products in key-query embedding spaces. As a step towards understanding how models encode semantic structure in their internal embedding spaces, we present a theory whose \textit{closed-form equations} describe the training dynamics of word embedding models (e.g., word2vec) trained with gradient flow on a self-supervised contrastive loss from small initialization. Our theory makes \textit{no assumption} about the data distribution. Using this theory: 1) We find that these models learn human-interpretable concepts one by one, each incrementing the effective rank of the embeddings until the model capacity is saturated. 2) We predict that \remind. 3) We give sufficient conditions under which one can expect a model to successfully complete analogies and show that these conditions accurately predict performance on real datasets. We perform practical experiments throughout and find excellent agreement with the theory.
\end{abstract}