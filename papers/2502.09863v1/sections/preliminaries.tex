\section{Preliminaries}

\textbf{Notation.} We use capital boldface to denote matrices and lowercase boldface for vectors. Subscripts denote elements of vectors and tensors ($\mA_{ij}$ is a scalar). The matrix $\mathrm{top}_d(\mA)$ is the rank-$d$ approximation of $\mA$ given by its truncated singular value decomposition (SVD).
We write $\mA_{[:p,:q]}$ to denote the upper-left $p\times q$ submatrix of $\mA$.

\textbf{Setup.} The training corpus is a long sequence of words drawn from a finite vocabulary of cardinality $V$. A \textit{context} is any length-$L$ continuous subsequence of the corpus. Let $i$ and $j$ index the vocabulary.
Let $\Pr(j|i)$ be the proportion of occurrences of word $j$ in contexts containing word $i$,
and let $\Pr(i)$ be the empirical unigram distribution. Define $\Pr(i,j)\defn \Pr(j|i)\Pr(i)$ to be the \textit{skip-gram distribution}. We use the shorthand $P_{ij}\defn \Pr(i,j)$ and $P_i\defn \Pr(i)$. 

The core principle underlying modern language modeling is the \textit{distributional hypothesis}, which posits that semantic structure in natural language can be discovered from the co-occurrence statistics of the words \citep{harris1954distributional}. Note that if natural language were a stochastic process with i.i.d. tokens, we would have $P_{ij}=P_i P_j$. Thus, the distributional hypothesis relies on deviations from independence. Indeed, measures of relative deviation from some baseline serve as the central quantity of interest in our theory, and will be our optimization target, e.g.,
\begin{equation*}
    \Mstarxeij = \frac{P_{ij} - P_i P_j}{P_i P_j} \qtext{or} \Mstarsymij = \frac{P_{ij} - P_i P_j}{\frac{1}{2}(P_{ij}+P_i P_j)}.
\end{equation*}
We want the algorithm to learn a compressed representation of the matrix $\Mstar\in\R^{V\times V}$. Effective compression is made possible in practice by the fact that natural language is highly structured and words tend to co-occur according to topics \citep{arora2016latent}.
To accomplish this, we define a word embedding model $\M\defn \T\mW\mW$, where $\mW\in\R^{d\times V}$ is the trainable weight containing the $d$-dimensional word embeddings. The word embedding $\vw_i$ is the $i^\text{th}$ column of $\mW$. $\M$ is thus the Gram matrix containing embedding inner products, $\M_{ij}=\T\vw_{i}\vw_{j}$.  We study the underparameterized regime, $d\ll V$, in accordance with practical settings. We note that some implementations (e.g., SGNS) have two distinct weight matrices, e.g., $\M=\T\mW_1\mW_2$, but this is unnecessary in our setting (see \cref{appdx:expts-model}).

\textbf{Subsampling.} To accelerate training and prevent the model from over-allocating fitting power to very frequent words, \cite{mikolov2013distributed} and \cite{pennington2014glove} adopt \textit{subsampling}: probabilistically discarding frequent words during iteration through the corpus. This is controlled by the hyperparameters $\{\Psi_i\}_i$, where $\Psi_i$ is a reweighting factor proportional to the probability that word $i$ is not discarded. The algorithm then sees the effective distributions
\begin{equation*}
    P_i\gets \frac{\Psi_iP_i}{Z_u} \qtext{and} P_{ij}\gets \frac{\Psi_i\Psi_j P_{ij}}{Z_j}
\end{equation*}
where $Z_u$ and $Z_j$ are $\Psi$-dependent normalizing constants. Subsampling can be seen as a preprocessing technique that directly modifies the unigram and skip-gram statistics; our results then describe how this influences training dynamics.  We define $Z\defn Z_u^2/Z_j=(\sum_k\Psi_kP_k)^2/\sum_{k\ell} \Psi_k \Psi_\ell P_{k\ell}$ and note that $Z$ is very close to 1 in practice.

\textbf{Self-supervised training.} To capture the self-supervisory nature of autoregressive language models, we must learn $\Mstar$ implicitly. This differs from direct methods such as GloVe \citep{pennington2014glove} and latent semantic analysis \citep{landauer1997solution}. We introduce a self-supervised contrastive algorithm for learning $\Mstar$.
