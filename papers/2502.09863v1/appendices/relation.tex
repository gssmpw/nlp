\section{Relation to known algorithms}
\label{appdx:relation}

Due to their simplicity, \wem s can be used as coarse proxies for a wide variety of known self-supervised learning methods.

\subsection{Relation to SimCLR}

SimCLR is a widely-used contrastive learning algorithm for learning visual representations \citep{chen2020simple}. It uses a deep convolutional encoder to produce latent representations from input images. Data augmentation is used to construct positive pairs; negative pairs are drawn uniformly from the dataset. The encoder is then trained using the \textit{normalized temperature-scaled cross entropy loss}:
\begin{equation}
    \loss(\M) = \E_{i,j\sim \Pr(\cdot,\cdot)} \left[ -\log\frac{\exp(\beta\M_{ij})}{\sum_{k\neq j}^B\exp(\beta\M_{ik})}\right],
\end{equation}
where $\Pr(\cdot,\cdot)$ is the positive pair distribution, $\M_{ij}$ is the inner product between the representations of inputs $i$ and $j$, $\beta$ is an inverse temperature hyperparameter, and $B$ is the batch size. In the limit of large batch size, we can Taylor expand this objective function around the origin:
\begin{align}
    \loss(\M) &= \E_{i,j\sim \Pr(\cdot,\cdot)}\! \bigg[ -\beta\M_{ij} + \log \left(\E_{k\sim \Pr(\cdot)} \!\big[\exp(\beta\M_{ik})\big]\right) + \log B \bigg] \\
    &\approx \E_{i,j\sim \Pr(\cdot,\cdot)} \!\bigg[ -\beta\M_{ij} + \E_{k\sim \Pr(\cdot)} \!\big[\exp(\beta\M_{ik})\big] - 1 \bigg] + \log B \\
    &\approx \E_{i,j\sim \Pr(\cdot,\cdot)}\! \bigg[ -\beta\M_{ij}\bigg] + \E_{\substack{i\sim \Pr(\cdot)\\k\sim \Pr(\cdot)}}\! \bigg[1 + \beta\M_{ik} + \half \beta^2\M_{ik}^2\bigg] - 1 + \log B \\
    &\approx \beta\left(\E_{i,j\sim \Pr(\cdot,\cdot)} \!\bigg[ -\M_{ij}\bigg] + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}} \!\bigg[\M_{ij} + \frac{\beta}{2} \M_{ij}^2\bigg]\right) + \mathrm{const.}\\
\end{align}
If we set the temperature $\beta=1$, we exactly obtain $\lossxe$ defined in \cref{eq:qwem_xe} (up to optimization-irrelevant additive constants). \cite{chen2020simple} find that $\beta\approx10$ performs much better; invoking \cref{prop:qwem}, this yields the target
\begin{equation}
    \Mstar_\mathrm{SimCLR} = \frac{1}{10} \Mstarxe.
\end{equation}
As a consequence, sigmoidal dynamics are still present even with different choices of $\beta$.

This resolves the previously unexplained observation in \cite{simon2023stepwise} that vision models trained with SimCLR from small initialization exhibit stepwise learning.

\subsection{Relation to SGNS}

One of the most well-known word embedding models is \wtv\ skip-gram with negative sampling (SGNS). Here, we will give a brief overview of the method and describe its relation to \wem s. We will find that both models share the same underlying learning structure.

The SGNS model is asymmetric, $\mM=\transpose\mW\mV$. We call $\mW\in\R^{d\times V}$ the word embeddings and $\mV\in\R^{d\times V}$ the context embeddings, although there is no real distinction between the two during training (i.e., both words and contexts are sampled identically so there is no explicit symmetry-breaking). All embeddings are initialized as i.i.d. isotropic Gaussian vectors with expected norm $O(1/\sqrt{d})$. The model is trained by SGD on the contrastive logistic loss
\begin{equation}
    \loss_\mathrm{SGNS}(\mM) = \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[{\log(1+\exp(-\mM_{ij}))}\bigg] 
    + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\! \bigg[{\log(1+\exp(\mM_{ij}))}\bigg].
\end{equation}
Like \wem, SGNS is a self-supervised contrastive loss expressed in terms of inner products between embeddings.

As we did above, we Taylor expand around the origin, yielding
\begin{align}
    \loss_\mathrm{SGNS}(\mM) &= \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[{\log(1+\exp(-\mM_{ij}))}\bigg] 
    + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\! \bigg[{\log(1+\exp(\mM_{ij}))}\bigg] \\
    &\approx \E_{i,j\sim \Pr(\cdot,\cdot)}\!\bigg[{-\mM_{ij} + \frac{1}{4}\mM_{ij}^2}\bigg] 
    + \E_{\substack{i\sim \Pr(\cdot)\\j\sim \Pr(\cdot)}}\! \bigg[\mM_{ij} + \frac{1}{4}\mM_{ij}^2\bigg],
\end{align}
which is precisely the $\losssym$ defined in \cref{eq:qwem_sym}.

\subsection{Relation to classical SVD methods}

Early word embedding algorithms obtained low-dimensional embeddings by explicitly constructing some target matrix and employing a dimensionality reduction algorithm. One popular choice was the \textit{pointwise mutual information} (PMI) matrix \citep{church1990word}, defined
\begin{equation}
    \Mstar_\mathrm{PMI} = \log \frac{P_{ij}}{P_i P_j}.
\end{equation}
However, due to the divergence at $P_{ij}=0$, a common alternative is the \textit{positive PMI} (PPMI), defined $\Mstar_\mathrm{PPMI}=\mathrm{ReLU}(\Mstar_\mathrm{PMI})$. Although we find that the rank-$d$ SVD of PPMI outperforms that of PMI on the analogy task, both are outperformed by contrastive learning algorithms.

One such algorithm is \wtv\ skip-gram with negative sampling (SGNS). Interestingly, \cite{levy2014neural} showed that $\Mstar_\mathrm{PMI}$ is the rank-unconstrained minimizer of $\loss_\mathrm{SGNS}$. Nonetheless, SGNS in the underparameterized regime (embedding dimension $\ll$ vocabulary size) vastly outperforms the SVD of $\Mstar_\mathrm{PMI}$. This implies that the low-rank approximation learned by SGNS is distinct from the SVD, and it is this difference that results in the performance gap. Unfortunately, the rank-constrained minimizer of $\loss_\mathrm{SGNS}$ is not known in closed form, let alone the exact training dynamics. A major contribution of our work is solving for both in \wem s, which are closely related models.

To see the relation between the \wem\ targets and $\Mstar_\mathrm{PMI}$, let us write
\begin{equation}
    \frac{P_{ij}}{P_i P_j} = 1+\Delta(x_{ij}),
\end{equation}
where the function $\Delta(x)$ yields the fractional deviation from i.i.d. statistics in terms of some small parameter $x$ of our choosing (so that $\Delta(0)=0$). This setup allows us to Taylor expand quantities of interest around $x=0$. If we choose the straightforward $\Delta(x)=x$ then we have that
\begin{equation}
    x_{ij} = \frac{P_{ij} - P_i P_j}{P_i P_j} = \Mstarxeij 
\end{equation}
and
\begin{equation}
    \Mstar_\mathrm{PMI} = \log(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots
\end{equation}
It is in this sense that $\Mstarxe$ is a first-order Taylor approximation to the PMI matrix. However, we note that in practice $x_{ij}$ can be very large, especially when $i$ and $j$ constitute a linguistic collocation. This is because $x$ is not bounded from above. We conjecture that this is the main reason for the lower performance of $\lossxe$ compared to SGNS and $\losssym$.

We can do better by exploiting the degree of freedom in choosing the function $\Delta(x)$. A judicious choice will produce terms that cancel the $-\half \Delta^2$ that arises from the Taylor expansion of $\log (1+\Delta)$, leaving only third-order corrections. One such example is $\Delta(x)=2x/(2-x)$, which yields
\begin{equation}
    x_{ij} = \frac{P_{ij} - P_i P_j}{\half(P_{ij}+P_i P_j)} = \Mstarsymij
\end{equation}
and
\begin{equation}
    \Mstar_\mathrm{PMI} = \log\left(1+\frac{2x}{2-x}\right) = x + \frac{x^3}{12} + \frac{x^5}{80} + \cdots
\end{equation}
This is a much better approximation, since $x$ is bounded ($-2 \leq \Mstarsymij \leq 2$) and the leading order correction is smaller. It is in this sense that $\Mstarsym$ learns a closer approximation to the PMI matrix.

\subsection{Relation to next-token prediction.}

Word embedding targets are order-2 tensors $\Mstar$ that captures two-token (skip-gram) statistics. These two-token statistics are sufficient for coarse semantic understanding tasks such as analogy completion. To perform well on more sophisticated tasks, however, requires modeling more sophisticated language distributions.

The current LLM paradigm demonstrates that the next-token distribution is largely sufficient for most downstream tasks of interest. The next-token prediction (NTP) task aims to model the probability of finding word $i$ given a preceding window of context tokens of length $L-1$. Therefore, the NTP target is an order-$L$ tensor that captures the joint distribution of length-$L$ contexts. NTP thus \textit{generalizes} the word embedding task. Both \wem\ and LLMs are underparameterized models that learn internal representations with interpretable and task-relevant vector structure. Both are trained using self-supervised gradient descent algorithms, \textit{implicitly} learning a compression of natural language statistics by iterating through the corpus.

Although the size of the NTP solution space is exponential in $L$ (i.e., much larger than that of \wem), LLMs succeed because the sparsity of the target tensor increases with $L$. We conjecture, then, that a dynamical description of learning sparse high-dimensional tensors is necessary for a general scientific theory of when and how LLMs succeed on reasoning tasks and exhibit failures such as hallucinations or prompt attack vulnerabilities.