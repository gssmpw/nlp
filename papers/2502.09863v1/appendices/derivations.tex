\section{Derivations}
\label{appdx:derivations}

\subsection{Analogy accuracy estimator}

We are interested in understanding the phenomenon in which performance on some analogy subtask $\mathcal{F}$ remains approximately at chance level ($\mathrm{acc} < 5\%$) until some critical model size $d_\mathrm{crit}(\mathcal{F})$ at which steady improvement begins. For ease of writing we refer to this phenomenon as the onset of \textit{emergent abilities}, adopting the terminology in \cite{wei2022emergent} despite convincing evidence from \cite{schaeffer2024emergent} that these sudden abilities arise due to the use of non-smooth metrics (as opposed to reflecting true discontinuities or phase transitions in the model's learning dynamics).

A model's performance on the analogy completion benchmark is computed by evaluating
\begin{equation}
    \mathrm{acc} = \frac{1}{|\data|}\sum_{(\va,\vb,\va',\vb')\in\data}
    \mathbf{1}_{\{\vb'\}} \left(\arg\min_{\vw\in\mW\setminus\{\va,\vb,\va'\}} \left\lVert \frac{\va}{\norm{\va}} - \frac{\vb}{\norm{\vb}} - \frac{\va'}{\norm{\va'}} + \frac{\vw}{\norm{\vw}}\right\rVert\right),
\end{equation}
where the 4-tuple of embeddings $(\va,\vb,\va',\vb')$ constitute an analogy from a list of analogies $\data$, $\mathbf{1}$ is the indicator function, and $\mW$ is the set containing the word embeddings. Since the vectors are normalized, the performance depends only on the cosine distance between the embeddings.

This expression has several important aspects that are empirically necessary for word embedding models (including SGNS) to succeed. First, the vector normalization is important. This poses a theoretical challenge: the embeddings are given by SVD of $\Mstar$, and it is not immediately obvious how to interpret the normalization step in terms of $\Mstar$. Second, the $\arg\min$ is over the set of embeddings \textit{excluding} the three that comprise the analogy. For some analogy families (e.g., the comparative and superlative analogies), evaluating the $\arg\min$ over all the embeddings yields significantly lower scores. Finally, the scoring function is non-smooth: the $\arg\min$ is over a discrete set, and the indicator function is discontinuous. This poses serious problems when trying to use our continuous dynamical solutions to estimate $d_\mathrm{crit}$ for a given family $\mathcal{F}$.

We found that replacing the accuracy with a smooth proxy eliminated the emergent phenomena and critical model sizes, consistent with the findings in \cite{schaeffer2024emergent} (see \cref{fig:z-appdx-analogy-smooth}). Of course, on downstream evaluations, we typically \textit{want} non-smooth metrics; we are often only interested in the binary of whether the model's prediction is correct or not. However, this means that our theoretical framework for estimating $d_\mathrm{crit}$ requires evaluating the top-1 accuracy. We leave it to future work to find clever alternative methods of estimating the top-1 accuracy using smooth functions.

To derive our estimator, we start by simplifying the $\arg\min$:
\begin{align}
    \arg\min_{\vw} \left\lVert \hat\va - \hat\vb -\hat\va' +\hat\vw\right\rVert
    &= \arg\min_{\vw} \left\lVert \hat\va - \hat\vb -\hat\va' +\hat\vw\right\rVert^2 \\
    &= \arg\min_{\vw} \T{\hat\vw} (\hat\va - \hat\vb -\hat\va') \\ 
    &= \arg\max_{\vw} \T{\hat\vw} (\hat\va' + \hat\vb -\hat\va) ,
\end{align}
where the hats denote unit vectors. When written this way, the role of the normalization becomes clearer: it is primarily to prevent longer $\vw$s from ``winning'' the $\arg\max$ just by virtue of their length. The lengths of $\va, \vb, \va'$ are only important if there is significant \textit{angular} discrepancy between $(\hat\va' + \hat\vb -\hat\va)$ and $(\va' + \vb - \va)$; in the high-dimensional regime with relatively small variations in embedding length, we expect such discrepancies to vanish. This justifies using the approximation
\begin{align}
    \arg\min_{\vw} \left\lVert \hat\va - \hat\vb -\hat\va' +\hat\vw\right\rVert
    &\approx \arg\max_{\vw} \T{\hat\vw} (\va' + \vb - \va)\\ 
    &\approx \arg\max_{\vw} \T{\hat\vw} (\va' + \vdelta),
\end{align}
where we introduced the \textit{linear representation} $\vdelta\defn \vb-\va$. Note that for a model to successfully complete a full family of analogies, the different $\vdelta_n$ must mutually align with each other. We provide empirical evidence of this mutual alignment in terms of the target statistics in $\Mstar$ in \cref{fig:z-appdx-dalign}.

This concentration of vectors suggests that we can make the approximation
\begin{equation}
    \E_{\vdelta\in\mathcal{F}} \!\bigg[\arg\max_{\vw} \T{\hat\vw} (\va' + \vdelta) \bigg] \approx
    \E_{\vxi\sim\mathcal{N}_{\vdelta}} \!\bigg[\arg\max_{\vw} \T{\hat\vw} (\va' + \vxi) \bigg],
\end{equation}
where $\vxi$ is a Gaussian random vector whose mean is $\E[\vdelta]$ and covariance is $\mathrm{Cov}(\vdelta, \vdelta)$.

In other words, we propose an ansatz in which the first and second moments of the linear representation are sufficient to estimate the model's ability to complete analogies. We empirically find that this ansatz is successful. Furthermore, we find that this eliminates the need to exclude $\va,\vb,\va'$ from the $\arg\max$.

The last remaining step is to replace all quantities with the theoretical predictions given by \cref{thm:sigmoidal}. This results in the proposed estimator

\begin{equation}
    \mathrm{acc}(\mathcal{\tilde F}) \defn \E_{(\tilde\va,\tilde\vb)\in\mathcal{\tilde F}}\bigg[\E_{\vxi\sim\mathcal{N}_{\tilde\vdelta}} \! \bigg[ \mathbf{1}_{\tilde\vb}\left(\arg\max_{\vw\in\tilde\mW} \frac{\T\vw}{\norm{\vw}}(\tilde\va+\vxi) \right) \bigg]\bigg],
\end{equation}

which can be evaluated numerically using only the corpus statistics. In particular, note that $\tilde\va$, $\tilde\vb$, and the statistics of $\vxi$ are functions of the embedding dimension. Given some performance threshold $P$, numerically solving $\mathrm{acc}(\mathcal{\tilde F})=P$ for $d$ will give a theoretical estimate for $d_\mathrm{crit}$. The threshold $P$ can be chosen arbitrarily; in our experiments we chose $P=0.05$.



\subsection{Evidence for dynamical alignment}
\label{appdx:derivations-QR}

Here we give theoretical evidence that the results of \cref{thm:sigmoidal} very closely approximate the dynamics of a model with small random initialization. Specifically, let $\tilde s_k(t)$ denote the singular value dynamics under aligned initialization (the setting of \cref{thm:sigmoidal}), and let $s_k(t)$ be the dynamics with arbitrary initialization with scale $\sigma$ (e.g., elements of $\mW$ initialized i.i.d. Gaussian with variance $\sigma^2$). We will show that as $\sigma^2\to 0$, we have that $\abs{\tilde s_k(t)-s_k(t)}\to0$ for all modes $k$ and all times $t$. Furthermore, defining again the eigenbasis overlap $\O\defn \T{\mV^\star}\mV$, we will show that $\O_{[:d,:d]}\to\mI_d$ as $\sigma^2\to 0$ and $t\to\infty$.

Our starting point will be \cref{eq:Wdot}:
\begin{equation}
    \dot \mW = \mW \left(\Mstar - \T\mW\mW\right),
    \label{eq:Wdot2}
\end{equation}
where we have conveniently rescaled time to absorb constant scalar factors.

We are never interested in the left singular vectors of $\mW$. Both optimization and downstream task performance are invariant to arbitrary orthogonal rotations from the left. For this reason, we consider all $\mU\mW$ to be in the same equivalence class as $\mW$, for any orthogonal $\mU$. Without loss of generality, we assume that at initialization the left singular vectors of $\mW$ are given by the identity matrix: $\mW(0)=\mS(0)\T\mV(0)$ where $\mS$ is the diagonal matrix of singular values.

Multiplying \cref{eq:Wdot2} by $\mV^\star$ from the right, we have
\begin{equation}
    \dv{t} (\mS\T\O) = \mS\T\O\left(\Lstar - \O\mS^2\T\O\right).
\end{equation}

The main trick will be in choosing a convenient reparameterization. Motivated by the expectation that we will see sequential learning dynamics starting from the top mode and descending into lower modes, we are interested in a parameterization in which the dynamics are expressed in an upper-triangular matrix. We can achieve this using a QR factorization. Reparameterizing $\mS\T\O \to \mQ\mR$, we have
\begin{equation}
    \dot\mQ\mR + \mQ\dot\mR = \mQ\mR\left(\Lstar - \T\mR\mR\right),
\end{equation}
where $\mQ$ is orthogonal and $\mR$ is upper triangular. Note that since we have only transformed $\mW$ with orthogonal rotations (from left and right), the singular values of $\mW$ are the singular values of $\mR$. Furthermore, since $\mR$ is upper triangular, its singular values are simply the diagonal elements. Thus, to examine the singular value dynamics of $\mW$, it is sufficient to examine the diagonal dynamics of $\mR$. To proceed, we left-multiplying by $\T\mQ$ and rearrange, finding that
\begin{align}
    \dot\mR &= \mR\left(\Lstar - \T\mR\mR\right) - \T\mQ\dot\mQ\mR \\
    &= \mR\Lstar - (\mR\T\mR + \T\mQ\dot\mQ)\mR\\
    &= \mR\Lstar - \tilde\mR\mR,
    \label{eq:Rdot}
\end{align}
where we define $\tilde\mR \defn \mR\T\mR + \T\mQ\dot\mQ$ and note that $\tilde\mR$ must be upper triangular. This is because the time derivative on the LHS is upper triangular (to maintain the upper-triangularity of $\mR$), and the first term on the RHS is upper triangular. Thus the second term must also be upper triangular. It is not hard to show that if $\mR$ is upper triangular and $\tilde\mR\mR$ is upper triangular for some matrix $\tilde\mR$, then $\tilde\mR$ must also be upper triangular.

In fact, this is enough to fully determine the elements of $\tilde\mR$. We know that $\T\mQ\dot\mQ$ is antisymmetric (since $\T\mQ\mQ=\mI$ by orthogonality, $\T\mQ\dot\mQ + \T{\dot\mQ}\mQ=\mathbf{0}$). Additionally using the fact that $\mR\T\mR$ is symmetric and imposing upper-triangularity on the sum, we have that
\begin{equation}
    \tilde\mR_{ij} =
    \begin{cases}
        2(\mR\T\mR)_{ij} &\qtext{if} i < j\\
        (\mR\T\mR)_{ii} &\qtext{if} i = j\\
        0 &\qtext{if} i > j\\
    \end{cases}.
\end{equation}

Here, we take a moment to examine the dynamics in \cref{eq:Rdot}. Treating the initialization scale $\sigma$ as a scaling variable, we expect that $\mR_{ij}\sim\sigma$. Thus, in the small initialization limit, we expect the second term (which scales like $\sigma^3$) to contribute negligibly until late times; initially, we will see an exponential growth in the elements of $\mR$ with growth rates given by $\Lstar$. Later, $\mR$ will (roughly speaking) reach the scale of ${\Lstar}^\half$, and there will be competitive dynamics between the two terms. We will now write out the elementwise dynamics of $\mR$ to see this precisely.
\begin{align}
    \dot\mR_{ij} &= \mR_{ij}\lambda^\star_j - \sum_{ j\geq k \geq i}\tilde\mR_{ik} \mR_{kj} \\
    &= \mR_{ij}\lambda^\star_j - \sum_{j\geq k \geq i} \sum_{\ell\geq k} (2-\delta_{ik})\mR_{i\ell}\mR_{k\ell}\mR_{kj} \\
    &= \mR_{ij}\lambda^\star_j - \sum_{\ell\geq i} \mR_{i\ell}^2\mR_{ij} - 2\sum_{j\geq k > i} \sum_{\ell\geq k} \mR_{i\ell}\mR_{k\ell}\mR_{kj} \\
    &= \mR_{ij}\lambda^\star_j - \sum_{\ell\geq i} \mR_{i\ell}^2\mR_{ij} - 2\sum_{j\geq k > i} \mR_{ij}\mR_{kj}^2
    - 2\sum_{j\geq k > i} \sum_{\ell\geq k} (1-\delta_{\ell j})\mR_{i\ell}\mR_{k\ell}\mR_{kj} \\
    &= \left(\lambda^\star_j - \sum_{\ell\geq i} \mR_{i\ell}^2 - 2\sum_{j\geq k > i} \mR_{kj}^2 \right)\mR_{ij}
    - 2\sum_{j\geq k > i} \sum_{\ell\geq k} (1-\delta_{\ell j})\mR_{i\ell}\mR_{k\ell}\mR_{kj}.
\end{align}

We have separated the dynamics of $\mR_{ij}$ into a part that is explicitly linear in $\mR_{ij}$ and a part which has no explicit dependence on $\mR_{ij}$. (Of course, there is coupling between all the elements of $\mR$ and $\mR_{ij}$ through their own dynamical equations.) So far, everything we have done is exact. Now, we make approximations.

Our first approximation is to completely ignore the second term on the RHS. We will justify this at the end of the derivation by arguing that its contribution to the dynamics is negligible compared to the first term at all times. This leaves the following approximate dynamics:
\begin{equation}
    \dot\mR_{ij} = \left(\lambda^\star_j - \mR_{ii}^2 - 2(1-\delta_{ij})\mR_{jj}^2 - \sum_{\ell> i} \mR_{i\ell}^2 - 2\sum_{j > k > i} \mR_{kj}^2 \right)\mR_{ij}.
\end{equation}
We will show that, at all times, only the diagonal elements of $\mR$ contribute non-negligibly. In this case, we may simplify further and obtain:
\begin{equation}
    \dot\mR_{ij} = \left(\lambda^\star_j - \mR_{ii}^2 - 2(1-\delta_{ij})\mR_{jj}^2 \right)\mR_{ij}.
\end{equation}
We may now directly solve for the diagonal dynamics.
\begin{equation}
    \dot\mR_{ii} = \left(\lambda^\star_i - \mR_{ii}^2  \right)\mR_{ii}.
\end{equation}
Recalling that  $\lambda_k=\mR_{kk}^2$, the solution to this equation is precisely the sigmoidal dynamics in \cref{thm:sigmoidal}, up to a rescaling of time. Since the diagonal values of $\mR$ are the singular values of $\mW$, we have proved that $\abs{\tilde s_k(t)-s_k(t)}\to0$ for all modes $k$ and all times $t$ under our approximations.

All that remains to show is that our approximations are increasingly exact in the limit $\sigma\to 0$. To do this, we examine the dynamics of the off-diagonals and show that the maximum scale they achieve (at any time) decays to zero as $\sigma\to 0$. For $i<j$ we have
\begin{align}
    \dot\mR_{ij} &= \left(\lambda^\star_j - \mR_{ii}^2 - 2\mR_{jj}^2 \right)\mR_{ij} \\
    &= \left(\lambda^\star_j - \lambda_i(t) - 2\lambda_j(t) \right)\mR_{ij}.
\end{align}

This is a linear first-order homogeneous ODE with a time-dependent coefficient, and thus it can be solved exactly:
\begin{align}
    \mR_{ij}^2(t) 
    &=  \lambda_j(0)\;e^{\lambda^\star_j t} \cdot
    \left(\frac{\lambda^\star_j}{\lambda^\star_j + \lambda_j(0)\;(e^{\lambda^\star_j t} - 1)}\right)^2 \cdot \frac{\lambda^\star_i}{\lambda^\star_i + \lambda_i(0)\;(e^{\lambda^\star_i t} - 1)}
    \\
    &= \frac{\lambda_i(t) \; \lambda_j^2(t)}{\lambda_i(0)\;\lambda_j(0) \;e^{(\lambda^\star_i+\lambda^\star_j) t}}.
\end{align}
Note that the numerator consists of factors with sigmoidal dynamics, with two different timescales. The denominator contributes an exponential decay to the dynamics. Thus, as $t\to\infty$, we see that the numerator saturates while the denominator diverges, driving the off-diagonal elements $\mR_{ij}$ to zero. Then, in the limit, we have that $\mR$ is diagonal, and therefore precisely equal to the singular value matrix $\mS$. Since the QR factorization is just a reparameterization of the SVD, we find that
\begin{equation}
    \lim_{t\to\infty}\mQ(t)\mS(t) = \lim_{t\to\infty}\mU(t)\mS(t)\T\mO(t)
\end{equation}
which is only possible if $\lim_{t\to\infty}\mO=\mI$. Thus we see that not only are the singular value dynamics identical (up to vanishing error terms) in the small initialization limit, the singular vectors also achieve perfect alignment.

Now, to finish the argument, we must show that all our previous approximations hold with increasing exactness as $\sigma\to 0$. Defining $\lambda_0\defn\sigma^2$, we will show that the maximum off-diagonal $\mR_{ij}$ across time vanishes as $\lambda_0\to 0$. We find the maximizer by solving $\dot\mR_{ij}=0$ in the limit $\lambda_0\to0$ and discarding $O(\lambda_0^2)$ terms. We obtain
\begin{equation}
    \max_t \mR_{ij}^2=\frac{\lambda^\star_i {\lambda^\star_j}^{\lambda^\star_j/\lambda^\star_i}}{\lambda^\star_i+\lambda^\star_j} \cdot \lambda_0^{(\lambda^\star_i-\lambda^\star_j)/\lambda^\star_i}
    \qqtext{when}
    t = \frac{1}{\lambda^\star_i}\log\frac{\lambda^\star_j}{\lambda_0}.
\end{equation}

We conclude that as long as the initialization scale satisfies
\begin{equation}
    \frac{\lambda^\star_i-\lambda^\star_j}{\lambda^\star_i}\log\lambda_0 \ll 0,
\end{equation}
for all $i$ and $j$, the off-diagonal dynamics will remain negligible compared to the on-diagonal dynamics. Thus our approximations are valid and the dynamics of \cref{thm:sigmoidal} apply broadly to random small initialization.


% \subsection{Evidence for dynamical alignment II}

% Here we give further theoretical evidence for dynamical alignment. We approximate the time constant for the alignment of $\O$ and show that it precedes the characteristic realization time.

% We derive this silent alignment result under a mean-field approximation in the limit of asymptotically large matrices. At a high level, the derivation technique is as follows. First, we choose some positive integer $k\leq d$ and partition the model space into a top-$k$ subspace and a bottom $(V-k)$ subspace. We then ``coarse-grain'' \cref{eq:eqmotion_diffuse} by averaging over the intra-subspace dynamics and retaining the inter-subspace dynamics. This averaging is akin to a mean-field approximation; while it is not rigorous, it may be thought of as an assumption of the theory, and is justified by excellent match to empirics. We solve the simplified dynamics and show that, with high probability, the overlap between the top-$k$ subspaces of $\mV$ and $\mV^\star$ quickly saturate to 1. Since this holds for all $k\leq d$, the top $d \times d$ submatrix of $\O$ must quickly converge to $\mI_d$.

% To begin, we rearrange \cref{eq:eqmotion_diffuse} using \cref{eq:Ldot}, and rescale time to absorb the effective learning rate. This yields
% \begin{align}
%     \dot\O\L\T\O + \O\L\T{\dot\O}
%     &= \frac{\Lstar\O\L\T\O + \O\L\T\O\Lstar}{2} - \O(\dot\L + \L^2)\T\O \\
%     &= \frac{\Lstar\O\L\T\O + \O\L\T\O\Lstar}{2} - \O\L\diag(\T\O\Lstar\O)\T\O.
%     \label{eq:eqmotion_3}
% \end{align}
% We partition \cref{eq:eqmotion_3} into four blocks, corresponding to the inter- and intra- dynamics of the subspaces of interest. For any matrix $\mA\in\R^{V\times V}$, we define its $k$-partition as
% \begin{equation}
%     \mA = 
%     \begin{pmatrix}
%         \begin{array}{c|c}
%         \mA_{k} & \hskip 1em \mA_{kb} \hskip 1em  \\[0.4em] \hline\\
%         \mA_{bk} & \mA_{b} \\[1em]
%         \end{array}
%     \end{pmatrix},
% \end{equation}
% where $\mA_k\in\R^{k\times k}$, $\mA_b\in\R^{(V-k)\times (V-k)}$, and so on. This is a conformal partition, meaning that the $k$-partition of $\mA\mB$ may be expressed purely in terms of the $k$-partitions of $\mA$ and $\mB$. This is precisely what we do for every matrix in \cref{eq:eqmotion_3}. Restricting our attention to the top $k\times k$ submatrix of \cref{eq:eqmotion_3}, we find
% \begin{align}
%     \dot\O_k\L_k\T\O_k + \dot\O_{kb}\L_b\T{\O_{kb}} + h.c.
%     &= \frac{\Lstar_k(\O_k\L_k\T\O_k + \O_{kb}\L_b\T{\O_{kb}}) + h.c.}{2} - \O_k\L_k\mD_k\T\O_k - \O_{kb}\L_b\mD_b\T{\O_{kb}}.
%     \label{eq:eqmotion_k}
% \end{align}
% where $\mD \defn \diag(\T\O\Lstar\O)$ and $+h.c.$ indicates adding the transpose of the preceding terms.

% We now take the asymptotic underparameterized limit: $d,V\to\infty$ and $d/V\to 0^+$. Since $\mW\in\R^{d\times V}$ is initialized i.i.d. Gaussian with variance $\sigma^2/V$, $\M$ is a Wishart matrix whose eigenvalues follow the Marchenko-Pastur distribution. In the underparameterized limit, we have two important properties: any randomly chosen eigenvalue of $\M$ is almost surely zero, and all the nonzero eigenvalues concentrate around $\lambda=\sigma^2$. Thus, at initialization, $\L$ is the diagonal matrix whose upper $d\times d$ block is $\sigma^2\mI_d$ and whose lower block is zero. This implies that in the limit we have
% \begin{equation}
%     \frac{\Tr\left[\O_{kb}\L_b\T{\O_{kb}}\right]}{\Tr\left[\O_{k}\L_k\T{\O_{kk}}\right]} \to 0 \qqtext{and} \L_k \to \sigma^2\mI_k.
% \end{equation}
% We will also apply a ``coarse-graining'' approximation, which quantitatively amounts to the following statement:
% \begin{assumption}
%     Let $\Tr[\Lstar_k\O_k\T\O_k]=\lambda_k^\star\Tr[\O_k\T\O_k]$ for some $\lambda_k^\star$. Then, in the limit,
%     $$\frac{|\lambda_k^\star-k^{-1}\Tr\Lstar|}{k^{-1}\Tr\Lstar} \ll 1.$$
% \end{assumption}
% In other words, $\Tr[\Lstar_k\O_k\T\O_k]$ approximately factors into the mean of $\Lstar_k$ and $\Tr[\O_k\T\O_k]$. This has the spirit of a mean-field approximation in the sense that we assume that the interaction between the top $k$ modes of the model and the top $k$ modes of the target have uniform strength equal to the mean interaction strength. We expect this to hold when the entries of $\O_k\T\O_k$ are sufficiently uncorrelated, or if $\O_k\T\O_k\approx\mI_k$.

% Tracing over \cref{eq:eqmotion_k}, applying the asymptotic results, and coarse-graining, we find a dramatically simplified equation:
% \begin{equation}
%     \Tr[\dot\O_k\T\O_k + h.c.]
%     = \lambda^\star_k\Tr[\O_k\T\O_k] - \Tr[\O_k\mD_k\T\O_k].
%     \label{eq:eqmotion_tr}
% \end{equation}
% Let us simplify the final term by two applications of the coarse-graining approximation:
% \begin{align}
%     \Tr[\O_k\mD_k\T\O_k] &= \Tr[\mD_k\T\O_k\O_k] = ck^{-1} \Tr[\mD_k] \Tr[\T\O_k\O_k] \\
%     &= k^{-1} \Tr[\O_k\Lstar_k\T\O_k + \O_{kb}\Lstar_b\T{\O_{kb}}] \Tr[\T\O_k\O_k]\\
%     &= k^{-1} \left(\lambda^\star_k\Tr[\T\O_k\O_k] + \lambda^\star_b\Tr[\T{\O_{kb}}\O_{kb}]\right) \Tr[\T\O_k\O_k],
% \end{align}
% where we have neglected any error terms that result from the coarse-graining.

% We are finally ready to quantify the dynamics of the model-target overlap. Let
% \begin{align}
%     q_k(t) \defn k^{-1}\norm{\O_k}^2_\mathrm{F} = k^{-1}\Tr[\O_k\T\O_k] = 1 - k^{-1}\norm{\O_{kb}}^2_\mathrm{F},
% \end{align}
% where the last equality follows from the fact that the rows of $\mO$ are orthonormal. Note that $0\leq q_k\leq 1$ quantifies the overlap between the top-$k$ subspace of $\M$ and the top-$k$ subspace of $\Mstar$. Then \cref{eq:eqmotion_tr} reads
% \begin{align}
%     \dot q_k
%     &= \lambda^\star_k q_k - (\lambda^\star_k q_k + \lambda^\star_b(1-q_k))q_k\\
%     &= (\lambda^\star_k - \lambda^\star_b) q_k(1-q_k).
%     \label{eq:qdot}
% \end{align}
% Thus, the evolution of $q_k$ is sigmoidal, and alignment is driven by the anisotropy of the target: $\dot q_k$ is faster if the average power in the target's upper subspace, $\lambda^\star_k$, dominates the average power in the target's lower subspace, $\lambda^\star_b$. Note that the target modes are enumerated by decreasing target power, so we always have that $\lambda^\star_k > \lambda^\star_b$.

% Solving, we find that the alignment of the top-$k$ subspace obeys
% \begin{equation}
%     q_k(t) = \frac{1}{1 + \frac{1-q_k(0)}{q_k(0)} e^{-(\lambda^\star_k - \lambda^\star_b) t}},
% \end{equation}
% where $q_k(0)\approx k/V$ for random initialization. We see that $q_k$ undergoes sigmoidal dynamics, starting at $q_k(0)$ and saturating at $1$ with a characteristic alignment time of
% \begin{equation}
%     t_{\mathrm{align},k} = \frac{1}{\lambda^\star_k - \lambda^\star_b}\ln \frac{V-k}{k}.
% \end{equation}
% Note that, at some $t\gg t_{\mathrm{align},d}$, the saturation of all $q_k$ for $k\leq d$ implies that $\O_d=\mI_d$. We have found that $\mO$, initialized as a random orthogonal matrix, monotonically approaches the identity matrix.