\section{Experimental details and additional plots}
\label{appdx:expts}

All our implementations use \texttt{jax} \citep{jax2018github}. In our comparison with the word2vec baseline, we use the \texttt{gensim} implementation of SGNS \citep{rehurek2010software}.

\subsection{Datasets.}
We train our word embedding models on two corpora. For small-scale experiments, we use the \texttt{text8} dataset found at \url{https://mattmahoney.net/dc/text.html}, which is a wikipedia subset containing 1.6 million words. For large-scale experiments, we use a subset of the November 2023 dump of English Wikipedia (\url{https://huggingface.co/datasets/wikimedia/wikipedia}), which contains 200,000 articles and 135 million words; we refer to this dataset as  \texttt{enwiki}. Both datasets were cleaned with the following steps: replace all numerals with their spelled-out counterparts, convert all text to lowercase, and replace all non-alphabetic characters (including punctuation) with whitespace. We tokenize the corpora by splitting over whitespace.

Each experiment is run with a predetermined vocabulary size $V$. Typically we chose $V=1000$ for small-scale experiments and $V=10,000$ for large-scale experiments. After computing the unigram statistics via a single pass through the corpus, the words are sorted by decreasing frequency and the words with index exceeding $V$ are removed from the corpus. Our experiments indicated that as long as the corpus is sufficiently large (as is the case here), it does not matter practically whether out-of-vocabulary words are removed or simply masked.

We use the Google analogies described in \cite{mikolov2013distributed} for the analogy completion benchmark. The analogies are available at \url{https://github.com/tmikolov/word2vec/blob/master/questions-words.txt}. We discard all analogies that contain any out-of-vocabulary words. The analogy accuracy is then computed by
\begin{equation}
    \mathrm{acc} = \frac{1}{|\data|}\sum_{(\va,\vb,\va',\vb')\in\data}
    \mathbf{1}_{\{\vb'\}} \left(\arg\min_{\vw\in\mW\setminus\{\va,\vb,\va'\}} \left\lVert \frac{\va}{\norm{\va}} - \frac{\vb}{\norm{\vb}} - \frac{\va'}{\norm{\va'}} + \frac{\vw}{\norm{\vw}}\right\rVert\right),
\end{equation}
where the 4-tuple of embeddings $(\va,\vb,\va',\vb')$ constitute an analogy from the dataset $\data$, $\mathbf{1}$ is the indicator function, and $\mW$ is the set containing the word embeddings.

\subsection{Algorithm.}
\label{appdx:expts-model}

When sampling from the positive distribution, we use a \textit{dynamic context length} to emulate the training setup of \cite{mikolov2013distributed}. While iterating, for any given word in the corpus, the width of its context is sampled uniformly between 1 and $L$, where $L$ is a hyperparameter (we often chose $L=32$). Dynamic windows effectively assign higher probability mass to more proximal word pairs, thus acting as a data augmentation technique. Importantly, since dynamic windows modify the joint skip-gram distribution $P_{ij}$, they directly alter the target $\Mstar$.

Another important empirical modification to the corpus statistics  involves the treatment of self-pairs. In particular, we enforce that pairs $(i,i)$ are sampled with equal frequency from both the positive and negative distributions (i.e., setting $P_{ii}= P_i P_i$ regardless of the true corpus statistics). This ensures that embedding vector lengths are determined primarily by words' relationships to other words, not by the circumstances of their self-cooccurrence statistics (which are typically uninformative). As a consequence, the modified $\Mstar$ is traceless.

Since $\Mstar$ is traceless and our model is positive semidefinite, one potential concern is that our model will not be able to reconstruct the negative eigenmodes of $\Mstar$. This concern becomes critical when $d\approx V$; in this case, it is necessary to use an asymmetric factorization ($\M=\T\mW_1\mW_2$) to remove the PSD constraint. However, in all our experiments we study the underparameterized regime, $d \ll \half V$. Since the top $d$ modes of $\Mstar$ have positive eigenvalues, and since the model learns greedy low-rank approximations throughout training, the model never has the opportunity to attempt fitting the negative eigenmodes before its capacity is expended. Thus, the positive semidefiniteness of our model poses no problem.

In all experiments, the model was trained with stochastic gradient descent with 100,000 word pairs (50,000 positive pairs and 50,000 negative pairs) in each minibatch. No momentum nor weight decay was used. In some experiments, the learning rate was linearly annealed at the end of training to improve convergence.

\newpage
\subsection{Specific experimental details.}

The plots in this paper were generated from different experimental setups. Here we clarify the experimental details.

\begin{itemize}
    \item \textbf{Experiment 1.} This experiment generated the plots in \cref{fig:fig1} panel B and \cref{fig:fig2} panel A. We train $\lossxe$ on \texttt{text8} with $d=128$, $V=1000$, and $L=48$. This large context window helps augment the dataset with more context pairs, since \texttt{text8} is small. We set $\Psi_i=P_i^{-1}$ and initialize with $\sigma^2=10^{-24}$. We train for 2 million steps with $\eta=0.33$ and no learning rate annealing.
    
    \item \textbf{Experiment 2.} This experiment generated the plots in \cref{fig:fig1} panel D and \cref{fig:fig2} panel B. We train $\losssym$ on \texttt{enwiki} with $d=200$, $V=10,000$, and $L=32$. We set $\Psi_i=P_i^{-1}$ and initialize with $\sigma^2=10^{-20}$. We train for 2 million steps with $\eta=2$ and no learning rate annealing. 
    
    \item \textbf{Experiment 3.} This experiment generated the plots in \cref{fig:fig1} panel C and \cref{fig:fig2} panel C. We train $\lossxe$ on \texttt{text8} with $d=100$, $V=1000$, and $L=48$. We vary $\Psi_i$ from $P_i^{-1}$ to $P_i^0$ and initialize with $\sigma^2=10^{-20}$. We train for 1 million steps with $\eta=1$ and linear learning rate annealing starting at 750000 steps. 
    
    \item \textbf{Experiment 4.} This experiment generated the plots in \cref{fig:fig3} panels A and B. We train $\losssym$ on \texttt{enwiki} with $V=10,000$, $L=32$, and $\Psi_i=P_i^{-1}$. We vary $d$ from 1 to 200 and initialize with $\sigma^2=10^{-6}$. We train for 500,000 steps with $\eta=5$ and no learning rate annealing. 
    
    \item \textbf{Experiment 5.} This experiment was used in the \cref{fig:fig2} panel D. It is identical to Experiment 2, except we use $\lossxe$ instead of $\losssym$.
\end{itemize}

\subsection{Additional plots.}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.7\textwidth]{plots/z-appdx-svlog-1k.png}
    \includegraphics[width=.7\textwidth]{plots/z-appdx-svlog-10k.png}        
    \end{center}
    \caption{
    Singular value dynamics of \textbf{Experiment 1} and \textbf{Experiment 2} (same empirical data as \cref{fig:fig2} panels A and B), shown in log-log scale. We see that \cref{thm:sigmoidal} approximately holds for $\losssym$.
    }
% \label{fig:z-appdx-svlog}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.45\textwidth]{plots/z-appdx-v1k-align.png} 
    \includegraphics[width=.45\textwidth]{plots/z-appdx-v10k-align.png}       
    \end{center}
    \caption{
    Silent alignment for different top-$k$ subspaces, \textbf{Experiment 1} on the left and \textbf{Experiment 2} on the right. \textbf{(Left)} We see that dynamical alignment coincides with the early accuracy peak at $t/\tau\approx 0.1$ and occurs well before the first singular value is realized at $t/\tau=1$. \textbf{(Right)} We empirically observe there is no silent alignment; singular vectors align with the target at roughly the same timescale as the realization timescale. Thus there is no early peak in analogy accuracy.
    }
\label{fig:z-appdx-align}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.45\textwidth]{plots/z-appdx-v1k-phenomena-log.png}
    \includegraphics[width=.45\textwidth]{plots/z-appdx-v10k-phenomena.png}
    \end{center}
    \caption{
    \textbf{(Left)} Same plot as \cref{fig:fig1} panel B, except the singular values are plotted on log scale. This reveals why the analogy accuracy is non-monotonic in time, locally peaking at $t/\tau\approx 0.1$. The dynamical alignment of the singular vectors is a necessary but not sufficient condition for analogy completion; for the embedding vectors to be performant, the singular vectors must align with $\mV^\star$ \textit{and} the singular values should satisfy $\L\approx c\Lstar$ for some scalar $c$. Serendipitously, these conditions are both approximately satisfied at $t/\tau\approx 0.1$; after that, the first singular value undergoes runaway dynamics, and the embeddings essentially collapse onto a 1D subspace (see \cref{fig:fig1} panel D). Thus the early peak in accuracy indirectly demonstrates that alignment occurs, but alignment alone is not enough to guarantee analogy accuracy. \textbf{(Right)} Equivalent plot to \cref{fig:fig1} panel B, except for \textbf{Experiment 2}. There is no early peak in analogy accuracy because there is no early dynamical alignment (see \cref{fig:z-appdx-align}).
    }
\label{fig:z-appdx-phenomena}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.8\textwidth]{plots/z-appdx-subsample0.png}
    \end{center}
    \caption{Training dynamics for \textbf{Experiment 3} in the case of no subsampling. We see that the singular value dynamics are still sequential, but there is interaction between the modes, resulting in deviations from sigmoidal dynamics.
    }
% \label{fig:z-appdx-subsample0}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.55\textwidth]{plots/z-appdx-superposition.png}
    \end{center}
    \caption{
    Plot of the \textbf{Experiment 2} normalized embeddings projected onto the subspace spanned by the fifth and eighth singular vectors of $\Mstarsym$. We omit the embeddings whose projections are below a threshold norm. We see that there are in fact three distinct concepts stored in an equiangular tight frame in this subspace: measured from the vertical, tourism is stored at $0^\circ$, science at $120^\circ$, and warfare at $240^\circ$. This suggests that some concepts are stored in superposition to account for semantic overlap.
    }
% \label{fig:z-appdx-superposition}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.6\textwidth]{plots/z-appdx-dalign.png}
    \end{center}
    \caption{Plot of the inner products between the $\tilde\vdelta_n$ of two different families of analogies. Recall that $\tilde\vdelta$ is the displacement between the theoretical embeddings (in this plot, evaluated using $d=200$) of an analogy word pair.
    We see that the $\tilde\vdelta_n$ within a class tend to be mutually aligned, whereas between classes they are uncorrelated.
    }
\label{fig:z-appdx-dalign}
\end{figure*}

\begin{figure*}[!ht]
    \begin{center}
    \includegraphics[width=.5\textwidth]{plots/z-appdx-analogy-smooth.png}
    \end{center}
    \caption{We empirically compute analogy scores as a function of model size and across different analogy subtasks. We use the following smooth metric instead of accuracy: $\mathrm{score}(\va,\vb,\va',\vb';d) = \sqrt{d}\cdot \T{\hat{\vb'}}(\hat\va'+\hat\vb-\hat\va)$.
    Since the magnitudes of inner products between random vectors in $\R^d$ scale as $1/\sqrt{d}$, we include a $\sqrt{d}$ scaling to normalize the scores and enable sensible comparisons across different $d$.
    % The $\sqrt{d}$ scaling accounts for the fact that the magnitudes of inner products between random vectors in $\R^d$ scale as $1/\sqrt{d}$; it is a way to normalize the scores to enable comparisons between different $d$.
    We see that there are no apparent emergent abilities; performance smoothly improves with model size. We see similar behavior with other smooth metrics such as MSE.
    }
\label{fig:z-appdx-analogy-smooth}
\end{figure*}