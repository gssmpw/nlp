\section{Proofs}
\label{appdx:proofs}

\sigmoidal*

\textit{Proof.} By \cref{prop:qwem}, the gradient descent dynamics of a \wem\ under $\lossxe$ with $\Psi_i=1$ are given by
\begin{equation}
    \loss(\M) = \sum_{i,j}P_iP_j(\M_{ij}-\Mstar_{ij})^2.
    \label{eq:p43-raw}
\end{equation}
We begin by showing that the gradient descent dynamics under arbitrary $\Psi_i$ are given by
\begin{equation}
    \loss(\M) = \sum_{i,j}\frac{\Psi_i \Psi_j P_i P_j}{(\sum_k\Psi_kP_k)^2}\left(\M_{ij}-Z\Mstar_{ij}+Z-1\right)^2.
    \label{eq:p43-c}
\end{equation}
This follows from the algorithmic definition of $\Psi_i$: it is a hyperparameter that modifies the unigram and skipgram distributions according to
\begin{equation}
    P_{ij} \gets \frac{\Psi_i \Psi_j P_{ij}}{\sum_{k\ell}{\Psi_k \Psi_\ell P_{k\ell}}} \qqtext{and} P_{i} \gets \frac{\Psi_i P_{i}}{\sum_{k}{\Psi_k P_{k}}}.
\end{equation}
Using $Z\defn (\sum_k\Psi_kP_k)^2/\sum_{k\ell} \Psi_k \Psi_\ell P_{k\ell}$ and evaluating \cref{eq:p43-raw}, we obtain \cref{eq:p43-c}. To justify our assumption that $Z=1$, let us substitute $\Psi_i=P_i^{-1}$ and evaluate:
\begin{equation}
    Z = \frac{V^2}{\sum_{k\ell} \Mstar_{k\ell}+1} = \frac{V^2}{V^2 \langle\Mstar_{ij}\rangle +V^2} = \frac{1}{1+\langle\Mstar_{ij}\rangle},
\end{equation}
where we used \cref{cor:qwem_xe} and use the notation $\langle\Mstar_{ij}\rangle\defn V^{-2}\sum_{ij}\Mstar_{ij}$. Note that since $\Mstar$ is simply the fractional deviation from i.i.d. statistics, we expect that $\langle\Mstar_{ij}\rangle\to 0$ as the corpus and vocabulary size get large. This justifies the assumption in the theorem. Empirically, we find that $\abs{\langle\Mstar_{ij}\rangle} < 0.02$ when using the \texttt{text8} dataset (a small standard Wikipedia subset) and using a small vocabulary $V=1000$. We expect the approximation $Z\approx 1$ to improve as the dataset gets larger and the vocabulary size increases.

Thus we assume $Z=1$, and \cref{eq:p43-c} simplifies to
\begin{align}
    \loss(\M) &= \sum_{i,j}\frac{\Psi_i \Psi_j P_i P_j}{(\sum_k\Psi_kP_k)^2}\left(\M_{ij}-\Mstar_{ij}\right)^2 \\
    &= \frac{1}{V^2}\sum_{i,j} \left(\M_{ij}-\Mstar_{ij}\right)^2.
\end{align}

Gradient flow induces the following equation of motion for the weights:
\begin{equation}
    \dot \mW = \frac{2\eta_\mathrm{alg}}{V^2} \mW \left(\Mstar - \T\mW\mW\right),
    \label{eq:Wdot}
\end{equation}
where $\eta_\mathrm{alg}$ is the algorithmic learning rate. Then the model's equation of motion is
\begin{equation}
    \dot \M = \T{\dot\mW}\mW + \T\mW\dot\mW = \frac{2\eta_\mathrm{alg}}{V^2} \left(\M\Mstar +\Mstar\M - 2\M^2\right) = \eta \left(\frac{\Mstar\M + \M\Mstar}{2} - \M^2\right),
\end{equation}
where we define the effective learning rate $\eta=4\eta_\mathrm{alg}/V^2$. Going forward, we rescale time to absorb this constant.

Let us consider the dynamics of the eigendecomposition of the model, $\M(t)=\mV(t)\L(t)\T{\mV(t)}$, in terms of the eigendecomposition of the target, $\Mstar=\mV^\star\Lstar\T{\mV^\star}$. We define the eigenbasis overlap $\O\defn \T{\mV^\star}\mV$. After transforming coordinates to the target eigenbasis, we find 
\begin{align}
    \T{\mV^\star} \dot\M \mV^\star
    &= \T{\mV^\star}(\dot\mV\L\T\mV + \mV\dot\L\T\mV + \mV\L\T{\dot\mV}) \mV^\star \\
    &= \dot\O\L\T\O + \O\dot\L\T\O + \O\L\T{\dot\O}\\
    &= \frac{\Lstar\O\L\T\O + \O\L\T\O\Lstar}{2} - \O\L^2\T\O.
\end{align}
For clarity, we rotate coordinates again into the $\O$ basis and find
\begin{align}
    \L\T{\dot\O}\O + \T\O\dot\O\L + \dot\L &= \frac{\L\T\O\Lstar\O + \T\O\Lstar\O\L}{2} - \L^2.
    \label{eq:eqmotion_diag}
\end{align}
Let us study this equation. $\O$ is an orthogonal matrix that measures the directional alignment between the model and the target. $\L$ is a diagonal matrix containing the variances of the embeddings along their principal directions. Since $\O$ is orthogonal, it satisfies $\T{\dot\O}\O + \T\O\dot\O = \mathbf{0}$ (this follows from differentiating the identity $\T\O\O=\mI$). Therefore the first two terms on the LHS of \cref{eq:eqmotion_diag}, which concern the eigenbasis dynamics, have zero diagonal; the third term, which concerns eigenvalue dynamics, has zero off-diagonal. This implies
\begin{align}
    \dot\L = \L\left(\diag(\T\O\Lstar\O) - \L\right),
    \label{eq:Ldot}
\end{align}
where $\diag(\cdot)$ is the diagonal matrix formed from the diagonal of the argument. While the scale of $\O$ is fixed by orthonormality, the scale of $\L$ is determined by the initialization scale, $\sigma^2$. Examining \cref{eq:eqmotion_diag,eq:Ldot}, we see that at initialization $\dot\L$ is order $\sigma^2$, whereas $\dot\O$ is order $1$. Therefore, in the limit of small initialization, we expect the model to align quickly compared to the dynamics of $\L$. This motivates the \textit{silent alignment ansatz}, which informally posits that with high probability, the top $d \times d$ submatrix of $\O$ converges to the identity matrix well before $\L$ reaches the scale of $\Lstar$. We give extensive theoretical and empirical justification for this ansatz in \cref{appdx:derivations-QR}.

For the purposes of this proof, we simply invoke our assumption that $\O_{[:d,:d]}=\mI_d$. Then \cref{eq:Ldot} reads
\begin{align}
    \dot\L &= \L\left(\Lstar - \L\right),
\end{align}
which are precisely the dynamics studied in \cite{saxe2014exact}. These dynamics are now decoupled, so we solve them separately. Reintroducing the effective learning rate, the solution to this equation is
\begin{equation}
    \lambda_k(t) = \frac{\lambda_k(0) \; \lambda_k^\star \; e^{\eta\lambda_k^\star t}}{\lambda_k^\star + \lambda_k(0)\left(e^{\eta\lambda_k^\star t} -1\right)}.
\end{equation}
We have thus solved for the singular value dynamics of the word embeddings (since $s_k=\sqrt{\lambda_k}$). Some useful limits:
\begin{align}
    \lambda(t) &\approx \lambda(0) \cdot e^{\lambda^\star t} &\qtext{when}\; \lambda^\star t \ll \ln \frac{\lambda^\star}{\lambda(0)} \\
    \lambda(t) &\approx \lambda^\star \left(1-\frac{\lambda^\star}{\lambda(0)}e^{-\lambda^\star t}\right) &\qtext{when}\; \lambda^\star t \gg \ln \frac{\lambda^\star}{\lambda(0)}.
\end{align}
Thus, the each singular direction of the embeddings is realized in a characteristic time
\begin{equation}
    \tau_k = \frac{1}{\lambda_k^\star}\ln \frac{\lambda_k^\star}{\lambda(0)}.
\end{equation}
Since $\lambda_k\to\lambda_k^\star$ as $t\to\infty$, in the limit we have that
\begin{equation}
    \mW(t\to\infty) = \mathrm{top}_d({\Lstar}^\half \T{\mV^\star}). \qquad \blacksquare
\end{equation}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%

\anisotropic*

\textit{Proof.} Using \cref{eq:p43-c}, setting $Z=1$, and substituting in $\mP$, algebra reveals that the loss may be written
\begin{align}
    \loss(\M) &= \half \left\lVert \mP^{\half} (\M-\Mstar) \mP^{\half} \right\rVert_\mathrm{F}^2.
\end{align}
After distributing factors and invoking the Eckart-Young-Mirsky theorem, we conclude that the rank-$d$ minimizer is
\begin{equation}
    \mP^{\half} \M_\mathrm{min} \mP^{\half} = \mathrm{top}_d \left(\mP^{\half} \Mstar \mP^{\half}\right) = \mathrm{top}_d \left(\mP^{\half} \mV^\star{\Lstar}^\half{\Lstar}^\half\T{\mV^\star} \mP^{\half}\right).
\end{equation}
It is easy to verify that $\mathrm{top}_d(\T\mA\mA)=\T{\mathrm{top}_d(\mA)} \mathrm{top}_d(\mA)$ for any matrix $\mA$. Therefore, we have that
\begin{equation}
     \M_\mathrm{min} = \T{\mW_\mathrm{min}}\mW_\mathrm{min} = \mP^{-\half} \T{\mathrm{top}_d \left(\mP^{\half} \mV^\star{\Lstar}^\half\right)} \mathrm{top}_d\left({\Lstar}^\half\T{\mV^\star} \mP^{\half}\right) \mP^{-\half}.
\end{equation}
Isolating $\mW$ yields the desired result (up to arbitrary rotations acting on the left singular vectors). We assume $\Psi_i>0$ to ensure the inverse of $\mP$ exists. $\qquad \blacksquare$