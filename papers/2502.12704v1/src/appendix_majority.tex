\section{Full Proof of Theorem~\ref{thm:nphardness_maj}}
\label{app:majority_proof}

As stated in \Cref{sec:maj}, we perform a reduction from \sat{} to \netlearn{}, adapting the proof of \Cref{thm:nphardness_bayes}.
The graph construction we use is described in \Cref{ssec:maj_graph}.

\ourparagraph{Literal notation}
We now introduce some notation for use in subsequent sections.
Let $ \ell $ be a literal of some variable $ x \in \vars $, meaning either $ \ell = x $ or $ \ell = \lnot x $.
We extend the notation of the cell to be $ \cell \ell \deq \cell x $.
We say that cell $\cell x$ is ``on'' under a decision ordering $\sigma$ if $\sigma(\lnot x_i) < \sigma(x_i)$; otherwise, cell $\cell x$ is ``off''.
We say that a \emph{literal} $ \ell $ is \emph{on} if $ \ell = x $ and $ \cell x $ is on, or $ \ell = \lnot x $ and $ \cell x $ is off; otherwise, literal $ \ell $ is off.


\subsection{Gadget Learning Rates}
We begin by examining the learning rate of an arbitrary cell under a pair of orderings in which the cell is either ``on'' or ``off''. The following lemma shows that cells achieve the same learning rate under either of these orderings, and that this learning rate is the best possible over all orderings. 

\begin{lemma}[Majority Dynamics Cell LR] \label{lemma:MD_cellLearningRate}
    Let $x \in \vars$.
    Let $ q = \frac 12 $, and $ p $ be given.
    Then \[
	\oclr (\cell x) = 2p + 3p^2 -2p^3.
    \]
\end{lemma}

\begin{proof}
    First, observe that for the optimal ordering $ \sigma^* $, it is always beneficial to put the dummy node $ d_x $ \emph{before} the nodes $ x $ and $ \lnot x $.
    This is because $ d_x $ itself has no incoming edges, so it cannot benefit from any further information, and it has edges going to $ x $ and $ \lnot x $, which can only increase their chances of getting the correct answer.
    Hence, we can see that $ \oclr(d_x) = p $.

    The case of the remaining two nodes is symmetric, WLOG let us assume that $ \sigma^*(x) < \sigma^*(\lnot x) $ (so the cell $ \cell x $ is ``off'').
    The node $ x $ then receives, apart from its private information, the action of $ d_x $.
    Due to tie-breaking, it still always chooses to believe its private signal, so $ \oclr (x) = p $.

    Finally, the node $ \lnot x $ receives the private signal, $ s_{\lnot x} $, the action of $ d_x $, $ a_{d_x} $ and the action of $ x $, $ a_x $.
    Notice that all these three pieces of information are independent (since $ x $ was not influenced by $ d_x $ due to tie-breaking).
    This makes it easy to compute the learning rate of node $ \lnot x $ as the probability that at least two of the three pieces of information are correct \begin{align*}
        \oclr(\lnot x) &= \prt{at least two of $ s_{\lnot x}, a_x, a_{d_x} $ are correct} \\
        &= 3p^2 - 2p^3.
    \end{align*}

    By linearity of expectation, it then holds that \[
	\oclr(\cell x) = 2p + 3p^2 -2p^3.
    \]
\end{proof}

We defer the question of which of the two orderings is optimal to the next section. Addressing this first requires examining the learning rates of the clause gadgets, which are also affected by cell node orderings. We begin with a general expression for the CLR of any clause gadget under a particular partial ordering. We then use this expression to compute the CLR for specific clauses, observing that the optimal clause node ordering must respect the partial ordering given earlier.

\begin{lemma}[Majority Clause LR] \label{lemma:MD_genOrderingCLR}
    For any clause $ C = \left( \ell_1 \lor \ell_2 \lor \ell_3 \right) $, let $ a,b,c $ be the literal nodes in the clause gadget corresponding to $\ell_1$, $\ell_2$, and $\ell_3$. Also, let $ \alpha, \beta, \gamma $ be the cell variable nodes corresponding to $\ell_1$, $\ell_2$, and $\ell_3$ (see \Cref{fig:alphabetagamma} for details of the construction). Suppose that $\alpha$, $\beta$, and $\gamma$ correctly predict the ground truth with probabilities $p_\alpha$, $p_\beta$, and $p_\gamma$, respectively.
    Consider a partial ordering $ \sigma $ which orders all variable and dummy nodes before all literal nodes. WLOG, suppose $ \sigma $ also orders $a$ before $b$ before $c$.
    Then the cumulative learning rate of the clause gadget under $\sigma$ is \begin{align*}
	\clr(\gadget C, \sigma) = 2p + \Pr[a] + \Pr[b] + \Pr[c], 
    \end{align*}
    where 
    \begin{align*}
        \Pr[a] &= p((1-p)p_\alpha + p(p+2)),\\
        \Pr[b] &= p^2 \left((1-p)^2 (p_\alpha + 5{p_\beta})+p (4-3 p)\right),\\
        \Pr[c] &= p^2 \left((1-p)^2 (p_\alpha(1-{p_\gamma} ({p_\beta}+p-1)+{p_\beta} p)+ 2p_\beta p -{p_\gamma}({p_\beta} (5 p-3)-3 p-2)) + p(4-3p)\right).
    \end{align*}
\end{lemma}


\begin{proof}
     Note that the dummy nodes have no incoming edges, and therefore each contributes a learning rate of $p$. We now compute the learning rates of $ a $, $ b $, and $ c $.
    
     \input{src/figures/maj_alphabetagamma}

    The first node, $a$, is correct iff \begin{enumerate}[ ]
        \item both $d_1$ and $d_2$ are correct, and at least one of their private signal and $\alpha$ is correct: \[
        \Pr[a \mid d_1, d_2] = 1 - (1-p)(1-p_\alpha),
        \]
        \item or one of $d_1$ and $d_2$ are correct, and their private signal is correct: \[
        \Pr[a \mid d_1, \bar d_2] = \Pr[a \mid \bar d_1, d_2] = p,
        \]
        \item or neither $d_1$ nor $d_2$ are correct, but both $\alpha$ and their private signal are correct: \[
        \Pr[a \mid \bar d_1, \bar d_2] = pp_\alpha.
        \]
	\end{enumerate}

	Therefore, the total probability of $a$ being correct is \begin{align*}
            \Pr[a] &= p^2 \left( 1-(1-p)(1-p_\alpha) \right) + 2p(1-p)p + (1-p)^2pp_\alpha = p (p_\alpha- p(p_\alpha -p-2))= p((1-p)p_\alpha + p(p+2)).
	\end{align*}
    
    Node $b$ is correct with conditional probabilities:
    \begin{align*}
        \Pr[b \mid d_1, d_2] &= 1 - (1-p_\beta)(1-\Pr[a \mid d_1, d_2])(1-p), \\
        \Pr[b \mid d_1, \bar d_2] &= p_\beta\Pr[a \mid d_1, \bar d_2]p + (1-p_\beta)\Pr[a \mid d_1, \bar d_2]p + p_\beta(1-\Pr[a \mid d_1, \bar d_2])p + p_\beta\Pr[a \mid d_1, \bar d_2](1-p) \\
                                  & = \Pr[b \mid \bar d_1, d_2], \\
        \Pr[b \mid \bar d_1, \bar d_2] &= p_\beta\Pr[a \mid \bar d_1, \bar d_2]p.
    \end{align*}

    The total probability of $b$ being correct is then \[
	\pr{b} = p^2 \left((1-p)^2 (p_\alpha + 5{p_\beta})+p (4-3 p)\right).
    \]

    We can also compute the following joint probabilities between $a$ and $b$, conditioning again on the dummy nodes: 
    \begin{align*}
        \Pr[a,b \mid d_1, d_2] &= \Pr[a \mid d_1, d_2],\\
        \Pr[\bar a, \bar b \mid d_1, d_2] &= (1-p_\beta)(1-p)(1-\Pr[a \mid d_1, d_2]),\\
        \Pr[a,b \mid d_1, \bar d_2] &= (1 - (1-p_\beta)(1-p)) \Pr[a \mid d_1, \bar d_2],\\
        \Pr[\bar a, \bar b \mid d_1, \bar d_2] &= (1-p_\beta p)(1-\Pr[a \mid d_1, \bar d_2]),\\
        \Pr[a,b \mid \bar d_1, \bar d_2] &= p_\beta p \Pr[a \mid \bar d_1, \bar d_2],\\
        \Pr[\bar a, \bar b \mid \bar d_1, \bar d_2] &= 1-\Pr[a \mid \bar d_1, \bar d_2].
    \end{align*}
    
    Similarly, $c$ is correct with conditional probabilities 
    \begin{align*}
        \Pr[c \mid d_1, d_2] &= p + (1-p)(p_\gamma(1-\Pr[\bar a, \bar b \mid d_1, d_2]) + (1-p_\gamma)\Pr[a, b \mid d_1, d_2]), \\
        \Pr[c \mid d_1, \bar d_2] &= p(1-(1-p_\gamma)\Pr[\bar a, \bar b \mid d_1, \bar d_2]) + (1-p) p_\gamma \Pr[a, b \mid d_1, \bar d_2] \\
        & = \Pr[c \mid \bar d_1, d_2], \\
        \Pr[c \mid \bar d_1, \bar d_2] &= p(p_\gamma(1-\Pr[\bar a, \bar b \mid \bar d_1, \bar d_2]) + (1-p_\gamma)\Pr[a, b \mid \bar d_1, \bar d_2]).
    \end{align*}
    
    Therefore, the total probability of $c$ being correct is: \[
	\pr{c} = p^2 \left((1-p)^2 (p_\alpha(1-{p_\gamma} ({p_\beta}+p-1)+{p_\beta} p)+ 2p_\beta p -{p_\gamma}({p_\beta} (5 p-3)-3 p-2)) + p(4-3p)\right).
    \]
    
    Finally, nodes $ d_1 $ and $ d_2 $ are both correct with probability $ p $.
    So the total learning rate of the clause is \begin{align*}
	2p + \pr{a} + \pr{b} + \pr{c}.
    \end{align*}
\end{proof}

\begin{lemma}[$1 \lor 0 \lor 0$ Learning Rate]\label{lemma:100CLR} 
    Let $C=\ell_1 \lor \ell_2 \lor \ell_3$ be a clause.
    Suppose that an optimal ordering $\sigma^*$ sets the literal $\ell_1$ to be ``on'', and the rest of the literals to be ``off''.
    Then, in the gadget for $C$, $\sigma^*$ places $\ell_1$ first, then the other two, and finally the node $C$.
    Further, the resulting CLR is \[
        \pone = p \left(2 + 2 p + 6 p^2 + 11 p^3 + 4 p^4 - 51 p^5 - 6 p^6 + 21 p^7 + 
   115 p^8 - 136 p^9 + 13 p^{10} + 36 p^{11} - 12 p^{12} \right).
    \]
\end{lemma}

\begin{proof}
    Since $\sigma^*$ is an optimal ordering, it must place the dummy nodes $d_1$ and $d_2$, as well as all variable nodes, before all literal nodes. Otherwise, the literal nodes receive strictly less information.
    To determine the order in which the literal nodes $\ell_1, \ell_2, \ell_3$ are placed, we can simply compute the expected learning rate for every permutation of them, and then pick the maximizing order.
    Note that $\ell_2$ and $\ell_3$ are identical, so we can treat permutations which just exchange these two as identical as well.

    We use the learning rate for the general clause gadget, as computed in \Cref{lemma:MD_genOrderingCLR}.
    We simply analyze the three non-isomorphic cases: $\ell_1=a, \ell_1=b,$ and $\ell_1=c$. Note that the first case corresponds to setting $p_\alpha = 3p^2 - 2p^3$ and $p_\beta = p_\gamma = p$, and similarly for the latter cases. We get the following cumulative learning rates: \begin{align*}
        (\ell_1=a) &\to p \left(2 + 2 p + 6 p^2 + 11 p^3 + 4 p^4 - 51 p^5 - 6 p^6 + 21 p^7 + 115 p^8 - 136 p^9 + 13 p^{10} + 36 p^{11} - 12 p^{12} \right),\\
        (\ell_1=b) &\to -p \left(-2 - 3 p + p^2 - 28 p^3 + 6 p^4 + 85 p^5 - 95 p^6 + 132 p^7 - 248 p^8 + 165 p^9 + 42 p^{10} - 84 p^{11} + 24 p^{12}\right),\\
        (\ell_1=c) &\to p \left(2 + 3 p + 2 p^2 + 21 p^3 - 25 p^4 - 29 p^5 + 143 p^6 - 440 p^7 + 684 p^8 - 458 p^9 + 54 p^{10} + 72 p^{11} - 24 p^{12}\right).
    \end{align*}
    
    The first polynomial is always strictly larger than the other two for $\frac 12 < p < 1$, so placing $l_1$ first in the ordering gives the highest learning rate in the clause. So we conclude that 
    \begin{align*}
        \pone = p \left(2 + 2 p + 6 p^2 + 11 p^3 + 4 p^4 - 51 p^5 - 6 p^6 + 21 p^7 + 
   115 p^8 - 136 p^9 + 13 p^{10} + 36 p^{11} - 12 p^{12} \right).
    \end{align*}
    Moving forward, we refer to this probability simply by $\pone$, regardless of the order that the literals appear in the clause itself.
\end{proof}

Before we continue with other clauses, note that the CLR of any clause increases with the number of true literals in the clause. One can verify this by checking that for any literal node or the clause node $C$ in the gadget, its probability of correctly predicting the ground truth increases monotonically with each of the variable probabilities. So we will only compute the CLR for clauses which involve the smallest and largest possible variable probabilities.

\begin{lemma}[{$0 \lor 0 \lor 0$, $1 \lor 1 \lor 1$ Learning Rates}]
    \label{lemma:000/111CLR}
    Let $C=\ell_1 \lor \ell_2 \lor \ell_3$ be a clause.
    Suppose that an optimal ordering $\sigma^*$ sets all three literals to false.
    Then the resulting CLR is \[
        \pzero = p \left(2 + 3 p + 2 p^2 + 25 p^3 - 42 p^4 + 23 p^5 - 67 p^6 + 102 p^7 - 
   31 p^8 - 24 p^9 + 12 p^{10}\right).
    \]
    If instead $\sigma^*$ sets all three literals to true, then the resulting CLR is  
    \begin{align*}
     \pthree = p \left(2 + 2 p + 3 p^2 + 14 p^3 + 22 p^4 - 66 p^5 - 69 p^6 + 310 p^7 - 688 p^8 + 710 p^9 + \right.\\
     \left.756 p^{10} - 2581 p^{11} + 2304 p^{12} - 558 p^{13} - 
   372 p^{14} + 264 p^{15} - 48 p^{16} \right).
    \end{align*}
\end{lemma}
\begin{proof}
    As before, the optimal ordering $\sigma^*$ must place node $C$ after all literal nodes. Observe that by symmetry, the ordering of $l_1$, $l_2$, and $l_3$ doesn't matter, since all three variable probabilities are identical. Using the \Cref{lemma:MD_genOrderingCLR} and supplying variable probabilities $p_\alpha = p_\beta = p_\gamma = p$, we get the desired CLR $p^000$.
    Supplying variable probabilities $p_\alpha = p_\beta = p_\gamma = 3p^2 - 2p^3$ gives the desired value of $\pthree$.
\end{proof}

\begin{lemma}[$ 0 \lor 1 \lor 1 $ Learning Rate]
    \label{lemma:110CLR}
    Let $C=\ell_1 \lor \ell_2 \lor \ell_3$ be a clause.
    Suppose that an optimal ordering $\sigma^*$ sets one literal to false, the other literals to true.
    Then the resulting CLR is \[
	\ptwo = p \left(12 p^8-54 p^7+76 p^6-14 p^5-40 p^4+9 p^3+12 p^2+2 p+2\right).
    \]
\end{lemma}

\begin{proof}
    WLOG assume that $ \ell_2 = \ell_3 = 1 $ and $ \ell_1 = 0 $
    We analyze the probabilities of all three (non-isomorphic) possible orderings by setting proper $ p_\alpha, p_\beta, p_\gamma $ from \Cref{lemma:MD_genOrderingCLR}. 
    This results in the following CLRs: \begin{align*}
        (\ell_1=\alpha) &\to p \left(-24 p^{10}+132 p^9-282 p^8+279 p^7-116 p^6+28 p^5-36 p^4+11 p^3+8 p^2+3 p+2\right),\\
        (\ell_1=\beta) &\to -8 p^{10}+44 p^9-86 p^8+57 p^7+18 p^6-21 p^5-11 p^4-7 p^3+15 p^2+2 p+2,\\
        (\ell_1=\gamma) &\to p \left(12 p^8-54 p^7+76 p^6-14 p^5-40 p^4+9 p^3+12 p^2+2 p+2\right).
    \end{align*}
    It is not hard to prove, that for $ p \in \left( \frac 12, 1 \right) $, the biggest value is received when $ \ell_1=\gamma $, and thus its CLR is the optimal CLR.
    This finishes the proof.
\end{proof}

\begin{lemma}[Clause Learning Rate Comparison]
\label{lemma:CLRComp}
	For $ p \in \left( \frac 12, 1 \right) $, it holds \[
	    \pthree \geq \ptwo \geq \pone \geq \pzero.
	\]
\end{lemma}

\begin{proof}
    We know the learning rates of $ \pthree ,\ptwo , \pone , \pzero $ by \Cref{lemma:100CLR,lemma:110CLR,lemma:000/111CLR}.
    The statement then follows by simple algebra.
\end{proof}


\subsection{Optimal Ordering \& Restrictions on p} 
Finally, we can now determine the optimal ordering by examining the learning rates derived in the previous section. First, we define an \emph{assignment-induced ordering} below:

\begin{definition}
    Let $\mathcal{A}: \chi \to \{0,1\}$ be any assignment of values to variables. Define the (partial) \emph{ordering induced by $\mathcal{A}$} as follows: if $\mathcal{A}(x_i)=1$, node $x_i$ is ordered after node $\lnot x_i$ (cell $\cell {x_i}$ is ``on''); otherwise, $x_i$ comes before $\lnot x_i$ (cell $\cell {x_i}$ is ``off'').
\end{definition}

\begin{definition}
    Let $\mathcal{A}: \chi \to \{0,1\}$ be an assignment of values to variables maximizing the number of satisfied clauses. Then $\mathcal{A}$ is a \emph{maximal assignment}.
\end{definition}

\begin{lemma}[Optimal Ordering] \label{lemma:bestOrder}
    Let $\mathcal{A}^*$ be a maximal assignment.  Let $p(M) < 1$ be a threshold probability determined by $M$, the number of clauses. Then for all $p \geq p(M)$, the decision ordering $\sigma^*$ which places all dummy nodes first, then all variable nodes respecting the partial ordering induced by $\mathcal{A}^*$, then all literal nodes, and finally all clause nodes, maximizes the network learning rate.
\end{lemma}
\begin{proof}
    First note that by the same reasoning as in Lemma \ref{lemma:MD_cellLearningRate}, there is an optimal ordering $\sigma^*$ which places all dummy nodes before all variable nodes. Similarly, the same reasoning as in Lemma \ref{lemma:100CLR} gives that there is an optimal $\sigma^*$ in which all literal nodes are ordered after all variable nodes, and all clause nodes are ordered after all literal nodes. So all that remains is to show that there is an optimal $\sigma^*$ which respects the ordering induced by $\mathcal{A}^*$. 

    First recall that by Lemma \ref{lemma:MD_cellLearningRate}, an optimal ordering always gives the same cumulative learning rate $p_i^0 = p_i^1$ per cell regardless of whether the cell is ``on'' or ``off''. So the ordering of variable nodes only affects the network learning rate through the clauses. Consider any other assignment $\mathcal{A}$ which is not maximal. Let $S^*$ be the number of satisfied clauses under $\mathcal{A}^*$ and $S < S^*$ that under $\mathcal{A}$. By \Cref{lemma:100CLR,lemma:000/111CLR}, the CLR for any satisfied clause is lower bounded by $\pone$, which is strictly greater than that for an unsatisfied clause, $\pzero$. So having more satisfied clauses should improve the CLR. However, note also that $\pthree \geq \ptwo \geq \pone \geq \pzero$, by \Cref{lemma:110CLR}. So in the most extreme case, all $S$ satisfied clauses under $\mathcal{A}$ are satisfied by having three true literals, while all $S^*$ satisfied clauses under $\mathcal{A}^*$ are satisfied by having one true literal. 

    Consider that extreme case, and further impose the worst-case choices of $S$ and $S^*$ by setting $S^* = M$ and $S = M-1$. Then over all clause gadgets, the ordering induced by $\mathcal{A}^*$ gives a CLR of $M \pone$, while the ordering induced by $\mathcal{A}$ gives a CLR of $(M-1) \pthree + \pzero$. In order for the ordering induced by $\mathcal{A}^*$ to maximize the network learning rate, we must have
    \begin{align*}
        M \pone &\geq (M-1) \pthree + \pzero\\
        \frac{\pone - \pzero}{\pthree - \pone} &\geq M-1
    \end{align*}
    One can verify that the left-hand side can be lower bounded by $\frac{1}{6-6p}$ for all $p \in (0.5,1)$, so the following is sufficient:
    \begin{align*}
        \frac{1}{6-6p} \geq M-1 \Longrightarrow p \geq p(M) \deq \frac{6M-7}{6M-6},
    \end{align*}
    which is well-defined for any $M \geq 2$. Thus, the $\sigma^*$ respecting the ordering induced by $\mathcal{A}^*$ is optimal for all $p \geq p(M)$.
\end{proof}

To recap, given any instance $\varphi$ of \sat{} with $N$ variables and $M$ clauses, we can construct a network $G_\varphi$ with ground truth prior $q = 1/2$ and signal accuracy $p \geq p(M)$. In particular, whenever $\varphi$ is satisfiable under some maximal assignment $\mathcal{A}^*$, there is an optimal decision ordering $\sigma^*$ which respects the ordering induced by $\mathcal{A}^*$, and which achieves a network CLR of at least $N(2p+3p^2-2p^3) + M\pone$. Otherwise, if $\varphi$ is non-satisfiable under any maximal assignment $\mathcal{A}^*$, then the optimal decision ordering $\sigma^*$ respecting the ordering induced by $\mathcal{A}^*$ achieves a network CLR of no more than $N(2p+3p^2-2p^3) + (M-1)\pthree + \pzero$. Choosing $p \geq p(M)$ allowed us to show the optimality of $\sigma^*$, as well as to separate the learning rates in networks corresponding to satisfiable and non-satisfiable formulas. All that remains to complete the reduction is to pick an appropriate choice of $\varepsilon$, such that a formula graph $G_\varphi$ achieves expected network learning rate greater than $\varepsilon$ under an optimal ordering iff $\varphi$ is satisfiable.

\subsection{Picking the Epsilon}
We will simply pick $\varepsilon$ to lie exactly halfway between the satisfiable and non-satisfiable network learning rates. Recalling that each variable gadget contains $3$ vertices and each clause gadget contains $4$ vertices, we can compute the learning rates below. For networks corresponding to satisfiable formulas, we have at least
\begin{align*}
    \frac{N\pcell + M\pone}{3N + 5M},
\end{align*}
and for networks corresponding to non-satisfiable formulas, we have at most
\begin{align*}
    \frac{N\pcell + (M-1)\pthree + \pzero}{3N + 5M}.
\end{align*}
This gives 
\begin{align*}
    \varepsilon = \frac{1}{2}\left(\frac{2N\pcell + M\pone + (M-1)\pthree + \pzero}{3N + 5M}\right),
\end{align*}
thus completing the reduction. 
