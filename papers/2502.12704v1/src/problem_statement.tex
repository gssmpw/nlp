\section{Problem Statement}
\label{sec:problem}

This work studies two popular models of opinion exchange on networks. The overarching goal is for a network of truthful, rational agents to learn a binary piece of information, which we call the state of the world or \emph{ground truth}. We can also think of this state as an optimal binary action (buying or selling a stock, voting for a political party's candidate, etc.). The agents are arranged on a directed graph $G=(V,E)$ and broadcast which of the two states they believe is more probable to their neighbors. 

More explicitly, we encode the ground truth in $ \theta \in \{0,1\} $, distributed according to $ \berd{q} $, Bernoulli distribution with probability of $q$ of taking $1$ and $1-q$ of taking $0$. Every agent $v \in V$ initially receives an independent \emph{private signal} $ s_v \in \{0,1\}$ correlated with the ground truth. They then announce a prediction $a_v \in \{ 0,1 \} $ of the ground truth along outgoing edges, so out-neighbors of $v$ may use $a_v$ to improve their own predictions. Importantly, the probabilities $p$ and $q$, as well as the graph $G$ are all common knowledge. 
This is captured in the following formal definition of a network.

\begin{definition}[Social Network]
    A \emph{social network} is $ \network \deq ( G,q,p ) $,
    where \begin{enumerate}
        \item $ G = ( V,E ) $ is a directed graph with agents as vertices,
        \item $ q \in ( 0,1 ) $ is the prior probability of $\theta = 1$,
        \item $ p \in ( \frac 12, 1 ) $ is the accuracy of agents' private signals $s_v \in \{0,1\}$, such that \[\pr{s_v = 1 \suchthat \theta = 1} = \pr{s_v = 0 \suchthat \theta = 0} = p, \quad \forall v \in V.
            \]
    \end{enumerate}
    We further denote $ n \deq \absolute{V} $.
\end{definition}

We consider a classic asynchronous \emph{sequential} model ~\cite{Golub2017-qo}, in which agents announce their predictions in a \emph{decision ordering}, given by a one-to-one mapping $\sigma: V \to [n]$.
We denote the set of all possible orderings by $\Sigma_n$.
At every time step $i$, agent $v = \sigma^{-1}(i)$ makes an announcement $a_v \in \{0,1\}$.
The announcement depends on the agent's private measurement $ s_v $, along with the \emph{previous announcements of in-neighbors}, which we denote as a tuple $ N_v $, defined as \[
    N_v \deq ( a_u \suchthat u \in V \land uv \in E \land \sigma(u)<\sigma(v) ).
\] 
We call the tuple $X_v = (s_v) \cup N_v$ the \emph{inputs} of node $v$. 
This setup of limiting visibility to an agent's neighborhood has been studied in a number of recent papers~\cite{Bahar2020-am,arieli2020social,lu24enabling}.

When making announcements, agents follow an \emph{aggregation rule}, which is a function $ \mu:  ( X_v, G, \sigma ) \mapsto a_v $.
Broadly speaking, aggregation rules can either be Bayesian or non-Bayesian. 
In the \emph{Bayesian} model,  agents are fully rational
and make predictions according to their posterior probability for $\theta$, given their inputs and knowledge of the network topology $G$. In particular, agents take into account the correlation between their inputs resulting from the network topology and from the current ordering $ \sigma $. \[
     \mu^B(X_v, G, \sigma) \deq \begin{cases}
         1 & \text{if $\Pr_{G,\sigma}[\theta = 1 \suchthat X_v] > \frac 12$,} \\
         0 & \text{if $\Pr_{G,\sigma}[\theta = 0 \suchthat X_v] > \frac 12$,} \\
         \berd{\frac 12} & \text{otherwise.}
     \end{cases}
 \]

We also consider a non-Bayesian model, in which agents have bounded rationality and instead use simpler heuristic rules.
This is perhaps a more practical model, as computing posterior probabilities in arbitrary networks can become computationally expensive. 
In particular, we examine the \emph{majority dynamics} model, in which agents simply follow the majority among their inputs~\cite{Bahar2020-am,Shoham1992-ir,Laland2004-ej}. Since this model does not require agents to take into account correlations between their inputs derived from the network topology or the ordering, we omit $G$ and $ \sigma $ as inputs to $\mu^M$: \[
    \mu^M(X_v) \deq \begin{cases}
        1 & \text{if $  \frac 1{\absolute{X_v}}\sum_{x \in X_v} x > \frac 12 $,} \\
        0 & \text{if $  \frac 1{\absolute{X_v}}\sum_{x \in X_v} x < \frac 12 $,} \\
        s_v & \text{otherwise.}
    \end{cases}
\]

Finally, we quantify how successful the network is in predicting the ground truth by defining the following notion of a learning rate.
\begin{definition}
    The \emph{cumulative learning rate} (CLR) of a network $ \network $ under the ordering $ \sigma $ and an aggregation rule $ \mu $ is \[
		\clr (\network, \sigma, \mu) \deq \E_{\theta, s} \left[ \sum_{v \in V}^{} \mathds{1}_{\{a_v = \theta\}} \right] = \sum_{v \in V} \Pr_{\theta, s}\left[a_v = \theta\right],
	\]
    where the equality follows from linearity of expectation.
    Further, the \emph{learning rate} (LR) of a network $ \network $ under the ordering $ \sigma $ is simply \[
		\lr (\network, \sigma, \mu) \deq \tfrac 1 n \clr (\network, \sigma, \mu).
	\]
\end{definition}

We are mainly interested in the \emph{optimal} learning rate of a network, defined as follows.

\begin{definition}[Optimal LRs]
    The \emph{optimal cumulative learning rate} of a network $ \network $ is \[
        \oclr (\network, \mu) \deq \max_{\sigma \in \Sigma_n} \clr (\network, \sigma, \mu),
    \]
    and the \emph{optimal learning rate} of a network $ \network $ is \[
        \olr (\network, \mu) \deq \max_{\sigma \in \Sigma_n} \lr (\network, \sigma, \mu).
    \]
\end{definition}

Note that when $ p $ and $ q $ are clear from the context, we use the learning rate notation with only the graph, for example $ \olr (G, \mu) = \olr(( G,p,q ), \mu) $.
We can now present a formal definition of our main focus, the \netlearnopt{} optimization problem, and \netlearn{}, its decision version.

\begin{definition}[\netlearnopt{}]
    Suppose $\mu$ is a fixed aggregation rule.
    Given a network $ \network $, the \netlearnopt{} problem is to maximize  $ \lr (\network, \sigma, \mu) $, over $ \sigma \in \Sigma_n $.
\end{definition}


\begin{definition}[\netlearn{}]\label{def:decnetlearn}
    Suppose $\mu$ is a fixed aggregation rule.
    Given a network $ \network $ and a constant threshold $ \varepsilon \in (0,1)$,
    the \netlearn{} decision problem asks whether \[
            ( \exists \sigma \in \Sigma_n )\quad \lr (\network, \sigma, \mu) \geq 1-\varepsilon.
        \]
\end{definition}

Note that \Cref{def:decnetlearn} can be formulated equivalently by asking whether an optimal ordering $\sigma^*$ which maximizes the network learning rate achieves LR at least $1-\varepsilon$.

In \Cref{sec:bayes,sec:maj}, we focus on the decision problem, offering a proof that it is \np-hard for $ \mu = \mu^B $ and $ \mu = \mu^M $.
Finally, in \Cref{sec:approx}, we use insights from the \np-hardness proofs to show \netlearnopt{} is hard to even approximate.
Surprisingly, this gives us a stronger \np-hardness statement, showing that \netlearn{} is \np-hard even if we arbitrarily fix the agents' accuracy $ p \in ( \frac 12, 1 ) $.
