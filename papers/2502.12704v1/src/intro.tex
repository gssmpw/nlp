\section{Introduction}

Information acquisition, opinion formation and decision making are deeply embedded in a social context.
There are many situations in which people make decisions using information that carries uncertainties, and such decisions are easily influenced by the decisions of others. It is therefore of paramount importance to understand how to effectively use information of inherent uncertainty, while considering how social exchanges can reduce such uncertainties.

Broadly, there are two primary families of models that capture decision making processes in a social network. The first family considers opinion dynamics~\cite{Mossel2017-sd, Acemoglu2011-tx}, where all agents have individual opinions that are repeatedly updated based on information exchange with others. Opinion dynamics studies the evolution of an opinion landscape over time and asks whether, and how quickly, a consensus can be obtained. Recent work also studies the lack of consensus (i.e., polarization) and asks whether this can be modeled and explained with natural factors~\cite{Wang2022-uy}. The second family considers sequential decision making processes~\cite{Golub2017-qo,Mobius2014-oy}, where agents make one-shot decisions. It is also assumed that there is an unknown ground truth state which all agents wish to learn, whereas opinion dynamics often omits such an assumption. Therefore, this second setting is termed sequential social `learning'. 

\ourparagraph{Our Model.}
We consider the classical sequential learning model, where $n$ agents sequentially predict an unknown ground truth $\theta$~\cite{Jadbabaie2012-ob,Mossel2014-mv}. Each agent has an independent private measurement of $\theta$. We consider the `bounded belief' setting, where each agents' private measurement has the same probability $p$, a constant away from 1, of being correct~\cite{Smith2000-wk,Acemoglu2011-vj}.
In addition, each agent has access to the predictions of agents earlier in the sequence. Ideally, agents can use the information extracted from the earlier predictions to improve their own prediction. However, a well-known problem that arises is \emph{information cascade} or \emph{herding}~\cite{Banerjee1992-ra,Bikhchandani1992-rs,Welch1992-yt,Smith2000-wk,Chamley2004-or}, where sufficiently many wrong predictions early on can trigger all subsequent agents to ignore their own private signals and `follow the herd'. Notably, this can occur even for fully rational agents, in the absence of behavioral factors like peer pressure. So herding arises not by fault of the agents, but as a result of the sequential structure of the setting. Indeed, whenever a large enough part of the crowd discards its private information, whether rationally or not, the crowd as a whole is unable to learn the ground truth. 

Motivated by the problem of information cascades, a lot of follow-up work examines how to restore truth learning in crowds.
One approach is to limit the visibility of agents~\cite{Smith1991-sy,Sgroi2002-rz,Acemoglu2011-vj} so that incorrect predictions do not propagate. A natural setup is to consider a social network, rather than an unstructured crowd, in which an agent can only see the actions of its neighbors earlier in the sequence~\cite{Bahar2020-am,arieli2020social,lu24enabling}.
It can be shown that certain network structures, coupled with a good agent decision ordering, can in fact achieve a strong learning result called asymptotic network-wide truth learning~\cite{lu24enabling}: that all $n$ agents, except $o(n)$ of them, successfully predict the ground truth as $n$ goes to infinity. In other words, the average network learning rate, or the average success probability over all agents, approaches $1$. 

One natural open problem from~\cite{lu24enabling} asks whether given a social network, we can either find a good decision ordering or decide if a good ordering exists. The authors in~\cite{lu24enabling} presented sufficient conditions and impossibility results, but the big picture is still largely unclear. In general, there are two factors that prohibit truth learning. If the network is too sparse or a constant fraction of agents make  decisions using only their private signals, then their success probability is bounded by a constant away from $1$, and network-wide truth learning is already doomed. On the other hand, if the network is too dense or almost every agent is well connected with agents earlier in the ordering, then herding happens. Again in this case, truth learning is not possible. Regarding the decision problem, ~\cite{lu24enabling} conjectures that deciding if a network admits an ordering enabling asymptotic truth learning is \np-hard. 

\ourparagraph{Our  Results.} This work focuses on the problem of deciding if the best possible average learning rate in a network exceeds a given threshold $1-\varepsilon$. We prove that this problem is NP-hard both for networks of fully rational agents and those with bounded rationality. Intuitively, we expect this problem to be hard---naively, there are exponentially many decision orderings to check---but proving it formally is highly non-trivial. At a cursory glance, the two barriers for truth learning suggest that a successful network should avoid very sparse and very dense structures. So a natural approach may be to relate network learning and the maximum independent set or maximum clique decision problems. However, it remains unclear how to characterize a network's learning rate by the sizes of its max independent set or its max clique. Thus, it is not obvious how to reduce either problem to the network learning problem. 

We instead use a reduction from \sat, the canonical \np-hard decision problem asking if a given 3-CNF formula is satisfiable.
We construct a network from an input 3-CNF instance and map decision orderings on the network to boolean variable assignments.
In our construction, satisfied clauses correspond to subgraphs with higher learning rates than unsatisfied ones.
Therefore, the larger the number of satisfied clauses, the higher the network learning rate.
We prove that all orderings corresponding to a satisfying assignment achieve a strictly higher network learning rate than those corresponding to non-satisfying assignments.
Thus, deciding if the optimal learning rate exceeds a well-chosen threshold immediately implies an answer to \sat{}.
While the high-level idea is clean, the details are fairly technical due to the dependence between predictions of neighboring agents.
We defer the most technical parts of the proof to the Appendix.

Next, we focus on the approximation hardness of this problem.
We construct a reduction to \maxsat{}, which asks for the maximum number of satisfiable clauses under any variable assignment in a \sat{} instance.
It is known~\cite{Hastad2001-fg} that computing a solution that satisfies more than $\frac 78 M^*$ clauses is \np-hard, where $M^*$ is the maximum number of clauses that can be satisfied.
Using the ideas from the \sat{} reduction, we prove that it is impossible to find an efficient approximation up to an arbitrary constant, unless $\p=\np$.
Specifically, this also allows us to strengthen our previous claim about \np-hardness---that finding the optimal learning rate is \np-hard for any fixed prior $ p \in (\sqrt{7/8}, 1) $.

As noted above, we present hardness for sequential social learning both when agents are fully rational and when they have bounded rationality. Fully rational agents can be realistic, for instance, in modelling financial traders, while agents with bounded rationality may be more faithful models for voters or users on a social platform. More concretely, fully rational agents predict via Bayesian inference, whereas agents with bounded rationality use a simpler heuristic, such as majority vote over all available signals. 

From a practical perspective, Bayesian inference is computationally expensive to implement in simulations, and all prior work on this topic~\cite{Bahar2020-am,arieli2020social,lu24enabling} used bounded rationality in experiments, such as majority vote. However, while simpler for implementation, majority vote is actually harder for analysis. With the Bayesian model, the success rate of an agent is at least as high as the highest success rate of its earlier neighbors, since a node can never do worse than copying the action of an earlier neighbor. This monotonicity can be helpful for constructing a good ordering. However, with majority vote, this nice property no longer holds, so the validity of the hardness reduction needs to be reconsidered for the majority vote model. We modify the construction and restore the same hardness claims for the majority vote model. 
 
\ourparagraph{Related Literature.}
A number of recent works consider models with repeated observations and information exchange, asking if the agents successfully learn the ground truth 
(see e.g., \cite{Jadbabaie2012-ob,Mossel2014-mv}). One major model choice is how an agent aggregates information from available signals in the network. The most natural choice is the Bayesian model, in which agents compute the posterior probability for $\theta$ using all available information and any common knowledge, such as the network topology. 
A recent line of work showed that computing an agent's prediction via Bayesian inference with repeated opinion exchange is PSPACE-hard~\cite{hazla2019reasoning,Hazla2021-vf}. It is one of the few works, to the best of our knowledge, that formally characterizes the complexity of decision making in a social network. 
From this perspective, our work adds to the relatively scarce literature that combinatorial complexity arises in a sequential, one-shot setup with Bayesian or non-Bayesian inferences, where decision orderings need to be carefully decided with respect to the network topology. 

Lastly, we remark that computing and approximating posterior probabilities in general Bayesian networks is known to be hard~\cite{Cooper1990-fn,Dagum1993-gd,Kwisthout2018-az}.
Our setting considers a restricted variant of a Bayesian network---for example, all private signals have the same probability of error.
Furthermore, we are interested in the average accuracy of the agents in the network, not their individual posterior probabilities.
Thus, to the best of our knowledge, there is no straightforward way to translate the existing complexity results for a general Bayesian network to our setting.
