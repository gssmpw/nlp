\section{Proof of \np-hardness for the Bayesian Model}
\label{sec:bayes}

We now state one of the main results of this paper---the hardness of \netlearn{}.

\begin{theorem}[ ]\label{thm:nphardness_bayes}
	\netlearn{} with the Bayesian learning rule $\mu = \mu^B$ is \np-hard.
\end{theorem}

\subsection{Proof Idea}

We perform a reduction from \sat{} to \netlearn{}.
Specifically, we assume in our reduction that all formulas have exactly 3 distinct literals in each clause, and never a literal along with its negation.
For a given formula $ \varphi $, we construct a network $ \network $ and an $ \varepsilon > 0 $, such that
\[
    \olr(\network, \mu^B) \geq 1-\varepsilon \quad \iff \quad \text{$ \varphi $ is satisfiable.}
\]

The network $ \network $ consists of a directed graph $ G $, the ground truth prior $ q $, and the prior of the agents' private signals $ p $.
Our reduction requires setting $ p $ and $ G $ based on the formula, but it allows us to set $ q=\frac 12 $, regardless of $ \varphi $.

\subsection{Graph Construction \& Notation}
\label{ssec:graph}

First, we construct the directed graph $G$.
Our construction consists of $ N $ \emph{variable cells}, and $ M $ \emph{clause gadgets}, where $ N $ and $ M $ are the number of variables and clauses in $ \varphi $, respectively.
We define the cell and gadget first, and then define the full graph in \Cref{def:bayesian_graph}.

\begin{restatable}[Variable cell]{definition}{variableCell}
\label{def:cell}
    Let $ x $ be a variable of a formula $ \varphi $.
    A \emph{variable cell} of $ x $ is a directed graph $ \cell x = \left( V_x, E_x \right) $, where \begin{enumerate}[ ]
    	\item $ V_x = \left\{ x, \lnot x, d_x \right\} $, and
	\item $ E_x = \left\{ \left( d_x, x \right), \left( d_x, \lnot x \right), \left( x, \lnot x \right), \left( \lnot x, x \right) \right\} $.
    \end{enumerate}
\end{restatable}

\Cref{fig:cell_bayesian} depicts a cell for some variable $x_i$.

\begin{definition}[Clause gadget]
    Let $ C = j \lor k \lor \ell $ be a clause of a 3-CNF formula $ \varphi $, where $ j \neq k \neq \ell $ are some literals.
    Then the \emph{clause gadget} is $ \gadget C = \left( V_C, E_C \right) $, where \begin{enumerate}[ ]
    	\item $ V_C = \left\{ j,k,\ell \right\} $, and
    	\item $ E_C = \left\{ \left( x,y \right) \suchthat x,y \in \left\{ j,k,\ell \right\} \land x \neq y \right\} $.
    \end{enumerate}
\end{definition}

\begin{restatable}[Formula graph]{definition}{bayesianGraph}\label{def:bayesian_graph}
	Let $ \varphi = C_1 \land C_2 \land \dots \land C_M $ be a CNF formula of variables $ \vars = \left\{ x_1, \ldots, x_N \right\} $, where each clause $ C_i $ is the disjunction of exactly three literals.
	Then $ G_\varphi $ is a disjoint union of the graphs $ \cell {x_i} $ for all $ {x_i} \in \vars $, and $ \gadget {C_i} $ for all clauses $ C_i \in \varphi $.
        Additionally, there is an edge to each of the vertices in every $ \gadget {C_i} $ from the corresponding literal nodes in the respective variable cell.
\end{restatable}

Note that that there are no incoming edges to any cell, except from within the same cell, so the learning rate of each cell is determined only by the ordering of its own vertices. Also, no two clause gadgets share any vertices, so the ordering of vertices within a gadget only affects that gadget.
For an illustration of the formula graph construction, refer to \Cref{fig:gphi_bayesian}, where we give a sample formula graph for the formula $ \varphi = x \lor y \lor \lnot z $.

\input{src/figures/bayes_cell}
\input{src/figures/bayes_gadget}

\ourparagraph{Ordering-assignment relation}\label{par:ordering_literals}
To determine satisfiability of $\varphi$ from the learning rate of $ G_\varphi $, we map vertex orderings to variable assignments.
We then show that orderings achieving higher learning rates correspond to assignments with more satisfied clauses.
For a more detailed description of both the mapping and its properties, see \Cref{ssec:assignment_bayes}.

For clarity, we introduce some more notation. Let $ \ell $ be a literal of some variable $ x \in \vars $, meaning $ \ell = x $ or $ \ell = \lnot x $.
We say that cell $\cell x$ is in one of two states: it is ``on'' under a decision ordering $\sigma$ if $\sigma(\lnot x) < \sigma(x)$; otherwise, cell $\cell x$ is ``off''.
We say that the literal $ \ell $ is ``on'' if $ \ell = x $ and $ \cell x $ is on, or $ \ell = \lnot x $ and $ \cell x $ is off; otherwise, literal $ \ell $ is ``off''.
For brevity, we further denote $ \cell \ell \deq \cell x $, regardless of whether $ \ell = x $ or $ \ell = \lnot x $.

\subsection{Gadget Learning Rates}
This section lists the learning rates of the cells and clause gadgets under Bayesian aggregation. We assume WLOG for this section that $\theta = 1$, and compute all probabilities in this section conditioned on $\theta = 1$. Since we always take $\theta$ to be uniform on $\{0,1\}$, this yields the same values as taking the probability over $\theta$ as well. We note that our computations were verified using Wolfram Mathematica.

We begin by examining the learning rate of an arbitrary cell under a pair of orderings in which the cell is either ``on'' or ``off''. The following lemma shows that cells achieve the same learning rate under either of these orderings, and that this learning rate is the best possible over all orderings. 

\begin{lemma}[Bayesian Cell LR] \label{lemma:bayesian_cellLearningRate}
    Let $x \in \vars$.
    Let $ q = \frac 12 $, and $ p > \frac 12 $ be given.
    Then \[
	\oclr (\cell x) = \tfrac 52 p + \tfrac 32 p^2 - p^3.
    \]
    In particular, if under an optimal ordering $ \sigma^* $ a literal $ \ell $ is on, then its corresponding literal node has learning rate $ \clr (\ell, \sigma^*, \mu^B) = \frac p2 + \frac 32 p^2 - p^3 $; otherwise $ \clr (\ell, \sigma^*, \mu^B) = p $.
\end{lemma}

\begin{proof}
    First, observe that for the optimal ordering $ \sigma^* $, it is always beneficial to put the dummy node $ d_x $ \emph{before} the nodes $ x $ and $ \lnot x $.
    This is because $ d_x $ has no incoming edges, so it cannot acquire more information by going later, and it has edges going to $ x $ and $ \lnot x $, which can only increase their chances of getting the correct answer.
    Hence, we can see that $ \oclr(d_x) = p $.

    The case of the remaining two nodes is symmetric, so WLOG, let us assume that $ \sigma^*(x) < \sigma^*(\lnot x) $ (so the cell $ \cell x $ is ``off'').
    The node $ x $ then receives the action of $ d_x $, which is i.i.d. from its private information.
    So node $ x $ chooses its action correctly either if both $ s_x $ and $ a_{d_x} $ are correct, where $a_{d_x}$ is the action chosen by node $d_x$, or if exactly one of the two are correct and $ x $ tiebreaks correctly. The first outcome occurs with probability $ p^2 $, and the second with probability $ 2p (1-p) \frac 12 $.
    Thus, the learning rate of node $ x $ is \[
        \oclr(x) = p^2 + p - p^2 = p.
    \]

    Finally, the node $ \lnot x $ receives its private signal, $ s_{\lnot x} $ and the actions $ a_{d_x} $ and $ a_x $.
    Notice that these three pieces of information are \emph{not} independent, since $ x $ was influenced by $ d_x $.
    We perform case analysis on the relative likelihood \[
        \Lambda \deq \frac{\pr{\theta = 1 \mid X_{\lnot x}}}{\pr{\theta = 0 \mid X_{\lnot x}}} = \frac{\pr{X_{\lnot x} \mid \theta = 1}}{\pr{X_{\lnot x} \mid \theta = 0}},
    \]
    where the equality follows from Bayes' theorem and from the fact that the prior $ q = \frac 12 $.
    Recall that $ X_{\lnot x} = \{s_{\lnot x}, a_x, a_{d_x}\} $, and that $ s_{\lnot x} $ is independent of the other two.
    Thus, $ \Lambda $ can be expressed as
    \begin{align*}
        \Lambda = \frac{\pr{s_{\lnot x} \mid \theta = 1}}{\pr{s_{\lnot x} \mid \theta = 0}} \cdot
        \frac{\pr{a_{d_x} \mid \theta = 1}}{\pr{a_{d_x} \mid \theta = 0}} \cdot
        \frac{\pr{a_x \mid \theta = 1, a_{d_x}}}{\pr{a_x \mid \theta = 0, a_{d_x}}}.
    \end{align*}
    We compute $ \Lambda $ for each case of $(s_{\lnot x}, a_x, a_{d_x})$:
    \begin{enumerate}
        \item $ \left( 1,1,1 \right) $: \[
                \Lambda = \tfrac p{1-p} \cdot \tfrac p{1-p} \cdot \tfrac{p + (1-p)/2}{(1-p) + p/2} = \tfrac {p^2}{(1-p)^2} \cdot \tfrac {1/2+ p/2}{1- p/2} > 1.
        \]
        \item $ \left( 1,1,0 \right) $: \[
                \Lambda = \tfrac p{1-p} \cdot \tfrac {1-p}p \cdot \tfrac{p/2}{(1-p)/2} = \tfrac {p}{(1-p)} > 1.
        \]

        \item $ \left( 1,0,1 \right) $: \[
                \Lambda = \tfrac p{1-p} \cdot \tfrac p{1-p} \cdot \tfrac{(1-p) /2}{p/2} = \tfrac {p}{(1-p)} > 1.
        \]

        \item $ \left( 1,0,0 \right) $: \[
                \Lambda = \tfrac p{1-p} \cdot \tfrac {1-p}p \cdot \tfrac{(1-p) + p/2}{p + (1-p) /2} =  \tfrac {1-p/2}{1/2+ p/2} < 1.
        \]
    \end{enumerate}
    The inequalities hold for $ \tfrac 12 < p < 1 $.
    The remaining cases (that is $(0,1,1)$, $(0,1,0)$, $(0,0,1)$, $(0,0,0)$) follow from symmetry.
    It is now clear that if $ \theta = 1 $, $ a_{\lnot x} $ is correct in cases 1, 2, 3, 5.
    We now compute $ \oclr(\lnot x) $, which is the probability of cases 1, 2, 3, or 5 occurring. \begin{align*}
        \oclr(\lnot x) &= \pr{X_{\lnot x} \in \left\{ \left( 1,1,1 \right) ,  \left( 1,0,1 \right) ,  \left( 1,1,0 \right) ,  \left( 0,1,1 \right) \right\}} \\
                       &= p ( p + (1-p)\tfrac p2)) + (1-p)p(p + (1-p)\tfrac 12) \\
                       &= \tfrac p2 + \tfrac 32 p^2 - p^3.
    \end{align*}

    The cumulative learning rate of the clause gadget is then the sum of the above, \[
        \oclr(\cell x) = \tfrac 52 p + \tfrac 32 p^2 - p^3.
    \]
\end{proof}

To summarize, there exist two orderings for each cell which yield the same cell learning rate.
The two orderings place $d_x$ first, and correspond to either the ``on'' state (when $ \sigma(x) < \sigma(\lnot x) $) or the ``off'' state ($ \sigma(x) > \sigma(\lnot x) $).
For now, we will defer the question of which of the two is optimal in the overall network ordering.
Addressing this first requires examining the learning rates of the clause gadgets.
Since the graph contains edges from cells to clause gadgets, there is an optimal ordering which orders all cell nodes before all clauses gadgets.
We begin by examining the learning rate for arbitrary clause gadgets. 

\begin{lemma}\label{lemma:bayesian_clause}
    Let $ C = \alpha \lor \beta \lor \gamma $ be a clause of $ \varphi $.
    Let $ \sigma^* $ be an optimal ordering on $ \network $.
    Let $p'\deq\frac p2 + \frac 32 p^2 - p^3 $.
    Then if under $ \sigma^* $, exactly $ i $ cells of $ \alpha, \beta, \gamma $ are ``on'', then $ \clr(\gadget C, \sigma^*) = \clr_i $, where \begin{align*}
        \pzero &\deq p (2 p^4-5 p^3+5 p+1). \\
        \pone &\deq  p^4 (2 p'-1)+p^3 (2-4 p')+p^2 (1-2 p')+4 p p'+p', \\
        \ptwo &\deq 4 p^3 (p'-1) p'+p^2 (-6 (p')^2+4 p'+1)+2 p p'+p' (p'+1), \\
        \pthree &\deq p' (p^2 (2 (p')^2-3 p'+1)-p (2 (p')^2+p'-3)+2 p'+1).
    \end{align*}
    Furthermore, if $ D \in \varphi $ is a satisfied clause under the ordering $ \sigma^* $, meaning at least one of the literals of $ D $ is ``on'' under $ \sigma^* $, then 
    \[
        \pthree \geq \clr(\gadget D, \sigma^*) \geq \pone \geq \pzero.
    \]
\end{lemma}

\input{src/figures/bayes_ps}

\begin{proof}[Proof Idea.]
    The full proof is long and technical.
    Here we offer the main idea, which is similar to that of \Cref{lemma:bayesian_cellLearningRate}.

    The actions of the nodes in $ \gadget C $ are exactly determined by \begin{enumerate}[ ]
        \item the ordering of the nodes in $ \gadget C $,
        \item the private signals of the nodes in $ \gadget C $,
        \item the actions taken by $ \alpha, \beta, \gamma $, as well as $ \alpha, \beta, \gamma $ being on or off.
    \end{enumerate}
    We compute the learning rate in each case.
    We then compute the expected value over the private signals and the actions of $ \alpha,\beta,\gamma $, the probabilities of which are given by \Cref{lemma:bayesian_cellLearningRate}.

    The resulting cumulative learning rate of $ \gadget C $ depends only on the states of $ \alpha, \beta, \gamma $.
    Among these are the learning rates $ \pone $ and $ \pthree $, which are the learning rates of the clause gadget when one or all of the literals are on, respectively.
     
    The case of $\pone$ actually corresponds to three sub-cases, depending on whether the node corresponding to the ``on'' literal is first, second, or third in the ordering. Notice that swapping the ordering of the three vertices inside the clause gadget does not affect the learning rate of other vertices in the network. Thus, since $ \sigma^* $ is an optimal ordering, it maximizes the learning rate for this clause, and as such, $\pone$ is the maximum over the three subcases.

    The full proof can be found in \appbayes.
\end{proof}

See \Cref{fig:bayes_ps} for the relationship between the different values as a function of $ p $.
Notice that $ \pthree,\ptwo,\pone$ and $\pzero $ converge as $p$ approaches $1$ and as it approaches $\frac 12$, regardless of the graph topology and ordering.
This is exactly what we expect: if $ p=1 $, the agents have perfect information from their private signals alone, while if $ p=\frac 12 $, then the private signals give the agents no extra information, and thus their the LR approaches $ \frac 12 $.


\subsection{Optimal Ordering \& Restrictions on p}
\label{ssec:assignment_bayes}
We can now determine the optimal ordering by examining the learning rates derived in the previous section. First, we define an \emph{induced ordering} below:

\begin{definition}
    Let $\mathcal{A}: \chi \to \{0,1\}$ be any assignment of values to variables. Define the (partial) \emph{ordering $\sigma(\mathcal{A})$ induced by $\mathcal{A}$} as follows: if $\mathcal{A}(x_i)=1$, then cell $\cell {x_i}$ is on; otherwise, $\cell {x_i}$ is off.
\end{definition}

In particular, the above definition gives a bijection between assignments and partial ordering over variables. We further say that a total ordering $\sigma$ \emph{respects} an assignment $\mathcal{A}$ (denoted by $\sigma \sim \mathcal{A}$) if it contains $\sigma(\mathcal{A})$ as a partial ordering over variable nodes. We also write $\sigma^*(\mathcal{A}) \deq \arg\max_{\sigma \sim \mathcal{A}} \mathcal{L}(\mathcal{N}, \sigma, \mu^B)$.

\begin{definition} \label{def:max_assignment}
    Let $\mathcal{A}: \chi \to \{0,1\}$ be an assignment of values to variables maximizing the number of satisfied clauses. Then $\mathcal{A}$ is a \emph{maximal assignment}.
\end{definition}

\begin{lemma}[Optimal Ordering] \label{lemma:bayes_bestOrder}
    Let $\mathcal{A}^*$ be a maximal assignment.  Let $p(M) \deq (3M-4)/(3M-3)$ be a threshold probability determined by $M$, the number of clauses. Then for all $p \geq p(M) $, $\sigma^*(\mathcal{A}^*)$ is an optimal ordering.
\end{lemma}
\begin{proof}
    Note that an assignment-induced ordering only specifies whether each cell is on or off. From \Cref{lemma:bayesian_cellLearningRate},  an optimal cell ordering exists both if the cell is on or off, and yields the same learning rate in either case. Therefore, the optimal ordering is determined by comparing clause learning rates. 
    
    Also note that any total ordering must respect some assignment. So we argue the optimality of $\sigma^*(\mathcal{A}^*)$ by comparing it to $\sigma^*(\mathcal{A}')$ for any non-maximal $\mathcal{A'}$. Let $S^*$ be the number of satisfied clauses under $\mathcal{A}^*$ and $S' < S^*$ that under $\mathcal{A'}$. By \Cref{lemma:bayesian_clause}, for any clause satisfied under an arbitrary assignment $\mathcal{A}$, the corresponding clause gadget under an ordering $\sigma \sim \mathcal{A}$ achieves learning rate lower bounded by $\pone$. Further, by \Cref{lemma:bayesian_clause}, $\pone >\pzero$,  so having more satisfied clauses in an assignment can never decrease the CLR under the induced ordering. However, note also that $\pthree >\pone$  (by \Cref{lemma:bayesian_clause}). Hence, in the most extreme case, all $S'$ satisfied clauses under $\mathcal{A}'$ are satisfied with three true literals, while all $S^*$ satisfied clauses under $\mathcal{A}^*$ are satisfied with one true literal. 

    Consider that extreme case, and further impose the worst-case choices of $S'$ and $S^*$ by setting $S^* = M$ and $S' = M-1$. Then over all clause gadgets, $\sigma^*(\mathcal{A}^*)$ gives a CLR of $M \pone$, $\sigma^*(\mathcal{A}')$ gives a CLR of $(M-1) \pthree + \pzero$. We can now solve for conditions on $p$ such that $\sigma^*(\mathcal{A}^*)$ achieves a higher network learning rate:
    \begin{align*}
        M \pone &> (M-1) \pthree + \pzero\\
        \tfrac{\pone - \pzero}{\pthree - \pone} &> M-1.
    \end{align*}
    \input{src/figures/bayes_pdff}
    The numerator and denominator is plotted in \Cref{fig:pdff} as a function of $ p $.
    Since the left-hand side can be lower bounded by $\frac{1}{3-3p}$ for all $p \in (0.5,1)$, it is sufficient to have $ \frac 1{3-3p} \geq M-1 $.
    We can thus define $ p(M) \deq \frac{3M-4}{3M-3} $, which respects the condition. Note that $p(M)$ is well-defined for any $ M \geq 2 $, and lies in the interval $ (\frac 12, 1) $.
\end{proof}

To recap, given any instance $\varphi$ of \sat{} with $N$ variables and $M$ clauses, we can construct a network $G_\varphi$ with ground truth prior $q = 1/2$ and signal accuracy $p = p(M)$. In particular, whenever $\varphi$ is satisfiable under some maximal assignment $\mathcal{A}^*$, there is an optimal decision ordering $\sigma^*$ which respects the ordering induced by $\mathcal{A}^*$, and which achieves a network CLR of at least $N(2p+3p^2-2p^3) + M\pone$. Otherwise, if $\varphi$ is non-satisfiable under any maximal assignment $\mathcal{A}^*$, then the optimal decision ordering $\sigma^*$ respecting the ordering induced by $\mathcal{A}^*$ achieves a network CLR of no more than $N(2p+3p^2-2p^3) + (M-1)\pthree + \pzero$. Choosing $p = p(M)$ allowed us to show the optimality of $\sigma^*$, as well as to separate the learning rates in networks corresponding to satisfiable and non-satisfiable formulas. All that remains to complete the reduction is to pick an appropriate choice of $\varepsilon$, such that a formula graph $G_\varphi$ achieves expected network learning rate greater than $\varepsilon$ under an optimal ordering iff $\varphi$ is satisfiable.

\subsection{Picking the Epsilon}\label{sec:bayes_epsilon}
We will simply pick $\varepsilon$ to lie exactly halfway between the satisfiable and non-satisfiable network learning rates. Recalling that each variable and clause gadget contains $3$ vertices, we can compute the learning rates below. For networks corresponding to satisfiable formulas, we have $\frac{N\pcell + M\pone}{3(N+M)},$ and for networks corresponding to non-satisfiable formulas, we have $\frac{N\pcell + (M-1)\pthree + \pzero}{3(N+M)}.$ This gives 
\begin{align*}
    \varepsilon = \frac{1}{2}\left(\frac{2N\pcell + M\pone + (M-1)\pthree + \pzero}{3(N+M)}\right),
\end{align*}
thus completing the reduction. 

\subsection{Generalization for a constant range of p}

Finally, we remark that, while this construction proves the Theorem, it uses $ p $ approaching 1 as the size of $ \varphi $ increases.
A natural question, then, is whether it is hard to compute the maximum learning rate for other values of $p$, especially when $ p $ is a fixed constant.
In fact, \Cref{sec:approx} shows a stronger version of \Cref{thm:nphardness_bayes}, stated as follows. 

\begin{theorem}[ ]
	\netlearn{} with Bayesian learning rule $\mu = \mu^B$ and a fixed $ p \in ( \sqrt{7/8},1 ) $ is \np-hard.
\end{theorem}

This follows as a direct corollary of \Cref{thm:approx}.

