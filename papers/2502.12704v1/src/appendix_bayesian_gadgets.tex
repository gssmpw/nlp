\section{Bayesian General Learning Rates}
\label{app:bayesian_gadgets}

\begin{lemma}[ ]\label{lemma:twoInformedOneUninformed}
    Let $ \frac 12 < p < 1 $.
    Let $ p' = \frac p2 + \frac 32 p^2 - p^3 $.
    Then \[
        \left( \frac p{1-p} \right)^2 \frac {1-p'}{p'} > 1.
    \]
\end{lemma}

\begin{proof}
    This Lemma is proven by solving the stated inequality.
    We check all our calculations using Wolfram Mathematica.
\end{proof}

\ourparagraph{General Clause Setup}
Consider a clause gadget with literal nodes $a$, $b$, and $c$.
WLOG, suppose the optimal ordering $\sigma^*$ orders $a$ before $b$ before $c$.
Let $\alpha$, $\beta$, and $\gamma$ be the literals corresponding to $ a $, $ b $, and $ c $, respectively.
We also use $\alpha$, $\beta$, and $\gamma$, to denote the corresponding nodes from their respective cells. (see \Cref{fig:bayesian_alphabetagamma} for details of the construction).
Suppose that $\alpha$, $\beta$, and $\gamma$ correctly predict the ground truth with probabilities $p_\alpha$, $p_\beta$, and $p_\gamma$, respectively.
Note that since the literal nodes have no outgoing edges except to each other, ordering all literal nodes after variable nodes gives the literal nodes more information at no cost to any other nodes.
So we assume $ \sigma^* $ orders all literal nodes after variable nodes, and also $a$ before $b$ before $c$. 

\input{src/figures/bayes_alphabetagamma}

\begin{observation}
    The predictions of $ \alpha $, $ \beta $, $ \gamma $ are independent.
\end{observation}

\begin{proof}
    The nodes $ \alpha $, $ \beta $, $ \gamma $ are from their variable cells.
    From each cell, there are only outward edges.
    This means that nothing outside of a cell can affect any of its nodes.
    This includes the nodes of other cells.
    Hence, the predictions are independent.
\end{proof}

\begin{lemma}\label{lemma:bayesian_l1_LR}
    For $a$ the earliest literal node in its clause gadget, $\oclr(a) = p$ when its corresponding literal $\alpha$ is off, and $\oclr(a) = p' \deq \frac p2 + \frac 32 p^2 - p^3$ when $\alpha$ is on. 
\end{lemma}
\begin{proof}
    We compute the learning rate of $ a $ under Bayesian aggregation.
    Recall that Bayesian agents base their predictions on the posterior probability for $\theta$ given their inputs, tie-breaking by selecting an action randomly.
    So, the prediction of node $a$ is determined by the posterior of $\theta$ given their private signal $s_a$ and their in-neighbor $\alpha$.
    Therefore, $a$ predicts correctly whenever its posterior for $\theta = 1$ exceeds $1/2$ or tiebreaks correctly.
    Equivalently, $a$ predicts correctly whenever its corresponding likelihood ratio exceeds $1$ or tiebreaks correctly.
    We denote the ratio as $ \Lambda $, defined as
    \[
        \Lambda(\theta = 1 : \theta = 0 \mid s_a, \alpha) \deq \frac {\Pr[s_a, \alpha \mid \theta = 1]} {\Pr[s_a = 1, \alpha = 0 \mid \theta = 0]}                                                            = \frac {\Pr[s_a \mid \theta = 1]} {\Pr[s_a \mid \theta = 0]} \cdot \frac {\Pr[ \alpha \mid \theta = 1]} {\Pr[\alpha \mid \theta = 0]},
    \]
    where the equality holds due to $ s_a $ and $ \alpha $'s prediction being independent.

    In general, there are four cases which may arise.
    \[
                \Lambda(1:0 \mid s_a, \alpha) = \begin{cases}
                     \frac p{1-p} \cdot \frac {p_\alpha}{1-p_\alpha} > 1 & \text{if $s_a = 1, \alpha = 1$,} \\
                     \frac {1-p}p \cdot \frac {1-{p_\alpha}}{p_\alpha} < 1 & \text{if $s_a = 0, \alpha = 0$,} \\
                     \frac {1-p}p\cdot \frac{p_\alpha}{1-p_\alpha} & \text{if $s_a = 0, \alpha = 1$,} \\
                     \frac p{1-p} \cdot\frac{1-p_\alpha}{p_\alpha} & \text{if $s_a = 1, \alpha = 0$.} \\
                \end{cases}
    \]
    The inequality in the first two cases holds because $ p_\alpha \geq p $.
    Cases 3 and 4 need to be analyzed separately for $ \alpha $ on and off.
    \begin{itemize}
        \item If $\alpha$ is off, then $ p_\alpha = \pr{\alpha = 1 \mid  \theta = 1} = \pr{\alpha = 0 \mid \theta = 0} = p $, as discussed in \Cref{lemma:bayesian_cellLearningRate}.
            Thus \begin{align*}
                \Lambda(1:0 \mid s_a = 0, \alpha=1) = 
                \Lambda(1:0 \mid s_a = 1, \alpha=0)                 = \frac p{1-p} \cdot\frac{1-p}{p} = 1.
            \end{align*}
        \item If $\alpha$ is on, then $ p_\alpha = \pr{\alpha = 1 \mid  \theta = 1} = \pr{\alpha = 0 \mid \theta = 0} = \frac p2 + \frac 32 p^2 - p^3 > p $, again, as discussed in \Cref{lemma:bayesian_cellLearningRate}.
            Thus $\Lambda(1:0 \mid s_a = 0, \alpha=1) > 1$ and $\Lambda(1:0 \mid s_a = 1, \alpha=0) < 1.$ \end{itemize}
    
            Now, we compute the final learning rate of $ a $, which is the probability $ \pr{a=\theta} $. Since we assume $ q = \frac 12 $, this is the same as $ \pr{a=1 \suchthat \theta=1} $.
    
	Therefore, the probability of $a$ being correct is the probability of any input configuration yielding either $\Lambda > 1$, plus the probability of a configuration yielding $\Lambda = 1$ and tie breaking successfully with probability $ \frac 12 $.
    \begin{align*}
        \Pr[a = 1 \mid \alpha \text{ off}\,] &= \Pr[s_a = \alpha = 1 \mid \theta = 1] + \frac 12 \pr{s_a \neq \alpha \mid \theta = 1} = p^2 + \frac 12 \left( p(1-p) + (1-p)p \right) = p, \\
            \Pr[a = 1 \mid \alpha \text{ on}\,] &= \Pr[\alpha = 1 \mid \theta = 1] = p_\alpha = \frac p2 + \frac 32 p^2 - p^3.
	\end{align*}
\end{proof}

\begin{lemma}\label{lemma:bayesian_l2_LR}
    For $b$ the second earliest literal node in its clause gadget, $\oclr(b)$ takes the following values:
    \begin{align*}
        \oclr(b) = \begin{cases}
        3p^2-2p^3 & \text{if $ \alpha $, $ \beta $ both off,} \\
        p'(p'+2p-2pp') & \text{if $ \alpha $, $ \beta $ both on,} \\
        p(p+2p'-2pp') & \text{otherwise,} \\
    \end{cases}
    \end{align*}
    where $p' = \frac p2 + \frac 32 p^2 - p^3 $.
\end{lemma}
\begin{proof}
    As in the previous Lemmas, we first compute the ratio of $ \frac{\pr{\theta=1 \mid X_b}}{\pr{\theta=0 \mid X_b}} $, denoted as $ \Lambda $, to get $ b $'s predictions for each of its input configurations. We then compute the probability that $b$ sees inputs that yield a correct prediction.

    The information available to $ b $ is the predictions of $ a $ and $ \beta $, along with $ s_b $, $ b $'s private signal.
    Observe that these are all independent.
    We again define the ratio $ \Lambda $ as \begin{align*}
        \Lambda &\deq \frac {\Pr[s_b, a, \beta \mid \theta = 1]} {\Pr[s_b, a, \beta \mid \theta = 0]} = \frac {\Pr[s_b \mid \theta = 1]} {\Pr[s_b \mid \theta = 0]} \cdot \frac {\Pr[ a \mid \theta = 1]} {\Pr[a \mid \theta = 0]}\cdot\frac {\Pr[ \beta \mid \theta = 1]} {\Pr[\beta \mid \theta = 0]}.
    \end{align*}
    The values of $ \Lambda $ are once again determined by $ p \deq \pr{s_b = \theta} $ and $ p_a \deq \pr{a = \theta} $, $ p_\beta = \pr{\beta = \theta} $.
    Notice also that $ p_a, p_\beta \geq p $. We now list $\Lambda$ for all possible inputs.

    \begin{enumerate}[ref=(\theenumi)]
        \item If $ s_b = a = \beta = 1 $, \[
            \Lambda = \frac p{1-p} \cdot \frac {p_a}{1-p_a} \cdot \frac {p_\beta}{1-p_\beta} > 1
        \]
    \item\label{badcase1} If $ s_b = a = 1, \beta = 0 $, \[
                \Lambda = \frac p{1-p} \cdot \frac {p_a}{1-p_a} \cdot \frac {1-p_\beta}{p_\beta}.
            \]
        \item\label{badcase2} If $ s_b = 1,  a = 0, \beta = 1 $, \[
                \Lambda = \frac p{1-p} \cdot \frac {1-p_a}{p_a} \cdot \frac {p_\beta}{1-p_\beta}.
            \]
	\item If $ s_b = 1,  a = \beta = 0 $, \[
                \Lambda = \frac p{1-p} \cdot \frac {1-p_a}{p_a} \cdot \frac {1-p_\beta}{p_\beta} < 1.
            \]
    \end{enumerate}
    The rest of the cases follow trivially from symmetry, since
    \[
        \Lambda(s_b, a, \beta) = 1/\Lambda(\bar s_b, \bar a, \bar \beta).
    \]

    For the cases \labelcref{badcase1,badcase2}, we perform case analysis on whether $ \alpha $ and $ \beta $ are on or off.
    By \Cref{lemma:bayesian_l1_LR}, $p_a = p$ when $\alpha$ is off and $p_a = p'$ when $\alpha$ is on.
    From \Cref{lemma:bayesian_cellLearningRate}, $\beta$ is correct with probability $p$ when $\beta$ is off and $p'$ when $\beta$ is on.
    We consider two cases:
    \begin{itemize}[ ]
        \item If $ \alpha, \beta $ are both on, or both off, then $ p_a = p_\beta $, leaving $\Lambda = \frac p{1-p} > 1 $ in both of the middle cases.
        \item Otherwise, WLOG $ \alpha $ is on, $ \beta $ off.
            Then $ p_a = p', p_\beta = p $, resulting in $\Lambda( s_b= a=1, \beta=0) = \frac {p'}{1-{p'}} > 1 $ and $\Lambda( s_b= 
            \beta=1, a=0) = \left( \frac{p}{1-p} \right)^2 \cdot \frac {1-p'}{p'} > 1 $ by \Cref{lemma:twoInformedOneUninformed}.
    \end{itemize}

    We can see that $ b $ simply follows the majority of signals.
    \begin{align*}
        \pr{b = \theta} &= \pr{b = 1 \mid \theta = 1} = \pr{s_b + a + \beta \geq 2} = 1-\pr{s_b + a + \beta \leq 1} \\
                        &= 1 - (1-p)(1-p_\alpha)(1-p_\beta) - p(1-p_\alpha)(1-p_\beta) - (1-p)p_\alpha(1-p_\beta) - (1-p)(1-p_\alpha)p_\beta.
    \end{align*}
    Substituting $ p_a = p $ (similarly, $p_b = p$) if $ \alpha $ ($\beta$) is off and $ p' $ otherwise yields the desired probabilities.
\end{proof}

\begin{lemma}\label{lemma:bayesian_l3_LR}
    For $c$ the last literal node in its clause gadget, $\oclr(c)$ takes the following values, depending on the state of $ \left( \alpha, \beta, \gamma \right) $:
    \begin{align*}
\text{(off},\text{off},\text{off)}&\to            p^2 \left(2 p^3-5 p^2+2 p+2\right), \\
\text{(off},\text{off},\text{on)}&\to            p \left(-p^2-p (p'-2)+p'\right), \\
\text{(off},\text{on},\text{off)}&\to            p \left(p^3 (2 p'-1)+p^2 (1-4 p')+p (p'+1)+p'\right), \\
\text{(off},\text{on},\text{on)}&\to            p \left(2 p^2 (p'-1) p'+p \left(-3 (p')^2+p'+1\right)+p' (p'+1)\right), \\
\text{(on},\text{off},\text{off)}&\to            p^4 (2 p'-1)+p^3 (2-4 p')+2 p p', \\
\text{(on},\text{off},\text{on)}&\to            p \left(p^2 \left(2 (p')^2-2 p'+1\right)-3 p (p')^2+p' (p'+2)\right), \\
\text{(on},\text{on},\text{off)}&\to            p \left(4 p^2 (p'-1) p'+p \left(-6 (p')^2+4 p'+1\right)+2 (p')^2\right), \\
\text{(on},\text{on},\text{on)}&\to            p' \left(p^2 \left(2 (p')^2-3 p'+1\right)+p \left(-2 (p')^2+p'+1\right)+p'\right).
    \end{align*}
\end{lemma}

\begin{proof}
    We again compute the value of $\Lambda = \frac{\pr{\theta = 1 \mid X_c}}{\pr{\theta = 0 \mid X_c}}$ for all possible configurations of $ X_c $.
    From that, we determine the action $ c $ chooses and compute the learning rate.

    First, let us rewrite \[
        \Lambda = \frac{\pr{s_c \mid \theta = 1}}{\pr{s_c \mid \theta = 0}} \cdot \frac{\pr{\gamma \mid \theta = 1}}{\pr{\gamma \mid \theta = 0}} \cdot \frac{\pr{a \mid \theta = 1}}{\pr{a \mid \theta = 0}} \cdot \frac{\pr{b \mid a,\theta = 1}}{\pr{b \mid a,\theta = 0}}.
    \]

    From the proofs of the previous two lemmas, we have \begin{align*}
        \pr{b = 1 \mid a = 1, \theta = 1} &= \pr{b = 0 \mid a = 0, \theta = 0} = p + p_\beta - pp_\beta, \\
        \pr{b = 1 \mid a = 0, \theta = 1} &= \pr{b = 0 \mid a = 1, \theta = 0} = pp_\beta, \\
        \pr{a = 1 \mid \theta = 1} &= \pr{a = 0 \mid \theta = 0} = p_\alpha, \\
    \end{align*}
    where $ p_\alpha = p' $ if $ \alpha  $ is on, $ p $ otherwise.
    We again use $ p_\beta, p_\gamma  $ similarly.

    We now perform case analysis on $ \left( s_c, \gamma, a, b \right) $:
    \begin{enumerate}[ ]
        \item $ \left( 1,1,1,1 \right) $: \[
            \Lambda = \frac{p}{1-p} \cdot \frac{p_\gamma}{1-p_\gamma} \cdot \frac{p_\alpha}{1-p_\alpha} \cdot \frac{p+p_\beta - pp_\beta}{1-pp_\beta} > 1,
        \]
        since it holds that $ 1>p,p_\alpha, p_\beta, p_\gamma> \frac 12  $.
        \item $ \left( 1,1,1,0 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{p_\gamma}{1-p_\gamma} \cdot \frac{p_\alpha}{1-p_\alpha} \cdot \frac{\left( 1-p \right)\left( 1-p_\beta \right)}{pp_\beta} = \frac{p_\gamma}{1-p_\gamma} \cdot \frac{p_\alpha}{1-p_\alpha} \cdot \frac{1-p_\beta}{p_\beta} \geq \frac{p}{1-p} \cdot \frac{p}{1-p}  \cdot \frac{1-p'}{p'} > 1.
        \end{align*}
        \item $ \left( 1,1,0,1 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{p_\gamma}{1-p_\gamma} \cdot \frac{1-p_\alpha}{p_\alpha} \cdot \frac{pp_\beta}{\left( 1-p \right)\left( 1-p_\beta \right)} \geq \left( \frac{p}{1-p} \right)^4 \cdot \frac{1-p'}{p'} > 1.
        \end{align*}
        \item $ \left( 1,1,0,0 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{p_\gamma}{1-p_\gamma} \cdot \frac{1-p_\alpha}{p_\alpha} \cdot \frac{1-pp_\beta}{p+p_\beta - pp_\beta}.
        \end{align*}
        This is $ <1 $ if $ \alpha, \beta  $ are on and $ \gamma $ is off, and $ >1 $ otherwise. (By Wolfram, need to write this out)
        \item $ \left( 1,0,1,1 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{1-p_\gamma}{p_\gamma} \cdot \frac{p_\alpha}{1-p_\alpha} \cdot \frac{p+p_\beta - pp_\beta}{1-pp_\beta} \geq \left( \frac{p}{1-p} \right)^2 \cdot \frac{1-p'}{p'} \cdot \frac{p+p_\beta - pp_\beta}{1-pp_\beta} > \frac{p+p_\beta - pp_\beta}{1-pp_\beta} > 1.
        \end{align*}
        \item $ \left( 1,0,1,0 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{1-p_\gamma}{p_\gamma} \cdot \frac{p_\alpha}{1-p_\alpha} \cdot \frac{\left( 1-p \right)\left( 1-p_\beta \right)}{pp_\beta} \leq \frac{p'}{1-p'} \cdot \left( \frac{1-p}{p} \right)^2 <1.
        \end{align*}
        \item $ \left( 1,0,0,1 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{1-p_\gamma}{p_\gamma} \cdot \frac{1-p_\alpha}{p_\alpha} \cdot \frac{pp_\beta}{\left( 1-p \right)\left( 1-p_\beta \right)} \geq \left( \frac{1-p'}{p'} \right)^2 \cdot \left( \frac{p}{1-p} \right)^3.
        \end{align*}
        The lower bound on $ \Lambda $ is reached for $ \gamma $ on and $ \alpha, \beta $ off, and in such a case it is $ <1 $.
        However, in any other state of $ \alpha, \beta, \gamma $, the value of $ \Lambda $ is lower-bounded by \[
             \frac{1-p'}{p'} \cdot \left( \frac{p}{1-p} \right)^2 > 1,
        \]
        by \Cref{lemma:twoInformedOneUninformed}.
        \item $ \left( 1,0,0,0 \right) $: \begin{align*}
                \Lambda &= \frac{p}{1-p} \cdot \frac{1-p_\gamma}{p_\gamma} \cdot \frac{1-p_\alpha}{p_\alpha} \cdot \frac{1-pp_\beta}{1-\left( 1-p \right)\left( 1-p_\beta \right)} \leq \frac{1-p}{p} \cdot \frac{1-p^2}{2p-p^2}<1.
        \end{align*}
    \end{enumerate}
    The remaining cases follow from symmetry.

    We now compute the learning rates in each case, and multiply them by the probability of that case taking place.
    Due to symmetry of $ \theta $ and $ 1-\theta $, this is equivalent to $\pr{\Lambda > 1 \mid \theta = 1}$.
    This can be written as the sum of the probabilities of all cases yielding $ \Lambda > 1 $ assuming $ \theta = 1 $.

    The configurations which always yield $ \Lambda > 1 $ are $ \left( 1,1,1,1 \right)$, $(1,1,1,0)$, $(1,1,0,1)$, $(1,0,1,1)$, $(0,1,0,1)$, $(0,1,1,1) $.
    Then, if $ \alpha,\beta $ are off and $ \gamma $ is on, then $ (0,1,1,0) $ gives $ \Lambda > 1 $, otherwise $ (1,0,0,1) $ gives $ \Lambda > 1 $.
    Finally, if $ \alpha,\beta $ are on and $ \gamma $ is off, then $ (1,1,0,0) $ gives $ \Lambda > 1 $, otherwise $ (0,0,1,1) $ gives $ \Lambda > 1 $.
    The learning rate of $ c $ is then the sum of the probabilities of these cases, given that $ \theta = 1 $.
\end{proof}


We now compute the CLR in the following three cases: when all literals are off, when all literals are on, and when only one literal is on.

\begin{lemma}\label{lemma:bayesian_000CLR} 
    Suppose that an optimal ordering $\sigma^*$ sets all literals in a clause $ C $ to be off. Then the resulting CLR of $ \gadget C $ is \[
        \oclr (\gadget C) = \pzero \deq p \left(2 p^4-5 p^3+5 p+1\right).
     \]
\end{lemma}

\begin{proof}
    Follows directly from \Cref{lemma:bayesian_l1_LR,lemma:bayesian_l2_LR,lemma:bayesian_l3_LR}.
\end{proof}

\begin{lemma}
    \label{lemma:bayesian_111CLR}
    Suppose that an optimal ordering $\sigma^*$ sets all literals in a clause $ C $ to be on. Then the resulting CLR of $ \gadget C $ is \begin{align*}
        \oclr(\gadget C) = \pthree \deq p' \left(p^2 \left(2 (p')^2-3 p'+1\right)-p \left(2 (p')^2+p'-3\right)+2 p'+1\right).
     \end{align*}
\end{lemma}

\begin{proof}
    Follows again directly from \Cref{lemma:bayesian_l1_LR,lemma:bayesian_l2_LR,lemma:bayesian_l3_LR}.
    The expression in the lemma statement is just the sum of $ \oclr(a),\oclr(b),\oclr(c) $.
\end{proof}

\begin{lemma}
    \label{lemma:bayesian_100CLR}
    Suppose that an optimal ordering $\sigma^*$ sets exactly one literal to be on. Then the resulting CLR is \[
        \oclr(\gadget C) =\pone \deq p^4 (2 p'-1)+p^3 (2-4 p')+p^2 (1-2 p')+4 p p'+p'.
     \]
\end{lemma}

\begin{proof}
    This lemma is a bit more involved than the two ealrier ones, since there are now three cases which are not symmetrical---the vertex corresponding to the ``on'' literal is either first, second, or third (in other words, it corresponds to either $ a $, $ b $, or $ c $ from the general lemmas).

    It turns out that the highest learning rate is achieved when $ \alpha $ is on.
    Intuitively, the fact that $ \alpha $ is on means that $ a $ receives a stronger signal, and it is able to be spread all over the gadget.
    Formally, we show that the learning rate corresponding to $ \left( \alpha, \beta,\gamma \right) = \left( \text{on},\text{off},\text{off} \right) $ is higher than $ \left( \text{off},\text{off},\text{on} \right) $ and $ \left( \text{off},\text{on},\text{off} \right) $.

    By \Cref{lemma:bayesian_l1_LR,lemma:bayesian_l2_LR,lemma:bayesian_l3_LR}, and assuming that all the variable cells are ordered before the clause gadget, the learning rate of the clause $ C $ is as follows.
    \begin{enumerate}[ ]
        \item If $ \alpha $ is on, then \[
             \clr(\gadget C) = p^4 (2 p'-1)+p^3 (2-4 p')+p^2 (1-2 p')+4 p p'+p',
        \]
        \item if $ \beta $ is on, then \[
            \clr(\gadget C) = p \left(p^3 (2 p'-1)+p^2 (1-4 p')-p (p'-2)+3 p'+1\right),
        \]
        \item if $ \gamma $ is on, then \[
            \clr(\gadget C) = p \left(-3 p^2-p (p'-5)+p'+1\right).
        \]
    \end{enumerate}
    It is not hard to see that indeed the first quantity is, assuming $ \frac 12 > p > 1 $, the largest of the three, and it is thus the optimal value for the cumulative learning rate of a cell with only one variable turned on.
\end{proof}

\begin{lemma}
    \label{lemma:bayesian_110CLR}
    Suppose that an optimal ordering $\sigma^*$ sets exactly two literals to be on. Then the resulting CLR is \[
        \oclr(\gadget C) =\ptwo \deq 4 p^3 (p'-1) p'+p^2 \left(-6 (p')^2+4 p'+1\right)+2 p p'+p' (p'+1).
     \]
\end{lemma}

\begin{proof}
    As in the previous lemma, we need to decide which of the literals should be the ``off'' one---in this case it is $ \gamma $.
    Intuitively, again, the fact that $ \alpha $ and $ \beta $ are on means that their stronger signals can be better spread over the gadget.
    Formally, we show that the learning rate corresponding to $ \left( \alpha, \beta,\gamma \right) = \left( \text{on},\text{on},\text{off} \right) $ is the highest.

    By \Cref{lemma:bayesian_l1_LR,lemma:bayesian_l2_LR,lemma:bayesian_l3_LR}, and assuming that all the variable cells are ordered before the clause gadget, the learning rate of the clause $ C $ is as follows.
    \begin{enumerate}[ ]
        \item If $ \alpha $ is off, then \[
             \clr(\gadget C) = p \left(2 p^2 (p'-1) p'-p \left(3 (p')^2+p'-2\right)+(p')^2+3 p'+1\right),
        \]
        \item if $ \beta $ is off, then \[
            \clr(\gadget C) = p'+p p' (4+p')+p^2 (1-2 p'-3 (p')^2)+p^3 (1-2 p'+2 (p')^2),
        \]
        \item if $ \gamma $ is off, then \[
            \clr(\gadget C) = 4 p^3 (p'-1) p'+p^2 \left(-6 (p')^2+4 p'+1\right)+2 p p'+p' (p'+1).
        \]
    \end{enumerate}
    It is not hard to see that indeed the last quantity is, assuming $ \frac 12 > p > 1 $, the largest of the three, and it is thus the optimal value for the cumulative learning rate of a cell with exactly two variables turned on.
\end{proof}

Lastly, observe that as the number of literals which are set to on increases, the optimal gadget learning rate increases because the literal nodes get stronger signals for $\theta$. We can verify this for the cases computed above:
\begin{corollary}\label{cor:bayesian_LR_comparison}
    For $p \in (\frac 1 2, 1)$, \[
    \pthree \geq \ptwo \geq \pone \geq \pzero.
    \]
\end{corollary}

\begin{proof}
    Follows by substituting the expressions from \Cref{lemma:bayesian_000CLR,lemma:bayesian_111CLR,lemma:bayesian_100CLR,lemma:bayesian_110CLR}.
    The calculations were verified by Wolfram Mathematica.
    For a visual representation of these learning rates, see \Cref{fig:bayes_ps}.
\end{proof}


\begin{corollary} \label{obs:satisfied_bounds}
    For clause gadgets with at least one on literal, the optimal gadget learning rate is lower bounded by $\pone$ and upper bounded by $\pthree$.
\end{corollary} 
