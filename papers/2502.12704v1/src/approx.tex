\section{Approximating the Optimal LR}
\label{sec:approx}

In this section, we extend the ideas from the 3-SAT reduction 
to show that even approximating a solution to \netlearn{} is hard.
More precisely, we show hardness for the optimization version of the problem, defined in \Cref{sec:problem} as \netlearnopt{}.
We give a proof for the Bayesian learning rule; we believe for the proof for majority vote to be similar.

\begin{theorem}[ ]\label{thm:approx}
    \netlearnopt{} with the Bayesian inference $\mu = \mu^B$ is \apx-hard. 
\end{theorem}

\begin{proof}
    We perform a PTAS reduction to \maxsat, where again each clause has exactly three literals (also referred to as \problem{Max E3-SAT}), which is known to be \apx-hard.
    In particular, we show that an approximation scheme of $ \olr(\network) $ implies an approximation scheme of \maxsat{}.

    Given an instance of \maxsat, we construct the graph $ G_\varphi $ from \Cref{def:bayesian_graph}.
    Let $ \asg^* $ be a maximal assignment, defined in \Cref{def:max_assignment}, satisfying $ M^* $ clauses.
    To prove the theorem, it suffices to show that for every $ \delta \in \left( 0,1 \right) $, there is an $ \alpha \in \left( 0,1 \right) $ such that if an ordering $ \sigma $ achieves a LR which is $ \alpha  $-close to the optimum, then the induced assignment $ \asg(\sigma) $ satisfies $ \delta M^*$ clauses.
    Abusing notation, for an assignment $ \asg $ we denote $ \clr(\asg) \deq \clr(\network,\sigma^*(\asg)) $.
    
    Note that for any 3-CNF formula, assigning truth values independently and uniformly at random satisfies $\frac 7 8$ of the clauses in expectation, implying that $ M^* \geq \frac 78 M $~\cite{Hastad2001-fg}. 
    From \Cref{lemma:bayesian_clause}, it follows that unsatisfied clauses receive optimal CLR $ \pzero $, and satisfied clauses receive at least $ \pone $ and at most $ \pthree $.
    It then follows that \begin{align*}
         \clr(\asg^*)&\geq M^*\pone+(M-M^*)\pzero+N \pcell.
    \end{align*}
    Let $ \asg' $ be an assignment which achieves $ \lr(\asg') \geq \olr(\network) - \varepsilon \geq \lr(\asg^*)  - \varepsilon $ for some $ \varepsilon > 0 $ specified later.
    Multiplying by the number of vertices in $ G_\varphi $, we get \[
        \clr(\asg') \geq \clr(\asg^*) - \varepsilon (3M+3N) \geq \clr(\asg^*) - 6\varepsilon M,
    \]
    where the second inequality holds since WLOG $ N \leq M $, by duplication of clauses.
    Re-arranging, we have that the number of satisfied clauses under $\asg'$ is
    $$M'\geq \tfrac{\pone - \pzero}{\pthree - \pzero}M^*-\tfrac{6\eps M}{\pthree - \pzero} \geq M^*\left( \tfrac{\pone - \pzero}{\pthree - \pzero}-\tfrac{48\eps}{7(\pthree - \pzero)}\right), $$
    where the second inequality holds since $M^*\geq \frac 78 M$.
    If we now set the final expression equal to $ \delta M^* $, we get an equality in $ \varepsilon $ and $ p $.
    Solving for $ \varepsilon $, we get
    \begin{equation}\label{eq:eps}
        \eps=\tfrac{7}{48}( (\pone - \pzero)-\delta(\pthree - \pzero)).
    \end{equation}
    This is a function of $ p $ and $ \delta $. 
    We only require that this $\varepsilon > 0$.
    Using that $p \in (\frac 12,1)$, this gives us \[
    \delta < \tfrac{4 p^4-8 p^3-4 p^2+8 p+2}{4 p^8-16 p^7+13 p^6+17 p^5-12 p^4-23 p^3+5 p^2+12 p+2}.
    \]
    On $p \in (\frac 12, 1)$, the RHS can be strictly lower-bounded by $p^2$. It thus suffices to set $p \geq \sqrt \delta$.
    
    This proves that an approximation to \netlearnopt{} within an additive bound $ \varepsilon $ for any $ p \geq \sqrt \delta$ implies an approximation to \maxsat{} within a factor of $\delta$. 
    Converting $\varepsilon$ to a multiplicative bound, since $ \olr \in (0,1) $, \[
        \olr - \varepsilon = \left( 1- \tfrac{\varepsilon}{\olr} \right)\olr \leq \left( 1-\varepsilon \right) \olr.
    \]
    Thus we also have a multiplicative bound $ \alpha = (1-\varepsilon) $ for each $ \delta $, which is what we wanted to prove. 
\end{proof}


\begin{corollary}[ ]
    For any fixed constant $ p \in (\sqrt{ {7}/{8}}, 1) $, an $ \alpha $-approximation of \netlearnopt{} with Bayesian inference rule is \np-hard for every $ \alpha > \alpha(p) $, defined as \[
        \alpha(p) \deq 1-\tfrac{7}{48}( (\pone - \pzero)-\tfrac 78(\pthree - \pzero)).
    \]
\end{corollary}

\begin{proof}
    This follows from \Cref{thm:approx}, since approximating \maxsat{} is \np-hard for $ \frac 78 + \xi $, where $ \xi > 0 $ \cite{Hastad2001-fg}.
    The condition of $ \alpha(p) $ is achieved by substituting $ \delta = \frac 78 $ to \Cref{eq:eps}.
\end{proof}


\input{src/figures/bayes_jie_eps.tex}
