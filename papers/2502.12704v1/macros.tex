% handle fonts encoding.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{dsfont}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{amsmath,amsfonts,amsthm,textcomp,enumitem,thm-restate}
\usepackage[capitalize]{cleveref}
\newtheorem{assumption}[equation]{Assumption}
\usepackage{tikz}
\usetikzlibrary{positioning}

% We will externalize the figures
% \usepgfplotslibrary{external}
% \tikzexternalize

\usepackage{pgfplots}
\pgfplotsset{width=\columnwidth,height=0.516*\columnwidth,compat=1.9}

\makeatletter
\theoremstyle{plain}
\theoremstyle{acmplain}
\@ifundefined{observation}{%
\newtheorem{observation}{Observation}[section]
}{}
\@ifundefined{Description}{%
	\newcommand{\Description}[2][foo]{}{}
}{}
\@ifundefined{theorem}{%
\newtheorem{theorem}{Theorem}[section]
}{}
\@ifundefined{conjecture}{%
\newtheorem{conjecture}[theorem]{Conjecture}
}{}
\@ifundefined{proposition}{%
\newtheorem{proposition}[theorem]{Proposition}
}{}
\@ifundefined{lemma}{%
\newtheorem{lemma}[theorem]{Lemma}
}{}
\@ifundefined{corollary}{%
\newtheorem{corollary}[theorem]{Corollary}
}{}
\theoremstyle{definition}
\theoremstyle{acmdefinition}
\@ifundefined{example}{%
\newtheorem{example}[theorem]{Example}
}{}
\@ifundefined{definition}{%
\newtheorem{definition}[theorem]{Definition}
}{}
\theoremstyle{plain}
\theoremstyle{acmplain}
\makeatother

% hyperlinks
\usepackage[capitalize]{cleveref}

\DeclareMathOperator*{\E}{\mathbb{E}}
\def\eps{\varepsilon}
\def\problem{\textsc}
\def\problemClass{\textsf}
\def\np{\problemClass{NP}}
\def\apx{\problemClass{APX}}
\def\p{\problemClass{P}}
\def\absolute#1{\left\lvert #1 \right\rvert }
\def\asg{\mathcal{A}}
\def\deq{\mathbin{:=}}
\DeclareMathOperator{\Berd}{Ber}
\def\berd{\fce{\Berd}}
\def\fce#1#2{#1\!\left(#2\right)}
\def\fceb#1#2{#1\!\left[ #2 \right]}
\def\suchthat{\;\vert\;}
\def\pr#1{\fceb{\Pr}{#1}}
\def\prt#1{\fceb{\Pr}{\text{#1}}}
\def\Acknowledgments{
Part of this work was carried out while Amanda Wang and Filip Úradník were participants in the DIMACS REU program at Rutgers University. Amanda Wang was supported by the NSF award CNS-2150186, the REU supplement to NSF 2208663, and by Princeton University's Office of Undergraduate Research Undergraduate Fund for Academic Conferences through the Hewlett Foundation Fund.
Filip Úradník was supported by CoSP, a project funded by European Union’s Horizon 2020 research and innovation programme, grant agreement No. 823748, by the Czech Science Foundation Grant 25-15714S, and by the Charles University Grant Agency (GAUK 206523).
Jie Gao would like to acknowledge funding through NSF IIS-2229876, DMS-2220271, DMS-2311064, CCF-2208663,  CCF-2118953.
The authors would also want to thank Kevin Lu, Júlia Križanová and Rhett Olson for their useful discussion and insightful comments.
}
\def\Abstract{
Sequential learning models situations where agents predict a ground truth in sequence, by using their private, noisy measurements, and the predictions of agents who came earlier in the sequence.
We study sequential learning in a social network, where agents only see the actions of the previous agents in their own neighborhood.
The fraction of agents who predict the ground truth correctly depends heavily on both the network topology and the ordering in which the predictions are made. 
A natural question is to find an ordering, with a given network, to maximize the (expected) number of agents who predict the ground truth correctly.
In this paper, we show that it is in fact \np-hard to answer this question for a general network, with both the Bayesian learning model and a simple majority rule model. 
Finally, we show that even approximating the answer is hard.
}
\def\Keywords{Social Networks, Social Learning, Sequential Learning, Bayesian Learning, Complexity, Majority Vote}

\def\netlearn{\problem{Network Learning}}
\def\netlearnopt{\problem{Opt Network Learning}}
\def\maxsat{\problem{Max \sat}}
\def\sat{\problem{3-SAT}}
\def\network{\mathcal{N}}
\def\lr{\overline{\mathcal{L}}}
\def\clr{\mathcal{L}}
\def\olr{\lr^*}
\def\oclr{\clr^*}
\def\cell#1{\fce{\mathbb{C}}{#1}}
\def\gadget#1{\fce{\mathbb{G}}{#1}}
\def\vars{\chi}

\def\ourparagraph{\paragraph*}
\newcommand{\figThickness}{thick}
\newcommand{\figCircSize}{20}
\newcommand{\figSamples}{300}
 
\def\pone{\clr_1}
\def\ptwo{\clr_2}
\def\pthree{\clr_3}
\def\pzero{\clr_0}
\def\pcell{\clr_{\text{cell}}}

\newcommand{\appbayes}{\Cref{app:bayesian_gadgets}}
\newcommand{\appmaj}{\Cref{app:majority_proof}}
