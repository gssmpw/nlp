\subsection{\metric{} analysis}
% \textbf{\(\bm{Grace_q^{h}}\)}}
\label{sec:analysis-metric}
%
\input{2025_acl_advcal/sections/metric-table}
%
We evaluate the calibration error of six LLMs on \name{} using existing metrics (ECE and Brier scores; Appendix~\ref{app:ece-brier}) and  \metric{}  (\S~\ref{sec:main-metric}). For each clue in a question, we collect  logit-based and verbalized confidences to compute metric scores. 
%

\metric{} correlates with ECE and Brier score results (Table~\ref{tab:calibration_table}); however, across both confidence elicitation methods, all models display greater error under human-adjusted \metric{} compared to MCE (\metric{} without human adjustment). \metric{} thus captures errors that existing methods overlook: cases where models underperform relative to humans by being confidently wrong or underconfident when correct. Thus, \textbf{\metric{} captures that models are especially ill-calibrated compared to humans}, and factoring in human performance reveals more room for improvement on LLM calibration. The gap between MCE and \metric{} widens for worse-performing models, suggesting that weaker models are even more miscalibrated relative to stronger models when factoring in human performance. Additionally, \metric{} reports higher errors than ECE and Brier scores across both confidence elicitation methods for most models, underscoring  calibration deficiencies that previous metrics underestimate.\footnote{All four metrics use a [0,1] scale; lower is better.} % These scores highlight significant room for improving LLM calibration.% and \metric{} provides a human-grounded evaluation framework for LLM calibration. 
%The range of confidence values exhibited by tested models indicate substantial room for improvement in LLM calibration. 

\subsection{Qualitative analysis and model errors}\label{sec:qual_analysis}
\textbf{Miscalibrated instances from \metric{}.}
All six models exhibit similar patterns for the questions on which they were most- and least-calibrated under \metric{} (examples in Appendix \ref{sec:poor-calib}). Models did best on questions that mention concrete proper nouns closely associated with the answer, even on obscure topics: for example, a question on \underline{Ireland} that gives the titles of Irish songs, a question on \underline{telomeres} that mentions the protein TRF2, and a question on \underline{Brooklyn} that mentions the neighborhood of Midwood. Models tend to be least-calibrated on questions with multiple plausible answers (e.g., one on \underline{fish} as a Buddhist symbol, since other animals also have symbolic meanings in Buddhism). Models also struggle on questions that use descriptions instead of titles (e.g. a question that describes music by \underline{Maurice Ravel}, and one that describes Jewish \underline{birth} ceremonies).

\textbf{Qualitative feedback.} We survey the human players for feedback on model abilities. Differences in model calibration are visible to the players: several find GPT-4 ``too aggressive,'' while Mistral seems much weaker, often buzzing late in the question. One player notes that the models ``obviously knew a lot, but were quite bad at gauging how well they knew something to [buzz].'' Others note that models tend to buzz on ``more concrete clues'' and struggle with multi-step reasoning. For example, a question on \underline{Alice Walker} mentions her trip to Eatonville to write about local author Zora Neale Hurston. Players note that GPT-4 incorrectly guesses ``Zora Neale Hurston,'' while human players correctly say ``Alice Walker.''

Players also note that when models were incorrect, they give more ``unreasonable'' answers than humans do. For example, models incorrectly answer a question on the treatise \underline{Philosophical Investigations} with ``Fermat's Little Theorem'' and ``\textit{The Lion, the Witch and the Wardrobe}.'' Guessing an equation and a children’s book with high confidence for a work of philosophy suggests serious miscalibration, since either option should be completely outside the realm of possibility; no human players gave answers so distant from the correct one. Other model errors not observed among human players include buzzing before any substantive clues are revealed, answering with a song title for a question asking for a surname, and hallucinating inexistent schools of philosophy.

Model and human strengths between question topics differ greatly. We examine models' and humans' ratios of correct to incorrect buzzes per category. Human players are best at literature, but this is the weakest or second-weakest category for all models. All models did relatively well on science. GPT-4o is much stronger at social science, arts, and science than other categories, and slightly outperforms humans for every category; GPT-4 was worse than the humans for all categories.

All human participants in our competitions were experienced players, but we find that calibration performance varies greatly even among these experts: stronger humans substantially outperform top models, but not all humans do. A general takeaway for future model-human comparisons on tasks involving calibration is that variance in human skill can greatly affect the outcome of a comparison.

A side benefit of conducting live human-model competitions was a significant degree of community involvement from trivia enthusiasts who were not researchers. In-person data collection, though more involved than crowdsourced data, offers other benefits: we found that participants were attentive and enthusiastic; moreover, in-person data collection (especially ``gamified’’ approaches) raises awareness of and interest in AI.


