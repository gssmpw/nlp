\section{Introduction}\label{sec:intro}

%\jbgcomment{I punched up the prose here, make sure citations are still appropriate}

Because language models are often miscalibrated, they are often confidently wrong~\citep{kaur2020interpreting}.
%
This mismatch between accuracy and confidence causes users to trust models more than they
should~\citep{caruana2019friends, deng2025development}, even over their own correct judgment~\citep{krause2023confidently,
  stengel2023calibrated,liu2024dellma}.
These issues are particularly severe when models are miscalibrated in ways that humans are not: users expect models to be at least as calibrated as humans, and when models are worse, users are often not prepared to address these errors. Thus, models should be \textit{at least} as calibrated as humans, making it especially crucial to identify when models commit calibration errors that humans do not. However, existing work on model calibration lacks comparison with human calibration.
%
%Thus, when we evaluate calibration, we should focus on the cases where humans need the most help~\citep{tomsett2020rapid,
%  bansal2019beyond}:
%
%when humans are wrong or humans lack confidence in their responses.
%
%However, existing methods capture mismatches between accuracy and
%confidence over all examples, including trivial cases where humans do
%not need help.
% The trustworthiness of an AI model's confidence in its predictions is crucial for high-risk applications, 
% such as medical diagnosis~\citep{}. 

%
% When models are miscalibrated in ways that
% differ from humans, it can be especially hard for users to gauge how
% much responses should be trusted.
%
% Miscalibration measurement remains a problem because of the dearth of calibration metrics applicable to open-ended generative settings, such as open-domain question answering.

%Although there exist some metrics that measures model calibration using model outputs (\S~\ref{sec:compare}), there is no standardized method that directly incorporates human judgments for evaluating how well model calibration aligns with human decision-making process~\citep{corvelo2024human}.
%
 % how humans measure their own calibration and comparing it to traditional metrics that assess model calibration .  Metrics like Expected Calibration Error (ECE)~\citep{naeini2015obtaining} and Brier Score~\citep{brier1950verification} evaluate the alignment between model outputs, confidence, and accuracy, but fail to reflect real-world variability. As a result, \textbf{models may produce incorrect answers accompanied by misleadingly high confidence scores that do not align with human judgment}. 
%F

\begin{figure*}[!t] 
    \centering
    \includegraphics[width=\linewidth]{figures/advcal_pipe_.pdf}
    \caption{To create the \name{} dataset, expert question writers develop questions with multiple clues of decreasing difficulty via an interface that shows where weaker models struggle to answer the questions.
      %
      These questions are used in human vs. model competitions where teams compete to be the first to interrupt the sequence of clues with a correct answer. We record when the human and model teams buzz in each question with their correctness (+) or incorrectness (-) (\textit{buzzpoints} \faBell). The dataset contains all buzzpoints throughout the competition. Then, \metric{} measures each model's human-grounded calibration performance (\S~\ref{sec:main-metric}).}
      %
    \label{fig:dataset_pipe}
\end{figure*}

% Thus, we introduce \name{}, a \textbf{Gra}nular, human-grounded Benchmark for
% Model \textbf{C}alibration \textbf{E}valuation.
We thus introduce \name{}, a  \textbf{Gra}nular, Human-grounded Benchmark for Model \textbf{C}alibration \textbf{E}valuation.
%
Each instance allows \textbf{fine-grained calibration measurement} using
an incremental question-answering (\abr{qa}) framework.
%
%\jbgcomment{Here we're using ``instances'' rather than questions.
%  Strongly advocate creating a macro so that we can make this match
%  (vote for ``elicitation'').}
%
Expert writers design \name{} questions, each consisting of at least five sentences of clues that gradually become easier.
%
%
% \jbgcomment{There was a mention of adversarial here, but it wasn't
%   introduced well.  So let's bring that up later.}
%
%\jbgcomment{cite ambigqa, crepe, etc.}
To prevent models from being confused by ambiguities or false
presuppositions, we require that clues challenge models but remain
clear for humans.
%
%This allows for accurate calibration measurement of both humans and
%models.
This format measures model calibration with human performance as a reference point: models should give correct answers earlier and more confidently than humans, while minimizing confidently incorrect guesses (\S~\ref{sec:dataset}).

%Model miscalibration is particularly salient when humans need help; so
% we find those situations. \efcomment{ I'd avoid casual phrasings like this in favor of more precise language}
%
\name{} incorporates human responses from live \abr{qa} competitions we conduct. Unlike prior calibration evaluation methods that only allow model--model
calibration comparisons, our dataset thus allows direct
\textit{human--model} calibration comparison.
%
\textbf{\name{} is the first benchmark dataset designed to
  evaluate model calibration grounded in human needs}.

This unique dataset is the foundation for a new metric (\metric{}, \S~\ref{sec:main-metric}).
%
In contrast to other calibration evaluation methods that only calculate aggregate calibration over
the entire dataset, \name{} also facilitates per-instance evaluation, which
helps in identifying specific contexts where models are much worse than humans at avoiding confidently incorrect answers.

%We apply \metric{} both with direct generative \abr{llm} logits and verbalized confidence.
%
We find that \textbf{language models are more overconfident than humans in incorrect
  answers, and relatively underconfident in
   correct answers.} In contrast, humans tend to be highly
  confident---over 50\%---when correct (\S~\ref{sec:analysis-bayes}).
%
Models struggle with abstract descriptions---they are both
overconfident and inaccurate---but excel in retrieving facts
given unambiguous clues (\S~\ref{sec:qual_analysis}).
%\yycomment{[Eve] add examples to what these are.}
%
We conclude with a discussion of how \name{} and \metric{} can aid in the
creation of models that are more accurate and better calibrated.

%-----------------------------------------------
%Previous Intro
% Thus, we introduce \textbf{Gra}nular, Human-grounded Benchmark for Model \textbf{C}alibration \textbf{E}valuation (\name{}), with three crucial features
% meant to improve measurement of model calibration.
% %
% First, each
% instance allows \textbf{granular calibration measurement} using an incremental question-answering (QA) framework. Expert authors
% design our QA instances to comprise four to five clues that are
% progressively easier. This format helps in measuring model calibration
% on both correct and incorrect guesses, where models are expected to
% provide correct guess on easy clues
% (\S~\ref{sec:dataset}). Second, \name{} uses
% \textbf{adversarial questions}, designed to challenge language models but remain answerable by humans.
% This ensures that \name{} captures model
% weaknesses and evaluates its genuine calibration issues without
% attributing errors to clue ambiguity.  Our authorship process focuses
% on these two features with multiple rounds of validation. 

% When models are miscalibrated in ways that differ from humans, it can be especially hard for human users to gauge the extent to which responses should be trusted. \name{} uses human datapoints from live QA competitions to
% constrast models' and \textbf{humans' calibration performance}. Unlike previous
% datasets that only enable model-model calibration comparisons, our
% dataset thus allows direct \textit{human-model} calibration comparison
% and identifies model weaknesses that are absent in humans. To our
% knowledge, \textbf{\name{} is the first benchmark dataset designed to
%   evaluate model calibration grounded in human performance}.

% To demonstrate a simple way to evaluate model calibration using
% \name{}, we propose a baseline metric for comparing human and model
% calibration (\S~\ref{sec:main-metric}). In contrast to other
% calibration metrics that aggregate results over the entire dataset,
% \name{} facilitates per-instance evaluation, which helps in
% identifying the instances where a model's confidence diverges from
% human-like judgments.
% %

% %
% Analyzing six strong LLMs' confidence---elicited with logits and verbalization---and their answer correctness compared to humans, LLMs exhibit greater overconfidence than humans in both correct and incorrect answers. Additionally, models show relatively low confidence in their correct answers, whereas humans tend to be highly confident---over 50\%---when correct (\S~\ref{sec:analysis-bayes}). Across all models, overconfidence is particularly evident in questions requiring an understanding of a work or conceptâ€™s description, rather than simple binary term associations.\yycomment{[Eve] add examples to what these are.}
% We also include validation of how each instance includes
% clues that are progressively adversarial
% (\S~\ref{sec:analysis-adversarial}) and potential avenues for \name{}
% to aid future research in model calibration evaluation.
%  % 
%-----------------------------------------------

% Add more as results come in.
% Our work thus aims to build on previous work via three main contributions:

% \begin{itemize}
%     \item \textbf{Instance granularity:} Incremental questions allow for fine-grained measurement of calibration at different levels of information.
%     \item \textbf{Adversariality:} Our new, expert-authored questions are designed to be more challenging for models than for humans.
%     \item \textbf{Human grounding of confidence calibration:} Human vs. computer competitions allow for comparison of human and model calibration on the same task. 
% \end{itemize}

% We collaborated with human experts to create adversarial questions in a pyramidal format~\citep{triviaquestions}, where the adversarialness against a strong LLM (GPT-3.5) decreases as more of the question is revealed. 
%

%\jbgcomment{Need one more paragraph forward pointing to related work and conclusion.}
%  We create a benchmark that allows for evaluating model performance under challenging scenarios, where questions are designed to exploit weaknesses in LLMs, requiring robust calibration and accuracy to achieve high reliability. 

% A key feature of our benchmark is its incremental question revelation. Humans and models generate a sequence of guesses and associated confidence scores at each step. This setup enables us to investigate three core aspects of model performance: (1) the step at which the model is confident enough to provide a correct guess, (2) the alignment between the modelâ€™s confidence scores and the correctness of its guesses, and (3) whether the modelâ€™s confidence calibration is superior to that of humans. %We design a framework that 

% To investigate the above three aspects, we simulate a process akin to a human-AI QA competition, where a human and a model ``buzzes in'' when confident about its answer (e.g., buzzpoints) as the question increments. Then, we compute the buzz probabilities based on the confidence scores, allowing us to evaluate the timing and accuracy of the modelâ€™s decisions across the question-revealing process. We demonstrate that our AdvCal dataset reveals whether and how much a model recognizes the limits of its knowledge and acknowledges that it does not know the answer.


% This process introduces a random variable \(b_t\), representing the probability of buzzing at each time step \(t\). Conceptually, \(b_t\) is modeled as flipping a weighted coin with probability \(c_t\), where \(c_t\) is the modelâ€™s confidence at step \(t\). Rather than explicitly flipping a coin,  

% Correctness is assessed using an \textit{answer equivalence} mechanism, which considers responses sufficiently similar to the gold answer as correct, while others are classified as incorrect. This approach allows for flexibility in evaluating responses beyond exact string matches, ensuring a more robust assessment of model performance.

