\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.3} 
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multicolumn{5}{c}{\textit{Verbalized-based Confidence}} \\
\midrule
\textbf{Model} & \textbf{Brier Score} & \textbf{ECE} & \textbf{MCE} & \textbf{\metric{}} \\
\midrule
GPT-4 & \cellcolor{Violet!20}0.274 \rankbox{2} & \cellcolor{Violet!20}0.259 \rankbox{2} & \cellcolor{RoyalPurple!20}0.584 \rankbox{1} & \cellcolor{RoyalPurple!20}0.588 \rankbox{1} \\
GPT-4o & \cellcolor{RoyalPurple!20}0.266 \rankbox{1} & \cellcolor{RoyalPurple!20}0.224 \rankbox{1} & \cellcolor{Violet!20}0.601 \rankbox{2} & \cellcolor{Violet!20}0.604 \rankbox{2} \\
Llama-3.1-70B-Instruct & \cellcolor{Purple!20}0.373 \rankbox{3} & \cellcolor{Purple!20}0.392 \rankbox{3} & \cellcolor{Purple!20}0.685 \rankbox{3} & \cellcolor{Purple!20}0.719 \rankbox{3} \\
LLama-2-70b-Chat & \cellcolor{WildStrawberry!20}0.490 \rankbox{4} & \cellcolor{WildStrawberry!20}0.570 \rankbox{4} & \cellcolor{WildStrawberry!20}0.739 \rankbox{4} & \cellcolor{WildStrawberry!20}0.803 \rankbox{4} \\
Llama-3.1-8B-Instruct & \cellcolor{RedOrange!20}0.623 \rankbox{5} & \cellcolor{RedOrange!20}0.693 \rankbox{5} & \cellcolor{RedOrange!20}0.774 \rankbox{5} & \cellcolor{RedOrange!20}0.843 \rankbox{5} \\
Mistral-7b-Instruct & \cellcolor{YellowOrange!20}0.716 \rankbox{6} & \cellcolor{YellowOrange!20}0.784 \rankbox{6} & \cellcolor{YellowOrange!20}0.790 \rankbox{6} & \cellcolor{YellowOrange!20}0.881 \rankbox{6} \\
\midrule
\multicolumn{5}{c}{\textit{Logit-based Confidence}} \\
\midrule
\textbf{Model} & \textbf{Brier Score} & \textbf{ECE} & \textbf{MCE} & \textbf{\metric{}} \\
\midrule
GPT-4o & \cellcolor{Purple!20}0.341 \rankbox{3} & \cellcolor{Violet!20}0.353 \rankbox{2} & \cellcolor{RoyalPurple!20}0.654 \rankbox{2} & \cellcolor{RoyalPurple!20}0.661 \rankbox{1} \\
Llama-3.1-70B-Instruct & \cellcolor{Violet!20}0.323 \rankbox{2} & \cellcolor{RoyalPurple!20}0.339 \rankbox{1} & \cellcolor{Violet!20}0.651 \rankbox{1} & \cellcolor{Violet!20}0.679 \rankbox{2} \\
GPT-4 & \cellcolor{WildStrawberry!20}0.380 \rankbox{4} & \cellcolor{Purple!20}0.388 \rankbox{3} & \cellcolor{Purple!20}0.672 \rankbox{3} & \cellcolor{Purple!20}0.684 \rankbox{3} \\
Llama-3.1-8B-Instruct & \cellcolor{RoyalPurple!20}0.302 \rankbox{1} & \cellcolor{WildStrawberry!20}0.397 \rankbox{4} & \cellcolor{WildStrawberry!20}0.675 \rankbox{4} & \cellcolor{WildStrawberry!20}0.718 \rankbox{4} \\
Mistral-7b-Instruct & \cellcolor{RedOrange!20}0.553 \rankbox{5} & \cellcolor{RedOrange!20}0.677 \rankbox{5} & \cellcolor{RedOrange!20}0.766 \rankbox{5} & \cellcolor{RedOrange!20}0.846 \rankbox{5} \\
Llama-2-70b-Chat & \cellcolor{YellowOrange!20}0.774 \rankbox{6} & \cellcolor{YellowOrange!20}0.829 \rankbox{6} & \cellcolor{YellowOrange!20}0.825 \rankbox{6} & \cellcolor{YellowOrange!20}0.921 \rankbox{6} \\
\bottomrule
\end{tabular}
}
\caption{Across both confidence elicitation methods, all models display greater error under human-adjusted \metric{} compared to MCE (\metric{} without human adjustment). \metric{} thus captures errors that existing methods overlook: cases where models underperform relative to humans by being confidently wrong or underconfident when correct.}
\label{tab:calibration_table}
\end{table}

% \begin{table*}[t!]
% \footnotesize
% \centering
% \setlength{\tabcolsep}{5pt}
% \renewcommand{\arraystretch}{1.3} 
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lccc|ccc}
% \toprule
% & \multicolumn{3}{c|}{\textit{Logit-based Confidence}} & \multicolumn{3}{c}{\textit{Verbalized-based Confidence}} \\
% %\(\bm{Grace_D^{h}}\)
% \textbf{Model} & \textbf{Brier Score} & \textbf{ECE} & \textbf{$\metric{}_D$} & \textbf{Brier Score} & \textbf{ECE} & \textbf{$\metric{}_D$} \\
% \midrule
% GPT-4o & \cellcolor{purple!20}0.341 \rankbox{3} & \cellcolor{violet!20}0.353 \rankbox{2} & \cellcolor{RoyalPurple!20}0.730 \rankbox{1} & \cellcolor{RoyalPurple!20}0.266 \rankbox{1} & \cellcolor{RoyalPurple!20}0.224 \rankbox{1} & \cellcolor{RoyalPurple!20}0.742 \rankbox{1} \\
% GPT-4 & \cellcolor{WildStrawberry!20}0.380 \rankbox{4} & \cellcolor{purple!20}0.388 \rankbox{3} & \cellcolor{violet!20}0.735 \rankbox{2} & \cellcolor{violet!20}0.274 \rankbox{2} & \cellcolor{violet!20}0.259 \rankbox{2} & \cellcolor{violet!20}0.772 \rankbox{2} \\
% Llama-3.1-70B-Instruct & \cellcolor{violet!20}0.323 \rankbox{2} & \cellcolor{RoyalPurple!20}0.339 \rankbox{1} & \cellcolor{purple!20}0.844 \rankbox{3} & \cellcolor{purple!20}0.373 \rankbox{3} & \cellcolor{purple!20}0.392 \rankbox{3} & \cellcolor{purple!20}0.828 \rankbox{3} \\
% Llama-3.1-8B-Instruct & \cellcolor{RoyalPurple!20}0.302 \rankbox{1} & \cellcolor{WildStrawberry!20}0.397 \rankbox{4} & \cellcolor{WildStrawberry!20}0.931 \rankbox{4} & \cellcolor{RedOrange!20}0.623 \rankbox{5} & \cellcolor{RedOrange!20}0.693 \rankbox{5} & \cellcolor{WildStrawberry!20}0.903 \rankbox{4} \\
% LLama-2-70b-Chat & \cellcolor{YellowOrange!20}0.774 \rankbox{6} & \cellcolor{YellowOrange!20}0.829 \rankbox{6} & \cellcolor{YellowOrange!20}0.960 \rankbox{6} & \cellcolor{WildStrawberry!20}0.490 \rankbox{4} & \cellcolor{WildStrawberry!20}0.570 \rankbox{4} & \cellcolor{RedOrange!20}0.914 \rankbox{5} \\
% Mistral-7b-Instruct & \cellcolor{RedOrange!20}0.553 \rankbox{5} & \cellcolor{RedOrange!20}0.677 \rankbox{5} & \cellcolor{RedOrange!20}0.948 \rankbox{5} & \cellcolor{YellowOrange!20}0.716 \rankbox{6} & \cellcolor{YellowOrange!20}0.784 \rankbox{6} & \cellcolor{YellowOrange!20}0.961 \rankbox{6} \\
% \bottomrule
% \end{tabular}
%  }
% \caption{\metric{} reports higher errors compared to ECE and Brier scores: all models exhibit poor calibration (ranging from 0.7 to 1.0). GPT-4o and GPT-4 are slightly better calibrated, with most rankings in first or second in all metrics. In contrast, Llama models and Mistral-7B-Instruct show high calibration errors (low rankings) when compared with human confidence.}
% \label{tab:calibration_table}
% \end{table*}