\section{Ethical Considerations} \label{Ethics}
We address ethical considerations for dataset papers, given that our work contains a new dataset, \name{}, and collects human responses in our user study. We reply to the relevant questions posed in the {\texttt{\abr{acl} 2022 Ethics \abr{faq}}}\footnote{\url{https://www.acm.org/code-of-ethics}}. 

When collecting human responses and questions, our study was pre-monitored by an official \abr{irb} review board to protect the participants' privacy rights. Buzzpoints in \name{} are anonymous; the feedback survey asked for respondents’ names for purposes of compensation, but only aggregate data and anonymous quotes are used in our study. %The identity characteristics of the participants were self-identified by the workers by answering the survey questions. 
%
Before distributing the survey, we collected consent forms for the workers to agree that their answers would be used for academic purposes. 
The trivia players were awarded a total $\$900$ \abr{USD} worth of online gift cards after the competitions. The prizes were \$150, \$100, \$50 for the first three places at each site where the tournament was held. 
The trivia writers were paid \$5 per question and editors paid \$2.50 per question edited based on the estimated completion time, and which was calculated to reach over $\$10$ \abr{usd} an hour (a rate higher than the \abr{us} national minimum wage of $7.50$ \abr{usd}
). 


% \begin{table*}[ht]
% \centering
% \begin{tabular}{@{}lcccccccc@{}}
% \toprule
% \textbf{} & \multicolumn{3}{c}{\textbf{MIT Mirror}} & \multicolumn{3}{c}{\textbf{Berkeley Mirror}} & \multicolumn{3}{c}{\textbf{Online Mirror}} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%                 & \textbf{M1: GPT4} & \textbf{M2: GPT4o} & \textbf{M3: Mistral} & \textbf{M1: GPT4} & \textbf{M2: GPT4o} & \textbf{M3: Mistral} & \textbf{M1: GPT4} & \textbf{M2: GPT4o} & \textbf{M3: Mistral} \\ \midrule
% \textbf{Total H vs M Game played} & 6 & 4 & 5 & 3 & 2 & 3 & 9 & 11 & 12 \\ 
% \textbf{H won}                    & 3 & 0 & 2 & 1 & 1 & 3 & 5 & 3 & 12 \\ 
% \textbf{Proportion of H win}      & 0.5 & 0 & 0.4 & 0.333 & 0.5 & 1 & 0.556 & 0.273 & 1 \\ 
% \bottomrule
% \end{tabular}
% \caption{Comparison of H wins across three tournaments for different models.}
% \label{table:h_vs_m}
% \end{table*

\section{Dataset Details}
\label{app:dataset-design}

\subsection{Distribution of questions in dataset}
Questions in \name{} are distributed as follows:
\begin{itemize}
    \item 20\% literature: 5\% American literature; 5\% British literature; 5\% European literature; 5\% world and other literature
    \item 20\% history: American history; 5\% world history; 5\% European history; 5\% other history
    \item 20\% science: 5\% Biology; 5\% Chemistry; 5\% Physics; 5\% computer science, math, and other science
    \item 15\% arts: 5\% painting/sculpture; 5\% classical music; 5\% other arts
    \item 15\% social sciences: 5\% religion; 5\% philosophy; 5\%
    \item 5\% geography and current events
    \item 5\% myth, pop culture, and other
\end{itemize}

This distribution is based on the standard quizbowl distribution at \texttt{acf-quizbowl.com/distribution}.

\subsection{Sample dataset questions}
\qbquestion{The protagonist of the story “Storm in a Teacup” obsessively reads a book with this number in its title. This number partly titles a work that opens by noting that people are born naturally good, used to teach children. One hundred times this number titles a poetry anthology whose length was inspired by the Classic of Poetry. A man visits a hut this number of times to recruit the Sleeping Dragon, and this many characters pledge to die on the same day while swearing the Peach Garden Oath. For 10 points, a Luo Guanzhong novel is titled for how many kingdoms?}
{\underline{three} [or \underline{san}; accept \underline{Three} Character Classic or \underline{Three} Hundred Tang Poems or Romance of the \underline{Three} Kingdoms; accept \underline{Sān}zì Jīng or Tángshī \underline{sān}bǎi shǒu or \underline{Sān}guó Yǎnyì]}
\vspace{10pt}

\qbquestion{During this decade, some members of the Circle of Seven participated in a group called the Golden Square and led a coup to remove the Iraqi government from power. The Pahlavi ruler and father of Mohammed Reza Shah was forced out of power in this decade, during which a supply route was developed to run through port cities to Tehran. The Syria-Lebanon campaign of this decade was led by Archibald Wavell, who later lost in the Western Desert, where the Battle of El Alamein took place. For 10 points, General Erwin Rommel fought in what decade when most of World War Two occurred?}
{\underline{1940s} [prompt on 40's]}
\vspace{10pt}
\qbquestion{One form of this adjective describes a distributed object whose state is equivalent to a strictly serializable database. Weight decay tends to pull a sigmoid activation function towards this adjective's namesake "regime". If all activation functions of a deep neural network have this form, the output is provably this type of mapping of the inputs. ReLU is a modified piecewise form of this type of function, which is the simplest separator in two-dimensional binary classification. The most common shape of least-squares regression is described by, for 10 points, what adjective that describes functions of the form “y=mx+b”?}
{\underline{linear} [accept \underline{linearizable} or word forms; prompt on planar or hyperplanar or word forms]}
\vspace{10pt}
\qbquestion{The NSF’s IDP sets standards for tools used to extract this substance, which include Foro 3000 and DISC. Coral, certain protists, and this substance are the primary sources of delta-O-18 information. Very thin layers of this substance that look like oil spills are nicknamed for grease. Due to diffusion, clathrates and trapped air in this substance help to track atmospheric gas concentrations, and may be retrieved from masses that undergo calving, resulting in growlers or bergy bits. For 10 points, name this material, which may be extracted in core samples from namesake sheets or from glaciers.}
{\underline{ice} [or \underline{frozen water}; or \underline{solid water}; accept \underline{ice} sheets; accept \underline{icebergs}; prompt on snow or firn; prompt on glaciers; prompt on water}
\vspace{10pt}
\qbquestion{Sephardic Jews have recently moved into this region near the original Vitagraph studios in Midwood. The world headquarters of the Chabad (“huh-BAHD”) movement is located in this region within a neighborhood where the Rebbe struck and killed Gavin Cato in 1991. Many Jews of the Bobov sect live in this region's neighborhood of Borough Park. This region, where Jews clashed with Black residents in the Crown Heights Riots, is where many Hasidic Jews live in the increasingly trendy neighborhood of Williamsburg. For 10 points, name this New York City borough whose Jewish residents often live near Coney Island.}
{\underline{Brooklyn} [or \underline{Kings County}; prompt on New York City]}
\vspace{10pt}
\qbquestion{At the 2023 Women’s World Cup, the Spanish and Dutch teams sparked controversy by seemingly performing this dance during training. Lyrics commonly sung to this dance describe putting one foot in front of the other “until the Sun shines on me.” The currently youngest lawmaker in one country led this dance while tearing papers in half. Participants in a November 2024 march performed this dance in protest over a bill introduced by David Seymour that would affect the Treaty of Waitangi. For 10 points, recent protests in New Zealand have made use of what Māori ceremonial dance?}
{\underline{haka}}

\subsection{Sample questions for calibration analysis}
\label{sec:poor-calib}

Sample question on which models are poorly calibrated:

\qbquestion{One thinker's argument that this claim is a "hyperbolic point which ought to be silent" was subject to a response titled "My Body, This Paper, This Fire." Jacques Derrida first coined the word "diff\'erance" in a book responding to Michel Foucault's Madness and Civilization and partially titled for this statement. In The Search for Natural Light, a premise involving doubt was added to this statement. This claim, which Pierre Gassendi criticized for being circular, was presented as an example of a "clear and distinct" idea. For 10 points, name this first principle coined in Ren\'e Descartes' Discourse on the Method.}
{\underline{"I think, therefore I am"} [or \underline{"cogito, ergo sum"}]} 

Sample question on which models are well-calibrated:
\label{sec:well-calib}

\qbquestion{Sephardic Jews have recently moved into this region near the original Vitagraph studios in Midwood. The world headquarters of the Chabad movement is located in this region within a neighborhood where the Rebbe struck and killed Gavin Cato in 1991. Many Jews of the Bobov sect live in this region's neighborhood of Borough Park. This region, where Jews clashed with Black residents in the Crown Heights Riots, is where many Hasidic Jews live in the increasingly trendy neighborhood of Williamsburg. For 10 points, name this New York City borough whose Jewish residents often live near Coney Island.}
{\underline{Brooklyn} [or \underline{Kings County}; prompt on New York City]}

% \subsection{Comparison of buzz performance between human and model teams}
% Figure~\ref{fig:buzz_margin} shows the buzz performance difference between human and model teams.

% \begin{figure}[!t] 
%     \centering
%     \includegraphics[width=\linewidth]{figures/buzz_correctness_bar_plot.pdf}
%     \caption{Comparison of buzz performance between human and model teams. The strongest model buzzes in correctly more often than most human teams, but does not buzz in correctly more often than the best human teams. Weaker models are more likely to be confidently incorrect (i.e. buzz in with the wrong answer) than nearly all human teams.}
%     \label{fig:buzz_margin}
% \end{figure}

\section{Dataset Comparison for Adversarialness}
\label{app:trickme-comparison}
We compare \name{} with the TrickMe dataset~\cite{wallace2019trick} on
(1) question length and
(2) question difficulty evaluation.
Overall, \name{} questions are longer: \name{} has an average of 5.09 sentences per question, while TrickMe has an average of 4.74 sentences.
% Avg Sent Len: 4.738864628820961 out of 1145
% Avg Sent Len: 5.08641975308642 out of 243
We compare the difficulty based on the accuracy rate as the clues are revealed in Table~\ref{tab:trickme-comparison}. \name{} exhibits higher adversarialness throughout the incremental questions.

\begin{table}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{@{}llccccc@{}}
\toprule
 &  & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%}\\ \midrule
\multirow{2}{*}{\textbf{TrickMe}} & \textsc{GPT-4} & 31.07\% & 65.97\% & 82.20\% & 89.96\% & 93.68\% \\
 & \textsc{GPT-4o} & 28.77\% & 61.56\% & 77.90\% & 86.64\% & 91.11\% \\
 \midrule
\multirow{2}{*}{\textbf{\name{}}} & \textsc{GPT-4} & 7.49\% & 23.62\% & 30.60\% & 40.40\% & 52.19\% \\
 & \textsc{GPT-4o} & 8.59\% & 22.65\% & 32.05\% & 42.86\% & 53.78\% \\
 \bottomrule
\end{tabular}
}
\caption{\name{} and TrickMe dataset accuracy rate comparison. Similar as Figure~\ref{fig:accuracy_rate}, we compare the accuracy when the percentage of clues revealed (\%) are 20\%, 40\%, 60\%, 80\%, 100\%.
\name{} results show much lower accuracy compared with TrickMe, indicating that our dataset is more adversarial incrementally.
}
\label{tab:trickme-comparison}
\end{table}


\section{Interface}
\subsection{Guesser details}\label{sec:guesser}
% TF-IDF retriever
Before deciding when to buzz, models need to generate answers as clues are revealed. We call this process ``guessing'' and the model is used as a \textit{guesser}.
The full prompt of the guesser is in Appendix~\ref{app:buzzpoint-prompt}, including the general instructions and retrieved examples.
We train a TF-IDF model as the retriever with past quizbowl questions following~\citet{rodriguez2019quizbowl}.\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html}}
The main goal is to reduce hallucinations and guide the model to learn the granular clue and guess format.

% Hope: We can include this in CR / ArXiv version! A little bit afraid that it will break the anonymous rule.
% \url{https://github.com/Pinafore/nlp-hw/tree/master/tfidf_guesser}

\subsection{Training buzzer for authoring interface}\label{logistic}
For the writing interface, the logistic regression model was trained with two kinds of features: GPT-3.5's logit based confidence, and pre-designed features derived from the question, answer, and metadata. Features include text-based metrics (e.g., TF-IDF scores, overlaps between Llama predictions and TF-IDF guesses), probabilistic outputs (Llama log and prompt probabilities), and contextual indicators (sentence index, length, and presence of phrases like ``10 points''). This model was trained on \citet{rodriguez2019quizbowl}, also pyramidal questions, using Llama-13b predictions~\citep{touvron2023llama}.

A primary
distinction from \citet{wallace2019trick} is that their interface only showed
the final correct guess.

\subsection{Question editors and writers}
\label{app:writer-qualifications}
Writers and editors were recruited via a public quizbowl forum and were located in the US and UK. All editors underwent IRB training and had written for at least three previous tournaments. Writers were paid \$5 per question and editors paid \$1 per question edited. A head editor with 5 years of experience writing and editing quizbowl questions, including as head editor of two previous tournaments, supervised the writers and provided an additional quality check.
The consent form is in Table~\ref{tab:consent-for-writers}.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}c@{}}
\toprule
% \textbf{Consent Form} \\ \midrule
\begin{tabular}[c]{@{}l@{}}
\textbf{Privacy Policy}\\
\midrule
\textbf{Introduction}\\
Welcome to 'Stump the Computer,' hosted by the research group at \texttt{xxx}. Your participation in our research is voluntary and deeply valued. \\ This Privacy Policy outlines how we handle the information you provide during your interaction with our research project.\\
\textbf{Purpose of the Study}\\
This research, conducted by \texttt{xxx}, aims to collect human-generated data for a fact verification system. \\ Participants, like you, who enjoy trivia knowledge and trivia-related games, are invited to help us understand how humans and computers handle challenging questions.\\
\textbf{Information Collection and Use}\\
As you participate in this project, you will interact with an online interface to submit questions designed to challenge both human and computer intelligence. \\You may use a Google Doc, automatically generated in your Google Account, to draft these questions. \\We collect this data to enhance our research and improve the interaction models between humans and computers.\\
\textbf{Data Confidentiality}\\
We take your privacy seriously. All data collected during this study will be stored on a password-protected web server with encrypted storage. \\The server is in a secure access data center, managed professionally. Access to the data is strictly limited to the principal investigators of this study. \\After the study concludes, any personally identifiable information will be anonymized or destroyed to ensure your privacy.\\
\textbf{Potential Risks}\\
We acknowledge the risk of breach of confidentiality in any online activity. \\We have implemented robust security measures to mitigate such risks and protect your data.\\
\textbf{Benefits and Compensation}\\
While there is no direct personal benefit from participating, your contributions are invaluable in advancing research on human-computer interactions. \\Compensation for participating includes \$5 per question written and \$2.50 for each question edited.\\
\textbf{Your Rights}\\
Participation is entirely voluntary. You may withdraw at any time without penalty. Should you have any questions or need to report a concern, please contact:\\
\texttt{xxx}\\
\textbf{Changes to This Policy}\\
We may update this policy periodically to reflect changes in our practices. Continued participation after such changes constitutes your acceptance of the new terms.\\
\textbf{Consent}\\
By signing up and participating, you affirm that you are at least 18 years old, have reviewed this policy, and consent to engage in this study. \\Your rights and privacy are paramount, and we are committed to protecting them.\\ \\
Thank you for participating in our task and contributing to the advancement of our research.\end{tabular} \\ \bottomrule
\end{tabular}
}
\caption{Consent form for question writers.}
\label{tab:consent-for-writers}
\end{table*}

\subsection{Model confidence elicitation}\label{sec:confidence_elicitation}
% token logits and verbalized confidence
We experiment two approaches to get the model confidence score for a generated guess.

\paragraph{Token-logit based confidence} Log probability of the generation is a common
method to estimate the model confidence~\cite{nguyen-oconnor-2015-posterior}.
To get the confidence score in our setup, we retrieve the logit for each generated token, and take the average of the exponentials of these logit values~\citep{huang2023look}.

\paragraph{Verbalized confidence} Recent study shows verbalized probabilities
can be better calibrated than log probabilities~\cite{tian-etal-2023-just, xiongcan}, which motivate us to include the verbalized confidence in our experiments.
We follow the previous prompt in Appendix~\ref{app:buzzpoint-prompt} and add the instructions from~\citet{tian-etal-2023-just} to return the confidence:
    \begin{quote}
        Given the following information, provide the title of the Wikipedia page that would best answer the last question fragment. If you are not sure, just give your best guess. If you don't know, answer None. The answer should be as short as possible. \textbf{While you give the guess, please also provide the probability that it is correct (0.0 to 1.0).} \\
        \textbf{Give ONLY the guess and probability, no other words or explanation. For example:}

        \textbf{The answer is: <most likely guess, as short as possible; not a complete sentence, just the guess!>} \\
        \textbf{Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; just the probability!>}

        Question: \{retrieved examples\} \\
        The answer is: \{retrieved examples\}
        
        Question: \{each clue\}
    \end{quote}

To ensure accurate extraction of probability scores from model outputs, we initially define the desired format based on the prompt. We then proceed to identify and print any cases where confidence scores are not successfully extracted. By observing these cases, we can discern patterns and refine our post-processing rules. This iterative approach allows us to capture as many corner cases as possible, enhancing the robustness of our data extraction process.

% Figure~\ref{fig:accuracy_rates_bins} shows the comparison between model raw and verbalized confidence.

% \begin{figure}[!t] 
%     \centering
%     \includegraphics[width=\linewidth]{figures/accuracy_rates_bins_combined.pdf}
%     \caption{Model raw and verbalized confidence by threshold}
%     % \caption{The accuracy rates of humans (blue) and models (red) across confidence threshold bins show that human's correctness rate surpasses that of models with increasing confidence values. Humans' confidence were determined by uniformly sampling within the bins.}
%     \label{fig:accuracy_rates_bins}
% \end{figure}

\section{Model Buzzpoint Generation}
\subsection{Guesser details}\label{app:buzzpoint-prompt}
To retrieve a model's top guess after $n$ clues have been revealed, we prompt the model with the first $n$ clues from the question. To retrieve the best guess, we employ a "retrieval and guess" approach to enhance QA performance, using the following prompt:
    \begin{quote}
        Given the following information, provide the title of the Wikipedia page that best answers the last question fragment. If unsure, provide your best guess. The answer should be concise.

        Question: \{retrieved examples\} \\
        The answer is: \{retrieved examples\}
        
        Question: \{each clue\} \\
        The answer is:
    \end{quote}

\subsection{Process for calculating model buzzpoints}\label{app:buzzpoint-algo}
The process for calculating model buzzpoints followed the steps below.
\begin{algorithm}
\small{
\begin{algorithmic}
\caption{Find model \buzzpoints{} for a question}
\label{alg:feature}
%\REQUIRE The previously picked feature $p$, depth $d \leq \texttt{max\_depth}$, vectors $V$ of token probabilities (from unigram, trigram, \texttt{ada}, and \texttt{davinci} models), scalar functions $F_s$, vector functions $F_v$
%\ENSURE A list of all possible features

\Require $N$, the number of clues in the question; and $t$, the buzz threshold.
\State Let $n = 0$ 
\While{$n < N$}
\State {Prompt the model to answer the question, given the first $n$ clues (Appendix \ref{app:buzzpoint-prompt}).}

\State Compute the model's confidence $c$ in its top guess by summing the log probabilities of the tokens comprising the guess.

\If{$c$ > $t$}
\State Buzz in
\State{\textbf{break}}
\EndIf
\State $n$ += 1
\EndWhile
\end{algorithmic}
}
\end{algorithm}

\subsection{Assigning buzzer threshold}\label{app:buzzpoint-threshold}
We used question data from the 2023 Expo quizbowl competition, where expert players competed against ChatGPT as a testbed to assign buzzer threshold. The dataset includes questions covering various topics, with recorded buzz and guess correctness.

We incorporate the explanation from \citet{rodriguez2019quizbowl} to explain how the threshold was set for our buzzer.
To estimate the probability $\pi(t)$ that a player has answered correctly by position $t$, the following formula is used:

\begin{equation}
    \pi(t) = 1 - \frac{N_t}{N},
\end{equation}

where $N$ is the total number of player-question records, and $N_t$ is the number of instances where a player has answered correctly by position $t$. 
This equation indicates how likely it is that a player has given the correct answer by a certain point in the question.
%
To make this probability easier to use in practice, it is approximated with a polynomial function:
%
\begin{equation}
    \pi(t) = 0.0775t - 1.278t^2 + 0.588t^3.
\end{equation}

This polynomial provides a smooth estimate of how human accuracy changes as the question progresses, 
%
allowing for a data-driven approach to determining optimal buzz thresholds.
Since we also want to get the optimal buzzpoints, we adopt a threshold from this study to determine the model buzzpoints. The confidence thresholds by model family are: -0.03 for GPT models, and  -0.05 for Mistral models.

\section{Tournament and Survey}
\label{app:tournament}
\subsection{Recruiting human players}\label{players}
Human teams were recruited by posting a call for players on social media and public forums for quizbowl players. To incentivize teams to play as well as possible, the top three teams in each tournament were awarded a prize. The prizes were \$150, \$100, \$50 for the first three places at each site where the tournament was held. The tournaments were all held in the US and the players were also located in the U.S. 
The consent form is in Table~\ref{tab:consent-for-players}.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}c@{}}
\toprule
% \textbf{Consent Form} \\ \midrule
\begin{tabular}[c]{@{}l@{}}
Welcome to a quick Trivia Quiz! You'll tackle 37 questions and decide if you're confident enough to buzz. \\After each clue, please write down your best guess even if you're not sure of the answer. \\ \\

To track your progress, refer to the progress bar at the top of the page (Please disregard the question numbers—they are randomized for fairness). \\ \\

Please enter your email address here so we can ensure prizes are sent to the right recipients at the end of the quiz. \\We're giving away 4 raffle prizes (each a \$25 gift card) to participants who complete the quiz. \\Additionally, the top scorer with the highest accuracy and fastest buzz will receive a \$5 bonus prize. \\Rewards are determined by a combination of your accuracy and buzzing speed. \\Correct answers receive 0 to 20 points depending on how early you buzz. Incorrect answers receive -5 points. \\Good luck and have fun! \\ \\

Here is the short summary of what this survey will be used for: \\

\textbf{Project Title}\\
A Leaderboard and Competition for Human-computer Adversarial Question Answering \\

\textbf{Purpose of the Study}\\
This research is being conducted by \texttt{xxx} at \texttt{xxx}. \\We are inviting you to participate in this research project because you are interested in trivia knowledge and enjoy trivia-related games. \\The purpose of this research project is to collect human-generated data and responses for building a deployable QA system. \\

\textbf{Procedures} \\
We would like to use two adversarial datasets to test our approach of determining how adversarial the questions are. \\You will provide your best guess and write down your answer in a short form.\\

\textbf{Potential Risks and Discomforts}\\
There is a risk of breach of confidentiality and that efforts to mitigate this risk are described in the Confidentiality section below. \\

\textbf{Potential Benefits} \\
While this research is not designed to benefit you personally, we hope that in the future, the researchers might benefit from human computer interaction studies \\with investigation of \\question answering behavior and AI development during this study. \\

\textbf{Confidentiality}\\
Only the investigators (\texttt{xxx}) of this study will have access to the study data. \\Data will be stored on a password-protected web server with encrypted storage. \\The server is professionally managed in a secure access data center. \\After the study ends, only user names associated with e-mail addresses will be retained and the associated e-mail addresses will be deleted. \\

\textbf{Right to Withdraw and Questions} \\
Your participation in this research is completely voluntary. You may choose not to take part at all. \\If you decide to participate in this research, you may stop participating at any time. \\If you decide not to participate in this study or if you stop participating at any time, you will not be penalized or lose any benefits to which you otherwise qualify. \\If you decide to stop taking part in the study, if you have questions, concerns, or complaints, or if you need to report an injury related to the research, please contact \\the investigator: \\
\texttt{xxx} \\ \\

If you have any questions, please use this address: \texttt{xxx}
\end{tabular} \\ \bottomrule
\end{tabular}
}
\caption{Consent form for players.}
\label{tab:consent-for-players}
\end{table*}

\subsection{Player expertise}\label{app:player_specifics}
Respondents had an average of 5.5 years of previous experience playing quizbowl. 22\% of players had studied or were currently studying in the physical sciences or engineering; 31\% studied computer science or math; 17\% studied the humanities; 13\% studied a combination of fields; and 17\% were undecided. Since quizbowl players typically specialize in certain categories and learn more about those areas, we also asked them for their areas of specialization. 39.13\% of respondents listed the sciences as an area of specialization; 21.74\% listed history; 39.13\% listed the social sciences; 52.17\% listed literature; 39.13\%	listed fine arts; and 21.74\% listed geography or current events.

\subsection{Ranking human performance from survey}
\label{app:survey-quartiles}
Because the survey measures individual accuracy, without a competition setting, we rank humans using the following metric: participants earn $(20-20c)$ points per question with a correct buzz and lose 5 points per question with an incorrect buzz, where $c$ is the proportion of clues seen at the buzzpoint. Human quartiles and top/bottom half categorization based on the offline buzzpoints (for Figures \ref{fig:accuracy_rate} and \ref{fig:bayes}) are taken from rankings under this metric.

% \subsection{Model Calibration and Alignment with Human Confidence Expectations}\label{app:human_adjusted_metric}
% \begin{table*}[t!]
% \footnotesize
% \centering
% \setlength{\tabcolsep}{5pt}
% \renewcommand{\arraystretch}{1.3} 
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccc|cccc}
% \toprule
% & \multicolumn{4}{c|}{\textbf{Logit-based}} & \multicolumn{4}{c}{\textbf{Verbalized-based}} \\
% \textbf{Model} & \textbf{Brier Score} & \textbf{ECE} & \textbf{MCE} & \textbf{\metric{}} & \textbf{Brier Score} & \textbf{ECE} & \textbf{{MCE}} & \textbf{\metric{}} \\
% \midrule
% GPT-4o & \cellcolor{purple!20}0.341 \rankbox{3} & \cellcolor{violet!20}0.353 \rankbox{2} & \cellcolor{RoyalPurple!20}0.830 \rankbox{1} & \cellcolor{RoyalPurple!20}0.730 \rankbox{1} & \cellcolor{RoyalPurple!20}0.266 \rankbox{1} & \cellcolor{RoyalPurple!20}0.224 \rankbox{1} & \cellcolor{RoyalPurple!20}0.834 \rankbox{1} & \cellcolor{RoyalPurple!20}0.742 \rankbox{1} \\
% GPT-4 & \cellcolor{WildStrawberry!20}0.380 \rankbox{4} & \cellcolor{purple!20}0.388 \rankbox{3} & \cellcolor{violet!20}0.833 \rankbox{2} & \cellcolor{violet!20}0.735 \rankbox{2} & \cellcolor{violet!20}0.274 \rankbox{2} & \cellcolor{violet!20}0.259 \rankbox{2} & \cellcolor{violet!20}0.857 \rankbox{2} & \cellcolor{violet!20}0.772 \rankbox{2} \\
% Llama-3.1-70B-Instruct & \cellcolor{violet!20}0.323 \rankbox{2} & \cellcolor{RoyalPurple!20}0.339 \rankbox{1} & \cellcolor{purple!20}0.904 \rankbox{3} & \cellcolor{purple!20}0.844 \rankbox{3} & \cellcolor{purple!20}0.373 \rankbox{3} & \cellcolor{purple!20}0.392 \rankbox{3} & \cellcolor{purple!20}0.890 \rankbox{3} & \cellcolor{purple!20}0.828 \rankbox{3} \\
% Llama-3.1-8B-Instruct & \cellcolor{RoyalPurple!20}0.302 \rankbox{1} & \cellcolor{WildStrawberry!20}0.397 \rankbox{4} & \cellcolor{WildStrawberry!20}0.960 \rankbox{4} & \cellcolor{WildStrawberry!20}0.931 \rankbox{4} & \cellcolor{RedOrange!20}0.623 \rankbox{5} & \cellcolor{RedOrange!20}0.693 \rankbox{5} & \cellcolor{WildStrawberry!20}0.940 \rankbox{4} & \cellcolor{WildStrawberry!20}0.903 \rankbox{4} \\
% LLama-2-70b-Chat & \cellcolor{YellowOrange!20}0.774 \rankbox{6} & \cellcolor{YellowOrange!20}0.829 \rankbox{6} & \cellcolor{YellowOrange!20}0.974 \rankbox{6} & \cellcolor{YellowOrange!20}0.960 \rankbox{6} & \cellcolor{WildStrawberry!20}0.490 \rankbox{4} & \cellcolor{WildStrawberry!20}0.570 \rankbox{4} & \cellcolor{RedOrange!20}0.944 \rankbox{5} & \cellcolor{RedOrange!20}0.914 \rankbox{5} \\
% Mistral-7b-Instruct& \cellcolor{RedOrange!20}0.553 \rankbox{5} & \cellcolor{RedOrange!20}0.677 \rankbox{5} & \cellcolor{RedOrange!20}0.965 \rankbox{5} & \cellcolor{RedOrange!20}0.948 \rankbox{5} & \cellcolor{YellowOrange!20}0.716 \rankbox{6} & \cellcolor{YellowOrange!20}0.784 \rankbox{6} & \cellcolor{YellowOrange!20}0.974 \rankbox{6} & \cellcolor{YellowOrange!20}0.961 \rankbox{6} \\
% \bottomrule
% \end{tabular}
% }
% \caption{Llama-3.1 models show a larger gap between model calibration error (MCE), 1 - \(\E {g_t c_t}\) and human-adjusted \metric{}, which suggests that they are less aligned with human confidence expectations. In contrast, Llama-2-70b-Chat and Mistral-7b-Instruct show marginal improvement, indicating a lack of alignment with human calibration.}
% \end{table*}
% We examine that GPT-4o and GPT-4 exhibit stronger calibration across the board. They show smaller differences between model calibration error (MCE), 1 - \(\E {g_t c_t}\) and human-adjusted \metric{} compared to other models. On the other hand,
% %


% \begin{figure*}[!t] 
%     \centering
%     \includegraphics[width=\linewidth]{figures/bootstrap_boxplot.pdf}
%     \caption{Comparison of the our two metrics on our dataset with previous metrics (expected calibration error and Brier score).}
%     \label{fig:buzz_metric}
% \end{figure*}

\section{ECE and Brier Score Details}
\label{app:ece-brier}
Expected Calibration Error (ECE)~\citep{naeini2015obtaining, guo2017oncalibration} and Brier scores \cite{brier1950verification} are widely used metrics for assessing model calibration.
Below, we define these metrics using the notations introduced in Section~\ref{sec:baseline-metric}:
\( g_t \) represents the answer correctness at clue \(t\) ($1$ if correct, $0$ otherwise, which is slightly different from \metric{} for simplicity);
\( c_t \) represents the corresponding model's confidence in its guess.

\paragraph{ECE} It measures the weighted average over the absolute difference between accuracy and confidence. To compute this, we first split confidence values into \( M = 10 \) bins equally.
\( B_m \) represents the confidence set of the \(m^{\text{th}}\) bin.
\( N \) is the total number of clues across the dataset.
% \[ \text{ECE} = \sum_{m=1}^M \frac{T}{n} \Bigg| \frac{\sum g_t}{T} - \frac{\sum c_t}{T} \Bigg| \]
% Hope: further simplified version of ECE
% \[ \text{ECE} = \sum_{m=1}^M \frac{\bigg| \sum\limits_{t \in B_m} g_t - \sum\limits_{t \in B_m} c_t \bigg|}{N} \]
\[ \text{ECE} = \frac{1}{N} \sum_{m=1}^M \bigg| \sum\limits_{t \in B_m} g_t - \sum\limits_{t \in B_m} c_t \bigg| \]

\paragraph{Brier Score} It measures the mean squared difference between the predicted probability and the actual binary outcome, measuring how well the predicted confidence aligns with the true correctness of the answer. A lower Brier Score indicates better calibration, as it reflects more accurate and well-calibrated probability estimates.
\[ \text{Brier Score} = \frac{1}{N} \sum_{t=1}^N (c_t - g_t)^2 \]


\section{Another Potential Human-grounded Calibration Metric using \name{}}
We use the same notation as \S~\ref{sec:metric-equation}. To quantify model calibration, we define the \textit{buzz confidence}, \( b_t \), representing the likelihood that the model will be confident at time step \( t \). This probability depends on the model's confidence \( c_t \) at \( t \) and the cumulative non-buzz probabilities from earlier steps, expressed as \( b_t = c_t \prod_{i=0}^{t-1} (1 - c_i) \), where \( c_t \) is the confidence score for the system’s guess at time \( t \).

The model score \( S_t \) for each step is calculated as \( S_t = b_t g_t \), where \( g_t \) is the correctness of the system's guess (\( g_t = 1 \) for a correct answer, \( g_t = 0 \) otherwise). The total system score \( S_q \) for a given question is the sum of scores across all steps: \( S_q = \sum_{t=0}^T S_t \). A key assumption of this framework is that the system must reach full confidence at some point, ensuring \( \sum_{t=0}^T b_t = 1 \). Consequently, if no correct answer is provided earlier, the confidence probability will reach 1 at the final step.

To compare model calibration with human calibration, we introduce the \textbf{human-adjusted calibration score} (\( SH_q \)), which uses human response timing as a benchmark. Human buzz probabilities (\( h_t \)) represent the proportion of humans answering correctly by step \( t \). The score accounts for cases where the system buzzes correctly before humans and when humans do not buzz at all. Let:
\[K_t = h_t \sum_{e=0}^{t} b_e g_e,\]
where \(b_e\) is the system's confidence probability at step \(e\), \(g_e\) indicates whether the system's guess is correct, \(h_t\) is the human buzz probability at step \( t \), and \(e\) is the index representing the step within the range from $0$ to $t$.

The human-adjusted calibration score is then defined as:
\[SH_q = \sum_{t=0}^{T} K_t + \left( 1 - \sum_{t=0}^{T} h_t \right) \sum_{t=0}^{T} b_t g_t,\]
where the first term captures the probability of the system buzzing before humans and being correct, and the second term accounts for cases where humans did not buzz. This adjustment penalizes models for overconfidence in scenarios where humans abstain from answering, ensuring that the calibration score reflects both the model's accuracy and its alignment with human calibration. 

\section{Experiment Details}
% Model Size And Budget: Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?
We query OpenAI APIs for GPT-4 (gpt-4-0613) and GPT-4o (gpt-4o-2024-08-06) experiments.
For other experiments, we implement model inference with vLLM~\cite{kwon2023efficientmemorymanagementlarge} using Hugging Face model names:
\begin{itemize}
    \item {meta-llama/Meta-Llama-3.1-8B-Instruct}
    \item {meta-llama/Meta-Llama-3.1-70B-Instruct}
    \item {mistralai/Mistral-7B-Instruct-v0.3}
    \item {meta-llama/Llama-2-70b-chat-hf}
\end{itemize}
For 7B/8B models, we use 1 NVIDIA RTX A6000 GPU and 32GB of RAM, and processing each question takes approximately about 5 seconds.
For 70B models, 8 NVIDIA RTX A5000 GPUs and 64GB of RAM are used, and approximately each question takes 40 seconds.
The temperature is set to be 0 for all experiments. The metric computation is highly efficient, taking only 5-10 minutes on a single CPU for datasets of moderate size. Our metric is a single-run metric that evaluates models based on their confidence values. The human-in-the-loop process introduces variability, they are consistent with the cost of crafting other common QA datasets.
% the paper itself needs to note that it’s a single-run metric (in an appendix) and we reference that section in the checklist
% If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation, such as NLTK, SpaCy, ROUGE, etc.), did you report the implementation, model, and parameter settings used?
We use NLTK, SpaCy, regex, and PEDANTS packages for data pre-processing of the collected buzzpoints.

\section{License Details}
\name{} is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). 
To view a copy of this license, visit \href{https://creativecommons.org/licenses/by/4.0/}{https://creativecommons.org/licenses/by/4.0/} or send a letter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.
When using this dataset, please attribute as follows:
\begin{quote}
\name{} is licensed under CC BY 4.0. To view a copy of this license, visit \url{https://creativecommons.org/licenses/by/4.0/}.
\end{quote}

\section{\metric{} Normalization}\label{app:normalization}
For our \metric{}, we apply a normalized sigmoid transformation:
\[\text{\metric{}}(x) = 1 - r \left( \mathbb{E} \left[ (1 - h_t) g_t c_t\right] \right).
\]
$r(x)$ is a normalized sigmoid function designed to map an expected value from a [-1, 1] range to a [0,1] range.
\[r(x) = \frac{\sigma(x) - \sigma(-1)}{\sigma(1) - \sigma(-1)},\]
where
\[\sigma(x) = \frac{1}{1 + e^{-x}}.\]

\section{\metric{} analysis by category}
With logit-based \metric{} scores, GPT-4o performs best in Arts and Science, while LLaMA-2-70B-Chat has the highest error in Arts. GPT-4o also excels in History and Literature, whereas LLaMA-2-70B-Chat struggles most in Literature.

In verbalized-based \metric{} scores, GPT-4o  achieves the lowest calibration errors across Arts and Science. Meanwhile, Mistral-7B-Instruct and LLaMA-2-70B-Chat show the highest errors in Literature, respectively. Verbalized outputs generally show improved calibration compared to logit-based confidence for Mistral and Llama-3 models.
\begin{table*}[]
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{@{}lcc|cc|cc|cc|cc|cc@{}}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multicolumn{2}{c}{\textbf{GPT-4}} & \multicolumn{2}{c}{\textbf{GPT-4o}} & \multicolumn{2}{c}{\textbf{Mistral-7b-Instruct}} & \multicolumn{2}{c}{\textbf{LLama-2-70b-Chat}} & \multicolumn{2}{c}{\textbf{Llama-3.1-8B-Instruct}} & \multicolumn{2}{c}{\textbf{Llama-3.1-70B-Instruct}} \\
 & \textbf{Logit} & \textbf{Verbalized} & \textbf{Logit} & \textbf{Verbalized} & \textbf{Logit} & \textbf{Verbalized} & \textbf{Logit} & \textbf{Verbalized} & \textbf{Logit} & \textbf{Verbalized} & \textbf{Logit} & \textbf{Verbalized} \\
 \midrule
% American Hist & 0.5908 & 0.5148 & 0.573 & 0.5815 & 0.7774 & 0.778 & 0.8491 & 0.7279 & 0.6683 & 0.7567 & 0.6397 & 0.6939 \\
% American Lit & 0.6939 & 0.5743 & 0.6661 & 0.6245 & 0.7308 & 0.7498 & 0.7925 & 0.7195 & 0.6432 & 0.759 & 0.6805 & 0.6887 \\
% Biology & 0.4699 & 0.49 & 0.4909 & 0.4874 & 0.6539 & 0.7471 & 0.7797 & 0.6838 & 0.657 & 0.7101 & 0.6214 & 0.5803 \\
% British Lit & 0.7808 & 0.6782 & 0.7947 & 0.7348 & 0.8234 & 0.8342 & 0.8808 & 0.818 & 0.6948 & 0.8559 & 0.7188 & 0.7564 \\
% Chemistry & 0.6536 & 0.5907 & 0.6317 & 0.5232 & 0.7029 & 0.7605 & 0.7782 & 0.7264 & 0.6731 & 0.7617 & 0.6311 & 0.6532 \\
% Classical Music & 0.6927 & 0.5998 & 0.6166 & 0.6325 & 0.8086 & 0.8266 & 0.8464 & 0.7404 & 0.7014 & 0.7703 & 0.6531 & 0.7088 \\
% Euro Hist & 0.7819 & 0.6313 & 0.7758 & 0.7115 & 0.7958 & 0.8027 & 0.8391 & 0.7461 & 0.6982 & 0.8269 & 0.6872 & 0.7434 \\
% Euro Lit & 0.7351 & 0.6304 & 0.6973 & 0.6404 & 0.753 & 0.741 & 0.8186 & 0.7773 & 0.7013 & 0.7427 & 0.6423 & 0.6684 \\
% Geo/CE & 0.6722 & 0.5931 & 0.6734 & 0.6294 & 0.7489 & 0.8004 & 0.8153 & 0.7266 & 0.6273 & 0.7299 & 0.6404 & 0.6868 \\
% Geo/CE/other & 0.3472 & 0.3134 & 0.5365 & 0.3299 & 0.5713 & 0.7162 & 0.7637 & 0.4905 & 0.5727 & 0.4644 & 0.5817 & 0.6579 \\
% Myth & 0.7728 & 0.6964 & 0.7208 & 0.6703 & 0.7747 & 0.7977 & 0.8272 & 0.7592 & 0.67 & 0.802 & 0.6995 & 0.7507 \\
% Myth/Other Academic & 0.5981 & 0.515 & 0.5701 & 0.5491 & 0.7967 & 0.8277 & 0.8467 & 0.7397 & 0.6719 & 0.8109 & 0.6762 & 0.7334 \\
% Other Arts & 0.6466 & 0.538 & 0.5711 & 0.4672 & 0.6662 & 0.7569 & 0.8082 & 0.7368 & 0.6417 & 0.751 & 0.6298 & 0.6784 \\
% Other Fine Arts & 0.7201 & 0.6791 & 0.7294 & 0.6699 & 0.7724 & 0.8113 & 0.7529 & 0.7211 & 0.5888 & 0.7483 & 0.625 & 0.6724 \\
% Other Hist & 0.7257 & 0.5959 & 0.6794 & 0.6216 & 0.7708 & 0.802 & 0.8055 & 0.7323 & 0.6527 & 0.7722 & 0.6634 & 0.7103 \\
% Other Sci & 0.608 & 0.5581 & 0.6126 & 0.5326 & 0.7806 & 0.7944 & 0.8145 & 0.711 & 0.6277 & 0.704 & 0.5588 & 0.5333 \\
% Painting/Sculpture & 0.6385 & 0.5257 & 0.5659 & 0.5303 & 0.7425 & 0.7681 & 0.7705 & 0.6906 & 0.6477 & 0.7173 & 0.611 & 0.6435 \\
% Philosophy & 0.6468 & 0.5235 & 0.6629 & 0.601 & 0.8034 & 0.811 & 0.8591 & 0.8021 & 0.714 & 0.822 & 0.6133 & 0.583 \\
% Physics & 0.6598 & 0.5948 & 0.6724 & 0.6269 & 0.765 & 0.7746 & 0.7955 & 0.72 & 0.669 & 0.7543 & 0.5419 & 0.6043 \\
% Religion & 0.7126 & 0.6577 & 0.6589 & 0.5636 & 0.8089 & 0.8131 & 0.8613 & 0.7505 & 0.7057 & 0.8204 & 0.6943 & 0.78 \\
% Social Science & 0.6836 & 0.6287 & 0.6656 & 0.5911 & 0.8063 & 0.8042 & 0.8417 & 0.7243 & 0.7017 & 0.7811 & 0.6799 & 0.7338 \\
% World Hist & 0.7376 & 0.6075 & 0.7359 & 0.6536 & 0.8177 & 0.8262 & 0.8668 & 0.7848 & 0.7184 & 0.8562 & 0.7152 & 0.7635 \\
% World/Other & 0.7246 & 0.6025 & 0.7151 & 0.6749 & 0.7602 & 0.7657 & 0.8089 & 0.7503 & 0.6702 & 0.7873 & 0.6958 & 0.7402 \\
% World/Other Lit & 0.6912 & 0.7085 & 0.7952 & 0.7436 & 0.8317 & 0.8398 & 0.9377 & 0.7946 & 0.8098 & 0.9171 & 0.8417 & 0.7838 \\
Arts & 0.6646 & 0.564 & 0.5946 & 0.5592 & 0.7489 & 0.7889 & 0.8063 & 0.7218 & 0.6623 & 0.7466 & 0.6317 & 0.6774 \\
Geo/CE & 0.618 & 0.5465 & 0.6506 & 0.5795 & 0.7193 & 0.7864 & 0.8067 & 0.6872 & 0.6182 & 0.6857 & 0.6307 & 0.6819 \\
History & 0.709 & 0.5874 & 0.691 & 0.6421 & 0.7904 & 0.8022 & 0.8402 & 0.7478 & 0.6844 & 0.803 & 0.6764 & 0.7278 \\
Literature & 0.7315 & 0.628 & 0.7233 & 0.673 & 0.7713 & 0.7773 & 0.8332 & 0.7691 & 0.6861 & 0.7944 & 0.6935 & 0.7162 \\
RMPSS & 0.6676 & 0.5888 & 0.6456 & 0.5812 & 0.8029 & 0.8128 & 0.8514 & 0.755 & 0.6982 & 0.8082 & 0.6669 & 0.7083 \\
Science & 0.5978 & 0.5584 & 0.6019 & 0.5425 & 0.7256 & 0.7691 & 0.792 & 0.7103 & 0.6567 & 0.7325 & 0.5883 & 0.5928 \\
\bottomrule
\end{tabular}
}
\caption{Verbalized and logit-based \metric{} per category. GPT-4 and GPT-4o, the strongest models, struggle with literature and history relative to other categories. Most models are strongest at science. For GPT-4, GPT-4o, and Llama-2-70b, logit-based calibration consistently exhibits higher error; for Mistral and the Llama-3 models, verbalized calibration exhibits higher error.}
\label{tab:calscore-stats}
\end{table*}