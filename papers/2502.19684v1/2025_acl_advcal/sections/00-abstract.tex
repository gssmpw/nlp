Language models are often miscalibrated, leading to confidently incorrect answers.
We introduce \name{}, a benchmark for language model
calibration that incorporates comparison with human calibration.
\name{} consists of question-answer pairs, in which each question contains a series of clues that gradually become easier, all leading to the same answer; models must answer correctly as early as possible as the clues are revealed.
This setting permits granular measurement of model calibration based on how early, accurately, and confidently a model answers.
After collecting these questions, we host live human vs. model competitions to gather 1,749 data points on human and model teams' timing, accuracy, and confidence.
We propose a metric, \metric{}, that uses \name{} to analyze model calibration errors and identify types of model miscalibration that differ from human behavior. We find that although humans are less \emph{accurate} than models, humans are
generally better \emph{calibrated}. Since state-of-the-art models
struggle on \name{}, it effectively evaluates progress on improving model calibration.\footnote{Code and data:  
% \url{https://anonymous.4open.science/r/grace-repo/}}
\url{https://github.com/yysung/advcalibration}}


% Old version
% AI models exhibit issues of lack of calibration, which often manifests as overconfidence in incorrect answers. We introduce \name{}, a human-grounded benchmark for measuring the calibration of language models. \name{} comprises 243 question-answer pairs, each containing a series of four to five adversarial clues that gradually decrease in difficulty.
% %
% This format permits measurement of how early the model can answer correctly and fine-grained measurement of its calibration on correct and incorrect guesses over the course of the question. 
% We validate \name{} through in-person human-model competitions and provide humans' performance in the benchmark as grounding for model evaluation. We find that top human teams are better-calibrated than all tested models (including GPT-4o) in their accuracy and confidence, and that even less-accurate human teams are typically better-calibrated than models. We also propose baseline metrics for \name{} and compare them with existing calibration methods. Finally, we discuss how our dataset uncovers cases of model calibration errors that are absent from human judgments.



% Old version
%AI models exhibit issues of lack of calibration, which often manifests as overconfidence in wrong answers. We introduce a benchmark, AdvCal, for measuring the calibration of language models. AdvCal instances consist of ordered clues of decreasing difficulty with the same answer; models must answer as early as possible with the correct answer. Our dataset is newly crafted by domain expert authors to challenge models. AdvCal's gradation of question difficulty permits fine-grained measurement of how much information a model needs to retrieve the correct answer, and models must be well-calibrated in order to answer at the appropriate time. AdvCal also incorporates information about humans' performance, permitting direct comparison between human and model calibration. We validate AdvCal through in-person model against human experts, benchmarking of existing models, and comparison with existing calibration methods. We also discuss how AdvCal reveals issues of  miscalibration exhibited by models but not by humans.
