% \jbgcomment{This section is either out of place or is not sufficiently motivated.

%   The reader is probably ask: why do we need this and how is it different from existing calibration metrics.

%   I think it also is not sufficiently parallel with the results section.  So I'd suggest reorganizing:

%   \begin{itemize}
%   \item Defining the things like guess and confidence generally
%   \item Computing the conditional accuracy, etc. that are used in the next section
%   \item Providing standard metrics like ECE in this notation (should be really quick)
%     \item \textbf{Then} provide the motivation and what our metric gives you.  
%   \end{itemize}

%   Make sure to use macros both for the metric and for expectation.
% }

\section{Human-Grounded Calibration Evaluation}\label{sec:main-metric}
%The previous sections describe the collection of adversarial questions and \buzzpoints{} from human-AI tournaments. 
To compare model and human calibration, we analyze response correctness and buzz decisions. Then, we introduce a baseline metric, \metric{}. Unlike traditional calibration metrics, \metric{} facilitates per-instance calibration analysis, which lets us identify specific questions on which models are especially miscalibrated. In addition, it factors in human performance on the same question, to penalize cases in which models are confidently incorrect when humans are uncertain (\S~\ref{sec:baseline-metric}).

%The human adjustment serves several purposes: (1) it permits us to identify specific questions on which models perform worse than humans; and (2) it allows our metric to especially penalize cases in which models are confidently incorrect when humans do not yet know the right answer--that is, cases when models are wrong but humans would be more reliant on model outputs.

% \subsection{Comparing model and human calibration}\label{sec:buzz-comparison}
% Using buzzpoints from the competitions, we assess human and model correctness based on when they buzz and whether their guesses are correct, capturing both confidence and accuracy. 
% %
% To further analyze each team's buzzing behavior relative to correctness, we use offline responses to compare how models and humans calibrate their confidence. Specifically, we estimate the probability of buzzing conditioned on the correctness of the top guesses (\S~\ref{sec:analysis-bayes}).

\subsection{Human-grounded metric: \metric{}}
\label{sec:baseline-metric}
%To address the limitations of traditional calibration metrics, which consider only model answers and confidence, w
\metric{} evaluates model calibration error while incorporating human buzzpoints. 
This adjustment reflects the structure of the competition---models must be confidently correct before humans know the answer. The adjustment also places higher weight on instances where model errors are more likely to mislead users---if a model is confidently incorrect when humans are still uncertain, humans are less likely to recognize and override the error (\S~\ref{sec:analysis-metric}).
% This metric simulates the tournament process by incorporating \textit{buzzing} to approximate humans' calibration levels, since humans buzz when they are confident and more humans can buzz correctly as more clues are revealed. 
% We then use it to compare model and human calibration 

Using the live competition data, we track the proportion of humans answering correctly up to a specific clue so that the metric applies higher penalties and rewards for earlier (harder) clues. %For models, we consider their correctness and confidence. 
To measure the expected probability of a team buzzing correctly on a given question, we consider teams' buzzes at each clue \(t\) of question \(q\). We define \( h_t \) as the cumulative probability of a human team correctly buzzing up to clue $t$, calculated as the number of correct buzzes by human teams up to \(t\) divided by total buzzes by human teams up to \(t\). For model responses, \( g_t \) indicates the correctness of a model's guess at clue \(t\) (1 if correct, -1 if not), and \( c_t \) indicates the model's confidence in its guess.

\subsection{Unadjusted model calibration error}
The normalized expectation $r (\E{g_t c_t})$, calculated over clues in a single question, measures calibration as the expectation that the model answers correctly, weighted by confidence (where $r(x)$ renormalizes to a [0, 1] range; see Appendix \ref{app:normalization}). Conversely, $1 - r (\E {g_t c_t})$ evaluates the model's calibration \textit{error} (MCE) on that question. High-confidence incorrect answers and low-confidence correct answers result in higher error, indicating poor calibration on question $q$. %As this value only considers model buzzepoints, we present our metric that incorporates human buzz performance.

\subsection{Human-adjusted calibration error}\label{sec:metric-equation}
%\textbf{\(\bm{Grace_q^{h}}\)}} 
\metric{} incorporates human performance into MCE. This facilitates identification of specific cases where models are less calibrated than humans; rewards models more for being confidently correct before humans know the answer (simulating the competition setting); and penalizes them more for being confidently incorrect at that stage, placing greater weight on a higher-stakes error.

We thus weight the calibration at clue $t$ by $(1-h_t)$, the proportion of humans who have not yet answered correctly by clue $t$. A high score of $r (\E {(1-h_t) g_t c_t})$ indicates that the model is well-calibrated relative to human buzz performance.\footnote{A model is perfectly calibrated when it buzzes with full confidence ($c_t = 1$), is always correct ($g_t = 1$), and answers before humans buzz correctly ($h_t = 0$), resulting in $r (\E{(1 - h_t) g_t c_t}) = 1$.}
 Conversely, we estimate the expected probability for cases where the model does \textit{not} improve over humans, either due to incorrect answers or low confidence:%~\footnote{When humans achieve a perfect buzz (\(h_t=1\) for all $t$), the model's calibration error is 0, reinforcing the assumption that humans outperformed models in calibration performance.}
\begin{equation}
    \metric{}_q = 1 - r (\E{(1 - h_t) g_t c_t}).
\end{equation}
%
This adjustment evaluates the model's calibration error relative to human calibration performance on the same question.
We then define \( \metric{}_D \), the human-adjusted model calibration error for a benchmark \(D\), as the average of \( \metric{}_q \) across all questions.
%
