
\section{\name{}: Dataset Development}\label{sec:dataset}
%In the previous section, we highlighted the use of an incremental
% framework for questions with difficulty granularity and the role of
% adversarialness in evaluating model calibration. Here, we present
% \name{}, detailing its content, creation process, interface design,
% and the skilled authors who crafted its format.  %\jbgcomment{Run
% style checker: ``In this section, we''}
% \jbgcomment{Style checker should also catch ``in order to''}
% %

%
To create our dataset, expert writers and editors first construct incremental,  adversarial, and rigorously quality-checked examples (\S~\ref{sec:question-writing}).
%
Then, we collect model guesses and confidences on these questions (\S~\ref{sec:buzzpoints}), and compare them against human performance in a live competition (\S~\ref{sec:tournament}).
%%\jbgcomment{This section is too mechanical on the \emph{what} motivateeverything by the \emph{why}.  
%It would also be good to have a macro
%  for examples so we know that they're all formatted consistently }
% %\jbgcomment{I think that we need to have macros for things like
%   ``buzz'' and ``buzzpoint''.  Those might not be the right words for
%   an academic paper.}

\subsection{Question writing process}
\label{sec:question-writing}

% %\jbgcomment{Again, I think this is too focused on the \emph{what} rather than being about the \emph{why}.  Here are new topic sentences that might work better:

%   \begin{itemize*}
%   \item To ensure high-quality---accurate, interesting, and challengins---questions, we hired expert writers an authors.
%   \item Because we wanted the questions to be adversarial, authors wrote the question using an interface\dots
%     \item Our focus is on adversarial calibration, so we ask the authors to create questions such that the model does not assign the correct answer a high probability until the penultimate clue (or later).
%     \end{itemize*}

% We also seem to be using ``clue'' and ``sentence'' interchangeably, I don't think that should be the case.
%   }
\paragraph{Collecting QA pairs from expert writers}
%
We recruit experienced question writers and editors to ensure that questions are high-quality.
%
We hire six writers and ten editors to author the questions (qualifications in Appendix \ref{app:writer-qualifications}).
%
The questions contain 575--650 words\footnote{We refer to these as \textit{questions} although they are not grammatical questions, but rather sequences of sentences with clues uniquely identifying an answer (examples in Figures  \ref{fig:sample_interface} and \ref{fig:qb_vangogh}).} and cover content across six
categories.\footnote{Science, history, literature, and other (Appendix~\ref{app:dataset-design}).}
%
All questions are reviewed by the writer, category editor, and
head editor to check that clues are unambiguous and factually correct.
%
%Writers use a real-time interface to help them craft and edit
%incremental, adversarial questions. % redundant w next line

\paragraph{Interface setup}
%
To create incremental and adversarial questions, writers and editors
use a human-AI collaborative writing interface
(Figure~\ref{fig:sample_interface}).
%
Because these examples are meant to be incremental, we break the input
into sentences $\{s_1, s_2,\dots s_k\}$.
\abr{gpt}-3.5 provides a guess
%(e.g., when the guess confidence exceeded a
%threshold value.) 
$\{a_1, a_2,\dots a_k\}$ for
each sentence in addition to its confidence
$\{c_1, c_2\dots c_k\}$.
%
The interface also highlights when the model confidence would be high enough to buzz 
(Appendix~\ref{logistic}).

To ensure that questions remain incremental for models, we instruct
writers to write questions for which the model's guess is incorrect until the penultimate sentence or later.\footnote{The model’s confidence for \emph{correct} answers should remain low for all but the last two sentences; clues that trigger high-confidence, \emph{incorrect} model guesses are encouraged.}
%
As experienced question writers, they use their domain knowledge to ensure difficulty also decreases for humans, so that most humans can
answer correctly by the end.
%
% Inspired by
% \citet{wallace-etal-2019-universal, sung2024your},
% \jbgcomment{Better to avoid self cites when we can, perhaps this is a good place for the you-lowd cite?}
Writers
dynamically interact with the models to refine their questions
~\citep{you-lowd-2022-towards}.
%
For example, the second line in Figure~\ref{fig:sample_interface}
was originally ``This number of characters appear in the name of a
Chinese classic,'' which the model answered correctly. Instead, the
editor revises the first line of the text, which
fools the models while allowing humans to answer correctly.
% \jbgcomment{Add the number of sentences; perhaps save the number of
%   packets until we describe what the games are or at least forward point there }
The final dataset consists of 243 QA pairs, with a total of 1,236 sentences of clues. Each sentence uniquely points to the answer, making it usable as a standalone QA pair.\footnote{For purposes of our analysis, a \textit{clue} is a substring of the full question, averaging 13 words or 35 characters, incrementally extending from the beginning. Clues are split at whitespace boundaries and may contain multiple pieces of information about the answer.}
%Each match was played on one set of 20 questions.


% \efcomment{Worth checking back later if there are additional stats we want here. Also need to add figure with the writing interface.

% After figure is added, add sample edit process; if it was too hard for humans so the editors edited it or it was adversarial until the end or the model had gotten the answer correct too quickly}
% \yycomment{YY:Add Figure after Eve's selection of question that would be ideal to go into the interface figure}
% (added)


\subsection{Collecting human--model buzzpoints}
\label{sec:buzzpoints}

The questions described above are designed to be read aloud and interrupted. In the competition, teams compete by buzzing to interrupt and answer, with this timing referred to as \buzzpoints{} \faBell~(Figure~\ref{fig:qb_vangogh}).
%
However, modern \abr{llm}s do not operate this way: they generate an output given an input.
%
Thus, we first extract guesses from models and humans \textit{offline} to assess teams on the same questions (\S \ref{sec:offline}). We then compute the model buzzpoints for each clue. Finally, using these precomputed model buzzpoints, we host live human–computer competitions to collect real-time human buzzpoints (\S \ref{sec:tournament}). 
%We hold human-model competitions to
% collect human and model answers and their confidence scores on each question, to measure calibration comparison on the same task.
% Questions in \name{} follow the incremental QA format: they consist of
% multiple clues which independently and unambiguously point
% to the same answer. 
%

\begin{figure}[t!]
    \centering
    \begin{minipage}{0.48\textwidth}
    \hrule\vspace{5pt}
    \qbquestion{In a reference to an object notably missing from one of these
    works, Diemut Strebe used genetic samples from a man's
    great-great-grandnephew to clone a certain feature. In one of these
    works, Utagawa Togokuni's Geishas in a Landscape
    \roundbox{red}{GPT-4o: ``Rodin statues''} hangs on a yellow wall
    behind a man in a fur-brimmed hat. The backside of The Potato Peeler
    includes one of these works featuring a man in a straw hat. One work
    shows a man in a light-blue green suit against a light-green-blue
    swirling background, and another dedicated to Gauguin shows subject
    with cropped hair and a red beard. For 10 points, a Dutch artist
    \textbf{\roundbox{ForestGreen}{H1: ``Van Gogh self-portraits''}}
    painted what portraits of himself with a bandaged ear?}
    {\underline{self-portraits of Vincent Van Gogh}}
    \vspace{5pt}\hrule
    \end{minipage}
    \caption{While GPT-4o buzzes too early with an \roundbox{red}{incorrect answer}, losing 5 points, the human team (H1) buzzes later with a \roundbox{ForestGreen}{correct answer}, earning 10 points. Both teams must balance accuracy and speed; here, \texttt{GPT-4o} shows poorer calibration than \texttt{H1}.}
    \label{fig:qb_vangogh}
\end{figure}

\subsubsection{Offline human and model buzzpoints}\label{sec:offline}
\textbf{Model guesses and confidence.}
% \jbgcomment{Would be
%   good to have full run generation algorithm in the appendix.}}
%
We first break each question into clues.
We then retrieve a model's guess given the first $n$ clues with a prompt using a \abr{tf-idf} retriever to select
similar question-answer pairs from \abr{qa} datasets (Appendix~\ref{sec:guesser}). 
To determine if model guesses were correct, our post-processing uses both transformer-based answer equivalence (PEDANT,~\citealp{li-etal-2024-pedants}) and manual verification by dataset editors.
%
We store the resulting guesses and two forms of confidence from LLMs,
token logits\footnote{\name{} answers are short, typically 3-4 words long, making token logits a reliable measure of confidence.} and verbalized confidence.
Logit-based confidence reflects the average of the exponentials of the
token logit probabilities, while verbalized confidence prompts models to directly express confidence in the output tokens
(Appendix~\ref{sec:confidence_elicitation}). 

% \jbgcomment{Perhaps say a little more about answer correctness, you can cite Zongxia's paper}

\textbf{Precomputed model \buzzpoints{}.}
%
While our metric, \metric{} (\S~\ref{sec:baseline-metric}), uses the raw confidence values from continuous probabilities, our
human--computer competitions (\S~\ref{sec:tournament}) require binarized confidence to indicate when the model buzzes.
%\footnote{Buzz positions are reviewed for plausibility, while \metric{} does not rely on the binarized \buzz{} positions.}
%
For each model, we set a threshold based on human gameplay data on
preexisting non-adversarial questions~\citep{pmlr-v48-he16} (Appendix~\ref{app:buzzpoint-threshold}).
%
This threshold is chosen to maximize the probability of the model
buzzing correctly before the average trivia player as estimated by the
\textit{expected wins} metric from \citet{rodriguez2019quizbowl}.
When the logit score exceeds the threshold, the model buzzes
in, marking its buzzpoint (Appendix \ref{app:buzzpoint-algo}).

\textbf{Offline human guesses.}
\label{sec:survey}
To compute humans' raw accuracy, independent of confidence (\S~\ref{sec:analysis-bayes}),
we survey fifteen players on 35–40 held-out \name{} questions.
%
Like the models, players view clues, submit their guess after each clue, and indicate whether they would \buzz{} at that point.
%
However, this data collection format is time-consuming and tedious (one player called it ``remarkably hard''), potentially reducing player engagement and response quality. Instead, we collect human calibration data through a fast-paced trivia tournament.
%% \paragraph{Model \buzzpoints{}.} \label{sec:buzzer}
%% %

%% Model buzzpoints are determined before the matches. First, we prompt the model to retrieve its top guess as clues are revealed, along with its confidence score.\footnote{} The prompt uses a pretrained TF-IDF retriever to. Answer correctness was pre-evaluated through post-processing and manual verification by dataset editors.

%
%%\jbgcomment{It also seems odd that the survey isn't here.  This makes it jarring when it's presented later.  I would talk about it in terms of ``text runs'' first (for both models and humans) and then transition to the ``oral'' version.  You can make the case that this is much more natural and efficient for humans and allows us to collect richer word-by-word data. }
%
% Figure~\ref{fig:qb_vangogh} shows real \buzzpoints{} from two teams on a question. While GPT-4o buzzes too early with an \roundbox{red}{incorrect answer}, the human team (H1) buzzes later with a \roundbox{ForestGreen}{correct answer}. \texttt{GPT-4o} demonstrates worse calibration than \texttt{H1}. 

\subsubsection{Human buzzpoints, \textit{live} competition}
\label{sec:tournament}

\name{} records human and computer guess correctness
on interruptible questions designed to challenge model calibration.
%
A human moderator reads each question to both teams (a model and a team
of humans).
%
Teams compete by buzzing to interrupt and answer.
%
Model \buzzpoints{} are computed in advance (\S~\ref{sec:offline}). When the model's confidence exceeds the threshold, the reading stops with a buzz sound, and the model's guess is announced. 


\textbf{Human \buzzpoints{}.} 
%Human \buzzpoints{} are recorded during the matches and are provided as part of our dataset along with the model \buzzpoints{}. 
% In these live competitions, a human moderator reads the question aloud
% to human team. 
In contrast, human buzzpoints are recorded in real time when the moderator is interrupted.
%
Players press a physical
buzzer when confident in an answer, and the moderator verifies if the answer is correct.
%
We log the timing of human teams'
buzzes and answer correctness.

If a team answers incorrectly, the moderator continues reading until the other team buzzes in.
%
Because earlier clues are harder, more skilled teams tend to buzz
earlier, while less skilled teams wait until near the end. Thus, teams must
be knowledgeable \emph{and} well-calibrated to buzz optimally.

\textbf{Human players in live competitions.}
Our three competitions consist of a total of 93 matches involving 17 human trivia teams and three LLMs (GPT-4o, GPT-4, and Mistral-7b-Instruct). Of these, 55 are human vs. model matches, while 38 are human vs. human matches. For the matches, the 243 QA pairs are divided into 12 sets of 20,
stratified by category, with three questions for tiebreakers.
%(two in person and one online)

Hosting real-time competitions with human players provides several benefits: (1) direct comparison of confidence calibration between humans and models on the same questions, (2) recruiting experienced players skilled in calibrating their answers,\footnote{After the competitions, we survey players about their experience levels and individual strengths (Appendix~\ref{app:player_specifics}).} and (3) validating that questions are human-answerable and unambiguous, as an additional quality check for the dataset. 


%


%\footnote{Model confidence by model family: 0.003 for GPT models, -0.05 for Mistral models, and 0.003 for Llama models.}


% \jbgcomment{I think we need to say something like (perhaps as a footnote?):

%   While not the goal of our research, for each game we compute a score
%   for each team (ten points for correct answers, minus five points for
%   incorrect interruptions).
%   %
%   The team with the highest score at the end of the match---a set of twenty questions---wins.
%   %
%   Our strongest human team won X of its Y matches, and only lost to Z.
%   %
%   The strongest model was A and won B of its C matches.
%   %
%   Full results available after the anon deadline.

% }
