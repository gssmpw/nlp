\section{Model Calibration Evaluation}\label{sec:analysis}
\name{} helps to evaluate differences between human and model calibration (\S \ref{sec:analysis-buzz_compare}). We also validate the dataset's difficulty granularity and discuss calibration errors using our proposed metric (\S \ref{sec:analysis-metric}) and qualitative analysis (\S \ref{sec:qual_analysis}). %The inherent characteristics of the benchmark, such as its ability to track accuracy progression as clues are revealed, can guide improvements in model training and design, ensuring that models not only make accurate predictions but also convey their confidence with appropriate granularity.
%
% \jbgcomment{Reorganize to be symmetric to metrics section.  Focus more on the results rather than the methodology.  In other words, the topic sentence should usually be the result and then back it up.}
%
\subsection{Comparing human and model calibration}\label{sec:analysis-buzz_compare}
\textbf{Buzz performance.}
\begin{figure}[!t] 
    \centering
    \includegraphics[width=\linewidth]{figures/normalized_cumulative_buzz_per_run.pdf}
    \caption{Each team's cumulative buzzes (normalized by the number of matches each team participated in). The top quartile of human teams (Q4) achieves the highest cumulative correct buzz rate, peaking over twice as high as the best model. Top human teams are thus more accurate and better-calibrated than models, even as the difficulty changes when more clues are revealed.}
    \label{fig:buzz_correctness_per_run}
\end{figure}
To compare human and model calibration, we first examine when and whether each team buzzes on the question, as well as the correctness of their answers. Figure~\ref{fig:buzz_correctness_per_run} gives each team's cumulative buzzes over the number of matches each team participated in. The 17 human teams are divided into quartiles, from Q1 (bottom) to Q4 (top), according to their total correct buzzes. Human teams, especially the top quartile, achieve the highest cumulative correct buzz rate (peaking in the middle of the questions), demonstrating their ability to confidently infer correct answers with fewer clues and indicating better accuracy and calibration than models. In contrast, GPT-4 exhibits a moderate cumulative correct buzz rate, which is only briefly higher than the top human teams and lower than 50\% of human teams for most of the question. Meanwhile, Mistral-7b-Instruct lags significantly behind all other teams, indicating poor calibration. In addition, GPT-4 and GPT-4o exhibit substantially higher incorrect buzz rates than human teams (right plot). All models, especially GPT-4, are overconfident early in the questions when little information is available: they are \textbf{especially miscalibrated relative to humans when the question is still hard}. Overall, the models tend to buzz incorrectly more often than humans and correctly less often, indicating \textbf{overconfidence in wrong answers and underconfidence in correct ones}.

% To collect model buzzpoints, we manually preassigned confidence thresholds to determine when each model would buzz during the tournaments (\S~\ref{sec:buzzer}). We also analyzed how varying these thresholds impacts model accuracy, which directly reflects each model's calibration level. As shown in Figure~\ref{fig:accuracy_rates_bins}, humans (blue) consistently achieve high accuracy rates across all confidence bins, with accuracy increasing slightly as confidence thresholds rise. This indicates that humans are generally well-calibrated, as higher confidence reliably corresponds to greater correctness. In contrast, models (red) exhibit much lower accuracy rates compared to humans across all confidence bins. Even in the highest confidence range (0.96â€“1.00), model accuracy fails to match human performance. Moreover, models show poor calibration, particularly in higher confidence bins, where their confidence often exceeds their actual correctness. These findings highlight the importance of improving model calibration to ensure that confidence more accurately reflects correctness.

\textbf{Difficulty granularity of each question.}\label{sec:analysis-adversarial}
\begin{figure}[!t] 
    \centering
    \includegraphics[width=\linewidth]{figures/accuracy_plot_.pdf}
    \caption{Comparison of human and model average accuracy rates as more clues are revealed (whether the team's guess is correct after seeing the first $n$ clues). As more clues are revealed, accuracy improves for both models and humans. Models often answer incorrectly until most clues are provided, and human accuracy increases more rapidly, validating that each instance becomes easier for both humans and models and that most humans can answer correctly by the end.}
    \label{fig:accuracy_rate}
\end{figure}
To evaluate model calibration over a range of difficulty levels for models, we asked the writers to write questions that are easier to answer as more clues are revealed (\S~\ref{sec:dataset}). To validate this design, 
we examine model and human correctness as the percent of clues revealed increases. For models, we consider the correctness of a model's guess for the first $n$ clues. The questions in \name{} are appropriately challenging and become easier for models and humans as more clues are revealed (Figure~\ref{fig:accuracy_rate}). Human team accuracy (blue) increases steadily, indicating that question difficulty indeed decreases as clues are revealed for human players. %
%
%Accuracy improves across all models as more clues are revealed, confirming that difficulty decreases over time. 
Moreover, even top models like GPT-4 and GPT-4o have under 50\% accuracy until at least 90\% of clues are provided, highlighting significant room for improvement on this benchmark.
%
%\footnote{Quartiles are those from Figure~\ref{fig:buzz_correctness_per_run}.} 
To measure human accuracy per corresponding clue, we used offline human responses (\S \ref{sec:survey}; quartiles calculated per Appendix \ref{app:survey-quartiles}).

Notably, the bottom three quartiles of humans are less accurate than top models for most of the question (Figure \ref{fig:accuracy_rate}), yet still typically outperform models on maximizing correct buzzes relative to incorrect buzzes (Figure \ref{fig:buzz_correctness_per_run}). This trend suggests that \textbf{models' relatively high rate of incorrect buzzes and low rate of correct buzzes is due to miscalibration, not inaccuracy.} We investigate this distinction further in the next section.

% \yycomment{plot: Accuracy rate for humans}
% \efcomment{The rate of correct buzzes for humans over the course of the question is also useful evidence here.}

\textbf{Conditional likelihood of correct answers.}
\label{sec:analysis-bayes}
\begin{figure}[!t] \centering\includegraphics[width=\linewidth]{figures/p_buzz_correct_halves.png}
    \caption{%Estimated probability of buzzing in, conditioned on having the correct or incorrect answer, for all models and quartiles of human teams. 
    Humans are far more likely than models to buzz in when they are correct (left), and typically less likely to buzz in when they are incorrect (right), indicating that models remain miscalibrated relative to humans even when explicitly controlling for accuracy. (Due to the smaller sample size of human buzzpoints in the survey data, we use halves instead of quartiles here.)}
    \label{fig:bayes}
    % \yycomment{change y axis}
\end{figure}
% To further analyze calibration independent of accuracy, we estimated the models' and human teams' probability of buzzing in, conditioned on the team's top guess being the correct answer, $P(buzz|correct)$; as well as the probability of buzzing in, conditioned on the team's top guess being an incorrect answer, $P(buzz|incorrect)$. This measurement allows us to estimate how models and humans compare at being more confident on correct answers than on incorrect answers, while controlling for likelihood of being correct.
% \jbgcomment{See preamble for how to write conditional probabilities, and 
%\jbgcomment{don't but real words in math font}
%\yycomment{define c and b, and change correct to c and buzz to b }
While the tournament allows us to observe $P(g=1 \g b)$, the likelihood that a team's guess is correct ($g=1$) when they buzz ($b$), we also aimed to compare how often teams are confident enough to buzz when correct, $P(b \g g=1)$, and when incorrect, $P(b \g g=0)$. Using the offline human responses (\S \ref{sec:offline}), we estimate $P(b \g g=1)$ for each human team. For each player, we calculate $P_{player}(b \g g=1)$: the number of instances when a player buzzed in correctly with $n$ clues revealed, divided by the number of instances when a player's guess was correct with $n$ clues revealed. We then estimate $P(b \g g=1)$ for the top and bottom half of respondents (Appendix \ref{app:survey-quartiles}) as the average of $P_{player}(b \g g=1)$ across all surveyed players in that half. Finally, we compare this estimation with $P(g=1 \g b)$ for the tested models. We follow the same process to estimate $P(b \g g=0)$. \footnote{For all estimates, we consider only the guess correctness and buzz statistics up to the point when a player first buzzes, as later guesses do not count in real competitions.}
 %
The results indicate that even \textbf{the strongest models are less confident than humans on correct answers and more confident on incorrect ones} (Figure \ref{fig:bayes}). For most questions, all humans are more than 50\% likely to buzz when correct, while models remain below 45\%, indicating lower confidence in correct answers. Among the models, GPT-4 was most likely to buzz incorrectly, reflecting its confidence in wrong answers.

% Furthermore, models and humans exhibit vastly different trends in confidence as more clues are revealed. Humans become more likely to buzz in when they know the correct answer as more clues are revealed. By contrast, models become slightly less likely to buzz when their top guess is correct as more clues are revealed. This suggests that while additional information strengthens human confidence, models do not profit from additional information to the same extent. Lower-performing human teams become somewhat more likely to buzz in when their top guess is incorrect toward the end of a question, but higher-performing teams do not display this trend.
As clues are revealed, humans become more likely to buzz when they know the correct answer, while models become less likely to buzz. This suggests that seeing more clues strengthens human confidence, but not model confidence.\footnote{A small fraction of human participants ($n$=1) has a sharp spike in incorrect buzzes near the end of a question.}

% \begin{figure}[!t] 
%     \centering
%     \includegraphics[width=\linewidth]{2025_acl_advcal/figures/buzz_metric_bar.pdf}
%     \caption{Comparison of the $SH_q$ metric on our dataset with previous metrics (expected calibration error and Brier score). $SH_q$ helps to make differences in calibration performance more visible, relative to previous metrics.}
%     \label{fig:buzz_metric}
% \end{figure}





% \begin{table}[ht]
% \centering
% \small 
% \setlength{\tabcolsep}{6pt}
% \begin{tabular}{@{}lccc@{}}
% \toprule
% \textbf{Metric} & \textbf{M1: GPT4} & \textbf{M2: GPT4o} & \textbf{M3: Mistral} \\ 
% \midrule
% H vs M Rounds & 18 & 17 & 20 \\ 
% H Wins         & 9  & 4  & 17 \\ 
% Win Rate   & 0.50 & 0.24 & 0.85 \\ 
% \bottomrule
% \end{tabular}
% \caption{Calibration performance across models}

% \jbgcomment{This seems to be based on rounds, but that's artificial.  Why not do it on a per-example basis?}

% \label{tab:cal_performance}
% \end{table}


%\efcomment{todo}\jbgcomment{Did I miss the verbalized confidence?  How is it extracted, what happens if there's a parsing error.Where are the results discussed?  Why are the verbalized uniformly worse, what do they look like (e.g., are they uniformly higher). It might be good to have a little more description of what the confidences look like.}
