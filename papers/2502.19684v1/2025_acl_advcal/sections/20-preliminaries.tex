\section{Preliminaries}\label{sec:compare}

% Preliminaries: Comparison with existing metrics and pyramidal QA


% https://arxiv.org/pdf/2405.21028
% Calibrate expressions of uncertainty/natural language expressions of confidence

% https://arxiv.org/pdf/2305.14975
% Just ask models for verbalized confidences: they're better calibrated than conditional probabilities

% https://arxiv.org/pdf/2403.05973
% Model calibration based on outputs only

% https://arxiv.org/pdf/2306.13063
% White box methods do better though likely because verbalized overconfidences are...overconfident

%https://arxiv.org/pdf/2210.01964
% Reduce the generalization gap to improve calibration

% https://arxiv.org/html/2402.06544
% Calibration for long-form responses

%https://arxiv.org/abs/2404.00474
% Calibration of long-form responses by having LLMs explicitly state confidence

% https://aclanthology.org/2021.emnlp-main.696/
% Synthetic adversarial questions

% https://arxiv.org/abs/1904.04792
% Incremental questions

% Our dataset builds on previous work regarding the importance of language model calibration. 
%Some QA datasets use incremental frameworks designed to be progressively challenging, while others follow adversarial frameworks, making questions difficult for models but easy for humans. We use these concepts to create a benchmark for model calibration evaluation. 
%\efcomment{kind of awkward intro}
Drawing on prior work, \name{} consists of incremental questions with adversarial clues to effectively ground model calibration evaluation in human performance.
%
%In the next section, we explain how our dataset was developed under model-human competition setting to capture discrepancies between human and model calibration.
%This section outlines the literature to discuss the dataset development pipeline in later sections.
%In addition, \citet{carrell2022calibrationgeneralizationgap} find that models that generalize better to out-of-distribution data are more calibrated and \citet{huang2024calibratinglongformgenerationslarge} consider calibration in long-form text generation settings.

% \jbgcomment{I think this section could be stronger by:
%   \begin{itemize}
%   \item Differentiate with prior work
%     \begin{itemize}
%             \item DADC doesn't do things incrementally, didn't have confidence
%     \item Wallace didn't show incorrect answers / confidence, which made it impossible to write questions that challenge calibration (just accuracy)
%       \item Wallace is trivially easy for today's models
%     \end{itemize}
%   %\item While Rodriguez argues for the format, their evaluation focuses only on accuracy: abstention is binary.  This is not how models work.  (Can forward point to our survey questions, contrast that with Rodriguez human data collection.)
%     \item This is how human trivia players learn calibration (hopefully Even knows some cites; prisoner of trebekistan comes to mind)
%     \end{itemize}

% }

\paragraph{Incremental and adversarial QA}
%
Incremental questions contain multiple clues in decreasing order of 
difficulty; models must answer correctly as early as
possible.
%
\citet{boyd-graber-etal-2012-besting} and \citet{he-16} argue that
this setting is a natural test of calibration:
participants should buzz only when confident in their answers % buzz was not defined?
~\citep{ferrucci2012introduction}. Unlike
selective \abr{qa} for non-incremental
examples~\citep{kamath-etal-2020-selective}, incremental decision processes offer more detail, since each clue is a decision point for whether to answer or abstain, with 
more information available as clues are revealed.

In addition, unlike prior incremental \abr{qa} research, we use a human-in-the-loop adversarial authoring process to specifically
target calibration.
%
\citet{quizbowl2019} uses publicly available questions, which are too easy for modern models.
%
While \citet{wallace2019trick} used model--human collaboration for
incremental adversarial data, their questions are also insufficiently adversarial.\footnote{ GPT-4 has 80\% accuracy on \citet{wallace2019trick}'s TrickMe dataset after only 60\% of the clues (Appendix~\ref{app:trickme-comparison}). Model accuracy remains 2-4x higher on TrickMe than on \name{} as clues are revealed.} In addition, they did not account for model
confidence, treating buzzing as a binary outcome.
%

To develop questions that challenge models, expert writers use our interface (\S~\ref{sec:question-writing}), followed by expert editing to ensure well-posed
questions.
%
% \jbgcomment{Too much of a cite dump, break apart perhaps by the individual facets each of these approaches target.}
Our dataset creation process is motivated by \citet{kiela-etal-2021-dynabench,
  ilyas2019adversarial, engstrom2019adversarial}, who argue
% add , sung2024your when CR
that adversarial benchmarks must be clear for humans and challenge
models, ensuring that model errors are due to model limitations rather than ambiguous or low-quality questions~\citep{min2020ambigqa, yu-etal-2023-crepe}.
%
% We thus recruit experienced question authors and
% provide them with our interface to write questions with more
% clues. These questions enable granular calibration measurement and
% undergo expert-supervised quality control.

\textbf{Grounding to human calibration.}
%
Humans increasingly use \abr{ai} to help make decisions, but such assistance can be detrimental when the model is
miscalibrated~\citep{StengelEskin2024LACIELF} or fails to
abstain~\citep{khurana2024crowdcalibratorannotatordisagreementinform}.
This is particularly concerning when models are confidently wrong but humans do not know the correct answer, which our metric, \metric{}, especially penalizes.
%
Furthermore, modern models are poorly
calibrated to human linguistic variation, causing
\citet{ilia2024predict} to question the
reliability of expected calibration error (ECE).
%
Thus, \metric{} focuses on where models can help users by
considering how early \textit{humans} can answer the
questions.
%
\input{2025_acl_advcal/sections/interface_sample}

\textbf{Calibration evaluation.}
%
Language models tend to be overconfident in their predictions, which
can lead to undue trust or erode user confidence in language models
\citep{zhou-etal-2024-relying}.
%add cite back in in CR: feng-19,
%
Proposed methods to measure model calibration include using raw
probabilities \cite{xiong2024llmsexpressuncertaintyempirical}, separate confidence
predictors \cite{ulmer-etal-2024-calibrating}, verbalized confidence
scores \cite{tian-etal-2023-just,
  band2024linguisticcalibrationlongformgenerations}, or natural
language expressions of uncertainty \cite{StengelEskin2024LACIELF,
  zhou-etal-2023-navigating}.
%
%These methods depend on model outputs from preexisting datasets not
% designed for calibration.
% \jbgcomment{It's not clear why this point is important: does that make the dataset less efficient?}
%
Our dataset aids finer-grained versions of these approaches by permitting per-instance, human-grounded
calibration measurement.
We also extend on existing calibration metrics, such as
ECE~\citep{naeini2015obtaining} and Brier scores
\cite{brier1950verification} by introducing a metric for calibration
on incremental questions (Appendix~\ref{app:ece-brier}).

% Our dataset expands on this work by incorporating newly crafted questions designed to be adversarial, to challenge models while remaining answerable by humans~

%Furthermore, incremental questions mean we get detailed information about how well models do (not just "can it answer the question right", but "how much info does it need to answer the question right?"). Our analyses indicate room for growth and strong separation between the models. Also, we have expert writers and brand-new questions.

%To summarize:
%- There are other adversarial datasets but they're not incremental (so no calibration/abstention measurement) and aren't grounded in human performance.
%- There are other incremental datasets but they're not adversarial (so too easy) or the questions are shorter.

% This \textit{buzzpoint} data allows us to ground measurement of model calibration to human performance on the same task. We also analyze differences in calibration errors between models and human respondents.



% \yycomment{References for pyramidal questions:  
% 1. https://arxiv.org/abs/1910.14464
% 2. Trick Me If You Can: Human-in-the-Loop Generation of
% Adversarial Examples for Question Answering
% 3. https://aclanthology.org/2024.emnlp-main.1140/}


% Added by Hope (I am taking the uncertainty class this semester and there are some papers might be interesting to cite or mention. I just put them here but feel free to ignore if they don't make sense!) 
% Credit: https://fumeng-yang.github.io/CMSC839E
% (1) Uncertainty in Natural Language Generation: From Theory to Applications
% https://arxiv.org/pdf/2307.15703
% (2) Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation
% https://arxiv.org/pdf/2302.09664
% (3) Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models
% https://arxiv.org/pdf/2305.19187
% (4) Teaching models to express their uncertainty in words
% https://arxiv.org/pdf/2205.14334

