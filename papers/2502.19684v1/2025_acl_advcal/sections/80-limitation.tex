\section*{Limitations}
Since \name{} focuses on a question-answering task, its applicability to broader NLP domains remains unexplored. Future work should explore calibration in other open-ended generation settings to assess generalizability. Furthermore, while our proposed metric, \metric{}, serves as a baseline, it is not exhaustive of all forms of uncertainty and miscalibration. For instance, enhancing this metric for human-AI collaboration could help users determine when to rely on models and when to defer to human judgment.

\section*{Acknowledgments}

Many thanks to the question editors who helped to create the dataset: Ankit Aggarwal, Jaimie Carlson, Bradley Kirksey, Linus Luu, Shahar Schwartz, Noah Sheidlower, and Jonathan Tran. Special thanks to the writers who assisted as well. This research was only possible due to the generous participation of the players who competed in the tournaments (special thanks to Joy An, Andrew Gao, Kevin Yu, Alex Stonemanxa, Priyanka Raghavan, and Magdalena Lederbauer) and the moderators who read the questions. We thank Lakshya Agrawal, Nishant Balepur,  Nicholas Tomlin, Swabha Swayamdipta, and Irene Ying for providing feedback on earlier drafts.

This research was partly supported by an NSF GRFP grant.
