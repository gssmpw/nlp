
\glsresetall

\section{Introduction}

\IEEEPARstart{O}{bject} detection and tracking is an essential part of the situational awareness system for an autonomous vehicle. For example, as an autonomous ferry navigates its environment, other vessels, such as kayaks and boats, can come into collision course with the ferry. Their paths will then need to be predicted for the ferry to move accordingly to avoid collision and follow the accepted movement patterns at sea. These near-shore relatively smaller vessels should therefore be detected and tracked. 

\Gls{lidar} is one of the most common sensors for autonomous vehicles, especially for autonomous ferries. This is likely due to its accurate and precise measurements of the environment. However, cameras often have denser measurements with rich information, while the field of view is often smaller, and the data is more complex to utilize. For example, the image can be used to detect vessels at relatively far distances and the \gls{lidar} can be used to get accurate position estimates for tracking the vessel near shore~\cite{Hilmarsen2025}. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/intro_land_masking_with_map.jpg}
    \caption{Overlain in cyan is an example of land masking. Overlain in orange is a visual and \gls{lidar}-based map, details are in \cref{sec:mapping}. The orange map is more detailed, without potentially moving objects, and gives earlier target detections than the cyan map. A photo of a relevant docking area is in the background. Base image courtesy of Google Maps: Imagery @2024 Airbus, CNES / Airbus, Maxar Technologies, Map data @2024. }
    \label{fig:intro_land_masking}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/intro_kayak_near_dock_with_markings.jpg}
    \caption{The kayak is undocking and moving into the canal. The approximate path it takes is marked in blue, and the approximate path for the day cruiser is marked in orange. A precise map enables earlier tracking of the kayak compared to an imprecise map, where the kayak can be differentiated from the dock. }
    \label{fig:intro_kayak_near_dock}
\end{figure}

As can be seen in \cref{tab:related_work_topics}, many methods use \gls{lidar} for \gls{mos} or \gls{datmo}. However, we suggest distinguishing between static and potentially dynamic objects during mapping. A mapping method that only does \gls{mos}, or \gls{datmo}, maps objects that are static during the mapping but are potentially moving objects, such as docked vessels. This is especially relevant in harbor areas, where a lot of vessels are docked, and a lot of traffic occurs. The map is then wrong if the docked vessel moves. Even worse, an autonomous ferry is approaching the dock and it does not detect that an earlier static vessel now moves into a potential collision area since the target vessel has not moved sufficiently from its docking position. 

Target tracking in the maritime domain typically employs land masking, e.g., see \cref{fig:intro_land_masking}. Land masking is where the areas on land and a bit into the water are marked. This is so that the land, and sometimes also static vessels, can be filtered out of the sensor data, resulting in only data from vessels that one would like to track. This has been done for tracking in \gls{radar} data in \cite{Wilthil2017, Wilthil2018Track} and in \gls{lidar} data in \cite{Helgesen2022}. However, it is often not precise enough to also detect vessels that are close to the quay. For example, kayaks are relatively nimble, meaning that a kayak might suddenly get into a collision path with the ferry when the ferry itself is close to the quay, see \cref{fig:intro_kayak_near_dock}. To avoid colliding with these kayaks, it is important to detect and track them as early as possible. 

To detect objects early, we suggest using a precise map that makes it easier to find what measurements do not correspond to static land and should be tracked for collision avoidance. In this paper, we use the appearance of the objects in images to evaluate this, with the use of a deep learning-based instance segmentation technique. 
The contributions of this paper are as follows:
\begin{itemize}
    \item We present a method for maritime \gls{lidar} and camera mapping that does not map potentially moving objects.
    \item We demonstrate the improved performance of close-to-shore target tracking with the use of a precise map, where targets are detected early, compared to less precise maps. 
    \item The method is demonstrated on real-world data. 
\end{itemize}






\section{Related work}

\begin{table*}[hbtp]
    \caption{This table shows the different topics for related work. ``Free space'' means the method explicitly estimates free space. With ``ENC'' we include the use of OpenStreetMap in the automotive domain. }
    \label{tab:related_work_topics}
    \centering
    \resizebox{0.99\linewidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
         & Camera & LiDAR & RADAR & SLAM & \makecell{MOS / \\DATMO} & Tracking & \makecell{Free \\ space} & \makecell{Deep \\ learning} & \makecell{Change \\ detection} & ENC & \makecell{Maritime \\ domain}\\
         \hline 
        \cite{Chen2022a}            &   & X &   & X & X & X & X &   &   &   &   \\
        ERASOR~\cite{Lim2021}       &   & X &   & X & X & X & X &   &   &   &   \\
        Removert~\cite{Kim2020}     &   & X &   & X & X & X &   &   &   &   &   \\
        OctoMaps~\cite{Arora2021}   &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Pfreundschuh2021}     &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Jinno2019}            &   & X &   &   &   &   & X &   & X &   &   \\
        \cite{Ding2018}             &   & X &   & X &   &   & X &   & X &   &   \\
        \cite{Pomerleau2014}        &   & X &   & X & X & X & X &   & X &   &   \\
        \cite{Mersch2023}           &   & X &   & X & X & X &   & X &   &   &   \\
        \cite{Hosseinyalamdary2015} &   & X &   &   &   & X & X &   &   & X &   \\
        \cite{Wang2003}             &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Miyasaka2009}         &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Thompson2019}         &   & X &   &   &   &   &   &   &   &   & X \\
        \cite{Xie2023}              &   & X &   &   & X &   &   &   &   &   &   \\
        \cite{Chen2021a}            &   & X &   &   & X &   &   &   &   &   &   \\
        \cite{Postica2016}          & X & X &   &   & X &   & X &   &   &   &   \\
        \cite{Yan2014}              & X & X &   &   & X & X & X &   &   &   &   \\
        OGM-HMM~\cite{Sun2024}      &   &   & X &   & X & X &   &   &   & X & X \\
        \cite{Dewan2016a}           & X &   &   &   & X & X &   &   &   &   &   \\
        \cite{Dewan2016}            & X &   &   &   & X & X &   &   &   &   &   \\
        \cite{Lenz2011}             & X &   &   &   & X & X &   &   &   &   &   \\
        \cite{Lee2010}              &   & X &   &   & X & X &   &   &   &   & X \\
        \cite{Nuss2018}             &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Negre2014}            &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Tanzmeister2014}      &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Schreiber2021}        &   & X &   & X & X & X & X &   &   &   &   \\
        \cite{Vatavu2020}           & X & X &   & X & X & X & X &   &   &   &   \\
        \cite{Hilmarsen2025}            & X & X &   &   & X & X &   & X &   &   & X \\
        \cite{Bibby2010}            &   &   & X & X & X & X & X &   &   & X & X \\
        \cite{Vu2011}               &   & X & X & X & X & X & X &   &   &   &   \\
        \cite{Gies2018}             & X & X & X & X & X & X & X &   &   & X &   \\
        \cite{Pieper2024}           &   & X &   &   &   &   &   &   &   & X & X \\
        \cite{Obradovic2024}        & X & X &   &   & X & X &   & X &   &   & X \\
        \cite{Yao2023}              &   & X &   &   & X & X &   &   &   &   & X \\
        \cite{Lin2022}              &   & X &   &   & X & X &   & X &   &   & X \\
        \cite{Chi2024}              & X & X &   & X & X & X &   & X &   &   &   \\
        \textbf{Ours}               & \textbf{X} & \textbf{X} &   &   &   & \textbf{X} &   & \textbf{X} &   & \textbf{X} & \textbf{X}
    \end{tabular}
    }
\end{table*}

Many of the methods in \cref{tab:related_work_topics} rely on occupancy grid maps, which explicitly model free space. However, in the maritime domain, this approach poses challenges because the \gls{lidar} detects very few points on water. Interpreting the absence of measurements as the presence of water may not be reliable. 
When generating a 2D map, a 3D \gls{lidar} cannot accurately measure free space in 2D unless it explicitly detects the points where the laser hits the water. Additionally, many methods rely on ground segmentation, which becomes problematic when water is not detected.

However, some methods have been applied in the maritime domain. \cite{Pieper2024} used \gls{enc} and mapped the deviations. This was for autonomous surface vessels to move safely without colliding into static structures that are not in the \gls{enc}. 
\cite{Yao2023} did near-shore multi-object tracking and static mapping using \gls{lidar}. They could then not detect static vessels. Furthermore, they argue against using cameras since cameras are sensitive to rain, fog, adverse weather conditions, and occlusions. This is also a reason why we split the mapping and tracking tasks so that the tracking does not require the camera when the weather is bad. 

Our earlier work \cite{Hilmarsen2025} did near-shore target tracking. However, it only detected moving objects unless they were in the image. That reduces the field of view of the tracking. This might work when moving fast forward like in the automotive domain. However, for the maritime domain, other vessels are not limited to the same location and movement patterns as on roads and we argue that it is beneficial to use the field of view of the \gls{lidar}. 
  
The most relevant work to ours is \cite{Chi2024}, where the authors performed \gls{slam} in the automotive domain. They utilized car detections in both images and \gls{lidar} point clouds to filter out dynamic objects, enabling the mapping of only the static environment. They had available a mature Yolov7 detector~\cite{Wang2023} for the automotive domain, which we do not have for the maritime domain. They were also limited to the field of view of the camera.




















\section{Mapping}

\label{sec:mapping}

The mapping process is done offline and combines context-aware segmentation from images with precise point measurements from a \gls{lidar}. It uses a neural network-based learning method for potentially moving object segmentation in the images, which is detailed in \cref{sec:potentially_moving_object_segmentation}. 

This section describes how these two sensor modalities are combined to get a precise 2D map: First, an \gls{enc} is used for initialization. Then, \gls{lidar} measurements are accumulated. Only regions in an image that are not regarded as a vessel are stored in the map. Lastly, the map is post-processed to remove inconsistencies. 




\subsection{Point Selection}

As new measurements are received from the \gls{lidar}, it is decided whether each point comes from a vessel or not. This is done using the before-mentioned image segmentation method. However, each point measurement has to be projected into the camera first. 

First, each point is transformed to the reference frame of the camera:
\begin{equation}
    \widetilde{\mathbf{X}}^{\mathrm{cam}} = \mathbf{H}^{\mathrm{cam}}_{\mathrm{LiDAR}} \widetilde{\mathbf{X}}^{\mathrm{LiDAR}}, 
    \label{eq:mapping_transformation}
\end{equation}
where $\widetilde{\mathbf{X}}^{(\cdot)}$ is the 3D homogeneous coordinate of a point in the $(\cdot)$ reference frame, and $\mathbf{H}^{\mathrm{cam}}_{\mathrm{LiDAR}}$ is the homogeneous transformation matrix between the reference frames. The transformation matrix is assumed known from calibration. 

Then, each point is projected into the camera using the standard projection formula~\cite[p. 472]{forstner2016photogrammetric}:
\begin{equation}
    \begin{bmatrix}
        Z \cdot x \\
        Z \cdot y \\
        Z
    \end{bmatrix} = \mathbf{K} \mathbf{X}^{\mathrm{cam}} = \begin{bmatrix}
        f_x & 0 & c_x \\
        0 & f_y & c_y \\
        0 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        X \\
        Y \\
        Z
    \end{bmatrix}, 
    \label{eq:mapping_projection}
\end{equation}
where $X$, $Y$ and $Z$ are the real-world coordinates of the point in the camera reference frame with unit meters and $x$ and $y$ are the image coordinates of the projected point into the image with unit pixels. The matrix $\mathbf{K}$ is the camera intrinsic matrix and is also assumed known from calibration. 

After points are projected into the image, all points that are not within the field of view of the camera are removed since we cannot know if these points belong to a vessel or not. The image segmentation method gives a segmentation mask for each vessel detected in the image. We call these vessels ``potentially moving objects''. The points that fall within such a mask are stored as detections. The points that fall within the image, but are not within a detection mask, are stored as candidates for mapping. 







\subsection{Point Accumulation}

An \gls{enc} is used for initialization of the map. These are pre-existing maps that are available that should include many of the detailed structures in regions of maritime traffic. However, they are not always as detailed as required for accurate target tracking near shore. The \gls{enc} enables the method to cover the key areas for mapping without requiring a detailed view of that area, unlike \gls{lidar} and camera-based approaches. We then create a grid out of the \gls{enc} which describes what level of details we want to store and what level of detailed view is required when mapping. Having smaller grid cells allows for finer details, but also requires the \gls{lidar} and camera data to measure enough details from all regions. 
The grid is in 2D. Each 3D point is transformed to the world frame from the \gls{lidar} frame using a similar transformation as in \cref{eq:mapping_transformation}. The transformation matrix is found using the \gls{ins} of the ego-vessel. Each point is then projected down to the horizontal plane and is reduced to being in one of the grid cells of the map. 

As new \gls{lidar} measurements are received, and each point is classified as unknown, a vessel detection or potential static structure, points are accumulated in a sliding window fashion. This is because a single image is not always perfectly segmented. Sometimes a vessel can be missed and sometimes land can be mistaken as a vessel. However, over time we can be more and more certain about how we segment the world. 
Over this sliding window, certain conditions need to be satisfied for a grid cell to become regarded as a static structure in the map: 
Firstly, the cell cannot be farther away than a specific distance threshold, this is a tunable parameter. This is because the vessel detections get worse at farther distances as well as the \gls{lidar} measurements being very sparse at great distances. 
Secondly, the cell must not be classified as a vessel at any point during the sliding window. If a cell is classified as a vessel, we must wait to determine whether sufficient evidence later supports its classification as land.
Lastly, the cell needs to be measured in at least a specific percentage of the frames within the sliding window, this is a tunable parameter. This ensures that sparse clutter from waves is not stored in the map, while allowing regions that are so far away that the \gls{lidar} does not consistently give measurements in that cell. 







\subsection{Post-processing}

After the measurements have been accumulated in the grid and the sliding window has passed, each cell is reduced to being a binary grid of land or not land. The grid is then processed to remove inconsistencies. For example, there might be a single cell that was not stored as a static structure, while all its neighboring cells were. This is likely then also a static structure. The opposite is also possible, where a bird or clutter in the \gls{lidar} data has given rise to a single cell that is mistakenly regarded as a static structure surrounded by what is otherwise water. To address these issues we use a series of morphological operations: dilation and erosion. The finished map can be used in tracking as described next. 





















\section{Tracking}

After the map has been created, it can be used for removing \gls{lidar} points belonging to static structures in the environment. This is done in a similar way as during mapping, by transforming the points to the 2D world frame and removing points that fall within a cell in the map that corresponds to a static structure. The 3D points in the \gls{lidar} point cloud that do not fall within such a cell can still be kept in 3D. However, we opt for 2D tracking in this paper as it is sufficient for demonstration. 

There are reasons to only use \gls{lidar} for tracking and not the camera. It would be possible to use the image boat detections for tracking as well, but relying on it would enforce the field of view of the camera. The field of view can therefore be bigger when using only the \gls{lidar}, unless the camera has really wide field of view. 




\subsection{Detection}

For simplicity, we choose to track 2D boat detections. While 3D \gls{eot} might give more accurate tracking, 2D point tracking is sufficient to demonstrate the improved tracking with a map from our mapping method compared to less detailed maps. 

The boat detections are based on \gls{dbscan}~\cite{ester1996density} run on the point cloud projected to 2D that is not considered a part of the static structures in the map. This is a clustering method that groups the points into clusters based on the distance between groups of points and the density inside the groups. Each cluster of points is then reduced to being a single-point measurement that can be processed in a point-target tracker. 





\subsection{VJIPDA Tracker}

The point-target tracker utilized here is the \gls{vjipda} tracker~\cite{Brekke2021}. We choose the \gls{vjipda} as its visibility state can improve the tracking during occlusions that are common in near-shore environments. 


































\section{Potentially Moving Object Segmentation}
\label{sec:potentially_moving_object_segmentation}

By ``potentially moving object'' we include static boats as well as moving boats. To detect both of these, we utilize the camera and an instance segmentation method for the images. The instance segmentation method used in this paper is Yolov8~\cite{Jocher_YOLO_by_Ultralytics_2023}. It is relatively fast and gives good enough detections for the most part. However, the masks are not always how we would like them. \Cref{fig:seg_mistake_dock} shows the challenge where the floating dock is detected as a boat. This happened consistently when using the pre-trained model that was trained on the COCO dataset~\cite{Lin2014}, even with detection scores of up to $80\%$. To alleviate this challenge, we trained the network on more relevant data. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/seg_mistake_dock.jpg}
    \caption{The static floating dock is mistakenly detected as two boats. This needs to be handled to map the dock. }
    \label{fig:seg_mistake_dock}
\end{figure}

There are several datasets available for training in addition to COCO. Firstly, the MariBoats dataset~\cite{Sun2023} consists of $6.2 \mathrm{k}$ images with instance segmentation of the class ``ship''. The data was gathered by searching online for images. The images are not very representative of the autonomous ferry scenario, with many big and industrial ships and plenty of text on the images. It is unlikely that such a dataset will clearly differentiate between the floating dock used here and a raft or something similar. We tried training on this data, but it did not improve the situation in \cref{fig:seg_mistake_dock}. Secondly, ABOships~\cite{Iancu2021} has boat detections from the autonomous ferry perspective. However, they have not created instance segmentation annotations, only bounding box detections. 

\begin{figure}
    \centering
    \subfloat[Annotation of boats and no annotation on the floating dock. ]{%
      \includegraphics[width=0.7\linewidth]{figures/seg_annotation_boats.jpg}
    }
    
    \subfloat[Purposefully not labeled dock. ]{%
      \includegraphics[width=0.7\linewidth]{figures/seg_annotation_docks.jpg}
    }
    
    \caption{Examples of annotations that differentiate boats from floating docks. This labeling is used to train the segmentation method. }
    \label{fig:seg_annotations}
\end{figure}

It is not obvious that the floating dock is not a boat. Even a human could think it is a boat if the image is not taken very close to the dock. Therefore, for an instance segmentation method to work well, we choose to create our own annotations on our own dataset to teach the network that it is a dock and not a boat. 

The way we choose to train the network is by training in multiple steps. Firstly we change the COCO dataset~\cite{Lin2014} to use all images with boat annotations, which is $3146$ images, and $5\%$ of the other images, about $6 \mathrm{k}$ images. The whole network is then trained on this. Secondly, we freeze the first $5$ layers of the network and train the rest on the ABOships dataset~\cite{Iancu2021}, with the whole detection boxes as masks. The idea here is that the finer segmentation can be learned by the last layers and that we want to utilize more data than only COCO. Lastly, the network is trained on our own data with the first $10$ layers frozen. The result is a method that manages most detections correctly with fewer false positive detections on the docks. 
This could in a sense be seen as implicit mapping or over-training. However, having a good enough detector is necessary when using images in some specific scenarios. Our dataset has $181$ images with instance segmentation annotations of boats. Examples are shown in \cref{fig:seg_annotations}. Note that the floating docks are not labeled as boats. 














\section{Results}

\subsection{Dataset}

The data used in this paper has not yet been published, but we intend to publish it soon. Without going into all the details of the dataset, we can explain the most important aspects. The sensors used are \gls{ins}, \gls{lidar}, two stereo cameras and two \gls{gnss} recorders for reference tracks. The \gls{ins} uses \gls{rtk} \gls{gnss} to get accurate position and combines it with an \gls{imu} to get accurate orientation. The sensors have all been calibrated with respect to each other. 
The \gls{ins} runs at $100 \mathrm{Hz}$, the cameras at $30$ fps and the \gls{lidar} at $10 \mathrm{Hz}$. All sensors have been synchronized to \gls{gnss} time, although they are not triggered simultaneously. 






\subsection{Implementation}

We will mention the parameters used in the mapping method. The sliding window is set to $5\mathrm{s}$, that is $50$ frames. The distance threshold for the mapping is $100\mathrm{m}$. The percentage of frames needed to be measured for a cell to be regarded as a static structure is $40\%$. The grid size is $0.5\mathrm{m}$. 

An illustration of the \gls{lidar} point projection to the image and the image segmentation is shown in \cref{fig:map_image_lidar_detections}. We can see that some large ships are detected correctly with good masks and that the docking area is not detected as a vessel. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/map_image_lidar_detections.jpg}
    \caption{What a single frame can look like during mapping. The boat detections are in light orange, the image is brightened in the background and the projected \gls{lidar} points are in blue. This illustrates how the point selection is done, differentiating between static objects and vessels. }
    \label{fig:map_image_lidar_detections}
\end{figure}

The \gls{enc} data used was sourced from OpenStreetMap~\cite{OpenStreetMap} and obtained using the OSMnx tool~\cite{boeing2024modeling}. The polygons were found and reduced to only the interesting area. 







\subsection{The Resulting Map}

If the mapping is done naively, without considering potentially moving objects, one might get a map similar to \cref{fig:map_lidar_accumulate_all} where also the docked boats are mapped. As mentioned before, this might make the tracking algorithm miss these boats, which could potentially result in a collision with the ferry. The resulting map using our mapping method is shown in \cref{fig:map_lidar_and_enc}. There we see that the boats are not stored and the \gls{enc} fills a lot of the unknown space in the pure accumulation map. Post-processing the map is particularly useful for sections of the walkway near land that lacks \gls{lidar} points due to the sensor's sparsity at that distance, leading to gaps in the data.

\begin{figure}
    \centering
    \subfloat[All \gls{lidar} points have been accumulated into a map. Three boats have been circled in red to show that these have become a part of the map.\label{fig:map_lidar_accumulate_all}]{%
      \includegraphics[width=0.45\linewidth]{figures/map_lidar_accumulate_all_drawn.pdf}
    }\hfill
    \subfloat[The \gls{enc} from \href{https://www.openstreetmap.org/copyright}{OpenStreetMap} is shown in blue. The map created from accumulating segmented \gls{lidar} points is shown in orange.\label{fig:map_lidar_and_enc}]{%
      \includegraphics[width=0.45\linewidth]{figures/map_lidar_and_enc.pdf}
    }
    
    \caption{Comparison of two different maps. Our mapping method improves the differentiation between land and vessels. }
\end{figure}







\subsection{Tracking Using Different Maps}

The main differences between not using a map, using less accurate maps and using accurate maps for tracking are the number of false tracks and the time of track initialization, not necessarily the precision of each track. Therefore, we will compare the use of different maps and show both qualitatively and quantitatively the difference. 

An example of the tracking results is shown in \cref{fig:track_result}. The kayak that undocks from the quay has been tracked while it is close to the quay. A day cruiser comes in front, occluding the kayak, but the kayak is tracked again afterward because of the accurate enough predictions of its movements. 

\begin{figure*}
    \centering
    \subfloat[Tracking without a map. The list of tracks is too long for the image. The number of tracks became $33$.\label{fig:result_tracking_without_map}]{%
      \includegraphics[width=0.32\linewidth]{figures/result_tracking_without_map.pdf}
    }\hfill
    \subfloat[Tracking using only the \gls{enc} as map. Gave rise to $22$ tracks.\label{fig:result_tracking_with_only_enc}]{%
      \includegraphics[width=0.32\linewidth]{figures/result_tracking_with_only_enc.pdf}
    }\hfill
    \subfloat[Tracking in a map with big margins.\label{fig:result_tracking_with_dilated_map}]{%
      \includegraphics[width=0.32\linewidth]{figures/result_tracking_with_dilated_map.pdf}
    }
    
    \caption{What the measurements, detections and tracks look like when using imperfect maps. }
    \label{fig:result_different_maps}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/result_tracking.pdf}
    \caption{Tracking with a good map, where all vessels close to the ferry have been detected and tracked. The tracking happens earlier than in the worse maps in \cref{fig:result_different_maps}. }
    \label{fig:track_result}
\end{figure}

Tracking all detections without a map can cause a lot of false tracks, see \cref{fig:result_tracking_without_map}. The same detection and tracking as earlier is used, just without any map. It results in $33$ tracks, while the correct number is $6$. While it is possible to do better boat detections in the \gls{lidar} point clouds, this shows that it is beneficial to remove the points that are known to be land. We also see that the kayak that is close to the dock is first detected when it comes far enough from the dock to not be clustered with it. The corresponding track in \cref{fig:track_result} starts when the kayak is right next to the dock, see id $11$. The day cruiser that crosses the path of the kayak is detected and tracked similarly in the two cases. 

Using only \gls{enc} as a map also gives rise to false tracks. This is shown in \cref{fig:result_tracking_with_only_enc}. The reason for this is the structures that are not in the map but are still detected.  Other than the number of false tracks, the results are similar to the ones with no map.

We have approximated a map based on land masking in \cref{fig:result_tracking_with_dilated_map}, with added tracking results, by using our estimated map in \cref{fig:map_lidar_and_enc} and adding a margin of $2 \mathrm{m}$. 
This is still more detailed than the land masking in \cref{fig:intro_land_masking}. Also, if the land masking had been made using the photo in \cref{fig:intro_land_masking}, and not with the details from the mapping done in this paper, then many regions would not have been masked correctly since the photo is not up to date. We can see that the points from the land are removed. However, points on boats have also been removed. Some boats are not detected at all, and the kayak is not detected before coming sufficiently far from the dock. 









\subsection{Tracking in More Scenarios}

\begin{figure*}
    \centering

    \subfloat[Multi target cross. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_cross.pdf}
    }\hfill
    \subfloat[Multi target day cruiser undock. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_day_cruiser_undock.pdf}
    }\hfill
    \subfloat[Multi target kayak undock 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_kayak_undock_1.pdf}
    }\hfill
    \subfloat[Multi target kayak undock 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_kayak_undock_2.pdf}
    }\hfill
    \subfloat[Multi target maneuver 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_maneuver_1.pdf}
    }
    
    \subfloat[Multi target maneuver 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_maneuver_2.pdf}
    }\hfill
    \subfloat[Multi target pass 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_pass_1.pdf}
    }\hfill
    \subfloat[Multi target pass 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_pass_2.pdf}
    }\hfill
    \subfloat[Multi target pass 3. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_pass_3.pdf}
    }\hfill
    \subfloat[Multi target pass 4. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_pass_4.pdf}
    }
    
    \subfloat[Multi target pass 5. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_multi_target_pass_5.pdf}
    }\hfill
    \subfloat[Single target maneuver 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_maneuver_1.pdf}
    }\hfill
    \subfloat[Single target maneuver 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_maneuver_2.pdf}
    }\hfill
    \subfloat[Single target pass north. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_pass_north.pdf}
    }\hfill
    \subfloat[Single target pass south. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_pass_south.pdf}
    }
    
    \subfloat[Single target undock south. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_undock_south.pdf}
    }\hfill
    \subfloat[Single target undock still 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_undock_still_1.pdf}
    }\hfill
    \subfloat[Single target undock still 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_undock_still_2.pdf}
    }\hfill
    \subfloat[Single target west 1. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_west_1.pdf}
    }\hfill
    \subfloat[Single target west 2. ]{%
      \includegraphics[width=0.19\linewidth]{figures/sequences/result_single_target_west_2.pdf}
    }
    
    \caption{All $20$ sequences we have gathered are plotted with tracks from the method in this paper. }
    \label{fig:result_all_sequences}
\end{figure*}

The tracking is performed on all our $20$ sequences, and the final tracks are shown in \cref{fig:result_all_sequences}. Similarly to the tracking in maps with fewer details, some of these sequences also show false tracks. This is due to certain regions not being mapped well. We used a specific mapping sequence to do the mapping, and not all regions were then seen in detail with the camera. A solution to this could be more detailed mapping, mapping on the tracking sequences as well, or an online mapping and tracking approach. Furthermore, one of the three maneuvering sequences shows what can happen when the constant velocity model is not good enough, where track is lost and re-created as the target does a quick maneuver. 






\section{Conclusion}

In this paper, we proposed a method of computer vision-aided \gls{lidar} mapping to create detailed maps for reliable vessel detection and tracking.
Our approach demonstrated that integrating precise \gls{lidar} data with a trained image segmentation method enabled detection of potentially moving objects.
By using the camera solely for offline mapping, we ensured full field of view of the \gls{lidar} sensor during tracking. The results confirmed the method's effectiveness, even in detecting close-to-dock vessels before they entered potential collision zones.
The main limitations of this method is that it requires a decent vessel detector and that the whole region is mapped sufficiently. However, detectors are constantly being improved and some maritime systems repeatedly operate in specific regions. This makes the method highly relevant for systems such as autonomous ferries. 







\section{Acknowledgements}
We use some map data copyrighted OpenStreetMap contributors and available from \url{https://www.openstreetmap.org}. 
