%大模型在化学的发展，怎么好
%但是仍遇到以下的问题
%问题的严重性
%我们提出了
%实验证明了

%近年来，大语言模型（LLMs）的快速发展为化学研究提供了新的可能性。然而，尽管 LLMs 在自然语言处理任务上表现优异，其在化学计算任务中的应用仍然面临诸多挑战。首先，LLMs 在化学任务中的扩展性受限，即便通过微调（fine-tuning）或指令调控（instruction tuning），仍难以泛化到复杂且多变的化学任务。其次，LLMs 缺乏高效的工具协同能力，无法充分调用专业化的化学计算工具，从而限制了其在科学计算中的应用潜力。

%但现有研究主要侧重于单一工具的调用，并没有考虑工具之间的联系，尚未探索工具的层次化组合。正如图1所示，对于Text-based Molecule Design Task，仅调用单一的工具，由于单个工具的单一和局限性或者模型的错误调用，单一工具往往不足以解决复杂的科学问题，而多种工具的协同调用则能发挥工具协同能力，有望增强llm在化学相关任务中的推理能力。
\input{figures/intro_fig}
\input{figures/methodfig}
In recent years, large language models (LLMs) ~\cite{touvron2023llama,achiam2023gpt,deepseekai2025deepseekr1incentivizingreasoningcapability} have made groundbreaking advancements in fields such as natural language processing (NLP) and knowledge reasoning, and are gradually being applied to scientific research, particularly demonstrating immense potential in the domain of chemistry~\cite{guo2023largelanguagemodelschemistry,ouyang2024structuredchemistryreasoninglarge}. LLMs leverage their robust capabilities for knowledge integration and reasoning computation to offer novel solutions for tasks such as molecular design~\cite{noutahi2023gottasafenewframework}, reaction prediction~\cite{shi2023relmleveraginglanguagemodels}, and property prediction~\cite{srinivas2024crossmodallearningchemistryproperty}. This progress significantly enhances the level of automation and intelligence in chemical research.

%现在存在的问题
Despite the promising prospects of LLMs in the field of chemistry, current research faces challenges. Existing models exhibit limitations in their ability to generalize across diverse tasks, and they lack access to external knowledge sources, which constrains their practical utility in scientific applications~\cite{zhao2024chemdfmlargelanguagefoundation,liao2024wordsmoleculessurveylarge,han2024generalistspecialistsurveylarge}. Consequently, studies have integrated expert-designed tools to enhance the performance of LLMs in chemical tasks~\cite{bran2023chemcrowaugmentinglargelanguagemodels,boiko2023autonomous,song2024multi}.

%说明现在化学agent tool的问题

However, chemical tool-augmented LLMs still exhibit numerous challenges when confronted with complex and diverse toolsets~\cite{bran2023chemcrowaugmentinglargelanguagemodels}.
%第一，由于大模型工具能力有限或容易出现工具调用错误等问题，仅调用一次工具可能导致结果偏差。正如图1所示，调用一次agent工具的结果出现偏差，则模型无法自我修正，生成错误答案。
Firstly, due to the limited capabilities of tools or the susceptibility of LLMs to tool calling errors, relying on a single tool invocation may result in biased outcomes~\cite{ye2024toolswordunveilingsafetyissues}. As illustrated in Fig.~\ref{fig:intro}, if the result of invoking an agent tool once is biased, the model is unable to self-correct and may produce incorrect answers.
%第二
Secondly, LLMs struggle to collaborate across different types of tools, limiting their capacity to maximize information gain~\cite{cheng2025toolunlearningtoolaugmentedllms}. During reasoning, they often depend on a single category of tools, overlooking the synergistic benefits of integrating multiple tool types. As a result, errors can propagate through the reasoning chain, ultimately impairing overall task performance.
%说明现有的研究不足以解决

Despite the emergence of numerous studies in recent years focused on optimizing tool usage, these efforts have not yet adequately addressed the aforementioned issues. Current research primarily concentrates on optimal tool selection for individual tasks, while overlooking the collaborative interactions between tools and their impact on task inference capabilities.
%开始举例各种研究
Some studies have attempted to leverage the context learning capabilities of LLMs by employing strategic prompting to optimize tool selection. For instance, frameworks such as Chain of Thought (CoT) ~\cite{wei2023chainofthoughtpromptingelicitsreasoning} and ReACT ~\cite{yao2023reactsynergizingreasoningacting,yang2023autogptonlinedecisionmaking} aim to guide model in effectively utilizing tools during the reasoning process. 
%另一类研究
Another category of methods involves fine-tuning LLMs on tool calling datasets to enhance their tool usage capabilities. For instance, approaches such as ToolBench~\cite{qin2023toolllmfacilitatinglargelanguage} utilizes supervised fine-tuning~\cite{shen2024rethinkingdataselectionsupervised} to improve the model's understanding and calling of tools. 
However, these approaches can only enhance the selection of individual tools to a limited extent and have not succeeded in achieving collaborative optimization among multiple tools.

%开始提出我们的方法
To address the aforementioned issues, we propose a novel tool calling optimization method termed \textbf{ChemHTS} (\textbf{Chem}ical \textbf{H}ierarchical \textbf{T}ool \textbf{S}tacking), 
which aims to enhance the tool invocation process of LLMs in chemistry-related tasks. The core idea of C he m is to explore the optimal tool invocation pathways through a hierarchical stacking strategy under the constraints of a given chemical task and multiple tools, thereby improving the reasoning and computational capabilities of LLMs in chemical applications. This method encompasses two key stages: tool self-stacking and tool multi-layer decision optimization.
%在第一阶段，
%在第二阶段，

In addition, we apply the ChemHTS method to four classic chemistry tasks: text-based molecular design, molecular description, molecular property prediction, and reaction prediction. We systematically explore the optimal tool invocation pathways for each task and conduct extensive experiments on the test sets.
The results demonstrate that the tool invocation paths optimized by ChemHTS outperform several baseline models, such as GPT4o~\cite{openai2024gpt4ocard} and DeepSeek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, as well as chemistry-specific models, including UniMol~\cite{ji2024unimol2exploringmolecularpretraining} and ChemDFM~\cite{zhao2024chemdfmlargelanguagefoundation}, across all tasks. Furthermore, we define four distinct tool stacking behavioral patterns—\textbf{Correct, Modify, Judge, and Reserve}-and provide case studies to elucidate the reasons behind the improvements in task performance due to tool stacking.
To summarize, our contributions are mainly three-fold: 

\begin{itemize}
    \item We are the first to highlight the advantages of agent tool collaboration and propose the ChemHTS method to optimize the optimal tool stacking path for agents across various chemical tasks, addressing the issue of low efficiency in the utilization of agent tools.
    \item Through extensive experiments, we have demonstrated that the optimal tool stacking paths derived from our method outperform numerous baseline models and chemistry-specific models across four classical chemical tasks, thereby validating the effectiveness and generalizability of our approach.
    \item We define four behavioural patterns of models during the tool stacking process, to conduct an in-depth analysis and interpretation of the reasons behind the performance improvements in tool stacking, thereby enhancing the interpretability of the experiments.
\end{itemize}







