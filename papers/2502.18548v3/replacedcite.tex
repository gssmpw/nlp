\section{Related work}
%RLHF pipeline ____

In this section, we review two existing alignment approaches, which we use as baselines to compare their alignment objectives with that of the GRPO algorithm. 

\paragraph{Reinforcement Learning from Human Feedback (RLHF)} The standard RLHF paradigm ____ consists of two main steps: learning the reward model and optimising the policy using the learned reward model. In the first step, the reward model $r_\phi(\cdot\mid q)$ is trained on a dataset containing examples of human preferences in the form of pairwise comparisons of outputs for given contexts. In the second step, the objective is to find a policy that maximises the following objective function:
\begin{equation}
\mathcal{J}_{RLHF}(\theta) = \mathbb{E}_{q\sim \mu, o\sim \pi_\theta(\cdot\mid q)}[r_\phi(o\mid q)] - \beta\ \mathbb{E}_{q\sim \mu}[\mathrm{KL}(\pi_\theta(\cdot\mid q)\mid\mid \pi_{\rf}(\cdot \mid q))]
\label{equ:RLHF}
\end{equation}
where $\pi_\rf(\cdot\mid q)$ is a reference model policy and $\mathrm{KL}(\pi\mid\mid \pi')$ is the Kullback-Leibler divergence between two distributions $\pi$ and $\pi'$, $\mathrm{KL}(\pi\mid \mid \pi')=\mathbb{E}_{x\sim \pi}[\log(\pi(x)/\pi'(x))]$. The objective function in (\ref{equ:RLHF}) is optimised by using PPO ____ or similar optimisation approaches. PPO is an actor-critic RL algorithm that is used in the RL fine-tuning stage of LLMs ____.

The RLHF can be seen as an approach for aggregating a reward preference and a reference-policy preference according to:
\begin{equation}
\pi_\theta(o\mid q ) = \frac{1}{Z_q}\pi_{\rf}(o\mid q) e^{\frac{1}{\beta}r_\phi(o\mid q)}
\label{equ:rlhfpi}
\end{equation}
where $Z_q$ is a normalisation constant. This follows directly by choosing $\pi_\theta(\cdot\mid q)$ that maximises the RLHF objective function (\ref{equ:RLHF}). The aggregate preference distribution in (\ref{equ:rlhfpi}) follows the logarithmic opinion pooling form ____. Logarithmic opinion pooling is a method for aggregating multiple probability distributions into a single consensus distribution, where the consensus distribution is proportional to some weighted geometric average of the individual distributions. Specifically, (\ref{equ:rlhfpi}) defines the consensus distribution as a weighted geometric average of the reference-policy distribution $\pi_\rf(\cdot\mid q)$ and the Luce's choice ____ distribution $e^{r_\phi(o\mid q)}/\sum_{o'}e^{r_\phi(o'\mid q)}$ parametrised with the reward values, with the respective weights of values $1$ and $1/\beta$.  

\paragraph{Nash Learning from Human Feedback (NLHF)} NLHF is an alignment approach introduced in ____, where the reward preference model is defined in terms of pairwise preferences over outputs for a given context. Specifically, the preference of an output $o$ over another output $o'$, given a context $q$, is expressed as a value $\mathcal{P}(o\succ o'\mid q)$ in the range $[0,1]$. The pairwise preferences are assumed to be antisymmetric, meaning that $\mathcal{P}(o'\succ o\mid q)=1-\mathcal{P}(o\succ o'\mid q)$. Given the pairwise preferences $\mathcal{P}(o\succ o'\mid q)$ and a reference policy $\pi_\rf(\cdot\mid q)$ for each context $q$, the aggregation of preferences is defined as the symmetric Nash equilibrium of a two-player zero-sum game. The expected payoff for a player deploying the mixed strategy $\pi$ against a player deploying the mixed-strategy $\pi'$, is given by:
\begin{equation}
\mathcal{J}_{NLHF}(\pi,\pi') = \mathbb{E}_{q\sim \mu, o\sim \pi(\cdot\mid q), o'\sim \pi'(\cdot\mid q)}\left[\mathcal{P}(o\succ o'\mid q) - \beta \log\frac{\pi(o\mid q)}{\pi_\rf(o\mid q)} + \beta \log\frac{\pi'(o'\mid q)}{\pi_\rf(o'\mid q)}\right]
\label{equ:nlhf}
\end{equation}
where $\beta$ is a positive-valued hyperparameter. The NLHF two-player zero-sum game has a unique Nash equilibrium, which is also the limit point of a mirror-descent iterative computation algorithm ____. 

A notable difference between NLHF and RLHF is that NLHF observes reward preferences as pairwise comparisons of outputs, whereas RLHF expresses preferences as absolute reward vaues assigned to individual outputs. It can be easily shown that the solution to the NLHF game satisfies:
$$
\pi(o\mid q) = \frac{1}{Z_q}\pi_\rf(o\mid q) e^{\frac{1}{\beta}\mathbb{E}_{o'\sim \pi(\cdot\mid q)}[\mathcal{P}(o\succ o'\mid q)]}
$$
where $Z_q$ is a normalisation constant. Notably, this can also be interpreted as a logarithmic pooling of distributions, where the geometric averaging weights depend on $\pi$.