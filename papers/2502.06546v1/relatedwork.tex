\section{Background and Related Work}
\label{sec:background}

In this section, we present background information and related work on explainability in software engineering.

\subsection{Explainability}
\label{sec:erklearbarkeit}

Explainability is a non-functional requirement that addresses explanations regarding different aspects of software systems~\cite{chazette2021exploring}. The rise of AI systems led to an increasing demand for explanations regarding the inner workings of these so-called black boxes~\cite{das2020XAISurvey}. However, since everyday software systems are becoming progressively complex independently of AI, explainability is no longer limited to explaining the inner workings of AI systems~\cite{brunotte2023privacy,chazette2022framework,deters2024qualitymodel,obaidi2025appKonwledge,obaidi2025automatingexplanationneedmanagement,droste2024explanations}. 

Droste et al.~\cite{droste2024explanations} conducted an online survey and revealed that there is a pressing need for explanations in several kinds of everyday software systems. They created a taxonomy that divides the expressed needs for explanation into five main categories. The most frequently mentioned category was the need for explanations regarding interactions with the system. This includes how certain operations can be performed or how to navigate to certain views or functionalities. The second category they describe is \textit{system behavior}, including the need for explanations regarding unexpected system behavior or consequences that arise from the user's actions. \textit{Domain knowledge} concerns questions about terminology or system-specific elements. The forth category covers \textit{privacy and security} issues. The last category they identified is \textit{user interface}, which includes an explanation of interface elements, especially if these have been changed. The survey data collected by Droste et al.~\cite{droste2024explanations} also served as the data basis for this work (as detailed in Section~\ref{sec:survey}).

Research into the user experience of explainable systems has shown that explanations can have a positive influence on aspects such as understandability~\cite{id955_dominguez2020,id1313_muhammad2016}, satisfaction~\cite{id731_bellini2018,id770_tran2019}, and trust~\cite{id731_bellini2018,id708_wang2018}. However, explanations may also have negative effects on user experience~\cite{chazette2020explainability,deters2024UXandExplainability,nunes2017systematic}. For example, they can negatively influence trust, if too much or misleading information is given~\cite{kizilcec2016trustAndTransparency,sadeghi2024explanation}. In addition, the users' cognitive load can be unnecessarily increased by misplaced explanations~\cite{chazette2020explainability,nunes2017systematic}. Poorly implemented explanations can also have a negative impact on the overall user experience~\cite{deters2024UXandExplainability}. It is therefore essential to implement explanations carefully to avoid unwanted side effects. Among other things, the personalization of explanations is a key factor here. For instance, the users' prior knowledge should be taken into account, as well as personal preferences based on the user's personality~\cite{deters2024qualitymodel}. To this end, Ramos et al.~\cite{ramos2021modeling} created personas to support the development of explainable systems.

Oberste and Heinzl~\cite{oberste2023explainabilityWithPriorKnowledge} explored how knowledge-informed machine learning enhances user-centric explanations in healthcare. Through a review of such systems, they highlighted improvements in formal understanding, medical knowledge delivery, and explanation intuitiveness. However, they noted the need for further research to tailor explanations to users’ backgrounds and improve trust and acceptance among medical professionals.

Unterbusch et al.~\cite{unterbusch2023explanation} investigated users' explanation needs in software systems by analyzing 1,730 app reviews from eight apps to develop a taxonomy of explanation needs. They also tested automated detection methods, achieving a weighted F-score of 86\% in identifying explanation needs across 486 reviews from four additional apps.

\subsection{Mood}
\label{sec:sentiment}
Mood is commonly measured with scales, such as the one created by Bohner et al. \cite{bohner1991stimmungs}, a German adaptation of Underwood and Froming’s "Mood Survey" \cite{underwood1980mood}. This scale assesses general, enduring mood \cite{bohner1997stimmungsskala} and includes two subscales: one for assessing enduring \textit{sentiment} and another for evaluating emotional \textit{reactivity} \cite{bohner1997stimmungsskala}, with reactivity measuring the strength of mood fluctuations. Comprising 15 statements rated on a 7-point scale \cite{bohner1997stimmungsskala}, the scale reverses negative polarity items during evaluation to ensure accuracy \cite{bohner1997stimmungsskala}, where higher values reflect a more positive mood or stronger emotional reactivity \cite{bohner1997stimmungsskala}.

For assessing mood over a defined period of time, such as over the past week, the \textit{Positive and Negative Affect Schedule} (PANAS) \cite{watson1988development} or its German version by Breyer and Bluemke \cite{breyer2016panas-de} is used. The scale comprises two 10-item subscales, one measuring positive affect (PA) and the other measuring negative affect (NA). Respondents indicate the extent to which they have experienced a given emotion on a 5-point Likert scale. PA reflects feelings of enthusiasm and high energy, whereas NA represents distress and unpleasurable emotions. It is a widely used measure in psychological research to assess emotional states and their implications for mental health.

Emotion classification frameworks categorize higher-level emotions, to which lower-level emotions are assigned~\cite{shaver1987emotion}. Shaver et al.~\cite{shaver1987emotion} developed one such framework based on data compiled by psychology students.