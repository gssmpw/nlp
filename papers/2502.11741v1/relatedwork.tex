\section{Related Work}
Recent significant progress in the Text2SQL task has primarily focused on LLMs, as their remarkable reasoning ability provides new directions and opportunities for the Text2SQL task. Currently, methods based on LLMs can generally be divided into two categories: Prompt Engineering and Agent-based interaction with LLMs.
% 最近Text2SQL任务的重大进展主要集中在大语言模型方面, 这是因为大语言模型非凡的推理能力为Text2SQL任务提供了新的方向和机遇. 目前基于LLM的方法一般可以分为Prompt Engineering 和 大语言模型交互代理两类。



\noindent \textbf{Prompt Engineering.}
In the early stages of LLMs, a direct and effective method to better exploit the potential of LLMs was to carefully design effective prompts to guide the models, which also applies to the Text2SQL task. Enhancing the reasoning ability of LLMs through Chain of Thought~\cite{DBLP:conf/emnlp/ZhangCCX023} is a promising attempt. Several methods~\cite{wang2024macsql, DBLP:conf/nips/PourrezaR23, DBLP:journals/pacmmod/LiZLFZZWP0024} utilize schema linking to combine natural language questions with database schema elements, achieving promising results. Among them, DAIL-SQL~\cite{DBLP:journals/pvldb/GaoWLSQDZ24} systematically investigates prompt engineering for LLM-based text-to-SQL methods, including question representation, prompt components, example selection, and example organization. 

Recently, some works have shifted attention from prompt engineering (e.g., GPT-4 and other closed-source models) to fine-tuning LLMs. SENSE~\cite{yang-etal-2024-synthesizing} synthesizes strong data and performs Direct Preference Optimization (DPO) on weak data from a weak LLM, while ROUTE~\cite{qin2024route} proposes a multitask collaborative fine-tuning approach, reducing potential errors in SQL generation and achieving better results.
% 在LLMs出现的早期阶段, 为了更好地发掘LLMs的潜力, 一个直接有效的方法就是精心设计有效的Prompt来引导LLMs, 这在Text2SQL任务中也不例外. 通过思维链~\cite{DBLP:conf/emnlp/ZhangCCX023}提升LLMs的推理能力是个很好的尝试。一些方法~\cite{wang2024macsql, DBLP:conf/nips/PourrezaR23, DBLP:journals/pacmmod/LiZLFZZWP0024}利用Schema linking将自然语言问题与数据库模式元素结合起来, 取得了不错的结果. 其中DAIL-SQL~\cite{DBLP:journals/pvldb/GaoWLSQDZ24}系统地研究了基于LLM的文本到SQL方法的提示工程，包括问题表示、提示组件、示例选择和示例组织. 最近，有些工作将注意力从提示GPT4等闭源模型放到了微调LLMs上, SENSE~\cite{yang-etal-2024-synthesizing}通过综合强数据并对来自弱 LLM 的弱数据执行直接偏好优化（DPO）, ROUTE~\cite{qin2024route}提出了一种多任务协作的微调方法, 降低SQL生成潜在错误并取得了较好的结果。


\noindent \textbf{Agent-based interaction with LLMs.}
The agent-based interactive methods~\cite{DBLP:conf/acl/ChenWMP0024} guide LLMs to generate accurate SQL queries by designing feedback signals. Early works~\cite{DBLP:conf/emnlp/ShiFGZW22} focused on improving SQL based on execution results, by executing SQL queries and selecting the most accurate translation based on execution risks. Other works~\cite{DBLP:conf/iclr/ChenLSZ24, DBLP:conf/pricai/GuoTTWWYW23} use LLMs to inspect results and correct discrepancies between the generated SQL and real SQL queries. MAC-SQL~\cite{wang2024macsql} introduces a multi-agent framework and other novel interactive methods~\cite{xiong2024interactive}. However, most of these methods rely heavily on high-quality external feedback, which is often unavailable in practical applications, and they primarily depend on closed-source LLMs, overlooking the potential of open-source LLMs in reasoning.
% 基于代理的交互方法~\cite{DBLP:conf/acl/ChenWMP0024}通过设计反馈信号，引导 LLM 生成准确的 SQL 查询。早期，有些工作~\cite{DBLP:conf/emnlp/ShiFGZW22}侧重于根据执行结果改进SQL, 执行抽样SQL查询，根据执行风险选择最准确的翻译。另外一些工作~\cite{DBLP:conf/iclr/ChenLSZ24, DBLP:conf/pricai/GuoTTWWYW23}则利用LLM通过检查结果来改进SQL, 纠正SQL与真实SQL语句的差异。MAC-SQL~\cite{wang2024macsql}引入了一种多代理框架以及其他一些新的交互方法~\cite{xiong2024interactive}。然而这些方法大多需要高质量外部反馈, 这在实际应用中通常并不存在，且大多依赖闭源LLMs GPT-4, 忽视了发掘开源LLMs的推理能力.

% mcts启发式（\textbf{Self-Reward-guide} / 搜索空间不大）数据合成 R-1
% sft - Model + MCTS -> generation SQL-> prediction
% MCTS的好处 COT -> TOT(没有reward guide)
% test time scalling law / Search-o1 + KBQA-o1 -> MT

\begin{figure*}[bht!]  % 使用figure*来跨越两栏
    \centering
    \includegraphics[width=\textwidth]{sql-o1_.pdf}  % 图片文件路径
    \caption{In the search step, an example demonstrates the MCTS heuristic search guided by Self-Reward.}
    \label{fig:sql-o1}
\end{figure*}



%