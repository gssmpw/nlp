% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{sql-o1}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% \usepackage[UTF8]{ctex}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow} 
\usepackage{booktabs}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{enumitem}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Shuai Lyu\textsuperscript{\rm 1}$^\ast$, Haoran Luo\textsuperscript{\rm 1,2}\thanks{Equal contribution.}, Zhonghong Ou\textsuperscript{\rm 3}\thanks{Corresponding author.}, Yifan Zhu\textsuperscript{\rm 1}, Xiaoran Shang\textsuperscript{\rm 1}, Yang Qin\textsuperscript{\rm 4}, Meina Song\textsuperscript{\rm 1} \\
    \textsuperscript{\rm 1}School of Computer Science, Beijing University of Posts and Telecommunications, China. \\
    \textsuperscript{\rm 2}Nanyang Technological University, Singapore. \\
    \textsuperscript{\rm 3}State Key Laboratory of Networking and Switching Technology,\\ Beijing University of Posts and Telecommunications, China. \\
    \textsuperscript{\rm 4}College of Computer Science, Sichuan University, Chengdu, China. \\
    \texttt{\{Lxb\_savior, luohaoran, zhonghong.ou, yifan\_zhu\}@bupt.edu.cn} \\
}






%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
The Text-to-SQL(Text2SQL) task aims to convert natural language queries into executable SQL queries. Thanks to the application of large language models (LLMs), significant progress has been made in this field. However, challenges such as model scalability, limited generation space, and coherence issues in SQL generation still persist. To address these issues, we propose SQL-o1, a Self-Reward-based heuristic search method designed to enhance the reasoning ability of LLMs in SQL query generation. SQL-o1 combines Monte Carlo Tree Search (MCTS) for heuristic process-level search and constructs a Schema-Aware dataset to help the model better understand database schemas. Extensive experiments on the Bird and Spider datasets demonstrate that SQL-o1 improves execution accuracy by 10.8\% on the complex Bird dataset compared to the latest baseline methods, even outperforming GPT-4-based approaches. Additionally, SQL-o1 excels in few-shot learning scenarios and shows strong cross-model transferability. Our code is publicly available\footnote{Code: \url{https://github.com/ShuaiLyu0110/SQL-o1}}.

\end{abstract}

\section{Introduction}
Text2SQL refers to the process of converting natural language questions into Structured Query Language (SQL), serving as an effective method for non-expert users to interact with databases using natural language. The development of this field can be categorized into three stages: first, encoding and decoding input sequences using pre-trained models or abstract syntax trees~\cite{DBLP:conf/acl/WangSLPR20}; second, employing sequence-to-sequence methods~\cite{ DBLP:conf/emnlp/XieW0ZSYWZYWZWL22}; and more recently, large language models (LLMs)~\cite{DBLP:journals/fcsc/ZhangWDZC25} have been proven to be an effective solution for Text2SQL.
\begin{figure}[bht!]  % 使用figure*来跨越两栏
    \centering
    \includegraphics[width=\columnwidth]{Intro.pdf}  % 图片文件路径
    \caption{The illustrations of the differences among End-to-End Text2SQL method and Heuristic Dynamic Search with Self-Reward.}
    \label{fig:Intro}
\end{figure}
However, accurately aligning natural language queries with the data in the database remains a significant challenge.
% Text2SQL是指将自然语言问题转换为结构化查询语言（SQL）, 成为了非专家人类使用自然语言与数据库交互的重要纽带. 该领域的发展主要分为三个阶段,利用预训练模型或者抽象语法树~\cite{DBLP:conf/acl/WangSLPR20}对输入序列进行编编码和解码; 采用序列到序列~\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20, DBLP:conf/emnlp/XieW0ZSYWZYWZWL22}的方法; 最近, 大语言模型~\cite{DBLP:journals/fcsc/ZhangWDZC25}被证明是Text2SQL的有效解决方案. 然而，准确对齐自然语言问题和数据库中的数据仍然是一个不小的挑战~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23}.


Recent research on LLM-based Text2SQL has mainly focused on improving model performance through contextual learning prompt strategies and supervised fine-tuning with domain-specific data. The crucial methods in this field include Schema Linking, Self-correction, and Chain-of-Thought (CoT) \cite{DBLP:conf/emnlp/TaiCZ0023}, which aim to enhance the model's understanding of schemas, improve its reasoning ability, and assist it in generating more accurate SQL queries.
% 最近，基于大语言模型（LLM）的 Text2SQL 研究主要集中在通过上下文学习提示策略和针对特定领域数据的监督微调来提高模型性能。该领域的关键方法包括模式链接（Schema Linking）、自我纠正（Self-correction）和思维链（Chain-of-Thought, CoT） \cite{DBLP:conf/emnlp/TaiCZ0023}，这些方法旨在增强模型对模式的理解，提高其推理能力，并帮助其生成更准确的 SQL 查询。


However, there are still three main challenges: \textbf{1. These methods are often limited by the scale of the model.} Smaller LLMs have limited ability to understand complex instructions, leading to poor generalization when handling complex tasks. \textbf{2. End-to-end generation methods suffer from the limitation of the generation space.} Due to the lack of opportunities for gradual verification and flexible adjustment, the model finds it difficult to explore more potential paths during the generation process, limiting the diversity and accuracy of the output. \textbf{3. There are coherence issues in reasoning process during SQL generation.} If an error occurs at any step, it often affects the correctness of subsequent steps, resulting in the final generated SQL query being unable to execute correctly.
% 然而，仍然存在三个主要挑战：1. 这些方法通常受限于模型的规模。小型大语言模型（LLM）在理解复杂指令时能力有限，导致其在处理复杂推理任务时的通用性较差。2. 端到端生成方法存在生成空间受限的问题。由于缺乏逐步校验和灵活调整的机会，模型难以在生成过程中探索更多可能的路径，限制了输出的多样性和准确性。3. SQL 生成过程中存在推理连贯性问题。如果某一步出现错误，往往会影响到后续步骤的正确性，进而导致最终生成的 SQL 查询无法正确执行。



Inspired by work on Process-supervised Reward Model~\cite{luo2023wizardmath}, we propose SQL-o1, a Self-Reward-based heuristic search method, as shown in Figure ~\ref{fig:Intro}. First, we extensively mine the database schema, collecting table column fields, representative entities, and other information to construct a Schema-Aware dataset for fine-tuning large language models (LLMs). In addition, we introduce Monte Carlo Tree Search (MCTS)~\cite{swiechowski2023monte} as an inference medium, leveraging process-level reasoning with Self-Reward to reduce logical errors in the LLM's generation process. By expanding the generation space while overcoming the consistency challenge in SQL generation, we significantly enhance the reasoning capabilities of LLMs.
% 受到Process-supervised Reward Model~\cite{luo2023wizardmath}类o1工作的启发, 我们提出了SQL-o1, 一种具有Self-Reward启发式搜索方法. 如图xxxx所示. 我们首先充分挖掘数据库的schema,收集表格列字段, 代表性实体等信息, 构造Schema-Aware的数据供LLMs进行微调. 此外, 我们采用蒙特卡洛树作为媒介, 利用Self-Reward的过程级推理方法, 可以减少LLM推理过程中的逻辑错误, 在扩展生成空间的同时克服SQL生成一致性的难题, 将大大提高LLM的推理能力.

We conduct experiments on the Bird and Spider datasets, as well as three variants of Spider. The experimental results show that SQL-o1, when combined with common open-source models such as Llama 3~\cite{Llama3-8b} and Qwen 2.5~\cite{yang2024qwen2}, significantly outperforms most existing methods, even surpassing other GPT-4-based approaches. Moreover, we apply SQL-o1 in the few-shot fine-tuning scenario, and the results indicate that when the sample size reaches 2000, nearly all performance metrics exceed those of models fine-tuned on the full dataset. Finally, we also discuss the transferability of SQL-o1 and the contributions of its components.
% 我们在 Bird、Spider 及其三个变体数据集上进行相关实验。实验结果表明，SQL-o1 在结合常见的开源模型如 Llama 3 和 Qwen 2.5 时，表现显著优于现有的大多数方法，甚至超过了基于 GPT-4 的其他方法。此外，我们将 SQL-o1 应用到少量样本微调的场景中，实验结果表明，当样本量达到 2000 时，几乎所有性能指标都超过了全量微调的同等模型。最后，我们还探讨 SQL-o1 的可迁移性及各组件的贡献。
Our contributions can be summarized as follows:
\begin{enumerate}
    \item We extract information from multiple perspectives of the database to build a Schema-Aware dataset, facilitating Progressive SQL Generation (PSG) for LLMs.
    \item We propose SQL-o1, a Self-Reward-based heuristic search method that significantly reduces coherence issues in the SQL generation process while expanding the SQL generation space.
    \item We conduct a comprehensive evaluation and extensive experiments to fully validate the effectiveness and transferability of SQL-o1.
\end{enumerate}




% Our contributions can be summarized as follows:
% 1. 我们从数据库的多个角度挖掘信息，构建了 Schema-Aware 数据集，为大语言模型（LLM）提供了基础的执行任务能力。
% 2. 我们提出 SQL-o1，一种基于自奖励（Self-Reward）的启发式搜索方法，能够在扩展 SQL 生成空间的同时，显著减少 SQL 生成过程中的一致性问题。
% 3. 我们对所提方法进行全面评估和大量实验，充分验证 SQL-o1 的有效性和可迁移性
\section{Related Work}
Recent significant progress in the Text2SQL task has primarily focused on LLMs, as their remarkable reasoning ability provides new directions and opportunities for the Text2SQL task. Currently, methods based on LLMs can generally be divided into two categories: Prompt Engineering and Agent-based interaction with LLMs.
% 最近Text2SQL任务的重大进展主要集中在大语言模型方面, 这是因为大语言模型非凡的推理能力为Text2SQL任务提供了新的方向和机遇. 目前基于LLM的方法一般可以分为Prompt Engineering 和 大语言模型交互代理两类。



\noindent \textbf{Prompt Engineering.}
In the early stages of LLMs, a direct and effective method to better exploit the potential of LLMs was to carefully design effective prompts to guide the models, which also applies to the Text2SQL task. Enhancing the reasoning ability of LLMs through Chain of Thought~\cite{DBLP:conf/emnlp/ZhangCCX023} is a promising attempt. Several methods~\cite{wang2024macsql, DBLP:conf/nips/PourrezaR23, DBLP:journals/pacmmod/LiZLFZZWP0024} utilize schema linking to combine natural language questions with database schema elements, achieving promising results. Among them, DAIL-SQL~\cite{DBLP:journals/pvldb/GaoWLSQDZ24} systematically investigates prompt engineering for LLM-based text-to-SQL methods, including question representation, prompt components, example selection, and example organization. 

Recently, some works have shifted attention from prompt engineering (e.g., GPT-4 and other closed-source models) to fine-tuning LLMs. SENSE~\cite{yang-etal-2024-synthesizing} synthesizes strong data and performs Direct Preference Optimization (DPO) on weak data from a weak LLM, while ROUTE~\cite{qin2024route} proposes a multitask collaborative fine-tuning approach, reducing potential errors in SQL generation and achieving better results.
% 在LLMs出现的早期阶段, 为了更好地发掘LLMs的潜力, 一个直接有效的方法就是精心设计有效的Prompt来引导LLMs, 这在Text2SQL任务中也不例外. 通过思维链~\cite{DBLP:conf/emnlp/ZhangCCX023}提升LLMs的推理能力是个很好的尝试。一些方法~\cite{wang2024macsql, DBLP:conf/nips/PourrezaR23, DBLP:journals/pacmmod/LiZLFZZWP0024}利用Schema linking将自然语言问题与数据库模式元素结合起来, 取得了不错的结果. 其中DAIL-SQL~\cite{DBLP:journals/pvldb/GaoWLSQDZ24}系统地研究了基于LLM的文本到SQL方法的提示工程，包括问题表示、提示组件、示例选择和示例组织. 最近，有些工作将注意力从提示GPT4等闭源模型放到了微调LLMs上, SENSE~\cite{yang-etal-2024-synthesizing}通过综合强数据并对来自弱 LLM 的弱数据执行直接偏好优化（DPO）, ROUTE~\cite{qin2024route}提出了一种多任务协作的微调方法, 降低SQL生成潜在错误并取得了较好的结果。


\noindent \textbf{Agent-based interaction with LLMs.}
The agent-based interactive methods~\cite{DBLP:conf/acl/ChenWMP0024} guide LLMs to generate accurate SQL queries by designing feedback signals. Early works~\cite{DBLP:conf/emnlp/ShiFGZW22} focused on improving SQL based on execution results, by executing SQL queries and selecting the most accurate translation based on execution risks. Other works~\cite{DBLP:conf/iclr/ChenLSZ24, DBLP:conf/pricai/GuoTTWWYW23} use LLMs to inspect results and correct discrepancies between the generated SQL and real SQL queries. MAC-SQL~\cite{wang2024macsql} introduces a multi-agent framework and other novel interactive methods~\cite{xiong2024interactive}. However, most of these methods rely heavily on high-quality external feedback, which is often unavailable in practical applications, and they primarily depend on closed-source LLMs, overlooking the potential of open-source LLMs in reasoning.
% 基于代理的交互方法~\cite{DBLP:conf/acl/ChenWMP0024}通过设计反馈信号，引导 LLM 生成准确的 SQL 查询。早期，有些工作~\cite{DBLP:conf/emnlp/ShiFGZW22}侧重于根据执行结果改进SQL, 执行抽样SQL查询，根据执行风险选择最准确的翻译。另外一些工作~\cite{DBLP:conf/iclr/ChenLSZ24, DBLP:conf/pricai/GuoTTWWYW23}则利用LLM通过检查结果来改进SQL, 纠正SQL与真实SQL语句的差异。MAC-SQL~\cite{wang2024macsql}引入了一种多代理框架以及其他一些新的交互方法~\cite{xiong2024interactive}。然而这些方法大多需要高质量外部反馈, 这在实际应用中通常并不存在，且大多依赖闭源LLMs GPT-4, 忽视了发掘开源LLMs的推理能力.

% mcts启发式（\textbf{Self-Reward-guide} / 搜索空间不大）数据合成 R-1
% sft - Model + MCTS -> generation SQL-> prediction
% MCTS的好处 COT -> TOT(没有reward guide)
% test time scalling law / Search-o1 + KBQA-o1 -> MT

\begin{figure*}[bht!]  % 使用figure*来跨越两栏
    \centering
    \includegraphics[width=\textwidth]{sql-o1_.pdf}  % 图片文件路径
    \caption{In the search step, an example demonstrates the MCTS heuristic search guided by Self-Reward.}
    \label{fig:sql-o1}
\end{figure*}



% \section{Preamble}
\section{Preliminaries}
\subsection{Problem Formulation}
Given a Text2SQL dataset $\mathcal{D} = \{(\mathcal{D}_i, \mathcal{Q}_i, \mathcal{S}_i)\}_{i=1}^{N}$, where each sample consists of an SQL database $\mathcal{D}_i$, a natural language question $\mathcal{Q}_i$, and the corresponding ground-truth SQL query $\mathcal{S}_i$, the goal of the Text2SQL task is to use a large language model to generate an SQL query ${\mathcal{Q}_i}^{'}$ and ensure that the execution results match $\mathcal{S}_i$.

\subsection{Definition: Heuristic Dynamic Search with Self-Reward}
Heuristic Dynamic Search with Self-Reward primarily consists of a sequence of states \(\mathcal{O} = \{o_0, o_1, o_2, ..., o_{t-1}\}\), and an action sequence \(\mathcal{A} = \{a_1, a_2, ..., a_t\}\) generated based on these states. Each time an action \(a_t\) is executed, the model will receive a corresponding reward \(R_t \in \mathcal{R}\). Both rewards and actions are generated by the model \(\pi\).


% Definition: Heuristic Dynamic Search with Self-Reward主要包括状态\(\mathcal{O}={o_0, o_1, o_2, ...o_t-1}\), 基于状态生成的动作\(\mathcal{A}={a_1, a_2, ..., a_t}\), 其中每执行一次动作就会获得相应的奖励R_t \in \(\mathcal{R}\).

% 给定一个Text2SQL数据集\(\mathcal{D}=\{(d_{i},\mathcal{Q}_i,s_{i})\}_{i=1}^{N}\), 其中每个样本包括一个对应的SQL数据库\(d_{i}\), 一个自然语言问题\(\mathcal{Q}_i\), 以及一个真实的SQL查询\(s_{i}\). Text2SQL任务的目标是利用一个大语言模型\(\mathcal{M}\), 基于数据库\(d_{i}\)的详细信息和附带的自然语言问题\(\mathcal{Q}_i\)生成一个SQL查询\(s_{i}^{'}\), 并且使其在数据库上的执行结果与\(s_{i}\)一致.

\section{Methodology}
In this section, we will introduce the three components of SQL-o1: Schema-Aware Data Construction, Progressive SQL Generation, and Heuristic Dynamic Search with Self-Reward.
% 在本节中,我们将介绍SQL-o1的三个组成部分: Schema-Aware Data Construction, Progressive SQL Generation 和 Heuristic Dynamic Search with Self-Reward.

\subsection{Schema-Aware Data Construction}
\label{sec:schema_aware}
SQL-o1 needs to accurately understand the database structure and query conditions before performing heuristic dynamic search. Therefore, we design strategies such as extracting table field types and sample data entries to help the model better grasp the database schema, thereby optimizing the heuristic search process.
% SQL-o1 在进行启发式动态搜索之前需要准确理解数据库结构和查询条件。因此，我们通过设计表格字段类型、示例数据条目等策略，帮助模型掌握数据库架构，从而优化启发式搜索过程。


% 许多先前的研究工作~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024, yang-etal-2024-synthesizing}通过实验证据表明，基于监督微调（Supervised Fine-Tuning，SFT）的方法能够显著提升开源大规模语言模型（LLMs）在Text2SQL任务上的最终表现。通过使用精心设计的标注数据集进行微调，模型能够更好地学习自然语言与SQL查询之间的映射关系，从而生成更加准确和高效的SQL查询。具体而言，本节将详细介绍微调训练中数据构建的过程，以及采用Progressive Query Generation（渐进式查询生成）策略来帮助模型更好地理解复杂的SQL语法。

\noindent \textbf{Table Column Field Types.}
The data type of a column determines the values that can be stored in the field and how those values are processed. WSpecifying the data type of a column is critical when building a Text2SQL hint because different data types require different processing. For instance, numeric (NUMBER) data supports mathematical operations such as addition and averaging, while text (TEXT) data is often used for filtering and matching. These type indicators help the model generate SQL queries correctly.


% 列的数据类型决定了字段中可以存储的值及其处理方式。在构建 Text2SQL 提示时，明确列的数据类型至关重要，因为不同类型的数据需要不同的处理。例如，数值型（NUMBER）数据支持数学运算，如加法和平均值计算，而文本型（TEXT）数据则常用于过滤和匹配。这些类型指示符帮助模型正确生成 SQL 查询。

\noindent \textbf{Example entries from each table.}
Example database entries refer to small pieces of data from a database table that helps the model understand the content and structure of the data. In the Text2SQL task, the example data in the prompt assists the model in mapping the natural language query to specific database entries. For instance, to generate a query like "orders.order\_date BETWEEN '2022-01-01' AND '2022-12-31'", the model needs to understand the date format of the "order\_date" column. Similarly, for the "products.category" field, the model should recognize specific values in the "category" column, such as "Electronics" or "Clothing". By providing representative example data, the model can better understand the content and format of the columns, and thus generate SQL queries more accurately.
% 示例数据库条目指的是数据库表中的一小部分数据，有助于模型理解数据的内容和结构。在 Text2SQL 任务中，提示中的示例数据帮助模型将自然语言查询映射到具体的数据库条目。例如，为了生成像 "orders.order_date BETWEEN '2022-01-01' AND '2022-12-31'" 的查询，模型需要理解 "order_date" 列的日期格式。类似地，对于 "products.category"字段, 模型应了解 "category" 列的具体值，如 "Electronics" 或 "Clothing"。通过提供代表性的示例数据，模型能够更好地理解列的内容和格式，生成更准确的 SQL 查询。

\noindent \textbf{Key Constraints.}
Primary and foreign keys define the relationship between database tables. For example, the "ID" and "Type" columns in Table A serve as primary keys, while the "ID" in Table B connects to the "ID" in Table A, forming the relationship A.ID = B.ID. This relationship helps the model understand how to join tables and correctly retrieve data when generating queries, and is key to identifying the dependencies and join conditions between tables.
% 数据库的主键（Primary Keys）和外键（Foreign Keys）定义了表与表之间的关系。例如，在数据库中，A表的“ID”和“Type”列可能组成了主键，而B表的“ID”列则与A表中的“ID”列存在关联，形成A.ID = B.ID的关系。通过主键和外键的关系，模型能够理解如何在生成查询时将不同的表连接起来，并正确提取相关数据，从而识别表之间的依赖和联接条件，这是生成有效SQL查询的关键要素。



% \subsection{Query Completion}
\subsection{Progressive SQL Generation}
\label{sec:PSG}
Progressive SQL Generation (PSG) is a variant of supervised fine-tuning (SFT), with the core idea of truncating a complete SQL query at specific keywords during training, where the model’s task is to reconstruct the full query based on the prompt. We mainly focus on SQL queries in pre-trained large language models that either have prediction errors or complex syntactic structures. For instance, in the query 'SELECT name, age FROM employees WHERE Department = 'HR' AND salary > 50000', truncation occurs at keywords such as 'WHERE' or 'AND', rather than at arbitrary positions. If truncation happens at 'SELECT name, age FROM employees WHERE', the model is required to generate the full query from this fragment."

This incremental generation method leverages the continuous generation capability of LLMs, helping the model better understand query structure and syntax, reducing generation errors, especially when dealing with multiple joins or complex filtering conditions.
% 渐进式查询生成（Progressive Query Generation）其实是一种特殊形式的监督微调（SFT）。它的核心思想是在训练过程中将完整的SQL查询语句根据关键词进行截断，形成一个不完整的查询片段。 然后，模型的任务就是根据这些不完整的片段，生成完整的SQL查询。值得注意的是我们仅对预训练LLMs预测错误或者存在复杂查询结构的SQL语句上进行操作.例如，在完整的SQL查询语句“SELECT name, age FROM employees WHERE department = 'HR' AND salary > 50000”中，我们在关键词“WHERE”或“AND”处进行截断，而不是在任意位置进行截断。假设我们在“SELECT name, age FROM employees WHERE”处截断，那么模型的任务就是基于这个片段生成完整的SQL。在面对复杂查询时，这种逐步生成的方式帮助模型更清楚地理解查询的结构和语法，逐层构建查询的各个部分，减少生成错误的可能性，特别是在需要多次联接或复杂筛选条件时。通过这种方法，模型能够更有效地处理长查询和复杂的逻辑条件。同样地, 由PQG构造的数据集我们记作\[ D_t = \{ \sigma_t(d_i, q_i, \widtidle{s}_i), s_i \}_{i=1}^{N_t} \] 其中，\(N_t\) 表示对应的数据规模，而 \(\widtidle{s}_i\) 表示截断后的 SQL 查询。
 
Based on the previously discussed content, we have developed a basic fine-tuning dataset for LLMs, which mainly includes the content from Sections ~\ref{sec:schema_aware} and ~\ref{sec:PSG}. We represent the constructed dataset as:
% 基于之前讨论的内容，我们为LLMs开发了一个基础的微调数据集，主要包括4.1和4.2的内容。我们将构建的数据集表示为：

\[
D_s = \{ \sigma_p(\mathcal{D}_i, \mathcal{Q}_i), \mathcal{S}_i \}_{i=1}^{N_s},
\]
where \(\sigma_p\) denotes the prompt construction function we define, and \(N_s\) represents the total number of samples in the dataset.

\subsection{Heuristic Dynamic Search with Self-Reward}
The method proposed in this section integrates a reinforcement learning framework, Monte Carlo Tree Search, and Self-Reward evaluation to guide the model's decision-making process during SQL query generation. Based on the components of the algorithm, the method is primarily divided into: SQL Generation Planning, Self-Reward Evaluation, and Heuristic Dynamic Search.
% 我们提出的方法结合了强化学习框架、蒙特卡罗树搜索（MCTS）和自我奖励评估，旨在引导模型在SQL查询生成过程中的决策。根据算法的组成部分，该方法主要分为：SQL生成规划、自我奖励评估和启发式动态搜索。

\subsubsection{SQL Generation Planning}
We define the SQL query generation task as a sequential decision-making task, where the model's objective is to choose the next SQL fragment (such as a table name, column name, or SQL keyword) based on the current context. This is treated as a policy generation problem, with the objective of teaching the model a strategy that maximizes the likelihood of generating correct SQL queries:
% 我们将SQL查询生成任务定义为一个顺序决策问题，其中模型的任务是根据当前上下文选择下一个SQL令牌（例如，表名、列名或SQL关键字）。
\begin{equation}
a_t=\underset{a'_t}{\text{argmax}} \, \pi(a'_t \mid o_{t-1}).
\label{eq:policy}
\end{equation}
Equation \eqref{eq:policy} describes how the policy model \( \pi \) selects the optimal action \( a_t \) (i.e., the SQL fragment generated at step \( t \)) based on the previous state \( o_{t-1} \) (i.e., the SQL fragment generated at the previous step). Specifically, the model selects one of the possible SQL fragments \( a'_t \) that maximizes the probability \( \pi(a'_t \mid o_{t-1}) \).
% 方程 \eqref{eq:policy} 描述了策略模型 \( \pi \) 如何根据前一个状态 \( o_{t-1} \)（即上一步生成的SQL片段）选择最优动作 \( a_t \)（即在第 \( t \) 步生成的SQL片段）。具体来说，模型从所有可能的SQL片段 \( a'_t \) 中选择一个，使得概率 \( \pi (a'_t \mid o_{t-1}) \) 最大化。 

\subsubsection{Self-Reward Evaluation}
The objective of this task is to evaluate the quality and validity of the generated SQL query fragments at each step based on the current state, providing rewards and feedback signals for the decision-making process. Specifically, we propose a scoring function \( R_{\pi} \), which utilizes the log-probability values from \( \pi \) to assess the likelihood of generating output \( y \) given input \( x \):
% 该任务的目标是在每一步根据当前状态评估生成的SQL查询片段的质量和有效性, 为决策过程提供奖励及反馈信号。具体来说，我们使用一个评分函数 \( R_{\pi} \)，该函数利用来自\( \pi \) 的对数概率值, 并用于评估在给定输入 \( x \) 下生成输出 \( y \) 的可能性。

\begin{equation} 
R_{\pi}(y \mid x) = \beta + \alpha \log \pi(y \mid x),
\label{eq:reward}
 \end{equation}
where \( \beta \) is the defined full score, set to 100, and \( \alpha \) is a positive temperature value used to control the disparity of the scores.
% 其中，\( \beta \) 是设定的满分，取值为 100，\( \alpha \) 是一个正的温度参数，用于控制评分的差异性。

\subsubsection{Heuristic Dynamic Search}
Monte Carlo Tree Search (MCTS) is a powerful decision-making algorithm widely used in game theory (e.g., AlphaGo) and planning problems. As shown in Figure \ref{fig:sql-o1}, we use MCTS as a heuristic search method to guide SQL query generation. It explores and generates SQL query sequences step by step, simulates the outcomes, and optimizes the search path based on Self-Reward guidance.
% 蒙特卡罗树搜索（MCTS）是一种强大的决策算法，广泛应用于博弈论（如AlphaGo）和规划问题等领域。在Text-to-SQL任务中，我们将MCTS作为一种启发式搜索方法，通过逐步探索生成不同的SQL令牌序列，模拟其结果，并根据模型自我评估方式改进搜索路径，来指导SQL查询的生成。


\noindent \textbf{Selection.}
The selection phase of MCTS begins at the root node and traverses through child nodes until reaching the leaf node. Each node represents a decision point in the SQL query generation process, where the model selects the next valid SQL token based on the Equation \eqref{eq:policy}, gradually generating the query. At crucial syntactic and semantic decision points, the model uses heuristic truncation to expand partial queries. The UCT algorithm is then applied to guide node selection, balancing the exploration of unvisited query structures with the exploitation of high-reward paths:
% 蒙特卡罗树搜索（MCTS）中的选择阶段从根节点开始，遍历子节点，直到到达叶节点。每个节点表示SQL查询生成过程中的一个决策点，在这个决策点上，模型通过选择下一个有效的SQL令牌（例如，SELECT、FROM、列名）来增量地构造查询。该模型不是在单一步骤中生成完整的SQL查询，而是逐步扩展部分查询，在关键语法和语义决策点利用启发式截断。然后应用树的上置信度界（UCT）算法来指导下一个节点的选择，平衡未访问查询结构的探索和高回报路径的利用：

\begin{equation}
n_t = \underset{n \in \mathcal{N}(o_{t-1})}{\text{argmax}} \left[ Q(o_{t-1} + n) + w \cdot \frac{\sqrt{\ln N(o_{t-1})}}{N(o_{t-1} + n)} \right],
\end{equation}
where \( \mathcal{N}(.) \) represents the candidate expansion paths for a given state \( o_{t-1} \), \( Q(.) \) denotes the Q-value of the current state, reflecting the expected return for executing an action. \( N(.) \) indicates the visit count of agent states.
% 其中，\( E(.) \) 表示给定状态 \( h_{t-1} \) 的候选扩展路径，\( Q(.) \) 表示当前状态的 Q 值，反映了执行某个动作的预期回报。\( N(.) \) 表示代理状态的访问次数，\( w \) 是一个温度参数，用于控制探索倾向。较大的 \( w \) 鼓励探索较少访问的节点，而较小的 \( w \) 则优先考虑具有较高 Q 值的节点。, 
% !!! and \( w \) is a temperature parameter that controls the exploration tendency


\noindent \textbf{Expansion.}
The selection process chooses the most relevant SQL query from the candidate extensions. When the maximum query depth \( L \) has not been reached, the model continues to expand the query by exploring the next possible SQL operation or clause:
\begin{equation}
\left\{ n_t^{(b)} \right\}_{b=1}^B \sim \pi \left( n_t | o_{t-1} \right)_{Beam},
\end{equation}
where, \(\pi(.)_{Beam}\)\ represents the Beam Search algorithm, and \(B\) is the beam width. Then, the model selects the most relevant SQL operation for expansion based on the semantic similarity with the previous query fragments: 
% \begin{equation}
% \mathcal{N}(s_{t-1})=\left\{ n_t^{(i)} \right\}_{i=1}^d \leftarrow \underset{d}{\text{argmax}} R_{\pi_{\text{policy}}} \left( \left\{ n_t^{(b)} \right\}_{b=1}^B \mid s_{t-1} \right),
% \end{equation}
\begin{multline}
\mathcal{N}(o_{t-1}) = \left\{ n_t^{(i)} \right\}_{i=1}^d \leftarrow \underset{d}{\text{argmax}} \\
R_{\pi} \left( \left\{ n_t^{(b)} \right\}_{b=1}^B \mid o_{t-1} \right),
\end{multline}
where \( R_{\pi} \) represents the reward function that evaluates the quality of each candidate expansion and d < $B$. For example, if the current state is a partial query on the "user" table, the model might generate "SELECT user.id" or "SELECT user.name" and select candidates based on their semantic relevance to the input question.

\noindent \textbf{Simulation and Back-propagation.}
After expanding the nodes, the model assigns scores to all newly added child nodes, as indicated by Equation \eqref{eq:reward} and \eqref{eq:Q_value}. According to Equation \eqref{eq:Q_value_max}, the node with the highest score is selected for further simulation, continuing until the final state is reached, thus generating the complete SQL query generation trajectory.
% 当 MCTS 完成一次查询生成的 模拟（Simulation）并达到最终状态时，模型会评估整个查询路径的质量，并将这一信息反向传播给路径上的所有节点，具体计算公式如下:
% \begin{equation}
% Q(s_t^{(n)}) \leftarrow \delta R_{\pi} (n_t \mid s_{t-1}^{(n)}) + (1 - \delta) R_{\pi} (SQL_{s_t^{(n)}} \mid Q) 
% \end{equation}
\begin{equation} 
Q(o_l^{(n)}) = \delta R_{\pi} (n_l \mid o_{l-1}^{(n)}) + (1 - \delta) R_{\pi} (\mathcal{S} \mid \mathcal{Q}),
\label{eq:Q_value}
\end{equation}
were \( \delta \) is a parameter between \( (0, 1) \), used to balance the process score and the overall score, and is often set to 0.5. The algorithm then performs backpropagation by updating the Q-values of all nodes along the trajectory from the leaf node to the root node.
\begin{equation} 
Q(o_t^{(n)}) = \max_{j=1}^{n} \left( \frac{\sum_{i=l}^{t} Q(o_i^{(j)})}{l - t + 1} \right).
\label{eq:Q_value_max}
\end{equation}
The parent node's Q-value is updated to the maximum average Q-value of its child nodes, and the visit count of each node along the path is incremented by 1 before the next simulation.

\subsubsection{Query Trajectory Optimization}
During generation, the model uses MCTS to perform \( N \) simulations, selecting the state with the highest Q-value in each simulation as the optimal trajectory. This method progressively optimizes query generation, yielding the optimal SQL query for question \( \mathcal{Q} \).
\begin{equation} 
\mathcal{S} \leftarrow \tilde{\mathcal{S}} = \text{MCTS}(\mathcal{Q}, \pi),
\end{equation}
where \( \tilde{\mathcal{S}}_l \) is the optimal SQL generated for the query \( \mathcal{Q} \).
% 其中 \( \tilde{\mathcal{S}}_l \) 是针对问题\mathcal{Q}生成的最佳SQL.

\begin{table*}[bth!]
\centering
\caption{We compare the performance of different models and methods on the Spider and Bird datasets. To ensure fairness, we only report the results of ROUTE with the same-sized open-source models that we used. The \textbf{bolded} numbers represent the best performance.}
\label{table:1}
\begin{tabular}{cccccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Model / Method}}             & \multicolumn{3}{c|}{Spider}                    & \multicolumn{2}{c}{Bird} \\ \cline{2-6} 
\multicolumn{1}{c|}{}                                            & Dev-EX & Dev-TS & \multicolumn{1}{c|}{Test-EX} & Dev-EX     & Dev-VES     \\ \hline
\multicolumn{6}{c}{\textit{\textbf{Prompting with Closed-Source LLMs}}}                                                                                   \\ \hline
\multicolumn{1}{c|}{GPT-4~\cite{achiam2023gpt}}                 & 72.9   & 64.9   & \multicolumn{1}{c|}{-}       & 46.4       & 49.8        \\
\multicolumn{1}{c|}{DIN-SQL + GPT-4~\cite{DBLP:conf/nips/PourrezaR23})} & 82.8   & 74.2   & \multicolumn{1}{c|}{85.3}    & 50.7       & 58.8        \\
\multicolumn{1}{c|}{DAIL-SQL + GPT-4~\cite{dail_sql}}        & 83.5   & 76.2   & \multicolumn{1}{c|}{86.6}    & 54.8       & 56.1        \\
\multicolumn{1}{c|}{MAC-SQL + GPT-4~\cite{wang2024macsql}}         & 86.8   & -      & \multicolumn{1}{c|}{82.8}    & 59.4       & 66.2        \\
\multicolumn{1}{c|}{MCS-SQL + GPT-4~\cite{lee-etal-2025-mcs}}          & 89.5   & -      & \multicolumn{1}{c|}{89.6}    & 63.4       & 64.8        \\ \hline
\multicolumn{6}{c}{\textit{\textbf{Prompting with Open-Source LLMs}}}                                                                        \\ \hline
\multicolumn{1}{c|}{Mistral-7B~\cite{jiang2023mistral}}             & 56.8   & 47.3   & \multicolumn{1}{c|}{60.1}    & 22.5       & 27.8        \\
\multicolumn{1}{c|}{Llama3-8B~\cite{Llama3-8b}}            & 69.3   & 58.4   & \multicolumn{1}{c|}{69.1}    & 32.1       & 31.6        \\
\multicolumn{1}{c|}{Qwen2.5-7B~\cite{yang2024qwen2}}             & 72.5   & 64.0   & \multicolumn{1}{c|}{75.9}    & 41.1       & 42.0        \\
\multicolumn{1}{c|}{Qwen2.5-14B~\cite{yang2024qwen2}}            & 76.9   & 66.3   & \multicolumn{1}{c|}{78.4}    & 48.4       & 49.2        \\
\multicolumn{1}{c|}{DIN-SQL + Llama3-8B}                         & 48.7   & 39.3   & \multicolumn{1}{c|}{47.4}    & 20.4       & 24.6        \\
\multicolumn{1}{c|}{DIN-SQL + Qwen2.5-7B}                        & 72.1   & 61.2   & \multicolumn{1}{c|}{71.1}    & 30.1       & 32.4        \\
\multicolumn{1}{c|}{MAC-SQL + Llama3-8B}                         & 64.3   & 52.8   & \multicolumn{1}{c|}{65.2}    & 40.7       & 40.8        \\
\multicolumn{1}{c|}{MAC-SQL + Qwen2.5-7B}                        & 71.7   & 61.9   & \multicolumn{1}{c|}{72.9}    & 46.7       & 49.8        \\
\multicolumn{1}{c|}{ROUTE + MCP + Llama3-8B}                             & 75.0   & 63.4   & \multicolumn{1}{c|}{72.0}    & 42.7       & 44.8        \\
\multicolumn{1}{c|}{ROUTE + MCP + Qwen2.5-7B}                            & 78.3   & 67.2   & \multicolumn{1}{c|}{78.7}    & 49.7       & 52.8        \\
\multicolumn{1}{c|}{ROUTE + MCP + Qwen2.5-14B}                           & 80.0   & 67.3   & \multicolumn{1}{c|}{80.6}    & 56.3       & 57.6        \\ \hline
\multicolumn{6}{c}{\textit{\textbf{Fine-Tuning with Open-Source LLMs}}}                                                                      \\ \hline
\multicolumn{1}{c|}{Llama3-8B + SFT~\cite{Llama3-8b}}      & 82.4   & 76.2   & \multicolumn{1}{c|}{83.1}    & 53.1       & 59.0        \\
\multicolumn{1}{c|}{Qwen2.5-7B + SFT~\cite{yang2024qwen2}}       & 80.9   & 75.6   & \multicolumn{1}{c|}{82.8}    & 51.4       & 53.1        \\
\multicolumn{1}{c|}{DTS-SQL-7B~\cite{DST-SQL}}      & 82.7   & 78.4   & \multicolumn{1}{c|}{82.8}    & 55.8       & 60.3        \\
\multicolumn{1}{c|}{CODES-7B + SFT~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024}}           & 85.4   & 80.3   & \multicolumn{1}{c|}{-}       & 57.2       & 58.8        \\
\multicolumn{1}{c|}{CODES-15B + SFT~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024}}          & 84.9   & 79.4   & \multicolumn{1}{c|}{-}       & 58.5       & 56.7        \\
\multicolumn{1}{c|}{SENSE-7B~\cite{yang-etal-2024-synthesizing}}               & 83.2   & 81.7   & \multicolumn{1}{c|}{83.5}    & 51.8       & -           \\
\multicolumn{1}{c|}{SENSE-13B~\cite{yang-etal-2024-synthesizing}}              & 84.1   & 83.5   & \multicolumn{1}{c|}{\textbf{86.6}}    & 55.5       & -           \\
\multicolumn{1}{c|}{ROUTE + Llama3-8B~\cite{qin2024route}}                           & 86.0   & 80.3   & \multicolumn{1}{c|}{83.9}    & 57.3       & 60.1        \\
\multicolumn{1}{c|}{ROUTE + Qwen2.5-7B~\cite{qin2024route}}                          & 83.6   & 77.5   & \multicolumn{1}{c|}{83.7}    & 55.9       & 57.4        \\
% \multicolumn{1}{c|}{ROUTE + Qwen2.5-14B~\cite{qin2024route}}                         & 87.3   & \textbf{80.9}   & \multicolumn{1}{c|}{\textbf{87.1}}    & 60.9       & 65.2        \\
\multicolumn{1}{c|}{\textbf{Ours:} \textbf{SQL-o1 + Llama3-8B}}             & \textbf{87.4}   & \textbf{79.6}   & \multicolumn{1}{c|}{85.4}    & 63.4       & 64.7        \\ 
\multicolumn{1}{c|}{\textbf{Ours:} \textbf{SQL-o1 + Qwen2.5-7B}}            & 84.7   & 78.5   & \multicolumn{1}{c|}{85.1}    & \textbf{66.7}       & \textbf{70.4}        \\ 

\hline
\end{tabular}
\end{table*}


\section{Experiments}
\subsection{Experimental Settings}
\noindent \textbf{Datasets.}
All experiments are conducted on the Spider~\cite{yu-etal-2018-spider} and Bird~\cite{Bird} datasets. The Spider dataset consists of 7,000 training Text-SQL pairs, 1,034 development pairs, and 2,147 test pairs, covering nearly 200 databases and 138 domains. Bird is a recently introduced benchmark that contains 9,428 training pairs, 1,534 development pairs, and 1,789 test pairs in total. Compared to Spider, Bird contains more complex databases, more challenging questions, and incorporates external knowledge, making it more difficult than Spider.

\vspace{3pt}
\noindent \textbf{Baselines.}
Similar to previous work~\cite{qin2024route, yang-etal-2024-synthesizing}, our baseline methods can be divided into three categories: Prompting with Closed-Source LLMs, Prompting with Open-Source LLMs, and Fine-Tuning with Open-Source LLMs. 
The Prompting with Closed-Source LLMs mainly include DIN-SQL + GPT-4~\cite{DBLP:conf/nips/PourrezaR23}, MAC-SQL + GPT-4~\cite{wang2024macsql}, DAIL-SQL + GPT-4~\cite{gao2023text}, and MCS-SQL + GPT-4~\cite{lee-etal-2025-mcs}. Fine-Tuning with Open-Source LLMs mainly include MAC-SQL and ROUTE-MCP~\cite{qin2024route}. Finally, the fine-tuning methods using open-source LLMs involve models fine-tuned on the Spider and Bird training sets. Apart from CODES~\cite{CODES}, SENSE~\cite{yang-etal-2024-synthesizing}, and ROUTE~\cite{qin2024route}, we report results from fine-tuning open-source models using the data constructed in Section \ref{sec:schema_aware}.
% 跟之前的工作类似，我们的基线可分为三类，使用闭源 LLM 的提示方法、使用开源 LLM 进行提示的方法与使用开源 LLM 进行微调的方法. 使用闭源 LLM 的提示方法主要包括DIN-SQL + GPT-4、MAC-SQL + GPT-4、MCS-SQL + GPT-4. 使用开源 LLM 进行提示的方法主要包括MAC-SQL与MCP。最后一组使用开源 LLM 进行微调的方法是专门在Spider和Bird训练集上微调的基本LLM, 除了CODES、SENSE及ROUTE外，我们报告了用\ref{sec:sft}小节构造的数据微调开源模型的结果。

\vspace{5pt}
\noindent \textbf{Evaluation Metrics.}
In our experiments, we follow previous works and use execution accuracy (EX) and test-suite accuracy (TS) to evaluate the performance of Text2SQL on Spider and its variant datasets. Specifically, for the Bird dataset, we report only EX as required by its official settings and introduce a new metric called Valid Efficiency Score (VES) to measure query execution efficiency. In addition, except for being unable to report TS on the Spider-DK and Spider test sets, we report EX and TS on all other datasets. It is important to note that higher values for all metrics indicate better model performance.






\subsection{Main Result}
\noindent \textbf{Results on Bird.}
As shown in Table~\ref{table:1}, prompt-based open-source model methods achieve better results compared to pre-trained models. However, due to limitations in model size and training data, they still lag behind closed-source prompt-based models in performance. Among the fine-tuning approaches using open-source models, our method, when combined with open-source models like Llama3-8B and Qwen2.5-7B, shows a marked advantage. Specifically, SQL-o1(Qwen2.5-7B) achieves a score of 66.7\% on the Dev-EX metric, outperforming ROUTE + Qwen2.5-7B (55.9\%). Furthermore, on the Dev-VES metric, SQL-o1 even surpasses the performance of methods utilizing GPT-4, demonstrating strong performance in handling complex and challenging tasks.
% 从表~\ref{table:1}可以看出, 基于提示的开源模型方法相比初始模型都取得了不错的效果，但受限于模型大小和训练数据与基于提示的闭源模型的方法相比仍有一定差距。在基于开源模型进行微调的方法中，我们的方法与开源模型(Llama3-8B、Qwen2.5-7B)展现了显著的优势, 其中,SQL-o1 + Qwen2.5-7B在Bird Dev-EX指标中获得66.7%的得分，领先于ROUTE + Qwen2.5-7B（55.9%）和ROUTE + Qwen2.5-14B（60.9%），Bird Dev-VES指标上，甚至超过了使用GPT4帮助的方法的性能，整体表现优于多个基线模型。
% 

\vspace{5pt}
\noindent \textbf{Results on Spider and its variants.}
In Table~\ref{table:1} and Table~\ref{table:2}, SQL-o1 performs better than ROUTE on average when using open-source models of the same size. It is worth noting that the performance on Spider is not as outstanding as on Bird, which can be attributed to the fact that our method relies on MCTS for heuristic search. MCTS is more effective for handling complex queries, which further supports the hypothesis that MCTS excels in dealing with intricate and computationally demanding tasks. In Table~\ref{table:2}, even without the use of additional training data, SQL-o1 still outperforms ROUTE and other baselines by approximately 1\% to 5\% on the Spider-derived dataset, which reveals its robustness.
% 在表\ref{table:1}和表\ref{table:2}中，当使用相同大小的开源模型时，我们的方法平均优于ROUTE。即使与ROUTE + Qwen2.5-14B相比，我们的方法和他们的方法之间的性能差异也相对较小。值得注意的是，Spider的性能不如Bird的出色，这可以归因于我们的方法依赖于MCTS进行启发式搜索。这种方法在处理复杂查询时更有效，这进一步支持了MCTS擅长处理复杂和计算要求高的任务的假设。表2显示, SQL-o1即使没有使用额外的数据训练, 在Spider衍生数据集上的整体性能依旧超过ROUTE跟其他基线1%~5%左右, 也说明了SQL-o1的鲁棒性.
% 表2显示, SQL-o1即使没有使用额外的数据训练, 在Spider衍生数据集上的整体性能依旧超过ROUTE跟其他基线1%~5%左右, 也说明了SQL-o1的鲁棒性.
% Even when compared to ROUTE + Qwen2.5-14B, the performance difference between our method and theirs is relatively small.


\begin{table}[htbp]
\centering
\caption{Evaluation of SQL-o1 and other previously proposed methods on Spider-based robustness benchmarks, including Spider-SYN, REALISTIC, and Spider-DK. TS results for DK are not reported due to compatibility issues. FS: Few-shot. FT: Fine-Tuned.}
\label{table:2}
\resizebox{\linewidth}{!}{
\fontsize{27}{12}\selectfont % 增大字体
\renewcommand{\arraystretch}{3.8} % 放宽行高
\begin{tabular}{c|cc|cc|c|c}
\hline
\multirow{2}{*}{Model / Method} & \multicolumn{2}{c|}{SYN}                & \multicolumn{2}{c|}{REALISTIC}          & $\boldsymbol{D K}$ & \multirow{2}{*}{Avg.} \\ \cline{2-6}
                                & $\boldsymbol{E X}$ & $\boldsymbol{T S}$              & $\boldsymbol{E X}$ & $\boldsymbol{T S}$ & $\boldsymbol{E X}$ &                       \\ \hline
SQL-PaLM (FS)                   & 74.6               & 67.4  & 77.6               & 72.4               & 66.5               & 71.7                  \\
SQL-PaLM (FT)                   & 70.9               & 66.4               & 77.4               & 73.2               & 67.5               & 71.1                  \\
SENSE-7B                        & 72.6               & 64.9               & 82.7               & 75.6               & 77.9               & 74.7                  \\
ROUTE (Llama3-8B)                & 77.4               & \textbf{70.2}               & 80.9               & 72.6               & 74.6               & 75.1                  \\
\textbf{SQL-o1  + Llama3-8B}               & \textbf{77.6}               & 69.2               & \textbf{82.7}               & \textbf{72.8}               & \textbf{78.7}               & \textbf{76.2}                                 \\ \hline
\end{tabular}}
\end{table}


\subsection{Ablation Study}
As shown in Table~\ref{table:3}, the ablation study on Llama-3-8B-based SQL-o1 investigates the impact of various components on the model's performance. The results indicate that each module plays a crucial role in the overall performance of the model. Specifically, removing the MCTS optimization leads to a significant drop in performance, particularly on the Bird-Dev dataset, where the score decreases substantially. Similarly, excluding PSG results in a slight decrease in performance, especially on the Spider-Dev EX and TS sets. The most significant performance decline occurs when the initial annotated supervised fine-tuning (SFT) is removed, with the scores dropping notably across both the Spider-Dev and Bird-Dev datasets.

\begin{table}[hbt!]
\centering
\caption{Evaluation results of SQL-o1 based on Llama-3-8B, including the main results with ablation study and exploratory experiments.}
\label{table:3}
\begin{tabular}{cccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Model / Method}}   & \multicolumn{2}{c|}{Spider-Dev}  & Bird- \\
\multicolumn{1}{c|}{}                         & EX   & \multicolumn{1}{c|}{TS}   & Dev  \\ \hline
\multicolumn{4}{c}{\textit{\textbf{Main Results Ablation Experiment}}}                                 \\ \hline
\multicolumn{1}{c|}{\textbf{SQL-o1 + Llama3-8b}}        & \textbf{87.4} & \multicolumn{1}{c|}{\textbf{79.6}} & \textbf{63.4} \\
\multicolumn{1}{c|}{w/o \textit{MCTS}}                 & 79.8 & \multicolumn{1}{c|}{74.3} & 52.0 \\
\multicolumn{1}{c|}{w/o \textit{PSG}}                  & 78.2 & \multicolumn{1}{c|}{73.4} & 61.8 \\
\multicolumn{1}{c|}{w/o \textit{initial annotated SFT}} & 72.8 & \multicolumn{1}{c|}{62.2} & 35.5 \\ \hline
\multicolumn{4}{c}{\textit{\textbf{Exploratory Experiments}}}                                             \\ \hline
\multicolumn{1}{c|}{Llama3-8b}                & \textbf{72.8} & \multicolumn{1}{c|}{62.2} & 35.5 \\
\multicolumn{1}{c|}{w/ \textit{MCTS}}                  & 70.1 & \multicolumn{1}{c|}{\textbf{63.1}} & \textbf{37.2} \\ \hline
\end{tabular}
\end{table}


\begin{figure}[h]
    \centering
    % First image (labeled as a)
    \begin{minipage}{0.9\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig_t_left.pdf}  % First image
        \subcaption{CodeLlama-7B (CL-7B)} \label{fig:left}
    \end{minipage}

    % \vspace{1em}  % Add some vertical space between the two images

    % Second image (labeled as b)
    \begin{minipage}{0.9\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{fig_t_right.pdf}  % Second image
        \subcaption{Deepseek-Coder-7B (DS-7B)} \label{fig:right}
    \end{minipage}

    \caption{The transferability results on different open-source LLMs on Spider and Bird.}
    \label{fig:transfer}
\end{figure}
Exploratory experiments show that pre-trained Llama-3-8B performs poorly without SQL-o1, highlighting the importance of the full model. Additionally, when MCTS optimization is applied to the pre-trained Llama-3-8B, there is a slight performance improvement, but it still falls short of the levels achieved by SQL-o1. These results indicate that MCTS optimization, PSG, and initial supervised fine-tuning are crucial factors behind SQL-o1's strong performance on the Spider and Bird datasets. This combination of strategies enhances the model's ability to handle complex queries and navigate limited data scenarios, resulting in more accurate SQL query generation. By leveraging the strengths of each component, the model is better equipped to make informed decisions, even with incomplete inputs. These improvements in reasoning accuracy highlight the potential of SQL-o1 in real-world applications.

\subsection{Analysis of Transferability}
We conduct portability experiments on several open-source models, such as CodeLlama and Deepseek-Coder. The experimental results are shown in Figure ~\ref{fig:transfer}. Without the heuristic MCTS exploration, both CodeLlama and Deepseek-Coder suffer a performance loss, leading to suboptimal results. While the results of other experiments meet expectations, the curves in the figure reveal that with the help of MCTS, the performance improvement of CodeLlama and Deepseek-Coder on the Bird-Dev dataset is higher than that on the Spider-Dev dataset. This suggests that Heuristic Dynamic Search with Self-Reward is more effective in handling complex tasks or incomplete data scenarios.
% 我们在不同的开源模型上(如 CodeLlama, Deepseek-Code)进行了可移植性实验. 实验结果如图xx所示, 在没有启发式MCTS探索的情况下, CodeLlama 和 Deepseek-Coder丧失了一部分性能, 导致结果不佳. 在其他情况符合预期的同时, 我们通过图中的曲线可以观察到在MCTS的帮助下,CodeLlama 和 Deepseek-Coder在Bird-Dev数据集上的提升是要高于Spider-Dev数据集的, 这表明Heuristic Dynamic Search with Self-Reward更擅长处理复杂任务或不完全数据的情况.

\begin{figure}[bht!]  % 使用figure*来跨越两栏
    \centering
    \includegraphics[width=\columnwidth]{model_performances.pdf}  % 图片文件路径
    \caption{Performance comparison of SQL-o1 and Fully Supervised Fine-Tuned (FSFT) Llama-3-8B on different datasets and sample sizes.}
    \label{fig:fewsample}
\end{figure}

\subsection{Model Performance vs. Sample Size}
we compare the performance of the SQL-o1 model with the fully fine-tuned Llama-3-8B model on the Spider and Bird datasets, specifically on the Spider-Dev-EX, Spider-Dev-TS, and Bird-Dev-EX subsets. As shown in Figure~\ref{fig:fewsample}, SQL-o1 outperforms Llama-3-8B when the sample size is small (e.g., 2000 to 5000 samples), and its advantage becomes more pronounced on the Spider-Dev-EX and Bird-Dev datasets as the sample size increases. It is noteworthy that SQL-o1 is particularly effective in few-shot learning tasks, where it makes better use of limited data. This highlights SQL-o1’s superiority in resource-constrained scenarios, where it can significantly enhance the model's reasoning capabilities, particularly in real-world applications with scarce or limited data.




% 接下来，模型根据与之前生成的片段的语义相似度选择最相关的SQL操作，从而对扩展进行细化。例如，如果当前状态对应于关于“user”表的部分查询，则模型可能生成诸如“SELECT user.item”之类的候选项。id”或“SELECT user.name”，然后根据它们与输入问题的语义相关性对它们进行过滤。


% 反向传播与奖励设计
% 在树节点生成过程中, 我们会基于当前节点的状态(SQL语句生成情况)给予奖励. 具体来讲, 我们使用Self-Reward机制，即通过模型自身对SQL查询与Prompt的相关程度和生成质量来进行奖励调整，所以每个动作



% 通过这种方式，模型能够更有针对性地生成SQL语句的每个部分，从而提高生成的准确性和结构化程度


% \section*{Limitations}

% Since December 2023, a "Limitations" section has been required for all papers submitted to ACL Rolling Review (ARR). This section should be placed at the end of the paper, before the references. The "Limitations" section (along with, optionally, a section for ethical considerations) may be up to one page and will not count toward the final page limit. Note that these files may be used by venues that do not rely on ARR so it is recommended to verify the requirement of a "Limitations" section and other criteria with the venue in question.

\section{Conclusion}
We propose SQL-o1, a Self-Reward-based heuristic search method that enhances the reasoning ability of large language models (LLMs) in Text-to-SQL tasks. By combining Monte Carlo Tree Search (MCTS) and Schema-Aware datasets, SQL-o1 improves SQL generation accuracy. Experimental results show a 10.8\% improvement in execution accuracy on the Bird dataset, outperforming GPT-4-based methods. SQL-o1 also excels in few-shot learning and cross-model transfer. Ablation studies validate the effectiveness of each component of SQL-o1. In the future, we plan to explore reinforcement learning techniques for continual learning (DPO) and multi-agent systems.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\clearpage
\section*{Limitations}
We analyze the reasoning complexity and processing efficiency of SQL-o1 in Appendix. These analyses reveal that SQL-o1 still has significant room for improvement.
% 我们在附录~\ref{sec:Complexity_Ana} 和 ~\ref{sec:MCTS_iter}中分析了SQL-o1的推理复杂度跟处理效率. 这些揭示了SQL-o1仍然具有显著的改进空间.


\section*{Ethics Statement}
This paper investigates Text-to-SQL using large language models and heuristic search methods. We use publicly available datasets and ensure that our research adheres to ethical guidelines. No sensitive or private data is involved, and our methods focus solely on improving SQL generation performance. Therefore, we believe our approach does not violate any ethical standards.

\bibliography{sql-o1}

% \appendix
%
% \clearpage
% \section{Appendix}
% \label{sec:appendix}
% Our Schema-Aware dataset consists of the Spider and Bird datasets, with a total of 16,428 data pairs. The Progressive SQL Generation dataset is specifically designed for cases where the pre-trained model makes prediction errors or encounters complex SQL structures, containing 4,826 data pairs. As a result, the final dataset includes a total of 21,254 data pairs.
% % 我们的Schema-Aware数据集主要由Spider和Bird组成, 共有16428个数据对. Progressive SQL Generation主要针对预训练模型预测错误或具有复杂SQL语句结构的情况构建，共有4826对数据. 所以我们最终一共有21254对数据。
%
% \subsection{Complexity Analysis of SQL-o1}
% \label{sec:Complexity_Ana}
% The time complexity of the MCTS process in SQL-o1 can be analyzed through its four phases: selection, expansion, simulation, and backpropagation. In the selection phase, the algorithm traverses the search tree up to depth $L$ and selects the optimal node using the Upper Confidence Bound for Trees (UCT), resulting in a complexity of $O(k \cdot L)$ for each rollout, where $k$ represents the number of possible actions at each step. In the expansion phase, $B$ candidate beams are generated and filtered based on similarity, adding $O(k \cdot \omega)$ complexity, where $\omega \in [0, 1]$. In the simulation phase, the algorithm explores the path to depth $L$, contributing $O(k \cdot L)$ complexity. Finally, in the backpropagation phase, the time required to update the rewards along the path is $O(L)$.
%
% Summing up these steps, the complexity of a single rollout is $O(k \cdot L)$, and for $N$ rollouts, the total complexity of MCTS is $O(N \cdot k \cdot L)$, which grows linearly with the number of rollouts and the search depth.
%
% \subsection{Impact of MCTS iterations on query processing rate}
% \label{sec:MCTS_iter}
% Figure~\ref{fig:mcts_n} shows the impact of MCTS iterations during heuristic search on the number of queries processed per minute. We conducted experiments using Llama3-8b as the base model on the Spider-Dev and Bird-Dev datasets. The results demonstrate that the EX metric improves progressively as the number of iterations increases. This reflects the fundamental nature of heuristic search, where the accuracy of SQL generation is constrained by the search space, which expands proportionally with the number of attempts. On the Spider dataset, the EX value peaks at 8 iterations, with approximately 2.8 queries processed per minute. As the iteration count increases, the EX value remains relatively unchanged. The results on the Bird dataset are similar to those on the Spider dataset, with the EX value being nearly the same at 8 iterations as it is at its peak. Considering both retrieval efficiency and generation accuracy, we set the number of MCTS iterations in SQL-o1 to 8.
% % 图~\ref{fig:mcts_n}展示了MCTS在启发式搜索过程中对每分钟查询处理次数的影响。我们使用Llama3-8b作为基座模型，在Spider-Dev和Bird-Dev数据集上进行了相关实验。结果表明，随着迭代次数的增加，EX指标持续提高，这符合启发式搜索的特性，因为SQL生成准确率受到搜索空间的限制，而搜索空间与尝试次数成正比。在Spider数据集上，当迭代次数为8时，EX值达到最高，每分钟处理约2.8个查询；随着迭代次数的增加，EX值几乎没有变化。Bird数据集的情况与Spider相似，迭代次数为8时，EX值接近最大值。考虑到检索效率和生成准确率，我们将SQL-o1中的MCTS迭代次数设置为8。
%
%
% \begin{figure}[h]
%     \centering
%     % First image (labeled as a)
%     \begin{minipage}{0.9\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{latex/mcts_iterations_Spider.pdf}  % First image
%         \subcaption{Spider-Dev} \label{fig:left1}
%     \end{minipage}
%
%     % \vspace{1em}  % Add some vertical space between the two images
%
%     % Second image (labeled as b)
%     \begin{minipage}{0.9\columnwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{latex/mcts_iterations_Bird.pdf}  % Second image
%         \subcaption{Bird-Dev} \label{fig:right1}
%     \end{minipage}
%
%     \caption{The transferability results on different open-source LLMs on Spider and Bird.}
%     \label{fig:mcts_n}
% \end{figure}
%
% \section{Comparison of Performance with Different Hardness}
% To conduct a more detailed performance comparison, we report the EX scores of SQL-01 on Spider and Bird development sets at different hardness levels. By analyzing the results in Tables \ref{table:A_1} and \ref{table:A_2}, it is clear that SQL-01 consistently ranks among the top across all hardness levels, and the conclusions regarding overall performance and granular performance are very consistent. This indicates that SQL-01 maintains stable and excellent performance regardless of the task hardness. Furthermore, we observe that the open-source model used by SQL-01 performs very well on both datasets, even outperforming many larger models. Notably, on the Bird validation set, SQL-01's performance is nearly on par with the GPT-4 model, further validating the effectiveness and superiority of our approach in handling complex tasks.
% % 为了探索更细粒度的性能比较，我们详细报告了SQL-o1在不同Hardness下在Spider 和 Bird 开发集上的 EX 分数。从表~\ref{table:A_1}和表~\ref{table:A_2}所示的结果来看，我们的SQL-o1的在各个hardness基本名列前茅, 总体性能与其细粒度性能的结论是一致的. 另外，我们不仅发现SQL-o1使用开源模型在两个数据集上比尺寸大的模型表现要好，特别地，在Bird验证集上的表现几乎与使用GPT-4帮助的模型性能相差无几。这验证了我们的方法的优越性。
% \begin{table*}[hbt!]
% \centering
% \caption{Performance (EX) comparison under different hardness levels on the Spider development set. \textbf{Bold} numbers represent the highest performance.}
% \label{table:A_1}
% \begin{tabular}{cccccc}
% \hline
% Methods                                & Easy & Medium & Hard & Extra & All  \\ \hline
% \multicolumn{6}{c}{\textit{Prompting-based methods}}                                  \\ \hline
% C3-SQL + ChatGPT~\cite{DBLP:journals/corr/abs-2307-07306}   & 92.7 & 85.2   & 77.6 & 62.0  & 82.0 \\
% DAIL-SQL + GPT-4 + Self-consistency    & 91.5 & 90.1   & 75.3 & 62.7  & 83.6 \\
% SuperSQL + GPT-4~\cite{DBLP:journals/pvldb/LiLCLT24}    & 94.4 & 91.3   & 83.3 & 68.7  & 87.0 \\
% MCS-SQL + GPT-4~\cite{lee-etal-2025-mcs}     & 94.0 & 93.5   & 88.5 & 72.9  & 89.5 \\ \hline
% \multicolumn{6}{c}{\textit{Fine-tuning-based methods}}                                \\ \hline
% RESDSQL-3B~\cite{DBLP:conf/aaai/Li00023}                             & 94.8 & 97.7   & 73.0 & 56.0  & 81.8 \\
% DTS-SQL~\cite{DBLP:conf/emnlp/PourrezaR24}                                & 92.7 & 90.1   & 74.1 & 56.6  & 82.7 \\
% RESDSQL-3B + NatSQL~\cite{DBLP:conf/aaai/Li00023} & 94.4 & 87.9   & 77.0 & 66.3  & 84.1 \\
% ROUTE + Llama3-8B~\cite{qin2024route}                      & 96.0 & 93.0   & 75.3 & 63.3  & 86.0 \\
% ROUTE + Qwen2.5-7B~\cite{qin2024route}                     & 92.7 & 89.7   & 77.0 & 60.2  & 83.6 \\
% ROUTE + Qwen2.5-14B~\cite{qin2024route}                    & 94.0 & \textbf{93.0}   & \textbf{81.6} & 68.1  & 87.3 \\
% \textbf{Ours: SQL-o1 + Llama3-8B}                      & \textbf{94.4} & \textbf{93.0}   & 81.0 & \textbf{68.7}  & \textbf{87.4} \\ \hline
% \end{tabular}
% \end{table*}
%
% \begin{table*}[hbt!]
% \centering
% \caption{Performance (EX) comparison under different hardness levels on the Bird development set. \textbf{Bold} numbers represent the highest performance.}
% \label{table:A_2}
% \begin{tabular}{ccccc}
% \hline
% Methods                              & Simple & Moderate & Challenging & All  \\ \hline
% \multicolumn{5}{c}{\textit{Prompting-based methods}}                                   \\ \hline
% C3-SQL + ChatGPT~\cite{DBLP:journals/corr/abs-2307-07306} & 58.9   & 38.5     & 31.9        & 50.2 \\
% DAIL-SQL + GPT-4 & 62.5   & 43.2     & 37.5        & 54.3 \\
% DAIL-SQL + GPT-4 + Self-consistency  & 63.0   & 45.6     & 43.1        & 55.9 \\
% SuperSQL + GPT-4~\cite{DBLP:journals/pvldb/LiLCLT24}  & 66.9   & 46.5     & 43.8        & 58.5 \\
% MAC-SQL + GPT-4~\cite{wang2024macsql}  & 65.7   & 52.7     & 40.3        & 59.4 \\
% MCS-SQL + GPT-4~\cite{lee-etal-2025-mcs}   & 70.4   & 53.1     & 51.4        & 63.4 \\ \hline
% \multicolumn{5}{c}{\textit{Fine-tuning-based methods}}                                 \\ \hline
% RESDSQL-3B~\cite{DBLP:conf/aaai/Li00023}        & 53.5   & 33.3     & 16.7        & 43.9 \\
% CodES-7B + SFT~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024}    & 64.6   & 46.9     & 40.3        & 57.0 \\
% CodES-15B + SFT~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024}   & 65.8   & 48.8     & 42.4        & 58.5 \\
% ROUTE + Llama3-8B~\cite{qin2024route}                    & 64.3   & 49.3     & 36.8        & 57.3 \\
% ROUTE + Qwen2.5-7B~\cite{qin2024route}                   & 63.8   & 45.4     & 39.6        & 55.9 \\
% ROUTE + Qwen2.5-14B~\cite{qin2024route}                  & 67.7   & \textbf{53.1}     & 42.4        & 60.9 \\
% \textbf{Ours: SQL-o1 + Llama3-8B}                      & \textbf{71.8} & 52.3   & \textbf{45.2} & \textbf{63.4} \\ \hline
% \end{tabular}
% \end{table*}
%
%
% \section{Prompt Templates}
% In this section, we provide the task prompts used in SQL-o1 for Schema-Aware and Progressive SQL Generation. Since the Spider and Bird datasets are quite similar, we only show the task prompts for the Bird dataset.
% % 本节中, 我们提供了SQL-o1中由Schema-Aware和Progressive SQL Generation使用的任务提示。由于Spider与Bird较为相似，我们只展示关于Bird数据集的任务提示。
% \begin{figure*}[h]  % 使用figure*来跨越两栏
%     \centering
%     \includegraphics[width=\linewidth]{latex/prompt_.pdf}  % 图片文件路径
%     \caption{The prompt for Text2SQL in the Fine-tuning Phase.}
%     \label{fig:prompt}
% \end{figure*}
%
%
% \section{Implementation Details}
% The experiments are conducted on 8 NVIDIA A40 GPUs (48GB), with a batch size of 32 for fine-tuning. Using the Llama-Factory framework, we experiment with popular open-source models, including Llama3, Qwen2.5, and other large language models (LLMs). The learning rate is set to \(1e^{-5}\) and decays to 0 by a cosine scheduler at the end of training. The number of MCTS iterations is set to 8, with the temperature parameter set to 0.6. The results of the ablation experiments are the averages of three randomly seeded experiments.
%
% \section{Baseline Details}
% In this section, we summarize all the baseline methods compared in the paper, with a focus on the methods and underlying principles they use. This will help link the evaluation and comparison to the proposed method.
% \begin{itemize}[]
%     \item \textbf{DIN-SQL}~\cite{DBLP:conf/nips/PourrezaR23} decomposes the text-to-SQL task into multiple subproblems, using different prompts for each subproblem to generate the final SQL query with GPT-4.
%     \item \textbf{DAIL-SQL + GPT-4}~\cite{dail_sql} integrates the lessons learned from comparing question representation and example strategies, selecting the most relevant examples based on the similarity between the question and the query.
%     \item \textbf{MAC-SQL}~\cite{wang2024macsql} is based on a multi-agent collaboration framework using LLMs. The MAC-SQL framework consists of a core decomposer agent responsible for generating text-to-SQL, along with two auxiliary agents that utilize external tools or models to retrieve smaller sub-databases and refine incorrect SQL queries.
%     \item \textbf{MCS-SQL + GPT4}~\cite{lee-etal-2025-mcs} optimizes the database schema using multiple prompts, generates candidate SQL queries, and selects the best one based on confidence scores through LLM multiple-choice.
%     \item \textbf{DTS-SQL + GPT4}~\cite{DST-SQL} proposes a two-stage decomposition framework for text-to-SQL fine-tuning, with a schema-linking pre-generation task before the final SQL generation.
%     \item \textbf{CODES}~\cite{DBLP:journals/pacmmod/LiZLFZZWP0024} adopts an incremental pre-training approach with a SQL-centric corpus, tackling schema linking and rapid domain adaptation challenges through strategic prompt construction and bidirectional data augmentation techniques.
%     \item \textbf{SENSE}~\cite{yang-etal-2024-synthesizing} introduces a synthetic data method that combines strong and weak data, using preference learning to help the model learn from mistakes, thereby improving the performance of open-source LLMs on the text-to-SQL task.
%     \item \textbf{ROUTE}~\cite{qin2024route} introduces multiple fine-tuning and task collaboration prompt (MCP) strategies. These strategies leverage collaboration between SQL-related tasks to reduce hallucinations during the SQL generation process.
%     \item \textbf{C3-SQL + ChatGPT}~\cite{DBLP:journals/corr/abs-2307-07306} uses ChatGPT’s zero-shot text-to-SQL method, C3, with ~1000 tokens per query. It includes three components: Clear Prompt (CP), Prompt Calibration (CH), and Consistent Output (CO), with CH reducing bias and improving performance.
%
%     \item \textbf{RESDSQL}~\cite{DBLP:conf/aaai/Li00023} is a framework for the text-to-SQL task, designed to improve performance and robustness by decoupling schema linking and skeleton parsing.

    
% \end{itemize}

\end{document}
