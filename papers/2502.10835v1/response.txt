\section{Related Work}
\subsection{Multi-Hop Reasoning in LLMs}
Improving the reasoning ability of LLMs has become a key focus of recent research **Bauer, "Beyond BERT: Multi-Hop Question Answering via Reasoning-Augmented Language Models"**. **Pérez et al., "Chain-of-Thought Adversarial Learning for Improving Reasoning Ability in Large Language Models"** use chain-of-thought to enhance the reasoning ability by articulating intermediate steps. **Huang et al., "Complexity-Based Prompt Engineering for Multi-Hop Question Answering"** propose complexity-based prompting, showing that selecting and generating reasoning chains with higher complexity significantly improves reasoning accuracy. **Liu et al., "Self-Consistency Enhanced Chain-of-Thought Reasoning in Large Language Models"** combine chain-of-thought with the self-consistency decoding strategy, achieving significant improvements by sampling diverse reasoning paths and selecting the most consistent answer. **Chen et al., "Self-Play Fine-Tuning for Improving Multi-Hop Question Answering Performance"** propose self-play fine-tuning, which enhances LLMs' reasoning abilities by refining their outputs through self-generated data, thereby reducing reliance on human-annotated datasets. **Wang et al., "Scaling Inference Compute in Large Language Models via Sample Diversification"** propose scaling inference compute by increasing the number of generated samples, demonstrating significant improvements across tasks like coding and math. **Kim et al., "Tree-Based Methods for Improving Multi-Hop Question Answering Performance"** use tree-based methods to improve the performance.

\subsection{Mechanistic Interpretability}
Mechanistic interpretability **Li et al., "Reverse Engineering Large Language Models: A Mechanistic Approach"** aims to reverse engineer the internal mechanisms of LLMs. Logit lens **Bhatia et al., "Logit Lens: Visualizing and Analyzing Hidden States in Transformers"** is a widely used method **Srivastava et al., "Understanding Hidden State Information through Logit Multiplication"** to analyze the information of hidden states, by multiplying the vectors with the unembedding matrix. A commonly used localization method is causal mediation analysis **Koh et al., "Causal Mediation Analysis for Understanding Large Language Model Mechanisms"**, whose core idea is to compute the change of the output when modifying a hidden state. Another types of studies focus on constructing the circuit in the model **Zhou et al., "Constructing Circuits within Transformers: A New Perspective on Mechanistic Interpretability"**. Due to the superposition phenomenon **Xu et al., "Superposition Phenomenon in Large Language Models: Challenges and Opportunities"**, sparse auto-encoder (SAE) is useful for interpreting the features **Wang et al., "Sparse Auto-Encoders for Interpreting Features in Large Language Models"**. A useful characteristic is the residual stream **Liu et al., "Residual Streams in Transformers: Unlocking Hidden Representations"**, revealing that the final embedding can be represented as the sum of layer outputs. Furthermore, **Chen et al., "Layer-wise Feature Analysis in Large Language Models"** find that the FFN output is the weighted sum of FFN neurons. **Huang et al., "Attention Head Outputs as Weighted Sums of Attention Neurons: A New Perspective"** find that the attention head outputs can also be regarded as the weighted sum of attention neurons.

While previous neuron-level studies primarily focus on ``localization''—identifying which neurons are important—they often lack a deeper ``analysis'' of how these neurons influence predictions. By applying our logit flow method, we gain a clearer understanding of how neurons are activated and contribute to the final prediction.