[
  {
    "index": 0,
    "papers": [
      {
        "key": "lightman2023let",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's verify step by step"
      },
      {
        "key": "huang2023large",
        "author": "Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny",
        "title": "Large language models cannot self-correct reasoning yet"
      },
      {
        "key": "li2024chain",
        "author": "Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu",
        "title": "Chain of thought empowers transformers to solve inherently serial problems"
      },
      {
        "key": "wang2024chain",
        "author": "Wang, Xuezhi and Zhou, Denny",
        "title": "Chain-of-thought reasoning without prompting"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fu2022complexity",
        "author": "Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar",
        "title": "Complexity-based prompting for multi-step reasoning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024self",
        "author": "Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan",
        "title": "Self-play fine-tuning converts weak language models to strong language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large language monkeys: Scaling inference compute with repeated sampling"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hao2023reasoning",
        "author": "Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting",
        "title": "Reasoning with language model is planning with world model"
      },
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Chris2022",
        "author": "Chris Olah",
        "title": "Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "nostalgebraist2020",
        "author": "Nostalgebraist",
        "title": "Interpreting GPT: the logit lens"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dar2022analyzing",
        "author": "Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan",
        "title": "Analyzing transformers in embedding space"
      },
      {
        "key": "katz2023visit",
        "author": "Katz, Shahar and Belinkov, Yonatan",
        "title": "VISIT: Visualizing and interpreting the semantic information flow of transformers"
      },
      {
        "key": "yu2024interpreting",
        "author": "Yu, Zeping and Ananiadou, Sophia",
        "title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "vig2020investigating",
        "author": "Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart",
        "title": "Investigating gender bias in language models using causal mediation analysis"
      },
      {
        "key": "meng2022locating",
        "author": "Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan",
        "title": "Locating and editing factual associations in GPT"
      },
      {
        "key": "stolfo2023mechanistic",
        "author": "Stolfo, Alessandro and Belinkov, Yonatan and Sachan, Mrinmaya",
        "title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis"
      },
      {
        "key": "geva2023dissecting",
        "author": "Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir",
        "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "olsson2022context",
        "author": "Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others",
        "title": "In-context learning and induction heads"
      },
      {
        "key": "zhang2023towards",
        "author": "Zhang, Fred and Nanda, Neel",
        "title": "Towards best practices of activation patching in language models: Metrics and methods"
      },
      {
        "key": "gould2023successor",
        "author": "Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur",
        "title": "Successor heads: Recurring, interpretable attention heads in the wild"
      },
      {
        "key": "hanna2024does",
        "author": "Hanna, Michael and Liu, Ollie and Variengien, Alexandre",
        "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"
      },
      {
        "key": "wang2022interpretability",
        "author": "Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob",
        "title": "Interpretability in the wild: a circuit for indirect object identification in gpt-2 small"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "elhage2022toy",
        "author": "Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others",
        "title": "Toy models of superposition"
      },
      {
        "key": "scherlis2022polysemanticity",
        "author": "Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck",
        "title": "Polysemanticity and capacity in neural networks"
      },
      {
        "key": "bricken2023towards",
        "author": "Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others",
        "title": "Towards monosemanticity: Decomposing language models with dictionary learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gao2024scaling",
        "author": "Gao, Leo and la Tour, Tom Dupr{\\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey",
        "title": "Scaling and evaluating sparse autoencoders"
      },
      {
        "key": "templeton2024scaling",
        "author": "Templeton, Adly",
        "title": "Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet"
      },
      {
        "key": "cunningham2023sparse",
        "author": "Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee",
        "title": "Sparse autoencoders find highly interpretable features in language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "elhage2021mathematical",
        "author": "Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others",
        "title": "A mathematical framework for transformer circuits"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "geva2020transformer",
        "author": "Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer",
        "title": "Transformer feed-forward layers are key-value memories"
      },
      {
        "key": "geva2022transformer",
        "author": "Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav",
        "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "yu2024neuron",
        "author": "Yu, Zeping and Ananiadou, Sophia",
        "title": "Neuron-level knowledge attribution in large language models"
      }
    ]
  }
]