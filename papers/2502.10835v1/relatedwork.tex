\section{Related Work}
\subsection{Multi-Hop Reasoning in LLMs}
Improving the reasoning ability of LLMs has become a key focus of recent research \cite{lightman2023let,huang2023large,li2024chain,wang2024chain}. \citet{wei2022chain} use chain-of-thought to enhance the reasoning ability by articulating intermediate steps. \citet{fu2022complexity} propose complexity-based prompting, showing that selecting and generating reasoning chains with higher complexity significantly improves reasoning accuracy. \citet{wang2022self} combine chain-of-thought with the self-consistency decoding strategy, achieving significant improvements by sampling diverse reasoning paths and selecting the most consistent answer. \citet{chen2024self} propose self-play fine-tuning, which enhances LLMs' reasoning abilities by refining their outputs through self-generated data, thereby reducing reliance on human-annotated datasets. \citet{brown2024large} propose scaling inference compute by increasing the number of generated samples, demonstrating significant improvements across tasks like coding and math. \citet{hao2023reasoning,yao2024tree} use tree-based methods to improve the performance.

\subsection{Mechanistic Interpretability}
Mechanistic interpretability \cite{Chris2022} aims to reverse engineer the internal mechanisms of LLMs. Logit lens \cite{nostalgebraist2020} is a widely used method \cite{dar2022analyzing,katz2023visit,yu2024interpreting} to analyze the information of hidden states, by multiplying the vectors with the unembedding matrix. A commonly used localization method is causal mediation analysis \cite{vig2020investigating,meng2022locating,stolfo2023mechanistic,geva2023dissecting}, whose core idea is to compute the change of the output when modifying a hidden state. Another types of studies focus on constructing the circuit in the model \cite{olsson2022context,zhang2023towards,gould2023successor,hanna2024does,wang2022interpretability}. Due to the superposition phenomenon \cite{elhage2022toy,scherlis2022polysemanticity,bricken2023towards}, sparse auto-encoder (SAE) is useful for interpreting the features \cite{gao2024scaling,templeton2024scaling,cunningham2023sparse}. A useful characteristic is the residual stream \cite{elhage2021mathematical}, revealing that the final embedding can be represented as the sum of layer outputs. Furthermore, \citet{geva2020transformer,geva2022transformer} find that the FFN output is the weighted sum of FFN neurons. \citet{yu2024neuron} find that the attention head outputs can also be regarded as the weighted sum of attention neurons. 

While previous neuron-level studies primarily focus on ``localization''—identifying which neurons are important—they often lack a deeper ``analysis'' of how these neurons influence predictions. By applying our logit flow method, we gain a clearer understanding of how neurons are activated and contribute to the final prediction.