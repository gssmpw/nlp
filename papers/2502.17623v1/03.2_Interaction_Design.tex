\section{System Design} \label{sec-system}

We designed a prototype system, \texttt{PAiREd}, an LLM-driven interface integrated with an educational robot that uses LLMs to generate learning content based on established educational frameworks for reading activities. The system enables parents to review and revise LLM-generated learning content \textit{before} the activity and adjust their involvement by modifying the role delegation between the robot and themselves \textit{during} the activity. The system consists of three core design components: (1) \textit{LLM-driven content generation}, (2) \textit{Interface design}, and (3) \textit{Robot interaction design} (Figure \ref{fig:syste-diagram}).

\begin{figure*}[t!]
  \includegraphics[width=\textwidth]{figures/figure-system-hho.pdf}
   \vspace{-6pt}
  \caption{\texttt{PAiREd} System Architecture Overview. The system consists of three key components: prompt instructions and data, the web application (which includes both the editor and activity interfaces), and the robot interaction module.}
  \label{fig:syste-diagram}
   \vspace{-10pt}
\end{figure*}

\subsection{LLM-driven Content Generation}
% limitation of gpt-4o
Content generation is powered by a state-of-the-art LLM, ChatGPT-4o \cite{achiam2023gpt} developed by OpenAI\footnote{OpenAI ChatGPT-4o: \url{https://openai.com/index/hello-gpt-4o/}} through prompt engineering \cite{sahoo2024systematic}. To generate the learning content for each book page, the input we provided for GPT-4o includes: (1) \textit{prompts}\footnote{Prompts: \url{https://osf.io/zfksg/?view_only=b59bd41287f543ce82ab85950aaf004f}}: instructions to generate learning content, (2) \textit{book image}: an image of the chosen book page, (3) \textit{visual context}\footnote{Visual context: \url{https://osf.io/zfksg/?view_only=b59bd41287f543ce82ab85950aaf004f}}: a manually-created JSON-based dataset containing structured visual information of the book page, and (4) \textit{educational frameworks} \footnote{Frameworks: \url{https://osf.io/zfksg/?view_only=b59bd41287f543ce82ab85950aaf004f}}: theoretical-driven frameworks defining critical pre-school concepts for math and literacy. The books, visual contexts, and educational frameworks are placed in our database and retrieved using prompt instructions. We explain \textit{visual context} and \textit{educational frameworks} below.

\subsubsection{Visual Context} 
The ChatGPT-4o model faces limitations in quantitative and spatial reasoning on images and recognition of animated or anthropomorphic drawings (\textit{e.g.,} mistaking clothed animals for humans), limiting its ability to accurately auto-generate question-answer pairs for literacy and math based on storybook images. To enhance content generation accuracy, we created a JSON-based dataset to capture the \textit{visual context} of each storybook page. Each page is represented as a unique entity with detailed descriptions of objects (\textit{e.g.,} characters, animals, and environmental elements), properties, as well as their spatial relationships. This context was embedded in the prompt design, instructing GPT-4o to retrieve precise object properties such as color, count, and location. Integrating this structured approach significantly improved the accuracy of ChatGPT-4o to create question-answer pairs based on storybooks, reducing object-identification errors and enhancing consistency across pages. Systematic evaluations by the first and third authors confirmed these improvements.

% \begin{figure}[b!]
%   \includegraphics[width=\columnwidth]{figures/figure-editor-amy.pdf}
%    \vspace{-6pt}
%   \caption{Top: Editor Interface of the PAiREd system. This interface allows users to navigate book content, modify LLM-generated learning content through regeneration, or manually edit it. Bottom: Activity Interface of the PAiREd system. This interface enables parents to flexibly adjust their involvement and seamlessly share responsibilities with the robot through the mode-switching and role-delegation mechanisms. }
%   \label{fig:editor}
%    \vspace{-6pt}
% \end{figure}

\begin{figure*}[b!]
  \includegraphics[width=\textwidth]{figures/figure-editor-amy.pdf}
     \caption{Editor Interface of the PAiREd system. This interface allows users to navigate book content, modify LLM-generated learning content through regeneration, or manually edit it.}
  \label{fig:editor}
\end{figure*}

\begin{figure*}[b!]
\centering
  \includegraphics[width=\textwidth]{figures/figure-activity-amy.pdf}
  \caption{Activity Interface of the PAiREd system. This interface enables parents to flexibly adjust their involvement and seamlessly share responsibilities with the robot through the mode-switching and role-delegation mechanisms.}
  \label{fig:activity}
   \vspace{-10pt}
\end{figure*}
 
\subsubsection{Educational Frameworks for Math and Literacy} 
We developed frameworks outlining varying proficiency levels in math \cite{purpura2013informal, engel2013teaching} and literacy \cite{kaminski2014preschool} concepts for preschoolers, drawing on educational psychology research. Math and literacy were selected as they are foundational skills for preschoolers \cite{weiland2013impacts}. The math framework incorporated informal numeracy skills identified by \citet{purpura2013informal} and aligned them with the four proficiency levels proposed by \citet{engel2013teaching}. The literacy framework was based on the PELI assessment benchmarks \cite{kaminski2014preschool}.

\subsection{Interface Design}
We developed a web-based interface using a React front-end\footnote{\url{https://reactnative.dev}} and a FAST API back-end,\footnote{\url{https://fastapi.tiangolo.com}} supported by a MongoDB database.\footnote{\url{https://www.mongodb.com}} The interface includes three key components: (1) \textit{LLM content generation}: parents can request LLM to generate a new set of learning content (\textit{e.g.,} question-answer pairs); (2) \textit{Editor interface}: parents can review and revise LLM-generated content to varying extents; and (3) \textit{Activity interface}: parents can flexibly adjust their involvement and role delegation between themselves and the robot. 

\subsubsection{LLM Content Generation}
Parents can choose a book from the digital library in the system and generate a new set of learning content using the LLM embedded within the platform. Before generating content, parents are prompted to select \textit{grade level} (\textit{e.g.,} preschool) and \textit{subject} (\textit{e.g.,} math or literacy). Once the content is generated, parents can either \textit{edit and review} the material or \textit{launch} it directly. For each page of the book, the LLM creates content following a structured approach, including three main components: (1) \textit{question and multiple choices}: a question related to a selected concept from the educational framework, with four answer options, one being correct; (2) \textit{explanation}: a description of how the correct answer is derived; (3) \textit{motivation}: encouraging words or a fun fact related to the content of the book page.


\subsubsection{Editor Interface}
After the LLM generated the content, if the user choose to \textit{edit and review}, the editor interface will display the content page by page. The main \textit{book content} is shown in the left panel, while the \textit{LLM-generated content} appears on the right. The \textit{concept}, initially selected by the LLM from the chosen educational framework, is displayed at the top of the book content. For the \textit{book content}, parents could navigate through the book and review it page by page. The \textit{LLM-generated content} was organized into its main components: \textit{questions and choices}, \textit{explanations}, and \textit{motivational content}. Parents had the option to either regenerate individual components or all components at once using the LLM, or manually edit the content. Additionally, the \textit{concept} was presented as a clickable drop-down list of all available concepts from the chosen framework. Hovering over the information icon provided details about each concept. If parents wanted to change the concept, they could select a new one from the list, triggering the LLM to automatically regenerate content based on the new concept (Figure \ref{fig:editor}).


\subsubsection{Activity Interface}
When an activity was launched, regardless of whether the parent had edited it, the \textit{book content} was presented on the right panel, with the \textit{LLM-generated content} and the \textit{modes-switching and role-delegation mechanisms} shown on the left. For each page, the components of the \textit{LLM-generated content} were organized into colored blocks, showing the \textit{book text}, \textit{questions and choices}, \textit{explanation}, and \textit{motivational feedback} in sequence. The \textit{mode-switching} mechanism enabled parents to control their involvement on a macro level by adjusting the overall task distribution for all components based on preset configurations. This mechanism used a driving metaphor, with positions for a driver, co-pilot, and exit. The parent was represented by a blue icon, and the robot by a green icon. By dragging the icons to the appropriate positions, parents could select their preferred mode for each task. The \textit{role-delegation} mechanism provided more detailed control, allowing parents to assign individual components of the activity to either themselves or the robot (Figure \ref{fig:activity}).

The system defined four modes: (1) \textit{parent-takeover} mode: the parent facilitates all content components; (2) \textit{parent-led} mode: the parent leads the activity, with the robot helping with specific tasks; (3) \textit{robot-led} mode: the robot leads, with the parent helping as needed; (4) \textit{robot-takeover} mode: the robot facilitates all components. The \textit{mode-switching} mechanism automatically adjusted the task roles based on the selected mode, assigning all roles to the parent in \textit{parent-takeover} and \textit{parent-led} modes, and all to the robot in \textit{robot-takeover} and \textit{robot-led} modes. In \textit{parent-led} and \textit{robot-led} modes, parents could further adjust the role distribution by delegating individual components using the \textit{role delegation} mechanism. Figure~\ref{fig:mode} details how icon positions correspond to specific modes and provides examples of a role delegation pattern for each mode.

\begin{figure*}[t!]
  \includegraphics[width=\textwidth]{figures/figure-mode-hho.pdf}
   \vspace{-6pt}
  \caption{Mode-Switching and Role Delegation Mechanisms of the PAiREd system. The system offers four modes: robot takeover, robot-led, parent-led, and parent takeover. Parents can drag their icon to the ``driver,'' ``co-driver,'' or ``exit'' position to select the desired mode. Additionally, they can fine-tune the role delegation by assigning specific tasks to either themselves or the robot.}
  \label{fig:mode}
   \vspace{-6pt}
\end{figure*}


   \vspace*{-3pt}
\subsection{Robot Interaction Design}
We used a Misty II social robot,\footnote{Misty Robot: \url{https://www.mistyrobotics.com/products/misty-ii/}} which is a semi-humanoid robot with a four-inch LCD display for its face where customizable facial expressions can be displayed. Misty robot was connected to our interface system through the Internet and the RestAPI.\footnote{\url{https://github.com/MistyCommunity/REST-API}} During the activities, the robot autonomously facilitated a component according to the role delegation, remaining silent when the parent was responsible for a component. The interface also displayed the robot's status and reminded parents when it was their turn to engage. For each component assigned to the robot, it verbalized the content using audio generated by Google's Cloud Text-to-Speech engine.\footnote{Google Cloud Text-to-Speech: \url{https://cloud.google.com/text-to-speech/}} Additionally, the system leveraged LLM to select an appropriate robot expressions per content component from a predefined database to enhance interaction through expressive gestures, which had been used in previous studies with children \cite{white2021designing}. The robot's front bumpers provided interactive controls, allowing young children to navigate the activity more easily. Pressing the left-front bumper \textit{repeated} the ongoing component, while pressing the right-front bumper \textit{proceeded} to the next component. When progressing to the next component, the interface updated accordingly by closing the current component and expanding the next one (Figure \ref{fig:robot}).





