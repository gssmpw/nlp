

\section{Background}
\label{sec:background}
\subsection{DRAM Organization and Operation}
\label{sec:dram_organization}

\head{\om{2}{DRAM} organization}~\figref{fig:dram_organization} {shows the hierarchical organization of a modern DRAM-based main memory. The memory controller connects to a DRAM module over a memory channel. A module contains one or multiple DRAM ranks that time-share the memory channel. A rank consists of multiple DRAM chips.
%that operate in lock-step. 
Each DRAM chip \agy{0}{has} multiple DRAM banks \agy{0}{each of which contains multiple subarrays}. A DRAM bank is organized as a two-dimensional array of DRAM cells, where a row of cells is called a DRAM row. A DRAM cell consists of i) a storage capacitor, which stores one bit of information in the form of electrical charge, and ii) an access transistor.}
%, which connects the capacitor to the row buffer through a bitline controlled by a wordline.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_DRAM_organization.pdf}
    \caption{\srev{DRAM \sql{R4}organization}}
    \label{fig:dram_organization}
\end{figure}


% When the row decoder (including wordline drivers) drives a wordline high, the access transistors of all DRAM cells in the row \circled{3} are enabled, electrically connecting each cell in the row to its corresponding bitline. DRAM cells in the same column share a bitline, which is used to read from and write to the cells via the row buffer \circled{4} (which contains bitline sense amplifiers, BLSA).

\head{DRAM operation and timing}
\iey{0}{There are four \yct{1}{main} DRAM commands to access a DRAM row: $ACT$, $RD$, $WR$, and $PRE$. \figref{fig:pr_timeline} illustrates the relationship between the commands issued to access a DRAM cell and their governing timing parameters in \om{3}{five} steps. Initially, the cell capacitor stores $V_{DD}$ and is precharged.~\circled{1}~\om{3}{The} memory controller issues the $ACT$ command. 
\om{3}{$ACT$ asserts the corresponding \om{2}{row's} wordline, which} consequently enables all the access transistors in the DRAM row, \om{3}{conducting} cell capacitors to corresponding bitlines, and \om{3}{initiating} an analog \om{3}{\emph{charge sharing}} process between each cell capacitor and its corresponding bitline.~\circled{2}~\om{3}{Once} the cell and bitline voltages equalize due to charge sharing, \om{3}{\emph{charge restoration}} starts. During charge restoration, the sense amplifiers are enabled to first detect the bitline voltage shift and later restore the bitline to a full $V_{DD}$ or $GND$ depending on the direction of the shift.~\circled{3}~\om{3}{Once} the cell is restored to a voltage level that is ready to access \yct{3}{($V_{min}$)}, which requires waiting for \om{3}{the} $t_{RCD}$ timing parameter (i.e., the timing parameter between $ACT$ and $RD$/$WR$ commands), $RD$ or $WR$ commands can be issued to the bank.~\circled{4}~\om{3}{The} memory controller can follow an $ACT$ with $PRE$ to the same bank after at least the time interval for \gls{tras}. $t_{RAS}$ ensures that enough time has passed to fully restore the DRAM cells of the activated row to a ready-to-precharge voltage. The latency of $PRE$ is governed by \yct{6}{\gls{trp}}.~\circled{5}~After $t_{RP}$, the memory controller can issue an $ACT$ to open a new row in the same bank.}
\yct{7}{Thus, an $ACT$ following another $ACT$ to the same bank can be issued only after $t_{RAS} + t_{RP}$ \om{8}{(i.e., $t_{RC}$)}.}
\yctcomment{6}{I will add tRC to fig.}
%(i.e., the minimum time interval between two consecutive $ACT$ commands targeting the same bank is equal to $t_{RAS}$+$t_{RP}$, which is called ($t_{RC}$)).}
%{To access a DRAM row, the memory controller issues a set of commands to DRAM over the memory channel. The memory controller sends an $ACT$ command to activate a DRAM row, which asserts the corresponding wordline, \agy{0}{consequently enabling all the access transistors in the DRAM row, conducting cell capacitors to corresponding bitlines and initiates an analog charge sharing process between each cell capacitor and its corresponding bitline. Charge sharing destroys the capacitor voltage and causes a minor deviation on the bitline voltage. Subsequently, sense amplifiers are enabled to boost the deviation on bitline voltage and to restore the destroyed capacitor voltage. This process is called charge restoration. Once the sense amplifiers boost the voltage deviation to a level that can be reliably read,} the memory controller can issue a $RD$/$WR$ command to read from/write into the DRAM row.
%in cache block granularity.  
%Subsequent accesses to the same row cause a row \agy{0}{buffer} hit \agy{0}{and serviced with a relatively low latency}. To access \agy{0}{data in} a different row, the memory controller must first close the bank by issuing a $PRE$ command \agy{0}{and activate the row that contains the desired data.}
%that deasserts the wordline. 
%Therefore, accessing \agy{0}{data in} different rows causes row \agy{0}{buffer conflicts and serviced with a relatively larger latency}.}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figures/fig2_DRAM_operation.pdf}
\caption{\om{3}{Timeline of a DRAM cell's activation process}}
\label{fig:pr_timeline}
\end{figure}


\head{{DRAM refresh}}
\agy{3}{DRAM is a volatile memory technology: a DRAM cell leaks charge over time and can lose its data (i.e., charge leakage).}
% \agycomment{3}{I changed this back to my version as it is more correct and more informative. I am ok to change it to your version if you convince me otherwise :)}
% \yct{3}{DRAM is a volatile memory technology in which cells lose their charge over time (i.e., charge leakage).}
% \agycomment{3}{DRAM is not ideal just like anything else. Nothing is ideal. Compare our versions and find the best synthesis}
If charge leakage exceeds a level, the data of the cell \emph{cannot} be sensed by the sense amplifier correctly. These errors are known as data retention failures. To prevent \yct{9}{these} failures, 
the memory controller issues $REF$ commands to refresh DRAM cells. A $REF$ command \agy{3}{internally} activates DRAM rows to sense and restore the charge \om{3}{levels} of the cells\om{3}{;} \agy{3}{which takes around \om{3}{$32-35 ns$}~\dramStandardCitations{}}. A single refresh can be thought of as opening (i.e., activating) a row and closing (i.e., precharging) the bank, as shown in~\figref{fig:pr_timeline}.
%\iey{Aside from the DRAM access commands (i.e.,  $ACT$, $RD$, $WR$, and $PRE$), the memory controller also periodically issues a $REF$ command to prevent data loss due to leakage of charge from the cell capacitors over time.}
{To maintain data integrity, the memory controller periodically refreshes each row in a time interval called \gls{trefw} ($32 ms$ for DDR5~\cite{jedec2020ddr5} and $64 ms$ for DDR4~\cite{jedec2017ddr4}). To ensure all rows are refreshed every \gls{trefw}, the memory controller issues $REF$ commands with a time interval called \gls{trefi} ($3.9 \mu s$ for DDR5~\cite{jedec2020ddr5} and $7.8 \mu s$ for DDR4~\cite{jedec2017ddr4}).} \agy{0}{Each $REF$ command blocks the whole DRAM bank or rank for a time window called \gls{trfc}. \yct{6}{\agy{3}{\gls{trfc} is} a \om{3}{function of the} charge restoration {latency} and the number of rows refreshed with a $REF$ command. \om{7}{\gls{trfc} is} \yct{6}{\om{3}{$195ns$ and $350ns$ for 8Gb DDR5~\cite{jedec2020ddr5} \agy{3}{and \om{7}{8Gb} DDR4~\cite{jedec2017ddr4} DRAM} chips}}\yct{7}{, respectively}.}}
%\head{Timing Parameters}
%{To ensure correct operation, the memory controller must obey specific timing parameters while accessing DRAM. In addition to \gls{trefw} and \gls{trefi}, we explain \param{three} timing parameters related to the rest of the paper: i)~\gls{tras} the minimum time needed to \agy{0}{fully restore the cell charge (i.e., the minimum time that a row has to remain open)} ($t_{RAS}$), ii)~the minimum time interval between two consecutive ACT commands targeting the same bank ($t_{RC}$), and iii) the minimum time needed to issue an ACT command following a PRE command ($t_{RP}$).}
%\yctcomment{need more?}
\subsection{DRAM Read Disturbance}
\label{sec:background_dram_read_disturbance}
% \agycomment{3}{Is this a copy-paste? It should be at this point.}
\yct{3}{As DRAM manufacturing technology
% advances and node sizes shrink, 
\agy{3}{node size scales down, DRAM cell \om{6}{sizes} and cell-to-cell \om{6}{distances} shrink. As a result, accessing a DRAM cell can \om{6}{more easily} disturb data stored in another physically nearby cell. \om{8}{This phenomenon} is called \emph{read disturbance}}.
Two prime examples of read disturbance in modern DRAM chips are \emph{RowHammer}~\cite{kim2014flipping} and \emph{RowPress}~\cite{luo2023rowpress}, where repeatedly accessing (\emph{hammering}) or keeping active (\emph{pressing}) a DRAM row induces bitflips in physically nearby DRAM rows, respectively. In RowHammer and RowPress terminology, the row that is hammered or pressed is called the \emph{aggressor} row, and the row that experiences bitflips \iey{7}{is} the \emph{victim} row. For read disturbance bitflips to occur\agy{3}{,} \gls{nrh}.
% \agycomment{3}{this is a dangerous use of glossary. It is safer to define NRH in the glossary as RowHammer threshold. Otherwise you might have problems before or after any glsreset command} \yctcomment{3}{That's why I hate glossary.}
Several characterization studies~\understandingRowHammerAllCitations{} show that as DRAM technology continues to scale to smaller nodes, DRAM chips are becoming increasingly vulnerable to RowHammer (i.e., newer chips have lower \gls{nrh} values).}\footnote{For example, one can induce RowHammer bitflips with 4.8K activations in the chips manufactured in 2020 while a row needs to be activated 69.2K times in older chips manufactured in 2013\om{6}{~\cite{kim2020revisiting}}.} 
\om{2}{Recent studies~\cite{olgun2023hbm, olgun2024read} also show that modern HBM2 DRAM chips are as \om{3}{vulnerable} to both RowHammer and RowPress as modern DDR4 and LPDDR4 DRAM chips.}
\agy{0}{To make matters worse, \gls{nrh} reduces significantly by crafting access patterns that exploit both RowHammer and RowPress~\cite{luo2023rowpress}.}
% In RowHammer, repeatedly activating (i.e., \emph{hammering}) a DRAM row (i.e., aggressor row) induces bitflips in physically nearby rows (victim rows). Similarly, in RowPress, keeping active an aggressor row for an extended period (i.e., \emph{pressing}) induces bitflips in victim rows.
% To induce RowHammer bitflips, an aggressor row must be activated more than the RowHammer threshold, \gls{nrh}. 
% Several characterization studies~\understandingRowHammerAllCitations{} show that as DRAM technology continues to scale to smaller nodes, DRAM chips are becoming increasingly vulnerable to RowHammer (i.e., newer chips have lower \gls{nrh} values).}\footnote{For example, one can induce RowHammer bitflips with 4.8K activations in the chips manufactured in 2020~\cite{kim2020revisiting} while a row needs to be activated 69.2K times in older chips manufactured in 2013~\cite{kim2014flipping}.} 
% {As DRAM \agy{0}{manufacturing technology node size shrinks,} interference \agy{0}{across rows} increases, causing circuit-level \agy{0}{read disturbance} mechanisms. \agy{0}{Two prime examples of such mechanisms are RowHammer~\cite{kim2014flipping} and RowPress~\cite{luo2023rowpress},} where repeatedly activating (i.e., opening) \agy{0}{a DRAM row (i.e., aggressor row) or keeping the aggressor row active for a long time} induces bitflips in physically nearby rows (i.e., victim rows), \agy{0}{respectively}. \agy{0}{To induce RowHammer bitflips,} an aggressor row needs to be activated more than the RowHammer threshold, \gls{nrh}. 
% \agy{0}{Several} characterization studies~\understandingRowHammerAllCitations{} show that as DRAM \om{3}{technology} scaling continues to smaller nodes, DRAM chips are \om{3}{becoming} more vulnerable to RowHammer (i.e., newer chips have lower \gls{nrh} values).\footnote{For example, one can induce RowHammer bitflips with 4.8K activations in the chips manufactured in 2020~\cite{kim2020revisiting} while a row needs to be activated 69.2K times in older chips manufactured in 2013~\cite{kim2014flipping}.} 
% \om{2}{Recent studies~\cite{olgun2023hbm, olgun2024read} also show that modern HBM2 DRAM chips are as \om{3}{vulnerable} to both RowHammer and RowPress as modern DDR4 and LPDDR4 DRAM chips.}
% \agy{0}{To make matters worse, \gls{nrh} reduces significantly (e.g., by orders of magnitudes) by crafting access patterns that exploit both RowHammer and RowPress~\cite{luo2023rowpress}.}}


\head{DRAM read disturbance mitigation \om{0}{mechanisms}}
Many prior works propose mitigation mechanisms~\mitigatingRowHammerAllCitations{} to protect DRAM chips against RowHammer bitflips\om{3}{,} leveraging different approaches. \agy{0}{These \om{0}{mechanisms} \om{3}{usually} perform two \om{3}{main} tasks: i)~execute a \om{3}{\emph{trigger algorithm}} and ii)~perform a \om{3}{\emph{preventive action}}. The trigger algorithm observes the memory access patterns and triggers a preventive action based on the result of a probabilistic or a deterministic process. 
The preventive action is \om{3}{usually} one of i)~refreshing victim rows \yct{3}{(i.e., \emph{preventive refresh})}, ii)~dynamically remapping aggressor rows, and iii)~throttling unsafe accesses.}
\yct{20}{The DDR5 standard introduces a new command called \emph{refresh management} (RFM)~\cite{jedec2020ddr5} to protect DRAM chips against RowHammer attacks. The memory controller tracks the activation count of each DRAM bank and issues an RFM command when the count exceeds a threshold value. When the DRAM module receives an RFM command, it internally takes preventive actions within the given time window. 
\agy{3}{RFM tracks the row activation counts \om{6}{at} bank granularity rather than row granularity. Therefore, RFM commands are issued based on the total activation count across thousands of rows in a bank with \emph{no} notion of the row-level temporal locality of those activations. 
As such, the memory controller can trigger an excessive \om{3}{number} of RFM commands\om{3}{,} causing high performance overheads~\cite{canpolat2024prac}.}
% \yct{3}{As RFM tracks the activation count of each bank rather than each row, the activation counters at the memory controller can trigger an excessive \om{3}{number} of RFM commands\om{3}{,} causing high performance overheads~\cite{canpolat2024prac}.} 
% Due to coarse-grained tracking, the memory controller \om{3}{can} issue an excessive \om{3}{number} of RFM commands\om{3}{,} causing high performance overheads~\cite{canpolat2024prac}. 
To \om{3}{reduce} the number of RFM commands, the latest DDR5 standard introduces a new \om{0}{mechanism} called \emph{Per Row Activation Counting} (PRAC)~\cite{jedec2024ddr5}. PRAC enables fine-grained tracking by implementing \om{3}{per-row} activation counters inside the DRAM chip. When a row's counter exceeds a threshold value, the DRAM chip requests an RFM command from the memory controller with a new \emph{back-off} signal. Once the memory controller receives this signal, it issues an RFM command and allows the DRAM chip to take preventive actions.}
\agy{0}{Existing RowHammer mitigations can also \om{3}{be used to} prevent \om{3}{\emph{RowPress}} bitflips when their trigger algorithms are configured to be more aggressive, which is practically equivalent to configuring them for sub-1K \gls{nrh} values~\cite{luo2023rowpress, luo2024experimental, luo2024rowpress}.} 


{RowHammer mitigation \om{0}{mechanisms} face an important trade-off between two \om{3}{sets of metrics:} i)~system performance \om{3}{(and energy efficiency)} and ii)~area cost. \om{3}{We} categorize RowHammer mitigation \om{1}{mechanisms} depending on their dominant overheads, i)~\emph{high-performance-overhead mitigations} and ii)~\emph{high-area-overhead mitigations}.
High-performance-overhead mitigations \om{3}{(e.g., PARA~\cite{kim2014flipping} and RFM~\cite{jedec2020ddr5})} employ basic detection algorithms that introduce lower area overhead but falsely detect many aggressor rows, causing many unnecessary preventive refreshes. Due to the high number of preventive refreshes, these mitigations introduce higher performance and energy \om{3}{overheads than high-area-overhead mechanisms}.
High-area-overhead mitigations \om{3}{(e.g., PRAC~\cite{jedec2024ddr5}, Hydra~\cite{qureshi2022hydra}, and Graphene~\cite{park2020graphene})} utilize \om{3}{more sophisticated} trigger algorithms that accurately detect aggressor rows and avoid performing unnecessary preventive refreshes. Therefore, these mitigations introduce lower performance and energy overheads but higher area overheads.} \om{1}{As DRAM read disturbance worsens with shrinking technology, \gls{nrh} values are expected to reduce even more\om{3}{~\understandingRowHammerAllCitations{}}.}
Unfortunately, existing RowHammer mitigation \om{6}{mechanisms} incur either high performance \om{3}{(and energy)} overheads or high area overheads. \om{7}{Both high-performance-overhead and high-area-overhead mitigation \om{6}{mechanisms} are not scalable with increasing RowHammer vulnerability.}
Therefore, reducing the overheads of existing RowHammer \yct{9}{mitigation mechanisms} is critical \om{3}{to enable efficient scaling of DRAM-based systems into the future}.

% \om{1}{Depending on the trigger algorithm, some RowHammer mitigations introduce low area overhead and high performance and energy overheads while others introduce low performance and energy overheads, and high area overhead.}
% Giray: Commented out the following because it is making the focus lost.
% Probability-based mitigations suffer from high performance, energy, and area overheads~\cite{kim2020revisiting, yaglikci2022hira}. Tracking-based mechanisms either prevent bitflips (i) with low-performance cost but incur high area overheads or (ii) at low area cost but while achieving sub-optimal performance.
