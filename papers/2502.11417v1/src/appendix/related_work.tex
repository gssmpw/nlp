\section{Additional Related Work}\label{appendix:related_work}

\paragraph{General LLM Inference.}
\label{sec:general_llm_inference}
LLMs generate text responses auto-regressively, producing one token at a time based on preceding tokens. The process consists of two stages that can potentially be executed on different endpoints: (i) \emph{Prefill stage}: The model processes the input text (prompt), calculates and stores intermediate model states--i.e., the key and value cache (KV cache) of tokens--to generate the first token. A token represents a word or part of a word the model can interpret. Once the first token is generated, it is appended to the end of the prompt and the generation process moves on to the (ii) \emph{Decode stage}: The model processes the updated prompt (including previously generated tokens) to generate the next token. The decode stage continues until a stopping condition is met (e.g., reaching an end-of-sequence token or the maximum generation length).

\paragraph{On-Server LLM Serving.}
Existing work have focused on GPU kernel optimization \citep{flashattention,flashinfer}, KV-cache management \citep{lin2024infinite,scissorhands,mooncake}, model parallelism \citep{shoeybi2019megatron,pope2023efficiently,liu2023ring}, quantization \citep{smoothquant,awq,dettmers2022gpt3}, and scheduling \citep{orca,vllm,sarathi,wang2025hygen}.
For example, Orca \cite{orca} introduced continuous batching to improve serving throughput, while vLLM \cite{vllm} developed PagedAttention to reduce LLM memory restraint. Sarathi \citep{sarathi} implemented chunked prefill to mitigate inter-request interference within batches. 
Andes \citep{andes} addresses QoE for individual requests from the server side but lacks awareness of network jitter and device potential. These server-side advancements complement \disco{} design.

\paragraph{On-Device LLMs.}
Google's Gemini Nano \cite{gemini_nano} and Apple's Apple Intelligence \cite{appleintelligence} have been integrated to Android OS and iOS devices, respectively.
MLC-LLM \cite{mlc-llm} and llama.cpp \citep{llama_cpp} efficiently deploy various LLM on devices. PowerInfer \cite{powerinfer} and PowerInfer-2 \cite{powerinfer2} optimize on-device LLM inference speed by leveraging sparsity in model activations. \disco{} acts as a middle layer to schedule and migrate response generation between servers and devices.
