\section{Experiment Settings for End-to-end Cost}
\label{appendix:unified_cost}

For on-device LLMs, we quantify cost using FLOPs (floating-point operations). For on-server LLM services, we use their respective pricing rates at the time of experimentation. We set the energy-to-monetary conversion ratio (\textit{energy\_to\_money}) to 0.3 \$ per million FLOPs for server-constrained experiments and 5 \$ per million FLOPs for device-constrained experiments. To establish a comprehensive cost model that enables direct comparison between device and server computation costs, we analyze both the computational complexity of on-device models through detailed FLOPs calculations (Section~\ref{sec:flops_analysis}) and the pricing structures of commercial LLM services (Section~\ref{appendix:llm_pricing}). The generation length limit is set to 128.


\subsection{FLOPs of On-Device LLMs}
\label{sec:flops_analysis}

To accurately quantify the computational cost per token in both prefill and decode stages, we conduct a detailed FLOPs analysis using three representative models: BLOOM-1.1B, BLOOM-560M, and Qwen1.5-0.5B. All models share a 24-layer architecture but differ in other parameters: BLOOM-1.1B ($d_{\text{model}}=1024$, 16 heads, FFN dim=4096), BLOOM-560M ($d_{\text{model}}=512$, 8 heads, FFN dim=2048), and Qwen1.5-0.5B ($d_{\text{model}}=768$, 12 heads, FFN dim=2048).

\paragraph{Per-token FLOPs computation.}
The total FLOPs for processing each token consist of five components:
\begin{align}
    \text{FLOPs}_{\text{total}} &= \text{FLOPs}_{\text{attn}} + \text{FLOPs}_{\text{ffn}} \nonumber \\
    &\quad+ \text{FLOPs}_{\text{ln}} + \text{FLOPs}_{\text{emb}} + \text{FLOPs}_{\text{out}}
\end{align}

For a sequence of length $L$, the attention computation differs between stages. In prefill:
\begin{align}
    \text{FLOPs}_{\text{attn}} &= n_{\text{layers}} \cdot \Big(3d_{\text{model}}^2 + \frac{L^2 d_{\text{model}}}{n_{\text{heads}}} \nonumber \\
    &\quad+ L d_{\text{model}} + d_{\text{model}}^2\Big)
\end{align}

While in decode, KV caching eliminates the quadratic term:
\begin{align}
    \text{FLOPs}_{\text{attn}} &= n_{\text{layers}} \cdot \Big(3d_{\text{model}}^2 + \frac{L d_{\text{model}}}{n_{\text{heads}}} \nonumber \\
    &\quad+ L d_{\text{model}} + d_{\text{model}}^2\Big)
\end{align}

Table~\ref{tab:prefill-decode} presents the total FLOPs across different sequence lengths. The decode phase maintains constant FLOPs regardless of sequence length due to KV caching, while prefill phase FLOPs increase with sequence length. A breakdown of computational cost by component (Table~\ref{tab:components}) reveals that embedding and output projection operations account for the majority of FLOPs, particularly in models with large vocabularies.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{lccc}
    \toprule
    \textbf{Length} & \textbf{BLOOM-1.1B} & \textbf{BLOOM-560M} & \textbf{Qwen-0.5B} \\
    \midrule
    \multicolumn{4}{l}{\textit{Prefill Phase}} \\
    L = 32 & 0.85 & 0.45 & 0.39 \\
    L = 64 & 0.93 & 0.50 & 0.45 \\
    L = 128 & 1.25 & 0.65 & 0.69 \\
    \midrule
    \multicolumn{4}{l}{\textit{Decode Phase}} \\
    L = 32 & 0.82 & 0.42 & 0.37 \\
    L = 64 & 0.82 & 0.42 & 0.37 \\
    L = 128 & 0.82 & 0.42 & 0.37 \\
    \bottomrule
    \end{tabularx}
    \caption{Prefill and Decode FLOPs (billions)}
    \vskip -0.1in
    \label{tab:prefill-decode}
\end{table}

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{lccc}
    \toprule
    \textbf{Component} & \textbf{BLOOM-1.1B} & \textbf{BLOOM-560M} & \textbf{Qwen-0.5B} \\
    \midrule
    Embedding & 31.24 & 25.00 & 31.51 \\
    Attention & 13.01 & 10.00 & 16.56 \\
    FFN & 24.48 & 20.00 & 20.38 \\
    LayerNorm & 0.02 & 0.02 & 0.04 \\
    Output & 31.24 & 25.00 & 31.51 \\
    \bottomrule
    \end{tabularx}
    \caption{Component Ratios at L=128 (\%)}
    \vskip -0.1in
    \label{tab:components}
\end{table}

\subsection{LLM Service Pricing}\label{appendix:llm_pricing}
This section provides further details on the pricing of LLM services. Table \ref{tab:model-pricing} presents the pricing models for several commercial Large Language Models (LLMs) as of October 28, 2024. The pricing structure follows a dual-rate model, differentiating between input (prompt) and output (generation) tokens. These rates represent the public pricing tiers available to general users, excluding any enterprise-specific arrangements or volume-based discounts.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{lcccc}
    \toprule
    \textbf{Model} & \textbf{Vendor} & \textbf{Input price} & \textbf{Output price} \\
    \midrule
    DeepSeek-V2.5     & DeepSeek   & 0.14 & 0.28 \\
    GPT-4o-mini       & OpenAI     & 0.15 & 0.60 \\
    LLaMa-3.1-70b     & Hyperbolic & 0.40 & 0.40 \\
    LLaMa-3.1-70b     & Amazon     & 0.99 & 0.99 \\
    Command           & Cohere     & 1.25 & 2.00 \\
    GPT-4o            & OpenAI     & 2.50 & 10.0 \\
    Claude-3.5-Sonnet & Anthropic  & 3.00 & 15.0 \\
    o1-preview        & OpenAI     & 15.0 & 60.0 \\
    \bottomrule
    \end{tabularx}
    \caption{LLM service pricing (USD per 1M Tokens). Input prices refer to tokens in the prompt, while output prices apply to generated tokens.}
    \label{tab:model-pricing}
\end{table}
