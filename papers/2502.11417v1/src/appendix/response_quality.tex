\section{Response Quality}\label{appendix:accuracy-eval}

This section examines the quality of responses generated by \disco{}, with a particular focus on quality preservation during endpoint transitions. We first establish bounds on generation quality, then present our evaluation methodology, and finally demonstrate through extensive experiments that \disco{} maintains consistent quality across different model configurations and tasks.

\subsection{Quality Bounds}

A critical aspect of \disco{} is maintaining generation quality during endpoint transitions. We employ a systematic approach to quality preservation~\cite{diba2017weakly,gupta2022semi,chen2023frugalgpt}. Specifically, for endpoints A and B with quality metrics $Q_A$ and $Q_B$ (measured by LLM scores or ROUGE scores), we find that any migrated sequence M with quality $Q_M$ satisfies:

\begin{equation}
    \min(Q_A, Q_B) \leq Q_M \leq \max(Q_A, Q_B)
\end{equation}

This bound ensures that migration does not degrade quality beyond the capabilities of individual endpoints.

\subsection{Evaluation Methodology}

\paragraph{Evaluation Framework}
We establish a comprehensive assessment framework encompassing both automated metrics and LLM-based evaluation. Our framework evaluates two distinct tasks:

\begin{itemize}
    \item \textbf{Instruction Following}: We evaluate 500 data items from the Alpaca dataset~\cite{alpaca} using our structured prompt template, with quality assessment performed by multiple LLM judges: Gemini1.5-pro, GPT-4o, and QWen2.5-72b-instruct.
    
    \item \textbf{Translation Quality}: We assess Chinese-to-English translation on 500 data items from Flores\_zho\_Hans-eng\_Latn dataset~\cite{nllb2022,goyal2022flores} using the ROUGE-1 metric.
\end{itemize}

These two tasks are popular on end-user devices. Understandably, for complex tasks such as advanced math reasoning, we notice DisCo can lead to accuracy drops compared to the on-server model due to the limited capability of the on-device models, yet still achieves better performance than the on-device counterpart. 

\paragraph{Experimental Setup}
We configure our experiments with:
\begin{itemize}
    \item A fixed maximum generation length of 256 tokens
    \item First endpoint's maximum generation length varied through [0, 4, 16, 64, 256] tokens
    \item Four model combinations: 0.5B-7B, 3B-7B, 7B-0.5B, and 7B-3B (prefix and suffix denote the model sizes of first and second endpoints respectively)
\end{itemize}

The generation transitions to the second endpoint when the first endpoint reaches its length limit without producing an end-of-generation token, creating natural boundary conditions for analysis.

For instruction-following tasks, we employ the following structured evaluation template:

\begin{footnotesize}
\begin{verbatim}
JUDGE_PROMPT = """Strictly evaluate the 
quality of the following answer on a scale
of 1-10 (1 being the worst, 10 being the 
best). First briefly point out the problem
of the answer, then give a total rating in
the following format.

Question: {question}

Answer: {answer}

Evaluation: (your rationale for the rating, 
as a brief text)

Total rating: (your rating, as a number 
between 1 and 10)
"""
\end{verbatim}
\end{footnotesize}


\subsection{Results and Analysis}

\subsubsection{Quality Metrics}
Our comprehensive evaluation reveals several key findings:
\begin{itemize}
    \item \textbf{Bounded Quality}: The combined sequence quality consistently remains bounded between individual model performance levels
    \item \textbf{Translation Performance}: ROUGE-1 scores maintain stability between 0.23 and 0.26
    \item \textbf{Instruction Following}: Scores show consistent ranges from 4 to 6
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/acc_translation.pdf}
    \\[12pt]
    \includegraphics[width=0.45\textwidth]{figs/acc_instruction-05b7b.pdf}
    \caption{Quality evaluation results of \disco{}. The top figure shows translation quality evaluation using ROUGE-1 scores, demonstrating that \disco{} consistently achieves higher quality than the on-device baseline. The bottom figure presents evaluation scores from different LLM judges on instruction-following capabilities, where each subplot represents a different model pair comparison with varied first-endpoint model's maximum sequence length. The consistent patterns across different LLM judges demonstrate the robustness of our evaluation framework.}
    \label{fig:accuracy}
\end{figure}
