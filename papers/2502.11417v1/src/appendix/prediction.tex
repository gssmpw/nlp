\section{Prediction-based Model Selection}\label{appendix:prediction}

This section provides a comparative analysis of several TTFT prediction methods. For selecting the endpoint with a lower TTFT for each request, TTFT prediction is imperative. For on-device inference, TTFT prediction is straightforward, as TTFT exhibits a linear relationship with prompt length. Conversely, on-server inference TTFT is characterized by high variability, rendering prediction challenging. Moreover, the prediction method itself must be computationally efficient, as its overhead also contributes to end-to-end TTFT.

Table~\ref{tab:model-comparison} presents a comparative analysis of four common lightweight time-series-based prediction methods applied to traces collected from three prevalent LLM services. Our correlation analysis (Table~\ref{tab:correlation-analysis}) revealed no significant correlation between prompt length and TTFT; thus, prompt length is omitted as a feature in these prediction methods. We demonstrate that none of these methods offers sufficient accuracy for TTFT prediction.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{p{3.5cm}cc}
    \toprule
    \textbf{Model} & \textbf{MAPE(\%)} & \textbf{MAE(s)} \\
    \midrule
    \multicolumn{3}{c}{\textbf{Command}} \\
    \midrule
    Moving Average & 39.40 & 0.0899 \\
    ExponentialSmoothing & 53.51 & 0.1047 \\
    Random Forest & 39.33 & 0.0966 \\
    XGBoost & 35.43 & 0.0905 \\
    \midrule
    \multicolumn{3}{c}{\textbf{DeepSeek-V2.5}} \\
    \midrule
    Moving Average & 27.80 & 0.3959 \\
    ExponentialSmoothing & 27.39 & 0.3771 \\
    Random Forest & 32.97 & 0.4745 \\
    XGBoost & 27.51 & 0.4001 \\
    \midrule
    \multicolumn{3}{c}{\textbf{GPT-4o-mini}} \\
    \midrule
    Moving Average & 24.55 & 0.0995 \\
    ExponentialSmoothing & 20.88 & 0.0844 \\
    Random Forest & 28.68 & 0.1128 \\
    XGBoost & 24.83 & 0.0997 \\
    \midrule
    \multicolumn{3}{c}{\textbf{LLaMA-3-70b-Instruct}} \\
    \midrule
    Moving Average & 42.18 & 0.3312 \\
    ExponentialSmoothing & 40.27 & 0.3154 \\
    Random Forest & 49.67 & 0.3875 \\
    XGBoost & 43.94 & 0.3451 \\
    \bottomrule
    \end{tabular}
    \caption{Comparative analysis of Moving Average, Exponential Smoothing, Random Forest, and XGBoost prediction models across Command, DeepSeek, GPT, and LLaMA model traces. Metrics include Mean Absolute Percentage Error (MAPE) and Mean Absolute Error (MAE).}
    \label{tab:model-comparison}
\end{table}