\section{Evaluation}
\label{sec:evaluation}

Through extensive experimentation with four production-grade LLM services and state-of-the-art open-source models, we demonstrate \disco{}'s exceptional performance. Our rigorous evaluation spanning diverse deployment scenarios reveals that \disco{} delivers remarkable improvements - reducing both mean TTFT (6-78\%) and tail TTFT (11-52\%) while achieving cost savings of up to 83.6\% compared to existing approaches, all while preserving consistent token generation throughput.

\subsection{Evaluation Setup}
\paragraph{Testbeds and Workloads.} 
Our testbed is a server with 4 NVIDIA A40 GPUs, each featuring 48GB memory. 
We evaluate \disco{} using both commercial LLM traces and on-device deployments. For server-side evaluation, we collect traces from four production services: OpenAI's GPT-4o-mini \citep{gpt-4o-mini}, DeepSeek-V2.5 \citep{deepseek-v2_5}, Cohere's Command \citep{command}, and Hyperbolic-hosted LLaMA-3-70b-Instruct \citep{llama3-70b}. For on-device evaluation, we test three representative device-model configurations \citep{edgebenchmark}: Pixel 7 Pro with Bloom 1.1B (31.32/13.93 tokens/s for prefill/decode), Pixel 7 Pro with Bloom 560M (51.80/20.14 tokens/s), and Xiaomi 14 with Qwen 1.5 0.5B (79.90/21.47 tokens/s). These configurations span different compute-capability trade-offs in mobile environments. 
For end-to-end cost comparison, we quantify server costs using commercial API token pricing and device costs using FLOPs-based energy consumption. The detailed cost analysis can be found in Appendix~\ref{appendix:unified_cost}.

\paragraph{Baselines.} 
We compare \disco{} with four on-server, on-device, and cooperative deployments: 

\begin{denseitemize}
    \item \emph{vLLM}~\cite{vllm}: Processes all requests using remote server-based deployment. 
    
    \item \emph{llama.cpp}~\cite{llama_cpp}: Processes all requests using local device-based deployment.
    
    \item \emph{Stoch-S}: A server-constrained approach that randomly routes requests to the device while capping the server budget.
    
    \item \emph{Stoch-D}: A device-constrained approach that randomly routes requests to the server while capping the device budget.
\end{denseitemize}

For end-to-end cost comparison, we include two additional baselines: \emph{DiSCo-D w/o Migration} and \emph{DiSCo-S w/o Migration}.

\paragraph{Metrics.} 
We evaluate the system performance using both TTFT and TBT, including their mean and tail values. They are analyzed across varying cost budgets, defined as the ratio of input tokens processed by the constrained endpoint (device or server) to the total input tokens. 

For each experiment, we report the mean value over 10 runs. 

\subsection{End-to-end Performance}

\paragraph{\disco{} improves TTFT performance.} 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/e2e-ttft-mean-ab.pdf}
    \vskip -0.1in
    \caption{Mean TTFT reduction of \disco{} remains significant on DiffusionDB trace.}
    \vskip -0.1in
    \label{fig:e2e-ttft-diffusiondb}
\end{figure}

Figure~\ref{fig:e2e-ttft} and Table~\ref{tab:tail-ttft} show that \disco{} significantly outperforms baseline methods in both device- and server-constrained settings, showing improvements across mean and tail (P99) TTFT metrics for various services, including GPT, LLaMA, DeepSeek, and Command. In the GPT experiments, \disco{} demonstrates particularly notable tail latency reductions, decreasing P99 TTFT by up to 40\% relative to Stochastic dispatching across all device configurations, while mean TTFT is also reduced substantially, with reductions between 20-30\% across diverse budget ratios.
In the LLaMA setup, we observe a unique trade-off pattern. For budget ratios below 20\% when the device is the constrained endpoint, \disco{} exhibits a slightly higher mean TTFT than the baseline. This outcome is intentional, as \disco{} prioritizes tail latency reduction in low-budget scenarios, yielding substantial gains in P99 TTFTâ€”reducing tail latency by up to 50\%. This prioritization enables more responsive performance under restrictive budget conditions.

DeepSeek and Command experiments demonstrate similar patterns of improvement as the previous two traces, with \disco{} consistently outperforming baseline approaches. In the DeepSeek scenario, \disco{} maintains stable latency even as the budget ratio increases, whereas the baseline systems show increasing latency variance. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/e2e-ttft-mean-new3.pdf}
    \vskip -0.1in
    \caption{Mean TTFT tested using four traces. \disco{} achieves superior TTFT performance than the baselines.
    }
    \vskip -0.1in
    \label{fig:e2e-ttft}
\end{figure*}

 \begin{figure*}[t]
     \centering
     \includegraphics[width=\textwidth]{figs/e2e-cost-comparison.pdf}
     \vskip -0.1in
     \caption{The migration mechanism in \disco{} achieves superior end-to-end cost.
     }
     \vskip -0.1in
     \label{fig:e2e-cost}
\end{figure*}

\setlength{\tabcolsep}{1.5pt}
\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ccccc}
    \toprule
    \multirow{3}{*}{\textbf{Platform}} & \multirow{3}{*}{\textbf{Constraint}} & \multicolumn{3}{c}{\textbf{Tail TTFT Reduction}} \\
    & & \textbf{\thead{Pixel 7Pro \\ B-1.1B}}  &  \textbf{\thead{Pixel 7Pro \\ B-560M}} & \textbf{\thead{Xiaomi 14 \\ Q-0.5B}} \\
    \midrule
    \multirow{2}{*}{GPT} & Server & 23.85\% & 37.41\% & 44.04\% \\
    & Device & 26.39\% & 21.48\% & 16.32\% \\
    \hline
    \multirow{2}{*}{LLaMA} & Server & 11.08\% & 23.09\% & 26.29\% \\
    & Device & 35.67\% & 29.30\% & 21.29\% \\
    \hline
    \multirow{2}{*}{DeepSeek} & Server & 0.00\%* & 3.88\% & 15.53\% \\
    & Device & 30.91\% & 28.01\% & 25.08\% \\
    \hline
    \multirow{2}{*}{Command} & Server & 47.93\% & 50.93\% & 52.23\% \\
    & Device & 34.78\% & 31.53\% & 24.42\% \\
    \bottomrule
    \end{tabularx}
    \caption{Average reduction of tail TTFT compared to stochastic dispatching across the whole cost budget range. Devices include Pixel 7Pro and Xiaomi 14, while models include Bloom-1.1B, Bloom-560M, and Qwen-1.5-0.5B. (*Tail TTFT remains constant.)}
    \label{tab:tail-ttft}
\end{table}


\setlength{\tabcolsep}{4pt}
\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{ccccc}
    \toprule
    \multirow{2}{*}{\textbf{Trace}} & \multirow{2}{*}{\textbf{Constraint}} & \textbf{Mean} & \textbf{P99} & \textbf{TBT} \\
    & & \textbf{delay\_num} & \textbf{delay\_num} & \textbf{P99} \\
    \midrule
    \multirow{2}{*}{GPT} & Server & 4.21 & 9.40 & 0.209 \\
    & Device & 6.59 & 6.59 & 0.217 \\
    \hline
    \multirow{2}{*}{LLaMA} & Server & 5.53 & 11.00 & 0.209 \\
    & Device & 10.01 & 10.01 & 0.217 \\
    \hline
    \multirow{2}{*}{DeepSeek} & Server & 8.13 & 11.00 & 0.209 \\
    & Device & 17.17 & 17.17 & 0.217 \\
    \hline
    \multirow{2}{*}{Command} & Server & 3.25 & 8.00 & 0.209 \\
    & Device & 8.54 & 8.54 & 0.217 \\
    \bottomrule
    \end{tabularx}
    \caption{Performance metrics for different models under server and device constraints, showing the number of delayed tokens during migration and TBT (Time Between Tokens) P99 statistics. The average is computed over the requests that have performed the migration. 
    }
    \label{tab:performance-metrics}
\end{table}

\paragraph{\disco{} retains TBT performance while lowering the cost.} 
Table~\ref{tab:performance-metrics} evaluates \disco{}'s TBT performance across various traces under both server and device constraints. For requests involving migration, we measure two key metrics: the average number of migrations per request and the tail (P99) TBT latency. Results show that while migrations delay only a negligible number of tokens compared to typical generation lengths of hundreds or thousands of tokens, they do not impact the perceived token delivery smoothness, demonstrating \disco{}'s ability to maintain consistent streaming performance even during endpoint transitions.

As shown in Figure~\ref{fig:e2e-cost}, our token-level migration mechanism substantially reduces the end-to-end cost across all evaluated scenarios. For device-constrained cases (\disco{}-D), the migration mechanism achieves up to 72.7\% cost reduction compared to the non-migration baseline, with the improvement being most significant at higher budget ratios. Similarly, in server-constrained scenarios (\disco{}-S), the cost reduction reaches 83.6\%, particularly evident in DeepSeek and Command model deployments. These significant cost reductions are consistently observed across device-model pairs.

\subsection{Performance Breakdown and Ablation Study}

\paragraph{Impact of Prompt Sending Interval.} 
To evaluate our system under realistic workload patterns, we conduct experiments using stratified sampling based on request frequency from DiffusionDB \citep{diffusiondb}. Specifically, we select traces from ten users across different activity levels to capture diverse interaction patterns. We pair these real-world request intervals with prompts randomly drawn from the Alpaca dataset \citep{alpaca}. The results shown in Figure~\ref{fig:e2e-ttft-diffusiondb} demonstrate that \disco{}'s performance advantages persist across varying user activity patterns.

\paragraph{Quality of Generated Responses.}
We conduct comprehensive experiments using instruction-following tasks on multiple model configurations. We employ three LLM-based judges (GPT-4o, Gemini1.5-pro, and QWen2.5-72b) to assess response quality, and examine two representative migration scenarios: from a smaller to larger model (3B-7B) and vice versa (7B-3B). Figure~\ref{fig:accuracy_analysis} shows that \disco{} maintains quality scores across different sequence lengths, migration patterns, and judges. Detailed results are presented in Appendix~\ref{appendix:accuracy-eval}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/acc_instruction-3b7b.pdf}
    \vskip -0.1in
    \caption{Response quality evaluation. Each subplot represents a distinct model pair configuration (e.g., 3B-7B indicates migration from a 3B to a 7B model). The x-axis shows the maximum sequence length processed by the first endpoint before migration, while the y-axis shows the quality scores assigned by different LLM judges. Results demonstrate consistent quality preservation across various migration scenarios.}
    \vskip -0.1in
    \label{fig:accuracy_analysis}
\end{figure}

\paragraph{Scalability Analysis.}
We conducted comprehensive performance evaluations of \disco{}-D and \disco{}-S on a MacBook Pro with M1 processor, using both synthetic datasets and a real-world GPT trace of 1,000 records, across target frequencies from 0 to 1. 
To generate synthetic data that accurately reflects real-world scenarios, we fitted log-normal distributions to the prompt lengths and TTFT from the real trace by following the mean and standard deviation of the logarithm. 
As shown in Figure~\ref{fig:overhead}, for \disco{}-S, the execution time showed remarkable efficiency: 0.128 ms for the real trace with 1K samples, scaling to just 0.969 ms and 9.082 ms for synthetic datasets of 10K and 100K samples respectively. \disco{}-D, while being more computationally intensive, still maintained practical performance levels: 0.486 ms, 1.741 ms, and 14.856 ms for 1K, 10K, and 100K samples, respectively.

\begin{figure}
    \subfigure[\disco{}-D \label{fig:overhead_device}]{\includegraphics[width=0.5\columnwidth]{figs/overhead_device.pdf}}\hfill
    \subfigure[\disco{}-S \label{fig:overhead_server}]{\includegraphics[width=0.5\columnwidth]{figs/overhead_server.pdf}}
    \caption{\disco{}'s overhead is trivial and can scale well.}
    \vskip -0.1in
    \label{fig:overhead}
\end{figure}
