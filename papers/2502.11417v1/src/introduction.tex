\section{Introduction}

Large language models (LLMs) have revolutionized various applications, with over 60\% focusing on conversational interactions such as chatbots \citep{llm-market-report}. 
Meeting high serving demands requires scaling deployments across on-premise servers in the cloud and on-device inference, as seen in Apple Intelligence \citep{appleintelligence} and Google's Gemini Nano \citep{gemini_nano}.
The Quality of Experience (QoE) for interactive applications is primarily evaluated by two critical metrics: Time-To-First-Token (TTFT) in the prefill stage, which quantifies the initial response latency, and Time-Between-Token (TBT) during the decode stage, which measures the consistency of token delivery speed~\citep{databricks2023llm,andes,cachegen}.

On-server deployments lower serving costs by sharing infrastructure among many requests but often introduce unpredictable high latency due to request queueing delays~\citep{sarathi}, and the internet speed to end users fluctuates. While on-device deployment enables increasingly capable LLMs with sufficient accuracy, it suffers from slow processing speeds for long prompts and high energy consumption. For example, an iPhone running a 7B parameter LLM can operate for less than two hours on a full charge~\citep{liu2024mobilellm}.

This paper introduces a novel paradigm for cost-constrained device-server cooperative inference. We incorporate both server usage (e.g., monetary costs) and device energy costs via a dynamic exchange rate, which end users can adjust to balance response generation between the cloud and devices. 
As such, we can strategically distribute inference requests between endpoints and dynamically migrate ongoing token generation to maximize QoE. 
However, realizing this vision presents several fundamental challenges: 

\begin{denseitemize}
   \item \textbf{Unified Cost Management:} 
   The total serving cost combines heterogeneous resource expenditures from both endpoints---monetary costs from server API usage and energy costs from device computation. The relative value of energy costs varies dynamically based on device context (e.g., battery level, charging status) and user preferences for server spending, making it challenging to establish a unified optimization. 
   
   \item \textbf{Runtime Uncertainty:} 
   The dynamic nature of networks (e.g., latency jitters) and serving loads make it challenging to accurately predict TTFT for in-flight request migration. Moreover, any scheduling mechanism must be lightweight to avoid introducing large overhead to the already latency-sensitive services.
   
   \item \textbf{Migration Impact on Token Delivery:}
   While dynamic migration between endpoints can reduce overall running costs, it risks disrupting TBT. The challenge lies in determining when and how to perform migration while minimizing the degradation of user experience and the increase in costs.
\end{denseitemize}

\begin{figure}[t]
   \centering
   \includegraphics[width=0.4\textwidth]{figs/overview.pdf}
   \caption{\disco{} acts as a middleware to optimize QoE by adaptively dispatching and migrating response generation between device and server endpoints under cost constraints. 
   }
   \vspace{-.3cm}
   \label{fig:overview}
\end{figure}

As shown in Figure~\ref{fig:overview}, we introduce \disco{}, a \underline{D}ev\underline{i}ce-\underline{S}erver \underline{Co}operative scheduler that addresses these challenges via two key innovations:

\begin{denseitemize}
   \item \textbf{Cost-Aware Dispatching Policies:} 
   We introduce two dispatching mechanisms targeting different cost constraints. For server cost constraints, we employ a length-threshold based dispatching that routes requests shorter than a dynamically computed threshold to devices. For device energy constraints, we implement a delay-based dispatching mechanism where devices wait for a computed interval before starting local inference. Both mechanisms adapt their thresholds based on unified cost measures that combine server monetary costs and device energy consumption. 

   \item \textbf{Token-Level Migration Framework:} 
   We enable seamless generation handoff between endpoints through a novel migration protocol that preserves token delivery consistency. Our framework employs delayed migration timing to minimize interruption, while a token buffer ensures smooth delivery during transitions. This design maintains user experience while saving resource costs across endpoints.
\end{denseitemize}

Through extensive evaluation using real-world traces from commercial LLM streaming API services (including GPT and DeepSeek) and on-device deployments, we demonstrate that \disco{} improves mean and tail TTFT by up to 50\% without violation of TBT, significantly reducing the costs.

Overall, we make the following contributions:
\begin{denseitemize}
   \item We characterize QoE challenges in device-server cooperative LLM inference through extensive real-world measurements. 
   
   \item We design novel scheduling policies that optimize QoE under cost constraints. 
   
   \item We develop a token-level migration framework to enable generation handoff between endpoints, preserving token delivery consistency.
   
   \item We demonstrate \disco{}'s effectiveness in commercial services and open-source benchmarks.
\end{denseitemize}
