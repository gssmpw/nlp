\section{Characterizing LLM Inference}
\label{sec:characteristics}
This section characterizes LLM inference performance in on-server and on-device paradigms, which informs our design.

We evaluate four commercial streaming LLM APIs: OpenAI's GPT-4o-mini~\citep{gpt-4o-mini}, DeepSeek's DeepSeek-V2.5~\citep{deepseek-v2_5}, Cohere's Command~\citep{command}, and Hyperbolic-hosted LLaMA-3-70b-Instruct~\citep{llama3-70b}. For on-device analysis, we deploy Qwen-2.5-7B-Instruct~\citep{qwen2_5} and Llama-3.1-8B-Instruct~\citep{grattafiori2024llama3herdmodels} on both server-grade (NVIDIA A40, 48GB) and consumer-grade (dual NVIDIA RTX 3080, denoted as 3080x2) GPUs. 
We sample 1,000 requests from the Alpaca dataset~\citep{alpaca}, following a Poisson distribution with a mean request arrival interval of 30 seconds.

\begin{figure}
    \subfigure[On-Server TTFTs.]{\includegraphics[width=0.5\columnwidth]{figs/ttft_analysis_server.pdf}}\hfill
    \subfigure[On-Device TTFTs.]{\includegraphics[width=0.5\columnwidth]{figs/ttft_analysis_device.pdf}}
    \caption{On-device TTFT performance is more stable.}
    \label{fig:ttft_repeat}
\end{figure}

\paragraph{TTFT characteristics.} 
Our measurements reveal contrasting TTFT patterns between on-device and on-server inference. As shown in Figure~\ref{fig:ttft_repeat}, on-device inference exhibits stable TTFTs when processing identical prompts at 60-second intervals, primarily reflecting the prefill duration due to dedicated local hardware resources. In contrast, on-server inference experiences high variations and significant tail latency, attributed to network delays, request queuing, and resource contention.

We summarize the TTFT performance of 1,000 requests in Table~\ref{tab:correlation-analysis}. We observe that on-device TTFT scales linearly with prompt length due to hardware constraints~\citep{edgebenchmark}, while on-server TTFT shows minimal prompt-length sensitivity through advanced resource scaling~\citep{distserve,splitwise,memserve}.

\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{lcc}
    \toprule
    \textbf{Model} & \textbf{Deployment} & \textbf{Pearson Coef.} \\
    \midrule 
    Command & Server & 0.0142 \\
    GPT-4o-mini & Server & 0.0236 \\
    DeepSeek-V2.5 & Server & -0.0273 \\
    LLaMA-3-70b-Instruct & Server & 0.0402 \\
    \hline
    LLaMA-3.1-8b-Instruct & Device & 0.8424 \\
    \bottomrule
    \end{tabularx}
    \caption{Pearson coefficient between prompt length and TTFT in on-server deployment is weak.}
    \vskip -0.1in
    \label{tab:correlation-analysis}
\end{table}

\paragraph{TBT characteristics.}
TBT characterizes the I/O-bound decode stage latency. Analysis of temporal samples and distributions across six setups (Figure~\ref{fig:tbt_analysis}) reveals higher TBT variability in on-server inference compared to on-device execution. More importantly, both deployment approaches achieve generation speeds exceeding user consumption rates~(\S \ref{sec:llm_applications}), making cooperative serving practical.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figs/tbt_analysis.pdf}
    \vskip -0.1in
    \caption{On-device TBT performance is more stable. \footnotemark[1]{}
    }
    \vskip -0.1in
    \label{fig:tbt_analysis}
\end{figure}

\footnotetext[1]{On-server inference, such as in GPT, streams tokens with each packet containing multiple tokens, resulting in near-zero perceived TBTs.}

\paragraph{Opportunities and challenges.}
Our studies further reveal that as on-device models continue to improve---often fine-tuned for specific tasks~\citep{appleintelligence,liu2024mobilellm}---their performance increasingly matches that of on-server models in popular applications like instruction-following and translation (detailed in \S\ref{sec:evaluation} and Appendix~\ref{appendix:accuracy-eval}). However, deploying these models on-device introduces challenges such as long prefilling latency and startup overhead.

On the other hand, our real-world studies of conversational workloads highlight key opportunities: (i) on-server TTFT is largely unpredictable and shows minimal correlation with prompt length, whereas on-device TTFT scales nearly linearly with prompt length and is highly predictable; and (ii) both paradigms achieve token generation speeds that exceed typical user consumption rates.

Taking these findings together---particularly the predictable performance of on-device inference and the elastic scaling capabilities of server-based inference---we observe opportunities for optimization in cost-constrained device-server cooperative serving. Dynamic request migration between server and device endpoints during response generation can yield significant cost savings. 
