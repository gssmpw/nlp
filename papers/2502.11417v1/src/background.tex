\section{Background and Motivation}

\subsection{LLM Token Mixture and Routing}
\label{sec:llm_routing}
Device-server collaborative approaches have evolved along two directions. First, systems like EdgeShard~\cite{zhang2024edgeshard} and WDMoE~\citep{xue2024wdmoe} partition LLMs across multiple endpoints when a single device cannot host the entire model. LLMCad~\cite{xu2023llmcad} uses on-device models to reduce server costs, while PerLLM~\citep{yang2024perllm} optimizes energy consumption across devices and servers under constraints. Second, routing-based approaches~\citep{routellm,hybridllm} balance cost and accuracy by directing simple requests to small models and complex queries to advanced ones. However, these approaches do not optimize token delivery metrics (TTFT and TBT) under cost constraints.

\subsection{LLM-Based Text Streaming Applications}
\label{sec:llm_applications}
Over 60\% of LLM-backed applications focus on streaming conversational interactions, such as chatbots, virtual assistants, and language translation.
QoE in these text streaming services is often quantified by two critical metrics: time-to-first-token (TTFT) for \emph{initial responsiveness} and time-between-tokens (TBT) for \emph{delivery smoothness} throughout the entire interaction timeline.

Current LLM systems struggle to meet user expectations for these metrics, with TTFTs ranging from hundreds of milliseconds to over ten secondsâ€”far exceeding the ideal latencies of tens of milliseconds for interactive applications~\cite{maki2004latency,latency_comp_surv}. Token consumption patterns vary by output modality: in visual text scenarios, reading speeds differ across demographic groups, with the majority (52\%) aged 25-44 reading at 4-5 tokens per second, while older groups generally read more slowly~\cite{andes,andes_read,word_to_token}. Audio output consumption shows more consistency, averaging 3-4 tokens per second across languages~\cite{andes,andes_speak,average-speaking-rate}. Notably, conventional evaluation metrics like token generation throughput or average time-per-output-token provide incomplete insights, as they fail to capture the crucial relationship between token delivery timing and actual user consumption patterns.


\subsection{Limitations of Existing Text Streaming Applications}
\label{sec:existing_limitations}
Existing LLM serving relies on two deployment paradigms: on-server and on-device inference. With rapid hardware and software advancements, on-device LLMs have achieved accuracy levels sufficient for many applications, as evidenced by the integration of Apple Intelligence~\citep{appleintelligence} and Google's Gemini Nano~\cite{gemini_nano} into iOS and Android platforms, where they effectively handle text completion and message composition tasks. While on-device LLMs may still be inadequate for complex tasks (e.g., advanced mathematical reasoning), we focus on the growing category of applications where current on-device models already achieve satisfactory accuracy. For these applications, the challenge is not model capability, but rather the substantial monetary or energy cost demands of LLM inference.

Unfortunately, both serving paradigms face challenges. On-device inference, despite enabling faster generation owing to its dedicated resources~\citep{powerinfer,powerinfer2}, suffers from extended TTFT for long prompts due to limited processing speeds, and substantial energy consumption that scales linearly with response lengths \citep{edgebenchmark}. For instance, a fully charged iPhone running a 7B parameter LLM can operate for less than two hours \citep{liu2024mobilellm}---insufficient for day-long mobile use.

On the other hand, on-server deployments require request batching to amortize costs due to the high resource demands, but this introduces issues like queuing delays, resource contention from batching \citep{orca,vllm,sarathi}, and last-hop network latency variations \cite{eloquent}. Our measurements reveal that these factors can cause significant TTFT spikes for GPT-4-mini, from 0.3 seconds to several seconds during high-load periods.

Given these complementary limitations, we investigate the following research question: \emph{Can a cooperative paradigm be designed to combine on-server and on-device inference to improve QoE while managing both energy and monetary costs?} 
