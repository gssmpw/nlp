\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[final]{ACL2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{amsthm} 
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{url}

\newenvironment{denseitemize}{
\begin{itemize}[topsep=2.5pt, partopsep=0pt, leftmargin=1.5em]
 \setlength{\itemsep}{2.5pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{denseenum}{
\begin{enumerate}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
 \setlength{\itemsep}{2pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newcommand{\disco}{{\emph{DiSCo}}}

\lstset{
   language=Python,
   basicstyle=\small\ttfamily,
   identifierstyle=\color{black},
   keywordstyle=\color{black},
   stringstyle=\color{black},
   commentstyle=\color{black},
   morekeywords={keeper_service},
   keywordstyle=\color{purple},
   breaklines=true,
   frame=single,
   columns=flexible,
   xleftmargin=0pt,
}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\setlength\titlebox{5cm}

\title{\disco: Device-Server Collaborative LLM-Based Text Streaming Services}

\author{Ting Sun$^{1}$,
Penghan Wang$^{1}$, 
Fan Lai$^{1}$
\\
\textsuperscript{1} University of Illinois Urbana-Champaign, United States \\
\texttt{fanlai@illinois.edu}
}

\begin{document}
\maketitle

\input{src/abstract.tex}
\input{src/introduction}
\input{src/background.tex}
\input{src/characterization.tex}
\input{src/disco_policy.tex}
\input{src/evaluation.tex}
\input{src/conclusion.tex}

\section{Limitations}
While \disco{} demonstrates significant improvements in LLM serving efficiency, we acknowledge several important limitations of our current work:

\paragraph{Model Coverage.} We focus on scenarios where on-device LLMs achieve sufficient accuracy for target applications. While this covers many common use cases, \disco{} may not be suitable for applications requiring complex reasoning.

\paragraph{Energy Modeling.} For device energy consumption, we use a linear energy model based on FLOPs. Real-world device energy consumption patterns can be more complex, varying with factors such as battery state, temperature, and concurrent workloads.

\paragraph{Scalability Considerations.} Our current implementation and evaluation focus on single-device scenarios. Extending \disco{} to handle multi-device collaborative serving presents additional challenges in terms of coordination overhead and resource allocation that warrant further investigation.

\section{Ethical Considerations}
Our work focuses solely on optimizing the efficiency of LLM serving systems through device-server collaboration and does not introduce new language generation capabilities or content. All experiments were conducted using publicly available models and datasets. While our work may indirectly benefit the accessibility of LLM services by reducing costs and improving performance, we acknowledge that broader ethical considerations around LLM deployment and usage are important but outside the scope of this technical contribution.

\bibliography{main}
\bibliographystyle{acl_natbib}

\appendix
\input{src/appendix/related_work.tex}
\input{src/appendix/cold_start.tex}
\input{src/appendix/prediction.tex}
\input{src/appendix/response_quality.tex}
\input{src/appendix/unified_cost.tex}
\input{src/appendix/pseudocode.tex}

\end{document}