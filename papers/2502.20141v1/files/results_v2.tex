\section{Experiments}
\label{sec:exp}

In this section, we conduct empirical evaluations to study the performance of our approach in both handling noisy and corrupted views and in domain generalization tasks. 

\vspace{-1mm}
\subsection{Comparison with CL Baselines}
\vspace{-1mm}

\paragraph{Experiment setup.}~To examine the robustness of our framework, we trained INCE and RINCE as baselines, and developed their GCA-based alternatives (+\ours). In addition, we also compared with our novel GCA-UOT method, two variants of IOT established in~\cite{shi2023understanding},  and other CL baselines, including BYOL and SimCLR. For experiments with SVHN \cite{netzer2011reading} and ImageNet100 \cite{deng2009imagenet} we use the ResNet-50 encoder as the backbone and use a ResNet-18 encoder as the backbone for CIFAR-10, CIFAR-100~\cite{krizhevsky2009learning} and a corrupted version of CIFAR called CIFAR-10C \cite{hendrycks2019benchmarking}. 

\input{files/table/normaltasks_table}


In all of these cases, we follow the standard self-supervised learning evaluation protocol \cite{chen2020simple}, where we train the encoder on the training set in an unsupervised manner and then train a linear layer on top of the frozen representations to obtain the final accuracy on the test set.
% The final test accuracy is obtained by passing the test set through both the frozen encoder and linear layer without finetuning.
In addition to standard data augmentation policies commonly used \cite{chuang2022robust}, we also apply three different extreme augmentation policies to examine the robustness of \ours~towards noisy views (details in Appendix~\ref{app:corruptset}). Learning rates and other training details for CIFAR-10, CIFAR-100, SVHN, and ImageNet100 are provided in Appendix~\ref{app:hpdetails}, while specific training details for CIFAR-10C are included in Appendix~\ref{app:corruptset}.




\vspace{-3mm}
\paragraph{Results on Standard Augmentations.}~ 
First, we performed experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet100 using standard sets of augmentations that are applied to achieve state-of-the-art performance (Table~\ref{tab:new_combine}, Standard Setting). 
We found the +GCA versions of INCE and RINCE exhibit performance gains in almost all settings except for SVHN, with bigger gains observed when adding GCA to RINCE. Additionally, we find that our unbalanced OT method, GCA-UOT, achieves the top performance across the board, on all four datasets tested. The transport plans obtained by each methods are provided in Figure~\ref{fig:diagnolline} along with a study of the sensitivity of the methods to hyperparameters (Appendix~\ref{fig:ablation_study}).

\vspace{-3mm}
\paragraph{Results on Corrupted Data and Extreme Augmentations.}~
Next, we tested the methods in two noisy settings. In the first set of experiments, we apply extreme augmentations to CIFAR-10 (Ex) and CIFAR-100 (Ex) (see Appendix~\ref{app:corruptset}) to introduce noisy views during training. In the second set of experiments, we used the CIFAR-10C to further test the ability of our method to work in noisy settings. 


Our experimental results demonstrate that the GCA-based strategy effectively enhances the model's generalization ability and adaptability to aggressive data augmentations. 
In addition to improving classification accuracy, the GCA-based methods also improve the representational alignment and uniformity, as shown in Appendix~\ref{app:representation}. This observation is in line with our theoretical analysis in Section~\ref{subsec:align_uni}, where we show that the obtained representations provide better overall alignment of positive views and better spread in terms of uniformity \cite{wang2020understanding}.

\vspace{-1mm}
\subsection{Block Diagonal Transport in Domain Generalization}
\vspace{-1mm}
\label{sec:domaingen}

\begin{wrapfigure}{r}{0.38\textwidth} 
\vspace{-8mm}
    \centering{
       \includegraphics[width=0.54\linewidth, clip, trim=0 1.3cm 25cm 0cm]{files/imgs/figure_v3.png}

 \includegraphics[width=\linewidth, clip, trim=13.7cm 0 0 0]{files/imgs/figure_v3.png}}
        \caption{\footnotesize{ {\em Incorporating different priors into learning across multiple domains.}} (A) Example target alignment plan \(\PP_{\text tgt}\), where the target over all samples from the same domain are set to \( \alpha \), the diagonal values are set to 1, and across-domain samples are set to \(\beta\). (B) The domain classification accuracy (red) and overall class accuracy (blue) with (\(\alpha-\beta\)) increases.}
        \vspace{-7mm}
        \label{fig:Ptgt}

\end{wrapfigure}

In a final experiment, we aimed to demonstrate the flexibility and robustness of our framework by applying it to a domain generalization task, where samples originate from different domains (e.g., Photo, Cartoon, Sketch, Art). We explored the effects of introducing domain-specific alignment constraints in our transport plan, hypothesizing that this could enhance the latent space organization to capture more nuanced domain similarities.

Our approach enables additional contextual information to be seamlessly integrated into the transport process. In this case, domain information was incorporated to distinguish the alignment of samples from the same versus different domains. To achieve this, we adjusted the target transport plan \({\bf P}_{tgt}\), selectively modifying parameters \((\alpha, \beta)\) to vary the influence of domain-based alignment constraints as shown in Figure~\ref{fig:Ptgt}(A). Specifically, we set \(\{\alpha=0, \beta>0\}\) to prioritize cross-domain alignment and \(\{\alpha>0, \beta=0\}\) to focus on intra-domain alignment.

The training was conducted on the PACS dataset \cite{li2017deeper} using a ResNet-18 encoder with the \ours-INCE objective. After training the encoder in an unsupervised manner, we freeze the encoder and then train a linear readout layer to predict either the sample’s class or the domain it belonged to. This setup allowed us to isolate the effect of our transport adjustments on the latent space’s capacity to encode both class and domain information.

The results, displayed in Figure~\ref{fig:Ptgt}(B), revealed that increasing the domain alignment weight enhances the accuracy of domain classification (from 72.11\% to 95.16\%) without diminishing classification performance. This outcome suggests that \ours~can effectively encode both domain and class information in a single latent representation. The ability to adjust alignment constraints provides a powerful tool for domain generalization tasks, enabling multiple types of similarity to be jointly encoded. This flexibility can potentially alleviate issues related to information loss from data augmentation, especially in fine-grained classification settings, by retaining essential domain-specific characteristics across transformations.


