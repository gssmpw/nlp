


\section*{Appendix}\label{app:outline}


\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\setcounter{appendixalgorithm}{1}
\renewcommand{\thealgorithm}{A\arabic{appendixalgorithm}}

\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC



\input{files/notations}


\subsection{Proximal operator setup}%~\hyperref[app:outline]{*}
\label{app:prox}
In this section, we are going to provide the detailed illustration about the proximal operators. How the proximal operator would convert to the projection. And how to solve the Bregman projection with KL divergence.

\subsubsection{Explanation of Definition 1 }

Let \( h: \mathcal{X} \rightarrow [-\infty, +\infty]\) be a proper, lower semi-continuous convex function on Hilbert space $\mathcal{X}$. The \textbf{proximal operator} of \(h\) at point \(\vv \in \mathcal{X}\) is defined as a unique minimizer of the function \(\x\mapsto h(\x)+\frac{1}{2}\|\x-\vv \|^2_2\)~\cite{parikh2014proximal}. For an instance, \( h \) is convex function relative to the constraint set \( \mathcal{B} \), like the indicator function \(h_\mathcal{B}\). Given the Euclidean norm on $\mathcal{X}$, we can write the proximal operator \(\text{Prox}_{h,\mathcal{B}}^{\|\cdot\|^2}(\mathbf{v}) \) as the most common way:
\[ \text{Prox}_{h,\mathcal{B}}^{\|\cdot\|^2}(\mathbf{v}) = \arg\min_{\mathbf{x} \in \mathcal{B}} \left\{ h(\mathbf{x}) + \frac{1}{2} \|\mathbf{x} - \mathbf{v}\|^2_2 \right\} \]

The solution to the proximal operator exists and is unique due to the strong convexity of the above function.

\subsubsection{Connection to the projection}
Here, if we treat \(h(x)\) as the indicator functions of the constraint set \(\mathcal{B}\) as:
\[
h(x) = 
\begin{cases} 
0, & \text{if } x \in \mathcal{B} \\
\infty, & \text{if } x \notin \mathcal{B}.
\end{cases}
\]
Then the proximal operators problem could be understood intuitively as finding the "shortest distance" between the point \(v\) with the constraint set \(\mathcal{B}\), which means the projection. And the following lemma holds:
\begin{lemma} \label{lm:unique}
    If the constraint set \(\mathcal{B}\) is closed and convex, then the projection of point \(v\) is unique on \(\mathcal{B}\).
\end{lemma}

\textbf{Proof of the Lemma~\ref{lm:unique}}: This lemma can be proved by the strict convexity of the proximal operator.






\begin{figure}[ht]
\centering
    \includegraphics[width=0.8\linewidth]{files/imgs/proximal.png}
\caption{ \footnotesize{{\em Illustration of the proximal operators}  A. Visualization of proximal operators in $\mathbb{R}^3$. On the surface defined by $h(x, y) = x^2 + y^2$ within the domain constraints $-1.2 < x < 1.2$ and $-1.2 < y < 1.2$. If $v=v_1=(0.76, 0.76, 1.16)$, it lies within the domain of $h$, represented on the surface at the exact location matching its third coordinate with $h(x, y)$. If $v=v_2=(1.5, 1.5, 6)$, which is outside the feasible region defined by $h$, the proximal operator projects it to the closest point within the domain, resulting in $v_2$'s projection to approximately $(0.85, 0.85, 1.45)$.  B. Visualization of proximal operators in $\mathbb{R}^2$.} The blue dashed line represents the function $h(x) = x^2$. The orange dash-dotted line illustrates the penalty term $\frac{1}{2} \|x - v\|^2$ with $v = (2, 0)$, indicating the squared distance from any $x$ to $v$. The green solid line is the proximal operator $2x^2 + \frac{1}{2} \|x - v\|^2$, which gets close to the minimization point of h(x) from $v$. The red point marks the $\text{Prox}_h(v)$ in this space.}
    \label{fig:proximal}
\end{figure}


\subsubsection{Connection to the Bregman divergence}
First, we define $d_\Gamma$ as a generic Bregman divergence on some convex set $\mathcal{B}$, and the proximal map of a convex function $d_\phi$ according to this divergence is:
\begin{equation}
\text{Prox}^{d_\Gamma}_{d_\phi,\mathcal{B}}(\K) := \arg\min_{\PP \in \mathcal{B}} d_\Gamma( \PP \|\K) + d_\phi( \PP).
\end{equation}
$\Gamma$ is a strictly convex function smooth on $\text{int}(\mathcal{B})$, and $\text{Prox}^{d_\Gamma} _{d_\phi}(\K) \in \text{int}(\mathcal{B})$ is always uniquely defined by strict convexity. (Note that this theory is general and does not need to parametrize the \(\K\) and \(\PP\) as models with \(\theta\)). As $\mathcal{B}= \text{dom}(\Gamma)$, $$\forall (\PP,\K) \in \mathcal{B} \times \text{int}(\mathcal{B}), d_\Gamma(\PP\|\K) = \Gamma(\PP) - \Gamma(\K) - \langle \nabla \Gamma(\K), \PP - \K \rangle,$$
which has its Legendre transform is also smooth and strictly convex: $$\Gamma^*(\rho) = \max_{\PP \in \mathcal{B}} \langle \PP, \rho \rangle - \Gamma(\PP)$$
 The Bregman divergence for a convex function \( \Gamma \) between points \( \x \) and \( \y \) is defined as:
\[
d_\Gamma(\x, \y) = \Gamma(\x) - \Gamma(\y) - \langle \nabla \Gamma(\y), \x - \y \rangle
\]
where  \( \nabla \Gamma(\y) \) is the gradient of \( \Gamma \) at \( \y \). Giving the squared L2 distance can be viewed as a Bregman divergence derived from the convex function \( \Gamma(\x) = \|\x\|^2 \). For this function, the Bregman divergence between two points \( \x \) and \( \y \) becomes:
\[
d_\Gamma(\x, \y) = \|\x\|^2 - \|\y\|^2 - 2\y^\top(\x - \y)=\|\x - \y\|^2
\]


\begin{table}[h]
\centering
\caption{\footnotesize {\em Examples of functions $\Gamma$ and their corresponding divergences $d_\Gamma$.}}
\begin{tabular}{c|c|c}
\hline
\textbf{$\Gamma$} & \textbf{$d_\Gamma$} & \textbf{Description} \\ \hline
$\|\x\|^2$ & $\|\x - \y\|^2$ & squared Euclidean distance \\
$\x \ln \x$ & $\y \ln \frac{\y}{\x} - (\y - \x)$ & Kullback–Leibler (KL) divergence \\
$-H(p) = \sum_j p_j \ln p_j$ & $KL(q\|p) = \sum_j q_j \ln \frac{q_j}{p_j}$ & KL divergence between distributions $p, q$ \\
 & $\sum p_j = \sum q_j = 1$ &  \\ \hline
\end{tabular}
\end{table}


\subsubsection{Connection to the Bregman projection}
Bregman projections solve the alignment problem onto the two constraints sets that encode the marginals along the rows and columns ~\cite{benamou2015iterative, bregman1967relaxation, peyre2019computational}.
 \begin{equation}\label{eq:birkhoffapp}
    C_1^\mu \coloneqq \{\PP : \PP\mathbbm{1}_B = \mu\}, C_2^\nu \coloneqq \{\PP : \PP^\top \mathbbm{1}_B = \nu\}
\end{equation}

If we specify the constraint set \(\mathcal{B}\) as some set \(C_1^\mu \coloneqq \{\PP : \PP\mathbbm{1}_m = \mu\}\), and select the \(h_\mathcal{F}(x)\) as some indicator function of \(C_1^\mu\), which satisfies:
\begin{equation}~\label{eq:indf}
h_\mathcal{F}(\x ) = 
\begin{cases} 
0, & \text{if } \x\in \C_1^\mu \\
\infty, & \text{if } \x \notin \C_1^\mu.
\end{cases}
\end{equation}

For the first step Bregman projection \(\text{Prox}_{C_1^\mu}^{\text KL}(\K)\) onto the set $\mathcal{C}_1^{\mu}$ with indicator function in Equation~\eqref{eq:indf}:
\begin{equation}~\label{eq:breg1}
\PP^{(1)}=\text{Prox}_{C_1^\mu}^{\text{KL}}(\K_\theta)  = \arg\min_{\PP\in C_1^\mu} \{ h_\mathcal{F}(\PP)+\text{KL}(\PP\|\mathbf{\K})\} = \arg\min \{\text{KL}(\PP\|\K): \PP\mathbbm{1}_B=\mu \}.
\end{equation}
We can minimize the function in Equation~\eqref{eq:breg1} with Lagrange multiplier \(f\) on \(C_1^\mu\):
\begin{equation}\label{eq:oriprob}
\varepsilon\text{KL}(\PP\|\K)- f(\PP\mathbbm{1}_B -\mu),
\end{equation}
Then we get \(\PP^{(1)}\) as the minimizer through the derivatives with respect to \(\PP\), we have 
\begin{equation}
\varepsilon\log\left(\PP^{(1)}/ \K\right) - f\mathbbm{1} = 0 \Rightarrow \PP^{(1)} = \uu\K,~\text{as}~\uu = e^{f/\varepsilon} > 0 , 
\end{equation}
and we can use these relationship into the constraints sets with $\PP^{(0)} = \diag(\mathbbm{1}) {\bf K} \diag(\mathbbm{1})$.
\begin{equation}\label{eq:scaleapp}
 \langle \PP^{(1)}, \mathbbm{1}\rangle = \mu \Rightarrow \langle \uu^{(1)}\K, \mathbbm{1}\rangle = \mu, \uu^{(1)} = \frac{\mu}{\sum_i \K_{ij}},  \PP^{(1)} =  \text{diag}\left(\frac{\mu}{\PP^{(0)}\mathbbm{1}_B}\right)\PP^{(0)}, 
\end{equation}

If we repeat this progress for the set \(C_2^\nu \coloneqq \{\PP : \PP^T\mathbbm{1}_n = \nu\}\), we will get the \(\text{Prox}_{C_2^\nu}^{\text{KL}}(\PP^{(t+1)})\).
 And in the second step, we project onto the second constraint set \(C_2^\nu\) with indicator function \(h_\mathcal{G}(x)\) defined on \(C_2^\nu\) and get:
% \begin{equation}
% \PP^{(2)} \coloneqq \text{Prox}^\text{KL}_{C_2^\nu}(\PP^{(1)})
% \end{equation} 
\begin{equation}\label{eq:Bregmaniterations}
\PP^{(2)} \coloneqq \text{Prox}^\text{KL}_{C_2^\nu}(\PP^{(1)}) = \PP^{(1)} \text{diag}\left(\frac{\nu}{\PP^{(1)\top}\mathbbm{1}_B}\right).
\end{equation}
Iterating over these two sets of projections \(\PP^{(t+1)} \coloneqq \text{Prox}^\text{KL}_{C_1^\mu}(\PP^{(t)})\) and \(\PP^{(t+2)} \coloneqq \text{Prox}^\text{KL}_{C_2^\nu}(\PP^{(t+1)})\) until convergence could be summarized as Sinkhorn algorithm with \(t\) via recursive form:
\begin{equation}\label{eq:uvupdatesapp}  
\uu^{(t+1)} \stackrel{\text { def }}{=} \frac{{\mu}}{\mathbf{K} \vv^{(t)}}, \quad \vv^{(t+1)} \stackrel{\text { def }}{=} \frac{{\nu}}{\mathbf{K}^{\mathrm{T}} \uu^{(t+1)}}, \quad \PP^{(2t+2)}=\operatorname{diag}(\uu^{(t+1)})\mathbf{K}\operatorname{diag}(\vv^{(t+1)}). 
\end{equation}

The Sinkhorn algorithm is composed with two steps Bregman projection, 
Similarly, we can write out this recursive relationship as: \(\PP^{t+1}\) can be updated with dual variables \(f\), \(g\) and \(\uu^{(t)}=e^{f^{(t)}/\varepsilon}, \vv^{(t)}=e^{g^{(t)}/\varepsilon}\). The set $ U(\mu, \nu) = C_1^\mu \cap C_2^\nu$, representing the feasible transport plans with given marginals. It could be any random sets, i. e. $\mathcal{B} = C_1^\mathbbm{1} \cap C_2^\mathbbm{1}$ denote the Birkhoff polytope of doubly stochastic matrices where $\mu = \mathbbm{1}$ and $\nu = \mathbbm{1}$ are the uniform distributions with all one element.







\subsection{Background on OT}%~\hyperref[app:outline]{*}
\label{app:ot}
This section defines discrete and continuous optimal transport. Since the section~\ref{sec:otppm} lacks a discrete OT definition, we discuss it here and show the equivalence between solving Bregman projection and the entropy-regularized OT (EOT) problem.

To support convergence proofs later, we introduce definitions of continuous measures. Symbols \( \mu \) and \( \nu \) may represent both discrete and continuous measures for intuitive consistency, with precise definitions at the start of each subsection.




\subsubsection{Background on discrete OT}
\label{app:descrete_ot}
In section~\ref{sec:otppm}, we provide a general definition of the discrete optimal transport. Here, we specifically define the optimal transport on the representations space after an encoder \(f_\theta\) with two augmented views \((\x',\x'')\).  As we mainly discussed the distribution on the representation space, so here we suppose there is an encoder \(f_\theta\) will project the augmented views into the latent. 
Here we define \( \mu = \sum_{i=1}^{N} \delta_{f_\theta(\x'_i)} p_i, \nu = \sum_{i=1}^{N} \delta_{f_\theta(\x''_i)} q_i, \) with vectors \( p \) and \( q \) in a simplex \( \Delta_{B} \) in \( \mathbb{R}^{B} \) defined by \( \Delta_B := \left\{ v \in \mathbb{R}^B : v_i \geq 0, \sum_{i=1}^B v_i = 1 \right\}\). Here, \(\PP\) is a \(B\times B\) joint coupling matrix of the marginal distributions \(\mu\) and \(\nu\), which describes how much mass is needed to convert one distribution to match another. \(\C\) is a \(B\times B\) cost matrix calculated by the cost function \(c(x,y)\) i.e. cosine dissimilarity, and we can write the OT problem as the constrained linear programming problem: 
\begin{align}\label{eq:dot}
   \min_{\bf{P}}\langle\bf{P},\C \rangle ~~\text{s.t. } \bf{P}\mathbbm{1} = \mu,~\bf{P}^\top\mathbbm{1} = \nu.
\end{align}

Even though directly solving Equation~\eqref{eq:dot} is high computational complexity $O(n^3)$, we introduce a common relaxation called entropic regularization to smooth the transport plan.


\subsubsection{Entropy regularized OT and the Sinkhorn algorithm.} Solving the exact OT problem above can be very computationally intensive.  In this case, we can add the Shannon entropy $H(\PP)= -(\PP_{ij} (\log(\PP_{ij}))$ to our objective in Equation~\eqref{eq:dot} and obtain an approximation of entropy-regularized optimal transport (EOT) plan as: 
\begin{equation}\label{eq:eotapp}
\min_{\PP \in \mathcal{B}} ~ \langle\PP,\C\rangle -\varepsilon H(\PP), \quad \text{where}~H(\PP)= -\sum \PP_{ij} \log(\PP_{ij}),
\end{equation}
where $\varepsilon$ is a user specified parameter that controls the amount of smoothing in the transport plan. The cost matrix \(\C\) could be transformed into the Gibbs kernel matrix \(\K\) on a Hilbert space with the given formula, 
\begin{equation}\label{eq:gibbs}  
\K_{ij} =\exp\left(-\varepsilon^{-1}\C_{ij}\right) 
\end{equation}

To solve (\ref{eq:eotapp}) under the kernel space induced by \(\K\), we can use the  iterative Sinkhorn algorithm with the initialization of $\uu^{(0)}$ and $\vv^{(0)}$ as all one vector divided by the batch size, and the update rules:
\begin{align}
\uu^{(t+1)} \stackrel{\text { def }}{=} \frac{{\mu}}{\mathbf{K} \vv^{(t)}} \text { and } \vv^{(t+1)} \stackrel{\text { def }}{=} \frac{{\nu}}{\mathbf{K}^{\mathrm{T}} \uu^{(t+1)}},
\end{align}
Then, the output of plan after $t$ iterations is 
\begin{equation}\label{Pt}
    \PP^{(t)}=\operatorname{diag}(\uu^{(t)})\mathbf{K}\operatorname{diag}(\vv^{(t)}).
\end{equation}
It also could be interpreted with dual variables \(f\) and \(g\):
\begin{equation}\label{eq:dual}
\PP^{(t)}_{i,j} = e^{f^{(t)}_i/\varepsilon} e^{-\C_{i,j}/\varepsilon} e^{g^{(t)}_j/\varepsilon}, \quad \uu^{(t)}=e^{f^{(t)}/\varepsilon}, \vv^{(t)}=e^{g^{(t)}/\varepsilon}
\end{equation}
After convergence, the resulting $\PP$ will be the optimal solution to Equation~\eqref{eq:eotapp}. The convergence and dynamics of OT and the dual formulation have been studied extensively in~\cite{berman2020sinkhorn, peyre2019computational, ghosal2022convergence, an2022efficient}. Here, iterations converge to a stable transport plan \(\PP^{(\infty)}\)as the optimal solution of Equation~\eqref{eq:eot}, which provides the minimum cost matching between two distributions. %(or sets in the discrete case). 
The convergence and dynamics of OT and its dual formulation have been studied extensively in~\cite{berman2020sinkhorn, peyre2019computational, ghosal2022convergence, an2022efficient}. Thus, these results guarantee that the iterates will converge to the optimal solution of the EOT objective, or that \(\PP^{(t)} \rightarrow \PP^{(\infty)}\) with \(t \rightarrow \infty\).  %and use it to measure the divergence between our source and target distributions.

This allows us to state the following lemma: 
\begin{lemma}\label{lm:eoteqbreg}
Solving the entropy optimal transport in Equation~\eqref{eq:eot} is consistent with iterative solving the Bregman projection.
\end{lemma}
\textbf{Proof of the Lemma~\ref{lm:eoteqbreg}}: 
Giving that some points \(\K\) and \(\PP\), their distance could be measured by KL divergence:
\[\text{KL}(\PP\|\K) = \sum_{ij} \PP_{ij} \log\left(\frac{\PP_{ij}}{\K_{ij}}\right) - \PP_{ij} + \K_{ij}\]
As \(\C_{ij} =-\varepsilon \log\K_{ij}\) in Sinkhorn, we can see find \(\PP\)  to minimize the Equation~\eqref{eq:eot} can be transformed into some formula about \(\K_{ij}\):
\begin{align}
 \min_\PP\langle\PP,\C\rangle -\varepsilon H(\PP) & = \min_\PP \sum_{i,j}  \C_{ij} \PP_{ij} +\varepsilon \sum_{i,j} \PP_{ij} (\log(\PP_{ij}) ) \\
&= \min_\PP \varepsilon \sum_{i,j} (- \PP_{ij}\log\K_{ij}+\PP_{ij} \log(\PP_{ij}))\\
&= \min_\PP \varepsilon \text{KL}(\PP\|\K) ~~\text{s.t. } \PP\mathbbm{1}= \mu,~\PP^\top\mathbbm{1} = \nu,
\end{align}
Consider the \(\K\) is a point in Hilbert kernel space, and \(\varepsilon \) is the constant, we set the \(\mu\) and \(\nu\) form the \(\mathcal{B}\), so here can have:
\begin{equation}~\label{eq:eoteqbreg}
\PP = \text{Prox}^{\text{KL}}_{\mathcal{B}}(\K)  = \arg\min_{\PP \in \mathcal{B}} \text{KL}(\PP||\K) =\arg\min_\PP \{\langle\PP,\C\rangle +\varepsilon H(\PP): \PP\mathbbm{1}= \mu,~\PP^\top\mathbbm{1} = \nu\}
\end{equation}





\subsubsection{Background on continuous optimal transport}
\label{app:continous}
To show the convergence of the Bregman projection, here we define the optimal transport problem with the continuous measure. 
Inherit the definition of \(\mathcal{X}\) and \(\mathcal{Y}\) in the Appendix~\ref{app:notations}, finding the optimal transport between two continuous measure \(\mu\) and \(\nu\) could be transformed into some problems with the minimization of Kantorovich functional. 

\begin{definition}[\textit{Continuous optimal transport}] 
We redefine $\mu$ and $\nu$ be two probability measures on latent manifold $\mathcal{M}$ with Hölder continuous and strictly positive densities $e^{f}$ and $e^{g}$, respectively: \(\mu = e^{f} dM, \nu = e^{g}dM \), where $dM$ is the Riemannian normalized volume form on $\mathcal{X}$. For each \(x \in \mathcal{X}\) and \(y \in \mathcal{Y}\):
\begin{equation}
W(\mu, \nu) = \inf_{\pi \in \Pi(\mu, \nu)} \int_{\mathcal{X} \times \mathcal{Y}} c(x, y) \, d\pi(x, y).
\label{eq:wdist}
\end{equation}

\end{definition}

\begin{definition}[\textit{Dual of continuous OT}] The dual of standard OT reads:
\begin{equation}
W(\mu, \nu) = \sup_{f,g \in \mathcal{U}(c)} \int_{\mathcal{X}} fd\mu(x)+\int_{\mathcal{Y}} d\nu(y)
\label{eq:continousotdual}
\end{equation}
where the constraint set \(\mathcal{U}(c)\)  is defined by \(\mathcal{U}(c):=\{(\mu,\nu)\in\mathcal{C}(\mathcal{X})\times\mathcal{C}(\mathcal{Y})\}| f(x)+g(y) \leq c(x,y)\} \).
\end{definition}

Here, \(C(\mathcal{X})\) is the space of all continuous functions on \(\mathcal{X}\), the functions which measured using the supreme norm \(||f||_\infty\), with the Legendre transform:

 
 \begin{definition}[\textit{Legendre c-transforms}]
For the dual variables, or so called potentials, there exists the Legendre c-transforms:
\begin{equation}
\label{eq:legendre}
f^c(y) := \sup_{x \in \mathcal{X}} (-c(x, y) + f(x)), \quad g^c(x) := \sup_{y \in \mathcal{Y}} (-c(x, y) + g(y)).
\end{equation}
In which \( g^c(x)  \) and \( f^c(y) \) are Legendre c-transforms of \(g(y) \in \mathcal{C}(\mathcal{Y})\) and  \(f(x)\in \mathcal{C}(\mathcal{X})\) with cost function \(c(x,y)\).  
\end{definition}
 
 

    


\begin{definition}[\textit{Pushforward measure}]
The pushforward measure of \( \mu \) under the map \( T \), denoted as \( T_{\mu} \), is a measure on \( \mathcal{X} \) defined by \( T_\mu(B) = \mu(T^{-1}(B)) \) for any Borel set \( B \) in \( \mathcal{X} \). \( T_\mu = \nu \) when \( T \) is an optimal transport map. Following the similar way we can define the push-forward measure \(T_\mu\) and \(T_\nu\) as:
\begin{equation}
T_\mu : C(\mathcal{X}) \rightarrow C(\mathcal{Y}): = \log \int{e^{-c(x,\cdot)+f(x)}}{\mu(x)},~~T_\nu : C(\mathcal{Y}) \rightarrow C(\mathcal{X}): = \log \int{e^{-c(\cdot,y)+g(y)}}{\nu(y)} 
\end{equation}

\end{definition}

\begin{definition}[\textit{$\varphi$-divergence regularized OT in continuous}]\label{def:phidivergence}
Given two dual variables (also called potentials) \( f \in \mathbb{R}^n \) and \( g \in \mathbb{R}^m \) for each marginal constraint, the entropy regularized optimal transport in Equation~\eqref{eq:eot} could be transformed into some problems with the Kantorovich functional:
\begin{equation}\label{eq:dualphiot}
\begin{aligned}
W^\varphi_{\varepsilon,c}(\mu, \nu) = \inf_{\pi in \Pi(\mu,\nu) }  ( \int_{\mathcal{X}\times\mathcal{Y}} c(x,y) d\pi(x,y) + \varepsilon\int_{\mathcal{X}\times\mathcal{Y}} \varphi  \left( \frac{d\pi(x,y)}{d\mu(x)d\nu(y)} \right) d\mu(x) d\nu(y) \Bigg)
\end{aligned}
\end{equation}
\end{definition}


\begin{proposition}[Dual of EOT]\label{eq:eotdual}
Consider OT between two probability measures \(\mu\) and \(\nu\) with a convex
regularizer \(\phi\) on \(\mathbbm{R}^+\) in Equation~\eqref{eq:dualphiot}
\begin{equation}
W^\varphi_{c,\varepsilon}(\mu, \nu) = \sup_{f,g \in \mathcal{C}(\mathcal{X})\times\mathcal{C}(\mathcal{Y})} \int_{\mathcal{X}} fd\mu(x)+\int_{\mathcal{Y}} d\nu(y)-\varepsilon\int_{\mathcal{X}\times\mathcal{Y}}\varphi^*(\frac{f(x)+g(y)-c(x,y)}{\varepsilon})d\mu(x)d\nu(y)
\label{eq:wdual}
\end{equation}
where \(\varphi^*\) is the Legendre transform of \(\varphi\) defined by \(\varphi^*({\bf v}):=\sup_\x \x{\bf v}-\phi(\x)\)
\end{proposition}

A good choice for \(\varphi^*\) is that the \(\varphi^*({\bf v})=e^{\bf v}\).
The entropy regularization term ensures the problem is solvable, especially for computational schemes. If the \( \varepsilon \rightarrow \infty \), the optimal primal plan \( \pi^* \) can be retrieved using, which corresponds to the mutual information formula:
\[
\frac{d \pi^*}{d\mu d\nu}(x, y) = \exp \left( \frac{f^*(x) + g^*(y) - c(x,y)}{\varepsilon} \right)
\]




In the discrete version in Equation~\eqref{eq:eot}, the optimal transport plan \( \PP \) can often be expressed in terms of the optimal transport map \( T^* \) when it exists,  one can define the so-called barycentric projection map
\[ T^* : \x_i \in \mathcal{X} \mapsto \frac{1}{\mu_i} \sum_{j} \PP_{i,j} \y_j \in \mathbb{R}^d, \]
This link provides the connection between the mutual information with the optimal mapping:

\[ T^* : x \in X \mapsto \int_{Y} y \frac{d\pi(x, y)}{d\mu(x)d\nu(y)} d\nu(y). \]

Note that the joint distribution $\pi$ always has a density $\frac{d\pi(x,y)}{d\mu(x)d\nu(y)}$ with respect to $\mu \otimes \nu$, and the mutual information method will lead us to the optimal solution.


\section{Analysis of GCA}


\subsection{Convergence of GCA}
\label{app:gca_converged}

In this section, we provide a proof of  convergence in the forward pass for our GCA algorithm.
To do this, we show the general form in Algorithms~\ref{alg:gca} for all Bregman divergence (\(d_\Gamma\)) in forward pass in GCA algorithms could be converged through Djkstra’s projection algorithms. Finally, we show the uniformly convergence of the transport plan \(\PP\), and the convergence of its dual variables \(f^{(t)}\) in each iteration. 


\input{files/algo/msbregman}



\subsubsection{Convergence of GCA-INCE}

\begin{corollary}  (Convergence of GCA-INCE~\cite{peyre2019computational}) \label{Convergence}
Given \(\uu^*=e^{f^{\infty}}\), \(\vv^*=e^{g^{(\infty)}}\) and a kernel space \(H\) with the Hilbert-Birkhoff metric \(d_H(\uu, \uu^*) :=\log \max_{i, j}  \frac{\uu_i \uu^*_j}{\uu_j \uu^*_i},\) for all positive pairs \((\uu, \uu^*)\), with \(\uu^{(t)}\rightarrow \uu^*\) and \(\vv^{(t)}\rightarrow \vv^*\), we can prove that:
\begin{equation}
\|\log\PP^{(t)} - \log\PP^* \|_{\infty} \leq d_H(\uu^{(t)}, \uu^*) + d_H(\vv^{(t)}, \vv^*),
\end{equation}
\end{corollary}

\textbf{Proof for Corollary~\ref{Convergence}:}
First, let's define the following Hilbert space:
 \(\forall (\uu, \uu') \in (\mathbb{R}^n_{+, *})^2, \quad d_H(\uu, \uu') :=\log \max_{i, j}  \frac{\uu_i \uu'_j}{\uu_j \uu'_i}.\) 
For any pairs of vectors that $(\vv, \vv') \in (\mathbb{R}^m_{+, *})^2$ holds:
\begin{equation}
d_H(\vv, \vv') = d_H\left(\frac{\vv}{\vv'}, \mathbbm{1}_m\right) = d_H\left(\mathbbm{1}_m/\mathbf{v}, \mathbbm{1}_m/\mathbf{v}'\right).
\end{equation}
Let $\K \in \mathbb{R}^{n \times m}_{+,*}$, then for $(\vv, \vv') \in (\mathbb{R}^m_{+, *})^2$ we have
\[
d_H(\uu^{(t+1)}, \uu^{*}) = d_H\left(\frac{\mathbbm{1}_n}{\K\mathbf{\vv}^{(t)}}, \frac{\mathbbm{1}_n}{\K\mathbf{\vv}^{*}}\right) = d_H(\K\mathbf{\vv}^{(t)}, \K\mathbf{\vv}^{*}) \leq \lambda(\K)d_H(\mathbf{\vv}^{(t)}, \mathbf{\vv}^{*}),
\]

where
\begin{equation}
\lambda(\K) := \frac{\sqrt{\eta(\K) - 1}}{\sqrt{\eta(\K) + 1}} < 1, \quad \eta(\K) := \max_{i,j,k,\ell} \frac{\K_{i,k} \K_{j,\ell}}{\K_{j,k} \K_{i,\ell}}.
\end{equation}
Based on the contraction mapping theory, one has $(\uu^{(\ell)}, \vv^{(\ell)}) \rightarrow (\uu^*, \vv^*)$ and
\begin{align}
d_H(\uu^{(t)}, \uu^{*})  & \leq d_H(\uu^{(t+1)}, \uu^{(t)}) + d_H(\uu^{(t+1)}, \uu^{*}) \\ & \leq d_H\left(\frac{\mu}{\K\mathbf{\vv}^{(t)}}, \uu^{(t)}\right) + \lambda(\K)^2d_H(\uu^{(t)}, \uu^{*}) \\ & = d_H\left(\mu, \uu^{(t)} \odot (\K\mathbf{\vv}^{(t)})\right) + \lambda(\K)^2d_H(\uu^{(t)}, \uu^{*}),
\end{align}
\begin{equation}
d_H(\uu^{(t)}, \uu^*) \leq \frac{d_H(\PP^{(t)} \mathbbm{1}_m, \mu)}{1 - \lambda(\K)^2}, \quad d_H(\vv^{(t)}, \vv^*) \leq \frac{d_H(\PP^{(t)\top} \mathbbm{1}_n, \nu)}{1 - \lambda(\K)^2},
\end{equation}
where we denoted $\PP^{(t)} := \diag(\uu^{(t)}) \K \diag(\vv^{(t)})$. Last, one has
\begin{equation}
\| \log(\PP^{(t)}) - \log(\PP^*) \|_{\infty} \leq d_H(\uu^{(t)}, \uu^*) + d_H(\vv^{(t)}, \vv^*),
\end{equation}
where $\PP^*$ is the unique solution of Equation~\eqref{eq:eot}. The above formula also shows that the t-step solution gives a better lower bound than the 1-step solution. \qed


\subsubsection{Convergence of the Djkstra's projection algorithms}
The previous subsection proved the convergence of GCA-INCE. Here, we extend this to show the convergence of all generalized proximal operators in Algorithm~\ref{alg:gca}. Additionally, we demonstrate that these operators can iteratively solve alignment problems in the forward pass, following Dykstra’s projection algorithm. 


We present a general convergence proof for Dykstra’s projection algorithm, sharing the form in Definition~\eqref{eq:prox}. 
First, we define $d_\Gamma$ as a generic Bregman divergence on some convex set $\mathcal{B}$, and the proximal map of a convex function $d_\phi$ according to this divergence is:
\begin{equation}
\text{Prox}^{d_\Gamma}_{d_\phi,\mathcal{B}}(\K) := \arg\min_{\tilde{\PP} \in \mathcal{B}} d_\Gamma( \tilde{\PP} \|\K) + d_\phi( \tilde{\PP}).
\end{equation}
$\Gamma$ is a strictly convex function smooth on $\text{int}(\mathcal{B})$, and $\text{Prox}^{d_\Gamma} _{d_\phi}(\K) \in \text{int}(\mathcal{B})$ is always uniquely defined by strict convexity. As $\mathcal{B}= \text{dom}(\Gamma)$, $$\forall (\PP,\K) \in \mathcal{B} \times \text{int}(\mathcal{B}), d_\Gamma(\PP\|\K) = \Gamma(\PP) - \Gamma(\K) - \langle \nabla \Gamma(\K), \PP - \K \rangle,$$
which has its Legendre transform is also smooth and strictly convex: $$\Gamma^*(\rho) = \max_{\PP \in \mathcal{B}} \langle \PP, \rho \rangle - \Gamma(\PP)$$
In particular, one has that $\nabla\Gamma$ and $\nabla\Gamma^*$ are bijective maps between $\text{int}(\mathcal{B})$ and $\text{int}(\text{dom}(\Gamma^*))$ such that $\nabla\Gamma^* = (\nabla\Gamma)^{-1}$. For $\Gamma = ||\cdot||^2$, one recovers the squared Euclidean norm $d_\Gamma = ||\cdot||^2$. One has KL = $d_\Gamma$ for $\Gamma(\PP) = h(\PP)=-\sum_{i,j=1}^B(\PP_{ij} (\log\PP_{ij}-1))$. Dykstra’s algorithm starts by initializing $\PP^{(0)} := \K$ and $\U^{(0)} = \U^{(-1)} := 0$. One then iterative defines, for $k > 0$, 
\begin{align}
\PP^{(k)} &:= \text{Prox}^{d_\Gamma}_{d_{\phi_{[k]_2}}}(\nabla\Gamma^*(\nabla\Gamma(\PP^{(k-1)})+\U^{(k-2)})),\\
\U^{(k)} &:= \U^{(k-2)} + \nabla\Gamma(\PP^{(k-1)}) - \nabla\Gamma(\PP^{(k)}),
\end{align}


\begin{proposition}~\label{prop:pi}
Giving $d_{\phi_1},d_{\phi_2}$ are two proper, lower-semicontinuous convex functions defined on $\mathcal{B}$. We also assume that the following qualification constraint holds:
\begin{equation}~\label{eq:qual}
\text{ri}(\text{dom}(d_{\phi_1})) \cap \text{ri}(\text{dom}(d_{\phi_2})) \cap \text{ri}(\text{dom}(d_\Gamma)) = \emptyset,
\end{equation}
where ri is the relative interior and $\text{dom}(\phi) = \{\pi;\phi(\pi) = +\infty\}$. Then the \(\PP^{t}\) converges to the solution of the following equation: 
\begin{equation}\label{eq:pi}
\text{prox}_{h,\mathcal{B}}^{d_\Gamma}(\K)  = \arg\min_{\PP \in \mathcal{B}}\{  d_\Gamma ( \PP \| \K)  + \lambda_1 d_{\phi_1}(\PP) + \lambda_2 d_{\phi_2}(\PP)\}
\end{equation}
\end{proposition}



\textbf{Proof of the Proposition~\ref{prop:pi}}: Proof in \cite{peyre2015entropic} section 3.2.


\subsubsection{Convergence of Bregman projection}
\label{app:monotonicity}
This section aims to show for the continuous measure, the convergence of Bregman projection holds~\cite{berman2020sinkhorn}. Finding the optimal transport map \( T \) could be derived in minimizes some functionals derived from the potential function \( f \) defined on \(\mathcal{X}\), with Legendre transform in Equation~\eqref{eq:legendre} defined on \(\mathcal{Y}\):
\begin{equation}~\label{eq:funtional}
J(f) := \int_{\mathcal{X}} f \mu(x) + \int_{\mathcal{Y}} f^\mathsf{c} \nu(y)= I_\mu  -L  
\end{equation}

\begin{lemma} [\textit{Uniformly convergence}]\cite{berman2020sinkhorn} \label{lm:monotonicity} When \(t_1\rightarrow \infty \), \(f^{(t_1)}\) converges uniformly to a fixed point \( f^{(\infty)} \), with \(f^{(t_1)}\leq f^{(\infty)}\).
\end{lemma}

\textbf{Proof for the Lemma~\ref{lm:monotonicity}} (\textit{Uniformly convergence}): We follow the procedures of methods in ~\cite{berman2020sinkhorn}. 


Giving push-forward measure \(T_\mu\) and \(T_\nu\) and a composed operator \(S= T_\nu \circ T_\mu \), which yields an iteration on \( C(\mathcal{X}) \) as \( S: C(\mathcal{X}) \rightarrow C(\mathcal{X}), f\rightarrow f\circ g\circ f\), \(f^{(m+1)}=S(f^{(m)})\), and \(e^{S(f)-f} \mu\) is the probability measure on \(\mathcal{X}\). 

\begin{lemma}[\textit{Existence and uniqueness}]%\label{lm:unique}
The following conditions are equivalent for a function \( f \) in the space \( C(X) \), where \( C(X) \) denotes the space of continuous functions on a set \( X \):
\begin{itemize}
    \item \( f \) is a critical point for the functional \( F \) on \( C(X) \).
    \item The function \(\exp(S(f)-f)=0\) hold almost everywhere (a.e.) with respect to (w.r.t.) $\mu$.
\end{itemize}
Moreover, if \( f \) is a critical point, then \( f^* := S(f) \) is a fixed point for the operator \( S \) on \( C(X) \).
\end{lemma}

\textbf{Proof of the Lemma~\ref{lm:unique}}
Consider the functional \( L \) defined in Equation~\eqref{eq:funtional}, the differential of \( L \) at an element \( f \in C(X) \) is represented by the probability measure \(\exp({S(f)-f})  \mu \). For some iterations \(f^{(m+1)}-f^{(m)}=S(f^{(m)})-f^{(m)}\), when \(f\) is a critical point (derivative is zero or undefined) for the functional $J$ on $C(X)$ , and $f^* := S(f)$ is a fixed point for the operator $S$ on $C(X)$, proved by realizing for any $\dot{f} \in C(X)$:

\begin{equation}
    \left. \frac{d}{dt} L(f + t\dot{f}) \right|_{t=0} = \int_X \dot{f} e^{(S(f)-f)} d\mu.
\end{equation}
This follows readily from the definitions by differentiating $t \mapsto g[(f + t\dot{f})]$ to get an integral over $(X, \mu)$ and then switching the order of integration. As a consequence, \( f \) is a critical point of the functional \( F \) on \( C^0(X) \) if and only if \( e^{(S(f)-f)} \mu = \mu \), i.e., if and only if \( e^{(S(f)-f)} = 1 \) almost everywhere with respect to \( \mu \). Finally, if this is the case, then \( S(f) = f \) almost everywhere with respect to \( \mu \) and hence \( S(S(f)) = S(f) \) (since \( S(f) \) only depends on \( f \) viewed as an element in \( L^1(X, \mu) \)).






\begin{lemma}
Given a point \( x_0 \in X \), the subset \( K_{x_0} \) of \( C(\mathcal{X}) \) defined as all elements \( f \) in the image of \( S \) satisfying \( f(x_0) = 0 \) is compact in \( C(\mathcal{X}) \).
\label{lm:compact}
\end{lemma}
\textbf{Proof of the Lemma~\ref{lm:compact}:} Based on the compactness of the product space \(\mathcal{X}\times\mathcal{Y}\), the continuous function \( c \) is uniformly continuous on \( \mathcal{X} \). So \( S(C(\mathcal{X})) \) is an equicontinuous family of continuous functions on \( X \). By Arzelà-Ascoli theorem, it follows that the set \( K_{x_0} \) is compact in \( C(\mathcal{X}) \).

\begin{proposition}
The operator \( S \) has a fixed point \(f^* \) in \( C(\mathcal{X}) \). Moreover, \( f^* \) is uniquely determined a.e. wrt \( \mu \) up to an additive constant, and \( f^* \) minimizes the functional \( F \). More precisely, there exists a unique fixed point in \( S(C(\mathcal{X}))/\mathbb{R} \).
\label{pp1}
\end{proposition}
\textbf{Proof of the Proposition~\eqref{pp1}:} Then based on the Jensen’s inequality, we have 
\begin{align}
& I_\mu(f^{(m+1)}) - I_\mu(f^{(m)}) = \int\log {\exp{(S(f^{(m)})-f^{(m)}}})d\mu \leq \log \int{\exp{(S(f^{(m)})-f^{(m)})}}{d\mu}  = 0, \\
&L(f^{(m)}) - L(f^{(m+1)}) = \int \log {\exp{(S(g^{(m)})-g^{(m)})}}{d\nu} \leq \log \int{\exp{(S(g^{(m)})-g^{(m)})}}{d\nu} = 0.
\end{align}
So we know the functionals are strictly decreasing at \( f^{(m)} \) unless \( S(f^*) = f^* \) for \( f^* := S(f^{(m)}) \). Then based on the Lemma~\ref{lm:compact}, we know for each initial data \( f_0 \), the closure of its images denoted as \( K_{f_0} \) in \( C(\mathcal{X})/\mathbb{R} \) is compact, under the operator \(S\). Hence, \( f^{(m)} \rightarrow f^{(\infty)} \) in \( C(\mathcal{X})/\mathbb{R} \). And \( J \) is decreasing along the orbit but has lower bound:
\[ J(f^{(\infty)}) = \inf_{K_{f^{(0)}}} J. \]
By the condition for strict monotonicity, it must be that \( S (f^{(\infty)}) = f^{(\infty)} \) a.e. wrt \( \mu \). It then follows from the Proposition~\eqref{pp1} that \( f^{(\infty)} \) is uniquely determined in \( C(\mathcal{X})/\mathbb{R} \) (by the initial data \( f^{(0)} \)), i.e. the whole sequence converges in \( C(\mathcal{X})/\mathbb{R} \). We first show that there exists a number \( \lambda \in \mathbb{R} \) such that
\( \lim_{m \rightarrow \infty} I_\mu(f^{(m)}) = \lambda. \) \( I_\mu \) is decreasing and hence it is enough to show that \( I_\mu(f^{(m)}) \) is bounded from below. By \( I_\mu = J + L \), and \( J \) is bounded from below (by \( F(f^{(\infty)}) \)). Moreover, by the first step \( L(f^{(m)}) \geq L(f^{(0)}) \). Next, decompose
\[ f^{(m)} = \tilde{f}^{(m)} + f^{(m)}(x_0), \]
By the Lemma~\ref{lm:compact} the sequence \( (\tilde{f}^{(m)} \) is relatively compact in \( C(\mathcal{X}) \) and we claim that \( |f^{(m)}(x_0)| \leq C \) for some constant \( C \). Indeed, if this is not the case then there is a subsequence \( f^{(m_j)} \) such that \( |f^{(m_j)}| \rightarrow \infty \) uniformly on \( X \). But this contradicts that \( I_\mu(f^{(m)}) \) is uniformly bounded. It follows that the sequence \( (f^{(m)}) \) is also relatively compact. Hence, by the previous step the whole sequence \( f^{(m)} \) converges to the unique minimizer \( f^* \) of \( F \) in \( S(C(\mathcal{X})) \) satisfying \( I_\mu(f^*) = \lambda \).




\subsection{GCA version of unbalanced optimal transport (GCA-UOT)}%~\hyperref[app:outline]{*}
\label{app:gcauot}

In this section, we are going to introduce the relaxation of the EOT plan as Unbalanced optimal transport plan (UOT). And its relationship with the dual formula of EOT. Here we need to emphasize that the GCA-UOT not just add constraint to the proximal operators which computes the coupling matrix \(\PP_\theta\), but also add the penalty (i.e. KL-divergence) to the loss function \(d_M\). For the specific function we used in the method of GCA-UOT in Table~\ref{tab:new_combine}, we employed a version with the loss in Equation~\eqref{eq:hsrince} plus the loss in Equation~\eqref{eq:kleqince} with a weight control parameter.

\subsubsection{Explanation of the unbalanced OT} Unbalanced optimal transport (UOT) in Equation~\eqref{eq:uot} seeks to generalize the OT problem in Equation~\eqref{eq:dot} by allowing for the relaxation of these constraints~\cite{chizat2018scaling}, as penalization by certain divergence measures \( d_{\phi_1} \) and \( d_{\phi_2} \) (e.g., Kullback-Leibler divergence). Here we provide the unbalanced OT for the entropic regularization optimal transport in Equation~\eqref{eq:eot}, which ensure that the transported mass respects the given source \(\mu\) and target distributions \(\nu\):

\begin{equation}\label{eq:ueot}
\U_{OT}(\mu, \nu) = \min_\PP \left\langle \PP,\C \right\rangle + \lambda_1 d_{\phi_1}(\PP\mathbbm{1} || \mu) + \lambda_2 d_{\phi_2}(\PP^\top\mathbbm{1} || \nu) + \varepsilon H(\PP)
\end{equation}

 
Here \( \left\langle P,C \right\rangle \) represents the total transport cost. \( \lambda_1 \) and \( \lambda_2 \) are regularization parameters that control the trade-off between the transport cost and the divergence penalties. 

\subsubsection{Connection to dual formula of EOT} 
\begin{lemma}\label{lm:uotdual}
The entropy regularized OT problem is a special case of a structured convex optimization problem of Equation~\eqref{eq:ueot} the by giving functions \(h_\mathcal{F}\) and \(h_\mathcal{G}\), $h_\mathcal{F} = \iota_{\{C_1^\mu\}}$ and $h_\mathcal{G} = \iota_{\{C_2^\nu\}}$, as the indicator function of a closed convex set     \(C_1^\mu \coloneqq \{\PP : \PP\mathbbm{1}_m = \mu\}, C_2^\nu \coloneqq \{\PP : \PP^\top \mathbbm{1}_n = \nu\}.\) 
\begin{equation}
\min_{\PP} \langle\PP,\C\rangle + \varepsilon H(\PP) + h_\mathcal{F}(\PP \mathbbm{1}_m) + h_\mathcal{G}(\PP^\top \mathbbm{1}_n).\quad \iota_C(x) = 
\begin{cases}
0 & \text{if } x \in C, \\
+\infty & \text{otherwise},
\end{cases} 
\end{equation}
\end{lemma}

\textbf{Proof of the Lemma~\ref{lm:uotdual}:} Let's start with the dual formula of the Equation~\eqref{eq:eot} with \(\mathcal{B}=C_1^\mu \cap C_2^\nu\), we can introduce the Lagrangian \(\mathcal{E}(\PP,f, g)\) of Equation~\eqref{eq:eot} reads:
\begin{align}
 \text{Prox}^{\text{KL}}_{\mathcal{B}}(\K) & := \min_{\PP\in\mathcal{B}}\langle\PP,\C\rangle -\varepsilon H(\PP) = \mathcal{E}(\PP,f, g) \\ &   = \min_{P} \max_{f \in \mathbb{R}^n, g \in \mathbb{R}^m} \langle \PP, \C \rangle - \varepsilon H(\PP) - \langle f, \PP \mathbbm{1}_m - \mu \rangle - \langle g, \PP^T \mathbbm{1}_n - \nu \rangle. \qed
\label{eq:LpC}
\end{align}
To solve this problem, we can use the first order condition:
\begin{equation}
\frac{\partial \mathcal{E}(\PP,f, g)}{\partial \PP_{ij}} = \C_{ij} + \varepsilon \log(\PP_{ij}) - f_i - g_j = 0 \quad \Rightarrow \quad log \PP =\frac{1}{\varepsilon }(f \mathbbm{1}_m^T + \mathbbm{1}_n g^T - \C) 
\end{equation}

The solution to the Equation~\eqref{eq:eot} is unique with scaling variabl \( (\uu, \vv) \in \mathbb{R}^n_+ \times \mathbb{R}^m_+ \) in Equation~\eqref{eq:uvupdatesapp}. And each items in the optimal transport matrix \( \PP \) is, and optimal \( (f, g) \) are linked to non-negative vectors \( (\uu, \vv) \) through $(\uu, \vv) = (e^{f/\varepsilon}, e^{g/\varepsilon})$.
\begin{equation}
\PP_{ij} = e^{f_i/\varepsilon} e^{-\C_{ij}/\varepsilon} e^{g_j/\varepsilon} = \uu_i \K_{ij} \vv_j, \quad (f^{(t)}, g^{(t)}) = \varepsilon(\log(\uu^{(t)}), \log(\vv^{(t)})),
\end{equation}




\subsection{Equivalence of INCE objective with single step Bregman projection}%~\hyperref[app:outline]{*}

In this section, we are going to discuss how to build the equivalence between minimizing the KL-divergence \(d_M\) between the \(\PP^{(1)}\) and the \(\PP_{\text tgt}\) with respect to \(\theta\) in GCA objective:
$$\min_{\theta}\text{KL} \big( {\bf I}  ||\text{Prox}_{C_1^\mu}^{KL}(\K_\theta)),$$ 
with the INCE loss minimization in Equation~\eqref{eq:INCE}.
Here \(\PP^{(1)}\) is the nearest point of \(\K_\theta\) on constraint set \(C_1^\mu\) measured by the KL-divergence $d_\Gamma$ defined in Equation~\eqref{eq:breg1}, through one step of proximal operator (Bregman projection).
And ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs} with cosine similarity.

\label{app:proof_cleqot}
\subsubsection{Proof of the Theorem~\ref{thm:cl_eq_ot}}  Suppose we had a encoder \(f_\theta\) with parameter \(\theta\) in INCE, with $\widetilde{f}_\theta$ to represent its normalized form,then we can use the following proposition to assist our proof:

\begin{proposition}~\label{prop:pij}
Given the cost matrix as $\C_{i,j}=1-\widetilde{f}_\theta(\x_i')^\top \widetilde{f}_{\theta}(\x_j'')$, and  Gibbs kernel \({\bf K}_\theta=\exp(-\C_{i,j}/\varepsilon)\), based on the cosine dissimilarity scores of the inner products \( \langle \z_{\theta i}, \z_{\theta j} \rangle \), with \( \z_i=\frac{f_\theta(\x'_i)}{\|f_\theta(\x'_i)\|} \) and \( \z_j = \frac{f_\theta(\x''_j)}{\|f_\theta(\x''_j)\|} \). Set \(d_M\) and \(d_\Gamma\) to KL-divergence, and the target  transport plan \({\PP}_{\text{tgt}}=\bf I\). The probability matrix \(\PP\) after one-step Bregman iteration of entropy optimal transport problem could be represented as: 
\begin{equation}
\PP_{ij} = \frac{\K_{\theta ij}}{\sum_{j=1}^B \K_{\theta ij} } = \frac{\exp\left(\varepsilon^{-1}\langle \z_i, \z_j \rangle\right)}{\sum_{j=1}^B \exp\left(\varepsilon^{-1}\langle \z_i, \z_k \rangle\right)}
\end{equation}
\end{proposition} 
\textbf{Proof of the Proposition~\eqref{prop:pij}}: 
%To prove the identity between contrastive learning with solving the \(\text{KL}(\I\|\PP)\) objective functions inspires us to consider the CL as solving matching problem on the kernel space with the given cost matrix. 
We assume that gibbs kernel \( \K_\theta \) is a matrix  which can be expressed as:
\[
\K_{\theta ij}=\exp\left(-\varepsilon^{-1}\C_{i,j}\right) = \exp\left(-\varepsilon^{-1} |1 -\langle \z_i, \z_j \rangle |\right),
\]
with a temperature parameter \(\varepsilon\). \( \mu \), \( \nu \), \(\uu^{(0)}\) and \(\vv^{(0)}\)  can be initialized as a vector of ones with the same size as B, the batch size, \[\mu = \mathbbm{1}, \nu = \mathbbm{1}\quad \uu^{(0)}=\mathbbm{1}, \vv^{(0)}=\mathbbm{1}.\]

For \( t \) iterations of the Sinkhorn algorithm, \( \uu^{(t)}\) is updated as:
\[
\mathbf{\uu}^{(t+1)} \stackrel{\text { def }}{=} \frac{\mu}{\K_\theta \mathbf{\vv}^{(t)}}, \quad
\mathbf{\vv}^{(t+1)} \stackrel{\text { def }}{=} \frac{\nu}{\K_\theta^T \mathbf{\uu}^{(t)}}.
\]
So we know that:
\[
\uu^{(1)}=\frac{1}{\sum_{j=1}^b \mathbf{K}_{\theta ij}}.
\]
Thus, half-step sinkhorn iteration or one-step Bregman interation for \( \PP \) can be expressed as:
\[
\PP_{ij} = \uu^{(1)}_i \mathbf{K_\theta}_{ij} \vv^{(0)}_j= \frac{\K_{\theta ij}}{\sum_{j=1}^b \K_{\theta ij} } = \frac{\exp\left(\varepsilon^{-1}\langle \z_i, \z_j \rangle\right)}{\sum_{j=1}^b \exp\left(\varepsilon^{-1}\langle \z_i, \z_k \rangle\right)} \qed 
\]

This concludes the expressions of \(\PP\) at half-step iteration. Reminds us the formula of the KL divergence \(\text{KL}(\I\|\PP)\) and the entropy \(H(\PP)\):
\begin{equation}
\text{KL}(\I\|\PP) \ \stackrel{\text { def }}{=} \sum_{i,j} \I_{i,j} \log \frac{\I_{i,j}}{\PP_{i,j}} - \I_{i,j} + \PP_{i,j}, \quad \text{where}\quad \I_{i,j} \log \frac{\I_{i,j}}{\PP_{i,j}}=0, \quad \text{if}\quad \I_{i,j}=0.
\end{equation}
And after the batch normalization of \(\PP\), the value of \(\sum_{i,j}\PP_{i,j}\) is equal to the batch size B and exactly the same as the \(\sum_{i,j} \I_{i,j}\), we can obtain:

\[
\operatorname{KL({\bf I}\|\PP}) = \sum_i \log\left(\frac{1}{\PP_{ii}}\right)= -\sum_i \log\frac{\exp\left(\varepsilon^{-1}\langle \z_i, \z_i \rangle\right)}{\sum^b_{j=1} \exp\left(\varepsilon^{-1}\langle \z_i, \z_j \rangle\right)}
\]
j represents the elements on the diagonal of the similarity matrix, which is the same structure as the INCE loss as:

\[
\mathcal{L}_{\text{INCE}} = -  \sum_i \log\left(\frac{\exp(f_\theta(\x'_i)^\top f_\theta(\x''_i))}{\sum_{j=1}^b\exp(f_\theta(\x'_i)^\top f_\theta(\x''_j))}\right) \qed
\]




\subsection{Proximal operator version of RINCE}%~\hyperref[app:outline]{*}

In this section, we are going to discuss how to build the equivalence between minimizing the some convex function of \(d_M\) with adjustable parameters \(q\) and \(\lambda\) between the \(\PP^{(1)}\) and the \(\PP_{\text tgt}\) as:

\begin{equation}\label{eq:rincedm}
d_M(\I,\PP)=-\frac{1}{q}\left(\left(\frac{\operatorname{diag}(\PP^{(1)}_\theta)}{\uu^{(1)}}\right)^q-\left(\frac{\lambda \I}{\uu^{(1)}}\right)^q\right)
\end{equation}

with respect to \(\theta\) in GCA objective:
$$L_{\text{RINCE}}^{\lambda,q} = \min_\theta -\frac{1}{q} \bigg( \frac{\operatorname{diag}(\PP^{(1)}_\theta)}{\uu^{(1)}} \bigg)^q+  \frac{1}{q}\bigg(\frac{\lambda \I}{\uu^{(1)}} \bigg)^q,~~\text{with}~~\PP^{(1)}_\theta = \text{Prox}_{C_1^\mu}^{\text{KL}}(\K_\theta),~~\uu^{(1)}=\text{diag}\left(\frac{\mu}{\PP^{(0)}\mathbbm{1}}\right)$$
with the RINCE loss minimization in Equation~\eqref{eq:RINCE}.
Here \(\PP^{(1)}\) is the nearest point of \(\K_\theta\) on constraint set \(C_1^\mu\) measured by the KL-divergence $d_\Gamma$ defined in Equation~\eqref{eq:breg1}, through one step of proximal operator (Bregman projection).
And ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs} with cosine similarity.

Also, we are going to discuss when the q=1, RINCE loss is the symmetry loss, which provides the robustness in the noisy view. 

\subsubsection{Proof of the Theorem~\ref{thm:rince}}
\label{app:rinceeqprox}
The loss function of RINCE looks like:
\begin{equation}\label{eq:rinceapp}
    \mathcal{L}^{\lambda,q}_\text{RINCE}=\frac{1}{q}\big(-e^{ q s_{ii}} + \lambda^q(e^{ s_{ii}}+\sideset{}{_{i\neq j}}\sum e^{s_{ij}})^q\big)
\end{equation}
For the specific parameters \(\theta\), we record the normalized latent of the \(\z^i_{\theta+} =s_{ii}\), and \(\z^i_{\theta-} =s_{ij},j\neq i\). The positive pairs are stored in the diagonal of the gibbs kernel \(\K_\theta\), and the negative pairs are stored in the off-diagonal elements, which means:
\begin{align}    
&\K_{ii}=\exp\left(-\varepsilon^{-1}\C_{i,i}\right) = \exp\left(-\varepsilon^{-1} |1-\langle \z'_{\theta i}, \z''_{\theta i} \rangle|\right)=\exp\left(\varepsilon^{-1}{\langle \z'_{\theta i}, \z''_{\theta i} \rangle}-\varepsilon^{-1}\right) \propto  e^{\z^i_{\theta+}}.\\
&\K_{ij}=\exp\left(-\varepsilon^{-1}\C_{i,j}\right) =\exp\left(\varepsilon^{-1}{\langle \z'_{\theta i}, \z''_{\theta j} \rangle}-\varepsilon^{-1}\right) \propto  e^{\z^i_{\theta-}}, j\neq i .
\end{align}

By solving the \(\langle \uu^{(1)}\K, \mathbbm{1}\rangle = \mu\) in the Equation~\eqref{eq:scaleapp}, we have the ith column elements \(\sum_{j=1}\K_{\theta ij}=\frac{\mu}{\uu^{(1)}_i}\), in which \(\uu^{(1)}\) is given in~\ref{eq:scaleapp}:
\begin{align}
& \frac{\mu}{\uu^{(1)}_i}= \sum^B_{j=1}\K_{\theta ij}=\frac{1}{e^{\varepsilon^{-1}}} ( e^{\varepsilon^{-1}\langle \z'_{\theta i}, \z''_{\theta i} \rangle} + \sum_{j=1,j\neq i}^{B} e^{\varepsilon^{-1}{\langle \z'_{\theta i}, \z''_{\theta j} \rangle}}), i\neq j, \\ & \operatorname{diag}(\K_{\theta})=\frac{e^{\z^i_{\theta+}}} {e^{\varepsilon^{-1}}}=\frac{\operatorname{diag}(\PP^{(1)})}{ \uu^{(1)}}.
\end{align}
%That means Rince is robust to noisy view as risk minimizer. 
The diagonal of K matrix contains the positive views and the marginal distribution of the u contains the negative view, we have: 
\begin{equation}
L^{\lambda,q}_\text{RINCE}(s_\theta^i) = -\frac{e^{ q s^i_{\theta+}}}{q} +  \frac{(\lambda\cdot (e^{s^i_{\theta+}} + \sum_{j=1,j\neq i}^{B} e^{s_{\theta-}^{ij}}))^q}{q} \propto  -\frac{\operatorname{diag} ({\K_\theta})_{ii}^{q}}{q} +  \frac{(\lambda\cdot (\sum_{j=1}^{B} {\K_\theta}_{ij} ))^q}{q}
\end{equation}
Furthermore, we have:
\begin{equation}\label{eq:w1}
-E(L_{\text{RINCE}}^{\lambda,q}({\bf K}_\theta)) = \frac{1}{q}\bigg( \operatorname{diag}(\K_{\theta}) \bigg)^q -  \frac{1}{q}\bigg(\frac{\lambda \I}{\uu^{(1)}}\bigg)^q.
\end{equation}
where $\PP^{(0)} = \diag(\mathbbm{1}) {\bf K}_{\theta} \diag(\mathbbm{1})$, $\PP^{(1)} = \diag({\uu^{(1)}}) {\bf K}_{\theta} \diag(\mathbbm{1})$, we have:
\begin{equation}
L_{\text{RINCE}}^{\lambda,q}(\PP_\theta^{(1)}) = -\frac{1}{q}(\frac{\operatorname{diag}(\PP^{(1)})}{\uu^{(1)}})^q+  \frac{1}{q}(\frac{\lambda \I}{\uu^{(1)}})^q.
\end{equation}


\subsubsection{Proof of the Symmetry and robustness of RINCE}
\label{app:symmetry}

Symmetry loss is said to be noise tolerant as the classifier will keep performance with the label noise in \textbf{Empirical Risk Minimization (ERM)}. In many practical machine learning scenarios, we aim to select a model or function $f_\theta$ that minimizes the expected loss across all possible inputs and outputs from a distribution $\mathcal{D}$, which is typically unknown. Instead of minimizing the true risk, which is often not feasible due to the unknown distribution $\mathcal{D}$, we minimize what is called the \textbf{empirical risk} $\hat{R}_L(\widetilde{f}_\theta)$, which is defined as the average loss over the training dataset of size $B$, which consists of independently and identically distributed (iid) data points. Mathematically, it is given by the following formula:
\begin{equation}\label{eq:emp_risk}
\hat{R}_L(f_\theta) = \frac{1}{B} \sum_{i=1}^{B} L(\widetilde{f_\theta}(\x_i), \y_i)
\end{equation}
Here, $L(\widetilde{f_\theta}(\x_i), \y_i)$ represents the loss function, which measures the discrepancy between the predicted value $\widetilde{f_\theta}(\x_i)$ and the true value $\y_i$. The function $\widetilde{f}_\theta$ that minimizes this empirical risk is chosen as the model for making predictions. This approach is based on the assumption that minimizing the empirical risk will also approximate the minimization of the true risk, especially as the size of the training set increases. 


First we show the symmetry loss is robust to the noisy view with the following Lemma~\cite{ghosh2015making}, which means they will achieve the same performance in ERM with the noisy labels. Then we show RINCE satisfy the symmetry condition when \(q=1\), so the lemma is:

\begin{lemma}~\label{lm:symmetry} Give a loss function $L(\widetilde{f}_\theta(\x),\y)$ exhibits a certain symmetry for some positive constant \( K \), with respect to the labels $\y=1$ and $\y=-1$:
\begin{equation}~\label{eq:symmetry}
L(\widetilde{f}_\theta(\x), 1) + L(\widetilde{f}_\theta(\x), -1) = K, \quad \forall x, \forall f, \quad \text{(Symmetry)}
\end{equation}
Symmetry loss is noise tolerant given the label noise \( \eta < 0.5 \), which corresponds to the flipped labels:
\begin{equation}
P_D[\text{sign}(\widetilde{f}^*_\theta(x))=\y_\x] = P_D [\text{sign}(\widetilde{f}^*_{\theta\eta}(\x))=\y_\x], \quad \text{(Noisy tolerant)}
\end{equation}
\end{lemma}

\textbf{Proof of the Lemma~\ref{lm:symmetry} is in~\cite{ghosh2015making}.}  


\textbf{Second we show the RINCE loss is a symmetry loss with $q\rightarrow 1$}, so we have the Equation~\eqref{eq:RINCE}:
\begin{equation}
L^{\lambda,q=1}_\text{RINCE} = -e^{\z^{ii}_{\theta +}} + \lambda \cdot (e^{\z^{ii}_{\theta +}} + \sum_{j=1,j\neq i}^{B} e^{\z_{\theta-}^{ij}})
\end{equation}



As we know that this formula has the same structure as the exponential loss function: \( L(\z_\theta, \y) = -\y e^{\z_\theta} \). To check for symmetry, we define a new binary classification loss function as:
   \[
   \tilde{L}_x(\z_\theta(\x), \y) = B + L_x(\widetilde{f}_\theta(\x), \y) = B - \y \cdot e^{\widetilde{f}_\theta(\x)} \geq 0
   \]
   where the prediction score \( \widetilde{f}_\theta(\x) \) is bounded by \( s_{\text{max}} = \log(B) \). Then we can establish that the loss satisfies the symmetry property:
   \begin{equation}
   \tilde{L}(\widetilde{f}_\theta(\x), 1) + \tilde{L}(\widetilde{f}_\theta(\x), -1) = 2B
   \end{equation}
So we prove that this loss function is symmetry.

\subsection{Proof for RINCE is the upper bound of the 1-Wasserstein distance}%~\hyperref[app:outline]{*}
\label{app:wdm}
In this section, we are trying to build the connection when change the \(d_M\) from the KL-divergence in Equation~\eqref{eq:kleqince} to the 1-Wasserstein distance in Equation~\eqref{eq:w1eqrince}, when q=1 in the RINCE loss.



\subsubsection{Proof of the Theorem~\ref{co:rince}}


WDM~\cite{chuang2022robust} is proposed as a replacement for the KL divergence by Wasserstein distance in Mutual Information estimation. The Wasserstein distance between the joint distribution \(\pi\) on \(\mathcal{X} \times \mathcal{Y}\) and the product of the marginal distributions \(\mu\) and \(\nu\) on \(\mathcal{X}\) and \(\mathcal{Y}\), respectively, is given by:
\[
W(\pi, \mu \otimes \nu) = \sup_{f \in \mathcal{C}(\mathcal{X} \times \mathcal{Y})} \left(\mathbb{E}_{\pi(x, y)}[f(x, y)] - \mathbb{E}_{\mu \otimes \nu(x, y)}[f(x, y)]\right)
\]
where \(\mathcal{C}(\mathcal{X} \times \mathcal{Y})\) denotes the set of all 1-Lipschitz functions from \(\mathcal{X} \times \mathcal{Y}\) to \(\mathbb{R}\). A function \( f : \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R} \) is defined to be 1-Lipschitz if, for any two points \( (x_1, y_1), (x_2, y_2) \in \mathcal{X} \times \mathcal{Y} \), the following condition is satisfied: \[|f(x_1, y_1) - f(x_2, y_2)| \leq d((x_1, y_1), (x_2, y_2))\]
where \( d((x_1, y_1), (x_2, y_2)) \) denotes the metric on \( \mathcal{X} \times \mathcal{Y} \) typically defined, for example, by the Euclidean distance:
\[
d((x_1, y_1), (x_2, y_2)) = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
\]

Based on the Lipschitz continuity and inner product, it is easy to know for two given point \((x_1,y_1)\), \((x_2,y_2)\), the following properties hold with $-\frac{1}{\varepsilon} \leq s \leq \frac{1}{\varepsilon}$, which implies $\lvert \nabla_s e^s \rvert \leq e^{1/\varepsilon}$. Therefore, by the mean value theorem, we have:
\begin{equation}~\label{eq:lip}
\begin{split}
    &\lvert e^{x_1^T y_1/\varepsilon} - e^{x_2^T y_2/\varepsilon} \rvert \leq e^{1/\varepsilon} \frac{1}{\varepsilon} \lvert \langle x_1, y_1 \rangle - \langle x_2, y_2 \rangle \rvert = e^{1/\varepsilon} \frac{1}{\varepsilon} \lvert \langle x_1- x_2, y_1 \rangle + \langle x_2, y_1 - y_2 \rangle \rvert \\
    &\leq e^{1/\varepsilon} \frac{1}{\varepsilon} \left( \| x_1 - x_2\| \|y_1\|  + \| y_1-y_2\| \| x_2\|\right) = e^{1/\varepsilon} \frac{1}{\varepsilon} \left( \lVert x_1 - x_2 \rVert + \lVert y_1-y_2 \rVert \right)
\end{split}
\end{equation}
Consider two pairs of views, \((\z'_{\theta 1}, \z''_{\theta 1})\) and \((\z'_{\theta 2}, \z''_{\theta 2})\), sampled from the joint distribution \(\pi\) of \(\mu\) and \(\nu\). Thus, each pair \((\z'_{\theta i}, \z''_{\theta i})\) for \(i = 1, 2\) represents a sample from the joint distribution \(\pi\), where \(\z'_{\theta i} \sim \mu\) and \(\z''_{\theta i} \sim \nu\). The RINCE loss is a symmetry loss with $q=1$, so we have the Equation~\eqref{eq:RINCE}:
\begin{equation}
L^{\lambda,q=1}_\text{RINCE} = -e^{\z^{ii}_{\theta +}} + \lambda \cdot (e^{\z^{ii}_{\theta +}} + \sum_{j=1,j\neq i}^{B} e^{\z_{\theta-}^{ij}}),
\begin{cases} 
\z^{ii}_{\theta+} = \varepsilon^{-1}\widetilde{f_\theta}(\x'_i)^\top\widetilde{f_\theta}(\x''_i), & \text{for } i=i, \\
\z_{\theta-}^{ij} =\varepsilon^{-1} \widetilde{f_\theta}(\x'_i)^\top\widetilde{f_\theta}(\x''_j), & \text{for } i \neq j.
\end{cases}
\end{equation}



So we know that:
\begin{align*}
& -\mathbb{E}(L_{\text{RINCE}}^{\lambda ,q=1}(z_\theta)) = \mathbb{E}_{\substack{\z'_{\theta i}\sim\mu \\ \z''_{\theta i}\sim \nu | \mu=\z'_{\theta i} \\ \z''_{\theta j}\sim \nu}} \left[ (1-\lambda) e^{\varepsilon^{-1} \z'^T_{\theta i} \z''_{\theta i}} - \lambda \sum_{j=1}^{B-1} e^{\varepsilon^{-1} \z'^T_{\theta i} \z''_{\theta j}}\right] \\ 
&= \mathbb{E}_{(\z'_{\theta i},\z''_{\theta i} ) \sim \pi} \left[ (1 - \lambda) e^{\frac{ \z'^T_{\theta i} \z''_{\theta i}}{\varepsilon}} \right] - \lambda (B-1) \mathbb{E}_{\z'^T_{\theta i} \sim \mu, \z''^T_{\theta j} \sim \nu} \left[ e^{\frac{\z'^T_{\theta i} \z''_{\theta j}}{\varepsilon}} \right] \\ 
&\leq (1 - \lambda)  \left( \mathbb{E}_{(\z'_\theta,\z''_\theta) \sim \pi} \left[ e^{\frac{\z'^T_\theta \z''_\theta}{\varepsilon}} \right] - \mathbb{E}_{\z'^T_\theta \sim \mu, \z''_\theta \sim \nu} \left[ e^{\frac{\z'^T_{\theta} \z''_{\theta}}{\varepsilon}} \right] \right) (\text{Giving setting}~\lambda (B-1) > 1 - \lambda)\\
\end{align*}
If we give two couples of two views (\(\z'_{\theta 1},\z''_{\theta 1}\)) and (\(\z'_{\theta 2},\z''_{\theta 2}\)) from joint distribution \(\pi\) of \(\mu\) and \(\nu\), \(\z'_\theta\sim\mu\) and \(\z''_\theta\sim\nu\), which means to maximize:
\begin{align*}
& \lvert e^{\varepsilon^{-1} \z'^T_{\theta 1} \z''_{\theta 1}} - e^{\varepsilon^{-1} \z'^T_{\theta 2} \z''_{\theta 2}} \rvert\\
& \leq (1-\lambda) e^{\frac{1}{\varepsilon}} \frac{1}{\varepsilon} \left( \| \z'_{\theta 1} - \z'_{\theta 2}\| \|\z''_{\theta 1}\| + \| \z''_{\theta 1}-\z''_{\theta 2}\| \| \z'_{\theta 2}\| \right) \text{(Mean value theorem from Equation~\eqref{eq:lip})}\\
& = (1-\lambda) e^{\frac{1}{\varepsilon}} \frac{1}{\varepsilon} \left( \lVert \z'_{\theta 1} - \z'_{\theta 2} \rVert_2 + \lVert \z''_{\theta 1}-\z''_{\theta 2} \rVert_2 \right) \\
& = (1-\lambda) e^{\frac{1}{\varepsilon}} \frac{1}{\varepsilon} d\left((\z'_{\theta 1},\z''_{\theta 1}),(\z'_{\theta 2},\z''_{\theta 2})\right) \\
& \leq  (1 - \lambda) e^{1/\varepsilon} \frac{1}{\varepsilon} W_1(\pi, \mu\ \otimes \nu).
\end{align*}





\subsection{Proof of connection with BYOL}%~\hyperref[app:outline]{*}
\label{app:byol}

In this section, we are going to who how the change of the augmetation kernel from the \(\K_\theta\) in Definition~\eqref{def:gibbs} into the BYOL kernel \(\bS_{\theta}\) would lead to the BYOL loss.

%Here, we expand our connection between GCA and BYOL.


\textbf{Proof for the Theorem~\ref{thm:byol}}

BYOL has the online network parameterized by \(\theta\) and target network parameterized by \(\xi\), where \(\z'_\theta=\widetilde{f}_{\theta}(\x')\) and \(\z''_\xi=\widetilde{f}_{\xi}(\x'')\) are the normalized outputs of the online and target networks, respectively. The kernel of BYOL looks like:
$$\bS_{\theta}(\x'_i, \x''_j) = \exp (-\langle \widetilde{q}_{\theta}(\widetilde{f}_{\theta}(\x'_i)), \widetilde{f}_{\xi}(\x''_j) \rangle),$$
The kernel here involves both the parameters \(\theta\) and \(\xi\), however, the target network has the stop gradient. Therefore, the only \(\theta\) needs to be updated, so we can rewrite the kernel as $\bS_{\theta}(\x'_i, \x''_j)$ as we show in the main text. As we give in the equation, the corresponding proximal operators evolving with \(d_\Gamma\) is equal to L2-distance has the formula, and \(h(x)=0\) for all \(\PP\in\mathcal{R}^{B\times B}\):
\[ \text{Prox}_{\mathcal{R}^{B\times B}}^{\|\cdot\|^2}(\mathbf{\bS_{\theta}}) = \arg\min_{\mathbf{P} \in \mathcal{R}^{B\times B}} \left\{ h(\mathbf{P}) + \frac{1}{2} \|\mathbf{P} - \mathbf{S}_{\theta}\|^2_2 \right\}\Rightarrow \PP=\bS_{\theta} \]



The BYOL loss can be written as normalized L2-distance between the normalized output after online network \(\widetilde{q}_{\theta}(\z'_{\theta})\) in which \(\widetilde{q}_\theta\) is predictor and the stop gradient results for the target network \(\widetilde{q}_{\theta} (\z')\), and the formula of BYOL object reads as $L_\text{BYOL}=  \| \widetilde{q}_{\theta}(\z'_{\theta}) - \z''_{\xi} \|_2^2$. 


In this case, there exists equivalence between 
\begin{equation}
\text{KL}(\I\|\bS_{\theta})=-\sum_i^{B} \log \bS_{\theta ii}=\sum_i^{B} \| \widetilde{q}_{\theta}(\z'_{\theta}) - \z''_{\xi} \|_2^2
\end{equation}
which is the BYOL loss.



\subsection{Complexity Analysis for GCA}
\label{app:time_complexity}

In the forward pass, iteratively running the GCA does not involve inner optimization for gradient back-propagation.
In the Sinkhorn algorithm, the transport plan $\PP_\theta $ is computed as:
\[\PP_\theta = \exp(f + g - \C_\theta)/\epsilon,\]
where $f$ and $g$ are dual variables iteratively updated in the Sinkhorn algorithm but do not involve gradients with respect to $\theta$. The Sinkhorn optimization primarily entails scaling the rows and columns of $\PP$ to satisfy the marginal constraints, which can be viewed as element-wise operations (scaling and exponentiation) on the cost matrix $\C_\theta$.

Since $\PP_\theta$ is computed through the fixed-point iteration of $f$ and $g$ that depend only on the current values of $ \C_\theta $, the gradient back-propagation process is simplified. Specifically, the gradient of the loss with respect to the cost matrix $ \C_\theta $ is the key part that needs to be differentiated, rather than through each iterative update of $ f $ and $g$. A typical workflow of these algorithms was shown in Figure 2 of~\cite{eisenberger2022unified}, the gradient flow primarily involves differentiating through $ \C_\theta $, which is done only once, and not through each step of the Sinkhorn iterations. This approach reduces computational complexity and avoids the need for back-propagation through every iterative update within the Sinkhorn algorithm, which might otherwise be computationally expensive.



%%%%%%%%%%%% ANALYSIS AND PROOFS
\section{Proofs that GCA methods improve the alignment and uniformity}%~\hyperref[app:outline]{*}

\subsection{Improving Alignment}
\label{app:alignment}
In this section, we are going to show the GCA methods minimize the difference between the target alignment plan with the coupling matrix on latent. 
The uniformity and alignment loss have been used to exam the quality of the representation in self-supervised learning, which is defined as the following~\cite{wang2020understanding}:

\begin{definition} [Alignment loss]
Given \(\pi\) as joint distribution of positive samples on the latent, \((\z'_{\theta i}, \z''_{\theta i})\) are the normalized positive pairs sampled from the joint distribution \(\pi\) with encoder parameterized by \(\theta\), the alignment loss is:

\begin{equation}\label{eq:alignment}
\mathcal{L}_{\text{align}}=\min_\theta~ \mathbb{E}_{(\z'_{\theta i}, \z''_{\theta i}) \sim \pi}\left[\|\z'_{\theta i}-\z''_{\theta i}\|_2^2\right] 
=\min_\theta~\sum_{i} \operatorname{diag}(\C_{ii}),
\end{equation}
where \(\C\) is the cost matrix defined in Equation~\eqref{eq:dot}.
\end{definition}


 
We can alter the constraint sets of proximal operators to provide the better alignment plans, i.e. GCA-INCE changes the constraint sets by considering both row and column normalization in coupling matrix Rather than just the row normalization. Such change will not affect the alignment loss in forward pass, it will benefit the alignment loss in the backward pass through a tighter bound of empirical risk minimization with the identity matrix.


\subsubsection{Proof of the tighter bound of GCA in ERM}%~\hyperref[app:outline]{*}  
\label{app:klfg}
In this section, we provide the evidence for using the converged coupling plan \(\PP^{(\infty)}_\theta\) is better than the \(\PP^{(1)}_\theta\) or \(\PP^{(t)}_\theta\) in Equation~\eqref{eq:breg1} for the GCA-methods loss in table~\ref{tab:diff}. This loss function will correspond to different alignment loss on the latent. And here the ERM is the definition as we provided in Appendix~\ref{app:symmetry}.

\begin{lemma}\label{lm:klfg}
Denote \(f^{(t_1)}\) and \(g^{(t_2)}\) the two dual variables in their \(t_1\) and \(t_2\) iterations, respectively.
Then the objective loss in Equation~\eqref{thm:cl_eq_ot} could be written as \(
\operatorname{KL({\bf I}\|\PP_\theta}) =\varepsilon^{-1}(\operatorname{diag}(\C)-(f^{(t_1)}+g^{(t_2)}))\).
\end{lemma}




\textbf{Proof of the Lemma~\ref{lm:klfg}}:

The above Lemma~\ref{lm:klfg} be derived form Equation~\eqref{eq:uvupdatesapp}. 
Recall that \(\uu=\exp{(f/\varepsilon)}, \quad \vv=\exp{(g/\varepsilon)}, \quad \K=\exp{(-\C/\varepsilon)}\)
\begin{align*}
\operatorname{KL({\bf I}\|\PP_\theta}) & = -\sum_i \log\left(\PP_{ii}\right) = -\sum_i(\log \operatorname{diag}(\uu)_{ii} +\log \K_{ii}+\log\operatorname{diag}(\vv)_{ii}) \\
                                 &= -\varepsilon^{-1}\sum_i(f_i-C_{ii}+g_i)=\varepsilon^{-1}(\operatorname{diag}(C)-(f+g))
                                 \qed
\end{align*}



Here we provide the proof of the \textit{Best Alignment} in \textbf{Theorem}~\ref{thm:lowest}:

\textbf{Proof of the Theorem~\ref{thm:lowest}}


Based on the Lemma~\ref{lm:klfg}, to show \(\operatorname{KL({\bf I}\|\PP^{(\infty)}}) \leq \operatorname{KL({\bf I}\|\PP^{(1)}}) \).
We have to show: 
$$\varepsilon^{-1}(\operatorname{diag}(C)-(f^{(\infty)}+g^{(\infty)}))\leq \varepsilon^{-1}(\operatorname{diag}(C)-(f^{(1)}+g^{(1)})).$$
Then give the Lemma~\ref{lm:monotonicity}, we know the \(f^{(t_1)}\) and \(g^{(t_2)}\) increase and converge weakly to their upper bound. As the \(\operatorname{diag}(\C)\) will be unchanged in each proximal operations, we know the objective function \(\text{KL}({\bf I}\|\PP_\theta)\) have lower upper bound with \(f^{(t_1)}\) and \(g^{(t_2)}\) increase and finally converged. \qed

Based on the Lemma~\ref{lm:klfg}, We have to show: 
$$\varepsilon^{-1}(\operatorname{diag}(C)-(f^{(t)}+g^{(t)}))\geq \varepsilon^{-1}(\operatorname{diag}(C)-(f^{(\infty)}+g^{(\infty)})).$$
Then give the Lemma~\ref{lm:monotonicity}, when \(t_1\rightarrow \infty \), \(f^{(t_1)}\) converge uniformly to a fixed point \( f^{(\infty)} \) with \(f^{(t_1)} \leq f^{(\infty)}\). So we know the \(f^{(t_1)}\) and \(g^{(t_2)}\) increase and converge weakly to their upper bound. As the \(\operatorname{diag}(\C)\) will be unchanged in each proximal operations, we know the objective function finally converged to \(f^{(\infty)}\) and \(g^{(\infty)}\) (Similarly, we prove \(g^{(t_1)} \leq g^{(\infty)}\) in Appendix~\ref{app:monotonicity}).\qed




\subsubsection{Proof of the Theorem~\ref{thm:gcarince}:}
To show:
\( L_{\text{GCA-RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(t)}) \leq  L_{\text{RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(1)})\).

we know that:
\begin{equation}
L_{\text{GCA-RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(t)}) = -\frac{\operatorname{diag}(\PP^{(t)})}{\uu^{(t)}} + \frac{\lambda \I}{\uu^{(t)}},
\end{equation}

\begin{equation}
L_{\text{RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(1)}) = -\frac{\operatorname{diag}(\PP^{(1)})}{\uu^{(1)}} + \frac{\lambda \I}{\uu^{(1)}},
\end{equation}

Given that the Lemma~\ref{lm:monotonicity}, we know \(\{\uu^{(t)}\}\) and \(\{\vv^{(t)}\}\) are a monotonically increasing sequence where 
\begin{align}
 \uu^{(1)}\leq \uu^{(t)}&\Rightarrow \frac{\lambda \I}{\uu^{(1)}}\geq \frac{\lambda \I}{\uu^{(t)}} \\
-\operatorname{diag}(\K_\theta)\vv^{(0)}\geq -\operatorname{diag}(\K_\theta)\vv^{(t)} & \Rightarrow - \frac{\operatorname{diag}(\PP^{(1)})}{\uu^{(1)}}\geq  -\frac{\operatorname{diag}(\PP^{(t)})}{\uu^{(t)}}
\end{align}
Combine the above two items, we have the equation like
\( L_{\text{GCA-RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(t)}) \leq  L_{\text{RINCE}}^{\lambda,q=1, \varepsilon}(\PP_\theta^{(1)})\).





\subsection{GCA methods improve the uniformity and benefit downstream classification tasks}%~\hyperref[app:outline]{*} 
\label{app:uniformity}
In this section, we provide theoretical evidence that the GCA approaches could improve the performance of downstream task, i.e. classification tasks, by providing the maximum uniformity through solving the EOT, as \textbf{Theorem}~\eqref{thm:uniformity} stated. Here, the uniformity loss is defined as~\cite{saunshi2022understanding}:

\begin{definition} [Uniformity loss]
Let \(\z'_{\theta i}\sim\mu\) and \(\z''_{\theta j}\sim\nu\) in which \(\mu\) and \(\nu\) are two distributions on the representation space, we define the uniformity loss as the following:
\begin{equation}\label{eq:uniformity}
    L_{\text{uniform}} = \log \mathbb{E}_{\z'_{\theta i},\z''_{\theta j} \, \text{i.i.d.} \sim p_{\text{data}}} [ e^{-\varepsilon \left\|  \z'_{\theta i}-  \z''_{\theta j} \right\|^2_2} ]
\end{equation}, in which \( p_\text{data}(\cdot) \) is the sample distribution over latent space \( \mathbb{R}^n \).
\end{definition}
Here, \( p_\text{data}(\cdot) \) should be the marginal distribution of the samples. As the \(\z'_{\theta i}\) and \(\z''_\theta j\) are normalized latent variables, we have the right items of the uniformity loss \(e^{-\varepsilon \|\z'_{\theta i}-  \z''_{\theta j} \|^2_2}\) is the same as the entropy-regularized kernel \(\K_{ij}=e^{-\varepsilon\C_{ij}}\) with cost matrix items \(\C_{ij}=\|  \z'_{\theta i}-  \z''_{\theta j} \|^2_2\). 

\subsubsection{Proof of the Theorem~\ref{thm:uniformity}}



Here we are going to compare two different coupling plans, \(\PP_\theta^{(1)}\) and \(\PP_\theta^{(\infty)}\), and show the converged plan \(\PP_\theta^{(\infty)}\) will achieve higher the uniformity after the forward pass.  %Since the \(\PP_\theta^{(1)}\) and \(\PP^{(\infty)}\) are the solution of Bregman projection with different iterations, number. 
The general logic is that we show the equivalence for the solving EOT with the minimizing the uniformity loss objective. Then we use the convergence of iterative Bregman projections to show it could achieve higher uniformity.

Based on the Entropy regularized OT defined in Definition~\ref{def:phidivergence}, we have:
\begin{equation}
W_{c, \varepsilon}(\mu, \nu) \coloneqq \min_{\pi \in \Pi(\mu, \nu)} \int_{X \times Y} c(x, y) d\pi(x, y) + \varepsilon H(\pi | \mu \otimes \nu)
\end{equation}
in which the entropy could be defined as:
\begin{equation}
H(\pi | \mu \otimes \nu) \coloneqq \int_{X \times Y} \left( \log \left( \frac{d\pi(x, y)}{d\mu(x)d\nu(y)} \right) - 1 \right) d\pi(x, y) + 1,
\end{equation}
is the relative entropy of the transport plan \( \pi \) with respect to the product measure \( \mu \otimes \nu \). So the corresponding dual problem of this EOT one is shown in the following formula:
\begin{align}
W_{c, \varepsilon}(\mu, \nu) &= \max_{f \in C(\mathcal{X}), g \in C(\mathcal{Y})}  \int_\mathcal{X} f(x) \, d\mu(x) + \int_\mathcal{Y} g(y) \, d\nu(y)\\ &~~~~~~~~~~~ - \varepsilon \int_{\mathcal{X}  \times \mathcal{Y}} e^{\frac{f(x) + g(y) - c(x,y)}{\varepsilon}} \, d\mu(x)d\nu(y) + \varepsilon \\
&= \max_{f \in C(\mathcal{X}), g \in C(\mathcal{Y})} \mathbb{E}_{\mu \otimes \nu}\left[ f(x) + g(y) - e^{\frac{f(x) + g(y) - c(x,y)}{\varepsilon}} \right] +\varepsilon
\end{align}

The \(\mu(x)\) and \(\nu(x)\) are defined as the uniformly distribution with Dirac delta function we have on the two latent supports \(\{\z'_{\theta i}\}_{i=1}^{B}\) and \(\{\z''_{\theta i}\}_{i=1}^{B}\), so the function \(f(x)\) and \(g(y)\) could be pull out of the expectation operators. 
Since the \( \|  \z'_{\theta i}-  \z''_{\theta j} \|^2_2 \) is the element in the cost matrix \(\C_{ij}\), which is computed through the cost function \(c(x,y)\). As the \(\z'_{\theta i}\) and \(\z''_{\theta j}\) are drawn independently from the latent distribution, so the remaining item \(\mathbb{E}_{\mu \otimes \nu}[e^{\frac{- c(x,y)}{\epsilon}}]\) is equivalent to the uniformity loss. The the above integral could be turned into the sum of the elements in matrix of dual variables of \(f^{(t_1)}\) and\(g^{(t_1)}\) in each iteration.
Meanwhile, based on the convergence provided in the Lemma~\ref{lm:monotonicity}, When \(t_1\rightarrow \infty \), \(f^{(t_1)}\) converge uniformly to a fixed point \( f^{(\infty)} \) with \(f^{(t_1)} \leq f^{(\infty)}\), which would provided the maximum value of the dual formula in the \(f^{(\infty)}\), which corresponding to the coupling plan the \(\PP^{(\infty)}\). \(\qed\)



\subsubsection{GCA benefits the downstream supervised classification task} \label{app:gca_sup}

Here, we further show how the minimizing the uniformity loss is equivalent to minimize the downstream supervised loss in classification tasks under several assumptions~\cite{dufumier2023integrating}.
Giving a labeled dataset $\mathcal{D} = \{(\bar{\x}_i, \y_i)\} \in \bar{\mathcal{X}} \times \mathcal{Y}$ where $\mathcal{Y} = [1..M]$ with $M$ classes, we consider a fixed, pre-trained encoder $f_\theta \in \mathcal{F}: \mathcal{X} \to \mathcal{S}$ with its representation $f_\theta({\mathcal{X}})$ and the input space $\mathcal{X}$ contains both positive and negative views of $n$ original samples $(\bar{\x}_i)_{i \in [1..n]} \in \bar{\mathcal{X}}$, sampled from the data distribution $p(\bar{\x})$. 
For each positive views $\bar{\x}'_i$ in $\mathcal{X}$, we sample from $\bar{\x}_i$ using $\x_i' \sim \mathcal{A}(\cdot|\bar{\x}_i)$, \(\mathcal{A}(\cdot|\bar{\x}_i)\) is augmentation distribution (e.g., by applying color jittering, flip, or crop with a given probability). For consistency, we assume $\mathcal{A}(\bar{\x}) = p(\bar{\x})$ so that the distributions $\mathcal{A}(\cdot|\bar{\x})$ and $p(\bar{\x})$ induce a marginal distribution $p(\x)$ over $\mathcal{X}$. Given an anchor $\bar{\x}_i$, all views $\x'' \sim \mathcal{A}(\cdot|\bar{\x}_j), j \neq i$ from different samples $\bar{\x}_j$ are considered as negatives. 

\textbf{Proof of claim~\ref{cl:ce_eq_uniform}:}
% 1. Expressivity of the Encoder: The encoder's representations belong to an RKHS where the kernel-based centroid estimator is consistent.
% 2. Small Intra-Class Variance: Representations within the same class are close to their class centroid.
% 3. Balanced Classes: \( p(y) = \frac{1}{M} \) for all \( y \in [1..M] \).
% Our general logic is to show the supervised loss in downstream classification tasks could be transformed into the measure dependent on the centroids of cluster in the latent space, which could be bounded by the GCA-improved uniformity loss under several assumptions.
From assumption~\ref{asm:express_encoder} we know that the representation ability of encoders is good enough via the augmented samples in the Reproducing Kernel Hilbert Space (RKHS) \(\mathcal{H}_{\bar{\mathcal{X}}}\) of the original sample spaces $\bar{\mathcal{X}}$. And the kernel \( K_{\bar{\mathcal{X}}} \) with any function \( g \) RKHS defined by \( (\mathcal{H}_{f_\theta}, \K_\theta) \) also belongs to \( H_{\bar{\mathcal{X}}} \) when conditioned on the distribution \(\mathcal{A}(\x|\cdot)\). So based on the assumption we have, we can obtain a centroid estimator by~\cite{dufumier2023integrating}:

\begin{definition}[Kernel-based centroid estimator]
Let $(\x_i, \bar{\x}_i)_{i \in [1..n]} \sim \mathcal{A}(\x, \bar{\x})$, asssuming a consistent estimator of $\mu_{\bar{\x}}$ is. 
\[
\forall \bar{\x} \in \bar{\mathcal{X}}, \hat{\mu}_{\bar{\x}} = \sum_{i=1}^{n} \alpha_i(\bar{\x}) f(\x_i),
\]
where $\alpha_i(\bar{\x}) = \sum_{j=1}^{n} [(\K_n + n\lambda \I_n)^{-1}]_{ij} \K_{\bar{\mathcal{X}}}(\bar{\x}_j, \bar{\x})$ and $\K_n = [\K_{\bar{\mathcal{X}}}(\bar{\x}_i, \bar{\x}_j)]_{i,j \in [1..n]}$. It converges to $\mu_{\bar{\x}}$ with the $\ell_2$ norm at a rate $\mathcal{O}(n^{-1/4})$ for $\lambda = \mathcal{O}(n^{-1/2})$. 
\end{definition}


The above estimator allows us to use representations of images close to an anchor $\bar{\x}$ to estimate $\mu_{\bar{\x}}$. From the assumption~\ref{asm:small_intra_variance}, we assume that all the samples in the same class is achievable when give the ideal augmentation or at least close to the augmented points in an \(\epsilon\) region. 


 Consequently, if the prior is “good enough” to connect intra-class images disconnected in the augmentation graph suggested by Assumption~\ref{asm:express_encoder}, then this estimator allows us to tightly control the classification risk of the representation of $f_\theta$ on a classification task with a linear classifier $g(\bar{\x}) = \W f_\theta(\bar{\x})$ (with $f_\theta$ fixed) that minimizes the multi-class classification loss.

 \textbf{First we show the cross-entropy could be transformed into centroid based distance (optimal supervised loss):}
\label{transform_ce_to_sup}
The cross-entropy (CE) to measure the difference between the true distribution (actual labels) and the estimated probability distribution (predicted probabilities from the model), which usually computes logits \( \z_k \) from the model, then apply the softmax function to obtain probabilities \( p_k \). 
The logits \( \z_k \) could be defined as negative distances between \( f(\bar{\x}) \) and class centroids \( \mu_k \) after the representation:
\[
\z_k = -\| f(\bar{\x}) - \mu_k \|^2, \quad \mu_k = \mathbb{E}_{p(\bar{\x}|\y=k)} \mu_{\bar{\x}}
\]
which encourages the model to reduce the distance to the correct class centroid while increasing distances to others. The probability of class \( k \) in M classes given input \( \bar{\x} \) is:
\[
p(\y = k | \bar{\x}) = \frac{e^{\z_k}}{\sum_{j=1}^M e^{\z_j}}, \quad  p(\y | \bar{\x}) \propto e^{-\| f(\bar{\x}) - \mu_\y \|^2}. 
\]
If the model predictions \( p(\y | \bar{\x}) \) are influenced by the distances between \( \bar{\x} \) and the class centroids \( \mu_\y \), then minimizing cross-entropy indirectly affects these distances.
The standard CE loss in supervised learning for classification tasks is:
\begin{align}
\mathcal{L}_{\text{CE}}(f_\theta) & = -\mathbb{E}_{(\bar{\x}, \y) \sim \mathcal{D}} \left[ \log p(\y | \bar{\x}) \right] \\ & =-\mathbb{E}_{(\bar{\x}, \y) \sim \mathcal{D}} \left[ -\| f(\bar{\x}) - \mu_\y \|^2 - \log Z \right] =-\frac{1}{N} \sum_{i=1}^N \sum_{k=1}^M \y_{i,k} \log(p_{i,k})
\end{align}

which focuses on maximizing the likelihood \( \hat{\y} = \arg\max_k p(\y = k | \bar{\x}) \) of the correct class for each individual sample \( \bar{\x}_i \), where \( y_{i,k} \) is the true label indicator for example \( i \) and class \( k \), \( p_{i,k} \) is the predicted probability for example \( i \) and class \( k \). Therefore, we can rewrite the  CE loss as optimal supervised loss in~\cite{dufumier2023integrating}, which is defined as:


\begin{lemma} [Optimal supervised loss]\label{lm:ot_sup_loss}
Let a downstream task $D$ with $M$ classes. We assume that $M \leq d + 1$ (i.e., a big enough representation space), that all classes are balanced and the realizability of an encoder $f^* = \arg\min_{f \in F} \mathcal{L}_{\text{sup}}(f_\theta)$ with
\[
\mathcal{L}_{\text{sup}}(f_\theta) = \log \mathbb{E}_{y, y' \sim p(y)p(y')} \left[ e^{-\| \mu_y - \mu_{y'} \|^2} \right],
\]
and $\mu_y = \mathbb{E}_{p(\bar{x}|y)} \mu_{\bar{x}}$. Then the optimal centroids $(\mu^*_y)_{y \in Y}$ associated to $f^*$ make a regular simplex on the hypersphere $S^{d-1}$ and they are perfectly linearly separable, i.e.,
\[
\min_{(w_y)_{y \in \mathcal{Y}} \in \mathbb{R}^d} \mathbb{E}_{(\bar{x}, y) \sim D} \mathbbm{1}(w_y \cdot \mu^*_y < 0) = 0.
\]
\end{lemma}
\textbf{Proof of the Lemma~\ref{lm:ot_sup_loss}}
All "labeled" centroids $\mu_y = \mathbb{E}_{p(\bar{x}|y)} \mu_{\bar{x}}$ are bounded by 1 ($\|\mu_y\| \leq \mathbb{E}_{p(\bar{x}|y)} \mathbb{E}_{A(x|x')} \| f(x) \| = 1$ by Jensen's inequality). Then, since all classes are balanced, we can re-write the supervised loss as:
\[
\mathcal{L}_{\text{sup}}(f_\theta) = \log \frac{1}{C^2} \sum_{y, y'=1}^{C} e^{-\| \mu_y - \mu_{y'} \|^2}.
\]
We have:
\[
\Gamma_\mathcal{Y}(\mu) := \sum_{y, y'} \|\mu_y - \mu_{y'}\|^2 = \sum_{y, y'} \|\mu_y\|^2 + \|\mu_{y'}\|^2 - 2 \mu_y \cdot \mu_{y'}
\leq \sum_{y, y'} (2 - 2 \mu_y \cdot \mu_{y'})
= 2C^2 - 2 \| \sum_{y} \mu_y \|^2 \leq 2C^2,
\]
with equality if and only if $\sum_{y=1}^{C} \mu_y = 0$ and $\forall y \in [1..C], \|\mu_y\| = 1$. By the strict convexity of $u \to e^{-u}$, we have:
\[
\sum_{y \neq y'} \exp(-\|\mu_y - \mu_{y'}\|^2) \geq C(C-1) \exp \left( - \frac{\Gamma_Y(\mu)}{C(C-1)} \right)
\geq C(C-1) \exp \left( - \frac{2C}{C-1} \right),
\]
with equality if and only if all pairwise distances $\|\mu_y - \mu_{y'}\|$ are equal (equality case in Jensen's inequality for a strict convex function), $\sum_{y=1}^{C} \mu_y = 0$, and $\|\mu_y\| = 1$. Thus, all centroids must form a regular $(C-1)$-simplex inscribed on the hypersphere $S^{d-1}$ centered at 0. Furthermore, since $\|\mu_y\| = 1$, we have equality in Jensen's inequality:
\[
\|\mu_y\| = \|\mathbb{E}_{\mathcal{A}(\x|\bar{\x}')} f_\theta(x)\| \leq \mathbb{E}_{ \mathcal{A}(\x|\bar{\x}')} \|f_\theta(x)\| = 1,
\] 
so $f$ must be perfectly aligned for all samples belonging to the same class: $\forall x, \bar{x}' \sim p(\cdot|y), f_\theta(\bar{x}) = f_\theta(\bar{x}')$. $\qed$



\textbf{Seond we show optimizing the uniformity loss is equivalent to the supervised loss:}

As we have uniformity Loss defined in Equation~\eqref{eq:uniformity}
\begin{equation}
L_{\text{uniform}}(f_\theta) = \log \mathbb{E}_{\z'_i, \z''_j \sim p_{\text{data}}} \left[ e^{ -\varepsilon \left\| \z'_i - \z''_j \right\|^2 } \right],
\end{equation}

where \( \z'_i = f(\x_i) \) and \( \z''_j = f(\x_j) \). Supervised Loss:
\[
\mathcal{L}_{\text{sup}}(f_\theta) = \log \mathbb{E}_{y, y' \sim p(y)p(y')} \left[ e^{ -\left\| \mu_y - \mu_{y'} \right\|^2 } \right],
\]
where \( \mu_y = \mathbb{E}_{p(\bar{\x}| y)} \hat{\mu}_{\bar{\x}} \). Express the expectation over all pairs in terms of class labels:
\[
\mathbb{E}_{\z'_i, \z''_j} = \mathbb{E}_{y, y'} \mathbb{E}_{\z'_i \sim p(\z | y), \z''_j \sim p(\z | y')}.
\]
So the uniformity loss could be decomposed into intra-class and inter-class components:
\[
L_{\text{uniform}}(f_\theta) = \log \left( \underbrace{\mathbb{E}_{y} \left[ \mathbb{E}_{\z'_i, \z''_j \sim p(\z | y)} \left[ e^{ -\varepsilon \left\| \z'_i - \z''_j \right\|^2 } \right] \right]}_{\text{Intra-Class Term}} + \underbrace{\mathbb{E}_{y \neq y'} \left[ \mathbb{E}_{\z'_i \sim p(\z | y), \z''_j \sim p(\z | y')} \left[ e^{ -\varepsilon \left\| \z'_i - \z''_j \right\|^2 } \right] \right]}_{\text{Inter-Class Term}} \right).
\]

Based on the assumption~\ref{asm:small_intra_variance}, we can approximate the Intra-Class term by:
\begin{align*}
\left\| \z'_i - \z''_j \right\|^2 & = \left\| (\mu_y + \delta_i) - (\mu_{y'} + \delta_j) \right\|^2 = \left\| \mu_y - \mu_{y'} + \delta_i - \delta_j \right\|^2\approx \left\| \mu_y - \mu_{y'} \right\|^2 \\
&  \implies \mathbb{E}_{\z'_i \sim p(\z | y), \z''_j \sim p(\z | y')} \left[ e^{ -\varepsilon \left\| \z'_i - \z''_j \right\|^2 } \right] \approx e^{ -\varepsilon \left\| \mu_y - \mu_{y'} \right\|^2 }
\end{align*}




for \( y = y' \), \( \z'_i \) and \( \z''_j \) are close to \( \mu_y \)
\[
\left\| \z'_i - \z''_j \right\|^2 \approx \left\| (\mu_y + \delta_i) - (\mu_y + \delta_j) \right\|^2 = \left\| \delta_i - \delta_j \right\|^2.
\]
Since \( \delta_i \) and \( \delta_j \) are small deviations:
\[
\mathbb{E}_{\z'_i, \z''_j \sim p(\z | y)} \left[ e^{ -\varepsilon \left\| \z'_i - \z''_j \right\|^2 } \right] \approx 1, \quad e^{ -\varepsilon \left\| \delta_i - \delta_j \right\|^2 } \approx 1.
\]


Then with \(M\) terms for \(y=y'\) and \(M(M-1)\) terms of \(y\neq y'\), we have:

\begin{equation}
L_{\text{uniform}} =\log (\frac{1}{M}e^{ -\varepsilon \left\| \delta_i - \delta_j \right\|^2 } +  \frac{1}{M^2} \sum_{y \neq y'} e^{ -\varepsilon \left\| \mu_y - \mu_{y'} \right\|^2 } ) 
\end{equation}

The supervised loss is:
\[
\mathcal{L}_{\text{sup}}(f_\theta) = \log ( \frac{1}{M^2} \sum_{y, y'} e^{ -\left\| \mu_y - \mu_{y'} \right\|^2 } ) = \log ( \frac{1}{M} e^{ -\left\| \mu_y - \mu_{y} \right\|^2 } + \frac{1}{M^2} \sum_{y \neq y'} e^{ -\left\| \mu_y - \mu_{y'} \right\|^2 } ) 
\]
Since \( e^{ -\left\| \mu_y - \mu_{y} \right\|^2 } = 1 \) (for \( y = y' \)), the difference will be mainly dependent on the inter-class term. Therefore, a tighter (smaller) uniformity loss leads to smaller values of the supervised loss. This supports the idea that improving uniformity in representations can benefit downstream supervised classification tasks. \(\qed\)




\subsection{Unbalanced OT assists to alleviate the feature suppression}%~\hyperref[app:outline]{*}
\label{app:gcauot-feature}

Although minimizing the uniformity loss can enhance downstream classification tasks, it may also lead the model to learn shortcut features that could impair the encoder's generalization ability. To show this, we incorporate two propositions from previous work by Robinson et al.~\cite{robinson2021can}.

\subsubsection{The uniformity loss causes feature suppression}

For an encoder $f_\theta : \mathcal{X} \rightarrow \mathbb{S}^{d-1}$ to map input data $\x$ to the surface of the unit sphere $\mathbb{S}^{d-1} = \{ u \in \mathbb{R}^d : ||u||_2 = 1 \}$. 
Suppose we have the latent feature spaces $\Z^1, \dots, \Z^n$ with a distribution $p_j$ on each latent space $\Z^j$ with $j \in [n]$ to model a distinct feature. We write $\Z$ instead of $\Z^{[n]}$ for the product as $\Z^S = \prod_{j \in S} \Z^j$, where $[n] = \{1, \dots, n\}$.
So the latent sample \( \z \) could be represented as a set of feature vectors \( \z = (\z^1, \z^2, \dots, \z^n)= (\z^j)_{j \in S} \in \Z \), where each \( \z^j \) comes from \( \Z^j \). Further, let $\lambda$ denote the measure on $\Z$ induced by $\z$ and 
$\lambda( \cdot | \z^S)$ denote the conditional measure on $\Z$ for fixed $\z^S$. For $S \subseteq [n]$ we use $\z^S$ to denote the projection of $\z$ onto $Z^S$. Finally, an injective map $g : \Z \rightarrow \mathcal{X}$ produces observations $\x = g(\z)$. The feature suppression is defined as:


\begin{definition}
Consider an encoder $f_\theta : \mathcal{X} \rightarrow \,\mathbb{S}^{d-1}$ and features $S \subseteq [n]$. For each $\z^S \in Z^S$, let $\mu(\cdot | \z^S)$ be the pushforward measure on $S^{d-1}$ by $f_\theta \circ g$ of the conditional $\lambda( \cdot | \z^S)$.




\begin{enumerate}
    \item $f_\theta$ suppresses $S$ if for any pair $\z^S, \tilde{\z}^S \in Z^S$, we have $\mu(\cdot | \z^S) = \mu(\cdot | \tilde{\z}^S)$.
    \item $f_\theta$ distinguishes $S$ if for any pair of distinct $\z^S, \tilde{\z}^S \in \Z^S$, measures $\mu(\cdot | \z^S), \mu(\cdot | \tilde{\z}^S)$ have disjoint support.
\end{enumerate}
\end{definition}


If one feature is uniformly distributed on the latent space, it might cause feature suppression
due to different features could both achieve the minimization of the uniformity loss as the following propositions~\cite{robinson2021can}:



\begin{proposition} [Feature suppression]\label{prop:feat_suppress}
For a set $S \subseteq [n]$ of features let
\[
L_S(f_\theta) = L_{\text{align}}(f_\theta) + \mathbb{E}_{\x^+} \left[ - \log \mathbb{E}_\x \left[ e^{f(\x^+ )^\top f(\x^- )} \middle| \z^S = \z^{S^-} \right] \right]
\]
denote the (limiting) InfoNCE conditioned on $\x^+, \x^-$ having the same features $S$. Suppose that $p_j$ is uniform on $Z^j = S^{d-1}$ for all $j \in [n]$. Then the infimum $\inf L_S$ is attained, and every $f_\theta \in \arg\min_f L_S(f_\theta')$ suppresses features $S$ almost surely.
\end{proposition} 

\textbf{Proof of proposition~\ref{prop:feat_suppress} is in \cite{robinson2021can}}.




\subsubsection{How the GCA methods and unbalanced OT and  alleviates the feature suppression}  

Here we extended the unbalanced OT in the Equation~\eqref{eq:uot} as the following: 
\begin{equation}
    %L_{\textrm{\tiny UOT}}  = 
    \min_{\theta}~ d_M ( \PP_{\text tgt} \| \PP_\theta)  + \lambda_1 d_{\phi_1}(\PP_\theta) +\lambda_2 d_{\phi_2}(\PP_\theta) +\cdots+\lambda_n d_{\phi_n}(\PP_\theta)
\end{equation} 


The UOT equation can be converted with finding the transport plan \( \PP_\theta \) that minimizes the transportation cost between two probability measures \( \mu \) and \( \nu \). Here we only need to show that the relaxation or adding penalties will change the optimal transport plan \(\PP_\theta\), which is empirically exhibited in the Figure~\ref{fig:diagnolline}.



Suppose we have empirical samples \( \{ \z'_i \}_{i=1}^n \) from \( \mu \) and \( \{ \z''_j \}_{j=1}^m \) from \( \nu \). We can approximate the measures using empirical distributions:
\[
\mu \approx \frac{1}{n} \sum_{i=1}^n \delta_{\z'_i}, \quad \nu \approx \frac{1}{m} \sum_{j=1}^m \delta_{\z''_j},
\]
where \( \delta_{\z} \) is the Dirac delta function at point \( \z \). The standard UOT objective can be written as:

\begin{align}
&  \min_{\PP \geq 0} \sum_{i=1}^n \sum_{j=1}^m \C(\z'_i, \z''_j) \PP_{ij} + \lambda_1 d_{\phi_1}\left( \sum_{j=1}^m \PP_{ij} \Big\| \frac{1}{n} \right) + \lambda_2 d_{\phi_2}\left( \sum_{i=1}^n \PP_{ij} \Big\| \frac{1}{m} \right)
\\ & = \min_{\PP \geq 0} \sum_{i=1}^n \sum_{j=1}^m \left[ \C_{ij} \PP_{ij} + \lambda_1 \PP_{ij} \left( \log \frac{\PP_{ij}}{r_i} - 1 \right) + \lambda_2 \PP_{ij} \left( \log \frac{\PP_{ij}}{c_j} - 1 \right) \right]
\end{align}

where \( \C \) is the cost matrix \( d_{\phi} \) could be any divergence (e.g., Kullback-Leibler divergence) with respect to a convex function \( \phi \).
\( \PP \mathbf{1}_{\mu} \) and \( \PP^\top \mathbf{1}_{\nu} \) are the marginal distributions.
\( \lambda_1, \lambda_2 \) are regularization parameters controlling the unbalancedness and \( r_i = \frac{1}{n} \) (source marginal mass for \( \z'_i \)), \( c_j = \frac{1}{m} \) (target marginal mass for \( \z''_j \)). 
Based on the UOT, here we can choose the divergence as \(\mathcal{L}\):
\[
   \mathcal{L}(\mathbf{P}) = \sum_{i,j} \left[ C_{ij} \mathbf{P}_{ij} + \lambda_1 \mathbf{P}_{ij} \left( \log \frac{\mathbf{P}_{ij}}{r_i} - 1 \right) + \lambda_2 \mathbf{P}_{ij} \left( \log \frac{\mathbf{P}_{ij}}{c_j} - 1 \right) \right]
   \]

To find the minimizer, we take the partial derivative of \(L(\mathbf{P})\) with respect to \(\mathbf{P}_{ij}\) and set it to zero:

\begin{align}
& \frac{\partial \mathcal{L}}{\partial \mathbf{P}_{ij}} = C_{ij} + \lambda_1 \left( \log \frac{\mathbf{P}_{ij}}{r_i} \right) + \lambda_2 \left( \log \frac{\mathbf{P}_{ij}}{c_j} \right) = 0
\\ & \implies \lambda_1 \left( \log \mathbf{P}_{ij} - \log r_i \right) + \lambda_2 \left( \log \mathbf{P}_{ij} - \log c_j \right) = -C_{ij}
\\ & \implies
   (\lambda_1 + \lambda_2) \log \mathbf{P}_{ij} - \lambda_1 \log r_i - \lambda_2 \log c_j = -C_{ij}
\\ & \implies    \log \mathbf{P}_{ij} = \frac{ -C_{ij} + \lambda_1 \log r_i + \lambda_2 \log c_j }{ \lambda_1 + \lambda_2 }
\\ & \implies
   \mathbf{P}_{ij} = \exp\left( \frac{ -C_{ij} + \lambda_1 \log r_i + \lambda_2 \log c_j }{ \lambda_1 + \lambda_2 } \right)
\end{align}

The minimizer \(\mathbf{P}_{ij}\) depends on \(\lambda_1\) and \(\lambda_2\) and the weights of \(r_i\) and \(c_j\), which determine the influence of the marginals \(r_i\) and \(c_j\), and through the scaling of the cost \(C_{ij}\) by \(\lambda_1 + \lambda_2\). This explicit relationship shows how \(\lambda_1\) and \(\lambda_2\) determine the minimizer.








\section{Details of Experiments}
\label{app:exp_details}

The following experiments involving with the GPU was set up on NVIDIA GeForce RTX 3090.


\subsection{Experimental details on image classification task}
\label{app:hpdetails}
In Table~\ref{tab:new_combine} standard settings, we used two different experimental setups. The first setup, referred to as the C0 or standard settings, was applied specifically to the CIFAR10 and CIFAR100 tasks. The second setup was used for the SVHN and ImageNet100 tasks, respectively. Below, we present the settings for CIFAR10 and CIFAR100, followed by the setups for SVHN and ImageNet100. %We train the self-supervised leanring model first and then frozen the parameters with the projectors.
Here is the setups for CIFAR10 and CIFAR100:
\begin{itemize}
    \item The SSL model has 512 feature dimensions with the base model (ResNet-18), which first convolutional changed as a layer with 3 input channels, 64 output channels, kernel size 3, stride 1, padding 1, and no bias. We replace the max-pooling layer as the identity.
    \item A sequential projector comprising a linear layer mapping from feature dimension to 2048, ReLU activation, and another linear layer mapping from 2048 to 128. %Max pooling replaced by an identity operation. 

\item For SSL training, an SGD optimizer is used with a learning rate of 0.6, momentum 0.9, and a weight decay of 1.0e-6. A LambdaLR scheduler is employed with linearly decay the learning rate to 1.0e-3 over total steps, which equals the length of the SSL training loader times the maximum epochs. The SSL model is trained for a maximum of 500 epochs, without loading a pre-trained model. The parameters of encoders are frozen after training. Temperature or epsilon: 0.5. 


\item For supervised training, an Adam optimizer is also used with a learning rate of 0.2, momentum 0.9 and a weight decay of 0. A same LambdaLR scheduler is applied, where the learning rate is reduced by a factor of 1.0e-3.  For supervised training, the model is trained for a maximum of 200 epochs using the specified train and test loaders. 



\end{itemize}


The setups for SVHN and ImageNet100 are:


\begin{itemize}
    \item The SSL model has number of feature dimensions equal to the fc layer incoming features of base model (ResNet-50). We replace the max-pooling layer as the identity.
    \item A sequential projector comprising a linear layer mapping from feature dimension to 2048, ReLU activation, and another linear layer mapping from 2048 to 128. %Max pooling replaced by an identity operation. 

\item For SSL training, an Adam optimizer is used with a learning rate of 3e-4. The SSL model is trained for a maximum of 200 epochs for ImageNet100 and 500 epochs for the SVHN, without loading a pre-trained model. The parameters of encoders are frozen after training. Temperature or epsilon: 0.5. 


\item For supervised training, an Adam optimizer is also used with a learning rate of 3e-4. The model is trained for a maximum of 100 epochs using the specified train and test loaders. 

\end{itemize}
\subsection{Settings for extreme data augmentations}\label{app:corruptset}


There is the "extreme DA" (Ex DA) column in Table~\ref{tab:new_combine}, which is the average of the following three settings:
\begin{itemize}
    \item  C1: Large Erase Settings: Here, we first employed the same standard augmentation as C0 in Appendix~\ref{app:hpdetails} does, than we apply the random erase with 'p=1' (random erasing is applied every time), the 'scale=(0.10, 0.33)'. The large erase is applied before the normalization.
    
    \item C2: Strong Crop Setting: This involves a strong cropping operation followed by resizing, which applied by 'transforms.RandomCrop' and 'transforms.Resize'. The crop size varies based on the severity level, with values ranging from 96 to 224 pixels. We selected level 3 during our experiments, than Resizes the cropped image back to 32x32 pixels.
    
    \item C3: Brightness settings: This augmentation alters the brightness of the images. We have 'severity' determines the degree of brightness change, with predefined levels ranging from `.05` to `.3`, corresponding to level 1 and level 5. And we chosse the level 5 as our C3 augmentation. The brightness is adjusted in the HSV color space, specifically altering the value channel to change the brightness.

\end{itemize}

To evaluate performance on CIFAR10-C, we use a pretrained SSL model with frozen parameters. Fine-tuning is performed by training only the linear layer with \(10\%\) of CIFAR10-C data for 50 epochs. We compute the final score by averaging results across all corruption types and severity levels in CIFAR10-C.
And the details of each column are provided in Table~\ref{tab:cifar10}, Table~\ref{tab:cifar1004c} and Table~\ref{tab:cifar10c4c}.


\input{files/table/rebuttal_table}


\subsection{Experimental setting for domain generalization}
\label{appendix:dgsetting}


This section is going to show the settings of experiments in Figure~\ref{fig:Ptgt}, which involves the domain generalization task. Training was executed under the DomainBed framework. Each model underwent training across multiple domains, with 5 distinct seeds (seed 71, 68, 42, 36, 15) used to ensure reproducibility:

%The model was optimized using the Adam optimizer. Specific hyperparameters included an initial learning rate set through a predefined schedule, and the regularization was controlled via weight decay.  

\begin{itemize}
  \item For SSL model configuration, we employed a ResNet-18 architecture as the encoder, following with a 2048-dimensional, 3-layer projector equipped with BatchNorm1D and ReLU activations. We improved the framework of the SelfReg algorithm in Domainbed~\cite{gulrajani2020search} by a self-supervised contrastive learning phase which involves the GCA-INCE, with regularized parameters \(\varepsilon=0.2\).
  \item For SSL training hyperparameters, an Adam optimizer is used with a learning rate of 3e-4, and a weight decay of 1.5e-6. A Cosine Annealing learning rate scheduler is employed with a maximum number of 200 iterations equal to the length of the SSL training. The learning rate is scheduled to decrease to a minimum value of 0. The SSL model is trained for a maximum of 1500 epochs.

%  \item The PACS dataset was used, testing domain generalization capabilities across multiple environmental settings, for example, we might use 0.2 percent data from each .
    \item In the self-supervised learning phase, we utilized 20\% of the data from each of the four datasets in the PACS dataset. The unsupervised holdout part employed contrastive learning augmentations to enhance generalization capabilities. Specifically, we implemented dual augmentation, including operations such as random resized crops, flips, color jitter, and grayscale conversion, standardized to an input shape of \(3 \times 224 \times 224\).
    \item The supervised learning rate was set at \(5 \times 10^{-5}\) using MSE loss, and the Adam optimizer with no weight decay. Training involved both domain and class labels over 3000 epochs, with checkpoints every 300 epochs to capture the model's best performance. This approach was supplemented by fine-tuning the model post-unsupervised training phase. Domain labels were categorized into four types corresponding to the PACS dataset, and class labels were divided into five categories. In domain classification, all four domains are used for training, with 70\% of the data held out for training and the remaining 30\% used for testing. Four domains are utilized for class classification tasks. We train supervised models on three domains and test on the fourth.
  \item The domain accuracy is computed as the average of the highest domain accuracies across five seeds, with each of the four test domains set sequentially as the test domain. The standard deviation for domain accuracy is calculated from the results across these five seeds.
  \item Class label accuracy is determined by averaging the accuracies of the four test environments for each domain. The average of highest performance across the domain is taken as the mean accuracy. The standard deviation for each domain is computed from the five seeds, and these values are then averaged to obtain the final class standard deviation.

\end{itemize}
Both the label classification tasks and the domain classification tasks use the Mean Squared Error (MSE) loss.



\section{Additional Experiments}\label{app:add_experiments}




\subsection{Complexity Analysis of GCA Algorithms}
\label{app:time}

\textbf{Time complexity analysis:}
The computational complexity of GCA including the forward pass and backward propagation phases. The complexity varies in different variants. For GCA-INCE, the computational complexity of forward pass is related to the speed of Sinkhorn when solving the EOT problem as $O(n^2/\varepsilon^3)$, in which $\varepsilon$ is the regularization parameter . For GCA-UOT, the forward complexity is the Sinkhorn algorithm solving unbalanced OT, which is characterized by $$O(\tau (\alpha + \beta)^2/\varepsilon \log(n) [\log(\|C\|_\infty) + \log(\log(n)) + \log(1/\varepsilon)]),$$ where $C$ is the cost matrix, $\alpha$ and $\beta$ denote the total masses of the measures, and $\tau$ is a regularization parameter related to KL divergences in the UOT framework~\cite{pham2020unbalanced}. 
Notably, the gradient backpropagation speed is not seriously affected by scaling operations in the EOT as we explained in Section~\ref{app:time_complexity}. Moreover, the relaxations of penalties in UOT provide a even faster speed compared with the INCE and GCA-INCE (see Figure ~\ref{fig:time_comp}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{files/imgs/figure_time.png}
    \caption{\footnotesize{\em Time complexity analysis}  (A) Time complexity analysis of different methods.  Here, we provide the time complexity for different contrastive methods (INCE, RINCE) and GCA-based methods (GCA-INCE, GCA-RINCE, and GCA-UOT) on CIFAR-10. (B) Time complexity for INCE (GCA-INCE-1), and GCA-INCE with different number of iterations GCA-INCE-100 denotes GCA-INCE with 100 iterations. We ran the methods on the CIFAR-10 as self-supervised learning task for 50 epochs, and compared their run time. (C) Performance of the INCE (iteration=1) and GCA-INCE (iterations>1) on the CIFAR10 with different number of iterations. The shaded blue region is the standard deviation across 5 seeds.
}
    \label{fig:time_comp}
\end{figure}


The complexity of the forward pass is affected by the choice of proximal operator, whereas the complexity of the gradient backward pass is influenced by the form of $d_M$~\cite{nguyen2010estimating}. Notably, utilizing Sinkhorn algorithms in GCA-UOT, GCA-RINCE, and GCA-INCE, only requires updating the coupling matrix \(\PP\) \((B \times B)\) without impacting the complexity of the backward pass, where \(B\) is the batch size. OT is known to have $B^2$ complexity and in many cases can converge very quickly in fewer than 10 iterations. In practice, we use a simple stopping criterion for the multiple iterations using a convergence criterion.

Upon analyzing the run time for the different methods (see Figure~\ref{fig:time_comp}) we observe that the GCA-based variants of the different base approaches (INCE, or RINCE) achieve very similar run time as their equivalent loss, but different losses (RINCE vs INCE) exhibit more significant variability. Specifically, we find that RINCE and GCA-RINCE have lower time complexity than INCE and GCA-INCE. So the runnning speed is even quicker if we utillized different \(d_M\) in Equation~\eqref{eq:mainobj}.







\subsection{Measuring the representation quality using alignment and uniformity}
\label{app:representation}

We study the uniformity and alignment of the representations learned by our GCA-INCE vs. INCE variants of GCA in Algorithms~\ref{alg:gca}.% \ranl{Mention where this is studied in main text}.
We train the model through the corresponding settings (C0: standard provided in the , C1: erase, C2: crop, C3: brightness) provided in the Appendix~\ref{app:hpdetails} and Appendix~\ref{app:corruptset}. We find that in general, the GCA variants improve the representation quality evaluated by alignment and uniformity on both CIFAR-10 and CIFAR-10C datasets. 


\begin{figure}[ht]
  \begin{minipage}[c]{0.49\textwidth}
    \centerline{\includegraphics[width=0.8\textwidth]{files/imgs/uniformalign.pdf}}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.5\textwidth}
  \caption{ \footnotesize{\em Alignment and uniformity metrics on CIFAR-10.} To visualize the ability of uniformity and alignment with different methods under different augmentation settings (C0: standard, C1: erase, C2: crop, C3: brightness). The bar above the x axis (zero line) represents the alignment loss, while the bar under the x axis represents the uniformity loss. The shorter the color bars i.e with lower alignment loss and higher uniformity loss, correspond to the better performance of SSL models.} % \label{fig:03-03}
  \end{minipage}
\end{figure}







\subsection{Visualizing transport plans of different methods after training}
\label{app:vistransplan}

Here we compared the optimal transport (OT) plans of different methods after training for 500 epochs under standard augmentation C0 settings in Appendix~\ref{app:hpdetails}. Specifically, we analyzed the \( -\log(\mathbf{P}) \) matrices of INCE, GCA-INCE, GCA-RINCE, and GCA-UOT, as shown in Figure~\ref{fig:diagnolline}. In these matrices, darker blue regions represent higher similarity, while lighter blue areas indicate less similarity. The matrices are rearranged based on class labels, so an effective model should display empty diagonals and block structures aligned along the main diagonal and sub-diagonals—reflecting high intra-class similarity and low inter-class similarity.

Figure~\ref{fig:diagnolline}(A) shows that INCE results in a matrix with only row normalization. In contrast, Figures~\ref{fig:diagnolline}(B) and (C) demonstrate that GCA-INCE and GCA-RINCE achieve both row and column normalization, leading to more uniform distributions. Figure~\ref{fig:diagnolline}(D) reveals that GCA-UOT produces a matrix highlighting greater differences between positive and negative pairs, underscoring its effectiveness in distinguishing them.


\begin{figure}[ht]
  \begin{minipage}[c]{0.6\textwidth}
    \centerline{\includegraphics[width=0.9\textwidth]{files/imgs/logp.pdf}}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.3\textwidth}
  \caption{\footnotesize { \em Comparison of the \( -\log(\mathbf{P}) \) matrix across different methods.} (A) The INCE matrix with row normalization. (B) The \( -\log(\mathbf{P}) \) matrix of GCA-INCE with five iterations in forward pass, both row and column normalization. (C) The \( -\log(\mathbf{P}) \) matrix of GCA-RINCE with five iterations in forward pass. (D) The \( -\log(\mathbf{P}) \) matrix of GCA-UOT with five iterations in forward pass}
 \label{fig:diagnolline}
  \end{minipage}
\end{figure}





\subsection{Hyperparameter Tuning and Sensitivity Analysis}

In our hyperparameter modifying experiments, we investigate the influence of key parameters in transport plan regularization, iteration counts, and augmentation strengths on CIFAR-10 classification performance. 


Figure~\ref{fig:epsilon_study} visualizes transport plans under varying entropic regularization (\(\epsilon\) values from 0.01 to 1) across INCE and GCA-UOT models, illustrating adjustments after five iterations using the same ResNet-18 weights. Figure~\ref{fig:sensitivity_study} examines the impact of iteration number and entropic regularization on compactness—measured by the average L2 distance to class centers—and accuracy, with 20 pre-training epochs followed by fine-tuning. Figure~\ref{fig:ablation_study} highlights the sensitivity of GCA-RINCE to the hyperparameters \(q\) and \(\lambda\), testing classification accuracy for different settings under strong augmentation conditions; this includes a comparison against INCE with large erase augmentation after substantial pre-training and evaluation epochs.



\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{files/imgs/epsilon.pdf}
  \caption{\footnotesize { \em Visualization transport plan P for different amounts of entropic regularization.} (Top) The transport plans for \(\epsilon\) from 0.01 to 1 for INCE and (Bottom) GCA-UOT after 5 iterations. To compute each plan, we took a mini-batch on CIFAR-10 with 1024 samples, and loaded the same weights of Resnet-18 for each subfigure.}
  \label{fig:epsilon_study}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{minipage}[c]{0.6\textwidth}
    \includegraphics[width=\textwidth]{files/imgs/sensitive.pdf}
  \end{minipage}%
  \hfill
  \begin{minipage}[c]{0.4\textwidth}
    \caption{\footnotesize {\em Hyperparameter sensitivity study.} The compactness and accuracy as a function of the (A) number of iterations and the (B) entropic regularization parameter. In our experiments, we use the same weights and perform 20 pre-training epochs for each point, then evaluate their performance by fine-tuning linear classifiers for 20 epochs. Here the compactness is the average L2 distance of each point to their corresponding class center on the representation space after the encoder.}
    \label{fig:sensitivity_study}
  \end{minipage}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{minipage}[c]{0.55\textwidth}
    \includegraphics[width=\textwidth]{files/imgs/ablation.pdf}
  \end{minipage}%
  \hfill
  \begin{minipage}[c]{0.4\textwidth}
    \caption{\footnotesize {\em Hyperparameter sensitivity for \(q\) and \(\lambda\) in GCA-RINCE.} Both experiments are tested on the CIFAR-10 dataset with a ResNet-18 encoder and involve strong augmentation with large erase. (Left) Given \(q = 0.98\), we change \(\lambda\) from 0 to 1. (Right) Given \(\lambda = 0.01\), we change \(q\) from 0 to 1. The red threshold line is the INCE performance with the large erase augmentation. Each point represents the CIFAR-10 classification accuracy of the ResNet-18 model pre-trained for 400 epochs and evaluated after 300 epochs.}
    \label{fig:ablation_study}
  \end{minipage}
\end{figure}



