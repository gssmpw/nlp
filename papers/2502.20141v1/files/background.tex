\section{Background}


\subsection{Contrastive learning}
\vspace{-1mm}
Contrastive learning (CL) is a representation learning methodology that uses positive and negative pairs to define similarity in the latent space. Let $\mathcal{D} = \{ \x_i \}_{i=1}^N$ denote our dataset. For each sample $\x_i$ in a batch of training data with size \(B\), %${\bf X}_b:=[\x_1,\x_2,\dots,\x_B]$, 
we create two augmented copies $\x'_i$ and $\x''_i$ independently, i.e., $\x_i'=\psi(\x_i)$ where $\psi$ is a randomly drawn augmentation function from some augmentation class $\mathcal{A}$ and likewise for $\x''_i$. The $(\x'_i,\x''_i)$ is called a positive pair of $\x_i$ while $(\x_i',\x''_j)$ is treated as a negative pair for any $j \neq i$. One of the most widely used formulations of the CL problem, InfoNCE (INCE)~\cite{chen2020simple}, seeks to maximize the negative log probability that a sample is correctly classified as
\begin{equation}
\label{eq:INCE}
    \mathcal{L}_\text{INCE}=  -  \log \bigg( \frac{e^{s_{ii}}}{e^{s_{ii}}+\sum_{i\neq j} e^{s_{ij}} } \bigg),
\end{equation}
where $s_{ij} = \varepsilon^{-1}f_{\theta}({\x_i'})^\top f_{\theta}(\x_j'')/\|f_{\theta}({\x_i'})\|\|f_{\theta}({\x_j''})\|$ is the score between augmented samples.




Building upon the principles of INCE, SimCLR \cite{chen2020simple} and MoCo \cite{he2020momentum} are two representative works that form the foundation of contrastive learning methods for visual representation tasks. Alternatively, BYOL \cite{grill2020bootstrap} and SimSiam \cite{chen2021exploring} discard the use of negative samples to avoid large batch size and instead use exponential moving average-based updates to avoid representational collapse. 
Recent contrastive methods have focused on improving the tolerance to noise in samples to enhance robustness in diverse scenarios \cite{chuang2020debiased}. 
Among them, Robust INCE (RINCE) is a robust contrastive loss function characterized by its symmetric properties and theoretical resistance to noisy labels \cite{robinson2020contrastive,chuang2022robust}.
Specifically, RINCE provides robustness to noisy views by introducing adjustable parameters $\lambda$ and $q$ ~\cite{chuang2022robust} which rebalance the cost of positive and negative views, resulting in the following loss: 
\begin{equation}\label{eq:RINCE}
    \mathcal{L}^{\lambda,q}_\text{RINCE}=\frac{1}{q}\big(-e^{ q s_{ii}} + \lambda^q(e^{ s_{ii}}+\sideset{}{_{i\neq j}}\sum e^{s_{ij}})^q\big)
\end{equation}
By optimizing the above loss functions, the encoder $f$ is trained to construct a semantically coherent representation space where positive pairs of samples are positioned nearby, while those negative pairs with divergent semantic attributes are separated~\cite{wang2020understanding}. 




\vspace{-2mm}
\subsection{Proximal Operators and Projections}

To make the connections between different CL losses clearer later, we use the notion of proximal operators. In words, the proximal operator will provide a way to find the closest point in some closed convex set. Formally, we can define the proximal operator as follows.
\begin{definition} [Proximal Operator]\label{def:prox} Let $d_\Gamma(\x, {\bf v}) = \Gamma(\x) - \Gamma({\bf v}) - \langle \nabla \Gamma({\bf v}), \x - {\bf v} \rangle$ be a Bregman divergence with a convex function \(\Gamma\). The proximal operator of $h: \mathcal{X} \to \mathbb{R} \cup \{+\infty\}$ is defined for a point ${\bf v} \in \mathcal{X}$ with a closed convex set $\mathcal{B} \subseteq \mathcal{X}$ :
\begin{equation*}~\label{eq:prox}
\text{Prox}_{h,\mathcal{B}}^{d_\Gamma}({\bf v}) = \arg\min_{{\bf x} \in \mathcal{B}} \left\{ h({\bf x}) +  d_\Gamma({\bf x}, {\bf v}) \right\}. 
\end{equation*}
\vspace{-2mm}
\end{definition}
\vspace{-3mm}

Moreover, we can define the concept of a projection as a special case of the proximal operator when we let \(h(\x)\) be an indicator function $
h_\mathcal{B}(x) = \{ 0, \text{if } x \in \mathcal{B}; 
\infty, \text{if } x \notin \mathcal{B}\}$ on constraint set \(\mathcal{B}\). See Appendix~\ref{app:prox} for more details.


\vspace{-2mm}
\subsection{Solving Optimal Transport Through Proximal Point Methods}\label{sec:otppm}
Optimal transport (OT) is widely used in characterizing the distance between two collections of samples $\{ {\bf x}_i \}_{i=1}^B $ and $\{ {\bf y}_j \}_{j=1}^B$ with associated measures \( \mu = \sum_{i=1}^{B} \delta_{{\bf x}_i} p_i~\text{and}~\nu = \sum_{j=1}^{B} \delta_{{\bf y}_j} q_j\) with Dirac delta function \(\delta_{{\bf x}}\) and \(\delta_\y\) on finite support~\cite{peyre2019computational}. Here, \( p \) and \( q \) are vertices of the \( \mathbb{R}^{B} \) simplex defined as \( \Delta_B := \{ v \in \mathbb{R}^B : v_i \geq 0, \sum_{i=1}^B v_i = 1 \}\). OT aims to learn a joint coupling matrix, or transport plan \(\PP \in \mathbbm R^{B\times B}_{+}\) that minimizes the cost of transporting mass encoded by cost matrix \(\C\in \mathbbm R^{B\times B}_{+}\), from one distribution to another.
In practice, entropy regularization is used to solve the OT objective, resulting in the following entropy-regularized OT (EOT) objective: 
\begin{equation}\label{eq:eot}
\min_{\textbf{P} \in \mathcal{B}} ~ \langle\textbf{P},\textbf{C} \rangle -\varepsilon H(\textbf{P}), \quad \text{where}~H(\PP)= -\sum_{ij} \PP_{ij} \log({ \PP}_{ij}),
\end{equation} 
where $\varepsilon$ is a user specified parameter that controls the amount of smoothing in the transport plan, and ${\bf C }({\bf x}, {\bf y}) = 1- \langle {\bf x} , {\bf y}\rangle / \| {\bf x} \| \| {\bf y} \|$ is often set to encode the cosine similarity between pairs of samples.

\vspace{-2mm}
\paragraph{The Sinkhorn Algorithm and its Interpretation as a Bregman Projection.} 
Solving Equation~\eqref{eq:eot} could be interpreted as iterative alignment problem on a Hilbert space generated from the kernel \(\K_{ij} =\exp(-\C_{i,j} / \varepsilon) \). 
This alignment problem can be solved through iterative Bregman projections onto the two constraints sets that encode the marginals along the rows and columns ~\cite{benamou2015iterative, bregman1967relaxation, peyre2019computational}:
 \begin{equation}\label{eq:Birkhoff}
    C_1^\mu \coloneqq \{\PP : \PP\mathbbm{1}_B = \mu\}, C_2^\nu \coloneqq \{\PP : \PP^\top \mathbbm{1}_B = \nu\}
\end{equation}
The first step of Bregman projection is to find the minimizer \(\PP^{(1)}=\arg\min \{\varepsilon\text{KL}(\PP\|\K): \PP\mathbbm{1}_B =\mu \}\) by the proximal operator \(\text{Prox}_{C_1^\mu}^{\text{KL}}(\K)\) with Lagrange multiplier \(f\) on the row constraint set $\mathcal{C}_1^{\mu}$, and compute its derivatives with respect to \(\PP\) with \(\uu = e^{f/\varepsilon} > 0\):
\begin{equation}
\varepsilon\log(\PP^{(1)}/ \K) - f\mathbbm{1} = 0 \Rightarrow \PP^{(1)} = \uu\K, \quad \langle \PP^{(1)}, \mathbbm{1}\rangle = \mu \Rightarrow \langle \uu\K, \mathbbm{1}\rangle = \mu, \uu = \frac{\mu}{\K \mathbbm{1}}
\end{equation}
Next, we project \(\PP^{(1)}\) onto the column constraint set \(C_2^\nu\), resulting in 
$
\PP^{(2)} \coloneqq \text{Prox}^\text{KL}_{C_2^\nu}(\PP^{(1)}) = \PP^{(1)} \text{diag}(\frac{\nu}{\PP^{(1)\top}\mathbbm{1}_B}).$ The iterative updates can be succinctly expressed as the Sinkhorn iterations:
\begin{equation}\label{eq:sinkp}
    {\bf P}^{(2t+1)}=\operatorname{diag}({\bf u}^{(t+1)})\mathbf{K}\operatorname{diag}({\bf v}^{(t)}), \quad {\bf P}^{(2t+2)}=\operatorname{diag}({\bf u}^{(t+1)})\mathbf{K}\operatorname{diag}({\bf v}^{(t+1)}),
\end{equation}
with the scaling vectors \(\uu^{(t)}\) and \(\vv^{(t)}\) updated according to:
\begin{equation}\label{eq:uvupdates}  
{\bf u}^{(t+1)} \stackrel{\text { def }}{=} \frac{{\mu}}{\mathbf{K} \vv^{(t)}}, \quad {\bf v}^{(t+1)} \stackrel{\text { def }}{=} \frac{{\nu}}{\mathbf{K}^{\mathrm{T}} {\bf u}^{(t)}}.
\end{equation}
Here, iterations converge to a stable transport plan \(\PP^{(\infty)}\)as the optimal solution of Equation~\eqref{eq:eot}, which provides the minimum cost matching between two distributions.
The convergence and dynamics of OT and its dual formulation have been studied extensively in~\cite{berman2020sinkhorn, peyre2019computational, ghosal2022convergence, an2022efficient}. Thus, these results guarantee that the iterates will converge to the optimal solution of the EOT objective, or that \(\PP^{(t)} \rightarrow \PP^{(\infty)}\) with \(t \rightarrow \infty\). 
See Appendix~\ref{app:ot} for more details on both the continuous and discrete formulations of OT.  
\vspace{-2mm}
\subsection{Wasserstein Dependency Measure}
The Wasserstein Dependency Measure (WDM) is a measure of deviation between two probability measures. We will use this later and thus provide the formal definition here~\cite{ozair2019wasserstein}.
\begin{definition}[Wasserstein Dependency Measure] \label{def:wdm} Define the WDM as the Wasserstein distance (\(W_1\)) between the joint distribution $\pi(x, y)$ and the product of marginal distributions $\mu \otimes \nu(x, y)$ of two random variables $x$ and $y$. \( W_1(\pi, \mu \otimes \nu) = \sup_{f \in \mathcal{C}(\mathcal{X} \times \mathcal{Y})} \left(\mathbb{E}_{\pi(x, y)}[f(x, y)] - \mathbb{E}_{\mu \otimes \nu(x, y)}[f(x, y)]\right)\), where \(\mathcal{C}(\mathcal{X} \times \mathcal{Y})\) denotes the set of all 1-Lipschitz functions from \(\mathcal{X} \times \mathcal{Y}\) to \(\mathbb{R}\). 
\end{definition}


\subsection{Optimal Transport and Alignment in Representation Learning}

Distribution alignment and OT have been widely used for domain adaptation \cite{lin2021making, courty2014domain, lee2019hierarchical, wang2024extraction}, and in generative modeling \cite{arjovsky2017wasserstein,tolstikhin2017wasserstein, rout2021generative, wang2024exploring}.
The connections between distribution alignment and contrastive learning, however, are still nascent.  In \cite{shi2023understanding}, the authors explore the connection between inverse OT (IOT) \cite{li2019learning, stuart2020inverse, fang2023s} and INCE. Our work builds on this connection to OT to build robust divergences (RINCE) and to build a novel unbalanced optimal transport (UOT) method (Section~\ref{subsec:relax}). Additionally, we show how our framework can be used to build flexible methods for encouraging contrast at multiple levels. We use this concept of hierarchical contrast and show that it can be used in  domain generalization settings (Section~\ref{sec:domaingen}). 
It is of note that GCA-UOT focuses on relaxing the hard constraints on the row and columns into the soft penalties,  which is different with the idea of ``unbalanced matching''  in \cite{shi2023understanding} which considers the case where the encoders may not have the same weights. 