
\section{Generalized Contrastive Alignment (GCA)}
\label{sec:gca}

In this section, we will introduce a new framework for {\em generalized contrastive alignment} and demonstrate the connections between contrastive learning and optimal transport.

\vspace{-2mm}
\subsection{Problem Formulation}
\vspace{-1mm}

Traditional contrastive learning methods focus on bringing positive examples, such as augmentations of the same sample, closer together in representation space. In contrast, our approach reframes contrastive learning as a distributional alignment problem, allowing flexible control over how pairs are matched by imposing specific constraints on the target transport plan, \(\mathbf{P}_{tgt}\).

Our objective is to learn an encoder \(f_\theta\) that minimizes the {\em transport cost} between positive samples. By defining \(\mathbf{P}_{tgt}\) with specific alignment rules, such as domain-specific or hierarchical constraints, we can influence how samples are organized in the latent space. For instance, setting \(\mathbf{P}_{tgt}\) to resemble a diagonal matrix encourages each positive to align primarily with itself or its augmentations, minimizing \(\text{div}({\bf I} || {\bf P}) \approx 0\), where \(\text{div}\) measures the deviation from an identity matrix (e.g., KL-divergence).

This flexibility allows us to encode more nuanced forms of similarity, adapting to tasks where alignment structure varies based on domain, class, or other high-level constraints. By expanding contrastive learning in this way, our method enhances separation of negatives while addressing complex relational patterns, making it suitable for a wider range of learning tasks.

\vspace{-2mm}
\paragraph{Defining the Kernel Space.}
Before formally stating our objective, we first need to define the concept of an augmentation kernel for our positive and negative examples.

\begin{definition}[Augmentation Kernel]
\label{def:gibbs}
Let $f_\theta$ denote an encoder with parameters $\theta$ and let $({\bf x}'_i,{\bf x}''_j) \sim \mathcal{A}$ be two views drawn from the family of augmentations $\mathcal{A}$. The augmentation kernel for the encoder $\theta$ is defined as ${\bf K}_\theta ( {\bf x}'_i , {\bf x}''_j ) = \exp(- \text{dist} (\widetilde{f}_\theta ({\bf x}'_i), \widetilde{f}_\theta ({\bf x}''_j))/\varepsilon)$, where $\text{dist}(\cdot)$ can be an arbitrary distance metric, and \(\widetilde{f}_\theta ({\bf x}'_i)\) is the normalized output of \(f_\theta\), and \(\varepsilon\) is the regularization parameter.
\end{definition}

\vspace{-2mm}
\paragraph{Main Objective.}

With this definition in hand, we can now formalize our objective as follows:
\begin{equation}\label{eq:mainobj}
    \min_{\theta} ~~ d_M \big( {\PP}_{\text{tgt}} || {\bf P}_\theta),~\text{with} \quad \PP_\theta= 
  \arg\min_{\PP \in \mathcal{B}}\{ h(\PP)+ d_\Gamma (\PP || \K_\theta)\},
\end{equation}

where ${\bf K}_\theta$ is the augmentation kernel defined in Definition~\eqref{def:gibbs}, \(h(x)\) is a convex function (typically an indicator function), $\mathcal{B}$ is a closed convex constraint set (i.e. Birkhoff polytope) that defines the constraints of proximal operators,  $d_{\Gamma}$ is a Bregman divergence that is used to find the nearest points \(\PP_\theta\) on the constraint set \(\mathcal{B}\) of \(\K_\theta\), \(d_M\) is a convex function (e.g., KL-divergence) that measures divergence between ${\bf P}_{\theta}$ and the target coupling plan \(\PP_{\text{tgt}}\). 

Our objective is a bi-level optimization problem which aims to learn a representation that minimizes the divergence between the transport plan \(\PP_\theta\) with the target alignment plan \({\PP}_{\text{tgt}}\) that encodes the matching constraints. When we consider a standard contrastive learning setup where we have pairs of positive examples the source and target distribution, then the target \({\PP}_{\text{tgt}}\) is the identity matrix \(\I\). However, we will show later that other alignment constraints can be considered. Moreover, when \(\mathcal{B}\) is the intersection of more constraint sets like \(C_1^\mu \cap C_2^\nu\) in Equation~\eqref{eq:Birkhoff}, a nature way to get the approximation of the nearest points \(\PP_\theta\) of \(\K_\theta\) is to run iterative projections algorithm~\cite{benamou2015iterative}, which could be extended into the intersection of several constraint sets like \(\{\cap_{i=1}^n C_i\}\), resulting in a multi-marginal problem~\cite{pass2015multi}.


 
\vspace{-2mm}
\subsection{A Proximal Point Algorithm for GCA }
\label{sec:multistep} 
\vspace{-1mm}
In practice, we can solve the alignment problem above by iteratively updating the two main components in our bi-level objective.  First, for a fixed encoder parameters $\theta$, we obtain the transport coupling $\PP_\theta$ through our corresponding proximal operator. 
Second, we measure the deviation between the transport plan $\PP_{\theta}$ with the target ${\PP}_{\text{tgt}}$ that encodes our matching constraints, which denotes the ideal alignment plan on the intersection of the constraint sets. 
We provide pseudocode for this iterative approach in Algorithm~\ref{alg:gca}, which we refer to as generalized contrastive alignment or GCA. The implementation of our methods is in 
\href{https://github.com/nerdslab/gca}{https://github.com/nerdslab/gca}.

\input{files/algo/gcageneral}


Computing the transport coupling \(\PP_\theta\)~\footnote{With a single constraint set like \(C_1^\mu\) in Equation~\eqref{eq:Birkhoff}, computing the proximal point only involves a single projection. However, if there are intersecting constraint sets like \(C_1^\mu\cap C_2^\nu\), solving for the  proximal point requires multiple projections before we approach the nearest point on their intersection.} (forward-pass) in GCA algorithms could be treated as a specific type of Dykstra's projection algorithms~\cite{bregman1967relaxation}, which computes the \textbf{iterative projection} on the intersection of affine convex sets~\cite{benamou2015iterative,peyre2015entropic}. The proofs of convergence are provided in  Appendix~\ref{app:gca_converged}.

\subsection{GCA-UOT Method}
\label{subsec:relax}

\vspace{-1mm}

We can also benefit from the rich literature on optimal transport to build different relaxations of our objective~\cite{peyre2019computational, chen2021wasserstein, taherkhani2021self, lin2021making, montesuma2023recent}. 
In particular, we choose to leverage a formulation of {\em unbalanced optimal transport} (UOT) to further relax the marginal constraints~\cite{chizat2018scaling} in our objective. 


In this case, we can add the dual form of \(d_\Gamma\) to the Equation~\eqref{eq:mainobj} and reformulate our objective as:
\begin{equation}
\min_{\theta}~ d_M ( \bf P_{\text{tgt}} \| \bf P_\theta)  + \lambda_1 h_{\mathcal{F}}(\bf P_\theta\mathbbm{1} || \mu) + \lambda_2 h_{\mathcal{G}}(\bf P_\theta^\top\mathbbm{1} || \nu) +\varepsilon H(\bf P_\theta).
\label{eq:uot}
\end{equation} 
Here \( h_{\mathcal{F}} \) and \( h_{\mathcal{G}} \) can be different divergence measures (e.g., KL divergence) that penalize deviations from the desired marginals \( \mu \) and \( \nu \), and \( \lambda_1 \) and \( \lambda_2 \) are regularization parameters that control the trade-off between the transport cost and the divergence penalties. This relaxation leads to different types of proximal operators which we outline in Appendix~\ref{app:gcauot}. The impact of the entropy regularization parameter \(\varepsilon\) on the coupling matrix is studied in Figure~\ref{fig:epsilon_study}, along with the number of iterations and corresponding sensitivity is provided in Figure~\ref{fig:sensitivity_study}.

\subsection{Modifying the Target Transport Plan to Encode Matching Constraints}\label{sec:ptgt}
Contrastive learning objectives can be cast as a minimization of the  deviations between the transport plan ${\bf P}_{\theta}$ and the identity matrix, i.e., ${\bf P}_{tgt} = {\bf I}$.
However, our \ours~formulation enables learning representations that extend beyond this one-to-one matching constraint. This flexibility allows us to incorporate additional matching constraints informed by domain-specific knowledge. For example, in domain generalization scenarios \cite{gulrajani2020search,kim2021selfreg}, where each batch contains samples from multiple domains, the target alignment plan can be structured as:
\[
{\bf P}_{\text{tgt}}[i,j] = {\bf I}[i,j] + \alpha \cdot \mathbb{I}(D_i = D_j, \, i \neq j) + \beta \cdot \mathbb{I}(D_i \neq D_j, \, i \neq j),
\]
Where \(\mathbb{I}(\cdot)\) is the indicator function, which equals 1 if the condition inside is true and 0 otherwise. \(D_i\) represents the domain of sample \(i\), where $\alpha \geq 0$ and $\beta \geq 0$. In this case, we can improve the representation by building the block constraints which encode either class information (in supervised setting) or domain information (in across domain generalization, visualized in Figure~\ref{fig:Ptgt}).



\subsection{Computational Complexity}
The forward-pass only involves the scaling operations in Equation~\eqref{eq:uvupdates} and doesn't affect the complexity of the backward-pass. Therefore, GCA methods can be thought of as a form of batch normalization operations with adaptive scaling. An analysis of the complexity is provided along with experiments in  Appendix~\ref{app:gca_converged}. Our results show that GCA iterations only slightly increase the computational complexity when compared with their single step equivalent (GCA-INCE vs. INCE). However, we found that GCA-UOT is faster than INCE due to the improved symmetry and smoothness of the loss. Moreover, we record the floating point operations per second (Flops) of running GCA methods. We find that GCA-INCE (6.65 MFlops) has $5\%$ more Flops than INCE (6.31 MFlops), while GCA-UOT saves $30\%$ Flops (4.54 MFlops).  These results show that our GCA-UOT method is not only superior in terms of accuracy but also in speed.


\section{Building Connections to Different CL Objectives}
\label{sec:connectionsCL}
\begin{wraptable}{ht}{0.49\textwidth}
  \begin{center}
    \vspace{-7mm}
    \caption{\footnotesize {\em Comparison of different contrastive alignment objectives.} Here we have $C_1^\mu$ and $C_2^\nu$ as constraint sets (denoted as \(\mathcal{B}\)) defined in Equation~\eqref{eq:Birkhoff} with their corresponding indicator function. "Iter" refers to iterative methods.}
    \vspace{-2mm}
    \resizebox{0.49\textwidth}{!}{
      \begin{tabular}{l|c|c|c|c} 
        \footnotesize{$\textbf{Methods}$} & \footnotesize{$d_M$} &  \footnotesize{$d_\Gamma$}    & \(\mathcal{B}\) & Iter\\
        \hline
        \footnotesize{INCE} & KL & KL  &\(C_1^\mu\) &    \\
        \footnotesize{GCA-INCE} & KL & KL & \(C_1^\mu\cap C_2^\nu\) & $\checkmark$ \\
        \footnotesize{RINCE (q=1)} & W1 & KL &\(C_1^\mu\) &   \\
        \footnotesize{GCA-RINCE (q=1)} & W1 & KL & \(C_1^\mu\cap C_2^\nu\) & $\checkmark$  \\
        \footnotesize{BYOL} & KL & L2 & \(R^{B\times B}\) &   \\
      \end{tabular}
    }
    \label{tab:diff}
    \vspace{-7mm}
  \end{center}
\end{wraptable}

In this section, we show how the modification of the different parts of our main objective (\(d_\Gamma, d_M, \mathcal{B}, \K_\theta\)) in Equation~\eqref{eq:mainobj} can be connected to different contrastive losses. See  Table~\ref{tab:diff} for a summary of how different losses can be mapped back to our formulation. 

\vspace{-2mm}
\subsection{Connection to INCE}
\vspace{-1mm}


An interesting connection that we can make between GCA main objective and contrastive learning is that we can interpret INCE as a \textbf{single step} in a iterative GCA objective~\cite{shi2023understanding}. 
This connection can be further summarized through the following theorem.
\begin{theorem}[INCE Equivalence]
Let ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs} with cosine similarity, $d_\Gamma$ and $d_M$ equal to KL-divergence, and constraint set as \(C_1^\mu\) in Equation~\eqref{eq:Birkhoff}.
The INCE objective in Equation~\eqref{eq:INCE} can be re-expressed as a GCA problem in Equation~\eqref{eq:mainobj} as follows:
\begin{equation}\label{eq:kleqince}
\min_{\theta}\text{KL} \big( {\bf I}  ||\text{Prox}_{C_1^\mu}^{KL}(\K_\theta)).\end{equation}
\label{thm:cl_eq_ot}
\end{theorem}
The proof is contained in Appendix \ref{app:proof_cleqot}. Theorem~\eqref{thm:cl_eq_ot} shows that the INCE loss can be viewed as solving the matching problems in Equation~\eqref{eq:eot} with  row normalization constraints \(C_1^\mu\). 
This connection between GCA and INCE allows us to derive the iterative algorithm for GCA-INCE by running Bregman projection iteratively on both row and column normalization sets.



\vspace{-2mm}
\subsection{Connection to RINCE}
We introduce the following result to build the connection between our framework and RINCE~\cite{chuang2022robust}.
\begin{theorem}[RINCE Equivalence]\label{thm:rince}
Let ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs}. Set target plan \(\PP_{\text tgt}=\bf I\), $d_\Gamma$ equal to the KL-divergence, \(d_M(\I\|\PP)=-\frac{1}{q}(\frac{\operatorname{diag}(\PP_\theta)}{\uu})^q+\left(\frac{\lambda \I}{\uu}\right)^q\) with \(\lambda\), \(q\), and \(\uu=\operatorname{diag}\left(\frac{\mu}{\PP^{(0)}\mathbbm{1}}\right)\), and constraint set \(C_1^\mu\) defined in Equation~\eqref{eq:Birkhoff}. The RINCE objective in Equation~\eqref{eq:RINCE} can be re-expressed as a GCA problem as follows:
\begin{equation}\label{eq:hsrince}
     \min_\theta d_M(\I\|\PP_\theta),~~\text{with}~~\PP_\theta = \text{Prox}_{C_1^\mu}^{\text{KL}}(\K_\theta),
\end{equation} 
\end{theorem}
 
\vspace{-4mm}
The proof is provided in Appendix~\ref{app:rinceeqprox}. As we can see, RINCE introduces adjustable parameters \( q \) and \( \lambda \), with \( \lambda \) controlling the weight of negative samples, while \( q \in (0,1]\) serves to switch between KL divergence and Wasserstein discrepancy. When $q=1$, we have the following theorem:
\begin{theorem}[W1 Equivalence]
\label{co:rince}

Let ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs} with cosine similarity. Set target plan \(\PP_{\text tgt}=\bf I\), \(d_\Gamma\) equal to the KL-divergence, $d_M$ equal to the 1-Wasserstein distance $(W_1)$ in Definition~\eqref{def:wdm}, and the constraint set as \(C_1^\mu\) defined in Equation~\eqref{eq:Birkhoff}. The RINCE object in Equation~\eqref{eq:RINCE} with $q=1$ can be re-expressed as a GCA problem as follows:
\begin{equation}\label{eq:w1eqrince}
\min_{\theta} W_1 \big( {\PP_{tgt}} ||\text{Prox}_{C_1^\mu}^{KL}(\K_\theta)).\end{equation}
\end{theorem}
\vspace{-3mm}
See Appendix~\ref{app:wdm} for the proof. 
This connection to RINCE suggests an extended iterative formulation to calculate the coupling plan as the projection point \(\PP^{(\infty)}=\text{Prox}_{C_1^\mu\cap C_2^\nu}^{\text{KL}}(\K_\theta)\) of \(\K_\theta\) on the constraint set \(C_1^\mu\cap C_2^\nu\). In this case, we can write an iterative algorithm for robust alignment called GCA-RINCE as follows:
\begin{equation}\label{eq:gcarince}
    L_{\text{GCA-RINCE}}^{\lambda,q} =\min_\theta -q^{-1}( \operatorname{diag}(\PP_\theta^{(2t-1)} )/{\uu^{(t)}})^q+ q^{-1}( \lambda \PP_{tgt}/\uu^{(t)})^q, 
\end{equation}
where $\lambda$ and $q$ are hyperparameters, $\PP^{(1)} \coloneqq \operatorname{diag}(\uu^{(1)})\K_\theta\operatorname{diag}(\vv^{(0)})$, and $t$ is the number of iterations.

\vspace{-2mm}
\subsection{Connection to BYOL}
\vspace{-1mm}
Our framework also allows us to make connections to BYOL \cite{grill2020bootstrap}.
BYOL learns by encouraging similarity between positive image pairs, without explicitly conditioning on negative examples. To build this connection, recall that BYOL has the online network parameterized by \(\theta\) and target network parameterized by \(\xi\), where \(\z'_\theta=\widetilde{f}_{\theta}(\x')\) and \(\z''_\xi=\widetilde{f}_{\xi}(\x'')\) are the normalized outputs of the online and target networks, respectively. A simplified version of the BYOL loss can be written as: $\label{eq:byol}L_\text{BYOL}=  \| \widetilde{q}_{\theta}(\z'_{\theta}) - \z''_{\xi} \|_2^2,
$
where \(\widetilde{q}_{\theta} (\z'_\theta)\) is the normalized output after online network and \(q_\theta\) is the predictor.\footnote{In practice, BYOL also switches the order of views to symmetrize the loss. For ease of discussion, we consider just one pair of views but the same could be argued for the full symmetric version.} In this case, we can provide the following connection between \ours and BYOL as follows.

\begin{theorem}[BYOL Equivalence]\label{thm:byol}
Let \(\bS_{\theta}(\x'_i, \x''_j) = \exp ( - \| \widetilde{q}_{\theta}({\bf z}_i') -  {\bf z}_j'' \|) \) denote the augmentation kernel. Set the target plan \(\PP_{\text tgt}=\bf I\), \(d_\Gamma\) equal to the L2-distance, $d_M$ equal to the KL-divergence, and constraint set as \(R^{B\times B}\). The BYOL objective can be re-expressed as a GCA problem as follows:
\begin{equation}\label{eq:eqbyol}
\min_\theta \text{KL} \big(  {\bf I} || \bS_\theta),~~\text{with}~~\bS_\theta=\text{Prox}_{R^{B\times B}}^{\|\cdot\|}(\bS_\theta).\end{equation}
\end{theorem}
See the proof in Appendix~\ref{app:byol}. 

\section{Theoretical Analysis}\label{sec:theoyanaylsis}

In this section, we aim to show how the GCA-methods can improve alignment and uniformity in the latent space~\cite{wang2020understanding}. 
Here, {\em alignment} means that the features of the positive samples are as close as possible, while {\em uniformity} means that the features of negative samples are uniformly distributed on latent space (see  Appendix~\ref{app:alignment} for formal definitions). These quantities have been studied in a number of related works~\cite{wang2020understanding, pu2022alignment}, where one can show that improved alignment and uniformity can lead to different  benefits in representation learning.


\subsection{Improved alignment with GCA}
\label{subsec:betterERM}
\vspace{-1mm}
Contrastive learning minimizes the deviation between the target alignment plan with the transport plan in Definition~\ref{def:gibbs} through empirical risk minimization (ERM). Therefore, a tighter bound on the empirical risk corresponds to a smaller difference between the ideal alignment with the coupling matrix. We show that this in turn leads to better alignment of the positive views.








\vspace{-2mm}
\paragraph{Analysis of INCE vs GCA-INCE.}GCA-INCE ensures that the final transport plan \(\PP^{(\infty)}\) is closer to the ideal identity matrix compared to the INCE, as we show in the following theorem.

\begin{theorem}[Improved Alignment with INCE]
\label{thm:lowest}

Let ${\bf K}_\theta$ denote the augmentation kernel as in Definition~\eqref{def:gibbs}. Set $d_M$ and $d_\Gamma$  to the KL-divergence, and  \({\PP}_{\text{tgt}}=\bf I\). The GCA-INCE loss with converged plan \(\PP_\theta^{(\infty)}\) is lower than the GCA-INCE loss with $\PP_\theta^{(t)}$ in Equation~\eqref{eq:sinkp} for all $t$. 
\end{theorem}
The full proof is provided in Appendix~\ref{app:klfg}. The above theorem tells us that solving Equation~\eqref{eq:mainobj} with iterative projection will converge to a transport plans $\PP_\theta^{(\infty)}$ with lower KL divergence than the one-step solution provided by INCE. We can establish the convergence of the \(\PP^{(t)} \rightarrow \PP^{(\infty)}\), based on the convergence of Bregman projection.

\vspace{-2mm}
\paragraph{Analysis of RINCE vs GCA-RINCE.}~

GCA also benefits from other Bregman divergences, like the WDM in RINCE, which provides robustness against distribution shift compared to the KL-divergence in INCE. GCA-RINCE provides a lower bound on the RINCE loss in Equation~\eqref{eq:RINCE}, which allows us to develop a tighter bound with \(\PP^{(\infty)}\) obtained by several proximal steps with GCA.


\begin{theorem}[Improved Alignment with RINCE]\label{thm:gcarince}
GCA-RINCE loss with $\PP_\theta^{(t)}$ in Equation~\eqref{eq:gcarince}  is lower than the loss in the Theorem~\eqref{thm:rince} as
 \( L_{\text{GCA-RINCE}}^{\lambda,q=1}(\PP_\theta^{(t)}) \leq  L_{\text{RINCE}}^{\lambda,q=1}(\PP_\theta^{(1)})\).
\end{theorem}

See Appendix~\ref{app:klfg} for the full proof and an analysis of GCA methods  for different choices of \(d_M\).


\vspace{-2mm}
\subsection{Improved Uniformity of Representations Through GCA}
\vspace{-1mm}
\label{subsec:align_uni}


The improved alignment of GCA-methods comes from maximization of the uniformity under the constraint of intersection \(C_1^\mu \cap C_2^\nu\) in Equation~\eqref{eq:Birkhoff}, rather than the constraint set \(C_1^\mu\) in INCE (see Table~\ref{tab:diff}). Finding the projection of \(\K_\theta\) on set of \(C_1^\mu \cap C_2^\nu\) through proximal steps is equivalent to solving the dual problem of EOT, which can be summarized through the following theorem.
\begin{theorem}[Improved Uniformity]\label{thm:uniformity}
Given the constraint sets in Equation~\eqref{eq:Birkhoff}, the optimal transport coupling upon convergence of Equation~\eqref{eq:sinkp}, denoted as \(\PP^{(\infty)}\), achieves a higher uniformity loss compared to the single-step transport plan \(\PP^{(1)}\) obtained by INCE.  
\vspace{-1mm}
\end{theorem}
The proof is provided in the Appendix~\ref{app:uniformity}. Through loss propagation, we show that the alignment plan offered by \(\PP^{(\infty)}\) will guide the subsequent iterations towards more uniform representations.

\vspace{-2mm}
\subsection{Impacts of GCA on a downstream classification task}
\vspace{-1mm}
We take this one step further and examine the impact of GCA on a downstream classification task.
For a classification task, using a labeled dataset $\mathcal{D} = \{(\bar{\x}_i, \y_i)\} \in \bar{\mathcal{X}} \times \mathcal{Y}$ where $\mathcal{Y} = [1,\dots,M]$ with $M$ classes, we consider a fixed, pre-trained encoder $f_\theta \in \mathcal{F}: \mathcal{X} \to \mathcal{S}$. Assume that positive and negative views of $n$ original samples $(\bar{\x}_i)_{i \in [1..n]} \subset \bar{\mathcal{X}}$ are sampled from the data distribution $p(\bar{\x})$. 


In this case, the uniformity loss is equivalent to optimizing the downstream supervised classification tasks with cross-entropy (CE) loss when  the following two assumptions are satisfied~\cite{dufumier2023integrating}.
\begin{assumption} [Expressivity of the Encoder]\label{asm:express_encoder}
Let us define $\mathcal{H}_{\bar{\mathcal{X}}}$ is the RKHS associated with the kernel $\K_{\bar{\mathcal{X}}}$ defined on \(\bar{\mathcal{X}}\), and \( (\mathcal{H}_{f_\theta}, \K_\theta) \) defined on $\mathcal{X}$ with augmentation kernel $\K_\theta = \langle f_\theta(\cdot), f_\theta(\cdot) \rangle_{\mathbb{R}^d}$ in Definition~\ref{def:gibbs}. And we assume that $\forall g \in \mathcal{H}_{f_\theta}, \ \mathbb{E}_{\mathcal{A}(x|\cdot)} g(x) \in \mathcal{H}_{\bar{\mathcal{X}}}$.
\end{assumption}



\begin{assumption} [Small Intra-Class Variance]\label{asm:small_intra_variance}
For \(y \neq y'\), the intra-class variance \(\delta_i,\delta_j\) are negligible compared to the distance among different class centroids, \(\mu_y, \mu_{y'}\) as \(\|\mu_y - \mu_{y'}\| \gg \| \delta_i-\delta_j\| \).
\end{assumption}
\begin{claim}\label{cl:ce_eq_uniform}
If Assumption~\ref{asm:express_encoder} and Assumption~\ref{asm:small_intra_variance} hold, then maximizing the uniformity is equivalent to minimizing the downstream CE loss.
\end{claim}
The proof is provided in Appendix~\ref{app:uniformity}. Optimizing the self-supervised loss under ideal conditions improves downstream CE tasks and helps to explain why maximizing uniformity aids classification.


{\bf Remark.}. Maximizing uniformity can enhance downstream classification but risks ``feature suppression'' by encouraging shortcut features that harm generalization~\cite{robinson2021can}. 
In GCA-UOT, adding penalties modifies the transport plan from that of a pure uniformity loss, helping to avoid feature suppression. We find empirical evidence that UOT provides a more robust transport plan which appears to circumvent some of these shortcut features from being learned (Figure~\ref{fig:diagnolline} in Appendix~\ref{app:gcauot-feature}).