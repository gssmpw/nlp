

%\vspace{-2mm}
\section{Conclusion}
%\vspace{-2mm}
In this work, we introduced \emph{generalized contrastive alignment} (GCA), a flexible framework that redefines contrastive learning as a distributional alignment problem using optimal transport to control alignment.
By allowing targeted control over alignment objectives, GCA demonstrates strong performance across both standard and challenging settings, such as noisy views and domain generalization tasks. This work opens up broader possibilities for learning robust representations in real-world scenarios, where data is often diverse, noisy, or comes from multiple domains.

Future work includes applications of GCA to graphs  and time series data, as well as multi-modal settings where our approach can integrate various forms of similarity. As alignment strategies become integral to contrastive learning, GCA offers a promising foundation for more adaptive and expressive self-supervised models.

