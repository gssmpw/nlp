%\vspace{-4mm}
\section{Introduction}

In machine learning, the availability of vast amounts of unlabeled data has created an opportunity to learn meaningful representations without relying on costly labeled datasets \cite{jaiswal2020survey,shurrab2022self,jing2020self}. Self-supervised learning has emerged as a powerful solution to this problem, allowing models to leverage the inherent structure in data to build useful representations. Among self-supervised methods, contrastive learning (CL) is widely adopted for its ability to create robust representations by distinguishing between similar (positive) and dissimilar (negative) data pairs. With success in fields like image and language processing \cite{chen2020simple,radford2021learning}, contrastive learning now also shows promise in domains where cross-modal, noisy, or structurally complex data make labeling especially challenging \cite{liu2021drop,vishnubhotla2024towards,chen2023instance}.


Traditional contrastive learning methods primarily aim to bring positive pairs---often augmentations of the same sample---closer together in representation space. While effective, this approach often struggles with real-world challenges such as noise in views, variations in data quality, or shifts introduced by complex transformations, where positive pairs may not perfectly align. Additionally, in tasks requiring domain generalization, aligning representations across diverse domains (e.g., variations in style or sensor type) is critical but difficult to achieve with standard contrastive learning, which typically lacks mechanisms for incorporating domain-specific relationships. These limitations highlight the need for a more flexible approach that can adapt alignment strategies based on the data structure, allowing for finer control over similarity and dissimilarity among samples. 


To address this challenge, we introduce a novel \emph{generalized contrastive alignment} (GCA) framework, which reinterprets contrastive learning as a distributional alignment problem. Our method allows flexible control over the alignment of samples by defining a target transport plan, \(\mathbf{P}_{tgt}\), that serves as a customizable alignment guide. For example, setting \(\mathbf{P}_{tgt}\) to resemble a diagonal matrix encourages each positive to align primarily with itself or its augmentations, thereby reducing the effect of noise between views. Alternatively, we can incorporate more complex constraints, such as weighting alignments based on view quality or enforcing partial alignment structures where noise or data heterogeneity is prevalent. This flexibility enables GCA to adapt effectively to a wide range of tasks, from simple twin view alignments to scenarios with noisy or variably aligned data.

Our approach also bridges connections between GCA and established methods, such as InfoNCE (INCE) \cite{oord2018representation}, Robust InfoNCE (RINCE) \cite{chuang2022robust}, and BYOL \cite{grill2020bootstrap}, demonstrating that these can be viewed as iterative alignment objectives with Bregman projections \cite{cai2022developments,grathwohl2019your}. This perspective allows us to systematically analyze and improve uniformity within the latent space, a property that enhances representation quality and ultimately boosts downstream classification performance.

We validate our method through extensive experiments on both image classification and noisy data tasks, demonstrating that GCAâ€™s unbalanced OT (UOT) formulations improve classification performance by relaxing our constraints on alignment. Our results show that \ours~offers a robust and versatile framework for contrastive learning, providing flexibility and performance gains over existing methods and presenting a promising approach to addressing different sources of variability in self-supervised learning.


The contributions of this work include:
\begin{itemize}
    \item A new framework called \emph{generalized contrastive alignment} (GCA), which reinterprets standard contrastive learning as a distributional alignment problem, using optimal transport to provide flexible control over alignment objectives. This approach allows us to derive a novel class of contrastive losses and algorithms that adapt effectively to varied data structures and build  customizable transport plans.

    \item We present GCA-UOT, a contrastive learning method that achieves strong performance on standard augmentation regimes and excels in scenarios with more extreme augmentations or data corrupted by transformations. GCA-UOT leverages unbalanced transport to adaptively weight positive alignments, enhancing robustness against view noise and cross-domain variations.

    \item We provide theoretical guarantees for the convergence of our GCA-based methods and show that our alignment objectives improve representation quality by enhancing the uniformity of negatives and strengthening alignment within positive pairs. This leads to more discriminative and resilient representations, even in challenging data conditions.

    \item Empirically, we demonstrate the effectiveness of \ours~in both image classification and domain generalization tasks. Through flexible, unbalanced OT-based losses, \ours~achieves superior classification performance and adapts alignment to include domain-specific information where relevant, without compromising classification accuracy in domain generalization.
\end{itemize}


