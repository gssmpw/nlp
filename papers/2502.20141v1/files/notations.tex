


\section{Background and Notation}%~\hyperref[app:outline]{*}

\subsection{Notation}
\label{app:notations}
\paragraph{Datasets and contrastive pairs:} Let $\mathbf{x}$ denotes a vector and $\mathbf{X}$ denotes a matrix, with right subscript \({\bf X}_b\) denote the batch of the input samples, ${\bf X}_b:=[\x_1,\x_2,\dots,\x_B]$, here \(B\) is equal to batch size. For each sample $\x_i$ in the input batch matrix $\mathbf{X}_b$, $\x'_i$ means augmented view 1 of $\x'_i$, $\x''_i$ means augmented view 2 of $\x_i$, the positive pairs in input data denoted as ($\x_i$, $\x''_i$), negative pairs in input data denoted as ($\x'_i$, $\x''_j$), $i\neq j$. Give a weights (\(\theta\)) parametrized representation function (artificial neural network) $f_\theta$ with adjustable adjustable temperature $\varepsilon$, which project the the positive pairs in latent space denoted as  \(s^+=\langle \varepsilon^{-1} \widetilde{f}_\theta(\x'_i),\widetilde{f}_\theta(\x''_i) \rangle\), and negative pairs in latent space denoted as \(s^-=\langle \varepsilon^{-1} \widetilde{f}_\theta(\x'_i),\widetilde{f}_\theta(\x''_j) \rangle, i\neq j\). Here, \(\langle \cdot,\cdot \rangle\) is the inner product, which means $\langle \widetilde{f}_\theta(\x'_i),\widetilde{f}_\theta(\x''_i) \rangle = f_{\theta}({\x_i'})^\top f_{\theta}(\x_j'')/\|f_{\theta}({\x_i'})\|\|f_{\theta}({\x_j''})\|$ is the normalized form.
     
\paragraph{Continuous settings for optimal transport}
     \(\mathcal{X}\) and \(\mathcal{Y}\) are topological spaces, \(\mathcal{X} \times \mathcal{Y}\) is the product space, or Torus. \(C(\mathcal{X})\) is the compact topological space which contains all of continuous functions on \(\mathcal{X}\) endowed with the sup-norm. On Torus \(\mathcal{X}\) and \(\mathcal{Y}\) we define $M$ as a compact n-dimensional manifold in product space \(\mathcal{X}\times\mathcal{Y}\) ($\mathcal{X}=\mathcal{Y}:= \mathbb{R}^n / \mathbb{Z}^n$) endowed with a cost function \(c(x, y) := d_{M}(x, y)^2/2\) (Euclidean dsitance function) on $\mathbb{R}^n$. Transport plan $\pi(x,y):\mathcal{X} \times \mathcal{Y}\rightarrow \mathbb{R}$ is an element in  \(\mathcal{P}(\mathcal{X}\times\mathcal{Y})\). \(\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) means the collections of the joint distributions of the two marginal distributions $\mu\in\mathcal{P}(\mathcal{X})$ and $\nu\in\mathcal{P}(\mathcal{Y})$.  $\mathcal{P}(\mathcal{X})$ the space of all (Borel) probability measures on \(\mathcal{X}\), $\mathcal{P}(\mathcal{Y})$ means the same to $\mathcal{Y}$.  To find a joint distribution (or plan) \( \pi(x,y) \) in collections \(U(\mu,\nu)\) with marginals \( \mu \) and \( \nu \) in the product space \(\mathcal{X} \times \mathcal{Y}\), we can formulate as:
\begin{equation*}
 \label{ot}
\min_{\pi\in U(\mu, \nu)}\sum_{x\in\mathcal{X},y\in\mathcal{Y}}\pi(x, y)c(x,y) \quad \text{s.t.} ~\sum_{y\in\mathcal{Y}}\pi(x,y) =\mu(x),~\sum_{x\in\mathcal{X}}\pi(x,y) =\nu(y)
\end{equation*} 

\paragraph{Discrete settings of optimal transport}: \( \mu \) and \( \nu \) can be discrete probability measures whose supports are finite sets \( X := \{ f_\theta(\x'_i) \}_{i=1}^{N}, Y := \{ f_\theta(\x''_i)\}_{i=1}^{B} \). And we define \( \mu = \sum_{i=1}^{B} \delta_{f_\theta(\x'_i)} p_i, \nu = \sum_{i=1}^{B} \delta_{f_\theta(\x''_i)} q_i, \) with vectors \( p \) and \( q \) in a simplex \( \Delta_{B} \) in \( \mathbb{R}^{B} \) defined by \( \Delta_B := \left\{ v \in \mathbb{R}^B : v_i \geq 0, \sum_{i=1}^B v_i = 1 \right\}\), which we identify with \( \mathcal{P}(\{1, \ldots, B\}) \).
      \(\C\) is a \(B\times B\) cost matrix calculated by \(c(x,y)\), whose sampled from finite sets \(X\) and \(Y\) defined previously, and N is the batch size. \(\uu\) and \(\vv\) are \(B\times 1\) scale factors matrix, \(\uu^{(t)}\) and \(\vv^{(t)}\) mean the scale factor matrix after t iterations of sinkhorn algorithms. \(\PP\) is a \(B\times B\) joint distribution matrix of \(\mu\) and \(\nu\), which represents the transport plan \(\pi(x,y)\) that corresponds to minimize the cost. \(\PP^{(2t-1)}\) means we use t iterations \(\uu^{(t)}\) and \(\vv^{(t-1)}\) to calculate \(\PP^{(t)}\). When \(t=1\) means we use \(\uu^{(1)}\) and \(\vv^{(0)}\) to calculate \(\PP^{(1)}\), which is called half-step OT or one step Bregman projection. When \(t=2\) means we use \(\uu^{(1)}\) and \(\vv^{(1)}\) to calculate \(\PP^{(2)}\), which is called half-step OT or one step Bregman projection.
