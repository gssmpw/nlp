@inproceedings{bhardwaj-etal-2024-language,
    title = "Language Models are {H}omer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
    author = "Bhardwaj, Rishabh  and
      Do, Duc Anh  and
      Poria, Soujanya",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.762/",
    doi = "10.18653/v1/2024.acl-long.762",
    pages = "14138--14149",
    abstract = "We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6{\%} to 5.1{\%} and from 9.2{\%} to 1.5{\%} in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model`s performance on the task. We release the source codes at: https://github.com/declare-lab/resta."
}

@misc{bowen2024taskvectorsselectivetask,
      title={Beyond Task Vectors: Selective Task Arithmetic Based on Importance Metrics}, 
      author={Tian Bowen and Lai Songning and Wu Jiemin and Shuai Zhihao and Ge Shiming and Yue Yutao},
      year={2024},
      eprint={2411.16139},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.16139}, 
}

@article{choi2024,
  title = {Revisiting Weight Averaging for Model Merging},
  author = {Choi, Jiho and Kim, Donggyun and Lee, Chanhyuk and Hong, Seunghoon},
  journal = {arXiv preprint arXiv:2412.12153},
  year = {2024}
}

@misc{gargiulo2025tasksingularvectorsreducing,
      title={Task Singular Vectors: Reducing Task Interference in Model Merging}, 
      author={Antonio Andrea Gargiulo and Donato Crisostomi and Maria Sofia Bucarelli and Simone Scardapane and Fabrizio Silvestri and Emanuele Rodol√†},
      year={2025},
      eprint={2412.00081},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.00081}, 
}

@inproceedings{guodong24neurips,
  title={Parameter Competition Balancing for Model Merging},
    author = {Guodong Du and
    Junlin Lee and Jing Li  and Runhua Jiang and Yifei Guo and Shuyang Yu and Hanting Liu and Sim Kuan Goh and Ho-Kin Tang and Daojing He and Min Zhang},
  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{hazra-etal-2024-safety,
    title = "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations",
    author = "Hazra, Rima  and
      Layek, Sayan  and
      Banerjee, Somnath  and
      Poria, Soujanya",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1212/",
    doi = "10.18653/v1/2024.emnlp-main.1212",
    pages = "21759--21776",
    abstract = "Ensuring the safe alignment of large language models (LLMs) with human values is critical as they become integral to applications like translation and question answering. Current alignment methods struggle with dynamic user intentions and complex objectives, making models vulnerable to generating harmful content. We propose Safety Arithmetic, a training-free framework enhancing LLM safety across different scenarios: Base models, Supervised fine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm Direction Removal to avoid harmful content and Safety Alignment to promote safe responses. Additionally, we present NoIntentEdit, a dataset highlighting edit instances that could compromise model safety if used unintentionally. Our experiments show that Safety Arithmetic significantly improves safety measures, reduces over-safety, and maintains model utility, outperforming existing methods in ensuring safe content generation."
}

@InProceedings{ilharco2023,
  title = {Editing Models with Task Arithmetic},
  author = {Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle = {Proceedings of the 11th International Conference on Learning Representations (ICLR)},
  year = {2023}
}

@misc{lai2025mediatormemoryefficientllmmerging,
      title={Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing}, 
      author={Kunfeng Lai and Zhenheng Tang and Xinglin Pan and Peijie Dong and Xiang Liu and Haolan Chen and Li Shen and Bo Li and Xiaowen Chu},
      year={2025},
      eprint={2502.04411},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.04411}, 
}

@InProceedings{lu2024,
  title = {Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging},
  author = {Lu, Zhenyi and Fan, Chenghao and Wei, Wei and Qu, Xiaoye and Chen, Dangyang and Cheng, Yu},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2024}
}

@InProceedings{matena2022,
  title = {Merging Models with Fisher-Weighted Averaging},
  author = {Matena, Michael S. and Raffel, Colin A.},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2022}
}

@InProceedings{wortsman2022,
  title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages = {23965--23998},
  year = {2022},
  publisher = {PMLR}
}

@InProceedings{yadav2023,
  title = {TIES-Merging: Resolving Interference When Merging Models},
  author = {Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin and Bansal, Mohit},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2023}
}

@InProceedings{yu2024,
  title = {Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch},
  author = {Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year = {2024}
}

@inproceedings{zhao-etal-2024-defending-large,
    title = "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing",
    author = "Zhao, Wei  and
      Li, Zhe  and
      Li, Yige  and
      Zhang, Ye  and
      Sun, Jun",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.293/",
    doi = "10.18653/v1/2024.findings-emnlp.293",
    pages = "5094--5109",
    abstract = "Large language models (LLMs) are increasingly being adopted in a wide range of real-world applications. Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning. While existing defense methods focus on either detecting harmful prompts or reducing the likelihood of harmful responses through various means, defending LLMs against jailbreak attacks based on the inner mechanisms of LLMs remains largely unexplored. In this work, we investigate how LLMs respond to harmful prompts and propose a novel defense method termed \textbf{L}ayer-specific \textbf{Ed}iting (LED) to enhance the resilience of LLMs against jailbreak attacks. Through LED, we reveal that several critical \textit{safety layers} exist among the early layers of LLMs. We then show that realigning these safety layers (and some selected additional layers) with the decoded safe response from identified \textit{toxic layers} can significantly improve the alignment of LLMs against jailbreak attacks. Extensive experiments across various LLMs (e.g., Llama2, Mistral) show the effectiveness of LED, which effectively defends against jailbreak attacks while maintaining performance on benign prompts. Our code is available at \url{https://github.com/ledllm/ledllm}."
}

@inproceedings{zhou-etal-2024-metagpt,
    title = "{M}eta{GPT}: Merging Large Language Models Using Model Exclusive Task Arithmetic",
    author = "Zhou, Yuyan  and
      Song, Liang  and
      Wang, Bingning  and
      Chen, Weipeng",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.102/",
    doi = "10.18653/v1/2024.emnlp-main.102",
    pages = "1711--1724",
    abstract = "The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks. Task arithmetic has emerged as a cost-effective approach for MTL. It enables performance enhancement across multiple tasks by adding their corresponding task vectors to a pre-trained model. However, the current lack of a method that can simultaneously achieve optimal performance, computational efficiency, and data privacy limits their application to LLMs. In this paper, we propose \textbf{M}odel \textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging \textbf{GPT}-scale models (MetaGPT) which formalizes the objective of model merging into a multi-task learning framework, aiming to minimize the average loss difference between the merged model and each individual task model. Since data privacy limits the use of multi-task training data, we leverage LLMs' local linearity and task vectors' orthogonality to separate the data term and scaling coefficients term and derive a model-exclusive task arithmetic method. Our proposed MetaGPT is data-agnostic and bypasses the heavy search process, making it cost-effective and easy to implement for LLMs. Extensive experiments demonstrate that MetaGPT leads to improvement of task arithmetic and achieves state-of-the-art performance on multiple tasks."
}

