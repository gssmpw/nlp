\section{Related Work}
\label{sec: Related Work}
Combining model capabilities without additional training has attracted growing attention. Model merging fuses weights of separately fine-tuned models for multi-task learning~\cite{choi2024}, and simple averaging can improve accuracy and robustness~\cite{wortsman2022}. TIES~\cite{yadav2023} resets negligible changes to address sign conflicts, reducing performance drops; Delta-sparsification (DARE)~\cite{yu2024} discards up to 99\% of fine-tuning deltas to merge multiple homologous models. Most research aims to minimize utility loss of merged LLMs~\cite{matena2022,jin2023dataless,zhou-etal-2024-metagpt,guodong24neurips,lu2024,dai2025leveraging,lai2025mediatormemoryefficientllmmerging}, while \citet{yang2024adamerging,yang2024representation,bowen2024taskvectorsselectivetask,gargiulo2025tasksingularvectorsreducing} explore merging computer vision models using key parts of task vectors.

An alternative line of research, \emph{task arithmetic}~(\textit{TA}), views tasks as weight update vectors composed via vector operations. \citet{ilharco2023} define a \emph{task vector} as the difference between a fine-tuned model and its base, enabling multiple tasks to be learned simultaneously and new tasks to be inferred without retraining. Negating a task vector selectively unlearns a specific task with minimal impact on others, implying that model weights shift independently per task. TA has been considered in fine-tuning~\citep{zhang2023composing,choi2024} and alignment~\citep{zhao-etal-2024-defending-large,li2025safety,hazra-etal-2024-safety} contexts.

In this paper, we focus on TA for both task learning and forgetting. Existing methods generally merge or edit entire models without distinguishing which layers encode task-specific versus general knowledge. In contrast, our proposed LATA performs a \emph{layer-wise analysis} to separate generic utility from task-specific effects, enabling selective amplification or removal of tasks while preserving overall performance.

%\paragraph{Enhancing LLMs' safety alignment} \citet{bhardwaj-etal-2024-language} have demonstrated that Task Arithmetic (TA) aids in enhancing model safety alignment. However, applying TA to models fine-tuned for different languages may yield less noticeable improvements, as safety vectors derived from general English models might not align well due to language differences. Obtaining safety vectors specific to each language by fine-tuning with malicious data is time-consuming and resource-intensive. For enhancing model alignment at the individual layer, \citet{zhao-etal-2024-defending-large} introduce Layer-specific Editing (LED). By identifying and realigning critical early "safety layers" with decoded safe responses from specific layers, LED enhances LLM resilience to adversarial prompts without compromising performance on benign inputs. Similarly, \citet{li2025safety} proposed another method to identify safety layers. By fine-tuning these layers, the model's safety is further enhanced. While these methods effectively improve model alignment, they still require language-specific data and fine-tuning for different languages. In contrast, LATA directly extracts the essential components of the safety vector from English models and applies TA, ensuring alignment improvements without being affected by language differences.