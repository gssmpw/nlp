\section{Related Work}
\label{sec: Related Work}
Combining model capabilities without additional training has attracted growing attention. Model merging fuses weights of separately fine-tuned models for multi-task learning____, and simple averaging can improve accuracy and robustness____. TIES____ resets negligible changes to address sign conflicts, reducing performance drops; Delta-sparsification (DARE)____ discards up to 99\% of fine-tuning deltas to merge multiple homologous models. Most research aims to minimize utility loss of merged LLMs____, while ____ explore merging computer vision models using key parts of task vectors.

An alternative line of research, \emph{task arithmetic}~(\textit{TA}), views tasks as weight update vectors composed via vector operations. ____ define a \emph{task vector} as the difference between a fine-tuned model and its base, enabling multiple tasks to be learned simultaneously and new tasks to be inferred without retraining. Negating a task vector selectively unlearns a specific task with minimal impact on others, implying that model weights shift independently per task. TA has been considered in fine-tuning____ and alignment____ contexts.

In this paper, we focus on TA for both task learning and forgetting. Existing methods generally merge or edit entire models without distinguishing which layers encode task-specific versus general knowledge. In contrast, our proposed LATA performs a \emph{layer-wise analysis} to separate generic utility from task-specific effects, enabling selective amplification or removal of tasks while preserving overall performance.

%\paragraph{Enhancing LLMs' safety alignment} ____ have demonstrated that Task Arithmetic (TA) aids in enhancing model safety alignment. However, applying TA to models fine-tuned for different languages may yield less noticeable improvements, as safety vectors derived from general English models might not align well due to language differences. Obtaining safety vectors specific to each language by fine-tuning with malicious data is time-consuming and resource-intensive. For enhancing model alignment at the individual layer, ____ introduce Layer-specific Editing (LED). By identifying and realigning critical early "safety layers" with decoded safe responses from specific layers, LED enhances LLM resilience to adversarial prompts without compromising performance on benign inputs. Similarly, ____ proposed another method to identify safety layers. By fine-tuning these layers, the model's safety is further enhanced. While these methods effectively improve model alignment, they still require language-specific data and fine-tuning for different languages. In contrast, LATA directly extracts the essential components of the safety vector from English models and applies TA, ensuring alignment improvements without being affected by language differences.