\begin{table*}[!t]
\centering
\caption{\textbf{Evaluation Results of Three Aspects of CoT in Each Category in \dataset.} Best performance is marked in \colorbox{backred!60}{red}.  $*$ denotes unreliable results due to the refusal to answer directly.}
\vspace{-3pt}
\renewcommand\tabcolsep{2.0pt}
\renewcommand\arraystretch{1.25}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l|ccc|ccc|ccc|cc|cc|cc}
\toprule
\multirow{2}*{\makecell*[l]{\large Model}} & \multicolumn{3}{c|}{\makecell*[c]{General Scenes}} & \multicolumn{3}{c|}{Space-Time} & \multicolumn{3}{c|}{OCR} & \multicolumn{2}{c|}{Math} & \multicolumn{2}{c|}{Science} & \multicolumn{2}{c}{Logic} \\
& Quality & Robustness & Efficiency & Quality & Robustness & Efficiency & Quality & Robustness & Efficiency & Quality & Efficiency & Quality & Efficiency & Quality & Efficiency \\
\midrule
Mulberry & 33.9 & \colorbox{backred!60}{4.3} & 76.0 & 18.2 & 1.0 & 38.4 & 26.7 & \colorbox{backred!60}{6.6} & 26.4 & 29.1 & 87.9 & 29.1 & 91.9 & 13.9 & \colorbox{backred!60}{99.1} \\
LLaVA-OV-7B & 41.8 & -6.2 & 81.8 & 23.8 & -6.7 & 24.8 & 44.1 & -0.2 & 42.7 & 27.4 & 97.3 & 28.5 & 95.1 & 12.2 & 98.0 \\
LLaVA-CoT & 38.2 & -2.2 & 89.9 & 33.6 & 2.8 & 68.9 & 37.4 & 0.0 & 77.8 & 35.3 & 91.0 & 36.4 & 93.4 & 14.9 & 97.1 \\
LLaVA-OV-72B & 41.8 & -2.3 & \colorbox{backred!60}{98.9} & 29.0 & -0.9 & 43.6 & 40.8 & -1.7 & 84.2 & 38.4 & 98.7 & 35.4 & 95.7 & 18.4 & 82.3 \\
MiniCPM-V-2.6 & 47.1 & 3.2 & 87.7 & 49.3 & -14.4 & 71.1 & 63.7 & -4.9 & 62.0 & 32.9 & 95.2 & 29.5 & 90.4 & 16.9 & 93.7 \\
InternVL2.5-8B & 43.8 & -6.4 & 87.1 & 50.7 & -8.9 & \colorbox{backred!60}{99.1} & 44.7 & -4.1 & \colorbox{backred!60}{98.9} & 40.9 & 98.0 & 40.8 & 97.1 & 19.5 & 96.8 \\
Qwen2-VL-7B & 46.7 & -3.4 & 79.3 & 51.7 & -11.8 & 73.0 & 65.9 & 0.9 & 86.2 & 34.0 & 97.9 & 34.6 & 95.0 & 18.4 & 76.7 \\
InternVL2.5-8B-MPO & 47.2 & 2.9 & 94.3 & 51.8 & -0.2 & 74.6 & 59.6 & -1.0 & 81.5 & 37.4 & 93.4 & 39.0 & 95.6 & 20.9 & 79.9 \\
InternVL2.5-78B-MPO & 47.9 & 0.0 & 89.3 & 55.5 & -2.3 & 91.9 & 72.2 & 2.2 & 73.1 & 50.6 & 95.1 & 48.5 & 97.7 & 24.2 & 87.2 \\
Qwen2-VL-72B & 51.9 & -2.9 & 88.9 & 59.7 & -5.3 & 86.7 & 77.6 & 2.5 & 81.7 & 49.6 & 97.8 & 53.6 & \colorbox{backred!60}{99.0} & 40.0 & 88.0 \\
Virgo-72B & 60.5 & 0.5 & 91.0 & 59.6 & -3.8 & 86.0 & 79.9 & -1.0 & 82.1 & 59.6 & 90.3 & 55.5 & 98.7 & 39.6 & 88.2 \\
QVQ-72B & \colorbox{backred!60}{62.6} & -1.5 & 86.9 & 58.2 & -2.5 & 57.7 & 76.9 & -1.4 & 52.6 & \colorbox{backred!60}{61.4} & 92.7 & 57.7 & 95.9 & \colorbox{backred!60}{44.6} & 94.9 \\
GPT4o & 62.3 & -1.7 & 96.2 & \colorbox{backred!60}{66.3} & \colorbox{backred!60}{5.5} & 64.7 & \colorbox{backred!60}{83.3} & -1.0 & 82.1 & 60.8 & \colorbox{backred!60}{98.8} & \colorbox{backred!60}{64.1} & 97.4 & 27.2 & 92.0 \\
\bottomrule
\end{tabular}
}
\label{table:category_result}
% \vspace{-0.3cm}
\end{table*}