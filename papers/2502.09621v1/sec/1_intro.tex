\section{Introduction}
\label{submission}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{fig/teaser_v3.pdf}
    \caption{\textbf{Chain-of-Thought Performance of Leading LMMs in MME-CoT.} Our evaluation suite assesses LMMs using three novel metrics that yield six distinct scores. Results reveal that current open-source models, including those with reflection capabilities, still lag behind closed-source models like GPT-4o and Kimi k1.5 in key aspects of chain-of-thought reasoning.}
    \label{fig:teaser}
\end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{fig/main.pdf} 
\caption{\textbf{An Overview of MME-CoT.} Our benchmark contains a comprehensive CoT evaluation suite with three novel aspects and a meticulously curated dataset encompassing six categories.}
% \vspace{-1cm}
\label{fig:demo1}
\vspace{-0.17cm}
\end{figure*}

The emergence of Chain-of-Thought (CoT)~\cite{wei2022chain} in Large Language Models (LLMs) has demonstrated promising advances in reasoning capabilities, exemplified by the recent OpenAI o1~\cite{o1} and DeepSeek-R1~\cite{guo2025deepseek}. By engaging in a more deliberate, stepwise reasoning process before reaching a final answer, this methodology presents an effective solution in tackling complex scenarios.

In parallel, the multimodal extensions of LLMs, termed Large Multimodal Models (LMMs), have demonstrated remarkable proficiency across diverse visual domains, e.g., general image recognition~\cite{zhang2023llava,zhu2023minigpt, openai2023gpt4v,zhang2024llama}, temporal video understanding~\cite{li2023videochat,chen2023videollm}, and 3D geometry perception~\cite{guo2024sam2point, xu2023pointllm,guo2023point,jia2024lift3d}. However, to what extent and how much CoT reasoning can benefit multimodal challenges still remains an open question. Although some previous efforts~\cite{zhang2024mathverse, yu2023mm,zhang2024mavis,guo2025can} have been made to evaluate the CoT capabilities of LMMs, their examination is insufficiently systematic and thorough, limiting our understanding of multimodal reasoning and its further development.

To bridge this gap, we propose \textbf{MME-CoT}, a comprehensive and specialized benchmark for evaluating the CoT reasoning skills within LMMs (Figure~\ref{fig:demo1}). Our benchmark spans six fundamental domains: math, science, OCR, logic, space-time, and general scenes, encompassing a broad range of CoT-relevant scenarios.
Unlike the simplistic metrics used in previous studies, MME-CoT introduces a rigorous evaluation framework that delves into the fine-grained CoT process of LMMs, assessing reasoning quality, robustness, and efficiency. Specifically, we address three critical research questions as follows:

\begin{enumerate}
    \item \textit{\textbf{Is each intermediate CoT step logically valid and faithful without hallucination?}}
    The outcome-oriented evaluation paradigm, where most current benchmark adapts, omits the scenario where the model reaches the correct answer through flawed logic or random guess. This causes an illusion of inflated reasoning capabilities in the model. To delve into the reasoning process, we introduce two interpretable metrics to evaluate \textbf{the Quality of CoT}: \textit{1) Recall}, which quantifies reasoning informativeness by measuring the proportion of ground-truth solution steps appearing in the response; \textit{2) Precision}, which measures faithfulness by evaluating how many of the generated steps are accurate. 

    \item \textit{\textbf{Does CoT interfere with perception tasks, and to what extent does it enhance reasoning tasks?}}
    While existing studies primarily focus on the performance improvements CoT brings to reasoning tasks, they often overlook whether CoT could inadvertently disrupt the model’s ability to solve perception tasks that require minimal reasoning.
    To this end, we present \textit{the first} investigation into \textbf{the Robustness of CoT} in LMMs. Our benchmark incorporates two task categories (perception and reasoning), and employs two distinct prompting strategies (`direct answer' and `step-by-step') to assess two metrics:
    \textit{1) Stability}, which examines whether CoT negatively impacts the model’s performance on direct perception tasks;
    \textit{2) Efficacy}, which measures the extent to which CoT enhances the model’s performance on complex reasoning tasks.

    
    \item \textit{\textbf{How can we assess the efficiency of CoT in a long reasoning process?}}
    Recent o1-like models have distinguished themselves by employing excessively long CoT and reflection steps. This raises a critical trade-off question: does this approach strike an optimal balance between accuracy and computational cost?
    To investigate this, we present \textit{the first} study on \textbf{the Efficiency of CoT} in LMMs. We evaluate efficiency using two key metrics:
    \textit{1) Relevance Rate}, which assesses the proportion of generated content that contributes to answering the question.
    \textit{2) Reflection Quality}, which analyzes whether each reflection step drives the question towards correctness. 


\end{enumerate}

\begin{figure*}[!t]
\vspace{0.15cm}
\centering
\begin{minipage}[c]{0.63\textwidth}
\includegraphics[width=\columnwidth]{fig/pie.pdf}
\figcaption{\textbf{Category and Subcategory Distribution of MME-CoT.}} 
\label{pie}
\end{minipage} \hspace{3pt}   
\begin{minipage}[c]{0.34\textwidth}
\small
\centering
\vspace{-2pt}
\begin{adjustbox}{width=0.82\textwidth}
\begin{tabular}{lc}
\toprule
\textbf{Statistic} & \textbf{Number} \\
\midrule
Total questions & 1,130 \\
\quad - Reasoning questions & 837 (74.1\%) \\
\quad \quad Multiple-choice questions & 431 \\
\quad \quad Free-form questions & 406 \\
\quad - Perception questions & 293 (25.9\%) \\
\quad \quad Multiple-choice questions & 275 \\
\quad \quad Free-form questions & 18 \\
\midrule
Total key step annotation & 3,865 \\
\quad - Total inference conclusions & 2,667 \\
\quad - Average inference conclusions & 3.2 \\
\quad - Total image captions & 1,198 \\
\quad - Average image captions & 1.4 \\
Reference image caption item & 1,579 \\
Average reference caption & 1.9 \\
\midrule
Number of unique images & 2,380 \\
Number of unique questions & 808 \\
Number of unique answers & 271 \\
\midrule
Maximum question length & 477 \\
Maximum answer length & 15 \\
Average question length & 41.2 \\
Average answer length & 1.2 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-3pt}
\tabcaption{\textbf{Key Statistics of \dataset}.}
\label{table:statistics}
\end{minipage}
\vspace{-0.2cm}
\end{figure*}


Through our systematic evaluation and analysis, we discover that the fine-grained reflection capability greatly enhances the CoT quality, e.g., QVQ achieves F1 Score of 62.0\%, largely surpassing Qwen2-VL-72B by 6.8\%. Kimi k1.5 beats GPT-4o and achieves the best quality. As for the robustness, we surprisingly find that most models are interfered with by CoT on the perception tasks, implying a harmful overthinking behavior. The worst case happens in InternVL2.5-8B, where we witness a 6.8\% degradation when applying CoT on the perception tasks. This significantly impedes the applicability of models using CoT reasoning as a default practice. Moreover, for CoT efficiency, we notice that not all steps within the long CoT are related to answering the question, and the model could be distracted by the image content, especially when handling general scenes, space-time, and OCR tasks. Around 30\% to 40\% of reflection steps fail to help answer questions, pointing out critical issues of current models' reflection capabilities.



The contributions of this paper are summarized as follows:
\begin{itemize}
    \item The MME-CoT benchmark is curated, covering a comprehensive scope of six multimodal reasoning scenarios.
    The data collection and annotation process undergoes rigorous human verification, aiming to provide the community with a high-quality evaluation dataset for multimodal reasoning.


    \item We identify critical issues in existing benchmarks, and introduce a thorough evaluation suite specialized for multimodal CoT reasoning, which meticulously examines the reasoning quality, robustness, and efficiency.

    \item We conduct extensive experiments and analysis on state-of-the-art LMMs with reasoning capabilities. We summarize our observations and insights, hoping to inspire future advancements of reasoning performance.
\end{itemize}