
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section*{Appendix Overview}
\begin{itemize}
    \item Section~\ref{appendix:related}: Related Work.
    \item Section~\ref{appendix:more_dataset}: More Dataset Details.
    \item Section~\ref{appendix:error_analysis}: Error Analysis.
    \item Section~\ref{appendix:more_qualitative}: More Qualitative Examples.
    \item Section~\ref{appendix:eval_setup}: Evaluation Prompts.
\end{itemize}


\section{Related Work}
\label{appendix:related}
\subsection{Large Multimodal Models}
The field of multimodal~\citep{Radford2021LearningTV, li2022blip, openai2023gpt4v, openai2024gpt4o} AI has experienced extraordinary growth, particularly through the development of Large Multimodal Models (LMMs)~\cite{liu2023llava,zhu2023minigpt,lin2023sphinx,Qwen2-VL}. These models build upon the achievements of Large Language Models (LLMs)~\citep{touvron2023llama,qwen2} and advanced vision models~\cite{Radford2021LearningTV}, expanding their capabilities to process multiple kinds of visual input~\cite{li2024llava,guo2023point,li2023videochat}.

Closed-source models, such as OpenAI's GPT-4o~\citep{openai2024gpt4o}, have demonstrated exceptional capabilities in visual understanding and reasoning. However, their closed-source nature creates barriers to widespread adoption and further development by the broader research community. In response, significant progress has been made in developing open-source alternatives. Early approaches like LLaVA~\cite{liu2023llava}, LLaMA-Adapter~\cite{zhang2024llamaadapter}, and MiniGPT-4~\cite{zhu2023minigpt} established a foundation by combining frozen CLIP models for image encoding with LLMs, enabling multimodal instruction tuning. Subsequent developments through projects such as InternVL2~\cite{chen2024far}, Qwen2-VL~\cite{Qwen2-VL}, SPHINX~\cite{gao2024sphinx,lin2023sphinx}, and MiniCPM-V~\cite{yao2024minicpm} have expanded these capabilities by incorporating more diverse visual instruction datasets and broadening application scenarios.

Recently, with the introduction of o1~\cite{o1}, the field of LMMs has also focused on enhancing the reasoning capability. \cite{wang2024enhancing} introduces mixed preference optimization with automatically constructed data. \cite{yao2024mulberry} proposes to leverage collective knowledge from multiple models to identify effective reasoning paths. Besides, several works~\cite{qvq-72b-preview,du2025virgo} have demonstrated the ability to replicate behaviors similar to o1 models, particularly regarding multi-step CoT reasoning with iterative self-reflection and verification processes.

\subsection{Reasoning Evaluation}
Several methods have been developed to evaluate reasoning in natural language processing, including ROSCOE~\cite{golovneva2022roscoe} and ReCEval~\cite{prasad2023receval}, which assess reasoning chains across multiple dimensions such as correctness and informativeness. However, these approaches are limited to text-only scenarios and do not address the unique challenges present in visual reasoning tasks. Furthermore, the emergence of long chain-of-thought (CoT) reasoning has introduced additional considerations, such as output efficiency and reflection quality, which existing evaluation methods do not adequately address.

On the other hand, various multimodal benchmarks have been developed to assess reasoning abilities across specific domains. Current exploration of visual reasoning predominantly focuses on the mathematics~\cite{zhang2024mavis,peng2024chimera} domains. 
MathVista~\cite{Lu2023MathVistaEM} provides a comprehensive collection of mathematical problems that assess mathematical and logical reasoning abilities. 
Building on this, MathVerse~\cite{zhang2024mathverse} introduces a new benchmark by eliminating redundant textual information to evaluate whether LMMs can accurately interpret graphical representations. 
OlympiadBench~\cite{he2024olympiadbench} further raises the complexity bar by incorporating challenging Olympiad-level mathematics and physics problems. Despite these advances in specialized domains, broader applications such as general-scene reasoning remain relatively unexplored.
Recent developments have begun to expand beyond purely scientific reasoning. For instance, MÂ³CoT~\cite{chen-etal-2024-m3cot} and SciVerse~\cite{sciverse} incorporate commonsense tasks alongside scientific reasoning and knowledge-based assessment in the multimodal benchmark. However, most existing benchmarks focus solely on evaluating final answers while overlooking the intermediate steps, thus providing limited insights into the process through which models arrive at their conclusions.


\section{More Dataset Details}
\label{appendix:more_dataset}
\subsection{Data Source Distribution}
We visualize the data source distributions in our benchmark, which consists of 15 sets, including MathVerse~\cite{zhang2024mathverse}, MMMUPro~\cite{yue2024mmmuprorobustmultidisciplinemultimodal}, OlympiadBench~\cite{he2024olympiadbench}, MMT-Bench~\cite{ying2024mmt}, MuirBench~\cite{wang2024muirbench}, ml-rpm-bench~\cite{zhang2024far}, MMSearch~\cite{jiang2024mmsearch}, CharXiv~\cite{wang2024charxiv}, and SciVerse~\cite{sciverse}.

\begin{figure*}[!h]
\centering
\includegraphics[width=0.4\textwidth]{fig/pie_supp.pdf} 
\caption{\textbf{Data Source Distribution of MME-CoT.}}
\label{appendix:more_dataset-source}
\end{figure*}

\newpage

\subsection{Preliminary Categorization Result}
\label{appendix:preliminary_result}
\begin{table}[htbp]
    \centering
    \caption{\textbf{Accuracy of MMT-Bench for different subcategories}. ACT: Action Understanding; AUT: Attribute Similarity; CNT: Cartoon Understanding; CIM: Counting; DOC: Diagram Understanding; EMO: Difference Spotting; HAL: Geographic Understanding; IIT: Image-Text Matching; IRT: Ordering; IQT: Scene Understanding; MEM: Visual Grounding; MIA: Visual Retrieval; OCR: Object Recognition; PLP: Physical Layout Prediction; RRE: Relationship Extraction; TMP: Temporal Reasoning; VCP: Visual Comprehension; VCR: Visual Coherence Reasoning; VGR: Visual Generation; VIL: Visual Identification; VPU: Visual Prediction Understanding; VRE: Visual Reasoning Evaluation.}
    \label{tab:hit_ratio}
    \setlength{\tabcolsep}{4pt} 
    \renewcommand{\arraystretch}{1.2}
    \small 
    \begin{tabularx}{\textwidth}{l *{22}{X}}
        \toprule
        File Name & 
        \rotatebox{90}{ACT} & \rotatebox{90}{AUT} & \rotatebox{90}{CNT} & \rotatebox{90}{CIM} & 
        \rotatebox{90}{DOC} & \rotatebox{90}{EMO} & \rotatebox{90}{HAL} & \rotatebox{90}{IIT} & 
        \rotatebox{90}{IRT} & \rotatebox{90}{IQT} & \rotatebox{90}{MEM} & \rotatebox{90}{MIA} & 
        \rotatebox{90}{OCR} & \rotatebox{90}{PLP} & \rotatebox{90}{RRE} & \rotatebox{90}{TMP} & 
        \rotatebox{90}{VCP} & \rotatebox{90}{VCR} & \rotatebox{90}{VGR} & \rotatebox{90}{VIL} & 
        \rotatebox{90}{VPU} & \rotatebox{90}{VRE} \\
        \midrule
        GPT4o-cot & 0.60 & 0.60 & 0.44 & 0.67 & 0.79 & 0.30 & 0.71 & 0.50 & 0.63 & 0.10 & 0.85 & 0.60 & 0.77 & 0.36 & 0.76 & 0.48 & 0.86 & 0.80 & 0.49 & 0.48 & 0.82 & 0.85 \\
        GPT4-direct & 0.53 & 0.60 & 0.44 & 0.67 & 0.81 & 0.23 & 0.69 & 0.33 & 0.66 & 0.25 & 0.80 & 0.43 & 0.78 & 0.42 & 0.78 & 0.36 & 0.89 & 0.85 & 0.41 & 0.37 & 0.85 & 0.85 \\
        Qwen2-VL-7B-cot & 0.53 & 0.61 & 0.34 & 0.65 & 0.77 & 0.53 & 0.74 & 0.40 & 0.31 & 0.20 & 0.78 & 0.58 & 0.60 & 0.43 & 0.69 & 0.43 & 0.85 & 0.90 & 0.54 & 0.35 & 0.79 & 0.81 \\
        Qwen2-VL-7B-direct & 0.49 & 0.67 & 0.40 & 0.78 & 0.75 & 0.52 & 0.73 & 0.43 & 0.31 & 0.10 & 0.78 & 0.55 & 0.60 & 0.54 & 0.69 & 0.40 & 0.85 & 0.85 & 0.67 & 0.38 & 0.85 & 0.82 \\
        \bottomrule
    \end{tabularx}
\end{table}


\begin{table}[htbp]
    \centering
    \caption{\textbf{Accuracy of MUIRBench for different subcategories}. AU: Action Understanding; AS: Attribute Similarity; CU: Cartoon Understanding; CO: Counting; DU: Diagram Understanding; DS: Difference Spotting; GU: Geographic Understanding; ITM: Image-Text Matching; OR: Ordering; SU: Scene Understanding; VG: Visual Grounding; VR: Visual Retrieval.}

    \label{tab:hit_ratio}
    \setlength{\tabcolsep}{4pt} 
    \renewcommand{\arraystretch}{1.2} 
    \small 
    \begin{tabularx}{\textwidth}{l XXXX XXXX XXXX XXXX}
        \toprule
        File Name & AU & AS & CU & CO & DU & DS & GU & ITM & OR & SU & VG & VR \\
        \midrule
        GPT4o-cot & 0.48 & 0.57 & 0.55 & 0.75 & 0.82 & 0.64 & 0.59 & 0.82 & 0.38 & 0.88 & 0.56 & 0.70 \\
        GPT4o-direct & 0.45 & 0.62 & 0.59 & 0.50 & 0.88 & 0.62 & 0.55 & 0.86 & 0.33 & 0.74 & 0.38 & 0.77 \\
        Qwen2-VL-7B-cot & 0.38 & 0.51 & 0.42 & 0.43 & 0.43 & 0.27 & 0.21 & 0.55 & 0.13 & 0.69 & 0.37 & 0.28 \\
        Qwen2-VL-7B-direct & 0.39 & 0.47 & 0.44 & 0.41 & 0.40 & 0.33 & 0.25 & 0.51 & 0.13 & 0.67 & 0.31 & 0.20 \\
        \bottomrule
    \end{tabularx}
\end{table}



\begin{table}[htbp]
    \centering
    \caption{\textbf{Accuracy of OlympiadBench for the mathematics and physics subcategories}.}
    \label{tab:hit_ratio_oe}
    \small 
    \begin{tabular}{lcc}
        \toprule
        File Name & Mathematics & Physics\\
        \midrule
        GPT4o-cot & 0.25 & 0.04 \\
        GPT4o-direct & 0.07 & 0.03 \\
        Qwen2-VL-7B-cot & 0.05 & 0.01 \\
        Qwen2-VL-7B-direct & 0.07 & 0.01 \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\section{Error Analysis}
\label{appendix:error_analysis}
We showcase the examples of the identified error types of reflection in Fig.~\ref{fig:ref_error_example}.
\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/ref_error_example.pdf} 
\caption{\textbf{Examples of Reflection Error Types.}}
\label{fig:ref_error_example}
\end{figure*}


\newpage

\section{More Qualitative Examples}
\label{appendix:more_qualitative}
\begin{figure*}[!h]
\centering
\includegraphics[width=0.6\textwidth]{fig/precision_recall_example_GPT.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example_GPT}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=0.9\textwidth]{fig/precision_recall_example_Qwen.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example_Qwen}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=0.58\textwidth]{fig/precision_recall_example_QVQ.pdf}
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example_QVQ}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/precision_recall_example_QVQ2.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example_QVQ2}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=0.51\textwidth]{fig/precision_recall_example2_GPT.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example2_GPT}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=0.79\textwidth]{fig/precision_recall_example2_Qwen.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example2_Qwen}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=0.81\textwidth]{fig/precision_recall_example2_QVQ.pdf} 
\caption{\textbf{Examples of Precision and Recall Evaluation.}}
\label{fig:precision_recall_example2_QVQ}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/relevance_example_GPT.pdf} 
\caption{\textbf{Examples of Relevance Rate Evaluation.}}
% \vspace{-1cm}
\label{fig:relevance_example_GPT}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/relevance_example_Qwen.pdf} 
\caption{\textbf{Examples of Relevance Rate Evaluation.}}
% \vspace{-1cm}
\label{fig:relevance_example_Qwen}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/relevance_example_QVQ.pdf} 
\caption{\textbf{Examples of Relevance Rate Evaluation.}}
% \vspace{-1cm}
\label{fig:relevance_example_QVQ}
\end{figure*}
\newpage

\begin{figure*}[!h]
\centering
\includegraphics[width=\textwidth]{fig/ref_example_QVQ.pdf} 
\caption{\textbf{Examples of Reflection Quality Evaluation.}}
% \vspace{-1cm}
\label{fig:ref_example_QVQ}
\end{figure*}
\newpage


\section{Detailed Evaluation Setup}
\label{appendix:eval_setup}
\subsection{CoT Quality Evaluation Prompts}

\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Recall Evaluation Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]

You are an expert system to verify solutions to image-based problems. Your task is to match the ground truth middle steps with the provided solution.\\

INPUT FORMAT:\\
1. Problem: The original question/task\\
2. A Solution of a model\\
3. Ground Truth: Essential steps required for a correct answer\\

MATCHING PROCESS:\\

You need to match each ground truth middle step with the solution:\\

Match Criteria:\\
- The middle step should exactly match in the content or is directly entailed by a certain content in the solution\\
- All the details must be matched, including the specific value and content\\
- You should judge all the middle steps for whether there is a match in the solution\\

OUTPUT FORMAT:
\begin{verbatim}
[
  {
    "step_index": \textless integer\textgreater,
    "judgment": "Matched" | "Unmatched"
  }
]
\end{verbatim}

ADDITIONAL RULES:\\
1. Only output the JSON array with no additional information.\\
2. Judge each ground truth middle step in order without omitting any step.\\

Here are the problem, answer, solution, and ground truth middle steps:\\

[Problem]\\

\{question\}\\

[Answer]\\

\{answer\}\\

[Solution]\\

\{solution\}\\

[Ground Truth Information]\\

\{gt\_annotation\}

\end{tcolorbox}

\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Precision Evaluation Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]

\# Task Overview\\
Given a solution with multiple reasoning steps for an image-based problem, reformat it into well-structured steps and evaluate their correctness.\\

\# Step 1: Reformatting the Solution\\
Convert the unstructured solution into distinct reasoning steps while:\\
- Preserving all original content and order\\
- Not adding new interpretations\\
- Not omitting any steps\\

\#\# Step Types\\
1. Logical Inference Steps\\
   - Contains exactly one logical deduction\\
   - Must produce a new derived conclusion\\
   - Cannot be just a summary or observation\\
\\
2. Image Observation Steps\\
   - Pure visual observations\\
   - Only includes directly visible elements\\
   - No inferences or assumptions\\
\\
3. Background Information Steps\\
   - External knowledge or question context\\
   - No inference process involved\\

\#\# Step Requirements\\
- Each step must be atomic (one conclusion per step)\\
- No content duplication across steps\\
- Initial analysis counts as background information\\
- Final answer determination counts as logical inference\\

\# Step 2: Evaluating Correctness\\
Evaluate each step against:\\

\#\# Ground Truth Matching\\
For image observations:\\
- Key elements must match ground truth observations\\
\\
For logical inferences:\\
- Conclusion must EXACTLY match or be DIRECTLY entailed by ground truth\\

\#\# Reasonableness Check (if no direct match)\\
Step must:\\
- Premises must not contradict any ground truth or correct answer\\
- Logic is valid\\
- Conclusion must not contradict any ground truth \\
- Conclusion must support or be neutral to correct answer\\

\#\# Judgement Categories\\
- "Match": Aligns with ground truth\\
- "Reasonable": Valid but not in ground truth\\
- "Wrong": Invalid or contradictory\\
- "N/A": For background information steps\\

\# Output Requirements\\
1. The output format must be in valid JSON format without any other content.\\
2. For highly repetitive patterns, output it as a single step.\\
3. Output maximum 40 steps. Always include the final step that contains the answer.\\

Here is the json output format:\\
\#\# Output Format
\begin{verbatim}
[
  {
    "step_type": "image observation|logical inference|background information",
    "premise": "Evidence (only for logical inference)",
    "conclusion": "Step result",
    "judgment": "Match|Reasonable|Wrong|N/A"
  }
]
\end{verbatim}

Here is the problem, and the solution that needs to be reformatted to steps:\\

[Problem]\\

\{question\}\\

[Solution]\\

\{solution\}\\

[Correct Answer]\\

\{answer\}\\

[Ground Truth Information]\\

\{gt\_annotation\}

\end{tcolorbox}

\subsection{CoT Efficiency Prompt}
\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Relevance Rate Evaluation Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]
\# Task Overview
Given a solution with multiple reasoning steps for an image-based problem, evaluate the relevance to get a solution (ignore correct or wrong) of each step.\\

\# Step 1: Reformatting the Solution
Convert the unstructured solution into distinct reasoning steps while:\\
- Preserving all original content and order\\
- Not adding new interpretations\\
- Not omitting any steps\\

\#\# Step Types \\
1. Logical Inference Steps\\
  - Contains exactly one logical deduction\\
  - Must produce a new derived conclusion\\
  - Cannot be just a summary or observation

2. Image Description Steps\\
  - Pure visual observations\\
  - Only includes directly visible elements\\
  - No inferences or assumptions

3. Background Information Steps\\
  - External knowledge or question context\\
  - No inference process involved\\

\#\# Step Requirements
- Each step must be atomic (one conclusion per step)\\
- No content duplication across steps\\
- Initial analysis counts as background information\\
- Final answer determination counts as logical inference\\

\# Step 2: Evaluating Relevancy\\
A relevant step is considered as: 75\% content of the step must be related to trying to get a solution (ignore correct or wrong) to the question.\\

IMPORTANT NOTE:\\
Evaluate relevancy independent of correctness. As long as the step is trying to get to a solution, it is considered relevant. Logical fallacy, knowledge mistake, inconsistent with previous steps, or other mistakes do not affect relevance. A logically wrong step can be relevant if the reasoning attempts to address the question.\\

The following behaviour is considered as relevant:\\
i. The step is planning, summarizing, thinking, verifying, calculating, or confirming an intermediate/final conclusion helpful to get a solution.\\
ii. The step is summarizing or reflecting on previously reached conclusion relevant to get a solution.\\
iii. Repeating the information in the question or give the final answer.\\
iv. A relevant image depiction should be in one of following situation:\\
1. help to obtain a conclusion helpful to solve the question later;\\
2. help to identify certain patterns in the image later;\\
3. directly contributes to the answer\\
v. Depicting or analyzing the options of the question is also relevant.\\
vi. Repeating previous relevant steps are also considered relevant.\\

The following behaviour is considered as irrelevant:\\
i. Depicting image information that does not related to what is asking in the question. Example: The question asks how many cars are present in all the images. If the step focuses on other visual elements like the road or building, the step is considered as irrelevant.\\
ii. Self-thought not related to what the question is asking.\\
iii. Other information that is tangential for answering the question.\\

\# Output Format

\begin{verbatim}
[
  {
    "step_type": "image observation|logical inference|background information",
    "conclusion": "A brief summary of step result",
    "relevant": "Yes|No"
  }
]
\end{verbatim}\\

\# Output Rules\\
Direct JSON output without any other output\\
Output at most 40 steps\\

Here is the problem, and the solution that needs to be reformatted to steps:

[Problem]\\

\{question\}\\

[Solution]\\

\{solution\}
\end{tcolorbox}

\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Reflection Quality Evaluation Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]

Here\'s a refined prompt that improves clarity and structure:\\

\# Task\\
Evaluate reflection steps in image-based problem solutions, where reflections are self-corrections or reconsideration of previous statements.\\

\# Reflection Step Identification \\
Reflections typically begin with phrases like:\\
- "But xxx"\\
- "Alternatively, xxx" \\
- "Maybe I should"\\
- "Let me double-check"\\
- "Wait xxx"\\
- "Perhaps xxx"\\
It will throw a doubt of its previously reached conclusion or raise a new thought.\\

\# Evaluation Criteria\\
Correct reflections must:\\
1. Reach accurate conclusions aligned with ground truth\\
2. Use new insights to find the mistake of the previous conclusion or verify its correctness. \\

Invalid reflections include:\\
1. Repetition - Restating previous content or method without new insights\\
2. Wrong Conclusion - Reaching incorrect conclusions vs ground truth\\
3. Incompleteness - Proposing but not executing new analysis methods\\
4. Other - Additional error types\\

\# Input Format\\

[Problem]\\

\{question\}\\

[Solution]\\

\{solution\}\\

[Ground Truth]\\

\{gt\_annotation\}\\

\# Output Requirements\\
1. The output format must be in valid JSON format without any other content.\\
2. Output maximum 30 reflection steps.\\

Here is the json output format:\\
\#\# Output Format
\begin{verbatim}
[
  {
    "conclusion": "One-sentence summary of reflection outcome",
    "judgment": "Correct|Wrong",
    "error_type": "N/A|Repetition|Wrong Conclusion|Incompleteness|Other"
  }
]
\end{verbatim}

\# Rules\\
1. Preserve original content and order\\
2. No new interpretations\\
3. Include ALL reflection steps\\
4. Empty list if no reflections found\\
5. Direct JSON output without any other output

\end{tcolorbox}

\subsection{Direct Evaluation Prompt}
\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Answer Extraction Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]
You are an AI assistant who will help me to extract an answer of a question. You are provided with a question and a response, and you need to find the final answer of the question. \\

Extract Rule:

[Multiple choice question]

1. The answer could be answering the option letter or the value. You should directly output the choice letter of the answer.

2. You should output a single uppercase character in A, B, C, D, E, F, G, H, I (if they are valid options), and Z.

3. If the meaning of all options are significantly different from the final answer, output Z. \\

[Non Multiple choice question]

1. Output the final value of the answer. It could be hidden inside the last step of calculation or inference. Pay attention to what the question is asking for to extract the value of the answer.

2. The final answer could also be a short phrase or sentence.

3. If the response doesn't give a final answer, output Z.\\

Output Format: 
Directly output the extracted answer of the response. \\

\{In Context Examples\}\\

Question: \{question\}

Answer: \{response\}\\

Your output: 

\end{tcolorbox}

\begin{tcolorbox}[breakable, colback=gray!5!white, colframe=gray!75!black, 
title=Answer Scoring Prompt, boxrule=0.5mm, width=\textwidth, arc=3mm, auto outer arc]

You are an AI assistant who will help me to judge whether two answers are consistent.\\

Input Illustration:
[Standard Answer] is the standard answer to the question. 
[Model Answer] is the answer extracted from a model's output to this question. 

Task Illustration:
Determine whether [Standard Answer] and [Model Answer] are consistent.\\

Consistent Criteria:

[Multiple-Choice questions]

1. If the [Model Answer] is the option letter, then it must completely matches the [Standard Answer].

2. If the [Model Answer] is not an option letter, then the [Model Answer] must completely match the option content of [Standard Answer].

[Nan-Multiple-Choice questions]

1. The [Model Answer] and [Standard Answer] should exactly match.

2. If the meaning is expressed in the same way, it is also considered consistent, for example, 0.5m and 50cm.\\

Output Format: 
1. If they are consistent, output 1; if they are different, output 0.

2. DIRECTLY output 1 or 0 without any other content.

\{In Context Examples\}\\

Question: \{question\}

[Model Answer]: \{extract\_answer\}

[Standard Answer]: \{gt\_answer\}

Your output:

\end{tcolorbox}