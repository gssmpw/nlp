\section{Dataset Curation}
\label{sec2_benchmark}

\subsection{Data Composition and Categorization.}
% TODO: Dataset Statistics
MME-CoT composes 6 major domains with 17 subcategories, as visualized in Fig.~\ref{pie}. Different from textual reasoning questions, the extra visual input significantly enriches the scope of the visual reasoning questions. 
With the image input, the model needs to frequently visit the image for relevant information according to current reasoning progress. Describing the image area of interest becomes a crucial part of the CoT process.
Thus, in addition to complex problems demanding rigorous logic, commonsense scenarios also pose a challenging reasoning problem,
as shown in the general scenes in Fig.~\ref{fig:demo1}. To maintain focus on the reasoning process, we exclude questions that require complex domain-specific theorems or specialized knowledge.

In addition, to evaluate CoT robustness detailed in Section~\ref{sec2_evaluation_robustness}, we incorporate a variety of perception tasks along with the reasoning tasks in the benchmark. 
The reasoning tasks contain questions that demand multi-step logical inference, while the perception tasks consist of questions that primarily test visual recognition abilities or require very minimal reasoning.
Existing benchmarks often conflate these two types of tasks, with perception and reasoning questions frequently appearing within the same categories. To address this, we implement a two-stage classification approach combining both model-based and human assessment. Initially, we leverage LMMs to guide the preliminary categorization by comparing their performance with and without CoT prompting. We employ GPT-4o~\cite{openai2024gpt4o} and Qwen2-VL-7B~\cite{wang2024qwen2} to answer questions using both direct and CoT approaches. Superior performance with CoT indicates a reasoning-dominant subcategory, while comparable or inferior CoT performance suggests either perception-focused content or insufficient model reasoning capabilities. The results are shown in Appendix~\ref{appendix:preliminary_result}. Subsequently, expert annotators review individual questions to finalize their classification. 
In total, MME-CoT contains 1,130 questions with 3,865 key step annotation. The detailed statistics of data compositions are shown in Table~\ref{table:statistics}. 
Please refer to Appendix~\ref{appendix:more_dataset} for more details about the distribution of data sources.

\subsection{Data Annotation and Review}
To facilitate CoT evaluation, we provide key steps annotation and reference image captions for all the reasoning questions. Key steps are defined as those that must be done to reach the correct answer. For efficient annotation, we first employ GPT-4o to generate the answer rationale and image captions. For the rationale, we provide both questions and ground truth answers to the model, which yields more accurate rationales compared to question-only prompting. Annotators are then asked to provide key intermediate steps with the help of GPT-4o's responses. For cases where GPT-4o fails to generate reasonable rationales, annotators develop solutions independently. The intermediate steps fall into two categories: inference conclusion and image caption. Note that the final answer is also included as a concluding inference. All the steps are reduced to the simplest form, retaining only core conclusions and relevant visual element descriptions. Notably, for problems with multiple solutions, annotators are required to provide all possible methods. For reference captions, we also ask annotators to verify and correct the details.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{fig/step_partition_1.pdf} 
\caption{\textbf{Illustration of Step Partition.} We instruct GPT-4o to divide each step into three categories: image caption, background information, or logical inference. The step partition result is later used to perform step-wise reasoning evaluation. We focus on evaluating the image caption and logical inference steps, which are the keys to visual reasoning.}
% \vspace{-1cm}
\label{fig-step-partition}
\end{figure*}

% \input{tab/stat}

\section{CoT Evaluation Strategy}
\label{sec2_evaluation}
Existing benchmarks only focus on evaluating the final answer of the questions, leaving the whole chain of thoughts unvisited. We argue that the CoT process reflects reasoning capability from multiple aspects, serving as a crucial medium to understand LMM's thinking pattern and deficiency. Here, we present the first holistic CoT evaluation suite to facilitate a comprehensive understanding of the LMMs' reasoning abilities. We detail the evaluation of correctness in Section~\ref{sec2_evaluation_correctness}, stability and efficacy in Section~\ref{sec2_evaluation_robustness}, and reflection quality in Section~\ref{sec2_evaluation_reflection}.

\subsection{CoT Quality Evaluation}
\label{sec2_evaluation_correctness}
 Existing methods typically rely on state-of-the-art LLMs or LMMs to directly evaluate Chain-of-Thought reasoning based on self-defined criteria, using only the final answer as a reference~\cite{hao2024llm,zhang2024mathverse}. We identify two primary issues with the strategy. First, the scoring process only attends to the logical validity of each step, omitting the helpfulness evaluation. Second, there is a large number of complex visual reasoning questions that even the scoring model cannot solve. It is unreasonable for the scoring model to judge another model's reasoning process on these questions without knowing the ground truth solution process. 
 Therefore, building upon our annotated key steps and reference image captions, we leverage two interpretable metrics to evaluate the CoT correctness: recall and precision (Figure~\ref{fig:quality}). The two metrics respectively attend to the two aspects of the CoT correctness: informativeness and accuracy. We denote the key steps as $\mathcal{S} = \mathcal{C} \cup \mathcal{I}$, where $\mathcal{C} = \{c_1, ..., c_M\}$ includes $M$ key inference conclusions and $\mathcal{I} = \{i_1, ..., i_N\}$ includes $N$ key image captions. 

 
\vspace{-1em}
 \paragraph{Recall.}
 We prompt GPT-4o~\cite{gpt4omini} to determine whether each key step occurs in the model's CoT response. Then we calculate the ratio of the matched key steps $\mathcal{S}_{\text{matched}}$ against all the annotated key steps:
\begin{align}
    &k_0 = \argmax_{k} \frac{\left | \mathcal{S}^{k}_{\text{matched}} \right |}{\left | \mathcal{S}^{k} \right |}, \\
    \text{Recall}_{\mathcal{C}} &= \frac{\left| \mathcal{C}^{k_0}_{\text{matched}} \right|}{\left| \mathcal{C}^{k_0} \right|}, \quad
    \text{Recall}_{\mathcal{I}} = \frac{\left| \mathcal{I}^{k_0}_{\text{matched}} \right|}{\left| \mathcal{I}^{k_0}\right|}, \\
    &\text{Recall} = \frac{\left | \mathcal{S}^{k_0}_{\text{matched}} \right |}{\left | \mathcal{S}^{k_0} \right |}.
\end{align}
 where $\mathcal{S}^{k}$ denotes the $k^{\text{th}}$ method of the problem.
 Intuitively, recall measures how many informative steps are reached by the model. From another perspective, this metric also strictly examines the process's rigorousness toward reaching the correct answer, eliminating the probability of random guessing. For questions with multiple methods, we compute the recall on the most matched method.

 \paragraph{Precision.} 
  We first instruct GPT-4o to partition the prediction into a sequence of steps $\mathcal{P}$, as shown in Fig.~\ref{fig-step-partition}.
  Each step is categorized into one of three classes: logical inference, image caption, and background information. The logical inference step draws an intermediate or final conclusion based on the previously obtained information. The image caption step depicts elements of interest in the image. The background information step states external knowledge or question information. Visual reasoning can be primarily characterized as an interleaved sequence of image captions and logical inferences, so we focus on measuring precision for these two key step types. We assess the correctness of logical inference steps ($\mathcal{C}^{\mathcal{P}}$) and image caption steps ($\mathcal{I}^{\mathcal{P}}$) using two criteria: 1. If the step exists in $\mathcal{S}$, the step is correct. 2. If the step is logically correct or faithfully depicts the image based on the annotations, the step is also correct. Thus, we compute precision as:
\begin{align}
    \text{Precision}_{\mathcal{C}} &= \frac{\left| \mathcal{C}^{\mathcal{P}}_{\text{correct}} \right|}{\left| \mathcal{C}^{\mathcal{P}} \right|}, \quad
    \text{Precision}_{\mathcal{I}} = \frac{\left| \mathcal{I}^{\mathcal{P}}_{\text{correct}} \right|}{\left| \mathcal{I}^{\mathcal{P}} \right|}, \\
    & \text{Precision} = \frac{\left| \mathcal{C}^{\mathcal{P}}_{\text{correct}} \cup \mathcal{I}^{\mathcal{P}}_{\text{correct}} \right|}{\left| \mathcal{C}^{\mathcal{P}} \cup \mathcal{I}^{\mathcal{P}} \right|}
\end{align}
Intuitively, precision evaluates the faithfulness of each step, considering all the possible reasoning output. Finally, we calculate the F1 score as the metric of CoT quality.


\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{fig/quality.pdf} 
\caption{\textbf{Illustration of CoT Quality Evaluation.} For recall, we prompt GPT-4o to match each key step annotation in the prediction. For precision, GPT-4o is instructed to split the prediction into steps and determine the correctness of all the image caption and logical inference steps.}
% \vspace{-1cm}
\label{fig:quality}
\vspace{-0.17cm}
\end{figure*}

\subsection{CoT Robustness Evaluation}
\label{sec2_evaluation_robustness}
Here, we perform the first investigation on the robustness of CoT in visual reasoning. The effectiveness of CoT on reasoning tasks has been verified in many works~\cite{wei2022chain, o1}. However, how CoT impacts visual perception tasks or tasks requiring minimal reasoning still remains unknown. Despite the neglect, this question bears great importance.
In real-world applications, what task is given is unknown in advance. Whether the model should perform CoT to solve the task is difficult to determine. In fact, there exists no golden standard to determine which question can benefit from CoT so far~\cite{sprague2024cot}.
Instead of trying to define this criterion, we examine the performance of CoT across all kinds of tasks, both reasoning and perception. We argue that an ideal CoT process should assist in reasoning and not interfere with pure perception. Therefore, it can be applied for any tasks.
Based on this, we propose to evaluate two metrics of CoT: stability and efficacy (Figure~\ref{fig:robustness}). We leverage two kinds of prompts: the direct prompt ($\textsc{dir}$) and the CoT prompt ($\textsc{cot}$). The direct prompt asks the model to directly provide the final answer, while the CoT prompt instructs the model to perform step-by-step reasoning and finally give the answer. To directly compare the performance difference caused by these two prompts, we conduct the direct evaluation, which only judges the correctness of the final answer, i.e., accuracy. We instruct GPT-4o mini~\cite{gpt4omini} to extract the final answer, and then compare it with the ground truth answer, following the two-step procedure introduced in~\cite{zhang2024mathverse}. 

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/robustness.pdf}}
% \includegraphics[width=\columnwidth]{fig/step_partition_1.pdf}
\caption{\textbf{Illustration of CoT Robustness Evaluation.} We compare the performance of applying CoT prompt and direct prompt on two types of tasks: perception and reasoning. The stability score measures whether CoT interferes with perception, while the efficacy score assesses the performance gain of CoT on reasoning tasks.}
\label{fig:robustness}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/efficiency.pdf}}
\caption{\textbf{Illustration of CoT Efficiency Evaluation.} For relevance rate, we partition the prediction into steps and determine if it is relevant by GPT-4o. For reflection quality, we prompt GPT-4o to identify the reflection steps by common indicators and judge the validity of the reflection. The definitions of relevance and validity are included.}
\label{fig:efficiency}
\end{center}
\vskip -0.2in
\end{figure}

\vspace{-1em}
\paragraph{Stability.}
We define the performance difference of the two prompts on the perception tasks $\mathbf{P}$ as the stability score:
\begin{equation}
    \text{Stability} = \text{Acc}^{\mathbf{P}}_{\textsc{cot}} - \text{Acc}^{\mathbf{P}}_{\textsc{dir}}.
\end{equation}
Intuitively, applying the CoT prompt to perception tasks should not degrade performance compared with the direct prompt. Thus, a model with stable CoT should be not less than 0. Otherwise, the model's thinking process demonstrates inconsistency and harm. The overthinking process pushes over the original correct judgment.

\vspace{-1em}
\paragraph{Efficacy.}
Similarly, the performance difference of the two prompts on the reasoning tasks $\mathbf{R}$ is defined as the score:
\begin{equation}
    \text{Efficacy} = \text{Acc}^{\mathbf{R}}_{\textsc{cot}} - \text{Acc}^{\mathbf{R}}_{\textsc{dir}}.
\end{equation}
Intuitively, CoT facilitates stepwise thinking and therefore benefits answering reasoning tasks. The difference reflects how much CoT can enhance reasoning.

\input{tab/main_result}


\subsection{CoT Efficiency Evaluation}
\label{sec2_evaluation_reflection}
% TODO: Pilot Study?
Models like o1 generate extremely long thinking processes with reflection and verification of current steps and outcomes. We perform the first exhaustive analysis of the CoT efficiency of visual reasoning with two carefully designed metrics (Figure~\ref{fig:efficiency}):
\paragraph{Relevance Rate.}
Although the long reasoning content allows for deeper thinking, it may also introduce a large amount of irrelevant information.
As shown in the bottom left of Fig.~\ref{fig:efficiency}, the model has identified the critical element in the image for answering the question, but it still generates a detailed description of other objects.
This irrelevant information provides no helpful information to work out the answer. In the meantime, this extra content slows down the generation speed. Similar to the calculation of precision, we employ the same method to partition the prediction into steps. Then, we instruct GPT-4o to determine all the relevant steps $\mathcal{P}_\text{relevant}$. The step is considered relevant only when the majority of its content works towards solving the question. 
We first compute the raw relevance rate and then apply a scaling factor to amplify the differences between models. Let $r_x$ denote the raw relevance rate:
\begin{align}
r_{\mathcal{C}} = \frac{\left| \mathcal{C}^{\mathcal{P}}_{\text{relevant}} \right|}{\left| \mathcal{C}^{\mathcal{P}} \right|}, \quad
r_{\mathcal{I}} = \frac{\left| \mathcal{I}^{\mathcal{P}}_{\text{relevant}} \right|}{\left| \mathcal{I}^{\mathcal{P}} \right|},
\end{align}
\vspace{-0.6cm}
\begin{align}
r = \frac{\left| {\mathcal{P}}_{\text{relevant}} \right|}{\left| \mathcal{P} \right|}.
\end{align}

Then, the final relevance rate $\text{Relevance Rate}_{x}$ is defined as:
\begin{align}
\text{Relevance Rate}_{x} = \frac{r_x-\alpha}{1-\alpha}, \quad x \in {\mathcal{C}, \mathcal{I}, \emptyset}
\end{align}
where $x = \emptyset$ corresponds to the overall relevance rate, and we take $\alpha$ as $0.8$.



\vspace{-1em}
\paragraph{Reflection Quality.}
The superior reasoning ability could be largely attributed to the reflection and verification process. However, our analysis reveals that not all reflective steps contribute meaningfully to finding correct answers. We identify distinct failure patterns in the reflection process. Some reflective steps mislead the reasoning by introducing new errors or incorrect assumptions, while others are redundant, simply echoing previous conclusions without contributing new insights. To account for failure reflection scenarios, we propose to measure the validity of the reflection. We define a valid reflection as either correctly pointing out the previous mistakes or verifying the previous conclusion with a new insight. Otherwise, the reflection only slows down the reasoning.
To instruct GPT-4o to determine all the valid reflection steps $\mathcal{R}$, we list a set of common indicators of the start of the reflection, such as ``Wait" and ``Alternatively", and illustrate the definition of valid reflection. For all the valid reflection steps $\mathcal{R}_\text{valid}$, the reflection quality is computed as:
\begin{equation}
    \text{Reflection Quality} =  \frac{\left| {\mathcal{R}}_{\text{valid}} \right|}{\left| \mathcal{R} \right|}.
\end{equation}

