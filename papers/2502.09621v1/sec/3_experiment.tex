\section{Experiments}
In this section, we conduct a systematic evaluation of state-of-the-art models on \dataset. We first detail the experiment setup in Section~\ref{sec:exp_steup}. Then in Section~\ref{sec:exp_quantitative}, we report the quantitative results and provide valuable insights derived from our analysis.

\subsection{Experiment Setup}
\label{sec:exp_steup}
\paragraph{Evaluation Models.} 
We select top-performing LMMs for comprehensive CoT evaluation. We test earlier models such as LLaVA-OneVision (7B, 72B)~\cite{li2024llava-ov}, Qwen2-VL (7B, 72B)~\cite{Qwen2-VL}, MiniCPM-V-2.6~\cite{yao2024minicpm}, and InternVL2.5 (8B)~\cite{chen2024expanding}, which are not trained for the reasoning capability. We also include GPT-4o~\cite{openai2024gpt4o} as a strong baseline model.
Besides, we test recent models targeting reasoning, including LLaVA-CoT (11B)~\cite{xu2024llavacot}, Mulberry (8B)~\cite{yao2024mulberry}, InternVL2.5-MPO (8B, 78B)~\cite{wang2024mpo}.
Finally, we evaluate LMMs with reflection capabilities, including both closed-source models like Kimi k1.5~\cite{team2025kimi} and open-source implementations such as QVQ-72B~\cite{qvq-72b-preview} and Virgo-72B~\cite{du2025virgo}.

Note that we sample 150 questions from \dataset to evaluate Kimi k1.5, due to the access limitations. The sample comprises 115 reasoning and 35 perception questions. 

\input{tab/category_result}


\paragraph{Implementation Details.}
We define the CoT prompt as: \textit{Please generate a step-by-step answer, include all your intermediate reasoning process, and provide the final answer at the end.} and the direct prompt as: \textit{Please directly provide the final answer without any other output.}
We only calculate recall of image observation and logical inference on questions where key inference conclusion or image observation exists.
We employ GPT-4o mini for the direct evaluation and GPT-4o for all other criteria. For hyperparameters, we follow the settings in VLMEvalKit~\cite{duan2024vlmevalkit}. 

\subsection{Quantitative Results}
\label{sec:exp_quantitative}
We conduct extensive experiments on various LMMs with our proposed CoT evaluation suite. 
The main results are presented in Table~\ref{table:main_result} and Table~\ref{table:category_result}. We begin by analyzing the overall performance and then highlight key findings.
\paragraph{Overall Results.}
In Table~\ref{table:main_result}, we present
the overall performance of three CoT evaluation perspectives with specific metrics. 
To provide a comprehensive understanding, we report precision, recall, and relevance for both logical inference and image caption steps. For robustness, we provide the direct evaluation result on the perception and reasoning tasks, with either CoT or direct prompt. We employ the average value of the stability and efficacy as the final robustness metric. Notably, we define the reflection quality as 100 on models incapable of reflection.

For CoT quality, Kimi k1.5 achieves the highest F1 score. Open-source models with larger sizes consistently demonstrate better performance, highlighting the scalability of LMMs. Notably, Qwen2-VL-72B outperforms all other open-source models without reflection, even surpassing InternVL2.5-78B-MPO, which is specifically enhanced for reasoning. Analysis reveals that GPT-4o achieves superior performance across all recall metrics, while Kimi k1.5 demonstrates the highest scores in precision evaluations.
For CoT robustness, Mulberry obtains the highest average score. However, when we look into its output, we find it still generates lengthy rationales despite receiving a direct prompt. Even worse, the direct prompt seems to be an out-of-distribution input for Mulberry, 
frequently leading to nonsensical outputs. Further analysis of other models’ predictions reveals that LLaVA-CoT, Virgo, QVQ, and Kimi k1.5 similarly neglect the direct prompt, instead generating extended rationales before answering. Consequently, their robustness scores may be misleading. Once again, GPT-4o achieves the highest robustness score. Among open-source models, only InternVL2.5-MPO, in both its 8B and 78B variants, attains a positive robustness score.
Finally, for CoT efficiency, InternVL2.5-8B obtains the maximum relevance of 98.4\%, suggesting its consistent focus on questions.

Now, we summarize our key observations as follows:
\paragraph{\textit{Models with reflection largely benefit CoT quality.}}
As shown in Table~\ref{table:main_result}, the F1 scores of the two models with reflection capability most closely approach GPT-4o. After specifically fine-tuning for the reasoning capabilities from Qwen2-VL-72B, QVQ surpasses its base model by 5.8\%. Notably, although QVQ generates longer CoT sequences than Qwen2-VL-72B, QVQ's precision still exceeds Qwen2-VL-72B by 2.9\%, indicating superior accuracy in each reasoning step. Kimi k1.5 also surpasses the previous state-of-the-art model GPT-4o, obtaining the highest CoT quality.


\paragraph{\textit{Long CoT does not necessarily cover key steps.}} 
Despite high precision in long CoT models, the informativeness of each step is not guaranteed. We observe that the recall trend among GPT-4o, QVQ, and Virgo does not align with their CoT Rea. performance (i.e., their final answer accuracy on the reasoning tasks under the CoT prompt). Specifically, while both Virgo and QVQ outperform GPT-4o in direct evaluation, they lag behind in recall. This suggests that long CoT models sometimes reach correct answers while skipping intermediate steps, which contradicts the principle of stepwise reasoning and warrants further investigation.

\paragraph{\textit{CoT impairs perception task performance in most models.}}% 比较stability
Surprisingly, most models exhibit negative stability scores, indicating that CoT interferes with perception tasks. The most significant degradation occurs in InternVL2.5-8B, where performance drops by 6.8\%. This reveals inconsistency and potential overthinking in current models, presenting a significant barrier to adopting CoT as the default answering strategy. Among models that provide direct answers, only LLaVA-OV-72B and InternVL2.5-8B-MPO achieve a modest positive score of 0.3\%.

\paragraph{\textit{More parameters enable models to grasp reasoning better.}} 
We find that models with larger parameter counts tend to achieve higher efficacy scores. This pattern is evident across LLaVA-OV, InternVL2.5-MPO, and Qwen2-VL. For instance, while Qwen2-VL-7B shows a 4.8\% decrease in performance when applying CoT to reasoning tasks, its larger counterpart, Qwen2-VL-72B, demonstrates a 2.4\% improvement. This discrepancy suggests that models with more parameters could better grasp the reasoning ability under the same training paradigm. 


\paragraph{\textit{Long CoT models may be more susceptible to distraction.}} 
Long CoT models may demonstrate lower relevance scores compared to other models. They frequently generate content unrelated to solving the given question, corresponding to their relatively low recall scores compared to direct evaluation, like QVQ. Although a few models with short CoT, like Mulberry and LLaVA-OV-7B, also obtain a low relevance rate, we find that it is because these models may keep repeating words when dealing with specific type of questions, resulting in irrelevant judgment. The fine-grained metric reveals that models tend to lose focus when describing images, often producing exhaustive captions regardless of their relevance to the question. From Table~\ref{table:category_result}, we find that this phenomenon prevails in general scenes, space-time, and OCR tasks. This behavior can significantly slow inference by generating substantial irrelevant content. Teaching long CoT models to focus on question-critical elements represents a promising direction for future research.


\paragraph{\textit{Reflection often fails to help.}} 
While reflection is a key feature of long CoT models for answer verification, both QVQ and Virgo achieve reflection quality scores of only about 60\%, indicating that approximately 40\% of reflection attempts fail to contribute meaningfully to answer accuracy. Even for the closed-source model Kimi k1.5, over 25\% reflection steps are also invalid. This substantial failure rate compromises efficiency by potentially introducing unnecessary or distracting steps before reaching correct solutions. Future research should explore methods to reduce these ineffective reflections to improve both efficiency and quality.

\begin{figure}[t]
\begin{center}
\vspace{0.2cm}
\centerline{\includegraphics[width=0.8\columnwidth]{fig/ref_error_pie.pdf}}
\caption{\textbf{Distribution of Reflection Error Types.} We identify four types of error: ineffective reflection, incompleteness, repetition, and interference.}
\label{fig:ref_error_distribution}
\end{center}
\vspace{-0.6cm}
\end{figure}

\subsection{Error Analysis}
\label{sec:exp_analysis}
In this section, we analyze error patterns in the LMM reflection process. An effective reflection should either correct previous mistakes or validate correct conclusions through new insights. We examined 200 model predictions from QVQ and identified four distinct error types that hinder productive reflection. These patterns are illustrated in Fig.~\ref{fig:ref_error_example} and their distribution is shown in Fig.~\ref{fig:ref_error_distribution}.

The four major error types are:

\begin{itemize}
    \item \textbf{Ineffective Reflection.} The model arrives at an incorrect conclusion and, upon reflecting, continues to make incorrect adjustments. This is the most common error type and is also witnessed most frequently.
    \item \textbf{Incompleteness.} The model proposes new analytical approaches but does not execute them, only stopping at the initial thought. The reflection slows down the inference process without bringing any gain.
    \item \textbf{Repetition.} The model restates previous content or methods without introducing new insights, leading to inefficient reasoning.
    \item \textbf{Interference.} The model initially reaches a correct conclusion but, through reflection, introduces errors.
\end{itemize}

Understanding and mitigating these errors is crucial for improving the reliability of LMM reflection mechanisms. The analysis provides the opportunity to focus on solving specific error types to enhance the overall reflection quality.

