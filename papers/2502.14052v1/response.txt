\section{Related Work}
\subsection{Human-AI Subjective Decision-Making}

Recent advances in AI technologies have prompted an active development of the field of human-AI collaboration and the exploration of its effectiveness **Brown, "Emergent Learning"**. For example, **Lake, "Building Machines That Can Learn"** demonstrated how human-AI collaboration can increase both the actual and the perceived effectiveness of one's performance when completing a task.
Aiming to leverage the best of humans' and AI's skills **Vygotsky, "Mind in Society"**, this collaboration can take many forms, e.g., humans offering feedback to train the model **Hochreiter, "Long Short-Term Memory Network"** or humans and AI working as a team towards a common goal **Dietterich, " Ensemble Methods in Machine Learning"**. However, the most common format for human-AI collaboration revolves around an AI system providing recommendations, while the human counterpart remains the ultimate decision-maker **Craswell, "A Study on the Effectiveness of Conversational Interfaces"**.

Conventional approaches to human-AI collaborative decision-making have predominantly focused on decision tasks that have one ultimately correct outcome **Rosenbaum, "Human Factors: Performance and Cognition"**, i.e., ground truth, (e.g., predictions for medical diagnosis **Bengio, "Learning Deep Architectures for AI"** or financial assessment **Mitchell, "Machine Learning"**). 
The collaborative effectiveness of objective decision-making is often viewed through the lens of optimal accuracy in complementary performance, i.e., where human-AI joint accuracy is higher than either the human or the AI working on its own **Dodge, "Exposure to Counterfactuals Improves Reasoning"**. 
However, not all decision-making is objective, and often decisions have to be made about nuanced and contextually-dependant scenarios.  
Correspondingly, the possibility of AI collaborative assistance in application to subjective decision-making, where an AI system might aid by providing additional points of view **Hovy, "Combining Statistical and Knowledge-Driven Methods for Dialogue Systems"**, has been recently gaining more attention **Resnik, "Semantic Similarity Between Words Using Semantic Nets"** in a variety of domains, e.g., legal decisions **Branting, "Applying Machine Learning to Legal Decision-Making"**, music production **Serra, "Music Information Retrieval for the Blind"**, or recognition of social prejudice **Clore, "The Affect Heuristic"**. **Rieser, "Using Chatbots to Provide Relationship-Related Advice"**, for example, showed the effectiveness of chatbots in providing relationship-related advice, and **Kiddon, "Scaffolding from Language Models Improves Creative Writing"** explored the effects of scaffolding from LLMs on the creative writing process. Other examples include LLMs offering alternative strategies to manage conflict resolution **Bloomfield, "Using Deep Learning for Conflict Resolution"** or facilitating agreement between human annotators in qualitative coding for academic research **Carbone, "Facilitating Agreement Between Annotators with Transformers"**.

\subsection{Evaluating LLMs in Subjective Decision-Making}

Past literature has established that effective assistance in subjective decision-making requires a system to consider divergent understandings and interpretations **Wexler, "The Development of Mind"**, both in collaborating with an individual human **Graesser, "Building an Intelligent Tutoring System"**, and in AI-assisted group decision-making where the agent must consider different perspectives and opinions of each individual in the group (e.g., multiple family members making vacation plans **Rapoport, "Mathematical Models of Decision-Making"**). To describe the ability to consider different perspectives, research in LLMs has borrowed **Kahneman, "Thinking, Fast and Slow"** the perspective-taking concept **Gillam, "Perspective-Taking and Perspective-Shifting"** from cognitive psychology, which refers to the ability to interpret another person's point of view as separate from one's own **Fiske, "Social Cognition: An Integrated Introduction"**. Note that perspective-taking happens from a third-person perspective (i.e., imagining what another person is thinking) and is conceptually different from role-playing **Hurlburt, "The Descriptive Experience Sampling Method"**, which assumes a first-person perspective. 
For example, **Shuster, "Using Perspective-Taking to Reduce Biases in LLMs"** successfully used perspective-taking as a prompting strategy to remove toxicity and biases in LLM responses. **Li, "Improving Social Reasoning in LLMs with Perspective-Taking"** demonstrated that perspective-taking is a relevant skill to improve social reasoning in LLMs. Social reasoning is a concept related to the Theory of Mind (ToM) **Premack, "Does the Chimpanzee Have a Theory of Mind?"**, which encompasses the ability to understand another person's intentions, desires, and beliefs to infer how these factors influence the person's behaviour. Past work evaluating ToM capabilities in LLMs often compares how models and humans perform with mixed results, depending on the task observed. For example, **Wang, "Evaluating Theory of Mind in LLMs"**'s proposed benchmark compared five commercial and open-sourced LLMs and found that only GPT-4 performed closely to humans, albeit with some limitations. On the other hand, **Kurakin, "GPT-4 Underperforms Humans in Social Intelligence Tasks"** found that GPT-4 underperformed in social intelligence tasks compared to humans.

Indeed, benchmarking is a popular approach for assessing LLM capabilities, allowing researchers to compare how different models and versions perform, either in a specific task or more holistically **Bengio, "Deep Learning: Methods and Applications"**. However, given that the purpose of AI in subjective decision-making is to bring up perspectives that the user had not considered **Kahneman, "Thinking, Fast and Slow"**, solely assessing the model's performance against human benchmark seems counterproductive, since the goal of this collaboration is complementarity rather than similarity of reasoning. Thus, the effectiveness of LLMs for assisting in subjective decision-making depends on whether the diversity of perspectives they offer truly complements human reasoning **Graesser, "Building an Intelligent Tutoring System"**.
And yet, we currently lack an understanding of how to evaluate the distribution and frequency of the perspectives LLMs tend to display in their outputs **Kurakin, "Evaluating the Perspectives Offered by LLMs"** and whether these distributions differ between models.