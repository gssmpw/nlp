@article{khamassi2024strong,
  title={Strong and weak alignment of large language models with human values},
  author={Khamassi, Mehdi and Nahon, Marceau and Chatila, Raja},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={19399},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{aoyagui2024exploring,
  title={Exploring Subjectivity for more Human-Centric Assessment of Social Biases in Large Language Models},
  author={Aoyagui, Paula Akemi and Ferguson, Sharon and Kuzminykh, Anastasia},
  journal={arXiv preprint arXiv:2405.11048},
  year={2024}
}

@article{10.1145/3687056,
author = {Ferguson, Sharon and Aoyagui, Paula Akemi and Rizvi, Rimsha and Kim, Young-Ho and Kuzminykh, Anastasia},
title = {The Explanation That Hits Home: The Characteristics of Verbal Explanations That Affect Human Perception in Subjective Decision-Making},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3687056},
doi = {10.1145/3687056},
abstract = {Human-AI collaborative decision-making can achieve better outcomes than either party individually. The success of this collaboration can depend on whether the human decision-maker perceives the AI contribution as beneficial to the decision-making process. Beneficial AI explanations are often described as relevant, convincing, and trustworthy. Yet, we know little about the characteristics of explanations that result in these perceptions. Focusing on collaborative subjective decision-making, using the context of subtle sexism, where explanations can surface new interpretations, we conducted a user study (N=20) to explore the structural and content characteristics that affect perceptions of human and AI-generated verbal (text and audio) explanations. We find four groups of characteristics (Tone, Grammatical Elements, Argumentative Sophistication and Relation to User), and that the effect of these characteristics on the perception of explanations for subtle sexism depends on the perceived author. Thus, we also identify which explanation characteristics participants use to identify the author of an explanation. Demonstrating the relationship between these characteristics and explanation perceptions, we present a categorized set of characteristics that system builders can leverage to produce the appropriate perception of an explanation for various sensitive contexts. We also highlight human perception biases and associated issues resulting from these perceptions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {517},
numpages = {37},
keywords = {collaborative decision-making, explainable ai, explanation characteristics, perceptions, subjectivity, verbal explanation}
}


@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@misc{Fermin_2023, 
title={Meet vera: Allvoices’ new AI tool that’ll help with employee relations}, 
url={https://www.allvoices.co/blog/ai-for-employee-relations}, 
journal={AllVoices}, 
author={Fermin, Jeffrey}, 
year={2023}, 
month={Mar}} 

@inproceedings{miller2023explainable,
  title={Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai},
  author={Miller, Tim},
  booktitle={Proceedings of the 2023 ACM conference on fairness, accountability, and transparency},
  pages={333--342},
  year={2023}
}

@article{ma2024beyond,
  title={Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making},
  author={Ma, Shuai and Zhang, Chenyi and Wang, Xinru and Ma, Xiaojuan and Yin, Ming},
  journal={arXiv preprint arXiv:2403.01791},
  year={2024}
}

@inproceedings{danry2023don,
  title={Don’t just tell me, ask me: Ai systems that intelligently frame explanations as questions improve human logical discernment accuracy over causal ai explanations},
  author={Danry, Valdemar and Pataranutaporn, Pat and Mao, Yaoli and Maes, Pattie},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--13},
  year={2023}
}

@article{Sarkarchallenge,
author = {Sarkar, Advait},
title = {AI Should Challenge, Not Obey},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {10},
ISSN = {0001-0782},
url = {https://doi.org/10.1145/3649404},
doi = {10.1145/3649404},
abstract = {Let’s transform our robot secretaries into Socratic gadflies.},
journal = {Commun. ACM},
month = sep,
pages = {18–21},
num pages = {4}
}

@article{sarkar2024large,
  title={Large Language Models Cannot Explain Themselves},
  author={Sarkar, Advait},
  journal={arXiv preprint arXiv:2405.04382},
  year={2024}
}


@inproceedings{eagan2020testing,
  title={Testing the reliability of inter-rater reliability},
  author={Eagan, Brendan and Brohinsky, Jais and Wang, Jingyi and Shaffer, David Williamson},
  booktitle={Proceedings of the Tenth International Conference on Learning Analytics \& Knowledge},
  pages={454--461},
  year={2020}
}

@inproceedings{ha2024clochat,
  title={CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models},
  author={Ha, Juhye and Jeon, Hyeon and Han, Daeun and Seo, Jinwook and Oh, Changhoon},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--24},
  year={2024}
}

@article{daniel2018quality,
  title={Quality control in crowdsourcing: A survey of quality attributes, assessment techniques, and assurance actions},
  author={Daniel, Florian and Kucherbaev, Pavel and Cappiello, Cinzia and Benatallah, Boualem and Allahbakhsh, Mohammad},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={1},
  pages={1--40},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@incollection{nicholls2018collaborative,
  title={Collaborative artificial intelligence in music production},
  author={Nicholls, Steven and Cunningham, Stuart and Picking, Richard},
  booktitle={Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion},
  pages={1--4},
  year={2018}
}

@article{dakuo.et.al,
  author    = {Dakuo Wang and
               Justin D. Weisz and
               Michael J. Muller and
               Parikshit Ram and
               Werner Geyer and
               Casey Dugan and
               Yla R. Tausczik and
               Horst Samulowitz and
               Alexander G. Gray},
  title     = {Human-AI Collaboration in Data Science: Exploring Data Scientists'
               Perceptions of Automated {AI}},
  journal   = {CoRR},
  volume    = {abs/1909.02309},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.02309},
  archivePrefix = {arXiv},
  eprint    = {1909.02309},
  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-02309.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{arous.et.al,
author = {Arous, Ines and Yang, Jie and Khayati, Mourad and Cudr\'{e}-Mauroux, Philippe},
title = {OpenCrowd: A Human-AI Collaborative Approach for Finding Social Influencers via Open-Ended Answers Aggregation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380254},
doi = {10.1145/3366423.3380254},
abstract = {Finding social influencers is a fundamental task in many online applications ranging
from brand marketing to opinion mining. Existing methods heavily rely on the availability
of expert labels, whose collection is usually a laborious process even for domain
experts. Using open-ended questions, crowdsourcing provides a cost-effective way to
find a large number of social influencers in a short time. Individual crowd workers,
however, only possess fragmented knowledge that is often of low quality. To tackle
those issues, we present OpenCrowd, a unified Bayesian framework that seamlessly incorporates
machine learning and crowdsourcing for effectively finding social influencers. To
infer a set of influencers, OpenCrowd bootstraps the learning process using a small
number of expert labels and then jointly learns a feature-based answer quality model
and the reliability of the workers. Model parameters and worker reliability are updated
iteratively, allowing their learning processes to benefit from each other until an
agreement on the quality of the answers is reached. We derive a principled optimization
algorithm based on variational inference with efficient updating rules for learning
OpenCrowd parameters. Experimental results on finding social influencers in different
domains show that our approach substantially improves the state of the art by 11.5%
AUC. Moreover, we empirically show that our approach is particularly useful in finding
micro-influencers, who are very directly engaged with smaller audiences.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1851–1862},
numpages = {12},
keywords = {Variational Inference, Influencer finding, Human-AI Collaboration},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{cai.et.al,
author = {Cai, Carrie J. and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
title = {"Hello AI": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359206},
doi = {10.1145/3359206},
abstract = {Although rapid advances in machine learning have made it increasingly applicable to
expert decision-making, the delivery of accurate algorithmic predictions alone is
insufficient for effective human-AI collaboration. In this work, we investigate the
key types of information medical experts desire when they are first introduced to
a diagnostic AI assistant. In a qualitative lab study, we interviewed 21 pathologists
before, during, and after being presented deep neural network (DNN) predictions for
prostate cancer diagnosis, to learn the types of information that they desired about
the AI assistant. Our findings reveal that, far beyond understanding the local, case-specific
reasoning behind any model decision, clinicians desired upfront information about
basic, global properties of the model, such as its known strengths and limitations,
its subjective point-of-view, and its overall design objective--what it's designed
to be optimized for. Participants compared these information needs to the collaborative
mental models they develop of their medical colleagues when seeking a second opinion:
the medical perspectives and standards that those colleagues embody, and the compatibility
of those perspectives with their own diagnostic patterns. These findings broaden and
enrich discussions surrounding AI transparency for collaborative decision-making,
providing a richer understanding of what experts find important in their introduction
to AI assistants before integrating them into routine practice.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {104},
numpages = {24},
keywords = {machine learning, human-ai interaction, clinical health}
}


@inproceedings{Jacobsen.et.al,
author = {Jacobsen, Rune M\o{}berg and Bysted, Lukas Bj\o{}rn Leer and Johansen, Patrick Skov and Papachristos, Eleftherios and Skov, Mikael B.},
title = {Perceived and Measured Task Effectiveness in Human-AI Collaboration},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383104},
doi = {10.1145/3334480.3383104},
abstract = {Human-AI Collaboration is emerging all around with the increasing utilisation of AI.
Few prior studies have investigated the perceived effectiveness of users solving tasks
with AI. To expand on these, we conducted a within-subjects repeated measures study
involving 35 participants sorting household waste according to recyclability both
with and without the help of an AI system. Our results show that people both sorted
more effectively and perceived themselves more effective. Furthermore, we document
a trend where people sorting without suggestions perceived themselves more effective
than they were, while the opposite was true for people when sorting receiving suggestions.
Based on our results we propose open questions for future research on perceived effectiveness
when collaborating with AI systems.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {artificial intelligence, effectiveness, human-ai collaboration, lab study},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}



@inproceedings{akhtar2020modeling,
  title={Modeling annotator perspective and polarized opinions to improve hate speech detection},
  author={Akhtar, Sohail and Basile, Valerio and Patti, Viviana},
  booktitle={Proceedings of the AAAI conference on human computation and crowdsourcing},
  volume={8},
  pages={151--154},
  year={2020}
}

@article{davani2022dealing,
  title={Dealing with disagreements: Looking beyond the majority vote in subjective annotations},
  author={Davani, Aida Mostafazadeh and D{\'\i}az, Mark and Prabhakaran, Vinodkumar},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={92--110},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{kumarage2024harnessing,
  title={Harnessing artificial intelligence to combat online hate: Exploring the challenges and opportunities of large language models in hate speech detection},
  author={Kumarage, Tharindu and Bhattacharjee, Amrita and Garland, Joshua},
  journal={arXiv preprint arXiv:2403.08035},
  year={2024}
}

@inproceedings{gao2024collabcoder,
  title={CollabCoder: a lower-barrier, rigorous workflow for inductive collaborative qualitative analysis with large language models},
  author={Gao, Jie and Guo, Yuchen and Lim, Gionnieve and Zhang, Tianqin and Zhang, Zheng and Li, Toby Jia-Jun and Perrault, Simon Tangi},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--29},
  year={2024}
}

@inproceedings{delic2024supporting,
  title={Supporting Group Decision-Making: Insights from a Focus Group Study},
  author={Deli{\'c}, Amra and Emamgholizadeh, Hanif and Ricci, Francesco and Masthoff, Judith},
  booktitle={Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
  pages={301--306},
  year={2024}
}

@article{cai2019hello,
  title={" Hello AI": uncovering the onboarding needs of medical practitioners for human-AI collaborative decision-making},
  author={Cai, Carrie J and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
  journal={Proceedings of the ACM on Human-computer Interaction},
  volume={3},
  number={CSCW},
  pages={1--24},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{binns2018s,
  title={'It's Reducing a Human Being to a Percentage' Perceptions of Justice in Algorithmic Decisions},
  author={Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
  booktitle={Proceedings of the 2018 Chi conference on human factors in computing systems},
  pages={1--14},
  year={2018}
}

@inproceedings{wang2024human,
  title={Human-LLM collaborative annotation through effective verification of LLM labels},
  author={Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--21},
  year={2024}
}

@article{tao2023opening,
  title={Opening a Pandora's box: things you should know in the era of custom GPTs},
  author={Tao, Guanhong and Cheng, Siyuan and Zhang, Zhuo and Zhu, Junmin and Shen, Guangyu and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2401.00905},
  year={2023}
}

@article{jarrahi2018artificial,
  title={Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
  author={Jarrahi, Mohammad Hossein},
  journal={Business horizons},
  volume={61},
  number={4},
  pages={577--586},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{disentangling.offensiveness1,
author = {Davani, Aida and D\'{\i}az, Mark and Baker, Dylan and Prabhakaran, Vinodkumar},
title = {Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659021},
doi = {10.1145/3630106.3659021},
abstract = {Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers’ socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators’ judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants’ moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2007–2021},
numpages = {15},
keywords = {Annotation, Offensiveness, Pluralism, Subjectivity, Value Alignment},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{rastogi2022unifying,
  title={A unifying framework for combining complementary strengths of humans and ML toward better predictive decision-making},
  author={Rastogi, Charvi and Leqi, Liu and Holstein, Kenneth and Heidari, Hoda},
  journal={arXiv preprint arXiv:2204.10806},
  year={2022}
}

@inproceedings{michele2022change,
  title={Change my mind: How syntax-based hate speech recognizer can uncover hidden motivations based on different viewpoints},
  author={Michele, Mastromattei and Basile, Valerio and Zanzotto, Fabio Massimo and others},
  booktitle={1st Workshop on Perspectivist Approaches to Disagreement in NLP, NLPerspectives 2022 as part of Language Resources and Evaluation Conference, LREC 2022 Workshop},
  pages={117--125},
  year={2022},
  organization={European Language Resources Association (ELRA)}
}

@article{frenda2024perspectivist,
  title={Perspectivist approaches to natural language processing: a survey},
  author={Frenda, Simona and Abercrombie, Gavin and Basile, Valerio and Pedrani, Alessandro and Panizzon, Raffaella and Cignarella, Alessandra Teresa and Marco, Cristina and Bernardi, Davide},
  journal={Language Resources and Evaluation},
  pages={1--28},
  year={2024},
  publisher={Springer}
}

@inproceedings{wan2023everyone,
  title={Everyone’s voice matters: Quantifying annotation disagreement using demographic information},
  author={Wan, Ruyuan and Kim, Jaehyung and Kang, Dongyeop},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={12},
  pages={14523--14530},
  year={2023}
}

@article{fok2023search,
  title={In search of verifiability: Explanations rarely enable complementary performance in AI-advised decision making},
  author={Fok, Raymond and Weld, Daniel S},
  journal={AI Magazine},
  year={2023},
  publisher={Wiley Online Library}
}

@article{vasconcellos.explanations,
author = {Vasconcelos, Helena and J\"{o}rke, Matthew and Grunde-McLaughlin, Madeleine and Gerstenberg, Tobias and Bernstein, Michael S. and Krishna, Ranjay},
title = {Explanations Can Reduce Overreliance on AI Systems During Decision-Making},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579605},
doi = {10.1145/3579605},
abstract = {Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {129},
numpages = {38},
keywords = {cost-benefit analysis, decision-making, explainable AI, human-AI collaboration}
}



@inproceedings{onlinehate,
author = {Wei, Miranda and Consolvo, Sunny and Kelley, Patrick Gage and Kohno, Tadayoshi and Roesner, Franziska and Thomas, Kurt},
title = {“There’s so much responsibility on users right now:” Expert Advice for Staying Safer From Hate and Harassment},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581229},
doi = {10.1145/3544548.3581229},
abstract = {Online hate and harassment poses a threat to the digital safety of people globally. In light of this risk, there is a need to equip as many people as possible with advice to stay safer online. We interviewed 24 experts to understand what threats and advice internet users should prioritize to prevent or mitigate harm. As part of this, we asked experts to evaluate 45 pieces of existing hate-and-harassment-specific digital-safety advice to understand why they felt advice was viable or not. We find that experts frequently had competing perspectives for which threats and advice they would prioritize. We synthesize sources of disagreement, while also highlighting the primary threats and advice where experts concurred. Our results inform immediate efforts to protect users from online hate and harassment, as well as more expansive socio-technical efforts to establish enduring safety.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {190},
numpages = {17},
keywords = {Security and privacy, advice, harassment, hate},
location = {Hamburg, Germany},
series = {CHI '23}
}


@inproceedings{counterspeakers,
author = {Mun, Jimin and Buerger, Cathy and Liang, Jenny T and Garland, Joshua and Sap, Maarten},
title = {Counterspeakers’ Perspectives: Unveiling Barriers and AI Needs in the Fight against Online Hate},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642025},
doi = {10.1145/3613904.3642025},
abstract = {Counterspeech, i.e., direct responses against hate speech, has become an important tool to address the increasing amount of hate online while avoiding censorship. Although AI has been proposed to help scale up counterspeech efforts, this raises questions of how exactly AI could assist in this process, since counterspeech is a deeply empathetic and agentic process for those involved. In this work, we aim to answer this question, by conducting in-depth interviews with 10 extensively experienced counterspeakers and a large scale public survey with 342 everyday social media users. In participant responses, we identified four main types of barriers and AI needs related to resources, training, impact, and personal harms. However, our results also revealed overarching concerns of authenticity, agency, and functionality in using AI tools for counterspeech. To conclude, we discuss considerations for designing AI assistants that lower counterspeaking barriers without jeopardizing its meaning and purpose.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {742},
numpages = {22},
keywords = {AI-mediated communication, AI-supported counterspeech, counterspeech, hate speech, online activism},
location = {Honolulu, HI, USA},
series = {CHI '24}
}
@article{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{steyvers2023three,
  title={Three challenges for AI-assisted decision-making},
  author={Steyvers, Mark and Kumar, Aakriti},
  journal={Perspectives on Psychological Science},
  pages={17456916231181102},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{bussmann2021explainable,
  title={Explainable machine learning in credit risk management},
  author={Bussmann, Niklas and Giudici, Paolo and Marinelli, Dimitri and Papenbrock, Jochen},
  journal={Computational Economics},
  volume={57},
  number={1},
  pages={203--216},
  year={2021},
  publisher={Springer}
}

@inproceedings{chiang.recidivism,
author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
title = {Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative Recidivism Risk Assessment},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581015},
doi = {10.1145/3544548.3581015},
abstract = {With the prevalence of AI assistance in decision making, a more relevant question to ask than the classical question of “are two heads better than one?’’ is how groups’ behavior and performance in AI-assisted decision making compare with those of individuals’. In this paper, we conduct a case study to compare groups and individuals in human-AI collaborative recidivism risk assessment along six aspects, including decision accuracy and confidence, appropriateness of reliance on AI, understanding of AI, decision-making fairness, and willingness to take accountability. Our results highlight that compared to individuals, groups rely on AI models more regardless of their correctness, but they are more confident when they overturn incorrect AI recommendations. We also find that groups make fairer decisions than individuals according to the accuracy equality criterion, and groups are willing to give AI more credit when they make correct decisions. We conclude by discussing the implications of our work.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {348},
numpages = {18},
keywords = {AI-assisted decision making, Group-AI interaction, Human-AI interaction},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inbook{Schaekerman.et.al,
author = {Schaekermann, Mike and Beaton, Graeme and Sanoubari, Elaheh and Lim, Andrew and Larson, Kate and Law, Edith},
title = {Ambiguity-Aware AI Assistants for Medical Data Analysis},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376506},
abstract = {Artificial intelligence (AI) assistants for clinical decision making show increasing
promise in medicine. However, medical assessments can be contentious, leading to expert
disagreement. This raises the question of how AI assistants should be designed to
handle the classification of ambiguous cases. Our study compared two AI assistants
that provide classification labels for medical time series data along with quantitative
uncertainty estimates: conventional vs. ambiguity-aware. We simulated our ambiguity-aware
AI based on real-world expert discussions to highlight cases likely to lead to expert
disagreement, and to present arguments for conflicting classification choices. Our
results demonstrate that ambiguity-aware AI can alter expert workflows by significantly
increasing the proportion of contentious cases reviewed. We also found that the relevance
of AI-provided arguments (selected from guidelines either randomly or by experts)
affected experts' accuracy at revising AI-suggested labels. Our work contributes a
novel perspective on the design of AI for contentious clinical assessments.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14}
}

@inproceedings{Inkpen.et.al,
author = {Inkpen, Kori and Chancellor, Stevie and De Choudhury, Munmun and Veale, Michael and Baumer, Eric P. S.},
title = {Where is the Human? Bridging the Gap Between AI and HCI},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3299002},
doi = {10.1145/3290607.3299002},
abstract = {In recent years, AI systems have become both more powerful and increasingly promising
for integration in a variety of application areas. Attention has also been called
to the social challenges these systems bring, particularly in how they might fail
or even actively disadvantage marginalised social groups, or how their opacity might
make them difficult to oversee and challenge. In the context of these and other challenges,
the roles of humans working in tandem with these systems will be important, yet the
HCI community has been only a quiet voice in these debates to date. This workshop
aims to catalyse and crystallise an agenda around HCI's engagement with AI systems.
Topics of interest include explainable and explorable AI; documentation and review;
integrating artificial and human intelligence; collaborative decision making; AI/ML
in HCI Design; diverse human roles and relationships in AI systems; and critical views
of AI.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {human computer interaction, machine learning, artificial interlligence},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{hayashi.et.al,
author = {Hayashi, Yugo and Wakabayashi, Kosuke},
title = {Can AI Become Reliable Source to Support Human Decision Making in a Court Scene?},
year = {2017},
isbn = {9781450346887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022198.3026338},
doi = {10.1145/3022198.3026338},
abstract = {Recently, advances in artificial intelligence (AI) have provided support to human
decision-making, and there has been some controversy regarding whether AI can be used
to support judges in court. This study investigates the influence of data that is
presented by a robotic system on human judgment in a sentence estimation situation
in court. The study includes an experiment in which participants played the role of
a juror in a sentence estimation task. Participants were presented with scripts of
the case, reference materials regarding similar cases, and sentences given by the
expert robotic system and human, and then, determined an adequate sentence based on
this information. Results show that participants interacting with the robotic system
tend to agree with the presented material (in the same way as with human experts)
when it is adequate. This result shows that robotic systems are treated the same way
as humans, and that intelligent systems can be a reliable source of information for
human decision-making in court. Moreover, the results show that robotic systems may,
in some cases, be considered more trustworthy than humans.},
booktitle = {Companion of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {195–198},
numpages = {4},
keywords = {human robot interaction, interface design, trust, recommender systems, sentence decision, human experimentation},
location = {Portland, Oregon, USA},
series = {CSCW '17 Companion}
}


@inproceedings{dhillon.creative,
author = {Dhillon, Paramveer S. and Molaei, Somayeh and Li, Jiaqi and Golub, Maximilian and Zheng, Shaochun and Robert, Lionel Peter},
title = {Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642134},
doi = {10.1145/3613904.3642134},
abstract = {Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {1044},
numpages = {18},
keywords = {Generative AI, Human-AI collaboration, co-writing, writing assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}



@inproceedings{zhang.medical,
author = {Zhang, Shao and Yu, Jianing and Xu, Xuhai and Yin, Changchang and Lu, Yuxuan and Yao, Bingsheng and Tory, Melanie and Padilla, Lace M. and Caterino, Jeffrey and Zhang, Ping and Wang, Dakuo},
title = {Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642343},
doi = {10.1145/3613904.3642343},
abstract = {Today’s AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection of sepsis development, visualize the prediction uncertainty, and propose actionable suggestions (i.e., which additional laboratory tests can be collected) to reduce such uncertainty. Through heuristic evaluation with six clinicians using our prototype system, we demonstrate that SepsisLab enables a promising human-AI collaboration paradigm for the future of AI-assisted sepsis diagnosis and other high-stakes medical decision making.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {445},
numpages = {18},
keywords = {Human-AI collaboration, Medical decision making, Sepsis diagnosis},
location = {Honolulu, HI, USA},
series = {CHI '24}
}



@inproceedings{yan.qualitative,
author = {Yan, Lixiang and Echeverria, Vanessa and Fernandez-Nieto, Gloria Milena and Jin, Yueqiao and Swiecki, Zachari and Zhao, Linxuan and Ga\v{s}evi\'{c}, Dragan and Martinez-Maldonado, Roberto},
title = {Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650732},
doi = {10.1145/3613905.3650732},
abstract = {Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers’ perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exploration, offering granular quantitative insights, and assisting comprehension for non-native speakers and non-experts. Yet, concerns about its trustworthiness and accuracy, reliability and consistency, limited contextual understanding, and broader acceptance within the research community persist. We contribute five actionable design recommendations to foster effective human-AI collaboration. These include incorporating transparent explanatory mechanisms, enhancing interface and integration capabilities, prioritising contextual understanding and customisation, embedding human-AI feedback loops and iterative functionality, and strengthening trust through validation mechanisms.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {191},
numpages = {7},
keywords = {ChatGPT, Generative Artificial Intelligence, Human-AI Collaboration, Qualitative Research, Thematic Analysis},
location = {
},
series = {CHI EA '24}
}




@inproceedings{chiang2024devil,
  title={Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate},
  author={Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  booktitle={Proceedings of the 29th International Conference on Intelligent User Interfaces},
  pages={103--119},
  year={2024}
}



@misc{castro-2017-fast-krippendorff,
  author = {Santiago Castro},
  title = {Fast {K}rippendorff: Fast computation of {K}rippendorff's alpha agreement measure},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/pln-fing-udelar/fast-krippendorff}}
}



@inproceedings{de2017people,
  title={How people explain action (and autonomous intelligent systems should too)},
  author={De Graaf, Maartje MA and Malle, Bertram F},
  booktitle={2017 AAAI Fall Symposium Series},
  year={2017}
}



Theory of mind and LLMs
@article{ullman2023largetom,
  title={Large language models fail on trivial alterations to theory-of-mind tasks},
  author={Ullman, Tomer},
  journal={arXiv preprint arXiv:2302.08399},
  year={2023}
}

@article{y2022largetom,
  title={Do large language models understand us?},
  author={y Arcas, Blaise Ag{\"u}era},
  journal={Daedalus},
  volume={151},
  number={2},
  pages={183--197},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{gandhi2024understanding,
  title={Understanding social reasoning in language models with language models},
  author={Gandhi, Kanishk and Fr{\"a}nken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


Cognitive Empathy and LLMs
@article{sorin2023large,
  title={Large language models (llms) and empathy-a systematic review},
  author={Sorin, Vera and Brin, Danna and Barash, Yiftach and Konen, Eli and Charney, Alexander and Nadkarni, Girish and Klang, Eyal},
  journal={medRxiv},
  pages={2023--08},
  year={2023},
  publisher={Cold Spring Harbor Laboratory Press}
}


@article{lee2024enhancing,
  title={Enhancing Empathic Reasoning of Large Language Models Based on Psychotherapy Models for AI-assisted Social Support.},
  author={Lee, Yoon Kyung and Lee, Inju and Shin, Minjung and Bae, Seoyeon and Hahn, Sowon},
  journal={Korean Journal of Cognitive Science},
  volume={35},
  number={1},
  year={2024}
}

@article{wang2024evaluating,
  title={Evaluating and Modeling Social Intelligence: A Comparative Study of Human and AI Capabilities},
  author={Wang, Junqi and Zhang, Chunhui and Li, Jiapeng and Ma, Yuxi and Niu, Lixing and Han, Jiaheng and Peng, Yujia and Zhu, Yixin and Fan, Lifeng},
  journal={arXiv preprint arXiv:2405.11841},
  year={2024}
}



%Perspective Taking
@article{batson1997perspective,
  title={Perspective taking: Imagining how another feels versus imaging how you would feel},
  author={Batson, C Daniel and Early, Shannon and Salvarani, Giovanni},
  journal={Personality and social psychology bulletin},
  volume={23},
  number={7},
  pages={751--758},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{Simon2019EmpathyVE,
  title={Empathy versus evidence: Does perspective-taking for a discrimination claimant bias judgments of institutional sexism?},
  author={Stefanie Simon and Meagan E. Magaldi and Laurie T. O’Brien},
  journal={Group Processes \& Intergroup Relations},
  year={2019},
  volume={22},
  pages={1109 - 1123},
  url={https://api.semanticscholar.org/CorpusID:151102306}
}

@article{xu2024walking,
  title={Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias},
  author={Xu, Rongwu and Zhou, Zi'an and Zhang, Tianwei and Qi, Zehan and Yao, Su and Xu, Ke and Xu, Wei and Qiu, Han},
  journal={arXiv preprint arXiv:2407.15366},
  year={2024}
}

@article{wilf2023think,
  title={Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities},
  author={Wilf, Alex and Lee, Sihyun Shawn and Liang, Paul Pu and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2311.10227},
  year={2023}
}

@article{lu2024role-play,
  title={Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment},
  author={Lu, Keming and Yu, Bowen and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2401.12474},
  year={2024}
}

@inproceedings{plepi2024perspective,
  title={Perspective Taking through Generating Responses to Conflict Situations},
  author={Plepi, Joan and Welch, Charles and Flek, Lucie},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={6482--6497},
  year={2024}
}

@article{sorensen2024roadmap,
  title={A roadmap to pluralistic alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2402.05070},
  year={2024}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{min2020ambigqa,
  title={AmbigQA: Answering ambiguous open-domain questions},
  author={Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2004.10645},
  year={2020}
}

@article{scherrer2024evaluating,
  title={Evaluating the moral beliefs encoded in llms},
  author={Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{healey2018cognitive,
  title={Cognitive and affective perspective-taking: evidence for shared and dissociable anatomical substrates},
  author={Healey, Meghan L and Grossman, Murray},
  journal={Frontiers in neurology},
  volume={9},
  pages={491},
  year={2018},
  publisher={Frontiers Media SA}
}


@article{vowels2024,
  title={Are chatbots the new relationship experts? Insights from three studies},
  author={Vowels, Laura M},
  journal={Computers in Human Behavior: Artificial Humans},
  pages={100077},
  year={2024},
  publisher={Elsevier}
}


@article{hua2024mental.healthcare,
  title={Large language models in mental health care: a scoping review},
  author={Hua, Yining and Liu, Fenglin and Yang, Kailai and Li, Zehan and Sheu, Yi-han and Zhou, Peilin and Moran, Lauren V and Ananiadou, Sophia and Beam, Andrew},
  journal={arXiv preprint arXiv:2401.02984},
  year={2024}
}

@inproceedings{Conflict.resolution,
author = {Shaikh, Omar and Chai, Valentino Emil and Gelfand, Michele and Yang, Diyi and Bernstein, Michael S.},
title = {Rehearsal: Simulating Conflict to Teach Conflict Resolution},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642159},
doi = {10.1145/3613904.3642159},
abstract = {Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill—one that can be learned through deliberate practice—but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual “what if?” scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own setting. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users towards counterfactual conflict resolution strategies that help de-escalate difficult conversations. In a between-subjects evaluation, 40 participants engaged in an actual conflict with a confederate after training. Compared to a control group with lecture material covering the same IRP theory, participants with simulated training from Rehearsal significantly improved their performance in the unaided conflict: they reduced their use of escalating competitive strategies by an average of 67\%, while doubling their use of cooperative strategies. Overall, Rehearsal highlights the potential effectiveness of language models as tools for learning and practicing interpersonal skills.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {920},
numpages = {20},
keywords = {conflict resolution, interests-rights-power, large language models},
location = {Honolulu, HI, USA},
series = {CHI '24}
}


FAccT Paper

Bibliography


Methods

@inproceedings{TurnhoutMixedMethods,
author = {van Turnhout, Koen and Bennis, Arthur and Craenmehr, Sabine and Holwerda, Robert and Jacobs, Marjolein and Niels, Ralph and Zaad, Lambert and Hoppenbrouwers, Stijn and Lenior, Dick and Bakker, Ren\'{e}},
title = {Design Patterns for Mixed-Method Research in HCI},
year = {2014},
isbn = {9781450325424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639189.2639220},
doi = {10.1145/2639189.2639220},
abstract = {In this paper we discuss mixed-method research in HCI. We report on an empirical literature study of the NordiCHI 2012 proceedings which aimed to uncover and describe common mixed-method approaches, and to identify good practices for mixed-methods research in HCI. We present our results as mixed-method research design patterns, which can be used to design, discuss and evaluate mixed-method research. Three dominant patterns are identified and fully described and three additional pattern candidates are proposed. With our pattern descriptions we aim to lay a foundation for a more thoughtful application of, and a stronger discourse about, mixed-method approaches in HCI.},
booktitle = {Proceedings of the 8th Nordic Conference on Human-Computer Interaction: Fun, Fast, Foundational},
pages = {361–370},
numpages = {10},
keywords = {methodology, triangulation, mixed-method research},
location = {Helsinki, Finland},
series = {NordiCHI '14}
}

@incollection{muller2014curiosity,
  title={Curiosity, creativity, and surprise as analytic tools: Grounded theory method},
  author={Muller, Michael},
  booktitle={Ways of Knowing in HCI},
  pages={25--48},
  year={2014},
  publisher={Springer}
}

QUALITATIVE RESEARCH
@article{braun2006using,
  title={Using thematic analysis in psychology},
  author={Braun, Virginia and Clarke, Victoria},
  journal={Qualitative research in psychology},
  volume={3},
  number={2},
  pages={77--101},
  year={2006},
  publisher={Taylor \& Francis}
}

@book{saldana2021coding,
  title={The coding manual for qualitative researchers},
  author={Salda{\~n}a, Johnny},
  year={2021},
  publisher={sage}
}


@article{adams2008qualititative,
  title={A qualitative approach to HCI research},
  author={Adams, Anne and Lunt, Peter and Cairns, Paul},
  year={2008},
  publisher={Cambridge University Press}
}

@article{williams2019art,
  title={The art of coding and thematic exploration in qualitative research},
  author={Williams, Michael and Moser, Tami},
  journal={International management review},
  volume={15},
  number={1},
  pages={45--55},
  year={2019}
}

@book{lazar2017research,
  title={Research methods in human-computer interaction},
  author={Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
  year={2017},
  publisher={Morgan Kaufmann}
}

@book{glaser2017discovery,
  title={Discovery of grounded theory: Strategies for qualitative research},
  author={Glaser, Barney and Strauss, Anselm},
  year={2017},
  publisher={Routledge}
}


==============
Toxicity detection
@article{mishra2023exploring,
  title={Exploring ChatGPT for Toxicity Detection in GitHub},
  author={Mishra, Shyamal and Chatterjee, Preetha},
  journal={arXiv preprint arXiv:2312.13105},
  year={2023}
}

@article{wang2022toxicity,
  title={Toxicity detection with generative prompt-based inference},
  author={Wang, Yau-Shian and Chang, Yingshan},
  journal={arXiv preprint arXiv:2205.12390},
  year={2022}
}

@article{tavarez2024better,
  title={Better Together: LLM and Neural Classification Transformers to Detect Sexism},
  author={Tavarez-Rodr{\'\i}guez, Judith and S{\'a}nchez-Vega, Fernando and Rosales-P{\'e}rez, Alejandro and L{\'o}pez-Monroy, Adri{\'a}n Pastor},
  journal={Working Notes of CLEF},
  year={2024}
}

@article{garg2023handling,
  title={Handling bias in toxic speech detection: A survey},
  author={Garg, Tanmay and Masud, Sarah and Suresh, Tharun and Chakraborty, Tanmoy},
  journal={ACM Computing Surveys},
  volume={55},
  number={13s},
  pages={1--32},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zhang2023efficient,
  title={Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models},
  author={Zhang, Jiang and Wu, Qiong and Xu, Yiming and Cao, Cheng and Du, Zheng and Psounis, Konstantinos},
  journal={arXiv preprint arXiv:2312.08303},
  year={2023}
}

@article{dutta2023modeling,
  title={Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities},
  author={Dutta, Senjuti and Mittal, Sid and Chen, Sherol and Ramachandran, Deepak and Rajakumar, Ravi and Kivlichan, Ian and Mak, Sunny and Butryna, Alena and Paritosh, Praveen},
  journal={arXiv preprint arXiv:2311.00203},
  year={2023}
}
@article{kumar2023watch,
  title={Watch Your Language: Large Language Models and Content Moderation},
  author={Kumar, Deepak and AbuHashem, Yousef and Durumeric, Zakir},
  journal={arXiv preprint arXiv:2309.14517},
  year={2023}
}

@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}

@article{schick2021self,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1408--1424},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

========

@inproceedings{ferguson2023something,
  title={Something Borrowed: Exploring the Influence of AI-Generated Explanation Text on the Composition of Human Explanations},
  author={Ferguson, Sharon A and Aoyagui, Paula Akemi and Kuzminykh, Anastasia},
  booktitle={Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2023}
}

@article{ferguson2024just,
  title={Just Like Me: The Role of Opinions and Personal Experiences in The Perception of Explanations in Subjective Decision-Making},
  author={Ferguson, Sharon and Aoyagui, Paula Akemi and Kim, Young-Ho and Kuzminykh, Anastasia},
  journal={arXiv preprint arXiv:2404.12558},
  year={2024}
}

QUANT

@article{boyd2022development,
  title={The development and psychometric properties of LIWC-22},
  author={Boyd, Ryan L and Ashokkumar, Ashwini and Seraj, Sarah and Pennebaker, James W},
  journal={Austin, TX: University of Texas at Austin},
  pages={1--47},
  year={2022}
}

@article{lai2021towards,
  title={Towards a science of human-ai decision making: a survey of empirical studies},
  author={Lai, Vivian and Chen, Chacha and Liao, Q Vera and Smith-Renner, Alison and Tan, Chenhao},
  journal={arXiv preprint arXiv:2112.11471},
  year={2021}
}




Open AI


@online{temperatureOpenAI,
  author = {OpenAI},
  title = {API Reference},
  year = {2023},
  note = {Last accessed 02 January 2023},
  url = {https://platform.openai.com/docs/api-reference/introduction},
}

@article{GPT-3documentation,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@online{Llama,
    title = {Introducing Llama 3.1: Our most capable models to date},
    author = {Meta},
    year = {2024},
    month = {July},
    date = {23},
    url = {https://ai.meta.com/blog/meta-llama-3-1/}, 
}


@online{GPT-4techreport,
    title = {GPT-4 Technical Report},
    author = {OpenAI},
    year = {2023},
    month = {December},
    date = {19},
    url = {https://arxiv.org/pdf/2303.08774.pdf}, 
}

@online{communityOpenAI2023,
    title = {Does temperature go to 1 or 2?},
    author = {DeM Alex},
    journal = {Community OpenAI},
    year = {2023},
    month = {April},
    url = {https://community.openai.com/t/does-temperature-go-to-1-or-2/174095/2}, 
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI et al.},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}

@inproceedings{davidson2017automated,
  title={Automated hate speech detection and the problem of offensive language},
  author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={11},
  number={1},
  year={2017}
}

@article{corlett2018reflexivity,
  title={Reflexivity and researcher positionality},
  author={Corlett, Sandra and Mavin, Sharon},
  journal={The SAGE handbook of qualitative business and management research methods},
  pages={377--399},
  year={2018},
  publisher={Sage Thousand Oaks, CA}
}

@inproceedings{elisabeth2020hate,
  title={Hate Code Detection in Indonesian Tweets using Machine Learning Approach: A Dataset and Preliminary Study},
  author={Elisabeth, Damayanti and Budi, Indra and Ibrohim, Muhammad Okky},
  booktitle={2020 8th International Conference on Information and Communication Technology (ICoICT)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}


@online{awsbedrock,
    title = {Amazon will offer human benchmarking teams to test AI models},
    author = {Emilia David},
    journal = {The Verge},
    year = {2023},
    month = {November},
    url = {https://www.theverge.com/2023/11/29/23981129/amazon-aws-ai-model-evaluation-bias-toxicity}, 
}

======

Evaluation of bias in LLMs

@misc{liang2023HELM,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang et al.},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gallegos2023bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={arXiv preprint arXiv:2309.00770},
  year={2023}
}

@article{chang2023survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={arXiv preprint arXiv:2307.03109},
  year={2023}
}

@inproceedings{jiang2023empowering,
  title={Empowering Domain Experts to Detect Social Bias in Generative AI with User-Friendly Interfaces},
  author={Jiang, Roy and Kocielnik, Rafal and Saravanan, Adhithya Prakash and Han, Pengrui and Alvarez, R Michael and Anandkumar, Anima},
  booktitle={XAI in Action: Past, Present, and Future Applications},
  year={2023}
}

@misc{kocielnik2023biastestgpt,
      title={BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models}, 
      author={Rafal Kocielnik and Shrimai Prabhumoye and Vivian Zhang and Roy Jiang and R. Michael Alvarez and Anima Anandkumar},
      year={2023},
      eprint={2302.07371},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chen2023chatgpt,
  title={How is ChatGPT's behavior changing over time?},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2307.09009},
  year={2023}
}

@online{narayanan2023gptgettingworse,
  title={Is GPT-4 getting worse over time?},
  author={Narayanan, Arvind and Kapoor, Sayash},
  journal={AI Snake Oil},
  year={2023}
}

@online{narayananquantifyingGPT,
  title={Quantifying ChatGPT’s gender bias},
  author={Narayanan, Arvind and Kapoor, Sayash},
  journal={AI Snake Oil},
  month = {April},
  day = {23},
  year={2023},
  ulr = {https://www.aisnakeoil.com/p/quantifying-chatgpts-gender-bias},
}

@online{narayanancontamination,
  title={GPT-4 and professional benchmarks: the wrong answer to the wrong question},
  author={Narayanan, Arvind and Kapoor, Sayash},
  journal={AI Snake Oil},
  month = {March},
  day = {23},
  year={2023},
  ulr = {https://www.aisnakeoil.com/p/gpt-4-and-professional-benchmarks},
}


@inproceedings{cabello2023independence,
  title={On the Independence of Association Bias and Empirical Fairness in Language Models},
  author={Cabello, Laura and J{\o}rgensen, Anna Katrine and S{\o}gaard, Anders},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={370--378},
  year={2023}
}


@inproceedings{goldfarb-tarrant-etal-2021-intrinsic,
    title = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
    author = "Goldfarb-Tarrant, Seraphina  and
      Marchant, Rebecca  and
      Mu{\~n}oz S{\'a}nchez, Ricardo  and
      Pandya, Mugdha  and
      Lopez, Adam",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.150",
    doi = "10.18653/v1/2021.acl-long.150",
    pages = "1926--1940",
    abstract = "Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.",
}

@article{caliskan2017semantics,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@misc{esiobu2023robbie,
      title={ROBBIE: Robust Bias Evaluation of Large Generative Language Models}, 
      author={David Esiobu and Xiaoqing Tan and Saghar Hosseini and Megan Ung and Yuchen Zhang and Jude Fernandes and Jane Dwivedi-Yu and Eleonora Presani and Adina Williams and Eric Michael Smith},
      year={2023},
      eprint={2311.18140},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kaneko2022debiasing,
  title={Debiasing isn't enough!--On the Effectiveness of Debiasing MLMs and their Social Biases in Downstream Tasks},
  author={Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki},
  journal={arXiv preprint arXiv:2210.02938},
  year={2022}
}

@article{akyurek2022challenges,
  title={Challenges in measuring bias via open-ended language generation},
  author={Aky{\"u}rek, Afra Feyza and Kocyigit, Muhammed Yusuf and Paik, Sejin and Wijaya, Derry},
  journal={arXiv preprint arXiv:2205.11601},
  year={2022}
}

@article{mozafari2020hate,
  title={Hate speech detection and racial bias mitigation in social media based on BERT model},
  author={Mozafari, Marzieh and Farahbakhsh, Reza and Crespi, No{\"e}l},
  journal={PloS one},
  volume={15},
  number={8},
  pages={e0237861},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={862--872},
  year={2021}
}


}
@misc{liang2021understanding,
      title={Towards Understanding and Mitigating Social Biases in Language Models}, 
      author={Paul Pu Liang and Chiyu Wu and Louis-Philippe Morency and Ruslan Salakhutdinov},
      year={2021},
      eprint={2106.13219},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@article{rudinger2018gender,
  title={Gender bias in coreference resolution},
  author={Rudinger, Rachel and Naradowsky, Jason and Leonard, Brian and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1804.09301},
  year={2018}
}


@article{parrish2021bbq,
  title={BBQ: A hand-built bias benchmark for question answering},
  author={Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2110.08193},
  year={2021}
}

@article{selvam2022tail,
  title={The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks},
  author={Selvam, Nikil Roashan and Dev, Sunipa and Khashabi, Daniel and Khot, Tushar and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2210.10040},
  year={2022}
}

inproceedings{blodgett-etal-2021-stereotyping,
    title = "Stereotyping {N}orwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
    author = "Blodgett, Su Lin  and
      Lopez, Gilsinia  and
      Olteanu, Alexandra  and
      Sim, Robert  and
      Wallach, Hanna",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.81",
    doi = "10.18653/v1/2021.acl-long.81",
    pages = "1004--1015",
    abstract = "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system{'}s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{---}originating from the social sciences{---}to inventory a range of pitfalls that threaten these benchmarks{'} validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
}

@article{blodgett2020language,
  title={Language (technology) is power: A critical survey of" bias" in nlp},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  journal={arXiv preprint arXiv:2005.14050},
  year={2020}
}

@article{vicente2023humansinherit,
  title={Humans inherit artificial intelligence biases},
  author={Vicente, Luc{\'\i}a and Matute, Helena},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={15737},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{kotek2023gender,
  title={Gender bias and stereotypes in Large Language Models},
  author={Kotek, Hadas and Dockum, Rikker and Sun, David},
  booktitle={Proceedings of The ACM Collective Intelligence Conference},
  pages={12--24},
  year={2023}
}

@article{bartl2020unmasking,
  title={Unmasking contextual stereotypes: Measuring and mitigating BERT's gender bias},
  author={Bartl, Marion and Nissim, Malvina and Gatt, Albert},
  journal={arXiv preprint arXiv:2010.14534},
  year={2020}
}

@article{zhao2018winobias,
  title={Gender bias in coreference resolution: Evaluation and debiasing methods},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1804.06876},
  year={2018}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@article{rudinger2018gender,
  title={Gender bias in coreference resolution},
  author={Rudinger, Rachel and Naradowsky, Jason and Leonard, Brian and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1804.09301},
  year={2018}
}


@article{sheng2019babysitter,
  title={The woman worked as a babysitter: On biases in language generation},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:1909.01326},
  year={2019}
}

==============================================================================

Hate Speech

@article{mollas2020ethos,
  title={Ethos: an online hate speech detection dataset},
  author={Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios},
  journal={arXiv preprint arXiv:2006.08328},
  year={2020}
}

@article{mathew2020hatexplain,
  title={Hatexplain: A benchmark dataset for explainable hate speech detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  journal={arXiv preprint arXiv:2012.10289},
  year={2020}
}

@inproceedings{waseem-hovy-2016-hateful,
    title = "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on {T}witter",
    author = "Waseem, Zeerak  and
      Hovy, Dirk",
    booktitle = "Proceedings of the {NAACL} Student Research Workshop",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-2013",
    doi = "10.18653/v1/N16-2013",
    pages = "88--93",
}

@inproceedings{waseem2016you,
  title={Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter},
  author={Waseem, Zeerak},
  booktitle={Proceedings of the first workshop on NLP and computational social science},
  pages={138--142},
  year={2016}
}

@article{kiritchenko2018examining,
  title={Examining gender and race bias in two hundred sentiment analysis systems},
  author={Kiritchenko, Svetlana and Mohammad, Saif M},
  journal={arXiv preprint arXiv:1805.04508},
  year={2018}
}

SEXISM

@article{mathew2020hate,
  title={Hate begets hate: A temporal study of hate speech},
  author={Mathew, Binny and Illendula, Anurag and Saha, Punyajoy and Sarkar, Soumya and Goyal, Pawan and Mukherjee, Animesh},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW2},
  pages={1--24},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{felmlee2020sexist,
  title={Sexist slurs: Reinforcing feminine stereotypes online},
  author={Felmlee, Diane and Rodis, Paulina Inara and Zhang, Amy},
  journal={Sex Roles},
  volume={83},
  number={1},
  pages={16--28},
  year={2020},
  publisher={Springer}
}

@article{hammond2018benevolent,
  title={Benevolent sexism and hostile sexism across the ages},
  author={Hammond, Matthew D and Milojev, Petar and Huang, Yanshu and Sibley, Chris G},
  journal={Social Psychological and Personality Science},
  volume={9},
  number={7},
  pages={863--874},
  year={2018},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{mitamura2017value,
  title={Value-based standards guide sexism inferences for self and others},
  author={Mitamura, Chelsea and Erickson, Lynnsey and Devine, Patricia G},
  journal={Journal of Experimental Social Psychology},
  volume={72},
  pages={101--117},
  year={2017},
  publisher={Elsevier}
}

@book{Benokraitis1997,
  title={Subtle Sexism: Current Practice and Prospect for Change},
  author={Hall, Katherine J},
  year={1997},
  publisher={Sage Publications.}
}

@article{barreto2005burden,
  title={The burden of benevolent sexism: How it contributes to the maintenance of gender inequalities},
  author={Barreto, Manuela and Ellemers, Naomi},
  journal={European journal of social psychology},
  volume={35},
  number={5},
  pages={633--642},
  year={2005},
  publisher={Wiley Online Library}
}

@article{swim&cohen1997,
  title={Overt, Covert, And Subtle Sexism: A Comparison Between the Attitudes Toward Women and Modern Sexism Scales},
  author={Swim, Janet K and Cohen, Laurie L},
  journal={Psychology of women quarterly},
  volume={21},
  number={1},
  pages={103--118},
  year={1997},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@book{hall2016they,
  title={" They believe that because they are women, it should be easier for them." Subtle and Overt Sexism toward Women in STEM from Social Media Commentary},
  author={Hall, Katherine J},
  year={2016},
  publisher={Virginia Commonwealth University}
}


SEXISM IN LLM's
@article{Gross_2023, title={What ChatGPT Tells Us about Gender: A Cautionary Tale about Performativity and Gender Biases in AI}, volume={12}, ISSN={2076-0760}, url={http://dx.doi.org/10.3390/socsci12080435}, DOI={10.3390/socsci12080435}, number={8}, journal={Social Sciences}, publisher={MDPI AG}, author={Gross, Nicole}, year={2023}, month=aug, pages={435} 

===

Complementarity
@article{baron2014groundless,
  title={Groundless truth},
  author={Baron, Sam and Miller, Kristie and Norton, James},
  journal={Inquiry},
  volume={57},
  number={2},
  pages={175--195},
  year={2014},
  publisher={Taylor \& Francis}
}

@inproceedings{donahuecomplementarity,
author = {Donahue, Kate and Chouldechova, Alexandra and Kenthapadi, Krishnaram},
title = {Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533221},
doi = {10.1145/3531146.3533221},
abstract = {Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1639–1656},
numpages = {18},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{bansal2021does,
  title={Does the whole exceed its parts? the effect of ai explanations on complementary team performance},
  author={Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2021}
}

@article{hemmer2021human,
  title={Human-AI Complementarity in Hybrid Intelligence Systems: A Structured Literature Review.},
  author={Hemmer, Patrick and Schemmer, Max and V{\"o}ssing, Michael and K{\"u}hl, Niklas},
  journal={PACIS},
  pages={78},
  year={2021}
}

@article{riemer2014looks,
  title={What looks like sexism and why? The effect of comment type and perpetrator type on women's perceptions of sexism},
  author={Riemer, Abigail and Chaudoir, Stephenie and Earnshaw, Valerie},
  journal={The Journal of general psychology},
  volume={141},
  number={3},
  pages={263--279},
  year={2014},
  publisher={Taylor \& Francis}
}


===

Human-AI collaboration Content Moderation

@article{kumar2023watch,
  title={Watch your language: large language models and content moderation},
  author={Kumar, Deepak and AbuHashem, Yousef and Durumeric, Zakir},
  journal={arXiv preprint arXiv:2309.14517},
  year={2023}
}


@article{molina2022ai,
  title={When AI moderates online content: effects of human collaboration and interactive transparency on user trust},
  author={Molina, Maria D and Sundar, S Shyam},
  journal={Journal of Computer-Mediated Communication},
  volume={27},
  number={4},
  pages={zmac010},
  year={2022},
  publisher={Oxford University Press}
}

@inproceedings{lai2022human,
  title={Human-ai collaboration via conditional delegation: A case study of content moderation},
  author={Lai, Vivian and Carton, Samuel and Bhatnagar, Rajat and Liao, Q Vera and Zhang, Yunfeng and Tan, Chenhao},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2022}
}

@inproceedings{cao2020deephate,
  title={DeepHate: Hate speech detection via multi-faceted text representations},
  author={Cao, Rui and Lee, Roy Ka-Wei and Hoang, Tuan-Anh},
  booktitle={Proceedings of the 12th ACM Conference on Web Science},
  pages={11--20},
  year={2020}
}

@article{yin2021towards,
  title={Towards generalisable hate speech detection: a review on obstacles and solutions},
  author={Yin, Wenjie and Zubiaga, Arkaitz},
  journal={PeerJ Computer Science},
  volume={7},
  pages={e598},
  year={2021},
  publisher={PeerJ Inc.}
}

@article{kim2020intersectional,
  title={Intersectional bias in hate speech and abusive language datasets},
  author={Kim, Jae Yeon and Ortiz, Carlos and Nam, Sarah and Santiago, Sarah and Datta, Vivek},
  journal={arXiv preprint arXiv:2005.05921},
  year={2020}
}

BibTeX
MODS XML
Endnote
Preformatted
@inproceedings{diaz-etal-2022-accounting,
    title = "Accounting for Offensive Speech as a Practice of Resistance",
    author = "Diaz, Mark  and
      Amironesei, Razvan  and
      Weidinger, Laura  and
      Gabriel, Iason",
    editor = "Narang, Kanika  and
      Mostafazadeh Davani, Aida  and
      Mathias, Lambert  and
      Vidgen, Bertie  and
      Talat, Zeerak",
    booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.woah-1.18",
    doi = "10.18653/v1/2022.woah-1.18",
    pages = "192--202",
    abstract = "Tasks such as toxicity detection, hate speech detection, and online harassment detection have been developed for identifying interactions involving offensive speech. In this work we articulate the need for a relational understanding of offensiveness to help distinguish denotative offensive speech from offensive speech serving as a mechanism through which marginalized communities resist oppressive social norms. Using examples from the queer community, we argue that evaluations of offensive speech must focus on the impacts of language use. We call this the cynic perspective{--} or a characteristic of language with roots in Cynic philosophy that pertains to employing offensive speech as a practice of resistance. We also explore the degree to which NLP systems may encounter limits to modeling relational context.",
}


@article{oliva2021fighting,
  title={Fighting hate speech, silencing drag queens? artificial intelligence in content moderation and risks to lgbtq voices online},
  author={Oliva, Thiago Dias and Antonialli, Dennys Marcelo and Gomes, Alessandra},
  journal={Sexuality \& Culture},
  volume={25},
  number={2},
  pages={700--732},
  year={2021},
  publisher={Springer}
}

@inproceedings{ovalle2023m,
  title={“I’m fully who I am”: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation},
  author={Ovalle, Anaelia and Goyal, Palash and Dhamala, Jwala and Jaggers, Zachary and Chang, Kai-Wei and Galstyan, Aram and Zemel, Richard and Gupta, Rahul},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1246--1266},
  year={2023}
}


@inproceedings{zhang2020effect,
  title={Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making},
  author={Zhang, Yunfeng and Liao, Q Vera and Bellamy, Rachel KE},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={295--305},
  year={2020}
}

@inproceedings{wischnewski2023measuring,
  title={Measuring and Understanding Trust Calibrations for Automated Systems: A Survey of the State-Of-The-Art and Future Directions},
  author={Wischnewski, Magdalena and Kr{\"a}mer, Nicole and M{\"u}ller, Emmanuel},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--16},
  year={2023}
}

@article{slack2021reliable,
  title={Reliable post hoc explanations: Modeling uncertainty in explainability},
  author={Slack, Dylan and Hilgard, Anna and Singh, Sameer and Lakkaraju, Himabindu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9391--9404},
  year={2021}
}

@article{adadi2018peeking,
  title={Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE access},
  volume={6},
  year={2018},
  publisher={IEEE}
}


@inproceedings{atallah2019heart,
  title={Heart disease detection using machine learning majority voting ensemble method},
  author={Atallah, Rahma and Al-Mousa, Amjed},
  booktitle={2019 2nd international conference on new trends in computing sciences (ictcs)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{gordon2022jury,
  title={Jury learning: Integrating dissenting voices into machine learning models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@inproceedings{wich2020impact,
  title={Impact of politically biased data on hate speech classification},
  author={Wich, Maximilian and Bauer, Jan and Groh, Georg},
  booktitle={Proceedings of the fourth workshop on online abuse and harms},
  pages={54--64},
  year={2020}
}

@inproceedings{al2020identifying,
  title={Identifying and measuring annotator bias based on annotators’ demographic characteristics},
  author={Al Kuwatly, Hala and Wich, Maximilian and Groh, Georg},
  booktitle={Proceedings of the fourth workshop on online abuse and harms},
  pages={184--190},
  year={2020}
}

@inproceedings{lease2011quality,
  title={On quality control and machine learning in crowdsourcing},
  author={Lease, Matthew},
  booktitle={Workshops at the twenty-fifth AAAI conference on artificial intelligence},
  year={2011}
}

@article{mori2012uncanny,
  title={The uncanny valley [from the field]},
  author={Mori, Masahiro and MacDorman, Karl F and Kageki, Norri},
  journal={IEEE Robotics \& automation magazine},
  volume={19},
  number={2},
  pages={98--100},
  year={2012},
  publisher={IEEE}
}

@inproceedings{alfina2017hate,
  title={Hate speech detection in the Indonesian language: A dataset and preliminary study},
  author={Alfina, Ika and Mulia, Rio and Fanany, Mohamad Ivan and Ekanata, Yudo},
  booktitle={2017 international conference on advanced computer science and information systems (ICACSIS)},
  pages={233--238},
  year={2017},
  organization={IEEE}
}

@inproceedings{demus2022comprehensive,
  title={A comprehensive dataset for german offensive language and conversation analysis},
  author={Demus, Christoph and Pitz, Jonas and Sch{\"u}tz, Mina and Probol, Nadine and Siegel, Melanie and Labudde, Dirk},
  booktitle={Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)},
  pages={143--153},
  year={2022}
}

@inproceedings{fortuna-etal-2020-toxic,
    title = "Toxic, Hateful, Offensive or Abusive? What Are We Really Classifying? An Empirical Analysis of Hate Speech Datasets",
    author = "Fortuna, Paula  and
      Soler, Juan  and
      Wanner, Leo",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.838",
    pages = "6786--6794",
    abstract = "The field of the automatic detection of hate speech and related concepts has raised a lot of interest in the last years. Different datasets were annotated and classified by means of applying different machine learning algorithms. However, few efforts were done in order to clarify the applied categories and homogenize different datasets. Our study takes up this demand. We analyze six different publicly available datasets in this field with respect to their similarity and compatibility. We conduct two different experiments. First, we try to make the datasets compatible and represent the dataset classes as Fast Text word vectors analyzing the similarity between different classes in a intra and inter dataset manner. Second, we submit the chosen datasets to the Perspective API Toxicity classifier, achieving different performances depending on the categories and datasets. One of the main conclusions of these experiments is that many different definitions are being used for equivalent concepts, which makes most of the publicly available datasets incompatible. Grounded in our analysis, we provide guidelines for future dataset collection and annotation.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@article{poletto2021resources,
  title={Resources and benchmark corpora for hate speech detection: a systematic review},
  author={Poletto, Fabio and Basile, Valerio and Sanguinetti, Manuela and Bosco, Cristina and Patti, Viviana},
  journal={Language Resources and Evaluation},
  volume={55},
  pages={477--523},
  year={2021},
  publisher={Springer}
}

@inproceedings{fortuna-etal-2019-hierarchically,
    title = "A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset",
    author = "Fortuna, Paula  and
      Rocha da Silva, Jo{\~a}o  and
      Soler-Company, Juan  and
      Wanner, Leo  and
      Nunes, S{\'e}rgio",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3510",
    doi = "10.18653/v1/W19-3510",
    pages = "94--104",
    abstract = "Over the past years, the amount of online offensive speech has been growing steadily. To successfully cope with it, machine learning are applied. However, ML-based techniques require sufficiently large annotated datasets. In the last years, different datasets were published, mainly for English. In this paper, we present a new dataset for Portuguese, which has not been in focus so far. The dataset is composed of 5,668 tweets. For its annotation, we defined two different schemes used by annotators with different levels of expertise. Firstly, non-experts annotated the tweets with binary labels ({`}hate{'} vs. {`}no-hate{'}). Secondly, expert annotators classified the tweets following a fine-grained hierarchical multiple label scheme with 81 hate speech categories in total. The inter-annotator agreement varied from category to category, which reflects the insight that some types of hate speech are more subtle than others and that their detection depends on personal perception. This hierarchical annotation scheme is the main contribution of the presented work, as it facilitates the identification of different types of hate speech and their intersections. To demonstrate the usefulness of our dataset, we carried a baseline classification experiment with pre-trained word embeddings and LSTM on the binary classified data, with a state-of-the-art outcome.",
}

@article{alkomah2022literature,
  title={A literature review of textual hate speech detection methods and datasets},
  author={Alkomah, Fatimah and Ma, Xiaogang},
  journal={Information},
  volume={13},
  number={6},
  pages={273},
  year={2022},
  publisher={MDPI}
}

@inproceedings{madukwe-etal-2020-data,
    title = "In Data We Trust: A Critical Analysis of Hate Speech Detection Datasets",
    author = "Madukwe, Kosisochukwu  and
      Gao, Xiaoying  and
      Xue, Bing",
    editor = "Akiwowo, Seyi  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.18",
    doi = "10.18653/v1/2020.alw-1.18",
    pages = "150--161",
    abstract = "Recently, a few studies have discussed the limitations of datasets collected for the task of detecting hate speech from different viewpoints. We intend to contribute to the conversation by providing a consolidated overview of these issues pertaining to the data that debilitate research in this area. Specifically, we discuss how the varying pre-processing steps and the format for making data publicly available result in highly varying datasets that make an objective comparison between studies difficult and unfair. There is currently no study (to the best of our knowledge) focused on comparing the attributes of existing datasets for hate speech detection, outlining their limitations and recommending approaches for future research. This work intends to fill that gap and become the one-stop shop for information regarding hate speech datasets.",
}

@inproceedings{albadi2018they,
  title={Are they our brothers? analysis and detection of religious hate speech in the arabic twittersphere},
  author={Albadi, Nuha and Kurdi, Maram and Mishra, Shivakant},
  booktitle={2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
  pages={69--76},
  year={2018},
  organization={IEEE}
}

@article{dellermann2021future,
  title={The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems},
  author={Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
  journal={arXiv preprint arXiv:2105.03354},
  year={2021}
}

@article{zhang2021ideal,
  title={" An ideal human" expectations of AI teammates in human-AI teaming},
  author={Zhang, Rui and McNeese, Nathan J and Freeman, Guo and Musick, Geoff},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW3},
  pages={1--25},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kelly2023capturing,
  title={Capturing Humans’ Mental Models of AI: An Item Response Theory Approach},
  author={Kelly, Markelle and Kumar, Aakriti and Smyth, Padhraic and Steyvers, Mark},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1723--1734},
  year={2023}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?��},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@inproceedings{guerdan2023ground,
  title={Ground (less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making},
  author={Guerdan, Luke and Coston, Amanda and Wu, Zhiwei Steven and Holstein, Kenneth},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={688--704},
  year={2023}
}

@inproceedings{fogliato2022goes,
  title={Who goes first? Influences of human-AI workflow on decision making in clinical imaging},
  author={Fogliato, Riccardo and Chappidi, Shreya and Lungren, Matthew and Fisher, Paul and Wilson, Diane and Fitzke, Michael and Parkinson, Mark and Horvitz, Eric and Inkpen, Kori and Nushi, Besmira},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1362--1374},
  year={2022}
}

@article{prabhumoye2021few,
  title={Few-shot instruction prompts for pretrained language models to detect social biases},
  author={Prabhumoye, Shrimai and Kocielnik, Rafal and Shoeybi, Mohammad and Anandkumar, Anima and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2112.07868},
  year={2021}
}

@article{gorwa2020algorithmic,
  title={Algorithmic content moderation: Technical and political challenges in the automation of platform governance},
  author={Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
  journal={Big Data \& Society},
  volume={7},
  number={1},
  pages={2053951719897945},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{meske2023design,
  title={Design principles for user interfaces in AI-Based decision support systems: The case of explainable hate speech detection},
  author={Meske, Christian and Bunde, Enrico},
  journal={Information Systems Frontiers},
  volume={25},
  number={2},
  pages={743--773},
  year={2023},
  publisher={Springer}
}

@article{bunde2021ai,
  title={AI-Assisted and explainable hate speech detection for social media moderators--A design science approach},
  author={Bunde, Enrico},
  year={2021}
}

@inproceedings{gadiraju2023wouldn,
  title={" I wouldn’t say offensive but...": Disability-Centered Perspectives on Large Language Models},
  author={Gadiraju, Vinitha and Kane, Shaun and Dev, Sunipa and Taylor, Alex and Wang, Ding and Denton, Emily and Brewer, Robin},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={205--216},
  year={2023}
}

@article{kwon2023efficacy,
  title={Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online},
  author={Kwon, Taeksoo and Kim, Connor},
  journal={arXiv preprint arXiv:2401.02974},
  year={2023}
}

@inproceedings{lai2022human,
  title={Human-ai collaboration via conditional delegation: A case study of content moderation},
  author={Lai, Vivian and Carton, Samuel and Bhatnagar, Rajat and Liao, Q Vera and Zhang, Yunfeng and Tan, Chenhao},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2022}
}

@misc{field2012discovering,
  title={Discovering Statistics Using R},
  author={Field, A},
  year={2012},
  publisher={Sage}
}