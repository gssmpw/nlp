\section{Related Work}
\subsection{Human-AI Subjective Decision-Making}

Recent advances in AI technologies have prompted an active development of the field of human-AI collaboration and the exploration of its effectiveness \cite{dakuo.et.al,arous.et.al, cai2019hello}. For example, \citet{Jacobsen.et.al} demonstrated how human-AI collaboration can increase both the actual and the perceived effectiveness of one's performance when completing a task.
Aiming to leverage the best of humans' and AI's skills \cite{steyvers2023three,jarrahi2018artificial}, this collaboration can take many forms, e.g., humans offering feedback to train the model \cite{dellermann2021future} or humans and AI working as a team towards a common goal \cite{zhang2021ideal, kelly2023capturing}. However, the most common format for human-AI collaboration revolves around an AI system providing recommendations, while the human counterpart remains the ultimate decision-maker \cite{guerdan2023ground, zhang2020effect, fogliato2022goes}. 

Conventional approaches to human-AI collaborative decision-making have predominantly focused on decision tasks that have one ultimately correct outcome \cite{lai2021towards}, i.e., ground truth, (e.g., predictions for medical diagnosis \cite{cai2019hello} or financial assessment \cite{binns2018s}). 
The collaborative effectiveness of objective decision-making is often viewed through the lens of optimal accuracy in complementary performance, i.e., where human-AI joint accuracy is higher than either the human or the AI working on its own \cite{bansal2021does, hemmer2021human, donahuecomplementarity}. 
However, not all decision-making is objective, and often decisions have to be made about nuanced and contextually-dependant scenarios.  
Correspondingly, the possibility of AI collaborative assistance in application to subjective decision-making, where an AI system might aid by providing additional points of view \cite{hemmer2021human}, has been recently gaining more attention \cite{Schaekerman.et.al,Inkpen.et.al} in a variety of domains, e.g., legal decisions \cite{hayashi.et.al}, music production \cite{nicholls2018collaborative}, or recognition of social prejudice \cite{ferguson2023something}. \citet{vowels2024}, for example, showed the effectiveness of chatbots in providing relationship-related advice, and \citet{dhillon.creative} explored the effects of scaffolding from LLMs on the creative writing process. Other examples include LLMs offering alternative strategies to manage conflict resolution \cite{Conflict.resolution} or facilitating agreement between human annotators in qualitative coding for academic research \cite{gao2024collabcoder}.

\subsection{Evaluating LLMs in Subjective Decision-Making}

Past literature has established that effective assistance in subjective decision-making requires a system to consider divergent understandings and interpretations \cite{frenda2024perspectivist, sorensen2024roadmap}, both in collaborating with an individual human \cite{ferguson2023something}, and in AI-assisted group decision-making where the agent must consider different perspectives and opinions of each individual in the group (e.g., multiple family members making vacation plans \cite{delic2024supporting}). To describe the ability to consider different perspectives, research in LLMs has borrowed \cite{xu2024walking,wilf2023think} the perspective-taking concept \cite{batson1997perspective} from cognitive psychology, which refers to the ability to interpret another person's point of view as separate from one's own \cite{healey2018cognitive, batson1997perspective}. Note that perspective-taking happens from a third-person perspective (i.e., imagining what another person is thinking) and is conceptually different from role-playing \cite{lu2024role-play} or personas \cite{ha2024clochat}, which assumes a first-person perspective. 
For example, \citet{xu2024walking} successfully used perspective-taking as a prompting strategy to remove toxicity and biases in LLM responses. \citet{wilf2023think} demonstrated that perspective-taking is a relevant skill to improve social reasoning in LLMs. Social reasoning is a concept related to the Theory of Mind (ToM) \cite{ullman2023largetom, y2022largetom}, which encompasses the ability to understand another person's intentions, desires, and beliefs to infer how these factors influence the person's behaviour. Past work evaluating ToM capabilities in LLMs often compares how models and humans perform with mixed results, depending on the task observed. For example, \citet{gandhi2024understanding}'s proposed benchmark compared five commercial and open-sourced LLMs and found that only GPT-4 performed closely to humans, albeit with some limitations. On the other hand, \citet{wang2024evaluating} found that GPT-4 underperformed in social intelligence tasks compared to humans.

Indeed, benchmarking is a popular approach for assessing LLM capabilities, allowing researchers to compare how different models and versions perform, either in a specific task or more holistically \cite{liang2023HELM}. However, given that the purpose of AI in subjective decision-making is to bring up perspectives that the user had not considered \cite{lai2021towards, ferguson2024just}, solely assessing the model's performance against human benchmark seems counterproductive, since the goal of this collaboration is complementarity rather than similarity of reasoning. Thus, the effectiveness of LLMs for assisting in subjective decision-making depends on whether the diversity of perspectives they offer truly complements human reasoning \cite{ferguson2024just, ferguson2023something}.
And yet, we currently lack an understanding of how to evaluate the distribution and frequency of the perspectives LLMs tend to display in their outputs \cite{frenda2024perspectivist} and whether these distributions differ between models.