\section{Introduction} \label{sec: introduction}

On-device machine learning has emerged as an important paradigm for tasks requiring low latency and high privacy~\cite{dhar2021survey}. 
Applications like Apple's CoreML~\cite{coreml} and Huawei's MindSpore~\cite{mindspore} enable user personalization on local devices. 
This trend has also extended to Graph Neural Networks (GNNs)~\cite{zeng2022gnn, shao2021branchy}, ensuring the privacy of user data during inference for tasks such as community detection~\cite{shchur2019overlapping}, e-commerce personalization~\cite{zhu2019aligraph}, and recommender systems~\cite{wu2022graph}. 
However, local GNN inference grants users significant privileges to local models and data, introducing additional security vulnerabilities~\cite{xue2021dnn}.

There are two primary security threats to inference on local devices: model intellectual property (IP) theft and data privacy breach. 
Valuable model IP includes the hyper-parameters, trained parameters, and model's functionality~\cite{gongye2024side}. 
Data privacy issues involve attacks such as membership inference~\cite{shokri2017membership} and link stealing attack~\cite{he2021stealing}, which can exploit the white-box accessibility of local models.
Existing defenses, including  watermarking \cite{guo2018watermarking}, non-transferable learning \cite{wang2021non}, and homomorphic encryption~\cite{xie2019bayhenn}, are often passive, inaccurate, or computation-expensive. 
Recently, studies have begun leveraging Trusted Execution Environments (TEEs) to protect local models and data~\cite{zhang2024no, liu2023mirrornet}, by partitioning them into slices and deploying critical components within secure compartments.


However, current TEE-based secure deployment mainly targets datasets for tasks such as computer vision and large language models, which is not applicable to GNN inference. 
Specifically, GNN operations require updating the graph node representations (i.e., embeddings) based on information from their neighboring data points. 
Thus, \textit{the entire graph data, including node features and their connectivity (adjacency matrix), has to be stored locally during inference}, posing additional challenges for data privacy. 
Moreover, TEEs have limited memory capacities, while large graphs require significant storage for node features. 
To bridge this gap, we propose a new method for local GNN deployment, \mymethod, which effectively preserves both critical model parameters and the most important structural information (i.e., edges) using TEE.

\mymethod employs the ``partition-before-training'' strategy~\cite{zhang2024no}, where the GNN model is designed with a public backbone and a private GNN rectifier. Specifically, the public backbone is computation-intensive but inaccurate, which is trained with a substitute graph and deployed in the untrusted environment. 
The GNN rectifier leverages the real private graph structure to rectify the backbone's node embeddings. It is designed to be much smaller and resides within the TEE. 
Our experiments show that \mymethod achieves performance comparable to the original unprotected GNN while preventing information leakage of private parameters and edges. 
We also evaluate our design on a real system using Intel SGX, demonstrating the low overhead of \mymethod. 

\textbf{Our Contributions:} We propose \mymethod, the first secure deployment framework designed specifically for graph neural networks. Our work makes the following contributions:

\begin{enumerate}[leftmargin=*]
\item We propose various communication schemes between public backbones and private rectifiers in \mymethod, and evaluate the performance and implementation cost of rectifiers.
\item Software evaluation demonstrates that \mymethod achieves a high inference accuracy with a small rectifier inside TEE while protecting critical model parameters and edge data. 
\item We implement a real-world deployment using Intel SGX to run GNNs locally. Experimental results show that the introduction of the enclave increases a small inference overhead and negligible accuracy degradation (less than $2\%$), demonstrating the effectiveness of \mymethod. 
\item We conduct a security analysis with link stealing attacks on the intermediate data communicated from the public backbone to the rectifier, showing that no private edge information is leaked in the untrusted environment.

\end{enumerate}
