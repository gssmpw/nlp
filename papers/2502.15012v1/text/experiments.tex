\section{Experimental  Validation} \label{sec: experiment}

\subsection{Experiment Setup} \label{exp: setup}

\noindent\textbf{Datasets:} 
\begin{table}[h]
    \centering
    \vspace{-5mm}
    \caption{Dataset Used in \mymethod Validation}
    \label{tab: dataset}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
    \hline
    \rowcolor{lightgray}
    Dataset  &  $\#$ Node  & $\#$ Edge & $\#$ Feature & $\#$ Class  & Dense $A$ (MB) \\
    \hline
    Cora~\cite{yang2016revisiting}     &  2,708  &  10,556  & 1,433 & 7 & 167.85 \\
    \hline 
    Citeseer~\cite{yang2016revisiting}     &  3,327  &  9,104  & 3,703 & 6 & 253.35 \\
    \hline
    Pubmed~\cite{yang2016revisiting}  & 19,717 & 88,648 & 500 & 3 & 8898.01\\
    \hline
    Computer~\cite{shchur2018pitfalls}    & 13,752  &491,722  &767 &  10 & 4328.56 \\
    \hline
    Photo~\cite{shchur2018pitfalls} &7,650  & 238,162  & 745  & 8 & 1339.47 \\
    \hline
    CoraFull~\cite{bojchevski2017deep}  &  19,793 &126,842  &8,710 & 70 & 8966.74 \\
    \hline
    \end{tabular}
    }
\end{table}
We focus on semi-supervised learning tasks for node classification with GNNs on multiple datasets at different scales, as shown in Table~\ref{tab: dataset}.
Specifically, we evaluate \mymethod on three standard datasets in~\cite{yang2016revisiting}, two larger graphs \textit{Amazon Computer} and \textit{Photo} in~\cite{shchur2018pitfalls}, and \textit{CoraFull}~\cite{bojchevski2017deep} which has many classes ($70$).
Table~\ref{tab: dataset} shows the memory requirement for a dense adjacency matrix.
To train the GNN, we follow the common practice: using $20$ labeled node per class~\cite{shchur2018pitfalls}, and the unlabeled nodes are used as the testing set.
If not specified, we use KNN with $k$=$2$ as substitute graphs in backbones, which are chosen based on the ablation study in~\ref{exp: ablation}.

\yf{here rather than giving the specific channel size, it is better to present design rationale, such as the input channel size is determined by the node feature dimension? is the output channel size determined by anything or you just freely choose it?}\rd{revised}
\noindent\textbf{Models:}
We evaluate \mymethod on three different GNN structures based on dataset features space size:
$\mathcal{M}_1$ has as a 3-layer GCN backbone with output channel size as $(128, 32, \mathcal{C})$ and a rectifier $(128, 32, \mathcal{C})$, which is designed for a smaller dataset including Cora, Citeseer, Pubmed, where $\mathcal{C}$ is the label space size.
$\mathcal{M}_2$ has wider output channels ($256$) for both the backbone and the rectifier, which is used for the case when the model has a high-class number, such as CoraFull.
We also test a larger and deeper design $\mathcal{M}_3$ with a backbone $(256, 64, 32, 16, \mathcal{C})$ and rectifier $(64, 32, \mathcal{C})$. 
Different rectifiers may have various input channel sizes, as shown in Fig.~\ref{fig: designs}.

\noindent\textbf{Metrics:}
We first compute the accuracy of the original GNN model, which has the same architecture as the backbone but uses the real adjacency matrix and therefore has different parameters, denoted as $p_{org}$.
The difference between the classification accuracy of the public backbone in the normal world, denoted as $p_{bb}$, and the accuracy of the GNN rectifier, denoted as $p_{rec}$, measures the \textit{protection performance}. 
We also compare the size of the public backbone and private rectifier in terms of the number of model parameters, denoted as $\theta_{bb}$, and $\theta_{rec}$.
To show accuracy degradation, we utilize the difference between $p_{rec}$ and $p_{org}$ and the silhouette score to show the node's clustering performance~\cite{shahapure2020cluster}.

\noindent\textbf{Platform:}
We run the training procedure and the software evaluation process on Ubuntu 18.04.6 with NVIDIA TITAN RTX.
We also follow a similar setting to~\cite{zhang2024no} as a real-world case study on an SGX-enabled Intel Core i7-7700, running Ubuntu 20.04.6 and utilizing Intel SGX SDK and PSW.


\subsection{Software-based Validation} \label{exp: software}
\begin{table*}[t]
    \centering
        \caption{\mymethod Performance with KNN  graph ($k=2$), \textbf{Bold indicates the best rectifier}; 
    \underline{Underline is the second best}.}
    \label{tab: performance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l||c|c|c||c|c|c||c|c|c||c|c|c}
    \hline
    \rowcolor{lightgray}
    & \multicolumn{3}{c||}{Original / Backbone}  & \multicolumn{3}{c||}{Parallel \mymethod}  & \multicolumn{3}{c||}{Series \mymethod}  & \multicolumn{3}{c}{Cascaded \mymethod}\\
    \hline
    Dataset       & $p_{org}$(\%) & $\theta_{bb}$ (M) & $p_{bb}$ (\%) & $p_{rec}$ (\%)& $\Delta p$ (\%)& $\theta_{rec}$ (M)& $p_{rec}$ (\%) &  $\Delta p$ (\%) & $\theta_{rec}$ (M) & $p_{rec}$ (\%) & $\Delta p$ (\%) & $\theta_{rec}$ (M)\\
    \hline
    \hline
    Cora~\cite{yang2016revisiting}  &   80.4 &  0.188 &  60.2   &   \textbf{78.8}     &  18.6  &  0.022        &   \underline{78.2}    &   18.0    &    0.0088      &   77.6    &    17.4    &    0.027      \\
    Citeseer~\cite{yang2016revisiting}  & 65.2 & 0.479 &  60.3   &   \textbf{70.1}    &  9.80  &  0.022        &    68.7   &    8.40    &    0.0087      &   \underline{69.0}    &    8.70    &    0.026      \\
    Pubmed~\cite{yang2016revisiting}  &   77.1 & 0.068  &   66.6    &   \textbf{75.2}    &  8.60  &  0.022         &   \underline{75.1}    &    8.50    &    0.0085      &   73.6    &    7.00   &    0.025      \\
    Computer~\cite{shchur2018pitfalls}  &  75.5  & 0.216 &   56.6   &   \underline{77.6}     &  21.0 &  0.021        &   \textbf{78.2}   &    21.6    &    0.0039     &   77.4    &    20.8   &    0.027      \\
    Photo~\cite{shchur2018pitfalls}  &  83.7   & 0.210 &  68.3    &   \underline{84.9}     &  16.6 &  0.021            &   84.2    &   15.9    &    0.0037     &   \textbf{85.1}   &   16.8    &    0.026     \\
    CoraFull~\cite{bojchevski2017deep}  &   59.5 & 2.27  &  43.1    &   \underline{57.8}     &  14.7  &  0.051          &   \textbf{58.0}    &   14.9     &    0.050     &   55.8    &    12.7    &    0.060      \\
    
    \hline
    \end{tabular}
}
\end{table*}
\begin{table}[t]
\vspace{-3mm}
    \centering
    \caption{Compare various backbone designs}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l||c|c||c|c||c|c||c|c}
    \hline
    \rowcolor{lightgray}
    &     \multicolumn{2}{c||}{DNN} & \multicolumn{2}{c||}{random} & \multicolumn{2}{c||}{cosine} & \multicolumn{2}{c}{KNN} \\
    \hline
    Dataset     & $p_{bb}$ & $p_{rec}$ & $p_{bb}$ & $p_{rec}$ & $p_{bb}$ & $p_{rec}$ & $p_{bb}$ & $p_{rec}$\\
    \hline
    Cora    & {54.4} & 76.8 & 17.2 & 51.5 & 55.3 & \textbf{79.1} & 60.2 & \underline{78.8} \\
    Citeseer & 53.9 & \underline{64.6} & 18.9 & 38.3 & {46.2}  & 64.3 & 66.6 & \textbf{70.1} \\
    Pubmed    & 71.9 & 73.9 & 34.5 & 52.1 & 72.1 & \textbf{76.0} & {66.6} & \underline{75.2}\\
    Computer & 52.6 & 73.6 & 7.16 & 28.9 & {44.6} & \underline{76.7} &56.6 & \textbf{77.6} \\
    Photo & {64.3} & 83.4 & 30.4 & 52.8 & 69.1 & \textbf{84.9} & 68.3 & \underline{84.9}\\
    CoraFull & 43.9 & \underline{57.7} & 2.69 & 27.3 & {40.1} & 55.6 & 43.1 & \textbf{57.8} \\
    \hline
     \end{tabular}
}
\vspace{-2mm}
    \label{tab: backbone}
\end{table}

\subsubsection{Evaluation of \mymethod Performance}
\label{exp: performance}
The performance of GNN deployment using TEE on a local device can be evaluated using the following attributes: 1. Accuracy degradation ($p_{\text{org}}$ - $p_{\text{rec}}$) (lower is better); 2. Protection performance $\Delta p = p_{\text{rec}} - p_{\text{bb}}$ (higher is better); 3. The model size inside the enclave $\theta_{rec}$ (smaller is better).
In Table~\ref{tab: performance}, we present the performance of \mymethod on three rectifier designs, following the experimental setups described in Section~\ref{exp: setup}. Among the three rectifier designs, the parallel rectifier achieves %
the highest accuracy on three datasets and ranks second-best in others, and it also offers the highest protection. 
The cascaded rectifier has the largest model size but shows lower accuracy, as the input GCN layer may not handle all complex inputs together well.
The series rectifier has the smallest size inside the TEE, as it only requires the final node embedding from the backbone as input, and yet it still achieves good performance overall. 

\subsubsection{Evaluation of Different Backbones}
\label{exp: backbones}
\mymethod's backbones show low accuracy when using the substitute graph, indicating that our design effectively prevents malicious attackers from accessing any high-precision models in untrusted environments.
We then compare the performance of \mymethod using different types of backbones: the DNN backbone (an MLP using only node features), and the GNN backbones with three different ways to generate the substitute graph: random  (the graph is randomly generated), cosine (the graph is generated based on cosine similarity), and KNN (the graph is generated using the KNN algorithm). For the GNN-based backbones, we sample the graph density to match that of the original graph. Among all four types of backbones, the cosine and KNN methods demonstrate the best performance, while the GNN with a random graph shows the lowest accuracy. Specifically, the random graph incorporates too much misinformation, degrading the embedding performance of the backbone model and making rectification difficult. Moreover, compared with the DNN model, the GNN backbones using cosine similarity and KNN substitute graphs enable the backbone not only to extract information from the node features but to gather additional information from similar nodes via message passing. This additional information enhances the rectification process when using the correct adjacency information.

\subsubsection{Rectifier Interpretation}

\begin{figure}[t]
\vspace{-3mm}
    \centering
    \includegraphics[width=\linewidth]{imgs/visualizations.pdf}
    \caption{\textbf{Visualization of the node embeddings.} We visualize the embedding latent space for parallel \mymethod on Cora dataset using the same structure for the original GNN, backbone and rectifier as \textit{$\mathcal{M}_2$}. The line chart shows the Silhouette Score of the clustering performance\textbf{(the higher the better).}}
    \label{fig: visualizations}
    \vspace{-2mm}
\end{figure}

To illustrate the rectification process, we visualize the node latent space of a parallel \mymethod applied to the Cora dataset, which contains 7 classes. In Fig.~\ref{fig: visualizations}, we illustrate how the latent space changes layer-by-layer between the backbone and the rectifier using t-SNE~\cite{van2008visualizing}. The rectifier layer significantly improves the clustering performance by gradually incorporating the real adjacency matrix, ultimately approaching the original model's performance. Additionally, we plot the silhouette score of each layer's embeddings to numerically assess their clustering performance. The results show that the rectifier's scores (\textcolor{green!40}{green} line) become close to those of the original GNN (\textcolor{gray}{gray} line), while the backbone model's scores (\textcolor{orange!40}{orange} line) remain low.


\subsubsection{Ablation Study}\label{exp: ablation}
We evaluate the impact of different hyperparameters on \mymethod's performance, as shown in Fig.~\ref{fig: ablations}. Specifically, we vary the number of neighbors in the KNN-based substitute graph, adjust the cosine similarity threshold for the cosine similarity graph, and change the percentage of random edges (relative to the number of real edges) for the random substitute graph.
For the KNN method, the performance remains stable because the algorithm consistently connects similar nodes based on their features. Increasing $k$ primarily affects the density of the substitute graph without significantly impacting performance. We select $k$=$2$ as the number of edges for it is close to the real graph for most of the datasets. In contrast, selecting a low cosine similarity threshold for the cosine similarity graph can connect unrelated nodes, which adversely affects \mymethod's performance (when the cosine similarity threshold is $\leq 0.2$). As we increase the number of random edges in the substitute graph, the performance of both the backbone and the rectifier decreases as more structural noise is introduced. Notably, when we use an extremely small number of edges, random \mymethod's accuracy approaches that of the DNN backbone.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/ablations.pdf}
    \caption{Impact of hyperparameters of different substitute graphs}
    \label{fig: ablations}
    \vspace{-2mm}
\end{figure}



\subsection{Real-world Implementation} \label{exp: implementation}

\subsubsection{Implementation Details} 
\rd{talk about the SGX implementation of SGX}
We implemented GNNVault on a desktop PC equipped with an Intel Core i7-7700 (3.60GHz) processor. The public backbone in the untrusted environment is developed using PyTorch 2.4.1, while the rectifier is implemented in C++ with Intel SGX SDK 2.25. 
To optimize the computation within the rectifier, we leveraged the Eigen linear algebra library to efficiently handle matrix operations, achieving approximately 20\% speed up for large models~\cite{eigen}. 
\yf{rephrase the last line}\rd{I remove the last line, I think no need to mention what implementation our code is based on.} 

\subsubsection{Inference Overhead and Memory Usage}
The inference time of \mymethod comprises of the backbone execution time, data transfer time, and the rectifier execution time. 
In Fig.~\ref{fig: execution time} (top), we present a breakdown of the inference time, for three rectifier designs and different GNN models: $\mathcal{M}_1$ (Cora), $\mathcal{M}_2$ (CoraFull), and $\mathcal{M}_3$ (Amazon Computer). The parallel and cascaded rectifiers transfer all intermediate node embeddings from the normal world to the secure world, resulting in higher latency due to data transfer and execution inside the TEE. The series rectifier exhibits the smallest overhead since it only requires the last layer's embedding, leading to an overhead of approximately $52\%$ to $131\%$ compared with running an unprotected GNN with CPU. Note that the backbone can be accelerated using GPUs to further increase the inference speed.

\rd{talk about the memory usage, and point out if it applicable to put the entire GNN inside the TEE.}

We also present the enclave runtime memory usage required for different GNNVault configurations, as shown in Fig.~\ref{fig: execution time} (bottom). As mentioned before, the memory allocated in the trusted enclave is limited by SGX's PRM and EPC.
Therefore, it is essential to %
ensure that the rectifier operates effectively within the enclave's secure boundaries and does not exceed the recommended limits.
The enclave memory usage is primarily for each layer’s input features, adjacency matrix, and model parameters. Notably, the highest memory usage across all models and rectifier designs remains at only 41.6MB, well within the 96MB EPC threshold. This confirms the feasibility of running these configurations securely within the enclave. In contrast, the backbone’s runtime memory usage for the three models significantly exceeds 128MB, underscoring the impracticality of executing a complete GNN model within the enclave due to memory constraints.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/memory.pdf}
    \caption{\mymethod Inference Time Breakdown and Memory Usage}
    \label{fig: execution time}

\end{figure}



\subsection{Security Analysis using Link Stealing Attacks}
\begin{table}[ht]
    \vspace{-3mm}
    \centering
    \caption{Link Stealing Attack Performance on \mymethod}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l||c|c|c||c|c|c||c|c|c}
    \hline
    \rowcolor{lightgray}
      
     Dataset &  $\mathcal{M}_{org}$   & $\mathcal{M}_{gv}$  &  $\mathcal{M}_{base}$ &  $\mathcal{M}_{org}$   & $\mathcal{M}_{gv}$  &  $\mathcal{M}_{base}$ &  $\mathcal{M}_{org}$   & $\mathcal{M}_{gv}$  &  $\mathcal{M}_{base}$ \\
    \hline 
    & \multicolumn{3}{c||}{Euclidean}  & \multicolumn{3}{c||}{Correlation} & \multicolumn{3}{c}{Cosine} \\
    \hline 
     Cora  & 0.844  &  \textbf{0.702} & 0.715 & 0.903 &\textbf{0.735} & 0.720 & 0.972 & \textbf{0.765 } & 0.754  \\
    Citeseer  & 0.915  &  \textbf{0.750} & 0.731 & 0.912 & \textbf{0.778} & 0.752 & 0.987 & \textbf{0.807} & 0.790 \\
    \hline
    & \multicolumn{3}{c||}{Chebyshev}  & \multicolumn{3}{c||}{Braycurtis} & \multicolumn{3}{c}{Canberra} \\
    \hline 
     Cora  & 0.847  &  \textbf{0.661} & 0.691 & 0.902 &\textbf{0.696} & 0.693 & 0.933 & \textbf{0.741} & 0.717  \\
    Citeseer  & 0.908  &  \textbf{0.711} & 0.698 & 0.953 & \textbf{0.751} & 0.732 & 0.976 & \textbf{0.785} & 0.746 \\

    \hline
    \end{tabular}
    }
    \label{tab: sla attack}
\end{table}

Link stealing attacks~\cite{he2021stealing, ding2023vertexserum} aim to infer the adjacency matrix by exploiting the similarity between node embeddings, based on the assumption that GNN layers produce more similar embeddings for connected nodes than for unconnected ones. \mymethod isolates the private graph within the secure world and ensures one-directional communication from the normal world to the secure world. To verify the effectiveness of this isolation, we conduct link stealing attacks using six different similarity metrics and present the attack ROC-AUC scores in Table~\ref{tab: sla attack} using all intermediate embeddings. A higher AUC score indicates greater leakage of private graph information. We use the attack performance on a DNN model with pure node features as the baseline ($\mathcal{M}_{\text{base}}$). The unprotected GNN ($\mathcal{M}_{org}$) shows very high AUC scores using all similarity metrics on both datasets, but with our \mymethod ($\mathcal{M}_{gv}$), the attack performance decreases to the level of the $\mathcal{M}_{base}$.
\yf{what does gv stand for?} \rd{revised}


