\section{Background} \label{sec: background}
\subsection{Graph Neural Network} \label{bg: gnn}
GNNs process graph-structured data, which contains relational information between nodes (or edges).
Without loss of generality, we follow the common GNN definition ~\cite{xu2018powerful}:
a graph $G=(V, E)$ consists of the node set $V$ with $n$ nodes, each with a feature represented by a $d$-dimensional vector, denoted as $X\in \mathbb{R}^{n\times d}$.
The set of edges $E$ is depicted by an adjacency matrix $A\in \{0, 1\}^{n\times n}$, indicating the existence of connections between nodes in $G$.
A GNN model is a function $f(X, A)$ to summarize global information in the graph for downstream tasks including node classification~\cite{zhou2019meta}, community detection~\cite{shchur2019overlapping}, and recommender systems~\cite{wu2019session}.
The forward propagation of GNN layer $k$ can be written as 

{\footnotesize
\begin{equation}
    H^{(k)} = \sigma(\hat{A}H^{(k-1)}W^{(k)})
\end{equation}}where $H^{(k)}\in \mathbb{R}^{N\times d^{(k)}}$ is the matrix of node embedding at layer $k$, $\hat{A}$ is the adjacency matrix with self-loops normalized with degree matrix, $W^{(k)}\in \mathbb{R}^{d^{(k-1)}\times d^{(k)}}$ is the learnable weight matrix, and $\sigma$ is an activation function. 
Note that GNN operations require $\hat{A}$ through message passing during the inference process, which causes edge data privacy concerns. 

\subsection{Model Deployment in Trusted Execution Environments} \label{bg: deployment}
A TEE, such as Intel SGX or ARM TrustZone, protects code and data within it, ensuring their confidentiality and integrity with hardware support against strong adversaries including a malicious host operating system.
The increasing demand for edge AI applications, e.g., those that require real-time responses or rely on local data or private data, has led to a trend of using TEEs for IP protection and privacy preservation.
Previous works focus on securely deploying DNNs by isolating the non-linear layers~\cite{sun2023shadownet} or by partitioning the model and placing the final dense layers inside the TEE \cite{mo2020darknetz}.
Recently, Zhang et.al~\cite{zhang2024no} also consider the privacy of training data and propose a ``partition-before-training'' strategy against membership inference attack~\cite{shokri2017membership}. 
Additionally, Liu et al.~\cite{liu2023mirrornet} design a deployment strategy that only allows single-directional data flow from the untrusted environment to the enclave. 
However, all previous works only focus on local DNN's IP and data privacy, which are not applicable to the secure deployment of a GNN on the edge. 
Our proposed method, \mymethod, bridges this gap by protecting the security of trained GNNs and private edge data using TEEs.
