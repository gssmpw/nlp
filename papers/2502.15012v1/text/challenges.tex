\section{Problem Statement} \label{sec: statement}


\subsection{Deploying GNN locally Causes Vulnerabilities} \label{ps: gnn inference}
Deploying GNNs on local devices requires access to graph data in addition to the trained model, which introduces unique security and privacy challenges. 
Similar to DNN deployment, the IP of the well-performed local model, including its trained weights and biases, is valuable asset that must be protected against model extraction attacks.
Beyond the model IP, local GNN inference raises additional privacy concerns due to the nature of GNN architecture. 
Specifically, during the message-passing phase of GNN inference, target nodes aggregate information from neighboring nodes to update their embeddings. 
This process involves accessing sensitive edge data, such as user-product interactions in recommender systems.
In our work, we will address the GNN IP infringement and edge data breach vulnerability during GNN deployment.

\subsection{Edge Privacy is Valuable} \label{motivation: edge importance}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/scenario.pdf}
    \caption{\textbf{Motivation Example:} Alice (victim) builds a graph of products and trains a GNN RS. She deploys both edge data and RS on local devices. Bob (attacker) accesses this device and steals the edge data and model parameters.}
    \label{fig: problem-statement}
\end{figure}

Membership inference attack is the most common data privacy threat to machine learning models~\cite{shokri2017membership}, where the goal is to determine whether a given data point belongs to the training set. 
However, in the context of GNNs, edge data raises additional privacy concerns. 
Link stealing attacks~\cite{he2021stealing, ding2023vertexserum} aim to infer the connectivity between any pair of given nodes. 
In this work, we focus on the adjacency information (edges), while considering the node features as public.
A real-world example is illustrated in Fig.~\ref{fig: problem-statement}, where Alice (victim) deploys a recommender system (RS) on local edge devices. 
In such a product graph, the node features are public attributes of the products—such as price, user reviews, or categories—that are available to any user. 
However, the internal relationships between products require intensive learning from user behavior data, which is valuable IP for the model vendor. 
Therefore, safeguarding the node connectivity information during GNN local inference is of great importance.


\subsection{TEE Has Memory Restrictions} \label{ps: TEE}
The introduction of TEE greatly enhances data security and privacy with secure compartments. 
However, TEE platforms face significant memory limitations, a critical constraint for secure computation. 
For instance, for Intel SGX trusted enclaves, the physical reserved memory (PRM) is limited to 128MB, with 96 MB of it allocated to the Enclave Page Cache (EPC)~\cite{intel2017sgx}. 
Excessive memory allocation will lead to frequent page swapping between the unprotected main memory and the protected enclave, which can cause high overhead and additional encryption/decryption to ensure data integrity~\cite{costan2016intel}.
This memory constraint poses a significant challenge for deploying GNN models and the entire graph (including node features and adjacency information) within the secure enclave, which often far exceed the PRM limitation of enclaves.

