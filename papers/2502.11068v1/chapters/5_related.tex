\section{Related Work}

Our work is related to model-agnostic explanation techniques and approaches to improving their efficiency.

Model-agnostic explanation techniques aim to explain the predictions of machine learning models without requiring access to their internal workings, treating them as black boxes. The prominent model-agnostic methods includes Local Interpretable Model-agnostic Explanations (LIME)~\cite{LIME}, SHapley Additive exPlanations (SHAP)~\cite{SHAP}, Partial Dependence Plots (PDP)~\cite{PDP}, IndividualConditional Expectation (ICE) plots~\cite{ICE}, LORE~\cite{lore} and Anchors~\cite{Anchor}. Among them, SHAP and Anchors exhibit outstanding performance in terms of fidelity and understandability, but their computational efficiency is extremely low, which is a significant drawback~\cite{Compare}.

There have already been some efforts to accelerate SHAP. TreeSHAP\cite{SHAP} accelerates SHAP computation for tree-based models (e.g., decision trees, XGBoost) by leveraging tree structures, reducing complexity to polynomial time, making it both fast and exact. However, TreeSHAP can only accelerate the SHAP computation for tree-based models, which greatly restricts the application scope of SHAP. Accelerated Marginal Effects (AcME)~\cite{ACME} is a model-agnostic method that approximates SHAP values, offering faster, scalable computations for any model type. But this method can only be applied to tabular datasets and is not suitable for text or image datasets.

In addition, to our knowledge, there is currently no effective acceleration method for Anchors.