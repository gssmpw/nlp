\section{Experiment}

% 3 main question to answer: 

% 1. How much is the efficiency improved compared to the original Anchors? How much sampling is reduced? (may be important in scenarios where sampling is expensive). For users, can our method solve the problem of low efficiency of Anchors? (Can it effectively speed up the case with high time cost)

% 2. Can the results output by our method have similar fidelity as Anchors?

In this section, we evaluate the performance of our accelerated Anchors explanation method compared to the original Anchors algorithm. The experiments were conducted on three types of data—tabular, text, and image—across various datasets. We conducted experiments on multiple machine learning models to demonstrate the effectiveness of our method in efficiency optimization. 
In addition, we compared the fidelity
%and understandability
of our method with the original Anchors results to show that our method still maintains the quality of Anchors.

\subsection{Experiment Setup}

For tabular data, we selected revenue forecasting as the target task; for text data, we selected sentiment analysis as the target task; for image data, we selected image classification as the target task.

In our experiments across all tasks, we set the parameter \(\tau\) to 0.8 during the pre-training phase and 0.95 during the refinement phase. Given that the parameter \(N\) plays a critical role in influencing optimization performance, we conducted comparative experiments using various values of \(N\). 
%For tabular data, we set \(\Delta = 0.05\) and restricted transformation to constraints within the same feature type. For text data, \(\Delta\) was set to 0.2, and for image data, \(\Delta\) was set to 0.5. The values of \(\Delta\) represent the optimal settings determined through extensive experimentation.

\textbf{Income Prediction.} The income prediction models take the numerical values of a person's multiple features (such as age, education level, race, etc.) as input and outputs whether their annual income will exceed \$50k or not, i.e. $f:X \rightarrow{\{0,1\}}$, where $X:=\bigcup_{i=0}^{i\leq k}F_i$ is the input domain, and $k$ represents the number of features, $F_i$ represents the value of i-th feature. We used the random forest(RF), gradient boosted trees(GBT) and a 3-layers neural network(NN) as models to explain. We used these models to predict the data of 12,345 individuals from the Adult dataset ~\cite{adult}, and explained the local behavior of the models around each input item in tabular.

\textbf{Sentiment Analysis.} Sentiment analysis models take a text sequence as input and predict if the text is positive or negative, i.e. $f: X \rightarrow \{0, 1\}$, where $X:= \bigcup_{i=1}^\infty W^i$ is the input domain, and $W$ is the vocabulary set. We used the random forest (RF) and the 7B version of Llama2.0 (Llama) as the models to explain. We used these models to predict 12,520 comments from the RT-Polarity dataset ~\cite{rt-polarity}, and explained the local behavior of the models around each input text.

\textbf{Image Classification.} Image classification models take an image as input and predict the category of the image, i.e. $f: X \rightarrow \{0, 1, ..., m\}$, where $m$ is the number of categories, $X:= R^{3\times h\times w}$ is the input domain, with $h$ and $w$ being the height and width of the image.

We used a pre-trained YOLOv8 to predict the category of 500 images from a partial dataset of ImageNet ~\cite{imagenet} and explained the local behavior of the models around each input image.

\subsection{Efficiency Improvement}

\subsubsection{Evaluation Metrics}

To quantify the performance of our accelerated Anchors method, we measured:

\textbf{Time Acceleration Rate}: The time acceleration ratio of our method compared to the original Anchors, calculated as (Time taken by the original Anchors/ Time taken by our method).

\textbf{Sampling Reduction Ratio}: The sampling reduction ratio of our method compared to the original Anchors is calculated as (1 - Sampling count of our method / Sampling count of the original Anchors). The difference between this metric and the Time Acceleration Ratio is that it can, to some extent, eliminate the impact of the computational overhead of the model being explained, thereby providing a more specific reflection of the acceleration effect of our method.

\subsubsection{Evaluation Results}

\begin{table*}[h]
    \small
    \centering
    \caption{Average acceleration effect of \textbf{Income Prediction}.}
    \label{tab:res_tabular}
    \begin{tabular}{ccccccc}
        \toprule 
        &Models to explain & N=100 & N=200 & N=500 & N=1000 & N=2000\\
        \midrule
        \multirow{3}*{Time Acceleration Ratio}&RF & 158\% & 172\% & 175\% & 221\% & 271\%\\
        &NN & 154\% & 181\% & 183\% & 189\% & 198\%\\
        &GBT & 153\% & 156\% & 149\% & 167\% & 185\%\\
        \midrule
        \multirow{3}*{Sampling Reduction Ratio}&RF & 41\% & 45\% & 47\% & 55\% & 64\%\\
        &NN & 34\% & 40\% & 42\% & 51\% & 53\%\\        
        &GBT & 34\% & 36\% & 31\% & 42\% & 46\%\\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h]
    \small
    \centering
    \caption{Average acceleration effect of \textbf{Sentiment Analysis}.}
    \label{tab:res_text}
    \begin{tabular}{ccccccc}
        \toprule 
        &Models to explain & N=100 & N=200 & N=500 & N=1000 & N=2000\\
        \midrule
        \multirow{2}*{Time Acceleration Ratio}&RF & 105\% & 113\% & 123\% & 158\% & 221\%\\
        &Llama & 117\% & 136\% & 144\% & 158\% & 169\%\\
        \midrule
        \multirow{2}*{Sampling Reduction Ratio}&RF & 6\% & 14\% & 20\% & 40\%& 57\%\\
        &Llama & 13\% & 29\% & 34\% & 39\% & 42\%\\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h]
    \small
    \centering
    \caption{Average acceleration effect of \textbf{Image Classification}.}
    \label{tab:res_image}
    \begin{tabular}{ccccccc}
        \toprule 
        &Models to explain & N=10 & N=20 & N=50 & N=100 & N=200\\
        \midrule
        Time Acceleration Ratio&YOLOv8 & 115\% & 138\% & 156\% & 163\% & 181\%\\
        \midrule
        Sampling Reduction Ratio&YOLOv8 & 12\% & 29\% & 34\% & 42\% & 47\%\\
        \bottomrule
    \end{tabular}
\end{table*}

Table~\ref{tab:res_tabular}, Table~\ref{tab:res_text} and Table~\ref{tab:res_image} shows the average time acceleration ratio and sampling reduction ratio in the income prediction, sentiment analysis and image classification tasks with different number of explanations generated during the pre-training \(N\). 

For the income prediction task, our approach achieved the highest average acceleration effect across all three models. When \( N = 2000 \), the acceleration ratios for RF, NN, and GBT models were 271\%, 198\%, and 185\%, respectively. Additionally, the proportion of reduced sampling counts was significant, reaching 64\%, 53\%, and 46\% for the three models. 
%
For more complex tasks, such as sentiment analysis and image classification, our average acceleration effect remained effective even with a limited number of pre-training inputs \(N\). Although the number of pre-training inputs for the sentiment analysis task is the same (\(N=2000\)), the optimization effect is better for tabular data. The reason is, compared to tabular data, text data is of higher dimensions after embedding. As a result, when the number of pre-training inputs is the same, it is more difficult to identify a sufficiently similar pre-training input during the horizontal transformation process, which ultimately leads to poorer optimization performance.
%
When \( N = 2000 \), the acceleration ratio for the RF in the sentiment analysis task was 221\%, while for the Llama, it was 169\%. The difference in acceleration effects is due to the Llama having a more complex decision boundary compared to the RF, which requires more samples to accurately measure its decision boundary, resulting in more vertical transformation iterations. In the image classification task with \( N = 200 \), the acceleration ratio for the YOLOv8 model was 161\%. 

Furthermore, across all three tasks, it was observed that as the number of pretraining inputs \( N \) increased, the optimization effect continued to improve. This aligns with our expectations: as the number of pre-training inputs increases, more rules can be generated during horizontal transfer, thereby reducing the iterations required for sampling in vertical transfer and ultimately leading to improved optimization performance. 
%
For inputs with higher time overhead, our method demonstrates more significant optimization effects. However, for certain cases, such as inputs with indices 4, 11, and 20, the optimization effect is quite limited due to insufficient pre-training, which results in fewer rules being generated during horizontal transformation.

\begin{figure*}[t]
\centering
\includegraphics[width=1.2\columnwidth]{figures/output.png} 
\caption{The absolute time cost comparison before and after acceleration.(Sentiment Analysis)}
\label{fig:abstime}
\end{figure*}

Additionally, Figure~\ref{fig:abstime} presents a comparison of the absolute runtime between Anchors and our approach. To better demonstrate the acceleration capability of our approach, we selected the sentiment classification task with the Llama model, which incurs the highest time overhead in Anchors, for comparison. We randomly selected 20 inputs from the dataset for testing and calculated the average time required to generate explanations 10 times for each input. The results were then sorted in ascending order of runtime in Anchors.
%

In summary, our method achieves acceleration ratios of 271\%, 221\%, and 181\% across various tasks on tabular, text, and image datasets, respectively. Moreover, the optimization effect improves progressively as the number of pre-training inputs \(N\) increases. Additionally, compared to cases with shorter runtime, our method demonstrates even better acceleration performance in cases with longer runtime.

\begin{figure}[]
\centering
\includegraphics[width=1.0\linewidth]{figures/data_tabular_pc1.png} 
\caption{The fidelity of our method and Anchors.(Income Prediction)}
\label{fig:fidelity}
\end{figure}

\subsection{Fidelity Evaluation}

\subsubsection{Evaluation Metrics}

\textbf{Coverage}: A parameter in Anchors used to evaluate fidelity, representing the proportion of samples that satisfy the constraints of the explanation.

\textbf{Precision}: A parameter in Anchors used to evaluate fidelity, representing the percentage of samples that it covers yielding the same output as the input to explain.

\subsubsection{Evaluation Results}

Figure~\ref{fig:fidelity} illustrates the fidelity of the explanations generated by our method. For this experiment, we selected the income prediction task, which has the largest dataset. For precision, our method ensures that the generated explanations maintain the same level of accuracy as Anchors. To achieve this, new predicates are iteratively added to the rule until the precision meets the threshold \(\tau\). As a result, the precision of our method is nearly identical to that of Anchors.

Besides, our method demonstrates a slight decline in coverage compared to Anchors. This is because the rules derived through horizontal transfer in our approach are not always the ones with maximum coverage that still meet the precision requirement, which may lead to the inclusion of additional predicates. However, this issue can be mitigated by increasing the number of pre-training inputs (\(N\)). As the number of pre-training inputs increases, more similar pre-training inputs can be identified during the horizontal transfer process. Consequently, when transferring rules from pre-training inputs to online inputs, the reduction in coverage diminishes, ultimately resulting in improved coverage in the final outcomes.
