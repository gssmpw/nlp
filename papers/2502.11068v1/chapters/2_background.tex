\section{Background}

This chapter provides the necessary background knowledge that is integral to understanding our approach. We first describe the form of Anchors' explanation and then its underlying algorithm.
% Initially, we will define the symbolic representation of Anchors, followed by an explanation of the specific algorithmic process and framework. We will then discuss key algorithms and models essential for implementing the framework used in this study.

\subsection{Representation of Anchors' Explanations}

Before diving into the symbolic representation of Anchors' explanation, we introduce several definitions.

The target model $f$ is defined as a function $f:\mathbb{X}\rightarrow \mathbb{Y}$, where $\mathbb{X}$ includes a variety of data types such as text, tabular, and images, collectively known as the input domain. $\mathbb{Y}$ represents the model's predicted outcome. Our discussion will focus on classification tasks, meaning $\mathbb{Y}$ refers to the categories' labels. In this context, the target model can be considered a ``black box", producing outputs (i.e., prediction results) from given inputs via an unknown process. Given the target model $f$ and an instance $x \in \mathbb{X}$, the aim of Anchors is to explain the rationale behind $f(x)$ to a user, where $f(x)$ is the individual prediction for the instance $x$.

An instance $x$ is composed of several features, formally expressed as $x = \boldsymbol{(x_1, x_2, \dots, x_n)}$. Here, $n$ represents the number of features of $x$, denoted as $n_x$, and for all $i \in \{1, 2, \dots, n_x\}$, $x_i$ is the $i$-th feature of input $x$.

Let $R$ be a rule consisting of one or more predicates, where each predicate corresponds to a constraint on a feature. The rule $R(x)=1$ if the input $x$ satisfies all predicates in $R$. In this case, we say that the rule $R$ covers the input $x$.

% todo: present some example here

Similar to the cases of many other local model-agnostic explanation techniques, a perturbation model 
% $t_{per}$ 
is used to obtain the local decision boundary of the target model by sampling inputs that are similar to the target input. Let $D_x(\cdot)$ be the distribution of inputs obtained via the perturbation model and the $D_x(\cdot|R)$ be the distribution  conditioning on the perturbed input satisfying the rule $R$.
% based on rule $R$. That is, $D_x(\cdot|R)$ contains multiple variants of instance $x$. For every variant, the features of instance $x$ present in $R$ will be kept, and others will be changed or deleted (may also be kept).

% todo: present some example here

Next, we define the two most important performance metrics of Anchors as follows:

\begin{itemize}
    \item \textbf{Precision}: For a rule $R$ generated by Anchors based on the input $x$ and the model to be explained $f$, its \emph{precision} is defined as: 
\[ Precision(R) = E_{(D_x(z|R))}[ 1_{f(x)=f(z)}] \]
    \item \textbf{Coverage}: For a rule $R$ generated by Anchors based on the input $x$ and the model to be explained $f$, its \emph{coverage} is defined as: 
\[ Coverage(R) = E_{(D_x(z))}[R\ covers\ z] \]
\end{itemize}

Precision and coverage are the main metrics of Anchors to measure the fidelity and faithfulness of the generated explanations. Next, we formally define the input and the output of Anchors.

The input of Anchors includes the model to be explained $f$, the input $x$, as well as a precision threshold $\tau$ and a probability threshold $\delta$.
We treat $\delta$ as a hyperparameter and omit it in the rest of the paper.
The output of Anchors is a rule, which is a set of predicates. Formally:
$$Anchors(f,x,\tau) = \{p_{a_1},p_{a_2},\dots,p_{a_m}\}:=R_x,m\leq n$$
$$\forall i \in \{1,2,\dots,m\},a_i\in \{1,2,\dots,n\} ,and\ \forall i\neq j,a_i\neq a_j,$$
where $p_{a_i}$ is a predicate constraining the $a_i$th feature.

\label{def:tau}
Anchors aims to generate a rule that has a precision not less than $\tau$ with a probability not less than $\delta$, while maximizing the coverage. Formally:
$$\begin{array}{c}maximize \quad coverage(R_x) \\ s.t.\quad P(Precision(R_x)\geq \tau) \geq 1-\delta
\end{array}$$

% % the generated rule satisfy the precision constraint with a high probability:
% $$P(Precision(R_x)\geq \tau) \geq 1-\delta$$
% If multiple $R_x$ meet this constraint, those that cover more instance is preferred. Formally, among the $R_x$ that meet the conditions, the one with the highest $Coverage(R_x)$ will be selected as the output of Anchors.
% $$R_x,s.t. max(coverage(R_x))$$
\subsection{The Algorithm of Anchors}

Now, we will present the algorithm of Anchors, since our method involves algorithmic changes to Anchors.

\begin{figure*}[t]
\centering
\includegraphics[width=1.5\columnwidth]{figures/pipe.png} 
\caption{The overall workflow of Anchors.}
\label{pip1}
\end{figure*}

As shown in Figure~\ref{pip1}, from the input to generate the output rule, Anchors mainly consists of the following four steps:

\begin{enumerate}
    \item It generates a set of available predicates $\mathbb{P}$ based on the input $x$.

    \item It adds each predicate $p$ from the predicates set $\mathbb{P}$ that has not yet appeared in a rule named $R_0$ (which initially contains no predicates) to form the candidate rule set $\mathbb{R}$. It Uses the KL-LUCB algorithm ~\cite{KL-LUCB} to determine the sample number $M$, and feeds $\mathbb{R^*}$, $M$, and $x$ into the perturbation model to generate the neighborhood $D_x$ of the input $x$. Then it calculates \emph{coverage} and \emph{precision} of all the rules in rule set $\mathbb{R}$ using $D_x$.

    \item If the precision of any rule is no less than $\tau$, it returns a rule that has the maximum \emph{coverage} among all rules satisfying the precision requirement. Otherwise, it sets $R_0$ to be the rule with the highest precision and continues to Step 2.

    % \item Based on the sampling results of the current neighborhood $D_x$, selecting the rule from the rule set $\mathbb{R}$ with the highest \emph{precision} as the $R$ for next iteration. Repeat the second step.

\end{enumerate}

It can be seen that after each iteration, one predicate is added to the rule $R$. As mentioned earlier, with the increase in the number of predicates in $R$, the number of inputs it can cover will decrease, while the probability that the prediction results of the covered inputs are the same as the original input's will increase. In other words, during the iteration process, the \emph{coverage} will gradually decrease while the \emph{precision} will gradually increase, and the rule $R$ will gradually transform from a ``general" explanation to a ``specific" explanation for a particular input.

% Before continuing the explanation, a brief introduction to the KL-LUCB \cite{KL-LUCB} algorithm is necessary. This algorithm is used to solve the multi-armed bandit problem, combining the ideas of confidence interval algorithms (such as UCB) and Kullback-Leibler (KL) divergence. In the multi-armed bandit problem, there are multiple potential choices (called arms), and the valuation of each arm is randomly sampled from an unknown distribution. The goal is to find the arm with the highest valuation as quickly as possible while minimizing sampling costs, i.e., the arm with the optimal average sampling result.

% The KL-LUCB algorithm calculates the uncertainty of each arm and decides which arm to explore or exploit based on this uncertainty. It uses KL divergence to measure the difference between two probability distributions, providing a more accurate uncertainty estimation. Compared to traditional confidence interval algorithms, KL-LUCB can explore arms more efficiently in some cases and achieve better performance in a shorter time. The core idea of the KL-LUCB algorithm is to optimize arm selection by minimizing KL divergence based on historical observation data, balancing the trade-off between exploration and exploitation.

% In this method, the KL-LUCB algorithm continuously calculates the number of times the perturbation model is called based on the valuation results of the two rule sets with the highest current valuations until the valuation confidence intervals of the highest and second-highest rule sets do not overlap, thus obtaining the highest-valued result with high probability. Specifically, the input and output of a single calculation in the Anchors method are as follows:

% $$KL-LUCB_1(precision(R^{*}\cup x_i),precision(R^{*}\cup x_j)) = N$$
% where
% $$x_i,x_j\notin R^{*}$$
% and
% $$\forall k \in\{1,2,...,n_x\}, k\neq i, k\neq j, x_k\notin R^{*}$$
% $$precision(R^{*}\cup x_k)\leq precision(R^{*}\cup x_i)$$
% $$precision(R^{*}\cup x_k)\leq precision(R^{*}\cup x_j)$$