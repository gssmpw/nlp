\section{Introduction}

% Local explanation methods in interpretable machine learning explain why a model make a specific prediction for an individual input. They highlight the contribution of each feature to the final output, focusing on the local decision-making process rather than the overall model behavior. Popular methods include LIME, SHAP, and Anchors, each providing accessible and intuitive explanations.

Anchors ~\cite{Anchor} is a popular local model-agnostic machine learning explanation technique, which generates rules that form a sufficient condition to explain why a model makes a certain prediction for a given input.
It is local in the sense that it explains a machine learning model's behavior around a particular input, which makes it able to scale to complex models in critical domains such as healthcare and finance, where understanding individual predictions is vital for decision-making and validation. 
It is model-agnostic in the sense that it does not exploit internal design of a model, which makes it applicable to a wide range of machine learning models.
% These distinct features make Anchors adopted in many important applications, such as explaining a triage-prediction system for COVID-19 to find critical features in the severity prediction of a COVID-19 patient~\cite{KHANNA2023100246}, and analyzing Existing Vegetation Type to explore features associated with vegetation types~\cite{GANJI2023317}.
These distinct features make Anchors adopted in many important applications, such as explaining a triage-prediction system for COVID-19 to enable knowledge discovery about patient risk factors ~\cite{KHANNA2023100246}, and analyzing Existing Vegetation Type to uncover insights for ecological patterns and land management ~\cite{GANJI2023317}.
However, a significant bottleneck hinders its widespread application: the inherent computational inefficiency. The interpretation time for a single input of Anchors can last for several hours in certain scenarios, impeding the practical deployment of Anchors in real-time applications where prompt explanations are crucial.

In this paper, we attempt to find a method to improve the computational efficiency of Anchors without compromising the quality of the generated explanations. 
%
In short, our work is a two-step rule transformation process that accelerates Anchors by leveraging pre-trained explanations from representative inputs. Specifically, horizontal transformation adapts pre-trained rules to similar online inputs, while vertical transformation refines these rules to achieve the required precision and coverage for high-quality explanations.
%
We make two key observations: 1) most of Anchors' computation time is spent in drawing samples that are variants of the input to explain upon, and 2) Anchors draw samples in an iterative manner to build the explanation incrementally that becomes more and more specific to the current input.
%
Based on these two observations, we propose a method to improve the computation efficiency of Anchors: we pre-train explanations for representative pre-training inputs in advance, and then during online computation, for an online input, start the iterative computation process with a pre-trained explanation that is obtained from a similar pre-training input, thereby reducing the time spent in drawing samples.
%

\begin{table*}[h]
    \small
    \centering
    \caption{The sampling process with different features.}
    \label{tab:featuretrans_example}
    \begin{tabular}{cccc}
        \toprule 
        Input sample $x$ & Generated explanation $R_x$ & $f(x)$ \\
        \midrule
        $x_1$=This is the best movie that Iâ€˜ve ever seen .& (``best") & positive\\
        $x_2$=He was really nice today and helped me a lot .& (``nice") & positive\\
        \bottomrule
    \end{tabular}
\end{table*}

However, our method faces two major challenges: 1) The decision boundaries of the model being explained in these neighborhoods may differ. Therefore, explanations that are effective for pre-training inputs may not be effective for online inputs. 2) For two similar but not identical inputs, their features are not completely consistent. Consider the example in Table~\ref{tab:featuretrans_example}, where pre-trained sample $x_1$ and input sample $x_2$ do not share the same features, although their explanations are very similar. Since Anchors generate explanations by using features in its rules, explanations applicable to pre-trained inputs may not necessarily be applicable to new inputs. Consequently, the rules generated from $x_1$ cannot initiate explanations for $x_2$. 

To address these two challenges, we propose a method based on transforming rules to accelerate the generation of explanations. 
% which constitutes the main contribution of our work.
%
In particular, an explanation is a boolean predicate that becomes stronger in the process: less samples will satisfy it but it is more likely for a sample satisfying it to yield the same output as the input to explain.
%
For an explanation, how many samples satisfy it is referred to as its \emph{coverage}, and the percentage of samples that it covers yielding the same output as the input to explain is referred to as its \emph{precision}.

We propose two forms of rule transformation: one is horizontal, transforming a rule for one input into a rule for another input by substituting the features; the other is 
vertical, transforming a general rule (rule with high coverage and low precision) into a specific rule (rule with low coverage and high precision) by strengthening it.

% First, our approach generates a general explanation. Our method, specifically, begins by identifying clusters within the training data and pre-train explanations using cluster centers as representative pre-training inputs. The choice of using cluster centroids as representative pre-training inputs is because we aim to cover as many samples as possible within a limited pre-training scale. This initial pre-training phase is designed with relaxed precision requirements, allowing results with lower precision and complexity. 

% Then, we transform the pretrained explanations into explanations for the online input through rule transformation. We associate the online input with the most similar pre-training input, and then apply horizontal rule transformation to convert the pre-trained rule into another feature as the rule for the online input. Since the pre-trained explanations are relatively general, we obtain a general explanation for the online input. Next, we use vertical rule transformation to convert the general explanation into a specific explanation. That is, we will integrate the rule set obtained through horizontal transformation with other rules for online input, thereby achieving higher precision to meet user requirements. The vertical rule transformation maintains high accuracy requirements consistent with the Anchors. To ensure the similarity between the training centroids of pre-training and inputs, we obtained a critical distance through a validation set. 
% Only when the distance between the input and its corresponding cluster centroid is within the critical distance, the training result of the cluster centroid will be used as the initial explanation.


% \begin{figure}[t]
% 	\begin{minipage}{0.19\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{icml2025/figures/inputimage.png}}
% 		\centerline{Image A}
% 	\end{minipage}\hspace{-1pt}
% 	\begin{minipage}{0.19\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{icml2025/figures/PretrainInputImage.png}}
% 		\centerline{Image B}
% 	\end{minipage}\hspace{-1pt}
% 	\begin{minipage}{0.19\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{icml2025/figures/Pretrainresult.png}}
% 		\centerline{Image C}
% 	\end{minipage}\hspace{-1pt}
% 	\begin{minipage}{0.19\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{icml2025/figures/Inputresult.png}}
% 		\centerline{Image D}
% 	\end{minipage}\hspace{-1pt}
% 	\begin{minipage}{0.19\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{icml2025/figures/Input_final_result.png}}
% 		\centerline{Image E}
% 	\end{minipage}
% 	\caption{An example of our workflow for image. (Display Plan A)  }
% 	\label{fig_example_image}
% \end{figure}
% Plan A

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{icml2025/figures/Workflow_image.png}
    \caption{An example of our workflow for image.}
    \label{fig:fig_example_image}
\end{figure*}
% Plan B

%Next, we will use a specific case in figure\ref{fig_example_image} to illustrate our workflow. In this case, we explain an image classification task model \( f \). In the image domain, our features and rules are some superpixels within the image, while \emph{coverage} is represented by the number of superpixels, and \emph{precision} is defined as the probability of consistent predictions when the unselected superpixels are randomly replaced with parts of other images. Image A serves as our input. For a given image A, we first find the most similar image B from the pre-trained inputs. 

We use an image classification example shown in Figure~\ref{fig:fig_example_image} to illustrate the two transformations and the overall workflow of our approach.
The Anchors explanation (\emph{anchor} for short) for an image is a set of superpixels (i.e., image segments), meaning if all these superpixels are present, the target model would produce the same result as it produces on the input to explain.
To produce the anchor for Image A, which is classified as a car, our approach first identifies a similar image in the training set, Image B, whose anchor has been pre-computed as shown in Image C (i.e., a wheel).
%
Note to make the anchor general, it consists of few superpixels, making it of low precision and high coverage.
%
Moreover, while there exists a similar pattern in Image A, the anchor in Image C is not directly applicable as the low-level pixels are different.
%
To address this challenge, our approach applies the horizontal transformation on the anchor to produce the anchor in Image D, by replacing the superpixels with similar ones in Image A (i.e., two wheels).
Now the Image D shows an anchor for Image A, but with high coverage and low precision, making it not accurate enough for Image A.

To address this problem, our approach continues to apply the vertical transformation on the anchor in Image D to produce a more specific anchor for Image A. It leverages the incremental nature of Anchors' algorithm and makes it start with the anchor in Image D,
which results in the anchor in Image E.
Since only a few more superpixels are added to the anchor, the Anchors algorithm does not need significantly more samples.
By applying the two transformations on the pre-computed anchor, our approach produces the same anchor as the original Anchors but with much less time.

% Subsequently, based on the pre-trained results of image B (i.e., image C), we can see that the model \( f \) being explained identifies the vehicle using the feature of the wheels. Then, based on image C, we use the horizontal transformation to obtain the rule set applicable to image A (i.e., image D). From image D, we can observe that it also focuses on the wheel part of image A. However, the precision of image D does not meet the user-specified threshold. Therefore, we apply the vertical transformation to expand image D and finally obtain image E, which satisfies the precision requirement and is consistent with the results directly calculated by Anchors.

We conduct the experiments to demonstrate that our method achieves acceleration ratios of 271\%, 221\%, and 181\% on various tasks across tabular, text, and image datasets, respectively. Moreover, with sufficient pre-training inputs, it can achieve the fidelity consistent with that of Anchors.
