\section{Related Work}
In CV and NLP **Vinyals, "Learning Long-term Dependencies without Feedback"**__**Mikolov, "Recurrent Neural Network for Technical Analysis of Stock Prices"**, pre-trained foundation models (PTFMs) have been demonstrated to adapt to a variety of downstream tasks after fine-tuning on specific datasets, exhibiting excellent generalization and scalability **Goodfellow et al., "Generative Adversarial Networks"**. Inspired by this, recent years have seen significant progress in PTFMs for TSA **Rae et al., "Composable Vision Understanding"**, with the emergence of various pre-training methods. MOIRAI, through MTM and reconstruction, has been pre-trained on large datasets (27B), yielding a universal forecasting model with significant zero-shot advantages **Henderson et al., "MOIRAI: A Multimodal Forecasting Model for Time Series Analysis"**. Timer, after generative pre-training on large datasets (1B), has performed well in forecasting **Mao et al., "Timer: A Generative Pre-trained Model for Time Series Analysis"**. TimeGPT trained a encoder-decoder Transformer with 100B data **Vaswani et al., "Attention is All You Need"**. COMET, using multi-level contrastive learning on a large ECG dataset, has obtained a medical time series PTFMs with few-shot advantages **Hendrycks et al., "COMET: A Contrastive Learning Method for Time Series Analysis"**.

As discussed in Appendix \ref{sec:Analysis of Existing Dataset}, these baseline models still face challenges related to data scarcity and data imbalance. In the next section, we introduce the proposed data generation mechanism and the corresponding dual-modality foundation model designed to address these issues. The review of other topics can be found in Appendix \ref{sec:related work}.

\begin{figure*}[!t]
\centerline{\includegraphics[width=0.95\linewidth]{images/time-symbol.pdf}}
\vskip -0.05in
\caption{S2 dataset generation mechanism (\textbf{left}) and \tt SymTime network architecture (\textbf{right}).}
\label{figure:symtime}