\section{Related Work}
In CV and NLP \cite{CLIP}, pre-trained foundation models (PTFMs) have been demonstrated to adapt to a variety of downstream tasks after fine-tuning on specific datasets, exhibiting excellent generalization and scalability \cite{TimeMIL}. Inspired by this, recent years have seen significant progress in PTFMs for TSA \cite{Transformer-in-TSA, KDD-Survey}, with the emergence of various pre-training methods. MOIRAI, through MTM and reconstruction, has been pre-trained on large datasets (27B), yielding a universal forecasting model with significant zero-shot advantages \cite{MOIRAI}. Timer, after generative pre-training on large datasets (1B), has performed well in forecasting \cite{Timer}. TimeGPT trained a encoder-decoder Transformer with 100B data \cite{TimeGPT}. COMET, using multi-level contrastive learning on a large ECG dataset, has obtained a medical time series PTFMs with few-shot advantages \cite{COMET}. 

As discussed in Appendix \ref{sec:Analysis of Existing Dataset}, these baseline models still face challenges related to data scarcity and data imbalance. In the next section, we introduce the proposed data generation mechanism and the corresponding dual-modality foundation model designed to address these issues. The review of other topics can be found in Appendix \ref{sec:related work}.

\begin{figure*}[!t]
\centerline{\includegraphics[width=0.95\linewidth]{images/time-symbol.pdf}}
\vskip -0.05in
\caption{S2 dataset generation mechanism (\textbf{left}) and \texttt{SymTime} network architecture (\textbf{right}).}
\label{figure:symtime}
\end{figure*}