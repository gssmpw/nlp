\section{Background and Related Work}
\noindent \textbf{Retrieval-Augmented Generation (RAG).} RAG is a technique designed to enhance the capabilities of LLMs by integrating external knowledge sources (\ie, knowledge bases) \citep{lewis2020retrieval}. Unlike traditional LLMs, which generate responses solely based on the knowledge encoded during pre-training, RAG combines both retrieval and generation mechanisms to produce more accurate, contextually relevant, and up-to-date outputs. Existing RAG systems implemented dual encoders to map queries and texts within the knowledge base into the embedding space and retrieve candidate texts that produce high similarity values with the given query. Recent works were proposed to improve the effectiveness of retrieval models by implementing different encoder architectures~\citep{nogueira2019passage,humeau2019poly,khattab2021relevance}, searching algorithms~\citep{xiongapproximate}, embedding capacity~\citep{gunther2023jina}, max tokens~\citep{muennighoff2022mteb}, \textit{etc}. In general, the knowledge base plays a critical role in the effectiveness of the RAG, containing valuable and often proprietary content. They are valuable intellectual property of their owners and their copyright deserves to be protected. 






\noindent \textbf{Poisoning and Backdoor Attacks against RAG Systems.} Recently, there are also a few pioneering works exploring data-centric threats in RAG systems~\citep{zou2024poisonedrag,xiangbadchain,chen2024agentpoison,cheng2024trojanrag}. Specifically, PoisonedRAG~\citep{zou2024poisonedrag} proposed the first data poisoning attack against RAG by injecting several malicious and wrong answers into the knowledge base for each pre-defined query. The adversaries could lead the compromised RAG to generate targeted wrong answers with these pre-defined queries. TrojanAgent~\citep{cheng2024trojanrag} proposed a backdoor attack by compromising its retriever; thus, leveraging queries attaching with adversary-specified optimized trigger patterns could activate the malicious behavior embedded in its compromised retriever. Most recently, AgentPoison~\citep{chen2024agentpoison} proposed the backdoor attack against RAG by injecting optimized malicious target texts (decisions) into the external knowledge base. AgentPoison also proposed an optimization framework to optimize a stealthy and effective trigger pattern for increasing the probability of retriever retrieving the hidden malicious target texts. These methods all seriously undermine the integrity of RAG systems, making them susceptible to anomaly detection and even introduce new security risks \cite{du2025sok}.


%Recently, Backdoor and Data Poisoning attacks have shown to pose great threaten to RAG systems~\citep{zou2024poisonedrag,xiangbadchain,chen2024agentpoison,cheng2024trojanrag}.
 




\noindent \textbf{Dataset Ownership Verification.} Dataset ownership verification (DOV) aims to verify whether a suspicious model is trained on the protected dataset ~\citep{li2022untargeted,li2023black,guo2023domain,wei2024pointncbw,yao2024promptcare}. To the best of our knowledge, this is currently the only feasible method\footnote{Arguably, membership inference attacks are also not suitable since they lead to a high false-positive rate \cite{du2025sok}. } to protect the copyright of public datasets in a retrospective manner. Specifically, it introduces specific prediction behaviors (towards verification samples) in models trained on the protected dataset while preserving their performance on benign testing samples, by solely watermarking the dataset before releasing it. Dataset owners can verify ownership by examining whether the suspicious model has dataset-specified distinctive behaviors. Previous DOV methods \citep{li2022untargeted,li2023black,tang2023did} exploited either backdoor attacks or others~\citep{guo2023domain} to watermark the original (unprotected) benign dataset or prompts. For example, backdoor-based DOV adopted poisoned-/clean-label backdoor attacks to watermark the protected dataset. %Regarding the harmless copyright protection for dataset or prompt, Guo \etal \citep{guo2023domain} and \citep{yao2024promptcare} proposed harmless watermark techniques for image classification and instruction fine-tuning applications, where the watermark samples are not allowed to cause malicious behavior (\eg, misclassification) for verification purposes. 
CPR~\citep{golatkar2024cpr} proposed copyright-protected RAG to provide copyright protection guarantees in a mixed-private setting for diffusion models. CPR focused on addressing privacy leakage issues in the generation procedure of diffusion models. However, the copyright protection technique for the knowledge base of RAG remains blank.