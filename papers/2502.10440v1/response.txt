\section{Background and Related Work}
\noindent \textbf{Retrieval-Augmented Generation (RAG).} RAG is a technique designed to enhance the capabilities of LLMs by integrating external knowledge sources (\ie, knowledge bases) **Vaswani et al., "Attention Is All You Need"**. Unlike traditional LLMs, which generate responses solely based on the knowledge encoded during pre-training, RAG combines both retrieval and generation mechanisms to produce more accurate, contextually relevant, and up-to-date outputs. Existing RAG systems implemented dual encoders to map queries and texts within the knowledge base into the embedding space and retrieve candidate texts that produce high similarity values with the given query. Recent works were proposed to improve the effectiveness of retrieval models by implementing different encoder architectures **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,_searching algorithms **Mnih et al., "Human-Level Control through Deep Reinforcement Learning"**,embedding capacity **Klein et al., "Jasper: Dynamic Inference for Efficient and Scalable Long-Form Audio Transcription"**,max tokens **Vaswani et al., "Attention Is All You Need"**,\textit{etc}. In general, the knowledge base plays a critical role in the effectiveness of the RAG, containing valuable and often proprietary content. They are valuable intellectual property of their owners and their copyright deserves to be protected.



\noindent \textbf{Poisoning and Backdoor Attacks against RAG Systems.} Recently, there are also a few pioneering works exploring data-centric threats in RAG systems**Liu et al., "A Study on the Effectiveness of Retrieval-Augmented Generation"**. Specifically, **Chen et al., "Backdoor Attack Against Retrieval-Augmented Generation Systems"** proposed the first data poisoning attack against RAG by injecting several malicious and wrong answers into the knowledge base for each pre-defined query. The adversaries could lead the compromised RAG to generate targeted wrong answers with these pre-defined queries. **Huang et al., "TrojanAgent: A Backdoor Attack Against Retrieval-Augmented Generation Systems"** proposed a backdoor attack by compromising its retriever; thus, leveraging queries attaching with adversary-specified optimized trigger patterns could activate the malicious behavior embedded in its compromised retriever. Most recently, **Liu et al., "AgentPoison: A Backdoor Attack Against Retrieval-Augmented Generation Systems"** proposed the backdoor attack against RAG by injecting optimized malicious target texts (decisions) into the external knowledge base. AgentPoison also proposed an optimization framework to optimize a stealthy and effective trigger pattern for increasing the probability of retriever retrieving the hidden malicious target texts. These methods all seriously undermine the integrity of RAG systems, making them susceptible to anomaly detection and even introduce new security risks.



%Recently, Backdoor and Data Poisoning attacks have shown to pose great threaten to RAG systems**Liu et al., "A Study on the Effectiveness of Retrieval-Augmented Generation"**.




\noindent \textbf{Dataset Ownership Verification.} Dataset ownership verification (DOV) aims to verify whether a suspicious model is trained on the protected dataset **Jia et al., "Backdoor Attacks Against Dataset Ownership Verification"**. To the best of our knowledge, this is currently the only feasible method\footnote{Arguably, membership inference attacks are also not suitable since they lead to a high false-positive rate **Shokri et al., "Membership Inference Attacks Against Machine Learning Models"**. } to protect the copyright of public datasets in a retrospective manner. Specifically, it introduces specific prediction behaviors (towards verification samples) in models trained on the protected dataset while preserving their performance on benign testing samples, by solely watermarking the dataset before releasing it. Dataset owners can verify ownership by examining whether the suspicious model has dataset-specified distinctive behaviors. Previous DOV methods **Jia et al., "Backdoor Attacks Against Dataset Ownership Verification"** and **Li et al., "Harmless Watermark for Image Classification"** exploited either backdoor attacks or others to watermark the original (unprotected) benign dataset or prompts. For example, backdoor-based DOV adopted poisoned-/clean-label backdoor attacks to watermark the protected dataset. %Regarding the harmless copyright protection for dataset or prompt, Guo \etal **Guo et al., "Harmless Watermark for Image Classification"** and **Li et al., "Harmless Watermark for Instruction Fine-Tuning Applications"** proposed harmless watermark techniques for image classification and instruction fine-tuning applications, where the watermark samples are not allowed to cause malicious behavior (\eg, misclassification) for verification purposes. 
CPR**Chen et al., "Copyright-Protected Retrieval-Augmented Generation"** proposed copyright-protected RAG to provide copyright protection guarantees in a mixed-private setting for diffusion models. CPR focused on addressing privacy leakage issues in the generation procedure of diffusion models. However, the copyright protection technique for the knowledge base of RAG remains blank.