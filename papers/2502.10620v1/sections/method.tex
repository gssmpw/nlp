
\section{Methodology}
%%\vspace{-6pt}
\subsection{System Overview}

The overall structure of ProMRVL-CAD is illustrated in Figure \ref{fig2_overall}. The proposed system consists of two major components, Proactive Question Generator (Pro-Q Gen) for proactive conversation and Multi-Vision Patient-Text Diagnostic Report Generator (MVP-DR Gen) for disease diagnosis. For the Pro-Q Gen, we generate a synthetic and proactive medical dialogue dataset that is conditional on the medical history from a real-world medical dataset to mimic doctor-patient conversations. Then, we fine-tune a pre-trained LLM with the synthetic dialogue dataset and medical histories to boost its reasoning and interaction capability in disease analysis. In the MVP-DR Gen, we feed the medical visuals and the textual inputs (medical history and dialogue) to the second LLM. We confer the multi-modality capability on the model by leveraging a MiniLM \cite{10.5555/3495724.3496209} for dialogue analysis and deploying a vision transformer followed by an alignment layer for visual processing. Lastly, we freeze the parameters of the second LLM to fully utilize its language and reasoning capability. We train the rest of the components with the knowledge of the diagnostic reports to grant the proposed model a professional understanding of medical inputs.

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{Figures/Fig2_over_flow.png}
%%\vspace{-12pt}
\caption{The architecture of ProMRVL-CAD. It consists of two modules: Pro-Q Gen to prompt patients to provide more informative inputs and MVP-DR Gen to generate diagnostic reports from multi-modality inputs. }
\label{fig2_overall}
%%\vspace{-15pt}
\end{figure*}

\subsection{Step 1: Proactive Question Generation (Pro-Q Gen)}
\label{step1}
The design of ProMRVL-CAD is inspired by the conventional diagnostic procedure in which doctors inquire about the health conditions of patients. %As such, we propose to develop a medical dialogue agent, Pro-Q Model, with a unique proactive feature to query disease-related information from patients.
The goal of our proposed Pro-Q Gen is to proactively pose queries to acquire patient's health status information that underlying the potential disease. This requires Pro-Q Gen with strong disease reasoning capability to perform disease diagnosis, making it more challenging than classic recommendation tasks that mainly target feature embedding and pattern recognition. Different from existing work where the ChatBot passively answers queries, the Pro-Q Gen model generates questions from the ChatBot side to lead the conversation through a better understanding of patients' health conditions. In particular, we developed a dialogue recommendation system that generates proactive questions during conversations. Unlike a traditional end-to-end doctor agent, our system offers more precise query selection and refinement. As illustrated in Figure \ref{fig2_overall}, our system operates in two stages: generating query candidates and ranking them using a knowledge graph. This approach enables the proposed model to thoroughly explore the candidate query space generated by the foundation model and select the most relevant queries for the current response.

During the training process, we use cross-entropy to fine-tune the proposed LLMs to generate query candidates. The loss function is defined as 
\begin{equation}
L = \sum^L_{i=1} logP_\theta^{Q}(x_i; X_t^{Q}, X_{r,<i}^{Q}),   
\end{equation}
where $\theta^{Q}$ stands for trainable parameters, $x_i$ represents current predicted tokens with $i = 1, ..., L$ indicating the location of current token, $X_t^{Q}$ stands for the textual inputs, and $X_{r,<i}^{Q}$ represents the token before the predicted token.




\textbf{Question Generation.} We let the patient initiate the dialogue. Based on patients' input $D_i$, we generate $N$ queries as candidates. Then the patient agent generates another response for each dialogue $D_i$ in the queue. To generate valid question candidates, we fine-tuned the LLM using both the synthetic dialogue dataset and the medical history in the MIMIC-CXR dataset to enable it to generate proactive medical questions based on the responses of patients. To reduce the number of trainable parameters, we deploy the low-rank adaptation (LoRA) strategy \cite{Hu2021LoRALA, balazy2024lora} to inject trainable rank decomposition matrices into different layers. The use of medical history is inspired by the clinical practice in which doctors review patients' medical history before starting the intervention. As such, it could serve as a health condition constraint for the generation of proactive questions to improve the consistency between the dialogue and the patient's health condition. During the fine-tuning process, we use the responses from the patients in the synthetic dialogue dataset as the response and the questions from the doctor as the reference. Benefiting from our proactive dataset, our model could proactively raise questions to query patients' health conditions and ask for necessary medical visuals in multiple rounds. The termination of the conversation is determined by the confidence of the disease identification model on the collected evidence. 
\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{Figures/Fig3_kg.png}
\caption{An illustration of the knowledge graph with edge widths indicating the correlation between a disease and a symptom.}
\label{fig3_graph}
\end{figure}


\textbf{Candidate Ranking through Clinical Concept Knowledge Graph.} 
In our study, we propose to use knowledge graph \cite{Hogan_2021} to enhance the performance of our recommendation system. Inspired by \cite{zhang2016collaborative}, we develop a clinical concept knowledge graph to embed the structural knowledge between various diseases and symptoms. Note that our clinical concept graph is built upon an item graph, with no patient feature involved to protect users' privacy. The diseases and the corresponding symptoms are extracted from real-world clinical dialogue and medical records to ensure clinical professionalism. In Figure \ref{fig3_graph}, we show an example of our knowledge graph with edge widths indicating the correlation between the disease and the symptom. 

To fully utilize the language and reasoning capability of the pre-trained LLM and further reduce the retraining efforts, our clinical concept knowledge graph is integrated into the ranking stage to form a novel ranking criterion, rather than directly building into the candidate generation network for feature embedding. This is different from classic (item) knowledge graph-based recommendation systems that embed the item features through latent layers and generate recommendations through collaborative joint learning \cite{zhang2016collaborative, zhang2018learning, cao2019unifying, guo2020survey}. Our ranking criterion is based on the correlation between the potential diseases and the patient's symptoms. We first extract the patients' symptoms from past conversations and build a symptom base to record all symptoms from the patient. Then, we locate the most relevant disease through the knowledge graph and rank the candidate query through its correlation with the target disease. The candidate queries are ranked from 0 to 10 by a LLM, with 10 indicates the highest relevence and 0 indicates non-relevence. Specifically, our ranking system rejects repeat queries if the symptom has already been checked by the patient. Lastly, our system will terminate or move to the next potential disease if the most relevant symptoms have already been checked.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{Figures/Fig4_MedDialogue.png}
\caption{Examples of synthetic proactive medical dialogue (training data) generated using medical history and medical images. The clinical concepts (highlighted by blue, orange, and green colors) are consistent among the medical history, the report, and the synthetic dialogue. Our synthetic dialogue is in line with the medical visuals and contains complementary information on health conditions to assist in disease diagnosis. }
\label{fig4_medialogue}
\end{figure*}

\textbf{Proactive Medical Dialogue Dataset Generation and Data Training on Hybrid Dataset.} Existing medical dialogue datasets only contain textual discussions of health conditions with limited descriptions of potential diseases \cite{zeng-etal-2020-meddialog,abacha2023empirical, yang2020generation, zhou2021generation}. In many cases, the conclusion of the discussion is just the initial guess of the potential disease, such as "Summary of the condition and initial impressions: Blepharitis" \cite{zeng-etal-2020-meddialog}. Moreover, these datasets do not include any visual evidence for diagnostic report generation and thus are not applicable to our study. As such, we generate a synthetic dataset to teach the model disease-related information and enable it to lead the conversation.  Inspired by the concept of VQA, we design an instructive protocol formulated by a virtual doctor and a patient to enable the model's reasoning and query capability to effectively collect the health condition from the patient. Based on the corresponding textual information, the contents of the dialogue are consistent with the medical visuals to assist further disease analysis. An example of synthetic proactive dialogue is shown in Figure \ref{fig4_medialogue}. Our Pro-Q Gen model is fine-tuned on a hybrid dataset that consists of both synthetic data and real-world medical conversation data \cite{zhang2023huatuogpt} to ensure its professionalism in medical dialogue generation.

\subsection{Step 2: Multi-Vision Patient-Text Diagnostic Report Generation (MVP-DR Gen)}\label{section_2_3}
The unique advantage of our proposed Multi-Vision Patient-Text Diagnostic Report Generator, namely MVP-DR Gen, lies in its multi-modality capability to simultaneously and synthetically process textual and visual inputs. Particularly, MVP-DR Gen could process medical visuals with different views if a single view is insufficient for disease diagnosis. As shown in Figure \ref{fig2_overall}, it employs a MiniLM \cite{10.5555/3495724.3496209, vergou2023readability} for textual analysis and a vision transformer (ViT) \cite{liu2021swin, yin2022vit, han2022survey} to learn the high-dimensional visual and texture features from the multi-view images. Inspired by \cite{covington2016deep}, the features extracted from different images/views are directly averaged to ensure a fixed input size for the followed vision-language generating task. Our vision-analysis task can be further divided into two sub-tasks, the disease identification task and the report generation task. Specifically, the report generation module aims to generate the diagnostic report from the embedded features, while the disease identification task contributes to improving the diagnosis capability. 
In the report generation task, we use an alignment layer, which serves as a soft prompt, to align the visual features with the LLMs. In this study, we use $Llama2-7B$ \cite{touvron2023llama, thakkar2023comprehensive} as our baseline LLM which has high efficiency to produce fast responses. During the entire training process, only the LLM remains frozen while the rest of the networks are randomly initialized. The loss function for the report generation task is jointly optimized by the combination of loss terms from each task. 

During the entire training process, only the LLM remains frozen while the rest of the networks are randomly initialized. The loss function for the report generation task is jointly optimized by the combination of loss terms from each task: 
\begin{equation}
\begin{aligned}[c]
L_{report}(\theta^{DR};X_r^{DR},X_v^{DR}, X_t^{DR},X_p^{DR})=&\\ 
    -\sum^{L}_{i=1}logp_{\theta^{DR}}(x_i;X_v^{DR},X_t^{DR},X_p^{DR},X_{r,<i}^{DR}) &, 
\end{aligned}
\end{equation}
where $\theta^{DR}$ represents the trainable parameters in the alignment layer, $L$ represents the length of the generated sentence, $X_r^{DR}$ represents the current prediction token, $X_v^{DR}$ represents the visual embedding, $X_t^{DR}$ represents the textual inputs, $X_p^{DR}$ represents prompts, and $X_{r,<i}^{DR}$ represents the token before the predicted token. We pose the disease identification task as a multi-label classification task to detect the potential disease in the input visuals. The model is optimized with the cross-entropy loss:
%\vspace{-10pt}
\begin{equation}
    L_{classification}(\hat{y}, y) = -\sum^C_{j=1}y_jlog(\hat{y}_j) + (1-y_j)log(1-\hat{y}_j),
%\vspace{-10pt}
\end{equation}
where $C$ stands for the number of classes, $\hat{y}$ denotes the prediction results, and $y$ denotes the ground truths. Overall, our loss function for the entire module is:
\begin{equation}\label{eq11}
    L = L_{classificaion} + \alpha L_{report}.
    %\vspace{-5pt}
\end{equation}