\section{Experiments}
% \begin{table*}[t!]

\begin{table*}[t!]
\caption{Baseline comparison on diagnostic report generation on MIMIC-CXR dataset. \textcolor{red}{Red} highlights the best performance and \textcolor{blue}{{blue}} highlights the second best performance.}
\label{ablation_comp}
\centering
%\renewcommand{\arraystretch}{1.25}
\scalebox{1}{
\begin{tabular}{lccccc}
\hline
\multicolumn{1}{l}{Approaches} & BLEU1 $\uparrow$ & BLEU2 $\uparrow$ & BLEU3 $\uparrow$ & BLEU4 $\uparrow$ & ROUGE $\uparrow$ \\ \hline
R2GCMN (2021)                  & 0.353 & 0.218 & 0.148 & 0.106 & 0.278 \\
% PPKED (2021)                   & 0.360 & 0.224 & 0.149 & 0.106 & 0.284 \\
% MSAT (2022)                    & 0.373 & 0.235 & 0.162 & 0.120 & 0.282 \\
METrans (2023)                 & 0.386 & 0.250 & 0.169 & 0.124 & \textcolor{blue}{{0.291}} \\
ChatCAD+ (2023)                & 0.219 & 0.127 & 0.081 & 0.056 & 0.204 \\
R2GenGPT (2024)                & \textcolor{blue}{{0.405}} & \textcolor{blue}{{0.260}} & \textcolor{blue}{{0.178}} & \textcolor{blue}{{0.127}} & 0.290 \\
ProMRVL (Ours)                 & \textbf{\textcolor{red}{0.430}} & \textbf{\textcolor{red}{0.305}} & \textbf{\textcolor{red}{0.231}} & \textbf{\textcolor{red}{0.182}} & \textbf{\textcolor{red}{0.331}} \\ \hline
\end{tabular}
}
\end{table*}


\subsection{Experiment Setup}
\textbf{Datasets.} We carry out experiments using two publicly available datasets: MIMIC-CXR \cite{DBLP:journals/corr/abs-1901-07042} and IU-Xray \cite{DinaDemnerFushman.2015}.
%For the IU-Xray dataset, we do not generate the medical dialogues due to the lack of medical history and the main.
We follow the same data partition policy in \cite{DBLP:journals/corr/abs-1901-07042} for the MIMIC-CXR dataset and the partition policy in \cite{Chen.2020} for the IU-Xray dataset. Additionally, we build two subset datasets, MIMIC-V2 and IU-V2, with cases that only contain frontal and lateral views of Xray images. 
%More details about our dataset pre-processing and model implementation are presented in \textbf{Supplementary Material}  (Implementation Details). 
Moreover, as mentioned in Section \ref{step1}, 
we generated a clinical dialogue dataset that contains both synthetic and real clinical dialogues to fine-tune our proposed Pro-Q Gen. Our clinical dialogue dataset consists of 78399 (66149 synthetic and 12250 real) clinical conversation records. For the synthetic dialogues, we used ChatGPT to generate dialogues using medical history from the MIMIC-CXR dataset. For the real clinical dialogues, we used part of the Huatuo-26M \cite{zhang2023huatuogpt} and CMtMedQA \cite{yang2024zhongjing} dataset, which is collected based on real conversations between the patient and doctor. 
Our diagnostic report generation task is evaluated on the MIMIC-CXR dataset and IU-Xray dataset, as they are among the largest real-world datasets consisting of Xray images and diagnosis reports. In the MIMIC-CXR dataset, each patient is associated with one or multiple Xray images, along with a diagnosis report containing impressions, findings, medical history, etc. The IU-Xray dataset consists of 7,470 Xray images, with 3,955 study cases and corresponding reports.



\textbf{Fine-tuning of Pro-Q Gen.} In the Pro-Q Gen module, we use Llama-3-8B Instruct \cite{llama3modelcard} as the backbone model for dialogue generation. During the fine-tuning process, we decompose the network parameter $\theta$ to $\theta_0+\Delta \theta(\Theta)$ using low-rank adaption with $N = 16$ ranks.

\textbf{Training of MVP-DR Gen.} We set $\alpha = 1$ for our overall loss function in Eq. \ref{eq11}. In the MVP-DR Gen module, we adopt the Swin Transformer \cite{9710580} pre-trained on ImageNet \cite{deng2009imagenet} as the ViT for feature embedding with a dimension of 1024. We deploy a linear layer for the alignment layer in the report generation and a fully connected layer as the classifier for disease identification. The disease identification results are transferred to the corresponding texts and integrated into the end of the generated report. We further deploy the MiniLM-L6-v2 \cite{10.5555/3495724.3496209} model to generate the word embeddings, with an output of 384 dimensions for each sentence. Similar to the visual task, the embeddings of sentences in the medical history and dialogues are averaged before sending to an alignment layer for downstream analysis. Lastly, the image embeddings and text embeddings are interrogated to a specific prompt to instruct Llama-2-7B for disease analysis and diagnostic report generation. The training process is conducted for 4 epochs for the MIMIC-CXR dataset, with a batch size of 6 and a learning rate of 1e-4. For the MIMIC-V2 and IU-V2 datasets, we fine-tune the models by using a batch size of 6 and a learning rate of 1e-4. During testing, we employ a beam search strategy with a beam size set to 3.

\textbf{Computational Resources.} The traing and testing experiments are carried out in parallel on 3 RTX A6000 GPUs (memory: 48GB) and 4 H100 Tensor Core GPUs (memory: 80GB). We use Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz with 36 cores, x86\_64 architecture. The operation system is Ubuntu 20.04.4 LTS.
% \textcolor{red}{
% \textbf{Synthetic Dataset Generation.} Our synthetic dataset is generated based on the medical history from MIMIC-CXR dataset with the following prompt:\\
% \textit{"Generate a conversation between doctor and patient based on the given file. The doctor mainly asks for the feelings and medical history of the patient. The patient will express the feeling based on the FINDINGS and IMPRESSION from the report. Do not repeat the report in the conversation. " + file}
% }


\begin{table}[t]
\centering

\caption{Evaluation on clinical efficacy of the proposed model and baselines. \textcolor{red}{Red} highlights the best performance and \textcolor{blue}{{blue}} highlights the second best performance.}
\label{table_clinical1}
\scalebox{0.9}{
\begin{tabular}{llll} 
\hline
                         & F1  $\uparrow$  & Recall $\uparrow$ & Precision $\uparrow$  \\ 
\hline
R2GCMN, \textit{(2021)} & 0.334 & 0.275 & 0.278 \\
METrans, \textit{(2023)} & 0.364 & 0.309 & 0.311 \\
ChatCAD+, \textit{(2023)} & 0.290 & \textcolor{blue}{{0.403}} & \textcolor{blue}{{0.394}} \\
R2GenGPT, \textit{(2024)} & \textcolor{blue}{{0.392}} & 0.387 & 0.389 \\
ProMRVL (Ours) & \textcolor{red}{0.416} & \textcolor{red}{0.426}  & \textcolor{red}{0.406}      \\
\hline
                         &       &        &           
\end{tabular}
}
\end{table}



\textbf{Evaluation Metrics.} We measure the quality of the generated Dialogue dataset using quantitative scores provided by ChatGPT \cite{openai2023gpt}. Following the evaluation methodology in existing work \cite{yang2024zhongjing}, we scored 100 cases of dialogues between patients and doctors and compare the scores among our Dialogue dataset, MedDialog \cite{zeng-etal-2020-meddialog} and MTS-Dialog \cite{abacha2023empirical}.
For the diagnostic report evaluation, we measure the quality of the generated medical reports from two perspectives: report text quality and clinical efficacy. We adopt bilingual evaluation understudy (BLEU) \cite{papineni-etal-2002-bleu, reiter2018structured} and recall-oriented understudy for gisting evaluation (ROUGE) \cite{lin-2004-rouge, ng2015better} to evaluate the text quality of generated diagnostic reports and proactive questions. In addition to the text quality, we also use classification evaluation metrics, including precision, recall, and F1 scores, to evaluate the clinical efficacy of disease identification. We use CheXpert \cite{Irvin2019CheXpertAL, smit2020chexbert} for annotating the generated reports, which is compared to the ground truth annotations across 14 distinct categories (atelectasis, cardiomegaly, consolidation, etc.). %Enlarged Cardiomediastinum, %Fracture, Lung Lesion, Lung Opacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, and Support Devices). 





\subsection{Evaluation on Diagnostic Report Generation} 
\textbf{Comparison with SOTAs.} We compare the diagnostic report generation performance of ProMRVL-CAD with the following SOTA: R2GCMN \cite{chen-etal-2021-cross-modal}, %PPKED \cite{9578840}, MSAT \cite{Wang.2022b},
METransformer \cite{10203079}, ChatCAD+ \cite{zhao2023chatcad+}, and R2GenGPT \cite{ZhanyuWang.2023}. In Table \ref{ablation_comp}, we present the overall performance of ProMRVL-CAD and baselines on both report text quality. Our model achieves the best performance in all evaluation metrics regarding BLEU and ROUGE, indicating high text quality in comparison with original medical report. Moreover, we comapre the clinical efficacy results in Table \ref{table_clinical1}. We observed that the clinical efficacy of the proposed model is much higher than baselines. Particularly, our model shows a significant improvement in Recall. These results evidently demonstrate our effectiveness in disease diagnosis and abnormality detection, as Recall directly indicates the disease detection capability. 

%\definecolor{MineShaft}{rgb}{0.121,0.121,0.121}

\begin{table*}[t]
\caption{Ablation study on diagnostic report generation on MIMIC-CXR dataset. \textcolor{red}{Red} highlights the best performance and \textcolor{blue}{{blue}} highlights the second best performance.}
\label{ablation}
\centering
\centering
%\renewcommand{\arraystretch}{1.25}
\scalebox{0.8}{\begin{tblr}{
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  column{7} = {c},
  column{9} = {c},
  cell{5}{5} = {fg=blue},
  cell{5}{6} = {fg=blue},
  cell{5}{7} = {fg=blue},
  cell{5}{8} = {fg=blue},
  cell{5}{9} = {fg=blue},
  cell{8}{5} = {fg=red},
  cell{8}{6} = {fg=red},
  cell{8}{7} = {fg=red},
  cell{8}{8} = {fg=red},
  cell{8}{9} = {fg=red},
  hline{1-2,9} = {-}{},
}
Approaches      & Single-View & Multi-View & Textual Input & BLEU1 $\uparrow$ & BLEU2 $\uparrow$ & BLEU3 $\uparrow$ & BLEU4 $\uparrow$ & ROUGE $\uparrow$ \\
ChatCAD+ (2023) & \checkmark           &            &               & 0.219            & 0.127            & 0.081            & 0.056            & 0.204            \\
ChatCAD+ (2023) & \checkmark           &            & \checkmark             & 0.329            & 0.194            & 0.121            & 0.070            & 0.299            \\
R2GenGPT (2024) & \checkmark           &            &               & 0.405            & 0.260            & 0.178            & 0.127            & 0.290            \\
R2GenGPT (2024) & \checkmark           &            & \checkmark             & 0.416    & 0.270    & 0.187    & 0.135    & 0.299    \\
ProMRVL (Ours)  & \checkmark           &            &               & 0.361            & 0.228            & 0.156            & 0.112            & 0.264            \\
ProMRVL (Ours)  &             & \checkmark          &               & 0.370            & 0.238            & 0.166            & 0.123            & 0.273            \\
ProMRVL (Ours)  &             & \checkmark          & \checkmark             & \textbf{0.430}   & \textbf{0.305}   & \textbf{0.231}   & \textbf{0.182}   & \textbf{0.331}   
\end{tblr}}
\end{table*}









\textbf{Ablation Study.}
We conduct an ablation analysis to show the necessity of our modules and strategies. In Table \ref{ablation}, we present the performance of MVP-DR Gen with different model inputs. Compared with single-view images, we observe that using images with multiple views can potentially improve the diagnosis performance in the medical report generation. It is also noticed that textual input boosted the report generation even on the baseline models, which is also a demonstration of scalability of ProMRVL, indicating the feasibility of integrating the framework with other VLM. More importantly, these results confirm the necessity of using textual health status information for diagnosis report generation. To further demonstrate the impact of text input, we add two additional ablation studies on the latest two approaches, ChatCAD+ and R2GenGPT. It is worth noting that baselines of ChatCAD+ and R2GenGPT don't support text input themselves. We embedded the textual input in the same way of MVP-DR. We similarly noticed that text input could improve the performance of report generation. However, the improved performances in both ChatCAD+ and R2GenGPT are still lower than ProMRVL. This results indicated that although text input could improve the text quality, it it the unique multi-modal design in ProMRVL that boost the performance in report generation. Other methods (i.e., ChatCAD+ and R2GenGPT) could benefit from text input but not as much as ProMRVL. 

\begin{table}[t]
\centering
\caption{Quantitative evaluation on the synthetic dialogue dataset. The best performance is highlighted by \textcolor{red}{red}. The second best performance is highlighted by \textcolor{blue}{{blue}}.}
\label{datasetcompare}
\scalebox{0.8}{
\begin{tblr}{
  cell{1}{1} = {c},
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  cell{2}{1} = {c},
  cell{2}{2} = {c},
  cell{2}{3} = {c},
  cell{3}{1} = {c},
  cell{3}{2} = {c,fg=blue},
  cell{3}{3} = {c, fg = blue},
  cell{4}{1} = {c},
  cell{4}{2} = {c,fg=red},
  cell{4}{3} = {c,fg=red},
  hline{1-2,5} = {1-3}{},
}
             & Professionalism  $\uparrow$   & Conciseness $\uparrow$    &  &  &  \\
MTS-Dialog (2023)         & 0.746           & 0.718          &  &  &  \\
MedDialog (2020) & {0.829}           & {0.788}          &  &  &  \\
ProDial (Ours)         & \textbf{0.875}  & \textbf{0.860} &  &  &  \\
             &                 &                &  &  &  
\end{tblr}
}
%\vspace{-10pt}
\end{table}
\subsection{Evaluation on Proactive Medical Dialogue}

\textbf{Quantitative Evaluation on Synthetic Medical Dialogue Dataset.} We generated 119,276 synthetic medical conversations based on corresponding files in the MIMIC-CXR dataset. In Table \ref{datasetcompare}, we show the quantitative evaluation of our synthetic medical dialogue dataset with two widely used medical dialogue datasets, MTS-Dialog \cite{abacha2023empirical} and MedDialog \cite{zeng-etal-2020-meddialog}.
We use Professionalism and Conciseness, which are considered key properties for natural and effective dialogue, to validate the quality of our synthetic dataset, ProDial. Same as \cite{zhang2023huatuogpt, yang2024zhongjing}, the Professionalism and Conciseness of the synthetic medical dialogue dataset are automatically evaluated by the quantitative scores provided by ChatGPT. As shown, our synthetic medical dialogue dataset has similar professionalism and conciseness properties as natural dialogue datasets. %In the \textbf{Supplementary Material} (Additional Results Section), we show representative samples of our synthetic dialogues.
In Figure \ref{engage1}, we show two representative dialogue samples from our proposed Pro-Q Gen model, which provide immediate responses to the previous reactions.




\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{Figures/Fig5_engagement.png}
\caption{Representative samples of the proactive dialogue produced by our proposed Pro-Q Gen. \textbf{(a).} A sample dialogue from a patient with weight loss and pneumonia. \textbf{(b).} A sample dialogue from a patient with several symptoms and various medical histories. Our proposed Pro-Q Gen could proactively pose queries to efficiently collect disease symptoms and medical history from the patients.}
\label{engage1}
\end{figure*}



\textbf{Quantitative Evaluation on Proactive Question Generator.} The query quality of the proposed Pro-Q Gen Model is evaluated on both language quality (Professionalism and Conciseness) and clinical professionalism (BLUEs and ROUGE) sides. In Table \ref{table4}, we compare the performance of the proactive questions generated by Pro-Q Gen with two state-of-the-arts, HuatuoGPT \cite{zhang2023huatuogpt} and Zhongjing \cite{yang2024zhongjing}. The Pro-Q Gen achieved higher performance on all evaluation metrics, showing its effectiveness in query generation. We further conduct an ablation study to show the necessity of our clinical concept knowledge graph on dialogue generation. As shown, introducing the clinical concept knowledge graph can significantly enhance clinical professionalism in query generation, with around $10\%$ performance improvements. 

\begin{table*}[t]
\centering

\caption{Baseline comparison and ablation study on proactive dialogue generation. The best performance is highlighted by \textcolor{red}{red}. The second best performance is highlighted by \textcolor{blue}{{blue}}.}
\label{table4}
\scalebox{0.8}{
\begin{tblr}{
  cell{2}{1} = {c},
  cell{2}{2} = {c},
  cell{2}{3} = {c},
  cell{2}{4} = {c},
  cell{2}{5} = {c},
  cell{2}{6} = {c},
  cell{2}{7} = {c},
  cell{2}{8} = {c},
  cell{3}{1} = {c},
  cell{3}{2} = {c},
  cell{3}{3} = {c},
  cell{3}{4} = {c},
  cell{3}{5} = {c},
  cell{3}{6} = {c},
  cell{3}{7} = {c},
  cell{3}{8} = {c},
  cell{4}{1} = {c},
  cell{4}{2} = {c},
  cell{4}{3} = {c},
  cell{4}{4} = {c},
  cell{4}{5} = {c},
  cell{4}{6} = {c},
  cell{4}{7} = {c},
  cell{4}{8} = {c},
  cell{5}{1} = {c,m},
  cell{5}{2} = {c,fg=red},
  cell{5}{3} = {c,fg=red},
  cell{5}{4} = {c,fg=blue},
  cell{5}{5} = {c,fg=blue},
  cell{5}{6} = {c,fg=blue},
  cell{5}{7} = {c,fg=blue},
  cell{5}{8} = {c,fg=blue},
  cell{6}{1} = {c},
  cell{6}{2} = {c,fg=blue},
  cell{6}{3} = {c,fg=blue},
  cell{6}{4} = {c,fg=red},
  cell{6}{5} = {c,fg=red},
  cell{6}{6} = {c,fg=red},
  cell{6}{7} = {c,fg=red},
  cell{6}{8} = {c,fg=red},
  hline{2-3,7} = {1-8}{},
}
                   &                                           &                                           &                         &                         &                         &                         &                         &  &  \\
                   & Professionalism $\uparrow$                           & Conciseness $\uparrow$                              & BLEU1 $\uparrow$                  & BLEU2 $\uparrow$                 & BLEU3 $\uparrow$                & BLEU4 $\uparrow$                  & ROUGE   $\uparrow$               &  &  \\
Huatuo (2023)            & 0.859                                     & 0.746                                     & 0.152                   & 0.066                   & 0.036                   & 0.023                   & 0.163                   &  &  \\
Zhongjing (2024)          & 0.641                                     & 0.682                                     & 0.115                   & 0.049                   & 0.024                   & 0.012                   & 0.119                   &  &  \\
{Pro-Q Model (Ours) \\ w/o knowledge graph} & \textbf{0.888} & \textbf{0.839} & {0.555} & {0.472} & {0.422} & {0.384} & {0.501} &  &  \\
Pro-Q Model (Ours)            & {0.880}                                    &  {0.826}                                  & \textbf{\textbf{0.622}}                            & \textbf{\textbf{0.532 }}                   & \textbf{\textbf{0.476}}                   & \textbf{\textbf{0.434}}                   & \textbf{\textbf{0.537}}                   &  &  \\
                   &                                           &                                           &                         &                         &                         &                         &                         &  &  \\
                   &                                           &                                           &                         &                         &                         &                         &                         &  &  
\end{tblr}}
\end{table*}

\subsection{Evaluation on Robustness of ProMRVL}

\textbf{Performance on a New Dataset.}
We conduct experiments on a similar Xray dataset, the IU-Xray dataset to show the robustness of our proposed ProMRVL-CAD system. The study cases with two views (frontal and lateral) images for the IU-Xray were selected and formed a new IU-V2 dataset.
Additionally, we create a sub-dataset of the MIMIC-CXR dataset, namely MIMIC-V2, which contains the study cases with two views.
In Table \ref{table2}, we show our evaluation results conducted on MIMIC-V2 and IU-V2 with different settings. 

%\definecolor{Shark}{rgb}{0.117,0.117,0.121}
\begin{table*}[t]
\centering
%%\vspace{-10pt}
\caption{Baseline comparison on diagnostic report generation on two datasets with various experimental settings. \textcolor{red}{Red} highlights the best performance and \textcolor{blue}{{blue}} highlights the second best performance.}
\label{table2}
\scalebox{1}{
\begin{tblr}{
  column{3} = {c},
  column{4} = {c},
  column{5} = {c},
  column{6} = {c},
  cell{1}{2} = {c=2}{},
  cell{2}{1} = {r=5}{c},
  cell{2}{2} = {r=5}{c},
  cell{2}{6} = {fg=blue},
  cell{2}{7} = {fg=red},
  cell{3}{6} = {fg=blue},
  cell{3}{7} = {fg=red},
  cell{4}{6} = {fg=blue},
  cell{4}{7} = {fg=red},
  cell{5}{6} = {fg=blue},
  cell{5}{7} = {fg=red},
  cell{6}{6} = {fg=blue},
  cell{6}{7} = {fg=red},
  cell{7}{1} = {r=5}{c},
  cell{7}{2} = {r=5}{c},
  cell{7}{6} = {fg=blue},
  cell{7}{7} = {fg=red},
  cell{8}{6} = {fg=blue},
  cell{8}{7} = {fg=red},
  cell{9}{6} = {fg=blue},
  cell{9}{7} = {fg=red},
  cell{10}{5} = {fg=blue},
  cell{10}{7} = {fg=red},
  cell{11}{6} = {fg=blue},
  cell{11}{7} = {fg=red},
  hline{1-2,7,12} = {-}{},
}
              &                           &       & {ChatCAD+\\(2023)} & {R2GenGPT\\(2024)} & {MVP-DR\\(w/o Textual Input)} & {~ ~ MVP-DR\\(w/ Textual Input)} \\
{MIMIC\\~-V2} & {Report \\text \\quality} & BLEU1 $\uparrow$  & 0.228                     & 0.430                     & {0.438}            & ~ ~ ~ ~\textbf{0.481}          \\
              &                           & BLEU2 $\uparrow$  & 0.139                     & 0.290                     & {0.295}            & ~ ~ ~ ~\textbf{0.364}          \\
              &                           & BLEU3 $\uparrow$  & 0.094                     & 0.207                     & {0.212}                    & ~ ~ ~ ~\textbf{0.293}          \\
              &                           & BLEU4 $\uparrow$  & 0.068                     & 0.154                     & {0.159}                    & ~ ~ ~ ~\textbf{0.245}          \\
              &                           & ROUGE $\uparrow$  & 0.227                     & 0.318                     & {0.321}                    & ~ ~ ~ ~\textbf{0.392}          \\
IU-V2         & {Report \\text \\quality} & BLEU1 $\uparrow$  & 0.332                     & 0.454                     & {0.491}            & \textbf{~ ~ ~ ~0.522}          \\
              &                           & BLEU2 $\uparrow$ & 0.180                     & 0.294                     & {0.306}            & \textbf{~ ~ ~ ~0.375}          \\
              &                           & BLEU3 $\uparrow$ & 0.108                     & 0.211                     & {0.214}            & \textbf{~ ~ ~ ~0.293}          \\
              &                           & BLEU4 $\uparrow$ & 0.006                     & {0.160}             & 0.157                    & \textbf{~ ~ ~ ~0.242}          \\
              &                           & ROUGE $\uparrow$ & 0.250                     & 0.366                     & {0.375}            & \textbf{~ ~ ~ ~0.435}          
\end{tblr}}
\end{table*}


\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{Figures/Fig6_resCompare2.png}
%%\vspace{-10pt}
\caption{Comparison of the clinical efficacy (recall) of the top-6 diseases in the MIMIC-V2 dataset.}
\label{compareRecall}
%%\vspace{-20pt}
\end{figure*}
We confirm that the MVP-DR outperforms existing methods. Moreover, the textual input can further improve the performance of MVP-DR, using the same image input. Note that the text quality in Table \ref{table2} is higher than the multi-view results in Table \ref{ablation_comp}. This is because MIMIC-V2 is a subset of the MIMIC dataset where each data subject has two images. For comparison, the dataset used in Table \ref{table2} has a portion of subjects that only has one image to ensure a fair comparison with other approaches. The discrepancy between Table \ref{table2} and Table \ref{ablation_comp} highlights the need of building a multi-view dataset for diagnosis. Besides, the MVP-DR achieves much higher recall in detecting the top-6 diseases (lung opacity, pleural effusion, atelectasis, pneumonia, cardiomegaly, and edema) in the MIMIC-CXR dataset as shown in Figure \ref{compareRecall}, which takes up  $16.9\%$,  $14.8\%$,  $13.5\%$,  $13.3\%$,  $11.1\%$, and  $10.2\%$ of the positive cases in the MIMIC-V2 dataset. This indicates that our proposed MVP-DR Gen is less likely to generate misdiagnosed medical reports. Lastly, our model achieves similar high performance on both datasets, which evidently shows its generalization capability on diagnostic report generation. 



\textbf{Scalability and Complexity}.
ProMRVL is generic and it can be easily scalable to other modules in terms of embedding, alignment, language model, etc. Streamlining the system's architecture could facilitate easier deployment and maintenance, enhancing its scalability. We notice that the integration of text input with a vision model in ProMRVL can also improve performance in ChatCAD+ and R2GenGPT, as we demonstrated in Table \ref{ablation}. To further improve the scalability, we conduct additional experiments to demonstrate the current network has room for a simplified implementation. We further reduce the complexity of the system using LoRA \cite{Hu2021LoRALA, balazy2024lora}
for Vision and LLM models. This approach reduces the parameters from \textbf{90.9M} to \textbf{5M}. The model performance, after reducing complexity nearly 20 times less, is satisfactory (\textbf{less than 4\%} overall performance drop) as shown in Table \ref{textual_model_scales}.

\begin{table*}[t]
\centering
\caption{Comparison the proposed model on different network scales.  \textcolor{red}{Red} highlights the best performance.}
\label{textual_model_scales}
\scalebox{1}{
\begin{tabular}{llllll}
\hline
                  & BLEU1          & BLUE2            & BLUE3           & BLEU4            & ROUGE          \\
\hline
Original (90.9 M) & \textcolor{red}{\textbf{0.430}} & \textcolor{red}{\textbf{0.305}}   & \textcolor{red}{\textbf{0.231}} & \textcolor{red}{\textbf{0.182}} & \textcolor{red}{\textbf{0.331}} \\
Simplified (5M)   & 0.413 & 0.295 & 0.230  & 0.189  & 0.323 \\
\hline             &                &                  &                 &                  & 
\end{tabular}
}
\end{table*}


\begin{table*}[t]
\caption{Evaluation on robustness of ProMRVL against image variation. \textcolor{red}{Red} highlights the best performance and \textcolor{blue}{{blue}} highlights the second best performance.}
\label{exp_noise_input}
\centering
\scalebox{1}{
\begin{tabular}{llllll}
\hline
            & BLEU1 & BLEU2 & BLEU3 & BLEU4 & ROUGE \\
\hline
Original dataset       & \textcolor{red}{\textbf{0.430}}           & \textcolor{red}{\textbf{0.305}}           & \textcolor{red}{\textbf{0.231}}           & \textcolor{red}{\textbf{0.182}}           & \textcolor{red}{\textbf{0.331}}           \\
Low resolution & 0.371           & 0.222           & 0.142           & 0.093           & 0.257           \\
Blurred        & \textcolor{blue}{{0.382}}           & \textcolor{blue}{{0.266}}           & \textcolor{blue}{{0.202}}           & \textcolor{blue}{{0.163}}           & \textcolor{blue}{{0.294}}           \\
\hline               &                &                &                &                &          
\end{tabular}
}
\end{table*}

\textbf{Robustness to Data Variability}. The generalization capability of our proposed method also lies in its robustness on data variability for different dataset. To test the robustness of ProMRVL, we conduct preliminary studies on two types of variations: 1) \textbf{low-resolution.} we tested the performance of ProMRVL on a dataset that the spatial resolution of is half of the original dataset; 2) \textbf{noisy images}. We examine the performance of ProMRVL on a dataset that the noise is added on original dataset. In this preliminary study, we set the noise as 7 photons variation at each pixel. The results are reported in In Table \ref{exp_noise_input}. Overall, the results show that our model is robust to the variations. For example, in low resolution case, BLEU1 is maintained to \textbf{86\%} of the performance on original dataset and in image with noisy quality, BLEU1 is maintained as \textbf{88.38\%} of the performance obtained from original dataset.
