Automatically generating patient-friendly diagnostic reports is crucial for mitigating clinical shortages and facilitating patient-doctor communication. Despite increasing interest, few studies have investigated building reliable interactive medical dialogue systems for automatic diagnostic report generation, especially with the consideration of both clinical visuals and medical history. Existing work on interactive medical services mainly targets question-answering (QA) tasks \cite{srivastava2020automatized,singhal2023towards,singhal2023large,chen2023meditron}, which only takes textual inputs but leaves visuals unsupported. However, medical images, such as radiography, are considered critical references for disease diagnosis due to their rich visual and textual features. Thus, these models could only serve as \textit{knowledge retrieval systems} to answer health-related questions, rather than \textit{medical dialogue systems} for diagnostic report generation. Another direction of studies is to directly integrate vision-language models into computer-aided systems to support visual inputs on disease diagnosis \cite{zhao2023chatcad+,zhou2023skingpt4,zhu2023minigpt4,chen-etal-2021-cross-modal}. Due to restricted scopes, these studies could only provide \textit{passive} responses to questions and \textit{minimal visual} interactions with the patients in which only a single visual input serves as the starting point of diagnosis. As such, these approaches merely rely on the evidence from medical visuals, while entirely ignoring the medical history and symptoms of the patients for disease diagnosis. This will lead to unreliable diagnostic results as the latter are considered critical clues for real-world clinical practices \cite{fukuzawa2024importance, sandler1980importance}. 

Similar to real-world clinical trials, a reliable medical diagnostic/dialogue system should mimic the procedure of proactive dialogue during regular patient-doctor interactions to collect medical history and symptom information for diagnostic references, rather than simply based on the medical visuals. Herein, performing proactive features to guide instructive conversations in computer-assisted diagnosis systems is necessary. Without effective queries, patients can hardly provide accurate and sufficient disease-associated descriptions, due to a dearth of domain expertise. The development of proactive features requires the model to find the logic and coherence behind the questions and responses, which is different from the conventional vision-language tasks such as visual question answering (VQA) and image-to-text generation \cite{antol2015vqa, wu2022medical, goyal2017making,tewel2022zerocap, ben2019vqa} that mainly targets the questions/descriptions related to the input visuals. This adds additional challenges to the development of proactive medical dialogue systems, as it seeks models with professional capability to understand context, perform interactions, and communicate logically.

So far, efforts to apply large language models (LLMs) \cite{openai2023gpt,hoffmann2022training,touvron2023llama,chowdhery2023palm,lee2024llmcxr} for automatic visual description generation have demonstrated promise in computer-aided diagnosis. However, the deployment of LLMs in proactive medical dialogue systems is still in its infancy. Existing work in \cite{wang2023chatcad, zhao2023chatcad+} developed the first medical dialogue system to provide medical advice on three imaging domains using large foundation models. Along with clinical concepts and doctors’ notes, Zhou et al \cite{zhou2023skingpt4} developed an interactive dermatology diagnostic system by fine-tuning Mini-GPT \cite{zhu2023minigpt4} using large skin disease images to generate skin disease reports. Lee et al \cite{lee2024llmcxr} proposed an instruction-tuning strategy to enhance LLMs' understanding of medical images by expanding their reasoning capability on input visuals. While powerful, these previous studies mainly chose a passive interaction scheme, in which the ChatBot could only passively respond to the questions proposed by the patient. Hence, these models fail to capture the medical history or disease-associated information from the patients and thus lead to unsatisfactory diagnostic reports.

As discussed above, existing solutions for medical diagnosis generation are less ideal, due to the lack of proactive patient-ChatBot interactions to collect disease-associated information. To bridge this gap, we propose an LLM-based dialogue system, namely proactive multi-round vision-language interactions for computer-aided diagnosis (ProMRVL-CAD), to reliably perform disease diagnosis with both visual evidence and patients' information. Guided by a knowledge graph-based recommendation system, ProMRVL-CAD supports multi-round interactions on a mixture of multi-view images and text inputs to enhance diagnosis performance, showing promise to outperform methods taking a single image for the entire diagnosis process. In Figure \ref{fig1_high_level}, we highlight the distinction between our proactive medical dialogue system and existing studies. Our major contributions are summarized as follows:

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{Figures/Fig1_high_level.png}
\caption{High-level comparisons between existing works and ProMRVL-CAD when medical images are necessary for the diagnosis/treatment. \textbf{a):} Question-Answering (QA) task; \textbf{b):} Visual QA task; \textbf{c):} ProMRVL-CAD (Ours). By supporting multi-round text-visual inputs, ProMRVL-CAD mimics the consultation process between doctors and patients to collect diagnosis-orientated information. In contrast, existing frameworks simply use the first visual for report generation and then passively answer questions from the patients (QA) or only propose questions related to the input visuals (VQA). }
\label{fig1_high_level}
\end{figure}

\begin{itemize}
    \item We propose a novel Proactive Question Generator (Pro-Q Gen) to effectively collect critical inputs for disease diagnosis. The proactiveness of Pro-Q Gen stems from a knowledge graph built upon two real-world clinical datasets and a recommendation system to lead a productive medical information collection. This is significantly different from previous arts that can only perform diagnosis on input visuals. To our best, this study proposes the first proactive medical dialogue system for diagnostic report generation. 
    
    \item We propose a Multi-Vision Patient-Text Diagnostic Report Generator (MVP-DR Gen) that integrates multi-view visual and textual features without substantial modifications to existing LLMs. This integration of multi-view medical images and text enhances the effectiveness of multi-round proactive dialogues by mimicking a diagnostic process where a doctor considers the patient’s medical images, dialogue, and medical history. 

    \item Evaluating capability on two real-world publicly available datasets, MIMIC-CXR \cite{DBLP:journals/corr/abs-1901-07042} and IU-Xray \cite{DinaDemnerFushman.2015}, we validate the report quality and clinical efficiency on generated diagnostic reports. The experiments demonstrate the proactive interaction capacity of ProMRVL-CAD as well as its state-of-the-art performance on medical diagnostic report generation. We also validated the robustness of our framework on dataset with noisy and low resolution image resources.

    \item We develop a synthetic medical dialogue dataset, ProDial, containing multi-round conversations to mimic the proactive diagnostic interactions between patients and doctors in clinical practices. Based on the medical history and findings from the MIMIC-CXR dataset \cite{DBLP:journals/corr/abs-1901-07042}, the generated dialogue reflects the patients' information but carries complementary information to the medical visuals for disease diagnosis. This synthetic image dataset with dialogue is the first kind of this type in medical image analysis, addressing a need to pre-train large vision language model.   
    
\end{itemize}
