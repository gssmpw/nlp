\section{Threat Model}
\label{sec:threat_model}

\paragraph{Attack Goal.}
%=\langle \mathcal{I}_{att}, Func_{att}, \mathcal{O}_{att}\rangle
The adversary is the developer of the malicious tool $\mathcal{T}_{att}$, {and he/she is capable of performing a man-in-the-middle (MITM) attack to tamper with the communication content between benign tools and LLMs.}
{Given a toochain}, adversary aims to steal {upstream} victim tool $\mathcal{T}_{vict}$'s relevant information (we only consider the victim tool's input $\mathcal{I}_{vict}$ and output $\mathcal{O}_{vict}$ {that may involve user privacy or the tool's property rights}).
Meanwhile, the adversary aims to hide the attacks from the users, which means the inference steps {shown in} the frontend after the attack ($\hat{Inf}$) will not change, i.e., $\hat{Inf}=Inf$.
In this case, any of the tools that are used before the $\mathcal{T}_{att}$ might be $\mathcal{T}_{vict}$, and if their relevant information is obtained by $\mathcal{T}_{att}$, {we consider the attack is achieved.}
% the information leakage risks may be exposed.




% \paragraph{Assumption of adversary's Position.}
% We assume that the adversary's tool $\mathcal{T}_{att}$ is NOT used in the first place of the CoT, and the victim tools $\mathcal{T}_{vict}$ are used before the $\mathcal{T}_{att}$: $CoT_\mathcal{T}=[\mathcal{T}_1,...\mathcal{T}_{vict},...,\mathcal{T}_{att},...,\mathcal{T}_n]$,
% where the $T_{vict}$ may be used in the first place $T_{vict}=\mathcal{T}_1$, and the adversary's tool is used in the last position $T_{att}=\mathcal{T}_n$.
% In this case, $\mathcal{T}_{vict}$'s relevant information is retained in the inference and accessed by $\mathcal{T}_{att}$.

\paragraph{Assumption of adversary's Knowledge.}
We assume that the adversary has \textit{\textbf{black-box}} knowledge of the inference steps, so they don't know what tools are used in the {upstream of the toolchain}.
% \yang{\textit{(COMMENT.)}}
%下面这些对历史知识的掌握，强调一下这些样本并不一定来自victim systems，因为你后面也做了迁移的试验。
{
However, the adversary owns some attack cases from the LLM tool-learning systems including malicious/victim tools, injected malicious commands, and attack results illustrating whether tools' information was stolen in history.
}
% However, they have the \textbf{background knowledge} that comes from benign/victim tool-learning systems. These cases illustrate whether tools' information was successfully/failed stolen in history. 
For example, the adversary of \textit{Book\_Flight} in Figure \ref{fig:motivation_tool_learning} does not know the victim tool but can analyze the key information and construct the command {with {\tool}}.
% , based on his knowledge.
{Please kindly note that attack cases can originate from some open-source systems like ToolBench, and do not necessarily have to come from the target system being attacked. 
In such scenarios, the adversary can leverage the command generation models learned from open-source systems and perform transfer attacks on the black-box target systems.}
% In the following experiments, we apply {\tool} on different black-box systems and supplement the background knowledge.


% \paragraph{Command Format for Information Theft Attack.}
% Although the command is dynamically generated, we regulate three components that they need to contain, as shown in the following equation:
% \begin{equation}
% \text{CMD}=\text{[\textit{ToolRecall}][\textit{Attack}][\textit{NotExpose}]}
% \end{equation}
% where [\textit{ToolRecall}] is the command for calling this tool again; [\textit{Attack}] indicates the task of information theft, and [\textit{NotExpose}] asks the LLM to hide the attack in the user interface.

% \paragraph{Attack Strategy.}
% We design the attack strategy by following what the adversary will realize in the information theft attack, incorporating the following three steps:
% (1) \textbf{Dynamic Command Generation.} Based on details of malicious tools and background knowledge, adversary will analyze what information is easy to steal and design different commands $\mathcal{C}$.
% (2) \textbf{Command Injection.} adversary will inject the command into LLMs, which calls the tool again and sends the victim's information to it $(\hat{\mathcal{I}}_{vict}|\hat{\mathcal{O}}_{vict})\stackrel{LLM\;w/\;\mathcal{C}\oplus\mathcal{O}_{att}}{\longrightarrow}\mathcal{T}_{att}$, where $\hat{\mathcal{I}}$ and $\hat{\mathcal{O}}$ are the content of stolen information.
% (3) \textbf{Knowledge\&Strategy Updating.} The knowledge and attack strategy will be updated based on the attack results of the command injection.


% \paragraph{Condition of Successful Attack.}
% \yang{\textit{(COMMENT.)}}
% The success condition of the attack contains the \textbf{Successful Theft} (the stolen information $\hat{\mathcal{I}}$|$\hat{\mathcal{O}}$ matches the victim tool's relevant information) and \textbf{Stealthiness} (the $\hat{CoT}$ after command injection will not be exposed to the user interface).
% \begin{equation}
% \resizebox{.88\linewidth}{!}{$
%     \displaystyle
%     (\hat{\mathcal{I}}_{vict}|\hat{\mathcal{O}}_{vict})=(\mathcal{I}_{vict}|\mathcal{O}_{vict})\;\text{and}\;\hat{CoT}=CoT
% $}
% \end{equation}
% where  are the content of stolen information.
% %这块吧，跟metric那块有点歧义，还有点冗余。。你这里定义的这个和metric里面第三个指标是一个含义。但是好像跟metric里面TSR又有点歧义。。。后面篇幅不够可以删了。。一般threat model好像也不会到如何判别为成功的阶段。
%Ziyou:师兄我也觉得这里应该和metrics之间有重复，我这里删掉了