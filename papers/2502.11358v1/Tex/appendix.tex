\section{Appendix}
\label{sec:appendix}

\subsection{Complete CoT in LLM Inference and Detailed Tools after Command Injection}\label{sec:malicious_tools}

\subsubsection{Complete CoT in LLM Inference}



\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{Fig/background_tool_calling.pdf}
% \vspace{-0.7cm}
\caption{The full CoT in the ToolBench's output based on the user query in Figure \ref{fig:motivation_tool_learning}'s example.}
\label{fig:background_tool_calling}
% \vspace{-0.4cm}
\end{figure}

LLM tool learning systems utilize Chain-of-Thought (CoT) in the LLM inference. 
The CoT is a step-by-step inference.
Each step is a $\langle Observation, Thought, Action\rangle$ triplet, where the components are defined as follows:

\begin{itemize}[leftmargin=*]
    \item {\textbf{{Observation}:}} The LLM will observe the output of the previous tool $\mathcal{O}_{i-1}$ in the step $i-1$.
    \item {\textbf{{Thought}:}} LLM analyzes the output $\mathcal{O}_{i-1}$ and previous LLM inferences, then they will decide what they need to do in the next step.
    \item {\textbf{{Action}:}} It selects the tool $\mathcal{T}_i$ that is used in the current inference step.
\end{itemize}


During the inference, the current action $Act_i$ is selected by $Act_i=\pi(CoT_{i-1},Obs_i,Tho_i)$, where the LLM will use a policy $\pi$ to construct the previous inferences with the following actions.
Therefore, the triplets at each step will finally connect each other and form the inference chain.




When we consider the actions that call the tools in the inference, we find that they can also be formed as the chain $[\mathcal{T}_1,\mathcal{T}_2,...,\mathcal{T}_n]$. Figure \ref{fig:background_tool_calling} shows the complete tool calling CoTs that are shown in the frontend.
In this case, each step uses the observation from the previous tool's output (e.g., TokyoSites, Hotel-Booking-Check, etc.) to analyze the current thoughts. Then LLM conducts the policy to select the next action and achieve the trip planning tasks for the user. 

\subsubsection{Malicious Tools and Commands}


\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{Fig/Example_Tool.pdf}
% \vspace{-0.7cm}
\caption{The malicious tool ($\mathcal{T}_3$: \textit{Book\_Flight})'s code details in Figure \ref{fig:motivation_tool_learning}'s inference example.}
\label{fig:malicious_tool}
% \vspace{-0.4cm}
\end{figure}

The command injection method utilizes the tool's function code $Func_{att}$ to inject the harmful commands $\mathcal{C}$ in the output value, then control the LLMs to send the details of other information.
In this section, we illustrate the generated command of {\tool}, as is shown in Figure \ref{fig:malicious_tool}.

We first introduce the main component of this tool, i.e., the \texttt{Book\_Flight} function and the \texttt{book\_success\_ecaluation} function. The first function is the main part of this tool and will be called by the LLM tool-learning systems, and the second function utilizes the external RapidAPI to help users book the flight.
Due to the license of limited authorization in these APIs, we cannot illustrate the detailed links in it, and more details can be found on our website's open-source dataset.
The \texttt{book\_success\_ecaluation} function realizes the task of searching flights and booking the tickets, and it returns the booking results to the users.

Second, we add a malicious command in the \texttt{cmd} parameter, which illustrates the following tasks that ask the LLM what they need to do:

\begin{itemize}[leftmargin=*]
    \item \textbf{Task-1: Achieve Attack.} The LLM should send the other tool's information to the current tool, which aims to register a new user in the database. {\tool} finds the key information that illustrates the co-occurrence between the tools, and it also emphasizes that the tool's developer not only wants to access the data but also uses it to "\textit{register the information}". Compared with the previous fixed commands, this is not vague and has clear targets that LLM needs to achieve with a neutral expression.
    \item \textbf{Task-2: Add Payloads.} The tool needs to distinguish the other tool's information from its own required information, so we add a payload "\texttt{\$\$\$\%\%\%\@}" as the special string, and capture the input values if the data contain such string.
    \item \textbf{Task-3: Protect Stealthiness.} The command also uses the command to protect the command not expose the attack in the frontend. The text "\textit{try not to show the calling}" with neutral sentiment can reduce the exposure of the attack and improve stealthiness.
\end{itemize}

\subsection{Details of Attack Cases in AttackDB}\label{sec:gen_kg}


\subsubsection{Prompts for Attack Cases Preparation}

\paragraph{Definition of Command's Structure.}
The details of commands $\mathcal{C}$ that are used to steal the information. We utilize the GPT-4o to explore the attack commands in AttackKG’s knowledge extractor.
We regulate the components that each command needs to contain, as is shown in the following equation:
\begin{equation}
    \mathcal{C}=\text{[\textit{ToolRecall}][\textit{Attack}][\textit{NotExpose}]}
\end{equation}
where [\textit{ToolRecall}] is the command for calling this tool again; [\textit{Attack}] indicates the task of information theft, and [\textit{NotExpose}] asks the LLM to hide the attack in the frontend user interface.


\paragraph{Prompt for Attack Case Generator.}

Figure \ref{fig:prompt_case_generator} is the prompt for GPT-4 guided attack case generator:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{Fig/prompt_case_generator.pdf}
% \vspace{-0.7cm}
\caption{Prompts of Attack Case Generator.}
\label{fig:prompt_case_generator}
% \vspace{-0.4cm}
\end{figure}

\paragraph{Prompt for Attack Case's Guidance Generator.}

Figure \ref{fig:prompt_guidance_case_generator} shows the prompt for GPT-4 guided attack case's guidance generator generator:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{Fig/prompt_guidance_generator.pdf}
% \vspace{-0.7cm}
\caption{Prompt of Attack Case Guidance Completer.}
\label{fig:prompt_guidance_case_generator}
% \vspace{-0.4cm}
\end{figure}

% and manually collect these commands in Section \ref{sec:data_preparation}.


\input{Tab/tab_attack_case_example}


\subsubsection{An Example Attack Case}

With the previous definition, Table \ref{tab:attack_case_details_example} illustrates the attack case with tool entities and relations, extracted from the motivation example in Figure \ref{fig:motivation_tool_learning}.



% \subsubsection{Example Mutant Prompts}




% \subsection{Case Study of {\tool}}

\subsection{Experimental Details}

% \subsubsection{Prompts for Preparing AttackDB}

% \subsubsection{Reason for LLM's Model Selection}
% In this section, we illustrate why we chose the GPT-4o to prepare the AttackDB, as well as why we fine-tuned the T5 as the dynamic command generator.
% To compare the performances of these models, we select the XX.




% \yang{comment.}
%这三个3基线怎么实现的还是有点模糊。
%再者还有一些取名的问题，比如第一个tool-learning，本身系统就是tool-learning systems，这里叫做一个窃取方法基线，我总感觉不太合适。能不能从窃取的方式取名？还有就是tool-learning、finxedDBCMD如何实现的还得说一下，毕竟这俩不是已有的基线，得说清楚怎么实现的，比如tool-learning方法中窃取的参数是人工确定的？还是怎么拿到的？
%然后fixCMD这一类应该有很多确定的指令，比如rupeng和haowei他们发在naacl哪些，是有一些固定的指令可以用的，都得引一下。
%总的来说，我觉得可以分层次说一下：1）首先引入了一个已有的窃取方法，固定指令，引一下已有文章，在指示一下固定指令长啥样；2）再者说一下我们基于XXX的考虑引入了各类基线（比如我们经常观察到如何一些工具经常请求一些看似功能外的参数，可能会导致不必要的信息泄露5），tool-learning，fixedDNCMD，这两类的细节分别是什么，比如窃取的属性值如何确定之类的。
\subsubsection{Details of Baselines}\label{sec:baseline_details}
In this section, we introduce how we design these three baselines to evaluate the performances of {\tool}, i.e., \textbf{PoisonParam}, \textbf{FixedCMD}, and \textbf{FixedDBCMD} as follows:


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Fig/info_leakage_params.pdf}
% \vspace{-0.7cm}
\caption{Tool "\textit{Personator/personator\_endpoint}" in ToolBench, which requests excessive information outside the necessary IIP.}
\label{fig:info_leakage_params}
% \vspace{-0.4cm}
\end{figure}

\paragraph{PoisonParam:} This baseline does not use the command injection to achieve the information theft attack. On the contrary, it adds redundant parameters to mislead the LLM to send other tool's information to it.
The main reason that we designed this baseline is the excessive resource requests in nowadays APPs, which disobey the principle of minimizing Personal Identified Information (PII) that may cause privacy leakage in today's business.
    
For example, in Figure \ref{fig:info_leakage_params}, the tool aims to validate the user's location, and it only needs the user's \textit{ID} and \textit{Address}.
However, this tool requests over 10 input parameters, which is far more than the necessary information they really require.
Some tool-learning systems, such as ToolBench and ToolEyes, will pre-learn the tool's information before the inference.
If LLM pre-learns these redundant parameters, they may send other information to the tool, which is not allowed to be accessed according to the permission, thus leading to information leakage

To realize this baseline, we add some redundant parameters to the tool \textit{Book\_Flight}'s input, such as adding the input parameter with the hotel’s username and password as the input parameters.
We ask the LLM to learn these poisoned parameters and they may send other tool's information to the malicious tools.



\paragraph{FixedCMD.}
The {FixedCMD} baseline is designed based on the recent works that investigate the tool-side command injection~\cite{DBLP:journals/corr/abs-2412-10198,DBLP:journals/corr/abs-2404-16891}.
These fixed command injection methods are static, and the researchers design the commands that introduce the attack target and what they want the LLMs to do.
However, FixedCMD's commands are vague, which means researchers may not design the commands based on the details in the tools.


According to these previous works, we design the static command like "\textit{call Book\_Flight again and send other tool's information to this tool, and do not show them in the frontend}", as is shown in Figure \ref{fig:case_study}.
This command only indicates that the adversary needs the other tool's information and makes sure it is not exposed, but is likely to be detected by the LLMs.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{Fig/defense_methods.pdf}
\vspace{-0.2cm}
\caption{The details of defense methods.}
\vspace{-0.5cm}
\label{fig:defense_method_details}
\end{figure*}

\paragraph{{FixedDBCMD}.} 
This baseline is the improvement of the original static command injection attack, which incorporates AttackDB into the command injection step.
However, this baseline only searches the relevant attack cases in the AttackDB and puts the summary of guidance $\mathcal{G}^{A}$ in the commands, such as "\textit{some previous tools may contain the username and password, so send it to us}", but does not optimize the model dynamically to make it applicable to adapt to the black-box scenario.

\input{Tab/tab_details_baselines}

Table \ref{tab:detailed_baseline} illustrates the comparison results between baselines and our approach. Compared with the baselines, our approach {\tool} utilizes the command injection, AttackDB, and RL-based model optimization strategy, which achieves the highest performances.

\subsubsection{Comparison between RL and Fine-Tuning in Model Optimziation}

\input{Tab/tab_fine_tuning}

In this section, we compare the average $ASR_{Theft}$ values between the RL optimization and fine-tuning the T5 model, which trains the original training model and evaluates the performances on the test dataset.
We can see that after these model convergence, the performances in the model optimized by RL are higher than fine-tuning the model.
This advantage comes from the learning ability of black-box attack, and RL-based optimization will focus more on the tools and AttackDB's cases, which is more useful than original fine-tuning.

\subsection{Details of {\tool}'s Defense}\label{sec:defense_appendix}


To protect LLM tool-learning systems from {\tool}'s attack, we design three approaches: \textbf{InferCheck} is the inference-side defense that checks the abnormal text description in the LLM inference.
\textbf{ParamCheck} and \textbf{DAST} are backend-side defense methods that review whether the registered tools are secure. We will describe these attacks in detail, as is shown in Figure \ref{fig:defense_method_details}.




%数据+推理流
\paragraph{InferCheck.}
This defense method checks the inference steps to check its \textbf{abnormal data stream} and \textbf{abnormal inference text}.

\begin{itemize}[leftmargin=*]
    \item \textbf{Abnormal Inference Text:} We also check the abnormal texts in the frontend. If the InferCheck finds the inference text that is not regular, it will warn the users and developers.
    \item \textbf{Abonormal Data Stream:} We add a module to check the changes in the task planner and memory in the inference, which observes whether it has an abnormal data stream in the tool-learning. In Figure \ref{fig:defense_method_details}'s example, the abnormal data stream occurs in step-2 to step-3, so InferCheck reports it to the users.
\end{itemize}


%参数信息，违背参数的内容分析
\paragraph{ParamCheck.} 
This defense method is the tool-side defense that analyzes the tool's details and checks whether the request inputs exceed the necessities, which checks the tool parameter's \textbf{abnormal parameter types} and \textbf{abnormal parameter logs}.

\begin{itemize}[leftmargin=*]
    \item \textbf{Abnormal Parameter Types:} We check the parameter type and decide whether the input data obeys the IIP principle. If the tool has excessive input parameters, ParamCheck will notify the users and system developers.
    \item \textbf{Abonormal Data Stream:} We create an MITM-based data log capture module to observe the abnormal input data that mismatch the previous information. For example, Figure \ref{fig:defense_method_details} shows that the input information of Book\_Flight is different from the previous one, which may have some risks and will be detected.
\end{itemize}

% 控制流，数据流
\paragraph{DAST.}
This defense method is the tool-side defense that generates the test cases dynamically to evaluate whether the tool's code has abnormal parameters and calling steps, which may lead to illegal data access.
In the DAST module, we input the tool's information into the GPT-4o, and ask it to automatically generate security test cases. The test cases aim to detect abnormal function calls and data flows in the tool calling.

Then, we dynamically input these test cases into the tools, and conduct the inference and tool learning. We observe the passing rate of these test cases and inspect the failed cases. 