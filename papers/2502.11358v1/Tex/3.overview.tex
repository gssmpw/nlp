\section{Overview of {\tool}}
% \yang{TO BE DONE AS DISCUSSED}

% \yang{\textit{(COMMENT).}}
%这块可以补1-2为啥通过历史样本的反馈强化，能够实现更有效信息窃取的core idea。比如在工具集合范围内，工具调用通常具有一定的规律或者特征，调用当前工具的时候通常都是有一定的前置。能够通过历史信息大概推测出调用当前工具的时候，前面大模型和其他工具完成过什么任务，交互过什么信息。比如调用一个订酒店tool的时候，前面大概调用过定飞机/火车（可能有更好的例子）。因此通过历史样本的学习，可以让你的攻击方法具有一定的泛化。大概花个1-2句话，让人先从直觉上get到你方法起效的原理。紧接着再说你要处理历史case，然后做强化。
%然后两个关键阶段，攻击图谱和强化，在这里写的时候，在重点明确一下他们的目标和产出是什么，用哪些样本评估、优化哪些（比如最后一句话），都可以不要。这一段让大家先从高层有一个目标认知。比如，指示图刻画了什么内容，为什么需要处理成KG，KG对于后面强化的意义。强化那部分也类似哈。现在读起来有点没法从high-level把握住你两步的目标是什么。
{Within a toolset, the invocation chains often exhibit certain patterns and regularities when processing different user queries. 
When invoking a specific tool, there are usually certain prerequisites or preconditions. }
For example, a tool for hotel reservation in LLM inference may be invoked simultaneously with the other tool for booking a flight/train ticket in the previous.
{
In such cases, it is generally possible to infer what tasks the upstream tools have completed in previous steps, as well as what information has been exchanged upstream, by learning from historical toolchains.
}
% In LLM tool learning systems, the invoked tools have some sequentially based logical relations in historical cases.
% \yang{
% {\tool} aims to achieve more effective information exfiltration by learning from examples generated by open-source tool learning systems. Based on the tool invoked at the current step, we infer the information keys of the upstream tools and generate more explicit instructions.
% }
% Learning from these cases, adversaries can design more reasonable attack commands for specific user queries and inference tasks.

{
Figure \ref{fig:model_tool_learning} shows the overview of {\tool}.
Guided by the concept, {\tool} first constructs AttackDB with attack cases that provide examples with key information to guide the generation of black-box commands.
After that, {\tool} incorporates AttackDB to train an initial command generation model, then reinforces it guided by the reward combined with attack results and the sentiment score of the generated command. 
}
% After that, {\tool} proposes the dynamic command generator and incorporates AttackDB to generate commands in the black-box scenario, then utilizes RL to continuously optimize this generator based on attack results.
% {In this section, we illustrate the framework of {\tool} in Figure \ref{fig:model_tool_learning}.
% Specifically, {\tool} first forms AttackDB with attack cases that provide examples with key information to guide the generation of black-box commands.
% Second, {\tool} proposes the dynamic command generator and incorporates AttackDB to generate commands in the black-box scenario, then utilizes RL to continuously optimize this generator based on attack results.
% In practice, the optimized model will utilize only the malicious tool's information and key information in AttackDB to generate effective info-theft commands.
% After these two steps, we apply the optimized dynamic command generator to the other black-box LLM tool learning systems to evaluate its practicability. 



% scenario=<vict/att, malicious tool>
% case=<vict, attack cases, malicious tool>
% caseDB=case1,case2,...,casen



\subsection{Attack-Case Database Preparation}
% To prepare the attack cases, we .
% In historical cases, we manually label whether the information theft attack is successfully achieved based on the specific commands.
Given inference examples $[E^A_1,E^A_2,...,E^A_n]$ that are used to generate attack cases, where $E^A_i$ is a white-box example with frontend inference and backend toolchain, we use white-box {Attack Case Generator} and {Attack Case's Guidance Completer} to prepare attack cases and form the AttackDB.


\paragraph{The Definition of Attack Cases.}
The attack case is a five-tuple array, which can be formalized as $\langle\mathcal{T}_{vict}^{A},\mathcal{T}_{att}^{A},\mathcal{C}^{A},\mathcal{R}^{A}\,\mathcal{G}^{A}\rangle$:
(1) $\mathcal{T}_{vict}^{A}$ and $\mathcal{T}_{att}^{A}$ are the victim and malicious tool's details and its relevant information, i.e., \textit{Tool's Name}, \textit{Description}, \textit{Function Code}, and \textit{Relevant Information to Attack}.
(2) $\mathcal{C}^{A}$ is the details of commands $\mathcal{C}$ that are used to steal the information.
% We regulate components that each command needs to contain as $\text{[\textit{ToolRecall}][\textit{Attack}][\textit{NotExpose}]}$,
% where [\textit{ToolRecall}] is the command for calling this tool again; [\textit{Attack}] indicates the task of information theft, and [\textit{NotExpose}] asks the LLM to hide the attack in the frontend.
(3) $\mathcal{R}^{A}$ is the result of whether the attack is successful and has stealthiness.
(4) $\mathcal{G}^{A}$ is the guidance that summarizes the current commands and attack results and finds the key information between the tools that may affect the attack success rate.
As is shown in Figure \ref{fig:example_db}, the key information in $\langle\mathcal{T}_2,\mathcal{T}_3\rangle$ indicates the commonalities between the tool's input value, and using some specific tasks such as "registration" can improve the success and stealthiness of this attack. 
We have illustrated more details in Appendix \ref{sec:gen_kg}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Fig/attack_case_db_example_v3.pdf}
\vspace{-0.6cm}
\caption{Example of attack cases.}
\label{fig:example_db}
\vspace{-0.6cm}
\end{figure}

% $\mathcal{G}=\{E_{att}, R_{att}\}$ are tool entities and relations with their attributes:
% \textit{\textbf{(1) Tool Entity $E_{att}$:}} The tool's details and its relevant information, i.e., \textit{Tool's Name}, \textit{Tool's Description}, \textit{Function Code}, and \textit{Relevant Information to Attack}.
% \textit{\textbf{(2) Relation $R_{att}$:}} The direction of the information theft, which starts from $\mathcal{T}_{vict}$ and ends at $\mathcal{T}_{att}$. 

% To better describe the attack details, we define three relation attributes as follows:
% \textit{\textbf{(1) Injected Command:}} The details of commands $\mathcal{C}$ that are used to steal the information.
% We regulate components that each command needs to contain as $\text{[\textit{ToolRecall}][\textit{Attack}][\textit{NotExpose}]}$,
% where [\textit{ToolRecall}] is the command for calling this tool again; [\textit{Attack}] indicates the task of information theft, and [\textit{NotExpose}] asks the LLM to hide the attack in the frontend.
% % and manually collect these commands in Section \ref{sec:data_preparation}.
% \textit{\textbf{(2) Attack Result:}} The results of whether the attack is successful and has stealthiness.
% \textit{\textbf{(3) Background Command Prompt:}} The target prompt that can be used to generate the injected commands. 
% We have illustrated more details of AttackKG in Section \ref{sec:gen_kg}.   


% \begin{equation}
% \begin{split}
%     &(HCase_i, \mathcal{T}^{A}_{att})\stackrel{LLM}{\longrightarrow} [\mathcal{C}^{A}_1,\mathcal{C}^{A}_2,...,\mathcal{C}^{A}_n]\\
%     &\mathcal{T}^{A}_{vict}\stackrel{\mathcal{O}^{A}_{att}\oplus\mathcal{C}^{A}}{\longrightarrow}[Res^{A}_1,Res^{A}_2...,Res^{A}_n]\\
%     &[Res^{A}_1,Res^{A}_2,...,Res^{A}_n]\stackrel{Update}{\longrightarrow}\mathcal{G}
% \end{split}
% \end{equation}

\paragraph{Attack Case Extractor.}
Given the historical case $H$ with the tool calling chain $\mathcal{T}_{1},\mathcal{T}_2,...,\mathcal{T}_N$, we construct ${N\times(N-1)}/2$ tool pairs $\langle\mathcal{T}_i,\mathcal{T}_j\rangle$.
Then, we treat $\mathcal{T}_j$ as $\mathcal{T}^{A}_{att}$, and $\mathcal{T}_i$ as $\mathcal{T}^{A}_{vict}$, then ask the GPT-4o to explore $K$ commands for each pair. We manually test each command and use the attack results to update the attack cases as follows:
\begin{equation}
\resizebox{.89\linewidth}{!}{$
    \displaystyle
\begin{rcases}
% \begin{split}
    &\langle\mathcal{T}^{A}_{vict}, \mathcal{T}^{A}_{att}\rangle\stackrel{LLM}{\longrightarrow} [\mathcal{C}^{A}_1,...,\mathcal{C}^{A}_K]\\
&\mathcal{T}^{A}_{vict}\stackrel{\mathcal{O}^{A}_{att}\oplus\mathcal{C}^{A}}{\longrightarrow}[\mathcal{R}^{A}_1,...,\mathcal{R}^{A}_K]\\
    % &[\mathcal{R}^{A}_1,\mathcal{R}^{A}_2...,\mathcal{R}^{A}_n]\stackrel{Update}{\longrightarrow}AttackDB
% \end{split}
\end{rcases}
\stackrel{Form}{\longrightarrow}AttackCase
$}
\end{equation}
where $[\mathcal{C}^{A}_1,\mathcal{C}^{A}_2,...,\mathcal{C}^{A}_K]$ are the generated commands of LLM.
%上面那个format应该已经删了吧，更新一下
Then, we manually inject these explored commands into the target $\mathcal{T}^{A}_{att}$ and observe $K$ attack results as $[\mathcal{R}^{A}_1,\mathcal{R}^{A}_2,...,\mathcal{R}^{A}_K]$.
Then, we utilize all the previous results to form the attack cases and update the $AttackDB$.
% \begin{center}
% \footnotesize
% \begin{tcolorbox}[colback=white,%gray background
%                   colframe=black,% black frame colour
%                   width=\columnwidth,% Use 8cm total width,
%                   arc=1mm, auto outer arc,
%                   boxrule=0.4pt,
%                   left=0.1pt,
%                   right=0.1pt,
%                   top=0.1pt,
%                   bottom=0.1pt,
%                 colbacktitle=white!80!gray, coltitle=black, %标题框的背景和线条颜色 
%                 title={{\textbf{Prompt for exploring the attack commands.}}}%标题
%                  ]
% {\textbf{Instruction:} We define the AttackKG with the following knowledge graph structure}

% \begin{itemize}[leftmargin=*]
%     \item {\textbf{AttackKG:} Introducing the }
% \end{itemize}
% \end{tcolorbox}
% \vspace{-0.2cm}
% \end{center}

\paragraph{Attack Case's Guidance Completer.}
With the generated commands and attack results, we introduce another GPT-4o model to output the guidance for the subsequent dynamic command generator.
This guidance includes the \textbf{key information} that GPT-4o observes between tools, and how to design a command that may have higher attack success rates, as the following equation:
\begin{equation}
\resizebox{.89\linewidth}{!}{$
    \displaystyle
    \langle\langle\mathcal{T}^{A}_{vict}, \mathcal{T}^{A}_{att},\mathcal{C}^{A},\mathcal{R}^{A}\rangle\stackrel{LLM}{\longrightarrow}\mathcal{G}^{A}
    \rangle\stackrel{Form}{\longrightarrow}AttackCase
$}
\end{equation}
where the guidance is mutated from the basic template, e.g., "\textit{The generated commands that may have the [ToolRecall][Attack][NotExpose] format, and will focus on the key information between tools}". We form the cases with all five tuples and insert this case to AttackDB: $AttackCases\rightarrow AttackDB$, which are references to guide the optimization of the dynamic command generator.

\subsection{RL-based Dynamic Command Generation}
Given inference examples $[E^O_1,E^O_2,..., E^O_m]$ that are used for model optimization, 
each tool can only access its relevant information and does not know the other invoked tools. 
% where we simulate the adversary's exploration steps for achieving the information theft and use the reward feedback to optimize this model dynamically.
We first incorporate the AttackDB to initialize the command generator.
Then, we randomly select one malicious tool $\mathcal{T}^{O}_{att}$ in $E^O_i$'s toolchain and generate the injected command.
Finally, we conduct the information theft attack with the command 
and calculate the rewards to optimize the AttackDB\&model with black-box attack cases.

\paragraph{Dynamic Command Generator.}
The dynamic command generator $f_{gen}$ is a model that simulates the adversary's learning ability (e.g., T5~\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20}), which can be fine-tuned based on the current knowledge and the results of the observed attack results.
In the black-box attacks, the adversary can only access the $\mathcal{T}^{O}_{att}$'s relevant information, so we generate the command $\mathcal{C}_i^{O}$ as follows:
\begin{equation}
    P_{gen}(\mathcal{C}^{O}|Case,\mathcal{T}^{O}_{att})=f_{gen}(\hat{{Case}}\oplus\hat{\mathcal{T}}^{O}_{att})
\end{equation}
where $\hat{Case}$ is the \textbf{textual description} of the retrieved attack cases in the AttackDB with similar types of input/output values in the $\mathcal{T}^{A}_{att}$,
% We use the entity/relation lists and the adjacent matrix to describe the details of the AttackKG.
and $\hat{\mathcal{T}}^{O}_{att}$ is the text description of the current malicious tool.
We generate the command $\mathcal{C}^{O}$ with its probability $P(\mathcal{C}^{O})$, and inject it into the target tool-learning system and obtain the attack results: $(\hat{\mathcal{I}/\mathcal{O}}^{O}_{vict},\hat{Inf})$,
% \begin{equation}
% \textit{TarSys}\stackrel{\mathcal{O}^{O}_{att}\oplus\mathcal{C}^{O}}{\longrightarrow}(\hat{\mathcal{I}/\mathcal{O}}^{O}_{vict},\hat{Inf})
% \end{equation}
where $\hat{\mathcal{I}/\mathcal{O}}^{O}_{vict}$ and $\hat{Inf}$ are theft results and inference after the attack.
 % and $r_{Theft}$ indicates the rewards that are calculated based on the results of the information theft attack.



\paragraph{Command Sentiment Reviewer.}
Our manual analysis of the command's sentiment polarity shows that commands with neutral sentiments are likely to be executed by LLMs.
We calculate the absolute sentiment score $|S_{sent}|$ with NLTK tool~\cite{DBLP:conf/acl/Bird06} as the reward penalty, which indicates that if the command sentiment tends to be positive or negative, the reward will be lower.
% \begin{equation}
%     r(E_i)=r_{Theft}(E_i)-|S_{sent}|
% \end{equation}
% where $r(E_i)$ is the final reward for the model optimization. 
% The equation indicates that if the command sentiment tends to be positive or negative, the reward will be lower.

\paragraph{RL-Based Model Optimization.}
Based on the thought of RL, the command generator $f_{gen}$ is a policy that determines what the adversaries will do to maximize the rewards, so we choose the PPO reward model~\cite{DBLP:journals/corr/SchulmanWDRK17} to calculate two rewards, i.e., the theft ($r_t$) and exposed ($r_e$) reward, which obtains the State-of-the-Art (SOTA) performance in our task's optimization. The total reward can be calculated as follows:
\begin{equation}
\resizebox{.89\linewidth}{!}{$
    \displaystyle
r(E^O_i)=\underbrace{\sigma(\hat{\mathcal{I}/\mathcal{O}}^{O}_{vict},{\mathcal{I}/\mathcal{O}}^{O}_{vict})}_{r_t}+\underbrace{\sigma(\hat{Inf},Inf)}_{r_e}-|S_{sent}|
$}
\end{equation}
where $r(E_i)$ is the final reward for the model optimization, and function $\sigma(\hat{y},y)$ is the reward model, which is calculated based on the attack results.


To dynamically optimize the {\tool}, we update AttackBD by creating an attack case with $\mathcal{T}^{O}_{att}$'s attack results. 
Since the attack is black-box, adversaries cannot access the victim tool, so we create a new tool with the stolen information. 
The new knowledge can guide adversaries to design harmful commands in black-box attack scenarios.

\input{algo/rl_optimization}


Then, we use the rewards to estimate the policy losses and gradient.
We introduce Reinforce Loss~\cite{DBLP:journals/ml/Williams92}, the novel approach to bridge the gaps between rewards and the command generation probabilities. The loss is calculated as: 
\begin{equation}\label{equa:loss}
\resizebox{.89\linewidth}{!}{$
    \displaystyle    \mathcal{L}_{gen}=\mathbb{E}_{[\mathcal{C}^{O}_{1:m}]\sim f_{gen}}[-\eta\log P_{gen}(\mathcal{C}^{O}|\mathcal{G},\mathcal{T}^{O}_{att})\cdot r(E_i)]
$}
\end{equation}
where the $\mathcal{L}_{gen}$ is the loss for optimizing the {\tool}.
In practice, we introduce the thought of Online Learning~\cite{DBLP:conf/nips/BriegelT99} to optimize the model, as is shown in Algorithm \ref{alg:rl_optimization}.
It means the loss is calculated (Line \ref{algo_line:loss_calculation}) and {\tool} is continuously optimized (Line \ref{algo_line:optimize}) based on the new evaluation cases and feedback in $t_{th}$ timestamp, i.e., $\mathcal{D}_t$.
After optimization, we can apply {\tool} on the new LLM tool-learning systems by registering the malicious tools in the ecosystems and generating injected commands to steal the information of other tools. 

