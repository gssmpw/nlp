\section{Related Work}
% \paragraph{LLM Agents and Tool Learning Systems.}


% \paragraph{RLHF for LLM Optimization.}

LLM tool-learning systems have recently been widely used in the industry~\cite{tang2023toolalpaca, DBLP:conf/iclr/QinLYZYLLCTQZHT24}, and their security risks have become concerns for researchers~\cite{DBLP:journals/corr/abs-2402-04247}.
Some of the risks come from abnormal inputs and failure executions during the task planner's inference process:
\citet{DBLP:conf/iclr/RuanDWPZBDMH24} identified risks of emulator-based LLM agents and exposed risks in agent execution;
\citet{DBLP:journals/corr/abs-2310-05746} evaluated the security in dynamic scenarios that agents will create long-term goals and plans and continuously revise their decisions;
\citet{DBLP:journals/corr/abs-2311-10538} proposed flexible adversarial simulated agents to monitor unsafe executions.
The other risks come from the RAG steps:
\citet{DBLP:journals/corr/abs-2402-07867} proposed PoisondRAG that investigated the malicious text injection in the knowledge base that affects RAG systems;
~\citet{DBLP:journals/corr/abs-2405-20485} proposed Phantom that injected poisoned texts based on the query's adversarial trigger.
Some recent investigate on the security of external tools. 
~\citet{DBLP:journals/corr/abs-2404-16891} generated misleading outputs by modifying a single output value of external APIs.
~\citet{DBLP:journals/corr/abs-2412-10198} designed static commands to conduct DoS to LLM inference.

Different from these works, our study explores the potential information theft attacks in LLM tool-learning systems, {
and we propose a dynamic command generator to achieve high attack success rates with more stealthiness. 
}

\section{Conclusion}

In this paper, we propose {\tool}, a dynamic command generator for information theft attacks in LLM tool-learning systems.
{\tool} prepares AttackDB to find key information for command generation, and then is continuously optimized with RL in black-box attack scenarios.
The evaluation results show that {\tool} outperforms the baselines with +13.2\% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose inherent information leakage risks.
In the future, we will expand the dataset to evaluate {\tool} on more black-box systems and improve the efficiency of model optimization.



% 数据量问题，工具类型和使用率问题
\section*{Limitations}
Although {\tool} shows effectiveness, it has some limitations that make {\tool} fail to steal the victim tool's information.
We manually investigate these bad cases and discuss the reasons for failed information theft and attack hiding.

For samples that fail to achieve the information theft attack, most of the bad cases (95\%) are caused by infrequently used malicious tools.
In these samples, tools that we select as malicious tools are newly created and are rarely used in tool learning.
Therefore, we cannot use the key information to guide the command generation for these tools, which leads to failed information theft attacks.

For samples whose attacks are exposed to the frontend, the misunderstanding of the LLM (56\%) and the ineffective commands (20\%) are the main reasons for the bad cases.
For the first reason, i.e., LLM misunderstanding, some benchmarks, such as ToolBench and ToolEyes, utilize the Llama3-70B model to understand the output and conduct the inference. Compared to GPT models, this LLM may not fully understand the meaning of these commands and is unable to execute commands for hiding the attacks in the frontend.
The second reason, i.e., ineffective commands, is mainly because the current AttackDB cannot cover all the attack cases, so we will enlarge the dataset to further continuously optimize our model.


\section*{Ethical Considerations}

We have injected commands into the external tools' output value to mislead the LLM tool-learning systems, and these commands will conduct information theft attacks.
It is worth noticing that the commands were generated by LLM, so there may be some biases in the real-world attack scenarios.


Moreover, some examples in this research may match the real-world adversaries' attack methods, which will be an incidental case.