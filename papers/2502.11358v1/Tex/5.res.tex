\section{Results}


% ,height=3.5cm
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth,height=3.5cm]{Fig/res_RL_v2.pdf}
\vspace{-0.3cm}
\caption{The component's contribution to total reward in the {\tool}'s optimization.}
\label{fig:rl_learning_steps}
\vspace{-0.4cm}
\end{figure*}


% \subsection{Performance of Evaluating {\tool} on Same Tool-Learning Systems}

% To evaluate {\tool}, we optimize and evaluate the performance of {\tool} on the same tool-learning benchmarks.
% For example, we prepare AttackDB with ToolBench and optimize and evaluate {\tool}'s performances on ToolBench as well.
% We obtain the attack results in the evaluation samples and separately illustrate the results based on the $\mathcal{T}_{vict}$'s types of stolen information.

% The upper part of Table \ref{tab:result_compare} shows the performances of {\tool}, where the \textbf{bold} values are the highest values in each column, and \underline{underline} values are the second highest.
% We can see that, {\tool} achieves the highest $ASR_{Theft}$ on all the target systems' information theft attacks, where the average results are over 70\%, outperforming the best baselines with +13.2\% (ToolBench), +17.2\% (ToolEyes), and +34.7\% (AutoGen).
% Separately, $IER$, $TSR$, and $ASR_{Theft}$ values (15/18) also achieve the highest performances, which means the dynamically generated command can not only steal the retained information in the backend but also hide the attacks in the frontend user interface.
% Some baselines, such as FixedDBCMD and FixedCMD, may expose the attacks in the user interfaces, which means the attack's stealthiness is low. 

% \textbf{Answering RQ1:} {\tool} is better than the baseline approaches when optimizing and evaluating the performances on the same tool-learning benchmarks, with over +13.2\% $ASR_{Theft}$.


% \subsection{Performance of Applying {\tool} to Different Tool-Learning Systems}
% To expose the potential risks in the new systems, we first optimize {\tool} on all three benchmarks in RQ1, then apply it to the new system.
% Since the malicious tools are not included in these new systems, we manually register all these tools of evaluation samples into the systems. 
% However, the black-box systems utilize the Retrieved-Augmented Generation (RAG) to retrieve the tools before the LLM inference, so we cannot guarantee our tools are used.
% To address it, we only analyze samples in which malicious tools are retrieved.




% The bottom part of Table \ref{tab:result_compare} shows the performances of migrating {\tool} to the new tool-learning systems.
% We can see that, in the cases where black-box systems retrieve our tools, {\tool} achieves the highest performances with over 80.9\% $ASR_{Theft}$, significantly outperforming the baselines with +34.0\% (LangChain), +85.1\% (KwaiAgents), and +21.3\% (QwenAgent).
% These results imply that these tool-learning systems may pose risks.
% If these malicious tools are retrieved and used in the LLM inference, these systems can detect the fixed command injection, but may not detect the dynamic commands that are generated by the attacker's learning ability.


% \textbf{Answering RQ2:} {\tool} can be applied to the new tool-learning systems with over 80.9\% $ASR_{Theft}$, which may pose information disclosure risks to the LLM tool-learning systems.





\subsection{Performance of {\tool}'s Baseline Comparison on Tool-Learning System}



\paragraph{Evaluation {on Open-Source Systems}.}
We first introduce three tool-learning benchmarks to evaluate the model, which build tools' ecosystems from the large-scale API marketplace (i.e., RapidAPI~\cite{DBLP:conf/ccs/Liao0LSCYH24}):
\textbf{ToolBench} is the Llama-based~\cite{DBLP:journals/corr/abs-2302-13971} system that utilizes the tree-level inference to conduct the tool learning; 
\textbf{ToolEyes} is a fine-grained
Llama-based system for the evaluation of the LLMs' tool-learning capabilities in authentic scenarios;
and \textbf{AutoGen} combines GPT-4 to utilize conversable and group-chat agents to analyze complex Q\&A queries.
We train and test the {\tool} on the same benchmarks.

% For example, we prepare AttackDB with ToolBench and optimize and evaluate {\tool}'s performances on ToolBench as well.
% We obtain the attack results in the evaluation samples and separately illustrate the results based on the $\mathcal{T}_{vict}$'s types of stolen information.
The upper part of Table \ref{tab:result_compare} shows the performances of {\tool}, where the \textbf{bold} values are the highest values in each column, and \underline{underline} values are the second highest.
We can see that, {\tool} achieves the highest $ASR_{Theft}$ on all the target systems' information theft attacks, where the average results are over 70\%, outperforming the best baselines with +13.2\% (ToolBench), +17.2\% (ToolEyes), and +34.7\% (AutoGen).
Separately, $IER$, $TSR$, and $ASR_{Theft}$ values (15/18) also achieve the highest performances, which means the dynamically generated command can not only steal the retained information in the backend but also hide the attacks in the frontend user interface.
Some baselines, such as FixedDBCMD and FixedCMD, may expose the attacks in the user interfaces, which means the attack's stealthiness is low. 



% \paragraph{Risk Exposure in New Systems.}
\paragraph{{Evaluation on Black-Box Systems}.}
We first train {\tool} on all the previous three benchmarks, then apply it to expose information leakage risks in three widely-used {black-box} systems:
\textbf{LangChain}~\cite{DBLP:journals/corr/abs-2406-18122} is a famous Python-based LLM inference framework that can freely combine LLMs with different tools; 
\textbf{KwaiAgents}~\cite{DBLP:journals/corr/abs-2312-04889} is Kwai's agent that integrates a tool library, task planner, and a concluding module for inference.
and \textbf{QwenAgent}~\cite{DBLP:journals/corr/abs-2412-15115} is Alibaba's tool-learning system that can efficiently retrieve external knowledge retrieval and plan inference steps.
These systems support self-customized tools, and some of them may have public code repositories, but they do not open-source the datasets for model optimization, so we treat them as black-box systems. 
Since the malicious tools in our test dataset may not be included in these new systems, we manually register all these tools in the systems. 
There is a potential risk that black-box systems retrieve the tools before the inference, so we cannot guarantee that our tools are used.
To address it, we only analyze samples in which malicious tools are retrieved.


The bottom part of Table \ref{tab:result_compare} shows the performances of migrating {\tool} to the new tool-learning systems.
We can see that, in the cases where black-box systems retrieve our tools, {\tool} achieves the highest performances with over 80.9\% $ASR_{Theft}$, significantly outperforming the baselines with +34.0\% (LangChain), +85.1\% (KwaiAgents), and +21.3\% (QwenAgent).
These results imply that these tool-learning systems may pose risks, i.e.,
if these malicious tools are retrieved in these systems, they may not detect the command injection attacks that are generated dynamically in over 80\% cases.



\textbf{Answering RQ1:} {\tool} outperforms baselines when evaluating the performances on {open-source} tool-learning benchmarks, with over +13.2\% $ASR_{Theft}$.
Moreover, it can be applied to {black-box} systems to expose their information leakage risks, with over 80.9\% $ASR_{Theft}$.
% \yang{comment.}
%第二句话列数值，不要笼统地说








\subsection{Component's Contribution to Rewards}
To analyze the contribution of components to rewards during the model optimization. We compare the {\tool} with two other variants that may affect the rewards:
\textbf{w/o $S_{sent}$} does not incorporate the sentiment scores in the rewards, and 
\textbf{w/o AttackDB} does not provide the prepared attack cases.

\input{Tab/attack_defense}


Figure \ref{fig:rl_learning_steps} shows the optimization procedure of {\tool}. We can see that, compared with w/o $S_{sent}$, the {\tool} will reach the convergence a little bit slower than the variant, mainly due to the sentiment penalty $|S_{sent}|$ is more strict and will consider whether the commands are neutral. 
However, {\tool}'s rewards will finally exceed with +0.2 higher after convergence.
Compared with the w/o AttackDB, {\tool} will reach convergence faster than the variant with around 10 iterations, since it has the background knowledge to help optimize the model, which reduces RL's cold-start.
Please note that the final rewards of w/o AttackDB are +0.07 higher than {\tool} in AutoGen.
This is because AutoGen has strong comprehension ability with the GPT-4, so it does not require key information to understand the attack commands, which achieves a high attack success rate.
It further illustrates a potential risk of LLM, i.e., a stronger LLM may be easier to understand abnormal commands and perform risky operations. 

% \textbf{Answering RQ2:} The components can balance the iterations and rewards in {\tool}'s optimization, enhancing its training efficiency.
\textbf{Answering RQ2:} The components contribute to {\tool}'s optimization, where {$S_{sent}$ can promote the model to generate neutral commands and obtain higher attack rewards, and AttackDB provides key information to guide model optimization and improve convergence speed.
% \yang{comment.}
%不同components的不同作用最好分开简略分析。


\subsection{Performances of {\tool}'s Defense}



To protect LLM tool-learning systems from {\tool}'s attack, we design three approaches: \textbf{InferCheck} is the inference-side defense that checks the abnormal text description in the LLM inference;
% The other two are backend-side defense methods that review whether the registered tools are secure, where 
\textbf{ParamCheck} is the tool-side defense that checks whether the request inputs exceed the necessary information;
\textbf{DAST} is the tool-side defense that utilizes Dynamic Application Security Testing (DAST)~\cite{DBLP:journals/ieeesp/StytzB06} to test the abnormal function calls and data access with GPT-generated test cases (Details in Appendix \ref{sec:defense_appendix}).

We introduce the defense method to {\tool} on the three tool-learning benchmarks in RQ1, and a higher absolute value of metric change means an effective defense.
Table \ref{tab:attack_defense} shows the results of defending the information theft attack.
We can see that, all the defense methods can effectively reduce the $ASR_{Theft}$ of {\tool}, where DAST has the largest reduction with -70.2\% (ToolBench), -83.9\% (ToolEyes), and -81.4\% (AutoGen).
These results indicate that tool reviewing can reduce the risks of information disclosure, which inspires us to study more effective tool review methods to reduce the risks of the LLM agents.  

\textbf{Answering RQ3:} Our targeted defense methods can effectively protect the systems from {\tool}'s attack, with over -70.2\% $ASR_{Theft}$. 
% More details of the defense methods and the application on black-box systems (RQ2) are shown in Section \ref{sec:defense_appendix}.