\section{Experimental Design}\label{sec:data_preparation}
To evaluate the performances of {\tool}, we introduce three Research Questions (RQs).


% {\textbf{RQ1:}} What are performances of {\tool} on the same LLM tool-learning systems?

% {\textbf{RQ2:}} What are performances on applying {\tool} to new tool-learning systems?

% {\textbf{RQ3:}} How does the component contribute to the rewards during model optimization?

% {\textbf{RQ4:}} How can we defend {\tool}'s dynamic information theft attack?


\textit{\textbf{RQ1: What are performances of applying {\tool} on various LLM tool-learning systems?}
{
We aim to explore the advantage of {\tool} in open-source systems and generalization to black-box systems, respectively.
}
}
% We aim to analyze the advantages of the method and the ability to expose the information leakage risks in other widely-used systems.

% {\textbf{RQ2:}} What are performances on applying {\tool} to new tool-learning systems?

\textit{\textbf{RQ2: How do components contribute to rewards during RL-based optimization?}
We aim to analyze the impact of AttackDB and sentiment polarity on {RL-based model} optimization.}

\textit{\textbf{RQ3: How can we defend {\tool}'s dynamic information theft attacks?}
{
We design three defense approaches and investigate whether they protect the systems from {\tool}'s attacks.
}
}
% We aim to effectively reduce the threat of {\tool}'s attacks.}




\paragraph{Dataset Preparation.}
We prepare the dataset of {\tool} in the following three steps:
{\textbf{(1) Original Dataset Collection.}} 
{We collect all the original data from three open-source tool-learning benchmarks  (i.e. ToolBench~\cite{DBLP:conf/iclr/QinLYZYLLCTQZHT24}, ToolEyes~\cite{DBLP:conf/coling/YeLGHWLFDJ0G025}, and AutoGen~\cite{DBLP:journals/corr/abs-2308-08155}) including user queries, system response, and innovation toolchain.}
% The three tool-learning benchmarks (i.e., ToolBench, ToolEyes, and AutoGen) are white-box and open-source inferences and tools in the repositories, so we collect all the queries, responses, and inferences from these datasets as our original dataset. 
% \yang{comment.}
%三个benchmark的引用加到这里来，现在写到后面了。
{\textbf{(2) Dataset Partition.}} We select 80\%/20\% as train/test samples, and partition training samples to attack case/RL-optimization examples.
\textbf{(3) Attack Case Collection.} We remove the unfinished inference samples (mainly due to the inability to access external tools) and collect the attack cases.
Table \ref{tab:autocmd_dataset} shows the statistics of our dataset.
In total, we collect 1,260 samples for evaluation, where 1,008 samples are used to train the model, and the remaining 252 are used for testing. 
% \yang{comment.}
%table1里面写的是train，文字写的是优化，统一一下概念。
\input{Tab/dataset}





\input{Tab/baseline_comparison_local}


% \paragraph{Target Systems.}
% We first introduce three tool-learning benchmarks to optimize and evaluate the {\tool}, which build tools' ecosystems from the large-scale API marketplace (i.e., RapidAPI~\cite{DBLP:conf/ccs/Liao0LSCYH24}):
% \textbf{ToolBench}~\cite{DBLP:conf/iclr/QinLYZYLLCTQZHT24} is the Llama-based~\cite{DBLP:journals/corr/abs-2302-13971} system that utilizes the tree-level inference and Deep First Search (DFS) to conduct the tool learning; 
% \textbf{ToolEyes}~\cite{DBLP:conf/coling/YeLGHWLFDJ0G025} is a fine-grained
% Llama-based system for the evaluation of the LLMs' tool-learning capabilities in authentic scenarios;
% and \textbf{AutoGen}~\cite{DBLP:journals/corr/abs-2308-08155} is a framework that combines GPT-4 to utilize conversable and group-chat agents to analyze complex Q\&A queries.


% Second, we use {\tool} to expose information leakage risks in three widely-used tool-learning systems.
% These systems support the self-customized tools, which is useful to test the ability of {\tool}.
% Some of them may have public code repositories, but they do not open-source the datasets for model optimization, so we treat them as black-box systems. 
% \textbf{LangChain}~\cite{DBLP:journals/corr/abs-2406-18122} is a component-based Python framework that can freely combine LLMs with different tools in LLM inference; 
% \textbf{KwaiAgents}~\cite{DBLP:journals/corr/abs-2312-04889} is Kwai's agent that integrates a tool library, task planner, and a concluding module for inference.
% and \textbf{QwenAgent}~\cite{DBLP:journals/corr/abs-2412-15115} is Alibaba's tool-learning system that can efficiently retrieve external knowledge retrieval and plan inference steps.

\paragraph{Attack Baselines.}
% \yang{comment.}
%这三个3基线怎么实现的还是有点模糊。
%再者还有一些取名的问题，比如第一个tool-learning，本身系统就是tool-learning systems，这里叫做一个窃取方法基线，我总感觉不太合适。能不能从窃取的方式取名？还有就是tool-learning、finxedDBCMD如何实现的还得说一下，毕竟这俩不是已有的基线，得说清楚怎么实现的，比如tool-learning方法中窃取的参数是人工确定的？还是怎么拿到的？
%然后fixCMD这一类应该有很多确定的指令，比如rupeng和haowei他们发在naacl哪些，是有一些固定的指令可以用的，都得引一下。
%总的来说，我觉得可以分层次说一下：1）首先引入了一个已有的窃取方法，固定指令，引一下已有文章，在指示一下固定指令长啥样；2）再者说一下我们基于XXX的考虑引入了各类基线（比如我们经常观察到如何一些工具经常请求一些看似功能外的参数，可能会导致不必要的信息泄露5），tool-learning，fixedDNCMD，这两类的细节分别是什么，比如窃取的属性值如何确定之类的。
% Since few works investigate command injection attacks in LLM tool-learning systems, 感觉这句话没必要讲
{We have established two additional baselines (PoisonParam and FixedDBCMD) on top of the existing static method (FixedCMD)}, and illustrate their details in {Appendix \ref{sec:baseline_details}}.
\textbf{FixedCMD}~\cite{DBLP:journals/corr/abs-2412-10198,DBLP:journals/corr/abs-2404-16891} uses the static command in the attack, as shown in Figure \ref{fig:case_study}; 
\textbf{PoisonParam} is a baseline where we manually add redundant input parameters with the victim tool's information to poison LLM;
\textbf{FixedDBCMD} introduces AttackDB but does not optimize the model in command generation.



\paragraph{Metrics.}

We utilize three metrics to measure attack stealthiness and success: \textbf{Inference Exposing Rate} ($IER$) measures the stealthiness, 
%增加了Stealthiness的内涵
which is the ratio of {attacks exposed in the frontend,
% exposed attacks, i.e., the attacks are exposed in the frontend, 
i.e., the LLM inference stops prematurely or the following invocation toolchain changes after attacking.}
% calls other tools.
% In the dataset preparation, we indicate that all cases are successfully executed, but if the attack is exposed
\textbf{Theft Success Rate} ($TSR$) calculates the ratio of stolen information that matches the victim tool's information.
% \yang{comment.}
%如何被认为是一个successful 的case还是不清楚？是请求后获取的信息和上游真实信息一致？还是怎么的？得解释一下。
\textbf{Attack Success Rate for Information Theft Attack} ($ASR_{Theft}$) is a comprehensive metric to measure the ratio of cases if $IER=0\land TSR=1$.
{This is a more stringent metric that requires both successful information theft and stealthiness.}
% \yang{comment.}
%这个地方不能说是tradeoff，我们想表达的不是两个指标的互相制约下的权衡，而是一种综合的、更严格的指标，对攻击方法有更好的全方位要求。我给你改了，你后面还有类似表述，都改掉。
%Ziyou:好的师兄，已修改
% \begin{equation}
    
% \end{equation}


\paragraph{Experimental Settings.}
For attack case database preparation, we set GPT-4's temperature as 0.05, $TopP$ and $max\_token$ as default, and $K=3$ for attack case generation.
For RL-based model optimization, we optimize T5 with the SGD optimizer, the learning rate as $10^{-3}$, and $Batch\_Size=32$.
All experiments run on GeForce RTX A6000 GPU.
% \yang{comment.}
%第一句和第二句两个for，最好跟你方法的阶段对上，不要起别的名字了。