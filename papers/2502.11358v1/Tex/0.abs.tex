\begin{abstract}
{Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches.}
%Information theft attacks exist in Large Language Model (LLM) tool-learning systems. 
%Adversaries may inject commands through the malicious tool's output, which controls LLMs to send inaccessible information to the malicious tool, thus posing privacy leakage threats to the systems.
% However, existing attack approaches are black-box oriented. They are static and remain unchanged no matter how the user queries or tool invocation changes, \yang{makes the comments easily detectable by LLMs and lead to to the failure of the attack. }
% which are vague in describing the theft target and are more likely to be ignored by LLMs. 
{
However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation toolchains.
It makes malicious commands more likely to be detected by LLM and leads to attack failure.
}
In this paper, we propose {\tool}, 
{
a dynamic attack command generation approach for information theft attacks in LLM tool-learning systems.
Inspired by the concept of mimicking the familiar, {\tool} is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with examples from the target systems, thereby generating more targeted commands for information theft.
}
% an explicit and dynamic command generator, which first prepares the attack cases to describe the key information between tools, then is continuously optimized with Reinforcement Learning (RL), enabling {\tool} to generate effective commands when they only know the malicious tools.
The evaluation results show that {\tool} outperforms the baselines with +13.2\% $ASR_{Theft}$, and can be generalized to new tool-learning systems to expose their information leakage risks. 
We also design four defense methods to effectively protect tool-learning systems from the attack.
\end{abstract}