\section{Introduction}

% LLMs have been widely used in research and industry, and their inference and text generation abilities obtain State-of-the-Art (SOTA) performances in various NLP tasks.

% 我们工作的优势和需要强调的点是什么？
% LLM Tool Learning指令注入攻击已经被研究了，但是目前都是基于fix的指令，容易被检测。
% 攻击者具备有基本知识，有能力去学习一些新的攻击场景，并且攻击更加隐蔽。
% 我们的工作在information theft场景研究，利用动态指令生成的方式，进行更多的、隐蔽的攻击指令动态生成。这些指令是动态生成的，具有隐蔽性。


% 前提条件：Stealthiness，窃取过程是隐蔽的，因此后台窃取信息不应该暴露在user interface中；Authority Limited：黑盒的，导致攻击者智能投入指令。
% To enhance the ability of LLMs to interact with real-world situations and ,

%Ziyou: 想确认一下概念就是，attacker/adversary；Instruction/Command
{The last few years have seen a surge in the development of Large Language Model (LLM) tool-learning systems, such as ToolBench~\cite{DBLP:journals/corr/abs-2304-08354}, KwaiAgents~\cite{DBLP:journals/corr/abs-2312-04889} and QwenAgent~\cite{DBLP:journals/corr/abs-2412-15115}. After being planned, invoked, and integrated by LLMs, the collective capabilities of many tools enable the completion of complex tasks.}
%{Recently, there has been a surge in the development of tool-learning systems, like ToolBench~\cite{DBLP:journals/corr/abs-2304-08354}, KwaiAgents~\cite{DBLP:journals/corr/abs-2312-04889} and QwenAgent~\cite{DBLP:journals/corr/abs-2412-15115}.
% which allow LLMs to manipulate a multitude of open or third-party tools.
%After planning, invoking, and integrating by LLMs, the collective capabilities of many tools enable the completion of complex tasks.
% , such as ticket booking, travel guide recommendations, etc.
%Despite the impressive capabilities granted to tool-learning systems, 
{Despite the powerful capabilities of LLM tool-learning systems,} 
malicious tools can introduce attacks by injecting malicious commands during interactions with LLMs and pose security threats to the entire system, such as denial of service (DoS)~\cite{DBLP:journals/corr/abs-2412-13879}, decision errors~\cite{DBLP:journals/corr/abs-2311-05232}, or information leakage~\cite{DBLP:journals/corr/abs-2409-11295}.
Especially from the information security perspective, {external tools are typically developed and maintained by many independent third parties.}
{If user queries containing sensitive information are not properly managed and protected, it can lead to issues including information theft, financial losses, and diminished user trust~\cite{article}.
Therefore, it is critical to investigate advanced information theft attacks and develop effective strategies to safeguard LLM tool-learning systems.}

%If the potential privacy involved in user queries cannot be precisely distributed and protected by the systems, it can lead to identity theft, financial losses, and a decline in user trust~\cite{article}. Therefore, it is of significant importance to explore advanced information theft attacks and develop targeted defense strategies for tool-learning systems.




\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{Fig/motivation_tool_learning_v3.pdf}
\vspace{-0.7cm}
\caption{The motivation example of information theft attacks through command injection.}
\vspace{-0.6cm}
\label{fig:motivation_tool_learning}
\end{figure}


%结合图这个例子，讲一下这个风险是如何产生的。先说一下任务是啥，工具规划路径是啥；说清楚user query中的隐私信息是啥；哪个步骤中哪个工具调用会用到这个隐私；再说如果下一个工具的哪个位置（参数、描述还是啥）黑盒注入一条简单的恶意指令，就能够获取到.
{Researchers have recently started investigating information leakage issues caused}
by malicious tools~\cite{DBLP:journals/corr/abs-2412-10198,DBLP:journals/corr/abs-2404-16891}.
%把浩伟他们那个工作，以及对比的基线那些方法都引用过来
%{For example, in Figure \ref{fig:motivation_tool_learning}, the user queries ToolBench to help him "\textit{plan a trip to Tokyo}", and he/she provides the username and password for booking the hotel\&flight, which are the privacy information related to specific tools.
{For example, in Figure \ref{fig:motivation_tool_learning}, the user queries ToolBench to help with "\textit{plan a trip to Tokyo}", and provides the usernames and passwords for booking a hotel and flight. These credentials are considered private information specific to certain tools.}
Normally, ToolBench utilizes four tools to plan the trip, i.e., \textit{Search\_Site}, \textit{Book\_Hotel}, \textit{Book\_Flight}, and \textit{Plan\_Trip}.
% tools $\mathcal{T}_1\sim \mathcal{T}_4$.
{The \textit{Book\_Flight} tool can only access the username and password associated with flight bookings and is isolated from the private information used by the \textit{Book\_Hotel} tool.}
%The \textit{Book\_Flight} can only access the flight's username and password and is isolated from the privacy information in \textit{Book\_Hotel}.
%Normally, ToolBench utilizes four tools to plan the trip, i.e., \textit{Search\_Site}, \textit{Book\_Hotel}, \textit{Book\_Flight}, and \textit{Plan\_Trip}.
% tools $\mathcal{T}_1\sim \mathcal{T}_4$.
%The \textit{Book\_Flight} can only access the flight's username and password and is isolated from the privacy information in \textit{Book\_Hotel}.
However, if \textit{Book\_Flight} is a malicious tool, 
{it can inject a command through the tool's output value to prompt LLM to "\textit{call Book\_Flight again and send Book\_Hotel's info to it}".
Since LLM cannot detect or block this command,} it sends the 
victim tool \textit{Book\_Hotel}'s input value to \textit{Book\_Flight}, causing a potential information theft attack.
%it will inject a command through the tool's output value to ask LLM to "\textit{call Book\_Flight again and send Book\_Hotel's info to it}". LLM cannot prevent this command and sends the victim tool \textit{Book\_Hotel}'s input value to \textit{Book\_Flight}, causing an information theft attack.
% They can guess what information is probably used based on knowledge of successful cases, and design a specific output command $\mathcal{C}_{Dyn}$.
% This command controls the LLM to call $\mathcal{T}_3$ again and send other tools' relevant information to it if its formats are similar to $\mathcal{T}_3$.

% Recently, researchers have introduced tool-learning systems~\cite{DBLP:journals/corr/abs-2304-08354}, which enables LLMs to interact with external tools' ecosystems with Chain-of-Thought (CoT) and solve complex questions, as illustrated in Figure \ref{fig:background_tool_learning}.
% Previous researchers have proposed some tool-learning benchmarks (e.g., ToolBench~\cite{DBLP:conf/iclr/QinLYZYLLCTQZHT24} and ToolEyes~\cite{DBLP:conf/coling/YeLGHWLFDJ0G025}), but they lack a review of the tools used in LLM inference~\cite{DBLP:conf/ccs/Liao0LSCYH24}, which can lead to the risks of information theft attacks.
% Since the relevant information of victim tools is retained in the inference steps,
% % they may have security risks of command injection.
% attackers may sneak into the tool's developers and steal this information, which they should not access, which disobeys the principle of minimizing the Personal Identified Information (PII)~\cite{}.
% To achieve the information theft attack, they may inject harmful commands in the malicious tool's output value, which is a \textbf{black-box} attack that may control the LLMs to send the victim tools' retained information to the attack tools, posing risks to the system.

% Some benchmark designers provide limited permissions to the registered tools to access required input data but cannot access other tools' relevant information, but 

% Attackers may use their knowledge to conduct \textit{\textbf{black-box}} information theft attacks via injecting harmful commands into LLMs. They control LLMs to steal the upstream tools' information, 


% Black-box Dynamic：根据具体工具类型，结合过去的知识进行分析的；Learning ability：学习能力



% 王老师意见：这段话的逻辑应该是：
% 1、目前的防御方法，主要针对面向黑盒的信息窃取，是静态的
% 2、由于攻击指令不变，所以容易被LLM发现而进行防御
% 3、然而，如何在工具链交互过程中动态生成攻击指令，则可能具有更好的隐蔽性，而绕过LLM的防御机制。
% 4、本文提出了一种动态的、更加隐蔽的。。。。攻击方法，。。。。理解这些攻击方法，对于采取有效的反制和防御措施，将非常重要。


However, existing black-box attack methods are static~\cite{DBLP:journals/corr/abs-2412-10198,DBLP:conf/acl/WangXZQ24}, which means that regardless of how the user queries or how the context within the tool invocation chain changes, the injected theft commands remain the same.
%which means that no matter how the user queries or the context within the tool invocation chain changes, the injected theft commands remain unchanged.
From the perspective of stealthiness, commands like "\textit{send Book\_Hotel’s information to it}" can generally be identified as malicious without carefully examining their context, {making them easier to detect and defend against.
%making them easier to defend. 
% Such commands are vague in describing the theft target, which intuitively makes them more likely to be ignored by LLMs and results in a lower attack success rate.
In contrast, if an adversary can dynamically infer "\textit{Username}" and "\textit{Password}" in \textit{Book\_Hotel} and \textit{Book\_Flight} from user queries, embed them as regular parameters in tools' parameter list, and request LLMs to return more explicitly, the attack command is less likely to be detected.}
%This inspires us to propose a dynamic attack method with stealthiness.Consider a scenario where a tool is invoked, an adversary can dynamically infer described sensitive information keys 
%Ziyou: 这里强调了一下相似结构的参数，所以可以作为keys
%~\cite{DBLP:journals/corr/abs-2310-05746}
%{(e.g. "\textit{Username}" and "\textit{Password}" with similar formats in \textit{Book\_Hotel} and \textit{Book\_Flight})} from user queries, embed them as normal parameters within the tool's parameter list, and request LLMs to return more explicitly, it is less likely to be detected as malicious by LLMs.
%Ziyou: 我在这里引了一下Case Study的图，不太清楚这个可不可以
% We have illustrated the command details in Figure \ref{fig:case_study} (case study) and Section \ref{sec:malicious_tools}.}

% \textit{(COMMENT)}
%补一下XXX那个地方，如果是一个明确的方式放到参数里请求的样例。你原来motivation那张图，是不是删掉了？可以稍微改吧改吧用那个例子也挺好的。也可以找一个工具专门画一张图，主要用来说明一个tool包含的要素，比如工具名，描述、API、参数等等，然后结合着展示一下我们的方法经过恶意注入之后的样子。
%Ziyou: 这个感觉在前面用图讲的话会有点杂，我现在放在Case Study里面展示这个例子和攻击结果，是不是可以refer一下对应的图之类的...关于指令注入的详细例子的话，是不是可以放在后面appendix里面，这里有个例子就是专门针对这个的

% Previous works have investigated the black-box command injection attacks by designing some fixed commands in the output values~\cite{}.
% However, these works cannot reflect real-world information theft scenarios, mainly because of the following three challenges:
% (1) First, the cases of fix-command injection are few, and they rarely consider the trade-off between the attack success rate (ASR) and its stealthiness. 
% (2) Second, they ignore the attacker's \textbf{background attack knowledge}. 
% Sometimes, the attackers are experienced and they may learn from the historical successful cases and infer what information is prone to be stolen (e.g., username\&password in Figure \ref{fig:motivation_tool_learning}).
% (3) Third, attackers will learn from the historical attack success/failure results and gradually refine their attack strategy. The learning step utilizes the downstream task's rewards, which is similar to Reinforcement Learning (RL). 
% These challenges inspire us to dynamically generate the commands to find more attack scenarios in the information theft attack.


% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{Fig/background_tool_calling.pdf}
% % \vspace{-0.7cm}
% \caption{The example query and output in a typical tool-learning benchmark, i.e., ToolBench.}
% \label{fig:background_tool_learning}
% % \vspace{-0.4cm}
% \end{figure}




In this paper, we propose a dynamic attack command generation approach, named {\tool}, for information theft attacks in LLM tool-learning systems.
% Unlike traditional static command injection attacks, {\tool} can learn from historical cases to find key information for command generation, and we continuously apply and optimize it based on evaluation cases' attack results.
Inspired by "mimicking the familiar", a concept in social engineering~\cite{DBLP:conf/iccr/FakhouriAOMHH24}, {\tool} can infer the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thus generating more targeted commands for information theft.
% \yang{comment.}
%这句话和abs对齐一下，方法也对齐一下
% \textit{(COMMENT)}
%1句话，从经过历史样本和反馈强化的角度，阐述下问啥我们的方法能够推理出窃取信息的key作为参数。
To achieve this, 
% To address the challenges, we propose a dynamic command generation approach, i.e., {\tool}, guided by the attacker's knowledge and learning ability.
{we first prepare the attack case database (AttackDB), which identifies the key information exchanges between tools that impact the success rate of information theft attacks.}
%we first prepare the attack case's database (AttackDB), which indicates the key information between tools affects the success rate of information theft attacks.
Second, we apply {\tool} in black-box attack scenarios, {where it generates commands with only malicious tools and AttackDB, and is optimized through reinforcement learning (RL)~\cite{DBLP:conf/aaaifs/HausknechtS15}, leveraging rewards to improve its attack effectiveness.}
The optimized {\tool} can generate commands that effectively conduct information theft attacks when only malicious tools are known.
% ,DBLP:conf/nips/Ouyang0JAWMZASR22, Finally, we replay these attacks in target tool-learning systems by calling the optimized command generation model.


To evaluate {\tool}'s performances, we conduct experiments on three popular benchmarks, i.e. ToolBench, ToolEyes, and AutoGen, with 1,260 inference cases and compare with three baselines.
The results show that {\tool} achieves the highest attack stealthiness and success rate, outperforming baselines on the trade-off metric $ASR_{Theft}$ with +13.2\%.
% \yang{\textit{(COMMENT).}}
% %提一下隐蔽性
We also apply the optimized model to three black-box LLM tool-learning systems developed by renowned IT companies, i.e., LangChain, KwaiAgents, and QwenAgent.
% \yang{\textit{(COMMENT).}}
%明确一下那三个，以及为啥是这三个，最流行？下载量最多？之类的理由
{\tool} can expose information leakage risks and achieve over 80.9\% $ASR_{Theft}$ in these systems.
We also design four defense methods to protect systems from {\tool}'s attack.
% \yang{\textit{(COMMENT).}}
%把你加固的那个东西也写一句话进来

This paper makes the following contributions:

\begin{itemize}[leftmargin=*]
    \item 
    We design a dynamic command generator for information theft attacks in LLM tool-learning systems. 
    % To the best of our knowledge, i
    {The approach infers the input and output of upstream tools through the toolchains and achieves more effective information theft by targeted information request commands.}
    % is the first work that analyzes tools' parameters as key information and generates targeted commands, which achieves effective attack results.
    % and incorporates background knowledge and dynamic command generation in tools' information theft attacks.
    % \yang{\textit{(COMMENT).}}
    %to our best knowledge这半句，不要从动态角度分析，要从动态对窃取成功率的影响的角度说。比如通过推理信息key，明确的方式请求参数来实现更有效的信息窃取。
    \item 
    We evaluate {\tool}'s performances on the dataset with 1,260 samples, which outperforms static baselines and can be generalized to expose information leakage risks in black-box systems.
    \item
    {
    We design the targeted defenses, and the evaluation results show that they can effectively protect the system from {\tool}'s attacks.
    }
    % \yang{\textit{(COMMENT).}}
    %可以把第一条前半句单独拆出来一个点，第一点只留后半句动静态的论述。新的点就是说在已知漏洞（信息窃取漏洞）方式的前提下，我们设计了一个更有效的攻击方法，能够帮助system更有效地威胁建模，并且我们提出了并试验了一些防御策略，能够知道现实应用中的安全加固。
    \item {We release the code and dataset\footnote{\href{https://anonymous.4open.science/r/AutoCMD-DB5C/}{{https://anonymous.4open.science/r/AutoCMD-DB5C/}}} to facilitate further research in this direction. }
    %at the following link: \href{https://anonymous.4open.science/r/LLMToolAttack-6EC0}{\texttt{https://anonymous.4open.science/r/LLMToolAttack-6EC0}}.
\end{itemize}


\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{Fig/model_optimization_v2.pdf}
\vspace{-0.2cm}
\caption{Overview of {\tool}.}
\vspace{-0.5cm}
\label{fig:model_tool_learning}
\end{figure*}

\section{Background of LLM's Tool Learning}
% \yang{\textit{(COMMENT).}}
%子攸，这一块是不是在现有LLM如何工作的技术上，再在前面简单加一段被调用工具结构介绍？比如包含一个工具包含哪些元素，哪些元素再tool-learning中被利用，哪些元素可以被注入指令。最后就是两端：一端介绍工具端的工具长啥样，一端是大模型端如何和这些候选工具交互和调用。
The components of the tool $\mathcal{T}$ in the LLM tool-learning system are the input value $\mathcal{I}$ with its parameter's description, the function code $Func$, and the output value $\mathcal{O}$ with its description.
LLMs invoke tools by analyzing the output values from the tools and sending information to tools' input value, and the adversary can inject the command $\mathcal{C}$ in the output value as $\mathcal{O}\oplus\mathcal{C}$ to conduct the information theft attack.
Therefore, we treat $\mathcal{T}$ as the triplet, i.e., $\langle \mathcal{I}, Func, \mathcal{O}\rangle$, in this work.


% Given the tools in the ecosystem $\mathcal{T}_i=\langle\mathcal{I}_{i}, Func_{i}, \mathcal{O}_i\rangle$, where $\mathcal{I}_{i}$ is the input values, and after executing the function $Func_{i}$, it will output the value $\mathcal{O}_i$.

% \yang{\textit{(COMMENT)}}
%这个地方用cot是不是有点宽泛，我感觉是得突出通过类似react的方式完成任务？我先写成step-by-step reasoning。你如果有更好的说法，可以改掉。核心就是我感觉cot没突出那种逐步的链式关系。。
{
With these available tools, a tool-learning system utilizes an LLM as an agent to achieve step-by-step reasoning through Chain-of-Thought (CoT)~\cite{DBLP:conf/iclr/YaoZYDSN023}.
The inference process can be formalized as $\langle$\textit{Observation}, \textit{Thought}, \textit{Action}$\rangle$.
For each step, LLMs receive the output of the upstream tool (\textit{Observation}), analyze the output $\mathcal{O}_{i-1}$ and upstream inferences in \textit{Thought}, and ultimately decide which tool they will call in the next step in \textit{Action}.
% \yang{\textit{(COMMENT)}}
%有点没搞清楚区分用户接口和后端的意义？是可以指导攻击？还是方法描述需要？如果需要区分的话，可能得进一步说明一下，什么叫做用户接口，是指用户还是工具能看到的东西？得稍微解释下。你看看咋改一下这句话
%Ziyou:这个是方法描述需要，主要是想说明约束条件，然后在threat model的第一个attack goal部分我把约束条件加上去
%我这里换一个CoT_T的描述，这个感觉太容易混淆了，就是Inf表示前端推理的结果
After several inference steps, the system eventually forms a toolchain $[\mathcal{T}_1,\mathcal{T}_2,...,\mathcal{T}_n]$ in the backend, and the LLMs will notify the queried users by showing the inference steps (indicated by $Inf$) in the frontend, as is illustrated in Figure \ref{fig:motivation_tool_learning}.
}
% LLM tool learning systems utilize Chain-of-Thought (CoT) in the LLM inference. 
% Each step is a $\langle Observation, Thought, Action\rangle$ triplet, where the components are defined as follows:
% \textit{\textbf{{(1) Observation}:}} The output of the previous tool $\mathcal{O}_{i-1}$.
% \textit{\textbf{{(2) Thought}:}} LLM analyzes the output $\mathcal{O}_{i-1}$ and previous inferences, then decides what tool they will call in the next step.
% \textit{\textbf{{(3) Action}:}} It selects the tool $\mathcal{T}_i$ that is used in the current inference step.
% After the LLM inference, LLM will call the tools in the sequence $CoT_\mathcal{T}=[\mathcal{T}_1,\mathcal{T}_2,...,\mathcal{T}_n]$ and analyze the complex queries.

% We use the $CoT$ to denote the inference steps exposed in the user interface, and $CoT_\mathcal{T}$ is the tool calling in the backend.

% Previously, the 
% 


% Since the benchmarks lack tool inspection, they may encounter information theft attacks during the LLM inference.
% example of how the attackers harm the tool-learning benchmarks. 

% 需要强调一下猜测上游信息，他并不知道



% \begin{figure}[t]
% \centering
% \includegraphics[width=\columnwidth]{Fig/motivation_tool_calling.pdf}
% % \vspace{-0.7cm}
% \caption{Motivation example of dynamic command generation for information theft attack.}
% \label{fig:motivation_tool_learning}
% % \vspace{-0.4cm}
% \end{figure}


% \paragraph{Motivation Example.}
% We illustrate how the attack is achieved with an example in Figure \ref{fig:motivation_tool_learning}. 
% Based on the query $q$, ToolBench utilizes four tools $\mathcal{T}_1\sim \mathcal{T}_4$.
% The benign LLM's inference steps call the tools $[\mathcal{T}_1, \mathcal{T}_2, \mathcal{T}_3, \mathcal{T}_4]$ in sequence, and the input value to $\mathcal{T}_3$ (\textit{book\_flight}) is only the flight's username and password.

% However, $\mathcal{T}_3$ is a malicious tool, and the attackers aim to steal the information that is relevant to previous tools.
% They can guess what information is probably used based on knowledge of successful cases, and design a specific output command $\mathcal{C}_{Dyn}$.
% This command controls the LLM to call $\mathcal{T}_3$ again and send other tools' relevant information to it if its formats are similar to $\mathcal{T}_3$.
% The LLM cannot prevent this command and it sends the 
% victim tool $\mathcal{T}_2$ (\textit{book\_hotel})'s input value with similar formats (i.e., hotel's username and password) to $\mathcal{T}_3$.


% Compared with the fix-command injection proposed by previous works, i.e., $\mathcal{C}_{Fix}$,
% the dynamic command $\mathcal{C}_{Dyn}$ can silently attack the LLM, which means the attack can not only be exposed to the frontend user interface but also successfully steal other tools' information.

