\section{Related Work and Motivation}
\label{sec:related}

\noindent\textbf{Fine-grained MoE.} 
Starting from GShard____, the Mixture-of-Experts (MoE) technology has been applied to the Transformer architecture, allowing for a significant increase in the number of parameters with only a sub-linear increase in computational resources. 
As illustrated in Figure~\ref{fig.arch}a, the dense Feed-Forward Network (FFN) modules in Transformer are replaced with MoE sub-layers, each consisting of multiple parallel experts. 
The number of experts activated by each token is defined as $top\_k$. 
However, it is challenging for these conventional MoE models to exploit expert specialization, since they are based on the top-1 or top-2 routing strategies with the coarse-grained expert activation. 
To address this issue, a new fine-grained MoE architecture is proposed in DeepSeekMoE____. 
To improve expert specialization, DeepSeekMoE maintains the same number of parameters as the conventional MoE models, while splitting experts into finer granularity and choosing a higher $top\_k$ for token distribution. 
Such an architecture with a large number of smaller experts has been adopted by DeepSeek-V2____ and Qwen2-57B-A14B____, demonstrating better model quality and computation efficiency than the conventional ones with a small number of large experts. 

Nevertheless, this fine-grained MoE architecture faces a severe communication problem for its training and inference tasks, due to the following reasons. 
First of all, to improve computation efficiency and cope with the single hardware accelerator's memory limit, the common practice is to leverage Expert Parallelism (EP) for assigning experts to different accelerators____. 
Second, EP requires injecting two costly All-to-All communication operations per MoE layer for distributing tokens to various experts and also gathering results for proceeding the computation to the next layer (green boxes in Figure~\ref{fig.arch}b). 
Third, the All-to-All communication already accounts for a large portion of the overall training or inference time, while its overhead as well as time ratio increases with the $top\_k$ value. 
Table~\ref{table.comm.ratio} shows the All-to-All time costs and time ratios of a fine-grained MoE model with 64 experts per layer and various $top\_k$ choices for training and inference jobs. 
When $top\_k$ is 1, the All-to-All overhead respectively contributes to 59.9\% and 51.2\% of the end-to-end time in training and inference. 
However, when $top\_k$ is 8, the All-to-All duration increases by 7.1$\times$ and 7.3$\times$, almost dominating the entire training and inference tasks, with proportions increasing to an astonishing 91.8\% and 90.6\%, respectively. 
In conclusion, considering that in the future new MoE models, it is very likely that their experts will become smaller and more numerous and the value of $top\_k$ will be larger, the optimization of All-to-All communication becomes urgent.



\begin{table}[!t]
    \centering
    \begin{tabular}{c|cc|cc}
    \toprule[0.8pt]
    \textbf{Expert Config} & \multicolumn{2}{c|}{\textbf{Training (ms)}} & \multicolumn{2}{c}{\textbf{Inference (ms)}} \\
    \textbf{$\bm{top\_k}$/\#experts } & \textbf{A2A}        & \textbf{Ratio}        & \textbf{A2A}        & \textbf{Ratio}        \\ \midrule[0.8pt]
    1 / 64   &     336.9          &  59.9\%            &     94.9			&	51.2\%    \\
    2 / 64   &     535.6          &  71.3\%            &     132.9			&	65.2\%        \\
    4 / 64   &     1,089.5          &  84.1\%            &     268.7			&	79.2\%    \\
    6 / 64   &     1,692.8          &  89.1\%            &     457.0			&	86.5\%         \\
    8 / 64   &     2,383.4          &  91.8\%            &     696.5			&	90.6\%    \\ 
    \bottomrule[0.8pt]
    \end{tabular}
    \caption{The All-to-All latency and its ratio in training and inference task of MoE models with small experts across various $top\_k$. The evaluation is conducted under the expert parallelism degree as 32 on 32 devices. The All-to-All duration increases by 7.1x and 7.3x when $top\_k$ is 8, corresponding to proportions of 91.8\% and 90.6\%.
}
    \label{table.comm.ratio}
\end{table}

\noindent\textbf{System-wise Optimizations. }
Fortunately, there have been a few initial attempts in the systems community to improve the scheduling of EP-enabled parallel training or inference. 
For instance, Lina____ leverages a fine-grained scheduling strategy to avoid bandwidth contention between All-to-All communication and All-Reduce communication. 
Tutel____ schedules transmission jobs in a network topology-aware fashion %%CL , e.g., local aggregation, 
to make full use of the intra-node and inter-node network bandwidth. 
Furthermore, FasterMoE____ and Tutel partition the input tokens into small chunks and overlap All-to-All communication with FFN computation in each expert. 
However, these efforts are designed for conventional MoE models, and when acting on fine-grained ones, their effect will be quite limited. This is mainly due to the fact that the amount of computation per expert is drastically reduced in the fine-grained MoE model, yet the amount of communication dominates the entire pipeline, and thus the space for bandwidth optimization and overlap optimization becomes very small.

\noindent\textbf{Communication Volume Reduction.} 
To address the communication bottleneck that is difficult to resolve at the system level, some have begun advocating data compression techniques. 
For instance, ScheMoE____ applies the ZFP compression algorithm____ to tokens before transmission and indicates that such a compression technique can significantly reduce the All-to-All communication overhead 
%zewen: it's wrong(e.g., up to 50\%) 
and accelerate MoE training. 
However, such lossy MoE structure-agnostic compression schemes can lead to a decline in model quality, making them unsuitable for downstream tasks with high precision requirements. Furthermore, the extra compression and decompression steps can bring non-negligible computational overhead. 
Therefore, there is an urgent need for a new fine-grained MoE architecture from an algorithmic perspective with the following advantages: 1) significantly reducing data volumes transferred in All-to-All communication; 2) maintaining the same model quality; and 3) avoiding extra computation overhead, comparing to the state-of-the-art fine-grained MoE structures, as well as some of the above optimizations.

\begin{table}[!t]
    \centering
    \begin{tabular}{c|c}
    \toprule[0.8pt]
    \textbf{Notation} & \textbf{Description} \\ \midrule[0.8pt]
    % $l$        & number of layers \\ \hline 
    $b$        & global batch size \\ \hline 
    $s$        & sequence length \\ \hline 
    $h$        & hidden dimension \\ \hline 
    $h\_f$     & FFN intermediate hidden dimension \\ \hline 
    $e$        & number of experts \\ \hline 
    $top\_k$   & number of experts to route to \\ \hline 
    $f$        & expert capacity factor \\ \hline 
    $ep$       & expert parallelism degree \\ \hline 
    $tp$       & tensor parallelism degree \\ \hline 
    % $v$        & vocabulary size \\ \hline
    $r$        & downscaling factor \\ \bottomrule[0.8pt]
    \end{tabular}
    \caption{Description of the notations used in this paper.}
    \label{table.notation}
\end{table}