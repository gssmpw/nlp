@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39.},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}
@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv:2006.16668},
  year={2020}
}
@article{riquelme2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}
@misc{deepseekai2024deepseekv2strongeconomicalefficient,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and others},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.04434}, 
}
@article{govreport,
  title={Efficient Attentions for Long Document Summarization.},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  journal={arXiv:2104.02112},
  year={2021}
}
@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv:2401.04088},
  year={2024}
}
@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv:2407.10671},
  year={2024}
}
@article{dai2024deepseekmoeultimateexpertspecialization,
      title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models}, 
      author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and others},
      year={2024},
      journal={arXiv:2401.06066},
}
@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}
@inproceedings{hu2024characterization,
  title={Characterization of large language model development in the datacenter},
  author={Hu, Qinghao and Ye, Zhisheng and Wang, Zerui and Wang, Guoteng and Zhang, Meng and Chen, Qiaoling and Sun, Peng and Lin, Dahua and Wang, Xiaolin and Luo, Yingwei and others},
  booktitle={21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
  pages={709--729},
  year={2024}
}

@article{hwang2023tutel,
  title={Tutel: Adaptive mixture-of-experts at scale},
  author={Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={269--287},
  year={2023}
}
@inproceedings{he2022fastermoe,
  title={Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}
@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}
@inproceedings{shi2024schemoe,
  title={ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling},
  author={Shi, Shaohuai and Pan, Xinglin and Wang, Qiang and Liu, Chengjian and Ren, Xiaozhe and Hu, Zhongzhe and Yang, Yu and Li, Bo and Chu, Xiaowen},
  booktitle={Proceedings of the Nineteenth European Conference on Computer Systems},
  pages={236--249},
  year={2024}
}
@inproceedings{li2023accelerating,
  title={Accelerating distributed $\{$MoE$\}$ training and inference with lina},
  author={Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={945--959},
  year={2023}
}
@misc{zfp.issue,
  author = {Peter Lindstrom},
  year = {2021},
howpublished="\url{https://github.com/LLNL/zfp/issues/23#issuecomment-907343129}",
  note={[last access: August 10, 2024]},
  title = {GitHub issue for ZFP: Half-precision(16-bit) floating-point compression}
}
@misc{megatron,
  author = {NVIDIA},
  year = {2019},
howpublished="\url{https://github.com/NVIDIA/Megatron-LM}",
  note={[last access: August 12, 2024]},
  title = {NVIDIA/Megatron-LM: Ongoing research training transformer models at scale}
}
@misc{korthikanti2022reducingactivationrecomputationlarge,
      title={Reducing Activation Recomputation in Large Transformer Models}, 
      author={Vijay Korthikanti and Jared Casper and Sangkug Lym and Lawrence McAfee and Michael Andersch and Mohammad Shoeybi and Bryan Catanzaro},
      year={2022},
      eprint={2205.05198},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.05198}, 
}
@misc{openwebtext,
  author = {EleutherAI},
  year = {2020},
howpublished="\url{https://github.com/EleutherAI/openwebtext2}",
  note={[last access: August 15, 2024]},
  title = {OpenWebText2}
}
@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}
@misc{megatron.paper,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08053}, 
}
@misc{paperno2016lambada,
    author={Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
    title={The LAMBADA dataset},
    DOI={10.5281/zenodo.2630551},
    publisher={Zenodo}, 
    year={2016}, 
    month={Aug}
}
@misc{merity2016pointer,
    title={Pointer Sentinel Mixture Models},
    author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
    year={2016},
    eprint={1609.07843},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{Bisk2020,
    author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
    title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
    booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
    year = {2020},
}
@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794"
}
@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}
@article{sakaguchi2019winogrande,
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
    journal={arXiv:1907.10641},
    year={2019}
}
@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitch and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993}
}
@article{zoph2022st,
  title={St-moe: Designing stable and transferable sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv:2202.08906},
  year={2022}
}
@inproceedings{ma2022bagualu,
  title={BaGuaLu: targeting brain scale pretrained models with over 37 million cores},
  author={Ma, Zixuan and He, Jiaao and Qiu, Jiezhong and Cao, Huanqi and Wang, Yuanwei and Sun, Zhenbo and Zheng, Liyan and Wang, Haojie and Tang, Shizhi and Zheng, Tianyu and others},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={192--204},
  year={2022}
}
@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}
@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}
@article{shazeer2017sparsely,
  title={The sparsely-gated mixture-of-experts layer},
  author={Shazeer, N and Mirhoseini, A and Maziarz, K and Davis, A and Le, Q and Hinton, G and Dean, J},
  journal={Outrageously large neural networks},
  year={2017}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}
@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@ARTICLE{zfp,
  author={Lindstrom, Peter},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Fixed-Rate Compressed Floating-Point Arrays}, 
  year={2014},
  volume={20},
  number={12},
  pages={2674-2683},
  keywords={Floating-point arithmetic;Image coding;Encoding;Bandwidth allocation;Data visualization;Computational modeling;Data compression;floating-point arrays;orthogonal block transform;embedded coding},
  doi={10.1109/TVCG.2014.2346458}}
@inproceedings{janus, author = {Liu, Juncai and Wang, Jessie Hui and Jiang, Yimin}, title = {Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models}, year = {2023}, isbn = {9798400702365}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3603269.3604869}, doi = {10.1145/3603269.3604869}, booktitle = {Proceedings of the ACM SIGCOMM 2023 Conference}, pages = {486–498}, numpages = {13}, keywords = {distributed training, mixture of experts, deep learning}, location = {New York, NY, USA}, series = {ACM SIGCOMM '23} }
@misc{xue2024openmoeearlyeffortopen,
      title={OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models}, 
      author={Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You},
      year={2024},
      eprint={2402.01739},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01739}, 
}
@misc {sanseviero2023moe,
    author       = { Omar Sanseviero and
                     Lewis Tunstall and
                     Philipp Schmid and
                     Sourab Mangrulkar and
                     Younes Belkada and
                     Pedro Cuenca
                   },
    title        = { Mixture of Experts Explained },
    year         = 2023,
    url          = { https://huggingface.co/blog/moe },
    note={[last access: August 15, 2024]},
    publisher    = { Hugging Face Blog }
}

@misc {needle,
    author       = { Greg Kamradt},
    title        = { Needle In A Haystack - Pressure Testing LLMs},
    year         = 2023,
howpublished="\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}",
    note={[last access: December 14, 2024]},
}
@misc {deepspeed.inference,
    author       = { Microsoft},
    title        = { DeepSpeed Inference},
    year         = 2024,
howpublished="\url{https://www.deepspeed.ai/inference/}",
    note={[last access: December 14, 2024]},
}
@misc {dataset.wikipedia.megatron,
    author       = { NVIDIA},
    title        = { Megatron-LM: Collecting Wikipedia Training Data },
    year         = 2024,
howpublished="\url{https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#collecting-wikipedia-training-data}",
    note={[last access: August 15, 2024]},
}
@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@misc {transformer.engine,
    author = {NVIDIA},
    title  = {Transformer Engine},
    year   = 2024,
    note={[last access: August 15, 2024]},
howpublished="\url{https://github.com/NVIDIA/TransformerEngine}"
}
@misc {wikidump,
    author = {Wikimedia},
    title  = {Wikimedia Downloads},
    year   = 2024,
    note={[last access: August 15, 2024]},
howpublished="\url{https://dumps.wikimedia.org}"
}
@misc {downstream.megatron,
    author = {Paresh Kharya and Ali Alvi},
    title  = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model},
    year   = 2021,
    note={[last access: August 16, 2024]},
howpublished="\url{https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/}"
}
@misc {setup.megatron,
    author = {NVIDIA},
    title  = {Setup for Megatron},
    year   = 2024,
    note={[last access: August 16, 2024]},
howpublished="\url{https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#setup}"
}
}
@misc {profiler,
    author = {PyTorch},
    title  = {PyTorch Profiler},
    year   = 2024,
    note={[last access: August 16, 2024]},
howpublished="\url{https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html}"
}
@article{megablocks,
  title={{MegaBlocks: Efficient Sparse Training with Mixture-of-Experts}},
  author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
    note={[last access: August 16, 2024]},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{flashattention,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08691}, 
}

