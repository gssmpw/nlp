[
  {
    "index": 0,
    "papers": [
      {
        "key": "lepikhin2020gshard",
        "author": "Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng",
        "title": "Gshard: Scaling giant models with conditional computation and automatic sharding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "dai2024deepseekmoeultimateexpertspecialization",
        "author": "Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and others",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "deepseekai2024deepseekv2strongeconomicalefficient",
        "author": "DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and others",
        "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others",
        "title": "Qwen2 technical report"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39."
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2023accelerating",
        "author": "Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong",
        "title": "Accelerating distributed $\\{$MoE$\\}$ training and inference with lina"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hwang2023tutel",
        "author": "Hwang, Changho and Cui, Wei and Xiong, Yifan and Yang, Ziyue and Liu, Ze and Hu, Han and Wang, Zilong and Salas, Rafael and Jose, Jithin and Ram, Prabhat and others",
        "title": "Tutel: Adaptive mixture-of-experts at scale"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "he2022fastermoe",
        "author": "He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin",
        "title": "Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shi2024schemoe",
        "author": "Shi, Shaohuai and Pan, Xinglin and Wang, Qiang and Liu, Chengjian and Ren, Xiaozhe and Hu, Zhongzhe and Yang, Yu and Li, Bo and Chu, Xiaowen",
        "title": "ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zfp",
        "author": "Lindstrom, Peter",
        "title": "Fixed-Rate Compressed Floating-Point Arrays"
      }
    ]
  }
]