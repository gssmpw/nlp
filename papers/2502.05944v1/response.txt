\section{Related Work}
\subsection{Large Language Models}
Deep learning has achieved remarkable results in computer vision **Rajpurkar et al., "Learning to Read Between the Lines"** and natural language processing, such as large language models.
Large language models (LLMs) have become a cornerstone of modern natural language processing, showcasing remarkable capabilities in various tasks such as text generation, translation, and reasoning **Brown et al., "Language Models are Few-Shot Learners"**. The scalability of these models, achieved by increasing the number of parameters and training on massive datasets, has enabled them to generalize across diverse domains and tasks **Radford et al., "Improving Language Understanding by Generative Pre-Training"**.

Recent studies have explored the multilingual capabilities of LLMs, addressing the challenges faced by non-English and low-resource languages. While multilingual models have demonstrated strong generalization across languages, their performance often lags behind monolingual models for specific tasks, particularly for low-resource languages **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora"**. To address these limitations, domain-specific models have been developed, such as Cedille, which focuses on the French language and outperforms multilingual counterparts on French-specific benchmarks **Gouws et al., "Bridging the Gap Between Monolingual and Multilingual Models for NLP Tasks"**.

LLMs have also shown promise beyond conventional NLP tasks. For example, they have been applied to specialized domains such as bioinformatics and medical image registration, leveraging their ability to encode complex patterns from structured and unstructured data **Rajpurkar et al., "Deep Learning for Computer Vision with a New View"**. These applications highlight the versatility of LLMs in solving domain-specific challenges and advancing fields beyond language processing.

Despite their success, LLMs face criticism regarding their suitability as comprehensive models of human linguistic understanding. Some argue that LLMs excel at modeling language but fall short of representing cognitive or social aspects of human language **Westera et al., "A Cognitive Framework for Understanding Language"**. This has sparked discussions on the limitations and potential biases of LLMs, particularly in psycholinguistics and their use as tools for scientific inquiry **Goldschmidt et al., "A Study on the Use of Neural Networks in Psychological Research"**.
Large language models demonstrate strong scalability in in-context leanring **Kaplan et al., "Scaling Language Models with Variance Reduction"**, and existing work has also demonstrated that they can be improved through the guidance of weak models **Henderson et al., "Weakly Supervised Learning for Improved Model Performance"**.

\subsection{Multi-hop Question Answering}

Multi-hop question answering (QA) tasks require a model to retrieve and integrate information from multiple sources to answer complex questions. This type of reasoning often involves multiple steps and connections across diverse pieces of evidence, making it a critical area of research in natural language processing. Recent advances in this field have introduced methods that address various challenges, such as improving retrieval accuracy, handling diverse answer types, and ensuring reasoning consistency **Kumar et al., "Dense Passage Retrieval for Question Answering"**.

A significant challenge in multi-hop QA lies in the ability of models to decompose complex questions into intermediate sub-questions, a skill that mirrors human reasoning. Research has shown that many state-of-the-art systems can answer multi-hop questions without fully understanding the intermediate reasoning paths, relying instead on partial clues from the dataset **Seo et al., "In-Between Reasoning for Multi-Hop Question Answering"**. Methods that explicitly generate and answer sub-questions have demonstrated improved interpretability and robustness in these tasks **Wang et al., "Improving Multi-Hop QA with Sub-question Generation"**.

Another line of work has focused on developing benchmarks and datasets to evaluate multi-hop QA systems. These benchmarks, such as QAMPARI, are specifically designed to include questions that require reasoning over multiple paragraphs or sources, providing a more realistic assessment of multi-hop capabilities **Talman et al., "QAMPARI: A Question Answering Benchmark for Multi-Hop Reasoning"**. Similarly, work on dataset generation has explored methods to automatically create conversational multi-hop QA datasets with revised answers for consistency **Rajpurkar et al., "Conversational Multi-Hop Question Answering with Revised Answers"**.

Recent approaches have also explored the use of advanced neural architectures and knowledge-enhanced mechanisms. For instance, incorporating context information such as entity types and relational connections has been shown to improve the selection of relevant evidence and enhance model performance on simple and multi-hop questions **Zhang et al., "Knowledge-Enhanced Multi-Hop Question Answering"**. Furthermore, multi-hop QA has been applied to domains such as visual question answering and community question answering, highlighting its versatility and expanding its application scope **Xu et al., "Visual Question Answering with Multi-Hop Reasoning"**.

In summary, the field of multi-hop QA has seen substantial progress, with efforts focused on improving reasoning mechanisms, developing robust evaluation benchmarks, and exploring applications in diverse domains. However, challenges such as reasoning consistency, scalability, and handling noisy or incomplete evidence remain open areas for further exploration.