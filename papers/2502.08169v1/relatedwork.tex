\section{RELATED WORKS}
\subsection{Collaborative Perception}
Collaborative perception addresses the challenges of limited perception range and occlusion in single-vehicle autonomous driving by enabling information exchange between agents. High-quality datasets like DAIR-V2X \cite{cp:Dair-v2x}, V2XSet \cite{cp:v2xvit}, OPV2V \cite{cp:OPV2V}, and V2X-Sim \cite{cp:v2x-sim} have be given, and notable methods are proposed, such as DiscoNet \cite{cp:disconet} using a teacher-student distillation framework to extract more information during training, V2X-ViT \cite{cp:v2xvit} the first heterogeneous Transformer for V2X perception, CoAlign \cite{cp:coalign} which enhances consistency between agents using pose graph modeling, and CoBEVFlow \cite{cp:cobevflow} which mitigates delays through motion prediction.

% To address the issues of limited perception range and occlusion in single-vehicle autonomous driving solutions, Collaborative Perception has been proposed as a potential solution. By exchanging information between multiple vehicles, Collaborative Perception enhances perception performance, driving the creation of high-quality datasets such as DAIR-V2X\cite{cp:Dair-v2x}, V2XSet\cite{cp:v2xvit}, OPV2V\cite{cp:OPV2V} and V2X-Sim\cite{cp:v2x-sim}. Many approaches have made significant progress on these datasets, such as DiscoNet\cite{cp:disconet}, which uses a teacher-student knowledge distillation framework to extract more perception information during training; V2X-ViT\cite{cp:v2xvit}, which designed the first heterogeneous Transformer architecture for V2X perception; CoAlign\cite{cp:coalign}, which enhances consistency between collaborative agents through pose graph modeling; and CoBEVFlow\cite{cp:cobevflow}, which compensates for delays through motion prediction.

\subsection{Temporal Asynchrony Issue in Collaborative Perception}

% In real-world environments, temporal asynchrony is unavoidable. It can amplify errors from noise and model deficiencies. A lot of methods are proposed to address this issue. For example, V2VNet \cite{cp:v2vnet} compensates for delays using convolutional neural networks. V2X-ViT \cite{cp:v2xvit} handles spatio-temporal distortions with a position encoding module. However, these approaches underutilize historical frame data. SyncNet \cite{cp:SyncNet} addresses delays with Conv-LSTM \cite{common:conv-lstm} but struggles with noise and irregular delays due to its RNN-based feature generation. FFNet \cite{cp:FFNet} processes irregular multi-frame data but introduces noise during frame generation and relies on specific infrastructure. CoBEVFlow \cite{cp:cobevflow} requires a high-quality ROI generator and its three-stage training can lead to error propagation from delays. To address these limitations, we optimized asynchronous collaborative perception framework.

Temporal asynchrony is unavoidable in real-world environments, exacerbating errors caused by noise and model deficiencies. Existing solutions like V2VNet \cite{cp:v2vnet} employ CNNs for delay compensation, while V2X-ViT \cite{cp:v2xvit} mitigates spatio-temporal distortions via position encoding. However, these methods insufficiently leverage historical data. Though SyncNet \cite{cp:SyncNet} applies Conv-LSTM \cite{common:conv-lstm} for delay handling, its RNN-based feature generation struggles with noise and irregular delays. FFNet \cite{cp:FFNet} processes irregular multi-frame data but introduces generation noise and infrastructure dependencies. CoBEVFlow \cite{cp:cobevflow} demands high-quality ROI generation, and its three-stage training risks error propagation from delays. To overcome these limitations, we propose an optimized asynchronous collaborative perception framework.

% In real-world environments, temporal asynchrony is inevitable and can exacerbate errors caused by noise and model inadequacies. Some methods have considered the issue of temporal asynchrony. For instance, V2VNet\cite{cp:v2vnet} compensates for delays through convolutional neural networks, while V2X-ViT\cite{cp:v2xvit} uses a position encoding module to handle spatiotemporal distortions. However, these methods do not fully utilize historical frame information. SyncNet\cite{cp:SyncNet} compensates for delays through Conv-LSTM\cite{common:conv-lstm}, but its RNN-based feature generation is prone to introducing noise and struggles with irregular delays. Although FFNet\cite{cp:FFNet} supports irregular multi-frame data processing, its frame generation still introduces noise, and its reliance on specific infrastructure limits its application scenarios. CoBEVFlow\cite{cp:cobevflow} depends on a high-quality ROI Generator and employs a three-stage training process, which leads to error propagation across modules due to delays. Given this situation, we further explored how to optimize the asynchronous collaborative perception framework to overcome these limitations.

\subsection{Uncertainty QuantiÔ¨Åcation}

Uncertainty quantification critically evaluates prediction reliability through two categories: aleatoric (data noise irreducible with data volume) and epistemic (reducible model limitations). Aleatoric uncertainty is captured through direct modeling (DM) \cite{unc:dm} with variance prediction outputs; epistemic uncertainty estimated via Monte Carlo dropout \cite{unc:mcdropout} approximating Bayesian inference or deep ensembles \cite{unc:deepensemble}. In autonomous driving, this enhances safety in perception \cite{unc:UMoE}, \cite{unc:multi-modal_DiFeng} and decision-making. Current collaborative perception research predominantly applies uncertainty to downstream tasks like planning and control \cite{unc:doubleM}, \cite{cp:mot-cup}, neglecting specific challenge resolution. In CoDynTrust, we quantify both uncertainties using DM and MC dropout to guide asynchronous collaborative perception, filtering out noise and low-quality data, and propagating uncertainty to tasks such as planning and control.
% Uncertainty quantification is essential for estimating prediction reliability. It can be divided into aleatoric and epistemic uncertainty. Aleatoric uncertainty stems from data noise and cannot be reduced with more data, while epistemic uncertainty arises from model limitations and can be reduced with additional training. Aleatoric uncertainty is often assessed via direct modeling (DM) \cite{unc:dm}, which adds a variance prediction output and optimizes via losses like negative log-likelihood. Epistemic uncertainty is typically measured using methods like monte carlo (MC) dropout \cite{unc:mcdropout}, which approximates Bayesian inference through random sampling, or deep ensembles \cite{unc:deepensemble} that combine outputs from multiple models for robustness. In autonomous driving, uncertainty quantification enhances safety and robustness in perception \cite{unc:UMoE}, \cite{unc:multi-modal_DiFeng} and decision-making. While it has been explored in collaborative perception, most research focuses on applying uncertainty    to downstream tasks like planning and control \cite{unc:doubleM, cp:mot-cup}, rather than addressing specific challenges. In CoDynTrust, we quantify both uncertainties using DM and MC dropout to guide asynchronous collaborative perception, filtering out noise and low-quality data, and propagating uncertainty to tasks such as planning and control.