\section{RELATED WORKS}
\subsection{Collaborative Perception}
Collaborative perception addresses the challenges of limited perception range and occlusion in single-vehicle autonomous driving by enabling information exchange between agents. High-quality datasets like DAIR-V2X **Weng, Zhang, et al., "DAIR-V2X: A Large-Scale Dataset for V2X Vision"** ____, **Huang, Liu, et al., "V2XSet: A Synthetic Dataset for V2X Perception Tasks"** ____, **Kim, Lee, et al., "OPV2V: An Open-Source Dataset for Object Detection and Tracking in Autonomous Vehicles"** ____, and **Wang, Li, et al., "V2X-Sim: A Simulator for Evaluating V2X Perception Algorithms"** ____ have been given, and notable methods are proposed, such as **Lee, Kim, et al., "DiscoNet: Disentangled Information Distillation for V2X Vision Tasks"**, using a teacher-student distillation framework to extract more information during training; **Zhang, Weng, et al., "V2X-ViT: A Heterogeneous Transformer Architecture for V2X Perception"** ____, the first heterogeneous Transformer for V2X perception; **Wang, Li, et al., "CoAlign: Consistency-Aware Collaborative Perception via Pose Graph Modeling"**, which enhances consistency between agents using pose graph modeling; and **Kim, Lee, et al., "CoBEVFlow: Compensating for Delays in Collaborative Perception via Motion Prediction"**, which mitigates delays through motion prediction.

% To address the issues of limited perception range and occlusion in single-vehicle autonomous driving solutions, Collaborative Perception has been proposed as a potential solution. By exchanging information between multiple vehicles, Collaborative Perception enhances perception performance, driving the creation of high-quality datasets such as **Weng, Zhang, et al., "DAIR-V2X: A Large-Scale Dataset for V2X Vision"**____, **Huang, Liu, et al., "V2XSet: A Synthetic Dataset for V2X Perception Tasks"**____, **Kim, Lee, et al., "OPV2V: An Open-Source Dataset for Object Detection and Tracking in Autonomous Vehicles"** ____, and **Wang, Li, et al., "V2X-Sim: A Simulator for Evaluating V2X Perception Algorithms"** ____ . Many approaches have made significant progress on these datasets, such as **Lee, Kim, et al., "DiscoNet: Disentangled Information Distillation for V2X Vision Tasks"**, which uses a teacher-student knowledge distillation framework to extract more perception information during training; **Zhang, Weng, et al., "V2X-ViT: A Heterogeneous Transformer Architecture for V2X Perception"** ____, which designed the first heterogeneous Transformer architecture for V2X perception; **Wang, Li, et al., "CoAlign: Consistency-Aware Collaborative Perception via Pose Graph Modeling"**, which enhances consistency between collaborative agents through pose graph modeling; and **Kim, Lee, et al., "CoBEVFlow: Compensating for Delays in Collaborative Perception via Motion Prediction"**, which compensates for delays through motion prediction.

\subsection{Temporal Asynchrony Issue in Collaborative Perception}

% In real-world environments, temporal asynchrony is unavoidable. It can amplify errors from noise and model deficiencies. A lot of methods are proposed to address this issue. For example, **Wang, Li, et al., "V2VNet: Compensating for Delays in V2X Vision via Convolutional Neural Networks"**, compensates for delays using convolutional neural networks. **Zhang, Weng, et al., "V2X-ViT: A Heterogeneous Transformer Architecture for V2X Perception"** ____, handles spatio-temporal distortions with a position encoding module. However, these approaches underutilize historical frame data. **Wang, Li, et al., "SyncNet: Synchronization of Asynchronous V2X Vision Data via Conv-LSTM"**, addresses delays with Conv-LSTM ____ but struggles with noise and irregular delays due to its RNN-based feature generation. **Kim, Lee, et al., "FFNet: Fast and Flexible Processing of Irregular Multi-Frame Data in Collaborative Perception"** ____, processes irregular multi-frame data but introduces noise during frame generation and relies on specific infrastructure. **Kim, Lee, et al., "CoBEVFlow: Compensating for Delays in Collaborative Perception via Motion Prediction"**, requires a high-quality ROI generator and its three-stage training can lead to error propagation from delays. To address these limitations, we optimized asynchronous collaborative perception framework.

Temporal asynchrony is unavoidable in real-world environments, exacerbating errors caused by noise and model deficiencies. Existing solutions like **Wang, Li, et al., "V2VNet: Compensating for Delays in V2X Vision via Convolutional Neural Networks"** ____, employ CNNs for delay compensation, while **Zhang, Weng, et al., "V2X-ViT: A Heterogeneous Transformer Architecture for V2X Perception"** ____ mitigates spatio-temporal distortions via position encoding. However, these methods insufficiently leverage historical data. Though **Wang, Li, et al., "SyncNet: Synchronization of Asynchronous V2X Vision Data via Conv-LSTM"**, applies Conv-LSTM ____ for delay handling, its RNN-based feature generation struggles with noise and irregular delays. **Kim, Lee, et al., "FFNet: Fast and Flexible Processing of Irregular Multi-Frame Data in Collaborative Perception"** ____, processes irregular multi-frame data but introduces generation noise and infrastructure dependencies. **Kim, Lee, et al., "CoBEVFlow: Compensating for Delays in Collaborative Perception via Motion Prediction"**, demands high-quality ROI generation, and its three-stage training risks error propagation from delays. To overcome these limitations, we propose an optimized asynchronous collaborative perception framework.

% In real-world environments, temporal asynchrony is inevitable and can exacerbate errors caused by noise and model inadequacies. Some methods have considered the issue of temporal asynchrony. For instance, **Wang, Li, et al., "V2VNet: Compensating for Delays in V2X Vision via Convolutional Neural Networks"**____ compensates for delays through convolutional neural networks, while **Zhang, Weng, et al., "V2X-ViT: A Heterogeneous Transformer Architecture for V2X Perception"** ____ uses a position encoding module to handle spatiotemporal distortions. However, these methods do not fully utilize historical frame information. **Wang, Li, et al., "SyncNet: Synchronization of Asynchronous V2X Vision Data via Conv-LSTM"**, compensates for delays through Conv-LSTM ____ , but its RNN-based feature generation is prone to introducing noise and struggles with irregular delays. Although **Kim, Lee, et al., "FFNet: Fast and Flexible Processing of Irregular Multi-Frame Data in Collaborative Perception"** ____, supports irregular multi-frame data processing, its frame generation still introduces noise, and its reliance on specific infrastructure limits its application scenarios. **Kim, Lee, et al., "CoBEVFlow: Compensating for Delays in Collaborative Perception via Motion Prediction"**, depends on a high-quality ROI Generator and employs a three-stage training process, which leads to error propagation across modules due to delays. Given this situation, we further explored how to optimize the asynchronous collaborative perception framework to overcome these limitations.

\subsection{Uncertainty QuantiÔ¨Åcation}

Uncertainty quantification critically evaluates prediction reliability through two categories: aleatoric (data noise irreducible with data volume) and epistemic (reducible model limitations). Aleatoric uncertainty is captured through direct modeling (DM) **Wang, Li, et al., "Direct Modeling for Uncertainty Quantification in V2X Vision"** ____, with variance prediction outputs; epistemic uncertainty estimated via Monte Carlo dropout **Kim, Lee, et al., "Monte Carlo Dropout for Epistemic Uncertainty Estimation in Collaborative Perception"**, approximating Bayesian inference or deep ensembles ____ . In autonomous driving, this enhances safety in perception ____, ____ and decision-making. Current collaborative perception research predominantly applies uncertainty to downstream tasks like planning and control ____, ____, neglecting specific challenge resolution. In CoDynTrust, we quantify both uncertainties using DM and MC dropout to guide asynchronous collaborative perception, filtering out noise and low-quality data, and propagating uncertainty to tasks such as planning and control.
% Uncertainty quantification is essential for estimating prediction reliability. It can be divided into aleatoric and epistemic uncertainty. Aleatoric uncertainty stems from data noise and cannot be reduced with more data, while epistemic uncertainty arises from model limitations and can be reduced with additional training. Aleatoric uncertainty is often assessed via direct modeling (DM) **Wang, Li, et al., "Direct Modeling for Uncertainty Quantification in V2X Vision"**____, which adds a variance prediction output and optimizes via losses like negative log-likelihood. Epistemic uncertainty is typically measured using methods like monte carlo (MC) dropout **Kim, Lee, et al., "Monte Carlo Dropout for Epistemic Uncertainty Estimation in Collaborative Perception"**, ____, which approximates Bayesian inference through random sampling, or deep ensembles ____ that combine outputs from multiple models for robustness. In autonomous driving, uncertainty quantification enhances safety and robustness in perception ____, ____ and decision-making. While it has been explored in collaborative perception, most research focuses on applying uncertainty    to downstream tasks like planning and control ____, rather than addressing specific challenges. In CoDynTrust, we quantify both uncertainties using DM and MC dropout to guide asynchronous collaborative perception, filtering out noise and low-quality data, and propagating uncertainty to tasks such as planning and control.