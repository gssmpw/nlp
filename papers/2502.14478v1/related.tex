\section{Related Work}
\label{sec:related}

%\vspace{-5pt}

\paragraph{Learning Data Structure Invariants} 
%
%
%
% On the other hand, \tool requires additional inputs for its synthesis, such as the pure value of the positive examples, which is our trade-off between expressivity and simplicity.
%


Other than \shape~\cite{LPAR23:Learning_Data_Structure_Shapes},
discussed extensively in \autoref{sec:done}, earlier work on shape
analysis also used inductive synthesis to generate shape
predicates~\cite{guo2007shape}, but the input of the synthesis
framework is a program that constructs the data structure instance,
providing more information (\eg, the recursion structure) compared to
memory graphs.
%
Similar to \shape, that work only considers the shape relation without
the data properties. \tname{DOrder}~\cite{zhu2016automatically} and
\tname{Evospex}~\cite{molina2021evospex} are two later works on
learning the data invariants from the constructors of the data
structures (in OCaml or Java).
\tname{Locust}~\cite{brockschmidt2017learning} infers shape predicates
from pre-defined definitions with statistical machine learning, with
no completeness guarantees. The work by
\citeauthor{molina2019training}~\cite{molina2019training} describes a deep learning-based
framework that implements a binary classifier for the data structure
invariants; unlike our work, it does not
provide logical descriptions of data structures but merely tells valid
structure instances from invalid ones. Though more machine learning
methods \cite{DBLP:conf/spin/UsmanWWYDK19} have shown to be effective
in learning data structure, the training data is required to be large
and diverse, which is not always available in practise.
\citet{DBLP:phd/basesearch/Dohrau22} describes a black-box approach to
infer SL specifications and predicates from programs based on ICE
learning~\cite{garg2014ice}, while it can neither deal with, nor be
easily extensible to nested data structures. \tname{SLING} is a
framework to infer program specifications in Separation Logic from
memory graphs~\cite{le2019sling}; it does not infer new heap
predicates and instead offers a number of pre-defined structure
shapes, where \tool can work as a complementary tool to help the user
to pre-define new predicates.

% %
% \tname{Hanoi}~\cite{miltner2020data} is another framework for data
% structure invariant synthesis.
% %
% Unlike \tool, which targets inference of shape and data descriptions
% in Separation Logic, \tname{Hanoi}, given a data structure
% implementation as self-contained module, infers properties satisfied
% by different \emph{compositions} of the functions defined within this module.

% We believe that other decidable logics for describing heap-based
% structures could be also used as a target language for our synthesis
% mechanism.
% %
% In the future, we are planning to generalise our inference mechanism
% to other formalisms, such as
% \tname{DRYAD}$\!_{\mathit{dec}}$~\cite{qiu2019decidable}, a decidable
% logic for tree-liked structures, which provides categorised functions,
% such as set functions, measure functions, \etc.
% %
% This will require us to extend \popper with a feature of categorising
% predicates.


% \todo{maybe ILP section, especially Mayur's works}


\paragraph{Synthesising Declarative Representations}

The approach of \tool extends two lines of work on synthesis of
declarative representations programs and data.
%
The first one is inductive logic programming (ILP), which aims to
learn logic programs from examples.
\tname{Progol}~\cite{muggleton1997learning} is an early notable ILP
system that achieves positive-only learning by Bayesian framework,
which is not sound in general. Importantly, \tname{Progol} does not
support learning recursive logic programs.
%
\tname{AMIE}~\cite{galarraga2013amie} is another knowledge rule-mining
framework with positive-only examples, but the learning is in
\tname{AMIE} mainly targeted knowledge base graphs, so the learned
rules were not as complex as in ILP settings.
%
Other than those conventional ILP methods that synthesise \prolog
programs, significant progress has been made recently on synthesising
\tname{Datalog}~\cite{thakkar2021example,10.1145/3622847} and ASP
programs~\cite{DBLP:conf/jelia/LawRB14,DBLP:conf/aaai/LawRBB020} from
examples.
%
While the syntax of Separation Logic assertions and predicates can be
expressed in the \tname{Datalog} or ASP domain, the approaches
developed in the past efforts are not immediately applicable, as they:
(1)~may require negative examples (in the case of \tname{Datalog}
synthesisers), and (2)~limit the pre-defined predicates expressed only
by grounded facts.
%
In particular, the latter means that the predicates used in the
synthesis cannot express arbitrary operations, because grounded facts
are fully instantiated and do not contain variables. 
%
This limitation makes impossible the representation of general rules
or operations that can be applied to a range of inputs. For example, a
grounded fact can state that a specific element belongs to a set, but
it cannot express a general rule for membership that applies to
\emph{any} element. 
%
At the same time, in \tool, the predicates are written in \prolog, a
Turing-complete language, which enables definitions of operations like
list append, set union, \etc,---a feature our approach has directly
inherited from \popper~\cite{cropper2021learning}.

The second relevant line of work is \emph{specification synthesis},
which aims at synthesising formulas within various but pre-defined
domains. 
%
Our graph generator (\autoref{sec:generator}) follows the ideas of
\precis~\cite{DBLP:journals/pacmpl/AstorgaSDWMX21}, which similarly
uses test cases as a learning oracle to generate positive example
for synthesising program contracts. 
%
At the same time, the formal guarantees provided by positive only
learning--generating all non-comparable and most specific predicates,
are similar to those of \tname{Spyro}~\cite{park2023synthesizing},
which defines a general framework for synthesising specifications for
customisable domains. The main difference between \tool and those
works is that \tool synthesises \emph{predicates} that can contain
\emph{recursive definitions}---an aspect cannot be handled by the
existing specification synthesisers.

The overlap between two lines of work above is the notion of
\emph{least general generalisation}~(LGG)~\cite{plotkin1970note}. The
example-based specification synthesisers can be considered as learning
the LGG of the examples, which is exactly what early bottom-up ILP
systems do. The limitation of existing LGG operations is well-known in
ILP~\cite{CropperD22}: there is no LGG operations for recursive logic
programs, which is the reason why modern ILP systems are defined in a
top-down fashion.

\paragraph{Answer Set Programming v. Satisfiability Modulo Theories}

ASP plays a crucial role in our work, similar to that of SMT solvers
most contemporary synthesis tools. 
%
As mentioned by \citeauthor{bembenek2023smt}~\cite{bembenek2023smt},
ASP is effective at search tasks involving fixpoints: pruning in \tool
can be encoded easily with recursive logic predicates, some of which
though can be expressed in SMT, need to be encoded in a more complex
and hard-to-understand way.
%
Another advantage of ASP is its efficiency when enumerating \emph{all}
solutions in a search space, which is crucial for exhaustively exploring
the space of SL predicates. 
%
In contrast, in most state-of-the-art SMT solvers, only one model is
returned at a time, so for obtaining a complete set of models, the
user must either block an obtained model and re-run the solver to get
the next one with high overhead, or use expert-level
techniques~\cite{bjorner2022user}.
%
On the other hand, SMT solvers usually come with a rich set of
theories, whereas ASP modulo theories is still limited to basic
theories like difference logic
\cite{DBLP:journals/tplp/JanhunenKOSWS17,DBLP:journals/algorithms/RajaratnamSWCLS23}
and acyclicity constraints \cite{DBLP:conf/lpnmr/BomansonGJKS15}.


