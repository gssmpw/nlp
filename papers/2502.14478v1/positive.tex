% !TEX root = ../paper.tex
\section{Positive-Only Predicate Learning}
\label{sec:positive}

In this section, we describe our approach for learning predicates from
positive-only examples not specific to SL predicates. To make the learning effective, we introduce the idea of predicate clause \emph{minimisation}, then explain how our specificity-based positive-only
learning works by showing its difference from the standard ILP systems \cite{cropper2021learning,cropper2022learning2}.






\vspace{-5pt}
\subsection{Normalising the Positive-Only Learning}
\label{sec:normalise}


As discussed in \autoref{sec:most-specific}, without negative
examples, adopting the smallest-size hypothesis, as done in many
program synthesisers~\cite{ji2021generalizable,lee2021combining}, can
lead to finding solutions that are too imprecise.
%
What about selecting the candidate with the largest size instead?
%
As shown by the {sorted list} (\pcode{srtl(X, S)}) example, more
literals in the body do not mean the predicate is more specific if the
body contains \emph{redundant} constraints among variables. Therefore, we first define a normalisation procedure to remove the redundancy in the predicate clauses with the help of the logical
\emph{entailment} (denoted as~$\models$ for \prolog clauses known as
\emph{definite clauses} in first-order logic).
%

\begin{definition}[Clause Minimisation]
  Let $\mathbf{numlit}(h)$ denotes the number of literals in the body
  of a clause $h$, and $ A \iff B$ denotes $A~\models~B$ and
  $B~\models~A$. A predicate clause $h$ is eliminated \Iff
  $\exists h_0,~h_0 = \arg\min_{X \in S} \mathbf{numlit}(X)~\land$
  $\mathbf{numlit}(h_0) < \mathbf{numlit}(h)$, where $S$ =
  $\{X \mid X \iff h\}$.
  
\end{definition}

In plain words, if for any clause $h$ there is a shorter clause $h_0$ equivalent (defined by entailments) to $h$, $h$ is
eliminated in the synthesis. Such minimisation is naturally encoded by ASP (detailed in \autoref{app:minimisation}).
Our experiments show that enumerable (also decidable) entailment rules
are effective for the synthesis of SL predicates within finite clause
length (\cf \autoref{sec:done}). Note that the minimisation is essentially a pruning for unnecessary clauses: we do not handle predicate-level redundancy (\aka Horn literal
minimisation~\cite{cepek1997stuctural}) because it is NP-hard~\cite{DBLP:journals/ai/HammerK93} in general. 



\subsection{An Algorithm for Positive-Only Learning}
\label{sec:popper2}
%
\input{figure/popper}

Our approach to learning predicates from positive-only examples can easily make use of components from the existing ASP-based ILP systems~\cite{cropper2021learning,cropper2022learning2}, seamlessly. In this section, we describe both the basic ILP learning loop by ASP and the procedure of our positive-only learning.

\autoref{alg:popper} (with the \graybox{light grey cover}) shows the
pseudocode of classic ILP learning loop, which follows the
``generate-test-constrain'' approach. It takes (1)~background
knowledge \pcode{bk}, (2)~positive and negative examples \pcode{exs},
and (3)~learning bias defining parameters of the search
\pcode{max_size}, such as the maximum size of a predicate to be
searched and other customisable parameters (\eg, whether to enable
mutually-recursive definitions), summarised as \pcode{in_cons}.
%
After encoding the iterative deepening search problem into an answer
set program, it first uses the ASP solver
\clingo~\cite{gebser2014clingo} to generate a hypothesis (line~4), and
then uses \prolog to test it against the provided examples (line~8).
%
A hypothesis is \emph{complete} if it entails all positive examples,
and \emph{consistent} if it entails no negative examples.
%
ILP returns a hypothesis as the solution if it is complete and
consistent (line~11); otherwise, it \emph{prunes} the search space
using the test outcome (line~12) and continues the search.
%
% The pruning procedure reducing it based on the failed
% \pcode{outcome} (\ie, not consistent and complete).
%
The test-based pruning works as follows.
%
Whenever hypothesis testing shows that not all positive examples are
true (incomplete), the pruning on \emph{specialisation} is applied;
whenever part of the negative examples are true (inconsistent), the pruning
on \emph{generalisation} is applied---ideas of the pruning are illustrated in
\autoref{sec:popper} and are detailed in~\cite{cropper2021learning}.
%
% The output of \popper is thus a complete and consistent hypothesis of
% the \emph{smallest size}, which is also a common optimality metric in
% program synthesis~\cite{ji2021generalizable,lee2021combining}.


% \subsubsection{The Effect of Introduced Enhancements.}

For the case of \emph{positive-only} learning
(\cf~\autoref{alg:popper} with the \darkgraybox{dark grey cover}), we
follow the approach from the work on \emph{specification
  synthesis}~\cite{park2023synthesizing} to output a set of
non-comparable (\ie, the element in the set is not entailed by any other) solutions as a conjunction: there are possibly several
solutions satisfying the positive examples where none of them is more
specific than the others, so the conjunction of them is the most
specific specification. 
%
The main difference from classic ILP starts from line~13, when a
hypothesis \pcode{h} is complete and is not entailed by the existing
solution set \pcode{sol}, the solution set is updated to adding
\pcode{h} to the set, together with removing all solutions in
\pcode{sol} that are comparable (semantically more general in our
case) to \pcode{h} (line~14).
%
The set of most specific solutions in the search space is returned
when the search is exhausted (line~16). The advanced pruning procedure
(line~15) is enabled by domain knowledge (\ie, Separation Logic
predicates in our case---\cf~\autoref{sec:sldomain}).

We conclude this section by proving the soundness of positive-only learning in \autoref{alg:popper}.

\begin{theorem}
  \label{thm:specific}
  The hypothesis set returned by the positive-only learning in
  \emph{\autoref{alg:popper}} contains all non-comparable, most specific predicates that is complete (\ie, entailing all examples) in the search
  space defined by the algorithm's initial constraints
  (\pcode{in_cons}) and the size limit (\pcode{max_size}) parameters.
\end{theorem}
\begin{proof}[Proof]
  By induction on the size limit \pcode{max_size} of the predicate: when \pcode{max_size} is 0, there is no predicate hypotheses, so \pcode{True} (the ``always true predicate'') is the only most specific one. Then assume that the theorem holds for \pcode{max_size} $n$, \ie, \pcode{sol_i} is the most specific hypothesis set; we prove it for \pcode{max_size} $n+1$.

  When \pcode{max_size} is $n+1$, based on the while loop in
  \autoref{alg:popper}, the search space for $n+1$ is the search space
  for $n$ plus when \pcode{size} is $n+1$. By the induction
  hypothesis, \pcode{sol_i} is the most specific hypothesis set in the search space
  for $n$, and the output \pcode{sol} is either \pcode{sol_i} or containing
  more specific predicates of space $n+1$ with all comparable predicates removed. Therefore, \pcode{sol} is the most
  specific hypothesis set in the search space with $n+1$ as \pcode{max_size}.

\end{proof}




