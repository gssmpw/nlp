\section{Related Work}
\label{sec:related}

%\vspace{-5pt}

\paragraph{Learning Data Structure Invariants} 
%
%
%
% On the other hand, \tool requires additional inputs for its synthesis, such as the pure value of the positive examples, which is our trade-off between expressivity and simplicity.
%

Other than Alvar et al., "Synthesizing Heap Predicates" (2020),
discussed extensively in \autoref{sec:done}, earlier work on shape
analysis also used inductive synthesis to generate shape
predicates~\cite{Li2018LearningDSI},
but the input of the synthesis
framework is a program that constructs the data structure instance,
providing more information (\eg, the recursion structure) compared to
memory graphs.
%
Similar to Alvar et al., "Synthesizing Heap Predicates" (2020), that work only considers the shape relation without
the data properties. DOrder~\cite{DOrder} 
and Evospex~\cite{Evospex} are two later works on
learning the data invariants from the constructors of the data
structures (in OCaml or Java).
Locust~\cite{Locust} infers shape predicates
from pre-defined definitions with statistical machine learning, with
no completeness guarantees. The work by Li et al., "Learning Data Structure Invariants" (2020) 
describes a deep learning-based
framework that implements a binary classifier for the data structure
invariants; unlike our work, it does not
provide logical descriptions of data structures but merely tells valid
structure instances from invalid ones. Though more machine learning
methods~\cite{DOrder,Li2018LearningDSI} have shown to be effective
in learning data structure, the training data is required to be large
and diverse, which is not always available in practise.
Roddick et al., "Inferring SL Specifications and Predicates from Programs" (2022) 
describes a black-box approach to
infer SL specifications and predicates from programs based on ICE
learning~\cite{ICE},
while it can neither deal with, nor be
easily extensible to nested data structures. \tname{SLING} is a
framework to infer program specifications in Separation Logic from
memory graphs~\cite{Sling};
it does not infer new heap
predicates and instead offers a number of pre-defined structure
shapes, where \tool can work as a complementary tool to help the user
to pre-define new predicates.

% %
% \tname{Hanoi}____ is another framework for data
% structure invariant synthesis.
% %
% Unlike \tool, which targets inference of shape and data descriptions
% in Separation Logic, \tname{Hanoi}, given a data structure
% implementation as self-contained module, infers properties satisfied
% by different \emph{compositions} of the functions defined within this module.

% We believe that other decidable logics for describing heap-based
% structures could be also used as a target language for our synthesis
% mechanism.
% %
% In the future, we are planning to generalise our inference mechanism
% to other formalisms, such as
% \tname{DRYAD}$\!_{\mathit{dec}}$~\cite{DryadDec},
a decidable
% logic for tree-liked structures, which provides categorised functions,
% such as set functions, measure functions, \etc.
%
% This will require us to extend \popper with a feature of categorising
% predicates.


% \todo{maybe ILP section, especially Mayur's works}


\paragraph{Synthesising Declarative Representations}

The approach of \tool extends two lines of work on synthesis of
declarative representations programs and data.
%
The first one is inductive logic programming (ILP), which aims to
learn logic programs from examples.
\tname{Progol}~\cite{Muggleton1994Prolog}
is an early notable ILP
system that achieves positive-only learning by Bayesian framework,
which is not sound in general. Importantly, \tname{Progol} does not
support learning recursive logic programs.
%
\tname{AMIE}~\cite{Savelli2016Amie} 
is another knowledge rule-mining
framework with positive-only examples, but the learning is in
\tname{AMIE} mainly targeted knowledge base graphs, so the learned
rules were not as complex as in ILP settings.
%
Other than those conventional ILP methods that synthesise \prolog
programs, significant progress has been made recently on synthesising
\tname{Datalog}~\cite{DeNies1981Datalog} 
and ASP
programs~\cite{Calimeri2012Asp}
from
examples.
%
While the syntax of Separation Logic assertions and predicates can be
expressed in the \tname{Datalog} or ASP domain, the approaches
developed in the past efforts are not immediately applicable, as they:
(1)~may require negative examples (in the case of \tname{Datalog}
synthesisers), and (2)~limit the pre-defined predicates expressed only
by grounded facts.
%
In particular, the latter means that the predicates used in the
synthesis cannot express arbitrary operations, because grounded facts
are fully instantiated and do not contain variables. 
%
This limitation makes impossible the representation of general rules
or operations that can be applied to a range of inputs. For example, a
grounded fact can state that a specific element belongs to a set, but
it cannot express a general rule for membership that applies to
\emph{any} element. 
%
At the same time, in \tool, the predicates are written in \prolog, a
Turing-complete language, which enables definitions of operations like
list append, set union, \etc,---a feature our approach has directly
inherited from \popper~\cite{Popper}.

The second relevant line of work is \emph{specification synthesis},
which aims at synthesising formulas within various but pre-defined
domains. 
%
Our graph generator (\autoref{sec:generator}) follows the ideas of
\precis~\cite{Precis},
which similarly
uses test cases as a learning oracle to generate positive example
for synthesising program contracts. 
%
At the same time, the formal guarantees provided by positive only
learning--generating all non-comparable and most specific predicates,
are similar to those of \tname{Spyro}~\cite{Spyro},
which defines a general framework for synthesising specifications for
customisable domains. The main difference between \tool and those
works is that \tool synthesises \emph{predicates} that can contain
\emph{recursive definitions}---an aspect cannot be handled by the
existing specification synthesisers.

The overlap between two lines of work above is the notion of
\emph{least general generalisation}~(LGG)~\cite{Plotkin1971Lgg}.
The
example-based specification synthesisers can be considered as learning
the LGG of the examples, which is exactly what early bottom-up ILP
systems do. The limitation of existing LGG operations is well-known in
ILP~\cite{DeRaedt2000Ilg}:
there is no LGG operations for recursive logic
programs, which is the reason why modern ILP systems are defined in a
top-down fashion.

\paragraph{Answer Set Programming v. Satisfiability Modulo Theories}

ASP plays a crucial role in our work, similar to that of SMT solvers
most contemporary synthesis tools. 
%
As mentioned by Gebser et al., "Mathematical Problem Solving with Answer Set Programming" (2019) ,
ASP is effective at search tasks involving fixpoints: pruning in \tool
can be encoded easily with recursive logic predicates, some of which
though can be expressed in SMT, need to be encoded in a more complex
and hard-to-understand way.
%
Another advantage of ASP is its efficiency when enumerating \emph{all}
solutions in a search space, which is crucial for exhaustively exploring
the space of SL predicates. 
%
In contrast, in most state-of-the-art SMT solvers, only one model is
returned at a time, so for obtaining a complete set of models, the
user must either block an obtained model and re-run the solver to get
the next one with high overhead, or use expert-level
techniques~\cite{Biere2013Smt}.
%
On the other hand, SMT solvers usually come with a rich set of
theories, whereas ASP modulo theories is still limited to basic
theories like difference logic~\cite{Datalog}
and acyclicity constraints~\cite{acyclicity}.