\section{Limitation and Future Work}
\label{sec:limit}

We have discussed some limitations of our approach in
\autoref{sec:fail} in the inductive predicate synthesis. Here we
discuss some other high-level limitations on the positive-only
learning and future works.

\paragraph{More on positive-only learning}


% Though we have mentioned that the positive-only learning is also applicable to other domains, we only discuss the issue of negative examples in  Separation Logic.
% Here we show different scenarios where (considerate) negative examples are not easy to obtain out of the context of Separation Logic.
% \begin{itemize}
%   \item \emph{Scenario 1:} To know what are the features on friendship among a group of people, we can ask them to fill a questionnaire about who are their friends. However, a questionnaire about who are \emph{not} their friends will not be an easy one to answer.
%   \item \emph{Scenario 2:} To learn the property of isosceles triangles, negative examples are not hard to provide; but if all negative examples provided are scalene, the learned predicates can be the property of equilateral triangles, which is not wrong but not specific enough.
% \end{itemize}

% It is both intuitive and supported by data that negation is not as
% welcomed as positive one. In daily knowledge representation,
% statements like "a tiger is an animal" are more common than "water is
% not an animal"; \citet{hossain2022analysis} conclude that negation is
% even less common in natural language understanding corpora than
% general-purpose English (22.6\%-29.9\%). Therefore, we believe that
% the positive-only learning can be a more practical setting for many
% applications.

Some basic assumptions of our positive-only learning restrict the
extension to more general settings. For example, noisy tolerance is
something not applicable to our current approach, because we need to
guarantee that the learned predicates are valid for all provided
examples. It is to be explored on whether the specificity can be
defined in other settings like probabilistic
ILP~\cite{de2008probabilistic} or differentiable
ILP~\cite{evans2018learning}.


\paragraph{Domain refinement and theory-based learning}

Since the learning domain of \popper is the definite clause, though we
have already encoded large number of SL-specific constraints in the ASP
encoding, either the hypothesis generation or the hypothesis testing
can be further refined. For example, the pure and spatial part of SL
predicates can be learned incrementally as the multilist example in
\autoref{sec:evaluation}. However, the encoding makes it hard to do
the incremental efficient.

Another direction is to use the theory-based learning approach. See in
most existing synthesisers, SMT solver is used to check the validity
of the candidate predicates. In our current synthesis, the search
space is traversed without awareness of the memory graphs' concrete
data, which is inherited from \popper. The good point of data
unawareness is to make less assumption on the input memory graphs: in
a real memory heap, some pointers can point to certain location which
is part of a data structure, but the pointer itself are not
necessarily to be part of the data structure though with reachability.
If we can have the assumption that the pointers with reachability is
of one data structure, we can use data-aware search to accelerate the
search.
