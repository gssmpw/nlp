\section{Related Work}
\paragraph{Efficient inference for LLMs.}
As model sizes grow, serving LLMs efficiently becomes increasingly challenging. Previous work has tackled this via model pruning~\citep{men2024shortgpt,sreenivas2024llm, xiasheared, hou2025instruction}, quantization~\citep{lin2024awq,van2024gptvq,frantar2022gptq,xiao2023smoothquant,yao2022zeroquant,dettmers2022gpt3}, and optimized decoding algorithms such as non-autoregressive decoding~\citep{santilli2023accelerating}, speculative decoding~\citep{kim2024speculative,elhoushi2024layer,miao2023specinfer}, or early-exiting~\citep{he2021magic,kong2022accelerating}. Like these approaches, our method also aims to enhance LLM inference efficiency by reducing inference-time computation.  

\paragraph{KV cache reuse.}
Recent work has explored reusing precomputed KV caches across multiple queries, often focusing on prefix-only reuse~\citep{jin2024ragcache,liu2024optimizing,zheng2023efficiently}. A few methods extend reuse to arbitrary positions, aiming for greater efficiency. {\promptcache}\citep{gim2024prompt} allows KV cache reuse with discontinuous position IDs but ignores cross-chunk attention, causing performance drops. It also relies on Prompt Markup Language (PML), limiting flexibility across tasks. {\cacheblend} reduces positional mismatches by selectively recomputing caches for tokens with the largest value-state variance; however, it suffers from performance degradation and the non-trivial cost of recomputing 10\%--20\% of tokens (plus all tokens at the first layer). {\blockattn}\citep{sun2024block} removes positional embeddings by re-encoding KV caches and directly fine-tunes on block-attention data, but lacks strategies to better connect reused contexts or refine data processing, leaving performance below ideal levels.

\paragraph{KV cache quantization and eviction.}
While our method focuses on accelerating inference by reusing KV caches, a complementary line of work aims to reduce memory overhead through cache eviction and quantization. For instance, ~\citep{zhang2024h2o,oren2024transformers,xiao2023efficient,ge2023model} introduces an eviction policy that significantly lowers the memory footprint, while ~\citep{liu2024kivi,xiao2023smoothquant,lin2024awq,frantar2022gptq,kim2023squeezellm,zhao2024atom,sheng2023flexgen} investigate quantizing KV caches to minimize storage requirements with only minor performance loss. Notably, our approach can be seamlessly integrated with these state-of-the-art eviction and quantization strategies, thereby addressing both speed and memory efficiency.