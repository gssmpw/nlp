[
  {
    "index": 0,
    "papers": [
      {
        "key": "men2024shortgpt",
        "author": "Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng",
        "title": "Shortgpt: Layers in large language models are more redundant than you expect"
      },
      {
        "key": "sreenivas2024llm",
        "author": "Sreenivas, Sharath Turuvekere and Muralidharan, Saurav and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo",
        "title": "Llm pruning and distillation in practice: The minitron approach"
      },
      {
        "key": "xiasheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"
      },
      {
        "key": "hou2025instruction",
        "author": "Hou, Bairu and Chen, Qibin and Wang, Jianyu and Yin, Guoli and Wang, Chong and Du, Nan and Pang, Ruoming and Chang, Shiyu and Lei, Tao",
        "title": "Instruction-Following Pruning for Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      },
      {
        "key": "van2024gptvq",
        "author": "van Baalen, Mart and Kuzmin, Andrey and Nagel, Markus and Couperus, Peter and Bastoul, Cedric and Mahurin, Eric and Blankevoort, Tijmen and Whatmough, Paul",
        "title": "Gptvq: The blessing of dimensionality for llm quantization"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      },
      {
        "key": "yao2022zeroquant",
        "author": "Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong",
        "title": "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers"
      },
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "santilli2023accelerating",
        "author": "Santilli, Andrea and Severino, Silvio and Postolache, Emilian and Maiorca, Valentino and Mancusi, Michele and Marin, Riccardo and Rodol{\\`a}, Emanuele",
        "title": "Accelerating transformer inference for translation via parallel decoding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "kim2024speculative",
        "author": "Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt",
        "title": "Speculative decoding with big little decoder"
      },
      {
        "key": "elhoushi2024layer",
        "author": "Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others",
        "title": "Layer skip: Enabling early exit inference and self-speculative decoding"
      },
      {
        "key": "miao2023specinfer",
        "author": "Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others",
        "title": "SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "he2021magic",
        "author": "He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul",
        "title": "Magic pyramid: Accelerating inference with early exiting and token pruning"
      },
      {
        "key": "kong2022accelerating",
        "author": "Kong, Jun and Wang, Jin and Yu, Liang-Chih and Zhang, Xuejie",
        "title": "Accelerating inference for pretrained language models by unified multi-perspective early exiting"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "jin2024ragcache",
        "author": "Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin",
        "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation"
      },
      {
        "key": "liu2024optimizing",
        "author": "Liu, Shu and Biswal, Asim and Cheng, Audrey and Mo, Xiangxi and Cao, Shiyi and Gonzalez, Joseph E and Stoica, Ion and Zaharia, Matei",
        "title": "Optimizing llm queries in relational workloads"
      },
      {
        "key": "zheng2023efficiently",
        "author": "Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody\\_Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others",
        "title": "Efficiently Programming Large Language Models using SGLang."
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gim2024prompt",
        "author": "Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin",
        "title": "Prompt cache: Modular attention reuse for low-latency inference"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sun2024block",
        "author": "Sun, East and Wang, Yan and Tian, Lan",
        "title": "Block-Attention for Efficient RAG"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      },
      {
        "key": "oren2024transformers",
        "author": "Oren, Matanel and Hassid, Michael and Yarden, Nir and Adi, Yossi and Schwartz, Roy",
        "title": "Transformers are multi-state rnns"
      },
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      },
      {
        "key": "ge2023model",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024kivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "Kivi: A tuning-free asymmetric 2bit quantization for kv cache"
      },
      {
        "key": "xiao2023smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "Smoothquant: Accurate and efficient post-training quantization for large language models"
      },
      {
        "key": "lin2024awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"
      },
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      },
      {
        "key": "kim2023squeezellm",
        "author": "Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt",
        "title": "Squeezellm: Dense-and-sparse quantization"
      },
      {
        "key": "zhao2024atom",
        "author": "Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris",
        "title": "Atom: Low-bit quantization for efficient and accurate llm serving"
      },
      {
        "key": "sheng2023flexgen",
        "author": "Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\\'e}, Christopher and Stoica, Ion and Zhang, Ce",
        "title": "Flexgen: High-throughput generative inference of large language models with a single gpu"
      }
    ]
  }
]