@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{elhoushi2024layer,
  title={Layer skip: Enabling early exit inference and self-speculative decoding},
  author={Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and others},
  journal={arXiv preprint arXiv:2404.16710},
  year={2024}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{ge2023model,
  title={Model tells you what to discard: Adaptive kv cache compression for llms},
  author={Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.01801},
  year={2023}
}

@article{gim2024prompt,
  title={Prompt cache: Modular attention reuse for low-latency inference},
  author={Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={325--338},
  year={2024}
}

@article{he2021magic,
  title={Magic pyramid: Accelerating inference with early exiting and token pruning},
  author={He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul},
  journal={arXiv preprint arXiv:2111.00230},
  year={2021}
}

@article{hou2025instruction,
  title={Instruction-Following Pruning for Large Language Models},
  author={Hou, Bairu and Chen, Qibin and Wang, Jianyu and Yin, Guoli and Wang, Chong and Du, Nan and Pang, Ruoming and Chang, Shiyu and Lei, Tao},
  journal={arXiv preprint arXiv:2501.02086},
  year={2025}
}

@article{jin2024ragcache,
  title={RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.12457},
  year={2024}
}

@article{kim2023squeezellm,
  title={Squeezellm: Dense-and-sparse quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}

@article{kim2024speculative,
  title={Speculative decoding with big little decoder},
  author={Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kong2022accelerating,
  title={Accelerating inference for pretrained language models by unified multi-perspective early exiting},
  author={Kong, Jun and Wang, Jin and Yu, Liang-Chih and Zhang, Xuejie},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={4677--4686},
  year={2022}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}

@article{liu2024optimizing,
  title={Optimizing llm queries in relational workloads},
  author={Liu, Shu and Biswal, Asim and Cheng, Audrey and Mo, Xiangxi and Cao, Shiyi and Gonzalez, Joseph E and Stoica, Ion and Zaharia, Matei},
  journal={arXiv preprint arXiv:2403.05821},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{miao2023specinfer,
  title={SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  journal={arXiv preprint arXiv:2305.09781},
  year={2023}
}

@article{oren2024transformers,
  title={Transformers are multi-state rnns},
  author={Oren, Matanel and Hassid, Michael and Yarden, Nir and Adi, Yossi and Schwartz, Roy},
  journal={arXiv preprint arXiv:2401.06104},
  year={2024}
}

@article{santilli2023accelerating,
  title={Accelerating transformer inference for translation via parallel decoding},
  author={Santilli, Andrea and Severino, Silvio and Postolache, Emilian and Maiorca, Valentino and Mancusi, Michele and Marin, Riccardo and Rodol{\`a}, Emanuele},
  journal={arXiv preprint arXiv:2305.10427},
  year={2023}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{sreenivas2024llm,
  title={Llm pruning and distillation in practice: The minitron approach},
  author={Sreenivas, Sharath Turuvekere and Muralidharan, Saurav and Joshi, Raviraj and Chochowski, Marcin and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan and Kautz, Jan and Molchanov, Pavlo},
  journal={arXiv preprint arXiv:2408.11796},
  year={2024}
}

@article{sun2024block,
  title={Block-Attention for Efficient RAG},
  author={Sun, East and Wang, Yan and Tian, Lan},
  journal={arXiv preprint arXiv:2409.15355},
  year={2024}
}

@article{van2024gptvq,
  title={Gptvq: The blessing of dimensionality for llm quantization},
  author={van Baalen, Mart and Kuzmin, Andrey and Nagel, Markus and Couperus, Peter and Bastoul, Cedric and Mahurin, Eric and Blankevoort, Tijmen and Whatmough, Paul},
  journal={arXiv preprint arXiv:2402.15319},
  year={2024}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@inproceedings{xiasheared,
  title={Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{yao2022zeroquant,
  title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27168--27183},
  year={2022}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={196--209},
  year={2024}
}

@article{zheng2023efficiently,
  title={Efficiently Programming Large Language Models using SGLang.},
  author={Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody\_Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
  year={2023},
  publisher={arXiv}
}

