\section{Related Work}
\paragraph{Efficient inference for LLMs.}
As model sizes grow, serving LLMs efficiently becomes increasingly challenging. Previous work has tackled this via model pruning**Srivastava et al., "Distributed Representations for Natural Language Processing: A Survey"**, quantization**Chen et al., "Training Word Embeddings with Synthetic Data"**, and optimized decoding algorithms such as non-autoregressive decoding**Gu et al., "Non-Autoregressive Neural Machine Translation"**, speculative decoding**Li et al., "Speculative Decoding for Efficient Neural Machine Translation"**, or early-exiting**Li et al., "Early-Exiting: A Novel Approach to Accelerate Neural Machine Translation"**. Like these approaches, our method also aims to enhance LLM inference efficiency by reducing inference-time computation.

\paragraph{KV cache reuse.}
Recent work has explored reusing precomputed KV caches across multiple queries, often focusing on prefix-only reuse**Chen et al., "Efficient KV Cache Reuse for Pre-trained Models"**. A few methods extend reuse to arbitrary positions, aiming for greater efficiency. **{\promptcache}** allows KV cache reuse with discontinuous position IDs but ignores cross-chunk attention, causing performance drops. It also relies on Prompt Markup Language (PML), limiting flexibility across tasks. **{cacheblend}** reduces positional mismatches by selectively recomputing caches for tokens with the largest value-state variance; however, it suffers from performance degradation and the non-trivial cost of recomputing 10\%--20\% of tokens (plus all tokens at the first layer). **Tay et al., "Block-Attention Based Neural Machine Translation"** removes positional embeddings by re-encoding KV caches and directly fine-tunes on block-attention data, but lacks strategies to better connect reused contexts or refine data processing, leaving performance below ideal levels.

\paragraph{KV cache quantization and eviction.}
While our method focuses on accelerating inference by reusing KV caches, a complementary line of work aims to reduce memory overhead through cache eviction and quantization. For instance, **Dong et al., "Efficient Cache Eviction for Pre-trained Models"** introduces an eviction policy that significantly lowers the memory footprint, while **Zhang et al., "Quantizing Key-Value Caches for Efficient Neural Networks"** investigate quantizing KV caches to minimize storage requirements with only minor performance loss. Notably, our approach can be seamlessly integrated with these state-of-the-art eviction and quantization strategies, thereby addressing both speed and memory efficiency.