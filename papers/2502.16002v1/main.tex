% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{pifont} 
\usepackage{bm}
\usepackage{enumitem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{booktabs}       % professional-quality tables

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url}            % simple URL typesetting


% \newcommand{\alg}{\textsc{MemorySum}}
\newcommand{\alg}{\textsc{KVLink}}
\newcommand{\promptcache}{\textsc{PromptCache}}
\newcommand{\cacheblend}{\textsc{CacheBlend}}
\newcommand{\blockattn}{\textsc{BlockAttention}}


\usepackage{color}
\usepackage[textsize=scriptsize]{todonotes}
\newcommand{\shiyu}[1]{\todo[color=blue!40]{Shiyu: #1}}
\newcommand{\yujia}[1]{\todo[color=orange!40]{Yujia: #1}}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{{\alg}: Accelerating Large Language Models via \\ Efficient KV Cache Reuse}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{
  \textbf{Jingbo Yang}\textsuperscript{1}\thanks{Equal contribution. Correspondence to: Jingbo Yang $<$jingbo@ucsb.edu$>$ and Bairu Hou $<$bairu@ucsb.edu$>$ }\quad
  \textbf{Bairu Hou}\textsuperscript{1}\samethanks\quad
  \textbf{Wei Wei}\textsuperscript{2}\quad
  \textbf{Yujia Bao}\textsuperscript{2}\quad
% \\
  \textbf{Shiyu Chang}\textsuperscript{1}
\\
\\
  \textsuperscript{1}UC Santa Barbara\quad\quad
  \textsuperscript{2}Center for Advanced AI, Accenture
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
We describe {\alg}, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we propose a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation of LLMs when using KV caches computed independently for each document, {\alg} introduces three key components: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, using trainable special tokens to restore self-attention across independently encoded documents, and applying mixed-data fine-tuning to enhance performance while preserving the model’s original capabilities. Experiments across 7 datasets demonstrate that {\alg} improves question answering accuracy by an average of 4\% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 90\% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Code is available at \url{https://github.com/UCSB-NLP-Chang/KVLink}.
\end{abstract}

\section{Introduction}

Large language models have demonstrated impressive capabilities across a broad array of applications—many of which involve processing contexts naturally divided into multiple segments. For example, in retrieval-augmented generation (RAG)~\citep{gao2023retrieval, wang2024searching, li2024retrieval}, each retrieved document forms a distinct context chunk, while in multi-agent conversation scenarios~\citep{wu2023autogen, liu2024dynamic}, outputs from different agents serve as separate segments. However, conventional autoregressive architectures require LLMs to encode the entire concatenated context as a single unit before generating a response. This approach incurs high prefilling costs for long contexts and prevents the model from separately encoding and reusing precomputed representations (\emph{i.e.}, key-value states) for each segment.  Consider RAG: for every query, the LLM encodes a large collection of retrieved documents. When different queries share common documents, the model redundantly re-encodes these identical texts, even though their content remains unchanged.

This inefficiency motivates us to explore an alternative strategy. Instead of re-encoding the entire concatenated context for every query, we propose precomputing the key-value (KV) states for each document or text segment independently, then reusing these precomputed states during inference. By encoding each segment (\emph{e.g.}, each retrieved document) separately and concatenating their KV states as needed, we can eliminate redundant computations and significantly improve efficiency.

However, naively encoding each document independently and concatenating their KV caches during inference can lead to performance degradation due to train-test discrepancies. Prior work \citep{yao2024cacheblend, sun2024block, zhang2024attention} has reported up to a 35\% relative decrease in accuracy on QA tasks.
To overcome this challenge, we introduce {\alg}, an approach designed to bridge the gap between separately encoded segments and restore self-attention across documents. {\alg} introduces three key enhancements:
\ding{182} KV cache positional re-encoding: we adjust positional embeddings during inference to ensure that the stored KV caches align correctly with the positions required for a given query.
\ding{183} Trainable cross-segment special tokens: To effectively ``link'' independently encoded segments, {\alg} appends a small set of trainable tokens between each segment’s precomputed KV states before concatenation.
The KV representations for these tokens are computed during inference.
While the documents are independently encoded into KV cache, the link tokens can attend to all the preceding tokens, which helps restore self-attention across segments while introducing only minimal computational overhead.
\ding{184} Fine-tuning with a diverse data mixture: we fine-tune both the base LLM parameters and the newly introduced cross-segment special tokens using a diverse dataset spanning retrieval-augmented QA, summarization, and other instruction-following tasks.
This process equips the model with the capability to reason across independently encoded documents while preserving its original capabilities.

We validate the effectiveness and efficiency of our method through comprehensive experiments across diverse question-answering and text summarization datasets. We evaluate two backbone LLMs including \texttt{Llama-3.2-1B} and \texttt{Llama-3.2-3B}, showing that our approach consistently outperforms state-of-the-art baselines~\citep{sun2024block, gim2024prompt, yao2024cacheblend}. For example, {\alg} surpasses the best baseline by 6.6\% on Natural Question~\citep{kwiatkowski2019natural} and 7.3\% on HotpotQA~\citep{yang2018hotpotqa}. Even compared to the upper-bound model (\emph{i.e.}, LLM using conventional full encoding for inference), our method sacrifices only minimal performance while reducing the time-to-first-token latency by up to 90\% by efficiently reusing precomputed KV caches.



\section{Methodology}

\subsection{Problem Formulation}
We aim to address the problem of reusing the key–value (KV) cache in LLMs without having to repeatedly encode the same context segments. In many applications, the same documents or context segments appear across different inference queries, yet current LLM pipelines redundantly re-encode these segments every time.

Consider the problem of retrieval augmented generation as an example, where the input question is denoted as $\bm q$ and we retrieve $N$ documents for each question.
These documents are concatenated with the query, forming the complete LLM input.

\begin{figure}[t]
    \centering
    \includegraphics[width=.95\linewidth]{figs/method_example.pdf}
    \caption{KV cache encoding in standard and the proposed settings. In the standard approach (top), the KV cache of each document is conditioned on preceding tokens, resulting in redundant and nonreusable KV cache encodings for shared documents (\emph{e.g.}, $\mathrm{Doc_{b}}$). In contrast, our method (bottom) encodes documents separately, allowing KV cache reuse across queries.}
    \label{fig:method_example}
\end{figure}

In the standard LLM inference pipeline, the entire input is passed into the LLM as a single contiguous sequence. As shown in the top portion of Figure~\ref{fig:method_example}, the resulting KV cache for each document is entangled with its preceding documents. Consequently, even if $\mathrm{Doc_{b}}$ has already been encoded into KV cache when processing $\bm q_1$, the resulting KV cache cannot be directly reused when processing $\bm q_2$ because it was conditioned on the preceding documents in $\bm q_1$.

To address this, we consider a scenario where the KV cache for every document in the knowledge base is pre-computed in a \textit{context-free} manner, as illustrated in the bottom of Figure~\ref{fig:method_example}.
Specifically, for each document in the knowledge base, we feed only that document’s tokens into the LLM and record the resulting KV cache.
At inference time, after we retrieve a set of documents for a given query, we concatenate the corresponding pre-computed KV caches. This design allows us to reuse the same cache for overlapping documents across different queries, thereby eliminating redundant computations.

However, the above approach often yields noticeable performance degradation, as LLMs are typically trained on fully concatenated sequences and each token attends to the preceding context. When we instead encode each document in a context-free manner, the model loses cross-document dependencies.
Previous work also empirically demonstrates up to a 35\% relative decrease in accuracy in question-answering tasks when each retrieved document is encoded into KV cache separately~\citep{sun2024block}.
Our method aims to enable the LLM to produce high-quality outputs when the KV cache of each document is pre-computed independently by introducing three key components, \ding{182} KV cache positional re-encoding.  \ding{183} cross-document reconnection with link tokens, and \ding{184} mixed data fine-tuning, which will be elaborated in the following sections.


\subsection{KV Cache Positional Re-encoding.}
\label{sec: postion_encoding}
The first issue with separately encoding documents is the position mismatch during inference.
Modern LLMs typically use Rotary Positional Encoding (RoPE)~\citep{su2024roformer}, where each token is assigned a distinct positional embedding based on its position in the full sequence. However, when documents are encoded independently, tokens are indexed within their own document, ignoring their actual position in the full concatenated input.
Take Figure~\ref{fig:method_example} as an example. Since we pre-compute the KV cache for each document separately, the second token in $\mathrm{Doc_{b}}$ is assigned position index $2$ when computing its KV cache. However, when we concatenate $\mathrm{Doc_{b}}$ with $\mathrm{Doc_{a}}$ during inference (e.g., when processing query $\bm q_1$), the actual position of that token should be $|\texttt{$\mathrm{Doc_{a}}$}| + 2$ where $|\texttt{$\mathrm{Doc_{a}}$}|$ is the length of the first document. Since the KV cache of \texttt{$\mathrm{Doc_{b}}$} was precomputed without awareness of this global position shift, the LLM applies incorrect positional embeddings, leading to erroneous attention computations.

To address this, we decouple the key-value states from the positional embeddings when storing them.
We still apply rotary position encoding to tokens when encoding each segment for local self-attention. However, we exclude those positional transformations before saving the segment KV caches.

More specifically, we denote the hidden state of a token at position $i$ at a particular transformer layer as $\bm x_i \in \mathbb{R}^{d}$, where $d$ is the hidden dimension. The key vector is computed as $R_{i}W_{k}\bm x_i$ and the value vector is computed as $W_{v}\bm x_i$. Here $W_{\{k,v\}}\in \mathbb{R}^{d\times d}$ represents the weight matrices that project the hidden states into the key and value spaces, and $R_{i} \in [-1, 1]^{d\times d}$ is the position-dependent rotation matrix used in RoPE. Because RoPE directly encodes positional information via these rotations, the KV cache can be stored without positional embeddings, \emph{i.e.}, $W_{\{k,v\}}\bm x_i$.
At inference time, the key-value states of all documents are concatenated, and we apply the global rotary embedding for the KV states of each token appropriate to its location in the full sequence.
Note that this operation only introduces negligible time, ensuring the efficiency of our method. This is also consistent with previous methods~\citep{yao2024cacheblend, zhang2024attention}, which adjust the position encoding of separately encoded KV cache during inference. 


\begin{figure}[t]
    \centering
    \includegraphics[width=.9\columnwidth]{figs/attention_train.pdf}
    \vspace{-1mm}
    \caption{The attention map for all tokens. The \texttt{link1} token attends only to the tokens in $\mathrm{Doc_{a}}$; \texttt{link2} attends to the tokens in $\mathrm{Doc_{a}}$ and $\mathrm{Doc_{b}}$ and \texttt{link2}; and \texttt{link3} attends to the tokens in all reused contexts and other link tokens.}
    \label{fig:attention_train}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=.9\columnwidth]{figs/attention_inference.pdf}
    \vspace{-3mm}
    \caption{The attention map for three link tokens and the first user input token during inference. This attention map is identical to Figure~\ref{fig:attention_train}.}
    \label{fig:attention_inference}
\end{figure}

\subsection{Cross-Document Reconnection with Link Tokens}
When documents are encoded and cached independently, tokens in later documents can no longer attend to those in earlier ones. This creates a gap between our inference scheme and standard LLM training and can potentially limit the model performance.
To mitigate this issue, we design a novel mechanism to restore the self-attention connection across segments while preserving the computational efficiency of KV cache reuse.
Specifically, {\alg} introduces a set of trainable ``link tokens''. For every document $c$ with length $L$, we append $K$ (\emph{e.g.}, $K=5$) link tokens:
\begin{equation*}
    \bm{c} = \bigl( c_{1}, \dots, c_{L},  c_{\mathrm{link1}}, \dots, c_{\mathrm{link5}}\bigr),
\end{equation*}
where $c_{1}, \dots, c_{L}$ are the tokens within the original document.
A customized attention map ensures that the link tokens of each document can attend to (i) all tokens (including the link tokens) in the preceding documents and (ii) tokens within the current document.
These tokens serve as an interface between segments, allowing the model to reconstruct dependencies that would otherwise be lost due to independent encoding.

Figure~\ref{fig:attention_train} illustrates this mechanism, where one link token per document ($K=1$) is append (token 3, 6, and 8). The tokens within a document can only maintain local causal attention, meaning they can only attend to earlier tokens within the same document. Each link token can attend to all preceding tokens. Therefore, different documents are implicitly connected through these link tokens during inference. For example, token 6 (the link token of document 2) attends to the first two documents and token 8 (the link token of document 3) attends to token 6, thus mixing the information across all retrieved documents. After the retrieved documents, the question and user tokens follow standard causal self-attention. During training, we fine-tune the LLM with this attention mechanism, jointly optimizing both the model and the newly introduced link tokens.

Figure~\ref{fig:attention_inference} illustrates our inference strategy, which avoids recomputing KV caches for retrieved documents. In practice, we first load and concatenate the precomputed KV caches for all retrieved documents. Since these caches were precomputed, they do not need to be re-encoded. We then append only the newly introduced link tokens (tokens 3, 6, and 8, corresponding to the three documents) to the end of the reordered sequence. A forward pass is performed on these tokens using the customized attention map, ensuring that their attention behavior matches the training phase.

\subsection{Mixed Data Fine-tuning}
\label{sec: data_for_training}
Current LLMs are not inherently designed to handle separately encoded segments or leverage link tokens. To address this gap, we propose fine-tuning the LLM with a mixed dataset drawn from different sources, enabling the model to integrate disjoint context segments while preserving its original capabilities.

\paragraph{Retrieval-augmented QA.} Since RAG is the most directly relevant task, we first consider fine-tuning the model on QA tasks with retrieved documents that are separately encoded to ensure it can produce high-quality responses under this setting. More specifically, we sample questions from \texttt{TriviaQA}~\citep{joshi2017triviaqa} and \texttt{2WikiMQA}~\citep{ho2020constructing}. For each question, we retrieve ten relevant Wikipedia passages using Contriever~\citep{izacard2021unsupervised}, each encoded separately. To obtain high-quality supervision, we prompt GPT-4 to generate reference answers, which serve as the ground truth for training.

\paragraph{Multi-turn conversation.}
Many existing instruction-following datasets~\citep{wang2024helpsteer2} contain multi-turn conversations between the user and the assistant, which provides a natural way to train the model on disjointed contexts. We randomly convert earlier conversation turns into independently encoded KV caches, and the LLM is then trained to produce the appropriate responses in subsequent turns. This ensures the model to learn to integrate segmented contexts while maintaining its instruction-following capability.

\paragraph{Summarization.} Summarization serves as another useful setting where the model must aggregate information from independently encoded document chunks to generate a coherent summary. We adopt the \texttt{XSum}~\citep{narayan2018don} dataset. Each document is randomly split into multiple consecutive segments, and each segment is independently encoded into KV cache. The model is trained to generate the ground-truth summary, ensuring it learns to integrate information across separate segments.

Additionally, to better preserve the model’s original capabilities, we also include a standard version of \texttt{TriviaQA} and \texttt{2WikiMQA}, where the retrieved documents are encoded as a whole rather than separately. In this setting, the model is trained to generate the ground-truth answer conditioned on the fully concatenated context. Lastly, we incorporate the \texttt{\textsc{T"ulu} 3} dataset~\citep{lambert2024t} to preserve the instruction-following ability and a small portion of pre-training data from \texttt{Fineweb}~\citep{penedo2024the} to preserve the language modeling capability. More details about the dataset mixture can be found in Appendix~\ref{app: data_mixture}.

\section{Experiments}

\subsection{Experiment Setup}

\paragraph{Comparison baselines.}
We evaluate our method against three existing approaches:
\ding{182} {\promptcache}~\citep{gim2024prompt}, which directly reuses the precomputed KV cache of each retrieved document or context segment for model inference. Since the original {\promptcache} does not handle positional encoding mismatches, we enhance it by applying our positional re-encoding method (Section\ref{sec: postion_encoding}) to ensure the stored KV cache aligns correctly with the concatenated input.
\ding{183} {\cacheblend}~\citep{yao2024cacheblend}, which concatenates the precomputed KV caches of retrieved documents and then recomputes the KV cache for a small number of selected tokens within the retrieved documents. Following the original implementation, we set the recomputation ratio to 18\%.
\ding{184} {\blockattn}~\citep{sun2024block}, which explicitly trains the model to handle separately encoded KV caches by fine-tuning on QA tasks where retrieved documents are encoded independently.
For all baselines and our method, we use Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct~\citep{meta2024llama} as the backbone models. Since BlockAttention and our method require fine-tuning, we ensure fair comparison by using the same data mixture and training hyperparameters for both. Further implementation details for each baseline are provided in Appendix~\ref{app: implementation_detail_baseline}.



\paragraph{Implementation.}
We adopt Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct as the backbone models, fine-tuning them for 6,000 steps using a global batch size of 64 across 8 A100 GPUs.  Further details on data preprocessing, dataset mixture, and training configurations are provided in Appendix~\ref{app: implementation_detail}. For our method, we train three versions, each appending 0, 1, or 5 link tokens to every document or context segment, denoted as {\alg}0, {\alg}1, and {\alg}5, respectively.

\paragraph{Evaluation configurations.}
We evaluate the effectiveness of our method in three dimensions: \ding{182} performance with separately encoded KV cache,
% the performance when using separately encoded documents or context segments,
\ding{183} the inference efficiency, and \ding{184} the general capability preservation (\emph{e.g.}, math reasoning and instruction-following ability).

\input{tables/main_exp}


To evaluate the model performance when using separately encoded documents or context segments, we focus on retrieval-augmented question answering tasks including \texttt{NaturalQuestions}~\cite{kwiatkowski2019natural}, \texttt{2WikiMQA}~\cite{ho2020constructing}, \texttt{TriviaQA}~\cite{joshi2017triviaqa}, \texttt{HotpotQA}~\cite{yang2018hotpotqa}, and \texttt{MuSiQue}~\cite{trivedi2022musique}. For \texttt{NaturalQuestions}, we adopt the evaluation protocol from~\citet{liu2024lost}, where the LLM is given a question along with a set of 10 retrieved documents. The document that contains the correct answer is systematically placed at each of the 10 possible positions across separate evaluation runs. The final accuracy is reported as the average over these 10 evaluations. For \texttt{2WikiMQA}, \texttt{HotpotQA}, and \texttt{MuSiQue}, we utilize the originally provided retrieved documents for evaluation. For \texttt{TriviaQA} we retrieve 10 documents using Contriever~\citep{izacard2021unsupervised} following the setting in {\blockattn}. In all cases, documents are encoded separately into KV cache.

Additionally, following {\cacheblend}, we evaluate on two text summarization datasets including MultiNews~\citep{fabbri2019multi} and Samsum~\citep{gliwa2019samsum}. The LLM is given several in-context examples per instance, each separately encoded into KV cache. We report the RougeL score~\citep{lin2004rouge} as the metric. Notably, our evaluation datasets cover all those used by our baselines, {\blockattn} and {\cacheblend}, ensuring a comprehensive and fair comparison.

To measure inference efficiency, we evaluate time-to-first-token (TTFT) under different document lengths. Specifically, we fix the number of retrieved documents at 10 and vary document lengths from 100 to 500 tokens, leading to total context lengths ranging from 1,000 to 5,000 tokens. Given pre-computed KV caches stored in CPU memory, we compare the TTFT of our method to standard LLM inference, which fully re-encodes all contexts for each query.

Finally, to ensure our method does not degrade the model’s original capabilities, we evaluate it on a range of reasoning and instruction-following benchmarks, including \texttt{IFEval}~\citep{zhou2023instruction}, \texttt{GSM8K}~\citep{cobbe2021training}, \texttt{MMLU}~\citep{hendrycks2020measuring}, \texttt{ARC-Challenge}~\citep{clark2018think}, \texttt{ARC-Easy}~\citep{clark2018think}, \texttt{PiQA}~\citep{bisk2020piqa}, \texttt{SciQ}~\citep{welbl2017crowdsourcing}, \texttt{Winogrande}~\citep{sakaguchi2021winogrande}, and \texttt{HellaSwag}~\citep{zellers2019hellaswag}. We report the accuracy on each dataset. For IFEval, we report both strict prompt-level accuracy (IFEval-P) and instruction-level accuracy (IFEval-I). More details of the evaluation configuration are available in Appendix~\ref{app: implementation_detail}.

\input{tables/normal}

\subsection{Main Results}
We first evaluate the effectiveness of our method when using separately encoded KV caches. For all QA tasks, each retrieved document is encoded into the KV cache independently. In summarization tasks, each in-context example is also encoded separately, following the same setup as our baselines. The evaluation results are presented in Table~\ref{tab: main_exp}.

To provide a more comprehensive understanding, we also include the performance of the original Llama models and their fine-tuned versions trained on the same data mixture as other methods. Importantly, these reference models are evaluated in the standard setting, where the retrieved documents or in-context examples are concatenated and encoded as a single contiguous sequence, rather than being separately encoded into KV cache. This serves as an upper bound for performance, helping contextualize our improvements.

We highlight the key observations below. First, {\alg} consistently outperforms all baselines across all datasets, demonstrating its strong effectiveness. In QA tasks, our approach surpasses the best baseline, {\blockattn}, with up to 6\% higher accuracy. For other baselines, the gap is more significant. The performance gap is even larger when compared to other baselines. Notably, our method not only outperforms the original Llama models without fine-tuning but also achieves accuracy close to fine-tuned Llama, which is evaluated with a fully concatenated context.

Second, the link tokens appended to each document effectively bridge separately encoded documents, restoring inter-document connections. Since one of the key differences between our method and baselines is the use of link tokens for cross-document reconnection, the observed performance improvements further validate the effectiveness of our design. With negligible computational cost, these link tokens significantly enhance performance. Compared to {\blockattn}, which also fine-tunes the model on QA tasks with separately encoded documents, our method demonstrates the necessity of link tokens. Without them, {\blockattn} performs worse than our approach despite using the same training data.

Finally, we observe a consistent performance gain as the number of link tokens increases, reinforcing their positive impact.

\subsection{Inference Efficiency Evaluation}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figs/efficiency.pdf}
  \caption{Inference speed comparison with ten reused contexts of varying lengths. Both {\alg}1 and {\alg}5 show considerably lower Time-to-First-Token (TTFT) than standard decoding as context size grows, demonstrating the efficiency gains from reusing precomputed KV caches.}
  \label{fig:efficiency}
\end{figure}

A key objective of {\alg} is to reduce the computational overhead incurred by repeatedly encoding long contexts. To demonstrate these efficiency gains, we measure the Time-to-First-Token (TTFT) when reusing the precomputed KV cache for ten documents of varying lengths. For a realistic assessment, we include the overhead of loading the precomputed KV caches onto the GPU. Each measurement is averaged over 100 runs, with an initial 10 warm-up trials to eliminate memory allocation overhead, following the setup in ~\citep{xiao2024duoattention}.

Figure~\ref{fig:efficiency} compares our methods, {\alg}1 and {\alg}5, against standard decoding, which encodes the entire context for each query. The key observations are as follows. First, reusing the precomputed KV cache significantly improves LLM inference efficiency. To leverage the precomputed KV cache, we perform three main operations: (1) loading KV caches from CPU to GPU, (2) applying new positional encoding, and (3) encoding the link tokens for each document. All of these operations introduce only negligible latency, reducing TTFT by 60\%–90\% compared to standard decoding.

Second, across all context sizes, both {\alg}1 and {\alg}5 consistently achieve substantially lower TTFT relative to standard decoding. As the reused context length increases, the TTFT gap further widens, highlighting the scalability of our approach. Notably, when the context length reaches 5,000 tokens, {\alg} decreases inference latency by 90\%. These results confirm that {\alg} effectively mitigates the computational cost of large-scale context encoding and is well-suited for scenarios requiring fast response times.


\subsection{General Capability Preservation}
While our primary objective is to enable efficient KV cache reuse, we also verify whether these modifications maintain the model’s general reasoning and instruction-following capabilities. Table~\ref{tab:normal_decoding} presents the results for two model variants: Llama-3.2-1B and Llama-3.2-3B. We compare the original models, their fine-tuned counterparts, and our method with different numbers of link tokens ({\alg}0, {\alg}1, and {\alg}5).

Our key findings are as follows. First, {\alg} maintains competitive performance across all tasks, with only marginal differences from the fine-tuned models. This confirms that {\alg} successfully preserves the model’s general capabilities. Across both model sizes, the results of {\alg} are highly comparable to the fine-tuned Llama, demonstrating that our method does not significantly degrade the model’s general capabilities despite restructuring context encoding.

Second, while there are minor drops in certain benchmarks (\emph{e.g.}, ARC-C and Winogrande) compared to the original models, these differences remain within a reasonable range, typically less than 3\%. We expect that performance could be further improved by refining the data mixture and incorporating additional supervised fine-tuning on reasoning and instruction-following tasks, which we leave for future work.

\subsection{Ablation Study}
We also perform ablation studies examining the robustness of our method against different data mixtures. Due to space constraints, the detailed results are provided in Appendix~\ref{app: ablation}. Our key observation is that our method can still achieve competitive performance given different data mixtures. Also, we find that specific combinations of tasks and domain coverage can slightly influence downstream performance, indicating the importance of balanced data selection. While our initial findings offer insight into more effective training mixtures, we make a comprehensive exploration of optimal data configurations for future work.

\section{Related Work}
\paragraph{Efficient inference for LLMs.}
As model sizes grow, serving LLMs efficiently becomes increasingly challenging. Previous work has tackled this via model pruning~\citep{men2024shortgpt,sreenivas2024llm, xiasheared, hou2025instruction}, quantization~\citep{lin2024awq,van2024gptvq,frantar2022gptq,xiao2023smoothquant,yao2022zeroquant,dettmers2022gpt3}, and optimized decoding algorithms such as non-autoregressive decoding~\citep{santilli2023accelerating}, speculative decoding~\citep{kim2024speculative,elhoushi2024layer,miao2023specinfer}, or early-exiting~\citep{he2021magic,kong2022accelerating}. Like these approaches, our method also aims to enhance LLM inference efficiency by reducing inference-time computation.  

\paragraph{KV cache reuse.}
Recent work has explored reusing precomputed KV caches across multiple queries, often focusing on prefix-only reuse~\citep{jin2024ragcache,liu2024optimizing,zheng2023efficiently}. A few methods extend reuse to arbitrary positions, aiming for greater efficiency. {\promptcache}\citep{gim2024prompt} allows KV cache reuse with discontinuous position IDs but ignores cross-chunk attention, causing performance drops. It also relies on Prompt Markup Language (PML), limiting flexibility across tasks. {\cacheblend} reduces positional mismatches by selectively recomputing caches for tokens with the largest value-state variance; however, it suffers from performance degradation and the non-trivial cost of recomputing 10\%--20\% of tokens (plus all tokens at the first layer). {\blockattn}\citep{sun2024block} removes positional embeddings by re-encoding KV caches and directly fine-tunes on block-attention data, but lacks strategies to better connect reused contexts or refine data processing, leaving performance below ideal levels.

\paragraph{KV cache quantization and eviction.}
While our method focuses on accelerating inference by reusing KV caches, a complementary line of work aims to reduce memory overhead through cache eviction and quantization. For instance, ~\citep{zhang2024h2o,oren2024transformers,xiao2023efficient,ge2023model} introduces an eviction policy that significantly lowers the memory footprint, while ~\citep{liu2024kivi,xiao2023smoothquant,lin2024awq,frantar2022gptq,kim2023squeezellm,zhao2024atom,sheng2023flexgen} investigate quantizing KV caches to minimize storage requirements with only minor performance loss. Notably, our approach can be seamlessly integrated with these state-of-the-art eviction and quantization strategies, thereby addressing both speed and memory efficiency.



\section{Conclusion}
In this paper, we introduced a method to improve LLM efficiency by reusing pre-computed KV caches. With precomputed KV caches for retrieved documents or context segments, our method can avoid redundant computation for overlapping contexts across different queries. In the future, we will refine our data mixture and investigate optimal fine-tuning strategies to further enhance performance. Additionally, we plan to explore real-world deployment scenarios to maximize the inference efficiency of LLMs.

\section{Limitations}
Although our approach achieves state-of-the-art performance while improving the inference efficiency, it still has several limitations. First, our method involves fine-tuning to adapt LLMs to the new paradigm of separately encoded documents. This could be costly for very large models. Second, our method leverages the pre-computed KV cache of retrieved documents or context segments for efficient inference. When the number of documents or context segments increases, there could be a significant overhead on CPU memory or disk storage. However, as the number of documents or context segments increases, it may introduce significant overhead in CPU memory or disk storage. Efficiently managing the storage and loading costs of the pre-computed KV cache remains a challenge for large-scale deployment.


\section{Societal Impact}
In this paper, our primary goal is to improve LLM efficiency by reusing KV caches. Our method is designed to improve both inference efficiency and also maintain the model performance on downstream tasks. The training data used in this paper is well-known and widely used in other projects, and we verify its quality and safety to ensure that no private or sensitive information is included in the training or evaluation process. Also, The KV cache reuse approach proposed in this paper does not introduce additional risks or bias in LLMs.  While we acknowledge that any LLM can have bias or potential misuse, the likelihood of risk or misuse of our proposed technique is considerably reduced.

\section*{Acknowledgments}
The work of Jingbo Yang, Bairu Hou and Shiyu Chang was partially supported by National Science Foundation (NSF) Grant IIS-2338252, NSF Grant IIS-2207052, and NSF Grant IIS-2302730. The computing resources used in this work were partially supported by the Accelerate Foundation Models Research program of Microsoft.

\bibliography{custom}

\newpage
\appendix

\section{Appendix}
\label{sec:appendix}

\input{tables/mixture}

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\linewidth]{figs/block_nq.pdf}
    \caption{\textbf{Impact of Answer Document Position on Accuracy.} 
    The accuracy substantially decreases when the correct document is located farther from the start, indicating the necessity of shuffling the retrieved documents in the QA training data.}
    \label{fig:block_nq}
\end{figure}

\subsection{Data Mixture}
\label{app: data_mixture}
Table~\ref{tab:context-reuse} provides an overview of the data used in our training. For the multi-turn conversation data, each prior user--assistant conversation is independently encoded as a reused context, and we compute the training loss only on the assistant's final response. In the summarization task, we split the source document into 100-token segments, each serving as a reused context. All training examples are truncated to a maximum length of 4096 tokens.

\subsection{Implementation Details}
\label{app: implementation_detail}

\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{figs/preprocess.pdf}
    \caption{\textbf{Data Preprocess for Context Reuse.}}
    \label{fig:preprocess}
\end{figure*}

\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{figs/system.pdf}
    \caption{\textbf{System Prompts Used for Training.} 
  We employ tailored system prompts for three primary task types—SFT, QA, and Summarization—reflecting different objectives and guiding the model’s responses accordingly.}
    \label{fig:system}
\end{figure*}

\paragraph{Training.}Figure~\ref{fig:system} illustrates the system prompts we use for each task category during training. These prompts are generated by GPT-4~\citep{achiam2023gpt}, and are randomly picked during training. Additionally, for the QA training data, we also shuffle the retrieved reused documents in the context~\ref{app: position_analysis}.

Figure~\ref{fig:preprocess} demonstrates how each data sample is processed when the context is reused in {\alg}. Particularly, we also include two special tokens, \texttt{KV-START} and \texttt{KV-END} to specify the boundaries of the reused contexts. For each reused document or context segment, the link tokens are dependent on the index of the document in all the reused documents, which means, in different prompts, for the $n$-th reused document, its link tokens are always \texttt{link$n$-1}, \texttt{link$n$-2}, ..., \texttt{link$n$-$K$}, if $K$ link tokens are used.

\paragraph{Evaluation.}For QA evaluation with separately encoded KV cache, we adopt the accuracy as the evaluation metric, following~\citep{sun2024block,liu2024lost,kandpal2023large,mallen2022not}, which considers the prediction is correct if any sub-string of the prediction exactly matches any gold answer.

For the evaluation of general capability preservation, we use LM Evaluation Harness~\citep{eval-harness}. We use few-shot examples for \texttt{GSM8K} and \texttt{MMLU}(8-shot for \texttt{GSM8K} and 5-shot for \texttt{MMLU}).

\subsection{More Implementation Details of Baselines}
\label{app: implementation_detail_baseline}

For {\blockattn}, we process the training data with context reuse in~\ref{tab:context-reuse} by separately encoding each reused document or context segment and concatenating the caches directly without inserting any special tokens in between. For the data with no context reuse, the data processing is the same as {\alg}.

For {\promptcache}, in its original implementation, the position encoding is not adjusted when reused in the new prompt. It uses discontinuous position information between reused caches, which is not accurate. We maximize the performance of {\promptcache} by giving all the reused contexts with gold position information during evaluation.

\subsection{Ablation Studies on Data Mixure}
\label{app: ablation}
\input{tables/ablation}
We experiment with various data mixtures under the {\alg}5 setting, each omitting a distinct subset from our original training mix (see Appendix~\ref{app: data_mixture}). Specifically, we explore three configurations: 
\textbf{No Summarization}, 
\textbf{No Multi-turn Conversation}, 
and 
\textbf{QA Only} (see Table~\ref{tab:ablation}). For the first two, we retain the same relative proportions among the remaining tasks and train for 6{,}000 steps, while \textbf{QA Only} is trained for two epochs to prevent overfitting. Our results show that tasks requiring cross-chunk reasoning are essential for robust cache reuse. Moreover, incorporating multiple types of cache-reuse data improves generalization. However, the optimal recipe of training data for fully equipping LLMs with general cache-reuse capabilities remains an open question, which we leave to future work.

\subsection{Impact of Answer Document Position}
\label{app: position_analysis}
Empirically, we have observed that directly constructing the QA training data using Contriever~\citep{izacard2021unsupervised} retrieved relevant documents is not ideal because the retriever typically places the answer-containing document near the front based on the relevance score. As a result, the fine-tuned model “learns” to find answers at the beginning of the context rather than reasoning across the entire reused context.

To validate our statement, we fine-tune the Llama-3.2-3B base model on the same training data without shuffling as {\blockattn} and evaluate under the~\citep{liu2024lost} setup. In~\citep{liu2024lost}, the test set for NaturalQuestions is also built with Contriever-retrieved contexts but places the document containing the correct answer at varying positions. As shown in Figure~\ref{fig:block_nq}, the model’s performance drops significantly whenever the ground-truth document is located further back in the context.

\subsection{Submission Checklist}
Various datasets are included for training and evaluation, and their licenses are listed below. 
The IFEval, NaturalQuestions, 2WikiMQA, TriviaQA, HotpotQA dataset is released under the Apache License 2.0. HumanEval, MMLU, GSM8K, HellaSwag, WinoGrande, XSum and LM-Evaluation-Harness are under the MIT License. The ARC dataset is provided under the Creative Commons Attribution Share Alike 4.0 license, and SciQ is under the Creative Commons Attribution Non-Commercial 3.0 license. The DaringAnteater dataset is under the license of Creative Commons Attribution 4.0. The MuSiQue dataset is under the Creative Commons Attribution 4.0 International license. The Samsum dataset is under the Creative Commons Attribution Non Commercial No Derivatives 4.0 license. The MultiNews dataset is under a legal agreement with LILY LAB. PiQA is licensed under the Academic Free License v. 3.0. The Tulu3-sft-mixture and fineweb datasets are under the Open Data Commons License Attribution family license.

All datasets and software packages in this study are used strictly for their intended purpose—namely, training and evaluating LLMs. We confirm that no personal identifying information or offensive content appears in our materials. Additionally, we employed GPT-4 to assist with proofreading and improving clarity throughout the text. Nevertheless, the research concepts, analysis, and original writing remain fully authored by us.
\end{document}
