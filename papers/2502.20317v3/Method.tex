\section{Framework}\label{sec-framework}
In a nutshell, we formulate our MoR as the conditional distribution $P_{\boldsymbol{\Theta}}(\mathcal{C}|Q, \mathcal{B})$ of retrieved candidates $\mathcal{C}$ given the user input query $Q$ over TG-KB $\mathcal{B}$, which is further factorized into three distributions corresponding to our proposed three modules: planning via generating the text-attributed planning graph $G$, reasoning via conducting mixture of structural-and-textual traversal to obtain intermediate candidates $\widetilde{\mathcal{C}}$ following the generated planning graph $G$, and organizing via applying structure-aware reranking to the obtained candidates $\widetilde{\mathcal{C}}$, obtaining final candidates $\mathcal{C}$:
\begin{equation}
\scalebox{0.9}{$
\begin{aligned}
    &P_{\boldsymbol{\Theta}}(\mathcal{C}|Q, \mathcal{B}) 
    = \sum_{G\in\mathbb{G}} \Bigl[\sum_{\widetilde{\mathcal{C}}\in\mathbb{C}}  
    \underbrace{P_{\boldsymbol{\Theta}_3}(\mathcal{C}|\widetilde{\mathcal{C}}, G, Q, \mathcal{B})}_{\text{Organizing}} \notag \\
    &\quad \times \underbrace{P_{\boldsymbol{\Theta}_2}(\widetilde{\mathcal{C}}|G, Q, \mathcal{B})}_{\text{Reasoning}}\Bigr] 
    \times \underbrace{P_{\boldsymbol{\Theta}_1}(G|Q, \mathcal{B})}_{\text{Planning}}
\end{aligned}
$}
\end{equation}
where $P_{\boldsymbol{\Theta}_1}(G|Q, \mathcal{B})$ is the probability distribution of generating the text-attributed planning graph $G$ given the input query $Q$ and TG-KB $\mathcal{B}$; $P_{\boldsymbol{\Theta}_2}(\widetilde{\mathcal{C}}|G, Q, \mathcal{B})$ is the probability distribution of retrieving intermediate candidates $\widetilde{\mathcal{C}}$ given the planning graph $G$ and the query $Q$ via our mixed traversal; $P_{\boldsymbol{\Theta}_3}(\mathcal{C}|\widetilde{\mathcal{C}}, G, Q, \mathcal{B})$ is the probability distribution of reranking the intermediate candidates so that Top-K positions form the ground-truth entities $\mathcal{C}$. $\mathbb{G}/\mathbb{C}$ denotes the space of all possible planning graphs and all possible configurations of size-K candidate node sets from all nodes $\mathcal{V}$ in TG-KB $\mathcal{B}$. The overall objective is to maximize the likelihood of retrieving ground-truth candidates $\mathcal{C}$ for each input query $Q \in \mathcal{Q}$:
\begin{equation}
\boldsymbol{\Theta}^{*} = \argmax_{\boldsymbol{\Theta}}\prod_{Q \in \mathcal{Q}}P_{\boldsymbol{\Theta}}(\mathcal{C}|Q, \mathcal{B})
\end{equation}


Following the above paradigm, we next introduce the three components: Planning via textual graph generation in Section~\ref{sec-planning}, Reasoning via mixed traversal in Section~\ref{sec-reasoning}, and Organizing via structure-aware reranking in Section~\ref{sec-organizing}.


\subsection{Planning via Textual Graph Generation}\label{sec-planning}

To effectively reason over the underlying logic of queries and answer them, we propose a planning module that constructs a planning graph to capture their underlying logical structures. Unlike conventional approaches relying on rigid heuristics, e.g., shortest-path retrieval in knowledge graphs~\cite{luo2023reasoning, Delile2024GraphBasedRC}, or step-by-step prompting of LLMs, which incurs high computational costs~\cite{Sun2023ThinkonGraphDA, wang2024knowledge}, our method generates the entire planning graph in one shot, eliminating repeated LLM calls. More importantly, as planning graphs integrate entity restrictions encoding query-specific constraints and entity categories capturing broader logical structure, our MoR can generalize learned patterns and efficiently adapt to new queries with the same underlying logic. For example, any query with the form \textit{Papers associated with <institution> and are in the field of <field>} shares the same patterns with the query in Figure~\ref{fig-framework}. Below, we first formalize the planning graph and then optimize its generation.

\subsubsection{Planning Graph Formulation}
A planning graph $G$ is a structured representation where nodes represent entities and edges denote their logical relations. Each entity is associated with both a category and query-specific restriction. For example, given the query \textit{Can you give me publications by Point Park University authors on stellar populations in tidal tails}, the generated planning graph is: $G=$ (Institution<\textit{Point Park University}> $\rightarrow$ Author $\rightarrow$ Paper $\leftarrow$ Field-of-Study<\textit{Stellar Population}>)  with Institution, Author, Paper, Field-of-Study as categories and <\textit{Point Park University}>, <\textit{Stellar Populations}> as restrictions. Note that edges in our planning graph can also possess different categories. For example, in the biomedical TG-KBs, the relation between Disease and Drug entities could be Indication or Contra-indication~\cite{wu2024stark}, adding a finer level of semantic distinction to the relation.  



\subsubsection{Planning Graph Optimization} 
To ensure that our generated planning graph captures the query logic, we train a textual graph generator to maximize the likelihood of generating ground-truth planning graphs given their queries. Formally, given the joint distribution of the training pairs between queries and planning graphs $P_{\mathbb{Q}\times \mathbb{G}}^{\text{Train}}$, we optimize the planning module $P_{\boldsymbol{\Theta}_1}$ by solving:
\begin{equation}
    \arg\max_{\boldsymbol{\Theta}_1} \mathbb{E}_{(Q, G) \sim P_{\mathbb{Q}\times \mathbb{G}}^{\text{Train}}} \log P_{\boldsymbol{\Theta}_1}(G | Q, \mathcal{B}) 
\end{equation}
To avoid the combinatorial explosion of exponentially growing planning graph candidates~\cite{you2018graphrnn}, we decompose each planning graph into multiple reasoning paths $G = \{\mathcal{P}_i\}_{i=1}^{|G|}$. Each path $\mathcal{P}_i = (p_{i1} \rightarrow, ..., \rightarrow p_{iL_i})$ represents a distinct reasoning chain, where node $p_{ij}$ denotes an entity in TG-KB sharing the same textual category $\mathcal{E}_{p_{ij}}$ and satisfying the restriction $\mathcal{T}_{p_{ij}}$ from the query. Given the sequential nature and textual formats of these decomposed reasoning paths, LLMs can be naturally employed here as the planning graph generator, which conducts next-token prediction by predicting $j^{\text{th}}$ token $t_j$ conditioned on preceding tokens $t_{<j}$, the query $Q$ and the TG-KB $\mathcal{B}$:
\begin{equation}
    P_{\boldsymbol{\Theta}_1}(G|Q) = \prod_{j=1}^{n} P_{\boldsymbol{\Theta}_1}(t_j | t_{<j}, Q, \mathcal{B}).
\end{equation}
Note that our proposed planning graph generator is not limited to LLMs. Any graph generative model preserving both structural dependencies and textual associations can be employed~\cite{zhu2022survey}.






\subsection{Reasoning via Mixed Traversal}\label{sec-reasoning}
Following the reasoning paths of the above planning graph $G = \{\mathcal{P}_i\}_{i=1}^{|G|}$, the reasoning module conducts a mixed traversal by interweaving neighbor fetching and textual matching to form intermediate candidates $\widetilde{\mathcal{C}}$, which are introduced next.

\subsubsection{Structural Traversal}
Following the definition in Section~\ref{sec-preliminary} that structural retrieval follows prescribed rules for knowledge retrieval, here we set these prescribed rules to be iteratively performing layer-wise breadth-first-search that traverses neighboring entities with categories aligning with those in the reasoning paths. Concretely, reasoning at the $l^{\text{th}}$-step of the planning path $\mathcal{P}_i$, we check for each node $v$ in candidates set of last layer $\forall v \in \widetilde{\mathcal{C}}_i^{l-1}$ and fetch its neighbors $\forall u \in \mathcal{N}_v$ with the same category as the corresponding node $p_{il}$ (i.e., $\mathcal{E}_u = \mathcal{E}_{p_{il}}$) in the reasoning path, which can be mathematically formulated as:
\begin{equation}
\widetilde{\mathcal{C}}_i^{l, \text{Struct}} = \cup_{v \in \widetilde{\mathcal{C}}_i^{l-1}}\{u|u \in \mathcal{N}_v, \mathcal{E}_u = \mathcal{E}_{p_{il}}\} 
\end{equation}
where $\widetilde{\mathcal{C}}_i^{l, \text{Struct}}$ denotes the set of structurally retrieved entities at the $l^{\text{th}}$ reasoning step according to the path $\mathcal{P}_i$ and $\mathcal{E}_u=\mathcal{E}_{p_{il}}$ ensures that the category of the traversed neighbor $u$ matches the corresponding entity category routine by the planning graph, resonating the nature of rule-based structural retrieval. Note that the seeding candidates $\widetilde{\mathcal{C}}^{1, \text{Struct}}_i$ at the very first layer are initialized by retrieving Top-K entities through textual matching, i.e., $\widetilde{\mathcal{C}}^{1, \text{Struct}}_i = \widetilde{\mathcal{C}}^{1, \text{Text}}_i$, which is introduced next.


\subsubsection{Textual Matching}
In addition to retrieving structural knowledge, our MoR also retrieves textual knowledge via Textual Matching, which retrieves candidates based on their textual similarity to queries. For each reasoning node $p_{il}$ at $l^{\text{th}}$ reasoning step along the reasoning path $\mathcal{P}_i$, we concatenate the query and the textual restriction of $p_{il}$, i.e., $Q' = [Q:\mathcal{T}_{p_{il}}]$, then compute its textual similarity to documents of nodes in TG-KB, i.e., $\phi(Q', \mathcal{D}_v), \forall v \in \mathcal{V}$, and finally retrieve the Top-K scored candidates:
\begin{equation}
\scalebox{0.86}{$
    \widetilde{\mathcal{C}}_i^{l, \text{Text}} = \operatorname{TopK} ( \{ v \mid v \in \mathcal{V}, \mathcal{E}_v = \mathcal{E}_{p_{il}} \}, \phi(Q', \mathcal{D}_v))
$}
\end{equation}

Integrating candidates from structural traversal and textual matching together, the final candidates at $l^{\text{th}}$-step of $\mathcal{P}_i$ are formed as:
\begin{equation}
\scalebox{0.9}{$
    \widetilde{\mathcal{C}}^{l}_i = \widetilde{\mathcal{C}}^{l, \text{Struct}}_i \cup \widetilde{\mathcal{C}}^{l, \text{Text}}_i, \forall l\in\{1, 2, ..., L_i\}
$}
\end{equation}
The integrated candidates $\widetilde{\mathcal{C}}^{l}_i$ serve as seeding nodes initializing the next round of planning graph-guided structural traversal and textual matching, which creates a mutual reinforcement between structural and textual knowledge since previously retrieved two knowledge can both inform next round of structural/textual knowledge retrieval.




We iteratively conduct mixed traversal for every reasoning path $\mathcal{P}_i\in G$ and integrate retrieved entities together by taking their intersection, i.e., $\widetilde{\mathcal{C}}=\cap_{\mathcal{P}_i\in G}\widetilde{\mathcal{C}}_i^{L_i}$, adhering to the fact that candidates should simultaneously satisfy the logic routine by all reasoning paths. Note that no training is involved in the mixed graph traversal, i.e., $P_{\boldsymbol{\Theta}_2}(\widetilde{\mathcal{C}}|G, Q, \mathcal{B}) = P(\widetilde{\mathcal{C}}|G, Q, \mathcal{B})$. Future works could explore optimizing graph traversal by rewards from agent-environment interactions~\cite{nguyen2024dynasaur}.





\newpage
\subsection{Organizing via Structure-aware Rerank}\label{sec-organizing}
Although the retrieved candidates from Section~\ref{sec-reasoning} strictly adhere to the prescribed rule given by the planning graph, the sheer volume of candidates misaligns with realistic constraints (e.g., Top-20 retrieval budget~\cite{Zeng2024FederatedRV}) and may even cause difficulty to downstream executors such as long-context challenges for LLMs. To better emulate human reasoning, where multiple clues are gathered, analyzed in relation to the query, and synthesized into a coherent answer, we propose a structure-aware reranker to organize and rerank the candidates $\widetilde{\mathcal{C}}$, and select Top-K ones as the final retrieved answers $\mathcal{C}$. Instead of relying only on textual features~\cite{hu2019retrieve}, our reranker assigns a ranking score based on features of structural trajectories obtained from the mixed traversal in Section~\ref{sec-reasoning}, innovatively leveraging both structural and textual knowledge in reranking.  

Previously, $\widetilde{\mathcal{C}}$ is defined as intermediate retrieved entities. To consider structural features in reranking, we pair each retrieved candidate in $\widetilde{\mathcal{C}}$ with its corresponding traversal trajectory obtained from the reasoning module. Specifically, each trajectory $\mathcal{P}_i$ of length $L_i$ is featuring three types of attributes:

\begin{itemize}[leftmargin=*]
    \item \textbf{Textual Fingerprint (TF)}: Concatenation of similarity scores between the expanded query and each node on the path: $\big\|_{l=1}^{L_i} \phi(Q', \mathcal{D}_{p_{il}})$.
    \item \textbf{Structural Fingerprint (SF)}: Concatenation of node categories at each step on the path: $\big\|_{l=1}^{L_i} \mathcal{E}_{p_{il}}$
    \item \textbf{Traversal Identifier (TI)}: Concatenation of the indicator specifying whether each step uses a structural or textual retrieval: $\big\|_{l=1}^{L_i} \mathcal{I}_{p_{il}}$.
\end{itemize}
We then train a reranker on these trajectories using the cross-entropy loss. For a training query $Q$ and its associated candidate trajectory $\mathcal{P}_i$, the loss is computed as follows:
\begin{equation}
\scalebox{0.85}{$
\begin{aligned}
    \mathcal{L}_{\boldsymbol{\Theta}_3} =-\sum_{(\mathcal{P}_i, Q) \in \widetilde{\mathcal{C}}}\sum_{j=1}^2 y^i_j\log ( \sigma(&f(\underbrace{\big\|_{l=1}^{L_i}\mathcal{E}_{p_{il}}}_{\text{Structural Fingerprint}}\\    :\underbrace{\big\|_{l=1}^{L_i}\phi(Q', \mathcal{D}_{p_{il}})}_{\text{Textual Fingerprint}} &:\underbrace{\big\|_{l=1}^{L_i}\mathcal{I}_{p_{il}}}_{\text{Traversal Identifier}}))_j).
\end{aligned}
$}
\end{equation}
where $f(\cdot)$ is the reranker producing a score for each $(Q, \mathcal{P}_i)$ pair, $\sigma(\cdot)$ denotes the softmax function, and $y^i_j \in \{0,1\}$ indicates whether the $i$-th candidate is a correct (positive) or incorrect (negative) match for $Q$. This formulation encourages the reranker to assign higher scores to positive trajectories, thereby improving ranking performance.






\begin{table*}[t!]
\setlength\tabcolsep{2.8pt}
\centering
\tiny
\begin{tabular}{ll|cccc|cccc|cccc|cccc}
\toprule
\multirow{3}{*}{\textbf{Category}} & \multirow{3}{*}{\textbf{Retrieval Baseline}}  & \multicolumn{4}{c|}{\textbf{AMAZON}} & \multicolumn{4}{c|}{\textbf{MAG}} & \multicolumn{4}{c|}{\textbf{PRIME}} & \multicolumn{4}{c}{\textbf{AVERAGE}} \\
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-14} \cmidrule(lr){15-18}
& & H@1 & H@5 & R@20 & MRR & H@1 & H@5 & R@20 & MRR & H@1 & H@5 & R@20 & MRR & H@1 & H@5 & R@20 & MRR \\
\midrule
\multirow{4}{*}{\textbf{Textual}}       & BM25~\cite{wu2024stark}             & 44.94 & 67.42 & 53.77 & 55.30 & 25.85 & 45.25 & 45.69 & 34.91 & 12.75 & 27.92 & 31.25 & 19.84 & 27.85  & 46.86 & 43.57 & 36.68\\
    & Ada-002~\cite{wu2024stark}         & 39.16 & 62.73 & 53.29 & 50.35 & 29.08 & 49.61 & 48.36 & 38.62 & 12.63 & 31.49 & 36.00 & 21.41 & 26.96 & 47.94 & 45.88 & 36.79\\
    & Multi-ada-002~\cite{wu2024stark}   & 40.07 & 64.98 & 55.12 & 51.55 & 25.92 & 50.43 & 50.80 & 36.94 & 15.10 & 33.56 & 38.05 & 23.49 & 27.03 & 49.66 & 47.99 & 37.33\\
    & DPR~\cite{karpukhin-etal-2020-dense}             & 15.29 & 47.93 & 44.49 & 30.20 & 10.51 & 35.23 & 42.11 & 21.34 & 4.46  & 21.85 & 30.13 & 12.38 & 10.09 & 35.00 & 38.91 & 21.31\\
\midrule
\multirow{2}{*}{\textbf{Structural (KG)}} & QAGNN~\cite{yasunaga2021qagnn}       & 26.56 & 50.01 & 52.05 & 37.75 & 12.88 & 39.01 & 46.97 & 29.12 & 8.85  & 21.35 & 29.63 & 14.73 & 16.10 & 36.79 & 42.88     & 27.20\\
& ToG~\cite{Sun2023ThinkonGraphDA} & - & - & - & - & 13.16 & 16.17 & 11.30 & 14.18 & 6.07  & 15.71 & 13.07 & 10.17 & 9.62 & 15.94    & 12.18 & 12.18\\
\midrule
\multirow{4}{*}{\textbf{Hybrid}} & AvaTaR~\cite{wu2024avatar}  & 49.87 & 69.16 & \textbf{60.57} & 58.70 & 44.36 & 59.66 & 50.63 & 51.15 & 18.44 & 36.73 & 39.31 & 26.73  & 37.56 & 55.18 & 50.17 & 45.53\\
& KAR~\cite{xia2024knowledge}             & \textbf{54.20} & 68.70 & 57.24 & \underline{61.29} & \underline{50.47} & 69.57 & 60.28 & \underline{58.65} & 30.35 & 49.30 & 50.81 & 39.22 & \underline{45.01} & 62.52  & 56.11 &53.05\\

& MFAR$^{*}$~\cite{li2024multi}           & 41.20 & \underline{70.00} & 58.50 & 54.20 & 49.00 & \underline{69.60} & \underline{71.70} & 58.20 & \textbf{40.90} & \textbf{62.80} & \textbf{68.30} & \textbf{51.20} & 43.70 & \underline{67.47} & \textbf{66.17} & \underline{54.53}\\
& \cellcolor{gray!30} MoR & \cellcolor{gray!30} \underline{52.19} & \cellcolor{gray!30} \textbf{74.65} & \cellcolor{gray!30} \underline{59.92} & \cellcolor{gray!30} \textbf{62.24} & \cellcolor{gray!30} \textbf{58.19} & \cellcolor{gray!30} \textbf{78.34} & \cellcolor{gray!30} \textbf{75.01} & \cellcolor{gray!30} \textbf{67.14} & \cellcolor{gray!30} \underline{36.41} & \cellcolor{gray!30} \underline{60.01} & \cellcolor{gray!30} \underline{63.48} & \cellcolor{gray!30} \underline{46.92} & \cellcolor{gray!30} \textbf{48.93} & \cellcolor{gray!30} \textbf{71.00}  & \cellcolor{gray!30} \underline{66.14} & \cellcolor{gray!30} \textbf{58.77}\\
\midrule
\multirow{3}{*}{\textbf{Ablation}} &\cellcolor{gray!30} MoR$_{\text{w/o R}}$ &\cellcolor{gray!30} 44.21  &\cellcolor{gray!30} 68.87 &\cellcolor{gray!30} 56.50 &\cellcolor{gray!30} 55.28 &\cellcolor{gray!30} 34.33 &\cellcolor{gray!30} 62.55 &\cellcolor{gray!30} 67.55 &\cellcolor{gray!30} 47.40 &\cellcolor{gray!30} 31.59 
&\cellcolor{gray!30} 53.48 &\cellcolor{gray!30} 60.74 & \cellcolor{gray!30} 41.81 &\cellcolor{gray!30} 31.07 &\cellcolor{gray!30} 57.04 &\cellcolor{gray!30} 57.73 &\cellcolor{gray!30} 43.03\\

&\cellcolor{gray!30} MoR$_{\text{w/o RT}}$& \cellcolor{gray!30} 34.04  &\cellcolor{gray!30} 53.41 &\cellcolor{gray!30} 45.16 &\cellcolor{gray!30} 42.85 &\cellcolor{gray!30} 51.81 &\cellcolor{gray!30} 73.54 &\cellcolor{gray!30} 74.17 &\cellcolor{gray!30} 61.68 &\cellcolor{gray!30} 28.95 &\cellcolor{gray!30} 46.12 &\cellcolor{gray!30} 49.54 &\cellcolor{gray!30} 36.56 &\cellcolor{gray!30} 36.39 &\cellcolor{gray!30} 56.73 &\cellcolor{gray!30} 55.73 &\cellcolor{gray!30} 45.53 \\

&\cellcolor{gray!30} MoR$_\text{w/o RS}$ &\cellcolor{gray!30} 43.05  &\cellcolor{gray!30} 69.36 &\cellcolor{gray!30} 57.38 &\cellcolor{gray!30} 54.69 &\cellcolor{gray!30} 31.05 &\cellcolor{gray!30} 51.84 &\cellcolor{gray!30} 50.56 &\cellcolor{gray!30} 40.64 &\cellcolor{gray!30} 22.27 &\cellcolor{gray!30} 38.45 &\cellcolor{gray!30} 39.21 &\cellcolor{gray!30} 29.41 &\cellcolor{gray!30} 28.95 &\cellcolor{gray!30} 51.28 &\cellcolor{gray!30} 48.02 &\cellcolor{gray!30} 38.98\\



\bottomrule
\end{tabular}
\caption{Comparing different retrieval methods with our proposed MoR and its ablations on Amazon, MAG, and Prime datasets. The best and runner-up results are in \textbf{bold} and \underline{underlined}. Overall, MoR achieves the best performance. Note that MFAR$^{*}$ denotes the best model variant proposed in~\cite{li2024multi}}
\label{tab-merged}
\vspace{-3ex}
\end{table*}
