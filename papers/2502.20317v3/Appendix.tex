\section{Summary of Notations}\label{sec-notation}
\vspace{-2ex}
\begin{table}[H]
\tiny
\setlength{\extrarowheight}{.095pt}
\setlength\tabcolsep{0.5pt}
\caption{Notations and the corresponding descriptions.}
\vspace{-1ex}
\label{tab-symbols}
\begin{tabularx}{0.49\textwidth}{c|c}
\toprule
\textbf{Notations}       & \textbf{Definitions or Descriptions} \\
\midrule
$\mathcal{B}$ & Text-rich Graph Knowledge Base (TG-KB)\\

$\mathcal{V, E, D}$ & Set of Nodes, Categories and Documents of TG-KB\\

$\mathcal{D}_v, \mathcal{E}_v$ & Document and Category of Node $v$\\

$Q \in \mathcal{Q}$ & Query $Q$ from Query set $\mathcal{Q}$\\

$\mathcal{Q}^{\text{Struct}}, \mathcal{Q}^{\text{Text}}$ & Query targeted by structural and textual retrieval\\

$G = \{\mathcal{P}_i\}_{i=1}^{|G|}$ & Planning Graph consisting of multiple reasoning paths\\

$\mathcal{P}_i = (p_{i1}\rightarrow ... \rightarrow p_{iL_i})$ & Reasoning path consisting of $L_i$ sequential entities \\

$\mathcal{E}_{p_{ij}}, \mathcal{T}_{p_{ij}}$ & Textual category and restriction of path entity $p_{ij}$ \\

$\widetilde{\mathcal{C}}$ & Retrieved candidates after reasoning module.\\
\midrule
$\widetilde{\mathcal{C}}^l_i = \widetilde{\mathcal{C}}^{l, \text{Struct}}_i\cup \widetilde{\mathcal{C}}^{l, \text{Text}}_i$ & \makecell[c]{Retrieved candidates at $l^{\text{th}}$ layer for $i^{\text{th}}$ path including \\structurally retrieved ones and textually retrieved ones.}\\
\midrule

$\mathcal{C}$ & Final retrieved candidates after organizing module.\\

$P_{\mathbb{Q}\times \mathbb{G}}$ & Joint distribution of query and planning graph.\\

$\mathcal{N}_v$ & Neighborhood of entity $v$\\

$\mathcal{I}_{p_{il}}$ & Traversal Identifier of Structural and Textual Retrieval\\

$P_{\boldsymbol{\Theta}_1}$ & Planning module with its parameters $\boldsymbol{\Theta}_{1}$\\
$P_{\boldsymbol{\Theta}_2}$ & Reasoning module with its parameters $\boldsymbol{\Theta}_{2}$\\
$P_{\boldsymbol{\Theta}_3}$ & Organizing module with its parameters $\boldsymbol{\Theta}_{3}$\\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Dataset} & \# \textbf{Entities} & \# \textbf{Text Tokens} & \# \textbf{Relations} & \textbf{Avg. Degree} \\
        \midrule
        \textbf{AMAZON} & 1,035,542 & 592,067,882 & 9,443,802 & 18.2 \\
        \textbf{MAG} & 1,872,968 & 212,602,571 & 39,802,116 & 43.5 \\
        \textbf{PRIME} & 129,375 & 31,844,769 & 8,100,498 & 125.2 \\
        \bottomrule
    \end{tabular}
    \caption{Statistics of text-rich graph knowledge bases in STaRK benchmark~\cite{wu2024stark}.}
    \label{tab:stark_stats}
\end{table}

\section{Experimental Details}\label{app-expr-setting}

\subsection{Datasets}
To evaluate the effectiveness of our proposed framework, we conduct experiments using three Text-rich Graph Knowledge Bases (TG-KBs) from STaRK~\cite{wu2024stark}. These TG-KBs cover a wide range of domains, including product reviews (Amazon), academic papers (MAG), and biomedical knowledge (Prime). Each TG-KB comprises a textual graph and an associated corpus, with the corpus containing documents linked to the nodes in the graph. Queries are meticulously crafted for each TG-KB and encompass varying levels of complexity, which desire different levels of textual and structural knowledge to answer.

\textbf{Amazon:} a dataset provides a realistic simulation of product search and recommendation. Its textual graph consists of four categories of nodes: \textit{product}, \textit{category}, \textit{color}, and \textit{brand}. Nodes are interconnected through relations such as \textit{has\_brand} and \textit{has\_category}. Textual documents encapsulate properties of corresponding nodes, such as product descriptions and customer reviews. 

\textbf{MAG:} a comprehensive resource for academic paper retrieval. In the textual graph, \textit{papers} can be connected to other nodes, such as \textit{field\_of\_study} via the \textit{paper\_has\_topic\_field\_of\_study} relation and \textit{institution} through a combination of relations like \textit{author\_affiliated\_with\_institution} and \textit{author\_writes\_paper}. Each \textit{paper} document includes the title, abstract, and metadata, such as the publication date and venue, providing rich contextual knowledge for retrieval and analysis. 

\textbf{Prime:} a highly domain-specific dataset. It focuses on medical inquiries and is sourced from the PrimeKG knowledge graph~\cite{chandak2023building}, which comprises ten entity types and eighteen relation types, offering multiple target node categories, such as \textit{disease}, \textit{gene/protein}, and \textit{drug}. The associated documents are aggregated from various databases, providing a rich and diverse source of medical knowledge.

Detailed dataset statistics are in Table~\ref{tab:stark_stats}.

\subsection{Implementation Details}\label{app-implementation}

\textbf{Prompt for Planning Graph Generation:}
For planning graph generation in Section~\ref{sec-planning}, we follow previous works~\cite{luo2023reasoning, wu2024stark} to linearize the planning process by decomposing the planning graph into sequential reasoning paths, which can be generated by LLMs via next token prediction. Given the lack of ground-truth planning graphs for training, we prompt LLMs to synthesize these ground-truth planning graphs due to their superior reasoning capability. The prompt for generating the ground-truth planning graph is:

\begin{tcolorbox}[
colback=blue!10!white, 
colframe=blue!80!black, 
title=Prompt 1: Planning Graph Generation, 
boxsep=0.75mm, % Adjust the internal padding
left=0.75mm, % Adjust left margin
right=0.75mm, % Adjust right margin
top=0.75mm, % Adjust top margin
bottom=0.75mm, % Adjust bottom margin
float=htbp!,     % This makes the tcolorbox float like a figure/table
floatplacement=tbp, % Specify placement, allowing t, b, and p
]
\small
\label{tab: prompt}
\textbf{System Message:} You are a planning graph finder agent. Your role is to:
\newline
1. Identify the underlying **meta-path** from a given question, which consists of the **entity types** at each reasoning step. 
\newline
2. Extract the **content restriction** for each **entity type** based on the question. If there is no restriction for an entity type, leave its value empty.
\newline
You will be provided with a predefined **Entity Type List**. Only use the entity types from this list when constructing the meta-path and restrictions. Your response must be concise and strictly adhere to the specified **output format**.
\newline
\newline
\textbf{Entity Type List:} 
\textcolor{blue}{Provide the entity type list.}
\newline
% \newline
\textbf{Demonstrations:} 
\textcolor{blue}{Examples for in-context learning.}
\newline
\textbf{Output Fromat:} 
\textcolor{blue}{\textit{Metapath: ""}, \textit{Restriction: \{\}}}.
\end{tcolorbox}

\textbf{Trajectory Collection:} As mentioned in Section~\ref{sec-organizing}, our reranker reorders the intermediate retrieved candidates based on their trajectory. To achieve this, we collect three key features: \textbf{Textual Fingerprint (TF)}, \textbf{Structural Fingerprint (SF)}, and \textbf{Traversal Identifier (TI)}. 

\textbf{Textual Fingerprint (TF):} We record the BM25 similarity scores between the query and the traversed nodes computed. Since empirical observations indicate that the length of reasoning paths is typically less than three, we fix the textual fingerprint to the length of three by padding additional 0 similarity scores for those reasoning paths whose length is less than three, allowing for batch-wise training. Additionally, we append the initial semantic ranking score of the candidate computed using cosine similarity coupled with Ada-002 embedding to the end of three BM25-based similarity scores to complement the lexical perspective. This vector is then passed through a linear layer to be transformed into an embedding of size 256. Note that this initial ranking score is also used to select the intermediate retrieved candidates used for reranking. 

\textbf{Structural Fingerprint (SF):} We concatenate the categories of all nodes in the corresponding reasoning path as a text sequence. If the reasoning path is shorter than three nodes, we prepend the sequence with "padding" tokens to ensure a fixed length. The structural fingerprint is then processed using a transformer model, which converts the sequence into an embedding of size 768, followed by a linear layer that projects it down to size 256. 

\textbf{Traversal Identifier (TI):} We track whether each node is retrieved via textual matching or structural traversal and encoding them with distinct values by initializing a learnable embedding matrix mapping each traversal identifier encoding to a 3x256-dimensional embedding vector.

After obtaining all above three trajectory features, we concatenate their obtained vectors into a unified vector (256 + 256 + 256x3 = 1280) and apply two fully connected layers to transform the combined representation into a reranking score. This score determines the final ranking.

\section{Additional Results}
\subsection{Query Pattern Analysis}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/logical-structure-mag.png}
    \caption{Imbalance number of queries and performance of different retrievers across different logic patterns.}
    \label{fig-analysis-mag}
\end{figure}
Figure~\ref{fig-analysis-mag} illustrates the analysis of query patterns in the MAG dataset. With richer relational information, queries in this dataset form a wider variety of patterns, including longer and more diverse structures. Similar to the Amazon dataset, we observe a general trend where the performance of MoR declines as the query count decreases across different patterns. Beyond this overall trend, certain query patterns in the MAG dataset stand out, such as "P → A → P" (Product-to-Author-to-Product) and "P → P" (Paper-to-Paper). Despite their relatively high occurrence, MoR still performs worse on these patterns. This is similar to low performance on the "Product → Product" pattern observed in the Amazon dataset, where repeated entity types appear within a single query. Such repetition causes the textual retriever to shift focus from the target to the repeated entities, leading to lower performance.


\section{Comprehensive Related Work}\label{app-comprehensive}
\subsection{Retrieval-augmented Generation (RAG)}
With the unprecedented success of recent LLMs in approaching human-level intelligence, retrieving relevant knowledge to support downstream generation has become increasingly crucial. Retrieval-augmented generation enhances generative tasks by integrating relevant information from external knowledge sources~\cite{he2024g, gao2023retrieval, han2024retrieval} and has been widely adopted to improve question-answering~\cite{liu2023knowledge}. In the context of LLMs, RAG has been utilized to mitigate hallucinations~\cite{yao2023llm}, enhance interpretability~\cite{gao2023chat}, and enable dynamic knowledge updates~\cite{wang2024knowledge}. This work leverages RAG to retrieve supporting entities from TG-KBs, providing contextual grounding for answer generation. Depending on the type of knowledge retrieved, existing retrievers can be classified into structural and textual retrieval approaches, which are reviewed next.

\subsection{Textual and Structural Retrieval}
Since real-world knowledge is commonly stored in both textual and structural formats~\cite{kolomiyets2011survey}, such as indexed texts and knowledge graphs, each requires a retrieval method tailored to its unique representation. Textual retriever retrieves knowledge based on its similarity to the given query and can be categorized into: lexical methods (e.g., TF-IDF and BM25~\cite{robertson2009probabilistic}) and semantic methods (e.g., DPR and Contriever~\cite{karpukhin-etal-2020-dense, izacardunsupervised}). Despite their broad applicability, the predefined linguistic rules and embedding-based semantics may struggle to capture the structural knowledge stored in graph-structured knowledge bases such as knowledge graphs and text-rich networks. To address this challenge, structural retrieval has been proposed by using graph analysis techniques (e.g., graph traversal~\cite{wang2024knowledge, Jiang2023StructGPTAG, Zhang2022GreaseLMGR, Edge2024FromLT}) and graph machine learning models (e.g., graph neural networks~\cite{yasunaga2021qagnn, Mavromatis2024GNNRAGGN}). Early methods extract local subgraphs of seeding nodes~\cite{yasunaga2021qagnn, Taunk2023GrapeQAGA} or pre-define paths approaching answers (e.g., shortest paths~\cite{luo2023reasoning, Delile2024GraphBasedRC}). To avoid exponentially expanding neighbors in the local subgraphs and break the rigid logic routined by pre-defined paths, recent advancements integrated LLMs to dynamically adjust graph traversal~\cite{Sun2023ThinkonGraphDA, wang2024knowledge, Jin2024GraphCA}. While promising, frequently invoking LLMs introduces prohibitive resource overhead. Despite the above advancements in both textual and structural retrieval, they are often applied independently and fail to mutually reinforce each other. This motivates the recent research trend of developing hybrid retrieval, which is reviewed next.

\subsection{Hybrid Retrieval}
Recently, several works have explored hybrid knowledge retrieval from TG-KBs. One approach~\cite{xia2024knowledge, li2024multi} aggregate documents from neighboring nodes, with~\citet{xia2024knowledge} applying relational filtering to remove irrelevant neighbors and~\citet{li2024multi} weighting neighbors based on field importance. Another approach~\cite{Lee2024HybGRAGHR} uses LLMs to choose either structural or textual retrieval. In contrast, our proposed MoR fully leverages the graph structure and rich texts by integrating textual matching and graph traversal into a unified framework, enabling a more seamless and interpretable interaction between structural and textual knowledge 



