
\newpage
\section{Experiment}\label{sec-experiment}
\subsection{Experimental Setup}
We briefly introduce experimental settings to verify our proposed MoR, including Datasets \& Baselines, Implementation Details, and Evaluation Metrics. More details are in Appendix~\ref{app-expr-setting}.

\textbf{Datasets \& Baselines:} We use three TG-KBs from STaRK~\cite{wu2024stark} covering three knowledge domains, including E-commerce Products (Amazon), Academic Papers (MAG), and Biomedicine (Prime). We compare our MoR with baselines established by~\citet{wu2024stark} and categorize them into textual/structural/hybrid-based ones. More recent state-of-the-art hybird retrieval approaches fro TG-KBs such as KAR~\cite{xia2024knowledge} and MFAR$^{*}$~\cite{li2024multi} are also compared.


\textbf{Implementation Details:} 
To enhance the planning capability of our planning module, we fine-tune the Llama 3.2 (3B) on 1000 sampled queries with their corresponding ground-truth planning graphs, serving as the textual graph generator. In the absence of ground-truths, we synthesize them using LLMs. For the Prime dataset, we empirically find that directly prompting LLMs can hardly generate accurate planning graphs due to the lack of biomedical domain knowledge~\cite{Shen2024TagLLMRG}. Therefore, we adopt an alternative approach. First, we instruct LLMs to extract triplets from each query and then construct the planning graphs by merging triplets with shared entities. 
During mixed traversal, textual matching can be implemented using any lexical or semantic methods. For this study, we employ BM25 for Amazon and MAG and fine-tune a contriever to complement the biomedical knowledge for Prime.
To initialize the structural traversal, we employ textual matching to locate the top 5 nodes that are most relevant to the query as seeds. Additionally, at each layer, we incorporate the top 10 nodes retrieved via textual matching and append them to the current candidate set for the next round of traversal. Notably, due to the uncertainty of LLMs, the generated planning graphs can be invalid. In this case, we will directly conduct textual matching to retrieve candidates. For our ablations without reranker, we employ Ada-002~\cite{wu2024stark} with cosine similarity as the scorer to rank candidates for evaluating performance.

\textbf{Evaluation Metrics:}
We follow~\citet{wu2024stark} for evaluation by reporting Hit@1 (H@1), Hit@5 (H@5), Recall@20 (R@20), and mean reciprocal rank MRR to evaluate in the full spectrum. 


 

\newpage
\subsection{Overall Retrieval Performance}
We compare MoR with other baselines on three TG-KBs in Table~\ref{tab-merged}. Generally, hybrid methods, AvaTAR, KAR, MFAR$^{*}$, and our MoR, achieve better performance than purely textual or structural methods owing to their ability to integrate both structural and textual knowledge. 
Among all baselines, our proposed MoR achieves the overall best performance with a substantial margin on average, with the first ranking on MAG and the second ranking on Amazon/Prime datasets. This demonstrates the effectiveness of our proposed mixture of structural and textual knowledge retrieval. 
Textual retrieval performs better on Amazon than on MAG, suggesting that Amazon queries rely more on textual knowledge. In contrast, its weaker performance on MAG is due to MAG's lower textual richness and stronger structural signals. This disparity aligns with the distribution analysis presented by~\citet{wu2024stark} and supports our hypothesis that queries in different TG-KB datasets require varying desires for textual and structural knowledge. Meanwhile, structural retrieval methods such as conventional knowledge graph-based ones perform poorly because they are designed for graphs with minimal textual information compared to TG-KBs.
Different from Amazon and MAG, all existing methods without supervised tuning (e.g., Ada-002) exhibit significantly lower performance on Prime. This is due to the extreme domain expertise required in biology, where word-count-based, pre-trained textual similarity-based, and even more powerful LLMs are all poorly applicable here. Through fine-tuning, MFAR$^{*}$ and our proposed MoR generally achieve better performance, demonstrating the necessity of domain-specific knowledge for answering queries in knowledge-intensive domains. 




\newpage
\subsection{Ablation Study}
After verifying the superiority of MoR, we conduct ablation studies to assess its different components, including module and feature ablation.

\subsubsection{Module Ablation}


To assess the contribution of each module in MoR, namely, Text Matching-based Retrieval, Neighborhood-Fetching-based Structural Retrieval, and Reranker, we conduct a series of ablation experiments. First, we remove the Reranker, resulting in the variant MoR$_{\text{w/o R}}$. On top of that, we further separately eliminate Text Retrieval and Structural Retrieval, yielding MoR$_{\text{w/o RT}}$ and MoR$_{\text{w/o RS}}$, respectively.
As shown in Table~\ref{tab-merged}, the complete MoR framework consistently achieves the highest performance across all datasets, demonstrating the synergistic effect of the Textual Retriever, Structural Retriever, and Reranker.
After removing Reranker, MoR$_{\text{w/o R}}$ exhibits a consistent performance drop across all datasets and evaluation metrics. This underscores the importance of the Reranker in refining retrieval by filtering noisy candidates from the intermediate reasoning stage. 
Eliminating Text Retrieval, i.e., MoR$_{\text{w/o RT}}$, leads to a notable performance drop on Amazon but an unexpected improvement on MAG. This suggests that while textual knowledge benefits Amazon, it introduces misleading hard negatives that compromise the ranking method (e.g., Ada-002) for MAG. Conversely, removing Structural Retrieval, MoR$_{\text{w/o RS}}$, results in a slight performance decrease further on MAG, reinforcing the importance of structural knowledge in MAG-related queries.
%
These results underscore the Reranker's crucial role in adaptively harmonizing, balancing, and selecting knowledge from both structural and textual retrieval experts.






\begin{table}[t!]
\small
\setlength\tabcolsep{4.5pt}
\centering
\begin{tabular}{l|ccc|cccc}
\toprule
\textbf{Dataset} &\textbf{TF} & \textbf{SF} & \textbf{TI} & \textbf{H@1} & \textbf{H@5} & \textbf{R@20} & \textbf{MRR} \\ \midrule
\multirow{7}{*}{\textbf{MAG}} 
& \cmark & \xmark & \xmark & 48.96 & 73.02 & 72.44 & 59.79 \\
&      \xmark            & \cmark       &         \xmark         & 18.79 & 41.91 & 52.85 & 29.84 \\
&        \xmark          &         \xmark         & \cmark       & 18.16 & 41.53 & 52.78 & 29.31 \\
\cline{2-8}
& \cmark       & \cmark       &    \xmark              & 58.04 & 77.14 & 74.42 & 66.75 \\
& \cmark       &        \xmark          & \cmark       & \underline{58.16} & \underline{77.59} & \underline{74.96} & \underline{66.85} \\
&          \xmark        & \cmark       & \cmark       & 17.93 & 38.01 & 46.79 & 27.48 \\
\cline{2-8}
& \cmark       & \cmark       & \cmark       & \textbf{58.19} & \textbf{78.34} & \textbf{75.01} & \textbf{67.14} \\ \midrule
\multirow{7}{*}{\textbf{Amazon}}    
& \cmark       &      \xmark            &       \xmark           & \underline{51.21} & \underline{74.05} & \underline{59.79} & \underline{61.27} \\
&        \xmark          & \cmark       &      \xmark            & 8.09  & 24.48 & 25.62 & 16.94 \\
&         \xmark         &      \xmark            & \cmark       & 5.84  & 16.62 & 12.94 & 11.57 \\
\cline{2-8}
& \cmark       & \cmark       &      \xmark            & 50.91 & 73.38 & 59.58 & 61.15 \\
& \cmark       &         \xmark         & \cmark       & 51.09 & 73.56 & 59.61 & 61.14 \\
&            \xmark      & \cmark       & \cmark       & 8.09  & 24.48 & 25.62 & 16.94 \\
\cline{2-8}
& \cmark       & \cmark       & \cmark       & \textbf{52.19} & \textbf{74.65} & \textbf{59.92} & \textbf{62.24} \\ \bottomrule
\end{tabular}
\caption{Ablation study investigating the importance of three features, Textual Fingerprint (\textbf{TF}), Structural Fingerprint (\textbf{SF}), and Traversal Identifier (\textbf{TI}), of the traversal trajectories used in our Structure-aware Reranker.}
\label{tab-feature-ablation}
\vspace{-2ex}
\end{table}



\subsubsection{Feature Ablation}
The above ablation study highlights the crucial role of Structure-aware Reranker in adaptively integrating structural and textual knowledge. To further analyze the contributions of its three key features, \textbf{Textual Fingerprint (TF)}, \textbf{Structural Fingerprint (SF)}, and \textbf{Traversal Identifier (TI)} defined in Section~\ref{sec-organizing}, we conduct a feature ablation analysis and report retrieval performance across different feature configurations in Table~\ref{tab-feature-ablation}.
%Overall and individual performance
Overall, using three features together yields the best performance on both MAG and Amazon, highlighting their synergistic effect. Individually, TF contributes the most and outperforms SF and TI on both datasets. 
The reason is that based on the definition in Section~\ref{sec-organizing}, TF directly captures the relevance between the query and the retrieved nodes along the trajectory, whereas SF and TI primarily characterize the structural patterns and retrieval types, serving more as complementary factors. Therefore, equipping TF with these complementary factors (i.e., SF or TI) yields around 10\% additional gains on MAG. This is because SF and TI help the reranker selectively emphasize the relevance scores given by TF for certain nodes along the path. However, this boost is not observed on Amazon. We hypothesize that the textual knowledge needed there is predominantly derived from the final node on each path, making the structural cues provided by SF and TI less beneficial and even prone to overfitting. A deeper analysis to further justify this hypothesis is in Section~\ref{sec-further}. Overall, these findings underscore the varying importance of structural features in ranking across datasets.



\begin{table}[t!]
\small
\setlength\tabcolsep{4pt}
\centering
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Feature}} & \multicolumn{3}{c|}{\textbf{MAG}} & \multicolumn{3}{c}{\textbf{Amazon}} \\

 & H@1 & R@20 & MRR & H@1 & R@20 & MRR \\
\midrule
Last Node & 49.91 & 73.49 & 59.92 & 50.36 & 59.62 & 61.05   \\
Full Path & \textbf{58.19} & \textbf{75.01} & \textbf{67.14} & \textbf{52.19} & \textbf{59.92} & \textbf{62.24}   \\
\bottomrule
\end{tabular}
\caption{Comparing reranking performance using last node in the retrieved trajectory and the whole trajectory.}
\label{tab-Reranker-ablation}
\vspace{-2ex}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\textwidth, height = 0.22\textwidth]{figures/query-pattern-20250215.png}
    \vspace{-4.5ex}
    \caption{Imbalance number of queries and performance of different retrievers across different logical structures.}
    \label{fig-analysis}
    \vspace{-3ex}
\end{figure}





\subsection{Further Analysis}\label{sec-further}
This section understands MoR’s behavior by examining three questions, each of which enriches our insight into MoR’s functionality and offers novel perspectives inspiring future query retrieval research.

\textbf{Do structure signals affect reranking?}
To assess the impact of trajectory information on the Reranker's decision-making, we introduce a node-based Reranker that constructs trajectory features using only TF/SF/TI of the last node. In Table~\ref{tab-Reranker-ablation}, the path-based Reranker outperforms the node-based variant, especially on MAG. This highlights the critical role of trajectory features/structural knowledge in reranking. The minor performance boost on Amazon after switching to the full path trajectory indicates its textual knowledge preference over the last node rather than the whole trajectory.


\textbf{How does MoR perform on different logical structures?}
Figure~\ref{fig-analysis} shows the average performance of MoR on each query group categorized by their logical structures, where "Others" refer to queries with undefined logical structures in~\citet{wu2024stark} MoR consistently outperforms structural and textual retrievers across different logical structures. Among all queries, MoR performs the worst on "P → P" queries due to the ambiguity, although well-known, uniquely caused by repeated product entities from multi-step traversal.
The average-performing ``Others" group underscores the utility of diverse planning strategies for the same query.
Lastly, the skewed query distribution and retrieval performance across planning patterns reflect the varying nature of real-world planning needs. We hope these insights inspire research on data-centric reasoning designs and error control of planning.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/heatmap-20250215.pdf}
    \vspace{-3ex}
    \caption{Saliency map visualization of query attention over three entities along the retrieved paths}
    \label{fig-map}
    \vspace{-2ex}
\end{figure}

\textbf{Does MoR indeed adaptively leverage the trajectory knowledge?} To understand how our proposed reranker prioritizes candidates in the Top-K results, we visualize the saliency map by computing the gradient of ranking scores with respect to the textual fingerprint (TF) of three nodes along the traversed path, which quantifies their importance for answering a given query. Figure~\ref{fig-map} illustrates this by analyzing trajectories for 100 ground-truth candidates across 100 queries on the Amazon and MAG datasets. Each dimension corresponds to a traversed node, with the final one representing the candidate itself. 
While the saliency score is concentrated in the last dimension for Amazon, 
MAG exhibits a more evenly distributed saliency pattern, where multiple nodes along the path contribute significantly to ranking score computation. This suggests that structural knowledge is more critical for answering queries in MAG, aligning with the previously observed lower performance of purely textual retrieval on MAG in Table~\ref{tab-merged}. Further case studies explain why the reranker attends different nodes for different queries. In Figure~\ref{fig-map}(a), the reranker favors the last two dimensions as the rich textual restriction (i.e., "Northwest Company..." and "NFL Seattle...") aids in identifying the correct node at the corresponding reasoning step, as discussed in Section~\ref{sec-reasoning}. The correct nodes, having higher similarity scores with the query, help guide the retrieval process toward the ground truth.
Conversely, in Figure~\ref{fig-map}(b),
since the first node ("University of Lausanne") helps narrow the search space and the last node ("frameless...") further filter candidates, both nodes have high saliency scores. Overall, our findings demonstrate that the reranker dynamically adapts its reliance on structural and textual knowledge depending on the dataset and query. 
