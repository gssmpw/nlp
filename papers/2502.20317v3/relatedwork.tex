\section{Related Work}
\label{sec-relatedwork}
\textbf{Retrieval-augmented Generation (RAG):}
RAG enhances generative tasks by retrieving relevant information from external knowledge sources~\cite{he2024g, gao2023retrieval} and has been widely used to improve question-answering ~\cite{liu2023knowledge}. With LLMs, RAG has been used for mitigating hallucinations~\cite{yao2023llm}, enhancing interpretability~\cite{gao2023chat}, and enabling dynamic knowledge updates~\cite{wang2024knowledge}. This work essentially leverages the idea of RAG to retrieve supporting entities from TG-KBs to contextualize answer generation. Depending on concrete types of knowledge being retrieved, existing retrievers can be categorized into structural and textual retrieval, which are reviewed next.


\noindent \textbf{Textual and Structural Retrieval:} Early textual retrieval models, such as TF-IDF and BM25~\cite{robertson2009probabilistic}, rely on lexical similarity and keyword matching~\cite{chen-etal-2017-reading,yang2019end,mao-etal-2021-generation}. Modern approaches address this limitation by learning dense representations~\cite{karpukhin-etal-2020-dense}. Beyond textual retrieval, structural retrieval leverages graph-based techniques to extract structured knowledge. Methods such as graph traversal~\cite{wang2024knowledge, Jiang2023StructGPTAG}, community detection~\cite{Edge2024FromLT}, and graph machine learning models, including graph neural networks~\cite{yasunaga2021qagnn, Mavromatis2024GNNRAGGN}, play a crucial role in structural retrieval. Our approach integrates the strengths of both textual and structural retrieval by infusing the mixture-of-expert philosophy into retrieval design.

Due to page limitation, a comprehensive version of the related work is attached in Appendix~\ref{app-comprehensive}.