\section{Introduction}
Text-rich Graph Knowledge Bases (TG-KBs), due to their structured representation of textual documents, ubiquitously store textual and structural knowledge~\cite{jin2024bridging}. For example, scholars retrieve relevant research from paper management systems to advance scientific discoveries where nodes represent papers and edges denote references~\cite{lu2024ai}. With large language models (LLMs)-powered generators approaching human intelligence in language comprehension and generation, retrieving supporting knowledge from TG-KBs to contextualize and ground generation has become increasingly crucial for correctly answering queries~\cite{gao2023retrieval, ni2025towards}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Motivation.pdf}
    \caption{(a) Textual retrieval and structural retrieval. (b) 
    The effectiveness of retrieval methods varies across different TG-KBs. (c) Within the same TG-KB, queries addressed by textual (i.e., $\mathcal{Q}^{\text{Text}}$) and structural retrieval (i.e., $\mathcal{Q}^{\text{Struct}}$) exhibit both overlaps and distinctiveness.}
    \label{fig-motivation}
    \vspace{-3ex}
\end{figure}


Since supporting knowledge in TG-KBs typically exhibits in both the textual and structural formats~\cite{jin2024bridging, kolomiyets2011survey}, retrieval methods should be tailored to both formats effectively as shown in Figure~\ref{fig-motivation}(a). Textual retrieval methods retrieve textual knowledge such as indexed documents based on its similarity to the given query and can be broadly categorized into lexical methods (e.g., BM25) and semantic methods (e.g., Contriever)~\cite{karpukhin-etal-2020-dense, izacardunsupervised}. Structural retrieval methods retrieve structural knowledge such as neighboring entities~\cite{Edge2024FromLT, Jiang2023StructGPTAG, wang2024knowledge} by applying graph traversal and graph machine learning~\cite{tian2024graph, yasunaga2021qagnn}. Despite the advancements in both textual and structural retrieval, they are often applied independently and fail to mutually reinforce each other. As shown by Figure~\ref{fig-motivation}(b), neither structural retrieval by following the logical structure of the query nor textual retrieval by conducting Top-K BM25 matching can achieve better performance on both Amazon and MAG datasets simultaneously.
 
To effectively retrieve both textual and structural knowledge from TG-KBs, recent works~\cite{xia2024knowledge, li2024multi} aggregate neighboring documents to fuse structural knowledge into textual narratives followed by textual retrieval, with \citet{xia2024knowledge} filtering irrelevant neighbors by their relations and \citet{li2024multi} weighted aggregating neighbors based on their fields. However, three challenges remain. First, rewording aggregated neighbors requires frequently invoking LLMs, resulting in prohibitive resources for long documents with exponentially growing neighbors. Second, structural signals humans use to form logical plans are completely discarded after neighbor aggregation. Third, rigid neighbor aggregation overlooks varying desires for structural and textual knowledge for different queries and TG-KBs. In Figure~\ref{fig-motivation}(c), even within MAG, queries answered by textual retrieval (i.e., $\mathcal{Q}^{\text{Text}}$) are different from queries answered by structural retrieval (i.e., $\mathcal{Q}^{\text{Struct}}$).

To address the above three challenges, we infuse the mixture-of-expert philosophy into retrieval design and propose a Mixture of Structural-and-Textual Retrieval (MoR) in Figure~\ref{fig-framework}. MoR begins with a planning module that generates planning graphs outlining query logics that preserve structural signals without rewording aggregated neighbors, overcoming the first and second challenges. Next, MoR interleaves structural traversal and textual matching in the reasoning module, enabling these two retrieval to reinforce each other. Finally, MoR devises a structure-aware reranker in the organization module that adaptively adjusts the importance of retrieved textual/structural knowledge, addressing the third challenge. Via Planning–Reasoning–Organizing, MoR intelligently retrieves structural and textual knowledge based on query logical structure. Our key contributions are:




\begin{itemize}[leftmargin=*]
    \vspace{-0.5ex}
    \item \textbf{Planning via Textual Graph Generation}: 
    We define retrieval planning as generating textual graphs that outline the logical structure, i.e., the plan, for identifying entities relevant to the query.

    \vspace{-0.5ex}
    \item \textbf{Reasoning via Mixture of Structural-and-Textual Traversal}: We devise a mixed traversal by interweaving textual matching and structural traversal to retrieve knowledge following query logical structure depicted by the generated plan.

    \vspace{-0.5ex}
    \item \textbf{Organizing via Structure-aware Rerank}: 
    With candidates obtained from mixed traversal, we propose a Structure-aware Rerank to select Top-K candidates based on their traversal trajectory.
\end{itemize}





