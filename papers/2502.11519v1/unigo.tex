%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%

\usepackage{graphicx}
\usepackage{subcaption} 
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{float}
\usepackage{balance}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}
\acmConference[WWW '25] {Proceedings of the ACM Web Conference 2025}{April 28--May 2, 2025}{Sydney, NSW, Australia.}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28--May 2, 2025, Sydney, NSW, Australia}
\acmISBN{979-8-4007-1274-6/25/04}
\acmDOI{10.1145/3696410.3714636}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\settopmatter{printacmref=true}
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{UniGO: A Unified Graph Neural Network for Modeling Opinion Dynamics on Graphs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hao Li}
\orcid{0009-0009-9322-0603}
\affiliation{%
  \institution{Wuhan University}
  \department{Electronic Information School}
  \city{Wuhan}
  \state{Hubei}
  \country{China}
  \postcode{430072}
}
\email{whulh@whu.edu.cn}

\author{Hao Jiang}
\orcid{0009-0006-5853-5544}
\authornote{Corresponds Author}
\affiliation{%
\institution{Wuhan University}
\department{Electronic Information School}
  \city{Wuhan}
  \state{Hubei}
  \country{China}
  \postcode{430072}
}
\email{jh@whu.edu.cn}

\author{Yuke Zheng}
\orcid{0009-0008-6338-882X}
\affiliation{%
\institution{Wuhan University}
\department{Electronic Information School}
  \city{Wuhan}
  \state{Hubei}
  \country{China}
  \postcode{430072}
}
\email{zyk389@whu.edu.cn}

\author{Hao Sun}
\orcid{0009-0001-1956-9921}
\affiliation{%
  \institution{Wuhan University}
  \department{Electronic Information School}
  \city{Wuhan}
  \state{Hubei}
  \country{China}
  \postcode{430072}
}
\email{2021202120053@whu.edu.cn}


\author{Wenying Gong}
\orcid{0009-0008-7659-4745}
\affiliation{%
  \institution{Wuhan University}
  \department{Electronic Information School}
  \city{Wuhan}
  \state{Hubei}
  \country{China}
  \postcode{430072}
}
\email{wenyinggong@whu.edu.cn}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Hao Li, Hao Jiang, Yuke Zheng, Hao Sun, and Wenying Gong}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Polarization and fragmentation in social media amplify user biases, making it increasingly important to understand the evolution of opinions. Opinion dynamics provide interpretability for studying opinion evolution, yet incorporating these insights into predictive models remains challenging. This challenge arises due to the inherent complexity of the diversity of opinion fusion rules and the difficulty in capturing equilibrium states while avoiding over-smoothing. This paper constructs a unified opinion dynamics model to integrate different opinion fusion rules and generates corresponding synthetic datasets. To fully leverage the advantages of unified opinion dynamics, we introduces UniGO, a framework for modeling opinion evolution on graphs. Using a coarsen-refine mechanism, UniGO efficiently models opinion dynamics through a graph neural network, mitigating over-smoothing while preserving equilibrium phenomena. UniGO leverages pretraining on synthetic datasets, which enhances its ability to generalize to real-world scenarios, providing a viable paradigm for applications of opinion dynamics. Experimental results on both synthetic and real-world datasets demonstrate UniGO's effectiveness in capturing complex opinion formation processes and predicting future evolution. The pretrained model also shows strong generalization capability, validating the benefits of using synthetic data to boost real-world performance.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
  <concept>
  <concept_id>10010405.10010455.10010461</concept_id>
  <concept_desc>Applied computing~Sociology</concept_desc>
  <concept_significance>500</concept_significance>
</concept>
<concept>
  <concept_id>10010147.10010257.10010293.10010294</concept_id>
  <concept_desc>Computing methodologies~Neural networks</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
<concept>
  <concept_id>10002950.10003714.10003727.10003728</concept_id>
  <concept_desc>Mathematics of computing~Ordinary differential equations</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
</ccs2012>
\end{CCSXML}
    
\ccsdesc[500]{Applied computing~Sociology}
\ccsdesc[500]{Computing methodologies~Neural networks}
\ccsdesc[500]{Mathematics of computing~Ordinary differential equations}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Opinion Dynamics, Graph Nerual Networks, Social Networks}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\section{Introduction}

Polarization in online social media fosters user bias and hostility, exacerbating societal divisions and undermining social cohesion. In recent years, the formation of opinions on social media platforms has emerged as an increasingly important issue. Opinion dynamics quantify individual views and model group opinion evolution during interactions using methods such as agent-based models and ordinary differential equations \cite{fusionsurvey}. As shown in \cref{fig1a}, in online social media scenarios, opinion interactions are constrained by an underlying graph topology. Opinion dynamics over graph structures have been extensively applied in such contexts, playing a significant role in social network analysis \cite{debate, mobilization}, marketing \cite{platforms, bibliometric}, recommendation systems \cite{trust}, and various other domains.

In contrast to data-driven opinion prediction models, opinion dynamics models focus on interpretability, providing mechanisms for opinion formation. Opinion dynamics explore the mechanisms of group opinion evolution by designing specific opinion fusion rules \cite{biased, analyzing, game}, such as the Friedkin-Johnsen (FJ) model, which considers individuals' stubbornness towards their own opinions \cite{fj}, and the Hegselmann-Krause (HK) model, which is based on a confidence threshold for others' opinions \cite{hk}. Additionally, opinion dynamics investigate equilibrium phenomena within the system, such as consensus, polarization, and fragmentation, implying that opinions cease to change over time \cite{boundedsurvey}. Opinion dynamics offer deep insights into the process of opinion formation. However, integrating these insights into data-driven prediction models remains an open challenge.

\begin{figure*}[ht]
  \centering
    \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=\textwidth]{fig1a.pdf}
    \caption{}
    \label{fig1a}
  \end{subfigure}\hfill
  \begin{subfigure}{0.5\textwidth}
    \includegraphics[width=\textwidth]{fig1b.pdf}
    \caption{}
    \label{fig1b}
  \end{subfigure}
  \caption{(a) Opinion Interactions in Different Scenarios. (b) A simple example illustrating the similarity between opinion update in opinion dynamics (Degroot model) and node embedding update in graph neural networks. }
  \label{fig1}
\end{figure*}

Inspired by works such as physics-guided neural networks and graph ODEs \cite{climate, Physicsguided, graphode}, recent research has explored the integration of neural networks with opinion dynamics \cite{sinn}. Graph neural networks (GNNs) have emerged as a natural choice for learning opinion dynamics on graphs. As shown in \cref{fig1b}, both opinion updates in dynamics and node embedding updates in GNNs fundamentally rely on iterative aggregation of neighbors' current states, where new node states are computed through combining a node's own state with its immediate neighbors' states. However, integrating insights from opinion dynamics into GNNs faces two primary challenges: (1) \textbf{Learning diverse opinion fusion rules.} Unlike dynamical systems in the physical world, opinion evolution data in social media is sparse and limited in availability. Using synthetic data generated from theoretical models to enhance model generalization has become an important research direction for data-driven methods in recent years. Different models design different opinion fusion rules based on distinct assumptions, allowing them to study the evolution of group opinions under specific rules. Integrating each of these assumptions into a single model is considerably limited. (2) \textbf{Capturing specific equilibrium phenomena.} Through multiple rounds of opinion updates, opinion dynamics effectively describe equilibrium phenomena during opinion evolution, while GNNs may encounter over-smoothing with a high number of layers \cite{oversmoothing}. Learning the equilibrium of opinions while avoiding over-smoothing remains a significant challenge.

To address the aforementioned challenges, this paper proposes a unified opinion dynamic to enhance model performance. By incorporating the characteristics of various opinion dynamics, the unified framework constructs opinion fusion rules based on four aspects: node attributes, edge attributes, community structure, and noise. Unlike traditional opinion dynamics models, this approach learns the complex intrinsic relationships between initial opinion formation and graph topology, enabling better predictions of future opinion evolution. Although the unified model does not directly analyze the impact of individual properties on opinion evolution, it facilitates the learning of complex opinion fusion rules by neural networks.

Based on the unified opinion dynamic, we propose UniGO, using a coarsen-refine mechanism that applies graph neural networks on the skeleton of the underlying graph. We utilize graph pooling to construct a skeleton structure from the original nodes to supernodes, and use graph neural networks on this skeleton to capture the equilibrium phenomena of opinions. Subsequently, a refine operation is used to reconstruct the features of the skeleton nodes back to the original nodes. This approach captures the equilibrium phenomena of opinion dynamics while ensuring that nodes remain distinguishable, thereby avoiding over-smoothing. In this manner, the gap between theoretical models of opinion dynamics and real-world opinion evolution is bridged. The main contributions are as follows:

\begin{enumerate}
\item We propose a unified opinion dynamic to generate complex synthetic data, enhancing the model's ability to learn the complex intrinsic relationships between opinion formation and graph topology, while incorporating complex opinion fusion rules.
\item We introduce UniGO, a graph nerual network model for modeling opinion evolution on graphs. By leveraging the coarsen-refine framework, we balance the equilibrium phenomena of opinion evolution while avoiding the over-smoothing problem commonly faced by graph neural networks.
\item We conduct experiments on both synthetic and real-world network datasets. The results show that UniGO outperforms existing methods in terms of prediction accuracy and generalization ability.
\end{enumerate}

\section{Related Works}

\subsection{Opinion dynamics model}

Opinion dynamics investigates the formation process of agent opinions in social systems over time by setting up fusion rules. The format of opinions, fusion rules, and underlying structures are pivotal components of opinion dynamics \cite{fusionsurvey}. This paper primarily concentrates on dynamic models grounded in continuous opinion forms, which offer a closer alignment with the real-world social systems\cite{boundedsurvey}.

Different opinion dynamics models assume different fusion rules and study the phenomena of opinion formation based on these rules \cite{sznajd}. Based on the assumption of opinion assimilation, the DeGroot model \cite{degroot} sets the fusion rule as a simple averaging form. Friedkin-Johnsen (FJ) model \cite{fj} introduces agent stubbornness to model situations where agent persist in their opinions in real-world scenarios. The Deffuant-Weisbuch (DW) \cite{dw} model and Hegselmann-Krause (HK) \cite{hk} model consider the phenomenon of bias assimilation \cite{frontiers}, where agents tend to accept opinions they are inclined to believe, introducing confidence thresholds where agents can only interact with others whose opinions are within a certain range. Subsequent research delves into more intricate fusion rules, including randomness in opinion fusion \cite{coevolving, hknoisy, asymptotic}, deceptive interactions in social networks \cite{deceptive}, group influence \cite{mepo}, and so forth.

In recent years, many studies have increasingly focused on the influence of underlying topology on opinion dynamics \cite{synchronization, altafini, fragility, community}. Unlike face-to-face offline social systems, agents in online social systems often only interact with a subset of agents. Through simple transformations, the above theoretical models can be effectively transferred to graph structures. These models utilize synthetic graphs and real-world graph data to study the impact of topologies on opinion dynamics.

These theoretical models have played a significant role in analyzing social systems. However, most opinion dynamics models can only capture certain aspects of opinion fusion in real-world scenarios and fail to model the complexity of opinion evolution. Integrating insights from opinion dynamics into data-driven models can effectively combine perspectives from both theory and empirical data.

\subsection{Data-driven methods for modeling opinion dynamics}

Benefiting from the data-driven paradigm, many researchers have studied learning the formation patterns of opinions from data. Some machine learning methods have recognized the importance of graph topology. The work \cite{transient} uses a linear model to capture transient changes in opinions to adapt to complex opinion scenarios. The work \cite{kmeans} extends the k-means method to social graphs, investigating the formation of opinion equilibrium from a community clustering perspective. AsLM \cite{aslm} proposed a linear influence model that explores how to estimate the influence strength of links in social graphs by observing the opinions evolving over time at the nodes. Additionally, this work considers the limitations of the interaction range of nodes, indirectly introducing the underlying topology. SLANT \cite{slant} describes users' latent opinions as continuous-time stochastic processes, where each expression of an opinion by a user is a noisy estimate of their current latent opinion. SLANT+ \cite{slant+} is a point-process-based framework for capturing the nonlinear dynamics of opinion fusion. It processes users' opinions and message timings as a temporal point process, influenced by the opinions and message timings of their neighbors. Some methods are inspired by the fusion rules in opinion dynamics. SINN \cite{sinn} incorporates the idea of \cite{climate, Physicsguided} and uses opinion dynamics to assist model training.

These approaches partially integrate knowledge from opinion dynamics models but fail to fully incorporate this knowledge into the internal mechanisms of the model. Additionally, these methods do not explicitly incorporate graph topology into the opinion fusion process, which is inconsistent with the interaction patterns observed in social media. A unified opinion dynamics model is proposed to integrate interaction rules from various classical opinion dynamics models. Furthermore, this work delves into learning opinion dynamics on graphs using graph neural networks and applies them to real-world data, further bridging the gap between theoretical models of opinion dynamics and real-world applications.

\section{Background}
This section introduces the concept of opinion dynamics on graphs, along with several significant theoretical models of opinion dynamics. Furthermore, the discussion covers the opinion fusion rules that each of these models focuses on. Then, in \cref{learn}, the connections between graph neural networks and opinion dynamics on graphs are explored, and the key challenges in learning opinion dynamics on graphs are summarized.

\subsection{Opinion dynamics models on graphs}

Opinion dynamics models describe the formation process of opinions in social systems. Classical opinion dynamics models are often formulated as difference equations to describe specific opinion fusion rules. The opinions of agents evolve over time steps, where the opinion at time \( t+1 \) is based on the fusion of opinions within the agents at time \( t \). With continuous iterations, the opinions of agents eventually converge to certain values, a process also known as equilibrium. By proposing more complex opinion fusion rules, opinion dynamics can effectively describe opinion formation phenomena in the real world, including consensus, polarization, and fragmentation.

In social systems in the real world, nodes can only interact with a small subset of others. Therefore, graph-based opinion dynamics are proposed to address such opinion formation constrained by underlying graph topology. Given a social graph \( \mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{X}) \), where \( \mathcal{V} \) is the set of nodes with \( |\mathcal{V}| = n \) representing the number of nodes, each node represents an agent. \( \mathcal{E} \) is the set of edges, with \( e_{ij} \in \mathcal{E} \) representing an edge between nodes \( v_{i} \) and \( v_{j} \) (such as friendships, follow relationships, etc.), and \( \mathcal{X} \) represents the opinions held by each node.

\paragraph{Node Attributes}
Some opinion dynamics models assume that nodes possess specific attributes that influence opinion fusion. For example, in the FJ model, each agent has a stubbornness coefficient, which represents the extent of the node's adherence to its own opinion.

\begin{equation}
  x_i (t+1) = (1-\lambda_i) x_i (0) + \lambda_i \sum_{v_j \in \mathcal{N}(v_i)} w_{ij} x_j (t).
\end{equation}
Here, \( \lambda_i \in [0,1] \) represents the weight assigned to social influences during opinion fusion, and \( 1-\lambda_i \) is the weight assigned to the agent's initial opinion value, i.e., the stubbornness of the nodes towards their initial opinion \( x_i(0)\).

\paragraph{Edge Attributes}  
Opinion dynamics based on confidence thresholds control the range of node interactions, essentially modifying the state of edges between nodes. For example, the HK model describes the influence of all neighbors' opinions within each agent's trust range. The condition for nodes to interact is that the difference in opinions between the two nodes must fall within a certain range, known as the confidence threshold \( d \). Specifically, agent \( v_i \) can interact with nodes whose opinion values lie within the interval \( [x_i-d, x_i+d] \), which is referred to as the confidence interval of \( v_i \). When \( |x_i(t) - x_j(t)| < d \), interaction occurs. 

\begin{equation}
  x_i (t+1) = \frac{1}{|\mathcal{N}^{\prime}_i(t)|} \sum_{j \in \mathcal{N}^{\prime}_i(t)} x_j (t).
\end{equation}

Here, \( \mathcal{N}^{\prime}_i(t) \) represents the set of nodes within the confidence threshold in the neighborhood of \( v_i \) at time \( t \), including \( v_i \) itself, and \( |\mathcal{N}^{\prime}_i(t)| \) represents the number of elements in set \( \mathcal{N}^{\prime}_i(t) \).

\paragraph{Community Structure}
Building on the aforementioned work, some opinion dynamics models consider the influence of community structure on opinion fusion. For example, certain models assume that the stubbornness coefficients and confidence thresholds are structurally correlated, meaning that nodes in different communities within the graph may have distinct stubbornness coefficients and confidence thresholds.
\begin{equation}
  \lambda_i = \lambda_c, d_i = d_c \quad \forall v_i \in \mathcal{C}_k
\end{equation}
Here, \( \mathcal{C}_k \) represents the \( k \)-th community, while \( \lambda_c \) and \( d_c \) denote the stubbornness coefficient and confidence threshold of nodes within the community, respectively.

\paragraph{Noise}
In real social media, the formation of user opinions is often influenced by various sources of noise. This noise may originate from algorithmic biases or the uncertainties of user behavior. To better simulate these effects, some opinion dynamics models introduce noise terms. In noise-considering models of opinion fusion \cite{hknoisy}, individuals may randomly change their opinions based on a certain probability, rather than merely fusing the opinions of their neighbors. Such noise can introduce new collective behaviors, making the model's behavior more complex and realistic.
\begin{equation}
    x_i(t+1) = \begin{cases} 
    \text{Randomly select an } x \in [0, L] & \text{with probability } m \\
    \frac{1}{|\mathcal{N}_i(t) |} \sum_{j \in \mathcal{N}_i (t)} x_j (t) & \text{with probability } 1-m
    \end{cases}
\end{equation}
Here, each individual has a probability \( m \) to make a random jump, after which the new opinion \( x_i(n+1) \) is randomly selected within the entire range \([0, L]\).

\subsection{Learning opinion dynamics on graphs}\label{learn}
When we aim to learn opinion dynamics on graphs, graph neural networks (GNNs) naturally become the preferred method for capturing this inductive bias. In this subsection, we explore the connections and differences between GNNs and graph-based opinion dynamics, and describe the key challenges in learning opinion dynamics on graphs. 

\paragraph{Opinion updates and node representation updates}
Consider a typical graph neural network, where node representations are updated by aggregating the information from neighboring nodes. The \( (k+1) \)-th layer can be defined as:

\begin{equation}
    \label{eq5}
    \mathbf{h}_i^{k+1} = {AGG}^{k+1} \left( \left\{ {MSG}^{k+1} (\mathbf{h}_j^{k}, \mathbf{h}_i^{k}) | v_j \in \mathcal{N}(v_i) \right\} \right),
\end{equation}

where \( {AGG}(\cdot ) \) is the aggregation function at layer \( k+1 \), \( {MSG}(\cdot) \) is the message passing function at layer \( k+1 \), \( \mathcal{N}(v_i) \) is the neighborhood of node \( v_i \), and \( \mathbf{h}_i^{k+1} \) is the representation of node \( v_i \) at layer \( k+1 \). Similarly, opinion dynamics on graphs can be written in a similar form:

\begin{equation}
    \label{eq6}
    x_i(t+1) = {FUS} \left( x_i(t), x_j(t) | j \in \mathcal{N}(v_i) \right),
\end{equation}

where \( x_i \) represents the opinion of \( v_i \) at time \( t+1 \), and \( {FUS}(\cdot) \) is the fusion function, computed based on the opinions of the node and its neighbors at time \( t \). This formal consistency indicates the close connection between GNNs and opinion dynamics on graphs. 

From the perspective of GNNs, each iteration of opinion dynamics allows nodes to receive information from an additional hop of their neighborhood. Graph-based opinion dynamics often require dozens to hundreds of steps to reach equilibrium, meaning each node eventually receives global information. Similarly, from the perspective of opinion dynamics, each layer of network updates can be seen as an iterative process in one time step. This also implies that the "time steps" in opinion dynamics do not correspond to the real-world concept of time, allowing the use of static graph neural networks for research.

\paragraph{Equilibrium and over-smoothing} 

Equilibrium is a crucial aspect studied in opinion dynamics theoretical models, referring to the state where, after multiple iterations, the opinions of nodes no longer change with each time step. In GNNs, over-smoothing refers to the phenomenon where, as the number of network layers increases, node representations become increasingly similar, eventually becoming indistinguishable and thus making it difficult to differentiate between different nodes \cite{oversmoothing}. The similarity between the two lies in the convergence of node states (representations). The fundamental difference is that equilibrium in opinion dynamics is related to the graph structure and fusion rules, reflecting the characteristics of the dynamic system, whereas over-smoothing is an undesirable effect during the training process.

The greatest challenge in using GNNs, which update based on local information, to learn opinion dynamics on graphs with global structural properties is learning a pattern of node convergence while avoiding the issue of over-smoothing.


\section{Method}
To address the aforementioned issues, we propose UniGO, a unified framework based on graph neural networks for modeling opinion evolution on graphs. As illustrated in \cref{fig2}, UniGO is trained on synthetic datasets generated under unified opinion dynamics and tested on real-world datasets. The data synthesis module consists of two components: graph construction and dynamics construction. Graph construction generates graph structures using various random graph generation methods, while dynamics construction generates evolutionary data based on unified opinion dynamics on the graph structure. 

The UniGO model comprises three parts: coarsening, dynamics simulation, and refinement. The coarsening module uses graph pooling methods to generate a skeleton of the original graph and construct aggregated representations of the supernodes. Then, in the dynamics simulation module, graph neural networks are employed to simulate the dynamics evolution on the supernodes. Finally, in the refinement module, the supernode representations are refined back to the original nodes to complete the simulation of opinion evolution on the graph. The coarsen-refine architecture allows learning the equilibrium results of opinion dynamics on graphs while avoiding over-smoothing.

\subsection{Problem definition}
Given a graph \( \mathcal{G}=(\mathcal{V}, \mathcal{E}, \boldsymbol{\mathcal{X}_{l}}) \), where \( \mathcal{V} \) is the set of nodes, \( \mathcal{E} \) is the set of edges, and \( \boldsymbol{\mathcal{X}_{l}} \in \mathbb{R}^{n \times t_{l}} \) represents the opinions of \( n \) nodes at the initial \( t_{l} \) time steps, with values in the range \( [0, 1] \). The objective is to learn a function \( F \) to predict the node opinions for the subsequent \( t_{h} \) time steps, \( \boldsymbol{\mathcal{X}_{h}} \in \mathbb{R}^{n \times t_{h}} \).

\begin{figure*}
  \centering
  \includegraphics[width=0.8\textwidth]{fig2.pdf}
  \caption{The framework of UniGO.}
  \label{fig2}
\end{figure*}

\subsection{Data Synthesis Module}  

The data synthesis module generates synthetic datasets that include various graph structures and opinion dynamics processes, addressing the scarcity of real-world opinion evolution data. Specifically, the dataset consists of two components: graph construction and dynamics construction. The graph construction component includes several random graph generation methods, such as ER random graphs, WS small-world graphs, and BA scale-free graphs\cite{ba, er, ws}, with each node receiving an initial opinion randomly. In the dynamics construction component, the unified opinion dynamics runs on the random graphs to generate the opinion evolution process:

\begin{equation}
  x_i^{(t+1)}=\alpha_i x_0^{(t)}+\left(1-\alpha_i\right)\left(\sum_{j \in \mathcal{N}_i} w_{i j} M_{i j}^{(t)} x_j^{(t)}\right)+\gamma_i \eta_i^{(t)}.
\end{equation}

Here, \( x_0^{(t)} \) represents the initial opinion of node \( i \) . \( \alpha_i \) is the stubbornness coefficient of node \( i \), \( \mathcal{N}_i \) represents the set of neighboring nodes of node \( i \), \( w_{ij} \) is the weight of the edge between nodes \( i \) and \( j \), and \( \eta_i^{(t)} \) is the noise for node \( i \) at time \( t \). \( M_{ij}^{(t)} \) indicates whether nodes \( i \) and \( j \) can interact in terms of opinion exchange, and is defined as follows:

\begin{equation}
  M_{ij}^{(t)} = \begin{cases} 
  1, & \text{if } |x_i^{(t)} - x_j^{(t)}| \leq d_i \\
  0, & \text{otherwise}.
  \end{cases}
\end{equation}
Here, \( d_i \) is the confidence threshold of node \( i \). Nodes \( i \) and \( j \) interact if the difference in their opinions is less than \( d_i \). 

In this way, traditional dynamics models can be effectively combined. For instance, when \( w_{ij} M_{ij}^{(t)} = \mathbf{1} \) and \( \gamma_i \eta_i^{(t)} = 0 \), the model reduces to the FJ model. If the stubbornness coefficients and confidence thresholds of nodes vary according to the community structure, the model represents an FJ and HK model that considers graph structure. Furthermore, by designing different noise functions, the model can flexibly simulate the process of opinion evolution under varying noise conditions. 

More importantly, this framework comprehensively incorporates different opinion fusion rules from various dynamics models, generating more realistic synthetic datasets for opinion dynamics. Although this framework cannot analyze equilibrium states and other properties in the traditional opinion dynamics paradigm, it can be used to train graph neural network-based models for predicting opinion evolution. Details of the data synthesis approach are provided in the \cref{app:synthetic}. The evaluation experiments on synthetic datasets are shown in \cref{app:valresult}

\subsection{UniGO Model}  
The UniGO model, under a coarsen-refine mechanism, uses graph neural networks to simulate the dynamics evolution on supernodes. The coarsening module coarsens the original graph through pooling methods to obtain the skeleton and construct aggregated representations of the skeleton supernodes. The dynamics evolution is simulated using a mean-aggregating graph neural network. The refinement module refines the supernode representations back to the original nodes to complete the simulation of opinion evolution on the graph.

\paragraph{Coarsening Module}  
Graph pooling methods are used to coarsen the original graph, obtaining the skeleton and constructing aggregated representations of the skeleton supernodes. First, a graph neural network is used to aggregate the dynamic representations of the nodes:


\begin{equation}
  \mathbf{x}_i^{k+1} = {AGG}^{k+1} \left( \left\{ {MSG}^{k+1} (\mathbf{x}_j^{k}, \mathbf{x}_i^{k}) | v_j \in \mathcal{N}(v_i) \right\} \right),
\end{equation}

here, \( {AGG}(\cdot) \) is the aggregation function at layer \( k+1 \), \( {MSG}(\cdot) \) is the dynamic representation function at layer \( k+1 \), and \( \mathcal{N}(v_i) \) represents the set of neighboring nodes of node \( v_i \). The initial node feature \(\mathbf{x}_i^{(0)} \) corresponds to the opinions from the first \( t_{l} \) time steps. 

Subsequently, following \cite{mempooling}, we use a multi-head attention-based soft clustering to compute the coarsened node representations. The core algorithm of this pooling layer is based on calculating distances from nodes to cluster centers and performing soft assignments. For each node \( \mathbf{x}_i \) and cluster center \( \mathbf{k}^{(h)}_j \) on each attention head, the distance \( d_{ij} \) is computed using the Euclidean distance, with a temperature parameter \( \tau \) introduced to adjust the influence of the distance:

\begin{equation}
d_{ij}^{(h)} = \left( 1 + \frac{\| \mathbf{x}_i - \mathbf{k}^{(h)}_j \|^2}{\tau} \right)^{-\frac{1+\tau}{2}},
\end{equation}

here, \( \mathbf{x}_i \in \mathbb{R}^{F} \) represents the feature vector of node \( i \), and \( \mathbf{k}^{(h)}_j \in \mathbb{R}^{F} \) represents the feature vector of cluster center \( j \) for the \( h \)-th head. \( \tau \) is a temperature parameter that controls the influence of the distance. The probability that node \( \mathbf{x}_i \) belongs to different clusters \( j \) is calculated by normalizing the distances. The probability \( S_{ij}^{(h)} \) of each node being assigned to different clusters is expressed as:

\begin{equation}
S_{ij}^{(h)} = \frac{d_{ij}^{(h)}}{\sum_{k=1}^{K} d_{ik}^{(h)}},
\end{equation}

here, \( S_{ij}^{(h)} \) represents the probability that node \( i \) is assigned to cluster \( j \). In the multi-head attention mechanism, the final soft assignment matrix \( \mathbf{S} \) is further processed through a convolution operation:

\begin{equation}
\mathbf{S} = \text{softmax}(\text{Conv2d}(\Vert_{h=1}^H \mathbf{S}^{(h)})) \in \mathbb{R}^{N \times K}.
\end{equation}

Finally, the node features \( \mathbf{X} \) are weighted and combined according to the assignment matrix \( \mathbf{S} \), resulting in the new node features \( \mathbf{H} \):

\begin{equation}
\mathbf{H} = \mathbf{S}^\top \mathbf{X} \mathbf{W} \in \mathbb{R}^{K \times F^{\prime}}
\end{equation}
Here, \( \mathbf{X} \in \mathbb{R}^{N \times F} \) represents the input node feature matrix, and \( \mathbf{W} \in \mathbb{R}^{F \times F^{\prime}} \) is the linear transformation matrix used to map the node features to the output space.

\paragraph{Dynamics Evolution Module}  
A mean-aggregating graph neural network is used to simulate the dynamics evolution process. Specifically, a graph neural network is employed to compute the dynamics evolution for each node, and these dynamics are then used to update the node features. A weighted mean aggregation approach is selected, with the specific formula given as follows:

\begin{equation}
  \mathbf{h}_i^{k+1} = \mathbf{W}_1\mathbf{h}_i^k + \mathbf{W}_2 \cdot \text{Mean}_{j\in\mathcal{N}(i)}\mathbf{h}_j^k,
\end{equation}

Here, \( \mathbf{W}_1 \) and \( \mathbf{W}_2 \) are learnable weight matrices. Finally, a linear layer is used to expand the supernodes to \( t_{h} \) time steps:

\begin{equation}
  \mathbf{Z} = \mathbf{H}^{K} \mathbf{W}_e + \mathbf{b}_e,
\end{equation}

here, \( K \) represents the number of layers in the graph neural network. \( \mathbf{H}^{K} \) is the evolved representation of the supernodes, \( \mathbf{W}_e \in \mathbb{R}^{t_l \times t_h} \) is the weight matrix, and \( \mathbf{b}_e \in \mathbb{R}^{t_h} \) is the bias vector. Through this linear transformation, the time dimension of the supernodes is expanded from \( t_l \) to \( t_h \), resulting in \( \mathbf{Z} \in \mathbb{R}^{K \times t_h} \).

\paragraph{Refinement Module}  
A refiner is used to refine the supernode representations back to the original nodes. First, the supernode representations are restored to the original nodes through the assignment matrix \( S \):

\begin{equation}
  \mathbf{Z}^{\prime} = \mathbf{S} \mathbf{Z}
\end{equation}

Here, \( \mathbf{S} \in \mathbb{R}^{N \times K} \) is the assignment matrix, and \( \mathbf{Z} \in \mathbb{R}^{K \times t_h} \) represents the evolved supernode representations. Subsequently, the original node representations \( \mathbf{X} \) and the restored representations \( \mathbf{Z}^{\prime} \) are passed through a shared refiner and concatenated to obtain the final node representations:

\begin{equation}
  \mathbf{Y} = \sigma ((\mathbf{X} \mathbf{W}_x || \mathbf{Z}^{\prime} \mathbf{W}_z) \mathbf{W}_y )
\end{equation}
Here, \( \mathbf{W}_x \) and \( \mathbf{W}_z \) are the weight matrices corresponding to \( \mathbf{X} \) and \( \mathbf{Z}^{\prime} \), respectively. \( \mathbf{W}_y \) is the weight matrix of the refiner, \( \sigma \) is the activation function, and \( \mathbf{Y} \) represents the final model prediction result.

\subsection{Training}  
The training loss of the model consists of two parts: the KL divergence of the assignment matrix and the mean squared error (MSE) of the node opinion prediction. The loss function is formulated as follows:

\begin{equation}
  \mathcal{L} = \lambda \mathcal{L}_{KL} + \mathcal{L}_{MSE}
\end{equation}

Here, \( \mathcal{L}_{KL} \) represents the KL divergence of the assignment matrix, \( \lambda \) is the weight of the KL divergence, and \( \mathcal{L}_{MSE} \) represents the mean squared error (MSE) of the node opinion prediction.

\section{Experiments}  
This section comprehensively evaluates UniGO's performance on both synthetic and real-world datasets. First, we introduce the synthetic and real-world datasets used in the experiments. In \cref{syex}, the evaluation focuses on UniGO's ability to fit opinion evolution on synthetic datasets. In \cref{realex}, the evaluation covers UniGO's ability to predict opinion formation on real-world datasets. The findings show that pretrained models on synthetic datasets can effectively predict opinions in real-world datasets, providing a viable paradigm for applying opinion dynamics to real-world scenarios.

\subsection{Experimental Setup}  

\paragraph{Datasets}\label{datasets}  
The synthetic dataset UniSyn is generated using the unified opinion dynamics. This synthetic dataset includes three types of random graph structures, and the dynamics involve various parameter combinations. The details of the synthetic dataset are presented in \cref{app:synthetic}.

A real-world dataset related to the Delhi elections \cite{transient} and four other datasets obtained from social media were used: Rumor, Food Safety, COVID, and U.S. Election. Corporate and bot accounts as well as irrelevant information were filtered out. The criteria for identifying active users varied by topic: the threshold for the U.S. election dataset was 50, for the food safety dataset it was 8, for the Israel-Palestine conflict rumor dataset it was 8, and for the COVID dataset it was 100. Edges were constructed based on retweet or comment relationships, and the largest connected component was retained. Detailed information on the real-world datasets is provided in \cref{app:real}.

\paragraph{Baselines}\label{baselines}  
In this experiment, there is a key difference between synthetic and real-world data: the graph structure in the synthetic dataset is not fixed, whereas the graph structure in the real-world dataset is fixed. Therefore, for the synthetic dataset, baselines that can handle flexible graph structures are selected, including \textbf{GCN} \cite{gcn}, \textbf{GAT} \cite{gat}, \textbf{GIN} \cite{gin}, \textbf{GraphSAGE} \cite{graphsage}, \textbf{SGC} \cite{sgc}, and \textbf{ODNET} \cite{odnet}. For the real-world dataset, several state-of-the-art dynamics prediction models are added, including \textbf{NCDN} \cite{ncdn}, \textbf{SINN} \cite{sinn}, and \textbf{DiskNet} \cite{disknet}. Detailed information about these models is provided in \cref{app:baselines}.


\subsection{Learning opinion formation on synthetic datasets}\label{syex}
\paragraph{Experimental setup}
In this subsection, UniGO's ability to learn opinion dynamics on UniSyn is evaluated. In this experiment, \( t_{l} \) is set to 10, and \( t_{h} \) is set to 90. For each dataset, only the first 100 time steps are used for the experiment. Unlike using a sliding window to construct multiple datasets, each graph structure and its corresponding dynamics evolution are sampled only once to learn the impact of graph structure and dynamics on evolution. For each graph, the mean squared error (MSE) of all nodes and the mean Wasserstein distance (MWD) are used to evaluate the model's performance. UniGO\(_c\) represents the UniGO model without the coarsen-refine architecture.

\begin{table}[h!]
  \centering
  \caption{Results on synthetic datasets ($\times 10^{-3}$).}
  \setlength{\tabcolsep}{3pt} 
  \label{synresult}
  \begin{tabular}{p{3cm} *{3}{c}} 
    \toprule
    Methods &  {MSE} & {MWD}  \\
    \midrule
    GCN   & 26.5 $\pm$ 3.2 & 153.6 $\pm$ 2.3  \\
    GAT   & 15.3 $\pm$ 3.3 & 102.6 $\pm$ 5.6  \\
    GIN   & 18.6 $\pm$ 2.6 & 90.3 $\pm$ 4.2 \\
    GraphSage & 7.1 $\pm$ 1.2 & 64.7 $\pm$ 1.2 \\
    SGC & 20.7 $\pm$ 1.8 & 123.4 $\pm$ 2.7 \\
    ODNET & 25.3 $\pm$ 3.4 & 177.1 $\pm$ 2.5 \\
    \midrule
    UniGO\(_c\) & 8.0 $\pm$ 1.8 & 66.8 $\pm$ 2.2 \\
    UniGO & \textbf{3.2 $\pm$ 0.8} & \textbf{27 $\pm$ 1.5} \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Results} 
Table \cref{synresult} shows the MSE and MWD for different methods on the synthetic dataset. Bold text indicates the best result. UniGO\(_c\) refers to UniGO without using the coarsen-refine architecture. UniGO effectively captures the opinion evolution process on synthetic data. Although ODNET adopts the confidence interval assumption from opinion dynamics, it fails to capture changes brought about by other opinion fusion rules, resulting in inferior performance compared to some common graph neural networks, such as GAT and GIN. It is noted that GraphSage's performance is second only to UniGO. One possible reason is that both UniGO\(_c\) and GraphSage use mean aggregation, but GraphSage employs a sampling technique when aggregating neighbor information. This allows GraphSage to better avoid overfitting during training, resulting in improved performance. The comparison between UniGO and UniGO\(_c\) also demonstrates the necessity of the coarsen-refine architecture.


\begin{table*}[!ht]
  \centering
  \caption{Results on real datasets ($\times 10^{-3}$).}
  \setlength{\tabcolsep}{1.5pt}
  \label{tab:realresultMSEMWD}
  \begin{tabular}{p{2cm} *{10}{c}} 
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{Delhi Election} & \multicolumn{2}{c}{U.S. Election} & \multicolumn{2}{c}{Rumor} & \multicolumn{2}{c}{Food Safety} & \multicolumn{2}{c}{COVID} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
    & MSE & MWD & MSE & MWD & MSE & MWD & MSE & MWD & MSE & MWD \\
    \midrule
    GNN & 10.6 $\pm$ 1.8 & 80.5 $\pm$ 0.5 & 11.2 $\pm$ 1.2 & 82.6 $\pm$ 1.9 & 35.6 $\pm$ 0.7 & 125.6 $\pm$ 1.4 & 15.6 $\pm$ 0.3 & 95.2 $\pm$ 1.6 & 9.6 $\pm$ 1.1 & 50.7 $\pm$ 0.8 \\
    GAT & 10.5 $\pm$ 0.4 & 79.7 $\pm$ 1.7 & 11.2 $\pm$ 0.9 & 81.9 $\pm$ 0.6 & 36.9 $\pm$ 1.5 & 123.5 $\pm$ 0.2 & 16.8 $\pm$ 1.8 & 89.5 $\pm$ 1.3 & 13.5 $\pm$ 0.7 & 79.4 $\pm$ 1.9 \\
    GIN & 12.4 $\pm$ 1.6 & 85.7 $\pm$ 0.8 & 9.5 $\pm$ 1.3 & 84.7 $\pm$ 1.1 & 37.8 $\pm$ 0.5 & 115.6 $\pm$ 1.7 & 20.8 $\pm$ 0.9 & 87.9 $\pm$ 0.4 & 12.1 $\pm$ 1.4 & 48.6 $\pm$ 0.6 \\
    GraphSage & 8.6 $\pm$ 0.7 & 54.6 $\pm$ 1.5 & 7.4 $\pm$ 0.3 & 72.3 $\pm$ 1.8 & 21.5 $\pm$ 1.2 & 100.2 $\pm$ 0.9 & 12.3 $\pm$ 1.6 & 70.4 $\pm$ 0.5 & 8.6 $\pm$ 1.0 & 30.9 $\pm$ 1.3 \\
    ODNET & 15.6 $\pm$ 1.1 & 90.5 $\pm$ 0.6 & 11.6 $\pm$ 1.9 & 78.2 $\pm$ 0.8 & 31.8 $\pm$ 0.4 & 105.4 $\pm$ 1.5 & 17.0 $\pm$ 1.2 & 75.9 $\pm$ 1.7 & 13.9 $\pm$ 0.3 & 38.1 $\pm$ 0.9 \\
    SINN & \underline{6.5 $\pm$ 1.4} & \underline{42.1 $\pm$ 0.7} & \underline{6.4 $\pm$ 1.0} & \underline{39.0 $\pm$ 1.6} & 19.8 $\pm$ 0.8 & 97.7 $\pm$ 1.3 & 9.5 $\pm$ 0.5 & 59.5 $\pm$ 1.9 & 5.9 $\pm$ 1.1 & 33.4 $\pm$ 0.2 \\
    NCDN & 10.6 $\pm$ 0.9 & 78.6 $\pm$ 1.2 & 9.7 $\pm$ 0.6 & 59.5 $\pm$ 1.4 & 18.7 $\pm$ 1.8 & 81.7 $\pm$ 0.3 & 10.0 $\pm$ 1.5 & 74.3 $\pm$ 0.7 & 4.6 $\pm$ 1.0 & 32.2 $\pm$ 1.6 \\
    DiskNet & 10.1 $\pm$ 0.5 & 76.4 $\pm$ 1.8 & 10.6 $\pm$ 1.3 & 74.5 $\pm$ 0.4 & 16.8 $\pm$ 1.7 & 72.4 $\pm$ 0.9 & 8.4 $\pm$ 1.1 & 42.9 $\pm$ 1.5 & 5.2 $\pm$ 0.6 & 42.2 $\pm$ 1.2 \\
    \midrule
    UniGO\(_c\) & 7.9 $\pm$ 0.6 & 41.8 $\pm$ 0.9 & 6.9 $\pm$ 0.6 & 52.9 $\pm$ 0.7 & 25.8 $\pm$ 2.0 & 108.6$\pm$ 1.1 & 15.4 $\pm$ 0.4 & 79.8 $\pm$ 1.6 & 8.9 $\pm$ 1.2 & 32.1 $\pm$ 1.5 \\
    UniGO\(_s\) & \textbf{4.0 $\pm$ 1.1} & \textbf{28.9 $\pm$ 0.7} & \textbf{5.2 $\pm$ 1.5} & \textbf{30.5 $\pm$ 1.8} & \underline{11.3 $\pm$ 0.4} & \underline{42.2 $\pm$ 1.3} & \underline{7.6 $\pm$ 0.6} & \underline{35.9 $\pm$ 1.0} & \textbf{4.0 $\pm$ 1.9} & \textbf{25.7 $\pm$ 0.2} \\
    UniGO & 7.4 $\pm$ 1.6 & 50.4 $\pm$ 0.8 & 6.8 $\pm$ 1.2 & 43.5 $\pm$ 0.5 & \textbf{8.7 $\pm$ 1.9} & \textbf{38.6$\pm$ 1.0} & \textbf{7.4 $\pm$ 0.3} & \textbf{30.8 $\pm$ 1.7} & \underline{4.2 $\pm$ 0.9} & \underline{31.8 $\pm$ 1.4} \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Learning opinion formation on real datasets}\label{realex}
\paragraph{Experimental Setup}  
Furthermore, the generalization ability and opinion formation prediction capability of UniGO are analyzed on real-world datasets. For real datasets, a sliding window approach is used to construct the dataset. Similarly, \( t_{l} \) is set to 10, and \( t_{h} \) is set to 90. Unlike synthetic datasets, the graph structure in real datasets is fixed, allowing multiple datasets to be constructed using the sliding window approach. This also makes the data suitable for most baselines. The dataset is split into training, validation, and test sets in a ratio of 0.7, 0.1, and 0.2, respectively.

Additionally, to validate the effectiveness of UniGO under a pretrain-finetune framework, the UniGO model trained on the UniSyn dataset is directly tested on the real-world dataset to evaluate its generalization capability. In the ablation study, UniGO\(_s\) represents the UniGO model without training on the UniSyn dataset, and UniGO\(_c\) represents the model without using the coarsen-refine mechanism.

\paragraph{Results}
Table \cref{tab:realresultMSEMWD} shows the MSE and MWD for different methods on the real-world dataset. Bold text indicates the best result, and underline indicates the second-best result. From the experimental results, it can be seen that UniGO\(_s\) performs well on smaller datasets, consistently achieving the best results, while UniGO performs better on relatively larger datasets. This is primarily because UniGO was not fine-tuned on the real-world data but directly tested on it, which led to suboptimal performance on smaller datasets. SINN, as an opinion dynamics-informed neural network model, performs well on most tasks, demonstrating the advantages of incorporating opinion dynamics into opinion prediction.

It is also observed that the time span and size of the dataset have a certain impact on the results. For the Delhi Election dataset, which has a shorter time span and smaller data volume, UniGO may have overfitted, resulting in less ideal performance. Almost all methods perform relatively worse on larger datasets. However, UniGO shows more stable performance. Similarly, under the coarsen-refine mechanism, the Graph Neural ODE-based model DiskNet performs worse than SINN on smaller datasets, possibly due to overfitting caused by the more complex Graph Neural ODE model compared to the MLP used in SINN. UniGO\(_c\) can be considered as a GraphSage model trained on the synthetic dataset. It can be seen that UniGO\(_c\) performs better than GraphSage on larger datasets, which may also be related to the overfitting phenomenon of the model.

Overall, the UniGO model shows good performance across different datasets. The UniGO model pretrained on synthetic data demonstrates stable performance across datasets of different sizes, also showing good generalization ability. The ablation experiments also demonstrate the necessity of synthetic datasets and the coarsen-refine mechanism.

\section{Conclusion}
This paper proposes UniGO, a unified opinion dynamics-based model for predicting opinion evolution on graphs, aiming to simulate and predict dynamic opinion formation. The unified opinion dynamics for neural network learning considers various opinion fusion rules to overcome the limitations of traditional opinion dynamics, which cannot simulate complex opinion fusion processes. By pretraining on synthetic datasets, UniGO effectively generalizes to real-world datasets, providing a viable paradigm for applying opinion dynamics in practical applications. This approach addresses challenges such as sparse real data, simplistic theoretical model assumptions, and over-smoothing in graph neural networks, combining the interpretability of theoretical models with the fitting ability of data-driven models, and offering a new direction for future research. Experimental results show that UniGO achieves good performance in opinion prediction tasks on both synthetic and real datasets. 

Limitations:
(1)Limitations of the Unified Opinion Dynamics. Although the proposed unified opinion dynamics can be applied to various opinion fusion rules, it still has certain limitations, such as the inability to handle coupled opinion dynamics or changes in the underlying graph structure over time. Future work will explore more general representations of opinion dynamics and apply them to more complex systems.
(2) Limitations of the Real Datasets. Larger-scale and more diverse real-world datasets could further verify the generalization capability of the model. Additionally, all real datasets used are based on text data, making the accurate transformation of text into opinion data an area worth investigating. Future research will focus on developing more accurate methods for generating opinion data from textual information.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{references}


\appendix
\section{Experimental details}
\begin{table*}[!ht]
  \centering
  \caption{Statistics of the real-world datasets.}
  \setlength{\tabcolsep}{3pt} 
  \label{tab:realset}
  \begin{tabular}{{lccccc}}
      \toprule
      datasets &  {nodes} & {edges} & {message}& {time span} & {time step} \\ 
      \midrule
      Delhi Election   & 548   & 5271  & 20026  & December 9, 2013 - December 15, 2013 & 280  \\
      U.S. Election    & 526   & 2482  & 10883  & December 1, 2023 - March 5, 2024 & 384  \\
      Rumor            & 10695 & 89248 & 201365 & October 7, 2023 - October 19, 2023 & 260 \\
      Food Safety      & 1390  & 4253  & 8506   & May 30, 2023 - July 14, 2023 & 276  \\
      COVID            & 894   & 21143 & 102635 & January 1, 2020 - March 31, 2020 & 364 \\
      \bottomrule
  \end{tabular}
  \end{table*}


\subsection{Build Synthetic Datasets}\label{app:synthetic}
The information of the datasets is shown in Table \ref{tab:syndataset}. BA, ER, and WS graph structures are used to generate random networks. The network size is set as \( n \in \{500, 1000, 2000, 3000\} \). For BA random graphs, the average degree is \( k \in \{2, 3, 4, 5\} \). For ER random graphs, the probability of any two nodes being connected is \( p \in \{0.1, 0.2, 0.3\} \). For WS random graphs, the average degree is \( k \in \{4, 6, 8\} \), and the probability of any two nodes being connected is \( p \in \{0.1, 0.2, 0.3\} \). For each set of parameters, five random network structures are generated to run the unified opinion dynamics model.

For the opinion dynamics model, the stubbornness parameter is set as \( \alpha \in \{0.3, 0.4, 0.5, 0.6, 0.7\} \), the confidence threshold is \( \epsilon \in \{0.1, 0.2, 0.3, 0.4\} \), and the noise parameter is \( \sigma \in \{0.05, 0.1, 0.15, 0.2, 0.25\} \). For models where parameters are determined based on network structure, the Louvain method is used for community detection, with the number of communities set to 5. For all parameters that need to be randomly selected, three levels are set: \( [0.1, 0.3] \), \( [0.2, 0.4] \), and \( [0.3, 0.5] \). On each graph structure, multiple opinion dynamics models are run, provided there are no conflicting parameters. Opinion dynamics are calculated using the Euler method, running for 200 time steps. If the average opinion change of nodes over 10 continuous time steps is within \( 1 \times 10^{-4} \), the model is considered to have converged. Finally, data with convergence steps greater than 80 are selected to generate the synthetic dataset.

\begin{table}[h!]
  \centering
  \caption{Synthetic datasets information.}
  \label{tab:syndataset}
  \begin{tabular}{l *{4}{c}} 
    \toprule
    Dataset & Number of & Average & Average  \\
             & Graphs & Nodes & Edges  \\
    \midrule
    UniSyn  & 3878 & 1652.4 & 16336.11 \\
    \bottomrule
    \end{tabular}
  \end{table}

\subsection{Synthetic Dataset Validation through Model Comparison}\label{app:valresult}
To validate the effectiveness of the synthetic dataset design, we conducted a validation study. The evaluation was performed using three opinion dynamics models: HK, FJ, and their combination (HK+FJ). The data was divided into two segments, with the initial 50\% of time steps allocated for training and the remaining 50\% for testing. Bayesian optimization was utilized to determine the optimal hyperparameters. The mean squared error on the test set served as the evaluation metric. The empirical results demonstrate that the integrated HK+FJ model achieves superior performance in approximating the dynamic equations compared to individual models. The quantitative results are presented in \cref{valresult}.

\begin{table}[h!]
  \centering
  \caption{Results on validation study ($\times 10^{-3}$).}
  \setlength{\tabcolsep}{3pt} 
  \label{valresult}
  \begin{tabular}{p{3cm} *{3}{c}} 
    \toprule
    Dataset &  HK & FJ & HK+FJ  \\
    \midrule
    Delhi Election    & 108.8  $\pm$ 3.2 & 107.8 $\pm$ 4.6 & 90.3 $\pm$ 4.2 \\
    U.S. Election   & 115.4 $\pm$ 3.8 & 119.6 $\pm$ 4.6 & 101.1 $\pm$ 2.8 \\
    Rumor   & 178.9 $\pm$ 7.8 &156.9 $\pm$ 8.3 & 116.1 $\pm$ 8.8 \\
    Food Safety & 354.7 $\pm$ 10.8 & 298.9 $\pm$ 16.7 & 153.8 $\pm$ 7.9 \\
    COVID & 58.9 $\pm$ 2.8 & 60.1 $\pm$ 2.6 & 46.6 $\pm$ 1.5 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Real datasets}\label{app:real}
We used a real dataset related to the Delhi elections \cite{transient} and four other datasets obtained from social media. We filtered out corporate and bot accounts as well as irrelevant information. The criteria for identifying active users varied by topic. The threshold for the U.S. election dataset was 50, for the food safety dataset it was 8, for the Israel-Palestine conflict rumor dataset it was 8, and for the COVID dataset it was 100. We constructed edges based on retweet or comment relationships and retained the largest connected component.

We used a combination of manual annotation and large language models \cite{stance} to label the opinions of the nodes. In the election dataset, user opinions corresponded to the political party they supported. In the food safety dataset, social network users discussed whether a food safety incident had occurred. In the rumor dataset, users chose whether or not to believe the rumors. In the COVID dataset, user opinions reflected their optimism or pessimism about the pandemic. We crawled data based on keywords, irrelevant information refers to content not related to the event. We extracted keywords using tf-idf and manually removed the top 50 irrelevant items based on keyword frequency. The table \cref{tab:realset} shows the information of these real datasets. 

In this study, we operationalize opinions as quantitative representations of user stances on specific topics, employing a continuous scale from 0 to 1 that maps negative to positive sentiments, aligning with established opinion dynamics frameworks. For opinion annotation, we utilized GPT-3.5 with a structured prompt engineering approach incorporating context-driven reasoning chains, formulated as: {event context} + {opinion detection target} + {step-by-step analysis through chain of thought}. Through empirical validation on a representative sample, the annotation accuracy of GPT-3.5 achieved 81.5\% concordance with human annotations. The datasets were systematically collected from two major social media platforms: Weibo (Food Safety and COVID datasets) and X/Twitter (U.S. Election and Rumor datasets). The data collection process involved keyword-based content extraction and user filtering based on engagement metrics. The network topology was constructed based on user interaction patterns, specifically retweets and comments, with preservation of the maximal connected component to ensure network cohesion. For ensuring data quality, we employed a hybrid approach to bot detection: manual verification for the Food Safety, U.S. Election, and COVID datasets, while implementing the Botometer\cite{botometer} framework for automated bot filtering in the Rumor dataset.

To evaluate model performance across datasets of varying scales while maintaining practical feasibility, we employed differential activity thresholds for dataset construction. Higher thresholds were implemented for the U.S. Election and COVID datasets to manage computational complexity, while a lower threshold of 8 was applied to the Food Safety network to preserve its inherently sparse structure. For the Rumor dataset, we strategically set the threshold to 8 to obtain a network of approximately 10,000 nodes, enabling comprehensive comparative analysis. Quantitative analysis of opinion dynamics revealed distinct temporal characteristics across datasets: the mean opinion transition steps were 5.3467 (Delhi Election), 9.5899 (U.S. Election), 1.4945 (Rumor), 3.3221 (Food Safety), and 2.0193 (COVID). Furthermore, we computed the mean absolute opinion shifts from initial to final states, yielding: 0.1548 (Delhi Election), 0.2597 (U.S. Election), 0.3022 (Rumor), 0.3932 (Food Safety), and 0.3021 (COVID), providing insights into the magnitude of opinion evolution across different contexts.

\subsection{Baselines}\label{app:baselines}
In the experiments, in addition to the commonly chosen graph neural networks—\textbf{GCN} \cite{gcn}, \textbf{GAT} \cite{gat}, \textbf{GIN} \cite{gin}, \textbf{SGC} \cite{sgc} and \textbf{GraphSAGE} \cite{graphsage}—the following baselines are used for comparison:

\begin{itemize}
  \item \textbf{ODNET} \cite{odnet}, a graph neural network inspired by HK opinion dynamics. It combines opinion dynamics with the message-passing mechanism in neural networks, using the concept of confidence thresholds to adjust influence weights between nodes.
  \item \textbf{NCDN} \cite{ncdn}, a continuous-time graph neural network model. Unlike traditional neural networks with discrete layers, NDCN processes GNN layers in continuous time using numerical integration, thereby capturing the dynamic changes in node states on the graph.
  \item \textbf{SINN} \cite{sinn}, a sociology-driven neural network that translates opinion dynamics theoretical models from sociology into ordinary differential equations (ODEs), using them as constraints within the neural network.
  \item \textbf{DiskNet} \cite{disknet}, a model that incorporates the renormalization group concept to construct a coarsening framework for networks, and uses graph neural ODEs to learn on the coarsened network.
\end{itemize}

\subsection{Additional experimental setups}\label{addset}
The experiments were conducted on a high-performance computing platform equipped with four NVIDIA RTX A6000 Graphics Processing Units and 128 GB of system memory, running a Linux operating system. The proposed UniGO framework incorporates several critical hyperparameters: the optimization learning rate, the cardinality of supernodes in graph pooling operations, the depth of the graph neural architecture for dynamics simulation, the regularization coefficient of the Kullback-Leibler divergence, and the dropout probability. The Adam optimizer's learning rate was systematically evaluated across \(\{0.01, 0.001, 0.0001\}\). The supernode cardinality in pooling operations was investigated across \(\{10, 20, 30, 40, 50, 60, 70, 80, 90, 100\}\), while the architectural depth was varied through \(\{2, 3, 4, 5, 6\}\) layers. The Kullback-Leibler divergence coefficient was examined across \(\{0, 0.2, 0.4, 0.6, 0.8, 1, 2, 5, 10\}\), and the dropout probability was evaluated at \(\{0.1, 0.2, 0.25\}\). The node proportion hyperparameter space was explored across \(\{0.001, \\ 0.005, 0.01, 0.05, 0.1, 0.2\}\). In the experimental evaluations presented in \cref{syex} and \cref{realex}, both UniGO and the baseline graph neural architectures were implemented with a three-layer configuration and 50 supernodes. For methodologies based on graph neural ODEs, the temporal discretization step \( dt \) was fixed at 0.1. The SINN framework \cite{sinn}, which eschews explicit network topology in favor of ODE-constrained opinion dynamics modeling with data stream inputs, was implemented according to its original specifications.

\subsection{Complexity Analysis}
In the coarsen module, the time complexity is \( O(NHKd) \), and space complexity is \( O(HNK) \), where \( H \) is the number of attention heads, \( N \) the number of nodes, \( K \) the number of clusters, and \( d \) the feature dimension. For simulating dynamics with GNNs, time complexity is \( O(LE'd + LV'd^2) \) and space complexity is \( O(V'd + E' + Ld^2) \), where \( L \) is the number of layers, \( E' \) the number of edges, \( V' \) the number of nodes, and \( d' \) the feature dimension after coarsening. In the refine module, time complexity is \( O(NKT + Nd''^2) \), and space complexity is \( O(d''^2 + NT + Nd'') \), where \( T \) is the number of timesteps and \( d'' \) the final output dimension.
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
