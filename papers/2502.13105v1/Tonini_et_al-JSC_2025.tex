\documentclass{siamart250106}
% Packages and macro definitions
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{dirtytalk}
\usepackage{cancel}
\usepackage{chngpage}
\parindent=5mm

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\newcommand{\numberset}{\mathbb}
\newcommand{\N}{\numberset{N}}
\newcommand{\R}{\numberset{R}}
\newcommand{\Z}{\numberset{Z}}
\newcommand{\Q}{\numberset{Q}}
\newcommand{\C}{\numberset{C}}
\newcommand{\Po}{\numberset{P}}
\newcommand{\B}{\numberset{B}}
\newcommand{\E}{\numberset{E}}
\newcommand{\g}{\mathbf}
\newcommand{\ti}{\tilde}
\renewcommand{\epsilon}{\varepsilon}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
%%%%%%

\newtheorem{thm}{Theorem}
\newtheorem{defn}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\renewcommand{\arraystretch}{1.2}

% Title, authors and headers
\title{Enhanced uncertainty quantification variational autoencoders for the solution of Bayesian inverse problems}

\author{Andrea Tonini \thanks{MOX, Dipartimento di Matematica, Politecnico di Milano, Milan, Italy (\email{andrea.tonini@polimi.it}).}
\and Luca Dede' \thanks{MOX, Dipartimento di Matematica, Politecnico di Milano, Milan, Italy.}}

\headers{Uncertainty quantification variational autoencoder}{Andrea Tonini and Luca Dede'}

\begin{document}

\maketitle

\begin{abstract}
Among other uses, neural networks are a powerful tool for solving deterministic and Bayesian inverse problems in real-time. In the Bayesian framework, variational autoencoders, a specialized type of neural network, enable the estimation of model parameters and their distribution based on observational data allowing to perform real-time inverse uncertainty quantification. In this work, we build upon existing research [Goh, H. et al., Proceedings of Machine Learning Research, 2022] by proposing a novel loss function to train variational autoencoders for Bayesian inverse problems. When the forward map is affine, we provide a theoretical proof of the convergence of the latent states of variational autoencoders to the posterior distribution of the model parameters. We validate this theoretical result through numerical tests and we compare the proposed variational autoencoder with the existing one in the literature. Finally, we test the proposed variational autoencoder on the Laplace equation.
\end{abstract}

\begin{keywords}
Bayesian inverse problem, variational autoencoder, deep learning, partial differential equations.
\end{keywords}

\begin{MSCcodes}
68T07, 62C10
\end{MSCcodes}

\section{Introduction}
%Neural networks are increasingly used to approximate the solution of partial differential equations (PDEs), effectively solving forward problems. Physics-Informed Neural Networks (PINNs) incorporate the governing PDEs into the loss function used for training, enabling the neural network to learn the underlying physics \cite{karniadakis2021physics}. Evolutional Deep Neural Networks (EDNNs) introduce a time evolution in the weights and biases of the neural network, enhancing its approximation capabilities at each timestep \cite{du2021evolutional, kast2024positional}. Latent Dynamics Networks (LDNets) represent a novel approach for approximating the PDE solutions \cite{regazzoni2024learning}. LDNets consists of a recursive neural network that generates outputs solving an ordinary differential equation (ODE), followed by another neural network that uses these outputs to compute the solutions of the PDE. These networks have showed improved approximation capabilities compared to established architectures, such as Long-Short Term Memory (LSTM) networks.\\
%In contrast to forward problems, inverse problems for PDEs have garnered significant interest in many fields, for example, clinical and geological applications. These problems are typically addressed by optimizing a loss function to estimate the PDE parameters (e.g., the diffusion coefficient) that best approximate known a priori solution data. Inverse problems are often ill-posed due to the limited availability of information and the large number of model parameters. To address this, the loss function is usually regularized. Tikhonov Networks (TNets) have been shown to effectively solve deterministic inverse problems \cite{nguyen2024tnet}. The loss function of TNets incorporates a priori parameter knowledge and a physics-aware regularization term. Furthermore, data randomization has been both theoretically and numerically shown to indirectly enforce additional regularization of the neural network's weights and biases. A different NETwork Tikhonov (NETT) has been applied to inverse problems involving images, using a data-driven regularization approach \cite{li2020nett}. Convolutional Neural Networks (CNNs) have also been extensively studied in the context of image-based inverse problems \cite{jin2017deep,adler2017solving,lucas2018using}.\\
%In a statistical framework, Bayesian inverse problems consider data noise, reflecting realistic scenarios. The goal in these problems is to estimate both the parameters of interest and their uncertainties. Solving Bayesian inverse problems is typically more computationally demanding than solving deterministic ones. Uncertainty Quantification Variational AutoEncoders (UQ-VAE) have been shown to effectively address Bayesian inverse problems by estimating the parameters of the posterior distribution \cite{goh2021solving}. This approach leverages a family of Jensen-Shannon divergences \cite{nielsen2010family} to construct an appropriate loss function. Such technique is inspired by generative modeling \cite{deasy2020constraining,sutter2020multimodal} where the posterior distribution of parameters is used to generate new samples.\\
Neural networks are emerging as an effective tool to approximate the solution of a mathematical model described by means of partial differential equations (PDE)\cite{karniadakis2021physics,du2021evolutional, kast2024positional,regazzoni2024learning,goswami2022physics,lu2021learning}. Observational data can usually be computed as a postprocessing of the PDE solution and the model parameters (e.g., the diffusion coefficient) need to be calibrated by solving inverse problems. Inverse problems are typically addressed by minimizing a loss function \cite{chong2013introduction,byrd1995limited,nelder1965simplex} to estimate the PDE parameters that best approximate known data. The solution of inverse problems is (much) more computationally demanding than solving the PDE itself, as it requires several model simulations. Moreover, inverse problems are often ill-posed, especially in presence of noisy data, due to the limited amount of data and the large number of model parameters. To address this issue, the loss function can be regularized\cite{tikhonov1977solutions}. Solving inverse problems by using neural networks as a surrogate model has garnered significant interest in several fields (e.g., applications in life sciences and computational medicine \cite{salvador2024digital,salvador2024whole,caforio2024physics}) as neural networks allow for fast model simulation, indirectly reducing the computational cost of minimizing the loss function.

Inverse problems can be solved more efficiently by using neural networks as surrogate solvers rather than surrogate models. Tikhonov Networks (TNets) have been shown to effectively solve deterministic inverse problems \cite{nguyen2024tnet}. The loss function of TNets incorporates a priori parameter knowledge and a physics-aware regularization term. Furthermore, data randomization has been both theoretically and numerically shown to indirectly enforce additional regularization of the neural network's weights and biases. Moreover, a new Tikhonov autoencoder neural network (TAEN) framework has improved the performances of TNets \cite{nguyen5081218taen}. A NETwork Tikhonov (NETT) has been applied to inverse problems involving images, using a data-driven regularization approach \cite{li2020nett}. Convolutional Neural Networks (CNNs) have also been extensively studied in the context of image-based inverse problems \cite{jin2017deep,adler2017solving,lucas2018using}.

In the statistical framework, Bayesian inverse problems consider noisy data, reflecting realistic scenarios. The goal is to estimate both the parameters of interest and their uncertainties, therefore making Bayesian inverse problems more computationally expensive than solving deterministic ones. Uncertainty Quantification Variational AutoEncoders (UQ-VAE) have been shown to effectively address Bayesian inverse problems by estimating the mean and covariance of the posterior distribution \cite{goh2021solving}. This approach leverages on a family of Jensen-Shannon divergences \cite{nielsen2010family} to construct a loss function. Jensen-Shannon divergence is extensively used in generative modeling \cite{deasy2020constraining,sutter2020multimodal} where the posterior distribution of parameters is used to generate new samples.

This work proposes an enhanced Uncertainty Quantification Variational AutoEncoder (eUQ-VAE) to improve the parameter distribution estimate of UQ-VAEs. We define a new loss function for the variational autoencoder and, when the forward problem is affine, prove the convergence of transformed latent states of the eUQ-VAE to the mean and covariance of the posterior distribution. Numerical tests validate this theoretical result, showcase the generalization capabilities of eUQ-VAEs and their performances on a Laplace problem.

The paper is organized as follows: \cref{sec:buildingLossFunction} describes the eUQ-VAE approach, including its derivation and theoretical results; \cref{sec:results} presents numerical tests; \cref{sec:concl} draws the conclusions.

\section{Bayesian inverse problems} \label{sec:buildingLossFunction}
We describe the general framework of Bayesian inverse problems and introduce a new method for solving them using variational autoencoders.

Let $U$ and $Y$ be random variables representing a set of parameters and a set of observational data, respectively. We assume that there exists a function $\mathcal F$ that maps the parameters to the observational data. Additionally, we assume that the observational data are subject to random noise $E$. The mathematical model under consideration is:
\begin{align}
Y = \mathcal F(U)+E. \label{eq:model}
\end{align}

The solution to a Bayesian inverse problem is the probability distribution of the parameters $\mathbf u \in \R^\mathrm{D}$ conditioned on a set of observational data $\mathbf y \in \R^\mathrm{O}$, where $D,O \in \N$. This is known as the posterior distribution $p_\mathrm{U|Y}(\mathbf u | \mathbf y)$. Note that $\mathcal F: \R^\mathrm{D} \to \R^\mathrm{O} $. Using the Bayes' theorem \cite{bayes1958essay}, the posterior distribution is typically maximized with respect to the parameters $\mathbf u$, yielding the Maximum A Posteriori estimate (MAP) $\mathbf u_\mathrm{MAP} \in \R^\mathrm{D}$, which represents the most likely parameter to generate the given $\mathbf y$. By Bayes' theorem:
\begin{align}
p_\mathrm{U|Y}(\mathbf u | \mathbf y) = \frac{p_\mathrm{U,Y}(\mathbf u, \mathbf y)}{p_\mathrm{Y}(\mathbf y)} = \frac{p_\mathrm{Y|U}(\mathbf y | \mathbf u)\,p_\mathrm{U}(\mathbf u)}{p_\mathrm{Y}(\mathbf y)},
\end{align} 
where $p_\mathrm{U,Y}(\mathbf u, \mathbf y)$, $p_\mathrm{Y}(\mathbf y)$, $p_\mathrm{U}(\mathbf u)$ and $p_\mathrm{Y|U}(\mathbf y | \mathbf u)$ represent the joint probability density function (pdf) of $U$ and $Y$, the marginal pdf of $Y$, the marginal pdf of $U$ and the conditional pdf of $Y$ given $U$ (likelihood), respectively. Assuming that the random noise $E$ is independent of $U$, the likelihood can be rewritten as:
\begin{align}
p_\mathrm{Y|U}(\mathbf y | \mathbf u) = p_\mathrm{E}(\mathbf y-\mathcal F(\mathbf u)), \label{eq:noiseProb}
\end{align}
where $p_\mathrm{E}$ is the pdf of the random variable E. Furthermore, we assume that $U \sim \mathcal N(\boldsymbol \mu_\mathrm{pr}, \Gamma_\mathrm{pr})$ and $E \sim \mathcal N(\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$, where $\boldsymbol \mu_\mathrm{pr} \in \R^{D}$ and $\boldsymbol \mu_\mathrm{E} \in \R^{O}$ represent the means of the prior and error distributions, respectively, and $\Gamma_\mathrm{pr} \in \R^\mathrm{D \times D}$ and $\Gamma_\mathrm{E} \in \R^\mathrm{O \times O}$ are the corresponding covariance matrices. In this work, we consider only symmetric positive definite (SPD) covariance matrices, as well as their inverses. Using \cref{eq:noiseProb}, the joint pdf of $U$ and $Y$can be rewritten as:
\begin{multline}
\notag p_\mathrm{U,Y}(\mathbf u, \mathbf y) = p_\mathrm{Y|U}(\mathbf y | \mathbf u)p_\mathrm{U}(\mathbf u)  =  \frac{1}{(2\pi)^\mathrm{\frac{O+D}{2}}|\Gamma_\mathrm{E}|^\mathrm{\frac{1}{2}} |\Gamma_\mathrm{pr}|^\mathrm{\frac{1}{2}}}\\
\text{exp}\left( -\frac{1}{2}\left( \norm{\mathbf y-\mathcal F(\mathbf u) -\boldsymbol \mu_\mathrm{E}}^\mathrm{2}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}} +  \norm{\mathbf u -\boldsymbol \mu_\mathrm{pr}}^\mathrm{2}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}} \right) \right),
\end{multline}
where $| \cdot |$ denotes the determinant of a matrix  and $\norm{\cdot}_\mathrm{\Gamma^\mathrm{-1}}$ is the norm induced by a SPD matrix $\Gamma^\mathrm{-1}$. The norms appearing in the exponent are adimensionalized thanks to the matrix weighted norm. Maximizing the posterior pdf $p_\mathrm{U|Y}(\mathbf u | \mathbf y)$, we obtain the MAP estimate:
\begin{align}
\mathbf u_\mathrm{MAP} = \underset{\mathbf u}{\mathrm{argmax}}\, p_\mathrm{U|Y}(\mathbf u | \mathbf y) = \underset{\mathbf u}{\mathrm{argmin}}\left( \norm{\mathbf y-\mathcal F(\mathbf u) -\boldsymbol \mu_\mathrm{E}}^\mathrm{2}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}} +  \norm{\mathbf u -\boldsymbol \mu_\mathrm{pr}}^\mathrm{2}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}\right).
\label{eq:mapMu}
\end{align}
The first term of the optimization problem, given by the likelihood model, represents the mismatch between observed data and the parameters to output map evaluations, while the second term incorporates prior knowledge about the parameters and acts as a regularization term. Solving \cref{eq:mapMu} typically requires an iterative gradient-based method, which can be computationally expensive. To perform inverse uncertainty quantification (UQ), the covariance matrix of the posterior distribution is often approximated using the Laplace approximation \cite{evans2000approximating}:
\begin{align}
\Gamma_\mathrm{Lap} = \left(J_\mathrm{\mathcal F}(\mathbf u_\mathrm{MAP})^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1}J_\mathrm{\mathcal F}(\mathbf u_\mathrm{MAP}) + \Gamma_\mathrm{pr}^\mathrm{-1} \right)^\mathrm{-1}, \label{eq:mapGamma}
\end{align}
where $J_\mathrm{\mathcal F}(\mathbf u_\mathrm{MAP}) \in \R^\mathrm{O \times D}$ is the Jacobian of $\mathcal F$ evaluated at $\mathbf u_\mathrm{MAP}$. Thus, computing $\Gamma_\mathrm{Lap}$ requires evaluating $J_\mathrm{\mathcal F}(\mathbf u_\mathrm{MAP})$, which is generally expensive.

The high computational cost of solving Bayesian inverse problems motivates the need for a more efficient methodology. We aim to develop an alternative optimization problem to find both $(\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$. Given the observational data $\mathbf y$, we seek to approximate $p_\mathrm{U|Y}(\mathbf u | \mathbf y)$ using a Gaussian distribution $q_\mathrm{\phi}(\mathbf u | \mathbf y) = \mathcal N (\boldsymbol \mu_\mathrm{post}(\mathbf y,\phi),\Gamma_\mathrm{post}(\mathbf y,\phi))$, where $\boldsymbol \mu_\mathrm{post} \in \R^\mathrm{D}$ and $\Gamma_\mathrm{post}\in \R^\mathrm{D \times D}$ are the mean and the covariance matrix that depend on $\mathbf y$ and $\phi$, a set of hyper-parameters. For brevity, we omit to explicit the dependence of $(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$ on $\mathbf y$ and $\phi$. To solve a Bayesian inverse problem we build a new loss function adapting the procedure proposed in \cite{goh2021solving}:
\begin{enumerate}
\item Measure the difference between the target posterior distribution $p_\mathrm{U|Y}(\mathbf u | \mathbf y)$ and its approximation $q_\mathrm{\phi}(\mathbf u | \mathbf y)$ by means of a family of Jensen-Shannon divergences (JSD) \cite{nielsen2010family}. This difference is the starting point to build the loss function to minimize (\cref{sec:JSD}).
\item Build an upper bound of the JSD, which will serve as the loss function for the optimization problem (\cref{sec:upperBound}).
\item  When $\mathcal F$ is affine, show that the stationary points of the upper bound with respect to $(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$ can be used to compute $(\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$. This step is essential to establish theoretical foundations for our method and to determine how to compute $(\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$ (\cref{sec:anRes}).
\item Solve the Bayesian inverse problem using an uncertainty quantification variational autoencoder (UQ-VAE) \cite{goh2021solving}, a type of neural network. In this case, $\phi$ represents the weights and biases of the UQ-VAE (\cref{sec:UQVAE}).
\end{enumerate}  

\subsection{Jensen-Shannon divergences} \label{sec:JSD}
We aim to redefine the optimization problem in \cref{eq:mapMu} because the computation of $\Gamma_\mathrm{Lap}$ involves evaluating $J_\mathrm{\mathcal F}(\mathbf u_\mathrm{MAP})$.

Consider two probability spaces $(\R^\mathrm{D},\mathcal B(\R^\mathrm{D}),\nu_\mathrm{i})$ $i=1,2$, where $\mathcal B(\R^\mathrm{D})$ is the Borel $\sigma$-algebra and $\nu_\mathrm{1},\nu_\mathrm{2}$ are two probability measures. Assume that $\nu_1$ and $\nu_2$ are absolutely continuous with respect to each other ($\nu_1 \equiv \nu_2$) and let $\lambda \equiv \nu_\mathrm{1}$ and $\lambda \equiv \nu_\mathrm{2}$. According to the Radon-Nikodym theorem, there exist two unique (up to sets of null measure) pdfs, $q(\mathbf u)$ and $p(\mathbf u)$ associated with $\nu_\mathrm{1}$ and $\nu_\mathrm{2}$, respectively, which are $\infty > q(\mathbf u),p(\mathbf u)>0$ $\lambda$-almost surely. The Kullback-Leibler divergence (KLD) \cite{kullback1951information} are then well-defined as:
\begin{align}
KL(q||p) = \E_\mathrm{q}\left[\log\left( \frac{q(\mathbf u)}{p(\mathbf u)}\right) \right], \qquad
KL(p||q) = \E_\mathrm{p}\left[\log\left( \frac{p(\mathbf u)}{q(\mathbf u)}\right) \right] ,
\end{align}
where $\E_\mathrm{q}$ denotes the expected value with respect to the probability measure $\nu_\mathrm{1}$ associated with $q$. It is a known property that $KL(q||p) \ge 0$, with equality holding if and only if $q=p$ $\nu_\mathrm{1}$-almost surely (since $\nu_\mathrm{1} \equiv \nu_\mathrm{2}$ the equality holds also $\nu_\mathrm{2}$-almost surely). We consider a family of JSDs defined for $\alpha \in [0,1]$:
\begin{align}
JS_\mathrm{\alpha}(q||p) = \alpha KL(q || (1-\alpha) q + \alpha p) + (1-\alpha) KL(p || (1-\alpha) q + \alpha p).
\label{eq:JSDfamily}
\end{align}
By varying $\alpha$, this formula allows us to explore a range of JSDs. Note that the second argument of each KLD is a convex combination of $q$ and $p$, ensuring that the KLDs remain well-defined. Additionally, the convex combination of the two KLDs ensures that $JS_\mathrm{\alpha}(q||p)\ge 0$. It is straightforward to verify that $JS_\mathrm{\alpha}(q||p)= 0$ if and only if $q = p$ $\nu_\mathrm{1}$- and $\nu_\mathrm{2}$-almost surely.

We state a theorem from \cite{goh2021solving} that is relevant to our discussion.
\begin{thm}
Let $\alpha \in [0,1]$. Then
\begin{align}
JS_\mathrm{\alpha}(q_\mathrm{\phi}(\mathbf u | \mathbf y) ||p_\mathrm{U|Y} (\mathbf u | \mathbf y) ) = &-\alpha \E_\mathrm{ q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log\left( \alpha + \frac{(1-\alpha)q_\mathrm{\phi}(\mathbf u | \mathbf y)}{p_\mathrm{U|Y}(\mathbf u| \mathbf y)}\right)  \right] \label{eq:JSDth}\\
\notag &+ \alpha \log(p_\mathrm{Y}(\mathbf y)) \\
\notag &- (1-\alpha) \E_\mathrm{p_\mathrm{U|Y}(\mathbf u| \mathbf y)} \left[  \log\left( \alpha + \frac{(1-\alpha)q_\mathrm{\phi}(\mathbf u | \mathbf y)}{p_\mathrm{U|Y}(\mathbf u| \mathbf y)}\right) \right]\\
\notag & -\alpha \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)} \left[ \log \left( \frac{p_\mathrm{U,Y}(\mathbf u, \mathbf y)}{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\right)  \right].
\end{align}
\end{thm}
 
This theorem provides an alternate expression of the family of JSDs. However, in practice, minimizing this expression with respect to $q_\mathrm{\phi}(\mathbf u | \mathbf y)$ is unfeasible because it involves the unknown $p_\mathrm{U|Y}(\mathbf u| \mathbf y)$ in a non-separable manner.

\subsection{Upper bound for the JSD family} \label{sec:upperBound}
We define an upper bound for \cref{eq:JSDth} that provides the loss function of the optimization problem we will solve.
\begin{cor}\label{cor:upperBound}
Let $\alpha \in [0,1)$. Then
\begin{subequations}
\begin{align}
JS_\mathrm{\alpha}(q_\mathrm{\phi}(\mathbf u | \mathbf y) ||p_\mathrm{U|Y} (\mathbf u | \mathbf y) )  \le & -\alpha KL(q_\mathrm{\phi}(\mathbf u|\mathbf y) || p_\mathrm{U|Y}(\mathbf u|\mathbf y)) \label{eq:cor1term}\\
&+\alpha \log(p_\mathrm{Y}(\mathbf y)) -\log(1-\alpha)\\
&+(1-\alpha)KL(p_\mathrm{U|Y}(\mathbf u|\mathbf y) || q_\mathrm{\phi}(\mathbf u|\mathbf y))\\
&-\alpha \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log(p_\mathrm{Y|U}(\mathbf y | \mathbf u)) \right] + \alpha KL(q_\mathrm{\phi}(\mathbf u | \mathbf y) || p_\mathrm{U}(\mathbf u)).
\end{align}
\end{subequations}
\end{cor}
 
The proof is given in \cite{goh2021solving}. Moving the right term of \cref{eq:cor1term} to the left hand side of the inequality, we obtain that up to constants in $(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$:
\begin{align}
-(1-\alpha) \E_\mathrm{p_\mathrm{U|Y}(\mathbf u|\mathbf y)}\left[ \log (q_\mathrm{\phi}(\mathbf u|\mathbf y)) \right] -\alpha \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log(p_\mathrm{Y|U}(\mathbf y | \mathbf u)) \right]
 + \alpha KL(q_\mathrm{\phi}(\mathbf u | \mathbf y) || p_\mathrm{U}(\mathbf u)) \label{eq:upperBound}
\end{align}
provides an upper bound to:
\begin{align}
JS_\mathrm{\alpha}(q_\mathrm{\phi}(\mathbf u | \mathbf y) ||p_\mathrm{U|Y} (\mathbf u | \mathbf y) ) + \alpha KL(q_\mathrm{\phi}(\mathbf u|\mathbf y) || p_\mathrm{U|Y}(\mathbf u|\mathbf y)). \label{eq:lowerBound}
\end{align}
Observe that the term \cref{eq:lowerBound} is still bigger than or equal to $0$ and it is null if and only if $q_\mathrm{\phi}(\mathbf u|\mathbf y) = p_\mathrm{U|Y}(\mathbf u|\mathbf y)$ almost surely as it holds for the JSD family \cref{eq:JSDfamily}.

We analyze each term of \cref{eq:upperBound}. Using the definition of the involved pdfs, the definition of expected value, the Bayes' theorem and rewriting the expected value in the last equality, we have:
\begin{align}
 - \E_\mathrm{p_\mathrm{U|Y}(\mathbf u|\mathbf y)}  \left[ \log (q_\mathrm{\phi}(\mathbf u|\mathbf y)) \right] & = \frac{D}{2}\log(2\pi)+\frac{ \log |\Gamma_\mathrm{post}|}{2} +  \frac{1}{2}\E_\mathrm{p_\mathrm{U|Y}(\mathbf u|\mathbf y)} \left[ \norm{\boldsymbol \mu_\mathrm{post}-\mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} \right]  \label{eq:pUexpval}\\
\notag & \lesssim  \log |\Gamma_\mathrm{post}| + \int_\mathrm{\R^\mathrm{D}} \norm{\boldsymbol \mu_\mathrm{post} - \mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} p_\mathrm{U|Y}(\mathbf u | \mathbf y) d\mathbf u \\
\notag & =  \log |\Gamma_\mathrm{post}| + \int_\mathrm{\R^\mathrm{D}} \norm{\boldsymbol \mu_\mathrm{post}-\mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} \frac{ p_\mathrm{E}(\mathbf y-\mathcal F(\mathbf u)) p_\mathrm{U}(\mathbf u)}{p_\mathrm{Y}(\mathbf y)} d\mathbf u \\
\notag & \lesssim \log |\Gamma_\mathrm{post}| + \int_\mathrm{\R^\mathrm{D}} \norm{\boldsymbol \mu_\mathrm{post}-\mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} 
e^\mathrm{-\frac{1}{2}\norm{\mathbf y-\mathcal F(\mathbf u) -\boldsymbol \mu_\mathrm{E} }_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}}
p_\mathrm{U}(\mathbf u) d\mathbf u \\
\notag & \le \log |\Gamma_\mathrm{post}| + \E_\mathrm{p_\mathrm{U}(\mathbf u)}\left[  \norm{\boldsymbol \mu_\mathrm{post} - \mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} \right]\\
\notag &\sim \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right),
\end{align}
where $\lesssim$ and $\sim$ indicate an inequality or an equality where constants in $(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$ are ignored and $\mathrm{tr}$ is the trace operator.

Using $p_\mathrm{Y|U}(\mathbf y | \mathbf u) = p_\mathrm{E}(\mathbf y-\mathcal F(\mathbf u))$ and rewriting the expected value, the second term of \cref{eq:upperBound} is estimated as:
\begin{align}
-\E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log(p_\mathrm{Y|U}(\mathbf y | \mathbf u)) \right]  &= \frac{O}{2}\log (2 \pi) + \frac{\log|\Gamma_\mathrm{E}|}{2}+ \frac{1}{2} \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right] \label{eq:qUexpval} \\
\notag &  \lesssim \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right] \\
\notag &  =  \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \mathcal F(\mathbf u) \right]}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr} \left( \Gamma_\mathrm{E}^\mathrm{-1} \mathrm{Cov}_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \mathcal F(\mathbf u)  \right] \right),
\end{align}

%where $\mathrm{Cov}_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}$ is the covariance matrix of a random variable computed with respect to the pdf $q_\mathrm{\phi}(\mathbf u | \mathbf y)$.
For the last term of \cref{eq:upperBound}, by using the definition of the pdfs and of the expected value, we get:
\begin{align}
\notag KL(q_\mathrm{\phi}(\mathbf u | \mathbf y) || p_\mathrm{U}(\mathbf u)) &= \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log \left(\frac{q_\mathrm{\phi}(\mathbf u | \mathbf y)}{ p_\mathrm{U}(\mathbf u)} \right) \right]\\
\notag & = \frac{1}{2} \left( \log \frac{|\Gamma_\mathrm{pr}|}{|\Gamma_\mathrm{post}|} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right) + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} -D  \right)\\
\notag & \lesssim -\log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right) .
\end{align}

Combining all the terms, the upper bound of \cref{eq:lowerBound} up to contants in $(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$ is given by:
\begin{gather}
\notag (1-\alpha) \left( \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
%& \alpha \left(  \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \mathcal F(\mathbf u) \right]}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr} \left( \Gamma_\mathrm{E}^\mathrm{-1} \mathrm{Cov}_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \mathcal F(\mathbf u)  \right] \right) \right) + \label{eq:minProblem}\\
\alpha  \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right] + \label{eq:minProblem}\\
%& \alpha \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right]  + \label{eq:minProblem}\\
\notag \alpha \left(  -\log |\Gamma_\mathrm{post}|  + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right).
\end{gather}
In what follows, we will look for the minimum of \cref{eq:minProblem}.

\emph{Remark.} While the minimum of upper bound  \cref{eq:minProblem} may not exactly match the minimum of the lower bound \cref{eq:lowerBound}, a relationship between the two minima will be discussed in the next section.

%\emph{Remark.} We can study also the case $\alpha = 1$ by adapting the result of \cref{cor:upperBound}.
%\begin{cor}
%Let $\alpha = 1$. Then
%\begin{align}
%\notag KL(q_\mathrm{\phi}(\mathbf u | \mathbf y) ||p_\mathrm{U|Y} (\mathbf u | \mathbf y) ) & = \log(p_\mathrm{Y}(\mathbf y)) -\E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \log(p_\mathrm{Y|U}(\mathbf y | \mathbf u)) \right] \\
%&+ KL(q_\mathrm{\phi}(\mathbf u | \mathbf y) || p_\mathrm{U}(\mathbf u)).
%\end{align}
%\end{cor}
%Then, we can proceed in the same way as before to build the loss function for the case $\alpha = 1$ and to retrieve \cref{eq:minProblem} where $\alpha$ has been imposed to be equal to $1$.\\
\emph{Remark.} To ensure that $\Gamma_\mathrm{post}$ is SPD, we use its lower triangular Cholesky factor $C_\mathrm{post} \in \R^\mathrm{D \times D}$ with positive diagonal entries, such that $\Gamma_\mathrm{post} = C_\mathrm{post} C_\mathrm{post}^\mathrm{T}$.

\subsection{Upper bound stationary points} \label{sec:anRes}
We now prove that the stationary points of \cref{eq:minProblem} are related to $(\mathbf u_\mathrm{MAP},\Gamma_\mathrm{Lap})$ when $\mathcal F$ is affine.
\begin{thm} \label{thm:convergence}
Let us assume that $\mathcal F(\mathbf u) = F \mathbf u + \mathbf f$, where $F \in \R^\mathrm{O\times D}, \mathbf f \in \R^\mathrm{O}$, and $\mathbf y \in \R^\mathrm{O}$. Assume Gaussian prior and noise models $\mathcal N(\boldsymbol \mu_\mathrm{pr}, \Gamma_\mathrm{pr})$ and $\mathcal N (\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$, respectively. Then the posterior distribution $p_\mathrm{U|Y}(\mathbf u| \mathbf y) = \mathcal N (\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$, where:
\begin{align}
\mathbf u_\mathrm{MAP} &= \Gamma_\mathrm{Lap} \left( F^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1} \left(  \mathbf y - \mathbf f - \boldsymbol \mu_\mathrm{E} \right) +\Gamma_\mathrm{pr}^\mathrm{-1} \boldsymbol \mu_\mathrm{pr}  \right) , \label{eq:mapMuAffine}\\
\Gamma_\mathrm{Lap} &= \left( F^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1}F + \Gamma_\mathrm{pr}^\mathrm{-1} \right)^\mathrm{-1}. \label{eq:mapGammaAffine}
\end{align}

Let $\alpha \in \left[ \frac{1}{2},1 \right)$ and suppose that the pdf $q_\mathrm{\phi}(\mathbf u | \mathbf y)$ is $\mathcal N(\boldsymbol \mu_\mathrm{post}, \Gamma_\mathrm{post})$, where $\Gamma_\mathrm{post} = C_\mathrm{post}C_\mathrm{post}^\mathrm{T}$ and $C_\mathrm{post}$ is a lower triangular matrix with positive diagonal entries. Then, the stationary points $(\hat {\boldsymbol \mu}_\mathrm{post}, \hat{C}_\mathrm{post})$ (with $\hat{\Gamma}_\mathrm{post} = \hat{C}_\mathrm{post}\hat{C}_\mathrm{post}^\mathrm{T}$) of the loss function
\begin{align}
\notag L(\boldsymbol \mu_\mathrm{post},C_\mathrm{post})  =& (1-\alpha) \left( \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
& \alpha \left(  \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - F \boldsymbol \mu_\mathrm{post} -\mathbf f }_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr} \left( \Gamma_\mathrm{E}^\mathrm{-1} F\Gamma_\mathrm{post}F^\mathrm{T} \right) \right) + \label{eq:minProblemAffine}\\
\notag &\alpha \left(  -\log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right)
\end{align}
satisfy
\begin{align}
\mathbf u_\mathrm{MAP} &= \frac{1-\alpha}{\alpha} \Gamma_\mathrm{Lap} \hat{\Gamma}_\mathrm{post}^\mathrm{-1} (\hat{\boldsymbol \mu}_\mathrm{post}-\boldsymbol \mu_\mathrm{pr}) + \hat{\boldsymbol \mu}_\mathrm{post}, \label{eq:muMap}\\
\Gamma_\mathrm{Lap} &= \hat{\Gamma}_\mathrm{post} A^\mathrm{-1} \hat{\Gamma}_\mathrm{post}, \label{eq:gammaMap} 
\end{align}
where
\begin{align}
A =  \frac{1-\alpha}{\alpha}\left( (\hat{\boldsymbol \mu}_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})(\hat{\boldsymbol \mu}_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})^\mathrm{T} + \Gamma_\mathrm{pr} \right) - \frac{1-2\alpha}{\alpha}\hat{\Gamma}_\mathrm{post}. \label{eq:Amatrix}
\end{align}
\end{thm}

% \emph{Remark.} For $\alpha = 1$ we get $\hat{\boldsymbol \mu}_\mathrm{post} = \mathbf u_\mathrm{MAP}$ and $\hat{\Gamma}_\mathrm{post} = \Gamma_\mathrm{Lap}$. Therefore, we obtain the desired solution without the need for transformations in the stationary point of \cref{eq:minProblemAffine}.\\
%{\color{red}\emph{Remark.} This theorem explicits the relationship between the stationary points of \cref{eq:minProblemAffine} and $(\mathbf u_\mathrm{MAP},\Gamma_\mathrm{Lap})$. Nonetheless, the stationary points could not be obtained minimizing \cref{eq:minProblemAffine} because
%, except for $\alpha = 1$,
% $\hat{\Gamma}_\mathrm{post}$ could be outside of the domain of the loss function restricted to SPD matrices. For example, there could be a stationary point of \cref{eq:minProblemAffine} for $\hat{\Gamma}_\mathrm{post}$ invertible, but not SPD. Retrieving from \cref{eq:gammaMap} an explicit expression for $\hat{\Gamma}_\mathrm{post}$ can solve this problem.} \\
%{\color{red}\emph{Remark.} The theorem does not prove that the stationary points achieved are minima. Therefore we have no guarantees that optimizing \cref{eq:minProblemAffine} we  get to a stationary point. A study of the constrained optimization problem \cref{eq:minProblemAffine} can solve this issue.}
\begin{proof}
The posterior distribution is $\mathcal N (\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$ by Theorem 6.20 of \cite{stuart2010inverse}.

Deriving $L$ with respect to $\boldsymbol \mu_\mathrm{post}$ and leveraging the symmetry of the covariance matrices, we find:
\begin{align}
\notag \frac{\partial L}{\partial \boldsymbol \mu_\mathrm{post}} =& 2(1-\alpha)\Gamma_\mathrm{post}^\mathrm{-1}(\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})\\
\notag &-2\alpha F^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1}(\mathbf y - \boldsymbol \mu_\mathrm{E} - F \boldsymbol \mu_\mathrm{post} -\mathbf f )\\
\notag & +2\alpha \Gamma_\mathrm{pr}^\mathrm{-1}(\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr}).
\end{align}
Using  \cref{eq:mapMuAffine} and \cref{eq:mapGammaAffine}, we get:
\begin{align}
\frac{\partial L}{\partial \boldsymbol \mu_\mathrm{post}} = 2(1-\alpha)\Gamma_\mathrm{post}^\mathrm{-1}(\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})+2\alpha \Gamma_\mathrm{Lap}^\mathrm{-1}(\boldsymbol \mu_\mathrm{post}- \mathbf u_\mathrm{MAP}). \label{eq:lossMu}
\end{align}
Setting $\frac{\partial L}{\partial \boldsymbol \mu_\mathrm{post}} = \mathbf 0$ gives:
\begin{align}
\mathbf u_\mathrm{MAP} = \frac{1-\alpha}{\alpha} \Gamma_\mathrm{Lap}\hat{\Gamma}_\mathrm{post}^\mathrm{-1}(\hat{\boldsymbol \mu}_\mathrm{post}- \boldsymbol \mu_\mathrm{pr}) + \hat{\boldsymbol \mu}_\mathrm{post}. \label{eq:stationaryMu}
\end{align}
Notice that we can divide by $\alpha$ since $\alpha \ge \frac{1}{2}$. 

Similarly, deriving $L$ with respect to $C_\mathrm{post}$ and using the symmetry of the covariance matrices, we get:
\begin{align}
\notag \frac{\partial L}{\partial C_\mathrm{post}} =& 2(1-\alpha)\left( C_\mathrm{post}^\mathrm{-T} - \Gamma_\mathrm{post}^\mathrm{-1} (\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr}) (\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})^\mathrm{T}C_\mathrm{post}^\mathrm{-T} - \Gamma_\mathrm{post}^\mathrm{-1} \Gamma_\mathrm{pr} C_\mathrm{post}^\mathrm{-T}\right) \\
\notag & + 2\alpha \left( F^\mathrm{T}\Gamma_\mathrm{E}^\mathrm{-1}FC_\mathrm{post}- C_\mathrm{post}^\mathrm{-T} + \Gamma_\mathrm{pr}^\mathrm{-1}C_\mathrm{post}\right).
\end{align}
We factor out $C_\mathrm{post}$ to the right and using \cref{eq:mapGammaAffine} to get:
\begin{align}
\notag \frac{\partial L}{\partial C_\mathrm{post}} =&2\left( (1-\alpha)\left( \Gamma_\mathrm{post}^\mathrm{-1}  - \Gamma_\mathrm{post}^\mathrm{-1} (\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr}) (\boldsymbol \mu_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})^\mathrm{T}\Gamma_\mathrm{post}^\mathrm{-1} - \Gamma_\mathrm{post}^\mathrm{-1} \Gamma_\mathrm{pr} \Gamma_\mathrm{post}^\mathrm{-1}\right)  \right.\\
&\left. + \alpha\left( \Gamma_\mathrm{Lap}^\mathrm{-1} - \Gamma_\mathrm{post}^\mathrm{-1} \right) \right) C_\mathrm{post}. \label{eq:lossGamma}
\end{align}
Setting $\frac{\partial L}{\partial C_\mathrm{post}} = 0$, we obtain:
\begin{align}
\Gamma_\mathrm{Lap} = \hat{\Gamma}_\mathrm{post} A^\mathrm{-1} \hat{\Gamma}_\mathrm{post}.
\end{align}
Notice that $A^\mathrm{-1}$ is well-defined since, for $\alpha \in \left[ \frac{1}{2},1 \right)$, $A$ is the sum of a positive definite matrix ($\Gamma_\mathrm{pr}$) and two positive semidefinite matrices, therefore $A$ is a positive definite matrix. %For $\alpha = 1$, $A$ is $\hat{\Gamma}_\mathrm{post}$.
\end{proof}
 
 \emph{Remark.} Expressions \cref{eq:mapMuAffine}, \cref{eq:mapGammaAffine} and \cref{eq:minProblemAffine} derive from \cref{eq:mapMu}, \cref{eq:mapGamma} and \cref{eq:minProblem}, respectively, when $\mathcal F$ is affine.

\emph{Remark.} In the statement of the theorem, we restricted the range for $\alpha$ from $[0,1)$ to $\left[ \frac{1}{2},1\right)$. We analyze this choice. If we study the case $\alpha = 0$ and impose $\frac{\partial L}{\partial \boldsymbol \mu_\mathrm{post}} = \mathbf 0$ and $\frac{\partial L}{\partial C_\mathrm{post}} = 0$, it follows from \cref{eq:lossMu} and \cref{eq:lossGamma} that $(\hat{\boldsymbol \mu}_\mathrm{post},\hat{\Gamma}_\mathrm{post}) =(\boldsymbol \mu_\mathrm{pr},\Gamma_\mathrm{pr})$. Since we are looking for $(\mathbf u_\mathrm{MAP},\Gamma_\mathrm{Lap})$, we exclude $\alpha = 0$. Additionally, $\alpha \ge \frac{1}{2}$ is a sufficient condition for A \cref{eq:Amatrix} to be invertible and for $ \hat{\Gamma}_\mathrm{post} A^\mathrm{-1} \hat{\Gamma}_\mathrm{post}$ to be SPD as $\Gamma_\mathrm{Lap}$. Consequently, we restrict $\alpha$ to the range $\left[ \frac{1}{2},1 \right)$.

In the general case when $\mathcal F$ is not affine, the expected value of \cref{eq:minProblem} cannot be computed explicitly. Instead, we estimate it using the sample mean:
\begin{align}
%&\E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \mathcal F(\mathbf u) \right] \sim \bar{\mathcal{F}} = \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw}), \\
%& \mathrm{Cov}_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)} \left[ \mathcal F(\mathbf u)  \right]  \sim \bar{\Sigma}_\mathrm{\mathcal{F}} = \frac{1}{L-1} \sum_\mathrm{l = 1}^\mathrm{L} \left(\mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw})-\bar{\mathcal{F}})\right) \left(\mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw})-\bar{\mathcal{F}}\right)^\mathrm{T}, \label{eq:estCov}
& \E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right] \sim \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw})}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \label{eq:estMean}
\end{align}
where $L \in \N $ is the number of samples and, for $l=1,\dots,L$,  $\mathbf u^\mathrm{l}_\mathrm{draw} = \boldsymbol \mu_\mathrm{post} + C_\mathrm{post} \boldsymbol \epsilon^\mathrm{l}$ with $\boldsymbol  \epsilon \in \R^\mathrm{D}$ and $\boldsymbol \epsilon^\mathrm{l} \sim \mathcal N(\mathbf 0, I)$. Here, each $\mathbf u^\mathrm{l}_\mathrm{draw}$ represents a sample drawn from $\mathcal N(\boldsymbol \mu_\mathrm{post}, \Gamma_\mathrm{post})$. The sampling process results in slow convergence to the actual mean, making computationally expensive retrieving the result of \cref{thm:convergence}. The corresponding loss function becomes:
\begin{gather}
\notag (1-\alpha) \left( \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
%& \alpha  \left( \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \bar{\mathcal{F}}}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}  + \mathrm{tr} \left(\Gamma_\mathrm{E}^\mathrm{-1}  \bar{\Sigma}_\mathrm{\mathcal{F}}\right)\right)+ \label{eq:minProblem2}\\
 \alpha \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw})}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}+ \label{eq:minProblem2}\\
\notag \alpha \left(  -\log |\Gamma_\mathrm{post}|  + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right)
\end{gather}

During the minimization of the loss function \cref{eq:minProblem2}, we encountered instabilities caused by the logarithmic terms. To mitigate these instabilities, we removed the logarithms, leading to a simplified loss function:
\begin{gather}
\notag (1-\alpha) \left( \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
%& \alpha  \left( \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \bar{\mathcal{F}}}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}  + \mathrm{tr} \left(\Gamma_\mathrm{E}^\mathrm{-1}  \bar{\Sigma}_\mathrm{\mathcal{F}}\right)\right)+ \label{eq:minProblem3}\\
 \alpha \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u^\mathrm{l}_\mathrm{draw})}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}+ \label{eq:minProblem3}\\
\notag \alpha \left( \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right)
\end{gather}

The following corollary extends \cref{thm:convergence} to this updated loss function, accommodating a wider range for $\alpha$ and modifying the definition of the matrix $A$.
\begin{cor} \label{cor:updatedConvergence}
Let us assume that $\mathcal F(\mathbf u) = F \mathbf u + \mathbf f$, where $F \in \R^\mathrm{O\times D}, \mathbf f \in \R^\mathrm{O}$, and $\mathbf y \in \R^\mathrm{O}$. Assume Gaussian prior and noise models $\mathcal N(\boldsymbol \mu_\mathrm{pr}, \Gamma_\mathrm{pr})$ and $\mathcal N (\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$, respectively. Then the posterior distribution $p_\mathrm{U|Y}(\mathbf u| \mathbf y) = \mathcal N (\mathbf u_\mathrm{MAP}, \Gamma_\mathrm{Lap})$, where:
\begin{align*}
\mathbf u_\mathrm{MAP} &= \Gamma_\mathrm{Lap} \left( F^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1} \left(  \mathbf y - \mathbf f - \boldsymbol \mu_\mathrm{E} \right) +\Gamma_\mathrm{pr}^\mathrm{-1} \boldsymbol \mu_\mathrm{pr}  \right) ,\\
\Gamma_\mathrm{Lap} &= \left( F^\mathrm{T} \Gamma_\mathrm{E}^\mathrm{-1}F + \Gamma_\mathrm{pr}^\mathrm{-1} \right)^\mathrm{-1}.
\end{align*}

Let $\alpha \in \left( 0,1 \right)$ and suppose that the pdf $q_\mathrm{\phi}(\mathbf u | \mathbf y)$ is $\mathcal N(\boldsymbol \mu_\mathrm{post}, \Gamma_\mathrm{post}) $, where $\Gamma_\mathrm{post} = C_\mathrm{post}C_\mathrm{post}^\mathrm{T}$ and $C_\mathrm{post}$ is a lower triangular matrix with positive diagonal entries. Then the stationary points $(\hat {\boldsymbol \mu}_\mathrm{post}, \hat{C}_\mathrm{post})$ (with $\hat{\Gamma}_\mathrm{post} = \hat{C}_\mathrm{post}\hat{C}_\mathrm{post}^\mathrm{T}$) of  the loss function
\begin{align}
\notag L(\boldsymbol \mu_\mathrm{post},C_\mathrm{post})  =&(1-\alpha) \left( \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
& \alpha \left(  \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - F \boldsymbol \mu_\mathrm{post} -\mathbf f }_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr} \left( \Gamma_\mathrm{E}^\mathrm{-1} F\Gamma_\mathrm{post}F^\mathrm{T} \right) \right) + \label{eq:minProblemAffine2}\\
\notag &\alpha \left( \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right)
\end{align}
satisfy
\begin{align}
\mathbf u_\mathrm{MAP} &= \frac{1-\alpha}{\alpha} \Gamma_\mathrm{Lap} \hat{\Gamma}_\mathrm{post}^\mathrm{-1} (\hat{\boldsymbol \mu}_\mathrm{post}-\boldsymbol \mu_\mathrm{pr}) + \hat{\boldsymbol \mu}_\mathrm{post}, \label{eq:muMap2}\\
\Gamma_\mathrm{Lap} &= \hat{\Gamma}_\mathrm{post} A^\mathrm{-1} \hat{\Gamma}_\mathrm{post}, \label{eq:gammaMap2}
\end{align}
where
\begin{align}
A =  \frac{1-\alpha}{\alpha}\left( (\hat{\boldsymbol \mu}_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})(\hat{\boldsymbol \mu}_\mathrm{post}- \boldsymbol \mu_\mathrm{pr})^\mathrm{T} + \Gamma_\mathrm{pr} \right). \label{eq:Amatrix2}
\end{align}
\end{cor}

\subsection{Enhanced uncertainty quantification variational autoencoders} \label{sec:UQVAE}
In this section, we introduce the eUQ-VAE framework, a semi-supervised model constrained learning approach. Let $\phi$ denote the weights and the biases of a feed forward neural network. Our aim is to investigate whether \cref{thm:convergence} still holds when $(\boldsymbol \mu_\mathrm{post}, C_\mathrm{post})$ are computed using a neural network. Specifically, we employ a fully connected neural network. As illustrated in \cref{fig:linearVAE}, the architecture of the eUQ-VAE consists of a neural network serving as the encoder and of $\mathcal F$ acting as the decoder. 
\begin{figure}[t!]
	\centering
 	\includegraphics[width=\linewidth, height=6cm,keepaspectratio]{linearVAEInputOutput}
	\caption{Scheme of the eUQ-VAE with the evaluation of $\mathcal F$}
	\label{fig:linearVAE}
\end{figure}

\begin{cor} \label{cor:convergenceNN}
Let $L(\phi) = L(\boldsymbol \mu_\mathrm{post}(\phi), \Gamma_\mathrm{post}(\phi))$ and suppose the assumptions of  \cref{cor:updatedConvergence} hold. Consider a single-layer linear neural network such that:
\begin{align}
\boldsymbol \mu_\mathrm{post} &= W_\mathrm{\boldsymbol \mu} \mathbf y + \mathbf b_\mathrm{\boldsymbol \mu}, \label{eq:muPost}\\
C_\mathrm{post} &= \mathrm{vec}_\mathrm{L}^\mathrm{-1}(\mathbf l) + \mathrm{diag}(\boldsymbol \sigma), \label{eq:cPost}\\
\boldsymbol \sigma &= \mathrm{exp} \left(  W_\mathrm{\boldsymbol \sigma} \mathbf y + \mathbf b_\mathrm{\boldsymbol \sigma} \right), \label{eq:sigma}\\
\mathbf l &= W_\mathrm{\mathbf l} \mathbf y + \mathbf b_\mathrm{\mathbf l}, \label{eq:l}
\end{align}
where $W_\mathrm{\boldsymbol \mu} \in \R^\mathrm{D \times O}$ and $W_\mathrm{\boldsymbol \sigma} \in \R^\mathrm{D \times O}, W_\mathrm{\mathbf l} \in \R^\mathrm{\frac{D(D-1)}{2} \times O}$ are matrices representing the weights of the neural network, $\mathbf b_\mathrm{\boldsymbol \mu} \in \R^\mathrm{D}, \mathbf b_\mathrm{\boldsymbol \sigma} \in \R^\mathrm{D}$ and $\mathbf b_\mathrm{\mathbf l} \in \R^\mathrm{\frac{D(D-1)}{2}}$ are the biases of the output layer. The functions $\mathrm{exp}$, $\mathrm{vec}_\mathrm{L}$ and $\mathrm{diag}$ are the elementwise exponential, the vectorization of a strictly lower triangular matrix and the construction of a diagonal matrix from its vector diagonal, respectively.

The stationary points $\hat \phi$ of $L$ satisfy \cref{eq:muMap2} and \cref{eq:gammaMap2}.
\end{cor}

\begin{proof}
The proof follows from the chain rule. 
\end{proof}
 
This corollary guarantees that, if a minimum $(\hat {\boldsymbol \mu}_\mathrm{post}, \hat{C}_\mathrm{post})$ of the loss function \cref{eq:minProblemAffine2} exists, the training process of the neural network attempts to enforce the relationships \cref{eq:muMap2} and \cref{eq:gammaMap2}.

If the neural network has nonlinear activation functions and contains more than one layer, $(\boldsymbol \mu_\mathrm{post},C_\mathrm{post})$ are computed as in \cref{eq:muPost}, \cref{eq:cPost}, \cref{eq:sigma} and \cref{eq:l}, using the outputs of the network's last layer in place of $W_\mathrm{\boldsymbol \mu}\mathbf y + \mathbf b_\mathrm{\boldsymbol \mu}$, $W_\mathrm{\boldsymbol \sigma}\mathbf y + \mathbf b_\mathrm{\boldsymbol \sigma}$ and $W_\mathrm{\mathbf l}\mathbf y + \mathbf b_\mathrm{\mathbf l}$.

If $\mathcal F$ is unknown or computationally expensive to evaluate, it can be approximated using a second neural network $\psi$ acting as the decoder (\cref{fig:VAE}) \cite{goh2021solving}. By substituting this approximation in \cref{eq:minProblem3}, the resulting loss function becomes:
\begin{gather}
\notag (1-\alpha) \left(\norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} + \mathrm{tr}\left(\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}  \right) \right) +\\
\alpha \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \psi(\mathbf u^\mathrm{l}_\mathrm{draw})}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}+\label{eq:minProblem4}\\
%& \alpha \left( \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \bar{\mathcal{\psi}}}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}  + \mathrm{tr} \left(\Gamma_\mathrm{E}^\mathrm{-1}  \bar{\Sigma}_\mathrm{\psi}\right)\right) + \label{eq:minProblem4}\\
\notag \alpha \left( \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right)  \right).
\end{gather}
%where
%\begin{align}
%\notag &\bar{\psi} = \frac{1}{L} \sum_\mathrm{l = 1}^\mathrm{L} \psi(\mathbf u^\mathrm{l}_\mathrm{draw}), \\
%\notag & \bar{\Sigma}_\mathrm{\psi} = \frac{1}{L-1} \sum_\mathrm{l = 1}^\mathrm{L} \left(\psi(\mathbf u^\mathrm{l}_\mathrm{draw})-\bar{\psi})\right) \left(\psi(\mathbf u^\mathrm{l}_\mathrm{draw})-\bar{\psi}\right)^\mathrm{T}.
%\end{align}
\begin{figure}[t!]
	\centering
 	\includegraphics[width=\linewidth, height=6cm,keepaspectratio]{VAEInputOutput}
	\caption{Scheme of the eUQ-VAE with a neural network in place of the evaluation of $\mathcal F$.}
	\label{fig:VAE}
\end{figure}
The simultaneous training of the encoder and the decoder could lead to unexpected results as both networks compete to minimize the loss function. Therefore, the decoder is trained before to the encoder. This also keeps a parallelism between the loss function \cref{eq:minProblem4} and \cref{eq:minProblem3}, where the map $\mathcal F$ from parameters to observational data is fixed. More theoretical insights can be analyzed by minimizing the loss function with respect to both the encoder and decoder weights and biases \cite{nguyen5081218taen}.

After the training of the eUQ-VAE, we retain only the encoder component which is used to compute $(\hat{\boldsymbol \mu}_\mathrm{post},\hat{C}_\mathrm{post})$.

In the previous theoretical analysis, we considered the case of a single observational data $\mathbf y$. When extending to $M\in \N$ samples, the loss functions \cref{eq:minProblem3} and \cref{eq:minProblem4} are replaced by the mean of the individual losses across all the samples.

\subsubsection{Data normalization}
Before the training, it is a good practice to normalize the data to improve the generalization capabilities of the neural network. In our case, normalization affects also the probability distributions. Consider a dataset $\{(\mathbf u^\mathrm{(m)}, \mathbf y^\mathrm{(m)})\}_\mathrm{m=1}^\mathrm{M}$ with $M \in \N$, where $\mathbf y^\mathrm{(m)} = \mathcal F(\mathbf u^\mathrm{(m)}) + \boldsymbol \epsilon^\mathrm{(m)}$ and $\mathbf u^\mathrm{(m)}$ and $\boldsymbol \epsilon^\mathrm{(m)}$ are sampled from $\mathcal N(\boldsymbol \mu_\mathrm{pr},\Gamma_\mathrm{pr})$ and $\mathcal N(\boldsymbol \mu_\mathrm{E},\Gamma_\mathrm{E})$, respectively.

Normalizing the data, for example to the range $[0,1]$, means that there exist vectors $\mathbf a, \mathbf b \in \R^\mathrm{D}$ and $\mathbf c, \mathbf d \in \R^\mathrm{O}$ such that $\forall \, m = 1, \dots ,M$:
\begin{align}
\bar{\mathbf u}^\mathrm{(m)} &= \mathbf u^\mathrm{(m)} \odot \mathbf a + \mathbf b \in [0,1]^\mathrm{D},\\
\bar{\mathbf y}^\mathrm{(m)} &= \mathbf y^\mathrm{(m)} \odot \mathbf c + \mathbf d \in [0,1]^\mathrm{O}, \label{eq:yNorm}
\end{align}
where $\odot$ denotes the Hadamard product. By the variables transformation and the model \cref{eq:model}, we get:
%We assume that there exists a model analogous to \cref{eq:model} for the normalized data, that is there exists a function $\bar {\mathcal F}$ such that:
\begin{align*}
\bar{\mathbf y}^\mathrm{(m)}= \mathbf y^\mathrm{(m)} \odot \mathbf c + \mathbf d = \mathcal F((\bar{\mathbf u}^\mathrm{(m)}-\mathbf b)\oslash \mathbf a)\odot \mathbf c +\mathbf d+\boldsymbol \epsilon^\mathrm{(m)} \odot \mathbf c = \bar{\mathcal F}(\bar{\mathbf u}^\mathrm{(m)})+\bar{\boldsymbol \epsilon }^\mathrm{(m)},
\end{align*}
where $\bar{\mathcal F} (\bar {\mathbf u}^\mathrm{(m)}) \coloneqq \mathcal F((\bar{\mathbf u}^\mathrm{(m)}-\mathbf b)\oslash \mathbf a) \odot \mathbf c + \mathbf d $ and $\bar {\boldsymbol \epsilon}^\mathrm{(m)} = \boldsymbol \epsilon^\mathrm{(m)} \odot \mathbf c$. As a result, an observational model analogous to \cref{eq:model} holds for the normalized scales:
\begin{align}
\bar{Y} = \bar{\mathcal F}(\bar{U})+\bar{E},
\end{align}
where $\bar{Y}, \bar{U}$ and $\bar{E}$ are the observable data, the parameters and the noise random variables of the normalized mathematical model. Since $U \sim \mathcal N(\boldsymbol \mu_\mathrm{pr}, \Gamma_\mathrm{pr}), E \sim \mathcal N(\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$ and the normalization is an affine function, the normalized random variables $\bar U$ and $\bar E$ are also Gaussian random variables, with means and covariances:
\begin{align}
\notag \bar{\boldsymbol \mu}_\mathrm{pr} &= \boldsymbol \mu_\mathrm{pr} \odot \mathbf a + \mathbf b,\\
\notag \bar{\Gamma}_\mathrm{pr} &= \Gamma_\mathrm{pr} \odot (\mathbf a \mathbf a^\mathrm{T}),\\
\notag \bar{\boldsymbol \mu}_\mathrm{E} &= \boldsymbol \mu_\mathrm{E} \odot \mathbf c,\\
\notag \bar{\Gamma}_\mathrm{E} &= \Gamma_\mathrm{E} \odot (\mathbf c \mathbf c^\mathrm{T}). 
\end{align}

Moreover, $\mathcal F$ is affine if and only if $\bar{\mathcal F}$ is affine, then \cref{cor:updatedConvergence} holds also for the normalized problem. Once we minimize \cref{eq:minProblem3} or \cref{eq:minProblem4} with respect to the normalized variables and obtain $(\hat{\bar{\boldsymbol \mu}}_\mathrm{post},\hat{\bar{\Gamma}}_\mathrm{post})$, we transform back to the original scale by:
\begin{align}
\hat{\boldsymbol \mu}_\mathrm{post} &= (\hat{\bar{\boldsymbol \mu}}_\mathrm{post}-\mathbf b) \oslash \mathbf a,\\
\hat{\boldsymbol \Gamma}_\mathrm{post}& = \hat{\bar{\Gamma}}_\mathrm{post} \odot ((\mathbf 1\oslash \mathbf a)(\mathbf 1\oslash \mathbf a)^\mathrm{T}),
\end{align}
where $\mathbf 1 \in \R^\mathrm{D}$ is a vector of ones.

\section{Numerical results}\label{sec:results}
In this section, we evaluate the performances of the new eUQ-VAE approach: in \cref{sec:test1}, we verify \cref{thm:convergence} and compare the results of the eUQ-VAE with that of the UQ-VAE \cite{goh2021solving}; in \cref{sec:test2}, we show the superior generalization properties of eUQ-VAE with respect to UQ-VAE; in \cref{sec:test3}, we apply eUQ-VAE on the Laplace equation.

To compare the two approaches (\cref{sec:test1} and  \cref{sec:test2}), we use the same network architectures. In all our tests, we use \emph{elu} activation functions \cite{clevert2015fast} and we normalize the data in $[0,1]$. For conciseness, we do not indicate the normalization in the following sections.

\subsection{Validation of the theoretical result} \label{sec:test1}
We assume that the parameters $U$ are distributed as a Gaussian random field in the interval $[-2,2]$. We take the parameter values at $D = 20$ equally spaced points $x_\mathrm{j}$. We choose $\boldsymbol \mu_\mathrm{pr} = \mathbf 1$ and define the entries of $\Gamma_\mathrm{pr}$ as:
\begin{align}
\left( \Gamma_\mathrm{pr} \right)_\mathrm{i,j} = \frac{\mathrm{e}^\mathrm{-k |x_\mathrm{j}-x_\mathrm{k} |}}{2k\gamma},
\end{align}
where $k = 1/5$ and $\gamma = 1/8$. This choice of $\Gamma_\mathrm{pr}$, together with regularity conditions on $\mathcal F$, ensures the well-posedness of general $1$-D Bayesian inverse problems \cite{stuart2010inverse,daon2018mitigating}. The dimension refers to the domain where the parameter random variable is defined.\\
We consider a linear function represented by a matrix $F \in \R^\mathrm{O \times D}$ with $O = 15$. The entries of the matrix are sampled form a uniform distributions over the interval $[0,1]$. Since $\mathcal F$ is linear, we use a variational autoencoder with the architecture shown in \cref{fig:linearVAE} for both the eUQ-VAE and UQ-VAE approaches. The encoder has no hidden layers and uses linear activation functions.\\
We generate a single sample $(\mathbf u^\mathrm{(1)}, \tilde{ \mathbf y}^\mathrm{(1)})$, where $\mathbf u^\mathrm{(1)}$ is sampled from $\mathcal N(\boldsymbol \mu_\mathrm{pr},\Gamma_\mathrm{pr})$ and $ \tilde{ \mathbf y}^\mathrm{(1)} = F \mathbf u^\mathrm{(1)}$. We assume $\boldsymbol \mu_\mathrm{E} = \mathbf 0$ and $\Gamma_\mathrm{E}$ diagonal with $(\Gamma_\mathrm{E})_\mathrm{i,i} = \eta^\mathrm{2} \tilde{y}^\mathrm{(1)2}_\mathrm{i}$ for $i = 1,\dots, O$, where $\eta \in \R$. We sample $\boldsymbol \epsilon^\mathrm{(1)} \sim \mathcal N (\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$ and add noise to the observational data $\mathbf y^\mathrm{(1)} = \tilde{\mathbf y}^\mathrm{(1)}+\boldsymbol \epsilon^\mathrm{(1)}$. We fix $L = 10^\mathrm{5}$. We use RMSProp \cite{tieleman2012lecture} as optimizer.

In \cref{fig:test1}, we compare the posterior mean estimates and their standard deviations, obtained using the eUQ-VAE and the UQ-VAE, for different values of $\eta$ and $\alpha$. For reference, we report the loss function for the UQ-VAE, where a single-sample Monte Carlo is used to estimate the expected values: $\E_\mathrm{p_\mathrm{U}(\mathbf u)} \left[ \norm{\boldsymbol \mu_\mathrm{post}-\mathbf u}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} \right]$ and $\E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right]$, which appear in the fifth row of \cref{eq:pUexpval} and in \cref{eq:qUexpval}, respectively. The loss function for UQ-VAE is given by:
\begin{align}
\notag \tilde L(\boldsymbol \mu_\mathrm{post},C_\mathrm{post}) =  &\frac{(1-\alpha)}{\alpha} \left( \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \mathbf u^\mathrm{(1)}}_\mathrm{\Gamma_\mathrm{post}^\mathrm{-1}}^\mathrm{2} \right)+\\
&  \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u^\mathrm{1}_\mathrm{draw})}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2}- \label{eq:gohProblem}\\
\notag & \log |\Gamma_\mathrm{post}| + \norm{\boldsymbol \mu_\mathrm{post} - \boldsymbol \mu_\mathrm{pr}}_\mathrm{\Gamma_\mathrm{pr}^\mathrm{-1}}^\mathrm{2} +\mathrm{tr}\left( \Gamma_\mathrm{pr}^\mathrm{-1} \Gamma_\mathrm{post}  \right),
\end{align}
where $\mathbf u^\mathrm{1}_\mathrm{draw}$ is sampled from $\mathcal N(\boldsymbol \mu_\mathrm{post},\Gamma_\mathrm{post})$.
\begin{figure}[t!]
	\centering
 	\includegraphics[width=\linewidth]{mu}
	\caption{True value of $\mathbf u_\mathrm{MAP}$ and its standard deviation (green dashed lines and areas) and their estimates by means of eUQ-VAE (blue dashed lines and areas) and UQ-VAE (red dotted lines and areas). According to the rows and columns, we modify the values of $\eta$ and $\alpha$, respectively. The green and blue lines and areas overlap as expected by \cref{cor:convergenceNN}.}
	\label{fig:test1}
\end{figure}
\\The eUQ-VAE outperforms the UQ-VAE in terms of accuracy. However, this comes with an increased computational cost to evaluate the loss function \cref{eq:minProblem3}. Indeed, its computational cost is $O(D^\mathrm{3}+LD^\mathrm{2}+L\mathrm{c}(\mathcal F) + LO^\mathrm{2})$ (where $c(\mathcal F)$ is the cost of a single evaluation of $\mathcal F$), compared to $O(D^\mathrm{3}+O^\mathrm{2} +\mathrm{c}(\mathcal F))$ for the loss function \cref{eq:gohProblem}. Specifically, when evaluating \cref{eq:minProblem3}, the term $D^\mathrm{2}$ represents the cost of computing a single $\mathbf u^\mathrm{l}$ and $\mathrm{c}(\mathcal F)$ corresponds to the cost of evaluating $\mathcal F$. The term $O^\mathrm{2}$ refers to the cost of solving the linear systems involving the matrix $\Gamma_\mathrm{E}$ (assuming to know its Cholesky factorization). These computations are repeated $L$ times. The term $D^\mathrm{3}$ represents the cost of solving the linear systems $\Gamma_\mathrm{post}^\mathrm{-1}\Gamma_\mathrm{pr}$ and $\Gamma_\mathrm{pr}^\mathrm{-1}
\Gamma_\mathrm{post}$. The cost of these operations dominates that of the other ones in \cref{eq:minProblem3}.

\subsection{Generalization properties of eUQ-VAE} \label{sec:test2}
We now assess the generalization properties of the eUQ-VAE and compare them with those of the UQ-VAE. The distribution of $U$, the function $\mathcal F$, $D$, $O$ and the variational autoencoder architecture are the same as in \cref{sec:test1}. Additionally, we generate the training dataset $\{(\mathbf u^\mathrm{(m)}, \tilde{ \mathbf y}^\mathrm{(m)})\}_\mathrm{m=1}^\mathrm{M}$, where $\mathbf u^\mathrm{(m)}$ is sampled from $\mathcal N(\boldsymbol \mu_\mathrm{pr},\Gamma_\mathrm{pr})$, $ \tilde{ \mathbf y}^\mathrm{(m)} = F \mathbf u^\mathrm{(m)}$ and $M\in \N$. We assume $\boldsymbol \mu_\mathrm{E} = \mathbf 0$ and $\Gamma_\mathrm{E}$ is diagonal with $(\Gamma_\mathrm{E})_\mathrm{i,i} = \eta^\mathrm{2} \underset{m}{\max} \, \tilde{y}^\mathrm{(m)2}_\mathrm{i}$ for $i = 1,\dots, O$, where $\eta \in \R$. We sample $\boldsymbol \epsilon^\mathrm{(m)} \sim \mathcal N (\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$ and add noise to the observational data $\mathbf y^\mathrm{(m)} = \tilde{\mathbf y}^\mathrm{(m)}+\boldsymbol \epsilon^\mathrm{(m)}$ for $m = 1,\dots,M$. We generate the test set in the same way, with its dimension being $1/8$ of that of the training set. Finally, we fix $L = 10^\mathrm{5}$. We use RMSProp \cite{tieleman2012lecture} as optimizer.

In \cref{fig:test2}, we compare the generalization capabilities of eUQ-VAE with those of UQ-VAE for different training set sizes $M$, $\eta = 0.05$ and $\alpha = 0.5$. We trained the variational autoencoders for $5000$ epochs. Despite the training is longer than necessary, our approach is not prone to overfitting and displays small oscillations in the loss function. The results for eUQ-VAE are comparable for the three analyzed dataset dimensions. The UQ-VAE exhibits large oscillations in the testing loss functions,  highlighting instabilities in the training of the UQ-VAE. Nonetheless, these oscillations are mitigated by increasing the dataset dimension. Note that comparing the values of the loss functions is not appropriate, as the two approaches use different losses.
\begin{figure}[t!]
	\centering
 	\includegraphics[width=\linewidth, height=10cm,keepaspectratio]{generalization}
	\caption{Values of the loss function on the training (light green) and test sets (dark red) for the UQ-VAE and the eUQ-VAE approaches for $\eta = 0.05$ and $\alpha = 0.5$. According to the rows the dimension of the training dataset $M$ changes.}
	\label{fig:test2}
\end{figure}

\subsection{Laplace problem}\label{sec:test3}
We consider the Laplace equation on the unit square $\Omega = (0,1)^\mathrm{2}$:
\begin{align}
\begin{cases}
-\nabla \cdot(e^\mathrm{u}\nabla y) = 0 \qquad & \text{in } \Omega\\
-e^\mathrm{u}(\nabla y \cdot \mathbf n) = -1 \qquad & \text{on } \Omega^\mathrm{root} \label{eq:heatConduction}\\
-e^\mathrm{u}(\nabla y \cdot \mathbf n) = \frac{y}{2} \qquad & \text{on } \partial \Omega\setminus \Omega^\mathrm{root}
\end{cases}
\end{align}
where $u$ is the (log) diffusion parameter, $\mathbf n$ is the unit external normal vector to the domain and $\Omega^\mathrm{root} =  [0,1] \times \{0\}$.

We assume that the parameters $U$ are distributed as a Gaussian random field $\mathcal N (\mu_\mathrm{pr},\mathcal C_\mathrm{pr})$ on $\Omega$. We set the prior mean $ \mu_\mathrm{pr}= 0$. To compute the covariance operator  $\mathcal C_\mathrm{pr}$ we define the differential operator $\mathcal A$ as:
\begin{align}
\mathcal A u = -\gamma \Delta u + \delta u \qquad \text{in } \Omega
\end{align}
where $\gamma>0, \delta >0$. $\mathcal C_\mathrm{pr}$ is defined as $G_\mathrm{2}$, the Green function solution of:
\begin{align}
\begin{cases}
\mathcal A G_\mathrm{1} = \delta_\mathrm{\mathbf x} \qquad &\text{in } \Omega\\
\nabla G_\mathrm{1} \cdot \mathbf n + \beta G_\mathrm{1} = 0  &\text{on } \partial \Omega\\
\mathcal A G_\mathrm{2} = G_\mathrm{1} & \text{in } \Omega\\
\nabla G_\mathrm{2} \cdot \mathbf n + \beta G_\mathrm{2} = 0 & \text{on } \partial \Omega
\end{cases} 
\end{align}
where, for $\mathbf x \in \Omega$, $\delta_\mathrm{\mathbf x}$ is the Dirac delta centered in $\mathbf x$ and $\beta$ is chosen to reduce the boundary artifacts on the Green function \cite{daon2018mitigating}. We set $\gamma = 0.1$ and $\delta = 0.5$. Priors of this type ensure the well-posedness of infinite dimensional inverse Bayesian problems\cite{stuart2010inverse,bui2013computational}. To compute $\mathcal C_\mathrm{pr}$, we use the Python library for inverse problems \emph{hIPPYlib} \cite{VillaPetraGhattas21,VillaPetraGhattas18,VillaPetraGhattas16}. In particular, we use linear finite element methods over a triangular mesh on the domain $\Omega$ with $257^\mathrm{2}$ degrees of freedom. This mesh is used also to compute the solution to \cref{eq:heatConduction}.\\
We restrain the dimension of the parameter space, since the number of neurons of the eUQ-VAE latent state used to compute $(\boldsymbol \mu_\mathrm{post}, \Gamma_\mathrm{post})$ scales as $D^\mathrm{2}$. Among all the degrees of freedom used to approximate the covariance matrix, we choose to use $D=25$ evenly spaced points $\mathbf x_\mathrm{i}$ for $i = 1,\cdots,D$ inside the domain $\Omega$. Therefore, $(\boldsymbol{\mu}_\mathrm{pr})_\mathrm{i} = \mu_\mathrm{pr}(\mathbf x_\mathrm{i})$ and $(\Gamma_\mathrm{pr})_\mathrm{i,j} = \mathcal C_\mathrm{pr}(\mathbf x_\mathrm{i},\mathbf x_\mathrm{j})$ for $i,j = 1,\cdots,D$. We then interpolate over the original mesh the eUQ-VAE mean and variance using multiquadric radial basis functions.\\
The observational data correspond to $O=9$ equally spaced evaluations of the solution of problem \cref{eq:heatConduction}.\\
We use loss function \cref{eq:minProblem4} to train the variational autoencoder, where the decoder $\psi$ is trained prior to the encoder using the mean squared error as loss function.

We generate the training dataset $\{(\mathbf u^\mathrm{(m)}, \tilde{ \mathbf y}^\mathrm{(m)})\}_\mathrm{m=1}^\mathrm{M}$, sampling $\mathbf u^\mathrm{(m)}$ from $\mathcal N(\boldsymbol \mu_\mathrm{pr},\Gamma_\mathrm{pr})$, $ \tilde{ \mathbf y}^\mathrm{(m)}$ is the solution of \cref{eq:heatConduction} with diffusive parameter $\mathbf u^\mathrm{(m)}$ and $M \in \mathbb N$. We assume $\boldsymbol \mu_\mathrm{E} = \mathbf 0$ and $\Gamma_\mathrm{E}$ is diagonal with $(\Gamma_\mathrm{E})_\mathrm{i,i} = \eta^\mathrm{2} \underset{j,m}{\max} \, \tilde{y}^\mathrm{(m)2}_\mathrm{j}$ for $i = 1,\dots, O$, where $\eta \in \R$. We sample $\boldsymbol \epsilon^\mathrm{(m)} \sim \mathcal N (\boldsymbol \mu_\mathrm{E}, \Gamma_\mathrm{E})$ and perturb the observational data $\mathbf y^\mathrm{(m)} = \tilde{\mathbf y}^\mathrm{(m)}+\boldsymbol \epsilon^\mathrm{(m)}$ for $m=1,\cdots,M$. We fix $L = 5\cdot10^\mathrm{4}$, $\eta = 0.05$ and $\alpha = 0.5$. We use RMSProp \cite{tieleman2012lecture} as optimizer. %We also tried to use Adam optimizer\cite{kingma2014adam}, but the loss function suffered from instabilities during the training procedure.

We train the decoder on a dataset of $M=80$ samples and subsequently train the encoder on the same dataset. The decoder comprises $5$ hidden layers, each containing $300$ neurons. We apply an $l_\mathrm{2}$ weights regularization of $10^\mathrm{-5}$. At the final training epoch, the decoder validation loss (mean squared error) is on the order of $10^\mathrm{-4}$.

The encoder comprises one hidden layer with $500$ neurons. The training of the encoder was unstable due to the terms in the first and third rows of the loss function \cref{eq:minProblem4}. As the minimum of these terms is achieved for $\boldsymbol \mu_\mathrm{post}=\boldsymbol \mu_\mathrm{pr}$ and $\Gamma_\mathrm{post}=\sqrt{\frac{1-\alpha}{\alpha}}\Gamma_\mathrm{pr}$, we initialize the biases of the output layers to be $\boldsymbol \mu_\mathrm{pr}$ and $C_\mathrm{pr}$ (Cholesky factor of $\Gamma_\mathrm{pr}$), all the weights of the encoder to $10^\mathrm{-4}$ and the biases to $0$. This initialization stabilizes the training procedure and reduces the value of the loss function.

We show the eUQ-VAE results on three test samples. The means and covariances estimated by the eUQ-VAE in some cases align with the MAP estimate (\cref{fig:1samplea,fig:1samplec,fig:2samplea,fig:2samplec}). Since the MAP parameter represents the most likely parameter generating the available data and that the eUQ-VAE parameter is an approximation of the MAP one, discrepancies between the estimates and the true parameter may occur. The estimated variance by the eUQ-VAE captures the scale of the Laplace approximation of the posterior variance (\cref{fig:1sampleb,fig:1samplec,fig:2sampleb,fig:2samplec,fig:3sampleb,fig:3samplec}), though the values differ. We verified that these differences were not due to the encoder's learning capacity by increasing the number of neurons and hidden layers, which yielded similar results. It is likely that the decoder does not sufficiently approximate $\mathcal F$, as indicated by its validation loss of the order of $10^\mathrm{-4}$.

\begin{figure}[t!]
	\centering
	\subfigure[][From left to right: solution and true parameter of the Laplace equation. For the solution the observational points are highlighted. Posterior parameter estimates with eUQ-VAE and MAP approaches. \label{fig:1samplea}]{\includegraphics[width=\linewidth]{parameter1L50000}} \\
	\subfigure[][Posterior variance estimates. \label{fig:1sampleb}]{\includegraphics[width=0.495\linewidth]{variance1L50000}}
	\hfill
 	\subfigure[][Cross sectional estimated parameter and standard deviation for $y=0.5$. \label{fig:1samplec}]{\includegraphics[width=0.495\linewidth]{crossSec1L50000}}
\caption{Test sample 1.}\label{fig:1sample}
\end{figure}

\begin{figure}[t!]
	\centering
 	\subfigure[][From left to right: solution and true parameter of the Laplace equation. For the solution the observational points are highlighted. Posterior parameter estimates with eUQ-VAE and MAP approaches. \label{fig:2samplea}]{\includegraphics[width=\linewidth]{parameter2L50000}} \\
	\subfigure[][Posterior variance estimates. \label{fig:2sampleb}]{\includegraphics[width=0.495\linewidth]{variance2L50000}}
	\hfill
 	\subfigure[][Cross sectional estimated parameter and standard deviation for $y=0.5$. \label{fig:2samplec}]{\includegraphics[width=0.495\linewidth]{crossSec2L50000}}
	\caption{Test sample 2.} \label{fig:2sample}
\end{figure}

\begin{figure}[t!]
	\centering
 	\subfigure[][From left to right: solution and true parameter of the Laplace equation. For the solution the observational points are highlighted. Posterior parameter estimates with eUQ-VAE and MAP approaches. \label{fig:3samplea}]{\includegraphics[width=\linewidth]{parameter3L50000}} \\
	\subfigure[][Posterior variance estimates. \label{fig:3sampleb}]{\includegraphics[width=0.495\linewidth]{variance3L50000}}
	\hfill
 	\subfigure[][Cross sectional estimated parameter and standard deviation for $y=0.5$. \label{fig:3samplec}]{\includegraphics[width=0.495\linewidth]{crossSec3L50000}}
	\caption{Test sample 3.} \label{fig:3sample}
\end{figure}


\section{Conclusions}\label{sec:concl}
In this work, we proposed an enhanced uncertainty quantification variational autoencoder approach for solving Bayesian inverse problems, building on the work of  \cite{goh2021solving}. In \cref{sec:test1}, we showed, in the affine case, that eUQ-VAE estimates the posterior mean and covariance more accurately than the UQ-VAE. We supplied this numerical results with a theorem showing that the stationary points of the loss function used to train the eUQ-VAE  are connected to the posterior distribution of the parameters. This result is presented for single-layer linear neural networks. Furthermore, the eUQ-VAE exhibits better generalization properties compared to the UQ-VAE (\cref{sec:test2}). The eUQ-VAE approach enables real-time Bayesian inverse problem solution and UQ after an offline training phase with an affine map $\mathcal F$. Additionally, the training is not subjected to the knowledge of the posterior distribution mean and covariance that the eUQ-VAE aims to estimate.

The posterior mean and covariance estimates of a Laplace equation are not optimal for eUQ-VAE (\cref{sec:test3}). Even if, the mean estimate of the eUQ-VAE shows quite often agreement with the MAP estimate, the estimated variance is different from the Laplace approximation (\cref{fig:1sample,fig:2sample}).

%This new methodology can be applied to various inverse problems, as the loss function of the variational autoencoder is independent of the specific application. The eUQ-VAE approach enables real-time Bayesian inverse problem solving and UQ after an offline training phase. Additionally, the training set does not need the knowledge of the posterior distribution parameters that the eUQ-VAE aims to estimate.\\
%While we have analyzed the properties of the eUQ-VAE, some limitations remain. 
Other limitations still remain. \Cref{cor:updatedConvergence} does not explicitly state whether a minimum (even local) of the training loss function exists. This would be beneficial to confirm that minimizing the loss function leads to the estimation of the posterior distribution.\\
Finally, the estimation of $\E_\mathrm{q_\mathrm{\phi}(\mathbf u | \mathbf y)}\left[ \norm{\mathbf y - \boldsymbol \mu_\mathrm{E} - \mathcal F(\mathbf u)}_\mathrm{\Gamma_\mathrm{E}^\mathrm{-1}}^\mathrm{2} \right]$ performed by the eUQ-VAE, is responsible for the high training computational cost. A different treatment of this term could eliminate the need for estimating the expected value and drastically reduce the training time of the neural network.

%\section*{Conflict of interest disclosure}
%The authors declare no competing interests.

%\section*{Correspondence}
%Correspondence and requests for materials should be addressed to {\color{blue} \href{mailto:andrea.tonini@polimi.it}{andrea.tonini@polimi.it}}

\section*{Acknowledgments}
AT and LD are members of the INdAM group GNCS \say{Gruppo Nazionale per il Calcolo Scientifico} (National Group for Scientific Computing). AT and LD acknowledge the INdAM GNCS project CUP E53C23001670001.\\

L.D. acknowledges the support by the FAIR (\say{Future Artificial Intelligence Research}) project, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2, investment 1.3, line on Artificial Intelligence), Italy.\\

The present research is part of the activities of Dipartimento di Eccellenza 20232027, MUR, Italy, Dipartimento di Matematica, Politecnico di Milano.

\bibliographystyle{siamplain}
\bibliography{Tonini_et_al-JSC_2025}

\end{document}