\section{Causes of Demographic Bias}
\label{sec:causes}

In this section, we consolidate findings from existing works related to causes
of demographic bias in FR. Considering the wide range of research in this area,
we have grouped the causes into categories for clarity and systematic review.
These categories encompass factors such as imbalances in training datasets,
variability in skin tones, algorithmic sensitivity, image quality and related
covariates, as well as combined or intersectional demographic factors. While
this categorization simplifies the organization, it is important to note that
many studies attribute demographic bias to multiple, overlapping factors, making
strict classification difficult. We have categorized works based on their
primary focus or findings, but some studies have been referenced across multiple
categories to reflect their broader relevance. This approach also highlights
that causes of demographic bias are inherently multifaceted and interconnected,
requiring a thorough understanding to analyze and mitigate these disparities.  A
quick summary of various works discussed in this section can be found in
Table~\ref{tab:causes}.


\subsection{Training Datasets}

The issue of demographic bias in FR systems often stems from imbalanced or
unrepresentative datasets, significantly influencing both training and
evaluation outcomes. Research by Krishnapriya \etal
\cite{krishnapriya2020issues} demonstrated how demographic groups, such as
African-American cohorts, exhibit higher false match rates (FMR), while
Caucasian cohorts face higher false non-match rates (FNMR), highlighting the
interplay between ethnicity and matching thresholds. The Face Recognition Vendor
Test (FRVT) conducted by NIST~\cite{frvt3} substantiated these findings,
reporting increased false positives in women, children, and the elderly,
alongside higher false negatives in under-represented racial groups, emphasizing
the intricate interactions between dataset characteristics and demographic
attributes.

Early studies like Klare \etal~\cite{klare2012face} advocated for balanced
datasets and the use of exclusive cohorts to enhance FR performance. Cavazos
\etal \cite{cavazos2020accuracy} identified how dataset complexity and
identification thresholds contribute to racial bias, such as the need for higher
thresholds for East-Asian faces to achieve comparable false acceptance rates
(FARs). Gwilliam \etal \cite{gwilliam2021rethinking} challenged the prevailing
assumptions about the necessity of balanced datasets by demonstrating that
skewed distributions favoring African faces reduced racial bias more effectively
than balanced datasets. Wu and Bowyer~\cite{wu2023should} expanded this
discussion, emphasizing that mere balance in identities or number of images is
insufficient to address bias, highlighting additional factors like brightness
and head pose during dataset assembly.

Other works delved into specific aspects of demographic balance in datasets.
Wang \etal~\cite{wang2019racial} observed that even race-balanced datasets
failed to eliminate racial bias, hypothesizing that certain ethnicities are
inherently more challenging to recognize. Kolla and
Savadamuthu~\cite{kolla2023impact} highlighted the influence of facial quality
and racial feature gradations on model fairness. Focusing on inter-sectional
bias, Muthukumar \etal~\cite{muthukumar2018understanding} identified structural
facial features as significant contributors, particularly for dark-skinned
females, over attributes like skin tone or hair length. Cook \etal
\cite{cook2019demographic} further analyzed the role of image acquisition
conditions, noting how factors such as skin reflectance and environmental
conditions disproportionately affect darker-skinned individuals, thus advocating
for standardized acquisition protocols to mitigate bias.

Although most studies acknowledge that demographically imbalanced training data
contribute to biased FR models and training with balanced datasets enhances
fairness, there is a consensus that these are neither the sole causes nor
complete solutions to the broader issue of demographic bias.
%-----

\subsection{Variability in Skin-tone}

The influence of skin tone on the performance of FR systems has been extensively
studied, revealing significant demographic disparities.
In~\cite{muthukumar2018understanding}, Muthukumar \etal identified notable
under-performance in recognizing dark-skinned females compared to other
demographic group for commercial classifiers. Their analysis attributed these
disparities to structural features such as lips, eyes, and cheeks, in addition
to skin-tone itself. Similarly, Buolamwini and Gebru \cite{buolamwini18a}
employed the Fitzpatrick skin classification system to evaluate gender
classifiers and reported the lowest accuracy for darker-skinned females. Their
findings further indicated that lighter-skinned males achieved the highest
performance, highlighting the intersection of skin-tone and gender as critical
factors influencing recognition accuracy. In another study, Krishnapriya
\etal~\cite{krishnapriya2020issues} examined FMR and FNMR across skin-tone
groups, observing higher FMR for African-American cohorts and higher FNMR for
Caucasians; however, they did not find a direct causation between darker skin
tones and higher error rates.

The Biometric Technology Rallies organized by MdTF have offered comprehensive
insights into the role of skin-related factors in FR bias. Their 2019
report~\cite{cook2019demographic} emphasized skin reflectance as more
significant predictor of performance disparities than race. Using systematic
linear modeling, their study demonstrated that darker skin-tones were associated
with longer transaction (processing overall pipeline) times and lower accuracy
in biometric systems. This dependency was found to vary substantially across
systems, highlighting important role of acquisition methods in determining the
extent of bias. Lu \etal~\cite{lu2019experimental} provided a quantitative
assessment of performance variations across five skin-tone groups, identifying
light-skinned individuals as the easiest to verify and darker-skinned
individuals as the most challenging. However, ambiguities in defining skin tone
categories complicate direct evaluations, highlighting the need for standardized
classification metrics.
%-------

\subsection{Algorithmic Factors}

In this section, we examine the sensitivity and limitations of algorithms in
addressing demographic attributes. Phillips \etal \cite{phillips2011other}
identified the ``other-race effect,'' where algorithms developed in Western and
East Asian contexts demonstrated superior performance for their respective
majority racial groups. This disparity persisted even when datasets were
balanced, pointing to underlying biases in algorithmic design and training
processes. Klare \etal~\cite{klare2012face} observed recognition challenges for
specific demographic groups, including females, Black individuals, and younger
cohorts. They reported improved performance when models were trained exclusively
on these groups, emphasizing the impact of training data composition.
In~\cite{nagpal2019deep}, Nagpal \etal demonstrated that deep learning models
encode in-group biases, mirroring human tendencies such as own-race and own-age
effects. By analyzing activation maps, they showed that these biases were
ingrained within the feature representations of the models. Wang \etal
\cite{wang2019racial}, using the Racial Faces in-the-Wild (RFW) dataset,
validated racial bias, revealing that error rates for African faces were nearly
double those for Caucasians, even with race-balanced training data. Serna \etal
\cite{serna2019algorithmic} similarly highlighted significant performance gaps
across demographic groups, advocating for diverse training datasets and
fairness-aware algorithmic designs.

Further investigations into gender-based biases by Albiero \etal
\cite{albiero2020analysis} revealed skewed impostor and genuine score
distributions as the primary reasons for lower accuracy in women. This bias
persisted across datasets, regardless of balanced training and neutral facial
expressions. Ricanek \etal \cite{ricanek2015review} noted unique challenges in
recognizing children's faces due to structural changes with age, finding that
algorithms effective on adult faces performed poorly for younger subjects. 

%---
\input{sections/causes_table}

\subsection{Image Quality}

The quality of input images and associated covariates significantly influence
the manifestation of demographic bias in FR systems. Numerous studies have
emphasized how disparities in image quality across different demographic groups
can lead to variations in system performance. For instance, Cavazos \etal
\cite{cavazos2020accuracy} analyzed both data-driven and scenario-based factors,
revealing that dataset complexity and decision thresholds have a notable impact
on recognition accuracy and racial bias. Their experiments across multiple
algorithms further demonstrated that East Asian faces required higher decision
thresholds compared to Caucasian faces to achieve equivalent error rates,
highlighting the interplay between dataset characteristics and demographic
attributes.

The study conducted by MdTF highlighted skin reflectance as a critical factor
influencing both the accuracy and efficiency of FR
systems~\cite{cook_tbiom,cook2023demographic}. Analyzing 158 FR systems, they
found that lower skin reflectance, typically associated with darker skin-tones,
correlated with reduced accuracy and higher transaction times. These effects
varied across systems, underscoring the role of image acquisition quality as a
stronger predictor of performance as mentioned earlier. Similarly, Wu \etal
\cite{wu2023face} explored the effects of brightness and illumination,
demonstrating that under-exposed or over-exposed images result in higher FMRs,
while significant brightness differences between image pairs diminish similarity
scores. They recommended controlled image acquisition processes to achieve
consistent brightness across demographic groups, thereby reducing accuracy
disparities.

Krishnapriya \etal \cite{vangara2019characterizing} further examined how
variations in image quality contributed to performance gaps between
African-American and Caucasian cohorts. Enhancing image quality notably reduced
these disparities, particularly by minimizing low-similarity errors within the
genuine distribution. Following ICAO compliance guidelines, they evaluated
biometric sample quality to support these findings. Albiero \etal
\cite{albiero2020analysis} investigated gender-based disparities in FR systems,
linking these to differences in genuine and imposter score distributions. They
also identified confounding factors such as cosmetics and image pose. Despite
using neutral and balanced datasets, their study revealed that such measures
alone were insufficient to fully eliminate observed disparities.

Work by Lu \etal provided a detailed analysis of the influence of covariates on
FR performance, incorporating variables such as skin tone, age, gender, pose,
facial hair, and occlusion across three datasets and five FR
systems~\cite{lu2019experimental}. Their findings highlighted that skin tone
significantly affects verification accuracy, with lighter skin tones
consistently outperforming medium-dark tones. However, they also emphasized the
challenges posed by ambiguities in skin tone classification, advocating for more
precise methodologies for performance assessments. In alignment with earlier
studies, Lu \etal observed that male subjects generally achieved better
recognition accuracy than female subjects. They attributed this disparity to
factors such as occlusion caused by longer hair and alterations in facial
appearance due to makeup. These observations corroborate prior findings
indicating that facial makeup can negatively impact recognition
accuracy~\cite{dantcheva2012can, kotwal2019detection}. Collectively, these
studies underscore that demographic bias in FR systems is intrinsically tied to
image quality and related covariates, necessitating focused efforts to address
these issues systematically.

%---

\subsection{Combined or Intersectional Factors}

In the preceding sections, we examined the individual factors contributing to
demographic bias in FR. This section shifts focus to studies that investigate
the combined effects of multiple demographic attributes, such as age, race, and
gender. Vera-Rodriguez \etal \cite{vera2019facegenderid} emphasized the
significance of gender as a covariate in FR, observing that males consistently
outperform females across various demographic groups. These findings highlight
the necessity of addressing combined demographic factors to achieve equitable
outcomes.

Age-related biases have been linked to structural transformations in facial
features over time, particularly among children. Ricanek \etal
\cite{ricanek2015review} observed increased complexity of child aging compared
to adults, attributing recognition challenges to changes in facial bone
structure and the proportions of facial components. Additionally, Best-Rowden
and Jain \cite{best2017longitudinal} reported nuanced patterns in age-related
recognition performance, noting that while males generally exhibit higher
genuine scores, their performance declines more rapidly with age compared to
females. These observations underline the intricate interplay of demographic
attributes in shaping biases in FR systems. The intersection of age, race, and
gender significantly amplifies biases in FR systems. In
\cite{sarridis2023towards}, Sarridis \etal identified a disproportionately high
mistreatment rate for African females over 60 years compared to Caucasians,
illustrating the compounded effects of intersecting demographic factors.
Similarly, El Khiyari \etal \cite{el2016face} demonstrated that face
verification accuracy is notably lower for younger individuals (aged 18--30),
females, and certain racial groups such as Black individuals, highlighting the
challenges posed by such intersections of demographic factors. 

Algorithmic evaluations further reinforce these findings. The FRVT report
\cite{frvt3} observed elevated false positives among children and the elderly,
particularly within Asian and American Indian groups. These disparities were
intensified in low-quality imaging conditions, with younger and older
demographics experiencing higher error rates. Cook \etal
\cite{cook2023demographic} extended this understanding by showing that
self-reported demographic factors like age and measured skin lightness
significantly impact recognition scores, often compounded by environmental
factors such as illumination. Collectively, these studies underscore the
intricate challenges of addressing intersecting biases in FR systems,
emphasizing the necessity of age-specific and multi-faceted considerations in
algorithm design and evaluation.

%-------
