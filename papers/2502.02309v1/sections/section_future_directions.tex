\section{Future Directions}
\label{sec:future}

In recent years, with advanced architectures and increase in the number of
layers and parameters, FR models have gained a substantial improvement in their
capacity to learn complex facial representations. These deeper architectures,
often comprising over a hundred layers with millions of parameters, have
significantly enhanced the ability of FR systems to generalize across
challenging scenarios, resulting in higher overall recognition performance (as
well as reduced bias). Additionally, the availability of larger, more diverse
datasets has contributed to better learning outcomes. These datasets, which
incorporate substantial variations demographics and covariates such as pose,
illumination, and expression (PIE), have facilitated measurable progress in
improving both accuracy and fairness FR.

However, despite these advancements, several key challenges in this area
continue to exist. Most of the existing efforts primarily center on extremely
deep models, which demand extensive computational resources and memory footprint
for both training and deployment. This emphasis on high-capacity architectures
does not adequately address quality issues in data or labels nor does it cater
to the requirements of resource-constrained environments. Thus, while the
combination of deeper models and diverse data has been pivotal, future research
must explore avenues to address residual biases and expand fairness to a broader
range of applications.  

In this section, we examine some of the emerging challenges associated with
demographic bias. These challenges highlight the need for ongoing research to
adapt bias mitigation strategies to align with the advancements in FR
applications.\\
%----

\noindent\textbf{Bias in Lightweight Models:}
The lightweight FR models, often used in handheld devices and
resource-constrained environments, encounter significant challenges concerning
demographic bias. These systems, crucial for privacy-sensitive applications,
often inherit limitations in capacity and architecture, leading to non-equitable
performance across demographic groups.  Bias in lightweight models has garnered
limited research attention, despite their widespread deployment in mobile and
IoT devices with varying sensor qualities. Techniques like knowledge
distillation (KD) and pruning, while essential for model compression, introduce
or amplify bias. For instance, Liu \etal highlighted that KD inherits biases
from larger teacher models \cite{liu2021rectifying}, while pruning has been
shown to disproportionately impacts underrepresented
groups~\cite{paganini2020prune, iofinova2023bias}.
%
Incorporating fairness-aware techniques is crucial for mitigating these issues.
Lin \etal \cite{lin2022fairgrape} introduced FairGRAPE, a pruning method that
evaluates network connections with demographic considerations, reducing
performance disparities. Achieving demographic fairness in lightweight models
requires targeted compression strategies and ethical evaluations of
demographic-specific impacts. By integrating fairness principles into
compression techniques, lightweight FR systems can achieve more equitable
outcomes while maintaining efficiency and accuracy.\\

%----

\noindent\textbf{Bias in Quantization:}
Quantization, a model compression technique, retains the original architecture
(as opposed to KD and pruning-- which often modify the structure) while reducing
parameter precision, producing smaller and faster models. However, it can lead
to higher demographic bias by prioritizing global performance over the accurate
classification of under-represented groups. Such bias underscores the need for
rigorous fairness evaluation across demographic subgroups when deploying
compressed models.
%
Quantization converts floating-point (FP) models into lower-precision formats
like 8-bit, balancing efficiency and accuracy~\cite{jacob2018quantization}. It
typically consists of two approaches: post-training quantization (PTQ) and
quantization-aware training (QAT). 
%PTQ applies quantization strategies to pre-trained FP networks, while QAT
%incorporates simulated quantization during training, better aligning parameters
%with reduced precision.
Some studies, such as Stoychev \etal demonstrated that 8-bit PTQ maintained
fairness and accuracy in gender bias for face expression recognition
\cite{jacob2018quantization}. However, reducing precision to 6 bits
significantly degraded fairness, indicating a trade-off between compression and
bias mitigation. Although similar investigations for FR systems remain limited,
these findings highlight the need to evaluate and ensure demographic fairness in
quantized models.\\

%-----

\noindent\textbf{Low Resolution:}
The research on demographic bias in FR has predominantly focused on
high-resolution images, often overlooking the challenges posed by low-resolution
images typically captured by surveillance cameras or from significant distances.
One of the primary impediment to research this issue is the scarcity of
low-resolution datasets that include demographic attributes. Consequently,
demographic disparities in low-resolution FR remain under-explored, despite
their importance in real-world applications.
%
A recent work from Atzori \etal attempted to address this gap by designing a
novel framework to investigate demographic bias in low-resolution FR
\cite{atzori2023demographic}. They trained state-of-the-art FR models on various
combinations of high- and low-resolution images. Testing on degraded images from
five datasets revealed significant disparities across gender and ethnic groups,
underscoring the need for timely interventions in low-resolution FR. It may be
noted that their approach involved use of a generative model to convert
high-resolution face images into realistic low-resolution counterparts.
%
The importance of low-resolution FR is evident in programs like
BRIAR\footnote{\href{https://www.iarpa.gov/research-programs/briar}{BRIAR
Programme} by IARPA.}, which aim to enhance recognition technologies for
challenging scenarios, such as long-distance identification and low-quality
image acquisition. To tackle demographic bias in low-resolution FR, there is a
need to develop both datasets and models tailored to these unique use-cases.\\
%--- 

\noindent\textbf{Training Datasets:}
Demographically balanced datasets, while not entirely eliminating bias,
significantly contribute to reducing non-equitable performance in FR. Thus,
large, diverse, and balanced datasets are pivotal for achieving fairer and
accurate models. However, acquiring such datasets is increasingly challenging
due to cost and ethical and privacy concerns surrounding biometric data
collection. The commercial sector, in particular, faces difficulties as most
available datasets are collected locally from consenting individuals--- which
are often in limited size and demographic representation. These constraints
necessitate innovative approaches to address demographic bias using smaller or
synthetic datasets.
%

The use of synthetic data in FR has recently gained traction as a potential
solution to privacy and data-sharing concerns. Competitions like the FRSyn
series have encouraged advancements in synthetic data
usage~\cite{melzi2024frcsyn1, deandres2024frcsyn, melzi2024frcsyn}. Despite
these efforts, FR models trained exclusively on synthetic datasets continue to
underperform compared to those trained on real datasets of similar
size~\cite{melzi2024frcsyn, george2024digi2real}. This gap is evident in both
recognition accuracy (measured by metrics like FMR and FNMR) and demographic
fairness (assessed by standard deviation of performance across groups). The
analysis of synthetic datasets by Huber \etal revealed that demographic bias
might worsen compared to the (real) training dataset~\cite{huber2024bias}.
Enhancing the quality and utility of synthetic datasets, beyond the aspect of
bias, remains an open problem, requiring further exploration.\\

%-----

\noindent\textbf{Use-Cases of Remote Checking:}
Last few years have witnessed tremendous surge in online activities: financial
Transactions, banking, user-onboarding, etc. These activities have driven
widespread adoption of remote identity verification (RIdV) technologies. These
systems authenticate individuals by comparing real-time images or selfies,
captured via smart devices, against official identity documents, such as work
permits or driver's licenses. Such solutions are integral to online Know Your
Customer (KYC) processes, which are now standard for banks and financial
institutions. While RIdV systems enhance convenience and scalability, it is
essential to ensure their fairness across demographic groups as they become more
prevalent.

Recognizing the increasing reliance on remote verification, the MdTF and DHS
S\&T introduced the Remote Identity Validation Technology Demonstration (RIVTD)
initiative\footnote{\href{https://mdtf.org/rivtd}{Remote Identity Validation
Technology Demonstration (RIVTD)}}. In addition to security, accuracy, and
liveness detection requirements, this program also places particular emphasis on
ensuring demographic fairness in such technologies.
%
A recent study by Fatima \etal\cite{fatima2024large} investigated demographic
fairness in RIdV technologies using statistical methods to analyze performance
disparities. Their analysis of five commercial RIdV systems revealed that only
two achieved equitable outcomes across demographic groups. Notably, higher FNMRs
were observed among African cohorts and individuals with darker skin-tones. Such
findings highlight the necessity of evaluating RIdV technologies across
demographic groups to ensure equitable and unbiased performance. \\
%----

\noindent\textbf{Complex Bias Factors (Intersectionality):}
The majority of research on mitigation of bias in FR, as discussed in
Sec~\ref{sec:mitigation} has focused on single demographic attribute, such as
race, age, or gender. However, several studies have identified that combination
or intersection of various demographic factors causes (or amplifies) bias in FR
models (cf. Sec~\ref{sec:causes}). Existing bias mitigation techniques typically
target one demographic attribute at a time, achieving measurable improvements in
fairness for that specific attribute. However, it remains unclear whether such
processing inadvertently introduces imbalances in other demographic attributes.
For instance, enhancing fairness for gender-related bias may increase
disparities linked to ethnicity and vice-versa. This highlights the need for
systematic evaluations of the intersectionality of demographic factors, such as
race and gender combined. Consequently, developing mitigation methods capable of
addressing multiple demographic attributes simultaneously remains an open
challenge. \\
%----

\noindent\textbf{Noisy Labels:}
The assignment of demographic attributes such as race, ethnicity, and skin-tone
in FR datasets typically involves discrete labeling into finite categories. Some
attributes, such as race, are often self-reported. In many cases, race and
ethnicity annotations may be derived from automatic classifiers or manual
efforts. Automatic classifiers, predominantly based on deep learning, are likely
to be susceptible to bias too; while manual annotations are prone to human
judgment errors. Similarly, skin-tone is frequently categorized using scales
like Fitzpatrick's, which discretizes it into specific values, ignoring its
continuous spectrum. This reliance on discrete labels introduces noise into the
training data, as samples near category boundaries are often inaccurately
labeled. Recent work \cite{kotwal2024demographic}, addressed this issue by using
probabilistic weights (soft labels) for demographic information instead of
utilizing rigif (categorical) labels. However, most existing methods overlook
the issue of errors in training data.
%

Noisy labels in training datasets pose a significant challenge, especially
considering massive scale of FR datasets, often in few hundreds of thousands of
images. Manually verifying or curating such datasets is labor-intensive,
impractical, and still prone to errors. Furthermore, removing samples with
ambiguous labels can lead to reducing dataset diversity and robustness. Thus,
developing robust mitigation strategies capable of handling noisy labels without
compromising the effectiveness of training processes or dataset diversity is
essential for improving fairness and accuracy in FR systems.
%----
