
%-------------------------------------
\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\linewidth, height=0.3\textheight]{images/mitigation_methods}
\caption{Illustration of categories of methods of bias mitigation in FR.}
\label{fig:mitigation_methods}
\end{figure*}
%-------------------------------------
    
\section{Bias Mitigation in Face Recognition Systems}
\label{sec:mitigation}

In an FR system, bias mitigation can be applied at different stages of the
recognition pipeline, providing a structured approach to addressing demographic
disparities. These stages align with general categories of bias mitigation
strategies commonly used in machine learning: pre-processing, in-processing,
and post-processing~\cite{pessach2022review, mehrabi2021survey,
singh2022anatomizing, hort2024bias}.

Pre-processing methods aim to address bias at the level of training data by
modifying or augmenting it to reduce discriminatory patterns or imbalances
before the data is used for training. This approach is particularly relevant
when representational disparities in the data contribute to demographic bias.
The in-processing methods focus on modifications to the FR model during training
or fine-tuning phase, often by incorporating constraints or objectives that
optimize fairness without significantly compromising recognition accuracy.
Finally, post-processing techniques involve adjustments to the output of trained
models to ensure fairness across demographic groups. These techniques modify the
results, such as classification scores or decision thresholds, to achieve
equitable performance without altering the underlying model.
Fig.~\ref{fig:mitigation_methods} summarizes the categorization of bias
mitigation methods-- that correlates with the corresponding stages of FR
pipeline. 

\subsection{Pre-Processing Methods}

Pre-processing methods, also known as data-based methods, focus on modifying
the biometric samples before feeding into the FR system. These techniques aim
to normalize the characteristics of the data to make them robust for
subsequent feature extraction. Most of the methods in this category can also be
regarded as special case of data augmentation, specifically designed to reduce
demographic biases in the training data.

In \cite{klare2012face}, Klare \etal showed that FR algorithms were performing
worse for female, Black, and younger individuals. To address this concern, they
proposed two mitigation strategies based on selection of training
data. First, training models on specific demographic cohorts to enhance
recognition for those groups, and second, implementing a dynamic face matcher
selection approach, where different algorithms, each trained on distinct
demographic groups, would be chosen based on probe information.  A similar
approach was followed by Deb \etal in the context of longitudinal study of
FR~\cite{deb2018longitudinal}.  Using the data of more than 900 children
captured over time, they demonstrated that recognition accuracy decreases as
the time gap between image captures grows. To address this, they fine-tuned
FaceNet~\cite{schroff2015facenet} on a separate child face dataset, showing
improved accuracy.  The authors advocated the need for tailored training and
evaluation of FR systems for different age groups. Lu \etal
\cite{lu2019experimental} investigated the influence of covariates—such as age,
gender, pose, and skin tone— toward the performance of face verification.
Their findings showed that using gender information to curate training data
improves performance, particularly at low FMRs.
%
In \cite{kortylewski2019analyzing}, Kortylewski \etal demonstrated the
effectiveness of using synthetic face images to mitigate the bias arising from
real-world datasets. By utilizing synthetic data for pre-training the FR model,
they showed that the negative effects of dataset bias, particularly with regard
to pose variations, could be significantly reduced. Their experiments revealed
that pre-training the CNNs with synthetic data, the need for real-world data
can be reduced by up to 75\%. Although this study did not directly address
demographic bias, we include it in the review as it highlights the potential of
synthetic data in improving the generalization performance of FR systems and
reducing (non-demographic) dataset bias.

Wang \etal explored the issue of highly-skewed class distributions in FR
datasets~\cite{wang2019deep}. They proposed Large Margin Feature Augmentation
(LMFA) and Transferable Domain Normalization (TDN) as methods to balance class
distributions by augmenting and normalizing the feature space. These methods
were shown to enhance the performance of underlying FR models by mitigating
issues arising from class imbalance, which often correlates with demographic
bias in under-represented groups.
%
In \cite{yin2019feature}, Yin \etal introduced a center-based feature transfer
framework to address the under-representation of certain demographic groups in
FR datasets. By transferring feature distributions from well-represented
classes to under-represented ones, they augmented the feature space for these
groups, reducing bias and improving recognition performance for
under-represented subjects. 
%
Recently, Kotwal and Marcel proposed an Image-to-Image
transformation module called Demographic Fairness Transformer (DeFT), which
enhances image representations before passing them to pretrained CNNs
\cite{kotwal2024demographic}. The DeFT uses multi-head encoders and
soft-attention mechanisms to selectively enhance images based on inferred
demographic information. The demographic labels of race or ethnicity are often
non-discrete-- but this concern has rarely been addressed. In
\cite{kotwal2024demographic}, they replaced hard labels with probabilistic
weights which are implicitly inferred at run-time. Their experiments show that
DeFT reduces bias and improves model fairness, with some models also achieving
slightly better accuracy compared to baseline systems.
%-----

\subsection{In-Processing Methods}

This category of works, also known as model-based methods, are applied during
the feature extraction stage by modifying the weights of the FR model. The goal
is to learn weights that generate features or embeddings that are less sensitive
to demographic differences.

Amini \etal \cite{amini2019uncovering} proposed a debiasing algorithm that
adjusts the sampling probabilities of data points in large datasets to reduce
hidden biases. When applied for face detection use-case, their algorithm led to
decrease in race and gender bias while improving classification accuracy. To
our knowledge, similar approaches have not been tested for recognition or
verification applications.
%
In \cite{wang2020mitigating}, Wang and Deng introduced a reinforcement
learning-based race balance network (RL-RBN) where they applied adaptive
margins through deep Q-learning. Their method aimed to reduce the skewness
of feature scatter between racial groups, leading to more balanced performance
across different demographics. As a part of this work, they also released two
datasets- BUPT-GlobalFace and BUPT-BalancedFace datasets- that were
specifically designed to study racial bias in FR systems. 

Gong \etal proposed a group-adaptive classifier (GAC) that uses adaptive
convolution kernels and attention mechanisms tailored to different demographic
groups \cite{gong2021mitigating}. By applying kernel masks and attention maps
specific to each group, their method activates facial regions that are more
discriminative for each demographic, thereby improving recognition accuracy and
fairness across demographic groups. 
%
Another approach by the same authors \cite{gong2020jointly} introduced an
adversarial network, called DebFace, which utilizes a multi-task learning
framework to simultaneously learn identity and demographic attributes. Their
method employed adversarial training to disentangle identity features from
demographic attributes such as gender, age, and race to effectively reducing
bias in the recognition process. Their experiments demonstrated that, for
DebFace, not only recognition but also demographic attribute estimation tasks
were less biased.

\input{sections/table_mitigation_methods}

Another approach for domain-specific bias mitigation using disentangled
representation learning was proposed by Liang \etal \cite{liang2019additive}.
They introduced a two-stage method combining modules for disentangled
representation learning with additive adversarial learning (AAL). While this
work does not directly address demographic bias, it provides useful insights
into how domain-specific biases can be mitigated by learning disentangled
representations. The effectiveness of this method in reducing bias across
various domains suggests its potential applicability in the context of
demographic bias in FR.
%
In \cite{li2021learning}, a Progressive Cross Transformer (PCT) was proposed to
mitigate racial bias by decoupling face representations into identity-related
and race-induced components. Using dual cross-transformers, the PCT refines
identity features and suppresses racial noise, demonstrating lower racial bias
without compromising recognition accuracy.
%
The concept of score normalization was incorporated as a regularization term
into the training objective  enabling simultaneous optimization of recognition
accuracy and demographic fairness~\cite{kotwal2024WACV}. This was facilitated
by constraining the output scores of mated and non-mated pairs to adhere to a
pre-defined distribution, and followed by minimizing differences in score
distributions across demographic groups. During inference, the overall
pipeline did not require modifications as the FR CNN architecture remained
unaltered while only the weights were fine-tuned to the new objective.

Finally, we discuss a couple of works dealing with fairness in facial attribute
recognition. Both of these are based on contrastive learning-- which can be
useful mechanism to address the demographic bias in FR as well. Park \etal
addressed fairness issues in attribute classification using a contrastive
learning framework~\cite{park2022fair}. They constructed a Fair Supervised
Contrastive Loss (FSCL) which reduces bias by normalizing intra-group
compactness and inter-group separability, penalizing sensitive attribute
information in representations. Although predominantly an in-processing method,
the authors also incorporate a loss function to address the imbalance in
training data where majority groups are constrained to have a better intra-
group compactness and inter-class separability compared to the
under-represented ones.
%
Similarly, Zhang \etal proposed Fairness-aware Contrastive Learning
(FairCL) for unsupervised representation learning and demonstrated the use-case
of facial attribute recognition \cite{zhang2022fairness}. In addition to fair
contrastive learning of feature representations, they also attempted to address
the dataset bias by specifically generating contrastive sample pairs that share
the same visual information apart from sensitive attributes. They also
suggested unsupervised feature reweighting to strike balance the utility and
fairness of learned representations.

%----
\subsection{Post-Processing Methods}

These techniques are applied after the feature extraction and matching
processes to adjust the final decision scores and reduce bias. While these
methods are less commonly used compared to pre- and in-processing techniques,
they can still play a role in ensuring equitable outcomes. Additionally, these
approaches are relatively easy to integrate into existing systems.

Michalski \etal investigated the impact of age variation on FR, particularly
for children~\cite{michalski2018impact}. They showed that dynamic thresholding
improves performance. To address age-related bias, they adjusted thresholds
based on age differences as opposed to a fixed threshold. In another work on
age-related bias, Srinivas \etal experimented with score-level fusion
strategies to improve recognition accuracy for the children (age as
demographic) \cite{srinivas2019face}. They considered six fusion schemes that
combined different score-normalization techniques and fusion rules. For
normalizing, they considered z-norm and min-max strategies; while fusion was
conducted using min, max, or sum rules.
%
Robinson \etal \cite{robinson2020face} showed that applying a single threshold
across different demographic groups leads to significant variations in the FMR.
They addressed this issue by using per-subgroup thresholds to balance the FMRs
across ethnic and gender groups, improving both recognition fairness and
performance. 

A typical FR pipeline employs similarity functions to obtain a matching score.
Terhorst \etal  replaced the conventional similarity function by a
fairness-driven neural network classifier \cite{terhorst2020comparison}. By
adding a penalization term in the loss function, their method was able to
equalizes score distributions across ethnic groups, reducing intra-ethnic bias
while maintaining high recognition performance. 
%
In another work \cite{terhorst2020post}, Terhorst \etal introduced an
unsupervised fair score normalization method based on individual fairness
principles, which treats similar individuals similarly. During training, they
partitioned the identities in finite number of groups using K-means clustering
on face embeddings. At inference, they computed the cluster-specific
thresholds for both samples contributing to the score, and  these threshold
were combined with a global threshold to yield normalized scores.
%
In a recent work, Linghu \etal extended traditional score normalization
methods, such as Z and T normalization, by incorporating demographic
information to enhance fairness in FR systems~\cite{linghu2024ijcb}.
Furthermore, they evaluated three cohort-based approaches based on imposter
scores, Platt scaling, and bi-modal cumulative distribution functions (CDF).
Their findings demonstrated that the proposed method improved fairness across
both race and gender demographic groups, particularly at low FMRs.
%-----

The works on mitigation are summarized in Table~\ref{tab:mitigation}.

%---

