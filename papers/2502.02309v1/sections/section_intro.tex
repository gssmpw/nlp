\section{Introduction}
\label{sec:intro}

\IEEEPARstart{D}{emographic} bias in face recognition (FR) systems has emerged as a critical
challenge in the deployment of biometric technologies for real-world
applications~\cite{rathgeb2022demographic, sixta2020fairface,
drozdowski2020demographic, jain2021biometrics}. Bias in these systems often
leads to disparities in performance across demographic groups- such as
variations in recognition accuracy- based on race, gender, and
age~\cite{jain2021biometrics, mehrabi2021survey, howard2019effect}. Such biases
can have far-reaching consequences, especially in critical applications like
border crossing, law enforcement~\cite{limante2024, jones2020law},
security~\cite{leslie2020understanding}, and hiring
processes~\cite{burgess2022watching, raji2020saving, pena2020bias}, where
fairness and accuracy are paramount. Fig.~\ref{fig:intro} illustrates the issue
of demographic bias based on race, gender, and age. Ideally, a fair model should
exhibit equitable performance across all demographic groups (\ie similar error
rates or comparable score distributions). The disparity between groups
contributes to the persistence of demographic bias. This issue is further
compounded by the growing reliance on FR technologies, making it imperative to
identify, evaluate, and mitigate sources of bias effectively.

\begin{figure}[!t]
\centering
\begin{subfigure}{0.9\columnwidth}
\includegraphics[width=\columnwidth]{images/diagrams_race}
\caption{}
\label{fig:race1}
\end{subfigure}
%
%\vspace{6mm}
%
\begin{subfigure}{0.8\columnwidth}
\includegraphics[width=\columnwidth]{images/diagrams_gender}
\caption{}
\label{fig:gender1}
\end{subfigure}
%
%
\begin{subfigure}{0.8\columnwidth}
\includegraphics[width=\columnwidth]{images/diagrams_age}
\caption{}
\label{fig:age1}
\end{subfigure}
%
\caption{Schematic illustration of issue of demographic bias in FR with reference to 
different demographic factors: (a) race or ethnicity, (b) gender, and (c) age.}
\label{fig:intro}
\end{figure}

Due to its severity and widespread range of applications, demographic bias has
emerged as a crucial area of research, drawing significant attention from both
the biometrics and computer vision communities~\cite{drozdowski2020demographic,
jain2021biometrics, ross2019some, busch2024challenges}. This issue has been
formally incorporated into the evaluation frameworks of prominent initiatives,
such as the National Institute of Standards and Technology's (NIST) Face
Recognition Vendor Tests (FRVT), which have included demographic effects in
their reports since 2019~\cite{frvt3, grother2021demographic}. Similarly, the
Maryland Test Facility (MdTF), supported by the United States Department of
Homeland Security (DHS), has conducted biometric technology rallies to evaluate
demographic disparities in FR systems\footnote{\href{https://mdtf.org}{MdTF}}.
In Europe, organizations like the European Association for Biometrics (EAB) have
hosted dedicated events on demographic fairness in biometric systems,
underlining the global importance of this
topic\footnote{\href{https://eab.org/events/program/237}{EAB event on
Demographic Fairness in Biometric Systems}}. This research area is often
positioned within the broader context of fairness and trustworthy biometrics,
and has been receiving substantial attention-- in the form of papers, workshops,
or special sessions-- from leading conferences such as IEEE/CVF CVPR, WACV, IEEE
FG, and ICPR, and from reputable journals including IEEE Transactions on
information forensics and security (TIFS), IEEE Transactions on Biometrics,
Behavior, and Identity Science (TBIOM)~\cite{9833480} and IEEE Signal Processing
Magazine~\cite{cheong2021hitchhiker}. Additionally, standards organizations have
recognized the need for systematic approaches, with ISO/IEC 19795-10 recently
publishing guidelines for quantifying demographic differentials in biometric
systems emphasizing the need for addressing bias in FR
technologies~\cite{british2023iso}.\\
%ICPR 2022, IEEE FG 2024, and CVPR 2024, 

\noindent\textbf{Scope:} Fairness and bias in machine learning are expansive
topics, and their application in biometrics has drawn significant attention in
recent years. Several comprehensive reviews have addressed fairness and bias in
machine learning broadly~\cite{mehrabi2021survey, pessach2022review}, while
others focus specifically on biometrics, offering insights into various
modalities such as face, fingerprint, and vein, alongside applications beyond
recognition, including region of interest (ROI) detection, quality assessment,
and presentation attack detection\cite{drozdowski2020demographic,
jain2021biometrics, ross2019some, yucer2023racial}. However, as face remains the
most commonly used biometric trait, a substantial portion of research on
demographic bias has concentrated on this modality. 

While previous reviews offer broad perspectives, the extensive literature and
emerging challenges specific to demographic bias in FR necessitate a dedicated
review. In this article, we provide a consolidated discussion of recent
advancements in the field, addressing the causes of demographic bias, available
datasets for research, evaluation metrics for assessment of bias, and recent
mitigation techniques. Furthermore, we explore ongoing challenges that persist
in addressing demographic disparities, particularly in the light of novel use
cases and emerging FR applications. Although our primary focus is on race and
ethnicity, as these are dominant areas of research, we also include
gender-related studies within the broader context. Age-related studies, given
their distinct nature and established body of research, are referenced only
where directly relevant.\\

\noindent\textbf{Naming Conventions:} In this review, we adopt the terminologies
used in the referenced works while acknowledging the minor differences in naming
conventions. Terms such as \textit{race} and \textit{ethnicity} are often used
interchangeably, although they represent distinct concepts. For clarity, we
retain the terms employed by the original studies. Similarly, the names of
ethnic groups vary across the literature: for instance, some works use terms
like Black and White, while others prefer African and Caucasian. Additionally,
the South Asian group is sometimes referred to as Indian, whereas Asians often
refer to East Asians. To maintain consistency and respect the source material,
we adhere to the original terminologies in this review. Readers are encouraged
to refer to the cited works for precise definitions and context. While fairness
in related topics such as face detection, image quality, expression recognition,
and attribute estimation is of interest to the research community, this review
exclusively focuses on demographic bias in FR.\\

\noindent\textbf{Contributions:} This review constitutes the first comprehensive
work dedicated to exploring demographic bias in FR, offering a unified and
holistic perspective to researchers in the field. We systematically analyze and
organize key aspects, including the causes of demographic bias, available
datasets, assessment metrics, and mitigation techniques, providing a structured
framework for understanding these areas. Finally, we identify emerging
challenges and unresolved questions, inviting further research and innovation to
advance fairness in FR systems.

The structure of this paper is as follows: Section~\ref{sec:causes} explores the
causes of bias, analyzing factors such as distribution of demographic groups in
dataset, skin-tone, image quality, and algorithmic sensitivities. We provide an
overview of datasets commonly used for bias-related research in
Section~\ref{sec:datasets}, highlighting their demographic attributes and
suitability for specific tasks. Section~\ref{sec:assessment} reviews existing
metrics for bias evaluation, discussing their strengths and limitations.
Section~\ref{sec:mitigation} outlines recent bias mitigation strategies across
different stages of the FR pipeline. We discuss open challenges and future
research directions in Section~\ref{sec:future}, and conclude the review in
Section~\ref{sec:conc}.

%---
