\section{Assessment of Demographic Bias in FR}
\label{sec:assessment}

For an FR system, the FMR represents the proportion of matching scores where a
non-mated pair of biometric samples (\ie samples from different subjects) is
incorrectly classified as a match, while FNMR corresponds to the scenarios where
a mated pair (\ie samples of the same subject) is incorrectly classified as
non-matching. These metrics are essential for evaluating the performance of FR
algorithms-- where lower values of error rates are preferred.  These errors are
determined by the score threshold (often denoted as $\tau$) that binarizes the
similarity score into match or no-match decision. As illustrated in
Figs.~\ref{fig:race1}--\ref{fig:age1}, the overlap between distributions of
mated scores (shown in green) and non-mated scores (shown in red) defines the
regions where classification errors (\ie false matches and false non-matches)
occur.


% dataset dist
%-------------------------------------
\begin{figure}[!t]
\centering
\includegraphics[width=0.9\columnwidth]{images/score_dist_plot}
\caption{Illustration of false matches and false non-matches arising from
distributions of mated and non-mated scores along with the score
threshold.}
\label{fig:score_dist_plot}
\end{figure}
%-------------------------------------

The choice of score threshold plays significant role in FMR and FNMR. A higher
threshold reduces FMR by limiting the likelihood of imposter pairs being
classified as matches but increases FNMR by rejecting more genuine pairs, and
vice-versa.  However, it is essential to distinguish between score overlap and
binary decision-making. The overlap reflects the inherent ambiguity in the
score distributions, while binarized decisions result from applying the
threshold to assign matches or non-matches. For instance, two demographic
groups with identical overlap may exhibit different FMR and FNMR due to
variations in their score distribution shapes or population characteristics.
Similarly, the distributions (mated, non-mated, or both) for two demographic
groups can be significantly different albeit exhibiting same error rates.

The demographic bias in FR system leads to variations in score distributions
and their overlaps across different demographic groups. Such disparities inherently
result in different FMR and FNMR values for each group when a single (global)
threshold is used. Fig.~\ref{fig:score_dist_plot} illustrates score
distributions of different groups may differ, leading to unequal error rates.
In this section we briefly review performance measures designed for
demographic-aware assessments of bias in FR systems.
%
Given the overlap and distinct characteristics of scores and decisions, 
establishing a well-defined evaluation framework is crucial for accurate analysis.
Howard \etal~\cite{howard2019effect} introduced the concepts of differential
performance and differential outcomes, providing two key terms that aid in
achieving a precise understanding and categorization of assessment metrics.

\begin{itemize}
\item \textbf{Differential Performance:} Variations in genuine (mated) or
imposter (non-mated) distributions across demographic groups, independent of
thresholds.
\item \textbf{Differential Outcome:} Differences in FMRs or FNMRs between
groups, based on decision thresholds.
\end{itemize}

Quantifying demographic bias, in terms of both- demographic performance and
outcomes, is critical for developing fair and reliable FR systems. By analyzing
the overlap of distributions and error disparities, one can identify specific
areas requiring intervention to ensure equity across diverse user groups.

Since the 2019 Face Recognition Vendor Test (FRVT)\footnote{Since 2023, the
FRVT initiative has been restructured into the Face Recognition Technology
Evaluation (FRTE) and Face Analysis Technology Evaluation (FATE) programs.}
report, NIST has included demographic effects in FR algorithms~\cite{frvt3}.
This assessment involved comparing non-mated pairs within the same demographic
group, setting thresholds for algorithms to achieve an FMR of 0.001 for white
males (since this demographic typically associated with the lowest FMR). The
report~\cite{frvt3} offered a comprehensive analysis of recognition processes
and identified areas where demographic effects might occur. 
%
To quantify demographic disparity, NIST initially employed Inequity Ratios
(IR), calculating the ratio of maximum to minimum of FMR and FNMR across
demographic groups. However, considering potentially large range of the error
rates, these ratios can become numerically unstable, especially in extreme
cases. To alleviate this shortcoming, NIST has considered few modified versions
of Inequity Ratios such as adjusting the score threshold ($\tau$) score,
incorporating fixed constants in the denominator, or expressing worst-case
error rates relative to arithmetic or geometric means~\cite{grother2022face}.
Using the geometric mean is particularly advantageous due to its extended range
over FMR/FNMR values. Another possible approach involves referencing a standard
FMR or FNMR in the denominator, which inherently resolves stability issues
while providing more robust evaluations of demographic bias.

The Fairness Discrepancy Rate (FDR) is one of the initial efforts of quantifying
bias in FR~\cite{de2021fairness}. For an FR system using a single decision
threshold, the FDR combines FMR and FNMR using weighted sums, and evaluates
fairness through a unified measure that captures the trade-offs between both
error rates. The FDR requires two hyper-parameters: one for the score threshold,
and another for defining relative importance of FMR over FNMR. Thus, it offers
flexibility of assessing the fair nature of the model at pre-defined score
threshold and application-dependent weighing of false matches to false
non-matches.  In a similar vein, to define the balance between false matches and
false non-matches, NIST proposed an approach to calculate the Inequity Measure
by raising the terms representing demographic disparities to specific exponents
(serving as weights) and then multiplying them. 

% summary table
\input{sections/table_assessment_methods}
%

Schuckers \etal highlighted the importance of accounting for statistical
variation when evaluating fairness in FR
systems~\cite{schuckers2022statistical}. They noted that the differences among
demographic groups can arise either from actual performance disparities or by
chance due to sampling variability, leading to potential Type-I errors. To
address this, they proposed two statistical methodologies: a bootstrap-based
hypothesis test and a simpler test methodology tailored for non-statistical
audiences. Their study also conducted simulations to explore the relationship
between margin of error and factors such as the number of subjects, attempts,
correlation between attempts, underlying FNMRs, and the number of demographic
groups.
%
In~\cite{howard2022evaluating}, the researchers at MdTF proposed new metric for
assessment of bias based on demographic outcomes. Their metric, GARBE (Gini
Aggregation Rate for Biometric Equitability), is inspired by the Gini
coefficient-- which has a long history of use as a dispersion measure in
socio-economic context. The GARBE evaluates statistical dispersion in error
rates and emphasizes equitable treatment across demographic groups. Similar to
the FDR and IR, this metric combines weighted contributions of FMR and FNMR to
produce a single fairness score for a given fixed score threshold.

Villalobos \etal proposed the Mean Absolute Percentage Error (MAPE) as a metric
to quantify differences in error rates across demographic
groups~\cite{villalobos2022fair}. MAPE measures the relative deviation of FMRs
from a policy-defined FMR, ensuring that low error rates for one group do not
mask higher error rates for another. High deviations in error rates,
particularly towards lower values of FMR, can negatively impact the system by
increasing FNMR. A MAPE score of zero indicates that all demographic groups
achieve the desired FMR, making it an effective metric for fairness evaluation.
%
The Sum of Group Error Differences (SED\textsubscript{G}) was introduced as the
fairness assessment metric address disparities in biometric verification systems
in \cite{elobaid2024sum}. The SED\textsubscript{G} calculates relative
deviations in FMR and FNMR across demographic groups from the FMR/ FNMR of
global scores. They consider the Equal Error Rate (EER) threshold as a reference
to compute the error rates. In other words, it adapts a relative difference
formula to quantify demographic bias by comparing individual group performances
to a global standard. Authors argue that by incorporating both
within-demographic (WDI) and cross-demographic (CDI) interactions,
SED\textsubscript{G} is able to provides provide better understanding of the
magnitude and type of bias making it a versatile measure.
%--- 

Relatively fewer attempts have been made to assess the demographic bias at
score-level (ie, based on differential performance).  Kotwal and Marcel
introduced three fairness evaluation measures that emphasize the separation,
compactness, and distribution of genuine and impostor
scores~\cite{kotwal2022fairness}. Unlike conventional approaches that depend on
system accuracy, these measures focus on assessing differential performance
without requiring external parameters such as score thresholds. By examining how
well the match is, rather than merely determining a match, this approach
provides a more nuanced evaluation of demographic fairness. Additionally, they
also discussed a weighted fusion strategy to improve relative contributions from
under-represented groups, addressing the challenges posed by imbalanced
datasets.
%
Building on the work from \cite{kotwal2022fairness}, Solano \etal developed the
Comprehensive Equity Index (CEI)-- combining error rate differences and
recognition score distribution disparities~\cite{solano2024comprehensive}.  The
CEI enhances bias quantification methods by considering both the distribution
tails and overall shapes of score distributions, enabling the detection of
subtle biases across demographic groups. They also conducted experiments on high
performing FR systems (as per NIST evaluations) using real challenging datasets.
Their experiments showed that CEI was able to effectively capture the
demographic bias on several challenging datasets with several covariates.
%--- 

Several studies have evaluated bias and fairness in FR systems using the
standard deviation of performance metrics calculated across demographic groups.
These metrics include FMR, FNMR, and True Match Rate (TMR)-- where higher value
in standard deviation corresponds to greater demographic
disparities~\cite{terhorst2021comprehensive, gong2020jointly,
lu2019experimental, robinson2020face, wang2020mitigating, villalobos2022fair}.
Another significant metric is the Skewed Error Ratio (SER), which specifically
focuses on worst-case error ratios, providing insights into the performance
imbalance across groups~\cite{villalobos2022fair, wang2020mitigating,
kotwal2024WACV}. Recent competitions~\cite{melzi2024frcsyn, melzi2024frcsyn1,
deandres2024frcsyn} exploring the use of synthetic data for FR and bias
mitigation have adopted a trade-off performance metric: the mean accuracy
adjusted by the standard deviation. This metric aims to ensure that efforts to
mitigate bias do not come at the expense of recognition performance. This
metric emphasizes the development of FR models that achieve both high
recognition performance and fairness across constituent demographic groups.

Table~\ref{tab:assessment} provides a brief summary of the bias assessment
metrics discussed in this section.
%---

