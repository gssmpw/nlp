\section{Evaluation}
\label{experiments}

We demonstrate the effectiveness of our approach by replacing static similarity thresholds with \textit{VectorQ}, highlighting the benefits of dynamic and embedding-specific threshold regions for semantic prompt caches. We show that \textit{VectorQ} achieves higher cache hit rates, lower error rates, imposes minimal latency overhead, and we include an ablation study to evaluate the impact of correctness sampling.

\textbf{Server and Model Configuration}. We deploy a VM on Google Cloud Platform using an e2-standard-4 instance (4 vCPUs, 2 cores, 16 GB memory). For inference, we use the vLLM engine with 0.8 GPU memory utilization on NVIDIA A100-SXM4-80GB GPUs—one GPU for Llama-3.1-8B and three GPUs for Llama-3.1-70B. To ensure deterministic outputs, we set the LLM temperature to 0.0. We evaluate VectorQ's dynamic, embedding-specific thresholds against a semantic prompt cache using a fixed static threshold. To assess generalizability, we test with two embedding models—gte-large-en-v1.5 (1024 dimensions) and e5-mistral-7b-instruct (4096 dimensions)—combined with two LLMs: Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct. The embeddings are stored in the HNSWLIB vector database \cite{malkov2018efficient} and compared using cosine similarity, a commonly used metric in semantic prompt caches \cite{bang2023gptcache, dasgupta2025wallmartcache}. The prompt construction is explained in Appendix \ref{app:datasets}

\textbf{Key Metrics}. We evaluate each dataset by measuring total latency and tracking the number of correct and incorrect cache hits. A cache hit is considered correct if the cached LLM-generated response of the nearest neighbor character matches the LLM-generated response for the candidate. Character-based matching ensures strict correctness in our evaluation. Otherwise, the cache hit is classified as incorrect because the returned response deviates from the expected result. The error rate reflects the number of incorrect cache hits over the number of processed entries.

\textbf{Datasets}. We use a combination of three datasets selected to reflect use cases relevant to LLM-based SQL queries in large-scale database systems \cite{liu2024optimizing}. For classification tasks, we use the E-Commerce Text Classification \cite{saurabh-2023} and CommonsenseQA dataset \cite{talmor2018commonsenseqa}. For sentiment analysis, we use the Amazon Instant Video Review dataset \cite{ni2019justifying}. Further dataset and prompting details can be found in Appendix \ref{app:datasets}. We introduce the Semantic Prompt Caching Benchmark combining these three datasets into a single workload to introduce varying levels of difficulty (see Appendix \ref{app:semantic-prompt-cache-benchmark}).

% <+=================================+>
% <+=================================+>
\subsection{Benchmark Results}

\textbf{\textit{VectorQ} Outperforms Every Static Threshold}. We compare the performance of \textit{VectorQ} to a state-of-the-art semantic prompt cache system across all static similarity thresholds and all user-definable \textit{uncertainty gate} parameters (Section \ref{user-defined-parameter}). Threshold and uncertainty gate selection details are provided in Appendix \ref{app:benchmarking-results}. The evaluation is conducted across multiple datasets, including Amazon Product Reviews, E-Commerce Classification, and the Semantic Prompt Cache Benchmark, which collectively test caching strategies under varying prompt contexts. \textit{VectorQ} consistently achieves lower error rates or higher cache hit rates across all static threshold and uncertainty gate configurations, as shown in Figure \ref{fig:cache-hit-vs-error-rate} and Appendix \ref{app:benchmarking-results}. \textit{VectorQ} consistently achieves lower error rates or lower latencies across all static threshold and uncertainty gate configurations, as shown in Figure \ref{fig:latency-vs-error-rate} and Appendix \ref{app:benchmarking-results}. These results demonstrate the limitations of static similarity thresholds in adapting to diverse workloads and validate the adaptability and efficiency of VectorQ’s embedding-specific threshold regions.

\textbf{\textit{VectorQ} Latency Overhead}. To evaluate the computational overhead introduced by \textit{VectorQ}'s embedding-specific region updates, and correctness sampling, we compare the latency of a semantic prompt cache using static thresholds with one that replaces them with \textit{VectorQ}. To ensure a fair comparison, the first 300 prompts are forced to result in cache misses, and the subsequent 300 prompts are forced to result in cache hits. This setup standardizes the LLM inference costs, as the computation of LLM responses remains identical across both methods. We calculate the average latency for both cache misses and cache hits and present the results in Figure \ref{fig:latency-comparison}. The findings demonstrate that \textit{VectorQ} introduces minimal overhead, achieving identical latency to static-threshold methods, even with embedding-specific threshold region updates and correctness sampling.

\textbf{Latency vs. Cache Size Trade-off}. The cache size grows with the number of stored embeddings. Each embedding has a fixed dimensionality in the vector database, but the associated metadata—particularly the cached LLM response—can vary in size and may dominate memory usage for long-form outputs. We measure the latency overhead introduced by the cache during cache hits and analyze how it varies with cache size (in megabytes) and the number of cache hits. The distribution is based on evaluating 45,000 samples from the Semantic Prompt Cache Benchmark, using an uncertainty gate of 0.6, the gte-large-en-v1.5 embedding model, and Llama-3.1-70B-Instruct as the LLM. Figure \ref{fig:cache-size} analyzes cache hit latency as a function of total cache size. A linear regression shows that cache hit latency increases by 6 microseconds per additional megabyte.

\textbf{Correctness Sampling Ablation Study.} This study evaluates the effectiveness of \textit{correctness sampling} compared to uniform sampling. The correctness sampling approach leverages embedding-specific certainty measures to prioritize re-evaluations for uncertain embedding similarity values within $R_{nn}^3$, ensuring that areas of higher uncertainty are addressed more frequently. In contrast, uniform sampling assigns equal probability to all embedding similarity values, disregarding their uncertainty levels. The results, presented in Figure \ref{fig:vectorq-vs-static-ablation-all}, demonstrate that correctness sampling is overall more robust, achieving lower error rates or higher cache hit rates. While uniform sampling occasionally matches or slightly outperforms the static threshold baseline, its performance is inconsistent and often suboptimal.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{images/latency_comparison_benchmark5.pdf}
    \caption{Latency comparison between \textit{VectorQ} and static-threshold methods on the Semantic Prompt Caching Benchmark (Appendix \ref{app:semantic-prompt-cache-benchmark}), where \textit{VectorQ} introduces minimal overhead.}
    \label{fig:latency-comparison}
\end{figure}

% <+=================================+>
% <+=================================+>

\subsection{Limitations}
\label{limitations}
\textbf{Similarity Comparison}. VectorQ relies on a similarity comparison (Algorithm~\ref{alg:vectorq-regions}, Lines~8 and 17) to determine whether a cached response is correct. In this paper, we use a character-based comparison to ensure strict correctness guarantees. However, in applications such as chatbots—where outputs are longer and semantically equivalent responses may differ in wording—character-based comparison may be too rigid. Supporting such use cases would require adapting the comparison method to account for semantic equivalence.

\textbf{Embedding Model}. The embedding model choice is critical in determining how effectively the prompt is represented in a compressed vector format. If the embedding model is trained on a use case that differs from the context of the prompts, it may fail to capture the relevant semantics required for accurate classifications \cite{tyshchuk2023isotropy}. For instance, many embedding models are trained in a specific language and struggle to interpret prompts in other languages \cite{chung2020rethinking}. To address this limitation, we propose two approaches. First, users should leverage domain expertise to select an embedding model that aligns with their application context. Second, automatic model fine-tuning, as proposed by \cite{zeighami2024nudge} or \cite{zhu2024efficient}, can be employed to adapt the model to the desired domain.

\subsection{Future Work}
\label{future-work}
Embedding-specific posterior distributions offer a natural way to derive certainty metrics, which can inform advanced eviction policies that potentially go beyond LRU and MRU \cite{mattson1970evaluation}. We will aim to incorporate error rate guarantees, enabling users to set target error rates while maximizing cache hit rates.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.49\textwidth]{processed_results/cache_hit_latency_vs_size.pdf}
  \caption{Latency overhead with increasing cache size. The distribution is based on evaluating 45,000 samples from the Semantic Prompt Cache Benchmark.}
  \label{fig:cache-size}
\end{figure}