\section{Related Work}
\label{related-work}

\textbf{KV Caching Inference Engines.} Systems like vLLM \cite{kwon2023efficient} and SGLang \cite{zheng2023efficiently} optimize  the efficiency of large language model inference. vLLM is a system designed to enhance the efficiency of large language model serving by optimizing memory management and scheduling. vLLM introduces PagedAttention, an attention algorithm inspired by virtual memory and paging techniques, to address inefficiencies in managing the key-value (KV) cache memory for LLMs \cite{kwon2023efficient}. SGLang is a serving framework for large language models that improves interaction speed and control through co-design of the backend runtime and frontend language. It includes features like RadixAttention for efficient prefix caching \cite{zheng2023efficiently}. This work is orthogonal as our adaptive semantic prompt cache complements these systems by reusing cached LLM-generated responses for semantically similar prompts. For prompts that cannot reuse a cached response, systems like vLLM or SGLang reduce the latency of inference itself.

\textbf{Semantic Prompt Caches.} Semantic prompt caches intercept prompts before they reach the LLM, matching them to previously stored prompts based on semantic similarity. If a similar prompt exists (cache hit), the system retrieves the cached response, avoiding a new LLM inference. If no match is found, the prompt is sent to the LLM, and its response is added to the cache for future reuse \cite{bang2023gptcache}. These systems can reduce LLM inference latency by up to 100x \cite{gptcache}. GPTCache \cite{gptcache} is the most prominent open-source implementation with 7.4k stars on GitHub. AWS \cite{aws}, Microsoft \cite{microsoft}, waLLMartCache \cite{dasgupta2025wallmartcache}, and SCALM \cite{li2024scalm} have proposed semantic prompt cache frameworks based on a similar architecture. These frameworks rely on vector similarity metrics to quantify the semantic similarity of the most similar prompt stored in the cache \cite{bang2023gptcache}. Metrics such as cosine similarity \cite{rahutomo2012semantic} and Euclidean distance \cite{globerson2004euclidean} provide numerical scores to quantify the similarity between two embeddings. However, it is unclear which similarity value, the threshold, is sufficient to classify the most similar prompt as similar enough. State-of-the-art semantic prompt caches use a single static threshold throughout the cache's runtime. Users either rely on a predefined threshold (i.e., 0.7) or determine one by testing multiple thresholds and selecting the value that best fits their requirements \cite{dasgupta2025wallmartcache}. If the threshold is too low the system might produce incorrect cache hits. If the threshold is too high the system may avoid cache hits entirely. We demonstrate the insufficiency of a static threshold and propose \textit{VectorQ}, a framework that learns adaptive and embedding-specific threshold regions.
