\section{Introduction}
\label{introduction}
Semantic prompt caches can reduce large language model (LLM) inference latency by up to 100x by reusing cached LLM-generated responses for semantically similar prompts \cite{bang2023gptcache}. These systems convert prompts into vector embeddings and cache them in a vector database along with their corresponding LLM-generated responses \cite{dasgupta2025wallmartcache}. A prompt that is currently being processed is referred to as a candidate. Given a new candidate, the system must decide whether to return a cached response, known as a reuse, or generate a new one with an expensive LLM inference \cite{li2024scalm}. The system retrieves its nearest neighbor from the vector database when processing a candidate. A cache hit occurs when the system returns the previously cached LLM-generated response of the nearest neighbor as the candidate's response. Conversely, a cache miss occurs when the system cannot return the cached response and instead performs an expensive LLM inference to generate a new response for the candidate.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{images/figure1.pdf}
    \caption{\textit{Left}: \textit{VectorQ} learns embedding-specific similarity threshold regions ($R_{nn}^1, R_{nn}^2, R_{nn}^3$) to make cache hit or miss decisions in semantic prompt caches. The correctness posterior (orange line) enables \textit{VectorQ} to prioritize re-evaluations for uncertain similarity values. \textit{Right}: \textit{VectorQ} consistently outperforms state-of-the-art semantic prompt caches across all static thresholds.}
    \label{fig:figure-1}
\end{figure}

Determining whether the nearest neighbor's cached response is suitable for a cache hit remains an open research question, as the candidate may require a different LLM response \cite{bang2023gptcache, dasgupta2025wallmartcache, li2024scalm}. Semantic prompt caches rely on vector similarity metrics to evaluate the contextual similarity of embeddings and determine whether the nearest neighbor's cached response can be reused. Metrics such as cosine similarity \cite{rahutomo2012semantic} and Euclidean distance \cite{globerson2004euclidean} provide numerical scores to quantify the similarity between two embeddings. We consider normalized scores, where 0.0 indicates no similarity between embeddings and 1.0 identical ones. At the extremes, a similarity score of 0.0 indicates no similarity, requiring the system to generate a new LLM response, while a score of 1.0 implies an identical match, allowing the cache to reuse the cached response confidently. However, it is unclear which threshold value between 0.0 and 1.0 is sufficient to classify the nearest neighbor as similar enough.

State-of-the-art semantic prompt caches, such as GPTCache \cite{bang2023gptcache}, use a single static threshold throughout the cache's runtime. Users either rely on a predefined threshold (e.g., 0.7) or determine one by testing multiple thresholds and selecting the value that best fits their requirements \cite{dasgupta2025wallmartcache}. If the threshold is set too low, the system might produce incorrect cache hits, where it reuses the cached response from the nearest neighbor, but the response differs from the one the candidate requires \cite{rekabsaz2017exploration}. Vice versa, if the threshold is set too high, the system may avoid cache hits entirely.

Our work demonstrates that a static threshold is inadequate for classifying different prompts as semantically similar (Section \ref{static-similarity-threshold}). Instead of one static threshold value uniformly applied across all embeddings, we propose \textbf{\textit{VectorQ}}, an online framework that learns embedding-specific threshold regions that adapt to each embedding (Section \ref{threshold-regions}). Given a candidate and its nearest neighbor, along with its cached LLM-generated response and threshold regions, the system determines its action based on the similarity between the candidate and the nearest neighbor. If the similarity falls within a region spanned by similarity values that previously led to correct cache hits, the system returns the cached response (cache hit). Otherwise, it generates a new LLM response (cache miss). \textit{VectorQ} defines these regions by analyzing the similarity scores linked to correct and incorrect cache hits for each embedding (Figure \ref{fig:figure-1}). Through a certainty-based sampling approach, \textit{VectorQ} prioritizes the correctness re-evaluation of uncertain similarity values and gives a threshold convergence guarantee (Section \ref{threshold-convergence-proof}) that minimizes the number of incorrect cache hits (Section \ref{correctness-sampling}). Users can adjust the cache's accuracy- latency trade-off using the \textit{uncertainty gate} parameter (Section \ref{user-defined-parameter}).

We demonstrate the effectiveness of our approach by replacing the static similarity threshold with \textit{VectorQ} to showcase the impact of dynamic and embedding-specific threshold regions. We evaluate our approach using a combination of three diverse datasets \cite{saurabh-2023, talmor2018commonsenseqa, ni2019justifying} and use them in our semantic prompt cache benchmark (Appendix \ref{app:semantic-prompt-cache-benchmark}) designed to simulate challenging workload conditions by alternating prompts across varying contexts. Our results show that \textit{VectorQ} consistently outperforms all static thresholds, achieving up to 26× increases in cache hit rate and error rate reductions by up to 74\% (Figure \ref{fig:figure-1}). Our main contributions are three-fold:

\textbf{1)}  We demonstrate that similarity value distributions vary across embeddings and require different thresholds.

\textbf{2)} We propose \textit{VectorQ}, an online framework that learns embedding-specific threshold regions using correctness sampling and guarantees threshold convergence.

\textbf{3)} We demonstrate that \textit{VectorQ} consistently outperforms static thresholds and achieves up to 26× more cache hits and up to 74\% lower error rates.
