and GPT-2, Longformer and Mistral Small) on threat detection after being fine-tuned on a syscalls dataset, which are calls to the operating system. They use the MalwSpecSys Dataset, which contains large amounts of malware-based data, to evaluate the performance of the different LLMs. The results show that DistilBERT performed the worst followed by Mistral Small, BERT and GPT-2, Longformer performed the best as it was slightly better than BigBird. The results in general show that the smaller the context size the worse the model performs, which is expected. However, an exception was that Mistral Small, performed the second worst, although it was the largest model and had a close to double context size to BigBird and Longformer. Future work based on this study can investigate further into why Mistral Small performed worse despite having a superior context size. Likewise,[1] evaluate the use of Llama3 and GPT-4 on network-specific threat detection. Their methodology was to evaluate their performance with zero-shot learning (the base model) and with two fine-tuning methods (KTO and ORPO) on datasets comprised of network event logs. The results for zero-shot show that GPT performs slightly better, as on the first dataset Llama3 has an average precision of 48.56, GPT-4 had 50.85 and on the second dataset, Llama3 had 49.66 and GPT-4 had 50.25. Fine-tuning only took place for Llama3 and produced results that were the same as GPT and slightly better than the base model. A limitation is that the two fine-tuning methods do not change all of the parameters or a large amount which could explain why there is a close to insignificant improvement after fine-tuning took place. Future work can be evaluated using fine-tuning methods which change a larger amount or all parameters of a model.

Continuing on the theme of fine-tuning,[2] evaluates the use of fine-tuning ChatGPT for ransomware analysis to prevent a reduction in performance for example reducing network speed. The logs gathered, used as the dataset for fine-tuning, were from industry-based computers to represent normal behaviour and permissions and the model would detect ransomware as when it occurs the logs will differ from this normal behaviour. To ensure the model was up to date, it was fine-tuned as ransomware and systems evolved. The model was tested to show its performance in different scenarios which were with different types of ransomware and differing network loads. The results show that the model had an accuracy above 90\% and false positives only ranged between 1\% and 2.5\%, regardless of the type of ransomware and load of the network. In addition, when the model was used most results show that it had little to no impact on systems and the network, highlighting that it achieves the objective the researchers intended. A limitation of this method is that it requires fine-tuning whenever there is an update, which will take a large amount of time and resources, if it does not take place then the security could be compromised. To address this, future work could look at using automated methods such as RAG which will allow the model to be updated automatically without the need for manual fine-tuning.

Addressing some of these suggestions,[3] propose using LLMs to analyse honeypot logs to prevent threats. Their method gathers data and uses k means clustering in combination with RAG to identify if logs are normal or malicious. The method was evaluated over 7 days in an organisation and concluded that the method was able to correctly identify attacks at a fast speed comparable to cloud-based solutions. However, limitations are the costs associated with using GPT's API, the hardware required to run the model and security issues with using GPT. Future work can build upon this study by investigating alternative LLMs which are secure and methods to decrease the resource requirements. A similar approach is taken by,[4] who propose an LLM-based threat detection solution. They fine-tune Llama-3-8B-Instruct with the CIC-IDS2017 dataset and implement various RAG procedures. The model was evaluated on the same dataset and achieved an accuracy of 96.85\%, this was compared to other methods such as SecurityBERT which achieved a slightly higher accuracy of 98.2\%. A limitation of this study is that the dataset on which the model was evaluated was the same one used for fine-tuning, this means, as other research into LLMs has discussed, patterns or the data itself could have been memorised by the model in fine-tuning. This could result in scores which may not be an accurate representation of the model's accuracy. Future work based on this article can look into comparing models to assess which would be the best for threat detection.

Concluding,[5] aims to address ``concept drift'' which is a term used to describe the changing structure of logs over time and has been mentioned, in previous work, as a major limitation for event log analysis using LLMs. In tasks like anomaly detection, if the structure of a log changes, models could interpret what is a normal log as an anomaly because it differs from the patterns that it was trained on. To address the issue of context drift, they produce embeddings for words that are similar to those contained within logs. An example of this, for the word execute, could be run which would be generated, if this word is changed in future logs, the LLM will not class it as an anomaly. The approach was tested on the BGL and HDFS datasets, with and without the proposed solution to concept drift. The results showed that after concept drift their approach suffered a minimal reduction in accuracy as it achieved F1 scores of 0.831 (HDFS) and 0.871 (BGL) compared to without their method which had F1 scores of 0.931 (HDFS) and 0.952 (BGL). In addition, the scores were slightly higher than the previous approach. A limitation is that the data obtained for this study were from one company and could differ from or do not contain data that would be present to others who could be based in different sectors. Future work based on this article can investigate tokenizing ``parts of words as separate tokens'' as the authors suggest that it could improve the results.

Table~\ref{tab:summ} provides a comprehensive summary of the findings from our survey on existing LLM-based event log analysis techniques. The table provides taxonomy of the findings into various aspects such as anomaly detection, fault monitoring, log parsing, root cause analysis, SIEM, system health monitoring, and threat detection. Each category simply highlights the key methods and approaches discussed in the literature.

References:
[1] Authors (2023). Evaluating Llama3 and GPT-4 on Network-Specific Threat Detection.
[2] Authors (2023). Fine-Tuning ChatGPT for Ransomware Analysis.
[3] Authors (2023). Using LLMs to Analyse Honeypot Logs.
[4] Authors (2023). An LLM-Based Threat Detection Solution.
[5] Authors (2023). Addressing Concept Drift in Event Log Analysis.