\section{Conclusion}
In this paper, we introduced Matrix3D, a unified model that effectively addresses multiple photogrammetry tasks including pose estimation, depth prediction, and novel view synthesis using a multi-modal diffusion transformer (DiT). By employing a mask learning strategy, Matrix3D supports flexible input/output combinations, and maximizes training data from incomplete datasets. Through multi-round, multi-view, multi-model interactive generation, users can perform single or few-shot generation with one single model. Extensive experiments show that Matrix3D achieves SOTA performance in pose estimation and novel view synthesis tasks, showing its versatility on photogrammetry applications. 

\section{Acknowledgments}
This work is supported by National Natural Science Foundation of China (62472213, 62025108), Gusu Innovation \& Entrepreneurship Leading Talents Program (ZXL2024361), and Hong Kong RGC GRF 16206722.
