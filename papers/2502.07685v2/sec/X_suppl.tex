\clearpage
\setcounter{page}{1}
\maketitlesupplementary
Here, we present an additional description of the model architecture(Sec.~\ref{sec:supp-arch}), dataset preprocessing(Sec.~\ref{sec:supp-dataset}), training details(Sec.~\ref{sec:supp-training}), and experiments(Sec.~\ref{sec:supp-exp}).

\section{Model Architecture}
\label{sec:supp-arch}
For RGB data, we use DINOv2~\cite{oquab2023dinov2} and Stable Diffusion~\cite{rombach2021highresolution} VAE to extract deep features from pixels before sending them into the modality-specific encoders. The modality-specific encoders are composed of stacked convolution and linear layers following~\cite{peebles2023scalable} to patchify image-like 2D data into 1D tokens. The patchify scale for RGB, pose, and depth are set to 2, 1, and 4. After the tokens are processed by the transformer, we use similar modality-specific linear layers~\cite{peebles2023scalable} to unpatchify each modality token back to the original shape according to the corresponding patchify scales. The whole multi-view transformer encoder includes 20 self-attention blocks with a hidden size of 1024, while the decoder includes 40 stacked self-attention and cross-attention blocks with a hidden size of 1408 following HunyuanDiT~\cite{li2024hunyuan}. 

For classifier-free guidance (cfg), we empirically found the following settings to perform best: 1.5 for RGB / poses, and 1.0 for depth (w/o cfg).


\section{Dataset Pre-processing}
\label{sec:supp-dataset}
As illustrated in the main paper, we train Matrix3D on a mixture of six datasets, including Objaverse~\cite{deitke2023objaverse}, MVImgNet~\cite{yu2023mvimgnet}, CO3D-v2~\cite{reizenstein21co3d}, RealEstate10k~\cite{zhou2018stereo}, Hypersim~\cite{roberts2021hypersim}, and ARKitScenes~\cite{baruch2021arkitscenes}. In each training batch, the datasets have a proportion of 4:4:4:4:4:1. Table~\ref{tab:supp-datasets} provides a summary of these datasets used for training, including the size (in terms of scenes and images), type (real or synthetic), scene categories, and supported modalities (RGB, camera poses, and depths). For all datasets, we apply scene normalization and camera normalization. Camera poses are represented as Pl\"ucker rays. Note that the depth images provided in each dataset are not always complete. Specifically, CO3D-V2 and ARKitScenes provide incomplete depth images, while for the Objaverse dataset we only have the rendered object foreground depth.

\pseudopara{Normalization}  
Due to the highly diverse distributions of existing datasets, including variations in scale and scene type, preprocessing them consistently poses a challenge. To address this, we apply the following normalization.
\begin{itemize}
    \item Scene Normalization:  
    To normalize the whole scene scale, we adapt our approach depending on the dataset type and available modalities. For object-centric datasets with camera poses provided (i.e., Objaverse~\cite{deitke2023objaverse}, MVImgNet~\cite{yu2023mvimgnet}, and CO3D-v2~\cite{reizenstein21co3d}), we follow RayDiffusion~\cite{zhang2024cameras} by setting the intersection point of the input camera rays as the origin of the world coordinates and defining the scene scale accordingly. For scene-type datasets that provide depth information (i.e., Hypersim~\cite{roberts2021hypersim} and ARKitScenes~\cite{baruch2021arkitscenes}), we use the depth of the first view as a reference, calculating its median value and normalizing it to 1.0. For those datasets without depth data (i.e., RealEstate10k~\cite{zhou2018stereo}), we determine the scale based on the camera distances to the average positions and set the maximum distance to 1.0.

    \item Camera Normalization:  
    We perform camera normalization after scene normalization. Specifically, we set the first view’s camera as the identity camera with rotation \( R = I \) and translation \( T = [0, 0, 1] \), while preserving relative transformations between cameras across views.
\end{itemize}

\pseudopara{Objaverse Rendering}  For the Objaverse dataset, we render all models into RGB and depth images for training. Specifically, each 3D object is first normalized at the world center within a bounding box of $[-1, 1]^3$, and we render the whole scene from 32 random viewpoints. The render camera FoV is set to 50$\degree$. The azimuth and elevation angles are randomly sampled in $[0\degree, 360\degree]$ and $[-45\degree, 90\degree]$. The camera distance to the world center is randomly sampled in [1.1, 1.6], and the height on the z-axis is set in [-0.4, 1.2]. We use a composition of random lighting from area lighting, sun lighting, point lighting, and spot lighting.


\section{Training Details}
\label{sec:supp-training}
Table~\ref{tab:supp-hyperparams} reports the detailed training hyper-parameter settings of three stages. We didn't apply any data augmentation techniques and center-cropped the input images into a square.

\begin{table*}[]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l@{\hskip 1.0cm}l@{\hskip 0.8cm}l@{\hskip 0.8cm}l@{\hskip 0.8cm}l}
    \toprule 
    Hyper-parameters & Ablation & Stage 1 & Stage 2 & Stage 3 \\
    \midrule
    Optimizer & AdamW~\cite{loshchilov2017decoupled} & AdamW~\cite{loshchilov2017decoupled} & AdamW~\cite{loshchilov2017decoupled} & AdamW~\cite{loshchilov2017decoupled} \\
    Learning rate & 1e-4 & 1e-4 & 1e-5 & 1e-5 \\
    Learning rate scheduler & Constant & Constant & Constant & Constant \\
    Weight decay & 0.05 & 0.05 & 0.05 & 0.05 \\
    Adam $\beta$ & (0.9, 0.95) & (0.9, 0.95) & (0.9, 0.95) & (0.9, 0.95) \\
    Max view num & 4 & 4 & 8 & 8 \\
    Batch size & 512 & 1024 & 1024 & 256 \\
    Steps & 100k & 200k & 30k & 30k \\
    Warmup steps & 4k & 4k & 1k & 1k \\
    Initialization & HunyuanDiT\cite{li2024hunyuan} & HunyuanDiT\cite{li2024hunyuan} & Stage 1 & Stage 2 \\
    Attention blocks (encoder) & 20 & 20 & 20 & 20 \\
    Attention blocks (decoder) & 40 & 40 & 40 & 40 \\
    \midrule
    Image resolutions & $256{\times}256$ & $256{\times}256$ & $256{\times}256$ & $512{\times}512$ \\
    Raymap resolutions & $16{\times}16$ & $16{\times}16$ & $16{\times}16$ & $32{\times}32$ \\
    Depth resolutions & $64{\times}64$ & $64{\times}64$ & $64{\times}64$ & $128{\times}128$ \\
    Datasets & Object-centric & Object-centric & All & All \\
    \bottomrule 
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{Detailed hyper-parameters. }
    \label{tab:supp-hyperparams}
\end{table*}

\begin{table*}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc|ccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{Size} & \multicolumn{2}{c|}{Type} & \multicolumn{3}{c}{Support Modalities} \\
    % \cmidrule{2-8}
     & Scenes & Images & Real/Synthetic & Scene Type & RGB & Poses & Depths \\
    \midrule
    Objaverse~\cite{deitke2023objaverse} & 800K & 25M & Synthetic & Object-centric & $\checkmark$ & $\checkmark$ & Foreground only \\
    MVImageNet~\cite{yu2023mvimgnet} & 220K & 6.5M & Real & Object-centric & $\checkmark$ & $\checkmark$ & $\times$ \\
    CO3Dv2~\cite{reizenstein21co3d} & 19K & 1.5M & Real & Object-centric & $\checkmark$ & $\checkmark$ & Incomplete \\
    RealEstate10K~\cite{zhou2018stereo} & 10K & 10M & Real & Indoor/Outdoor Scene & $\checkmark$ & $\checkmark$ & $\times$ \\
    Hypersim~\cite{roberts2021hypersim} & 461 & 77K & Synthetic & Indoor Scene & $\checkmark$ & $\checkmark$ & Complete \\
    ARKitScenes~\cite{baruch2021arkitscenes} & 5K & 450K & Real & Indoor Scene & $\checkmark$ & $\times$ & Incomplete \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{Dataset details.}
    \label{tab:supp-datasets}
\end{table*}

\section{Experiments}
\label{sec:supp-exp}
\subsection{DTU Dataset Split for Depth Evaluation}
In Sec.~\ref{sec:exp-depth}, we use different evaluation set for monodepth and multi-view depth evaluation. Specifically, we use the IDR \cite{yariv2020multiview} subset for monodepth because perfect foreground masks are provided, and follow previous work \cite{wang2024dust3r} to use the MVSNet \cite{yao2018mvsnet} subset for multi-view depth evaluation. 

\subsection{Point Cloud Fusion}
In Sec.~\ref{sec:exp-depth}, we back-project multi-view depth maps to point cloud. In practice, we additionally conduct geometric consistency filtering and fusion to clean the point cloud. The filtering follows \cite{yao2018mvsnet, zhang2023vis}, consisting of a combination of the following operations.
\begin{itemize}
    \item Geometric filtering. We project the pixels from the reference view to source views, find the pixel at the projection location, and project it back to reference view. Then we check the difference of the original position and the reprojected position, as well as their depths. 
    \item Geometry fusion. We project all pixels from source views to the reference views, and each pixel in the reference view may receive multiple values. We then change the original depth result to the average or the median of all the gathered values. 
\end{itemize}

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|cccc|cccccccc}
    \toprule
    \multirow{2}{*}{Methods} & GT & GT & GT & \multirow{2}{*}{Align} & \multicolumn{2}{c}{DTU} & \multicolumn{2}{c}{ETH3D} & \multicolumn{2}{c}{T\&T} \\
    \cline{6-11}
    & Pose & Range & Int. &  & rel $\downarrow$ & $\tau \uparrow$ & rel $\downarrow$ & $\tau \uparrow$ & rel $\downarrow$ & $\tau \uparrow$ \\
    \midrule
    DeepV2D & $\times$ & $\times$ & $\checkmark$ & med & 7.7 & 33.0 & 11.8 & 29.3 & 8.9 & 46.4 \\
    DUSt3R & $\times$ & $\times$ & $\times$ & med  & \underline{2.76} & \underline{77.32} & \textbf{4.71} & \textbf{61.74} & \textbf{5.54} & \textbf{56.38}\\
    Ours & $\times$ & $\times$ & $\checkmark$ & med & \textbf{1.85} & \textbf{85.46} & \underline{7.83} & \underline{38.80} & \underline{6.16} & \underline{49.43} \\
    \bottomrule
\end{tabular}
}
%\vspace{-3.5mm}
\caption{Unposed MVD evaluation on DTU, ETH3D, and T\&T.}
%\vspace{-4mm}
\label{tab:supp-multiview_depth}
\end{table}


\subsection{Ablation Study on Multi-task Training}\label{sec:supp-ablation}
In this section we compare the model trained by masked learning and the task-specific models including NVS, pose estimation and depth estimation. The latter ones have the same network architecture as the stage 1 model, but the input/output configuration of the training samples is set to only one task. All 4 models are trained from HunyuanDiT \cite{li2024hunyuan} initialization with halved batch size and total steps due to limited time and compute resources. The evaluation metrics for each task is the same as the main paper. 

Quantitative reuslts are shown in Table \ref{tab:supp-abl}. The model with masked learning strategy (\textit{Multi-task}) surpasses the task-specific model in the NVS task, but fails for pose estimation and depth estimation. One possible reason is that the model capacity is shared by different tasks. Another reason related to practice is that the models for ablation studies do not fully converge. According to the evaluation curve with respect to training steps in Figure \ref{fig:supp-curve}, the model with halved batch size at 100k steps has similar performance as the full model at 60k-70k steps which still has large room of improvement. Given that our model is initialized by an RGB diffusion model, the functionality of outputing ray maps and depth maps may need longer time to converge, and thus task-specific models achieves better results within limited training time. Although not a fair comparison, note that all models for ablation study are weaker than the stage 1 and the stage 3 model. Also, in Sec.~\ref{sec:exp-hybrid} we show that the model trained by masked learning can support flexible input and boost the performance by utilizing additional input. 


\begin{table*}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc||l|ccc||l|cc}
    \toprule
    \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{RRA @ 15$\degree\uparrow$} & \multicolumn{3}{c||}{CA @ 0.1$\uparrow$} & \multirow{2}{*}{Methods} & \multirow{2}{*}{PSNR$\uparrow$} & \multirow{2}{*}{SSIM$\uparrow$} & \multirow{2}{*}{LPIPS$\downarrow$} & \multirow{2}{*}{Methods} & \multirow{2}{*}{rel$\downarrow$} & \multirow{2}{*}{$\tau\uparrow$} \\
     & 2 & 3 & 4 & 2 & 3 & 4 &  &  &  &  &  &  &  \\
    \midrule
    Pose only & \textbf{89.2} & \textbf{86.7} & \textbf{85.8} & 100.0 & \textbf{83.1} & \textbf{77.0} & NVS only & 16.30 & 0.77 & 0.30 & Depth only & \textbf{9.07} & \textbf{26.21} \\
    Multi-task & 81.1 & 77.8 & 75.3 & 100.0 & 75.8 & 64.5 & Multi-task & \textbf{17.21} & \textbf{0.79} & \textbf{0.25} & Multi-task & 10.76 & 18.52 \\
    \midrule
    Stage 1 & 92.2 & 91.5 & 89.6 & 100.0 & 87.8 & 80.8 & Stage 1 & 18.13 & 0.81 & 0.19 & Stage 1 & 4.30 & 49.81 \\
    Stage 3 & 95.6 & 96.0 & 96.3 & 100.0 & 93.5 & 91.7 & Stage 3 & 18.87 & 0.85 & 0.21 & Stage 3 & 1.83 & 85.45 \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{Ablation study on multi-task training and task-specific training. Besides different training target, the ablation models have halved batch size and total steps. The multi-task model achieves better results in NVS task but fails for pose estimation and depth estimation. One possible reason is that the multi-task model converges slower than the task-specific models. Please refer to Sec.~\ref{sec:supp-ablation} for more analysis. }
    \label{tab:supp-abl}
\end{table*}

\begin{figure*}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c@{\hskip 1pt}c@{\hskip 1pt}c}
    \includegraphics[width=0.287\linewidth]{figures/curve_pose.pdf} &
    \includegraphics[width=0.35\linewidth]{figures/curve_nvs.pdf} &
    \includegraphics[width=0.35\linewidth]{figures/curve_depth.pdf} \\
    a) Pose (RRA@15$\degree\uparrow$) & b) NVS (PSNR$\uparrow$) & c) Depth (rel$\downarrow$)
    \end{tabular}}
    \caption{Evaluation results of stage 1 model for a) pose estimation, b) NVS and c) Depth estimation with respect to training step. For pose estimation we report the results for multiple view numbers. Note that the stage 1 model is only trained with view number $\leq$ 4. }
    \label{fig:supp-curve}
\end{figure*}


\subsection{3D Reconstruction}
\pseudopara{Camera trajectory generation} We build different camera trajectories for generating novel views depending on different reconstruction tasks. For monocular image input, we create an orbital trajectory and sample 80 cameras evenly. All cameras are set as look-at to the world center. For sparse-view image input, we fitted a spline trajectory from the input poses, and scaled up the trajectories two times, resulting in $240 (=80 \times 3)$ views.

\pseudopara{3DGS optimization} The proposed 3DGS optimization system in built upon the open-source pipeline~\cite{nerfstudio} with several modifications. For each optimization step, we optimize Gaussian points on mini-batch images instead of single images. Besides of original L1 loss and SSIM loss, we adopt additional losses to improve the reconstruction robustness, including LPIPS loss $L_{\textrm{LPIPS}}$~\cite{zhang2018perceptual}, mask loss $L_{\textrm{mask}}$, accumulation regularization $L_{\textrm{accum}}$, absolute depth loss $L_{\textrm{depth}}$, and relative depth ranking loss $L_{\textrm{rel-depth}}$~\cite{wang2023sparsenerf}. The accumulation regularization is designed to constrain the alpha values of Gaussian points to be either fully opaque or completely transparent, aiming to reduce floaters in the scene. It is composed of a binary cross-entropy loss and entropy loss:
\begin{equation*}
    L_{\textrm{accum}} = BCE(\alpha, 0.5) -\alpha \log(\alpha) + (1-\alpha) \log(1-\alpha),
\end{equation*}
where $\alpha$ denotes the accumulation values. 


For monocular 3D reconstruction, the value for each loss is set to $w_{\textrm{L1}}=1.0, w_{\textrm{SSIM}}=0.2, w_{\textrm{LPIPS}}=10.0, w_{\textrm{mask}}=5.0, w_{\textrm{accum}}=5.0$. Depth loss is not applied in the optimization. The mini-batch size for each step is set to $10$. For the input view, the weight of L1 loss is specifically set to $10.0$ for high significance. 

For sparse-view 3D reconstruction, the weight values are set to $w_{\textrm{L1}}=1.0, w_{\textrm{SSIM}}=0.2, w_{\textrm{LPIPS}}=10.0, w_{\textrm{mask}}=5.0, w_{\textrm{accum}}=0.5, w_{\textrm{depth}}=10.0, w_{\textrm{rel-depth}}=20.0$. The mini-batch size for each step is set to $5$, and the L1 loss weight of input views is set to $20$. 

We use the back-projected point cloud as Gaussian point initialization. Similar to CAT3D~\cite{gao2024cat3d}, we conduct in total of 1200 and 3000 optimization steps for two tasks, respectively. We apply the scale regularization~\cite{xie2024physgaussian} to constrain the extreme Gaussian scales.


\subsection{Limitation}

During experiments, we found that our model performs well on object-centric and indoor scenes but degrades in outdoor environments, primarily due to the lack of large-scale outdoor training data—our dataset consists of objects and limited indoor scenes. The lack of high-quality outdoor data is a common issue in the community, and similar problem has been noticed in other models.

\cref{tab:supp-multiview_depth} demonstrates unposed depth prediction results on ETH3D and T\&T. Our model performs worse than DUSt3R (trained on outdoor datasets), but still surpass DeepV2D.




\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/supp-unposed-vis.pdf}
    \vspace{-6mm}
    \caption{More unposed sparse-view 3D reconstruction results. }
    \vspace{-4mm}
    \label{fig:supp-unposed-vis}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/supp-depth-vis.pdf}
    \vspace{-6mm}
    \caption{Visualization of multi-view depth prediction results. }
    \vspace{-4mm}
    \label{fig:supp-depth-vis}
\end{figure*}

\subsection{More visualization}
Here we present more visualization results about unposed sparse-view 3D reconstruction (Fig.~\ref{fig:supp-unposed-vis}) and multi-view depth predictions (Fig.~\ref{fig:supp-depth-vis}).