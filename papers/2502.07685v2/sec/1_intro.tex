\section{Introduction}
\label{sec:intro}

Photogrammetry is a crucial technology for reconstructing 3D scenes from 2D images. However, the traditional photogrammetry pipeline has two significant weaknesses. First, it typically requires a dense collection of images--often hundreds--to achieve robust and accurate 3D reconstruction, which can be troublesome in practical applications. Second, the pipeline involves multiple processing stages that utilize completely different algorithm blocks, including feature detection, structure-from-motion (SfM), multi-view stereo (MVS), and etc. While each step is an independent task that requires unique algorithm, they are not correlated or jointly optimized with one another, which can result in suboptimal outcomes and accumulated errors throughout this multi-stage process.

The first challenge is commonly tackled by combining reconstruction with generation \cite{shi2023mvdream, li2023instant3d, wu2024reconfusion, gao2024cat3d}, where denser RGB images are generated by diffusion models conditioned on sparse inputs. In practice, however, when there are more than one input image, it is challenging to obtain accurate relative poses of them since they often come with low overlaps. The second challenge has not yet been extensively addressed. Representative works are PF-LRM \cite{wang2023pf} and DUSt3R \cite{wang2024dust3r}, which use single feed-forward models to perform both pose estimation and scene reconstruction. They are thus end-to-end optimizable and eliminate the need of multi-stage processing.

Inspired by previous methods, we try to take one step further and tackle these two challenges together, by building a unified model that can do multiple photogrammetry sub-tasks, including pose estimation, depth estimation and novel view synthesis (for sparse view reconstruction). We call this model \textbf{Matrix3D}, featuring an all-in-one generative model designed to support various sub-tasks in photogrammetry, through altering input/output combinations.
At its core, Matrix3D represents data of all modalities using unified 2D representations: camera geometries are encoded as Pl√ºcker ray maps, while 3D structures are presented as 2.5D depth maps. This makes it possible to leverage the capabilities of modern image generative models. 
We extend the diffusion transformer into a multi-view, multi-modal framework, capable of generating all necessary modalities. Inspired by the principles of masked auto-encoder (MAE), our model is trained by randomly masking inputs, while predicting the remaining unseen observations.
This masking learning design not only effectively manages varying degrees of input sparsity, but also substantially increases the volume of available training data by utilizing partially complete data samples such as bi-modality image-pose and image-depth pairs. 

With the densified camera / image / depth predictions generated by Matrix3D, a 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} optimization can be applied to produce the final output, where depth maps are back-projected into 3D point clouds for 3DGS initialization.

In summary, our key contribution is the unified diffusion transformer model that has flexible input/output configurations and enables several tasks in photogrammetry process. The proposed method eliminates the need for multiple task-specific models, streamlining the photogrammetry process with one single model. Extensive quantitative and qualitative experiments, demonstrate the state-of-the-art performance of our methods compared with existing task-specific approaches.