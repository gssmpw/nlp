

\section{Method}
\label{sec:method}
In this section, we introduce Matrix3D, an all-in-one photogrammetry model for unified 3D reconstruction and generation. 
%Matrix3D is built upon a multi-modal diffusion transformer. 
In the following, we describe the details of the framework design (Section~\ref{sec:3.1}), masking strategies (Section~\ref{sec:3.2}), dataset preparation (Section~\ref{sec:3.3}), training setup (Section~\ref{sec:3.4}), and downstream tasks (Section~\ref{sec:3.5}).


\subsection{Multi-Modal Diffusion Transformer}
\label{sec:3.1}
As demonstrated in Section~\ref{sec:intro}, our framework is designed around three key principles: a unified probabilistic model, flexible I/O, and multi-task capability. The emerging diffusion transformer (DiT)~\cite{peebles2023scalable} offers an ideal foundation, with its transformer architecture naturally supporting flexible I/O configurations and multi-modal fusion.

\pseudopara{Network architecture} 
The proposed model consists of two novel components compared with a standard image DiT model: a multi-view encoder and a multi-view decoder $D$. The encoder processes conditioning data from multiple views across different modalities (i.e., RGB, poses and depth), and embeds them into a shared latent space, enabling better cross-view and cross-modal feature integration. Similarly, the decoder processes noisy maps corresponding to different targets at different diffusion timestamps. The latent codes from different views and modalities are concatenated sequentially and passed through transformer layers to capture correspondence across views and modalities. Both the encoder and decoder are composed of multiple self-attention blocks, with the decoder additionally incorporating a cross-attention block after each self-attention layer to enable communication between conditioning inputs and generated outputs. Our diffusion model is built upon the pre-trained Hunyuan-DiT~\cite{li2024hunyuan} architecture with the aforementioned module modifications. In our experiments, the maximum number of views is set to 8, though this number can be further extended subject to computation budget. Mathematically, let $\mathbf{x}_c$ denotes multi-view/modality conditions, $\mathbf{x}_{g,t}$ the desired generation corrupted by noise $\epsilon$ at time $t$, and $\mathbf{x}_0$ the groundtruth. The diffusion model is trained using v-prediction~\cite{salimans2022progressive} loss:
\begin{align}
    \mathcal{L} &= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}, t, y} \left[ \left\| D(E(\mathbf{x}_c), \mathbf{x}_{g,t}, t) - \mathbf{v} \right\|^2 \right], \\
    \mathbf{v} & = \alpha_t \boldsymbol{\epsilon} - \sigma_t \mathbf{x}_0.
\end{align}
%where the velocity target \( \mathbf{v} \) is defined as  
%\begin{align}
%    \mathbf{v} = \alpha_t \boldsymbol{\epsilon} - \sigma_t \mathbf{x}_0.
%\end{align}


\pseudopara{Multi-modality encoding} To handle multiple modalities, we apply modality-specific encoding methods before feeding the data into transformers. Specifically, the VAE from SDXL~\cite{podell2023sdxl} is used to encode RGB images into low-dimensional latent space. For camera poses, we follow RayDiffusion~\cite{zhang2024cameras} to represent cameras as Pl\"ucker ray maps, which naturally takes the form of image-like 2D data. For depth modality, our model adopts multi-view aligned depth (i.e., affine-invariant depth), which would be converted into disparities (i.e., the inverse of depth) to ensure a more compact data range. 
Additionally, a fixed shift and scale factor is applied to regularize ray maps and depth maps so that their distribution is closer to standard Gaussian distribution, as required by the diffusion process~\cite{ho2020denoising}.

\pseudopara{Positional encoding} We incorporate three types of positional encodings to preserve spatial relationships across viewpoints, patch token positions, and modalities. Specifically, we apply Rotary Positional Embedding (RoPE)~\cite{su2024roformer} to encode the positions of individual patch tokens because we care more about their relative position, while apply absolute sinusoidal positional encoding~\cite{dosovitskiy2020image} to viewpoints and modalities, each with different base frequencies, because we only need to distinguish between different view or modality IDs.

\subsection{Masked Learning}
\label{sec:3.2}

Unlike masked autoencoders (MAE), which mask portions of a single image, we extend this concept to the image level across multi-view, multi-modal settings, following a similar approach to 4M~\cite{mizrahi20244m}, to enable flexible I/O configurations. By masking specific views or modalities, the model learns to predict the missing content during both training and inference, facilitating dynamic and adaptable task handling.


\pseudopara{Training strategy} During training, in addition to the standard fully random masking strategy, we apply task-specific assignments. Specifically, we divide the training tasks into novel view synthesis, pose estimation, and depth prediction, along with the full random tasks, following a 3:3:3:1 ratio. We adopt a multi-stage training strategy: first training on 4-view models at 256 resolution, followed by 8-view models, and finally on 8-view models at 512 resolution.  Following Hunyuan-DiT~\cite{li2024hunyuan}, we use v-prediction~\cite{salimans2022progressive} as training objective. We adopt a 10\% probability of dropping conditions to enable classifier-free guidance (cfg)~\cite{ho2022classifier}. 

\subsection{Dataset Preparation}
\label{sec:3.3}
\pseudopara{Training data} We train Matrix3D on a mixture of six datasets: Objaverse~\cite{deitke2023objaverse}, MVImgNet~\cite{yu2023mvimgnet}, CO3D-v2~\cite{reizenstein21co3d}, RealEstate10k~\cite{zhou2018stereo}, Hypersim~\cite{roberts2021hypersim}, and ARKitScenes~\cite{baruch2021arkitscenes}. The first three datasets are indoor and object-centric, while the latter three cover large-scale indoor and outdoor scenes. 
Note that not all datasets provide all modalities and we use the available modalities for each dataset accordingly.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pose-co3d.pdf}
    \vspace{-6mm}
    \caption{Sparse-view pose estimation results on CO3D dataset. The black axes are ground-truth and the colored ones are the estimation.
    }
    \vspace{-4mm}
    \label{fig:pose}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/nvs-gso-v2.pdf}
    \vspace{-6mm}
    \caption{Qualitive evaluation results of novel view synthesis from single images on GSO and ARKitScenes dataset: a) random novel views; b) and c) follow the view configuration of SyncDreamer and Wonder3D respectively; d) indoor scenes from ARKitScenes dataset. Note that our method supports NVS of \textbf{arbitrary poses}.}
    \vspace{-4mm}
    \label{fig:nvs-gso}
\end{figure*}


\pseudopara{Normalization}  
Due to the highly diverse distributions of existing datasets, including variations in scale and scene type, preprocessing them consistently poses a challenge. To address this, we apply scene normalization and camera normalization. Please check the supplementary for details.

%To address this, we apply the following normalization.
%\begin{itemize}
%    \item Scene Normalization:  
%    To normalize the whole scene scale, we adapt our approach depending on the dataset type and available modalities. For object-centric datasets with camera poses provided (i.e., Objaverse~\cite{deitke2023objaverse}, MVImgNet~\cite{yu2023mvimgnet}, and CO3D-v2~\cite{reizenstein21co3d}), we follow RayDiffusion~\cite{zhang2024cameras} by setting the intersection point of the input camera rays as the origin of the world coordinates and defining the scene scale accordingly. For scene-type datasets that provide depth information (i.e., Hypersim~\cite{roberts2021hypersim} and ARKitScenes~\cite{baruch2021arkitscenes}), we use the depth of the first view as a reference, calculating its median value and normalizing it to 1.0. For those datasets without depth data (i.e., RealEstate10k~\cite{zhou2018stereo}), we determine the scale based on the camera distances to the average positions and set the maximum distance to 1.0.

%    \item Camera Normalization:  
%    We perform camera normalization after scene normalization. Specifically, we set the first viewâ€™s camera as the identity camera with rotation \( R = I \) and translation \( T = [0, 0, 1] \), while preserving relative transformations between cameras across views.
%\end{itemize}

\pseudopara{Incomplete depth}
Real-world datasets often provide incomplete depth. A default solution to use these data is discarding patches with invalid pixels, which results in low data utilization. Instead, we concatenate valid masks to all the depth maps for both condition maps and noisy maps, and let the model learn how to discard invalid pixels. 
%This also allow us to utilize incomplete or even sparse depth input during inference. 
This also allow us to utilize sparse depth input during inference. 

\begin{table*}[t]
    \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|ccccccc|ccccccc}
\toprule
Metrics & \multicolumn{7}{c|}{Relative Rotation Accuracy @ 15$^\circ$($\uparrow, \%$)} & \multicolumn{7}{c}{Camera Center Accuracy @ 0.1 ($\uparrow, \%$)} \\
\midrule
\# of Images & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
\midrule

COLMAP (SP+SG)~\citep{schoenberger2016sfm} & 31.3 &  29.0 &  27.3 &  27.6 &  28.0 &  29.3 &  31.9  & 100.0 &  34.8 &  24.1 &  19.1 &  16.0 &  15.0 &  15.7\\
PoseDiffusion~\citep{wang2023posediffusion} & 73.6 & 74.3 & 74.6 & 75.4 & 76.0 & 76.7 & 76.9 & 100.0 &  75.1 &  66.4 &  62.5 &  60.2 &  59.1 & 58.1\\
RelPose++~\citep{lin2023relpose++}  & 79.8 & 80.8 & 82.0 & 82.7 & 83.0 & 83.4 & 83.7 & 100.0 &  82.5 &  74.7 &  70.7 &  68.2 &  66.5 &  65.0\\
DUSt3R~\cite{wang2024dust3r} & 85.6 & 88.6 & 90.1 & 90.7 & 91.3 & 91.7 & 92.0 & 100.0 & 87.8 & 83.9 & 81.3 & 80.3 & 80.1 & 79.2\\
RayDiffusion~\cite{zhang2024cameras} & 90.4 & 91.2 & 91.5 & 91.9 & 92.1 & 92.3 & 92.4 & 100.0 & 93.1 & 88.9 & 86.0 & 84.1 & 82.8 & 81.9\\
\midrule
Ours {\fontsize{8}{16}\selectfont RGB Only} & \underline{95.6} & \underline{96.0} & \textbf{96.3} & \textbf{96.5} & \textbf{96.5} & \underline{96.3} & \underline{96.1} & 100.0 & \underline{93.5} & \underline{91.7} & \underline{90.6} & \underline{90.0} & \underline{89.1} & \underline{87.8}\\
Ours {\fontsize{8}{16}\selectfont RGB + Depth} & \textbf{95.8} & \textbf{96.3} & \underline{96.2} & \textbf{96.5} & \textbf{96.5} & \textbf{96.4} & \textbf{96.3} & 100.0 & \textbf{93.8} & \textbf{92.0} & \textbf{91.5} & \textbf{91.0} & \textbf{90.4} & \textbf{89.5} \\
\bottomrule
\end{tabular}
}
    \vspace{-3mm}
    \caption{Pose evaluation on CO3D. The percentage of relative rotations within 15 degrees and camera center errors within 10\% of the groundtruth scene scale are reported. The best results are in \textbf{bold} and the second bests are \underline{underlined}.
    }
    \vspace{-4mm}
    \label{tab:co3d_pose}
\end{table*}


\subsection{Training Setup}
\label{sec:3.4}
We initialize our model with pre-trained Hunyuan-DiT checkpoints~\cite{li2024hunyuan}. 
In the first stage, we train the model for 180K steps with a learning rate of 1e-4 on 64 NVIDIA A100 80G GPUs. In the latter two stages, we train the model for 20K steps for each stage with a learning rate of 1e-5 on 128 GPUs. 
The entire training takes approximately 20 days.


\subsection{Downstream Tasks}
\label{sec:3.5}

\pseudopara{Reconstruction tasks as modality conversions}
The Matrix3D model is able to perform multiple downstream tasks including pose estimation, multi-view depth estimation, novel view synthesis, or any hybrid of these tasks. By feeding any combination of conditional information and the desired output maps as noise, the model denoises the outputs according to the learned joint distribution. For example, pose estimation can be conducted by providing images of all input views, the identity camera pose for the reference view, and noisy ray maps for other views; novel view synthesis can be formulated as providing posed images for all reference views, ray maps for the novel views, and noisy novel-view images. 
Moreover, our model allows any reasonable input/output combinations, which cannot be achieved by previous task-specific models. For example, Matrix3D can achieve better results during NVS and pose estimation if depth maps are additionally provided. 

\pseudopara{3DGS Optimization}
In this section, we describe how to utilize our model for single or few-shot image reconstruction. We first use the Matrix3D model to complete multi-modality input (images/poses/depth maps) and also the image viewpoints. 
For few-shot inputs, we 1) estimate their camera poses, which can only be achieved by external methods in previous few-shot reconstruction systems; 2) estimate depth for the inputs as an initialization for 3DGS optimization; 3) synthesize novel views for stabilizing 3DGS optimization. 
For single input, we 1) synthesize more images to reach 8 key views with relatively large baselines to have an overall coverage of the target object or scene; 2) estimate depth maps for these key views; 3) synthesize novel views to interpolate the key images. 
Finally, we forward all the previous results to an open-source 3DGS reconstruction~\cite{nerfstudio} with several modifications. It is noteworthy that the 3DGS reconstruction is specially tailored to mitigate the multi-view inconsistency among the generated images. 

Please check the supplementary materials for more details about architectures, data pre-processing, training, and 3DGS optimization designs.
