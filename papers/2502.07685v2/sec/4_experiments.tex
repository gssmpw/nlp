\section{Experiments}
In the following, we present the experiment results of different photogrammetry tasks. 


\subsection{Pose Estimation}
We first evaluate our model for pose estimation under sparse views on the CO3D dataset. The proposed model is compared with multiple types of previous works: 1) traditional SfM: COLMAP~\cite{schoenberger2016sfm}; 2) discriminative neural networks: RelPose++~\cite{lin2023relpose++} and DUSt3R~\cite{wang2024dust3r}; 3) generative neural networks: PoseDiffusion~\cite{wang2023posediffusion} and RayDiffusion~\cite{zhang2024cameras}. 

We evaluate two metrics: relative rotation accuracy and camera center accuracy following RayDiffusion. For each scene, we estimate all 7 source views out of 8 views in one batch. The metrics are evaluated in a pair-wise manner. Our method outperforms other baselines by a significant margin (Table~\ref{tab:co3d_pose}). Figure~\ref{fig:pose} also presents a qualitative comparison between our predictions and ground truth poses. Our method performs better by a large margin than all baselines.

\begin{table}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{ll|ccc}
\toprule
View Settings & Methods & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
\midrule
\multirow{2}*{\shortstack{$\phi \in \{0\degree, 360\degree\}$ \\ $\theta =30\degree$}} & SyncDreamer~\cite{liu2023syncdreamer} & \underline{19.22} & \underline{0.82} & \textbf{0.16} \\
 & Ours & \textbf{20.45} & \textbf{0.86} & \textbf{0.16} \\
\midrule
\multirow{2}*{\shortstack{$\phi \in \{0\degree, 360\degree\}$ \\ $\theta =0\degree$}} & Wonder3D~\cite{long2023wonder3d} & \underline{13.28} & \underline{0.78} & \underline{0.33} \\
 & Ours & \textbf{18.97} & \textbf{0.86} & \textbf{0.18} \\
\midrule
\multirow{2}*{\shortstack{$\phi \in \{0\degree, 360\degree\}$ \\ $\theta \in \{30\degree, -20\degree\}$}} & InstantMesh~\cite{xu2024instantmesh} & 13.78 & 0.80 & 0.25 \\
 & Ours & \textbf{18.66} & \textbf{0.85} & \textbf{0.19} \\
\midrule
\multirow{4}*{\shortstack{$\phi \in \{0\degree, 360\degree\}$ \\ $\theta \in \{-90\degree, 90\degree\}$}} & Zero123~\cite{liu2023zero} & 17.56 & 0.80 & \underline{0.18} \\
& Zero123-XL~\cite{deitke2024objaverse} & 18.75 & 0.81 & \textbf{0.17} \\
& Ours & \underline{18.87} & \underline{0.85} & 0.21 \\
& Ours +Depth & \textbf{19.16} & \textbf{0.86} & 0.19 \\
\bottomrule
\end{tabular}
}
    \vspace{-3mm}
    \caption{Novel view synthesis (diffusion samples) evaluation on GSO. Results are grouped by different view settings because some methods uses fixed poses. Best results are in \textbf{bold} and the second best are \underline{underlined}. $\phi$ and $\theta$ denote azimuth and elevation angles.}
    \vspace{-3mm}
    \label{tab:gso_nvs}
\end{table}

\begin{table}[t]
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|cccccc}
    \toprule
    Method & $\boldsymbol{\delta_{1}}$$\uparrow$ & $\boldsymbol{\delta_{2}}$$\uparrow$ & $\boldsymbol{\delta_{3}}$$\uparrow$ & AbsRel$\downarrow$ & log10$\downarrow$ & RMS$\downarrow$  \\
    \midrule
    Metric3D v2~\cite{yin2023metric3d, hu2024metric3d}  & 0.969   & 0.992    & 0.996  & 0.064   & 0.039    & 75.538   \\
    Depth Anything v2~\cite{depth_anything_v1, depth_anything_v2} & 0.950 & 0.992 & 0.997  & 0.077 & 0.045 & 85.188     \\
    \midrule
    Ours \ {\fontsize{8.5}{16}\selectfont CFG=1.5}  & 0.985 & 0.997 & 0.999 & \textbf{0.036} & 0.023 & 47.806   \\ 
    Ours \ {\fontsize{8.5}{16}\selectfont w/o CFG}  & \textbf{0.992} & \textbf{0.999}  & \textbf{1.000}  & 0.038 & \textbf{0.022} & \textbf{40.214}   \\ 
    \bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Quantitative evaluation of monocular metric depth prediction tasks on DTU. The best results are in \textbf{bold}. }
\vspace{-6mm}
\label{tab:mono_depth}
\end{table}

\subsection{Novel View Synthesis}
In this section, we benchmark novel view synthesis task on diffusion samples on GSO~\cite{downs2022google} dataset against prior multi-view diffusion methods, including Zero123~\cite{liu2023zero}, Zero123XL~\cite{deitke2024objaverse}, SyncDreamer~\cite{liu2023syncdreamer}, Wonder3D~\cite{long2023wonder3d}, and InstantMesh~\cite{xu2024instantmesh}. 
For methods generating novel views at fixed camera poses, we follow their view configuration and use different rendered ground truths when comparing with them. 
For methods allowing arbitrary view synthesis, we render 32 random viewpoints for evaluation. 
PSNR, SSIM, and LPIPS are adopted as evaluation metrics. Figure~\ref{fig:nvs-gso} shows qualitative results on CO3D and ARKitScenes dataset. The quantitative results are shown in Table~\ref{tab:gso_nvs}. Our method achieves the best results for most of the metrics. 



\subsection{Depth Prediction}\label{sec:exp-depth}
\pseudopara{Monocular Depth Prediction} We first evaluate our model on the monocular metric depth prediction task. Although our model is trained on at least two views, we found that it can still predict high-quality depth from single images. Table~\ref{tab:mono_depth} shows the comparison against Metric3D v2~\cite{hu2024metric3d} and Depth Anything v2~\cite{depth_anything_v2}. We adopt the IDR~\cite{yariv2020multiview} subset of the DTU dataset, and metrics following previous methods~\cite{hu2024metric3d} to evaluate all methods. All methods are not trained on the DTU dataset. Our method performs significantly better than the baselines. Qualitatively, we found that monodepth methods often produce distorted geometry which cannot be recovered by global linear alignment. This may be resulted from focal ambiguity issue and large domain gap between object-centric and open-scene data.

\begin{table}[t]
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{ll|cccc|cc}
    \toprule
    \multicolumn{2}{l}{Methods} & Pose & Range & Int. & Align & rel$\downarrow$ & $\tau \uparrow$  \\
    \midrule
    %\specialrule{1.5pt}{0.5pt}{0.5pt}
    \multirow{2}*{(a)} & COLMAP~\cite{schoenberger2016sfm, schoenberger2016mvs} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & \textbf{0.7}   & \textbf{96.5}    \\
    & COLMAP Dense~\cite{schoenberger2016sfm, schoenberger2016mvs} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & 20.8   & 69.3  \\
    \midrule
    \multirow{3}*{(b)} & MVSNet~\cite{yao2018mvsnet} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$  & \textbf{(1.8)} & (86.0) \\
    & MVSNet Inv. Depth~\cite{yao2018mvsnet} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$  & \textbf{(1.8)} & (86.7) \\
    & Vis-MVSNet~\cite{zhang2023vis} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$  & \textbf{(1.8)} & \textbf{(87.4)} \\
    & MVS2D DTU~\cite{yang2022mvs2d} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$  & (3.6) & (64.2) \\
    \midrule
    \multirow{4}*{(c)} & DeMon~\cite{ummenhofer2017demon} & $\times$ & $\times$ & $\checkmark$ & $\|\mathbf{t}\|$  & 21.8   & 16.6 \\ 
    & DeepV2D~\cite{teed2018deepv2d} & $\times$ & $\times$ & $\checkmark$ & med & 7.7 & 33.0 \\
    & DUSt3R 224~\cite{wang2024dust3r} & $\times$ & $\times$ & $\times$ & med  & \textbf{2.76}   & \textbf{77.32} \\
    & DUSt3R 512~\cite{wang2024dust3r} & $\times$ & $\times$ & $\times$ & med  & 3.52   & 69.33 \\
    \midrule
    \multirow{9}*{(d)} & DeMon~\cite{ummenhofer2017demon} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & 23.7   & 11.5  \\
    & Deepv2D~\cite{teed2018deepv2d} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & 9.2   & 27.4  \\
    & MVSNet~\cite{yao2018mvsnet} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & (4429.1)  & (0.1) \\
    & MVSNet Inv. Depth~\cite{yao2018mvsnet} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & (28.7)  & (48.9) \\
    & Vis-MVSNet~\cite{zhang2023vis} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & (374.2)  & (1.7) \\
    & MVS2D DTU~\cite{yang2022mvs2d} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & (1.6) & (92.3) \\
    & Robust MVD Baseline~\cite{schroppel2022benchmark} & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & 2.7 & 82.0 \\
    & Ours & $\checkmark$ & $\times$ & $\checkmark$ & $\times$  & \textbf{1.83}   & \textbf{85.45} \\
    \bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Quantitative evaluation of multi-view metric depth prediction tasks on DTU. The methods are categorized into: a) traditional methods; b) with poses and depth range; c) without poses and depth range, but with alignment; and d) with poses, without range and alignment. Parentheses
refers to training on the same set. The best results are in \textbf{bold}. Results of other methods are reported in DUSt3R~\cite{wang2024dust3r}.}
\vspace{-5mm}
\label{tab:multiview_depth}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mono-recon.pdf}
    \vspace{-7mm}
    \caption{Monocular 3D reconstruction. Additional novel view renderings of our method are shown in the last two columns.
    }
    \vspace{-6mm}
    \label{fig:mono-recon}
\end{figure}



\pseudopara{Multi-view Depth Prediction} We then evaluate Matrix3D on the multi-view stereo depth prediction task. We evaluate it on the DTU~\cite{aanaes2016large} dataset. Following DUSt3R, we compute the Absolute Relative Error (rel) and Inlier Ratio on all test sets. Table~\ref{tab:multiview_depth} shows the quantitative results. We also back-project the depth maps to point clouds (with and without pose input) and evaluate their Chamfer distance to the ground truths. 
Quantitative results on DTU are shown in Table~\ref{tab:multiview_pcd}. More details about the conversion from depth maps to point clouds can be found in the supplementary material. 

Here we mainly discuss DUSt3R and our method which regress 3D information from images by a network architecture without any 3D-specific operations. We found that the results of our method is better than DUSt3R for depth maps, but is worst for point cloud. Given that our method also achieves higher pose estimation accuracy, one possible reason is that DUSt3R is supervised directly by point positions. So it can achieve good point cloud evaluation, but fails for the two decoupled tasks. Overall, these two methods cannot achieve the same accuracy level as the methods with 3D domain knowledge embedded. But the results are accurate enough to serve as good 3DGS initialization.



\begin{table}[t]
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{ll|c|ccc}
    \toprule
    & Methods & GT cams & Acc.$\downarrow$ & Comp.$\downarrow$ & Overall$\downarrow$       \\
    \midrule
    \multirow{4}{*}{(a)} & Camp~\cite{campbell2008using} & $\checkmark$ & 0.835 & 0.554 & 0.695 \\
    &Furu~\cite{furukawa2009accurate} & $\checkmark$ & 0.613 & 0.941 & 0.777 \\
    &Tola~\cite{tola2012efficient} & $\checkmark$ & 0.342 & 1.190 & 0.766 \\
    &Gipuma~\cite{galliani2015massively} & $\checkmark$ &\textbf{ 0.283} & 0.873 & 0.578\\
    \midrule
    \multirow{4}{*}{(b)} &MVSNet~\cite{yao2018mvsnet} & $\checkmark$ &0.396 & 0.527 & 0.462 \\
    & CVP-MVSNet~\cite{Yang_2020_CVPR} & $\checkmark$ & 0.296 & 0.406 & 0.351 \\
    & UCS-Net~\cite{cheng2020deep} & $\checkmark$ & 0.338 & 0.349 & 0.344 \\
    & CIDER~\cite{xu2020learning} & $\checkmark$ & 0.417 & 0.437 & 0.427 \\
    & CasMVSNet~\cite{gu2020cascade} & $\checkmark$ & 0.325 & 0.385 & 0.355 \\
    & PatchmatchNet~\cite{wang2020patchmatchnet} & $\checkmark$ & 0.427 & 0.277 & 0.352 \\
    & Vis-MVSNet~\cite{zhang2023vis} & $\checkmark$ & 0.369 & 0.361 & 0.365 \\
    & CER-MVS~\cite{ma2022multiview} & $\checkmark$ & 0.359 & 0.305 & 0.332 \\
    & GeoMVSNet~\cite{zhe2023geomvsnet} & $\checkmark$ & 0.331 & \textbf{0.259} & \textbf{0.295} \\
    \midrule
    & DUSt3R~\cite{wang2024dust3r} & $\times$ &   2.677  &  0.805  & 1.741  \\ %
    & Ours & $\times$ &   3.261 & 1.170 & 2.266 \\ %
    & Ours & $\checkmark$ &   2.930 & 1.265 & 2.098 \\ %
    \bottomrule
    \end{tabular}
}
\vspace{-3mm}
\caption{Quantitative evaluation of point clouds back-projected from multi-view metric depths on DTU. The methods are categorized into: a) traditional methods and b) learning-based MVS. The best results are in \textbf{bold}. Results of other methods are reported in DUSt3R~\cite{wang2024dust3r}.}
\label{tab:multiview_pcd}
\vspace{-4mm}
\end{table}

\begin{table}[t]
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l|c|}
    \toprule
    Method & CLIP (Image) $\uparrow$\\
    \midrule
    ImageDream~\cite{wang2023imagedream} & 83.77 $\pm$ 5.2 \\
    One2345++~\cite{liu2024one2345} & 83.78 $\pm$ 6.4\\
    IM3D~\cite{melas20243d} & \textbf{91.40} $\pm$ 5.5\\
    CAT3D~\cite{gao2024cat3d} & 88.54 $\pm$ 8.6\\
    Ours & \underline{88.76} $\pm$ 7.2 \\
    \bottomrule
\end{tabular}
    \begin{tabular}{|l|ccc}
    \toprule
    Method & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
    \midrule
    Zip-NeRF~\cite{barron2023zip} & 14.34 & 0.496 & 0.652 \\
    ZeroNVS~\cite{sargent2023zeronvs} & 17.13 & 0.581 & 0.566 \\
    ReconFusion~\cite{wu2024reconfusion} & 19.59 & \underline{0.662} & 0.398 \\
    CAT3D~\cite{gao2024cat3d} & \textbf{20.57} & \textbf{0.666} & \textbf{0.351} \\
    Ours & \underline{20.02} & 0.633 & \underline{0.396} \\
    \bottomrule
\end{tabular}
}
\vspace{-3mm}
\caption{Quantitative evaluation of monocular (left) and posed few-shot (right) 3d reconstruction. The best results are in \textbf{bold} and the second bests are \underline{underlined}.}
\vspace{-3mm}
\label{tab:recon}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sparse-gs.pdf}
    \vspace{-7mm}
    \caption{Sparse view 3D Gaussian Splatting reconstruction results from 3-view images input on CO3D dataset.}
    \vspace{-6mm}
    \label{fig:sparse-recon}
\end{figure}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/unposed-vis.pdf}
    \vspace{-3mm}
    \caption{Unposed sparse-view 3D reconstruction results. }
    \vspace{-5mm}
    \label{fig:unposed-vis}
\end{figure*}


\subsection{3D Reconstruction}
\pseudopara{Monocular}
In the following, we evaluate the 3D reconstruction performance from single images. Specifically, we compare Matrix3D with diffusion-based optimization methods including ImageDream~\cite{wang2023imagedream}, One2345++~\cite{liu2024one2345}, IM-3D~\cite{melas20243d}, and CAT3D~\cite{gao2024cat3d} in terms of CLIP scores following CAT3D. Figure~\ref{fig:mono-recon} and Table~\ref{tab:recon} illustrate the comparisons. Our method achieves comparable results to SOTA methods.


\pseudopara{Sparse-view}
We perform 3D reconstruction from sparse-view unposed images. 
Although previous methods have made similar attempts, they require given poses which is challenging to estimate for traditional SfM methods in the case of sparse view, and just use ground truth for experiments, leaving pose estimation problem unsettled. 
Our framework seamlessly integrates the pose estimation and reconstruction process into a single pipeline. Figure~\ref{fig:unposed-vis} shows the results of unposed 3-view images from CO3D and ARKitScene datasets. The reconstruction process can be found in Sec.~\ref{sec:3.5}. Results show that our method successfully performs the reconstruction given unposed images.



We also evaluate the task using ground truth poses as input. Following previous methods, we conduct 3-view reconstruction experiments on the same train/test split of the CO3D dataset, as done in CAT3D\cite{gao2024cat3d}. We use PSNR, SSIM, and LPIPS to evaluate performance (Table \ref{tab:recon}). Figure~\ref{fig:sparse-recon} shows the comparisons. Note that we use fewer than half of the novel views employed in CAT3D for reconstruction. Our method performs slightly worse than CAT3D, primarily due to the smaller number of views used for 3DGS training. Additionally, while our reconstruction method can be run on a single GTX 3090 GPU, CAT3D requires 16 A100 GPUs, making it impractical for most users.





\subsection{Hybrid Tasks}\label{sec:exp-hybrid}
One major advantage of multi-modal masked learning is that the model can accept flexible input combinations. If additional information is provided, the model knows how to take advantage of them, and thus produces better outputs. We show this quantitatively by adding depth ground truth to the NVS and the pose estimation tasks mentioned above. For pose estimation, as shown by Table~\ref{tab:co3d_pose}, \textit{Ours RGB+Depth} consistently outperforms \textit{Ours RGB Only} in terms of camera center accuracy. One possible reason is that depth information mitigates scale ambiguity. For NVS, as shown by Table~\ref{tab:gso_nvs}, \textit{Ours+Depth} also achieves better results than \textit{Ours}. Intuitively, depth maps provide parital geometry, and thus facilitate NVS. In application, users can utilize depth measurement from active sensors to boost the performance of these two tasks. 






