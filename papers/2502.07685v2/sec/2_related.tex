\section{Related Work}
\label{sec:related}

Photogrammetry, also known as image-based 3D reconstruction, is a foundational pillar in the field of 3D vision. A typical photogrammetry pipeline consists of several critical sub-steps. Below, we provide an overview of the most relevant works related to image-based 3D reconstruction.

\textbf{Structure-from-Motion (SfM)} is a classical approach for simultaneously recovering sparse 3D structures and estimating camera poses from multiple overlapping 2D images. These pipelines~\cite{hartley2003multiple, crandall2012sfm, schoenberger2016sfm, cui2017hsfm} typically begin with camera parameter estimation through feature matching~\cite{harris1988combined, lowe2004distinctive, rosten2006machine, bay2006surf} across images, followed by bundle adjustment to jointly optimize the 3D point cloud and camera poses. Recent advances have focused on enhancing the robustness of SfM through learning-based feature extractors~\cite{detone2018superpoint, dusmanu2019d2, revaud2019r2d2, luo2020aslfeat}, improved image-matching techniques~\cite{sun2021loftr, sarlin2020superglue, chen2022aspanformer, lindenberger2023lightglue}, and neural bundle adjustment~\cite{lin2021barf, lindenberger2021pixel, xiao2023level, wang2024vggsfm}. However, challenges still persist in sparse input scenarios, though gradually alleviated. Limited observations still introduce multi-view ambiguity and performance degradation.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/training-pipeline.pdf}
    \vspace{-6mm}
    \caption{We train the Matrix3D by masked learning. Multi-modal data are randomly masked by noise corruption. Observations (green) and noisy maps (yellow) are fed into the encoder and the decoder respectively. By attaching the view and modality information to the clean and noisy inputs via different positional encodings, the model learns to denoise the corrupted maps and generate the desired outputs.}
    \vspace{-6mm}
    \label{fig:pipeline}
\end{figure*}

\textbf{Multi-view Stereo (MVS)} builds upon the camera poses obtained by SfM to create dense 3D geometry. Traditional MVS methods~\cite{hirschmuller2007stereo, schoenberger2016mvs, galliani2015massively, furukawa2015multi} depend on hand-crafted features and engineered regularizations to build dense correspondences and recover 3D points~\cite{lhuillier2005quasi, furukawa2009accurate}, volumes~\cite{kutulakos2000theory, seitz1999photorealistic, furukawa2009accurate}, or depth maps~\cite{campbell2008using, tola2012efficient, galliani2015massively}. Learning-based methods~\cite{yao2018mvsnet, yao2019recurrent, chen2019point, gu2020cascade, zhang2023vis} offer more powerful reconstruction with improved completeness and generalization capabilities. Typically, these methods assume well-calibrated camera parameters, which limits their robustness and applicability in real-world scenarios where calibration may be imprecise or unavailable.

\textbf{Sparse-view Pose Estimation} is extremely challenging due to the limited overlap between input images, making it difficult for traditional methods~\cite{snavely2006photo, mur2015orb} to build correspondences. Recent research has explored various strategies to address these ambiguities and predict relative poses from sparsely sampled images, including energy optimization~\cite{lin2023relpose++, sinha2023sparsepose}, exploiting synthetic data~\cite{jiang2024few}, leveraging data priors~\cite{wang2023posediffusion}, and applying probabilistic models~\cite{chen2021wide, zhang2022relpose}. Recently, RayDiffusion~\cite{zhang2024cameras} employs ray-based cameras and diffusion methods to model pose distributions, while PF-LRM~\cite{wang2023pf} and DUSt3R~\cite{wang2024dust3r} predict point maps in reference frames and recover poses using PnP algorithms.

\textbf{Feed-forward RGB-to-3D} approaches aim to directly infer 3D representations from single or a few RGB images via feed-forward models, without requiring per-scene optimization. These methods leverage strong 3D priors learned from large-scale object-level~\cite{chang2015shapenet} or larger datasets~\cite{deitke2023objaverse} to address inherent ambiguities. Pre-trained geometry models, such as monocular depth predictors~\cite{ranftl2021vision, bian2021auto, yin2021learning, Ranftl2022}, are usually adopted to further improve robustness. Recently, people proposed feed-forward methods~\cite{hong2023lrm, li2023instant3d, xu2023dmv3d, zhang2024gs, tang2024lgm, wu2024direct3d} which combines large transformer models to directly map RGB images into 3D representations~\cite{Chan2022Efficient, gao2024relightable, lin2024gaussian}. While these methods enable efficient 3D generation, their results remain less accurate than optimization-based methods.

\textbf{3D Generation with 2D Priors} refers to methods that use pre-trained 2D vision models to guide 3D generation. DreamFusion~\cite{poole2022dreamfusion} first proposed Score Distillation Sampling (SDS) to synthesize NeRFs from text by iteratively distilling knowledge from text-to-image diffusion models. Subsequent research has enhanced performance by improving distillation strategies~\cite{huang2023dreamtime, wang2023prolificdreamer, chen2023fantasia3d, zeng2024stag4d, haque2023instruct, tang2023dreamgaussian, zhang2024jointnet} or fine-tuning 2D diffusion models with camera conditioning~\cite{wu2024reconfusion, liu2023zero, raj2023dreambooth3d, chan2023generative, gu2023nerfdiff, sargent2023zeronvs, liu2024one}. Recent studies have shown that fine-tuning the model to generate multi-view images simultaneously~\cite{shi2023mvdream, tang2023mvdiffusion, liu2024one2345, shi2023zero123++, yang2024consistnet} provides stronger priors. Beyond SDS, researchers also explored offline 3D reconstruction~\cite{liu2023syncdreamer, long2023wonder3d, lu2024direct2, gao2024cat3d} directly from the generated multi-view information. Moreover, fine-tuning video diffusion models, which inherently encode 3D knowledge, has emerged as a promising approach~\cite{kwak2024vivid, voleti2024sv3d, melas20243d, guo2023animatediff}, though their higher computational demands remain a challenge.

\textbf{Masked Learning} has achieved significant success in pre-training tasks for NLP~\cite{kenton2019bert, brown2020language, radford2018improving, radford2019language} and computer vision~\cite{he2022masked, bachmann2022multimae, li2021mst}. These methods are proven to capture high-level semantics by masking parts of input and training models to reconstruct the masked contents. Recent research has extended this idea to multi-view image settings~\cite{weinzaepfel2022croco, weinzaepfel2023croco}, demonstrating improvements in downstream tasks like optical flow and stereo matching. In this work, we further apply masked learning to multi-view and multi-modal training to develop an all-in-one photogrammetry model.

