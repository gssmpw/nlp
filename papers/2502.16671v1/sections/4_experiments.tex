

\section{Experiments}

\begin{table*}[ht!]
\centering
\vspace{-4mm}
\scalebox{0.78}{
\begin{tabular}{ccc |cc |cc|cc|cc| cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Model}}&   & &  \multicolumn{2}{c}{\textbf{Grounding}} & \multicolumn{6}{|c|}{\textbf{Scene-Level}} & \multicolumn{6}{c}{\textbf{Global-Level}}  \\ \cline{4-17}

& \multicolumn{2}{c}{\textbf{Avg}}  & \multicolumn{2}{|c|}{\textbf{GI}} & \multicolumn{2}{c}{\textbf{I}} & \multicolumn{2}{|c|}{\textbf{AR}} & \multicolumn{2}{c}{\textbf{T}} & \multicolumn{2}{|c}{\textbf{ToM}} & \multicolumn{2}{|c|}{\textbf{SJ}} & \multicolumn{2}{c}{\textbf{WM}} \\  
\cline{4-17}

& VL & L & VL & L  & VL & L  & VL & L & VL  & L & VL  & L & VL  & L & VL  & L \\
\hline
\qwen& 14.8 & 9.4& 4.4 & \textbf{8.0}& 9.5 & 7.0 & 13.8 & 10.9&  12.2 & 3.1 & 32.0 & 20.0 & 28.7 & 10.3 & 16.9 &10.4 \\
\llava  & 14.8& \textbf{13.9}& 8.0& 5.8& 10.1& \textbf{10.1}& 17.8& 16.7 & 7.1 & \textbf{12.2} & 25.3 & 25.3& 26.4 &\textbf{19.5}& 15.6&\textbf{14.3}\\
\hline
\gpt & 24.7 & 12.5 & 12.4 & 6.6 & 20.9 & 9.5 & 23.6 & 13.2& 22.4 & 6.1 & \textbf{41.3} & \textbf{29.3} & \textbf{35.6} & 18.4&31.2 & 13.0\\
\gemini & \textbf{29.9} & 10.7 & \textbf{20.4} & 4.4 & \textbf{21.5} & 8.2 & \textbf{44.0} & \textbf{18.7} & \textbf{33.3} & 12.1 & 35.6 & 19.5 & 32.7 & 7.1 & \textbf{32.5} & 10.4 \\
 \bottomrule
 
\end{tabular}
}
\vspace{2mm} 
\caption{\textbf{Model accuracies on vision-language and language-only inputs across different questions.} VL=Video and text, L=Text only. \textbf{Avg}=Average performance across all questions. \textbf{GI}=Grounding the Imagined, \textbf{I}=Intention, \textbf{AR}=Affect Recognition, \textbf{T}=Temporal, \textbf{ToM}=Theory of Mind, \textbf{SJ}=Social Judgment, \textbf{WM}=Working Memory.}
\vspace{-2mm}
\label{tab:models}
\end{table*}



In this section, we evaluate closed and open-source video LLMs on the {\data} dataset. We detail the evaluation setup, present quantitative results, and conduct error analysis to understand model behavior in non-verbal social reasoning.

\subsection{Experimental setup}

We evaluate state-of-the-art closed-source and open-source video LLMs on {\data} based on performance on current video understanding datasets \cite{fu2024video, wu2025longvideobench}. For closed-source, we selected \gemini~\citep{team2024gemini}  and \gpt~\citep{achiam2023gpt}, and for open-source, we selected models \qwen~\citep{qwen2.5-VL} and \llava~\citep{zhangVideoInstructionTuning2024}. We use a standardized prompt, where we introduce the task of understanding mime performances and subsequently ask a question, potentially including timestamps if it is a grounding or scene-level question. For models that do not natively support the video format, we uniformly sample a number of frames and include the timestamps of the frames in the prompt. See Appendix \ref{app:prompt} for the evaluation prompt template and Appendix \ref{app:model_settings} for model settings.

To evaluate the model accuracy on our open-ended QA task, we use \gpt~for LLM-as-a-judge \cite{zheng2023judging} to automatically verify the model-generated response against ground truth answers. We define a response as correct if it is semantically equivalent to either of the annotated ground truths per question. We evaluate the quality of the LLM grader on a sample of 352 questions and find that the automated grader aligns with a human grader 92.0\% percent of the time. 
See Appendix \ref{app:prompt} for LLM grader prompt, which is adapted from \citet{nagrani2024neptune}.

\subsection{Results}

We report the performance of open-source and closed-source models in Table \ref{tab:models}. All models achieved low performance on the dataset: the open-source models achieve approximately 15\% average accuracy, whereas {\gpt} achieves 24.7\% and {\gemini} obtains 30\%. This highlights the continued challenge for current models in visual abstraction and recognizing subtle social cues. In general, models perform better on global-level questions than on scene-level and grounding questions, suggesting that models struggle more with fine-grained video understanding compared to grasping the overall context of a video. Notably, models perform especially poorly on the grounding category, indicating a significant limitation in models' abstract visual cognition on imagined objects derived from human embodied experience. {\gemini} outperforms open-source models by a factor of 2–3$\times$ across most categories.

To assess language bias in our dataset, we ablate the effect of video information by evaluating all models on text-only input, excluding video. We observe that models achieve higher accuracy on global-level questions than on scene-level ones without access to video. For example, without video, \llava~achieved 25.3\% accuracy on the Theory of Mind category, but only 5.8\% on grounding. This bias in global-level questions likely arises because some questions often include additional context to avoid referring to specific video segments, making it easier for models to infer information from annotations alone. Among open-source models, we observe a 5.4\% drop in overall accuracy for \qwen~and a smaller 1.4\% drop for \llava~when transitioning from video to text-only evaluation. Interestingly, open-source models do not always benefit from video input, suggesting they struggle to integrate visual information effectively in question answering. In contrast, {\gpt} and {\gemini} demonstrate significantly better video comprehension, showing substantial accuracy improvements across all categories when provided with video input.

\begin{table}[t!]
\centering
\vspace{-0mm}
\scalebox{0.9}{
\begin{tabular}{ccc}
\toprule
\textbf{Model}& \textbf{With Text} & \textbf{Without Text}   \\
\hline
\qwen &  18.2 & 11.3 \\
\llava & 15.0 & 16.5\\
\hline
\gpt & 30.3 & 19.0\\
\gemini & 34.5 & 25.3 \\
\bottomrule
\end{tabular}
}
\vspace{2mm}
\caption{\textbf{Model performance on videos with and without text.} Text in the video frames is detected automatically with manual verification. All models except for {\llava} have significantly improved performance on videos containing text.}
\vspace{-2mm}
\label{tab:models_text}
\end{table}

\begin{figure*}[ht]
    \centering
    \vspace{-4mm}
    \includegraphics[width=\linewidth]{figures/mime_errors.pdf}
    \caption{\textbf{{\data} model error examples.} We identify four common error categories. \textbf{Top-Bottom}. Story hallucination errors are when the model's response is unrelated to the video plot. Imagined objects denote errors where the model misidentifies the imagined objects. Nuanced signals denote instances of the model lacking a nuanced understanding of human behavior. Language bias denotes errors when the model is misled by the framing of the question and ignores the video.}
    \vspace{-4mm}
    \label{fig:error_examples}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/error_types_pie.pdf}
    \caption{\textbf{Error types distribution.} We annotate the error types for 20 videos and plot the distribution.}
    \vspace{-2mm}
    \label{fig:error_types}
\end{figure}

\subsection{Error Analysis}

We highlight the main sources of errors by the video LLMs on \data, focusing on \gemini~which achieved best results. We plot the distribution of sources of errors in Fig. \ref{fig:error_types}.

\paragraph{Story hallucination from missing language grounding.} One common pitfall is hallucinating an answer disconnected from the performance narrative. Due to the abstract and nonverbal nature of mime performances, video LLMs may interpret narratives in ways that deviate from commonsense understanding. Fig. \ref{fig:error_examples} contains an example where the mime is acting as a woman who, initially living peacefully with her family, tragically lost her family during a battle. However, {\gemini} misunderstands the narrative and hallucinates that the mime is conducting an orchestra. 

We hypothesize that the model hallucinations stem from the lack of language grounding in mime performances, which provide no verbal context as in existing video datasets with spoken communication. To test this hypothesis, we examine how model performance varies between videos containing meaningful text—such as hand-held signs or banners indicating the performance topic—and those without text. We sample frames from videos at one frame per second and use EasyOCR \cite{EasyOCR} for text detection. A human then verifies the detected text, filtering meaningless texts like watermarks. Model accuracy on videos with and without text is reported in Table \ref{tab:models_text}, where we observe that most models achieve higher accuracy on videos containing text.

Additionally, we investigate whether providing video titles as supplementary language context improves model accuracy. For open-source models, we report the results in Table \ref{tab:models_title}, where we observe that incorporating titles in the input prompt enhances accuracy across most categories. These results highlight a fundamental limitation: models heavily rely on language input for social commonsense reasoning. To advance nonverbal social intelligence, we must \textbf{rethink visual cognition} in multimodal foundation models, ensuring better alignment of social signals across diverse modalities rather than over-relying on language.
\begin{table*}[ht!]
\centering
\vspace{-4mm}
\scalebox{0.78}{
\begin{tabular}{ccc |cc |cc|cc|cc| cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Model}}&   & &  \multicolumn{2}{c}{\textbf{Grounding}} & \multicolumn{6}{|c|}{\textbf{Scene-Level}} & \multicolumn{6}{c}{\textbf{Global-Level}}  \\ \cline{4-17}

& \multicolumn{2}{c}{\textbf{Avg}}  & \multicolumn{2}{|c|}{\textbf{GI}} & \multicolumn{2}{c}{\textbf{I}} & \multicolumn{2}{|c|}{\textbf{AR}} & \multicolumn{2}{c}{\textbf{T}} & \multicolumn{2}{|c}{\textbf{ToM}} & \multicolumn{2}{|c|}{\textbf{SJ}} & \multicolumn{2}{c}{\textbf{WM}} \\  
\cline{4-17}

& T & NT & T & NT & T & NT & T & NT & T & NT & T & NT & T & NT & T & NT \\
\hline
\qwen& 18.9 & 14.8 & 7.3& 4.4 & 14.6& 9.5 & 14.9 & 13.8 & 19.4&  12.2 & 40.0  & 32.0 & 29.9& 28.7 & 23.4 & 16.9 \\
\llava & 15.4 & 14.8& 8.0 & 8.0&  10.8& 10.1& 18.4& 17.8& 7.1 & 7.1 & 24.0 & 25.3 & 29.9 & 26.4 & 16.9& 15.6\\
\bottomrule
 
\end{tabular}
}
\vspace{2mm}
\caption{\textbf{Model performance with and without video title provided.} NT: text prompt does not include title, T: text prompt includes title. \textbf{Avg}=Average performance across all questions. \textbf{GI}=Grounding the Imagined, \textbf{I}=Intention, \textbf{AR}=Affect Recognition, \textbf{T}=Temporal, \textbf{ToM}=Theory of Mind, \textbf{SJ}=Social Judgment, \textbf{WM}=Working Memory. Both {\qwen} and {\llava} performance improve across all categories when given the title.}
\vspace{-2mm}
\label{tab:models_title}
\end{table*}

\paragraph{Failure to interpret imagined objects.}
Understanding mime performances requires the audience to imagine invisible objects or activities from fine-grained gestures and body language \cite{sibierska2022s}. Our analysis suggests that models struggle to perceive imagined objects, leading to downstream reasoning errors. For example, in Fig. \ref{fig:error_examples}, a girl throws a firecracker on the ground, causing a boy to fall and appear injured. However, the model incorrectly identifies the firecracker as a flower pot. We also observe that the accuracy of grounding is positively correlated with correctness in other question categories (Appendix \ref{app:add_fig}).

To assess the impact of misperceived imagined objects on reasoning accuracy, we qualitatively analyze sample questions and examine how model responses change as object references become more explicit. In Fig. \ref{fig:incorrect_ref}, when initially asked what happens after the man in the video raises his hands, {\gemini} provides an incorrect response, misinterpreting the mime’s action as holding a trapeze. However, when the question is augmented with a clear description of the imagined objects—two children the man lifts onto his shoulders—{\gemini} correctly responds that he juggles them in the air. Building upon prior studies examining foundation models’ abstract visual cognition \cite{hsu2024makes, yiu2024kiva, schulze2025visual}, our findings highlight the need for \textbf{better human-AI perception alignment} \cite{muttenthaler2024aligning} to advance multimodal social intelligence.

\begin{figure}[t!]
    \centering
    \vspace{-2mm}
    \includegraphics[width=\linewidth]{figures/ref_error.pdf}
    \vspace{-2mm}
    \caption{Adding explicit reference to imagined objects in question improves {\gemini} accuracy.}
    \vspace{-2mm}
    \label{fig:incorrect_ref}
\end{figure}

\paragraph{Lack nuanced understanding of social signals.}
While models perform relatively well on social judgment and perspective-taking compared to other categories, a closer examination reveals frequent errors stemming from a lack of nuanced understanding of human social signals. Fig. \ref{fig:error_examples} illustrates such a case: a man begins reading a book but eventually loses interest and switches to playing a game. When asked whether the man enjoys reading, \gemini~ incorrectly responds affirmatively, relying on a naive interpretation of his initial reading behavior rather than recognizing his loss of interest. These global-level questions require models to integrate various local signals into a comprehensive narrative, highlighting the limitations of video LLMs in the complexity of social reasoning, and underscoring the need for research on \textbf{fine-grained social reasoning} which has been relatively understudied \cite{mathur-etal-2024-advancing}.

\paragraph{Language bias over video content.}
Finally, we observe that models have strong bias towards language inputs, where models infer answers based on the question prompt rather than the video content. For example, in Fig. \ref{fig:error_examples}, the mimes depict a scene where surgeons use their phones during surgery, accidentally leaving one inside the patient, resulting in their death. However, \gemini~incorrectly identifies the surgeons as professionals, relying on prior assumptions from its language pretraining rather than accurately interpreting the visual narrative. This analysis is further supported by models' text-only accuracy results in Table \ref{tab:models} which show that, particularly for open-source models, performance improves marginally when video context is provided alongside the question text. This suggests a reliance on the question prompt rather than genuine video understanding.

The above findings underscore the need for multimodal models that effectively integrate all input modalities rather than over-relying on language. Additionally, while social bias in language models has been widely studied \cite{liang2021towards, gallegos2024bias}, our results emphasize the need of \textbf{understanding and mitigating how these biases transfer} in multimodal social reasoning, given the current models' dependence on language.