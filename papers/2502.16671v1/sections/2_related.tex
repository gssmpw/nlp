\section{\mbox{Theoretical Grounding \& Related Work}}

Building \textbf{socially intelligent AI} involves creating agents that can sense, perceive, reason about, learn from, and respond to the affect, behavior, and cognition of other agents (human or artificial), and is a key part of AI as it becomes increasingly involved in our everyday lives~\cite{mathur-etal-2024-advancing}. To push the frontiers of socially intelligent AI, a rich body of work has examined various modalities, including language, video, audio, and more. For example,~\citet{gandhiUnderstandingSocialReasoning2023} evaluates the capabilities of AI to model human mental states from language to predict human goals and future behavior. Related work has also focused on extracting fine-grained visual features from gaze~\citep{singhCombiningGazeAI2020, zhangHumanGazeAssisted2020}, expressions~\citep{zheng-etal-2023-facial, zheng2024unimodal}, and body language~\citep{xu-etal-2024-llm,ozaki2024bqabodylanguagequestion,yoon2019robots, liu2022learning}. Multimodal approaches have also been proposed to gain a more holistic understanding of human intent.~\citet{siq2} evaluate video understanding of social situation via question-answering,~\citet{jin-etal-2024-mmtom} evaluate Theory of Mind question answering on human activities in a household environment, and~\citet{li2023intentqa} evaluate human intent understanding in videos. 

Recent advances in \textbf{large multimodal models} have shown impressive video understanding capabilities in various domains, such as egocentric understanding and navigation \cite{mangalam2023egoschema}, multimedia content analysis \cite{liVideoVistaVersatileBenchmark2024a}, and human language understanding~\citep{liang2024hemm,tsai2019multimodal}. Popular state-of-the-art enterprise models, such as Google Gemini \cite{team2024gemini} and GPT-4 \cite{achiam2023gpt}, and open-source models such as Qwen-VL 
\citep{qwen2.5-VL} and LLaVA-Video \cite{zhang2024video} have long context windows capable of handling video and audio inputs. These multimodal models have significantly improved performance on recent challenging video question-answering benchmarks \cite{nagrani2024neptune, mangalam2023egoschema, rawal2024cinepile, fu2024video}. Despite significant progress, most existing models rely primarily on the language modality~\citep{liang2024hemm}, resulting in commonsense biases in question prompts and, in extreme cases, good performance even without access to video at all \cite{min2024morevqa}. Consequently, there is a lack of benchmarks that effectively evaluate the social intelligence capabilities of AI beyond language.

\textbf{Mime performances} serve as a good case for measuring nonverbal social intelligence. Mimes, or pantomimes when the performance has a coherent narrative, are often considered a peripheral form of communication due to their independence of speech and lack of structured conventions \cite{mcneill2008gesture, mcneill2012language}. Nevertheless, pantomimes have a crucial place in developing the human's natural language system; they are often seen as the fundamental building block to human language evolution, where systematic grammatical systems arise from increasingly complex gestural interaction over time \cite{kendon2017reflections, mineiro2017emerging, zlatev2020pantomime, ferretti2023influence}. From a cognitive development perspective, \citet{arbib2017toward, arbib2024pantomime} posits that pantomimic gestures are crucial in the development from ``language-ready'' to ``language use'' brains, and studies have found that pantomime understanding is related to causal reasoning, working memory, and theory of mind capabilities \cite{adornetti2023comprehending, gardenfors2024relations}. In human everyday communication, the highly iconic and transparent nature of pantomimic gestures leads to their frequent use in language-restrained settings, such as language impairment \cite{fex1998use, goldin2005resilience}, cross-cultural communication \cite{ortega2020types, zywiczynski2021pantomimic}, and neurodivergent communication \cite{yavuz2019social}. Thus, mime performance presents a rich and untapped source for benchmark nonverbal social understanding in modern AI systems.