
\section{{\data} Dataset}

We operationalize the opportunities and challenges of building nonverbal social intelligence through mime videos in a new open-ended video question-answering benchmark called {\data}. This benchmark consists of questions that evaluate social understanding at varying levels, from basic perception to complex reasoning about social dynamics across the full video.

\subsection{Question Hierarchy}
The {\data} questions are structured into three levels across the temporal scale, progressing from low-level visual recognition to scene-level interpretation and global-level cognitive reasoning. See Fig. \ref{fig:question_examples} for example questions for each category.

\paragraph{Grounding the Imagined.} An important element of mime performances is its use of abstract iconic gestures or body movements to convey an imagined object or activity \cite{zywiczynski2018defining}. For example, a movement of flapping one's wings may represent a flying bird. These gestures are grounded in humans' embodied experience, and understanding their meaning is crucial for mimic communication \cite{gardenfors2017demonstration, zlatev2020pantomime}. To measure the VLMs' capabilities to ground these imagined objects and actions, our first level of questions involves recognizing basic visual elements in the mime performance, such as objects and activities. This foundational perceptual information is a precursor for higher-level reasoning about interactions and intentions, as shown by \citet{sibierska2022s}.

\paragraph{Scene-Level.} This level moves beyond perception to examine social interactions within a short video segment. Inspired by previous benchmarks \cite{xiao2021next, siq2} and cognitive development research \cite{burris2014all}, we define three categories to assess fine-grained social understanding at the scene level.
\begin{itemize}[noitemsep,topsep=0pt,nosep,leftmargin=*,parsep=0pt,partopsep=0pt]
    \item \textbf{Temporal reasoning} \cite{trabasso1989logical} requires structuring events into a causal chain linked by logical necessity and transitivity. This category involves identifying sequences of events in a scene and their temporal-causal relationships, beyond mere event ordering.
    \item \textbf{Affect recognition} \cite{pantic2003toward} involves identifying and analyzing emotional states through nonverbal cues. Other than static emotion classification, this category also requires detecting subtle emotional shifts, group sentiment, and changes in expression.
    \item \textbf{Intention and behavior understanding} \cite{blakemore2001perception} involves inferring the motivations behind actions and interpreting how observed behavior reflects unobserved internal goals and mental states.
\end{itemize}

\paragraph{Global-Level.} This level assesses the ability to synthesize and reason social information across multiple scenes. Unlike scene-level understanding, it prioritizes organizing and weighing social cues to form higher-order interpretations rather than isolated moments. Drawing from research on non-linguistic narrative comprehension \cite{baron1986mechanical, kuijper2017narrative, adornetti2023comprehending}, we define three categories to evaluate global social intelligence.
\begin{itemize}[noitemsep,topsep=0pt,nosep,leftmargin=*,parsep=0pt,partopsep=0pt]
    \item \textbf{Working memory} \cite{daneman1996working} involves retrieving, integrating, and reasoning information across the entire video. Beyond single events, these questions require the ability to determine the relevance of past information, recall key events, and synthesize a coherent narrative. 
    \item \textbf{Social judgment} \cite{kahneman1986norm} involves evaluating behaviors, assessing personality traits, and identifying social constructs like rapport, trust, and cooperation. This category requires comparing observations to social norms and counterfactual alternatives, highlighting unexpected or abnormal behavior.
    \item \textbf{Theory of mind} \cite{astington1995theory} measures the ability to infer beliefs, goals, and perspectives. This ability enables perspective-taking, reasoning about unseen motives, and anticipating how different individuals understand the same situation.
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/annotation_pipeline.pdf}
    \vspace{2mm}
    \caption{\textbf{Dataset construction pipeline:} 1) Collecting videos from YouTube with various search terms that are summarized by the word cloud. 2) Annotating approximately 6 grounding and scene-level questions and 4 global-level questions per video, removing 120 videos in the process. 3) Verifying the annotated questions and answers, with 97.58\% verifier agreement.}
    \vspace{-2mm}
    \label{fig:dataset_pipeline}
\end{figure}


\subsection{Dataset Construction}
We summarize our dataset construction pipeline in Fig. \ref{fig:dataset_pipeline} and detail individual steps below.

\paragraph{Video collection.} We collect videos from YouTube using various search terms that include the keyword ``mime'', downloading up to 50 videos per keyword. See Fig. \ref{fig:dataset_pipeline} for a word cloud of the search terms. We restrict video durations to between one and ten minutes. Additionally, we only select videos licensed under Creative Commons. This process yields a dataset of 221 videos.

\paragraph{Video validation and annotation.} We asked two human annotators familiar with the question hierarchy to generate questions for each video, along with one-sentence answers to the question. The annotators are provided with a comprehensive description of the question hierarchy alongside a few examples per category. To ensure a diversity of categories, for each video, the annotators are asked to annotate approximately six scene-level questions, four global questions, and as many grounding questions as relevant, although the actual number of questions may vary based on the video. For grounding and scene-level questions, we asked them to provide start and end timestamps denoting the segment that the question is referring to. During the annotation process, annotators eliminated videos that lack a plot, are too difficult to understand, or involve explicit language such as song lyrics or verbal explanations. We use the VGG Image Annotator \cite{dutta2019vgg} for all annotations.

\paragraph{Annotation verification.} After an annotator has created a set of questions and answers for a video, a second person who has not seen the video verifies the quality of the annotation. The verifier is asked to watch the videos, answer the set of questions, and compare their answer with the originally annotated ground truth. The verifier then marks whether the two answers are consistent or otherwise provides suggestions to refine the questions. Finally, we manually review the verification results, remove any questions with inconsistent answers to avoid ambiguity, and refine the questions based on suggestions. By the end of this process, we reduced the original set of questions to 806 questions. We preserve both the original answer and the verifier answer as ground truths to increase the accuracy of automatic evaluators. See Fig. \ref{fig:dataset_pipeline} for an illustration of the dataset construction pipeline.

\subsection{Dataset Statistics}

\begin{figure}[t!]
  % \vspace{-2mm}
  \centering
  \begin{subfigure}{\linewidth}
      \includegraphics[width=\linewidth]{figures/statistics_small.pdf}
  \end{subfigure}
  
  \begin{subfigure}{\linewidth}
  \centering
  \resizebox{0.75\linewidth}{!}{
        \begin{tabular}{cc}
        \toprule
        \textbf{Statistic} & \textbf{Value}   \\
        \hline
        Avg. \# of Questions per Video &  7.98  \\
        Avg. Video Length & 4.57 min \\
        Total \# of Annotated Questions & 826 \\
        \% of Discarded Questions  & 2.42\%\\
        \% of Modified Questions& 2.49\% \\
        \bottomrule
        \end{tabular}
        \label{tab:statistics}
    }
  \end{subfigure}
  \vspace{2mm}
  \caption{\textbf{{\data} dataset statistics.} Distribution of video lengths shows the range of short to long timescales. The distribution of number of questions per video shows that each video is densely annotated, and the distribution of number of questions per category is balanced. The word cloud of answers to the grounding questions reflects the mime video content diversity.}
  \vspace{-2mm}
  \label{fig:statistics}
\end{figure}

We report dataset statistics in Fig. \ref{fig:statistics}. The videos are densely annotated, with 806 total questions, and most videos have more than five questions. We balanced questions across categories, with over 70 questions for each global category and over 100 questions for each local category. A word cloud of the ground truth answers of the grounding questions demonstrates the diverse imagined objects in the videos. 