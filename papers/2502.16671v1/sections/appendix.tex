\section{Additional Experimental Details}

\subsection{Model Settings and Parameters}\label{app:model_settings}
We set the maximum output tokens for each model to be 128 tokens. We detail the settings of the models below. 
\begin{itemize}[noitemsep,topsep=0pt,nosep,leftmargin=*,parsep=0pt,partopsep=0pt]
    \item \gemini~\citep{team2024gemini}: natively supports video as input, including audio. 
    \item \gpt~\citep{achiam2023gpt}: We sample 1 frame per second up to a maximum of 64 frames, in which case the frames are uniformly sampled. We resize the image to 512x512 to fit in the context window.
    \item Qwen2.5-VL-72B \citep{qwen2.5-VL}: natively supports video as input, sampled at 2 frames per second for a maximum of 768 frames.
    \item LLaVA-Video-72B \citep{zhangVideoInstructionTuning2024}: We sample 1 frame per second up to a maximum of 384 frames, in which case the frames are uniformly sampled.
\end{itemize}

\subsection{Prompt Details}\label{app:prompt}

Below is the prompt template for {\gemini} and {\qwen}, which natively take in video input.
\begin{lstlisting}[style=markdownstyle]
You are an expert in mime performance understanding and question answering. 
Typically, the mime would use exaggerated gestures or pretend objects to convey a message.
Answer the question in one sentence using the video, with brief explanations. 
Do not describe the frames just answer the question, and say nothing else.
If the mime is using imaginary objects, describe the objects as if they were real.
Question: <question>
\end{lstlisting}

As {\gpt} and {\llava} require frames to be sampled from the video, we additionally specify the length of the video and the timestamps of the sampled frames in the text prompt. Below is the prompt template for {\gpt} and {\llava}.

\begin{lstlisting}[style=markdownstyle]
You are an expert in mime performance understanding and question answering. 
Typically, the mime would use exaggerated gestures or pretend objects to convey a message.
The video lasts for {video_time}, and {num_frames} frames are uniformly sampled from it.
These frames are located at {frame_time}. Answer the question in one sentence using the video, with brief explanations. 
Do not describe the frames just answer the question. 
If the mime is using imaginary objects, describe the objects as if they were real.
Question: <question>
\end{lstlisting}

Below is the prompt to {\gpt} for LLM-as-a-judge.

\begin{lstlisting}[style=markdownstyle]
Answer Equivalence Instructions:
Carefully consider the following question and answers regarding understanding of a mime performance.
You will be shown a few "gold-standard" answers from human annotators, referred to as the "Reference Answers", and a "Candidate Answer".
Your task is to determine whether the candidate answer is semantically equivalent to either of the reference answers.
In general, a candidate answer is a good answer in place of the "gold" reference if all of the following are satisfied:

1. The candidate directly answers the question without deviation or misunderstanding.
2. The candidate contains all relevant information as the reference, taking into account the question; in particular it does not omit any relevant information present in the reference.
3. The candidate contains neither misleading or excessive superfluous information not present in any of the reference answers, taking into account the question; ; in particular it does not hallucinate story plots not present in the reference.
4. Since the videos are mime performances, invisible actions, objects, or the mime actor portraying objects should be considered correct if and only if they are relevant to the question.

Your response should be one word, "TRUE" or "FALSE", and a brief explanation of your decision. You should respond "TRUE" if the candidate is a good answer in place of any of the reference answers, and "FALSE" otherwise.
\end{lstlisting}

\subsection{Additional Figures}\label{app:add_fig}

\subsubsection{Correlation between Grounding and Other Question Categories}

To analyze the effect of the model's inability to understand localized events, we compute the correlation between performance on grounding the imagined questions to the other question categories. Intuitively, we expect that a model's ability to perform grounding would correlate more strongly with temporal understanding, as one needs to understand individual events before reasoning about a sequence. In Table \ref{tab:models_corr}, \qwen's grounding performance correlates with temporal understanding, whereas for \llava, it correlates with affect recognition and theory of mind. For \gemini, we see that grounding performance contributes both to understanding localized temporal sequences as well as to a more holistic understanding of the video, as shown by higher correlation scores with temporal understanding, social judgment, and working memory. For \gpt, grounding performance correlates with affect recognition and theory of mind.  See Fig. \ref{fig:vl_corr} for correlation between all pairs of question categories when the input is video and language and Fig. \ref{fig:l_corr} for all correlations when the input is language. This suggests that improved understanding of the fine-grained cues would lead to a better grasp of the video plot. Our results demonstrate that an important line of future work is to improve the ability of vLLMs to reason without explicit objects or human-object interactions, which can bottleneck performance on holistic video understanding. 


\begin{table*}[ht!]
\centering
\begin{tabular}{ccccccc}
\toprule
\textbf{Model}& I & AR & T & ToM & SJ & WM   \\
\hline
\qwen &  -0.091 & -0.013 & 0.330 & 0.003 & 0.030  & 0.132 \\
\llava &  -0.040 & 0.329 & -0.106 & 0.228 & 0.129 & -0.050 \\
\hline
\gpt & -0.122& 0.179 & -0.040 & 0.163 & -0.014 & -0.156\\
\gemini & 0.146 & -0.053 & 0.301 & -0.088 & 0.313 & 0.302 \\
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\textbf{Performance correlation between grounding the imaged questions and other categories.} \textbf{I}=Intention, \textbf{AR}=Affect Recognition, \textbf{T}=Temporal, \textbf{ToM}=Theory of Mind, \textbf{SJ}=Social Judgment, \textbf{WM}=Working Memory. }
\label{tab:models_corr}
\end{table*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/qwen_text-only_False_fps2.0_768frames_corr.pdf}
    \includegraphics[width=0.48\linewidth]{figures/llava_video_max-frames_64_text-only_False_title_False_corr.pdf} \\
    \includegraphics[width=0.48\linewidth]{figures/gpt4o_url_max-frames_64_text-only_False_corr.pdf}
    \includegraphics[width=0.48\linewidth]{figures/gemini_text-only_False_corr.pdf}
    \caption{\textbf{Question type performance correlation matrices on video and text input.} For each video, we compute the accuracy over all question types and plot the correlation between accuracies on different question types. From left to right, we show the correlation matrices for {\qwen}, {\llava}, {\gemini}, and {\gpt}.   }
    \label{fig:vl_corr}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.48\linewidth]{figures/qwen_text-only_True_fps2.0_768frames_corr.pdf}
    \includegraphics[width=0.48\linewidth]{figures/llava_video_max-frames_64_text-only_True_title_False_corr.pdf}
    \includegraphics[width=0.48\linewidth]{figures/gpt4o_url_max-frames_64_text-only_True_corr.pdf}
    \includegraphics[width=0.48\linewidth]{figures/gemini_text-only_True_corr.pdf}
    \caption{\textbf{Question type performance correlation matrices on only text input.} To ablate the effect of video, we perform inference for all models with only the text prompt. For each video, we compute the accuracy over all question types and plot the correlation between accuracies on different question types. From left to right, we show the correlation matrices for {\qwen}, {\llava}, {\gemini}, and {\gpt}.   }
    \label{fig:l_corr}
\end{figure*}
\subsubsection{Other Error Types}
We report other sources of errors in Fig. \ref{fig:other_errors}. One common issue is the model referencing an incorrect timestamp or entity when answering a question. For instance, in the figure, while the video segment in question only depicts the man taking the bag from the thief, the model incorrectly responds with a later event—the man returning the bag to the woman. Additionally, the model occasionally exhibits self-repetition in its responses. As shown in the figure, \gemini~ generates a sequence of repeated 'auto's when answering the given prompt.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/other_errors.pdf}
    \caption{\textbf{Other model error examples.} We identify other common error categories. \textbf{Top-Bottom}. Incorrect reference errors are when the model refers to the incorrect timestamp or entity when answering the question. Self repetition errors are when the model falls into repetition when responding to the prompt. }
    \label{fig:other_errors}
\end{figure*}

\section{Human Annotation Details}\label{app:annotation_details}
\subsection{Guidelines for Annotators}

\begin{tcolorbox}[colframe=black, colback=white, arc=3mm, boxrule=1pt, width=\linewidth, title=\textbf{Annotation Instructions}, breakable]
Each video ranges from 1 minute to 10 minutes. For each video, aim for approximately 6 scene-level questions, 4 global-level questions, and any relevant grounding questions. Mark explicit START and END timeframes for shot-level and local-level questions. Here is the question hiearchy:

\textbf{Grounding} This level mainly serves as a sanity check for whether the model understands the portrayed object in mime videos – what’s the person doing, holding, and describing the imagined objects depicted.
    \begin{itemize}
        \item \textit{E.g. What is the person in black shirt doing/holding/etc.?}
    \end{itemize}


\textbf{Scene-Level} — local social information and temporal connection
\begin{itemize}
    \item \textbf{Temporal} Interpreting sequences of events, causality, and the flow of actions within a specific timeframe.
    \begin{itemize}
        \item \textbf{Template:} What caused (some event) to happen? 
        \item \textit{E.g. What caused the person to fall over?}
        \item \textit{E.g. What happened before the person placed the spoon on the table?}
    \end{itemize}

    \item \textbf{Affect Recognition} Tracking and analyzing emotions within a local scene, including subtle transitions and group sentiment. 
    \begin{itemize}
        \item \textbf{Template:} What is the attitude of (some person) towards (some event)? 
        \item \textbf{Template:} How does the (person) feel when (some event) happened?
        \item \textit{E.g. How is the person in black shirt feeling after placing the stone?} 
        \item \textit{E.g. What is the attitude of the man towards the woman?}
        \item \textit{E.g. How did the group's emotional tone shift during the interaction?}
    \end{itemize}

    \item \textbf{Intention and Behavior} Interpreting goals, plans, and motivations within a scene. 
    \begin{itemize}
        \item \textbf{Template:} Why did the (person) do (some action)?
        \item \textit{E.g. Why did the person holding the ice cream cry?}
        \item \textit{E.g. Why is the person in black outfit not speaking?}
        \item \textit{E.g. Why did the woman pretend not to notice the man?}
        \item textit{E.g. Why did the individual wait their turn before speaking?}
    \end{itemize}
\end{itemize}

\textbf{Global-Level} — focus on the overall narrative and high-level concepts
    \begin{itemize}
        \item \textbf{Working Memory} Retrieving, integrating, and reasoning with information across the entire video, beyond localized linear sequences. Requires the ability to decide relevance of information and present a coherent narrative.
        \begin{itemize}
            \item \textbf{Template:} What happened after (some event)? 
            \item \textbf{Template:} How has the relationship between (person) and (person) changed?
            \item \textbf{Template:} What would happen if (an event) didn’t happen? 
            \item \textit{E.g. What object in the beginning of the video foreshadowed the outcome?}
            \item \textit{E.g. How has the actions of the person changed throughout the video?}
            \item \textit{E.g. What event in the start of the video triggered the conflict in the final scene?}
        \end{itemize}
        \item \textbf{Social Judgment} Evaluating behaviors, morality, and adherence to social norms, with consideration for cultural context and moral reasoning.
        \begin{itemize}
            \item \textbf{Template:} How are the (person) and (person) getting along?
            \item \textbf{Template:} How do the (person) actions demonstrate (social concept)?
            \item \textit{E.g. How does the person in the black shirt demonstrate rapport with the person in the blue dress?}
            \item \textit{E.g. What do the person’s actions tell about his personality?}
            \item \textit{E.g. How might the group perceive this individual’s behaviour?}
            \item \textit{E.g. How do the characters’ behaviors suggest they are cooperating?}
        \end{itemize}

        \item \textbf{Perspective Taking (Theory of Mind)} Inferring beliefs, desires, and emotions of others, including both cognitive and affective understanding.
        \begin{itemize}
            \item \textbf{Template:} Does (person A) understand what (person B) was feeling?
            \item \textbf{Template:} What is the (person) hoping to achieve?
            \item \textbf{Template:} Would (person) do (action) after (event)? 
            \item \textit{E.g. What goal does the main character pursue throughout the video?}
            \item \textit{E.g. How is the character’s hope different from the reality?}
            \item \textit{E.g. Why is the main character motivated to change his behavior?}
        \end{itemize}
    \end{itemize}
\end{tcolorbox}

\subsection{Guidelines for Verifiers}

\begin{tcolorbox}[colframe=black, colback=white, arc=3mm, boxrule=1pt, width=\linewidth, title=\textbf{Verification Instructions}, breakable]
This is a video question-answering dataset consisting of mime videos. The goal of this dataset is to evaluate whether current video language models can perform rich visual social reasoning without relying on natural language. 
\begin{itemize}
    \item Watch the entire video before reviewing the questions. The videos can be found [\textit{link}]
    \item Answer each question based on the video content. If a question refers to a specific timestamp, focus on that section; otherwise, consider the whole video.
    \item Compare your answer with the “Reference Answer” column: Mark T in the “Answer Aligned” column if they align. ark F if they are clearly misaligned.
    \item For ambiguous questions, suggest a clearer version in the “Alternative Question” column.
\end{itemize}
\end{tcolorbox}