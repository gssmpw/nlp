
\section{Introduction}

\begin{figure}[t!]
    \centering
    \vspace{-4mm}
    \includegraphics[width=\linewidth]{figures/mime_overview.pdf}
    \caption{\textbf{\data} is a new benchmark testing nonverbal social reasoning in multimodal large language models, with 101 videos of mimes (the art of expression through gesture and movement without spoken words), and 806 question-answer pairs at three levels: 1) grounding the imagined object or activity, 2) scene-level localized understanding, and 3) global-level questions testing holistic social understanding. Today's models only achieve 15-30\% accuracy on \data.}
    \vspace{-4mm}
    \label{fig:overview}
\end{figure}

Social intelligence is integral to human interactions and enables nuanced understanding and communication with others~\citep{mathur-etal-2024-advancing,gweon2023socially,breazeal2003toward}. There is increasing interest in developing socially intelligent AI systems that can understand and interact seamlessly with humans to help them in daily lives, such as stimulating empathic conversations in online mental health forums \cite{sharma2023human}, assisting patients in geriatric care \cite{gonzalez2021social, fleming2003caregiver}, supporting children with autism spectrum conditions \cite{hurst2020social, scassellati2012robots}, and helping educators in classrooms teaching \cite{woo2021use}. However, the majority of research towards socially intelligent AI focuses on language-only data and tasks (e.g., question-answering and dialogue)~\citep{kim2023soda,sap2019socialiqa}, or multimodal data where language is often primary and nonverbal modalities (e.g., vocal and visual expression) are treated as second-class citizens~\citep{liang2024foundations,zadeh2018multimodal}. This results in a fundamental mismatch where today's foundation models are strong at language understanding but have a generally poor command of nonverbal social interactions; for example, nonverbal theory-of-mind~\citep{kampisNonverbalComponentsTheory2017}, facial expression~\cite{huang2023language,liang2024hemm}, group social dynamics~\citep{shum2019theory}, egocentric goal-oriented reasoning~\citep{jiaEgoTaskQAUnderstandingHuman2022} are all challenges for today's language and multimodal foundation models.

\begin{figure*}
    \centering
    \vspace{-4mm}
    \includegraphics[width=\linewidth]{figures/mime_examples.pdf}
    \vspace{-1mm}
    \caption{\textbf{Examples of {\data} question types.} \textbf{Left:} Grounding the imagined questions includes recognizing the activity or pretend object that the mime is acting out. \textbf{Top right:} Scene-level questions include temporal reasoning about a localized sequence of events, affect recognition questions about the emotional state of the characters, and intention and behavior questions that require interpreting the goals and motivations within a scene. \textbf{Bottom right:} Global-level questions involve working memory questions that probe understanding of the plot beyond localized sequences, social judgment questions about how the characters' actions adhere to cultural and social norms, and theory of mind questions about the characters' beliefs, desires, and motivation.}
    \vspace{-2mm}
    \label{fig:question_examples}
\end{figure*}

To address these limitations, we tap into a novel data source rich in nonverbal social interactions -- \textbf{mime performances}. Mimes refer to the art of expression through gesture and movement without spoken word \cite{zywiczynski2018defining} which presents unique challenges and opportunities for AI \cite{phutela2015importance}. Since mime performances are devoid of props and actual objects, instead relying solely on the mime's ability to convey messages, emotions, and narratives through nonverbal communication, AI models must have an acute understanding of human behavior, theory of mind, and the `imagined' objects and actions they convey. 
Furthermore, mimes often depict complex interpersonal relationships and affective states that need to be inferred from nonverbal interactions alone, without explicit narration and dialogue.


To systematically assess proficiency on these tasks, we create a benchmark called \data, obtained by sourcing 221 mime videos from Youtube, annotating each video with questions ranging from local grounding tasks to broader theory of mind and social norm understanding, and meticulous verification of the annotations, resulting in 101 videos and 806 QA pairs. We benchmark state-of-the-art open-source and closed-source vLLMs and find that the overall accuracy ranges from 15\% and 30\%. Our extensive error analysis and ablations point to fundamental shortcomings of vLLMs' visual understanding capabilities, as common failure modes include failing to recognize imagined objects, misinterpreting nuanced social cues, and hallucinating responses based on the text input. We release our benchmark and evaluation framework to drive future research toward verbal and nonverbal social intelligence in AI systems.
