\section{Related Works}
\subsubsection{General Signal Case}
In the noiseless model described by (\ref{eq: original measurement}), a convex programming technique known as "lifting" was proposed, which reformulates the blind deconvolution problem as the estimation of a rank-1 matrix \cite{ARR14, Romberg_RandomMask}. The observations in (\ref{eq: original measurement}) are identical to those in \cite{Romberg_RandomMask}, but without the subsampling applied.  In \cite{Romberg_RandomMask}, Bahmani and Romberg demonstrated that, when $\boldsymbol{d}_l$, $l = 1, \dots, L$, are Rademacher random vectors, the reconstruction of $\widehat{\boldsymbol{h}} \boldsymbol{x}^T$ can be achieved through nuclear-norm minimization, provided the sampling complexity satisfies
 \[
L\gtrsim\mu\log^{2}n\log(n/\mu)\log\log(n+1),
\]  
where 
\begin{equation}
\label{eqn: mu}
\mu=\|\widehat{\boldsymbol{h}}\|_{\infty}^{2}/\|\boldsymbol{h}\|_{2}^{2}
\end{equation} 
represents the coherence parameter of the blurring kernel $\boldsymbol{h}$. Here $\widehat{\boldsymbol{h}}$ is the discrete fourier transform of $\vh$. However, their results were restricted to the noiseless case, and no conclusions were drawn for scenarios involving noise.

The sampling complexity was subsequently improved by Lin and Strohmer \cite{BD_LS}, who investigated the minimization of a least squares problem. In their work, they showed that the optimal solution $\boldsymbol{z}^{\#}$ for the least squares problem satisfies
\begin{equation}\label{eqn: LS_error}
\frac{\|{\boldsymbol{z}^{\#}}-\tau\boldsymbol{z}_0\|_{2}}{\|\tau\boldsymbol{z}_0\|_{2}}\leq\kappa(\mathcal{A}_{\boldsymbol{w}})\eta\left(1+\frac{2}{1-\kappa(\mathcal{A}_{\boldsymbol{w}})\eta}\right),
\end{equation}
provided that $L \gtrsim \log^2 n$, where $\boldsymbol{z}_0 = \begin{bmatrix} \boldsymbol{s} \\ \boldsymbol{x} \end{bmatrix}$ with $\boldsymbol{s}$ being the element-wise inverse of $\widehat{\boldsymbol{h}}$, and $\tau = \frac{c}{\boldsymbol{w}^* \boldsymbol{z}_0}$ for suitably chosen $\boldsymbol{w} \in \mathbb{C}^{2n}$ and $c \in \mathbb{C}$. The noise level $\eta$ is related to the Frobenius norm of $\vZ$ as described in (\ref{eqn: z_term}) in the frequency domain, specifically, $\eta = \|\vF \vZ\|_F = {\sqrt{n}} \|\vZ\|_F,$ where $\vF$ is the $n \times n$ discrete Fourier transform (DFT) matrix, with its $(j,k)$-th element given by $F_{j,k} = \exp\left(-\frac{2\pi \mathrm{i}(j-1)(k-1)}{n}\right).$ Additionally, $\kappa(\mathcal{A}_{\boldsymbol{w}})$  denotes the condition number of the matrix $\mathcal{A}_{\boldsymbol{w}}$, which is explicitly given in \cite{BD_LS}. It follows from this analysis that robust recovery is achievable only when the noise level $\eta$ satisfies the condition $\eta \leq \frac{1}{\kappa(\mathcal{A}_{\boldsymbol{w}})}.$

\subsubsection{Subspace Signal or Sparse Signal Case. }
If $\boldsymbol{x}$ resides within a specified subspace, expressed as $\boldsymbol{x} = \boldsymbol{D}\boldsymbol{z}$, where $\boldsymbol{D}$ is a known $n \times K$ tall orthonormal matrix, a sequence generated by a gradient descent algorithm will converge to the true solution in the noiseless case, provided that the sampling complexity satisfies $L \gtrsim \nu^2(\mu^2\nu^2_{\max}\frac{KL^2}{n} + \nu^2)\log^4n$ \cite{BD_randomSign}. Here, $\nu_{\max}^2 = n \|\boldsymbol{D}\|_\infty^2$ and $\widetilde{\nu}^2 = \frac{n \|\boldsymbol{D} \boldsymbol{z}\|_\infty^2}{\|\boldsymbol{z}\|_2^2}$. However, when $\boldsymbol{D}$ is chosen as the first $K$ columns of the identity matrix $\boldsymbol{I}$, it follows that $\nu_{\max}^2 = n$ and $\widetilde{\nu}^2 \geq 1$. In this case, it becomes untenable to satisfy the condition  $L \gtrsim \widetilde{\nu}^2(\mu^2\nu^2_{\max}\frac{KL^2}{n} + \widetilde{\nu}^2)\log^4n$ even for sufficiently large $L$.


When sparsity is incorporated into the recovery model, several algorithmic advancements have been proposed to address blind deconvolution problem \cite{CE16, SAS, MC99, Ji,XS18}. In the context of calibration, Corollary 3 in \cite{CompressedDeconvolution} shows that when $\boldsymbol{h} \in \mathbb{C}^n$
  is known, the true signal $\boldsymbol{x}$ can be recovered with probability at least $1 - 2 \exp(-Ck)$ for some positive constant $C$, provided that the sampling complexity satisfies $L \gtrsim K \log n$, assuming $\boldsymbol{x}$ is $K$-sparse. However, in our self-calibration problem, $\boldsymbol{h}$ is unknown. Moreover, when $K$ becomes sufficiently large, the required sampling complexity in \cite{CompressedDeconvolution} exceeds the typical $O(\text{polylog}(n))$ bound for the general $\boldsymbol{x}$ case. The results in \cite{Qiao2025} indicate that the proximal alternating linearized minimization (PALM) model can effectively recover both $\boldsymbol{x}$ and $\boldsymbol{h}$ in noisy environments. Convergence results demonstrate that the generated sequence converges to a critical point of the corresponding optimization model. Nevertheless, the sampling complexity required for successful or robust recovery remains unknown. 

In summary, although several studies have tackled the sampling complexity and algorithmic guarantees for sparse recovery, the existing sampling complexities for self-calibration problem  remain suboptimal and have yet to fully address the challenges posed by noisy scenarios.