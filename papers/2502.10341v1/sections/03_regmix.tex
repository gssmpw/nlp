\section{Optimizing Domain Mixtures for Downstream Tasks} \label{sec:regmix}
\input{figures/mixtures_regmix}



The promise of organizing a corpus with WebOrganizer is that we can learn the importance of each domain in a principled way.
In this section, we study how to rebalance these domains to align with the needs of  downstream tasks.
This reflects the typical goal of data curation, which is to improve task performance when using a dataset for training language models \citep{wettig2024qurating, penedo2024finewebdatasetsdecantingweb}---for instance, this is the protocol of the DataComps-LM competition \citep{li2024datacomplm}.

\paragraph{Mixture prediction}
While many methods have been developed to optimize domain mixtures
\citep{xie2023doremi, chen2023skillit, albalak2023efficient, fan2023doge, chen2024aioli, jiang2024adaptive}, most focus on minimizing the in-distribution loss.
We decide to use RegMix \citep{liu2024regmix} due to its simplicity and adapt it to optimize the mixture distribution for downstream tasks.
For each set of domains---topics, formats, and $k$-mean clusters%
---we train 512 models of 50M parameters for 1B tokens and fit a gradient-boosted tree regression model \citep{guolin2017light}.
We make a mixture prediction by searching for the lowest loss in the input space of the regression model, restricting our search to mixtures which upsample domains at most 6.5$\times$, which ensures that we do not exhaust all documents when selecting training data in \autoref{sec:experiments}.
We use an iterative search method, deviating from RegMix. \autoref{app:regmix} discusses our implementation in detail.


\paragraph{Target tasks}
Whereas RegMix \citep{liu2024regmix} uses the C4 loss as a proxy loss for task performance, 
we directly focus on two popular question-answering tasks, MMLU \citep{hendrycks2021measuring} and HellaSwag \citep{zellers2019hellaswag}, as well as their average. 
MMLU requires diverse world knowledge and problem solving abilities, whereas HellaSwag is an adversarially filtered dataset for commonsense reasoning.
To avoid contamination, we use the training and validation set of these two tasks, respectively.
We seek a mixture that minimizes the next-token prediction loss over the correct response normalized by the response length (bits-per-byte) given a 5-shot prompt. This loss has also been used for extrapolating model task performance \citep{bhagia2024establishing}.

\paragraph{Predicted mixtures}
\autoref{fig:mixtures_regmix} visualizes the training distributions predicted by RegMix across the topic and format domains constructed by WebOrganizer.
We observe that the two target tasks call for remarkably different data mixtures.
The MMLU mixture heavily upsamples {\atopic Science \& Technology}, followed by {\atopic History} and {\atopic Health}, and in terms of formats, promotes {\aformat Academic Writing} and {\aformat Q\&A Forums}.
HellaSwag exhibits smoother mixtures, notably amplifying {\atopic Home \& Hobbies} and {\atopic Fashion \& Beauty}, and strongly boosting {\aformat Tutorials}.
Meanwhile, the mixtures tailored towards the average of both tasks tend to combine the prominent components of each task mixture.
