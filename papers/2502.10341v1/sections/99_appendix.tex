\appendix
\onecolumn

\section{Domain Descriptions} \label{app:domain_descriptions}

\FloatBarrier
\input{tables/list_of_topics}
\input{tables/list_of_formats}
\FloatBarrier

\paragraph{Full prompt}
We provide descriptions of the domains in \autoref{tab:list_of_topics} and \autoref{tab:list_of_formats}.
These domain descriptions are given to the model as part of the prompt (with minor adjustments in phrasing).
The prompt template is shown in \autoref{tab:templates} and contains instructions, the text contents and URL of the web page, and the the list of domain descriptions.
We randomly permute the order in which we list the domains for every new document, and enumerate the randomly shuffled choices as: \texttt{ \\
\phantom{------}A: $\{$domain description 1$\}$ \\
\phantom{------}B: $\{$domain description 2$\}$ \\
\phantom{------}... \\
}
The random order avoids spurious positional bias from the large language model, and the alphabetic IDs are useful for obtaining single-token outputs from the model, and we use normalize the next-token probabilities of the characters A-X to obtain a soft prediction of domain categories that reflects model uncertainty. We truncate the text contents of web pages at 50K characters and add a truncation hint to the model.
We also provide 5 few-shot examples to the model, formatted as previous conversation turns with the same prompt format. Each example is carefully curated to be an interesting case of potential domain conflict and provides an explanation of how the conflict should be resolved. The few-shot examples are also presented in a random order for each annotation.

\input{tables/templates}
\FloatBarrier

\section{Training Domain Classifiers} \label{app:domain_classifiers}

\paragraph{Data annotation}
We obtain training data by prompting Llama models to annotate web pages using the prompts described in \autoref{app:domain_descriptions}.
This includes randomizing the order in which domain descriptions and few-shot examples are presented to the model for each annotation.
For all annotations, we leverage the SGLang inference framework \citep{zheng2024sglang}, and obtain soft probabilities over all category labels by normalizing the next-token probabilities over the alphabetical category labels.
We sample web pages for annotations from the RefinedWeb reproduction released by DataComps-LM \citep{li2024datacomplm}---which undergoes similar pre-processing steps as our 200B token pre-training corpus (RefinedWeb filtering and deduplication).
For the first stage of training, we annotate 1M web pages with Llama-3.1-8B-Instruct, and for the second stage, a subset of 100K web pages is annotated with Llama-3.1-405B-Instruct, using FP8 inference and 8x H100 NVIDIA GPUs.
In both datasets, we reserve the same set of 20K web pages as validation and test sets, therefore leaving 80K annotations for the second phase of training.
We repeat the annotation process for both the topic and format taxonomies, and train two separate domain classifiers.


\paragraph{Fine-tuning setting}
We fine-tune a \mbox{gte-base-en-v1.5} embedding model, a 140M parameter embedding model, which reports strong performance on benchmarks for a small model and also features a 8192 token context window \citep{li2023gte}, allowing us to process longer documents.
In each training stage, we train for a total of 5 epochs with a total batch size of 512 sequences, a learning rate of 1e-4 which is warmed up for the first 10\% of training steps and linearly decayed.
Our main domain classifiers are shown the same web page features as the prompted Llama models, i.e., the text contents and web page URL, using the template of {\tt ``$\{$url$\}\backslash{}$n$\backslash{}$n$\{$text$\}$''}.
However, for the potential use case of annotating other documents without URL information, we also produce a version of the domain classifiers trained with only the website test as input.

\paragraph{Classifier accuracy} We consider how well the domain classifiers imitate the annotations by the Llama-3.1-405B-Instruct models on the validation set of 10K web pages and focus on the subset where the large language model chooses a category with at least 75\% confidence---which is the case for 86\% of topic annotations and 79\% of format annotations.
On this subset, we report both the overall accuracy and the worst-group accuracy, i.e., the worst accuracy when predicting a certain label.
The results are shown in \autoref{tab:domain_classifier_accuracies}. We make the following observations: (1) 2-stage training is particularly effective for improving the worst-group accuracy of the classifiers, and (2) it slightly helps to provide the web page URL to the domain classifiers. Despite these efforts, we note that there remains a gap between the 150M parameter domain classifiers and the 405B parameter Llama-3.1-Instruct model. However, we note that the ceiling for the domain classifier is not 100\%. Llama-3.1-Instruct-405B is sensitive to the order in which categories and few-shot examples are presented, and a different random seed produces an agreement of only 98\% and 97\% on this validation subset for topic and formats respectively, suggesting that the domain classifier introduces an additional 4.4\%-5.1\% error into the annotation process.

\paragraph{Domain analysis}
\autoref{fig:pmi_topic_type_full} shows the full matrix of normalized PMI scores between topic and format annotations, computed across the 200B token annotated pre-training corpus. \autoref{fig:pmi_clusters} visualizes the normalized PMI values between $k$-means clusters and either the topic or the format domains.
In \autoref{fig:url_stats}, we visualize the frequency of URL domains to highlight the need for meaningful coarse-grained domains.

\input{tables/domain_classifer_accuracies}


\input{figures/url_stats}
\input{figures/pmi_topic_type_full}
\input{figures/pmi_clusters}


\section{RegMix Implementation} \label{app:regmix}

\begin{minipage}[t]{0.63\linewidth}
\paragraph{Sampling training mixtures}
For each domain definition, we generate 512 random domain mixtures for training small models.
The mixtures are sampled in a similar fashion to the official RegMix implementation \citep{liu2024regmix}.
We compute the domain proportions in the pre-training corpus and soften the distribution by applying a temperature of $\tau = 2$ to obtain the prior distribution $\boldsymbol{p}$.
We then sample training mixtures $\boldsymbol{\pi}$ hierarchically via $\log \alpha \sim \text{Uniform}(\log 0.1, \log 10)$ and $\boldsymbol{\pi} \sim \text{Dirichlet}(\alpha \boldsymbol{p})$.

\vspace{\baselineskip}

\paragraph{Small model training}
We sample 1B tokens according to each training mixture and train small 50M parameter models on this data.
The data is tokenized with the GPT-NeoX tokenizer \citep{black2022gpt}, as used by the DCLM model runs.
The model architecture is based on the Llama architecture \citet{touvron2023llama}, featuring SwiGLU activations \citep{shazeer2020glu} and RoPE positional embeddings \citep{su2024roformer}.
The 512 model runs require approximately 360 NVIDIA H100 hours. The hyperparameters are given in \autoref{tab:hyperparameters_small}.

\end{minipage}\hfill
\begin{minipage}[t]{0.35\linewidth}
\centering
\vskip 0.1in
\captionof{table}{Hyperparameters for small model training}
\label{tab:hyperparameters_small}
\icmlskip{0.1in}
\small
\begin{tabular}{lc}
\toprule
    Parameter & Value \\
    \midrule
    Hidden size & 512 \\
    Intermediate size & 1536 \\
    Activation function & SwiGLU \\
    Attention heads & 8 \\
    Num. blocks & 8 \\
    RoPE base frequency & 10000 \\
    \midrule
    Peak learning rate & 3e-3 \\
    Cosine cooldown & 3e-4 \\
    Warmup ratio & 10\% \\
    Adam $\beta$'s & (0.9, 0.95) \\
    Batch size & 128 \\
\bottomrule
\end{tabular}
\end{minipage}


\paragraph{Simulation} We follow \citet{liu2024regmix} and train a boosted tree regression model to predict downstream loss from the training mixture weights.
However, our implementation diverges in the so-called ``simulation phase'' which seeks to predict the best performing mixture. \citet{liu2024regmix} generate $N=1$M random mixtures according to $\boldsymbol{\pi} \sim \text{Dirichlet}(\boldsymbol{p})$, where $\boldsymbol{p}$ is the prior domain distribution in the corpus (without applying temperature here).
The regression model is used to predict a loss for each mixture, and \citet{liu2024regmix} average $K=100$ mixtures with the lowest loss to produce a prediction for the best mixture.
In our exploration, we encountered the issue that the predicted mixture would be sensitive to the random seed and the hyperparameters $N$ and $K$, and it was also not clear how $K$ should vary when increasing $N$.
\citet{liu2024regmix} do not provide a clear motivation for averaging, but it likely reflects a prior towards smoother distributions.
We found that it was more convenient to express this by adding a soft KL constraint to the objective, encouraging the prediction to remain closer to the corpus distribution, $\gamma\text{KL}(\boldsymbol{p}\;||\;\boldsymbol{\pi})$, where the coefficient $\gamma$ is independent of $N$.
We also found that increasing $N$ led to diminishing returns and developed a multi-step adaptive search method, which reliably identifies better mixtures under the regression mode.
In each iteration, the algorithm updates the prior for generating mixtures with the best current candidate mixture. \autoref{alg:adaptive_search} lays out the algorithm. We use the hyperparameters $N=0.5M$ mixtures, $T=15$ steps, $\gamma = 0.002$, $\eta =0.2$ and run the simulation with two random seeds, choosing the better mixture according to the objective.
We make a final modification to RegMix when targeting two downstream tasks, i.e., HellaSwag and MMLU. In this case, we fit two separate regression models for the two tasks, and combine them by averaging their outputs.


\begin{algorithm}[!ht]
\small
\caption{Adaptive search for RegMix}
\label{alg:adaptive_search}
\begin{algorithmic}[1]
    \REQUIRE corpus prior $\boldsymbol{p}$, num. mixtures $N$, KL coefficient $\gamma$, steps $T$, smoothing $\eta$, regression model $f$
    \ENSURE predicted mixture $\boldsymbol{\tilde q}$

    \STATE $\boldsymbol{\tilde q} \gets \boldsymbol{p}$
        \hfill {$\triangleright$ Best mixture overall}
    \STATE $\boldsymbol{w} \gets \boldsymbol{p}$
        \hfill {$\triangleright$  Soft average of best mixtures in each iteration}

    \FOR{t in $1..T$}
    \STATE $\log \alpha^{(i)} \sim \text{Uniform}(\log 1, \log 1000), \quad i \in 1..N$
    \STATE $\boldsymbol{\pi}^{(i)} \sim \text{Dirichlet}(\alpha^{(i)} \boldsymbol{w}), \; \text{s.t.} \; {\boldsymbol{\pi}^{(i)}} \le 6.5{\boldsymbol{p}^{(i)}}$
        \hfill {$\triangleright$   No repetitions when selecting 30B out of 200B tokens.}
    \STATE $\boldsymbol{\tilde w} \gets \argmin_{\pi^{(i)}} f\left(\boldsymbol{\pi}^{(i)}\right) + \gamma KL\left( \boldsymbol{p}\;||\;\boldsymbol{\pi^{(i)}} \right)$

    \vskip 0.1in
    \STATE $\pi^{(j)} \gets \beta_j \boldsymbol{w} + (1-\beta_j)\boldsymbol{\tilde w}, \quad \beta_j \in \text{Linspace}(0, 1, 500)$
        \hfill {$\triangleright$ Line search between $\boldsymbol{\tilde w}$ and $\boldsymbol{w}$}
    \STATE $\boldsymbol{\tilde w} \gets \argmin_{\pi^{(j)}} f\left(\boldsymbol{\pi}^{(j)}\right) + \gamma KL\left( \boldsymbol{p}\;||\;\boldsymbol{\pi^{(j)}} \right)$

    \vskip 0.1in
    \STATE $\boldsymbol{w} \gets \eta \boldsymbol{\tilde w} + (1-\eta)\boldsymbol{w}$
        \hfill {$\triangleright$ Update search prior with the best current mixture}
    \STATE $\boldsymbol{\tilde q} \gets \argmin_{\boldsymbol{\pi} \in \{\boldsymbol{\tilde w}, \boldsymbol{\tilde q}\}} f\left(\boldsymbol{\pi} \right) + \gamma KL\left( \boldsymbol{p}\;||\;\boldsymbol{\pi} \right)$
        \hfill {$\triangleright$ Keep track of best mixture so far}

    \ENDFOR{}
\end{algorithmic}
\end{algorithm}



\paragraph{Analysis} For our mixture predictions, we use all 512 mixtures to train the regression model. In an ablation, we reserve 50 mixtures for evaluations and compute the Spearman correlation between the RegMix predictions and small model evaluations.
\autoref{tab:regmix_correlations} shows the results.
The correlation coefficient hovers around 0.90, despite the small size of the models and the out-of-distribution setting of few-shot downstream evaluation.
We also explored predicting the held-out distributions using Data Mixing Laws \citep{ye2024datamixinglaws}. However, this achieves worse Spearman correlations and seems overall less stable.
We also note that predicting the average loss across MMLU and HellaSwag is slightly more accurate predictions when fitting two separate regression models.

\input{tables/regmix_correlations}

\input{tables/mixtures_weights}

\FloatBarrier

\section{Experimental Details} \label{app:data_preprocessing}

\paragraph{Data pre-processing}
We use the \texttt{1b-1x} data pool from DataComps-LM \citep{li2024datacomplm} to facilitate comparisons with future work. The raw pool consists of 1.64T tokens, extracted from CommonCrawl with \texttt{resiliparse}. We follow the best practice established by \citep{li2024datacomplm} and run heuristic filtering to eliminiate noisy web artifacts, specifically, the set of filters from the RefinedWeb dataset \citep{penedo2023refinedweb}. However, to the best of our knowledge, our reproduction differs slightly from \citet{li2024datacomplm}, since we do not use the ``high-quality URL filter'', which was originally meant to exclude documents from high-quality domains such as Wikipedia and Github, such that they can be added to the data mix manually. In the next step, we perform deduplication using Bloom filter \citep{soldaini-etal-2024-dolma}, while \citet{li2024datacomplm} use the MinHash algorithm in their \texttt{1b-1x} baseline \citep{broder1997resemblance}. The resulting corpus contains 200B tokens and constitutes the ``token universe'' for our data selection experiments and analyses. We annotate this corpus with quality scores from the FineWeb-edu classifier \citep{penedo2024finewebdatasetsdecantingweb} and the DCLM \texttt{fasttext OH-2.5 + ELI5} model \citep{joulin2017bag, li2024datacomplm}, as well as with the top-1 prediction from the topic and format classifiers, and $k$-means cluster assignments.

\paragraph{Data selection}
From this 200B token base corpus, we set apart approximately 1B token as a validation set and use the rest for selecting training data.
For each training run, we include enough documents to amount to 30B tokens.
This is slightly more than the 29B tokens required by DCLM for a \texttt{1b-1x} training run,
but it ensures that there are enough tokens during model training, since some tokens are dropped in the subsequent tokenization and packing stage.
For quality selection, we select the highest scoring documents until the token budget is reached.
We speed up the tokenization process by allowing ``imbalanced'' chunks, decreasing the chunk size to 2048 sequences, and scaling across many workers. The DCLM default choice of balancing chunks before writing was prohibitively slow on the slurm cluster we used.

\paragraph{Model training}
We use the {\tt 1b-1x} reference setting from \citet{li2024datacomplm}. The models have 1,439,795,200 parameters and are trained for 28,795,904,000 tokens with a batch size of 256 and a sequence length of 2048 tokens. We speed up training by adding {\tt torch.compile}, making a single training run take 183 NVIDIA H100 hours.

\paragraph{Evaluation setting}
We use the OLMES evaluation framework \citep{gu2024olmes} and evaluate on 9 tasks with a 5-shot in-context learning prompt: MMLU \citep{hendrycks2021measuring}, HellaSwag (HSwag) \citep{zellers2019hellaswag}, PIQA \citep{bisk2020piqa}, WinoGrande (WinoG) \citep{sakaguchi2021winogrande}, CommonSenseQA (CSQA) \citep{talmor2019commonsenseqa}, Social IQa (SIQA) \citep{sap2019social}, ARC-easy/challenge (ARC-e/ARC-c) \citep{clark2018think}, and OpenBookQA (OBQA) \citep{mihaylov2018suit}.
The OLMES task suite also includes BoolQ \citep{clark2019boolq}. However, we found that it produced unreliable results, e.g., the random sampling baseline would achieve 63.8\%, and DCLM-fasttext selection would 54.4\%, which is 9.4 percentage points lower and would have a large impact on the average performance.

We also used the DCLM evaluation framework to measure the {\tt Core} score, a normalized task average across 22 tasks \citep{li2024datacomplm}, which we report in \autoref{tab:results_regmix_detailed}.
However, we find that OLMES routinely measures higher accuracies in common tasks (HellaSwag, PIQA, and WinoGrande), which is useful for discriminating between models.
We also observed that some {\tt Core} tasks from BigBench and AGI eval are close to random performance at the {\tt 1b-1x} scale. Furthermore, given the symbolic nature of some tasks, e.g., dyck sequence completion, MMLU and HellaSwag are likely not good proxies for finding the best domain mixture. Note that we were unable to reproduce the exact {\it Baseline} and {\it DCLM-fasttext} performance by \citet{li2024datacomplm}, likely due to small differences in the data pre-processing stage, as discussed at the start of this section.

\input{tables/results_regmix_detailed}
