
\section{Quality Filters as Implicit Domain Mixers}
\label{sec:quality_filters}

\input{figures/mixtures_implicit}

In \autoref{sec:experiments}, we combine domain mixing and quality filtering by using the mixture to specify how many tokens to select per subset.
Without an explicit domain mixture, a quality filter will naturally upsample certain domains, 
which is equivalent to applying an \textit{implicit domain mixture} and subsequently selecting the top documents within each domain.
This process offers insights on two quality classifiers considered in this work, and presents a richer way to describe differences between them.

We reconstruct the implicit domain mixture by computing the domain statistics of the quality filtered training datasets.
\autoref{fig:mixtures_implicit} visualizes these distributions for FineWeb-Edu and DCLM-fasttext.
We observe that FineWeb-Edu deviates more strongly from the corpus than DCLM-fasttext in terms of topics, while DCLM-fasttext amplifies a larger number of categories in terms of formats.
Their mixtures also share notable similarities with the RegMix predictions in \autoref{fig:mixtures_regmix}---all amplifying {\atopic Politics}, {\atopic Health}, {\atopic Science \& Tech.}, and {\atopic History}, to varying degrees, as well as {\aformat Knowledge Articles}, {\aformat Tutorials}, {\aformat Academic Writing}, and {\aformat Q\&A Forums}. 
However, the exact proportions differ substantially and their behaviors also diverge. For example, DCLM-fasttext retains by far the most documents from {\atopic Entertainment} and {\atopic Games} topics, as well as from {\aformat Comment Sections} and {\aformat Creative Writing} formats.

\ificml\else\begin{minipage}[t]{0.45\linewidth}\fi
\paragraph{Approximating quality filters by domains}
We adopt only the implicit domain mixtures of quality classifiers for pre-training data curation,
replacing the ``local'' selection within each domain with random sampling.
The results of training 1B parameter models are shown in \autoref{tab:results_quality}.
Both topic and format domains help to approximate the performance of quality classifiers. 
Of the two quality classifiers under study, we find FineWeb-Edu to be better approximated by domain effects.
In this case, implicit Topic $\times$ Format mixture recovers its performance gains by 73\% on MMLU and 84\% on average.
However, a substantial gap remains for approximating DCLM-fasttext, suggesting that this classifier relies more on selecting the ``right'' documents within each domain.
\ificml
    \input{tables/results_quality}
\else
\end{minipage}\hfill
\begin{minipage}[t]{0.5\linewidth}
    \input{tables/results_quality}
\end{minipage}
\fi

Finally, we report the held-out perplexity of the models and observe that the values for domain mixing and substantially lower than using the quality filtering and close to the baseline corpus.
This suggests that document-level quality filtering is a far stronger intervention on the pre-training distribution than rebalancing domains or topics.

\paragraph{Nature of data quality}
It has become common 
to claim that datasets which produce better benchmark scores have ``higher quality'' \citep{li2023textbooks, wettig2024qurating, sachdeva2024train, penedo2024finewebdatasetsdecantingweb, li2024datacomplm}.
In domain mixing, the ``quality'' of a domain is reflected by how much it should be upsampled, but our findings in \autoref{sec:regmix} suggest that MMLU and HellaSwag exhibit very different domain preferences,
and optimizing for both tasks requires making trade-offs.
This highlights how ``data quality'' is sensitive to the choice of downstream tasks, %
and we observe that the notion of ``quality'' by FineWeb-Edu is particularly biased to specific domains that benefit downstream tasks.
However, there are many aspects of web content that are not captured by \mbox{WebOrganizer}, e.g., the prevalence of misspellings or factual errors, and these might be better modeled by scoring individual documents.
Such effects may explain why DCLM-fasttext is not well approximated by domain effects, 
and why both quality filters substantially outperform random sampling when imposing the same Topic $\times$ Format mixture (\autoref{tab:results_quality}).



