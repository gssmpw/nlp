\section{Evaluating Pre-Training Data Curation with WebOrganizer}
\label{sec:experiments}


\input{tables/results_regmix}

We demonstrate the practical value of constructing domains with WebOrganizer by training models with the domain mixtures produced by RegMix in \autoref{sec:regmix}. We show how to combine data mixing for topics and formats, and how domains can be used together with quality filters.

\subsection{Experimental Setting}
All our experiments are implemented in the DataComps-LM (DCLM) framework \citep{li2024datacomplm}, using the \texttt{1b-1x} competition pool.
We follow best practices and use heuristic filters, followed by deduplication to reduce the 1.6T raw token pool to a base corpus of 200B tokens. From this dataset, we select 29B tokens by sampling according to a domain mixture and train a 1B parameter model.
Full details of our experimental setup can be found in \autoref{app:data_preprocessing}.



\paragraph{Evaluation suite} We use OLMES \citep{gu2024olmes} to evaluate models and their domain mixtures. We use a 5-shot setting on a suite of 9 tasks: MMLU \citep{hendrycks2021measuring}, HellaSwag (HSwag) \citep{zellers2019hellaswag}, PIQA \citep{bisk2020piqa}, WinoGrande (WinoG) \citep{sakaguchi2021winogrande}, CommonSenseQA (CSQA) \citep{talmor2019commonsenseqa}, Social IQa (SIQA) \citep{sap2019social}, ARC-easy/challenge (ARC-e/ARC-c) \citep{clark2018think}, and OpenBookQA (OBQA) \citep{mihaylov2018suit}.
OLMES measures task performance in both the multiple-choice format and a cloze formulation, as well as curating few-shot examples, 
producing a more reliable evaluation for smaller models.


\subsection{Topic \texorpdfstring{$\times$}{x} Format selection}

We construct a new taxonomy, consisting of all 576 pairs of topic and format domains.
Finding a training mixture of this cardinality would be expensive and sensitive to noise.
Here, we make the assumption that we can select topics and formats independently, and select data according to 
$$\tilde P_{T \times F}(\text{topic},\;\text{format}) = \tilde P_{T}(\text{topic})\tilde P_{F}(\text{format}),$$ 
where {$\tilde P_{T}(\text{topic})$} and {$\tilde P_{F}(\text{format})$}, are the predictions from separate RegMix pipelines. 
In practice, there are cases where $\tilde P_{T}(\text{topic})\tilde P_{F}(\text{format})$ can exceed the amount of data available for that pair.
In such cases, we take all available documents and up-sample everything else to compensate.

\subsection{Combining quality filters and domain mixing}
Quality filters assign scores to individual documents \citep{xie2023data, wettig2024qurating, sachdeva2024train}, and select data at a more granular level than is possible with domain rebalancing. 
Therefore, they are a powerful baseline. We compare to two state-of-the-art quality filters: FineWeb-Edu \citep{penedo2024finewebdatasetsdecantingweb}, a 110M parameter model distilled from prompting Llama-3-70B to rate the educational value of web pages, and DCLM-fasttext \citep{li2024datacomplm}, a bigram model trained to identify text resembling a reference corpus consisting mostly of GPT-4 conversations. For both methods, we select all the highest-ranking documents until the token budget is reached.

We explore a simple strategy for composing quality filters and domain mixtures: We use the domain mixture to determine the desired number of tokens from each domain subset. Then we perform the data selection with the quality filter separately for each subset---effectively varying the quality threshold per domain, depending on the mixture.

\subsection{Results}
\autoref{tab:results_regmix} shows the results of our main experiments with mixtures optimized for both MMLU and HellaSwag. In the first setting, we consider how domain mixing improves upon the inherent data mixture of the baseline corpus. 
Then, we show how domain mixing also improves the performance of quality filtering. 
Results for individual task mixtures are reported in the appendix (\autoref{tab:results_regmix_detailed}).


\paragraph{Domain mixing is broadly effective}
We observe that reweighting the domain proportions of the pre-training corpus improves downstream performance across all three of the topic, format, and $k$-means cluster domains (rows 1-4 in \autoref{tab:results_regmix}).
Rebalancing formats achieves the best trade-off between MMLU and HellaSwag, and improves performance on 6 out of the 7 transfer tasks.
Despite the target task accuracy, the topic mixture produces the best overall accuracy with 2.1\% absolute gain over the random sampling baseline, with excellent transfer to ARC-easy/challenge and OpenBookQA. 
We note that reweighting $k$-means clusters performs well with an overall 1.6\% point improvement, and  
\autoref{tab:results_regmix_detailed} shows that they are the most well-suited for targeting only HellaSwag.

\paragraph{Topic and format mixtures can be combined}
Our domains offer the advantage that topic and format mixtures can be combined. Our experiment (row 5) demonstrates that this is effective, improving performance in 8 out of 9 tasks and achieving a 3.0\% absolute gain overall, which narrowly beats the FineWeb-Edu quality classifier.
It also consistently improves performance when only aiming for one of the two downstream tasks in \autoref{tab:results_regmix_detailed}, notably attaining an MMLU score of 33.2\%.
This illustrates that both topic and format are important axes for data curation.

\paragraph{Domain mixtures improve quality filters}
Finally, we show that our domain mixtures can also boost the overall performance of two state-of-the-art quality filters, improving the average performance of FineWeb-Edu and DCLM-fasttext by 2.0\% and 1.0\%, points respectively (rows 6-9).
We highlight that our domain mixing addresses the weaknesses of the quality classifiers---for instance on HellaSwag, FineWeb-Edu underperforms the random sampling baseline by 1.5\% points, which our tailored mixture converts to a 5\% absolute gain.
This reflects the fact our domain mixtures can be subtly calibrated to meet the demands of the downstream tasks, whereas it would be hard to encode exactly the right preference for certain sub-distributions by changing the prompt for the FineWeb-Edu classifier or the reference corpus for the DCLM-fasttext classifier.
With the same domain reweighting,
FineWeb-Edu and DCLM-fasttext achieve similar performance across tasks.
