\section{Conclusions}

We introduce WebOrganizer---a tool for organizing unstructured web corpora into topic and format domains. By annotating a 200B token pre-training corpus, 
we demonstrate how WebOrganizer documents the internal contents of the pre-training data, and that we can re-balance these subsets to increase the performance of downstream tasks.
Importantly, we show that topic and format selection can be combined, and that domain mixing can be integrated with quality filtering, which combines the benefits of document-level selection with well calibrated domain ratios.

Increasing the transparency of data curation is an interesting avenue for future work. 
Better documentation of pre-training the data can inform model developers about potential strengths and weaknesses of the model and also improve the understanding of other stakeholders such as policy makers or end users.
In this work, we make initial progress in this direction by introducing WebOrganizer and two high-level domain taxonomies. 
This enables analyzing the internal composition of web-crawled pre-training corpora (as in \autoref{fig:treemaps}) and examining how it changes after  quality filtering (in \autoref{fig:mixtures_implicit}). 
There is wide scope for refining these data representations in future work, including hierarchical taxonomies, e.g., breaking down {\atopic Science \& Technology} into the various scientific disciplines; or multi-label classification which could better account for ambiguous cases where a document covers multiple topics or does not fit cleanly to any one label.

\ificml
\section*{Impact Statement}
\else
\section*{Impact Statement}
\fi

Our work advances data curation for language models, and thus carries the broader societal implications associated with improving the capabilities of these models. By taxonomizing web data, we develop a tool that aims to enhance the transparency of pre-training corpora---potentially helping both researchers and the broader public develop a better grasp of the available pre-training data for language models.
At the same time, we acknowledge the risks inherent in this process: Reducing the rich diversity of online content to a limited set of discrete domains can obscure important phenomena and may lead to errors, biases, or misrepresentations.
We highlight that there are many valid ways to define web taxonomies, and our efforts do not represent a definite ``ground truth''.
Similarly, the predictions of how to rebalance the domains are sensitive to noise, as they are based on relatively few small model runs. Furthermore, is uncertain how well they transfer across model scales.
Despite these challenges, in the absence of other meaningful metadata, 
we believe that our domain annotations contribute to a more informed understanding of web-scale pre-training data.

\section*{Acknowledgments}
We thank Pang Wei Koh, Tyler Murray, and Mayee Chen for helpful discussion. We also thank Mengzhou Xia, Dan Friedman, Tianyu Gao, Alex Fang, Maria Antoniak, Ben Lee, and Catherine Chen for feedback on the draft. 
This research is partially funded by the National Science Foundation (IIS-2211779).




