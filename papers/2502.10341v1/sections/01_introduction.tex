
\vspace{1em}
\section{Introduction} \label{sec:introduction}

\ificml\setcounter{footnote}{1}\fi
Curating good training data is crucial for enhancing the capabilities of language models.
Early pre-training datasets, like the Pile \citep{pile} or RedPajama \citep{together2023redpajama}, were created by curating data from multiple sources---such as Wikipedia, Reddit, or BookCorpus \citep{zhu2015aligning}---giving rise to the research problem of how to balance these domains\footnote{Throughout the paper, we use the term {\it domain} to denote dataset partitions, rather than conventional web domains, which we will refer to as {\it URL domains}.} \citep{xie2023doremi}.
However, as the demand for data has grown to trillions of tokens, 
the majority of data is now obtained from crawling the web, and the importance of curating domains has diminished.

Recent efforts in data curation, such as FineWeb \citep{penedo2024finewebdatasetsdecantingweb} and DCLM \citep{li2024datacomplm}, produce multi-trillion-token datasets with CommonCrawl as the singular source, 
offering no summary of their contents.
In the absence of domains, the focus has shifted to cleaning corpora using heuristic rules \citep{raffel2020exploring,rae2021scaling,penedo2023refinedweb} and quality filters \citep{wettig2024qurating, sachdeva2024train,penedo2024finewebdatasetsdecantingweb,li2024datacomplm}.

\input{figures/treemaps}

In this paper, we propose WebOrganizer, a framework to construct meaningful domains for monolithic web corpora.
Our approach consists of designing taxonomies for unstructured web content, and 
scaling automatic labeling of documents according to these taxonomies by distilling a large language model classifier (Llama-3.1-405B-Instruct) to small and efficient models (140M parameters). %
\mbox{WebOrganizer} establishes a rich, two-dimensional structure for pre-training data by introducing two complementary domain taxonomies---{\topics topic} and {\formats format}---which classify web pages into 24 categories based on subject matter and style, respectively. This paper, for instance, would fall under the {\atopic Science \& Technology} topic and the {\aformat Academic Writing} format.
\autoref{fig:treemaps} provides an overview of these domains and demonstrates how WebOrganizer shines a light on the composition of different types of internet content in a cleaned pre-training corpus derived from CommonCrawl.
We also compare our domains to $k$-means clustering of document embeddings, 
and find that the clusters mostly align with topics and do not reveal different formats.


How effective are these domains for data curation?
Partitioning a corpus into domains provides rich affordances for data curation, as we can flexibly up or down-sample domains.
More importantly, it enables principled methods that can explore possible data mixtures in systematic ways and optimize the domain proportions to meet the objectives of data curation.
We adapt the RegMix framework \citep{liu2024regmix} to predict which domains should be up-sampled to improve two downstream tasks, MMLU and HellaSwag, which are commonly used for measuring the quality of pre-training data. 
For example, we find that up-sampling documents from the {\atopic Science \& Technology} topic favors MMLU while the {\aformat Tutorial} format suits HellaSwag.


Our experiments show that constructing domains and optimizing their mixture towards specific tasks is effective.
The reweighted topics, formats, and $k$-means clusters all improve downstream performance across a range of downstream tasks.
Furthermore, we show that, since the topic and format domains capture different aspects of web pages, we can combine their data mixtures, which boosts performance considerably---matching the overall results of selecting documents with the FineWeb-Edu quality filter \citep{penedo2024finewebdatasetsdecantingweb}.
Even more compelling, we find that our optimized domain mixtures work well with quality filters and further enhance their performance.
Notably, the average accuracy of FineWeb-Edu increases from 54.2\% to 56.2\% when adding domain mixing---almost doubling the gain of FineWeb-Edu over a 51.6\% baseline accuracy.

Finally, we observe that data selection with quality filters implicitly changes the domain proportions of the dataset, and we quantify how much their performance gain can be accomplished from this domain mixing alone.
We observe that the FineWeb-Edu quality filter has similar domain preferences to the mixtures optimized for MMLU
and the implicit topic and format mixture retains up to 84\% of the performance gains from quality filtering.
Meanwhile, while the DCLM-fasttext quality filter \citep{li2024datacomplm} amplifies certain formats, we find that its implicit data mixtures perform considerably worse, suggesting that it utilizes aspects of quality beyond broad domain effects.



    

We open-source WebOrganizer as a tool for understanding, documenting and curating pre-training data.
To encourage future work, we include the code for constructing domains and training domain classifiers, as well as the annotated pre-training corpus.
