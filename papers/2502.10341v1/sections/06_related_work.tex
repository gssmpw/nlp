\section{Related Work}

\paragraph{Data selection}
Many methods have been developed selecting pre-training data for training large language models. It has become common practice to remove noisy web sites using heuristic filtering rules \citep{raffel2020exploring, rae2021scaling, penedo2023refinedweb}, focusing on surface statistics such as mean word length or word repetitions. 
This is typically followed by deduplication \citep{lee2022deduplicating, jiang2023fuzzy, abbas2023semdedup, tirumala2023d4, soldaini-etal-2024-dolma}.
Additional data selection techniques include measuring n-gram similarity to high-quality reference corpora \citep{brown2020language, xie2023data, li2024datacomplm, brandfonbrener2024colorfilter}, using perplexity of existing language models \citep{wenzek2020ccnet, muennighoff2023scaling, marion2023more, ankner2025perplexed}, and prompting large language models to rate documents based on qualities such as factuality or educational value \citep{gunasekar2023textbooks, wettig2024qurating, sachdeva2024train, penedo2024finewebdatasetsdecantingweb}.

\vspace{-0.05in}
\paragraph{Data curation with domains}
Several language models add specially curated domains to their pre-training data \citep{touvron2023llama, soldaini-etal-2024-dolma, olmo20242}, but CommonCrawl data forms typically the majority of data and has also been shown to outperform domain curation \citep{penedo2023refinedweb, li2024datacomplm}.
Several works have investigated the specific impact of varying the proportion of code in the pre-training data \citep{ma2024at, petty2024does, viraat2025tocode, chen2025scaling}.
\citet{dubey2024llama} briefly mention using knowledge classifiers to downsample ``over-represented'' data for training Llama-3. 
Instead of using domains for rebalancing data, \citet{gao2025metadata} observe performance improvements when conditioning on domain metadata during pre-training.

\vspace{-0.05in}
\paragraph{Data mixture optimization}
Many techniques have been developed 
for tuning domains proportions
while training language models.
These seek to minimize the validation losses across the domains
\citep{xie2023doremi, albalak2023efficient, jiang2024adaptive, chen2024aioli}, 
although some methods apply to out-of-domain settings
\citep{chen2023skillit, fan2023doge}.
Most methods adjust mixtures dynamically during training, and some make predictions from many static mixtures \citep{liu2024regmix, ye2024datamixinglaws, kang2024autoscale}.
\citet{trush2025improving} partition a web corpus into almost 10k domains based on frequent URL domains and rank them based on correlations between domain perplexities and benchmark scores from 90 existing open language models. 
\citet{hayase2024data} extracts the data mixture of a private pre-training corpus from tokenization rules.
Instead of optimizing domain mixtures, researchers have also developed approximations for the impact of training examples on the validation set~\citep{engstrom2024dsdm, yu2024mates, wang2024greats}.

\vspace{-0.05in}
\paragraph{Analysis of pre-training data} 
WebOrganizer can serve as a tool for analyzing the contents of web corpora and the effects of quality filtering.
In related work, \citet{longpre2024pretrainers} study data curation in terms of toxicity, source composition, and dataset age, and
\citet{longpre2024consent} analyze licensing issues in web corpora.
\citet{elazar2024whats} provide a scalable tool for searching web-scale corpora and study the prevalence of toxicity, duplicates, and personally identifiable information.
\citet{lucy2024aboutme} use self-descriptions of website creators to measure how quality filters amplify and suppress speech across topics, regions, and occupations.
\citet{ruis2024procedural} employ influence functions to find pre-training documents important for learning factual knowledge and mathematical reasoning respectively.
In a separate line of work, large language models have been used for clustering large corpora \citep{wang2023goal, zhang2023clusterllm, pham2024topicgpt} and describing clusters post-hoc \citep{zhong22describing, tamkin2024clio}.
