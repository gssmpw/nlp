\section{Introduction}
Click-through rate (CTR) prediction is a pivotal task in online advertising and recommender systems, aimed at estimating the likelihood of a user clicking on a given item~\cite{other_18_clustering,other_16_user}. Traditional approaches such as Logistic Regression (LR) \cite{LR} % \hao{ref} 
and Factorization Machines (FM) \cite{FM}  % \hao{ref} 
model feature interactions to capture linear and pairwise relationships, effectively laying the groundwork for CTR prediction. However, these methods often fall short of capturing complex high-order interactions. With the fast development of deep learning~\cite{other_21_learning,other_22_context,other_23_graph,other_24_hierarchical,other_25_cglb,other_26_exploring,other_27_configure,other_28_lever,other_29_live}, some deep learning-based models have emerged to address these limitations, significantly improving the ability to capture intricate feature interactions~\cite{important_1_survey,important_2_perlaw,important_5_multi,other_6_breaking,other_7_bridging,other_11_exploring,other_13_efficient,other_14_mf}. Notable examples include DeepFM \cite{DeepFM}, which combines FM \cite{FM} with deep neural networks, and Product-based Neural Networks (PNN) \cite{PNN}, which explicitly model feature interactions through product layers. Further advancements like Adaptive Factorization Networks (AFN) \cite{AFN} and Deep \& Cross Networks (DCN) \cite{DeepCross} introduce adaptive mechanisms and innovative cross-layer structures. Recently, Gated Deep \& Cross Networks (GDCN) \cite{GDCN} have been proposed to offer greater flexibility and improved performance. Additionally, user modeling-based CTR models such as Deep Interest Network (DIN) \cite{DIN} and Deep Interest Evolution Network (DIEN) \cite{DIEN} further enhance CTR prediction by capturing user interests and their evolution over time.

    % \hao{should include DIN?}
    % \gw{user modeling based CTR models should also be mentioned.}

    
    % \gw{Most of the above methods follow a similar Embedding \& Neural Networks architecture. Embedding layer looks up embedding vectors for the input features from a randomly initialized embedding table, then these embedding vectors are concatenated and fed to neural networks to learn feature interactions.}\kf{Seems this paragraph is able to be simply put here, should I do that?}
    % \tj{issue?}
    %tj: 这里是在说issues而不是challenge? 应该是现在CTR存在feature很多的问题吧，可能也需要一个名词来描述这个现象？比如Intense Embedding Table Expansion？
    
    % Despite these advancements, a significant issue remains: the embedding tables used to represent features can become exceedingly large, often reaching hundreds of gigabytes in industrial settings \cite{HydridHash,QRTrick,BinaryCode}.  
    % These large tables consume substantial memory, often exceeding GPU capacity, leading to frequent data transfers between GPU and CPU during inference \cite{wang2022merlin,guo2021scalefreectr}.
    % These demands impact performance and efficiency, introducing latency that cannot be neglected in real-time systems.
    % To mitigate this issue, various memory-efficient embedding techniques have been proposed. These techniques typically fall into two categories: hash-based methods and quantization-based methods. Hash-based methods, such as DHE \cite{DHE}, HydridHash \cite{HydridHash}, DoubleHash \cite{DoubleHash}, and QRTrick \cite{QRTrick}, reduce memory usage by mapping features to fewer buckets using hash functions.

    % However, these methods suffer from collision issues, where overly aggressive hashing may impair the model's ability to differentiate inputs by confusing unrelated concepts, ultimately degrading performance. 
    % In contrast, quantization methods, particularly Product Quantization (PQ) \cite{PQ}, maintain better performance while reducing memory consumption. 
    % PQ achieves significant storage reduction by decomposing high-dimensional embeddings into multiple subspaces and quantizing them separately. 
    % Recently, several advancements based on PQ, such as DPQ~\cite{DPQ}, AutoDPQ~\cite{AutoDPQ}, and CCE\cite{CCE}, have been proposed. 


    Despite advancements in CTR prediction, embedding tables can become exceedingly large~\cite{others_1_scaling,other_10_entropy,others_2_td3}, often reaching hundreds of gigabytes in industrial settings \cite{QRTrick,BinaryCode,important_3_scaling}. 
    This substantial memory consumption frequently exceeds GPU capacity, causing data transfers between GPU and CPU during inference, which impacts performance and introduces latency in real-time systems \cite{wang2022merlin}. 
    To address this, memory-efficient techniques have been proposed, generally falling into hash-based and quantization-based methods. Hash-based methods, such as DHE \cite{DHE}, DoubleHash \cite{DoubleHash}, and QRTrick \cite{QRTrick}, reduce memory usage by mapping features to fewer buckets using hash functions. 
    However, they suffer from collision issues that degrade performance by confusing unrelated concepts.

    
    \begin{figure}[t]
        \centering
        \setlength{\abovecaptionskip}{5pt}   % 图注上方的距离
        \setlength{\belowcaptionskip}{0pt}   % 图注下方的距离
        \includegraphics[width=0.9\textwidth]{Sections/Figures/motivation.pdf}
        \caption{Distribution of different quantization methods}
    \label{fig:motivation}
    % \vspace{-15pt}
    \end{figure}

    % \tj{challenge?}
    %相对的，这里应该说上述方法尝试解决这个问题，然而还是有AB挑战
    % However, these quantization methods, while primarily focusing on improving compression efficiency or training strategies, often neglect the quality of the quantized representation distribution, resulting in suboptimal representations.
    % This challenge arises primarily from \textsl{\textbf{imbalances in code allocation}} and \textsl{\textbf{uneven distributions of code embeddings}}, which present two main challenges. 
    % Firstly, in click-through rate (CTR) recommendation scenarios, frequency-independent quantization methods can be interfered with by a large number of low-frequency tail features, overshadowing the high-frequency features that should be prioritized and weakening the model's ability to represent these high-frequency features, so it is necessary to consider the frequency of features.   
    % As shown in Figure \ref{fig:motivation}(a), the quantization centers are far from the high-frequency features, severely affecting the embedding quality of these high-frequency features.
    % However, as shown in Figure \ref{fig:motivation} (b), directly accounting for feature frequency may lead to low-frequency tail features being overwhelmed by a multitude of repeatedly occurring high-frequency features, adversely affecting their performance and subsequent interactions. 
    % Secondly, The diverse range of features in click-through rate (CTR) tasks requires that feature embeddings be evenly distributed to maintain their distinctiveness during interactions.  
    % However, traditional quantization methods frequently ignore this need \cite{letter}, leading to a situation where feature embeddings become overly concentrated and homogenized.  
    % This lack of diversity in the embeddings ultimately compromises the model's predictive performance in CTR scenarios.

    In contrast, quantization methods like Product Quantization (PQ) \cite{PQ} offer better performance with reduced memory consumption by decomposing high-dimensional embeddings into subspaces and quantizing them separately~\cite{other_17_guesr}. 
    Recent advancements include DPQ~\cite{DPQ}, AutoDPQ~\cite{AutoDPQ}, and CCE\cite{CCE}. 
    Yet, these methods often overlook the quality of quantized representation distribution, leading to suboptimal results due to \textsl{\textbf{imbalances in code allocation}} and \textsl{\textbf{uneven distributions of code embeddings}}. 
    Frequency-independent quantization can be disrupted by low-frequency features, overshadowing high-frequency ones and weakening representation, as shown in Fig. \ref{fig:motivation}(a). 
    Moreover, directly considering frequency might overwhelm low-frequency features, as depicted in Fig. \ref{fig:motivation}(b). 
    Additionally, CTR tasks require evenly distributed embeddings to maintain distinctiveness, but traditional methods often lead to concentrated embeddings, compromising predictive performance \cite{letter}.


    % To address the challenges of imbalances in code allocation and uneven distributions of code embeddings, we propose a Model-agnostic Embedding Compression (MEC) Framework.
    % Firstly, to tackle the challenge of \textbf{imbalances in code allocation} when incorporating feature frequency information, we introduce popularity-weighted regularization (PWR). 
    % PWR incorporates popularity-weighted regularization to preserve high-frequency feature information, preventing it from being overshadowed by the long-tail distribution of low-frequency features. 
    % It penalizes code allocation imbalances, ensuring that low-frequency features are not dominated by their high-frequency counterparts. 
    % By adaptively assigning unique cluster centers to high-frequency features while promoting code-sharing among low-frequency features, PWR increases the training volume for similar low-frequency features. 
    % This enhances the model's capacity to effectively represent a wide range of features.

    % Secondly, to mitigate the \textbf{uneven distributions of code embeddings}, we integrate a contrastive learning mechanism into our MEC framework.
    % Specifically, we utilize a strategy of random code replacement to generate semi-synthetic negative examples. 
    % By comparing these semi-synthetic negative examples with the normally quantized results, the contrastive loss encourages a more uniform distribution of the quantization codes. 
    % This approach ensures that the embeddings of different features are better separated, reducing the overlap and improving the distinctiveness of each quantized representation. 
    % Consequently, as shown in Figure \ref{fig:motivation} (c), this method not only mitigates code allocation bias but also enhances the overall performance of the recommendation system by providing more accurate and diverse feature representations.

    To address these challenges, we propose a Model-agnostic Embedding Compression (MEC) Framework. 
    Firstly, to tackle \textbf{imbalances in code allocation}, we introduce popularity-weighted regularization (PWR). PWR preserves high-frequency feature information, preventing it from being overshadowed by low-frequency features. 
    It penalizes imbalances, ensuring low-frequency features aren't dominated by high-frequency ones. 
    By adaptively assigning unique cluster centers to high-frequency features while promoting code-sharing among low-frequency features, PWR enhances representation capacity. 
    Secondly, to mitigate \textbf{uneven distributions of code embeddings}, we integrate a contrastive learning mechanism.  
    Using random code replacement to generate semi-synthetic negatives, the contrastive loss encourages uniform distribution of codes, ensuring distinctiveness. 
    As shown in Fig. \ref{fig:motivation} (c), this method enhances recommendation performance by providing more accurate and diverse representations.
    
    

    Our proposed framework operates in two stages, which enhances its generalization capabilities and allows for easy integration with any state-of-the-art quantization model or CTR model. 
    We instantiated the MEC algorithm on three representative CTR models and conducted extensive experiments on three datasets. The results demonstrate that MEC can adaptively perform high-quality quantization for data with varying popularity levels, achieving comparable or even superior results to existing advanced CTR models while saving more than 50x the memory.
    In summary, the contributions of this work are as follows:
    \begin{itemize}[leftmargin=*,align=left]
        \item We successfully address imbalances in code allocation by leveraging data popularity distribution through an adaptive two-stage quantization-based CTR prediction method, thereby enhancing recommendation performance while significantly reducing memory usage.
        \item To tackle the challenging issue of uneven distributions of code embeddings in PQ quantization for CTR tasks, we introduce PWR and contrastive learning methods to significantly enhance the quality of quantized codes, thereby improving recommendation performance.
        \item Extensive experiments on three real-world datasets validate that our MEC framework achieves over 50x memory optimization compared to models with conventional embedding layers, surpassing baselines in both recommendation performance and memory usage.
    \end{itemize}