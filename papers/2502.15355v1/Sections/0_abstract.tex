\begin{abstract}
% The prediction of click-through rates (CTR) is crucial for online advertising and recommender systems.
% With recent advancements in deep learning, existing methods have markedly enhanced the capacity to capture feature interactions and mine user interests.
% However, most of them neglect the optimization of the embedding layer.
% % While existing methods effectively address basic feature interactions and user interest mining, 
% % they often encounter difficulties when handling more complex interactions. In contrast, recent advancements in deep learning models have markedly enhanced the capacity to capture intricate feature interactions.
% % Click-through rate (CTR) prediction is a crucial task in online advertising and recommender systems, aimed at estimating the likelihood of a user clicking on a given item. Traditional methods like Logistic Regression (LR) and Factorization Machines (FM) capture linear and pairwise feature interactions but struggle with complex high-order interactions. Recent deep learning-based models, such as DeepFM, Product-based Neural Networks (PNN), Adaptive Factorization Networks (AFN), and Deep \& Cross Networks (DCN), have significantly improved the ability to capture intricate feature interactions. \hao{background is too long, be brief}
% The embedding tables used to represent categorical and sequential features often become extremely large, exceeding GPU memory capacity. As a result, they must be stored in CPU memory, leading to significant memory consumption and increased latency due to frequent data transfers between the GPU and CPU.
% % However, the embedding tables used to represent categorical features \gw{not only categorical features, but also sequential features} often become exceedingly large, exceeding GPU memory capacity and causing substantial latency \gw{time cost} due to frequent data transfers \gw{between GPU and memory}.
% To address these issues, we propose a \underline{M}odel-agnostic \underline{E}mbedding \underline{C}ompression (MEC) framework that quantizes pre-trained embeddings, thereby compressing embedding tables while maintaining recommendation performance. Our two-stage approach first introduces popularity-weighted regularization to balance the code distribution of high- and low-frequency features. Next, we incorporate a contrastive learning mechanism to ensure an even distribution of quantized codes, enhancing the distinctiveness of embeddings. Experiments on three datasets demonstrate that our method significantly reduces memory usage by over 50x while achieving comparable or superior recommendation performance to existing models. 
% % \kf{almost fully rewritten}
% % To tackle this issue, we propose Memory-Efficient CTR Prediction through Popularity Regularized Embedding Quantization (MemE-CTR)\hao{claim the solved problem and final object first}. This method leverages Product Quantization (PQ) to map features to quantization centers and employs popularity-weighted regularization to allocate unique cluster centers for high-frequency features, ensuring high-quality representations. Additionally, MemE-CTR introduces a contrastive loss to enhance the diversity of code center embeddings, mitigating code allocation bias. Our framework operates in two stages, enhancing its generalization capabilities and allowing easy integration with any state-of-the-art CTR model. \hao{illustration of the pipeline is not clear. Especially, the connections and sequence between the modules are unclear.} Extensive experiments on three datasets demonstrate that MemE-CTR can achieve comparable or superior results to existing advanced CTR models while saving more than 50x the memory.
% Our implementation code is available in an anonymous repository {\color{blue} \url{https://anonymous.4open.science/r/MEC/}}.

% 
% \vspace{-5pt}

Accurate click-through rate (CTR) prediction is vital for online advertising and recommendation systems. Recent deep learning advancements have improved the ability to capture feature interactions and understand user interests. However, optimizing the embedding layer often remains overlooked. Embedding tables, which represent categorical and sequential features, can become excessively large, surpassing GPU memory limits and necessitating storage in CPU memory. This results in high memory consumption and increased latency due to frequent GPU-CPU data transfers. 
To tackle these challenges, we introduce a \underline{M}odel-agnostic \underline{E}mbedding \underline{C}ompression (MEC) framework that compresses embedding tables by quantizing pre-trained embeddings, without sacrificing recommendation quality. Our approach consists of two stages: first, we apply popularity-weighted regularization to balance code distribution between high- and low-frequency features. Then, we integrate a contrastive learning mechanism to ensure a uniform distribution of quantized codes, enhancing the distinctiveness of embeddings. Experiments on three datasets reveal that our method reduces memory usage by over 50x while maintaining or improving recommendation performance compared to existing models. The implementation code is accessible in our project repository {\color{blue} \url{https://github.com/USTC-StarTeam/MEC}}.

\end{abstract}
