% \section{Click-Through Rate (CTR) Prediction}
\section{Preliminary}

The CTR prediction task is formulated as predicting the probability $\hat{y} = P(\text{click} | \mathbf{x})$ of a user clicking based on input features $\mathbf{x}$, which is crucial for online recommendation platforms. 
The input features can be divided into two types: categorical features and numerical features.
For categorical features, we start from an embedding layer that transforms the raw features of a training sample into embedding vectors. 
For a feature $A$ with vocabulary size $v_A$, we first encode the categorical information into a one-hot or multi-hot vector $\mathbf{x}_{\text{A}} \in \{0,1\}^{v_A}$, then an embedding lookup operation is conducted to transform the high-dimensional feature vector into a low-dimensional embedding:
\begin{equation}
\mathbf{e}_\text{A} = \mathbf{E}_\text{A} \mathbf{x}_\text{A},
\end{equation}
where $\mathbf{E}_\text{A} \in \mathbf{R}^{d_A \times v_A}$ is the embedding table of feature $A$.
Noticed that for sequential multi-hot categorical features here, we apply the average pooling here to transform it into a single vector.
For numerical features, each one is processed through a DNN layer to map the raw value into a vector. Let $\mathbf{x}_{a}$ be the $a$-th continuous feature, transformed as follows:
\begin{equation}
\mathbf{e}_{a} = \text{DNN}_a(\mathbf{x}_{a}),
\end{equation}
where $\text{DNN}_a$ is the layer specific to the $a$-th feature.
By concatenating all features, we represent an instance as:
\begin{equation}
\mathbf{e} = [\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{N}],
\end{equation}
with $N$ being the total number of features. The combined embedding vectors are then input into the CTR prediction model to capture complex interactions and predict click probabilities.