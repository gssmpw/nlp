\section{EXPERIMENTS}


    %  We conduct extensive experiments on three real-world datasets (\gw{including two public accessible datasets and one private industrial dataset}) to answer the following research questions:
    % \begin{itemize}
    %     % \item \textbf{(RQ1)} 我们的模型与sota的CTR模型与量化推荐模型相比效果如何？（对应我们的overall performance部分）
    %     \item \textbf{(RQ1)} How does MemE-CTR perform compared to state-of-the-art CTR models and quantized recommendation models?
    
    %     % \item \textbf{(RQ2)} 我们的模型的不同模块如何带来提升？（对应我们的ablation study部分）
    %     \item \textbf{(RQ2)} How do the different modules of MemE-CTR contribute to performance improvement?
    
    %     % \item \textbf{(RQ3)} 我们的模型对实验设置的变化是否敏感，是否会明显受到超参数或预训练模型改变的影响？（对应我的Hyper-Parameter Analysis和Pretraining method analysis）
    %     \item \textbf{(RQ3)} Is our model responsive to variations in the experimental setup, and is it significantly influenced by alterations in hyperparameters or the pre-trained model?
    %     % \gw{Maybe put the industrial datasets experiments on a single section?} \kf{OK.}
        
    %     % \item \textbf{(RQ4)} 我们的模型在节约内存和提升速度方面能够带来什么样的提升？（对应我们的Time\&Memory Efficiency部分）
    %     \item \textbf{(RQ4)} What improvements can MemE-CTR bring in terms of memory efficiency and speed enhancement?

    %     % \item \textbf{(RQ5)} 为什么MemE-CTR要使用PQ作为它的量化方法，而不使用RQ等方法？
    %     \item \textbf{(RQ5)} Why does MemE-CTR use PQ as its quantization method instead of other methods?

    %     % RQ6 我们的模型能否在大规模工业数据集上同样取得优秀的表现？
    %     \item \textbf{(RQ6)} Can our model achieve high performance on large-scale industrial datasets?
    % \end{itemize}

   
    
    \input{Sections/Tables/dataset}
    % \subsection{Experiment Setup}
    %     \subsubsection{Datasets.}
    %         To evaluate the performance of our proposed models, we utilized three different datasets, including two public accessible datasets and one private industrial dataset. 
    %         \begin{itemize}[leftmargin=*,align=left]
    %             \item Criteo: An industry-standard dataset available on Kaggle\footnote{https://www.kaggle.com/c/criteo-display-ad-challenge/}, which consists of user click data collected over one week to predict ad click-through rates (CTR). This comprehensive dataset contains 45 million samples and 39 features, including 13 continuous features and 26 categorical features, providing valuable insights for model evaluation and performance benchmarking.
                
    %             \item Avazu: A benchmark dataset available on Kaggle\footnote{https://www.kaggle.com/c/avazu-ctr-prediction/}, that is wildly used for CTR prediction and contains detailed user click data over 11 consecutive days. The dataset consists of approximately 40 million samples and includes 24 features, offering a robust foundation for algorithm testing and model improvement.

    %             \item Industrial: A private industrial dataset drawn from an online ad platform that has impressions of more than 400 million users. 
    %             The dataset contains hundreds of features, including categorical features, numerical features, and sequential features. 
    %             Among these features, there are 32 features with more than 100,000 vocab sizes, and the largest vocab size can reach six million.
    %         \end{itemize}
    %         We followed the preprocessing steps outlined in AFN \cite{AFN} to split
    %         Criteo and Avazu into training, validation, and test sets in a 7:2:1 ratio following the time order.
    %         For the Industrial dataset, we split them into train/val/test sets with a 6:1:1 proportion.
    %         Table \ref{tab:dataset} provides detailed statistics for these two public datasets. 



\subsection{Experiment Setup}
    \subsubsection{Datasets.}
        We evaluated our model using three datasets: two public and one private industrial dataset. 
        \begin{itemize}[leftmargin=*,align=left]
            \item Criteo: A standard dataset from Kaggle\footnote{https://www.kaggle.com/c/criteo-display-ad-challenge/} with one week of user click data for CTR prediction. It includes 45 million samples and 39 features (13 continuous, 26 categorical), useful for model evaluation.

            \item Avazu: Another Kaggle dataset\footnote{https://www.kaggle.com/c/avazu-ctr-prediction/}, used for CTR prediction with 11 days of user click data. It contains about 40 million samples and 24 features, serving as a solid base for testing algorithms.

            \item Industrial: A private dataset from the Huawei ad platform with over 400 million user impressions. It includes hundreds of features, both categorical and numerical, with 32 features having vocab sizes over 100,000, and the largest reaching six million.
        \end{itemize}
        We followed AFN \cite{AFN} preprocessing, splitting Criteo and Avazu datasets into training, validation, and test sets in a 7:2:1 ratio by time order. The Industrial dataset was split into train/val/test sets in a 6:1:1 ratio. Table \ref{tab:dataset} provides detailed statistics for the public datasets.





            
    \subsubsection{Baseline Models}
    To demonstrate the effectiveness of the proposed model, we select some representative CTR models for comparison.
    We also compare our proposed model with some hashing and quantization-based methods to validate the superiority of our model. 
    The details are listed as follows:
         
\textbf{Representative CTR Models:}
    \begin{itemize}[leftmargin=*,align=left]
        \item \textbf{LR} \cite{LR}: Logistic Regression is a linear model using a logistic function for binary variables. It is often a baseline in recommendation systems due to its simplicity and interpretability.
    % \end{itemize}     
    % \textbf{Factorization-based Models}
    % \begin{itemize}[leftmargin=*,align=left]
        \item \textbf{FM} \cite{FM}: Factorization Machine models pairwise interactions between features efficiently, making it suitable for sparse data.
        \item \textbf{AFM} \cite{AFM}: Attentional Factorization Machine enhances FM by using attention to model feature interaction importance.
        \item \textbf{DeepFM} \cite{DeepFM}: Combines FM for low-order and DNN for high-order interactions, improving recommendation accuracy.
    % \end{itemize}          
    % \textbf{Neural Network Models}
    %     \begin{itemize}[leftmargin=*,align=left]
            \item \textbf{PNN} \cite{PNN}: Product-based Neural Network uses product operations between feature embeddings to model feature interactions.
            \item \textbf{DCNv2} \cite{DCNv2}: Deep \& Cross Network v2 captures explicit and implicit feature interactions by stacking cross and deep layers.
            \item \textbf{AutoInt}\cite{AutoInt}: Utilizes self-attention mechanisms to automatically learn feature interactions without manual feature engineering.
            \item \textbf{AFN} \cite{AFN}: Adaptive Factorization Network dynamically learns the importance of feature interactions using a neural network.
            \item \textbf{SAM} \cite{SAM}: Self-Attentive Model leverages self-attention to capture complex feature interactions effectively.
            \item \textbf{GDCN} \cite{GDCN}: Gate-based Deep Cross Network uses gating mechanisms to model the complex relationships between users and items, enhancing collaborative filtering signals.
        \end{itemize}
              
    \textbf{Hashing and Quantization Models:}
        \begin{itemize}[leftmargin=*,align=left]
            \item \textbf{DHE} \cite{DHE}: Deep Hash Embedding uses hashing techniques for dimensionality reduction, improving memory and time efficiency.
            \item \textbf{xLightFM} \cite{xLightFM}: An extremely memory-efficient Factorization Machine that uses codebooks for embedding composition and adapts codebook size with neural architecture search.
        \end{itemize}
                % \gw{Where is AFM, AFN, SAM, DHE, and LightFM?}
    % \subsubsection{Evaluation Metrics.}
        \input{Sections/Tables/main_result} 
    %         We evaluate the algorithms using two popular metrics: AUC \cite{AUC} and Logloss \cite{LogicR}. The AUC (Area Under the ROC Curve) metric measures the ability of the model to rank positive items (i.e., samples with label 1) higher than negative ones. A higher AUC indicates better recommendation performance. Logloss measures the distance between the predicted probabilities and the ground-truth labels.
    %         A smaller Logloss value indicates better prediction performance. 
    %         For our task, it is important to notice that our approach saves up to over 90\% of memory.  
    %         Therefore, even achieving performance on par with the baselines can still bring significant business value in practical recommendation scenarios.

    % \subsubsection{Parameter Settings.}
    %     We implemented all models utilizing FuxiCTR, an open-source CTR prediction library \footnote{https://fuxictr.github.io/}. 
    %     To ensure a fair comparison, we standardized the embedding dimension across all models to 40 and set the batch size to 10,000. 
    %     The learning rate was tuned from the set {1e-1, 1e-2, 1e-3, 1e-4}, while $L_2$ regularization was adjusted within the range {0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5}, and the dropout ratio varied from 0 to 0.9. 
    %     All baseline models were trained using the Adam \cite{adam} optimizer. 
    %     We also varied the codebook size among {256, 512, 1024, 2048} and the number of code layers for PQ layers among {2, 4, 8}. 
    %     To ensure the robustness and reliability of our findings, each experiment was conducted five times, with the average performance being reported. 
    %     We measured the memory usage of each model based on the complete model size. 
    %     Moreover, we present the experimental results using the optimal parameter configurations. 
    %     Moreover, we employed the pre-trained embedding layer of DeepFM \cite{DeepFM} as the initial embedding before quantization.



\subsubsection{Evaluation Metrics}
% We assess the algorithms using AUC and Logloss metrics. AUC (Area Under the ROC Curve) evaluates the model's ability to rank positive instances higher than negatives, with higher values indicating better performance. Logloss measures the discrepancy between predicted probabilities and actual labels, where lower values indicate better accuracy. Our method achieves over 90\% memory savings, maintaining competitive performance with baselines and offering significant practical value.
We evaluate the algorithms using AUC and Logloss metrics to ensure a comprehensive performance assessment. AUC (Area Under the ROC Curve) evaluates the model's capability to rank positive instances above negatives, with higher values indicating superior discrimination ability. Logloss, on the other hand, measures the accuracy of predicted probabilities in relation to actual labels, with lower values indicating better model calibration and precision. Our method achieves over 90\% memory savings while maintaining performance on par with baseline models. This efficiency, coupled with competitive accuracy, underscores the method's practical value, especially in environments where memory constraints are a concern.



\subsubsection{Parameter Settings}
All models were implemented using the FuxiCTR library\footnote{https://fuxictr.github.io/}. We standardized the embedding dimension to 40 and batch size to 10,000. The learning rate was chosen from \{1e-1, 1e-2, 1e-3, 1e-4\}, with $L_2$ regularization from \{0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5\}, and dropout ratios from 0 to 0.9. Models were trained using the Adam optimizer. Codebook sizes were \{256, 512, 1024, 2048\} and PQ layers \{2, 4, 8\}. Experiments were repeated five times to ensure reliability, reporting average results. Memory usage was based on full model size, and results are presented with optimal parameters. We used the pre-trained embedding layer of DeepFM as initialization before quantization.






    \subsection{Performance Comparison}
        % \subsubsection{Public Dataset Performance}
        % \label{subsec:public_performance}
        % In this section, we evaluate the performance of our proposed framework, MemE-CTR, on two widely-used CTR prediction datasets: Criteo and Avazu. 
        % In this section, we compare MemE-CTR with baseline models, and the results are presented in Table \ref{tab:MainTable}.
        % In this section, we compare MEC with baseline models in terms of both CTR performance and memory usage, and the results are presented in Table \ref{tab:MainTable}. We also conduct Wilcoxon signed rank tests \cite{p_test} to evaluate the statistical significance of MEC with the base model. We have the following observations:
        % The results in Table \ref{tab:MainTable} highlight several key findings:

        % \textbf{Finding 1:} Meme-CTR achieves performance that is on par with or superior to traditional CTR models. \gw{which model? on par with or superior? give a definite conclusion} 

        % \textbf{(1) The embedding tables play a great part in model parameters.} 
        % The parameters of traditional CTR models are mainly concentrated in the embedding layer. 
        % Despite the model structure and complexity varying a lot from each other, the overall parameters are much of the same size. 
        % As shown in Table \ref{tab:MainTable}, optimizing the embedding tables can reduce model parameters by over 90\%, highlighting the significant potential for quantization and compression techniques to improve efficiency and performance.
        
        \begin{figure}[t]
            \centering
            \setlength{\abovecaptionskip}{-5pt}   % 图注上方的距离
            \setlength{\belowcaptionskip}{0pt}   % 图注下方的距离
            \includegraphics[width=0.75\textwidth]{Sections/Figures/bars.pdf}
            \caption{Time efficiency performance}
        \label{fig:time_eff}
        % \vspace{-14pt}
        \end{figure}
        
        % \textbf{(2) Hashing and quantization models bring improvements but still have limitations. } 
        % An examination of rows 9 to 16 in Table \ref{tab:MainTable} reveals that existing hashing and quantization techniques, such as DHE \cite{DHE} and xLightFM \cite{xLightFM} have achieved a significant reduction of parameters but lead to a marked deterioration in performance. 
        % The primary reason for this decline is that both DHE and xLightFM prioritize optimal compression of the embedding table during quantization while overlooking the imbalances in data distribution and the unevenness of embedding caused by traditional quantization methods.
        % Apart from that, the existing quantization methods must be trained end-to-end and cannot be migrated to other CTR models directly.
        % In contrast, Our approach addresses these challenges by using a portable method that rebalances the distribution of quantized data, thus achieving superior performance compared to alternative compression models.


        % \textbf{(3) Embedding lookup time is closely tied to the size of the embedding tables.}
        % As shown in Figure \ref{fig:time_eff}, we compared the total time taken by each model to perform embedding lookup 100 times under identical experimental conditions.
        % With quantization algorithms, we achieved about a 50\% reduction in time, thanks to the smaller vocabulary size. 
        % This shows a strong correlation between inference time and memory size. 
        % Moreover, we also analyzed the training latency, and our experiments demonstrated that the modifications to the PQ quantization algorithm introduced negligible latency compared to the training process itself, underscoring the method's practicality. 
        % Full experimental results can be found in Appendix C.
        % Therefore, with MEC's significant parameter compression and high-quality quantization, we improved both inference efficiency and model performance.
        
        % \textbf{(4) MEC achieves superior performance across all datasets.} 
        % As shown in Table 2, when GDCN is used as the base model, $\text{MEC}_{GDCN}$ performs better than GDCN by 0.4\% on the Avazu dataset and 0.07\% on the Criteo dataset, while using fewer parameters. 
        % It is important to note that, as a pluggable model, MEC achieves improvements of up to 0.33\% and 0.41\% over both PNN and GDCN, with the most significant enhancements seen in the results from the Avazu dataset.
        % The more significant improvements in the Avazu dataset can be attributed to its pronounced long-tailed distribution, particularly in certain feature fields with a large number of features. 
        % This long-tail effect typically hampers the embedding capability of conventional models. 
        % Our method effectively addresses this issue by employing popularity-based regularization, which enhances the overall quality of feature embeddings.




        In this section, we compare MEC with baseline models in terms of both CTR performance and memory usage, and the results are presented in Table \ref{tab:MainTable}. We also conduct Wilcoxon signed rank tests \cite{p_test} to evaluate the statistical significance of MEC with the base model. We have the following observations:
        
\textbf{(1) Embedding tables significantly impact model parameters.} 
In traditional CTR models, most parameters are concentrated in the embedding layer. Despite variations in model structure and complexity, the overall parameter size remains similar. Table \ref{tab:MainTable} shows that optimizing embedding tables can reduce model parameters by over 90\%, highlighting the potential for quantization and compression to enhance efficiency and performance.

\textbf{(2) Hashing and quantization models have benefits but limitations.} 
Rows 9 to 16 in Table \ref{tab:MainTable} indicate that methods like DHE \cite{DHE} and xLightFM \cite{xLightFM} reduce parameters significantly but degrade performance. This is due to their focus on compression without addressing data distribution imbalances caused by traditional quantization. These methods also require end-to-end training and lack portability. Our approach overcomes these issues by rebalancing quantized data distribution, achieving better performance.

\textbf{(3) Embedding lookup time correlates with embedding table size.}
Fig. \ref{fig:time_eff} shows that embedding lookup time is reduced by about 50\% with quantization due to smaller vocabulary sizes, indicating a strong link between inference time and memory size. Training latency analysis reveals that PQ quantization modifications add negligible latency, confirming the method's practicality. Full results are in Section 5.4. MEC's parameter compression and high-quality quantization enhance both inference efficiency and model performance.

\textbf{(4) MEC excels across all datasets.} 
Table 2 shows that $\text{MEC}_{GDCN}$ outperforms GDCN by 0.4\% on Avazu and 0.07\% on Criteo, with fewer parameters. As a pluggable model, MEC improves performance by up to 0.33\% and 0.41\% over PNN and GDCN, especially on Avazu due to its long-tailed distribution. This distribution often limits conventional models, but our method's popularity-based regularization enhances feature embedding quality.











    % \subsection{Industrial Dataset Performance}
    %     In Table \ref{tab:industry}, we present a comprehensive analysis of the performance and memory usage of MEC applied to an industrial dataset. 
    %     The comparison demonstrates that MEC, by utilizing Product Quantization's robust quantization capabilities reduces more than 99.7\% memory consumption of the embedding layer. 
    %     Additionally, our implementation of popularity-weighted regularization and contrastive learning effectively addresses common challenges encountered in quantized models, such as code allocation imbalance and uneven distribution of quantization centers. 
    %     Consequently, even with substantial parameter quantization, our method achieves performance that is comparable to, or even superior to, that of the baseline model.

% \subsection{Industrial Dataset Performance}

% Table \ref{tab:industry} shows that MEC reduces embedding layer memory usage by over 99.7\% using Product Quantization. The use of popularity-weighted regularization and contrastive learning addresses issues like code imbalance and uneven quantization distribution. Despite heavy quantization, our method matches or exceeds baseline model performance.

\subsection{Industrial Dataset Performance}

Table \ref{tab:industry} clearly demonstrates that MEC achieves over 99.7\% reduction in embedding layer memory usage through efficient Product Quantization. The integration of popularity-weighted regularization and contrastive learning effectively addresses typical challenges like code imbalance and uneven quantization distribution. Despite significant quantization, our method consistently maintains or even surpasses the performance of the baseline model.




    
    % \subsection{Time \& Memory Efficiency}
    
    %     \input{Sections/Tables/time_memory}
    %     \kf{Considering changing this table to a chart, being drawn by Lvhang}
    %     This study compares the memory and time consumption during the embedding step across various models, including the baseline PNN \cite{PNN}, DHE \cite{DHE}, xLightFM \cite{xLightFM}, and our proposed MemE-PNN. We utilize PNN as the baseline model, focusing exclusively on measuring the inference time and parameter count of the embedding layer. Table \ref{tab:Time_memory} displays the total time required to infer 100 batches.
        
    %     From Table \ref{tab:Time_memory}, it is evident that the baseline PNN \cite{PNN} exhibits the highest memory consumption and processing time, attributable to its large embedding table. In practical recommendation scenarios, the use of extensive embedding tables can lead to challenges associated with exceeding GPU memory limitations, thereby causing notable delays in data retrieval and processing.
        
    %     MemE-CTR, DHE \cite{DHE}, and xLightFM \cite{xLightFM} exhibit similar compression capabilities in terms of Time Costs and Params. 
    %     With the same parameters, all three methods achieve approximately 99\% compression in Params. However, as shown in Section 2, MemE-CTR significantly outperforms DHE and xLightFM in recommendation performance.
        
    %     Specifically, MemE-CTR achieves an AUC improvement of up to 0.003 over DHE and xLightFM. Given that a 0.001 improvement in AUC is considered highly significant for CTR tasks, this level of enhancement is crucial for real-world recommendation scenarios. 
    %     This performance advantage arises because DHE and xLightFM focus too much on maximizing memory savings, which compromises the quality of embeddings. 
    %     In contrast, MemE-CTR employs Product Quantization (PQ) to maintain high quantization efficiency while using popularity-weighted regularization and contrastive learning to allocate better codes for features. 
    %     This approach not only enhances the quality of quantized embeddings but also significantly improves the model's recommendation performance, making it more effective in real-world industrial applications.


    \begin{figure}[t]
        \centering
        \includegraphics[width=\textwidth]{Sections/Figures/line_v5.pdf}
        \vspace{-2em}
        \caption{Hyper-Parameter Performance}
    \label{fig:line_chart}
    \end{figure}
% \vspace{-3em}
    \subsection{In-depth Analysis}
        \input{Sections/Tables/industry}
        

        % \subsubsection{\textbf{Ablation Study of MEC}}
        %     \input{Sections/Tables/ablation}
        %     To investigate the effect of each part of MEC, we investigated the impact of regularization and contrastive learning on the performance of MEC by evaluating three variants on PNN \cite{PNN}: (1) w/o cons., which excludes contrastive learning; (2) w/o reg., which excludes popularity-weighted regularization; (3) basic PQ, which employs standard Product Quantization (PQ) \cite{PQ}; and (4) freq. PQ, which integrates frequency information before applying PQ quantization.
        %     The results, presented in Table \ref{tab:Ablation}, are based on experiments conducted with the Criteo and Avazu datasets. Several key insights can be derived from these findings. 
            
        %     Direct application of PQ for quantization results in a notable performance drop. 
        %     While PQ effectively alleviates memory constraints by directly quantizing the feature embedding table, the suboptimal distribution of codes during quantization diminishes the uniqueness of features, thereby weakening the expressive power of embeddings and subsequently degrading CTR prediction performance. 
            
        %     Furthermore, incorporating frequency information leads to an even greater performance decline. 
        %     The results for freq. PQ demonstrates that directly adding frequency information significantly harms performance due to the highly imbalanced nature of data distribution. 
        %     The quantization centers become biased towards high-frequency features, resulting in poor representation of low-frequency features and a marked decrease in CTR prediction accuracy. 
            
        %     On the other hand, the use of regularization significantly improves quantization outcomes, maintaining performance levels close to the base model. 
        %     Regularization mitigates the dominance of high-frequency features in the quantization centers, ensuring that low-frequency features do not share quantized representations with high-frequency features. 
        %     This enhancement improves the model's ability to represent low-frequency features, counteracting the negative impact caused by frequency-based enhancements. 
        %     Contrastive learning enhances quantization by increasing separation between representations, ensuring balanced code distribution and performance. The combination of regularization and contrastive learning achieves optimal results by balancing high- and low-frequency features and homogenizing quantized representations.


\subsubsection{Analysis of Hyper-Parameter.}
As shown in Fig. \ref{fig:line_chart}, we analyzed key hyper-parameters in our MEC framework: the regularization loss coefficient ($\alpha$), contrastive loss coefficient ($\beta$), embedding dimension ($d$), and number of embedding layers ($m$). 
Optimal values were $\alpha = 0.001$, $\beta = 0.01$, $d = 2048$, and $m = 4$. Deviations from these values caused issues like underfitting, overfitting, information loss, or suboptimal relationship modeling.


\subsubsection{\textbf{Ablation Study of MEC}}
\input{Sections/Tables/ablation}

To thoroughly assess the contributions of different components in MEC, we evaluated three distinct variants on PNN \cite{PNN}: (1) without contrastive learning (w/o cons.); (2) without popularity-weighted regularization (w/o reg.); (3) using basic Product Quantization (PQ); and (4) frequency-based PQ (freq. PQ). The results, as shown in Table \ref{tab:Ablation}, were obtained using the Criteo and Avazu datasets.


% Applying PQ directly causes a significant performance drop. Although PQ reduces memory usage by quantizing the embedding table, it leads to suboptimal feature representation, weakening model expressiveness and CTR prediction accuracy.
% Including frequency information further decreases performance due to data imbalance. High-frequency features dominate the quantization centers, poorly representing low-frequency features and reducing accuracy.
% Regularization improves outcomes by preventing high-frequency features from dominating quantization, thus better representing low-frequency features. This counters the negative effects of frequency imbalance.
% Contrastive learning enhances separation between representations, promoting balanced code distribution. The combination of regularization and contrastive learning provides the best results by effectively balancing feature frequencies and improving quantized representation quality.

Directly applying PQ reduces performance by weakening feature representation and CTR prediction accuracy. Simply including frequency information exacerbates this by allowing high-frequency features to dominate, poorly representing low-frequency features. Regularization mitigates this imbalance by preventing domination and better representing low-frequency features. Contrastive learning enhances representation separation and code distribution. Together, regularization and contrastive learning balance feature frequencies and improve quantized representation quality.


\input{Sections/Tables/training_latency}

\subsubsection{Analysis of Training Latency}
\label{subsub:latency}
In this section, we analyze the training latency, given the computational constraints in real-world scenarios where training time needs careful consideration. 
Table \ref{tab:Latency} presents the results based on the Criteo and Avazu datasets, averaged over 10 runs. The PQ component adds negligible training time compared to the main CTR model. Popularity-weighted regularization does not increase time complexity, and while contrastive learning slightly increases training time, it remains manageable. Overall, our approach improves quantized embedding quality without significantly impacting training latency.


        % \subsubsection{\textbf{Analysis of Quantization Methods.}}
        %     \input{Sections/Tables/rq_vae}
        %     In this study, we evaluated various quantization methods for our MEC framework to balance memory efficiency and model performance. We compared Product Quantization (PQ) \cite{PQ} with Additive Quantization (AQ) \cite{AQ} and Residual Quantization (RQ) \cite{RQVAE}. Our experiments showed that PQ was the most effective among the three quantization methods.
            
        %     As shown in Table \ref{tab:rq_result}, RQ's performance lagged behind PQ due to high correlation among generated codes, causing instability in contrastive learning and unreliable negative samples. PQ, however, manages high-dimensional embedding tables by generating multiple sub-codebooks, preserving expressive power while reducing memory usage. The independence of these sub-codebooks ensures stability in contrastive learning, making PQ a robust choice for MEC.
        %     Similarly, AQ performed less effectively than PQ. AQ's additive process introduces redundancy and noise, reducing memory efficiency and stability. PQ avoids these issues with independent sub-codebooks, minimizing redundancy and optimizing memory use, thus enhancing contrastive learning stability.


\subsubsection{\textbf{Analysis of Quantization Methods}}

\input{Sections/Tables/rq_vae}

We evaluated quantization methods for MEC, comparing Product Quantization (PQ) with Additive Quantization (AQ) and Residual Quantization (RQ). Table \ref{tab:rq_result} shows that PQ outperforms the others. RQ suffers from code correlation, destabilizing contrastive learning, while AQ introduces redundancy and noise. PQ's independent sub-codebooks maintain stability and memory efficiency, making it the best choice for MEC.

\subsubsection{\textbf{Analysis of Pre-train Models}}
\input{Sections/Tables/pretrain_model_analysis}

We evaluated the impact of different pre-trained models (FM \cite{FM}, DeepFM \cite{DeepFM}, DCNv2 \cite{DCNv2}) on our framework for embedding generation and quantization, using PNN for downstream CTR prediction. Table \ref{tab:pretrain_model_analysis} shows minor performance variations across models, highlighting our framework's robust generalizability. This enables the use of simpler models in industrial applications without significant performance loss, allowing efficient deployment in various scenarios.




        % \subsubsection{\textbf{Additional Analyses}}
        %     We also conducted further analysis, including hyperparameter analysis in Appendix \ref{app:hyper} and analysis of pre-train models in Appendix \ref{app:pretrain_models}.To provide readers with an intuitive understanding of the uniformity in quantized embedding distributions, we present a visualization of code distribution in Appendix ~\ref{app:visual}. Additionally, the analysis of training latency is in Appendix \ref{app:latency}, providing deeper insights into the framework's time efficiency.

        




