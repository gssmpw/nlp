\section{Conclusion}
This paper tackles the memory consumption challenge in CTR prediction models caused by large embedding tables. We proposed a Model-agnostic Embedding Compression (MEC) framework, which combines popularity-weighted regularization (PWR) and contrastive learning to compress embeddings while maintaining high recommendation performance. Experiments on three real-world datasets demonstrate that MEC reduces memory usage by over 50x while achieving comparable or superior performance to state-of-the-art models. Our findings highlight MEC's potential for efficient and scalable CTR prediction.
\vspace{2pt}
