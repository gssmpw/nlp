% \section{Related Work}
%     \subsection{Click-Through-Rate Prediction}
%         CTR prediction methods focus on modeling the likelihood of a user clicking on an item. 
%         To capture the user interests and model the contextual association more accurately, existing methods can be primarily classified into two categories, i.e., feature interaction-based and user behavior modeling-based. 
%         Representative works in the formal area include Wide\&Deep \cite{WideDeep}, PNN \cite{PNN}, DeepFM \cite{DeepFM}, DCNv2 \cite{DCNv2}, AutoInt \cite{AutoInt}, AFN \cite{AFN}, SAM \cite{SAM} and GDCN \cite{GDCN}. 
%         These methods adopt single tower networks or dual tower networks to model the feature interactions explicitly or implicitly to improve the accuracy \cite{zhang2021deep}. 
%         For instance, PNN \cite{PNN} adopts a single tower network that leverages product layers to model high-order feature interactions. 
%         GDCN \cite{GDCN} utilizes a dual tower network that combines a gated cross-network and a deep neural network to effectively capture both explicit and implicit feature interactions.
        
%         For the latter direction, the core is to mine users' personal preferences from users' historical behavioral interactions.
%         The modeling architecture have evolved from early  attention network \cite{zhou2018deep,zhou2019deep}, memory network \cite{pi2019practice,ren2019lifelong},  retrieval network \cite{pi2020search,chen2021end}, to efficient Transformers \cite{yu2024ifa}.
%         DIN \cite{zhou2018deep} uses an attention network to assign different scores to different user behaviors for user representation learning to extract users' diverse interests. 
%         ETA \cite{chen2021end} proposes to use the smash algorithm to map the embeddings of behaviors and target items to binary signatures, then retrieve the behaviors with the smallest Hamming distances.
        
%         Though great progress has been made, these CTR methods face significant challenges when dealing with high-dimensional feature spaces and large embedding tables. 
%         Since a single GPU cannot accommodate all the embedding parameters, one solution is to put these embedding tables into CPU memory \cite{wang2022merlin,guo2021scalefreectr}.
%         However, the huge embedding table required can lead to substantial memory consumption and significantly increase the latency with pull \& push operations between CPU and GPU, posing a bottleneck for real-time recommendation systems. 

%     \subsection{Memory-Efficient Recommendation}
%         Given the constraints of memory and computational resources in real-time recommendation systems, exploring memory-efficient methods is imperative. 
%         Memory-efficient recommendation can be broadly classified into hashing-based and quantization-based techniques. 
%         Hashing-based methods \cite{hashing_trick,improved_hash_1,improved_hash_2,improved_hash_3} convert continuous user and item embeddings into compact hash codes, thereby significantly enhancing memory efficiency. 
%         For instance, the hashing trick \cite{hashing_trick} addresses large vocabulary features and out-of-vocabulary values through efficient one-hot encodings. 
%         Deep Hash Embedding (DHE) \cite{DHE} employs deep learning techniques to generate high-quality hash codes, reducing memory consumption while preserving recommendation accuracy. 
%         While multiple hash functions can offer unique feature representations by mapping them to a small shared parameter table, they still risk random parameter sharing between unrelated concepts, introducing noise to subsequent machine learning models.
        
%         Quantization \cite{DPQ,improved_PQ_1,improved_PQ_2} mitigates the memory footprint of embeddings by representing them through combinations of codebooks, thereby achieving higher compression rates and improved retrieval accuracy compared to hash-based methods. 
%         This capability is vital for managing extensive data in real-world recommendation scenarios. 
%         Generative recommendation methods such as Tiger \cite{tiger} and Letter \cite{letter} leverage quantization to optimize memory usage, concentrating on delivering high-quality recommendations with compact embeddings. 
%         Likewise, end-to-end click-through rate (CTR) models like xlightfm \cite{xLightFM} and LightRec \cite{LightRec} use vector quantization at the encoding layer to conserve memory. 
%         Recent approaches have explored adaptive PQ based on deep product quantization (DPQ)~\cite{DPQ}, aiming to enhance compression capabilities while enabling end-to-end training. For example, AutoDPQ~\cite{AutoDPQ} utilizes an AutoML process to enable the model to adaptively determine the optimal codebook size. Additionally, CCE~\cite{CCE} introduces a method that combines PQ and hashing, allowing for dynamic model updates while maintaining a high compression rate. 
        
%         However, these approaches often overlook the distribution quality of quantized embeddings.
%         In contrast, we propose an adaptive two-stage training framework that optimizes embedding distributions post-quantization, enhancing representation quality without significant computational overhead. 
%         This approach complements existing methods and offers potential for future research advancements. Additionally, our scalable MEC framework integrates with the latest quantization methods and CTR models to further improve quantized embedding quality.


\section{Related Work}
    \subsection{Click-Through-Rate Prediction}
        CTR prediction focuses on estimating the likelihood of user clicks. Existing methods are mainly divided into feature interaction-based and user behavior modeling-based approaches. Feature interaction models, such as Wide\&Deep \cite{WideDeep} and DeepFM \cite{DeepFM}, utilize network architectures to capture complex interactions. For example, GDCN \cite{GDCN} employs dual tower networks to model both explicit and implicit interactions.

        User behavior modeling aims to extract personal preferences from historical interactions~\cite{important_2_perlaw,important_4_dataset,other_3_fuxi,other_12_learning,other_15_apgl4sr,other_19_hypersorec,other_20_mcne}. Techniques have evolved from attention networks \cite{zhou2018deep} to efficient Transformers. DIN \cite{zhou2018deep} uses attention to assign scores to user behaviors, capturing diverse interests.

    \subsection{Memory-Efficient Recommendation}
        Memory-efficient techniques, including hashing and quantization, are crucial for resource-constrained systems. Hashing methods convert embeddings into compact codes, improving memory efficiency but risking noise from unrelated mappings \cite{hashing_trick,DHE}. Quantization reduces memory usage by representing embeddings through codebooks, achieving higher compression rates \cite{DPQ}. Methods like AutoDPQ adaptively determine codebook sizes, while CCE combines PQ and hashing for dynamic updates \cite{AutoDPQ}.

        However, these approaches often overlook the distribution quality of quantized embeddings. Our adaptive two-stage training framework optimizes embedding distributions post-quantization, enhancing representation quality without significant computational overhead. This approach complements existing methods and offers potential for future research advancements.
