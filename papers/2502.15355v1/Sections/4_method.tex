    \begin{figure*}[ht]
        \centering
        \setlength{\abovecaptionskip}{5pt}   % 图注上方的距离
        \setlength{\belowcaptionskip}{-6pt}   % 图注下方的距离
        \includegraphics[width=\textwidth]{Sections/Figures/main_v3.pdf}
        \caption{Overview of our MEC framework. The framework consists of two stages: pre-training and downstream task training. In the first stage (left), a PQ codebook is learned by combining existing embeddings. In the second stage (right), the input features are quantized based on the PQ codebook and used to train a CTR model for the downstream task. During online inference, the pre-quantized features are fed into the downstream task to achieve memory-efficient CTR prediction.}
    \label{fig:main}
    \vspace{1em}
    \end{figure*}
\section{Methodology}
    

    % As previously mentioned, contemporary click-through rate (CTR) prediction methods face challenges in handling large embedding spaces, leading to increased storage requirements and decreased inference efficiency. 
    % Traditional quantization techniques often do not adequately prioritize high-frequency features, resulting in subpar embedding quality for these crucial features. 
    % However, directly considering feature frequency can overshadow low-frequency tail features with high-frequency ones, negatively impacting their performance and interactions. 
    % In addition, traditional quantization methods frequently neglect the importance of evenly distributing feature embeddings, leading to excessively concentrated and uniform embeddings that can compromise the model's predictive abilities in CTR scenarios. 
    % % \gw{check the challenges here.}
    % This paper aims to compress embedding spaces while maintaining comparable accuracy. 
    % Section 4.1 introduces the MEC framework, Section 4.2 discusses the adaptation of popularity-weighted regularization (PWR) for product quantization tailored to CTR data, and Section 4.3 explores the implementation of contrastive learning to address the issue of uneven representation distribution in PQ.


    % \subsection{Overview}
    %     Existing CTR prediction models predominantly depend on dense embeddings for features, resulting in significantly large embedding tables as datasets expand. 
    %     This phenomenon leads to substantial degradation of online inference speed in practical applications. 
    %     While quantizing the embedding table is a widely adopted solution, the extensive feature interactions inherent in CTR tasks necessitate that the quantization process retains as much feature information as possible while preserving the richness of the quantized embeddings. 
    %     Currently, existing quantization models are not optimized to accommodate this characteristic of CTR tasks, which adversely affects their recommendation performance. 
        
    %     To tackle these challenges, we propose a two-stage framework that decouples the quantization process from the inference process, as shown in Figure \ref{fig:main}. 
    %     This design facilitates easy adaptation to various CTR models and allows for efficient model updates by only modifying the relatively small pre-trained model. 
    %     In the initial phase, we employ an auxiliary CTR model for pre-training. Experimental results demonstrate that even a basic auxiliary model (e.g., FM \cite{FM}) does not cause significant performance degradation. 
    %     During the quantization process, we introduce a popularity regularization loss to enhance the retention of feature information within the quantized embedding table. 
    %     Furthermore, we incorporate contrastive learning to improve the uniformity of the quantized embeddings. 
    %     These enhancements enable the quantized embeddings to deliver superior performance in CTR tasks.
    %     The overall algorithm is delineated in Algorithm~\ref{algo: algorithm}.


    %     \subsubsection{Stage 1: Embedding Layer Quantization}
    %         \label{subsubsec:stage1}
    %         In our proposed framework, we utilize a pre-trained embedding layer derived from an auxiliary CTR model specifically for quantization. 
    %         This embedding layer $F_{\text{Emb}}$ effectively transforms input features $\mathbf{x}$ into embedding vectors $\mathbf{e}$ through the embedding lookup operation:
    %         \begin{equation}  
    %         \mathbf{e} = F_{\text{Emb}}(\mathbf{x}).  
    %         \label{eq:embedding}
    %         \end{equation}  
    %         Upon the completion of the pre-training phase, these embeddings are processed by an encoder designed to adjust their dimensions appropriately for the quantization process:
    %         % Each embedding vector $\mathbf{e}$ is transformed into $\mathbf{s}$ by this encoder:  
    %         \begin{equation}  
    %         \mathbf{s} = \text{Encoder}(\mathbf{e}).  
    %         \end{equation}  
    %         The vector $\mathbf{s}$ is subsequently partitioned into $M$ sub-vectors $\{\mathbf{s}_1$, $\mathbf{s}_2$, $\ldots$, $\mathbf{s}_M\}$, with each sub-vector $\mathbf{s}_i \in \mathbb{R}^{d/M}$. 
    %         Here $d$ is the dimension of the encoded vector and $M$ denotes the number of code layers for quantization.
    %         Then Product Quantization (PQ) is employed to quantize each sub-vector $\mathbf{s}_i$.
    %         A codebook $\mathbf{C}_i \in \mathbf{R}^{K \times d/M}$ is randomly initialized to represent the original sub-vector $\mathbf{s}_i$, where $K$ is the the codebook size. 
    %         For each sub-vector $\mathbf{s}_i$, we identify the closest codeword $\mathbf{c}_{i,j_i}$ from $\mathbf{C}_i$ by minimizing the Euclidean distance:  
    %         \begin{equation}  
    %         j_i = \arg \min_j \| \mathbf{s}_i - \mathbf{c}_{i,j} \|_2,   
    %         \end{equation}  
    %         where $\mathbf{c}_{i,j} \in \mathbf{R}^{d/M} $ is the $j$-th codeword in the codebook $\mathbf{C}_i$ and $1 \leq j \leq K$.
    %         Thus we have
    %         \begin{equation}
    %         \mathbf{q}_i = \mathbf{c}_{i,j_i},
    %         \end{equation}
    %         where $\mathbf{q}_i$ represents the quantized version of the sub-vector $\mathbf{s}_i$.
    %         After determining the closest codewords for all sub-vectors, the quantized vector $\mathbf{q} = [\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{M}]$, where $[\cdot]$ represents the concatenating operation.
    %         To ensure the quantized vector $\mathbf{q}$ closely approximates the original vector $\mathbf{s}$, we minimize the reconstruction loss, which is defined as the squared difference between the original vector and its quantized version:  
    %         \begin{equation}  
    %         \mathcal{L}_{\text{recon}} = \| \mathbf{s} - \mathbf{q} \|_2^2. 
    %         \label{eq:loss_recon}
    %         \end{equation}  
    %         After the quantization stage convergence, we retain the learned codebook $\mathbf{C}$ for subsequent CTR prediction by utilizing it as the embedding initialization.
    %          We also save the quantization method $\phi$ that transforms input features $\mathbf{x}$ into codes $\mathbf{c}$ for latter usage:  
    %         \begin{equation}  
    %         \mathbf{c} = \phi(\mathbf{x}).  
    %         \end{equation}  
    %         where $\mathbf{c} = [j_1, j_2, \ldots, j_M]$ is the generated codes. 
    %         This preprocessing step facilitates efficient feature coding and embedding lookup in the CTR prediction stage. 
    %         Furthermore, our approach introduces two additional improvements to the quantization process: 
    %         \textsl{Popularity-weighted Regularization}$~(\S~\ref{subsec:reg})$, which enhances the quality of code allocation by ensuring a more balanced distribution of quantized features, and \textsl{Contrastive Product Quantization}$~(\S~\ref{subsec:cons})$, which further improves the distribution uniformity of the codewords by incorporating contrastive learning, ultimately leading to better performance in downstream tasks.

    %     \subsubsection{Stage 2: Memory-Efficient Embedding for CTR Prediction}
    %     \label{subsubsec:stage2}
    %         In this stage, we firstly use the previously saved quantization method $\phi$ to transform the input features $\mathbf{x}$ into quantized codes $\mathbf{c}$ using Eq.~(\ref{eq:loss_recon}).
    %         Then, we get the matching embedding vector $\mathbf{e}$ for codes $\mathbf{c}$ by applying the embedding lookup operation from the compressed embedding table: 
    %         \begin{equation}
    %         \mathbf{e} = \mathbf{E}_{\phi} \mathbf{c},
    %         \label{eq:compress_emb}
    %         \end{equation}
    %         where $\mathbf{E}_{\phi}$ is initialized from the retained codebook $\mathbf{C}$.
    %         Subsequently, we feed the obtained embedding vector $\mathbf{e}$ into the downstream CTR model for training and inference.
    %         The final CTR model predicts the click-through rate probability using the following formula:
    %         \begin{equation}
    %         \hat{y} = \psi(\mathbf{e}),
    %         \end{equation}
    %         where $\psi$ represents the feature interaction operations in the downstream CTR model.
    %         The widely used binary classification loss function is adopted as the objective function:
    %         \begin{equation}
    %             \mathcal{L}_{\text{binary}} = - (y\log(\hat{y} + (1-y)\log(1-\hat{y}),
    %             \label{eq:binary}
    %         \end{equation}
    %         where $y$ represents the real labels.
    %         By compressing the original embedding table with the PQ codebook, this approach not only addresses the memory consumption issue of large-scale embedding tables but also enables efficient model inference and generalization.




% \subsection{Before Overview}

%%%%% 下面是一个试做版本，但是已经迭代成了最新的版本（目前的版本）
% Contemporary CTR prediction methods face challenges with large embedding spaces, leading to increased storage demands and reduced inference efficiency. Traditional quantization techniques often overlook high-frequency features and fail to evenly distribute embeddings, degrading model performance. Our approach compresses embedding spaces while maintaining accuracy, as detailed in Section 4.1 for the MEC framework, Section 4.2 for popularity-weighted regularization (PWR), and Section 4.3 for contrastive learning to address uneven representation distribution.

% \subsection{Overview}
% Existing models rely on dense embeddings, resulting in large tables and slower inference. While quantization is a common solution, it must retain feature richness for effective CTR prediction. Current models are not optimized for this, affecting recommendation performance. Our two-stage framework decouples quantization from inference, allowing easy adaptation and efficient updates. 

% In the first stage, an auxiliary CTR model pre-trains embeddings. We transform input features $\mathbf{x}$ into embedding vectors $\mathbf{e}$ using the embedding layer $F_{\text{Emb}}$:
% \input{Sections/Algorithm/main_algorithm}

% \begin{equation}
% \mathbf{e} = F_{\text{Emb}}(\mathbf{x}),
% \end{equation}

% where $\mathbf{x}$ represents the input features and $\mathbf{e}$ is the resulting embedding vector. These embeddings are encoded and partitioned for quantization:

% \begin{equation}
% \mathbf{s} = \text{Encoder}(\mathbf{e}),
% \end{equation}

% where $\mathbf{s}$ is the encoded vector. The vector $\mathbf{s}$ is split into $M$ sub-vectors, each quantized using Product Quantization (PQ). A codebook $\mathbf{C}_i$ is initialized for each sub-vector, and the closest codeword is identified by minimizing Euclidean distance:

% \begin{equation}
% j_i = \arg \min_j \| \mathbf{s}_i - \mathbf{c}_{i,j} \|_2,
% \end{equation}

% where $\mathbf{c}_{i,j}$ is the $j$-th codeword in the codebook $\mathbf{C}_i$. The quantized vector $\mathbf{q}$ is formed by concatenating the closest codewords, minimizing reconstruction loss:

% \begin{equation}
% \mathcal{L}_{\text{recon}} = \| \mathbf{s} - \mathbf{q} \|_2^2.
% \end{equation}

% After convergence, the learned codebook $\mathbf{C}$ and quantization method $\phi$ are retained for CTR prediction. We enhance quantization with Popularity-weighted Regularization and Contrastive Product Quantization, using the update rule:

% \begin{equation}
% \phi' \leftarrow \phi - (\nabla_{\phi}\mathcal{L}_{\text{recon}} + \alpha \nabla_{\phi} \mathcal{L}_{\text{reg}} + \beta \nabla_{\phi} \mathcal{L}_{\text{con}}),
% \end{equation}

% where $\alpha$ and $\beta$ are hyperparameters controlling the influence of regularization and contrastive losses, respectively.

% \subsubsection{Stage 2: Memory-Efficient Embedding for CTR Prediction}
% Using the saved quantization method $\phi$, input features $\mathbf{x}$ are transformed into quantized codes $\mathbf{c}$, retrieving embeddings for CTR model training and inference:

% \begin{equation}
% \mathbf{e} = \mathbf{E}_{\phi} \mathbf{c},
% \end{equation}

% where $\mathbf{E}_{\phi}$ is the embedding table initialized from the retained codebook. The CTR model predicts click-through rate probability:

% \begin{equation}
% \hat{y} = \psi(\mathbf{e}),
% \end{equation}

% where $\psi$ represents the feature interaction operations in the downstream CTR model. The prediction is optimized using a binary classification loss:

% \begin{equation}
% \mathcal{L}_{\text{binary}} = - (y\log(\hat{y}) + (1-y)\log(1-\hat{y})),
% \end{equation}

% where $y$ is the true label. By compressing the embedding table with the PQ codebook, we address memory consumption and enhance model inference efficiency and generalization.




% \section{Before Overview}

Contemporary CTR prediction methods face challenges with large embedding spaces, leading to increased storage demands and reduced inference efficiency. Traditional quantization techniques often overlook high-frequency features and fail to evenly distribute embeddings, degrading model performance. Our approach compresses embedding spaces while maintaining accuracy, as detailed in Section 4.1 for the MEC framework, Section 4.2 for popularity-weighted regularization (PWR), and Section 4.3 for contrastive learning to address uneven representation distribution.

\subsection{Overview}
Existing models typically rely on dense embeddings, resulting in large tables and slower inference. While quantization is a common solution, it must effectively retain feature richness for accurate CTR prediction. Current models are not fully optimized for this, adversely affecting recommendation performance. As shown in Fig. \ref{fig:main}, our innovative two-stage framework decouples quantization from inference, allowing easy adaptation and efficient updates.

In the first stage, an auxiliary CTR model pre-trains embeddings. The input features $\mathbf{x}$ are transformed into embedding vectors $\mathbf{e}$ via an embedding function $F_{\text{Emb}}$, then encoded and partitioned for quantization:

\begin{equation}  
\mathbf{e} = F_{\text{Emb}}(\mathbf{x}), \quad \mathbf{s} = \text{Encoder}(\mathbf{e}).
\end{equation}  

Here, $\mathbf{x}$ represents the input features, $\mathbf{e}$ is the resulting embedding vector, and $\mathbf{s}$ is the encoded vector prepared for quantization. The vector $\mathbf{s}$ is split into $M$ sub-vectors $\{\mathbf{s}_1, \mathbf{s}_2, \ldots, \mathbf{s}_M\}$, each of dimension $d/M$, where $d$ is the dimension of the encoded vector. For Product Quantization (PQ), each sub-vector $\mathbf{s}_i$ is quantized using a codebook $\mathbf{C}_i$, initialized with size $K$. The closest codeword is identified by minimizing the Euclidean distance:

\begin{equation}  
j_i = \arg \min_j \| \mathbf{s}_i - \mathbf{c}_{i,j} \|_2, \quad \mathbf{q}_i = \mathbf{c}_{i,j_i},
\end{equation}  

where $\mathbf{c}_{i,j}$ is the $j$-th codeword in the codebook $\mathbf{C}_i$. The quantized vector $\mathbf{q}$ is formed by concatenating all $\mathbf{q}_i$:

\begin{equation}  
\mathbf{q} = [\mathbf{q}_{1}, \mathbf{q}_{2}, \ldots, \mathbf{q}_{M}].
\end{equation}  

To ensure $\mathbf{q}$ closely approximates $\mathbf{s}$, we minimize the reconstruction loss:

\begin{equation}  
\mathcal{L}_{\text{recon}} = \| \mathbf{s} - \mathbf{q} \|_2^2.
\end{equation}  

After convergence, the learned codebook $\mathbf{C}$ and quantization method $\phi$ are retained for CTR prediction. Enhancements include Popularity-weighted Regularization and Contrastive Product Quantization, which improve the distribution of quantized features.

\subsubsection{Stage 2: Memory-Efficient Embedding for CTR Prediction}
Using the saved quantization method $\phi$, input features $\mathbf{x}$ are transformed into quantized codes $\mathbf{c}$, retrieving embeddings for CTR model training and inference:

\begin{equation}
\mathbf{c} = \phi(\mathbf{x}), \quad \mathbf{e} = \mathbf{E}_{\phi} \mathbf{c},
\end{equation}

where $\mathbf{E}_{\phi}$ is initialized from the retained codebook $\mathbf{C}$. The CTR model predicts click-through rate probability using the embedding vector $\mathbf{e}$:

\begin{equation}
\hat{y} = \psi(\mathbf{e}),
\end{equation}

where $\psi$ represents the feature interaction operations in the downstream CTR model. The prediction is optimized using a binary classification loss:

\begin{equation}
\mathcal{L}_{\text{binary}} = - (y\log(\hat{y}) + (1-y)\log(1-\hat{y})),
\end{equation}

where $y$ is the true label. By compressing the embedding table with the PQ codebook, we address memory consumption and enhance model inference efficiency and generalization.













    \subsection{Popularity-Weighted Regularization}
        \label{subsec:reg}

        In quantized CTR recommendation tasks, models often focus on memory efficiency, which may lead to imbalanced code allocation and overshadow critical high-frequency features. 
        Methods ignoring frequency fail to account for varying feature importance, causing representation issues. 
        However, introducing feature frequency directly usually overwhelms low-frequency features, leading to degraded performance. 
        
        To address these issues, we propose a popularity-weighted regularization method. We weigh features based on popularity, using a logarithmic transformation to smooth frequency disparities and calculate weights more effectively.
        The formula is as follows:
        \begin{equation}
        r_j = \lfloor \log_2 (n_j) \rfloor
        \end{equation}
        where $n_j$ denotes the frequency count of feature $j$.
        This allows the quantization model to adaptively distinguish features of different popularities. 
        However, this also causes the cluster centers to shift too much towards high-frequency features, making it difficult to effectively model low-frequency features. This is because, during the weighting process, although the weights of high-frequency features are smoothed, they still occupy a large proportion, thereby affecting the clustering effect of low-frequency features.

        To enhance the learning of low-frequency features, it is essential to incorporate regularization to balance the influences of high-frequency and low-frequency features. 
        We propose a loss function that integrates a regularization term to penalize imbalances in code allocation. 
        Given the efficacy of entropy-based tests in detecting uniformity \cite{dudewicz1981entropy}, we employ a modified entropy-based metric as the loss function to evaluate the uniformity of code distribution. This metric is selected for its capacity to measure the deviation from a uniform distribution, thereby facilitating fair code assignments.
        The regularization loss is calculated as follows:
        \begin{equation}
        \mathcal{L}_{\text{reg}} = \exp\left(-\sum_{i=1}^{K} p_i \log(p_i + \epsilon)\right), p_i = \frac{\sum_{j \in S_i} r_j}{\sum_{j \in S} r_j}
        \label{eq:loss_reg}
        \end{equation}
        where $K$ is the number of codes (embeddings), $\epsilon$ is a very small number (e.g., $1e-10$) to prevent the logarithm from being zero, $p_i$ is the probability of the $i$-th code, $S_i$ is the set of features assigned to code $i$, $S$ is the set of all features, and $r_j$ represents the popularity weighting of feature $j$. This calculation ensures that $p_i$ reflects the proportion of total feature frequency assigned to code $i$.
        Through this popularity-weighted regularization method, we can better balance the modeling of high-frequency and low-frequency features, thereby improving the overall performance.
        
        In summary, the popularity-weighted regularization method weights and smooths samples, allowing the quantization model to better handle features with different frequencies, avoiding excessive bias towards high-frequency features and neglect of low-frequency features. This method not only improves the accuracy of the model but also enhances its robustness and generalization ability.
        
    \subsection{Contrastive Product Quantization}
        \label{subsec:cons}
        
        While PQ embeddings and popularity-weighted regularization address code allocation imbalances, prior research \cite{letter} highlights that quantized embeddings often suffer from uneven distributions, leading to homogenization and reduced diversity.
        This lack of diversity hampers the model's ability to differentiate features, compromising CTR prediction accuracy. To address this, we integrate contrastive learning to enhance embedding diversity, ensuring balanced code allocations and improving recommendation performance.
        
        In contrastive learning, informative negative samples are crucial. We synthesize enhanced feature indices as negatives. However, fully synthetic indices may be too distant from real features. Thus, we create semi-synthetic codes based on real item codes to serve as effective hard negatives.
        
        Given a real feature code $\mathbf{c} = [j_1, j_2, \ldots, j_M]$, we randomly replace one index with a probability $\rho \in (0, 1)$ while keeping all others unchanged. This effective technique ensures the semi-synthetic codes resemble real features but still offer enough variability to act as challenging hard negative samples. The semi-synthetic code $\mathbf{z}$ is generated as follows:
        \begin{equation}
        G(\mathbf{z_i}) = 
        \begin{cases} 
        \text{Uni}(\{1, \ldots, K\}), & \text{if } X = 1 \\
        \mathbf{c_i}, & \text{if } X = 0
        \end{cases}
        \end{equation}
        where $X \sim \text{Bernoulli}(\rho)$, and $\text{Uni}(\cdot)$ uniformly samples item codes from the input set. This uniform sampling method guarantees that the code distribution of semi-synthetic indices is similar to that of real indices. The embedding of the real feature code $\mathbf{c}'$ and the corresponding semi-synthetic hard negative sample instance $\mathbf{z}'$ is given by:
        \begin{equation}
        \mathbf{c}' = \text{Emb-Pool}(\mathbf{c}), \mathbf{z}' = \text{Emb-Pool}(\mathbf{z}),
        \end{equation}
        where $\text{Emb-Pool}(\cdot)$ denotes the embedding lookup and aggregation. We use concatenation for aggregation here for simplicity.
    
        To incorporate contrastive learning into our framework, we define a contrastive loss function that encourages the model to distinguish between real item codes and their semi-synthetic hard negative samples. The contrastive loss $\mathcal{L}_{\text{con}}$ is defined as follows:
    
        \begin{equation}
        \mathcal{L}_{\text{con}} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(\mathbf{s}, \mathbf{c}'))}{\exp(\text{sim}(\mathbf{s}, \mathbf{c}')) + \sum_{j=1}^{K} \exp(\text{sim}(\mathbf{s}, \mathbf{z}'))},
        \label{eq:loss_con}
        \end{equation}
        where $N$ is the number of features, $\text{sim}(\cdot, \cdot)$ denotes a similarity function (e.g., cosine similarity), 
        $\mathbf{s}$ is the encoded vector,
        $\mathbf{c}'$ is the quantized embedding vector, and $\mathbf{z}'$ are the quantized embeddings of $K$ semi-synthetic hard negative samples.
        By minimizing this contrastive loss, the model learns to bring the embeddings of the encoded vector and quantized vector closer together while pushing the semi-synthetic hard negative samples further apart. This results in a more balanced and diverse code embedding distribution.
