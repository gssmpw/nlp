\section{Related Work}
\subsection{Unsupervised Representation Learning on Graphs}
\citep{jin2021automated} proposes a method for composing multiple self-supervised tasks for GNNs. They introduce a pseudo-homophily measure to evaluate representation quality without labels. \citep{zhang2021canonical} proposed the removal of negative sampling and MI estimator optimization entirely. The authors propose a loss function that contains an invariance term that maximizes correlation between embeddings of the two views and a decorrelation term that pushes different feature dimensions to be uncorrelated. \citep{hou2022graphmae} employs a re-mask decoding strategy and uses expressive GNN decoders instead of Multi-Layer Perceptrons(MLPs) enabling the model to learn more meaningful compressed representations. It lays focus on masked feature reconstruction rather than structural reconstruction. \citep{hassani2020contrastive} proposed MVGRL that makes use of two graph views and a discriminator to maximize mutual information between node embeddings from a first-order view and graph embeddings from a second-order view. It leverages both node and graph-level embeddings and avoids the need for explicit negative sampling. \citep{ju2022multi} proposed ParetoGNN which simultaneously learns from multiple pretext tasks spanning different philosophies. It uses a multiple gradient descent algorithm to dynamically reconcile conflicting learning objectives, showing state-of-the-art performance in node classification, clustering, link prediction and community prediction. 

\subsection{Bootstrapping Methods}
 \citep{ding2023eliciting} introduced  multi-scale feature propagation to capture long-range node interactions without oversmoothing. The authors also enhance inter-cluster separability and intra-cluster compactness by inferring cluster prototypes using a Bayesian non-parametric approach via Dirichlet Process Mixture Models(DPMMs). \citep{thakoor2021large} makes use of simple graph augmentations such as random node feature and edge masking, making it easier to implement on large graphs while achieving state-of-the-art results. It leverages a cosine similarity-based objective to make the predicted target representations closer to the true representations. \citep{jin2021multi} introduced a Siamese network architecture comprising an online and target encoder with momentum-driven update steps for the target. The authors propose 2 contrastive objectives i.e cross-network and cross view-contrastiveness. The Cross-network contrastive objective incorporates negative samples to push disparate nodes away in different graph views to effectively learn topological information.


\subsection{Joint Predictive Embedding Methods}
Joint predictive embedding has been explored in the field of computer vision and audio recognition. \citep{assran2023self} introduced I-JEPA which eliminates the need for hand-crafted augmentations and image reconstruction. They make use of a joint-embedding predictive model that predicts representations of masked image regions in an abstract feature space rather than in pixel space, allowing the model to focus on high-level semantic structures. It makes use of a Vision Transformer(ViT) with a multi-block masking strategy ensuring that the predictions retain semantic integrity. \citep{fei2023jepa}  extends the masked-modeling principle from vision to audio, enabling self-supervised learning on spectrograms. The key technical contribution is the introduction of a curriculum masking strategy, which transitions from random block masking to time-frequency-aware masking, addressing the strong correlations in audio spectrograms.