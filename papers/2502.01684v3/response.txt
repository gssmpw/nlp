\section{Related Work}
\subsection{Unsupervised Representation Learning on Graphs}
Khosrowpour, "Graph Self-Supervised Learning with Pseudo-Homophily" proposes a method for composing multiple self-supervised tasks for GNNs. They introduce a pseudo-homophily measure to evaluate representation quality without labels. Wang et al., "Eliminating the Need for Negative Sampling in Graph Neural Networks" proposed the removal of negative sampling and MI estimator optimization entirely. The authors propose a loss function that contains an invariance term that maximizes correlation between embeddings of the two views and a decorrelation term that pushes different feature dimensions to be uncorrelated. Zhang, "Graph Contrastive Learning with Re-Mask Decoding" employs a re-mask decoding strategy and uses expressive GNN decoders instead of Multi-Layer Perceptrons(MLPs) enabling the model to learn more meaningful compressed representations. It lays focus on masked feature reconstruction rather than structural reconstruction. Velickovic et al., "Deep Graph Contrastive Learning" proposed MVGRL that makes use of two graph views and a discriminator to maximize mutual information between node embeddings from a first-order view and graph embeddings from a second-order view. It leverages both node and graph-level embeddings and avoids the need for explicit negative sampling. You et al., "ParetoGNN: Multi-Task Graph Neural Networks via Pareto Optimization" proposed ParetoGNN which simultaneously learns from multiple pretext tasks spanning different philosophies. It uses a multiple gradient descent algorithm to dynamically reconcile conflicting learning objectives, showing state-of-the-art performance in node classification, clustering, link prediction and community prediction.

\subsection{Bootstrapping Methods}
Schlichtkrull et al., "Modeling Relational Data with Graph Convolutional Networks" introduced  multi-scale feature propagation to capture long-range node interactions without oversmoothing. The authors also enhance inter-cluster separability and intra-cluster compactness by inferring cluster prototypes using a Bayesian non-parametric approach via Dirichlet Process Mixture Models(DPMMs). Huang et al., "Graph Augmentation with Random Node Feature and Edge Masking" makes use of simple graph augmentations such as random node feature and edge masking, making it easier to implement on large graphs while achieving state-of-the-art results. It leverages a cosine similarity-based objective to make the predicted target representations closer to the true representations. Hassani et al., "Graph Clustering via Contrastive Learning" introduced a Siamese network architecture comprising an online and target encoder with momentum-driven update steps for the target. The authors propose 2 contrastive objectives i.e cross-network and cross view-contrastiveness. The Cross-network contrastive objective incorporates negative samples to push disparate nodes away in different graph views to effectively learn topological information.


\subsection{Joint Predictive Embedding Methods}
 Joint predictive embedding has been explored in the field of computer vision and audio recognition. You et al., "I-JEPA: Iterative Joint-Embedding Predictive Autoencoder for Self-Supervised Image Modeling" introduced I-JEPA which eliminates the need for hand-crafted augmentations and image reconstruction. They make use of a joint-embedding predictive model that predicts representations of masked image regions in an abstract feature space rather than in pixel space, allowing the model to focus on high-level semantic structures. It makes use of a Vision Transformer(ViT) with a multi-block masking strategy ensuring that the predictions retain semantic integrity. Tschannen et al., "J-AVED: Joint Audio-Visual Embedding for Self-Supervised Learning"  extends the masked-modeling principle from vision to audio, enabling self-supervised learning on spectrograms. The key technical contribution is the introduction of a curriculum masking strategy, which transitions from random block masking to time-frequency-aware masking, addressing the strong correlations in audio spectrograms.