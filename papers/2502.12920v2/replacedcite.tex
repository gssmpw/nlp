\section{Related Work}
\label{sec:related_work}
Currently, the main paradigm to construct (zero-shot) time series FMs is to collect a large pretraining set of time series data and then use it to learn an LLM-like transformer model ____. For example, \textit{Chronos} ____, \textit{Moirai} ____ and \textit{TimesFM} ____ are all time series FMs which are trained on large curated pretraining sets of time series, consisting of billions of training points, and whose backbones consist of LLM-based transformer architectures---such as Chronos, which uses the T5 architecture ____. Also, recently there have been new time series FMs which have gone against this trend by not using LLM architectures as their backbones. For instance, \textit{TTM} ____ uses the TSMixer architecture ____ as its backbone, which is specific to time series, resulting in a much smaller model size when compared to methods using LLM backbones. While, \textit{VisionTS} ____ uses a masked autoencoder ____ as its backbone and does not use time series data for pretraining, instead using images from the ImageNet dataset.

A major focus of this work is the exploitation of online feedback available at the deployment stage of a time series forecaster. The idea of leveraging online feedback in deployment to improve performance of an ML system has a long history ____. Currently, this concept falls within the domain of continual or lifelong learning for deep learning methods ____. Some of these works, for instance, investigate how to update a model online given the newly available data, which is either from the same distribution or from a shifting distribution ____. Importantly, much of the research on continual learning has focused on vision or text tasks ____, with comparatively little attention given to the time series domain ____. The studies that have explored continual learning for time series forecasting concentrate on methods for updating the weights of deep learning models ____. This has two problems in the context of time series: \textbf{a)} updating the weights of a deep learning model, especially of the size of a FM, at the frequency often required for time series data and with the resources usually available (e.g. CPUs) can often make such methods infeasible to use in practice ____. And \textbf{b)} online updating of deep models suffers from the problems of catastrophic forgetting and plasticity loss ____. Solutions to this currently require retraining on large amounts of historic data and complex, model-specific learning routines ____. This is in contrast to the focus of our work, which looks at the efficient online adaption of FM forecasts, so that it can be widely used in the real world.