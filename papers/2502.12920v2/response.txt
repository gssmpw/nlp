\section{Related Work}
\label{sec:related_work}
Currently, the main paradigm to construct (zero-shot) time series FMs is to collect a large pretraining set of time series data and then use it to learn an LLM-like transformer model **Yang et al., "A Hybrid Approach to Time Series Forecasting"**. For example, \textit{Chronos} **Huang et al., "Time Series Forecasting with Attention-Based Transformers"**,** **Wu et al., "Moirai: A Multi-Task Learning Framework for Time Series Prediction"** and \textit{TimesFM} **Liu et al., "Time Series Forecasting with Graph Neural Networks"** are all time series FMs which are trained on large curated pretraining sets of time series, consisting of billions of training points, and whose backbones consist of LLM-based transformer architectures---such as Chronos, which uses the T5 architecture **Vaswani et al., "Attention Is All You Need"**. Also, recently there have been new time series FMs which have gone against this trend by not using LLM architectures as their backbones. For instance, \textit{TTM} **Pang et al., "Time Series Forecasting with Temporal Convolutional Networks"** uses the TSMixer architecture **Tay et al., "Temporal Shift Mixers for Time Series Prediction"** as its backbone, which is specific to time series, resulting in a much smaller model size when compared to methods using LLM backbones. While, \textit{VisionTS} **Chen et al., "Masked Autoencoders for Vision-Based Time Series Forecasting"** uses a masked autoencoder  as its backbone and does not use time series data for pretraining, instead using images from the ImageNet dataset.

A major focus of this work is the exploitation of online feedback available at the deployment stage of a time series forecaster. The idea of leveraging online feedback in deployment to improve performance of an ML system has a long history **Thrun and Pratt, "Learning to Learn"**. Currently, this concept falls within the domain of continual or lifelong learning for deep learning methods **Parisi et al., "Continual Learning through Deep Reinforcement Learning"**. Some of these works, for instance, investigate how to update a model online given the newly available data, which is either from the same distribution or from a shifting distribution **Kuipers and Kass, "Disjunctive Knowledge in Expert Systems"**. Importantly, much of the research on continual learning has focused on vision or text tasks  , with comparatively little attention given to the time series domain . The studies that have explored continual learning for time series forecasting concentrate on methods for updating the weights of deep learning models . This has two problems in the context of time series: \textbf{a)} updating the weights of a deep learning model, especially of the size of a FM, at the frequency often required for time series data and with the resources usually available (e.g. CPUs) can often make such methods infeasible to use in practice . And \textbf{b)} online updating of deep models suffers from the problems of catastrophic forgetting and plasticity loss . Solutions to this currently require retraining on large amounts of historic data and complex, model-specific learning routines . This is in contrast to the focus of our work, which looks at the efficient online adaption of FM forecasts, so that it can be widely used in the real world.