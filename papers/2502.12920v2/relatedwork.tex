\section{Related Work}
\label{sec:related_work}
Currently, the main paradigm to construct (zero-shot) time series FMs is to collect a large pretraining set of time series data and then use it to learn an LLM-like transformer model \citep{rasul2023lag,chen2024visionts,liang2024foundation}. For example, \textit{Chronos} \citep{Ansari2024Chronos}, \textit{Moirai} \citep{Woo2024moirai} and \textit{TimesFM} \citep{Das2024TimesFM} are all time series FMs which are trained on large curated pretraining sets of time series, consisting of billions of training points, and whose backbones consist of LLM-based transformer architectures---such as Chronos, which uses the T5 architecture \citep{roberts2019exploring}. Also, recently there have been new time series FMs which have gone against this trend by not using LLM architectures as their backbones. For instance, \textit{TTM} \citep{Ekambaram2024Tiny} uses the TSMixer architecture \cite{ekambaram2023tsmixer} as its backbone, which is specific to time series, resulting in a much smaller model size when compared to methods using LLM backbones. While, \textit{VisionTS} \citep{chen2024visionts} uses a masked autoencoder \citep{he2022masked} as its backbone and does not use time series data for pretraining, instead using images from the ImageNet dataset.

A major focus of this work is the exploitation of online feedback available at the deployment stage of a time series forecaster. The idea of leveraging online feedback in deployment to improve performance of an ML system has a long history \citep{hoi2021online, polikar2001learn++, bottou2003large}. Currently, this concept falls within the domain of continual or lifelong learning for deep learning methods \citep{de2021continual, wang2024comprehensive}. Some of these works, for instance, investigate how to update a model online given the newly available data, which is either from the same distribution or from a shifting distribution \citep{aljundi2019task, lee2024chunking}. Importantly, much of the research on continual learning has focused on vision or text tasks \citep{qu2021recent, wu2024continual}, with comparatively little attention given to the time series domain \citep{besnard2024continual}. The studies that have explored continual learning for time series forecasting concentrate on methods for updating the weights of deep learning models \citep{ao2023continual, pham2022learning, zhang2024addressing}. This has two problems in the context of time series: \textbf{a)} updating the weights of a deep learning model, especially of the size of a FM, at the frequency often required for time series data and with the resources usually available (e.g. CPUs) can often make such methods infeasible to use in practice \cite{diao2024forecasting, Ekambaram2024Tiny}. And \textbf{b)} online updating of deep models suffers from the problems of catastrophic forgetting and plasticity loss \citep{kirkpatrick2017overcoming, dohare2023maintaining, de2021continual}. Solutions to this currently require retraining on large amounts of historic data and complex, model-specific learning routines \citep{yang2024recent}. This is in contrast to the focus of our work, which looks at the efficient online adaption of FM forecasts, so that it can be widely used in the real world.