%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
%
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{subcaption}

\usepackage{wrapfig}


%
%
%
%
\usepackage{hyperref}


%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[preprint]{icml2025}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{newtxmath}

\input{math_commands}

%
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{caption}
\captionsetup{size=small}

\renewcommand{\eqref}[1]{(\ref{#1})}

\usepackage[createShortEnv,conf={no link to proof, text link},commandRef=Cref]{proof-at-the-end}

\usepackage[textsize=tiny]{todonotes}


%
%

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}

\makeatletter
\crefname{section}{\S\@gobble}{\S\@gobble}
\crefname{subsection}{\S\@gobble}{\S\@gobble}
\crefname{proposition}{Prop.}{Props.}
\crefname{figure}{Fig.}{Figs.}

% \parskip=5pt
% \linespread{0.98}
% \newcommand{\zerodisplayskips}{%
%   \setlength{\abovedisplayskip}{1mm}%
%   \setlength{\belowdisplayskip}{1mm}%
%   \setlength{\abovedisplayshortskip}{0.5mm}%
%   \setlength{\belowdisplayshortskip}{0.5mm}}
% \appto{\normalsize}{\zerodisplayskips}
% \appto{\small}{\zerodisplayskips}
% \appto{\footnotesize}{\zerodisplayskips}


\renewcommand{\paragraph}[1]{\textbf{#1}}
% \def\section{\@startsection{section}{1}{\z@}{-0.05in}{0.02in}
%              {\large\bf\raggedright}}
% \def\subsection{\@startsection{subsection}{2}{\z@}{-0.05in}{0.01in}
%                 {\normalsize\bf\raggedright}}
% \def\subsubsection{\@startsection{subsubsection}{3}{\z@}{-0.05in}{0.01in}
%                 {\normalsize\sc\raggedright}}
\makeatother

\icmltitlerunning{Outsourced Diffusion Sampling: Efficient Posterior Inference in Latent Spaces of Generative Models}

\begin{document}

\twocolumn[
%
\icmltitle{Outsourced Diffusion Sampling:\\Efficient Posterior Inference in Latent Spaces of Generative Models}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Siddarth Venkatraman}{equal,mila,udem}
\icmlauthor{Mohsin Hasan}{equal,mila,udem}
\icmlauthor{Minsu Kim}{mila,udem,kaist}
\icmlauthor{Luca Scimeca}{mila,udem}
\icmlauthor{Marcin Sendera}{jag}
\icmlauthor{Yoshua Bengio}{mila,udem,cifar}
\icmlauthor{Glen Berseth}{mila,udem,cifar}
\icmlauthor{Nikolay Malkin}{edin}
%
%
\end{icmlauthorlist}

\icmlaffiliation{mila}{Mila -- Qu\'ebec AI Institute}
\icmlaffiliation{udem}{Universit\'e de Montr\'eal}
\icmlaffiliation{kaist}{KAIST}
\icmlaffiliation{jag}{Jagiellonian University}
\icmlaffiliation{cifar}{CIFAR}
\icmlaffiliation{edin}{University of Edinburgh}

\icmlcorrespondingauthor{Siddarth Venkatraman}{siddarth.venkatraman@mila.quebec}
\icmlcorrespondingauthor{Mohsin Hasan}{mohsin.hasan@mila.quebec}

%
%
%
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

%

%
%
%
%
%

%
\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
\looseness=-1
Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous (\emph{`outsourced'}) Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. 
In such a model (\eg, a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable.
We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_\theta(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). 
For many models and constraints of interest, the posterior in the noise space is smoother than the posterior in the data space, making it more amenable to such amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably both with current amortized and non-amortized inference methods. We demonstrate the proposed \emph{outsourced diffusion sampling} in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.\vspace*{-1em}
\end{abstract}




\section{Introduction}

\newcommand{\yes}{\checkmark}
\newcommand{\no}{\texttimes}

\begin{figure*}
    \centering
    \vspace*{-0.5em}
    \begin{minipage}{0.40\linewidth}
    \caption{\looseness=-1\textbf{Left:} \textit{Top row:} Marginal densities of a CNF that transforms a Gaussian distribution ($t=0$) to a Swiss roll ($t=1$). \textit{Middle row:} The constraint function -- a mixture of two Gaussians centered an an observation $\mathbf{y}$ ($\bullet$) and its reflection through the origin -- pulled back to $\mathbf{x}_t$. \textit{Bottom row:} Posterior densities at $\mathbf{x}_t$, proportional to the product of the first two rows. The \textit{rightmost column} shows samples in the data space. \textbf{Right:} The same objects shown in noise and data space for a GAN that transforms noise ($\mathbf{z}$) to data ($\textbf{x}$). \textbf{Outsourced diffusion samplers} approximate $p(\mathbf{x}_0\mid \mathbf{y})$ or $p(\mathbf{z}\mid \mathbf{y})$, which are smoother than $p(\mathbf{x}\mid\mathbf{y})$ (see \cref{fig:figure2}).\label{fig:figure1}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.595\linewidth}
    \hspace*{-1mm}
    \includegraphics[height=1.75in,trim=40 30 30 10, clip]{figures/toy/flow.pdf}~\hspace{-1.5mm}~\includegraphics[height=1.75in,trim=25 30 136 10, clip]{figures/toy/gan.pdf}~\includegraphics[height=1.75in,trim=131 30 30 10, clip]{figures/toy/gan.pdf}
    \end{minipage}
\end{figure*}

\begin{table}[t]
    \caption{Common families of generative models can be expressed as deterministic transformations of noise. Here, $d_{\rm latent}$ refers to the dimension of the latent variable as typically understood in the model; typically, $d_{\rm latent} \ll d_{\rm data}$ (\cref{sec:outsourced_noise}). Posteriors under these generative model priors are generally intractable, but some models can be fine-tuned by asymptotically unbiased variational objectives to sample intractable posteriors (\cref{sec:existing_methods}). Outsourced diffusion sampling approximates posteriors in the noise space (\cref{sec:latent_space_posterior,sec:outsourced_diffusion_sampling}).}
    \label{tab:outsourcing}
    \centering
    \resizebox{1\linewidth}{!}{
        \begin{tabular}{@{}lccc}
            \toprule
            Model 
            & Noise dim. $d_{\rm noise}$
            & Invertible? 
            & Variational tuning?
            \\\midrule
            (H)VAE
            & $d_{\rm latent}\cdot N+d_{\rm data}$
            & \no 
            & \yes
            \\
            GAN
            & $d_{\rm latent}$
            & \no 
            & \no \\
            NF / CNF
            & $d_{\rm data}$
            & \yes 
            & \yes/\no
            \\
            Diffusion
            & $d_{\rm data}\cdot(T+1)$
            & \no 
            & \yes
            \\
            Latent diffusion
            & $d_{\rm latent}\cdot(T+1)+d_{\rm data}$
            & \no
            & \yes \\
            \bottomrule
        \end{tabular}
    }
\end{table}


%
%
%
%

Generative models, trained on a dataset to maximize likelihood or related quantities, can become priors for Bayesian inference problems. The aim is to approximate or sample from the product of the modeled distribution over a data space with an observation likelihood or other constraint function. 
While such diverse applications as conditional generation \citep{dhariwal2021diffusion, ho2022classifier}, inverse problems \citep{song2022solving, chung2023diffusion, venkatraman2024amortizing}, and constrained improvement from human feedback \citep{korbak2022rlklpenaltiesbetter, fan2023reinforcement} can be cast as posterior inference tasks, sampling from such posterior distributions \emph{when no unbiased target data is available} is generally intractable. For some model families, approximate solutions, such as MCMC, approximate guidance, and variational inference, may be possible. Each of those methods has limitations, such as high cost to reach convergence for multimodal posteriors, intractability of accurate density estimation, and reliance on techniques specialized to the model and constraint.
%

Fundamentally, generative models are probabilistic programs that produce samples from the distributions they define by a combination of deterministic computation and injection of random noise.\footnote{We refer here to models that produce samples in a bounded number of operations, not to objects such as deep energy-based models, for which sampling is intractable.} 
This paper argues that the noise space of generative models -- the space where the noise injected during generation resides -- is an effective target for posterior inference. To be precise, we consider a generative model that expresses data as a deterministic transformation of noise, $\mathbf{x}=f_\theta(\mathbf{z})$, where the noise variable $\mathbf{z}$ follows a known distribution $\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})$ and $\theta$ are the model parameters. This model defines a distribution over $\mathbf{x}$ -- the pushforward $[f_\theta]_*p_\mathbf{z}$ of the noise distribution by the deterministic transformation -- with density $p(\mathbf{x})$. A constraint function $r(\mathbf{x},\mathbf{y})$ in the data space, depending on an auxiliary variable $\mathbf{y}$, defines a posterior distribution $p(\mathbf{x}\mid\mathbf{y})\propto p(\mathbf{x})r(\mathbf{x},\mathbf{y})$. This posterior can be sampled by inferring a distribution over the noise variable $\mathbf{z}$ that, when transformed by $f_\theta$, aligns with the posterior in the data space. The posterior in the noise space is often smoother (and lower-dimensional) than the corresponding posterior in the data space (\cref{fig:figure1}), making it more amenable to efficient sampling (\cref{fig:figure2}).
%

\looseness=-1
While posterior sampling in noise space is still intractable, it can be addressed by methods of black-box variational inference. Recent advances in \emph{diffusion samplers} -- diffusion models trained not on a dataset, but to match a given unnormalized density \citep{zhang2021path, vargas2023denoising, richter2024improved, sendera2024improved} -- open an opportunity to model complex posteriors in noise space. We call such amortized posterior inference in the noise space \emph{outsourced diffusion sampling}. 

Our exposition and experiments support three claims: 
\begin{enumerate}[left=0pt,nosep,label=(\arabic*)]
    \item Outsourced diffusion sampling is agnostic to the form of the mapping $f_\theta$ and applicable to a wide range of prior models, including VAEs, GANs, normalizing flows, and continuous-time flow-based models (\cref{tab:outsourcing}).
%
    \item Outsourced diffusion sampling is an effective posterior inference method under large pretrained generative model priors in a variety of domains: conditional image generation, reinforcement learning with human feedback, discriminator-adjusted GAN sampling, and protein structure generation (\cref{tab:exp}).
    \item Outsourced diffusion sampling is more efficient than amortized inference methods that fit a model to sample the data space posterior directly and than non-amortized methods like MCMC, illustrating the flexibility of diffusion sampling in outsourced noise spaces for sampling complex posteriors (\cref{sec:experiments}).
\end{enumerate}
    
%
%

\begin{figure}
    \centering
    \includegraphics[width=\linewidth,trim=10 10 10 10,clip]{figures/toy/diff.pdf}\vspace*{-1em}
    \caption{Marginal densities of a diffusion sampler of the posteriors from the CNF example in \cref{fig:figure1} in data space and noise space. The data space posterior (top row) has well-separated modes and is harder to sample from than the outsourced posterior (bottom row).}
    \label{fig:figure2}
\end{figure}


\section{Outsourcing Noise in Generative Models}
\label{sec:outsourced_noise}

Consider a probabilistic model over a variable $\mathbf{x}$ taking values in $\mathbb{R}^{d_{\rm data}}$, with auxiliary latent variables $\mathbf{w}$ valued in $\mathbb{R}^{d_{\rm latent}}$. The model is a joint distribution over $\mathbf{x}$ and $\mathbf{w}$, and it induces a distribution over $\mathbf{x}$, its marginalization over $\mathbf{w}$. In terms of densities (if they exist), if $p(\mathbf{w},\mathbf{x})$ is the joint density, then the marginal density of $\mathbf{x}$ is $p(\mathbf{x})=\int p(\mathbf{w},\mathbf{x})\,d\mathbf{w}$.

A form of the \emph{noise outsourcing lemma} \citep[see, \eg,][]{austin2015exchangeable} states that, under basic assumptions, any such model is equivalent to one augmented with additional latent variables $\mathbf{w}'$, independent of $\mathbf{w}$ and following a fixed distribution, such that $\mathbf{x}$ is a (deterministic) function of $\mathbf{w}$ and $\mathbf{w}'$. In particular, if $\mathbf{w}$ and $\mathbf{w}'$ are both standard Gaussian, then $\mathbf{x}$ is a deterministic function of a Gaussian noise variable $\mathbf{z}$ (the concatentation of $\mathbf{w}$ and $\mathbf{w}'$), called the \emph{`outsourced'} noise:
\begin{proposition}[Noise outsourcing lemma for Gaussians]
    Let $\mathbf{w}$ and $\mathbf{x}$ be Borel-measurable random variables valued in $\mathbb{R}^{d_{\rm latent}}$ and $\mathbb{R}^{d_{\rm data}}$, respectively, with $\mathbf{w}$ marginally standard Gaussian, and let $d_{\rm noise}>d_{\rm latent}$. There exists a random variable $\mathbf{z}$ in $\mathbb{R}^{d_{\rm noise}}$ such that:
    \begin{enumerate}[left=0pt,nosep,label=(\arabic*)]
        \item $\mathbf{z}$ is standard Gaussian;
        \item $\mathbf{w}$ is the projection of $\mathbf{z}$ onto its first $d_{\rm latent}$ coordinates;
        \item there exists a measurable function $f:\mathbb{R}^{d_{\rm noise}}\to\mathbb{R}^{d_{\rm data}}$ such that $(\mathbf{w},\mathbf{x})=(\mathbf{w},f(\mathbf{z}))$ almost surely.
    \end{enumerate}
    \label{prop:noise_outsourcing}
\end{proposition}
\Cref{prop:noise_outsourcing} ensures that any generative model with marginally Gaussian latent variables can be rewritten as a deterministic function of a higher-dimensional Gaussian noise variable, but does not specify the form of the function $f$ (which is very non-unique). We will be interested in modeling Bayesian posteriors over $\mathbf{x}$ given observations by pulling them back to the noise variable $\mathbf{z}$, using methods agnostic to the form of $f$, which is seen as a black-box transformation. 

Common families of generative models have a natural form for $f$, obtainable explicitly from their latent variable structure, that we will exploit. We now explain how several model families, in their basic form, can be expressed as deterministic transformations of noise $\mathbf{z}$ following a Gaussian distribution over $\mathbb{R}^{d_{\rm noise}}$. See \cref{tab:outsourcing} for a summary.

\paragraph{Variational autoencoders \citep[VAEs;][]{kingma2014autoencoding}.} The generative model in a simple VAE may have the form $\mathbf{x}\sim\gN(\boldsymbol{\mu}_\theta(\mathbf{w}),\sigma^2_\theta(\mathbf{w})I_{d_{\rm data}})$, where $\mathbf{w}$ follows a Gaussian distribution in $\mathbb{R}^{d_{\rm latent}}$ and $\boldsymbol{\mu}_\theta$ and $\sigma_\theta$ are neural networks outputting a vector and scalar, respectively. This model may be reparametrized as 
\begin{equation}
    \mathbf{x}=\boldsymbol{\mu}_\theta(\mathbf{w})+\sigma_\theta(\mathbf{w})\xi,
    \label{eq:outsourced_vae}
\end{equation}    
where $\xi\sim\gN(0,I_{d_{\rm data}})$. Thus $\mathbf{x}$ is a deterministic transformation of the concatenation of $\mathbf{w}$ and $\xi$, which follows a Gaussian distribution in $\mathbb{R}^{d_{\rm latent}+d_{\rm data}}$. (The encoder, an auxiliary object used in training the VAE, does not form part of the generative model.) \textbf{Hierarchical VAEs \citep[HVAEs;][]{JimenezRezende2014StochasticBA}}, generalize VAEs, using a Markovian chain of latent variables in the generative process \ie, a graphical model structure of $\mathbf{w}_N\rightarrow\dots\rightarrow\mathbf{w}_1\rightarrow\mathbf{x}$ with each transition a conditional Gaussian distribution. If these variables are all $\mathbb{R}^{d_{\rm latent}}$-valued, then $\mathbf{x}$ can be similarly reparametrized as a function of the $N$ $d_{\rm latent}$-dimensional Gaussian noises injected on each transition $\mathbf{w}_{i+1}\rightarrow\mathbf{w}_i$ and the $d_{\rm data}$-dimensional noise on the last step, as in \eqref{eq:outsourced_vae}.

\looseness=-1
\paragraph{Generative adversarial networks \citep[GANs;][]{NIPS2014_5ca3e9b1}.} In a GAN, a generator $G_\theta$ maps Gaussian-distributed noise $\mathbf{z}\sim\gN(0,I_{d_{\rm latent}})$ deterministically to data, $\mathbf{x}=G_\theta(\mathbf{z})$. Thus a GAN is naturally a model with outsourced noise in $\mathbb{R}^{d_{\rm latent}}$. (The discriminator is an auxiliary object used in training, not a part of the generative model.)

\looseness=-1
\paragraph{Normalizing flows \citep[NFs;][]{rezende2015variational}.} In a NF -- also naturally a generative model with outsourced noise -- the generator $f_\theta$ maps Gaussian noise $\mathbf{z}\sim\gN(0,I_{d_{\rm data}})$ to data $\mathbf{x}$ deterministically, $\mathbf{x}=f_\theta(\mathbf{z})$. Unlike a GAN generator, the function $f_\theta$ is constrained to be invertible and necessarily (in order to model a full-support distribution) must have noise of the same dimension as the data.

\paragraph{Continuous normalizing flows \citep[CNFs;][]{chen2018neural}} A CNF is an invertible transformation from noise $\mathbf{z}=\mathbf{x}_0$ to data $\mathbf{x}=\mathbf{x}_1$ that is the solution of a neural ordinary differential equation (ODE) \[d\mathbf{x}_t=v_\theta(\mathbf{x}_t,t)\,dt.\] 
This includes ODEs derived from diffusion models, \eg, DDIMs \citep{song2021denoising}, and those trained with flow matching, the family of methods introduced by \citet{lipman2023flow,albergo2023building,liu2023flow}. 

Under regularity conditions on $v_\theta$, a distribution over initial conditions $\mathbf{x}_0$ induces marginal distributions over $\mathbf{x}_t$ for $t>0$, and in particular over the data variable $\mathbf{x}_1$. The CNF is a generative model with outsourced noise variable $\mathbf{z}=\mathbf{x}_0$. 

\paragraph{Diffusion models.} Diffusion models \citep{sohl2015diffusion, ho2020ddpm} and latent diffusion models \citep{rombach2021high} can also be expressed as deterministic transformations of noise; see \cref{sec:diffusion_appendix} for discussion and connections.

\section{Posteriors under Generative Model Priors}
\label{sec:posteriors}

%
For a generative model $p(\mathbf{x})$ of any of the types described in \cref{sec:outsourced_noise}, and a positive constraint function $r(\mathbf{x},\mathbf{y})$ such that $\E_{\mathbf{x}\sim p(\mathbf{x})}[r(\mathbf{x},\mathbf{y})]$ is finite, we are interested in sampling the posterior distribution $p(\mathbf{x}\mid\mathbf{y})\propto p(\mathbf{x})r(\mathbf{x},\mathbf{y})$. Various sources of constraints will be described in \Cref{sec:experiments} (see \cref{tab:exp}).

\subsection{Posterior Sampling and Approximation}
\label{sec:existing_methods}

In this section, we describe existing methods for sampling approximately from such intractable posteriors.

\looseness=-1
\paragraph{Model-agnostic methods.} The most general methods for sampling from distributions defined by unnormalized densities are Markov chain Monte Carlo (MCMC) methods. These methods may not require fitting parametric models, although hybrid methods -- such as  adaptive importance sampling \citep{bugallo2017adaptive}, twisted SMC variants \citep{maddison2018twisted,lawson2022sixo}, and neural boostrap algorithms \citep{midgley2022flow} -- can accelerate their convergence.  

MCMC methods are agnostic to the form of the target distribution and are guaranteed to converge to it under mild conditions in the limit of infinite time (or memory in the case of particle filtering methods like SMC \citep{del2006sequential,doucet2009tutorial}), making them anytime algorithms that can trade off computation cost for accuracy. However, MCMCs assume access to the target density and possibly to its gradient, limiting their applicability:
\begin{itemize}[left=0pt,nosep]
    \item For \textbf{(H)VAEs} and their special case \textbf{diffusion models}, the density cannot be computed exactly; only variational bounds are available. 
    \item For \textbf{GANs}, the density cannot be computed because the generator is not injective (invertible) and may not even define a full-support distribution over the target space.  
    \item For \textbf{CNFs}, the density can be approximated using the Hutchinson trace estimator \citep{hutchinson1989, grathwohl2018ffjord}, but accurately computing the gradient is expensive, as it requires backpropagating through the computation graph of a neural ODE integrator.
\end{itemize}

\paragraph{Monte Carlo methods in latent space.}
MCMC sampling can be performed at intermediate time points of continuous-time flow-based models \citep{cabezas2024markovian}. MCMC techniques are also used for sampling in GAN latent spaces for discriminator-guided sampling \citep{che2020gan,hou2025wlgan} and for conditional generation by sampling in intermediate activation spaces \citep{nguyen2017plug}. Similar approaches are applied to normalizing flows \citep{coeurdoux2024normalizing}. Monte Carlo techniques are also used in diffusion models (\cref{sec:diffusion_appendix}).

\paragraph{Amortized inference and fine-tuning.} For some families of models, it is possible to train a model that, at convergence to the global optimum, samples from the posterior distribution exactly. This is a problem of variational inference: the model is trained -- or perhaps fine-tuned using the prior model as initialization -- to be close to the target distribution in some measure of divergence.

For \textbf{CNFs}, a method for fine-tuning the drift function $v_\theta$ to yield a CNF that samples from the posterior distribution, known as \emph{adjoint matching}, has recently been proposed by \citet{domingoenrich2025adjoint}. While this method is asymptotically unbiased, it requires access to the gradient of the likelihood function. Furthermore, it is only applicable to a narrow class of flow-based models, namely, those that are trained from certain marginal couplings and interpolants and closely related to the probability flow ODEs of diffusion models. This restrictiveness is due to adjoint matching converting the neural ODE to an equivalent neural SDE\footnote{In diffusion models (SDEs), the prior model can also be fine-tuned to sample from the posterior using objectives closely related to those proposed here; see \cref{sec:diffusion_appendix}.}, which is not possible in general (\eg, for flow-based models trained using minibatch optimal transport couplings \citep{tong2024improving,pooladian2023multisample} or with non-Gaussian source distributions). Na\"ively applying adjoint matching to such CNFs gives biased results (\cref{fig:moons}).



\subsection{Bayesian Posterior in Noise Space}
\label{sec:latent_space_posterior}

We describe how posterior distributions can be pulled back to the noise space of a generative model expressed as a deterministic transformation of an outsourced variable. This relies on a basic measure-theoretic fact regarding the transformation of density functions under pushforward measures:
\begin{propositionE}[][end,restate]\label{prop:pushforward}\vspace*{-1em}
    Suppose that $(Z,\Sigma_Z)$ and $(X,\Sigma_X)$ are measurable spaces and $f:Z\to X$ is measurable. If $\mu$ is a $\sigma$-finite measure on $Z$ and $\nu$ is a $\sigma$-finite measure on $X$ with $\nu\ll f_*\mu$, then
    $f_*\left(\left(\frac{d\nu}{df_*\mu}\circ f\right)\cdot\mu\right)=\nu$.

    In particular, if $\mu$ is a probability measure and $h:X\to\mathbb{R}_{\geq0}$ is $f_*\mu$-integrable, then $\lambda:=\frac{1}{\int h\,d(f_*\mu)}(h\circ f)\cdot\mu$ is a probability measure on $Z$, and $f_*\lambda=\frac{1}{\int h\,d(f_*\mu)}(h\cdot f_*\mu)$ is a probability measure on $X$.
\end{propositionE}
\begin{proofE}
    Let $h=\frac{d\nu}{df_*\mu}$; we must show that $f_*((h\circ f)\cdot\mu)=h\cdot f_*\mu$. Let $E\in\Sigma_X$ and $D=f^{-1}(E)\in\Sigma_Z$. By definitions and properties of pushforward measures,
    \begin{align*}
        f_*((h\circ f)\cdot\mu)(E)
        &=((h\circ f)\cdot\mu)(D)\\
        &=\int_Z(h\circ f)\boldsymbol{1}_D\,d\mu\\
        &=\int_Z(h\boldsymbol{1}_E)\circ f\,d\mu\\
        &=\int_Xh\boldsymbol{1}_E\,df_*\mu \\
        &=(h\cdot f_*\mu)(E),
    \end{align*}
    as required.

    For the second part of the proposition, if $\nu=h\cdot f_*\mu$, then $\frac{d\nu}{df_*\mu}=h$ $f_*\mu$-almost everywhere, and the result follows easily from the first part by linearity of the pushforward.
\end{proofE}
\looseness=-1
(See \cref{sec:proof} for the proof.) In common terms, in terms of densities, the relevance of the proposition of our setting is as follows. Let $f:\mathbb{R}^{d_{\rm noise}}\to\mathbb{R}^{d_{\rm data}}$ be a function from the noise space to the data space. A prior density $p(\mathbf{z})$ in the noise space, transformed via $\mathbf{x}=f(\mathbf{z})$, defines a prior $p(\mathbf{x})$ (a density with respect to some reference measure, such as the volume measure on the image of $f$). If $h(\mathbf{x})=r(\mathbf{x},\mathbf{y})$ is a constraint function with $\int r(\mathbf{x},\mathbf{y})p(\mathbf{x})\,d\mathbf{x}<\infty$, then if $\mathbf{z}'$ is a variable in the noise space distributed with density proportional to $p(\mathbf{z}')r(f(\mathbf{z'}),\mathbf{y})$, and $\mathbf{x}'=f(\mathbf{z}')$, then $\mathbf{x}'$ is distributed with density proportional to $p(\mathbf{x'})r(\mathbf{x'},\mathbf{y})$ in the data space. This means that to sample the posterior in latent space, we can sample from the posterior in noise space (with density $p(\mathbf{z})r(f(\mathbf{z}),\mathbf{y})$) and transform the sample by $f$.
(Note that such sampling does not require computation of the pushforward density. Indeed, $f$ need not be injective or smooth, as would typically be required for such computations.)

\paragraph{Amortizing outsourced posterior sampling.}
Although noise space posteriors might be simpler than the distribution in target space (\cref{fig:figure1,fig:figure2}), they can still be multimodal and high-dimensional. MCMC methods have been used to sample from noise spaces of NFs and GANs \citep{che2020gan, cannella2021projectedlatentmarkovchain}, but suffer from long mixing times. In addition, many MCMC methods assume that the target density $p(\mathbf{z})r(f(\mathbf{z}),\mathbf{y})$ is (efficiently) differentiable, which is not the case when $f$ is a CNF.

Instead, it can be desirable to use amortized variational inference to fit a fast sampler to the latent posterior, that is, to approximate it by a parametric model. We have no samples from this posterior, but have access to its unnormalized density $R(\mathbf{z}\mid\mathbf{y})\coloneqq p(\mathbf{z})r(f(\mathbf{z}),\mathbf{y})$. 

We call such a model an \emph{outsourced sampler}, a name motivated by the fact that the factors of variation in the posterior are `outsourced' to the noise space via the pullback operation. We shall use diffusion models as the variational family, as will be discussed in \cref{sec:diffusion_noise_space}. 



\section{Outsourced Diffusion Sampling}
\label{sec:outsourced_diffusion_sampling}

\subsection{Diffusion Samplers for Amortized Inference}
\label{sec:diffusion_samplers}

\emph{Diffusion sampling} is the variational inference problem of approximating a distribution over $\mathbb{R}^d$, with a given unnormalized density $R:\mathbb{R}^d\to\mathbb{R}_{>0}$, by a diffusion model. Samples from the target distribution, which has density $p_{\rm target}(\mathbf{z})=\frac1ZR(\mathbf{z})$, are not available, nor do we have access to the normalizing constant $Z$; however, we have the ability to query for the unnormalized density $R(\mathbf{z})$ at any point $\mathbf{z}$. The goal is to train a neural stochastic differential equation
\begin{equation}\label{eq:neural_sde}
    \mathbf{z}_0\sim\gN(0,I_d)
    \quad
    d\mathbf{z}_t=u_\phi(\mathbf{z}_t,t)\,dt+\sigma_t\,d\mathbf{B}_t,
\end{equation}
where $u_\phi$ is a neural network, $\sigma_t$ is a scalar function of time, and $\mathbf{B}_t$ is standard Brownian motion, so that the induced distribution over $\mathbf{z}_1=\mathbf{z}$ is close to the target distribution $p_{\rm target}(\mathbf{z})$ in some measure of divergence. (Note that, unlike for diffusion models trained from data, it is standard for generation to proceed in increasing time (from noise at $t=0$ to the target at $t=1$.) The model can be sampled by simulating \eqref{eq:neural_sde} in a time discretization, \eg, using the Euler-Maruyama method.

Training diffusion samplers is more difficult than training typical diffusion models, \ie, maximizing a variational bound on log-likelihood of a dataset. Various objectives have been proposed, including: (1) ones that rely on differentiable simulation of the generative process during training \citep{li2020scalable,kidger2021efficient,zhang2021path,vargas2023denoising} and are linked with optimal control \citep{berner2022optimal,vargas2024transport}, (2) ones using biased but asymptotically consistent Monte Carlo estimates of the score function (\citet{vargas2022bayesian,huang2024reverse,akhoundsadegh2024iterated}), and (3) `off-policy' divergences that can be optimized on arbitrary generative trajectories not necessarily sampled from the current iteration of the model \citep[][\emph{inter alia}]{richter2020vargrad,nusken2021solving,sendera2024improved}. A unifying perspective on these methods and analysis in the continuous-time limit were recently given by \citet{berner2025discrete}.
%

In this work, we adopt training methods of the third kind, using off-policy divergences, as they have have two notable advantages. First, they treat the target $R$ as a black-box reward function and do not require access to the score $\nabla\log R$, as differentiable simulation methods and some Monte Carlo methods do. Second, they can be trained off-policy, on trajectories obtained through exploration, allowing the flexible use of exploration strategies and thus promoting mode discovery, as demonstrated for continuous-space samplers by \citet{malkin2023gflownets,sendera2024improved,phillips2024metagfn,kim2025adaptive}. The exact form of the off-policy divergence will be described in \cref{sec:diffusion_noise_space}. %

\begin{figure*}[t]
  \centering
  \begin{minipage}{0.43\textwidth} %
    \caption{\looseness=-1\textbf{(a)} Flow paths of a CNF trained with OT-CFM \citep{tong2024improving} from a source distribution ($\bullet$) to the `2 moons' distribution ($\color{blue}\bullet$). The source is Gaussian (top row) or a mixture of 8 Gaussians (bottom row). \textbf{(b)} The constraint is constructed such that the posterior is the lower moon. CNF flow paths from the lower moon (target posterior) to the source latents (outsourced posterior). \textbf{(c)} Flow paths from naive application of Adjoint Matching \citep{domingoenrich2025adjoint}, which is is biased for OT flows and non-Gaussian sources. \textbf{(d)} Flows starting at samples from an outsourced diffusion model, which samples the latent posterior, give target samples close to the ground truth.}%
    \label{fig:moons}
  \end{minipage}%
    \begin{minipage}{0.58\textwidth} %
    \centering
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/gmm_prior.png}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/gmm_target.png}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/gmm_adj.png}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/gmm_rtb.png}
    \end{minipage}
    
    \vspace*{-1em}
    
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/8gmm_prior.png}
      \subcaption{ OT-CFM prior}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/8gmm_target.png}
      \subcaption{True Posterior}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/8gmm_adj.png}
      \subcaption{Adj. Matching}
    \end{minipage}
    \begin{minipage}{0.23\textwidth}
      \includegraphics[width=\textwidth]{figures/toy/gmm_to_moon/8gmm_rtb.png}
      \subcaption{Ours\vphantom{j}}
    \end{minipage}
  \end{minipage}
\end{figure*}

\subsection{Diffusion Sampling in Noise Space}
\label{sec:diffusion_noise_space}

Diffusion models are an attractive choice of variational family due to their ability to sample from complex high-dimensional distributions. We train \emph{outsourced diffusion samplers} by using the methods introduced in \cref{sec:diffusion_samplers} to approximate posteriors in outsourced noise spaces (\cref{sec:latent_space_posterior}).  

The target density we wish to sample takes the form $R(\mathbf{z}\mid\mathbf{y})\coloneqq p(\mathbf{z})r(f(\mathbf{z}),\mathbf{y})$. The sampler can be conditioned on the auxiliary variable $\mathbf{y}$ (taking it as an input, resulting in amortization over $\mathbf{y}$ and the possibility of generalizing to new $\mathbf{y}$) or can be trained for a single, fixed value of $\mathbf{y}$ to sample the corresponding posterior.


\paragraph{Training objective.}
To train a diffusion model to sample from the target density $R(\mathbf{z}\mid\mathbf{y})$, we use the trajectory balance objective \citep[TB;][]{malkin2022trajectory}. TB was first introduced in the context of (discrete) generative flow networks \citep[GFlowNets;][]{bengio2021flow, bengio2021foundations} and generalized to the continuous setting in \citet{lahlou2023theory}; it is also a close relative of the VarGrad objective \citep{richter2020vargrad}. It was comprehensively evaluated for diffusion samplers in \citet{sendera2024improved} and its asymptotic consistency in the continuous-time limit was established in \citet{berner2025discrete}. We briefly review the TB objective and refer to those works for further details.

A diffusion model -- a neural network with parameters $\phi$ and possibly conditioned on $\mathbf{y}$ -- defines a Markovian distribution over denoising trajectories $\tau=(\mathbf{z}_0\rightarrow\mathbf{z}_{\Delta t}\rightarrow\dots\rightarrow\mathbf{z}_1)$, where $\Delta t=\frac1T$ is the time step of the discretization of the SDE \eqref{eq:neural_sde}, via
\begin{equation}\label{eq:pf_probability}
    p_F^\phi(\tau\mid\mathbf{y})=p(\mathbf{z}_0)\prod_{i=1}^Tp_F^\phi(\mathbf{z}_{i\Delta t}\mid\mathbf{z}_{(i-1)\Delta t},\mathbf{y}).
\end{equation}
\looseness=-1
Here $p(\mathbf{z}_0)$ is the density of a fixed distribution over the initial noise (recall that generation goes forward in time) and $p_F^\phi$ is the density of the transition kernel defined by the model, \ie, the probability of transitioning from a sample at a given noise level to a sample at the next-lowest noise level. Similarly, the (fixed) noising process defines a distribution over noising trajectories conditioned on their terminal endpoint:
$
    p_B(\tau\mid\mathbf{z}_1)=\prod_{i=1}^Tp_B(\mathbf{z}_{(i-1)\Delta t}\mid\mathbf{z}_{i\Delta t}).
$
The TB objective associated with a trajectory $\tau$ is a squared log-ratio:
\begin{equation}\label{eq:tb_loss}
    \gL_{\rm TB}(\tau;\mathbf{y},\phi)=\left(\log\frac{Z_\phi(\mathbf{y}) p_F^\phi(\tau\mid\mathbf{y})}{R(\mathbf{z}_1\mid\mathbf{y})p_B(\tau\mid\mathbf{z}_1)}\right)^2,
\end{equation}
where $Z_\phi$ is a learned model that, at optimality, estimates the partition function $\int R(\mathbf{z}\mid\mathbf{y})\,d\mathbf{z}$. This objective aims to match two distributions over trajectories: the one defined by the denoising model and that defined by the target distribution and the noising kernel. If the two distributions are equal, then their marginal densities at $t=1$ also coincide.

For training, one draws trajectories $\tau$ from some training distribution (which is not necessarily the current model $p_F^\phi$) and optimizes \eqref{eq:tb_loss} with respect to the parameters $\phi$. If the TB loss is optimized to 0 for \emph{every} trajectory $\tau$ in the continuous-time limit, the model $p_F^\phi$ asymptotically samples from the target density $R(\mathbf{z}\mid\mathbf{y})$ \citep{berner2025discrete}.

\paragraph{Exploration and credit assignment techniques.} We borrow a number of off-policy exploration techniques (such as replay buffers), as well as methods to make training more stable (such as temperature annealing) from the diffusion samplers literature. For details, see \cref{sec:experiment_details}.


\section{Experiments}
\label{sec:experiments}

%
The goal of our experiments is to demonstrate the general applicability of outsourced sampling, highlighting tasks which lack specialized techniques for posterior inference. We list the different tasks, alongside the sources of priors, constraints, and the dimension of noise sampled by the outsourced diffusion model in \cref{tab:exp}.

\begin{table*}[tbp]
    \caption{The priors and constraints studied in \cref{sec:experiments}. Outsourced diffusion sampling works in noise spaces of a wide range of generative models and is agnostic to their specific form.}
    \label{tab:exp}
    \centering
    %
    \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}l l c c c c c}
            \toprule
            %
            Task
            & Constraint
            & Prior
            & Prior type
            & $d_{\rm noise}$
            & $d_{\rm data}$
            \\
            \midrule
            %
            %
            \multirow{2}{*}{CIFAR-10 classifer guidance}
            & \multirow{2}{*}{\makecell{CIFAR-10 classifer}}
            & SN-GAN
            & GAN
            & ${128}$
            & ${3\times32\times32}$
            \\
            & & I-CFM
            & CNF
            & ${3\times 32\times32}$
            & ${3\times32\times32}$
            \\
            \midrule
            \multirow{2}{*}{FFHQ text conditioning}
            & \multirow{2}{*}{\makecell{ImageReward}}
            & StyleGAN3
            & GAN
            & ${512}$
            & ${3\times256\times256}$
            \\
            & & NVAE
            & Hierarchical VAE
            & ${4\times 20\times8 \times 8}$
            & ${3\times256\times256}$
            \\
            \midrule
            Text-to-Image model RLHF
            & ImageReward
            & Stable Diffusion 3
            & Latent-CNF
            & ${16\times64\times64}$
            & ${3\times512 \times 512}$
            \\
            \midrule
            Protein structure 
            & Structure Diversity
            & FoldFlow 2
            & Riemannian CNF
            &${7 \times 64}$ & ${7\times64}$             \\
            \bottomrule
        \end{tabular}
    }
\end{table*}

\subsection{Class-Conditional Sampling}

\label{sec:cifar_sampling}

\looseness=-1
\paragraph{Setup.} The prior model $p_{\theta}(\textbf{x})$ is an off-the-shelf unconditional image generator trained on the CIFAR-10 dataset \citep{krizhevsky2009learning}. Using a CIFAR-10 classifier $p(\textbf{y} \mid \textbf{x})$, we train a posterior class-conditioned generative model $p(\textbf{x}\mid \textbf{y}) \propto p_{\theta}(\textbf{x})p(\textbf{y}\mid \textbf{x})$. For our experiments we work with two priors which achieve high fidelity unconditional generation: a flow matching (CNF) model trained with independent coupling and linear interpolants \citep[I-CFM;][]{tong2024improving}, and a spectrally normalized GAN \citep[SN-GAN;][]{miyato2018spectral}. We use a 13-layer VGG-net model as the classifier \citep{simonyan2015very}. We compare outsourced diffusion sampling against a powerful MCMC baseline and a recent method for fine-tuning flow-based models:
\begin{itemize}[left=0pt,nosep]
    \item \textbf{Hamiltonian Monte Carlo \citep[HMC;][]{Brooks_2011}} applied in the noise spaces of both the SN-GAN and CFM priors to sample the outsourced posterior. We highlight that HMC with CNF priors can be quite slow, since it requires differentiating through the ODE integrator.
    \item \textbf{Adjoint Matching \citep{domingoenrich2025adjoint}}: see \cref{sec:existing_methods} and \cref{fig:moons} for discussion. Note that adjoint matching requires access to the gradient of the constraint function.
\end{itemize}
For the SN-GAN prior, we train an outsourced diffusion model to sample generator noise \( \mathbf{z} \in \mathbb{R}^{512} \). For the CFM prior, we sample the initial noise latent at \( t = 0 \), where \( \mathbf{z} \in \mathbb{R}^{3 \times 32 \times 32} \). See details in \cref{sec:cifar_details}.

\paragraph{Results.}
We report average log-reward (classifier log-likelihood) of samples and FID scores (computed with the dataset images of the given class) in \cref{tab:cifar_results}. The CFM posteriors consistently outperform SN-GAN posteriors, reflecting the superior quality of the CFM prior. Among the methods evaluated, adjoint matching with the CFM prior achieves the best performance -- as expected, since it is specifically designed for fine-tuning I-CFM models with a Gaussian source (samples in \cref{fig:cifar_samples_w_adj}). Outsourced diffusion, a more general approach, also delivers strong conditional generation (visualized in \cref{fig:cifar_samples}). Unlike adjoint matching and HMC, outsourced diffusion does not rely on gradient information from either the classifier or the prior. Additionally, the outsourced diffusion model offers significant advantages in training efficiency, requiring approximately 5 hours on an A100 GPU compared to 12 hours for adjoint matching (see \cref{sec:efficiency_analysis}). Moreover, we show how we can distill one-step outsourced diffusion samplers in \cref{supp:sec:distillation}, with no performance degradation and significant sampling speed advantage. 

\begin{figure}[t]
    \centering
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/prior_sngan.png}
        \caption{SN-GAN Prior}
    \end{subfigure}
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/dog_sngan.png}
        \caption{Posterior (Dog)}
    \end{subfigure}
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/car_sngan.png}
        \caption{Posterior (Car)}
    \end{subfigure}
    
    \vspace{0.125cm}
    
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/prior_icfm.png}
        \caption{I-CFM Prior}
    \end{subfigure}
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/dog_icfm.png}
        \caption{Posterior (Dog)}
    \end{subfigure}
    %
    \begin{subfigure}{0.15\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/car_icfm.png}
        \caption{Posterior (Car)}
    \end{subfigure}
    \vspace*{-1em}
    \caption{CIFAR-10 samples generated using SN-GAN (top row) and CFM (bottom row) priors and posterior samples from trained outsourced diffusion models for the `Dog' and `Car' classes.}
    \label{fig:cifar_samples}
\end{figure}

\begin{table}[b]
    \caption{CIFAR-10 posterior sampling results for GAN and CNF priors. We report expected classifier log probability and FID scores for the class posteriors, averaged over all 10 classes.}
    \label{tab:cifar_results}
    \centering
    \resizebox{1\linewidth}{!}{
        \begin{tabular}{@{}ll@{}cc}
            \toprule
            Prior & Sampler
            & $\mathbb{E}[\log p(\textbf{y} \mid \textbf{x})] (\uparrow)$
            & FID $(\downarrow)$
            \\\midrule
            \multirow{3}{*}{SN-GAN} & Prior
            & $-5.37$
            & $97.14$
            \\
            & Latent HMC
            & $-3.26$
            & $75.33$\\
            %
            %
            %
            %
            & \textbf{Outsourced Diff.}
            & $-3.84$
            & $68.12$
            \\
            \midrule
            \multirow{4}{*}{I-CFM} & Prior
            & $-5.88$
            &  $84.79$\\
            & Latent HMC
            & $-2.80$
            & $46.69$\\
            %
            & Adj. Matching
            & $-3.09$
            & 19.45\\
            %
            %
            %
            %
            
            %
            %
            %
            & \textbf{Outsourced Diff.}%
            & $-3.35$
            &  $34.28$\\
            %
            %
            %
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Conditional High-Resolution Face Generation}
\label{sec:ffhq}


\begin{figure*}[h]
    \begin{minipage}{0.55\linewidth}
    \centering
    
    %
    \newcolumntype{M}[1]{>{\centering\arraybackslash\small}m{#1}}

    %
    %
    %
    \hspace*{-5mm}
    \begin{tabular}{%
        M{0.1cm}  %
        M{1.6cm}    %
        M{1.6cm}    %
        M{1.6cm}    %
        M{1.6cm}    %
        M{1.6cm}    %
    }
        %
        \rotatebox{90}{\textbf{NVAE}} &
        \includegraphics[width=1.8cm]{figures/ffhq/nvae/prior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/nvae/old_man/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/nvae/glasses_asian_girl/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/nvae/bald_black_beard/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/nvae/brown_hair_child/posterior.jpg} \\[-2pt]%
        
        %
        \rotatebox{90}{\textbf{StyleGAN3}} &
        \includegraphics[width=1.8cm]{figures/ffhq/stlygan3/prior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/stlygan3/old_man/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/stlygan3/glasses_asian_girl/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/stlygan3/bald_black_beard/posterior.jpg} &
        \includegraphics[width=1.8cm]{figures/ffhq/stlygan3/brown_hair_child/posterior.jpg} \\
        &
        Prior &
        Old man &
        Asian girl with glasses&
        Bald man, black beard &
        Brown haired child 
    \end{tabular}
    \vspace*{-1em}
    \caption{Sampled images from the FFHQ priors and outsourced diffusion posteriors for different prompts. More examples in \cref{sec:ffhq_samples_all}.}
    \label{fig:ffhq_samples}
    \end{minipage}\hfill
    \begin{minipage}{0.39\textwidth}
        \vspace*{-5em}
        \captionof{table}{FFHQ text-conditioning results for NVAE and StyleGAN3 priors. We report average log-reward and CLIP cosine distance (diversity) for posteriors, averaged over the prompts shown in \cref{fig:ffhq_samples}.}
        \label{tab:ffhq_results}
        \centering
        \resizebox{1\linewidth}{!}{
            \begin{tabular}{@{}ll@{}cc}
                \toprule
                Prior &
                Sampler
                & $\mathbb{E}[\log r(\textbf{x},\textbf{y})] (\uparrow)$
                & CLIP Diversity $(\uparrow)$
                \\\midrule
                \multirow{3}{*}{NVAE} & Prior
                & $-1.94$
                & $0.30$
                \\
                & Latent HMC
                & $-1.2$
                &  $0.30$\\
                %
                %
                %
                %
                & \textbf{Outsourced Diff.}%
                & $0.98$
                & $0.26$
                \\
                \midrule
                \multirow{3}{*}{StyleGAN3}& Prior
                & $-1.52$
                &  $0.36$\\
                & Latent HMC
                & $-0.62$
                &  $0.31$\\
                & \textbf{Outsourced Diff.}
                & $1.23$
                & $0.26$\\
                \bottomrule
            \end{tabular}
        }
    \end{minipage}
\end{figure*}


\looseness=-1
\paragraph{Setup.} Given a generator of high-resolution ($256 \times 256$) human face images trained on the FFHQ dataset \citep{karras2021style} as the prior $p_{\theta}(\textbf{x})$, we aim to generate faces aligned with a specified text caption $\textbf{y}$. To achieve this, we use a constraint function given by ImageReward \citep{xu2023imagereward}, a text-image reward model built on the BLIP backbone \citep{li2022blip} that scores images based on their alignment with the provided text prompt and aesthetic quality. The ImageReward score serves as the log-constraint function $\log r(\textbf{x},\textbf{y})$ in our formulation, enabling us to frame the text-conditional face generation problem as posterior inference. For the prior models, we employ NVAE \citep{vahdat2020NVAE} and StyleGAN3 \citep{Karras2021}, both of which achieve high-fidelity unconditional generation.

NVAE is a deep hierarchical VAE with a large number of latents of different scales. \citet[][Appendix B.6]{vahdat2020NVAE} notes that almost all the feature variance is captured by the first $4$ levels of the latent hierarchy. We train the outsourced diffusion model to sample noise $\mathbf{z} \in \mathbb{R}^{4\times20\times8\times8}$ for these levels, which turns out to be sufficient for conditional generation. For StyleGAN3, we sample the generator noise space $\mathbf{z} \in \mathbb{R}^{512}$. 
Due to the absence of specialized variational techniques for posterior inference with GANs and VAEs, we use HMC sampling of the outsourced noise, targeting the same distribution as the outsourced diffusion sampler, as the baseline. More details in \cref{sec:ffhq_details}.

\paragraph{Results.}
We report average ImageReward score and diversity, measured as average cosine distance of CLIP \citep{radford2021learning} embeddings for $100$ generated images from the posterior, for $4$ different prompts in \cref{tab:ffhq_results}. We find HMC for StyleGAN3 can get stuck in bad reward modes, but sometimes obtains high reward. HMC is consistently poor with the NVAE prior, which we attribute to a combination of dimensionality and high energy barriers. Outsourced diffusion samplers consistently generate prompt-accurate posterior samples. Illustrative samples are displayed in \cref{fig:ffhq_samples} and more uncurated samples in \cref{sec:ffhq_samples_all}. StyleGAN3 posteriors are of higher quality than NVAE posteriors, likely because the prior is also stronger.

\subsection{Text-to-Image RLHF}
\paragraph{Setup.} Diffusion and flow matching models that generate images conditioned on textual prompts often struggle with complex prompts that involve compositional relationships. A promising strategy to address this limitation is to fine-tune such models using reward functions that quantify image-caption alignment. In previous work, \citet{fan2023reinforcement, venkatraman2024amortizing} used Stable-Diffusion-1.5 \citep{rombach2021high} as a caption-conditioned prior $ p(\textbf{x} \mid \textbf{y})$ and ImageReward as the unnormalized log-likelihood \(\log r(\mathbf{x}, \mathbf{y})\). In both of those works, the prior diffusion model was tuned to sample approximately from the posterior $p^{\rm aligned}(\textbf{x} \mid \textbf{y}) \propto p(\textbf{x} \mid \textbf{y})r(\textbf{x},\textbf{y})$. In our experiments, we instead align Stable Diffusion 3 \citep[SD3;][]{esser2024scalingrectifiedflowtransformers}, which is a CNF, not a diffusion model, and is thus unsuitable for fine-tuning using the mentioned techniques.

We train outsourced diffusion samplers of the CNF's $16\times64\times64$-dimensional noise space. Gradient-based posterior inference techniques, such as adjoint matching and HMC, are prohibitive for a flow model at the scale of SD3, since they involve differentiating through the reward model, high resolution multiscale decoder, and ODE integrator. Instead, as a baseline, we tune the classifier-free-guidance weight \citep{ho2022classifier} individually for each prompt and report the best performance. See \cref{sec:rlhf_details} for details.

\looseness=-1
\paragraph{Results.}
We report the ImageReward score and the average CLIP cosine distance averaged over $4$ prompts in \cref{tab:sd3_results}. We present illustrative examples in \cref{fig:sd3_samples}, with further uncurated samples provided in \cref{sec:sd3_samples_all}. We find that latent sampling greatly improves reward (and our qualitative assessments) compared to the prior. These results, along with the analysis in \cref{sec:ffhq}, demonstrate the effectiveness of our proposed method to fine-tune high-dimensional image priors.

\begin{table}[t]
    \caption{RLHF finetuning results for SD3 prior. We report expected log reward and CLIP cosine distance (diversity) for posteriors, averaged over the prompts listed in \cref{fig:sd3_samples}.}
    \label{tab:sd3_results}
    \centering
    \resizebox{1\linewidth}{!}{
        \begin{tabular}{@{}lcc}
            \toprule
            Sampler
            & $\mathbb{E}[\log r(\textbf{x},\textbf{y})] (\uparrow)$ 
            & CLIP diversity $(\uparrow)$
            \\\midrule
            Prior
            & $0.791$
            & $0.19$
            \\
            CFG
            & $0.84$
            & $0.17$ \\
            %
            %
            %
            %
            \textbf{Outsourced Diff.}
            & $1.27$
            & $0.16$
            \\
            \bottomrule
        \end{tabular}
    }
    %
\end{table}

\begin{figure}[t]
\centering
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\newcommand{\imagecaption}[1]{\begin{minipage}{0.45\linewidth}\small\centering #1\end{minipage}}
\let\savecolsep=\tabcolsep
\tabcolsep=3pt
\centering
\begin{tabular}{%
  >{\centering\arraybackslash}m{0.23\linewidth}
  >{\centering\arraybackslash}m{0.23\linewidth}
  >{\centering\arraybackslash}m{0.23\linewidth}
  >{\centering\arraybackslash}m{0.23\linewidth}
}
\multicolumn{2}{c}{\imagecaption{A cat and a dog.}} & 
\multicolumn{2}{c}{\imagecaption{A cat riding a llama.}} \\\cmidrule(lr){1-2}\cmidrule(lr){3-4}
   %
   \includegraphics[width=1\linewidth]{figures/sd3/cat_dog/prior.jpg}
   & \includegraphics[width=1\linewidth]{figures/sd3/cat_dog/posterior.jpg} 
   & \includegraphics[width=1\linewidth]{figures/sd3/cat_llama/prior.jpg}
   & \includegraphics[width=1\linewidth]{figures/sd3/cat_llama/posterior.jpg} \\[-2pt]
   \textbf{Prior} & \textbf{Posterior} & \textbf{Prior} & \textbf{Posterior} \\[-1pt]
   \includegraphics[width=1\linewidth]{figures/sd3/village_meteor/prior.jpg}
   & \includegraphics[width=1\linewidth]{figures/sd3/village_meteor/posterior.jpg} 
   & \includegraphics[width=1\linewidth]{figures/sd3/human_horse_wolf/prior.jpg}
   & \includegraphics[width=1\linewidth]{figures/sd3/human_horse_wolf/posterior.jpg} \\[-2pt]\cmidrule(lr){1-2}\cmidrule(lr){3-4}
      \multicolumn{2}{c}{\imagecaption{Quiet village disrupted by meteor strike.}} & 
\multicolumn{2}{c}{\imagecaption{A human with a horse face and a human with a wolf face.}} 
\end{tabular}
\tabcolsep=\savecolsep
\vspace*{-1em}
\caption{Sampled images from the SD3 prior and outsourced diffusion posterior for different prompts. More examples in \cref{sec:sd3_samples_all}.}
\label{fig:sd3_samples}
\vspace*{-1em}
\end{figure}


\subsection{Protein Secondary Structure Diversity}

\paragraph{Setup.} Many protein generative models have been proposed to tackle the problem of designing novel yet realistic protein structures \citep{Watson2023DeND, bose2024foldflow}. They learn to produce proteins of $N$ residues by sampling rotations and translations applied to each residue backbone  (the space $\mathrm{SE}(3)^N$). As our prior $p_{\theta}(\textbf{x})$, we use the recent FoldFlow 2 model, which is a Riemannian CNF (on the manifold $\mathrm{SE}(3)^N$ embedded in $\mathbb{R}^{7\times N}$) trained with minibatch OT coupling \cite{huguet2024foldflow2}.

\looseness=-1
Protein residues fold into patterns called secondary structures, which include $\alpha$-helices, $\beta$-sheets, and coils. Many protein generative models have issues producing proteins with diverse secondary structures, typically under-sampling proteins with $\beta$-sheets. A natural problem is to produce samples which are both probable under the prior model and exhibit this structural diversity. This can be framed as sampling from $p_{\theta}(\textbf{x}) r_{\mathrm{div}}(\textbf{x})$, where the constraint function $r_{\mathrm{div}}(\textbf{x})$ assigns high-values to proteins with diverse secondary structures (in-particular, the presence of $\beta$-sheets). 

Let $p = [p_\alpha, p_{\beta}, p_c]$ be a vector representing the proportion of residues that are in $\alpha$-helices, $\beta$-sheets or coils. The particular constraint function that we use, adapted from \citet{huguet2024foldflow2}, is $r_{\mathrm{div}}(\textbf{x}) = \frac{e^{-1.2} w^\top p}{1.2 - \mathcal{H}[p]}$, where $w = [1, 2, 0.5]$ is a weight vector, and $\mathcal{H}[p]$ is the entropy of $p$. 

This problem poses two challenges. First, since the model is a Riemannian CNF, the flow ODE cannot be converted into a diffusion SDE, so that adjoint matching and diffusion fine-tuning techniques are not applicable. Second, the constraint function is not differentiable with respect to the generative model's output, ruling out methods such as HMC. We therefore compare our method to a gradient-free MCMC method in the noise space $\textbf{z}$. We fix the protein length to 64 residues, and evaluate the model achieving the highest diversity score during training. See \cref{sec:protein_exp_details} for details.

\paragraph{Results.}
We report the average $\log r_{\mathrm{div}}(\textbf{x})$, as well as a diversity metric (the pairwise TM-Score) in \cref{tab:protein_results}. The latter calculates the similarity between pairs of proteins, averaged across a set of generated samples \citep{Zhang2004ScoringFF}. Uncurated protein samples are shown in \cref{fig:protein_samples}. We find that our proposed method samples diverse protein structures rich in $\beta$-sheets more frequently than the baselines, while maintaining a TM-Score comparable to the prior. 

\begin{table}[t]
    \caption{Results for protein structure experiments. We report the average log-reward, as well as the pairwise TM-Score (as a measure of sample diversity), averaged over 64 samples. Standard deviation over 3 seeds is reported.}
    \label{tab:protein_results}
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{@{}lcc}
            \toprule
            Method & $\mathbb{E}[\log r_{\mathrm{div}}(\textbf{x})]$ ($\uparrow$) &  Pairwise TM-Score ($\downarrow$)  \\ \midrule
            Prior & $-1.325 \pm 0.014$ & $0.4480 \pm 0.0044$\\ 
            RW MCMC & $-0.640 \pm 0.073$ & $0.4181 \pm 0.0057$\\  
            \textbf{Outsourced Diff.} & $0.422 \pm 0.225$  & $0.4407 \pm 0.0706$ \\
            \bottomrule
        \end{tabular}
        }
\end{table}

\begin{figure}[t]
    \centering
    %
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/proteins/proteins_prior_1.png}
        \caption{CFM Prior}
    \end{subfigure}
    \vspace{0.25cm}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/proteins/proteins_posterior_outsourced_1.png}
        \caption{Outsourced Diffusion Posterior}
    \end{subfigure}
    \vspace*{-3em}
   \caption{\looseness=-1 Protein samples with pertinent secondary structures highlighted: $\alpha$-helices (blue), $\beta$-sheets (red), and coils (green).}
\label{fig:protein_samples}
\end{figure}


\section{Conclusion}

We have proposed outsourced diffusion samplers for efficient posterior inference in the noise spaces of generative models. These samplers take advantage of the expressiveness of diffusion models and the flexibility of off-policy training algorithms for black-box target distributions and can be applied to any model that can be written as a deterministic transformation of noise. While we have demonstrated the effectiveness of this method in a variety of settings, there are many questions for future work. One natural direction is to extend the method to discrete problems, where the noise space is discrete or the transformation involves discretization. Another is to adapt outsourced diffusion sampling to general probabilistic programs, where the generative model includes both stochasticity and nontrivial control flow, and where current inference methods use MCMC sampling in outsourced noise spaces \citep{lazyppl}.

\section*{Acknowledgments}

The authors thank Moksh Jain for comments and suggestions in the course of this project, Kirill Tamogashev for comments on a draft of the manuscript, and Prudencio Tossou and Dominique Beaini for helpful discussions and suggestions about the protein conformation experiments.

% The authors acknowledge funding from ...
%CIFAR, NSERC, IVADO, UNIQUE, FACS Acuit\'e, NRC AI4Discovery, Samsung, and Recursion.
L.S.\ acknowledges support from Recursion.
The research of M.S.\@ was funded by National Science Centre, Poland, 2022/45/N/ST6/03374.
Y.B.\ acknowledges the support from CIFAR and the CIFAR AI Chair program as well as NSERC funding for the Herzberg Canada Gold medal. 
G.B.\ acknowledges funding support from Samsung AI Lab and CIFAR.

The research was enabled in part by computational resources provided by the Digital Research
Alliance of Canada (\url{https://alliancecan.ca}), Mila (\url{https://mila.quebec}), and
NVIDIA.


\bibliography{references}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix

\onecolumn

\section{Theory and Method Details}

\subsection{Outsourced Sampling for Diffusion Priors}
\label{sec:diffusion_appendix}

We describe posterior sampling, outsourced posterior inference and diffusion sampling when the prior generative model is itself a diffusion model.

\paragraph{Monte Carlo methods.} The `guidance' term in diffusion posteriors -- the difference between the scores of the noised prior and posterior distributions -- can be estimated by Monte Carlo integration \citep{song2023loss,cardoso2024montecarlo} or using approximations specialized for constraints arising from linear inverse problems \citep{kawar2021snips,kadkhodaie2021solving,song2022solving,chung2023diffusion}. Posterior estimation can also be achieved through stochastic optimization \citep{graikos2022diffusion,mardani2024variational}

Methods related to sequential Monte Carlo, which treat a modification to the denoising transition kernel as a proposal, have also been proposed \citep{doucet2022score,dou2024diffusion,chen2025sequential}.

\paragraph{Amortized methods.}
Because generation proceeds in a long sequence of sampling steps, and the modes of the posterior are not known \emph{a priori}, these methods use reinforcement learning techniques to discover regions of high posterior density. Asymptotically unbiased methods include ELEGANT \citep{uehara2024fine} and relative trajectory balance \citep{venkatraman2024amortizing}.

\paragraph{Outsourcing noise in diffusion models.}
Generation of data $\mathbf{x}=\mathbf{x}_0$ is modeled as a Markov process $\mathbf{x}_T\rightarrow\dots\rightarrow\mathbf{x}_1\rightarrow\mathbf{x}_0$, where $\mathbf{x}_T\sim\gN(0,I_{d_{\rm data}})$ and the transition from $\mathbf{x}_t$ to $\mathbf{x}_{t-1}$ is conditionally spherical-Gaussian. Via the reparametrization trick, the trajectory of latent variables can be expressed as a function of the initial sample $\mathbf{x}_T$ and the $T$ standard Gaussian noises injected at each step of sampling, just as in a VAE. Thus a diffusion model is a generative model with outsourced noise in $\mathbb{R}^{d_{\rm data}\cdot(T+1)}$ (see \eqref{eq:diffusion_reparam} below).

Generalizing this setting, a typical \textbf{latent diffusion model} chains a diffusion model in a latent space $\mathbb{R}^{d_{\rm latent}}$, $\mathbf{w}_T\rightarrow\dots\rightarrow\mathbf{w}_1\rightarrow\mathbf{w}_0$, with a Gaussian decoder $\mathbf{w}_0\to\mathbf{x}$ of the same form as a VAE decoder. Combining the two, the data $\mathbf{x}$ is a deterministic transformation of the concatenation of the initial latent variable $\mathbf{w}_T$, the $T$ standard Gaussian noises injected at each step of sampling, and the noise in the final decoder. Thus a latent diffusion model is a generative model with outsourced noise in $\mathbb{R}^{d_{\rm latent}\cdot(T+1)+d_{\rm data}}$. 

(Note the similarity to the outsourced interpretation of HVAEs above: a diffusion model indeed be understood as a deep hierarchical VAE. However, a diffusion model is also a neural stochastic differential equation \citep{tzen2019neural,song2021score} integrated in discrete time. In this view, in the continuous-time limit, the outsourced noise is a sample of Brownian motion, and indeed an It\^o integral is a deterministic transformation of a Brownian noise random variable.)

\paragraph{Outsourced autoregressive sampling under diffusion priors recovers relative trajectory balance.} 
\citet{venkatraman2024amortizing} studied the problem of fine-tuning a diffusion model $p_\theta$ -- seen as a transition policy $p_\theta(\mathbf{x}_{i\Delta t}\mid\mathbf{x}_{(i-1)\Delta t})$ -- to yield a diffusion model $p_\phi^{\rm post}$ that samples the product of the distribution $p_\theta(\mathbf{x}_1)$ defined by the prior model with a constraint $r(\mathbf{x}_1)$, where the prior and posterior diffusion model share the noising process and standard Gaussian noise distribution $p(\mathbf{x}_0)$. The \emph{relative trajectory balance} (RTB) objective was proposed; for a trajectory $\tau$,
\begin{equation}\label{eq:rtb}
    \gL_{\rm RTB}(\tau;\phi)=\left(\log\frac{Z_\phi}{r(\mathbf{x}_1)}+\sum_{i=1}^T\log\frac{p_\phi^{\rm post}(\mathbf{x}_{i\Delta t}\mid \mathbf{x}_{(i-1)\Delta t})}{p_\theta(\mathbf{x}_{i\Delta t}\mid \mathbf{x}_{(i-1)\Delta t})}\right)^2,
\end{equation}
where $Z_\phi$ is a learned scalar (note the resemblance to \eqref{eq:tb_loss}). 

Let $\xi_i$ be the standard Gaussian noise injected in sampling $x_{i\Delta t}$ conditionally on $x_{(i-1)\Delta t}$, so that the prior model can be rewritten as a deterministic function of the noises $\xi_0,\xi_1,\dots,\xi_T$ via
\begin{equation}\label{eq:diffusion_reparam}
    \mathbf{x}_0=\xi_0,\quad \mathbf{x}_{i\Delta t}=\mathbf{x}_{(i-1)\Delta t}+v_\theta(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\Delta t+\sigma_{i\Delta t}\sqrt{\Delta t}\xi_i,
\end{equation}
where $\mu_\theta$ outputs the drift of the generative SDE. Similarly, let $\mu_\phi^{\rm post}$ be the drift of the posterior generative SDE. For a trajectory $\tau$ sampled using a sequence of noises $\xi_0,\xi_1,\dots,\xi_T$ under the prior model, the RTB loss \eqref{eq:rtb} can then be rewritten in terms of the noises:
\begin{align}
    \gL_{\rm RTB}(\tau;\phi)
    &=\left(\log\frac{Z_\phi}{r(\mathbf{x}_1)}+\sum_{i=1}^T\log\frac{\gN(\mathbf{x}_{i\Delta t}-\mathbf{x}_{(i-1)\Delta t};\mu_\phi^{\rm post}(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\Delta t,\sigma_{i\Delta t}^2\Delta t)}
    {\gN(\mathbf{x}_{i\Delta t}-\mathbf{x}_{(i-1)\Delta t};\mu_\theta(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\Delta t,\sigma_{i\Delta t}^2\Delta t)}\right)^2\nonumber\\
    &=\left(\log\frac{Z_\phi}{r(\mathbf{x}_1)}+\sum_{i=1}^T\log\frac{\gN(\xi_i;\mu_{\rm diff}(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\sqrt{\Delta t}/{\sigma_{i\Delta t}},I_{d_{\rm data}})}
    {\gN(\xi_i;0,I_{d_{\rm data}})}\right)^2\nonumber\\
    &=\left(\log
    \frac{Z_\phi\gN(\xi_0;0,I_{d_{\rm data}})\prod_{i=1}^T\gN(\xi_i;\mu_{\rm diff}(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\sqrt{\Delta t}/{\sigma_{i\Delta t}},I_{d_{\rm data}})}{r(\mathbf{x}_1)\prod_{i=0}^T\gN(\xi_i;0,I_{d_{\rm data}})}
    \right)^2
\label{eq:rtb_outsourced}
\end{align}
where $\mu_{\rm diff}(\mathbf{x},t)\coloneqq\mu_\phi^{\rm post}(\mathbf{x},t)-\mu_\theta(\mathbf{x},t)$.

Consider now an amortized sampler of the outsourced noises $\xi_0,\xi_1,\dots,\xi_T$ that generates the variables autoregressively ($\xi_0$ from a standard Gaussian, then subsequently each $\xi_i$ from a Gaussian with unit variance and mean conditioned on the previously sampled noises). The transition policy density of this sampler can be written
\[
    p_F(\xi_i\mid \xi_0,\dots,\xi_{i-1})=\gN(\xi_i;\mu_{\rm outsourced}(\xi_0,\dots,\xi_{i-1}),I_{d_{\rm data}}).
\]
Under a model parametrization in which the policy takes as input the intermediate state $\mathbf{x}_{(i-1)\Delta t}$, computed as a function of the noises $\xi_0,\dots,\xi_{i-1}$ using the prior model, 
\[
\mu_{\rm outsourced}(\xi_0,\dots,\xi_{i-1})
=
\mu_{\rm diff}(\mathbf{x}_{(i-1)\Delta t},(i-1)\Delta t)\sqrt{\Delta t}/{\sigma_{i\Delta t}}
,
\]
the numerator in \eqref{eq:rtb_outsourced} is \emph{precisely} $Z_\phi p_F(\tau)$, where $\tau$ is the sampling trajectory of the autoregressive sampler generating $\xi_0,\xi_1,\dots,\xi_T$. The denominator is $r(\mathbf{x}_1)$ multiplied with the prior density of the noise, which is the target density for the sampler (the unnormalized density of the outsourced posterior).

Thus we see that \eqref{eq:rtb_outsourced} exactly recovers the trajectory balance objective \citep{malkin2022trajectory} for an autoregressive sampler of outsourced noise.

\subsection{Proof of \Cref{prop:pushforward}}
\label{sec:proof}

\vspace*{1em}

\printProofs

\newpage

\section{Experiment Details}
\label{sec:experiment_details}
\subsection{Diffusion model}
For all experiments, we use a convolutional UNet architecture \citep{ronneberger2015unetconvolutionalnetworksbiomedical} for the diffusion model. This architecture is naturally well-suited for latent spaces structured as image feature maps. However, for other types of latent representations, we found that simply reshaping them into $H \times W \times C$ feature maps and treating them as images yielded surprisingly effective results.

We use the variance preserving SDE \citep{song2021score} as backward policy $P_B$ for the trajectory balance loss in \eqref{eq:tb_loss}, with a discretisation of $25$ steps. For all of the experiments, we use off-policy training with a replay buffer similar to \citet{sendera2024improved}. Specifically, for every training update we randomly sample either from the replay buffer, or generate new on-policy samples which are added to the replay buffer according to the buffer probability $\alpha$ -- whose value specific to experiments is provided in the following sections. 

\subsection{Class-Conditional Sampling}
\label{sec:cifar_details}
We set the classifier inverse temperature $\beta=4$ for all our baselines, otherwise the soft logits of the classifier results in a fuzzy posterior. We provide some additional information regarding implementation of the baselines below:

\paragraph{Outsourced Diffusion.} We learn to sample the $128$ dimensional generator noise with the outsourced sampler. To keep architectures consistent across experiments, we used the convolutional UNet to sample this noise by reshaping them to $2\times8\times8$ feature maps. For sampling from the CNF prior, we use $45$ Euler steps to discretise the ODE. The outsourced sampler operates on the noise space which the same dimensionality as the output image -- $3\times32\times32$.

\paragraph{Adjoint Matching.} To fine-tune flow matching models with a classifier, we use adjoint matching following the approach described in \citet{domingoenrich2025adjoint}. Due to the unavailability of the open source code at this time, we made our own implementation which we found works extremely well. First, we convert the ODE inference of conditional flow matching into a memoryless SDE, ensuring that both have the same marginal distribution as outlined in the referenced paper. Next, we apply the adjoint matching method for the stochastic optimal controls of the memoryless SDE. This process requires gradient information of the log-reward (\ie, the classifier's log-likelihood), which we compute using PyTorch's autograd functionality.

All hyperparameters remain identical to those of the outsourced diffusion sampler, including the learning rate, except for the number of training iterations and the temperature annealing schedule. Training is significantly slower because it requires computing the log-reward gradients and simulating the full trajectory for adjoint matching. Therefore, we set a total of 2,500 iterations with 1,000 linear temperature annealing steps. This configuration results in similar or reduced wall time compared to the outsourced diffusion sampler while achieving stable convergence and high performance on CIFAR-10. Figure~\ref{fig:cifar_samples_w_adj} shows the qualitative results of applying adjoint matching to the CIFAR-10 task. 

Note that for Stable Diffusion 3, the actual implementation of the inference pipelinewhich involves a diffusion transformer with various combinations of language embedding fusion for multimodalityis not fully open-sourced. Implementing a memoryless SDE on top of this pipeline is non-trivial and requires careful tuning and integration with the internal processes of Stable Diffusion's inference mechanisms.

\paragraph{Temperature Annealing.} We anneal the inverse temperature from $\beta=2$ to the final $\beta=4$. We tune the schedule linearly over the first $2,000$ steps of training for outsourced diffusion, and $1,000$ steps for adjoint matching.

\paragraph{HMC baselines.} We use hamiltorch \citep{cobb2020scaling} to implement HMC. We use step size of $10^{-2}$, with $5$ leapfrog integration steps. We use a burn-in chain of length 100 before starting to collecting samples, spaced out by 10 samples. Chains are run for $1,000$ samples, after which we reset the seed to help diversity. The runtime of HMC is quite slow with the CNF prior due to gradient computation. A chain of $1,000$ samples takes close to an hour on $A100$ GPUs. We only keep $90$ samples from this chain to preserve diversity, but these are still correlated. It would take  close to $10$ hours to generate $1,000$ samples for FID computation, but using extra resources we can run parallel chains. For comparison, outsourced diffusion takes close to $5$ hours for full training, after which sample generation is extremely cheap due to amortization. Despite the need to train a model, outsourced diffusion is actually more memory-efficient than even a single HMC chain, as it eliminates the need to compute gradients through the ODE integrator. We visualize samples from a latent HMC chain in \cref{fig:hmc_chain}.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/adj/more_step/cifar_target_class_5.png}
        \caption{Adj. (Dog)}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/adj/more_step/cifar_target_class_1.png}
        \caption{Adj. (Car)}
    \end{subfigure}
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    
    \vspace*{-1em}
    %
    %
    %
    \caption{
        CIFAR-10 samples generated  Adjoint Matching (Adj.) for the \textit{Dog} and \textit{Car} classes. 
    }
    \vspace*{-1em}
    \label{fig:cifar_samples_w_adj}
\end{figure}

All our results in this task are presented in \cref{tab:sngan_results} and \cref{tab:icfm_results}.

\begin{table}[ht]
\centering
\caption{SN-GAN results on CIFAR-10}
\label{tab:sngan_results}
%
\begin{tabular}{@{}lcccccc}
\toprule
\textbf{Class} & \multicolumn{2}{c}{\textbf{Prior}} & \multicolumn{2}{c}{\textbf{HMC}} & \multicolumn{2}{c}{\textbf{Outsourced Diff.}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & \textbf{Reward} & \textbf{FID} & \textbf{Reward} & \textbf{FID} & \textbf{Reward} & \textbf{FID} \\
\midrule
Airplane & -5.82 & 100.28 & -2.53 & 82.56 & -3.89 & 79.55 \\
Car & -5.36 & 125.16 & -3.91 & 94.81 & -4.22 & 85.88 \\
Bird & -5.40 & 71.30 & -4.86 & 52.14 & -3.91 & 59.76 \\
Cat & -5.40 & 68.30 & -2.21 & 67.55 & -3.97 & 55.05 \\
Deer & -5.40 & 70.10 & -3.67 & 51.10 & -3.38 & 43.04 \\
Dog  & -5.32 & 95.62 & -4.00 & 82.32 & -4.23 & 68.52 \\
Frog & -4.98 & 93.00 & -4.20 & 68.12 & -3.91 & 74.70 \\
Horse & -5.23 & 97.53 & -3.21 & 64.60 & -3.70 & 52.87 \\
Ship & -5.19 & 116.89 & -2.11 & 94.45 & -3.76 & 79.73 \\
Truck & -5.59 & 133.23 & -2.25 & 97.88 & -3.50 & 82.07 \\
\midrule
\textbf{AVG} & \textbf{-5.37} & \textbf{97.14} & \textbf{-3.26} & \textbf{75.33} & \textbf{-3.847} & \textbf{68.117} \\
\bottomrule
\end{tabular}%
%
\end{table}


\begin{table}[h]
\centering
\caption{I-CFM results on CIFAR-10}
\label{tab:icfm_results}
%
\begin{tabular}{@{}lcccccccc}
\toprule
\textbf{Class} & \multicolumn{2}{c}{\textbf{Prior}} & \multicolumn{2}{c}{\textbf{HMC}} & \multicolumn{2}{c}{\textbf{Adj. Matching}} & \multicolumn{2}{c}{\textbf{Outsourced Diff.}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
        & \textbf{Reward} & \textbf{FID} & \textbf{Reward} & \textbf{FID} & \textbf{Reward} & \textbf{FID} & \textbf{Reward} & \textbf{FID} \\
\midrule
Airplane & -5.81 & 73.09 & -2.45 & 62.24 & -3.30 & 27.64 & -3.36 & 47.61 \\
Car & -6.22 & 92.07 & -2.12 & 24.85 & -2.93 & 17.51 & -3.02 & 19.12 \\
Bird & -5.94 & 73.48 & -2.93 & 60.75 & -3.16 & 23.09 & -3.11 & 41.04 \\
Cat & -5.53 & 70.33 & -3.60 & 54.32 & -3.33 & 20.11 & -3.57 & 43.66 \\
Deer & -5.59 & 72.19 & -2.72 & 49.46 & -3.08 & 15.52 & -3.78 & 31.87 \\
Dog  & -5.83 & 89.38 & -3.89 & 41.13 & -3.21 & 23.11 & -3.57 & 34.33 \\
Frog & -6.06 & 93.08 & -3.36 & 55.64 & -3.27 & 20.86 & -3.98 & 34.74 \\
Horse & -6.13 & 82.88 & -2.10 & 46.22 & -2.79 & 16.59 & -2.95 & 31.85 \\
Ship & -5.85 & 102.33 & -2.28 & 37.80 & -3.09 & 18.00 & -2.92 & 32.13 \\
Truck & -5.82 & 99.10 & -2.55 & 34.48 & -2.55 & 12.34 & -3.20 & 26.46 \\
\midrule
\textbf{AVG} & \textbf{-5.88} & \textbf{84.79} & \textbf{-2.80} & \textbf{46.69} & \textbf{-3.09} & \textbf{19.45} & \textbf{-3.35} & \textbf{34.28} \\
\bottomrule
\end{tabular}%
%
\end{table}

\subsection{Conditional High-Resolution Face Generation}
\label{sec:ffhq_details}
For all prompts we use fixed inverse temperature $\beta=100$, which we found to be a suitable reward scale.

\paragraph{Outsourced Diffusion.} StyleGAN3 uses $512$ dimensional generator noise. Similar to our SN-GAN experiment for CIFAR, we  reshape this into a $2\times16\times16$ feature map to be passed to the UNet model. The NVAE prior for FFHQ is a very deep latent variable model, having $36$ total latent groups starting from $8\times 8$ scale all the way up to $128\times128$. The joint dimensionality of this latent space is extremely large, making joint posterior inference very challenging. Luckily most variable features of interest are captured in the first $4$ latent groups of size $20\times8\times8$ each. We stack the noise groups to create $80\times8\times8$ feature maps jointly diffused by the UNet. We use $25$ steps for diffusion sampling.

\paragraph{HMC baseline.} We use $5\cdot10^{-3}$ as the step size for StyleGAN3, and $10^{-2}$ as the step size for NVAE. We use $5$ leapfrog integration steps. We only require 100 samples for evaluation in this task (unlike 1000 needed for FID in CIFAR), and so we can afford to only collect 2 samples for every $1000$ length chain. We collect the samples at $t=500$ and $t=1000$. This takes around 3 hours for StyleGAN3 and 5 hours for NVAE without parallelism. We only sample the first $4$ latent groups with NVAE prior.

We find that unlike the CIFAR-10 priors, HMC struggles to obtain high reward with these priors, but performs better with StyleGAN3 than NVAE. We attribute the particularly poor performance of NVAE to the high dimensionality and high energy barriers. Interestingly, outsourced diffusion performs significantly better for sampling these posteriors. We suspect that it is primarily the nice mode mixing properties of the diffusion annealing path that facilitate this. However, an additional factor might be the benefits of amortization, which is an interesting direction for future work to investigate. We present the results for different prompts in \cref{tab:stylegan}, and showcase the first 10 samples for each prompt, generated from a fixed random seed, in \cref{sec:ffhq_samples_all}.

\begin{table}[h]
    \centering
        \caption{StyleGAN3 results on FFHQ}
    \begin{tabular}{@{}lcccccc}
        \toprule
        \textbf{Prompt} & \multicolumn{2}{c}{\textbf{Prior}} & \multicolumn{2}{c}{\textbf{HMC}} & \multicolumn{2}{c}{\textbf{Outsourced Diff.}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} \\
        \midrule
        An old man & -1.02 & 0.35 & -0.60 & 0.3 & 1.62 & 0.32 \\
        A young Asian girl with glasses & -1.97 & 0.36 & -0.81 & 0.32 & 1.13 & 0.21 \\
        A bald man with a black beard & -1.89 & 0.38 & -0.71 & 0.32 & 1.02 & 0.27 \\
        A brown-haired child & -1.2 & 0.36 & -0.35 & 0.28 & 1.14 & 0.23 \\
        \midrule
        \textbf{AVG} & \textbf{-1.52} & \textbf{0.36} & \textbf{-0.62} & \textbf{0.31} & \textbf{1.23} & \textbf{0.26} \\
        \bottomrule
    \end{tabular}
    \label{tab:stylegan}
\end{table}

\begin{table}[h]
    \centering
    \caption{StyleGAN3 results on FFHQ}
    \begin{tabular}{@{}lcccccc}
        \toprule
        \textbf{Prompt} & \multicolumn{2}{c}{\textbf{Prior}} & \multicolumn{2}{c}{\textbf{HMC}} & \multicolumn{2}{c}{\textbf{Outsourced Diff.}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} \\
        \midrule
        An old man. & -1.74 & 0.30 & -1.24 & 0.32 & 1.38 & 0.31 \\
        A young Asian girl with glasses. & -2.12 & 0.30 & -0.95 & 0.31 & 0.70 & 0.25 \\
        A bald man with a black beard. & -2.16 & 0.29 & -1.40 & 0.29 & 1.32 & 0.26 \\
        A brown-haired child. & -1.76 & 0.30 & -1.21 & 0.31 & 0.53 & 0.23 \\
        \midrule
        \textbf{AVG} & \textbf{-1.94} & \textbf{0.30} & \textbf{-1.20} & \textbf{0.30} & \textbf{0.98} & \textbf{0.26} \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table}

\subsection{Text-To-Image RLHF}
\label{sec:rlhf_details}
We intentionally choose prompts that pose a challenge for Stable Diffusion 3 while still receiving reliable feedback from ImageReward. Since SD3 is generally a more powerful model than ImageReward, this approach is not applicable to most prompts. However, in a real-world scenario, we anticipate the use of a better preference model trained with human feedback, which would offer more reliable guidance for improving the generative model. For training, we use a fixed inverse temperature $\beta=30$.

\paragraph{Outsourced Diffusion.} We learn to sample from the noise space of SD3, which is a latent CNF. This means we can sample at a reduced dimensionality from the full image (of size $3\times512\times512$), however the latent space is still fairly high dimensional with shape $16\times64\times64$.

\paragraph{Classifier-Free Guidance.} Since SD3 is trained as both an unconditional and conditional model, we can use CFG to approximately sample from lowered temperature conditional distribution:
\begin{equation}
    \hat{\textbf{v}}_{\theta}(\textbf{x}_t,\textbf{y}) = (1+w)\textbf{v}_{\theta}(\textbf{x}_t,\textbf{y}) - w\textbf{v}_{\theta}(\textbf{x}_t)
\end{equation}
Increasing the guidance scale $w$ generally guides the model to be more prompt accurate at the cost of diversity and if increased too much, image fidelity. We find tuning the CFG weight slightly improves score with ImageReward. Increasing $w$ resulted in degraded performance ("A cat riding a llama."), so we report score with the default guidance scale $w=5.0$ (same as prior).

We present the results for different prompts in \cref{tab:rlhf_results_all}, and showcase the first 10 samples for each prompt, generated from a fixed random seed, in \cref{sec:sd3_samples_all}. We set $w=(6.0,5.0,5.5,5.5)$ for the prompts ordered as in the table.

\begin{table}[h]
    \centering
    \caption{Text-to-image RLHF Results}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}lcccccc}
        \toprule
        \textbf{Prompt} & \multicolumn{2}{c}{\textbf{Prior}} & \multicolumn{2}{c}{\textbf{CFG}} & \multicolumn{2}{c}{\textbf{Outsourced Diff.}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} & \textbf{Reward} & \textbf{Diversity} \\
        \midrule
        A cat and a dog. & 0.5 & 0.14 & 0.61 & 0.1 & 1.23 & 0.09 \\
        A cat riding a llama. & 0.79 & 0.18 & 0.79 & 0.18 & 1.53 & 0.14 \\
        A quiet village is disrupted by a meteor strike. & 0.65 & 0.24 & 0.71 & 0.2 & 0.94 & 0.21 \\
        A human with a horse face and a human with a wolf face. & 1.22 & 0.2 & 0.71 & 0.2 & 0.94 & 0.2 \\
        \midrule
        \textbf{AVG} & \textbf{0.79} & \textbf{0.19} & \textbf{0.84} & \textbf{0.17} & \textbf{1.27} & \textbf{0.16} \\
        \bottomrule
    \end{tabular}}
    \label{tab:rlhf_results_all}
\end{table}

\subsection{Protein Structure Prediction}
\label{sec:protein_exp_details}

The specific form of the constraint function $r_{\mathrm{div}}(\textbf{x})$ (which can also be thought of as a reward) is adapted from \citep{huguet2024foldflow2}. It is a monotonic function of the entropy over secondary structure proportions (restricted to $\alpha$-helices, $\beta$-sheets and coils). The non-linear transformation increases the discrepancy between the rewards of samples with and without $\beta$-sheets. The expression is adjusted so that it is positive, to allow us to take the logarithm. 

The $e^{-1.2}$ multiplier ensures the log-constraint function is roughly centered around $0$. We found this to improve numerical stability in the diffusion training.

\paragraph{Outsourced Diffusion.}
In the outsourced diffusion experiments, an inverse temperature of $\beta=400.0$ was used to allow for improvement in this sparse reward setting. The diffusion sampler used 20 sampling steps. 

The $\mathrm{SE}(3)^N$ elements representing protein structure were parameterized as $\mathbb{R}^{7\times N}$ vectors, with the first 4 coordinates being a quaternion representation for a rotation matrix and the 3 other numbers representing the translation vector. For the UNet model, the protein coordinates were shaped into $7 \times 8 \times 8$ vectors. Additionally, the diffusion sampler was first pre-trained (for 200 epochs) using the denoising score matching objective \citep{ho2020ddpm}, with samples from $\mathcal{U}(\mathrm{SO}(3)^N) \times \mathcal{N}(0, 10^2 I_3)$ (where $\mathcal{U}$ is the Haar measure, \ie, the unique invariant probability measure on $\mathrm{SO}(3)^N$), the initial $\textbf{x}_0$ distribution used for training the CNF prior model. All inference parameters for the flow model were based on the default configuration from the FoldFlow 2 paper, including 50 steps for integrating the flow ODE \citep{huguet2024foldflow2}.

For TB training, we used a replay buffer, where $1/4$ of the samples are drawn proportional to their reward, and the rest are sampled uniformly. The buffer is used at each iteration with a probability of $0.2$ ($\alpha = 0.2$). The reason for the modification from the standard uniform buffer used in other experiments is to make the best use of high reward samples, which are rare especially early on in training. A learning rate of $10^{-5}$, and a batch size of $16$ was used. Gradient $l_2$ norms were clipped to $0.05$.

Due to policy collapse and training instability, models were saved every 100 training iterations, and the model with highest reward was selected for evaluation. The diffusion model was trained for 4 A100 GPU hours.

\paragraph{Random Walk MCMC baseline.} A Gaussian proposal, $p(\textbf{z}' \mid \textbf{z}) = \mathcal{N}(\textbf{z}'; \textbf{z}, 0.01^2 I)$ (ie. a step-size of $0.01$). The quaternion dimensions are normalized to have unit norm (projected back to $\mathrm{SO}(3)^N$).

Note this proposal is symmetric. The MCMC chain was run for 1000 iterations, with 32 chains in parallel. Metrics were evaluated on samples from the iterations $900$ and $1000$ (to reduce sample correlation). This was run for 8 A100 GPU hours. 

\newpage
 
\section{Additional Experiments}
\label{supp:sec:additional}
\subsection{Distillation and One-Step Outsourced Samplers} \label{supp:sec:distillation}

\begin{figure}[t]
    \centering
    \includegraphics[width=.7\linewidth]{figures/distillation.pdf}
    \caption{Posterior samples via the original outsourced diffusion sampler and the distilled one-step sampler in three experiments. We use the prompt "A cat and a dog." for the stable diffusion experiments, and "An old man" for the SN-GAN experiments.}
    \label{fig:distillation_figure}

\end{figure}

\begin{wraptable}[9]{r}{0.17\textwidth}
    \vspace*{-3em} 
    \centering
    \caption{FID scores of standard and distilled outsourced diffusion samplers for a CFM prior on CIFAR-10.}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lc@{}}
    \toprule
    \textbf{Sampler}           & \textbf{FID}  \\ \midrule
    SDE                        & 38.65            \\
    ODE                        & 36.71            \\
    Distilled         & 33.85            \\     
\bottomrule
    \end{tabular}
    }
    \label{tab:fid_scores}
\end{wraptable}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Training outsourced samplers of the kind described in this paper may have the downside of an increase in the number of sampling steps necessary to produce posterior samples of interest. We perform further experiments to show how we can mitigate these side effects via distillation. We distill outsourced models for the CMF posterior on the `car' class in CIFAR-10, the SN-GAN posterior for ``A dog and a cat", and a stable diffusion posterior for a ``A green car". We train distilled one-step samplers equivalent in architecture and size to our original diffusion samplers. We use the trained outsourced samplers as \emph{teacher} models and employ a simple training regime whereby we sample $\mathbf{z}_0\sim\gN(0,I_d)$, use an ODE sampling scheme to sample $\mathbf{z}_1$, and then learn a one-step mapping from $\mathbf{z}_0$ to $\mathbf{z}_1$ with the student model. We use Mean Square Error (MSE) loss and a simple variance agnostic regularizer to train the student model and encourage diversity.

We report in \cref{fig:distillation_figure} original and posterior samples for all experiments. We observe high-quality distilled samples, undistinguishable from the original outsourced diffusion sampler. Furthemore, we report in \cref{tab:fid_scores} the FID score computed from samples from the original and distilled sampler, with respect to the original CIFAR-10 samples of the class of interest. We observe comparable, if not improved, FID for our distilled model. In line with the results in the main paper, we posit that the ability to fit high fidelity one-step samplers is due to the simplicity of the properties of the target distribution in latent space, often smoother and lower dimensional than the distribution in data space, leading to easy-to-learn transforms from $\mathbf{z}_0$ to $\mathbf{z}_1$. 


\subsection{Efficiency Analysis}
\label{sec:efficiency_analysis}

%
%
%
%
%
%
%
%
%
%
    
%
%
%
%
%
%
%
%

%

%


%

The results in \cref{supp:sec:distillation} show that the diffusion sampler can be distilled into a single-step generator, enabling inference of the fine-tuned model without incurring additional sampling time. As a result, the inference efficiency remains comparable to that of direct finetuning methods. %

%
%

On the CIFAR-10 dataset, our method achieves approximately twice the training speed of adjoint matching when using an NVIDIA A100 GPU. This performance advantage is expected to grow for higher-dimensional outputs. It is important to note that the requirement for gradient computation not only raises training memory costs but also limits the flexibility of the method. For tasks where the reward gradient is unavailable, such as many protein or molecule tasks, Adjoint Matching is not applicable. In contrast, the outsourced diffusion sampler can be efficiently applied to arbitrary black-box tasks where the reward (or prior) is not differentiable.



\newpage

\section{FFHQ Samples}
\label{sec:ffhq_samples_all}
\subsection{Prior.}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/prior/\i.jpg}
        }
    }
    \caption{NVAE prior}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/prior/\i.jpg}
        }
    }
    \caption{StyleGAN3 prior}
\end{figure}


\subsection{An old man.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/old_man/hmc/\i.jpg}
        }
    }
    \caption{NVAE HMC}
\end{figure}

\begin{figure}[!htb]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/old_man/nvae/\i.jpg}
        }
    }
    \caption{NVAE Outsourced Diffusion}
\end{figure}

\begin{figure}[!htb]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/old_man/hmc/\i.jpg}
        }
    }
    \caption{StyleGAN3 HMC}
\end{figure}

\begin{figure}[!htb]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/old_man/gan/\i.jpg}
        }
    }
    \caption{StyleGAN3 Outsourced Diffusion}
\end{figure}

\newpage

\subsection{An asian girl with glasses.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/glasses_asian_girl/hmc/\i.jpg}
        }
    }
    \caption{NVAE HMC}
\end{figure}

\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/glasses_asian_girl/nvae/\i.jpg}
        }
    }
    \caption{NVAE Outsourced Diffusion}
\end{figure}

\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/glasses_asian_girl/hmc/\i.jpg}
        }
    }
    \caption{StyleGAN3 HMC}
\end{figure}

\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/glasses_asian_girl/gan/\i.jpg}
        }
    }
    \caption{StyleGAN3 Outsourced Diffusion}
\end{figure}

\newpage

\subsection{Bald man with black beard.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/bald_black_beard/hmc/\i.jpg}
        }
    }
    \caption{NVAE HMC}
\end{figure}

\begin{figure}[htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/bald_black_beard/nvae/\i.jpg}
        }
    }
    \caption{NVAE Outsourced Diffusion}
\end{figure}

\begin{figure}[htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/bald_black_beard/hmc/\i.jpg}
        }
    }
    \caption{StyleGAN3 HMC}
\end{figure}

\begin{figure}[htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/bald_black_beard/gan/\i.jpg}
        }
    }
    \caption{StyleGAN3 Outsourced Diffusion}
\end{figure}

\newpage

\subsection{Brown haired child.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/brown_hair_child/hmc/\i.jpg}
        }
    }
    \caption{NVAE HMC}
\end{figure}

\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/nvae/brown_hair_child/nvae/\i.jpg}
        }
    }
    \caption{NVAE Outsourced Diffusion}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/brown_hair_child/hmc/\i.jpg}
        }
    }
    \caption{StyleGAN3 HMC}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/ffhq/stlygan3/brown_hair_child/gan/\i.jpg}
        }
    }
    \caption{StyleGAN3 Outsourced Diffusion}
\end{figure}

\newpage 
\section{Stable Diffusion 3 Samples}
\label{sec:sd3_samples_all}
\subsection{A cat and a dog.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_dog/prior/\i.jpg}
        }
    }
    \caption{Prior}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_dog/cfg/\i.jpg}
        }
    }
    \caption{Classifer-Free Guidance}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_dog/posterior/\i.jpg}
        }
    }
    \caption{Outsourced Diffusion}
\end{figure}

\subsection{A cat riding a llama.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_llama/prior/\i.jpg}
        }
    }
    \caption{Prior}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_llama/cfg/\i.jpg}
        }
    }
    \caption{Classifer-Free Guidance}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/cat_llama/posterior/\i.jpg}
        }
    }
    \caption{Outsourced Diffusion}
\end{figure}


\newpage
\subsection{A quiet village is disrupted by a meteor strike.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/village_meteor/prior/\i.jpg}
        }
    }
    \caption{Prior}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/village_meteor/cfg/\i.jpg}
        }
    }
    \caption{Classifer-Free Guidance}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/village_meteor/posterior/\i.jpg}
        }
    }
    \caption{Outsourced Diffusion}
\end{figure}

\subsection{A human with a horse face and a human with a wolf face.}
\begin{figure}[H]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/human_horse_wolf/prior/\i.jpg}
        }
    }
    \caption{Prior}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/human_horse_wolf/cfg/\i.jpg}
        }
    }
    \caption{Classifer-Free Guidance}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
        {
            \includegraphics[width=0.08\textwidth]{figures/sd3/human_horse_wolf/posterior/\i.jpg}
        }
    }
    \caption{Outsourced Diffusion}
\end{figure}

\section{CIFAR-10 HMC Chain}

\label{sec:cifar_hmc}
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/1.png}
        \caption{t=100}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/6.png}
        \caption{t=150}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/11.png}
        \caption{t=200}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/16.png}
        \caption{t=250}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/21.png}
        \caption{t=300}
    \end{subfigure}
    \\
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/26.png}
        \caption{t=350}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/31.png}
        \caption{t=400}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/36.png}
        \caption{t=450}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/41.png}
        \caption{t=500}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/46.png}
        \caption{t=550}
    \end{subfigure}
    \\
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/51.png}
        \caption{t=600}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/56.png}
        \caption{t=650}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/61.png}
        \caption{t=700}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/66.png}
        \caption{t=750}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/71.png}
        \caption{t=800}
    \end{subfigure}
    \\
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/76.png}
        \caption{t=850}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/81.png}
        \caption{t=900}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/86.png}
        \caption{t=950}
    \end{subfigure}
    \begin{subfigure}{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/cifar/hmc/88.png}
        \caption{t=1000}
    \end{subfigure}
    \caption{Samples from HMC chain of length 1000 after 100 steps of burn-in, for CIFAR class 'Airplane' with CNF prior. We find that in the latent space, MCMC smoothly traverses through different modes. The samples \textbf{(d,o,q,r,s)} are distinctly identifiable as airplanes and \textbf{(c)} is partially identifiable as an airplane. The samples approach the correct class after long mixing time.}
    \label{fig:hmc_chain}
\end{figure}

\end{document}
