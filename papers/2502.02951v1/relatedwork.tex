\section{Related Work}
\label{sec:relatedWork}
There are several datasets popular with researchers in VQA. Some of
them classify the questions into groups based on question
\emph{keywords}. 
\begin{description}
\item[COCO-QA]\cite{ren2015} have questions in four categories on
  object identification, number, color and location. All the questions
  have one word answers. It has 123287 images and 117684 questions.
\item[VQA 1.0, VQA 2.0]\cite{antol2015,goyal2017} Both these
  annotated datasets group questions based on the starting words of
  the question like ``What is'', ``Is there'', ``How many'',
  ``Which'', ``What time'', ``What sport is'' , \etc. VQA 2.0 is one
  of the most comprehensive datasets for VQA with many categories of
  images and questions.
\item[CLEVR]\cite{johnson2017} asks questions based on identification
  of attributes,  comparison of attributes, number of objects,
  existence and integer comparison of objects. It has 100,000 images
  and more than 850,000 questions. 
\item[Visual7W]\cite{zhu2016} has seven kinds of queries starting with
  ``what, how, where, who, when, why''  and ``which.''
\item[Visual Genome]\cite{krishna2017} has six question types ``what,
  why, where, who, when'' and ``how'' in a multiple
  choice setting.
\item[TDIUC]\cite{kafle2017} has eight categories of questions
  like presence of object, recognition of subordinate object, count,
  color, other attributes, recognition of activity, sport recognition
  and positional recognition. It has about 1.6 million questions on
  170,000 images.
\end{description}
The above datasets are all extremely large in size but the questions
are not organised systematically into categories with increasing
complexity. The question categories are non-hierarchical and are also
based almost completely \emph{only} on the visual content in the
image. A human being, on the other hand, asks and answers questions
based on the \emph{semantics} and \emph{object properties}. For
example, in a mixed double match showing three players -- two males
and a female -- a question on the gender of the fourth unseen player
in the image is easily answered by a human but such questions are not
present and categorised in the above datasets. Questions that require
external knowledge are also not systematically identified in these
datasets. An example is, ``Which two fruits in the image are similar
in taste?'' Taste has no visual counterpart.

The VQA-Levels dataset systematically categorises the questions into 7
levels (see \cref{7leves}). These questions are also based on
external knowledge which a human is generally expected to have. An
example is, ``How many real towels are there in the image?'' with the
image having a mirror reflection of a single towel.