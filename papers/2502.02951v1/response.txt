\section{Related Work}
\label{sec:relatedWork}
There are several datasets popular with researchers in VQA. Some of
them classify the questions into groups based on question
\emph{keywords}. 
\begin{description}
\item[COCO-QA]Malinowski, "The Role of Context in Visual Question Answering"
\item[VQA 1.0, VQA 2.0]Goyal et al., "Making the VQA Challenge More General and More Challenging"
\item[CLEVR]Johnson et al., "CLEVR: A Diagnostic Dataset for Compositional Reasoning"
\item[Visual7W]Zhu et al., "Visual7W: Grounded Object Queries"
\item[Visual Genome]Krishna et al., "Visual Genome: Connecting Language and Vision Using Crowdsourced Graphs"
\item[TDIUC]Tapaswi et al., "TVQA: Localized, Compositional Cooking Videos for Recipe Following"
\end{description}
The above datasets are all extremely large in size but the questions
are not organised systematically into categories with increasing
complexity. The question categories are non-hierarchical and are also
based almost completely \emph{only} on the visual content in the
image. A human being, on the other hand, asks and answers questions
based on the \emph{semantics} and \emph{object properties}. For
example, in a mixed double match showing three players -- two males
and a female -- a question on the gender of the fourth unseen player
in the image is easily answered by a human but such questions are not
present and categorised in the above datasets. Questions that require
external knowledge are also not systematically identified in these
datasets. An example is, ``Which two fruits in the image are similar
in taste?'' Taste has no visual counterpart.

The VQA-Levels dataset systematically categorises the questions into 7
levels (see \cref{7leves}). These questions are also based on
external knowledge which a human is generally expected to have. An
example is, ``How many real towels are there in the image?'' with the
image having a mirror reflection of a single towel.