Large language models (LLMs) have demonstrated impressive capabilities in various tasks \cite{citechatgpt,openai2023gpt4,liu2024deepseek}.
As a result, many recent works have leveraged LLMs as agents to solve real-world sequential decision-making tasks.
Specifically, some recent works have adopted powerful pre-trained LLMs to solve \emph{multi-armed bandit} (MAB) problems \cite{krishnamurthy2024can,chen2024efficient,xia2024beyond,mukherjee2024pretraining}.
These works usually directly instruct a pre-trained LLM to select the next arm to pull and do not require the costly LLM fine-tuning.
However, this paradigm has been demonstrated to lead to sub-optimal MAB algorithms in many scenarios \cite{krishnamurthy2024can}.
In other words, it has been observed that directly using an LLM for arm selection often struggles to explore efficiently in real-world environments.
To this end, we propose an alternative paradigm which
combines classical MAB algorithms with LLMs such that we can \emph{achieve the best of both worlds}.
Specifically, we leverage a classical MAB algorithm as the high-level framework, and adopt a pre-trained LLM (without fine-tuning) to perform the sub-task of \emph{reward prediction} based on the history of (the features of) the selected arms and their observed rewards.
Compared to the previous approach of directly employing an LLM for arm selection \cite{krishnamurthy2024can}, this allows us to \emph{leverage the strength of LLMs in in-context learning} (ICL) to solve prediction (i.e., supervised learning) tasks.
In other words, instead of using an LLM to replace the MAB algorithm, we \textbf{leverage LLMs to enhance classical MAB algorithms}.

We further motivate our approach by drawing analogy to
recent works aiming to improve the performance of LLMs in complex reasoning tasks via tree search methods \cite{hao2023reasoning,yao2024tree,zhang2024llama,bi2024forest}.
Specifically, these methods often adopt a classical tree search algorithm as the high-level framework (e.g., Monte-Carlo tree search), and use LLMs to perform different sub-tasks such as \emph{reward/value prediction}, action generation, etc.
Therefore, their overall paradigm aligns with our approach of \emph{using classical algorithms to guide the high-level decision-making while leveraging the strengths of LLMs in performing some sub-tasks}.
For example, the work of \citet{koh2024tree} has also used a pre-trained LLM for reward prediction based on the past history to improve classical algorithms.
Specifically, they have adopted best-first search as the high-level 
reasoning framework 
in web automation and used a pre-trained multimodal LLM as a reward/value function in the framework.

In order to incorporate an LLM as a reward predictor into MAB in a principled way, we adopt two classical MAB algorithms as our high-level framework which are naturally amenable to the integration of an LLM-based reward predictor.
Firstly, we adopt the classical Thompson sampling (TS) algorithm \cite{thompson1933likelihood} and use a powerful pre-trained LLM to sample the reward values used in TS, hence introducing our \emph{Thompson Sampling with LLM} (\algts) algorithm.
We ensure a proper balance between exploration and exploitation by carefully controlling the temperature of the LLM. That is, we ensure that the temperature is large enough in the initial stages to achieve sufficient exploration and gradually decay its value to promote more exploitation in later stages.
Secondly, we adopt a \emph{regression oracle}-based MAB algorithm \cite{foster2020beyond} and leverage the LLM as the regression oracle for reward prediction, to introduce our \emph{Regression Oracle-based bandit with LLM} (\algro).
Since the algorithm from \cite{foster2020beyond} is equipped with an explicit exploration mechanism and hence only needs the LLM to provide an accurate reward prediction, we set the LLM temperature to $0$ to remove the randomness in the reward prediction.

In addition to classical stochastic MAB, we also introduce an LLM-enhanced algorithm for \emph{dueling bandits} \cite{JCSS12_yue2012k,li2024feel,verma2024neural}.
In dueling bandits, instead of a single arm, a pair of arms are selected in every iteration, after which a \emph{binary preference observation} is revealed indicating which arm is preferred over the other.
Thanks to the prevalence of preference feedback, dueling bandits are widely applicable in various important real-world scenarios,
such as recommender systems \cite{yang2024conversational}, alignment of LLMs (via reinforcement learning from human feedback) \cite{dwaracherla2024efficient}, among others.
However, adapting our algorithms to dueling bandits is non-trivial due to the need to handle preference feedback (rather than numerical feedback) and to select a pair of arms.
We adapt our \algts~algorithm discussed above to introduce the \emph{Thompson Sampling with LLM for Dueling Bandits} (\algtsduel) algorithm.
In order to achieve a seamless integration of the LLM (as a reward predictor) into dueling bandits, we have leveraged the theoretical equivalence between the maximizers of the \emph{Borda function} and the latent reward function in dueling bandits \cite{mehta2023sample} (more details in Sec.~\ref{subsec:algo:ts:duel}).

Note that in addition to the strong reward prediction capability of LLMs, another benefit of our LLM-enhanced MAB algorithms is that they do not require us to specify the form of the unknown reward function.
Specifically, classical MAB algorithms are usually only able to handle a specific class of reward functions, such as linear reward functions \cite{NIPS11_abbasi2011improved}.
As a result, misspecification of the reward function (i.e., when the groundtruth reward function does not lie in the pre-specified function class) has been an important challenge in MAB, and many efforts have been made to address this difficulty \cite{ghosh2017misspecified,wang2024online}.
In contrast, due to the flexibility of LLMs to predict reward functions of varying degrees of complexity, our algorithms can automatically adapt to the level of difficulty of the problem.
As a result, we are free from the requirement to specify the class of reward functions beforehand.

We use extensive experiments to demonstrate the empirical advantage of our algorithms.
We firstly use synthetic stochastic MAB experiments to show that our \algts~and \algro~algorithms both consistently outperform baseline methods which directly instruct the LLM to select actions (Sec.~\ref{subsec:exp:classical}).
Next, we show that our \algtsduel~algorithm achieves small regrets in synthetic dueling bandit experiments (Sec.~\ref{subsec:exp:dueling}).
We also apply our \algts~to contextual MAB experiments designed using two real-world text datasets (Sec.~\ref{subsec:exp:text}).
The results show that in tasks where the LLM can exploit the semantic meanings of the arm features (to accurately predict the association between the contexts and arms), directly instructing the LLM to select actions leads to strong performance which is comparable to our \algts.
In other more challenging tasks in which the arms lack such semantic information, LLM-based direct arm selection suffers from significant performance degradation, and our \algts~performs dramatically better.
We expect our findings to provide useful and practical guidelines for future works and applications adopting LLMs as agents to solve real-world sequential decision-making tasks.
