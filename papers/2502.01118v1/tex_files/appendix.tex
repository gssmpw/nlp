\section{More Experimental Details}
In all our synthetic experiments (Secs.~\ref{subsec:exp:classical} and \ref{subsec:exp:dueling}), the MAB tasks have $K=16$ features and the feature vectors of the arms are $4$-dimensional.

\subsection{The Prompt Template Adopted by Our Algorithms}
\label{app:subsec:prompt:template:our:algorithm}


Below is the prompt we have used for our \algts~algorithm (Algo.~\ref{algo:ts}) and \algro~algorithm (Algo.~\ref{algo:ro}) in classical stochastic bandits experiment in Sec.~\ref{subsec:exp:classical}. Here every {\color{blue}[INPUT]} contains the feature vectors of an arm, and every {\color{blue}[OUTPUT]} corresponds to its corresponding observed reward.
\begin{mycolorbox}{Query}{Prompt for Our \algts~and \algro}
\small
Help me predict the function value at the last input. Each function value is associated with a Normal distribution with a fixed but unknown mean. Your response should only contain the function value in the format of \#function value\#.\\
input: {\color{blue}[INPUT]}, output: {\color{blue}[OUTPUT]}\\
input: {\color{blue}[INPUT]}, output: {\color{blue}[OUTPUT]}\\
...\\
input: {\color{blue}[INPUT]}, output:
\\
\end{mycolorbox}

The template below is the prompt we have used for our \algtsduel~algorithm (Algo.~\ref{algo:ts:duel}) in the dueling bandit experiment in Sec.~\ref{subsec:exp:dueling}.
Here every {\color{red}[INPUT]} contains the difference or concatenation of the feature vectors of a pair of arms (see Sec.~\ref{subsec:exp:dueling} for more details), and every {\color{red}[OUTPUT]} corresponds to a binary observation which is equal to $1$ if the first arm is preferred over the second arm and $0$ otherwise.
Although the output labels for each data point in the prompt is binary, here we have instructed the LLM to predict a continuous value, to ensure that the LLM-generated output can be used as the preference probability.
\begin{mycolorbox}{Query}{Prompt for Our \algtsduel}
\small
Help me predict the value for the last input as a continuous value between 0 and 1. Your response MUST only contain the value in the format of \#value\#.\\
input: {\color{red}[INPUT]}, output: {\color{red}[OUTPUT]}\\
input: {\color{red}[INPUT]}, output: {\color{red}[OUTPUT]}\\
...\\
input: {\color{red}[INPUT]}, output:
\\
\end{mycolorbox}


\subsection{More Details on The Synthetic Experiments (Secs.~\ref{subsec:exp:classical} and \ref{subsec:exp:dueling})}
\label{app:subsec:more:details:synth:exp}
In our synthetic experiments in Sec.~\ref{subsec:exp:classical}, we have adopted synthetic functions as the reward functions $f$, including linear function: $f(x) = \theta^{\top} x$, square function: $f(x) = (\theta^{\top} x)^2$, sinusoidal function: $f(x) = \sin(\theta^{\top} x)$, and a function sampled from a Gaussian process with a length scale of 0.4.
We repeat each experiment $10$ times with a different random seed for each repetition. We run each method for $100$ iterations, with the initial $2$ arms randomly selected. We add a Gaussian noise with a noise variance of $0.02$ to each observation.
In all our experiments here, we have adopted the optimal schedule for the temperature discovered in Sec.~\ref{ablation:subsec:temperature} (Fig.~\ref{fig:different:temperatures}).

In our synthetic experiments on dueling bandits in Sec.~\ref{subsec:exp:dueling}, we adopt the following latent reward functions: linear function: $f(x) = \theta^{\top} x$, and square function: $f(x) = (\theta^{\top} x)^2$.
We repeat each experiment $5$ times with a different random seed for each repetition. We run each method for $150$ iterations, with the initial $2$ arms randomly selected.
As we have discussed in Sec.~\ref{subsec:exp:dueling}, we use a decaying schedule of LLM temperatures, and adopt a smaller schedule of temperatures when selecting the first arm to encourage exploitation. 
Specifically, for linear latent reward function, in iteration $t$, we use $\text{temp}(t) = 1.5 - \min(0.1 \times \sqrt{t}, 1.4)$ as the temperature when selecting the first arm and use $\text{temp}(t) = 1.5 - \min(0.1 \times \sqrt{t}, 1.1)$ when choosing the second arm.
For the square latent reward function, we adopt larger values of the temperature, because the non-linear reward function makes the dueling bandit problem more challenging and hence a larger degree of exploration is needed. Specifically, we use $\text{temp}(t) = 1.6 - \min(0.13 \times \sqrt{t}, 1.5)$ when choosing the first arm and let $\text{temp}(t) = 1.6 - \min(0.13 \times \sqrt{t}, 1.1)$ when selecting the second arm.

In our experiments here, we use the BTL model to obtain the preference observation (Sec.~\ref{subsec:problem:setting:dueling}).
Specifically, after a pair of arms $i_{t,1}$ and $i_{t,2}$ are selected, we firstly calculate their preference probability: 
\begin{equation}
\mathbb{P}(x_{i_{t,1}} \succ x_{i_{t,2}}) =  \frac{1}{1+e^{-10(f(x_{i_{t,1}}) - f(x_{i_{t,2}}))}}.
\end{equation}
We have added a $10$ in the exponent to reduce the noise in the preference observations and hence simplify the dueling bandit problem.
Then, we sample the binary reward observation $r_t$ from a Bernoulli distribution with the probability $\mathbb{P}(x_{i_{t,1}} \succ x_{i_{t,2}})$.



\subsection{More Details about the Baseline Algorithms}
Here we present the prompts we have used for different baseline algorithms we have used in Sec.~\ref{subsec:exp:classical}.
Specifically, how we have modified the prompt from the LLM-based MAB method from \cite{krishnamurthy2024can} in different ways in order to incorporate the features of the arms, to make their method comparable with our algorithms.
We have highlighted the arm features we have added in {\color{blue}blue}.


\begin{mycolorbox}{Query}{Baseline: NoFeature}
\scriptsize
You are in a room with 16 buttons labeled\\
\mbox{['blue', 'green', 'red', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'lime', 'pink', 'teal', 'lavender', 'brown', 'beige', 'maroon', 'mint']}\\
Each button is associated with a Normal distribution with a fixed but unknown mean; the means for the buttons could be different and are associated with features of buttons. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution.\\
You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps. So far you have played [TIMES] times with the following choices and rewards:\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
...\\
You MUST output a distribution over the 16 buttons as probabilities, formatted EXACTLY like this example: \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#. Each probability value(p1,p2,...,p16) MUST be a number between 0 and 1, and the total of all probabilities MUST equal 1.\\
Let's think step by step to make sure we make a good choice. Which button will you choose next? YOU MUST provide your final answer within the tags \textless Answer\textgreater DIST \textless /Answer\textgreater where DIST is \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#.\\
\end{mycolorbox}



\begin{mycolorbox}{Query}{Baseline: FramingFeature}
\scriptsize
You are in a room with 16 buttons labeled\\
\mbox{['blue', 'green', 'red', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'lime', 'pink', 'teal', 'lavender', 'brown', 'beige', 'maroon', 'mint']}\\
{\color{blue}Feature of \mbox{[COLOR]} button: \mbox{[FEATURE]}\\
Feature of \mbox{[COLOR]} button: \mbox{[FEATURE]}\\
...}\\
Each button is associated with a Normal distribution with a fixed but unknown mean; the means for the buttons could be different and are associated with features of buttons. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution.\\
You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps. So far you have played \mbox{[TIMES]} times with the following choices and rewards:\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
...\\
You MUST output a distribution over the 16 buttons as probabilities, formatted EXACTLY like this example: \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#. Each probability value(p1,p2,...,p16) MUST be a number between 0 and 1, and the total of all probabilities MUST equal 1.\\
Let's think step by step to make sure we make a good choice. Which button will you choose next? YOU MUST provide your final answer within the tags \textless Answer\textgreater DIST \textless /Answer\textgreater where DIST is \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#.\\
\end{mycolorbox}

\begin{mycolorbox}{Query}{Baseline: HistoryFeature}
\scriptsize
You are in a room with 16 buttons labeled\\
\mbox{['blue', 'green', 'red', 'yellow', 'purple', 'orange', 'cyan', 'magenta', 'lime', 'pink', 'teal', 'lavender', 'brown', 'beige', 'maroon', 'mint']}\\
Each button is associated with a Normal distribution with a fixed but unknown mean; the means for the buttons could be different and are associated with features of buttons. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution.\\
You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps.\\
{\color{blue}Feature of \mbox{[COLOR]} button: \mbox{[FEATURE]}\\
Feature of \mbox{[COLOR]} button: \mbox{[FEATURE]}\\
...}\\
So far you have played \mbox{[TIMES]} times with the following choices and rewards:\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
\mbox{[COLOR]} button, reward \mbox{[REWARD]}\\
...\\
You MUST output a distribution over the 16 buttons as probabilities, formatted EXACTLY like this example: \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#. Each probability value(p1,p2,...,p16) MUST be a number between 0 and 1, and the total of all probabilities MUST equal 1.\\
Let's think step by step to make sure we make a good choice. Which button will you choose next? YOU MUST provide your final answer within the tags \textless Answer\textgreater DIST \textless /Answer\textgreater where DIST is \#[COLOR]:p1,[COLOR]:p2,...,[COLOR]:p16\#.\\
\end{mycolorbox}



\subsection{More Details on the Text Experiments}
\label{app:subsec:exp:text}
Here we present more details on the experiment in Sec.~\ref{subsec:exp:text} in which we have adopted a real-world text dataset.
Every experiment in this section is repeated $10$ times with a different random seed in every repetition.

In the experiment using the \texttt{OneShotWikiLinks} dataset, contexts exceeding 400 words were first removed. Then, 10 concept names were randomly selected, each associated with 2,000 to 3,000 contexts. Finally, 2,000 contexts were randomly sampled for each of these 10 concept names.
For the experiment using the \texttt{AmazonCat-13K} dataset, contexts exceeding 500 characters in length were first removed. Then, only data containing a single item tag was retained. Finally, the top 10 or 30 item tags with the highest number of contexts were selected, and all corresponding data were used as experimental data. The number of data samples for the 10-arm and 30-arm experiments were 34,227 and 40,287, respectively.

We display below the prompts we have used for our \algts~algorithm
and the baseline algorithm 
in the two text datasets.
For fair comparisons, we keep most of the contents between the prompts of the two methods identical. Therefore, the only major difference between the prompts of the two methods is that the prompt for the baseline method directly instructs the LLM to select the next arm to pull.
On the other hand, in the prompt for our \algts~algorithm, we let the LLM predict the score of a combination of a context and an arm.
As a result, the prompt for our \algts~bears a larger degree of resemblance to standard in-context learning, because we are effectively leveraging the LLM to solve a supervised learning task.

\begin{mycolorbox}{Query}{Prompt for \algts~in OneShotWikiLinks task}
\small
**Task Description**\\
At the TEST DATA, Please assign a reward indicating how well the Incomplete Text aligns with the Previous Text and Next Text.\\\\
**reward**:\\
- 0 indicates poor alignment.\\
- 1 indicates perfect alignment.\\
- A reward closer to 1 should only be assigned when the Incomplete Text is perfectly aligned with the surrounding texts.\\\\
**The Incomplete Text can be one of the following words**:\\
\mbox{['Microsoft Windows', 'Telugu', 'XML', 'Moscow', 'help', 'MTV', 'Halloween', 'Ottoman Empire', 'Soviet', 'Bangladesh'].}\\\\
The reward value MUST be a number between 0 and 1. Your response MUST be the reward value only, formatted as \#reward value\#.\\\\
Below are previous examples:\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: \mbox{[INCOMPLETE TEXT]}\\
**Reward**: \mbox{[REWARD]}\\\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: \mbox{[INCOMPLETE TEXT]}\\
**Reward**: \mbox{[REWARD]}\\

...\\

\#\#\#TEST DATA:\\
This is the TEST DATA for which the reward needs to be assigned:\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: \mbox{[INCOMPLETE TEXT]}\\
**Reward**:
\end{mycolorbox}





\begin{mycolorbox}{Query}{Prompt for the Baseline Method in OneShotWikiLinks task}
\small
The task is to choose the most suitable word to complete the Incomplete Text from the following list of options in order to earn the most reward:\\
\mbox{['Microsoft Windows', 'Telugu', 'XML', 'Moscow', 'help', 'MTV', 'Halloween', 'Ottoman Empire', 'Soviet', 'Bangladesh'].}\\
Your response MUST only contain one word from the list.\\\\
Reward indicates how well the Incomplete Text aligns with the Previous Text and Next Text.\\
- 0 indicates poor alignment.\\
- 1 indicates perfect alignment.\\\\
Below is the historical data:\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: \mbox{[INCOMPLETE TEXT]}\\
**Reward**: \mbox{[REWARD]}\\\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: \mbox{[INCOMPLETE TEXT]}\\
**Reward**: \mbox{[REWARD]}\\

...\\

Below is the incomplete text for which you need to complete:\\
**Previous Text**: \mbox{[PREVIOUS TEXT]}\\
**Next Text**: \mbox{[NEXT TEXT]}\\
**Incomplete Text**: 
\end{mycolorbox}



\begin{mycolorbox}{Query}{Prompt for \algts~in AmazonCat task}
\small
There are Titles and Contents of some items. \\\\
Labels and items correspond one-to-one.\\
There are a total of 10 items.The Labels MUST be ONE of the following numbers: \mbox{[2571, 1471, 7961, 12246, 5754, 342, 5456, 5960, 11235, 10688]}\\\\
The Reward is a number between 0 and 1 determined by whether the Label is correct or not.\\\\
Help me predict the Reward at the last Title, Content and Label.\\\\
Your response MUST be the predicted Reward only, formatted as \#predicted Reward\#.\\\\
**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**: \mbox{[Label]}\\
**Reward**: \mbox{[REWARD]}\\\\
**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**: \mbox{[Label]}\\
**Reward**: \mbox{[REWARD]}\\

...\\

**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**: \mbox{[Label]}\\
**Reward**:
\end{mycolorbox}


\begin{mycolorbox}{Query}{Prompt for the Baseline Method in AmazonCat task}
\small
There are Titles and Contents of some items. \\\\
Labels and items correspond one-to-one.\\
There are a total of 10 items.The Labels MUST be ONE of the following numbers: \mbox{[2571, 1471, 7961, 12246, 5754, 342, 5456, 5960, 11235, 10688]}\\\\
The Reward is a number between 0 and 1 determined by whether the Label is correct or not.\\\\
Help me choose the correct Label at the last Title and Content. Your response MUST be the chosen Label only, formatted as \#chosen Label\#.\\\\
**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**: \mbox{[Label]}\\
**Reward**: \mbox{[REWARD]}\\\\
**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**: \mbox{[Label]}\\
**Reward**: \mbox{[REWARD]}\\

...\\

**Title**: \mbox{[Title]}\\
**Content**: \mbox{[Content]}\\
**Label**:
\end{mycolorbox}




