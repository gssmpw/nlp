\subsection{Thompson Sampling with LLM (\algts)}
\label{subsec:algo:ts}
\begin{algorithm}
\begin{algorithmic}[1]
	\FOR{iteration $t=1,\ldots,T$}
            \FOR{arm $i=1,\ldots,K$}
                \STATE $\widehat{r}_{t,i} = \text{LLM}(\mathcal{D}_{t-1}, x_i)$ // predict reward
            \ENDFOR
            \STATE Select arm $i_t = {\arg\max}_{i=1,\ldots,K}\widehat{r}_{t,i}$, observe reward $r_t$
            \STATE Update history $\mathcal{D}_t = \mathcal{D}_{t-1} \cup \{(x_{i_t},r_t)\}$
        \ENDFOR
\end{algorithmic}
\caption{\algts}
\label{algo:ts}
\end{algorithm}

Our \algts~algorithm (Algo.~\ref{algo:ts}) employs the LLM to predict the reward of every arm and leverages \emph{the inherent randomness in the LLM-generated text} to achieve exploration. 
Specifically, in every iteration $t$, we include the current history of observations $\mathcal{D}_{t-1}=\{x_{i_s}, r_s\}_{s=1,\ldots,t-1}$ in the prompt for the LLM.
For each arm $i=1,\ldots,K$, we append its feature vector $x_i$ to the end of the prompt and instruct the LLM to predict its reward $\widehat{r}_{t,i}$ (line 3 of Algo.~\ref{algo:ts}).
The prompt adopted in this step is illustrated in 
App.~\ref{app:subsec:prompt:template:our:algorithm}.
Then, the arm with the largest predicted reward $\widehat{r}_{t,i}$ is selected (line 3 of Algo.~\ref{algo:ts}).

To achieve a gradual transition from exploration to exploitation, we choose a schedule for the temperature of the LLM which \emph{decays across iterations}. 
As a result, at the initial stage when significant exploration is required, we use a large temperature to induce sufficient randomness in the LLM-generated reward prediction. 
In later stages when a larger degree of exploitation is more beneficial, we use a small temperature to reduce the randomness in the reward prediction.
This allows us to naturally combine the powerful reward prediction form the LLM, thanks to its impressive in-context learning (ICL) capability, and the classical TS algorithm to derive a coherent algorithm for arm selection in MAB.
We empirically verify that such a decaying schedule of temperatures indeed achieves better performance than using a fixed temperature in Sec.~\ref{ablation:subsec:temperature}.

\textbf{Justifications for \algts~(Algo.~\ref{algo:ts}).}
Our \algts~algorithm shares a similar motivation with some previous works which have also relied on the randomness in the output generated by the LLM to achieve exploration in sequential decision-making tasks.
For example, the work of \citet{yang2023large} has adopted an LLM with a large temperature to select a batch of diverse input queries for Bayesian optimization; the work of \citet{liu2024large} has used an LLM to predict the performance achieved by different hyperparameter configurations in Bayesian optimization, and used the variance of multiple independently sampled predictions from the LLM as the exploration term in their upper confidence bound-based algorithm.
Another line of works with similar underlying principles as our \algts~is approximating Thompson sampling (TS) with neural networks.
Some previous works have adopted an ensemble of neural networks (NNs) \cite{osband2016deep,osband2023epistemic,dwaracherla2024efficient} and performed approximate TS by randomly sampling from the ensemble.
In contrast, we approximate the posterior distribution of rewards in TS using the stochastic predictions generated by the LLM in our \algts.

\subsection{Regression Oracle-Based Bandit with LLM (\algro)}
\label{subsec:algo:ro}
\begin{algorithm}[h]
\begin{algorithmic}[1]
	\FOR{iteration $t=1,\ldots,T$}
            \FOR{arm $i=1,\ldots,K$}
                \STATE $\widehat{l}_{t,i} = \text{LLM}(\mathcal{D}_{t-1}, x_i)$ // predict loss
            \ENDFOR
            \STATE Let $j_t = {\arg\min}_{i=1,\ldots,K}\widehat{l}_{t,i}$
            \FOR{arm $i=1,\ldots,K$ and $i\neq j$}
                \STATE $p_{t,i} = \frac{1}{\mu + \gamma (\widehat{l}_{t,i} - \widehat{l}_{t,j_t})}$
            \ENDFOR
            \STATE Let $p_{t,j_t} = 1 - \sum_{i\neq j_t}p_{t,i}$
            \STATE Sample $i_t \sim p_{t}$, observe loss $l_t$ (negated reward)
            \STATE Update history $\mathcal{D}_t = \mathcal{D}_{t-1} \cup \{(i_t,l_t)\}$
        \ENDFOR
\end{algorithmic}
\caption{\algro}
\label{algo:ro}
\end{algorithm}
A line of works have proposed to adopt a generic \emph{regression oracle} for reward prediction in MAB, and incorporated explicit exploration mechanisms to derive theoretically principled algorithms \cite{foster2018practical,foster2020beyond}.
Interestingly, the high-level principle of these works aligns well with our approach in this work, i.e., adopting a model capable of reward prediction (i.e., a regression oracle in these previous works and an LLM in our work) and utilizing a separate high-level framework to achieve exploration.
Therefore, here we incorporate an LLM as the regression oracle into the \texttt{SquareCB} algorithm from \citet{foster2020beyond}, hence proposing our \algro~algorithm (Algo.~\ref{algo:ro}).
To be consistent with \citet{foster2020beyond}, instead of rewards, we consider the observations as \emph{losses} (line 10 of Algo.~\ref{algo:ro}), which are simply the negation of rewards.

In every iteration $t$ of our \algro~algorithm, we use the LLM to predict the loss $\widehat{l}_{t,i}$ of every arm $i$ (line 3 of Algo.~\ref{algo:ro}).
Here we adopt the same prompt as the \algts~algorithm (shown in App.~\ref{app:subsec:prompt:template:our:algorithm}), except that here we use losses as observations rather than rewards.
Next, we choose the arm with the smallest predicted loss and denote it as $j_t$ (line 4).
After that, we use the LLM-based loss predictions to construct a distribution $p_{t}$ over all $K$ arms (lines 5-7), from which the next arm $i_t$ is sampled (line 8).
Note that the \texttt{SquareCB} algorithm from \citet{foster2020beyond} is equipped with an explicit exploration mechanism (via the sampling distribution $p_t$).
As a result, unlike our \algts~algorithm, here we no longer need to exploit the inherent randomness in the LLM-generated output to achieve exploration.
Therefore, when using the LLM for loss prediction in our \algro~algorithm (line 3 of Algo.~\ref{algo:ro}), we set the temperature of the LLM to $0$ and hence obtain deterministic reward predictions.


\subsection{Thompson Sampling with LLM for Dueling Bandits (\algtsduel)}
\label{subsec:algo:ts:duel}
\begin{algorithm}[h]
\begin{algorithmic}[1]
	\FOR{iteration $t=1,\ldots,T$}
            \FOR{arm $i=1,\ldots,K$}
                \FOR{uniformly sampled arm $j=1,\dots,N$}
                    \STATE $\widehat{p}_{t,i,j} = \text{LLM}(\mathcal{D}_{t-1}, [x_i, x_j])$
                \ENDFOR
            \STATE    Calculate $\widehat{r}_{t,i} = \frac{1}{N}\sum^N_{n=1}\widehat{p}_{t,i,n}$
            \ENDFOR
            \STATE Select the first arm $i_{t,1} = {\arg\max}_{i=1,\ldots,K}\widehat{r}_{t,i}$
            \FOR{arm $j=1,\ldots,K$}
                \STATE $\widehat{p}_{t,j} = \text{LLM}(\mathcal{D}_{t-1}, [x_j, x_{i_{t,1}}])$ 
            \ENDFOR
            \STATE Select the second arm $i_{t,2} = {\arg\max}_{i=1,\ldots,K}\widehat{p}_{t,i}$
            \STATE Observe binary preference $r_t = \mathbbm{1}(i_{t,1} \succ i_{t,2})$
            \STATE Update history $\mathcal{D}_t = \mathcal{D}_{t-1} \cup \{([i_{t,1}, i_{t,2}],r_t)\}$
        \ENDFOR
\end{algorithmic}
\caption{\algtsduel}
\label{algo:ts:duel}
\end{algorithm}
Here we introduce our \algtsduel~algorithm 
for dueling bandit,
in which we select a pair of arms $i_{t,1}$ and $i_{t,2}$ in every iteration and collect a binary observation indicating their relative preference $r_t = \mathbbm{1}(i_{t,1} \succ i_{t,2})$.

\textbf{Preference Probability Prediction.}
In contrast to our \algts~(Algo.~\ref{algo:ts}) and \algro~(Algo.~\ref{algo:ro}) which use an LLM to predict the \emph{reward} of every arm, our \algtsduel~algorithm (Algo.~\ref{algo:ts:duel}) instead adopts an LLM to predict the \emph{probability that an arm is preferred over another arm}.
Specifically, when adopting the LLM for preference probability prediction via ICL (line 4 of Algo.~\ref{algo:ts:duel}), for the $s^{\text{th}}$ input-output pair in the dataset $\mathcal{D}_{t-1}$ included in the prompt, the input corresponds to \emph{the features of the pair of arms $x_{i_{s,1}}$ and $x_{i_{s,2}}$} (instead of a single arm in Algo.~\ref{algo:ts} and Algo.~\ref{algo:ro}).
The corresponding output represents the observed preference $r_s = \mathbbm{1}(i_{s,1} \succ i_{s,2})$.
When predicting the preference probability of a pair of arms $x_i$ and $x_j$, we append their features at the end of the prompt, denoted as $[x_i,x_j]$ (line 4 of Algo.~\ref{algo:ts:duel}).
As a result, the LLM is able to \emph{predict the probability that the first arm $x_i$ is preferred over the second arm $x_j$}, i.e., predict $\mathbb{P}(x_i \succ x_j)$.
The prompt template we have adopted here is shown in 
App.~\ref{app:subsec:prompt:template:our:algorithm}.

\textbf{Representing The Features of Arm Pairs.}
We adopt two approaches to incorporate the features of a pair of arms into the prompt.
Firstly, when the latent reward function $f$ is linear: $f(x) = \theta^{\top}x$, we have that $\mathbb{P}(x_1 \succ x_2) = \mu(f(x_1) - f(x_2))=\mu\left(\theta^{\top}(x_1 - x_2)\right)$.
That is, the preference probability $\mathbb{P}(x_1 \succ x_2)$ is a function of the difference $x_1 - x_2$.
Therefore, we use the difference between the feature vectors of the first arm and second arm (i.e., $x_1 - x_2$) in the prompt.
Secondly, when the latent reward function is non-linear, the preference probability is no longer a function of $x_1 - x_2$.
In this case, we concatenate the feature vectors of $x_1$ and $x_2$ and included them in the prompt.

\begin{figure*}[t]
     \centering
     \begin{tabular}{cccc}
        \hspace{-5mm}
         \includegraphics[width=0.26\linewidth]{figures/bandits/bandits_linear.pdf} & \hspace{-5.7mm} 
         \includegraphics[width=0.26\linewidth]{figures/bandits/bandits_square.pdf} & \hspace{-5.7mm}
         \includegraphics[width=0.26\linewidth]{figures/bandits/bandits_sin.pdf}& \hspace{-5.7mm}
         \includegraphics[width=0.26\linewidth]{figures/bandits/bandits_GP.pdf}\\
         {\hspace{-3mm}\small  Linear Reward Function} & {\hspace{-3mm}\small  Square Reward Function} & \hspace{-5.7mm}
         {\hspace{-3mm} \small  Sinusoidal Reward Function} & {\hspace{-3mm} \small  Function Sampled from GP}
     \end{tabular}
     \caption{
    The performance of our \algts~and \algro~algorithms in classical stochastic MAB tasks.
     }
    \label{fig:synth:mab}
\end{figure*}
\textbf{Selection of A Pair of Arms.}
To select the pair of arms $i_{t,1}$ and $i_{t,2}$ in every iteration,
we draw inspirations from the arm selection strategy from the work of \citet{verma2024neural}.
Specifically, in iteration $t$, for every arm $i$, we use the LLM to predict the probability that arm $i$ is preferred over $N$ uniformly sampled arms and calculate their average predicted probability $\widehat{r}_{t,i}$ (line 3-5 of Algo.~\ref{algo:ts:duel}).
Then, we choose the first arm by maximizing $\widehat{r}_{t,i}$ (line 6).
This is equivalent to approximately maximizing the \emph{Borda function} $f_{\text{borda}}$ \cite{xu2020zeroth}, which is defined as the expected probability that an arm is preferred over a randomly selected arm: $f_{\text{borda}}(x)=\mathbb{E}_{j\in\mathcal{U}([K])}[\mathbb{P}(x \succ x_j)]$ where $\mathcal{U}([K])$ denotes the uniform distribution among all $K$ arms.
Specifically, we estimate the expectation in $f_{\text{borda}}(x)$ by uniformly and independently sampling $N$ arms (lines 3-5 of Algo.~\ref{algo:ts:duel}).
Theoretically, maximizing the Borda function $f_{\text{borda}}$ is equivalent to maximizing the latent reward function $f$ (see Sec.~\ref{subsec:problem:setting:dueling}) \cite{mehta2023sample}.
Therefore, the first arm $i_{t,1}$ is selected greedily, i.e., via pure exploitation.
As a result, after each iteration $t$, we let our \algtsduel~algorithm recommend the first arm as the best arm.
To choose the second arm, we firstly predict the probability that each arm is preferred over the first arm $i_{t,1}$ (lines 7-8 of Algo.~\ref{algo:ts:duel}), and then select the second arm by maximizing this predicted probability (line 9).
This is 
inspired by the TS-based algorithm from \citet{verma2024neural}, which encourages the second selected arm to both have large reward and be different from $i_{t,1}$ and all previously selected arms.
The work of \citet{verma2024neural} has theoretically shown that such an approach to selecting the pair of arms lead to strong performances (i.e., small cumulative regrets) both in theory and in practice.


