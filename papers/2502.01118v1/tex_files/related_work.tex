\paragraph{LLM-Based Multi-Armed Bandits (MAB).}
The work of \citet{krishnamurthy2024can} has used an LLM to sequentially choose the arms in MAB. They have consider standard MAB problems with a finite number of arms, and their results have shown that LLMs struggle in MAB tasks in most scenarios (i.e., for most of their prompt designs).
More recently, the work of \cite{chen2024efficient} has proposed to adopt an LLM-based arm selection strategy in the initial stage of MAB and gradually switch to classical MAB algorithms in later stages. However, their method requires the availability of the likelihood of the LLMs and are hence not able to adopt the typically more powerful black-box LLMs such as ChatGPT \cite{citechatgpt,openai2023gpt4}.
The work of \citet{xia2024beyond} has proposed an LLM-based algorithm for dueling bandits. Compared with our \algtsduel~(Sec.~\ref{subsec:algo:ts:duel}), they have considered a simpler setting of dueling bandits in which the preference feedback is generated by a preference matrix. In contrast, we have adopted the BTL model (Sec.~\ref{subsec:problem:setting:dueling}), which allows us to take into account the arm features and hence makes our setting more general.
The work of \citet{mukherjee2024pretraining} has proposed to train a decision transformer to predict the rewards of different arms in MAB and hence to assist in arm selection.
In contrast, our algorithms can adopt any black-box LLM and does not require the potentially expensive training procedure.

\paragraph{Other LLM-Based Sequantial Decision-Making Methods.}
In addition to MAB, some previous works have proposed methods to incorporate LLMs into other sequential decision-making algorithms.
For example, some prior works have used LLMs to improve the performance of Bayesian optimization (BO) by either directly instructing the LLM to sequentially select the input queries in BO \cite{yang2023large} or using LLMs to enhance different components of BO (such as initial input selection, surrogate model prediction, etc.) \cite{liu2024large}.
A number of recent works have used the transformer model to learn a policy for action selection in reinforcement learning \cite{dai2024context,laskin2022context,lee2024supervised}.
The field of LLM-based agents is broad and has garnered significant attention due to the rapidly advancing capabilities of modern LLMs.
Many surveys on LLM-based agents have been released \cite{cheng2024exploring,wang2024survey,xi2023rise}, offering comprehensive overviews of this area.
