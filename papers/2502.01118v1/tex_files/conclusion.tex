In this work, we propose an alternative paradigm of LLM-based sequential decision-making and focus on the MAB problem.
We adopt a classical MAB algorithm as the high-level framework and leverage the strong in-context learning capability of LLMs to perform the sub-task of reward prediction in MAB.
We propose our \algts~and \algro~for classical stochastic MAB and our \algtsduel~for dueling bandits.
Synthetic experiments demonstrate that our algorithms consistently outperform baseline methods of LLM-based direct arm selection.
Through contextual MAB experiments designed using two real-world text datasets, we show that 
in challenging tasks where the arm features are not associated with semantic meanings exploitable by the LLM,
our \algts~achieves dramatically better performance than LLM-based direct arm selection.
As future work, we plan to apply our algorithms to handle more complicated sequential decision-making problems, such as those from commonly used benchmarks for LLM-based agents such as \citet{liu2023agentbench,wu2023smartplay,xi2024agentgym}.
