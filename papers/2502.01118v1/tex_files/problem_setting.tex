\subsection{Multi-Armed Bandits (MAB)}
In our problem setting, every arm $i=1,\ldots,K$ is associated with a $d$-dimensional feature vector $x_i\in\mathbb{R}^d$ and the reward of an arm $i$ is a function of its feature vector $x_i$: $f(x_i)$.
For example, in the classical linear bandits, the reward of arm $i$ is given by a linear function: $f(x_i) = \theta^{\top} x_i$ with an unknown $\theta$.
In every iteration $t$, an MAB algorithm selects an arm $i_t$ to pull, and observes a corresponding noisy reward $y_t = f(x_{i_t}) + \epsilon$ where $\epsilon$ is usually a zero-mean Gaussian noise.
The goal of an MAB algorithm is usually to minimize the \emph{cumulative regret}: $R_T = \sum^T_{t=1} [f(x_{i^*}) - f(x_{i_t})]$ where $i^* = {\arg\max}_{i=1,\ldots,K}f(x_{i})$ represents the optimal arm.

We also consider the setting of contextual MAB (Sec.~\ref{subsec:exp:text}), in which in every iteration $t$, we receive a new set of $K$ arms denoted as $\mathcal{I}_t=\{i^t_1,\ldots,i^t_K\}$ and choose an arm $i_t$ from $\mathcal{I}_t$. 
When selecting an arm in iteration $t$, an MAB algorithm needs to make use of (the feature vectors of) the previously selected arms and their corresponding rewards: $\mathcal{D}_{t-1}=\{(x_{i_s}, r_s)\}_{s=1,\ldots,t-1}$.
Therefore, we will include $\mathcal{D}_{t-1}$ in the prompt for the LLM-based agent in our algorithms.


\subsection{Dueling Bandits}
\label{subsec:problem:setting:dueling}
In dueling bandits, in every iteration $t$, we select a pair of arms $i_{t,1}$ and $i_{t,2}$ and observe binary preference feedback $r_t = \mathbbm{1}(i_{t,1}\succ i_{t,2})$, which is equal to $1$ if $i_{t,1}$ is preferred over $i_{t,2}$ and $0$ otherwise.
We assume that the preference observation $r_t$ is generated by the commonly adopted BTL model \cite{Book_luce2005individual,AS04_hunter2004mm}.
Specifically, there exists a latent reward function $f$ which maps the feature vector $x_i$ of an arm $i$ to its corresponding latent reward value $f(x_i)$.
For a pair of arms $i_{t,1}$ and $i_{t,2}$, the preference probability (i.e., the probability that arm $i_{t,1}$ is preferred over arm $i_{t,2}$) under the BTL model is given by
$$
\mathbb{P}(i_{t,1} \succ i_{t,2}) = \mu(f(x_{i_{t,1}}) - f(x_{i_{t,2}})),
$$
in which $\mu: \mathbb{R} \rightarrow [0,1]$ is the logistic function: $\mu(z) = 1/(1+e^{-z})$.
The preference observation $r_t=\mathbbm{1}(i_{t,1}\succ i_{t,2})$ is then assumed to be sampled from a Bernoulli distribution with the probability $\mathbb{P}(i_{t,1} \succ i_{t,2})$.
The performance of a dueling bandit algorithm is also often measured by regret. A common notion of regret is $R_T = \sum^T_{t=1} [2f(x_{i^*}) - f(x_{i_{t,1}}) - f(x_{i_{t,2}})]$.
However, in practical applications, we usually need to devise a method to recommend an arm during the dueling bandit algorithm \cite{lin2024prompt}.
Our LLM-based algorithm for dueling bandits recommends the first selected arm $i_{t,1}$ as the best arm (more details in Sec.~\ref{subsec:algo:ts:duel}).
Therefore, in our experiments (Sec.~\ref{subsec:exp:dueling}), we report the regret of the first arm: $R_T = \sum^T_{t=1} [f(x_{i^*}) - f(x_{i_{t,1}}))]$, which we believe is more relevant in practice.
