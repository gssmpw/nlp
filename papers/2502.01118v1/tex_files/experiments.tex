We firstly apply our \algts~and \algro~algorithms to synthetic stochastic MAB tasks with both linear and non-linear reward functions (Sec.~\ref{subsec:exp:classical}).
Next, we apply our \algtsduel~algorithm to solve synthetic dueling bandit problems (Sec.~\ref{subsec:exp:dueling}).
Lastly, we adopt MAB tasks designed using two real-world text datasets (Sec.~\ref{subsec:exp:text}) to unveil some interesting insights about our algorithms.
We adopt GPT-3.5-Turbo \cite{citechatgpt} as the black-box LLM in the majority of our experiments, and also use DeepSeek-V3 \cite{liu2024deepseek} in the experiments in (Sec.~\ref{subsec:exp:text}).

\subsection{\algts~and \algro~for Classical Stochastic MAB}
\label{subsec:exp:classical}
Here we compare our \algts~and \algro~algorithms with some baseline algorithms from the work of \citet{krishnamurthy2024can}.
Specifically, we adopt the best prompt design from \citet{krishnamurthy2024can}, i.e., the prompt design which achieved the largest median reward among a total of $32$ prompt designs when using GPT-3.5 in the hard bandit instance. 
Note that the prompt designs from \citet{krishnamurthy2024can} do not take into account the features of the arms, therefore, we have proposed and tested multiple variants of their baseline algorithm which differ in terms of the position of the arm features:
(a) \emph{Baseline NoFeature}: the original algorithm from \citet{krishnamurthy2024can}; (b) \emph{Baseline FramingFeature}: we add the arm features after the problem framing; (c) \emph{Baseline History Feature}: we add the arm features immediately before the history of interactions.

Here we adopt 4 different reward functions: a linear function, a square function, a sinusoidal function and a function sampled from a Gaussian process (GP).
Every arm is associated with a $d=4$-dimensional feature vector, and we use $K=16$ arms in all experiments here.
The cumulative regrets of different algorithms are shown in Fig.~\ref{fig:synth:mab}.
The figures show that both our \algts~and \algro~algorithms consistently achieve smaller regrets than the baseline algorithms.
In addition, our \algts~significantly outperforms our \algro~algorithm, which is likely attributed to the strong exploration capability enabled by the inherent randomness in the LLM-generated output (Sec.~\ref{subsec:algo:ts}).
On the other hand, our \algro~algorithm generally have smaller variance across multiple trials, which is indicated by the narrower error bars.
This is likely due to the use of a temperature of $0$ in our \algro~algorithm (Sec.~\ref{subsec:algo:ro}) and may make our \algro~algorithm more desirable in scenarios where more consistent performance is preferred.


\subsection{Dueling Bandits}
\label{subsec:exp:dueling}

\begin{figure}[h]
     \centering
     \begin{tabular}{cc}
        \hspace{-7mm}
         \includegraphics[width=0.53\linewidth]{figures/dueling_bandits/dueling_bandits_2.pdf} & \hspace{-5mm} 
         \includegraphics[width=0.53\linewidth]{figures/dueling_bandits/dueling_bandits_3.pdf}\\
         {\hspace{-2mm}\small  Linear Reward Function} & {\hspace{-5mm}\small Square Reward Function}
     \end{tabular}
    \caption{
    The performance of our \algtsduel~algorithm in dueling bandits with linear and square latent reward functions.
    }
\label{fig:dueling}
\end{figure}
Here we apply our \algtsduel~algorithm to solve dueling bandit problems with two different latent reward functions $f$: a linear function and a square function.
Same as the experiments in Sec.~\ref{subsec:exp:classical}, we also let $d=4$ and $K=16$.
In our experiments here, when selecting the first arm, we use $N=15$ uniformly sampled arms to approximate the Borda function (Sec.~\ref{subsec:algo:ts:duel}).
Similar to the experiments on classical stochastic MAB (Sec.~\ref{subsec:exp:classical}), we also adopt a decaying schedule of temperature when selecting both arms.
Since our \algtsduel~selects the first arm greedily (i.e., pure exploitation) and chooses the second arm optimistically by balancing exploration and exploitation (Sec.~\ref{subsec:algo:ts:duel}), we adopt a schedule of smaller temperatures when selecting the first arm to encourage exploitation.
As we have discussed in Sec.~\ref{subsec:algo:ts:duel}, for the linear latent reward function, we use the difference between the feature vectors of the first arm and the second arm as the feature vector in the prompt; for the non-linear square function, we instead adopt the concatenation of the pair of feature vectors.

The results are shown in Fig.~\ref{fig:dueling}.
Following the common practice in dueling bandits \cite{lin2024prompt,verma2024neural}, here we have reported the reward of the first selected arm (i.e., $f(x_{i_{t,1}})$) in every iteration $t$. This is because the first arm is selected to be the one that is predicted to achieve the largest reward (Sec.~\ref{subsec:algo:ts:duel}).
Here we have only compared with the baseline of random search, because it is highly non-trivial to adapt the algorithm from \citet{krishnamurthy2024can} to the sophisticated dueling bandit problem.
As shown in the figures, our \algtsduel~significantly outperforms random search for both 
reward functions. Moreover, the regrets are generally larger in the more challenging problem of non-linear (square) reward function.

\begin{figure*}[h]
% \vspace{-6mm}
     \centering
     \begin{tabular}{cccc}
        \hspace{-5mm}
         \includegraphics[width=0.26\linewidth]{figures/contextual_bandits/contextual_bandits_wiki.pdf} & \hspace{-5.7mm} 
         \includegraphics[width=0.26\linewidth]{figures/contextual_bandits/deepseek_OneShotWikiLinks.pdf} & \hspace{-5.7mm}
        % \hspace{-5mm}
         \includegraphics[width=0.26\linewidth]{figures/contextual_bandits/contextual_bandits_AmazonCat.pdf}& \hspace{-5.7mm}
         \includegraphics[width=0.26\linewidth]{figures/contextual_bandits/deepseek_AmazonCat.pdf}\\
         {\hspace{-3mm}\footnotesize  \texttt{OneShotWikiLinks}} & {\hspace{-3mm}\footnotesize  \texttt{OneShotWikiLinks}} &
         {\hspace{-3mm} \footnotesize  \texttt{AmazonCat-13K}} & {\hspace{-3mm} \footnotesize \texttt{AmazonCat-13K}}\\
         {\hspace{-3mm}\footnotesize  (GPT-3.5-Turbo)} & {\hspace{-3mm}\footnotesize  (DeepSeek-V3)} &
         {\hspace{-3mm} \footnotesize  (GPT-3.5-Turbo)} & {\hspace{-3mm} \footnotesize (DeepSeek-V3)}
     \end{tabular}
\vspace{-3mm}
    \caption{
    The cumulative rewards in the text experiments using the \texttt{OneShotWikiLinks} and \texttt{AmazonCat-13K} datasets (Sec.~\ref{subsec:exp:text}).
    }
\label{fig:text:exp}
\end{figure*}

\subsection{Real-World Datasets with Text Features}
\label{subsec:exp:text}
Here we perform experiments using two real-world text dataset: the \texttt{OneShotWikiLinks} dataset \cite{singh2012wikilinks,oneshotwikilink} and the \texttt{AmazonCat-13K} dataset \cite{Bhatia16}, both of which have been widely used in previous works on contextual bandits \cite{chen2024efficient}.
The \texttt{OneShotWikiLinks} dataset \cite{singh2012wikilinks,oneshotwikilink} is a named-entity recognition task in which the contexts consist of text phrases surrounding the mention text (both preceding and following it), and \emph{the arms are text phrases} representing concept names. \texttt{AmazonCat-13K} \cite{Bhatia16} is an extreme multi-label dataset where the contexts are text phrases derived from the title and content of an item, and \emph{the arms are integers} representing item tags.
Thus, in the former dataset, the arm features (i.e., the text phrases) contain semantic information that is likely beneficial for the LLM in selecting arms, whereas in the latter dataset, the arm features lack such semantic content. As a result, the latter dataset (i.e., \texttt{AmazonCat-13K}) \emph{requires a larger degree of exploration} and is hence more challenging.

We apply our \algts~to the tasks here, since it performs better than \algro~in the synthetic experiments (Sec.~\ref{subsec:exp:classical}).
Since it is non-trivial to adapt the method from \citet{krishnamurthy2024can} to the sophisticated problem setting here, we instead compare our \algts~with a baseline which is obtained by modifying the prompt of our algorithm (originally designed for reward prediction) to instead directly select an arm. We refer to this baseline method as \emph{Baseline (Direct Arm Selection)}.
We have included the prompt templates used by our \algts~and the baseline (for both experiments) in App.~\ref{app:subsec:exp:text}.
We consider $K=10$ randomly sampled arms (i.e., $10$ concept names in \texttt{OneShotWikiLinks} and $10$ items in \texttt{AmazonCat-13K}) in the experiments, and adopt two powerful black-box LLMs: GPT-3.5-Turbo and DeepSeek-V3.

The results are shown in Fig.~\ref{fig:text:exp}.
The figures show that our \algts~algorithm achieves comparable performance with the baseline of direct arm selection in the \texttt{OneShotWikiLinks} task and \emph{significantly outperforms the baseline in the \texttt{AmazonCat-13K} task}.
This is likely because in \texttt{OneShotWikiLinks} task, the powerful LLMs possesses in-depth knowledge about the semantic meanings of the individual arms, i.e., the names of the entities.
As a result, given some context (i.e., the text before and after the entity), the LLM is able to accurately choose the corresponding arm whose semantic meaning is associated with the context, which explains the strong performance of the baseline of direct arm selection in the \texttt{OneShotWikiLinks} task.
On the other hand, in the \texttt{AmazonCat-13K} task, since the arms lack such semantic information useful for the LLMs, the LLMs are not able to accurately infer the association between the contexts (i.e., text phrases describing an item) and the arms (i.e., integers representing item tags).
Therefore, in such tasks, an algorithm \emph{needs to perform substantial exploration} in order to learn the association between the contexts and the arms and hence to achieve small regrets.
The inadequate performance of the baseline algorithm in this task can likely be attributed to \emph{the inability of LLM-based direct arm selection to engage in efficient exploration}, which aligns with the findings from \citet{krishnamurthy2024can}.
Meanwhile, thanks to \emph{the strong exploration capability of the high-level classical TS mechanism} (Sec.~\ref{subsec:algo:ts}), our \algts~algorithm is able to efficiently explore the space of arms and hence to achieve small regrets in this task.

To further verify this insight, we have additionally conducted an experiment using the \texttt{AmazonCat-13K} dataset in a more challenging setting, i.e., with a larger number of arms (i.e., $30$).
The results (Fig.~\ref{fig:text:exp:more:arms}) show that the performance advantage of our \algts~over the baseline is further enlarged.
Therefore, the results in Figs.~\ref{fig:text:exp} and~\ref{fig:text:exp:more:arms} show that compared with the approach of directly instructing the LLM to select arms, \textbf{our \algts~algorithm is particularly beneficial in challenging tasks where considerable exploration is required}.
On the other hand, LLM-based direct arm selection is expected to perform well in scenarios where the LLM has significant knowledge about the arms or the association between the contexts and the arms.




\begin{figure}[h]
% \vspace{-6mm}
     \centering
     \begin{tabular}{cc}
        \hspace{-5mm}
         \includegraphics[width=0.53\linewidth]{figures/contextual_bandits/contextual_bandits_AmazonCat_30arm.pdf} & \hspace{-5mm}
         \includegraphics[width=0.53\linewidth]{figures/contextual_bandits/deepseek_30arm_amazoncat.pdf}\\
         {\hspace{-2mm}\footnotesize  \texttt{AmazonCat-13K}} & {\hspace{-5mm}\footnotesize \texttt{AmazonCat-13K}}\\
         {\hspace{-2mm}\footnotesize  (GPT-3.5-Turbo, 30 arms)} & {\hspace{-5mm}\footnotesize (DeepSeek-V3, 30 arms)}
     \end{tabular}
% \vspace{-1.4mm}
    \caption{
    The cumulative rewards in the text experiments using the \texttt{AmazonCat-13K} dataset with $K=30$ arms.
    }
\label{fig:text:exp:more:arms}
\end{figure}


\section{Ablation Study}

\subsection{Impact of Different Temperatures}
\label{ablation:subsec:temperature}
Here we investigate the impact of the temperature of the LLM on the performance of our \algts~(Algo.~\ref{algo:ts}).
As we have discussed in Sec.~\ref{subsec:algo:ts}, we adopt a decaying schedule for the LLM temperature to ensure a transition from exploration to exploitation.
We follow the same experimental setting as Sec.~\ref{subsec:exp:classical} and adopt the linear reward function.
The results in Fig.~\ref{fig:different:temperatures} show that the best performance is achieved by adopting decaying LLM temperatures, whereas fixing the temperature to various values leads to inferior performance.
This is because fixing the temperature to a large value hinders the exploitation capability of \algts~in later stages, while the use of a fixed small temperature results in insufficient exploration in the initial stage.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.29\textwidth]{figures/bandits/bandits_linear_different_t.pdf}
    \vspace{-2mm}
    \caption{
    The performance of our \algts~algorithm in 
    stochastic MAB tasks 
    with different temperatures.
    }
    \vspace{-2mm}
    \label{fig:different:temperatures}
\end{figure}


\subsection{Impact of the Number $N$ of Samples When Selecting the First Arm in \algtsduel}
Recall that our \algtsduel~algorithm selects the first arm by approximately maximizing the Borda function $f_{\text{borda}}$ (Sec.~\ref{subsec:algo:ts:duel}), in which we use $N$ randomly sampled arms to approximate the expectation in $f_{\text{borda}}$ (lines 3-5 of Algo.~\ref{algo:ts:duel}).
Fig.~\ref{fig:dueling:differnet:n} presents the results of our \algtsduel~with different values of $N$, which demonstrate that a larger $N$ improves the performance because it leads to a better approximation of $f_{\text{borda}}$.
However, also note that the use of a larger $N$ increases the number of API calls to the LLM and hence incurs more cost.
Therefore, in practice, the value of $N$ should be selected based on the trade-off between the desired performance and the budget.

\begin{figure}[h]
% \vspace{-6mm}
     \centering
     \begin{tabular}{cc}
        \hspace{-5mm}
         \includegraphics[width=0.53\linewidth]{figures/dueling_bandits/dueling_bandits_4.pdf} & \hspace{-5mm} 
         \includegraphics[width=0.53\linewidth]{figures/dueling_bandits/dueling_bandits_5.pdf}\\
         {\hspace{-2mm}\footnotesize  \algtsduel} & {\hspace{-5mm}\footnotesize \algtsduel}\\
         {\hspace{-2mm}\footnotesize  (Linear Reward Function)} & {\hspace{-5mm}\footnotesize (Square Reward Function)}
     \end{tabular}
% \vspace{-1.4mm}
    \caption{
    The impact of the number $N$ of uniformly sampled arms when estimating the Borda function to select the first arm in our \algtsduel~algorithm (lines 3-5 of Algo.~\ref{algo:ts:duel}).
    }
\label{fig:dueling:differnet:n}
\end{figure}




\subsection{Impact of The Exploration Parameter in Our \algro~Algorithm}
The parameter $\gamma$ in our \algro~can be used to control the degree of exploration. As can be seen from lines 4-7 of Algo.~\ref{algo:ro}, a larger value of $\gamma$ results in a larger weight (in the arm sampling distribution $p_t$) on the arm $j_t$ that is predicted to be the best arm.
Therefore, a larger $\gamma$ leads to greater emphasis on \emph{exploitation}, thereby reducing the focus on \emph{exploration}.
Here we test three values of $\gamma$ and display the results in Fig~\ref{fig:ablation:gamma}.
The figures show that an overly small value of $\gamma=1$ significantly deteriorates the performance due to excessive exploration.
In the relatively simpler MAB problem with a linear reward function, a larger $\gamma=10$ (i.e., more emphasis on exploitation) benefits the algorithm since only minimal exploration is required to learn the simple reward function.
Meanwhile, in the more difficult problem with a non-linear (square) function, a larger $\gamma=10$ leads to worse regrets than $\gamma=5$ since a larger degree of exploration is needed compared to the linear function.


\begin{figure}[h]
% \vspace{-6mm}
     \centering
     \begin{tabular}{cc}
        \hspace{-5mm}
         \includegraphics[width=0.53\linewidth]{figures/regression_oracle_diff_gamma/regression_oracle_linear.pdf} & \hspace{-5mm} 
         \includegraphics[width=0.53\linewidth]{figures/regression_oracle_diff_gamma/regression_oracle_square.pdf}\\
         {\hspace{-2mm}\footnotesize  \algro} & {\hspace{-5mm}\footnotesize \algro}\\
         {\hspace{-2mm}\footnotesize  (Linear Reward Function)} & {\hspace{-5mm}\footnotesize (Square Reward Function)}
     \end{tabular}
\vspace{-2mm}
    \caption{
    The impact of the exploration parameter $\gamma$ in our \algro~algorithm.
    }
\label{fig:ablation:gamma}
\end{figure}

