\documentclass[sigconf,nonacm]{aamas}
\usepackage{balance}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{afterpage}
\usepackage{cuted}
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{bm}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mdframed}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage[figuresright]{rotating}
\usepackage{pdflscape}
\usepackage{float}
\usepackage{cleveref}
\usepackage[english]{babel}
\usepackage{amsthm}


\let\labelindent\relax
\newcommand{\commentsymbol}{//}
\algrenewcommand\algorithmiccomment[1]{\hfill \commentsymbol{} #1}
\makeatletter
\newcommand{\LineComment}[2][\algorithmicindent]{\Statex \hspace{#1}\commentsymbol{} #2}
\makeatother
\newcommand{\varfont}{\texttt}

\newcommand\Tau{\mathrm{T}}

\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }
\newcommand{\wrt}{w.\,r.\,t.\ }
\newcommand{\et}{{et al.\ }}
\newcommand{\cf}{{cf.\,}}

\usepackage{breqn}
\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage[english]{babel}


\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\renewcommand{\arraystretch}{1.4}


\addto\extrasenglish{
  \def\algorithmautorefname{Algorithm}
  \def\theoremautorefname{Theorem}
  \def\lemmaautorefname{Lemma}
  \def\corollaryautorefname{Corollary}
}
\newtheorem*{remark}{Remark}

\usepackage{etoolbox}
\newtoggle{inappendix}
\togglefalse{inappendix}

\apptocmd{\appendix}{\toggletrue{inappendix}}{}{\errmessage{failed to patch \appendix}}

\addto\extrasenglish{%
  \def\appendixautorefname{Appendix}
}

\makeatletter
\patchcmd{\hyper@makecurrent}{%
    \ifx\Hy@param\Hy@chapterstring
        \let\Hy@param\Hy@chapapp
    \fi
}{%
    \iftoggle{inappendix}{
        \@checkappendixparam{chapter}%
        \@checkappendixparam{section}%
        \@checkappendixparam{subsection}%
        \@checkappendixparam{subsubsection}%
    }{}%
}{}{\errmessage{failed to patch}}

\newcommand*{\@checkappendixparam}[1]{%
    \def\@checkappendixparamtmp{#1}%
    \ifx\Hy@param\@checkappendixparamtmp
        \let\Hy@param\Hy@appendixstring
    \fi
}
\makeatother





\newcommand{\triangleqq}{\coloneqq}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}

\newcommand{\infdiv}{D_{\mathrm{KL}}\infdivx}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}



\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\definecolor{LightCyan}{rgb}{0.75,0.9,1}
\usepackage{caption} 
\captionsetup[table]{skip=6pt}


\newcommand{\paddedcolorbox}[2]{
  \begingroup
  \setlength{\fboxsep}{5pt}
  \colorbox{#1}{#2}
  \endgroup
}

\usepackage{paracol}
\usepackage{afterpage}
\usepackage{blindtext}

\newcommand{\fillrightcolumn}{
  \noindent
  \begin{minipage}[t][\textheight][t]{\columnwidth}
    \null
  \end{minipage}
}




\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.2\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{by}}
  \end{minipage}\hfill
  \begin{minipage}{0.8\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Now√©  (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}



\acmSubmissionID{6}

\title{Enhancing Offline Reinforcement Learning with Curriculum Learning-Based Trajectory Valuation}


\author{Amir Abolfazli}
\affiliation{
  \institution{L3S Research Center}
  \city{Hannover}
  \country{Germany}}
\email{abolfazli@l3s.de}

\author{Zekun Song}
\affiliation{
  \institution{Technical University of Berlin}
  \city{Berlin}
  \country{Germany}}
\email{zekun.song@tu-berlin.de}


\author{Avishek Anand}
\affiliation{
  \institution{Delft University of Technology}
  \city{Delft}
  \country{Netherlands}}
\email{avishek.anand@tudelft.nl}


\author{Wolfgang Nejdl}
\affiliation{
  \institution{L3S Research Center}
  \city{Hannover}
  \country{Germany}}
\email{nejdl@l3s.de}

\begin{abstract}
The success of deep reinforcement learning (DRL) relies on the availability and quality of training data, often requiring extensive interactions with specific environments. In many real-world scenarios, where data collection is costly and risky, offline reinforcement learning (RL) offers a solution by utilizing data collected by domain experts and searching for a batch-constrained optimal policy. This approach is further augmented by incorporating external data sources, expanding the range and diversity of data collection possibilities. However, existing offline RL methods often struggle with challenges posed by non-matching data from these external sources. In this work, we specifically address the problem of source-target domain mismatch in scenarios involving mixed datasets, characterized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data generated from higher-quality policies. To tackle this problem, we introduce Transition Scoring (TS), a novel method that assigns scores to transitions based on their similarity to the target domain, and propose Curriculum Learning-Based Trajectory Valuation (CLTV), which effectively leverages these transition scores to identify and prioritize high-quality trajectories through a curriculum learning approach. Our extensive experiments across various offline RL methods and MuJoCo environments, complemented by rigorous theoretical analysis, demonstrate that CLTV enhances the overall performance and transferability of policies learned by offline RL algorithms.
\end{abstract}


\keywords{Offline Reinforcement Learning, Trajectory Valuation}

  
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}


\begin{document}


\pagestyle{fancy}
\fancyhead{}


\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{sections/Introduction}
\input{sections/Background}
\input{sections/Problem_Description}
\input{sections/Related_Work}
\input{sections/Method}
\input{sections/Experiments}
\input{sections/Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{acks}
This work was supported by the research projects ``QuBRA'' and ``BIFOLD'', funded by the Federal Ministry of Education and Research (BMBF) under grant IDs 13N16052 and BIFOLD24B, respectively.
\end{acks}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{ACM-Reference-Format} 
\bibliography{references}

\clearpage
\newpage

\appendix
\section*{Appendix}
\input{appendix/theoretical_analysis}
\section{Additional Experimental Results}
\input{appendix/runtime_analysis}
\input{appendix/rew_func}
\input{appendix/param_impact}


\clearpage  
\makeatletter
\@twocolumnfalse
\makeatother


\begin{paracol}{2}  
    \switchcolumn[0]
    \input{appendix/experimental_details}
    \switchcolumn
    \vfill
\end{paracol}

\end{document}