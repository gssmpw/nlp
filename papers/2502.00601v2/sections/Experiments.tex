\section{Experiments}
\label{sec:experiments}
\textbf{Datasets.}  We used D4RL~\citep{fu2020d4rl} datasets to create three mixed datasets for each of the considered MuJoCo~\citep{todorov2012mujoco} domains: Ant, HalfCheetah, Hopper, and Walker2d. These datasets were created by mixing 90\% of transition items from the source datasets with 10\% from the target datasets. In D4RL, the \textit{medium} dataset is generated by first training a policy online using Soft Actor-Critic (SAC)~\citep{haarnoja2018soft}, early-stopping the training, and collecting 1M transition items from this partially trained policy. The \textit{random} datasets are generated by unrolling a randomly initialized policy in these domains. Similarly, the \textit{expert} datasets are generated by an expert policy~\citep{fu2020d4rl}.

\vspace{0.5em}
\noindent\textbf{Baselines.} We use CQL~\citep{kumar2020conservative} and IQL~\citep{kostrikov2021offline} as our base offline RL algorithms as they are the most widely used offline RL algorithms and have demonstrated good results in tasks similar to ours, making them reliable benchmarks~\citep{fujita2021chainerrl,seno2022d3rlpy,sun2023offlinerl}. 

We consider Vanilla (the base algorithm without any additional methods applied), CUORL~\citep{cai2023curriculum}, and Harness~\citep{hong2023harnessing} as our \textit{baselines}, applied on top of the base offline RL algorithms (CQL and IQL). 

\vspace{0.5em}
\noindent\textbf{Performance Metric.} We measure the performance of offline RL algorithms using a normalized score, as introduced in D4RL~\citep{fu2020d4rl}. The score is calculated as: \scalebox{0.85}{$100 \times \frac{\text {score} - \text{random score}}{\text {expert score} - \text{random score}}$}. This formula is used to normalize the scores for each domain, roughly scaling them to a range between 0 and 100. A normalized score of 0 corresponds to the average returns (over 100 episodes, with each episode containing 5,000 steps) of an agent that takes actions uniformly at random across the action space. A score of 100 corresponds to the average returns of a domain expert. Scores below 0 indicate performance worse than random, while scores above 100 indicate performance surpassing expert level.

\vspace{0.5em}
\noindent\textbf{Ablation Study.} The choice of reward function and the impact of similarity-reward trade-off parameters of our method (\(\lambda\) and \(\delta\)) are discussed in \autoref{rew-choice} and \autoref{par-impact}, respectively.


\vspace{0.5em}
\noindent\textbf{Implementation and Parameter Tuning.} The implementation details and hyperparameter tuning are presented in \autoref{ex-details}. Our code
is available at \href{https://github.com/amir-abolfazli/CLTV}{https://github.com/amir-abolfazli/CLTV}.



\subsection{Performance of Offline RL Methods}
\label{subsec-score}
\autoref{tab:normalized-score} provides a comprehensive evaluation of three methods (CUORL, Harness, and CLTV) alongside the Vanilla version, using two popular offline RL algorithms (CQL and IQL). The evaluation reports their performance, measured by normalized score, and standard deviations over 100 episodes and 5 seeds on mixed D4RL datasets. The highest performing scores are highlighted in blue, and the total scores of the second best performing method are marked in bold. Additionally, the percentage increase (PI) in score of CLTV, compared to the second best performing method, is reported. The runtime analysis is presented in \autoref{runtime-analysis}.


The results show that CLTV consistently outperforms the other methods (Vanilla, CUORL and Harness) across all domains and algorithms. For example, in the Ant domain using CQL, CLTV achieves a total score of 238.18, which is 95\% higher than the second-best method (Harness) with a score of 120.97. Similarly, in the IQL variant for the Walker2d domain, CLTV surpasses the second-best method by 28\%, scoring 268.62 against 208.80. Across the HalfCheetah and Hopper domains, CLTV demonstrates significant improvements, with percentage increases of 43\% and 62\% respectively, underscoring its superior performance as reflected by the relatively low standard deviations. Moreover, CLTV often highlights the highest individual dataset scores, indicating its robustness across different datasets within each domain.



\begin{table}[!ht]
\caption{Average normalized score along with standard deviation over 100 episodes and 5 seeds on mixed D4RL datasets.}
\label{tab:normalized-score}
\centering
\resizebox{\linewidth}{!}{
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{@{}ccclllll@{}}
\toprule
\textbf{Domain} & \textbf{\shortstack{RL \\ Algorithm}} & \textbf{Method} & \multicolumn{3}{c}{\textbf{Dataset}} & \textbf{Total} & \textbf{PI} \\
\multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{c}{\textbf{random-medium}} & \textbf{random-expert} & \textbf{medium-expert} &  &  \\
\hline
\multirow{9}{*}{\rotatebox{90}{\Large Ant}} & \multirow{4}{*}{CQL} & Vanilla & 51.99 ± 22.32  & 49.81 ± 25.85 & -6.25 ± 14.12 & 95.55 &  \\
 &  & CUORL & -8.37 ± 2.59 & -7.81 ± 4.61 & 6.32 ± 23.51  & -9.86 &  \\
 &  & Harness & 67.19 ± 6.47 & 38.30 ± 38.32 & 15.48 ± 22.20 & \textbf{120.97} &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{97.48} ± 3.63 & \paddedcolorbox{LightCyan}{115.86} ± 6.82 & \paddedcolorbox{LightCyan}{24.84} ± 13.70  & \paddedcolorbox{LightCyan}{238.18} & $\uparrow$ 95\% \\ \cmidrule{3-8}
 & \multirow{4}{*}{IQL} & Vanilla & \paddedcolorbox{LightCyan}{83.75} ± 7.18 & 82.96 ± 9.07 & \paddedcolorbox{LightCyan}{117.20} ± 4.39 & \textbf{283.91} &  \\
 &  & CUORL & 4.60 ± 1.18 & 4.66 ± 0.49  & 116.23 ± 2.50 & 125.49 &  \\
 &  & Harness & 74.45 ± 6.52 & 82.56 ± 8.26 & 110.57 ± 2.56 & 267.58 &  \\
 &  & CLTV & 78.67 ± 8.26 & \paddedcolorbox{LightCyan}{88.26} ± 4.67 & 117.00 ± 7.28 & \paddedcolorbox{LightCyan}{283.93} & $\uparrow$ 0\% \\ 
  \hline
 \multirow{9}{*}{\rotatebox{90}{\Large HalfCheetah}} & \multirow{4}{*}{CQL} & Vanilla & 35.15 ± 2.78 & -0.09 ± 0.75 & 29.17 ± 18.12 & 64.23 & \\
 &  & CUORL & -3.42 ± 0.46 & -3.47 ± 0.30 & 36.89 ± 16.87 & 30.00 &  \\
 &  & Harness & 34.79 ± 4.12 & 0.53 ± 1.41 & 44.19 ± 10.86 & \textbf{79.51} &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{44.13} ± 3.47 & \paddedcolorbox{LightCyan}{10.37} ± 2.51 & \paddedcolorbox{LightCyan}{59.88} ± 7.91 & \paddedcolorbox{LightCyan}{114.38} & $\uparrow$ 43\% \\ \cmidrule{3-8}
 & \multirow{4}{*}{IQL} & Vanilla & 37.78 ± 1.51 &  6.97 ± 2.33 & 58.05 ± 3.48 & \textbf{102.80} &  \\
 &  & CUORL & 2.97 ± 1.50 & 2.55 ± 1.40 & 60.53 ± 4.25 & 66.05 &  \\
 &  & Harness & 36.55 ± 2.78 & 8.37 ± 2.80 & 55.90 ± 3.22 & 100.82 &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{41.83} ± 0.63 & \paddedcolorbox{LightCyan}{16.28} ± 7.22 & \paddedcolorbox{LightCyan}{77.10} ± 5.16 & \paddedcolorbox{LightCyan}{135.21} & $\uparrow$ 31\%  \\ 
  \hline
\multirow{9}{*}{\rotatebox{90}{\Large Hopper}} & \multirow{4}{*}{CQL} & Vanilla & 18.93 ± 12.57 & 22.47 ± 23.90 & 75.90 ± 11.12 & \textbf{117.30} &  \\
 &  & CUORL & 0.83 ± 0.23 & 0.73 ± 0.15 & 71.36 ± 40.45 & 72.90 &  \\
 &  & Harness & 12.90 ± 14.26 & 22.63 ± 7.59 & 65.33 ± 29.68 & 100.86 &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{51.04} ± 3.21 & \paddedcolorbox{LightCyan}{56.23} ± 15.14 & \paddedcolorbox{LightCyan}{83.44} ± 16.95 & \paddedcolorbox{LightCyan}{190.71} & $\uparrow$ 62\% \\ \cmidrule{3-8}
 & \multirow{4}{*}{IQL} & Vanilla & 53.29 ± 2.71 & 28.30 ± 6.21 & 25.63 ± 16.76 & 107.22 &  \\
 &  & CUORL & 6.70 ± 3.80 & 14.91 ± 12.51 & 12.59 ± 10.06 & 34.20 &  \\
 &  & Harness & 55.17 ± 3.55 & 32.05 ± 9.78 & 24.79 ± 12.61 & \textbf{112.01} &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{55.79} ± 4.44 & \paddedcolorbox{LightCyan}{39.56} ± 2.78 & \paddedcolorbox{LightCyan}{70.73} ± 4.11 & \paddedcolorbox{LightCyan}{166.08} & $\uparrow$ 48\% \\ 
  \hline
 \multirow{9}{*}{\rotatebox{90}{\Large Walker2d}} & \multirow{4}{*}{CQL} & Vanilla & 27.55 ± 20.29 & 83.81 ± 19.68 & 2.94 ± 3.80 & \textbf{114.30} &  \\
 &  & CUORL & -0.01 ± 0.09 & -0.03 ± 0.07 & 8.22 ± 9.01 & 8.18 &  \\
 &  & Harness & 23.70 ± 15.89 & 87.04 ± 17.11 & 1.77 ± 1.35 & 112.51 &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{70.45} ± 8.72 & \paddedcolorbox{LightCyan}{97.43} ± 7.74 & \paddedcolorbox{LightCyan}{30.23} ± 41.25 & \paddedcolorbox{LightCyan}{198.11} & $\uparrow$ 73\% \\ \cmidrule{3-8}
 & \multirow{4}{*}{IQL} & Vanilla & 63.98 ± 5.90 & 45.67 ± 10.30 & 96.65 ± 5.87 & 206.30 &  \\
 &  & CUORL & 5.05 ± 1.94 & 3.58 ± 0.72 & 88.71 ± 5.00 & 97.34 &  \\
 &  & Harness & 68.20 ± 3.17 & 45.12 ± 14.86 & 95.48 ± 3.81 & \textbf{208.80} &  \\
 &  & CLTV & \paddedcolorbox{LightCyan}{68.37} ± 4.12 & \paddedcolorbox{LightCyan}{89.51} ± 9.03 & \paddedcolorbox{LightCyan}{110.74} ± 0.66 & \paddedcolorbox{LightCyan}{268.62} & $\uparrow$ 28\% \\ 
\bottomrule
\end{tabular}
}
\end{table}




\subsection{Impact of CL on Performance of CLTV}
\begin{figure}[!ht]
\center
\includegraphics[width=0.497\textwidth,keepaspectratio]{figures/performance.pdf}
\caption{Performance of TS method, compared with CLTV. The curves are averaged over 5 seeds, with the shaded areas showing the confidence interval across seeds.}
\label{fig-wuthoutCL}
\end{figure}



Our method consists of two key components: Transition Scoring (TS) and Curriculum Learning-Based Trajectory Valuation (CLTV). TS assigns scores to transitions and adjusts rewards, enabling the agent to focus on the transitions most relevant to the target domain. Building upon TS, CLTV integrates curriculum learning (CL) to prioritize high-quality trajectories during training.

\autoref{fig-wuthoutCL} illustrates the impact of CL on the performance of CLTV. The results demonstrate that while TS provides a strong foundation for improving agent performance, CLTV consistently achieves better results across various environments, including faster convergence and higher normalized scores. For example, in the Ant domain, CLTV achieves scores approaching or exceeding 100 in the random-medium and random-expert datasets, whereas TS reaches around 50. Similarly, in HalfCheetah, Hopper, and Walker2d, CLTV builds on the framework of TS to deliver competitive results, especially in the random-medium and medium-expert datasets. Additionally, CLTV tends to reduce variance and improve stability in some environments, as evidenced by the smoother learning curves for Walker2d and Ant.


Although performance improvements in HalfCheetah and Hopper are less pronounced, CLTV consistently surpasses TS in most cases, highlighting its robustness across a wide range of conditions.

In general, CLTV improves the learning performance of TS, particularly in settings with diverse data distributions. By prioritizing high-value trajectories, CLTV enables the agent to focus on relevant experiences, resulting in more efficient learning and better adaptability to variations in data quality.


In general, CLTV improves TS by prioritizing high-value trajectories, leading to faster convergence and improved stability. Its adaptability across diverse environments makes it a robust approach for optimizing trajectory valuation in offline RL.
