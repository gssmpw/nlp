\section{Conclusion}
\label{sec:conclusion}
In this work, we introduced Curriculum Learning-Based Trajectory Valuation (CLTV) to enhance the performance of offline RL algorithms when dealing with mixed datasets, characterized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data from higher-quality policies. Our approach employs Transition Scoring (TS) to evaluate transition items based on their relevance to the target domain. By leveraging these scores, CLTV prioritizes high-quality trajectories through curriculum learning, enabling the agent to utilize valuable transitions from datasets generated by diverse policies. Experimental results across various algorithms and MuJoCo environments demonstrated that CLTV significantly improves performance and accelerates convergence. CLTV is particularly effective in batch-constrained scenarios, operating efficiently with limited data to ensure robust learning even when data is scarce. Furthermore, our theoretical analysis validated the effectiveness of our approach.