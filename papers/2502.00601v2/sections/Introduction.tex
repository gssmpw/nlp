\section{Introduction}
Offline Reinforcement Learning (RL) is a class of RL methods that requires the agent to learn from a dataset of pre-collected experiences without further environment interaction~\citep{lange2012batch}. This learning paradigm decouples exploration from exploitation, rendering it particularly advantageous in scenarios where the process of data collection is costly, time-consuming, or risky~\citep{isele2018safe,fujimoto2019off}. 

By utilizing pre-collected datasets, offline RL can bypass the technical challenges that are associated with online data collection, and has potential benefits for a number of real environments, such as human-robot collaboration and autonomous systems~\citep{breazeal2008learning,tang2021model}.


However, this task is challenging, as offline RL methods suffer from the \emph{extrapolation error}~\citep{fujimoto2019off,kumar2019stabilizing}. This issue arises when offline deep RL methods are trained under one distribution but evaluated on a different one. More specifically, value functions implemented by a function approximator have a tendency to predict unrealistic values for unseen state-action pairs for standard off-policy deep RL algorithms such as BCQ~\citep{fujimoto2019off}, TD3+BC~\citep{fujimoto2021minimalist}, CQL~\citep{kumar2020conservative} and IQL~\citep{kostrikov2021offline}. This highlights the necessity for approaches that restrict the action space, forcing the agent to learn a behavior that is closely aligned with on-policy with respect to a subset of the source data~\citep{fujimoto2019off}.



In recent years, there have been a number of efforts within the paradigm of supervised learning for overcoming the \emph{source-target domain mismatch problem} valuating data, including \emph{data Shapley}~\citep{ghorbani2019data} and \emph{data valuation using reinforcement learning} (DVRL)~\citep{yoon2020data}. Such methods have shown promising results on several application scenarios such as robust learning and domain adaptation~\citep{yoon2020data}.


Despite the success of such methods in the supervised learning setting, adapting them to the offline reinforcement learning (RL) setting presents several challenges. One major issue is the non-i.i.d. nature of the data. In supervised learning, data samples are typically assumed to be independent and identically distributed (i.i.d.), but this assumption is violated in offline RL since the data is generated by an agent interacting with an environment~\citep{levine2020offline}. The presence of correlated and non-i.i.d. data samples complicates the valuation of these transitions and hinders generalization to the target domain. Additionally, the data distribution in offline RL often changes due to the agent's evolving policy or the environment's dynamics, leading to \emph{distributional shifts} that aggravate the valuation of transitions, as their value may fluctuate with changing dynamics~\citep{kumar2019stabilizing,kumar2020conservative}. 

Furthermore, unlike supervised learning, where the objective is to optimize a loss function given input-output pairs, the objective in RL is to maximize cumulative rewards by learning a policy that maps states to actions~\citep{sutton2018reinforcement}. Therefore, designing a reward function that accounts for distributional similarities between transition items from different domains is essential for effective policy learning.

These challenges highlight the complexities involved in adapting supervised learning methods to the offline RL setting and underscore the need for novel approaches to handle these challenges.


However, a recent work~\citep{cai2023curriculum} introduced CUORL, a curriculum learning-based approach aimed at enhancing the performance of offline RL methods by strategically selecting valuable transition items. The limitation of CUORL is that only the current policy is considered to valuate trajectories, which lacks consideration of the information in the target dataset, making it challenging for the agent to adapt to different environments. 

Another recent work~\citep{hong2023harnessing} introduced Harness, which addresses a major challenge in offline RL involving mixed datasets, where the prevalence of low-return trajectories can limit the effectiveness of advanced algorithms, preventing them from fully exploiting high-return trajectories. Harness tackles this problem by re-weighting the dataset sampling process to create an artificial dataset that results in a behavior policy with a higher overall return, enabling RL algorithms to better utilize high-performing trajectories. By optimizing the sampling strategy, Harness enhances the performance of offline RL algorithms in environments characterized by mixed-return datasets. However, by re-weighting the dataset sampling process, Harness introduces a bias in favor of high-return trajectories, potentially neglecting important and high-quality transition items from low-return trajectories that could enhance the robustness of the policy.




In this work, we introduce a transition scoring (TS) method, which assigns a score to each transition in a trajectory, and a curriculum learning-based trajectory valuation method (CLTV) that operates based on the scores computed by TS to enable the agent to identify and select the most promising trajectories. Unlike existing methods, our approach leverages high-quality transitions from different trajectories generated by various policies. 

Our results, using existing methods (CUORL and Harness) and two base offline RL algorithms (CQL and IQL) in four MuJoCo environments, show that our CLTV method improves the performance and transferability of offline RL policies. We also provide a theoretical analysis to demonstrate the efficacy of our approach.
