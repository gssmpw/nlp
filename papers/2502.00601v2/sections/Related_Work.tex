\section{Related Work}
\label{sec:related_work}
\noindent\textbf{Offline RL.}
The RL literature contains numerous techniques for dealing with the source-target domain mismatch problem. DARLA ~\citep{higgins2017darla}, a zero-shot transfer learning method that learns disentangled representations that are robust against domain shifts; and SAFER~\citep{slack2022safer}, which accelerates policy learning on complex control tasks by considering safety constraints. Meanwhile, the literature on off-policy RL includes principled experience replay memory sampling techniques. Prioritized Experience Replay (PER)~\citep{schaul2015prioritized} (e.g., \citep{hou2017novel,horgan2018distributed,kang2021deep}) attempts to sample transitions that contribute the most toward learning. However, most of the work to date on offline RL is focused on preventing the training policy from being too disjoint with the behavior policy~\citep{fujimoto2019off,kumar2020conservative,kidambi2020morel}. 

Batch-Constrained deep Q-learning (BCQ)~\citep{fujimoto2019off} is an offline RL method for continuous control, restricting the action space, thereby eliminating actions that are unlikely to be selected by the behavior policy and therefore rarely observed in the batch. 

Conservative Q-Learning (CQL)~\citep{kumar2020conservative} prevents the training policy from overestimating the Q-values by utilizing a penalized empirical RL objective. More precisely, CQL optimizes the value function not only to minimize the temporal difference error based on the interactions seen in the dataset but also minimizes the value of actions that the currently trained policy takes, while at the same time maximizing the value of actions taken by the behavior policy during data generation. 


Twin Delayed Deep Deterministic (TD3) policy gradient with Behavior Cloning (BC) is a model-free algorithm that trains a policy to emulate the behavior policy from the data~\citep{fujimoto2021minimalist}. TD3-BC is an adaptation of TD3~\citep{fujimoto2018addressing} to the offline setting, adding a behavior cloning term to policy updates to encourage agents to align their actions with those found in the dataset. 

Advantage Weighted Actor-Critic (AWAC)~\citep{nair2020awac} uses a Q-function to estimate the advantage to increase sample efficiency. 



To increase the generalization capability of offline RL methods, ~\citep{kostrikov2021offline} propose in-sample Q-learning (IQL), approximating the policy improvement step by considering the state value function as a random variable with some randomness determined by the action, and then taking a state-conditional expectile of this random variable to estimate the value of the best actions in that state. 
Policy in Latent Action Space (PLAS)~\citep{zhou2021plas} constrains actions to be within the support of the behavior policy. PLAS disentangles the in-distribution and out-of-distribution generalization of actions, enabling fine-grained control over the method's generalization. \citep{prudencio2023survey} provide a comprehensive overview of offline RL. 
~\citep{hong2023harnessing} introduce Harness, which addresses a challenge in offline RL involving mixed datasets. The prevalence of low-return trajectories in these datasets can limit the effectiveness of algorithms, preventing them from fully exploiting high-return trajectories. Harness tackles this problem by re-weighting the dataset sampling process to create an artificial dataset that results in a behavior policy with a higher return. This enables RL algorithms to better utilize high-performing trajectories.


\vspace{0.5em}
\noindent\textbf{Curriculum Learning in RL.}
In reinforcement learning, curriculum learning (CL) sequences tasks in a strategic manner, enhancing agent performance and accelerating training, especially for complex challenges~\citep{narvekar2020curriculum,turchetta2020safe,liu2021curriculum,ren2018self}. CL has shown promising performance in real-world applications~\citep{xu2019macro,el2020student,matavalam2022curriculum}, helping agents solve complex problems and transfer knowledge across different tasks~\citep{narvekar2020curriculum,klink2021boosted}.


Among the works on curriculum learning and transition valuation in RL, the most relevant to ours is CUORL~\citep{cai2023curriculum}. CUORL enhances offline RL methods by training agents on pertinent trajectories but struggles with non-deterministic policy properties. Additionally, CUORL only considers the current policy to evaluate trajectories, neglecting information from the target dataset, which hinders the agent's adaptability to different environments. 

Our method addresses these issues by using KL divergence to capture non-deterministic properties and by identifying and selecting high-quality transition items from different trajectories.