\section{Background}
\label{sec:background}
\noindent\textbf{Reinforcement Learning.}
The RL problem is typically modeled by a Markov decision process (MDP), formulated as a tuple \scalebox{0.9}{$(\mathcal{X}, \mathcal{U}, p, r, \gamma)$}, with a state space $\mathcal{X}$, an action space $\mathcal{U}$, and transition dynamics $p$. 


At each discrete time step, the agent performs an action \( u \in \mathcal{U} \) in a state \( x \in \mathcal{X} \), transitions to a new state \( x^{\prime} \in \mathcal{X} \) based on the transition dynamics \(p(x^{\prime} \mid x, u)\), and receives a reward \( r(x, u, x^{\prime}) \). The agent's goal is to maximize the expectation of the sum of discounted rewards, also known as the return \( R_{t} = \sum_{i=t+1}^{\infty} \gamma^{i} r(x_{i}, u_{i}, x_{i+1}) \), which weights future rewards with respect to the discount factor \( \gamma \in [0,1) \), determining the effective horizon. The agent makes decisions based on the policy \( \pi: \mathcal{X} \rightarrow \mathcal{P}(\mathcal{U}) \), which maps a given state \( x \) to a probability distribution over the action space \( \mathcal{U} \). For a given policy \( \pi \), the value function is defined as the expected return of an agent starting from state \( x \), performing action \( u \), and following the policy \( Q^{\pi}(x, u) = \mathbb{E}_{\pi}[R_{t} \mid x, u] \). The state-action value function can be computed through the Bellman equation for the Q function:
\begin{equation}
Q^{\pi}(x, u)=\mathbb{E}_{s^{\prime} \sim p}\left[r\left(x, u, x^{\prime}\right)+\gamma \mathbb{E}_{u^{\prime} \sim \pi} Q^{\pi}(x^{\prime}, u^{\prime})\right].
\end{equation}

Given $Q^{\pi}$, the optimal policy $\pi^{*}=\operatorname{max}_{u} Q^{*}(x, u)$, can be obtained by greedy selection over the optimal value function $Q^{*}(x, u)=\max_{\pi} Q^{\pi}(x, u)$. 

For environments confronting agents with the curse of dimensionality, the value can be estimated with a
differentiable function approximator $Q_\theta(x, u)$, with parameters $\theta$. 



\noindent\textbf{Offline Reinforcement Learning.}
Standard off-policy deep RL algorithms such as deep Q-learning (DQN)~\citep{mnih2015human} and deep deterministic policy gradient (DDPG)~\citep{lillicrap2015continuous} are applicable in batch RL as they are based on more fundamental batch RL algorithms~\citep{fujimoto2019benchmarking}. 

However, they suffer from a phenomenon, known as \textit{extrapolation error}, which occurs when there is a mismatch between the given fixed batch of data and true state-action visitation of the current policy~\citep{fujimoto2019off}. This is problematic as incorrect values of state-action pairs, not contained in the batch, are propagated through temporal difference updates of most off-policy algorithms~\citep{sutton1988learning}, resulting in poor performance of the model~\citep{thrun1993issues}. 

In offline RL, the goal is to learn a policy based on a previously collected dataset of transition items, optimizing decision-making without any additional interaction with the environment.