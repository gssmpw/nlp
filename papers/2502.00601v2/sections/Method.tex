\section{Methods}
\label{sec:proposed_method}
Our proposed method includes two key components: Transition Scoring (TS) and Curriculum Learning-Based Trajectory Valuation (CLTV). TS assigns scores to transitions based on their relevance to the target domain, helping to identify high-quality transitions. 

CLTV leverages TS scores to prioritize high-quality trajectories through curriculum learning, guiding the agent's development in a structured manner. These components are discussed below.


\noindent\textbf{Transition Scoring (TS).}
Inspired by DVRL~\citep{yoon2020data}, we adopt the REINFORCE algorithm~\citep{williams1992simple} and use a DNN $v_{\phi}$ as the transition scoring method. The goal is to find the parameters $\phi^*$ of the DNN  so that the network returns the optimal probability distribution over the set of all possible transitions. 


The TS model $v_{\phi}: \mathcal{X} \times \mathcal{U} \times \mathcal{X^{\prime}} \rightarrow[0,1]$ is optimized to output scores corresponding to the similarity of transitions in the source dataset to the target domain. We formulate the corresponding optimization problem as:




\begin{equation}
\resizebox{0.72\hsize}{!}{%
$\max _\phi J\left(\pi_\phi\right)=\mathbb{E}_{\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \sim P^{\mathcal{S}}}\left[r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)\right]$,
}
\end{equation}
where
\begin{equation}
\label{eq-reward}
\resizebox{0.72\hsize}{!}{
$r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)=2 \cdot \frac{r_\phi^{\prime}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)-r_{\text {min }}^{\prime}}{r_{\text {max }}^{\prime}-r_{\text {min }}^{\prime}}-1$.
}
\end{equation}


\autoref{eq-reward} represents the normalized reward, keeping the rewards in the range \([-1, 1]\), where \scalebox{0.8}{$r_\phi^{\prime}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)$} is the unnormalized reward and defined as follows:

\begin{equation}
\label{eqq4}
\resizebox{0.85\hsize}{!}{%
\begin{math}
\begin{split}
r_\phi^{\prime}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) &= \delta \cdot \sum_{\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \in \mathcal{B}} v_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \cdot \Delta_\theta\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \\
&\quad + (1-\delta) \cdot \sum_{\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \in \mathcal{B}} v_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right).
\end{split}
\end{math}
}
\end{equation}


In \autoref{eqq4}, the term \scalebox{0.8}{$\sum_{\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \in \mathcal{B}} v_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)$} regularizes the unnormalized reward and prevents assigning low scores to the majority of transitions. \(\Delta_\theta\) represents the dynamics factor, quantified by the classifiers \(q_{xu} = p_{\theta_{xu}}(y \mid x, u)\) and \(q_{xu{x}^{\prime}} = p_{\theta_{xu{x}^{\prime}}}(y \mid x, u, {x}^{\prime})\), where \scalebox{1}{\(y \in \{\text{source}, \text{target}\}\)}. 


The optimization problem involves adjusting \(\phi\) to maximize the expected reward by aligning transition dynamics with the target domain. The classifiers, parameterized by \(\theta\), remain fixed during the optimization of \(\phi\). These classifiers are implemented as fully connected multi-layer neural networks, similar to those used in DARC~\citep{eysenbach2020off}. They measure the likelihood of transitions belonging to either the source or target domains. By applying Bayes' rule, the classifier probabilities are related to transition probabilities, enabling the expression of posterior probabilities in terms of likelihoods and prior probabilities. 

The classifier \(q_{xu}\) is defined as follows:
\begin{equation}
\label{qxu}
\resizebox{0.55\hsize}{!}{%
\begin{math}
q_{xu} = p_{\theta_{x u}}\left(y \mid x^{\mathcal{S}}, u^{\mathcal{S}}\right)=\frac{p\left(x^{\mathcal{S}}, u^{\mathcal{S}} \mid y\right) p(y)}{p\left(x^{\mathcal{S}}, u^{\mathcal{S}}\right)}.
\end{math}
}
\end{equation}

Similarly, the classifier $q_{xu{x}^{\prime}}$ is defined as follows:
\begin{equation}
\label{qxux}
\resizebox{0.72\hsize}{!}{%
\begin{math}
q_{xu{x}^{\prime}}=p_{\theta_{x u x^{\prime}}}\left(y \mid x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)=\frac{p\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}} \mid y\right) p(y)}{p\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)}.
\end{math}
}
\end{equation}

\newpage
We train the classifiers $q_{xu{x}^{\prime}}$ and $q_{xu}$ to minimize the standard cross-entropy loss. The loss function for the classifier $q_{xu}$ is presented in \autoref{loss-xu}.




\begin{equation}
\label{loss-xu}
\resizebox{0.72\hsize}{!}{
\begin{math}
\begin{aligned}
\mathcal{L}_{xu}\left(\theta_{xu}\right) = & -\mathbb{E}_{\mathcal{D}_{\text{target}}}\left[\log q_{\theta_{xu}}\left(\text{target} \mid x^{\mathcal{S}}, u^{\mathcal{S}}\right)\right] \\
& -\mathbb{E}_{\mathcal{D}_{\text{source}}}\left[\log q_{\theta_{xu}}\left(\text {source} \mid x^{\mathcal{S}}, u^{\mathcal{S}}\right)\right] .
\end{aligned}
\end{math}
}
\end{equation}


Similarly, the loss function for the classifier $q_{xu{x}^{\prime}}$ is presented in \autoref{loss-xux}.


\begin{equation}
\label{loss-xux}
\resizebox{0.85\hsize}{!}{
\begin{math}
\begin{aligned}
\mathcal{L}_{xu{x}^{\prime}}\left(\theta_{xu{x}^{\prime}}\right) = & -\mathbb{E}_{\mathcal{D}_{\text {target}}}\left[\log q_{\theta_{xu{x}^{\prime}}}\left(\text{target} \mid x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)\right] \\
& -\mathbb{E}_{\mathcal{D}_{\text{source}}}\left[\log q_{\theta_{xu{x}^{\prime}}}\left(\text {source} \mid x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)\right]
\end{aligned}
\end{math}
}
\end{equation}



The dynamics factor $\Delta$ (as defined in ~\citep{eysenbach2020off}) measures the discrepancy between the distribution of the source and target datasets and is defined as follows:

\begin{equation}
\label{eq-delata}
\resizebox{0.70\hsize}{!}{%
\begin{math}
\begin{aligned}
\Delta_\theta\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right)= & \log q_{\theta_{x u x^{\prime}}}\left(\operatorname{target} \mid x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \\
& -\log q_{\theta_{x u}}\left(\operatorname{target} \mid x^{\mathcal{S}}, u^{\mathcal{S}}\right) \\
& -\log q_{\theta_{x u x^{\prime}}}\left(\operatorname{source} \mid x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \\
& +\log q_{\theta_{x u}}\left(\operatorname{source} \mid x^{\mathcal{S}}, u^{\mathcal{S}}\right).
\end{aligned}
\end{math}
}
\end{equation}


\color{black}

Optimizing $\Delta$ enables the identification of transitions with lower distribution discrepancies. $w$ is the sum of selection probabilities, reflecting the likelihood of each transition to be selected. This factor prevents the TS model from selecting only a limited number of transitions. The parameters \(\delta\) and \(1-\delta\) serve as balancing factors for $v_{\phi}  \Delta$ and \(v_{\phi}\), respectively, in the calculation of \(r^{\prime}_\phi\).


For training the TS, all transitions in the source buffer \(\mathcal{D}_{\mathcal{S}}\) are divided into batches. Each batch \(D_B^{\mathcal{S}} = \left(t_j\right)_{j=1}^{B_s} \sim \mathcal{D}^{\mathcal{S}}\) is provided as input to the TS, with shared parameters across the batch. Let \(w_j = v_\phi\left(x_j^{\mathcal{S}}, u_j^{\mathcal{S}}, x_j^{\prime \mathcal{S}}\right)\) denote the probability that transition item \(j\) from the source buffer is selected for training the offline RL model. 


The sampling step \(z_j \sim \text{Bernoulli}(w_j)\) is then used to choose transition items based on their importance scores \(w_j\), following the approach in \citep{yoon2020data}. Here, $z_j$ is a binary indicator that decides whether the corresponding transition is selected ($z_j=1$) or not ($z_j=0$). This method prioritizes high-quality transitions by sampling them with higher probability, effectively focusing on those that contribute most to the performance of the RL agent.


Our adopted version of the REINFORCE algorithm has the following objective function for the policy $\pi_\phi$:

\begin{equation}
\label{objective}
\resizebox{0.75\hsize}{!}{%
\begin{math}
\begin{aligned}
J\left(\pi_{\phi}\right)&= \mathbb{E}_{\substack{\substack{(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}) \sim P^{\mathcal{S}}\\ w \sim \pi_{\phi}(\mathcal{D_S}, \cdot)}}}\left[r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)\right] \\
&=\int P^{\mathcal{S}}\left((x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}})\right) \sum_{w \in[0,1]^{N}} \pi_{\phi}(\mathcal{D_S}, w) \\
&\quad \cdot\left[r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right)\right] d \left((x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}})\right).
\end{aligned}
\end{math}
}
\end{equation}



In the above equation, \(\pi_{\phi}(\mathcal{D_S}, w)\) represents the probability of the selection probability vector \(w\) occurring. The policy uses the scores output by the TS. This contrasts with DVRL~\citep{yoon2020data}, which employs a binary selection vector \(\mathbf{s} = \left(s_1, \ldots, s_{B_s}\right)\), where \(s_{B_s}\) denotes the batch size, \(s_i \in \{0,1\}\), and \(P\left(s_i = 1\right) = w_i\). Thus, in our training, the TS does not control exploration, but instead provides scores for the transition items and is tuned accordingly. 


We calculate the gradient of the above objective function (\autoref{objective}) as follows:

\begin{equation}
\label{grad-eq}
\resizebox{0.92\hsize}{!}{%
\begin{math}
\begin{aligned}
    \nabla_{\phi} J\left(\pi_{\phi}\right) 
    &= \mathbb{E}_{\substack{(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}) \sim P^{\mathcal{S}}\\ w \sim \pi_{\phi}(\mathcal{D_S}, \cdot)}} \Bigg[ r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \cdot \nabla_{\phi} \log \pi_{\phi}(\mathcal{D_S}, w) \\
    & \quad + \nabla_{\phi} r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \Bigg].
\end{aligned}
\end{math}
}
\end{equation}



Then, the parameters $\phi$ of $J\left(\pi_{\phi}\right)$ are updated using the calculated gradient $\nabla_{\phi} J\left(\pi_{\phi}\right)$ as follows:
\begin{equation}
\label{eq:phi_update}
\phi \leftarrow \phi + k \left(r_{\phi} \cdot \nabla_{\phi} \log \pi_{\phi}(D_B^{\mathcal{S}}, (z_1, \ldots, z_{B_{\mathcal{S}}})) + \nabla_{\phi} r_\phi \right),
\end{equation}

where $k$ is the learning rate and $r_{\phi}$ denotes the normalized reward. 

The complete derivation of \autoref{grad-eq} is provided in \autoref{derivation}. Our transition scoring approach is presented in \autoref{alg:ts}.


\begin{algorithm}[!ht]
\caption{Transition Scoring (TS)}
\label{alg:ts}
\begin{algorithmic}[1]
\State \textbf{Input:} Source dataset $\mathcal{D_S}$, target dataset $\mathcal{D_T}$; \\classifiers $q_{xu}$  and $q_{xu{x}^{\prime}}$; \\ratio of transition similarity to reward $\delta$; \\learning rate $k$
\State \textbf{Output:} TS model $v_{\phi}$
\State Train TS $v_{\phi}(t_j)$ with transition item $t_j = (x_j, u_j, {x_{j}}^{\prime})$
\While{not converged}
    \State Sample a mini-batch: $D_B^{\mathcal{S}}=(t_j)_{j=1}^{B_{\mathcal{S}}} \sim \mathcal{D_S}$
    \State Initialize weighted sum $\mathcal{V}_{\phi} = 0$
    \State Initialize selection probability sum $\mathcal{W}_{\phi} = 0$
    \For{$j=1, \ldots, B_{\mathcal{S}}$}
        \State $w_j = v_{\phi}(t_j)$
        \State $z_j \sim \text{Bernoulli}(w_j)$
        \State $\Delta_j = \log \frac{q_{xu{x}^{\prime}}(\text{target} \mid x_j,u_j,{x_{j}}^{\prime})}{q_{xu{x}^{\prime}}(\text{source} \mid x_j,u_j,{x_{j}}^{\prime})} - \log \frac{q_{xu}(\text{target} \mid x_j,u_j)}{q_{xu}(\text{source} \mid x_j,u_j)}$
        \State $\mathcal{V}_{\phi} \leftarrow \mathcal{V}_{\phi} + w_j \cdot \Delta_j$
        \State $\mathcal{W}_{\phi} \leftarrow \mathcal{W}_{\phi} + w_j$
    \EndFor
    \State $r'_{\phi} = \delta \cdot \mathcal{V}_{\phi} + (1 - \delta) \cdot \mathcal{W}_{\phi}$
    \State $r_{\phi} = 2 \cdot \frac{r'_{\phi} - r'_{\min}}{r'_{\max} - r'_{\min}} - 1$
    \State $\phi \leftarrow \phi + k \cdot r_{\phi} \cdot \nabla_{\phi} \log \pi_{\phi}(D_B^{\mathcal{S}}, (z_1, \ldots, z_{B_{\mathcal{S}}}))$
\EndWhile
\end{algorithmic}
\end{algorithm}




\noindent\textbf{Curriculum Learning-based Trajectory Valuation (CLTV).} 
Building upon the transition scoring (TS) mechanism, CLTV focuses on enhancing policy learning by prioritizing valuable trajectories. CLTV employs curriculum learning to present the agent with the most valuable trajectories at different stages of learning. 

CLTV uses two distinct sets of actor-critic networks: one for the source domain \(\left(Q_{\omega_1}^{\mathcal{S}}, \pi_{\theta_1}^{\mathcal{S}}\right)\) and another for the target domain \(\left(Q_{\omega_2}^{\mathcal{T}}, \pi_{\theta_2}^{\mathcal{T}}\right)\). By maintaining separate networks, CLTV tailors the learning process to the unique dynamics and reward structures of each domain, facilitating effective policy transfer. 

CLTV integrates the TS method by training classifiers \(q_{xu{x}^{\prime}}\) and \(q_{xu}\) to adjust the rewards, helping to address the distributional shift between the source and target domains in offline RL. 

Each trajectory $\tau_k^{\mathcal{S}}$ in the source dataset $\mathcal{D}_{\mathcal{S}}$ is defined as a sequence of transitions $\tau_k^{\mathcal{S}}=\left(x_i^{k, \mathcal{S}}, u_i^{k, \mathcal{S}}, x_i^{\prime, k, \mathcal{S}}, r_i^{k, \mathcal{S}}\right)_{i=1}^{L_k^{\mathcal{S}}}$, where $L_k^{\mathcal{S}}$ denotes the length of the $k$-th trajectory. 

The degree of similarity between these transitions in $\mathcal{D}_{\mathcal{S}}$ and those in the target dataset $\mathcal{D}_{\mathcal{T}}$ varies. When the source data does not perfectly align with the target environment, relying solely on the original rewards can cause the agent to learn policies that are suboptimal or misaligned with the target setting. 

Moreover, not all transitions within a trajectory contribute equally to learning; only a subset of transitions contributes to improving the policy. By assigning a relevance score to each transition based on its similarity to the target domain dynamics, we effectively re-weight the rewards associated with these transitions. 

This approach increases the influence of transitions that are more representative of the target environment, guiding the agent to focus on the most valuable experiences. As a result, the agent learns policies that are better aligned with the target domain, enhancing overall performance. This method is especially beneficial in scenarios with sparse or noisy reward signals, as it helps the model identify and leverage valuable experiences.


During the curriculum learning process, the value of the trajectory \(i\) in the source dataset is computed as follows:

\begin{equation}
\resizebox{0.8\hsize}{!}{%
\begin{math}
v_i = \exp\left(-\infdiv{\pi_{\theta_2}^{\mathcal{T}}(u_t|x_t)}{\pi_{\theta_1}^{\mathcal{S}}(u_t|x_t)}\right) \cdot \sum_{t=1}^{T} \gamma^{t-1} r_t.
\end{math}
}
\end{equation}


This valuation formula uses the KL divergence between the target policy \(\pi_{\theta_2}^{\mathcal{T}}\) and the policy we need to train \(\pi_{\theta_1}^{\mathcal{S}}\), along with discounted rewards, to valuate trajectories. It includes two main components: the \emph{similarity component} \scalebox{0.9}{\(\exp\left(-\infdiv{\pi_{\theta_2}^{\mathcal{T}}(u_t|x_t)}{\pi_{\theta_1}^{\mathcal{S}}(u_t|x_t)}\right)\)}, which measures the similarity between policies, and the \emph{return component} \scalebox{0.9}{\(\sum_{t=1}^T \gamma^{t-1} r_t\)}, representing the discounted sum of rewards. 


This approach effectively guides the agent toward states with higher expected value by considering both policy similarity and trajectory returns, ensuring that the trajectories closely resemble those in the target dataset while leveraging information from both the source and target domains. After obtaining the trajectory values, we sort all trajectories in decreasing order based on these values. 

We then sample the top \(m\) trajectories, where \(m\) is a hyperparameter that can be adjusted according to the quality of the dataset. Since these selected trajectories are the most valuable ones in terms of similarity to the target dataset, we merge them with the target dataset and use the combined data to update the critic and actor networks. Our CLTV approach is detailed in \autoref{alg:cltv}.


\begin{algorithm}[!ht]
\caption{Curriculum Learning-Based Trajectory Valuation}
\label{alg:cltv}
\begin{algorithmic}[1]
\State \textbf{Input:} Critic networks $Q_{\omega_1}^{\mathcal{S}}$, $Q_{\omega_2}^{\mathcal{T}}$, actor networks $\pi_{\theta_1}^{\mathcal{S}}$, $\pi_{\theta_2}^{\mathcal{T}}$; \\offline RL algorithm $\mathcal{A}$; \\source dataset $\mathcal{D^S}$, target dataset $\mathcal{D^T}$; \\epoch size $E$; \\training steps $S$; \\the ratio of transition score to transition reward $\lambda$
\State \textbf{Output:} Critic network $Q_{\omega_1}^{\mathcal{S}}$, actor network $\pi_{\theta_1}^{\mathcal{S}}$
\State \textbf{Initialization:}
\State Train classifiers $q_{xu}$ and $q_{xu{x}^{\prime}}$
\State Train TS model $v_{\phi}(t_j)$
\LineComment{Modify rewards in the source dataset}
\For{$j = 1, \ldots, |\mathcal{D^S}|$}
    \State $w_j = v_{\phi}(t_j)$
    \State $r_j \leftarrow (1 - \lambda) \cdot r_j + \lambda \cdot w_j$
\EndFor
\LineComment{Curriculum learning loop}
\For{$e = 1$ to $E$}
    \State $\mathcal{D} \leftarrow \mathcal{D^S}$
    \LineComment{Compute values for each trajectory in the source dataset}
    \For{each trajectory $\tau_i = \{(x_j^i, u_j^i, r_j^i)\}_{j=1}^T$ in $\mathcal{D}$}
        \State \scalebox{0.9}{$v_i = \exp\left(-\infdiv{\pi_{\theta_2}^{\mathcal{T}}(u_t|x_t)}{\pi_{\theta_1}^{\mathcal{S}}(u_t|x_t)}\right) \cdot \sum_{t=1}^{T} \gamma^{t-1} r_t$}
    \EndFor
    \LineComment{Sort trajectories by $v_i$ in decreasing order}
    \For{$s = 1$ to $S$}
        \LineComment{Sample $m$ trajectories $\{\tau_i\}_{i=1}^m$ from $\mathcal{D}$}
        \State $\mathcal{D}^{train} = \{\tau_i\}_{i=1}^m \cup \mathcal{D}^{\mathcal{T}}$
        \State Update $\omega_1, \theta_1$ with $\mathcal{A}(\mathcal{D}^{train}, Q_{\omega_1}^{\mathcal{S}}, \pi_{\theta_1}^{\mathcal{S}})$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

To show the effectiveness of our approach, we first present \autoref{theorem2}, which provides a formal justification for TS by establishing bounds on the policy's performance in the target domain, and then present \autoref{theorem1}, which demonstrates how CLTV ensures value alignment with the target domain through policy improvement. Both theorems are discussed in detail in \autoref{proofs}.




\subsection{Theoretical Analysis}
\label{proofs}
In this section, we present the foundational theoretical aspects of our offline RL approach, focusing on the integration of behavior policies and the critical role of KL divergence in policy optimization. 

We begin by applying several lemmas—namely, the \emph{performance difference lemma}~\citep{agarwal2019reinforcement}, the \emph{state value estimation error}~\citep{lange2012batch}, the \emph{total variance and KL divergence relation}~\citep{csiszar2011information}, and the \emph{total variance and \(L_1\) norm relation}~\citep{IntroductiontoNonparametricEstimation}—which set the foundation for our analysis.


\begin{lemma}
\label{lemma1}
Let $\pi'$ and $\tilde{\pi}$ denote any two policies, and let $d_{\pi'}$ be the discounted state distribution induced by policy $\pi'$ over the state space $\mathcal{X}$. The following inequality holds:
\begin{equation}
\label{eq-lemma1}
\begin{aligned}
(1-\gamma)\left(V^{\pi'} - V^{\tilde{\pi}}\right) &= \mathbb{E}_{x \sim d_{\pi'}, u \sim \pi'(\cdot \mid x)}\left[A_{\tilde{\pi}}(x, u)\right] \\
&\leq \frac{2}{1-\gamma} \mathbb{E}_{x \sim d_{\pi'}}\left\|\pi'(\cdot \mid x) - \tilde{\pi}(\cdot \mid x)\right\|_1.
\end{aligned}
\end{equation}
\end{lemma}





\begin{lemma}
\label{lemma2}
Let $V^\pi$ be the value function for any policy $\pi$, and let $\hat{V}^\pi$ be an estimate of $V^\pi$. Let $r$ and $\hat{r}$ be the true reward and its estimate, respectively, both bounded in $[0,1]$. Similarly, let $p$ and $\hat{p}$ be the true and estimated transition probabilities. Then, for any discount factor $\gamma \in[0,1)$, the following inequality holds:
\begin{equation}
\label{eq-lemma2}
\begin{aligned}
\left\|V^\pi-\hat{V}^\pi\right\|_{\infty} \leq \frac{1}{1-\gamma}\left(\|r-\hat{r}\|_{\infty}+\frac{\gamma}{1-\gamma}\|p-\hat{p}\|_{\infty}\right).
\end{aligned}
\end{equation}
\end{lemma}




\begin{lemma}
\label{lemma3}
For any two probability distributions $P$ and $Q$, their total variance and KL divergence are bounded as follows, according to Pinsker’s inequality:
\begin{equation} 
\label{eq-lemma3}
{D_{\mathrm{TV}}}(P, Q) \leq \sqrt{\frac{1}{2} \infdiv{P}{Q}}.
\end{equation}
\end{lemma}

\begin{lemma}
\label{lemma4}
The total variance between any two probability distributions $P$ and $Q$ is equal to half the norm of the difference of $P$ and $Q$:
\begin{equation}
    \frac{1}{2} \| P - Q \|_1 = {D_{\mathrm{TV}}}(P, Q).
\end{equation}
\end{lemma}


\begin{corollary} 
\label{corollary2}
Let $p^{\mathcal{S}}$ and $p^{\mathcal{T}}$ represent the transition probability distributions of the source and target domains, respectively, and let $\pi$ be the policy we aim to train. The following inequality holds:
\begin{equation} 
\label{eq-corollary2}
\begin{aligned}
\left\|V_{p^{\mathcal{S}}}^{\pi}-V_{p^{\mathcal{T}}}^{\pi}\right\|_{\infty} \leq \frac{\gamma}{(1-\gamma)^2} \sqrt{\frac{1}{2} \infdiv{p^{\mathcal{S}}}{p^{\mathcal{T}}}}.
\end{aligned}
\end{equation}
\end{corollary}

\begin{proof}
We begin by applying the result from \autoref{lemma2}, noting that the source and target domains share the same reward function. Thus, we have:
\begin{equation}
\begin{aligned}
\left\| V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right\|_{\infty} \leq \frac{\gamma}{(1-\gamma)^2} \left\| p^{\mathcal{S}} - p^{\mathcal{T}} \right\|_{\infty}.
\end{aligned}
\end{equation}
To further refine this bound, we apply Pinsker's inequality (\autoref{lemma3}) which provides:
\begin{equation}
\left\| p^{\mathcal{S}} - p^{\mathcal{T}} \right\|_{\infty} \leq \sqrt{\frac{1}{2} \infdiv{p^{\mathcal{S}}}{p^{\mathcal{T}}}}.
\end{equation}
Substituting the result from Pinsker's inequality into the inequality from \autoref{lemma2}, we obtain:
\begin{equation}
\left\| V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right\|_{\infty} \leq \frac{\gamma}{(1-\gamma)^2} \sqrt{\frac{1}{2} \infdiv{p^{\mathcal{S}}}{p^{\mathcal{T}}}}.
\end{equation}
\end{proof}



\setcounter{theorem}{0}
\begin{theorem} 
\label{theorem2}
Let \( d(s) \) be a distribution over states. If the condition $\mathbb{E}_{s \sim d(s)} \left[ \sqrt{\frac{1}{2} \infdiv{p^{\mathcal{S}}}{p^{\mathcal{T}}}} \right] \leq \epsilon$ is satisfied, the expected value of the policy \(\pi\) in the target domain can be bounded below by:
\begin{equation} 
\label{eq-theorem2}
\mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{T}}}^{\pi} \right] \geq \mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{S}}}^{\pi} \right] - \frac{\gamma \epsilon}{(1-\gamma)^2}.
\end{equation}
\end{theorem}

\begin{proof}
We begin by applying the expectation operator to \autoref{corollary2}, which provides:
\begin{equation} 
\label{eq1-theorem2-proof}
\resizebox{0.9\hsize}{!}{$
\mathbb{E}_{s \sim d(s)} \left[ \left\| V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right\|_{\infty} \right] \leq \frac{\gamma}{(1-\gamma)^2} \mathbb{E}_{s \sim d(s)} \left[ \sqrt{\frac{1}{2}\infdiv{p^{\mathcal{S}}}{p^{\mathcal{T}}}} \right].
$}
\end{equation}
By the non-negativity of expectations, we have:
\begin{equation} 
\label{eq2-theorem2-proof}
\mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right] \leq \mathbb{E}_{s \sim d(s)} \left[ \left\| V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right\|_{\infty} \right].
\end{equation}
Substituting the condition into \autoref{eq1-theorem2-proof}, we obtain:
\begin{equation} 
\label{eq3-theorem2-proof}
\mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{S}}}^{\pi} - V_{p^{\mathcal{T}}}^{\pi} \right] \leq \frac{\gamma \epsilon}{(1-\gamma)^2}.
\end{equation}
Consequently, this yields the lower bound:
\begin{equation} 
\label{eq4-theorem2-proof}
\mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{T}}}^{\pi} \right] \geq \mathbb{E}_{s \sim d(s)} \left[ V_{p^{\mathcal{S}}}^{\pi} \right] - \frac{\gamma \epsilon}{(1-\gamma)^2}.
\end{equation}
\end{proof}


\begin{corollary}
\label{corollary1}
Let \(\pi_b\) denote the behavior policy, which induces a distribution \(d_{\pi_b}\) over the state space \(\mathcal{X}\). For the current policy $\pi_i$, and the policy in the next episode $\pi_{i+1}$, the following relationship is valid:
\begin{equation} 
\label{eq-corollary1}
\begin{aligned}
& \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{2}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} D_{\mathrm{TV}}\left(\pi_b^{}(\cdot \mid x), \pi_{i+1}(\cdot \mid x)\right) \\
& \leq (1-\gamma)\left(V^{\pi_{i+1}}-V^{\pi_i}\right).
\end{aligned}
\end{equation}
\end{corollary}


\begin{proof}
By substituting $\pi^*$ with $\pi_{b}$ and $\tilde{\pi}$ with $\pi_{i+1}$ in \autoref{eq-lemma1}, we obtain the first part of the inequality. 
\begin{equation}
\resizebox{0.9\hsize}{!}{$
(1-\gamma)(V^{\pi_b}-V^{{\pi_{i+1}}}) \leq \frac{1}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} \| {\pi_b}(\cdot \mid x)-{\pi_{i+1}}(\cdot \mid x) \|_1
$}
\label{eq1-corollary-proof}
\end{equation}


Similarly, by substituting $\pi^*$ with $\pi_{b}$ and $\tilde{\pi}$ with $\pi_{i}$ in \autoref{eq-lemma1}, we derive \autoref{eq2-corollary-proof}, which represents the second part of the inequality in \autoref{eq-lemma1}.
\begin{equation}
\resizebox{0.8\hsize}{!}{$
(1-\gamma)(V^{\pi_b}-V^{{\pi_{i}}}) = \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg]
$}
\label{eq2-corollary-proof}
\end{equation}

By applying \autoref{eq1-corollary-proof} and \autoref{eq2-corollary-proof}, we derive:
\begin{equation}
\resizebox{0.9\hsize}{!}{$
\begin{aligned}
(1-\gamma)(V^{\pi_{i+1}}-V^{{\pi_{i}}}) & \geq \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{1}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} \| {\pi_b}(\cdot \mid x)-{\pi_{i+1}}(\cdot \mid x) \|_1
\end{aligned}
$}
\label{eq3-corollary-proof}
\end{equation}

Utilizing \autoref{lemma4}, we derive:
\begin{equation}
\resizebox{0.9\hsize}{!}{$
\begin{aligned}
(1-\gamma)(V^{\pi_{i+1}}-V^{{\pi_{i}}}) & \geq \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{2}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} D_{\mathrm{TV}}\left(\pi_b^{}(\cdot \mid x), \pi_{i+1}(\cdot \mid x)\right)
\end{aligned}
$}
\label{eq4-corollary-proof}
\end{equation}
\end{proof}



\begin{theorem}
\label{theorem1}
The KL divergence between the behavior policy $\pi_b$ and the policy in the subsequent episode $\pi_{i+1}$ establishes a lower bound for policy improvement:
\begin{equation} 
\label{eq-theorem1}
\resizebox{0.7\hsize}{!}{$
\begin{aligned}
& (1-\gamma)\left(V^{\pi_{i+1}}-V^{\pi_i}\right) \\
& \geq \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{\sqrt{2}}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} \sqrt{\infdiv{\pi_b(\cdot \mid x)}{\pi_{i+1}(\cdot \mid x)}}.
\end{aligned}
$}
\end{equation}
\end{theorem}

\begin{proof}
In \autoref{corollary1}, we derived bounds for policy improvement by introducing the total variation distance. To relate total variation to KL divergence, we utilize \autoref{lemma3}, which gives us Pinsker's inequality:

\begin{equation}
\label{eq1-theorem1-proof}
\resizebox{0.48\hsize}{!}{
\begin{math}
\begin{aligned}
    {D_{\mathrm{TV}}}(P, Q) & \leq \sqrt{\frac{1}{2} \infdiv{P}{Q}}.
\end{aligned}
\end{math}
}
\end{equation}

Next, we substitute the total variation distance in \autoref{eq4-corollary-proof} with the KL divergence bound using \autoref{eq1-theorem1-proof}:
\begin{equation}
\resizebox{0.9\hsize}{!}{$
\begin{aligned} 
\label{eq2-theorem1-proof}
(1-\gamma)(V^{\pi_{i+1}}-V^{{\pi_{i}}}) & \geq \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{2}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} D_{\mathrm{TV}}\left(\pi_b(\cdot \mid x), \pi_{i+1}(\cdot \mid x)\right) \\
& \geq \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{2}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} \sqrt{\frac{1}{2}\infdiv{\pi_b(\cdot \mid x)}{\pi_{i+1}(\cdot \mid x)}} \\
& = \mathbb{E}_{x \sim d_{\pi_b}, u \sim \pi_b(\cdot \mid x)} \bigg[ A^{\pi_i}(x, u) \bigg] \\
& \quad\; -\frac{\sqrt{2}}{1-\gamma} \mathbb{E}_{x \sim d_{\pi_b}} \sqrt{\infdiv{\pi_b(\cdot \mid x)}{\pi_{i+1}(\cdot \mid x)}}.
\end{aligned}
$}
\end{equation}
\end{proof}

