\section{Experimental Details}
\label{ex-details}

\subsection{Implementation and Computational Resources}
\label{sec-technical-details}
All the experiments were conducted on a high-performance GPU cluster consisting of five interconnected compute nodes, each equipped with eight NVIDIA A100 Tensor Core GPUs with 40 GB of memory per unit. Our method and the experiments were implemented in Python 3.10 under Ubuntu 22.10. Our code
is available at \href{https://github.com/amir-abolfazli/CLTV}{https://github.com/amir-abolfazli/CLTV}.




\subsection{Parameter Tuning}
\label{param-tuning}
For the base offline RL algorithms (CQL and IQL), we used the optimal hyperparameter values reported in~\citep{seno2022d3rlpy}, as listed in \autoref{tab:parameters-base}. 


\begin{table}[!h]
\caption{Hyperparameter values of base offline RL methods.}
\label{tab:parameters-base}
\centering
\resizebox{0.95\linewidth}{!}{
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l l c l}
 \toprule
        & Hyperparameter  & Value & Description \\
    \hline
    CQL & Actor Learning Rate & $1 \times 10^{-4}$ & Learning rate for training policy \\
         & Critic Learning Rate & $3 \times 10^{-4}$ & Learning rate for training Q network \\
         & Temperature Learning Rate & $1 \times 10^{-4}$ & Learning rate for temperature parameter of SAC \\
         & Alpha Learning Rate $\tau$ & $1 \times 10^{-4}$ & The learning rate for parameter alpha \\
         & Discount Factor & $0.99$ & The factor of discounted return \\
         & Target Network $\tau$ & $5 \times 10^{-3}$ & The target network synchronization coefficiency \\
    \hline
    IQL & Actor Learning Rate & $3 \times 10^{-4}$ & Learning rate for training policy \\
        & Critic Learning Rate & $3 \times 10^{-4}$ & Learning rate for training Q network \\
        & Discount Factor & $0.99$ & The factor of discounted return \\
        & Target Network $\tau$ & $5 \times 10^{-3}$ & The target network synchronization coefficiency \\
        & Expectile & $0.7$ & The expectile value for value function training \\
 \bottomrule
\end{tabular}
}
\end{table}

For our CLTVORL method, we selected the optimal hyperparameter values by grid search, as listed in \autoref{tab:parametes-cltvorl}. 


\begin{table}[!h]
\caption{Hyperparameter values of our CLTV method.}
\label{tab:parametes-cltvorl}
\centering
\resizebox{0.95\linewidth}{!}{
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l c l}
 \toprule
     Hyperparameter  & Value & Description \\
    \hline
    Score-Reward Ratio $ \lambda$ & $0.8$ & The ratio of transition score to transition reward of TS \\
    Similarity-Reward Ratio $\delta$ & $0.7$ & The ratio of transition similarity to the reward of TS \\
    Episode Ratio $m$ & 0.1 & The number of episodes sampled from the source dataset \\
    Batch Size & $200$ & The batch size of TS\\
    Hidden Layers & [256, 256] & The size of hidden layers for TS  \\
    Classifier Hidden Size & $256$ & The hidden size of classifiers \\
    Classifier Learning Rate & $3 \times 10^{-4}$ & Learning rate of classifiers \\
    Gaussian Standard Deviation & $0.1$ & The standard deviation of Gaussian distribution \\
 \bottomrule
\end{tabular}
}
\end{table}
