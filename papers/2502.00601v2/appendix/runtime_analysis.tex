\subsection{Runtime Analysis of Offline RL Methods}
\label{runtime-analysis}
\autoref{fig-runtime} shows the runtimes of CLTV compared to Vanilla, CUORL, and Harness across different datasets and offline RL algorithms.

In the Ant domain, CLTV has a higher runtime compared to CQL and IQL. A similar pattern is observed in the Hopper and Walker2d domains, where CLTVâ€™s runtime is generally higher than the other methods across most cases. However, the gains achieved in key learning tasks, particularly in the expert-level datasets, justify the additional computational cost. In the HalfCheetah domain, the runtime differences between CQL and IQL methods are smaller, but CLTV still incurs higher computational overhead compared to other methods, particularly in the random-medium setting. 

In conclusion, while CLTV may not always achieve the shortest runtimes, the performance improvements it provides (as reported in \autoref{tab:normalized-score}) justify the additional computational time. Across different environments and datasets, CLTV offers a reasonable trade-off between runtime and learning performance, making it a practical and effective option for offline RL tasks.

\begin{figure}[!ht]
\center
\includegraphics[width=0.32\textwidth]{figures/runtime.pdf}
\caption{Runtime analysis of offline RL algorithms.}
\label{fig-runtime}
\end{figure}