\section{Theoretical Analysis}
\label{derivation}
\subsection*{Derivation of \autoref{grad-eq}}
The gradient of the objective function \( J\left(\pi_\phi\right) \) (\autoref{objective}) with respect to the policy parameters \( \phi \) is given by:


\begin{equation}
\resizebox{0.8\hsize}{!}{%
$\displaystyle
\begin{aligned}
\nabla_{\phi} J\left(\pi_{\phi}\right) &= \int P^{\mathcal{S}}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \Bigg[ \sum_{w \in [0,1]^{N}} \nabla_{\phi} \pi_{\phi}(\mathcal{D_S}, w) \cdot r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \\
&\quad + \sum_{w \in [0,1]^{N}} \pi_{\phi}(\mathcal{D_S}, w) \cdot \nabla_{\phi} r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \Bigg] \, d\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right),
\end{aligned}$
}
\end{equation}
% }

where \( P^{\mathcal{S}}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \) represents the probability distribution over state--action--next-state triples, \( \pi_{\phi}(\mathcal{D_S}, w) \) is the policy parameterized by \( \phi \), and \( r_{\phi} \) is the reward function.

We apply the log-derivative trick to the policy gradient:

\begin{equation}
\resizebox{0.6\hsize}{!}{%
$\displaystyle
\nabla_{\phi} \pi_{\phi}(\mathcal{D_S}, w) = \pi_{\phi}(\mathcal{D_S}, w) \nabla_{\phi} \log \pi_{\phi}(\mathcal{D_S}, w).
$
}
\end{equation}

Substituting this into the integral, we obtain:

\begin{equation}
\resizebox{0.98\hsize}{!}{%
$\displaystyle
\begin{aligned}
\nabla_{\phi} J(\pi_{\phi}) &= \int P^{\mathcal{S}}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \Bigg[ \sum_{w \in [0,1]^{N}} \pi_{\phi}(\mathcal{D_S}, w) \nabla_{\phi} \log \pi_{\phi}(\mathcal{D_S}, w) \cdot r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \\
&\quad + \sum_{w \in [0,1]^{N}} \pi_{\phi}(\mathcal{D_S}, w) \cdot \nabla_{\phi} r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \Bigg] \, d\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right).
\end{aligned}$
}
\end{equation}

The sum over \( w \) can be interpreted as an expectation with respect to the policy distribution \( \pi_{\phi} \). Therefore, we simplify the expression to:


\begin{equation}
\resizebox{0.9\hsize}{!}{%
$\displaystyle
\begin{aligned}
\nabla_{\phi} J(\pi_{\phi}) &= \int P^{\mathcal{S}}\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right) \, \mathbb{E}_{w \sim \pi_{\phi}(\mathcal{D_S}, \cdot)} \Bigg[ \nabla_{\phi} \log \pi_{\phi}(\mathcal{D_S}, w) \cdot r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \\
&\quad + \nabla_{\phi} r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \Bigg] \, d\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}\right).
\end{aligned}$
}
\end{equation}


Finally, by interpreting the integral over \( P^{\mathcal{S}} \) and the expectation over \( \pi_{\phi} \) jointly as an expectation with respect to the distribution of trajectories under the current policy, we arrive at the final expression:


\begin{equation}
\resizebox{0.67\hsize}{!}{%
$\displaystyle
\begin{aligned}
    \nabla_{\phi} J\left(\pi_{\phi}\right) 
    &= \mathbb{E}_{\substack{(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}) \sim P^{\mathcal{S}}\\ w \sim \pi_{\phi}(\mathcal{D_S}, \cdot)}} \Bigg[ r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \cdot \nabla_{\phi} \log \pi_{\phi}(\mathcal{D_S}, w) \\
    & \quad + \nabla_{\phi} r_\phi\left(x^{\mathcal{S}}, u^{\mathcal{S}}, x^{\prime \mathcal{S}}, \Delta_\theta\right) \Bigg].
\end{aligned}$
}
\end{equation}