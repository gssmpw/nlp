\subsection{Similarity-Reward Trade-off Parameters}
\label{par-impact}
We examine how the parameters \(\delta\) and \(\lambda\) affect performance across different domains, datasets, and algorithms in offline RL.

The parameter \(\delta\) balances the importance of transition similarity, which refers to the transition score or its relevance to the target domain, against the actual reward received, allowing the model to focus appropriately on both aspects during learning. Tuning \(\delta\) is important when applying the model to datasets with different dynamics, ensuring the model generalizes well without overfitting to the source data. 

The parameter \(\lambda\), on the other hand, plays a key role in balancing exploration and exploitation. A higher \(\lambda\) value encourages more exploration by allowing the model to take actions that may not immediately seem optimal, but could lead to better long-term outcomes. In contrast, a lower \(\lambda\) value favors exploitation, where the model sticks to actions that have previously yielded high rewards.

The heatmaps in \autoref{fig-deltalambda-cql} show that the choice of \(\delta\) and \(\lambda\) has a noticeable effect on CLTV (CQL) performance across different environments. Moderate values for both parameters generally lead to better results. For instance, in environments like Ant and Hopper, higher \(\lambda\) values enhance the balance between exploration and exploitation, while moderate \(\delta\) values allow for greater flexibility in learning without overfitting.

Similarly, the heatmaps in \autoref{fig-deltalambda-iql} show that performance in CLTV (IQL) is influenced by \(\delta\) and \(\lambda\), though the algorithm tends to be more stable across different settings. The heatmaps indicate that higher \(\lambda\) values help the model explore effectively, preventing it from getting stuck in suboptimal solutions. At the same time, moderate \(\delta\) values strike a balance between leveraging current estimates and improving both policy and value functions. This suggests that fine-tuning \(\lambda\) helps the model explore better, while \(\delta\) adjusts how much it sticks to what it has already learned.


When we analyze the impact of \(\lambda\) and \(\delta\) on CLTV (CQL) and CLTV (IQL) across different datasets, we see clear differences in how each algorithm reacts. In CLTV (CQL), especially in the Ant environment with the medium-expert dataset, performance is sensitive to changes in these parameters. For instance, as \(\lambda\) increases from 0.0 to 1.0, we see improvements in rewards. This highlights how \(\lambda\) helps manage the balance between policy conservativeness and exploration. In contrast, CLTV (IQL) appears to be less affected by changes in \(\lambda\) and \(\delta\). For example, in the Ant environment, CLTV (IQL) achieves consistently high rewards across various parameter settings. This shows that CLTV (IQL) handles exploration and exploitation more efficiently without needing significant tuning of these parameters. 

In summary, \(\lambda\) and \(\delta\) have different impacts on CLTV (CQL) and CLTV (IQL). For CLTV (CQL), higher \(\lambda\) values and moderate \(\delta\) values tend to result in better performance, especially in environments like Ant. On the other hand, CLTV (IQL) performs well across a wide range of \(\lambda\) and \(\delta\) values, reducing the need for fine-tuning. Understanding the effect of these parameters is essential for configuring algorithms in offline RL, where they can have a major influence on the performance of the learned policies.


\begin{figure*}[!ht]
\center
\includegraphics[width=\textwidth]{figures/heatmaps_cql.pdf}
\caption{{Heatmaps illustrating the performance of CLTV (CQL) on mixed datasets with respect to different $\delta$ (Delta) and $\lambda$ (Lambda) values, ranging from 0.2 to 1.0 in increments of 0.2, evaluated over 100 episodes with one seed.}}
\label{fig-deltalambda-cql}
\end{figure*}



\begin{figure*}[!ht]
\center
\includegraphics[width=\textwidth]{figures/heatmaps_iql.pdf}
\caption{{Heatmaps illustrating the performance of CLTV (IQL) on mixed datasets with respect to different $\delta$ (Delta) and $\lambda$ (Lambda) values, ranging from 0.2 to 1.0 in increments of 0.2, evaluated over 100 episodes with one seed.}}

\label{fig-deltalambda-iql}
\end{figure*}




