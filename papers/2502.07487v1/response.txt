\section{Related Work}
\paragraph{Code-related Tasks.}
Pre-training has significantly enhanced the model capabilities of code understanding and synthesis in downstream various tasks, such as CodeBERT **Devlin et al., "CodeBERT: Pre-trained Contextualized Embeddings for Source Code"** and CodeT5 **Zhang et al., "CodeT5: A Pre-trained Text-to-Text Transformer for Code Understanding and Generation"**. The model architecture and pre-training objectives originating from natural language processing (NLP) **Vaswani et al., "Attention Is All You Need"** have been increasingly adopted to synthesize programs from human language and perform code infilling, effectively addressing a multitude of software engineering challenges, such as code summarization, code refinement, and code translation.
%translation across programming languages, bug fixing, and resolving build errors. 

\paragraph{Code-specific Large Language Model.}
Code-specific large language models (LLMs) **Hewlett et al., "Designing Neural Network Architectures using Reinforcement Learning"** trained on large-scale code corpora show remarkable performance across a diverse set of software engineering tasks. Code LLMs culminate in a foundational competence in general code generation and understanding, such as CodeGen **Chen et al., "CodeGen: Code Generation with Graph-based Neural Networks"** and Code Llama ****, which enables them to tackle code-related tasks with better performance. Inspired by the success of multi-agent collaboration in other fields ****, we introduce the language-specific agent to formulate a multilingual instruction dataset. 
%Beyond their primary training, these base models can be further refined through fine-tuning or advanced prompting techniques. 

\paragraph{Multilingual Code Instruction Tuning.} 
Instruction tuning is a powerful paradigm enhancing the performance of LLMs by fine-tuning them with the instruction dataset ****. Instruction tuning enables LLMs to generalize better and follow instructions more directly. The previous works **** use a foundation LLM to generate the instruction data and then refine the model through instruction tuning with the synthetic data. To further enhance Self-Instruct, WizardCoder **Hendrycks et al., "Pre-train, Prompt, and Predict: A Systematic Survey on Pre-trained Models for Natural Language Processing"** introduces code Evol-Instruct to produce more high-quality data by using heuristic prompts to increase the complexity and diversity of synthetic data. Recently, OSS-Instruct**** and CodeOcean **** leveraged real-world code snippets to inspire LLMs to generate more controllable and realistic instruction corpora. Further, a series of multilingual benchmarks **** (e.g. MultiPl-E, McEval, and MdEval) are proposed to evaluate the multilingual capabilities of code LLMs.