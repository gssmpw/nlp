\section{Related Work}
\paragraph{Code-related Tasks.}
Pre-training has significantly enhanced the model capabilities of code understanding and synthesis in downstream various tasks, such as CodeBERT \cite{CodeBERT} and CodeT5 \cite{CodeT5}. The model architecture and pre-training objectives originating from natural language processing (NLP) \cite{CodeXGLUE,CodeTransOcean,refine_gpt,chat_unitest} have been increasingly adopted to synthesize programs from human language and perform code infilling, effectively addressing a multitude of software engineering challenges, such as code summarization, code refinement, and code translation.
%translation across programming languages, bug fixing, and resolving build errors. 

\paragraph{Code-specific Large Language Model.}
Code-specific large language models (LLMs) \cite{starcoder,code_llama,guo2024deepseekcoder,codearena,execrepobench} trained on large-scale code corpora show remarkable performance across a diverse set of software engineering tasks. Code LLMs culminate in a foundational competence in general code generation and understanding, such as CodeGen \cite{codegen} and Code Llama \cite{code_llama}, which enables them to tackle code-related tasks with better performance. Inspired by the success of multi-agent collaboration in other fields \cite{multi_agents_survey,autonomous_agents_survey}, we introduce the language-specific agent to formulate a multilingual instruction dataset. 
%Beyond their primary training, these base models can be further refined through fine-tuning or advanced prompting techniques. 

\paragraph{Multilingual Code Instruction Tuning.} 
Instruction tuning is a powerful paradigm enhancing the performance of LLMs by fine-tuning them with the instruction dataset \cite{instructGPT,llama_adapter,self_instructions}. Instruction tuning enables LLMs to generalize better and follow instructions more directly. The previous works \cite{self_instructions,codealpaca} use a foundation LLM to generate the instruction data and then refine the model through instruction tuning with the synthetic data. To further enhance Self-Instruct, WizardCoder \cite{wizardcoder} introduces code Evol-Instruct to produce more high-quality data by using heuristic prompts to increase the complexity and diversity of synthetic data. Recently, OSS-Instruct~\cite{magicoder} and CodeOcean~\cite{wavecoder} leveraged real-world code snippets to inspire LLMs to generate more controllable and realistic instruction corpora. Further, a series of multilingual benchmarks~\cite{multipl_e,mceval,mdeval,fullstack,bigcodebench} (e.g. MultiPl-E, McEval, and MdEval) are proposed to evaluate the multilingual capabilities of code LLMs.