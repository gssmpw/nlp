\section{Related Work}
\paragraph{Code-related Tasks.}
Pre-training has significantly enhanced the model capabilities of code understanding and synthesis in downstream various tasks, such as CodeBERT ____ and CodeT5 ____. The model architecture and pre-training objectives originating from natural language processing (NLP) ____ have been increasingly adopted to synthesize programs from human language and perform code infilling, effectively addressing a multitude of software engineering challenges, such as code summarization, code refinement, and code translation.
%translation across programming languages, bug fixing, and resolving build errors. 

\paragraph{Code-specific Large Language Model.}
Code-specific large language models (LLMs) ____ trained on large-scale code corpora show remarkable performance across a diverse set of software engineering tasks. Code LLMs culminate in a foundational competence in general code generation and understanding, such as CodeGen ____ and Code Llama ____, which enables them to tackle code-related tasks with better performance. Inspired by the success of multi-agent collaboration in other fields ____, we introduce the language-specific agent to formulate a multilingual instruction dataset. 
%Beyond their primary training, these base models can be further refined through fine-tuning or advanced prompting techniques. 

\paragraph{Multilingual Code Instruction Tuning.} 
Instruction tuning is a powerful paradigm enhancing the performance of LLMs by fine-tuning them with the instruction dataset ____. Instruction tuning enables LLMs to generalize better and follow instructions more directly. The previous works ____ use a foundation LLM to generate the instruction data and then refine the model through instruction tuning with the synthetic data. To further enhance Self-Instruct, WizardCoder ____ introduces code Evol-Instruct to produce more high-quality data by using heuristic prompts to increase the complexity and diversity of synthetic data. Recently, OSS-Instruct____ and CodeOcean____ leveraged real-world code snippets to inspire LLMs to generate more controllable and realistic instruction corpora. Further, a series of multilingual benchmarks____ (e.g. MultiPl-E, McEval, and MdEval) are proposed to evaluate the multilingual capabilities of code LLMs.