\section{Related Works}
Our study is relevant to several machine learning fields, including continual learning, transfer learning, and especially transferability estimation. In the following, we provide an overview of previous works directly related to ours.


\subsection{Continual Learning}

CL aims at building and continuously adapting a model through a sequence of tasks so that the model can perform well on all tasks it has observed~\citep{ring1994continual,thrun1995lifelong}. However, when the model is updated on new tasks, SGD training, which assumes i.i.d.~data, usually leads to a decline in performance on earlier tasks. This phenomenon is referred to as catastrophic forgetting~\citep{mccloskey1989catastrophic} and has been the primary subject of research on CL. Numerous strategies have been suggested to mitigate this issue, including regularization-based methods~\citep{farajtabar2020orthogonal, yin2020optimization, bian2024make}, episodic memory-based methods~\citep{chaudhry2019tiny, buzzega2020dark, bellitto2024saliency}, parameter isolation-based methods~\citep{yoon2017lifelong, mallya2018packnet, tang2025mind}, and Bayesian methods~\citep{nguyen2024lifelong, hai2024continual}. Besides algorithmic efforts to reduce catastrophic forgetting, some recent works also look at the role of forward transfer~\citep{vinyals2016matching, snell2017prototypical} and backward transfer~\citep{lin2022beyond, wan2022continual}.

Although a learner may retain information from previous tasks, it is often more crucial to employ the acquired knowledge to efficiently learn new tasks (referred to as forward transfer) and to retain the previous knowledge (known as backward transfer)~\citep{lopez2017gradient}. \citet{raghavan2021formalizing} argued that the enhancement in accuracy does not necessarily improve the forward transfer and backward transfer. On the other hand, \cite{prado2022theory} showed that the relatedness between tasks can affect the model's performance. In this paper, we will explore the relationships between accuracy, forward transfer, and backward transfer in CL. To the best of our knowledge, the most similar to our work is \cite{chen2023forgetting}, which also showed that less forgetting leads to better forward transfer, but their models are overconfident on the trained auxiliary output layer and are more time-consuming to train.

In terms of task selection, real-world data can be accumulated to form a batch of tasks~\citep{caccia2022anytime}, and training them sequentially in chronological order may not be optimal. Therefore, optimizing the model's performance requires selecting the right tasks in the right order. \cite{bell2022effect} indicated that task orders could significantly impact CL algorithms' effectiveness. In this work, we also develop a novel algorithm to determine a task order that can give a good accuracy for CL algorithms.


\subsection{Transferability Measures}

CL can be considered a generalization of transfer learning where we need to continuously transfer knowledge from previous tasks to current ones. In transfer learning, transferability measures~\citep{nguyen2020leep, you2021logme, yang2023pick} have emerged as a tool to estimate the easiness of knowledge transfer from a source to a target task. These measures could be used to analyze task relations~\citep{achille2019task2vec}, ranking checkpoints~\citep{li2021ranking}, or model selection~\citep{you2021logme}. Recent attempts in this area~\citep{tran2019transferability, nguyen2020leep, you2021logme, li2023exploring} aim to develop transferability measures that are well-correlated with the test accuracy of the target task. 

Label-based estimators~\citep{tran2019transferability, nguyen2020leep} rely on the relationship between source and target labels to construct transferability measures. However, they could be restrictive due to the assumption that source and target tasks shared the same inputs~\citep{tran2019transferability} or could be inaccurate due to the overfitting of the last layer to source domains~\citep{nguyen2020leep}. These issues are addressed in subsequent works using the feature layer. For instance, LogME~\citep{you2021logme} measures the transferability based on the log marginal likelihood of the target labels given the extracted features. TransRate~\citep{huang2022frustratingly} uses the mutual information between extracted features and labels to estimate transferability. Other works compute the domain distance~\citep{tan2021otce}, intra-class distance~\citep{xu2023fast}, or combine with inter-class distance~\citep{bao2019information, pandy2022transferability} to construct their measures. Recently, physics-inspired and energy-based methods~\citep{gholami2023etran, li2023exploring} have emerged as a promising approach to this problem. In this paper, we shall generalize this notion of transferability measure to define the sequence transferability measures, which can estimate the transferability of CL algorithms on a sequence of tasks.