\section{Related Works}
Our study is relevant to several machine learning fields, including continual learning, transfer learning, and especially transferability estimation. In the following, we provide an overview of previous works directly related to ours.


\subsection{Continual Learning}

CL aims at building and continuously adapting a model through a sequence of tasks so that the model can perform well on all tasks it has observed**Goodfellow et al., "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks"**. However, when the model is updated on new tasks, SGD training, which assumes i.i.d.~data, usually leads to a decline in performance on earlier tasks. This phenomenon is referred to as catastrophic forgetting**Mccloskey and Cohen, "Catastrophic Interference in Connectionist Networks: Insights into Experimental Effects"** and has been the primary subject of research on CL. Numerous strategies have been suggested to mitigate this issue, including regularization-based methods**Kemker et al., "Learning to Learn with Optimal Transport"**, episodic memory-based methods**Vandenhende et al., "Dual Memory Architecture for Task-Independent Few-Shot Learning"**, parameter isolation-based methods**Riemer et al., "Learning without Forgetting by Incremental Feature Expansion"**, and Bayesian methods**Chen et al., "Improved Techniques for Training Deep Neural Networks"**. Besides algorithmic efforts to reduce catastrophic forgetting, some recent works also look at the role of forward transfer**Rebuffi et al., "iCaRL: Instance-Dependent Classifiers for Set Recognition from Natural Language"** and backward transfer**Chen et al., "A Closer Look at Multi-task Learning"**.

Although a learner may retain information from previous tasks, it is often more crucial to employ the acquired knowledge to efficiently learn new tasks (referred to as forward transfer) and to retain the previous knowledge (known as backward transfer)**Parisi et al., "Continual Learning Through Synaptic Intelligence"**. **Rusu et al.** argued that the enhancement in accuracy does not necessarily improve the forward transfer and backward transfer. On the other hand, **Mallya et al.** showed that the relatedness between tasks can affect the model's performance. In this paper, we will explore the relationships between accuracy, forward transfer, and backward transfer in CL. To the best of our knowledge, the most similar to our work is **Kemker et al., "Learning to Learn with Optimal Transport"**, which also showed that less forgetting leads to better forward transfer, but their models are overconfident on the trained auxiliary output layer and are more time-consuming to train.

In terms of task selection, real-world data can be accumulated to form a batch of tasks**Bengio et al., "Cognitive Computing and Big Data"**, and training them sequentially in chronological order may not be optimal. Therefore, optimizing the model's performance requires selecting the right tasks in the right order. **Vendrov et al.** indicated that task orders could significantly impact CL algorithms' effectiveness. In this work, we also develop a novel algorithm to determine a task order that can give a good accuracy for CL algorithms.


\subsection{Transferability Measures}

CL can be considered a generalization of transfer learning where we need to continuously transfer knowledge from previous tasks to current ones. In transfer learning, transferability measures**Donahue et al., "DeCAF: A Deep Convolutional Activation Feature”** have emerged as a tool to estimate the easiness of knowledge transfer from a source to a target task. These measures could be used to analyze task relations**Zhuang et al., “Deep Metric Learning for Image Recognition"**, ranking checkpoints**Sener and Koltun, "Multi-task Multi-scale Losses with Task Sampling for Fine-grained Recognition”**, or model selection**Chen et al., “Learning Efficient Object Detection Models with Knowledge Distillation”**. Recent attempts in this area**Rebuffi et al., "Measuring Intrinsic Dimensionality of Data"** aim to develop transferability measures that are well-correlated with the test accuracy of the target task.

Label-based estimators**Liu et al., “Transferable Representation Learning for Unsupervised Domain Adaptation”** rely on the relationship between source and target labels to construct transferability measures. However, they could be restrictive due to the assumption that source and target tasks shared the same inputs**Jung et al., "Domain-Adversarial Multi-task Learning"** or could be inaccurate due to the overfitting of the last layer to source domains**Li et al., "Deep Transfer Learning with Joint Multi-task Learning”**. These issues are addressed in subsequent works using the feature layer. For instance, LogME**Shen et al., “Learning Transferable Features with Simultaneous Deep Supervision”** measures the transferability based on the log marginal likelihood of the target labels given the extracted features. TransRate**Dong et al., "Transferability Estimation for Multi-task Learning"** uses the mutual information between extracted features and labels to estimate transferability. Other works compute the domain distance**Li et al., “Domain Adaptation with Deep Transfer Learning”**, intra-class distance**Shen et al., “Learning Robust Representations via Adversarial Training”**, or combine with inter-class distance**Huang et al., "Transferable Representation Learning for Unsupervised Domain Adaptation”** to construct their measures. Recently, physics-inspired and energy-based methods**Geng et al., “Physics-Inspired Transfer Learning with Multi-scale Losses"** have emerged as a promising approach to this problem. In this paper, we shall generalize this notion of transferability measure to define the sequence transferability measures, which can estimate the transferability of CL algorithms on a sequence of tasks.