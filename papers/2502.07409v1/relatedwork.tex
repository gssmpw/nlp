\section{Related Work}
\label{section:related_work}

\subsection{Large-scale Pre-trained Models for Pathology}
Recent advancements in large-scale pre-trained models for pathology can be broadly classified into two categories. \textit{Vision models}, such as 
\texttt{Virchow} \cite{ikezogwo2024quilt}, 
\texttt{Hibou} \cite{nechaev2024hibou}, \texttt{UNI} \cite{chen2024towards}, and \texttt{Prov-GigaPath} \cite{xu2024whole} leverage massive pathology image datasets to learn robust visual representations. Among these, \texttt{Prov-GigaPath} stands out as the largest model, trained on 1.3 billion pathology image patches, and excels in resolving complex tissue patterns at high resolution. On the other hand, \textit{vision-language models} (VLMs) like \texttt{PLIP} \cite{huang2023visual} (trained 200K image-text pairs), \texttt{CONCH} \cite{lu2024visual} (1.17M), or \texttt{QUILTNET}\cite{ikezogwo2024quilt} (1M), integrate visual and textual information to enhance contextual understanding and improve pathology slide interpretation. In contrast,  our \texttt{\textsc{MGPath}} combines the strengths of both approaches by using a \textit{parameter-efficient adaptor} to link \texttt{Prov-GigaPath} (the largest pre-trained vision encoder) with a text encoder from VLMs like \texttt{PLIP} or \texttt{CONCH}, leveraging both rich visual features and semantic textual embeddings. Although we use the \texttt{PLIP} text encoder in our experiments due to its common use in baselines, the method can be extended to other larger pre-trained text models.


\subsection{Few-shot learning in WSI}
MIL treats a WSI as a bag of patches and aggregates these instances into a bag of features, with early methods using non-parametric techniques like mean or max pooling. However, since disease-related patches are rare, these methods can overwhelm useful information with irrelevant data. To address this, attention-based methods, graph neural Networks (GNNs), and Transformer-based methods have been introduced \cite{lu2021data,chen2021whole,ilse2018attention,li2021dual,shao2021transmil,zheng2022graph}. In contrast, VLMs have gained popularity through contrastive learning, aligning image-text pairs to enhance performance on a variety of tasks. While collecting large-scale pathology image-text pairs remains challenging, models like MI-Zero, \texttt{PLIP}, and \texttt{CONCH} have been trained on hundreds of thousands to over a million pathology image-text pairs \cite{lu2023visual,huang2023visual,lu2024visual}.  Some approaches also integrate multi-magnification images and multi-scale text to mimic pathologistsâ€™ diagnostic processes, especially for detecting subtle abnormalities \cite{shi2024vila,han2024mscpt}. Our \texttt{\textsc{MGPath}} extends on the VLMs strategy by further \textit{amplifying the benefits of using a large pre-trained pathology} VLM model and introducing a new \textit{parameter-efficient multi-granular prompt learning} to adapt these models to few-shot settings.

\subsection{Prompt Learning for Vision-Language Adaptations}
Prompt tuning is proposed to transfer large pre-trained model task-specific downstream tasks and has shown strong results in multimodal models like CLIP. Rather than design a heuristic template, several methods like \texttt{CoOp} \cite{zhou2022learning}, \texttt{CoCoOp} \cite{zhou2022conditional}, or \texttt{MaPLe} \cite{khattak2023maple} among others \cite{rao2022denseclip,shu2022test} have allowed models to determine optimal prompts from multiple perspectives, such as domain generalization \cite{ge2023domain,yao2024tcp}, knowledge prototype \cite{zhang2022prompting,li2024steering}, or diversity \cite{lu2022prompt,shu2022test}. However, these approaches focus on natural images and do not address the unique challenges of whole-slide pathology images, which require multi-scale and structural contextual information. While a few current methods typically integrate prompts with frozen visual features via self-attention \cite{shi2024vila,qu2024rise}, these approaches might struggle with the complex relationships in WSIs. Our solution introduces multi-granular prompt learning, bridging \textit{attention} on both \textit{individual image patches} and \textit{spatial groups} to better align with the hierarchical structure of WSI data.