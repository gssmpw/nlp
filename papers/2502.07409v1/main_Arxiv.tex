%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt,twoside]{article} % For LaTeX2e
%\usepackage{iclr2021_conference,times}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-6cm}
\setlength{\textheight}{\paperheight}
\addtolength{\textheight}{-4cm}
\addtolength{\textheight}{-1.1\headheight}
\addtolength{\textheight}{-\headsep}
\addtolength{\textheight}{-\footskip}
\setlength{\oddsidemargin}{0.5cm}
\setlength{\evensidemargin}{0.5cm}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}
\usepackage{eqnarray,amsmath, bm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
       % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[numbers]{natbib}
\usepackage{makecell}
% \usepackage{subfigure}
\usepackage{epsf}
\usepackage{epsfig}
\usepackage{fancyhdr}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{float}
% \usepackage{epstopdf}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{pdfpages}
\usepackage{colortbl}
\usepackage{tcolorbox}
% \usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{floatflt}
\usepackage{tikz}
\usepackage{soul}
\definecolor{cyan}{cmyk}{.3,0,0,0}
\definecolor{LightCyan}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.95}


\usepackage{url}% for url's in bib
% for theorem hyperlink colors
\usepackage[colorlinks,linkcolor=magenta,citecolor=blue, pagebackref=true]{hyperref}
% \renewcommand*{\backref}[1]{\ifx#1\relax \else Page #1 \fi}
\renewcommand*{\backrefalt}[4]{%
    \ifcase #1 \footnotesize{(Not cited.)}%
    \or        \footnotesize{(Cited on page~#2.)}%
    \else      \footnotesize{(Cited on pages~#2.)}%
    \fi}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
%\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{color}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb,bbm}
\usepackage{caption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{multirow}
\usepackage{multicol}% colors
\usepackage{parskip}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{titling}
\newcommand{\bs}{\boldsymbol}
\newcommand{\strongconvex}{\mu}
\newcommand{\smooth}{L_1}
\newcommand{\smoothprior}{L_2}
\newcommand{\subgaussian}{\sigma}
\newcommand{\barFn}{\bar{F}_n}
\newcommand{\hlc}[2][yellow]{{\sethlcolor{#1} \hl{#2}} }
\newtheorem*{remark}{Remark}


\newtheorem{assumption}{Assumption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength{\textwidth}{\paperwidth}
%\addtolength{\textwidth}{-6cm}
%\setlength{\textheight}{\paperheight}
%\addtolength{\textheight}{-4cm}
%\addtolength{\textheight}{-1.1\headheight}
%\addtolength{\textheight}{-\headsep}
%\addtolength{\textheight}{-\footskip}
%\setlength{\oddsidemargin}{0.5cm}
%\setlength{\evensidemargin}{0.5cm}

% baselinestretch trick to save some space
 %   \renewcommand{\baselinestretch}{0.99}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}%opening
%\newtheorem{assumption}{Assumption}
%\newtheorem{conjecture}{Conjecture}
\newenvironment{assumptionprime}[1]
  {\renewcommand{\theassumption}{\ref{#1}$'$}%
   \addtocounter{assumption}{-1}%
   \begin{assumption}}
  {\end{assumption}}


%\def\argmin{\textnormal{arg} \min}
\def\st{\textnormal{s.t.}}
\def\sgn{\texttt{sign}}
\newcommand{\todo}[1]{\textcolor{red}{[Todo: #1]}}
%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}




\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}

\newcommand{\notiff}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{"}$\hidewidth\cr$\iff$}}}}


% Attempt to make hyperref and algorithmic work together better:






\input{macro_commands}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2021}
% \DeclareMathOperator*{\argmax}{arg\,max}  % in your preamble
% \DeclareMathOperator*{\argmin}{arg\,min}  % in your preamble 
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{BoMb-OT: On Batch of Mini-batches Optimal Transport}
% \predate{}  % Remove the date
% \postdate{}
\date{}
\begin{document}

\begin{center}

{\bf{\LARGE{MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification}}}
  
\vspace*{.2in}
{\large{
\begin{tabular}{ccccc}
Anh-Tien Nguyen$^{1,2,3}$ & Duy Minh Ho Nguyen$^{6,7,8*}$ & Nghiem Tuong Diep$^{8,*}$ & \\
Trung Quoc Nguyen$^{8}$ & Nhat Ho$^{5}$ & Jacqueline Michelle Metsch$^{1,2}$ &\\
Miriam Cindy Maurer$^{1,2}$ & Daniel Sonntag $^{4,8}$ & Hanibal Bohnenberger  $^{1,2}$ & \\
& Anne-Christin Hauschild $^{1,2}$ & &
\end{tabular}
}}

\vspace*{.2in}

\begin{tabular}{ccc}
$^1$ University of Göttingen, Germany \\
$^2$ University Medical Center Göttingen, Germany \\
$^3$ Max Planck Institute for Multidisciplinary Sciences, Germany \\
$^4$ University of Oldenburg, Germany\\
$^5$ The University of Texas at Austin$^{\dagger}$, USA\\
$^6$ Max Planck Research School for Intelligent Systems (IMPRS-IS), Germany \\
$^7$ University of Stuttgart, Germany \\
$^8$ German Research Center for Artificial Intelligence (DFKI), Germany\\
\end{tabular}

\vspace*{.1in}

\vspace*{.2in}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\blfootnote{$^\star$ Equal second contribution}
\vspace{-0.1in}
\begin{abstract}
Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model’s ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this \href{https://github.com/HauschildLab/MGPATH}{MGPATH}.
\end{abstract}
\end{center}


\section{Introduction}

Whole slide imaging (WSI) \cite{niazi2019digital} has become essential in modern pathology for capturing high-resolution digital representations of entire tissue samples, enabling easier digital storage, sharing, and remote analysis \cite{pantanowitz2011review}. Unlike conventional methods that depend on examining slides under a microscope, WSI provides faster, detailed structural and cellular insights essential for disease diagnosis across multiple tissue layers, which is particularly valuable in cancer screening \cite{barker2016automated,cheng2021robust}.
 Nevertheless, WSIs are massive images, often containing billions of pixels \cite{farahani2015whole,song2023artificial}, making detailed annotations and analysis difficult and expensive. To tackle these challenges, machine learning techniques incorporating few-shot and weakly supervised learning have been developed \cite{madabhushi2016image, li2023task, lin2023interventional, ryu2023ocelot, shi2024vila}. Among these, \textit{multiple instance learning} (MIL) and \textit{vision-language models} (VLMs) have gained particular attention for their ability to effectively manage limited annotations and interpret complex whole-slide pathology images.

 \begin{wrapfigure}{r}{0.6\textwidth}
    \vspace{-0.25in}
    \centering % Centers the figure within the wrapfigure
    \includegraphics[width=1.0\linewidth]{figures/MGPATH-overview.pdf}
\vspace{-0.25in}
   \caption{Unlike previous methods that add prompts at prefix positions or patch-level attention - disrupting structural correlations - our \textsc{MGPath} framework integrates prompts at both regional and individual patch levels (multi-granular attention).}
   \vspace{-0.2in}
   \label{fig:overview}
\end{wrapfigure}


% \begin{figure}[t]
%   \centering
% \includegraphics[width=0.7\linewidth]{figures/MGPATH-overview.pdf}
%    \caption{Unlike previous methods that add prompts at prefix positions or patch-level attention—disrupting structural correlations —our \textsc{MGPATH} framework integrates prompts at both regional and individual patch levels (multi-granular attention).}
%    \label{fig:overview}
% \end{figure}



In \textit{MIL} \cite{ilse2018attention,xu2019camel,li2023task,lin2023interventional,tang2023multiple,shi2023structure}, each WSI is first divided into smaller patches or instances. These instances are extracted feature embeddings using pre-trained vision encoders before being grouped into a "bag", i.e., a whole slide-level representation for the entire WSI. The MIL model mainly focuses on learning ensemble functions to identify patterns in specific patches, contributing to the overall label prediction for each bag (e.g., cancerous or non-cancerous), hence reducing the need for detailed annotations.  
Nonetheless, these methods often struggle to select relevant patches due to complex correlations and tissue variability \cite{gadermayr2024multiple,qu2024pathology}. To overcome those obstacles, VLMs \cite{lu2023visual,huang2023visual,ikezogwo2024quilt,shi2024vila} have emerged as a promising solution, combining slide-level visual features with textual descriptions to enrich contextual understanding and support predictions in sparse data scenarios with approaches such as zero-shot learning \cite{xu2024whole,ahmed2024pathalign}.
Specifically, VLMs incorporate multi-scale images \cite{shi2024vila,han2024mscpt}, permitting the extraction of global and local WSI features at different resolutions. To adapt the pre-trained vision-language model efficiently, prompt learning \cite{zhou2022learning,gao2024clip} is employed where learnable prompts are treated as part of the input text to guide the model, and contextual prompts \cite{li2021prefix,yao2024tcp} are integrated into feature embeddings using a self-attention mechanism \cite{vaswani2017attention}. Despite their strong classification performance across diverse tasks, these approaches still encounter certain limitations.

\textit{First}, (i) adapting prompt learning with frozen visual features often neglects the hierarchical relationships among learnable prompts and the visual features they interact with - specifically, \textit{the multi-granular attention between prompts to individual patches and groups of patches}.
 This limitation lessens the model’s ability to capture interdependence across distinct scales — from fine-grained local features to broader contextual information, leading to less accurate comprehension of complex patterns in pathology images. \textit{Second}, (ii) many VLMs rely on the \texttt{CLIP} architecture \cite{radford2021learning}, which was not explicitly pre-trained on pathology images, thereby limiting its adaptability in few-shot settings, especially when the architecture is primarily frozen and prompt learning is applied. While there exist recent works that have incorporated \texttt{PLIP} \cite{huang2023visual}, a model pre-trained on 200k pathology image-text pairs curated from Twitter and showed significant improvements, an open question remains whether scaling pre-training to millions or billions of pathology-specific samples could further boost performance. 
\textit{Lastly}, (iii) most VLM models for whole-slide pathology rely on cosine similarity to align visual and textual features. This metric, however, can struggle with multiple text descriptions for sub-regions \cite{chen2022plot} and with augmented data perturbations \cite{nguyen2024dude}, as it lacks the precision to capture fine-grained alignments between varied image-text pairs.

In this work, we present \texttt{\textsc{MGPath}}, a novel VLM method developed to address the challenges in whole-slide pathology classification. Our approach begins by adapting \texttt{Prov-GigaPath} \cite{xu2024whole} - one of the largest pre-trained vision models trained on 1.3 billion pathology image patches - into a vision-language model. We accomplish this through contrastive learning with a pre-trained text encoder from the \texttt{PLIP} model \cite{huang2023visual}, which was trained on approximately 200K pathology image-text pairs. To strengthen this alignment, we collected an additional 923K image-text pairs from  ARCH \cite{gamper2021multiple}, PatchGastricADC22 \cite{tsuneki2022inference} and Quilt-1M \cite{ikezogwo2024quilt} and trained adaptor-based cross-alignment \cite{gao2024clip,cao2024domain} between \texttt{Prov-GigaPath}’s visual encoder and \texttt{PLIP}’s text encoder. Crucially, only lightweight adaptors are updated, making this process highly parameter-efficient. To the best of our knowledge, \texttt{\textsc{MGPath}} is the first parameter-efficient vision-language model trained for pathology at this data scale — utilizing 923K image-text pairs compared to the 200K in \texttt{PLIP}, and further benefiting from \texttt{Prov-GigaPath}’s 1.3 billion sample pre-training.

Next, we leverage these pre-trained models for few-shot WSI tasks by introducing \textit{multi-granular prompt learning}. First, visual embeddings and descriptive text prompts are generated for image patches at different resolutions using large language models, which have been shown to improve performance \cite{han2024mscpt,shi2024vila,qu2024rise}. Unlike prior methods that concatenate or use basic attention on individual patches \cite{li2021prefix,zhou2022learning,yao2024tcp,shi2024vila}, our attention integrates learnable prompts with frozen visual features at both fine- and coarse-grained perspectives (Figure \ref{fig:overview}). We represent image patches from each WSI as a spatial graph, using bounding box coordinates to enable region-level aggregation through message passing along local connections. This spatial structure is encoded as tokens within the \textit{Key}-\textit{Value} matrices, which interact with \textit{Query} matrices derived from prompt embeddings. By directing attention from Query to Key-Value matrices across both patch and region levels, our approach effectively captures hierarchical information, enriching feature representation and selectively emphasizing features across diverse tissue areas.

Finally, to measure the distance between prompt-fused visual embedding and multiple text prompts, we resort to the optimal transport (OT) method \cite{robust_OT, pham2020unbalanced, sejourne2023unbalanced,chen2022plot,dong2023partial,nguyen2024dude,zhan2021unbalanced}, providing flexibility in aligning heterogeneous data distributions. This property is beneficial in few-shot WSI classification when it can (i) handle data augmentation with noise, as OT can adapt to perturbations without losing meaningful structural relationships, and (ii) capture imbalances masses between two modality embeddings when text prompts only describe sub-regions in WSI samples. Through extensive evaluations of three datasets with various architectures (\texttt{CLIP-ResNet50}, \texttt{CLIP-ViTB16}, \texttt{PLIP}, and \texttt{(Prov-GigaPath)-integrated PLIP}), we observe that \texttt{\textsc{MGPath}} demonstrate consistent improvements over several state-of-the-art MIL and VLM in literature ($14$ competitors). As an example, \texttt{\textsc{MGPath}} with \texttt{(Prov-GigaPath)-PLIP} variant outperforms \texttt{MSCPT} \cite{han2024mscpt} by $5\%$ in F1 and $8\%$ in AUC on the \texttt{TCGA-BRCA} dataset. Additionally, it also surpasses two state-of-the-art VLMs models, CONCH \cite{lu2024visual} and QUILT \cite{ikezogwo2024quilt}, by approximately 6\% in accuracy on \texttt{TCGA-BRCA}.


\section{Related Work} \label{section:related_work}

\subsection{Large-scale Pre-trained Models for Pathology}
Recent advancements in large-scale pre-trained models for pathology can be broadly classified into two categories. \textit{Vision models}, such as 
\texttt{Virchow} \cite{ikezogwo2024quilt}, 
\texttt{Hibou} \cite{nechaev2024hibou}, \texttt{UNI} \cite{chen2024towards}, and \texttt{Prov-GigaPath} \cite{xu2024whole} leverage massive pathology image datasets to learn robust visual representations. Among these, \texttt{Prov-GigaPath} stands out as the largest model, trained on 1.3 billion pathology image patches, and excels in resolving complex tissue patterns at high resolution. On the other hand, \textit{vision-language models} (VLMs) like \texttt{PLIP} \cite{huang2023visual} (trained 200K image-text pairs), \texttt{CONCH} \cite{lu2024visual} (1.17M), or \texttt{QUILTNET}\cite{ikezogwo2024quilt} (1M), integrate visual and textual information to enhance contextual understanding and improve pathology slide interpretation. In contrast,  our \texttt{\textsc{MGPath}} combines the strengths of both approaches by using a \textit{parameter-efficient adaptor} to link \texttt{Prov-GigaPath} (the largest pre-trained vision encoder) with a text encoder from VLMs like \texttt{PLIP} or \texttt{CONCH}, leveraging both rich visual features and semantic textual embeddings. Although we use the \texttt{PLIP} text encoder in our experiments due to its common use in baselines, the method can be extended to other larger pre-trained text models.


\subsection{Few-shot learning in WSI}
MIL treats a WSI as a bag of patches and aggregates these instances into a bag of features, with early methods using non-parametric techniques like mean or max pooling. However, since disease-related patches are rare, these methods can overwhelm useful information with irrelevant data. To address this, attention-based methods, graph neural Networks (GNNs), and Transformer-based methods have been introduced \cite{lu2021data,chen2021whole,ilse2018attention,li2021dual,shao2021transmil,zheng2022graph}. In contrast, VLMs have gained popularity through contrastive learning, aligning image-text pairs to enhance performance on a variety of tasks. While collecting large-scale pathology image-text pairs remains challenging, models like MI-Zero, \texttt{PLIP}, and \texttt{CONCH} have been trained on hundreds of thousands to over a million pathology image-text pairs \cite{lu2023visual,huang2023visual,lu2024visual}.  Some approaches also integrate multi-magnification images and multi-scale text to mimic pathologists’ diagnostic processes, especially for detecting subtle abnormalities \cite{shi2024vila,han2024mscpt}. Our \texttt{\textsc{MGPath}} extends on the VLMs strategy by further \textit{amplifying the benefits of using a large pre-trained pathology} VLM model and introducing a new \textit{parameter-efficient multi-granular prompt learning} to adapt these models to few-shot settings.

\subsection{Prompt Learning for Vision-Language Adaptations}
Prompt tuning is proposed to transfer large pre-trained model task-specific downstream tasks and has shown strong results in multimodal models like CLIP. Rather than design a heuristic template, several methods like \texttt{CoOp} \cite{zhou2022learning}, \texttt{CoCoOp} \cite{zhou2022conditional}, or \texttt{MaPLe} \cite{khattak2023maple} among others \cite{rao2022denseclip,shu2022test} have allowed models to determine optimal prompts from multiple perspectives, such as domain generalization \cite{ge2023domain,yao2024tcp}, knowledge prototype \cite{zhang2022prompting,li2024steering}, or diversity \cite{lu2022prompt,shu2022test}. However, these approaches focus on natural images and do not address the unique challenges of whole-slide pathology images, which require multi-scale and structural contextual information. While a few current methods typically integrate prompts with frozen visual features via self-attention \cite{shi2024vila,qu2024rise}, these approaches might struggle with the complex relationships in WSIs. Our solution introduces multi-granular prompt learning, bridging \textit{attention} on both \textit{individual image patches} and \textit{spatial groups} to better align with the hierarchical structure of WSI data.


\section{Methods} 
\label{sec:methods}
Figure \ref{fig:architecture} provides an overview of the key steps in our method. Before diving into the details of each section, we first introduce our \texttt{PLIP} model enhanced by \texttt{Prov-GigaPath} through the use of adaptors.

\begin{figure}[H]
  \centering
\includegraphics[width=1.0\linewidth]{figures/MGPATH-detail.pdf}
   \caption{The pipeline of the proposed \textsc{MGPath} method. Low- and high-resolution image patches are processed with large language models to generate visual contextual descriptions (Section \ref{sec:text_prompt}). Visual prompts are integrated with frozen features through multi-granular attention at both patch and group-of-patch levels \ref{sec:visual_prompt}. The final output is obtained by aligning visual and text embeddings using optimal transport (Section \ref{sec:ot_distance}).}
   \label{fig:architecture}
\end{figure}

\subsection{Bridging Pathology Visual and Text Encoders}
To leverage \texttt{Prov-GigaPath}’s extensive pre-trained visual features for pathology, we implement lightweight adaptors that map image patch-level features to an embedding space aligned with the \texttt{PLIP} text encoder. These adaptors allow us to train joint image-text representations with parameter efficiency by updating only the adaptor weights.

Given a set of collected pathology image-text pairs $\left\{(\mathbf{I}_{i}, \mathbf{T}_{i})|\, i = 1, 2.., N\,\right\}$ (Sec. \ref{sec:settings}), we denote by $E_{I}(.)$ be a pre-trained vision encoder from \texttt{Prov-GigaPath}, extracting patch-level feature, and $E_{T}(.)$ the pre-trained text encoder from \texttt{PLIP} model. Given a batch size of $B$ samples, the image and text embeddings are computed as $\mathbf{x}_{i} = E_{I}(\mathbf{I}_{i}) \in \mathbb{R}^{d_v},\, \mathbf{t}_{i} = E_{T}(\mathbf{T}_{i}) \in \mathbb{R}^{d_t}$. \textit{We then design two trainable adaptors} $A_{I}(.)$ and $A_{T}(.)$, that maps $\left(\mathbf{x}_{i}, \mathbf{t}_{i}\right)$ into the same hidden dimension $\mathbb{R}^{d}$ and minimizes the noise contrastive loss \cite{oord2018representation}:
\setlength{\abovedisplayskip}{5pt} % Adjust space above
\setlength{\belowdisplayskip}{5pt} % Adjust space below
{\small
\begin{equation}
    \mathcal{L}_{con} = \mathbb{E}_{B}\left[-\log \frac{\exp\left(\cos(A_{I}(\mathbf{x}_{i}),A_{T}(\mathbf{t}_{i}))/\tau\right)}{\sum_{j} \exp\left(\cos(A_{I}(\mathbf{x}_{i}),A_{T}(\mathbf{t}_{j}))/\tau\right)}\right],
    \label{eq:contrast}
\end{equation}
}
where $\cos(.)$ is the cosine similarity, and $\tau$ denotes for temperature of the softmax function. 
For parameter efficiency, we train only the adaptors $A_{I}(.), A_{T}(.)$ while keeping the \texttt{Prov-GigaPath} visual encoder and \texttt{PLIP} text encoder frozen. After optimizing Eq.\eqref{eq:contrast}, we use the outputs of the adaptors as visual and text embeddings for downstream tasks. Unless otherwise specified, we refer to this model as \texttt{GigaPath-PLIP}.

\subsection{Multi-Magnification Descriptive Text Prompts}
\label{sec:text_prompt}

To improve vision-language models (VLMs) for whole-slide image (WSI) analysis, designing effective text prompts is essential. Pathologists typically examine WSIs by first assessing tissue structures at low magnification before zooming in to analyze finer details such as nuclear size and shape. Inspired by this diagnostic workflow and the inherently multi-scale nature of WSIs, recent studies ~\cite{shi2024vila,han2024mscpt} have introduced dual-scale visual descriptive text prompts to guide VLMs, leading to significant improvements in classification performance. Building on this observation, we further extend and refine this strategy to enhance model effectiveness.

First, to ensure that generated prompts remain robust across varying WSI magnifications, we design shared prompts that combine both high- and low-scale descriptive elements, treating them as contextual embeddings.  Specifically, we leverage the API of a frozen language model (GPT-4) and query it with the prompt as Figure \ref{fig:prompt_tem_1}

\begin{figure}[!hbt] % Use [H] to keep the figure in place (requires float package)
    \centering
    \resizebox{1.\textwidth}{!}{
    \begin{tcolorbox}[colback=white, colframe=black,
    boxrule=0.5pt, % Adjust the thickness of the frame
    title=LLM Prompt
    ]

    \texttt{\small{What visually descriptive features characterize \{class name\} at both low and high resolutions within the whole-slide image? Please summarize into a single paragraph.}}
    \end{tcolorbox}}
    \caption{LLM template prompt.}
    \label{fig:prompt_tem_1}
\end{figure}

In the above query, we replace \texttt{\{class name\}} by specific categories, for e.g., they are invasive ductal carcinoma  (IDC) and invasive lobular carcinoma (ILC) in the TCGA-BRCA dataset. 

Second, at each low/high scale, rather than inserting \textit{a single learnable text prompt} of length $K$ alongside a frozen contextual prompt from LLMs \cite{shi2024vila,han2024mscpt}, we propose \textit{using $M$ learnable prompts}. This approach aims to capture different sub-regions or structural features within each patch that might be overlooked with only a single prompt. Specifically, we define visual descriptive text prompts for both low and high resolutions as follows:
\begin{equation}
\begin{split}
        \mathbf{T}_{i}^{(l)} & = \left\{ \left([\mathbf{\omega}_{i}^{(l)}]_{1}\,[\mathbf{\omega}_{i}^{(l)}]_{2}\,...[\omega_{i}^{(l)}]_{K} [\mathrm{LLM\,context}]\right)|_{i=1}^{M}\right\} \\
        \mathbf{T}_{i}^{(h)} & = \left\{ \left([\mathbf{\omega}_{i}^{(h)}]_{(1)}\,[\mathbf{\omega}_{i}^{(h)}]_{2}\,...[\omega_{i}^{(h)}]_{K} [\mathrm{LLM\,context}]\right)|_{i=1}^{M}\right\},
        \label{eq:text_prompt}
\end{split}
\end{equation}
where $[\omega_{i}^{\beta}]_{j}, j \in [1,...,K], i \in [1,..,M]$ are $KM$ \textit{trainable textual prompts} for each resolution $\beta \in \{l, h\}$.

\subsection{Granularity-aware Visual Prompt Learning}
\label{sec:visual_prompt}
We propose to adapt visual prompts to frozen features extracted by a pre-trained vision encoder in the VLM model by taking into account the image patch level and spatial groupings of patches. Specifically, for each WSI $W$, we denote by $\left\{W^{(l)}, W^{(h)}\right\}$ are representations of $W$ at low and high magnification. We define a bag of multiple instances of $W$ as $I = \left\{I^{(l)}, I^{(h)}\right\}$ where $I^{(l)} \in \mathbb{R}^{N_{l}\times N_{b}\times N_{b}\times 3} $, $I^{(h)} \in \mathbb{R}^{N_{h} \times N_{b}\times N_{b}\times 3}$ with $N_{l}, N_{h}$ indicate the number of low and high-resolution image patches and $N_{b}$ is the patch size. Following prior works \cite{shi2024vila,ilse2018attention,lu2021data,han2024mscpt}, we employ a non-overlapping sliding window technique to extract patches $I$  from the WSI.
\vspace{-0.1in}
\subsubsection{Patches-based Prompting}
The frozen image encoder $E_{I}(.)$ (or $A_{I}(E_{I}(.))$ in case of \texttt{GigaPath-PLIP}) is used to map patches $I$ into a feature vector $H = \{H^{(l)} \in \mathbb{R}^{N_{l}\times d}, H^{(h)} \in \mathbb{R}^{N_{h}\times d}\}$ where $d$ denotes the feature dimension. To effectively consolidate the extensive set of patch features into a final slide-level representation, we introduce a set of learnable visual prompts $\mathbf{p}_{v} \in \mathbb{R}^{N_{p}\times d}$,  which facilitate the progressive merging of patch features in $H^{(l)}$(similarly for $H^{(h)}$) (Figure \ref{fig:architecture}). In particular, we formulate $\mathbf{p}_{v}$ as Query and take all features in $H^{(l)}$ as the Keys $K^{(l)}_{p}$ and Values $V^{(l)}_{p}$ in in self-attention \cite{vaswani2017attention}. We then associate $\mathbf{p}_{v}$ with patch features as:
{
\begin{equation}
    \mathbf{p}_{v,p}^{(l)} = \operatorname{Normalize}\left(\operatorname{SoftMax}\left(\frac{\mathbf{p}_{v}K_{p}^{(l)^{T}}}{\sqrt d}\right)V_{p}^{(l)}\right) + \mathbf{p}_{v},
    \label{eq:att-patch}
\end{equation}
}
where $\operatorname{Normalize(.)}$ and $\operatorname{Softmax(.)}$ indicate the layer normalization operator and activation function respectively. Intuitively, Eq.\eqref{eq:att-patch} computes the correlations between the visual prompt $\mathbf{p}_{v}$ and all individual feature patches in $H^{l}$, subsequently grouping patches with high similarity to form fused prompt embeddings. However, since cancerous tissues in WSIs often appear as large, contiguous regions of adjacent image patches, this motivates the introduction of spatial patch group-based attention.
\vspace{-0.1in}
\subsubsection{Spatial Patch Group-based Prompting}
We build spatial correlations for multiple instances in $I$ by using their image patch coordinates inside each WSI $W$. In particular, taken $I^{(l)} = \left\{I^{(l)}_{1},\,I^{(l)}_{2},...,I^{(l)}_{N_{l}}\right\}$ with their corresponding extracted features in $H^{(l)} = \left\{H^{(l)}_{1}, H^{(l)}_{2},...,H^{(l)}_{N_{l}}\right\}$, we construct a graph $G^{(l)} = (V^{(l)}, E^{(l)})$ to capture regional tissue structures where the set of vertices $V^{(l)} = I^{(l)}$, and $E^{(l)} \in \{0, 1\}^{N_{l}\times N_{l}}$ is the set of edges. Edges in $E^{(l)}$ can be defined by linking inner patches to their K-nearest neighbors based on the coordinates. We define the node-feature embedding as $X^{(l)} = H^{(l)} \in \mathbb{R}^{N_{l}\times d}$ that associates each vertex $v^{(l)}_{i}$ with its feature node $x_{i}^{(l)} = H^{(l)}_{i}$.

We next design a trainable message-passing network $g_{\epsilon}(.)$ based on the graph attention layer (GAT) \cite{velivckovic2017graph} to capture the feature representation of each node and its local neighbors. The message passing of the GAT layer is formulated as:

\allowdisplaybreaks
\setlength{\abovedisplayskip}{3pt} % Adjust space above
\setlength{\belowdisplayskip}{3pt} % Adjust space below
\vspace{-0.15in}
\begin{equation}
\begin{split}
    \alpha_{i,j} & = \frac{\exp{\left(\sigma(a_s^T\Theta_s\,x_i^{(l)} + a_t^T\Theta_t\,x_j^{(l)})\right)}}{\sum_{k \in \mathcal{N}(i) \cup \{i\}}\exp({\sigma(a_s^T\Theta_s\,x_i^{(l)} + a_t^T\Theta_t\,x_k^{(l)}))}} \\
    x_i^{(l)'} & = \alpha_{i,i}\Theta_s\,x_i^{(l)} + \sum_{j \in \mathcal{N}(i)} \alpha_{i,j}\Theta_t\,x_j^{(l)}, 
\end{split}
\end{equation}

where $x_i^{(l)'}$ is aggregated features of $x_{i}^{(l)}$ with its local region after GAT layer, $\sigma(.)$ is the \texttt{LeakyReLU} activation function, $\mathcal{N}(i)$ denote the neighboring nodes of the $i$-th node, $\alpha_{i,j}$ are the attention coefficients and $a_s, a_t, \Theta_{s}, \Theta_{t}$ are weight parameters of $g_{\epsilon}(.)$.  

After doing a message passing by $g_{\epsilon}(.)$, the graph of patch-image features $G^{(l)}$ is updated to $G^{(l)'}$, where each node now represents a super-node that encapsulates its corresponding feature region. We then squeeze all feature nodes in $G^{(l)'}$ as a vector $H^{(l)}_{gr}$ and treat them as another Keys $K_{gr}^{(l)}$ and Values $V_{gr}^{(l)}$ for region-level features. Similar to Eq.\eqref{eq:att-patch}, we associate prompt $\mathbf{p}_{v}$ with those group-level features:
{
\begin{equation}
    \mathbf{p}_{v,gr}^{l} = \operatorname{Normalize}\left(\operatorname{SoftMax}\left(\frac{\mathbf{p}_{v}K_{gr}^{(l)^{T}}}{\sqrt d}\right)V_{gr}^{(l)}\right) + \mathbf{p}_{v}.
    \label{eq:att-group}
\end{equation}}

The final output of our multi-granular is computed as:
\setlength{\abovedisplayskip}{5pt} % Adjust space above
\setlength{\belowdisplayskip}{5pt} 
\begin{equation}
    \mathbf{p}_{v}^{(l)} = (1-\alpha) \cdot \mathbf{p}_{v,p}^{(l)} +  \alpha \cdot \mathbf{p}_{v,gr}^{(l)},
    \label{eq:visual_prompt}
\end{equation}
which interpolates between image patches and spatial patch groups.

\subsection{Optimal Transport for Visual-Text Alignment}
\label{sec:ot_distance}

 Given descriptive text prompts  $\mathbf{T}^{(l)}$  and  $\mathbf{T}^{(h)}$  (Eq.\eqref{eq:text_prompt}) and visual prompt-guided slide features  $\mathbf{p}_{v}^{(l)}$  and  $\mathbf{p}_{v}^{(h)} $ (Eq.\eqref{eq:visual_prompt}) for low and high resolutions, our goal is to maximize the similarity between slide and text embeddings for each class $c$. Rather than relying on cosine distance, as in prior works \cite{zhou2022learning,zhou2022conditional,zhao2024learning,qu2024rise,qu2024rise,singh2020model}, we propose using optimal transport (OT)-based distance to capture a more nuanced cross-alignment between visual and text domains. Although OT has been explored for prompt learning in natural images and multi-modal learning \cite{kim2023zegot,chen2022plot,nguyen2024logra,sejourne2023unbalanced}, we are the first to adapt it for whole-slide imaging (WSI), effectively handling the alignment of multi-magnification patches to capture rich structural details across scales.
\vspace{-0.1in}
\paragraph{Recap OT:} Given two sets of points (features), we can represent the corresponding discrete distributions as follows:
\begin{equation}
\bs{\mu} = \sum^{M}_{i=1} p_{i} \delta_{f_{i}}, \quad \bs{\nu} = \sum^{N}_{j=1} q_{j} \delta_{ g_{i}}, 
\end{equation}
where $\delta_f$ and $\delta_g$ represent Dirac delta functions centered at $\bs{f}$ and $\bs{g}$, respectively, and $M$ and $N$ indicate the dimensions of the empirical distribution. 
The weight vectors $\boldsymbol{p} = \{p_i\}^M_{i=1}$ and $\boldsymbol{q} = \{q_i\}^{N}_{j=1}$ lie within the $M$ and $N$-dimensional simplex, respectively, meaning they satisfy $\sum_{i=1}^{M} p_i = 1$ and $\sum_{j=1}^{N} q_j = 1$. The discrete optimal transport problem can then be expressed as:
\vspace{-0.05in}
\begin{eqnarray}
\bs{T}^{\ast} = \underset{\bs{T}\in \mathbb{R}^{MXN}}{\arg{\min}} \sum^{M}_{i=1}\sum^{N}_{j=1}\bs{T}_{ij} \bs{C}_{ij} \nonumber \\ \textrm{s.t.} \quad \bs{T}\bs{1}^{N} = \bs{\mu}, \quad \bs{T}^{\top}\bs{1}^{M} = \bs{\nu} .
\label{DOT}
\end{eqnarray}
where $\bs{T}^{\ast}$ is denoted as the optimal transport plan, which is optimized to minimize 
the total distance between the two probability vectors, $\bs{C}$ is the cost matrix which measures the distance between $\boldsymbol{f}_i$ and $\boldsymbol{g}_j$. 
We then define the OT distance between $\bs{\mu}$ and $\bs{\nu}$ as:
\begin{equation}
    d_{\mathrm{OT}}(\bs{\mu}, \bs{\nu}) = \langle \bs{T}^{\ast}, \bs{C} \rangle.
    \label{eq:OT-distance}
\end{equation}
\vspace{-0.3in}
\paragraph{Objective functions:}
Given the visual prompt-guided slide features $\mathbf{p}_{v}^{(l)} \in \mathbb{R}^{N_{p}\times d}$ in Eq.\eqref{eq:visual_prompt} and the descriptive text prompts $\mathbf{T}^{(l)}$ in Eq.\eqref{eq:text_prompt}, we compute the textual embedding for $\mathbf{T}^{(l)}$ as $\mathbf{p}_{t}^{(l)} = E_{T}(\mathbf{T}^{(l)}) \in \mathbb{R}^{M\times d}$. 
\vspace{-0.1in}
We next denote $\mathbf{T}^{(l)}_{c}$ as the input text prompts, $\left(\mathbf{p}_{t}^{(l)}\right)_{c}$ as the extracted textual embedding, and $\left(\mathbf{p}_{v}^{(l)}\right)_{c}$  as the visual prompt-guided slide features associated with class $c$. We then aim to minimize the distance between $\mathbf{T}^{(l)}_{c}$ and $\left(\mathbf{p}_{v}^{(l)}\right)_{c}$, indicated as $d_{\mathrm{OT}}\left(\mathbf{T}^{(l)}_{c}, \left(\mathbf{p}_{v}^{(l)}\right)_{c}\right)$ in the paper, by computing optimal transport distance between $\left(\mathbf{p}_{t}^{(l)}\right)_{c}$ and $\left(\mathbf{p}_{v}^{(l)}\right)_{c}$. Specifically, we treat $\left(\mathbf{p}_{t}^{(l)}\right)_{c}  \rightarrow \boldsymbol{F} = \left\{\boldsymbol{f}_{i}|_{i=1}^{M}\right\}$ and $\left(\mathbf{p}_{v}^{(l)}\right)_{c}  \rightarrow \boldsymbol{G} = \left\{\boldsymbol{g}_{j}|_{j=1}^{N_{p}}\right\}$ and compute the cost matrix $\bs{C}$ as \, $\bs{C} = \left(\bs{1} - \boldsymbol{F}^{T}\,\boldsymbol{G}\right) \in \mathbb{R}^{M\times N_{p}}$, which used to compute $\bs{T}^{\ast}$ in Eq.\,\eqref{DOT} for estimate optimal transport distance defined in Eq.\,\eqref{eq:OT-distance}. Following the same procedure, we can also compute $d_{\mathrm{OT}}\left(\mathbf{T}^{(h)}_{c}, \left(\mathbf{p}_{v}^{(h)}\right)_{c}\right)$ at high-resolution image patches. Then, the prediction probability is written as:
{
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\begin{equation}
    \mathrm{P}_{c} =  \frac{\exp(2 - \sum_{k \in \{l, h\} }\,d_{\mathrm{OT}}\left(\mathbf{T}^{(k)}_{c}, \left(\mathbf{p}_{v}^{(k)}\right)_{c}\right))}{\sum_{c'=1}^{C}\exp(2 - \sum_{k \in \{l, h\} }\,d_{\mathrm{OT}}\left(\mathbf{T}^{(k)}_{c'}, \left(\mathbf{p}_{v}^{(k)}\right)_{c}\right))},
\end{equation}
}
where $\lambda_{k}$ controls contribution of each-resolution. Finally, we can train the model with the cross-entropy as:
\begin{equation}
    \mathcal{L}_{class} = \operatorname{Cross}(\mathrm{P}, \mathrm{GT}),
\end{equation}
with $\operatorname{Cross}(.)$ be the cross-entropy and $\mathrm{GT}$ denotes slide-level ground-truth.


The details for solvers of Eq.(\ref{eq:OT-distance}) and a relaxed version with unbalanced optimal transport are presented in Sections~(\ref{sec:uot-ot}) and (\ref{subsec:solver_ot}) in Appendix. Intuitively, using OT, in this case, offers several key advantages over cosine similarity. Pathology images exhibit complex, heterogeneous patterns that can be described from multiple perspectives. OT models these relationships as a distribution, enabling a more holistic alignment that handles variability and incomplete details while reducing noise from irrelevant prompts. This enhances the model’s ability to generalize to unseen or complex disease cases. 
\vspace{-0.1in}
\section{Experiments}
\label{sec:experiments}
\label{sec:settings}
\vspace{-0.1in}
\subsection{Settings}
\vspace{-0.05in}
\paragraph{Datasets for contrastive learning.}
\texttt{PatchGastricADC22}\cite{tsuneki2022inference} consists of approximately 262K patches derived from WSI of H\&E-stained gastric adenocarcinoma specimens, each paired with associated diagnostic captions collected from the University of Health and Welfare, Mita Hospital, Japan. \texttt{QUILT-1M} \cite{ikezogwo2024quilt} includes approximately 653K images and one million pathology image-text pairs, gathered from 1,087 hours of educational histopathology videos presented by pathologists on YouTube. \texttt{ARCH} \cite{gamper2021multiple} is a pathology multiple-instance captioning dataset containing pathology images at the bag and tile level. However, our work focuses on tile-level images from all datasets for our contrastive training strategy. In total, we collected approximately 923K images from these datasets.
\vspace{-0.1in}
\paragraph{Downstream tasks.}
For the classification task, the proposed method was evaluated in three datasets from the Cancer Genome Atlas Data Portal\cite{TCGA}: \texttt{TCGA-NSCLC}, \texttt{TCGA-RCC}, and \texttt{TCGA-BRCA}. We followed the \texttt{ViLa-MIL}\cite{shi2024vila} experimental settings for \texttt{TCGA-NSCLC} and \texttt{TCGA-RCC}, randomly selecting proportions for training, validation, and testing. For \texttt{TCGA-BRCA}, we adapted the training and testing slide ID from \texttt{MSCPT} \cite{han2024mscpt}. The detailed description is included in the appendix section.

\paragraph{Implementation Details.}

We followed the \texttt{ViLa-MIL} preprocessing pipeline for tissue region selection and patch cropping. To integrate our attention module with \texttt{CLIP50} and \texttt{PLIP}, we extracted tile-level embeddings from their frozen vision encoders (1024-dimensional for \texttt{CLIP50} and 512 for \texttt{PLIP}). We used the visual encoder of \texttt{Prov-GigaPath} to produce 1536-dimensional embeddings. To align it with \texttt{PLIP}'s frozen text encoder, we developed two MLP-based adaptors that project both encoders into a shared feature space during a contrastive learning process, using datasets outlined in Section \ref{sec:settings}.
 \begin{wrapfigure}{r}{0.6\textwidth}
 \captionsetup{type=table}
\centering
\footnotesize
\setlength{\tabcolsep}{2pt}
\caption{Comparison of methods on TCGA-BRCA with few-shot settings. Results are shown for AUC, F1, and Accuracy (ACC). FVM denotes for foundation vision-language models.}
\label{tab:TCGA-BRCA}
\resizebox{0.58\textwidth}{!}{
\begin{tabular}{c|lcccc}
\toprule
\multirow{22}{*}{\makebox[0pt][c]{\rotatebox{90}{\textbf{CLIP ImageNet Pretrained}}}} & \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{\# Param.}}  & \multicolumn{3}{c}{\textbf{TCGA-BRCA}}  \\
\cmidrule(lr){4-6} %\cmidrule(lr){7-9} \cmidrule(lr){10-12}
& & & \textbf{AUC} & \textbf{F1} & \textbf{ACC}  \\
\midrule
& Max-pooling & 197K & 60.42$\pm$4.35 & 56.40$\pm$3.58 & 68.55$\pm$6.54  \\
& Mean-pooling & 197K  & 66.64$\pm$4.21 & 60.70$\pm$2.78 & 71.73$\pm$3.59  \\
& ABMIL~\cite{ilse2018attention} & 461K & 69.24$\pm$3.90 & 61.72$\pm$3.36 & 72.77$\pm$3.15  \\
& CLAM-SB~\cite{lu2021data} & 660K &  67.80$\pm$5.14 & 60.51$\pm$5.01 & 72.46$\pm$4.36  \\
& CLAM-MB~\cite{lu2021data} & 660K &  60.81$\pm$4.87 & 55.48$\pm$4.96 & 67.31$\pm$4.19  \\
& TransMIL~\cite{shao2021transmil} & 2.54M  & 65.62$\pm$3.20 & 60.75$\pm$4.04 & 67.52$\pm$4.16 \\
& DSMIL~\cite{li2021dual} & 462K  & 66.18$\pm$3.08 & 59.35$\pm$3.18 & 67.52$\pm$1.56  \\
& RRT-MIL~\cite{tang2024feature} & 2.63M & 66.33$\pm$4.30 & 61.14$\pm$5.93 & 71.21$\pm$8.94  \\
& CoOp~\cite{zhou2022learning} & 337K  & 68.86$\pm$4.35 & 61.64$\pm$2.40 & 71.08$\pm$3.22  \\
& CoCoOp~\cite{zhou2022conditional} & 370K  & 69.13$\pm$4.27 & 61.48$\pm$2.62 & 72.41$\pm$1.87  \\
& Metaprompt~\cite{zhao2024learning} & 360K  & 69.12$\pm$4.46 & 63.39$\pm$4.38 & 74.65$\pm$7.20  \\
& TOP~\cite{qu2024rise} & 2.11M &  69.74$\pm$3.14 & 63.39$\pm$4.62 & 74.41$\pm$5.27  \\
& ViLa-MIL~\cite{shi2024vila} & 2.77M & 72.25$\pm$6.16 & 62.04$\pm$2.38 & 75.01$\pm$6.14  \\
& {MSCPT} \cite{han2024mscpt} & 1.35M  & \underline{74.56$\pm$4.54} & \textbf{65.59$\pm$1.85} & {75.82$\pm$2.38} \\
&\cellcolor{cyan!15}\textbf{\textsc{MGPath} (ViT)} & \cellcolor{cyan!15}592K & \cellcolor{cyan!15}\textbf{74.96$\pm$6.98} & \cellcolor{cyan!15}\underline{64.60$\pm$5.39} & \textbf{77.10$\pm$2.39}\cellcolor{cyan!15}  \\
\midrule
\multirow{2}{*}{\makebox[0pt][c]{\rotatebox{90}{\textbf{\small{FVM}}}}} & {CONCH \cite{lu2024visual}}  & 110M & {84.11$\pm$15.44} & {65.63$\pm$10.81} & {73.24$\pm$8.89} \\
&  {QUILT ~\cite{ikezogwo2024quilt}}  & 63M & {73.48$\pm$10.57} & {63.78$\pm$8.72} & {73.26$\pm$10.13} \vspace{0.04in}
\\
\midrule
\multirow{16}{*}{\rotatebox{90}{\textbf{PLIP Pathology Pretrained}}} & Max-pooling & 197K  & 66.50$\pm$2.74 & 61.50$\pm$2.88 & 71.57$\pm$4.82 \\
& Mean-pooling & 197K &  71.62$\pm$2.41 & 66.34$\pm$2.96 & 74.45$\pm$2.49  \\
& ABMIL~\cite{ilse2018attention} & 461K  & 72.41$\pm$4.25 & 63.04$\pm$3.62 & 74.09$\pm$4.38  \\
& CLAM-SB~\cite{lu2021data} & 660K &  72.34$\pm$6.17 & 65.51$\pm$3.28 & 76.16$\pm$4.36  \\
& CLAM-MB~\cite{lu2021data} & 660K &  73.41$\pm$3.76 & 66.11$\pm$1.94 & 77.88$\pm$2.30  \\
& TransMIL~\cite{shao2021transmil} & 2.54M  & 74.98$\pm$6.01 & 67.50$\pm$6.00 & 77.04$\pm$6.14  \\
& DSMIL~\cite{li2021dual} & 462K &  71.44$\pm$2.72 & 64.48$\pm$1.64 & 75.26$\pm$2.28  \\
& RRT-MIL~\cite{tang2024feature} & 2.63M &  71.21$\pm$6.46 & 64.15$\pm$1.38 & 75.92$\pm$5.10  \\
& CoOp~\cite{zhou2022learning} & 337K  & 71.53$\pm$2.45 & 64.84$\pm$2.40 & 74.22$\pm$5.02  \\
& CoCoOp~\cite{zhou2022conditional} & 370K & 72.65$\pm$4.63 & 66.63$\pm$3.55 & 66.98$\pm$3.35  \\
& Metaprompt~\cite{zhao2024learning} & 360K &  74.86$\pm$4.25 & 65.03$\pm$1.81 & 77.88$\pm$3.22 \\
& TOP~\cite{qu2024rise} & 2.11M & 76.13$\pm$6.01 & 66.55$\pm$1.72 & 78.58$\pm$5.30  \\
& ViLa-MIL~\cite{shi2024vila} & 2.77M & 74.06$\pm$4.62 & 66.03$\pm$1.81 & 78.12$\pm$4.88  \\
& {MSCPT} \cite{han2024mscpt} & 1.35M & {75.55$\pm$5.25} & {67.46$\pm$2.43} & {79.14$\pm$2.63} \\
& \cellcolor{cyan!15}\textbf{\textsc{MGPath} }& \cellcolor{cyan!15}592K & \cellcolor{cyan!15}\underline{79.02$\pm$6.43} & \cellcolor{cyan!15}\underline{68.25$\pm$4.42} & \cellcolor{cyan!15}\textbf{79.65$\pm$1.72} \\
& \cellcolor{cyan!15}\textbf{\textsc{MGPath} (PLIP-G)} & \cellcolor{cyan!15}5.35M & \cellcolor{cyan!15}\textbf{87.36$\pm$1.85} & \cellcolor{cyan!15}\textbf{73.13$\pm$3.49} & \cellcolor{cyan!15}\underline{79.56$\pm$4.77}  \\
\bottomrule
\end{tabular}
}
\end{wrapfigure}
\vspace{-0.2in}

To implement spatial attention, we use a Graph Attention Network (GAT) to model spatial relationships between WSI patches. Each tile-level embedding serves as a node, connected to its left, right, top, and bottom neighbors, ensuring local spatial dependencies are captured. We then integrate spatial patch group-based attention $\mathbf{p}_{v,gr}$ into patch-based attention $\mathbf{p}_{v,p}$ using Equation \ref{eq:visual_prompt}. The hyperparameter $\alpha$ (0 to 1) controls the balance between spatial context and prototype-based guidance.

\vspace{-0.1in}
\subsection{Comparison to State-of-the-Art}
We compare our \texttt{MGPath} with state-of-the-art multi-instance learning methods, including \texttt{Maxpooling}, \texttt{Mean-\\pooling}, \texttt{ABMIL}~\cite{ilse2018attention}, \texttt{CLAM}~\cite{lu2021data}, \texttt{TransMIL}~\cite{shao2021transmil}, \texttt{DSMIL}~\cite{li2021dual}, \texttt{GTMIL}~\cite{zheng2022graph}, \texttt{DTMIL}~\cite{zhang2022dtfd}, \texttt{RRT-MIL}~\cite{tang2024feature} and
\texttt{IBMIL}~\cite{lin2023interventional}, and vision-language methods, including \texttt{CoOp}~\cite{zhou2022learning}, \texttt{CoCoOp}~\cite{zhou2022conditional}, \texttt{Metaprompt}~\cite{zhao2024learning}, \texttt{TOP}~\cite{qu2024rise}, 
\texttt{ViLa-MIL}~\cite{shi2024vila}, \texttt{MSCPT}~\cite{han2024mscpt}, \texttt{QUILT}~\cite{ikezogwo2024quilt},
\texttt{CONCH}~\cite{lu2024visual}. Among these, \texttt{QUILT} and \texttt{CONCH} are foundation VLMs.

% \paragraph{MGPath versions}
We provide different versions for our \texttt{MGPath} including \texttt{CLIP} backbone \texttt{ReNet-50} (\texttt{CLIP50}) for \texttt{TCGA-NSCLC} and \texttt{TCGA-RCC} and \texttt{ViT-16} backbone for \texttt{TCGA-BRCA}. We also provide a version using \texttt{PLIP} backbone, as well as \texttt{GigaPath-PLIP}, which was pre-trained on the Pathology dataset.


\subsection{Results on Few-shot and Zero-shot Settings.}
\paragraph{\textsc{MGPath} with CLIP and PLIP backbones outperform several competitive MIL and VLM methods.}

As shown in Tables \ref{tab:TCGA-NSCLC-RCC} and \ref{tab:TCGA-BRCA}, our \texttt{MGPath}, based on \texttt{CLIP50} and \texttt{PLIP}, outperforms several baseline models and achieves significant improvements over other VLMs with similar architectures, such as ViLa and MSCPT. The performance gain is particularly notable with the \texttt{PLIP} backbone. For example, on TCGA-BRCA using \texttt{CLIP} (ViT), \texttt{MGPath} achieves an accuracy of 77.10\%, compared to 75.82\% for MSCPT and 75.01\% for ViLA-MIL. Additionally, with the \texttt{PLIP} backbone, \texttt{MGPath} surpasses MSCPT and ViLa-MIL by margins of approximately 3.5\% to 5\%.

\vspace{-0.15in}
\paragraph{GigaPath-PLIP is a strong pre-trained VLM.}
We validated our whole-slide vision foundation model, pre-trained on 1.3 billion pathology images, using the \texttt{PLIP} text encoder. By incorporating pathology-specific features from \texttt{Prov-GigaPath} \cite{xu2024whole}, the integrated \texttt{MGPath (PLIP-G)} model demonstrated strong performance across multiple metrics on the \texttt{TCGA-NSCLC}, \texttt{TCGA-RCC}, and \texttt{TCGA-BRCA} datasets. When compared to other foundation VLMs such as \texttt{CONCH} and \texttt{QUILT}, our model consistently outperforms them. For example, we achieve a 3\% improvement in AUC over \texttt{CONCH} on both the TCGA-BRCA and TCGA-NSCLC datasets.

\begin{table}
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
% \begin{minipage}{.66\hsize} % <-- scale here
\caption{Comparison of methods on TCGA-NSCLC, and TCGA-RCC datasets with few-shot settings. Results are shown for AUC, F1, and Accuracy (ACC).}
\vspace{-0.05in}
\label{tab:TCGA-NSCLC-RCC} % Add label here
% \resizebox{1.0\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{\# Param.}} & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}} &  \multicolumn{3}{c}{\textbf{TCGA-RCC}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} %\cmidrule(lr){10-12}
& & \textbf{AUC} & \textbf{F1} & \textbf{ACC} & \textbf{AUC} & \textbf{F1} & \textbf{ACC} \\
\midrule
Max-pooling & 197K & 53.0$\pm$6.0 & 45.8$\pm$8.9 & 53.3$\pm$3.4 & 67.4$\pm$4.9 & 46.7$\pm$11.6 & 54.1$\pm$4.8 \\
Mean-pooling & 197K & 67.4$\pm$7.2 & 61.1$\pm$5.5 & 61.9$\pm$5.5 & 83.3$\pm$6.0 & 60.9$\pm$8.5 & 62.3$\pm$7.4 \\
ABMIL~\cite{ilse2018attention} & 461K & 60.5$\pm$15.9 & 56.8$\pm$11.8 & 61.2$\pm$6.1 & 83.6$\pm$3.1 & 64.4$\pm$4.2 & 65.7$\pm$4.7 \\
CLAM-SB~\cite{lu2021data} & 660K & 66.7$\pm$13.6 & 59.9$\pm$13.8 & 64.0$\pm$7.7 & 90.1$\pm$2.2 & 75.3$\pm$7.4 & 77.6$\pm$7.0 \\
CLAM-MB~\cite{lu2021data} & 660K & 68.8$\pm$12.5 & 60.3$\pm$11.1 & 63.0$\pm$9.3 & 90.9$\pm$4.1 & 76.2$\pm$4.4 & 78.6$\pm$4.9 \\
TransMIL~\cite{shao2021transmil} & 2.54M & 64.2$\pm$8.5 & 57.5$\pm$6.4 & 59.7$\pm$5.4 & 89.4$\pm$5.6 & 73.0$\pm$7.8 & 75.3$\pm$7.2 \\
DSMIL~\cite{li2021dual} & 462K & 67.9$\pm$8.0 & 61.0$\pm$7.0 & 61.3$\pm$7.0 & 87.6$\pm$4.5 & 71.5$\pm$6.6 & 72.8$\pm$6.4 \\
GTMIL~\cite{zheng2022graph} & N/A & 66.0$\pm$15.3 & 61.1$\pm$12.3 & 63.8$\pm$9.9 & 81.1$\pm$13.3 & 71.1$\pm$15.7 & 76.1$\pm$12.9 \\
DTMIL~\cite{zhang2022dtfd} & 986.7K & 67.5$\pm$10.3 & 57.3$\pm$11.3 & 66.6$\pm$7.5 & 90.0$\pm$4.6 & 74.4$\pm$5.3 & 76.8$\pm$5.2 \\
IBMIL~\cite{lin2023interventional} & N/A & 69.2$\pm$7.4 & 57.4$\pm$8.3 & 66.9$\pm$6.5 & 90.5$\pm$4.1 & 75.1$\pm$5.2 & 77.2$\pm$4.2 \\
ViLa-MIL~\cite{shi2024vila} & 8.8M/47M & 74.7$\pm$3.5 & 67.0$\pm$4.9 & 67.7$\pm$4.4 & 92.6$\pm$3.0 & 78.3$\pm$6.9 & 80.3$\pm$6.2 \\
CONCH (\cite{lu2024visual}) &  110M& \underline{89.46$\pm$10.2} & \underline{78.5$\pm$9.31} & \underline{78.78$\pm$9.1} & 88.08$\pm$4.59 & 78.21$\pm$4.2 & 71.67$\pm$19.4 \\
QUILT~\cite{ikezogwo2024quilt} &  63M& 79.66$\pm$13.19 & 72.30$\pm$13.35 & 72.42$\pm$13.24 & \underline{96.92$\pm$1.6} & 78.46$\pm$5.55 & \underline{86.34$\pm$1.56} \\
\rowcolor{cyan!15}\textbf{\textsc{MGPath} (CLIP)} & 1.6M/39M & 77.2$\pm$1.3 & 70.9$\pm$2.0 & 71.0$\pm$2.1  & 92.1 $\pm$ 2.8 & 76.5 $\pm$ 5.2  &  81.7 $\pm$ 2.9\\
\rowcolor{cyan!15}\textbf{\textsc{MGPath} (PLIP)} & 592K & 83.6 $\pm$ 4.5 & 76.41 $\pm$ 4.8  & 76.5 $\pm$ 4.8   & 94.7 $\pm$ 1.6 & \underline{78.6 $\pm$ 4.9} & 83.6 $\pm$ 3.5 \\
\rowcolor{cyan!15}\textbf{\textsc{MGPath} (PLIP-G)} & 5.35M & \textbf{ 93.02$\pm$2.99 }& \textbf{ 84.64$\pm$4.75 }  & \textbf{84.77$\pm$4.67 } & \textbf{98.2$\pm$0.31} & \textbf{88.33$\pm$3.41}  & \textbf{91.72$\pm$1.74} \\
\bottomrule
\end{tabular}
\vspace{-0.15in}
% }%
% \end{minipage}\quad
\end{table}
\begin{table}[h]
\centering
% \hlc[cyan!15]{Zero-shot classification}
\caption{Zero-shot classification performance on TCGA-NSCLC, TCGA-RCC, and TCGA-BRCA datasets. Metrics reported include balanced accuracy (B-Acc) and weighted F1-score (W-F1). }
\label{tab:zero_shot}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Zero-shot}} &
  \multicolumn{2}{c}{\textbf{TCGA-NSCLC}} &
  \multicolumn{2}{c}{\textbf{TCGA-RCC}} &
  \multicolumn{2}{c}{\textbf{TCGA-BRCA}} &
  \multicolumn{2}{c}{\textbf{Average}} \\ \cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9}   
                         & B-Acc & W-F1   & B-Acc   & W-F1   & B-Acc & W-F1   & B-Acc    & W-F1    \\ \midrule
\textbf{QuiltNet}        & 61.3        & 56.1          & 59.1          & 51.8          & 51.3        & 40.1          & 57.23          & 49.33          \\
\textbf{CONCH}           & \textbf{80.0} & \textbf{79.8} & 72.9          & 69.1          & 64.0          & 61.2          & 72.3           & 70.03          \\
\textbf{PLIP}            & 70.0          & 68.5          & 50.7          & 46.0            & 64.7        & 63.8          & 61.8           & 59.43          \\
\midrule
\textbf{PLIP-G (Our)} & 72.7        & 72.6          & \textbf{81.3} & \textbf{81.4} & \textbf{70.0} & \textbf{69.9} & \textbf{74.67} & \textbf{74.63} \\ \bottomrule
\end{tabular}%
}
\end{table}
\vspace{-0.1in}
\paragraph{GigaPath-PLIP achieves competitive performance in zero-shot tasks.}
We evaluate the zero-shot capabilities of our model on three datasets and compare its performance against foundation VLMs such as CONCH, QUILT, and PLIP. The results, summarized in Table \ref{tab:zero_shot}, show that the proposed VLM model achieves the best average performance across datasets, followed by CONCH and PLIP. This consistent top-tier performance across multiple benchmarks underscores the robustness and generalizability of our model.   






% \begin{wraptable}{l}{0.8\textwidth} 
% \begin{table}
%     \caption{Ablation studies on  multi-granular(M-Gran),\\ ratio combines two attention levels ($\alpha$), and message\\ passing network types.}
% \label{tab:granular}
% % \resizebox{1.\columnwidth}{!}{%
% \begin{tabular}{lccc} 
% \toprule
% \multirow{2}{*}{\textbf{Configurations}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
% \cline{1-4}
% \rowcolor{gray!15}\textsc{MGPath} (CLIP) & 76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
% \textsc{ - } w/o M-Gran (CLIP) & 74.6$\pm$2.2 & 67.8$\pm$2.4 &  67.8$\pm$2.5 \\
% \rowcolor{gray!15}\textsc{MGPath} (PLIP-G)  & 91.7$\pm$3.6 & 84.2$\pm$4.6 & 84.4$\pm$4.5 \\
% \textsc{-} w/o M-Gran (PLIP-G) & 90.6$\pm$4.5 & 82.4$\pm$5.7 & 82.5$\pm$5.7 \\
% \cline{1-4}
% \rowcolor{gray!15}\textsc{MGPath}, $\alpha = 0.2$ & 76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
% \hspace{0.05in}\textsc{-} $\alpha = 0.5$ & 73.7$\pm$3.1 & 67.4$\pm$2.6 & 67.8$\pm$2.7  \\
% \hspace{0.05in}\textsc{-} $\alpha = 0.8$ & 72.2$\pm$5.2 & 66.4$\pm$5.5 & 66.8$\pm$5.2  \\
% \midrule
% \multirow{2}{*}{}  & \multicolumn{3}{c}{\textbf{TCGA-RCC}}        \\ 
% % \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
% \cline{1-4}
% \rowcolor{gray!15}\textsc{MGPath} (CLIP) & 92.1$\pm$2.8 & 76.5$\pm$5.2  &  81.7$\pm$2.9  \\
% \textsc{-} w/o M-Gran (CLIP) & 91.6$\pm$3.5 & 72.3$\pm$6.4 & 80.2$\pm$4.4  \\
% \rowcolor{gray!15}\textsc{MGPath} (PLIP-G)  & 98.1$\pm$0.6 & 85.7$\pm$1.1  & 89.9$\pm$2.0 \\
% \textsc{-} w/o M-Gran (PLIP-G) & 98.1$\pm$0.6 & 85.0$\pm$4.0 & 89.3$\pm$3.0 \\
% \bottomrule
% \end{tabular}
% % }
% % \end{minipage}
% \end{table}


% \begin{table*}
%     \centering
%     \caption{Ablation studies on multi-granular (M-Gran), ratio combines two attention levels ($\alpha$), and message passing network types.}
%     \label{tab:granular}
%     \centering
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccc|cc} 
%     \toprule
%     \textbf{Dataset} &  \textbf{Metric} & \makecell{\textbf{MGPATH (CLIP)} \\ $\alpha = 0.2$} & \makecell{\textbf{MGPATH (CLIP)} \\ w/o M-Gran} & \makecell{\textbf{MGPath (PLIP-G)} \\ $\alpha = 0.2$} & \makecell{\textbf{MGPATH (PLIP-G)} \\ w/o M-Gran }  & \makecell{ \textbf{MGPATH (CLIP)} \\ $\alpha = 0.5$} & \makecell{ \textbf{MGPATH (CLIP)} \\ $\alpha = 0.8$} \\ 
%     \midrule
%     \multirow{3}{*}{\textbf{TCGA-NSCLC}} 
%     & AUC  & 76.2$\pm$2.2 & 74.6$\pm$2.2 & 91.7$\pm$3.6 & 90.6$\pm$4.5 & 73.7$\pm$3.1 & 72.2$\pm$5.2 \\
%     & F1   & 69.0$\pm$3.5 & 67.8$\pm$2.4 & 84.2$\pm$4.6 & 82.4$\pm$5.7 & 67.4$\pm$2.6 & 66.4$\pm$5.5 \\
%     & ACC  & 69.3$\pm$2.8 & 67.8$\pm$2.5 & 84.4$\pm$4.5 & 82.5$\pm$5.7 & 67.8$\pm$2.7 & 66.8$\pm$5.2 \\
%     \midrule
%     \multirow{3}{*}{\textbf{TCGA-RCC}}  
%     & AUC  & 92.1$\pm$2.8 & 91.6$\pm$3.5 & 98.1$\pm$0.6 & 98.1$\pm$0.6 &  - & - \\
%     & F1   & 76.5$\pm$5.2 & 72.3$\pm$6.4 & 85.7$\pm$1.1 & 85.0$\pm$4.0 &  - & - \\
%     & ACC  & 81.7$\pm$2.9 & 80.2$\pm$4.4 & 89.9$\pm$2.0 & 89.3$\pm$3.0 &  - & - \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table*}

\subsection{Ablation Studies} 
\paragraph{PLIP enhanced Prov-GigaPath.}
We validate the use of \texttt{Prov-GigaPath} and \texttt{PLIP} under the following settings: (i) using full vision-language PLIP model; (ii) combining \texttt{Prov-GigaPath} with \texttt{PLIP} through the MLP layer which was pre-trained on the large-scale dataset; (iii) integrating \texttt{Prov-GigaPath} with \texttt{PLIP} through adaptor layers which were randomly initialized; (iv) utilizing \texttt{Prov-GigaPath} and an adaptor layer to map to the class output and only train MLP and last FFN layer of slide encoder; (v) only using \texttt{Prov-GigaPath} and an MLP layer to map to the class output and train MLP, the query matrix of last layer and the last FFN layer of slide encoder. Table \ref{tab:gigapath} shows that using \texttt{Prov-GigaPath} combined with \texttt{PLIP} boosts the final performance compared to only using \texttt{PLIP} or \texttt{Prov-GigaPath}. 

\begin{table}[H]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\begin{minipage}{.47\hsize} % <-- scale here
\caption{Ablation studies on \hlc[cyan!15]{multi-granular} (M-Gran), ratio combines two attention levels ($\alpha$), and message passing network types.}
% \vspace{-3mm}
\label{tab:TCGA-NSCLC-RCC} % Add label here
\resizebox{0.99\columnwidth}{!}{

\begin{tabular}{lccc} 
\toprule
\multirow{2}{*}{\textbf{Configurations}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
\cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-4}
\rowcolor{gray!15}\textsc{MGPath} (CLIP) & 76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
\textsc{ - } w/o M-Gran (CLIP) & 74.6$\pm$2.2 & 67.8$\pm$2.4 &  67.8$\pm$2.5 \\
\rowcolor{gray!15}\textsc{MGPath} (PLIP-G)  & 91.7$\pm$3.6 & 84.2$\pm$4.6 & 84.4$\pm$4.5 \\
\textsc{-} w/o M-Gran (PLIP-G) & 90.6$\pm$4.5 & 82.4$\pm$5.7 & 82.5$\pm$5.7 \\
\cline{1-4}
\rowcolor{gray!15}\textsc{MGPath}, $\alpha = 0.2$ & 76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
\hspace{0.05in}\textsc{-} $\alpha = 0.5$ & 73.7$\pm$3.1 & 67.4$\pm$2.6 & 67.8$\pm$2.7  \\
\hspace{0.05in}\textsc{-} $\alpha = 0.8$ & 72.2$\pm$5.2 & 66.4$\pm$5.5 & 66.8$\pm$5.2  \\
\midrule
\multirow{2}{*}{}  & \multicolumn{3}{c}{\textbf{TCGA-RCC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-4}
\rowcolor{gray!15}\textsc{MGPath} (CLIP) & 92.1$\pm$2.8 & 76.5$\pm$5.2  &  81.7$\pm$2.9  \\
\textsc{-} w/o M-Gran (CLIP) & 91.6$\pm$3.5 & 72.3$\pm$6.4 & 80.2$\pm$4.4  \\
\rowcolor{gray!15}\textsc{MGPath} (PLIP-G)  & 98.1$\pm$0.6 & 85.7$\pm$1.1  & 89.9$\pm$2.0 \\
\textsc{-} w/o M-Gran (PLIP-G) & 98.1$\pm$0.6 & 85.0$\pm$4.0 & 89.3$\pm$3.0 \\
\bottomrule
\end{tabular}}%
\end{minipage}\quad
\begin{minipage}{.48\hsize} % <-- scale here
\caption{Ablation studies on adaptor learning for \hlc[cyan!15]{Prov-GiGaPath} and \hlc[gray!15]{PLIP}. PLIP-G denotes for mixed version between Prov-GiGaPath and PLIP.}
\label{tab:granular}
\vspace{-3mm}
\resizebox{0.99\columnwidth}{!}{%
\begin{tabular}{lcccc} 
\toprule
\multirow{2}{*}{\textbf{Methods}} &\multirow{2}{*}{\textbf{\# Param.}} & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
\cline{3-5}  & & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-5}
 \textsc{MGPath} (PLIP) & 592K& 83.6$\pm$4.5 & 76.41$\pm$4.8   & 76.5$\pm$4.8  \\
\rowcolor{gray!15} \textsc{MGPath} (PLIP-G) & 5.35M&  91.7$\pm$3.6& 84.2$\pm$4.6  & 84.4$\pm$4.5 \\
\textsc{MGPath} Random Adaptors & 5.35M& 91.4$\pm$4.2 & 82.8$\pm$5.7 & 83.0$\pm$5.6 \\
\textsc{GiGaPath} Tuning (MLP + last FFN) & 4.7M & 62.7$\pm$3.5 & 64.66$\pm$5.3 & 52.8$\pm$3.4 \\
 \textsc{GiGaPath} Tuning (MLP + last Q-ViT)  & 5.8M & 83.1$\pm$6.9 & 74.3$\pm$7.5 & 75.8$\pm$6.1 \\
 % \rowcolor{cyan!15}
\bottomrule
\end{tabular}
}
\vspace{2mm}
\caption{\hlc[cyan!15]{Contribution of OT} and multiple descriptive text prompts}
\label{tab:OT}
\vspace{-3mm}
\resizebox{0.99\columnwidth}{!}{%
\begin{tabular}{lccc} 
\toprule
\multirow{2}{*}{\textbf{Methods}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
\cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-4}
% \rowcolor{cyan!15}
\textsc{MGPath} (OT, 4 text prompts) &  76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
\rowcolor{gray!15} \textsc{MGPath} (OT, 2 text prompts) & 77.2$\pm$1.3 & 70.9$\pm$2.0 & 71.0$\pm$2.1  \\
% \textsc{MGPath} (Cosine, 4 text prompts) & 76.6$\pm$1.1 & 69.5$\pm$1.4 & 69.7$\pm$1.5 \\
\textsc{MGPath} (Cosine, 2 text prompts) & 75.8$\pm$3.7 & 68.3$\pm$4.5 & 68.4$\pm$4.5 \\
\midrule
\multirow{2}{*}{}  & \multicolumn{3}{c}{\textbf{TCGA-RCC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-4}
% \rowcolor{cyan!15}
\rowcolor{gray!15} \textsc{MGPath} (OT, 4 text prompts) &  92.1$\pm$2.8 & 76.5$\pm$5.2  &  81.7$\pm$2.9  \\
\textsc{MGPath} (OT, 2 text prompts) & 92.1$\pm$2.6 & 75.6$\pm$3.9 & 80.4$\pm$2.4  \\
\textsc{MGPath} (Cosine, 4 text prompts) & 91.8$\pm$2.8 & 75.9$\pm$4.3 & 80.5$\pm$2.6 \\
% \textsc{MGPath} (Cosine, 2 text prompts) & 92.0$\pm$1.8 & 76.2$\pm$4.3 & 81.1$\pm$3.7 \\
\bottomrule
\end{tabular}}

\end{minipage}
\vspace{-4mm}
\end{table}

\paragraph{Multi-Granular Prompt Learning.}
In Table \ref{tab:granular}, we show the performance of \texttt{\textsc{MGPath}} with and without multi-granular (\texttt{M-Gran}) for \texttt{CLIP} (row 1 and 2) and \texttt{PLIP-G} (row 3 and 4) on \texttt{TCGA-NSCLC} dataset. It shows that using \texttt{M-Gran} improves the final performance of \texttt{\textsc{MGPath}}. This also happens on \texttt{TCGA-RCC} dataset. Table \ref{tab:granular} also shows the impact of ratio when combining attention with graph and attention no graph on \texttt{TCGA-NSCLC}. It shows that with a ratio of 0.2/0.8 (0.2 for spatial attention obtained from graph structure and 0.8 for prototype-guided attention), \texttt{\textsc{MGPath}} achieves the highest performance. 

% \begin{table}
% \centering
% \caption{Contribution of OT and multiple descriptive text prompts}
% \begin{tabular}{lccc} 
% \toprule
% \multirow{2}{*}{\textbf{Methods}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
% \cline{1-4}
% \textsc{MGPath} (OT, 4 text prompts) &  76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8  \\
% \rowcolor{gray!15} \textsc{MGPath} (OT, 2 text prompts) & 77.2$\pm$1.3 & 70.9$\pm$2.0 & 71.0$\pm$2.1  \\

% \textsc{MGPath} (Cosine, 2 text prompts) & 75.8$\pm$3.7 & 68.3$\pm$4.5 & 68.4$\pm$4.5 \\
% \midrule
% \multirow{2}{*}{}  & \multicolumn{3}{c}{\textbf{TCGA-RCC}}        \\ 
% \cline{1-4}
% \rowcolor{gray!15} \textsc{MGPath} (OT, 4 text prompts) &  92.1$\pm$2.8 & 76.5$\pm$5.2  &  81.7$\pm$2.9  \\
% \textsc{MGPath} (OT, 2 text prompts) & 92.1$\pm$2.6 & 75.6$\pm$3.9 & 80.4$\pm$2.4  \\
% \textsc{MGPath} (Cosine, 4 text prompts) & 91.8$\pm$2.8 & 75.9$\pm$4.3 & 80.5$\pm$2.6 \\

% \bottomrule
% \end{tabular}
% % }

% \label{tab:OT}
% \end{table}



% \begin{table}[h]
%     \centering
%     \caption{Ablation studies on multi-granular (M-Gran), ratio combines two attention levels ($\alpha$), and message passing network types.}
%     \label{tab:granular}
%     \centering
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccc|cc} 
%     \toprule
%     \textbf{Dataset} &  \textbf{Metric} & \makecell{\textbf{MGPATH (CLIP)} \\ $\alpha = 0.2$} & \makecell{\textbf{MGPATH (CLIP)} \\ w/o M-Gran} & \makecell{\textbf{MGPath (PLIP-G)} \\ $\alpha = 0.2$} & \makecell{\textbf{MGPATH (PLIP-G)} \\ w/o M-Gran }  & \makecell{ \textbf{MGPATH (CLIP)} \\ $\alpha = 0.5$} & \makecell{ \textbf{MGPATH (CLIP)} \\ $\alpha = 0.8$} \\ 
%     \midrule
%     \multirow{3}{*}{\textbf{TCGA-NSCLC}} 
%     & AUC  & 76.2$\pm$2.2 & 74.6$\pm$2.2 & 91.7$\pm$3.6 & 90.6$\pm$4.5 & 73.7$\pm$3.1 & 72.2$\pm$5.2 \\
%     & F1   & 69.0$\pm$3.5 & 67.8$\pm$2.4 & 84.2$\pm$4.6 & 82.4$\pm$5.7 & 67.4$\pm$2.6 & 66.4$\pm$5.5 \\
%     & ACC  & 69.3$\pm$2.8 & 67.8$\pm$2.5 & 84.4$\pm$4.5 & 82.5$\pm$5.7 & 67.8$\pm$2.7 & 66.8$\pm$5.2 \\
%     \midrule
%     \multirow{3}{*}{\textbf{TCGA-RCC}}  
%     & AUC  & 92.1$\pm$2.8 & 91.6$\pm$3.5 & 98.1$\pm$0.6 & 98.1$\pm$0.6 &  - & - \\
%     & F1   & 76.5$\pm$5.2 & 72.3$\pm$6.4 & 85.7$\pm$1.1 & 85.0$\pm$4.0 &  - & - \\
%     & ACC  & 81.7$\pm$2.9 & 80.2$\pm$4.4 & 89.9$\pm$2.0 & 89.3$\pm$3.0 &  - & - \\
%     \bottomrule
%     \end{tabular}
%     }
% \end{table}


% \begin{table}[h]
% \centering
% \setlength{\extrarowheight}{0pt}
% \setlength{\tabcolsep}{3.0pt}
% \renewcommand{\arraystretch}{0.8}
% \addtolength{\extrarowheight}{\aboverulesep}
% \addtolength{\extrarowheight}{\belowrulesep}
% \setlength{\aboverulesep}{0pt}
% \setlength{\belowrulesep}{0pt}
% %\hlc[cyan!15]{Prov-GiGaPath}
% % \hlc[gray!15]{PLIP}
% \caption{Ablation studies on adaptor learning for Prov-GiGaPath and PLIP. PLIP-G denotes for mixed version between Prov-GiGaPath and PLIP.}
% \label{tab:gigapath}
% % \resizebox{1.\columnwidth}{!}{%
% \begin{tabular}{lcccc} 
% \toprule
% \multirow{2}{*}{\textbf{Methods}} &\multirow{2}{*}{\textbf{\# Param.}} & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
% \cline{3-5}  & & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
% \cline{1-5}
%  \textsc{MGPath} (PLIP) & 592K& 83.6$\pm$4.5 & 76.41$\pm$4.8   & 76.5$\pm$4.8  \\
% \rowcolor{gray!15} \textsc{MGPath} (PLIP-G) & 5.35M&  91.7$\pm$3.6& 84.2$\pm$4.6  & 84.4$\pm$4.5 \\
% \textsc{MGPath} Random Adaptors & 5.35M& 91.4$\pm$4.2 & 82.8$\pm$5.7 & 83.0$\pm$5.6 \\
% \textsc{GiGaPath} Tuning (MLP + last FFN) & 4.7M & 62.7$\pm$3.5 & 64.66$\pm$5.3 & 52.8$\pm$3.4 \\
%  \textsc{GiGaPath} Tuning (MLP + last Q-ViT)  & 5.8M & 83.1$\pm$6.9 & 74.3$\pm$7.5 & 75.8$\pm$6.1 \\
%  % \rowcolor{cyan!15}
% \bottomrule
% \end{tabular}
% % }
% \end{table}


\paragraph{OT as Alignment between Contextual Prompts.}

Table \ref{tab:OT} validates the use of OT in our \texttt{\textsc{MGPath}} on \texttt{TCGA-NSCLC} and \texttt{TCGA-RCC}. We see that using OT helps to boost the performance of \texttt{\textsc{MGPath}} (rows 1 and 2) compared to the use of cosine (rows 3 and 4). It also shows that the number of prompt vectors depends on each dataset. In the appendix, we also run with another version using unbalanced optimal transport (UoT). We observe that both UoT and OT provide good alignment quality, with UoT slightly outperforming OT. However, this advantage comes at the cost of increased running time.

\subsection{Discussion}

While we demonstrate significant improvements in few-shot and zero-shot WSI classification across several settings, this paper does not explore other important challenges. For example, how can we scale the current attention mechanism to handle even larger image patches (e.g., using Flash Attention \cite{dao2022flashattention}), or extend the model from classification to tumor segmentation tasks \cite{khened2021generalized}. Additionally, the potential for extending GiGaPath to integrate with other large-scale VLM models, such as CONCH \cite{lu2024visual}, remains unexplored.





\section{Conclusion} 
High-resolution WSI is crucial for cancer diagnosis and treatment but presents challenges in data analysis. Recent VLM approaches, which utilize few-shot and weakly supervised learning, have shown promise in handling complex whole-slide pathology images with limited annotations. However, many overlook the hierarchical relationships between visual and textual embeddings, ignoring the connections between global and local pathological details or relying on non-pathology-specific pre-trained models like \texttt{CLIP}. Additionally, previous metrics lack precision in capturing fine-grained alignments between image-text pairs. To address these gaps, (i) we propose \texttt{\textsc{MGPath}}, which integrates \texttt{Prov-GigaPath} with \texttt{PLIP}, cross-aligning them with 923K domain-specific image-text pairs. (ii) Our multi-granular prompt learning approach captures hierarchical tissue details effectively, (iii) while OT-based visual-text distance ensures robustness against data augmentation perturbations. Extensive experiments on three cancer subtyping datasets demonstrate that \texttt{\textsc{MGPath}} achieves state-of-the-art results in WSI classification. We expect that this work will pave the way for combining large-scale domain-specific models with multi-granular prompt learning and optimal transport to enhance few-shot learning in pathology.




\newpage
\appendix
\begin{center}
{}\textbf{\Large{Supplement to
``MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification''}}
\end{center}

\label{sec:implement_detail}
\section{Description of Dataset Splitting}
\texttt{TCGA-BRCA}. This dataset contains 1056 whole slide images of breast invasive carcinoma.  To conduct fair experiments, we adapted training and testing slides provided by the GitHub repository of MSCPT \cite{han2024mscpt}. In the MSCPT setup, 20\% of the dataset was allocated for training, while the remaining 80\% (833 slides) served as the test set. A fixed set of 16-shot WSIs was randomly sampled from the training set. Additionally, MSCPT specified the exact training and testing slides used in its experiments. However,  there are 35 slides in which we got errors in the pre-processing steps; thus, we replaced those slides with the other ones (same number of WSI per class) downloaded from Cancer Genome Atlas (TCGA) Data Portal (GDC) \cite{TCGA}.

\noindent\texttt{TCGA-RCC} $\&$ \texttt{TCGA-NSCLC}. We adopt the same data splitting as in ViLa-MIL \cite{shi2024vila}, using 16-shot samples for training in each dataset. For testing, 192 samples were used for TCGA-RCC and 197 samples were used for TCGA-NSCLC.

\subsection{Other hyper-parameters}
For all experiments, we trained \textsc{MGPath} with the Adam optimizer with a learning rate of $9 \times 10^{-6}$ and a weight decay of $1 \times 10^{-5}$ to fine-tune all versions of \textsc{MGPath} presented in Tables ~\ref{tab:TCGA-BRCA} and ~\ref{tab:TCGA-NSCLC-RCC}. The training process was conducted for a maximum of 200 epochs, with a batch size set to 1. The best checkpoints are picked based validation performance with F1 score.

\subsection{Baseline Setups}
\texttt{TCGA-BRCA}: The baselines in \textbf{Table ~\ref{tab:TCGA-BRCA}} are sourced from the MSCPT \cite{han2024mscpt} paper, where various methods are evaluated using two backbones: Vision Transformer (ViT) ~\cite{alexey2020image} from the CLIP model (top section of Table ~\ref{tab:TCGA-BRCA}) and PLIP ~\cite{huang2023visual} (bottom section of Table ~\ref{tab:TCGA-BRCA}). In this context, we introduce three variations of \textsc{MGPath} - ViT, PLIP, and GigaPath-PLIP (abbreviated as PLIP-G), where all versions utilize frozen vision and text encoders.

\noindent\texttt{TCGA-RCC} $\&$ \texttt{TCGA-NSCLC}: The baselines in \textbf{Table ~\ref{tab:TCGA-NSCLC-RCC}} are adapted from ViLa-MIL \cite{shi2024vila} where methods employ ResNet-50 from the CLIP model as the primary backbone. We present \textsc{MGPath} results using three architectures: ResNet-50, PLIP, and PLIP-G. With ResNet-50, we follow the ViLa-MIL approach by training the text encoder and reporting performance for this setup. To assess efficiency, we provide the total parameter counts for both ViLa-MIL and \textsc{MGPath}, considering scenarios with frozen backbones and trainable text encoders. For PLIP and PLIP-G, all visual and text encoders are kept frozen.

\noindent\texttt{CONCH $\&$ QUILT}: We download the pre-trained weights of these foundation models and adapt them for zero-shot evaluation on TCGA datasets following the authors’ guidelines from ~\cite{lu2024visual}, which randomly sample 75 samples for each class. For few-shot settings, since official implementations are not provided, we initialize the models with their pre-trained weights and allow fully fine-tune the text encoder and evaluate on the same subsets that we use for other baselines. While \texttt{CONCH} provides prompts for the datasets in its publication, \texttt{QUILT} does not. Therefore, we fine-tune the model using CONCH’s prompts and our own generated prompts for QUILT.


\begin{table}[H]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{2pt}
\begin{minipage}{.48\hsize} % <-- scale here
\caption{Comparison of message passing algorithms in \textsc{MGPath}, including GAT-CONV, Graph Isomorphism Network (GIN), and Graph Convolutional Network (GCN). Performance is evaluated on the TCGA-NSCLC dataset using 5-fold cross-validation.}
% \vspace{-3mm}
\label{tab:gin_standard_attention} % Add label here
\resizebox{0.99\columnwidth}{!}{
\begin{tabular}{lccc} 
\toprule
\multirow{2}{*}{\textbf{Configurations}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
\cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-4}
\rowcolor{gray!15}\textsc{MGPath} (GAT CONV) & 77.2$\pm$1.3 & 70.9$\pm$2.0 & 71.0$\pm$2.1  \\
\textsc{MGPath} (GIN)  & 77.1$\pm$2.9 & 69.8$\pm$3.9 & 69.9$\pm$4.0 \\
\textsc{MGPath} (GCN)  & 75.1$\pm$2.9 & 67.6$\pm$2.5 & 67.1$\pm$2.8 \\
\bottomrule
\end{tabular}}%
\end{minipage}\quad
\begin{minipage}{.45\hsize} % <-- scale here
 \captionsetup{type=figure}
\includegraphics[width=0.99\linewidth]{figures/plip_gigapath_curves.png}
\caption{AUC performance comparison over epochs for PLIP (blue) and PLIP combined with GigaPath (red). GigaPath significantly enhances the AUC, achieving more stable and higher values, particularly in the early epochs.}
   \label{fig:onecol}
\end{minipage}
\vspace{-4mm}
\end{table}

\section{Impact of PLIP enhanced Prov-GigaPath}
% \vspace{-0.2in}
%  \begin{figure}[H]
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    % \includegraphics[width=0.95\linewidth]{OTP.png}
%    \includegraphics[width=0.6\linewidth]{figures/plip_gigapath_curves.png}
%    \caption{AUC performance comparison over epochs for PLIP (blue) and PLIP combined with GigaPath (red). GigaPath significantly enhances the AUC, achieving more stable and higher values, particularly in the early epochs.}
%    \label{fig:onecol}
% \end{figure}
% \vspace{-0.2in}
Figure \ref{fig:onecol} presents the AUC curves for three randomly selected folds, illustrating the impact of \texttt{Prov-GigaPath} on model performance. The results show that integrating \texttt{Prov-GigaPath} leads to consistently higher AUC values across all folds, demonstrating its effectiveness in enhancing the proposed model. Notably, the improvements are most pronounced during the early training epochs, where the model converges faster and achieves more stable performance compared to the baseline. This suggests that \texttt{Prov-GigaPath} facilitates better feature extraction and generalization, ultimately leading to a more robust model.


\section{Ablation Study on Message Passing Networks}
\label{sec:ablation_gnn}
\vspace{-0.05in}
In Table~\ref{tab:gin_standard_attention}, we evaluate the performance of \textsc{MGPath} (CLIP) using the Graph Attention Network (GAT-CONV) against alternatives like the Graph Isomorphism Network (GIN) \cite{gin} and the Graph Convolutional Network (GCN) \cite{kipf2016semi}. The results show that \textsc{MGPath} (GIN) achieves comparable performance to \textsc{MGPath} (GAT-CONV), however, with higher variance. In contrast, \textsc{MGPath} (GAT-CONV) significantly outperforms the GCN-based version, likely due to GAT’s ability to dynamically assign attention weights to neighboring image patches, enabling it to prioritize the most relevant neighbors for each node.


% \begin{table}[!hbt]
% \centering
% \caption{Comparison of message passing algorithms in \textsc{MGPath}, including GAT-CONV, Graph Isomorphism Network (GIN), and Graph Convolutional Network (GCN). Performance is evaluated on the TCGA-NSCLC dataset using 5-fold cross-validation.}
% \resizebox{0.6\columnwidth}{!}{%
% \begin{tabular}{lccc} 
% \toprule
% \multirow{2}{*}{\textbf{Configurations}}  & \multicolumn{3}{c}{\textbf{TCGA-NSCLC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
% \cline{1-4}
% \rowcolor{gray!15}\textsc{MGPath} (GAT CONV) & 77.2$\pm$1.3 & 70.9$\pm$2.0 & 71.0$\pm$2.1  \\
% \textsc{MGPath} (GIN)  & 77.1$\pm$2.9 & 69.8$\pm$3.9 & 69.9$\pm$4.0 \\
% \textsc{MGPath} (GCN)  & 75.1$\pm$2.9 & 67.6$\pm$2.5 & 67.1$\pm$2.8 \\
% \bottomrule
% \end{tabular}}
% \label{tab:gin_standard_attention}
% \end{table}

\section{Additional Details on Optimal Transport Distance}
\label{sec:uot-ot}
\vspace{-0.05in}
The following paragraphs will provide detailed information on the implementation of (un-balanced) optimal transport (OT) \cite{villani2009optimal,peyre2019computational} and specifically the alignment of prompt-guided visual-text distances in \textsc{MGPath}.

\subsection{OT Formulation and Efficient Solver}
\label{subsec:solver_ot}
Given two set of feature embeddings $\boldsymbol{F} = \left\{\boldsymbol{f}_{i}|_{i=1}^{M}\right\} \in \mathbb{R}^{M \times d }$ and $\boldsymbol{G} = \left\{\boldsymbol{g}_{j}|_{j=1}^{N}\right\} \in \mathbb{R}^{N \times d }$, we can represent them as two discrete distributions $\bs{\mu}$ and $\bs{\nu}$ by:

\begin{equation}
\bs{\mu} = \sum^{M}_{i=1} p_{i} \delta_{\boldsymbol{f}_{i}}, \quad \bs{\nu} = \sum^{N}_{j=1} q_{j} \delta_{\boldsymbol{g}_{j}}, 
\end{equation}
where $\delta_{\boldsymbol{f}_{i}}$ and $\delta_{\boldsymbol{g}_{j}}$ represent Dirac delta functions centered at $\boldsymbol{F}$ and $\boldsymbol{G}$, respectively and the weights are elements of the marginal
$\boldsymbol{p} = \{p_i\}^M_{i=1}$ and $\boldsymbol{q} = \{q_i\}^{N}_{j=1}$ and can be
selected as the uniform weight with $\sum_{i=1}^{M} p_i = 1$, $\sum_{j=1}^{N} q_j = 1$.


Then we can compute the distance between $\boldsymbol{F}$ and $\boldsymbol{G}$ through $\bs{\mu}$ and $\bs{\nu}$ (Eq.(9)) as 

\begin{equation}
    d_{\mathrm{OT}}(\bs{\mu}, \bs{\nu}) = \langle \bs{T}^{\ast}, \bs{C} \rangle.
    \label{eq:OT-distance2}
\end{equation}
where
\vspace{-0.2in}
\begin{eqnarray}
\bs{T}^{\ast} = \underset{\bs{T}\in \mathbb{R}^{MXN}}{\arg{\min}} \sum^{M}_{i=1}\sum^{N}_{j=1}\bs{T}_{ij} \bs{C}_{ij} \nonumber \\ \textrm{s.t.} \quad \bs{T}\bs{1}^{N} = \bs{\mu}, \quad \bs{T}^{\top}\bs{1}^{M} = \bs{\nu} .
\label{eq:DOT}
\end{eqnarray}
with $\bs{C} \in \mathbb{R}^{M \times N}$ is the cost matrix which measures the distance between $\boldsymbol{f}_i \in \bs{\mu}$ and $\boldsymbol{g}_j \in \bs{\nu}$.

Because directly solving Eq\,\eqref{eq:DOT} is high-computational costs ($O(n^3\log n)$ with $n$ proportional to $M$ and $N$), Sinkhorn algorithm \cite{cuturi2013sinkhorn} is proposed to approximate solution by solving a regularized problem:
\vspace{-0.2in}
\begin{eqnarray}
\bs{T}^{\ast} = \underset{\bs{T}\in \mathbb{R}^{MXN}}{\arg{\min}} \sum^{M}_{i=1}\sum^{N}_{j=1}\bs{T}_{ij}\bs{C}_{ij} - \lambda H(\bs{T}) \nonumber \\ \textrm{s.t.} \quad \bs{T}\bs{1}^{N} = \bs{\mu}, \quad \bs{T}^{\top}\bs{1}^{M} = \bs{\nu} .
\label{eq:Sinkhorn}
\end{eqnarray}
where $H(\bs{T})$ = $\sum_{ij} \bs{T}_{ij} \log \bs{T}_{ij}$ be an entropy function and $\lambda > 0$ is the regularization parameter.
The optimization problem in Eq.\, \eqref{eq:Sinkhorn} is strictly convex, allowing us to achieve a solution efficiently with fewer iterations as outlined below: %a few iterations.
\begin{eqnarray}
\bs{T}^{\ast} = \text{diag}(\bs{a}^{t})\exp(-\bold{C}/\lambda)\text{diag}(\bs{b}^{t})
\label{Sinkhorn2}
\end{eqnarray}
where $t$ is the iteration and $\bs{a}^t = \bs{\mu}/ \exp(-\bold{C}/\lambda)\bs{b}^{t-1}$ and $\bs{b}^{t} = \bs{\nu}/\exp(-\bold{C}/\lambda)\bs{a}^{t}$, with the initialization on $\bs{b}^{0}=\bs{1}$. In our experiments, we used $t = 100$ and $\lambda = 0.1$ based on validation performance.

\subsection{Relaxed Marginal Constraints with Unbalanced Optimal Transport}
\label{sec:uot}
Due to strict marginal constraints in Eq\,\eqref{eq:DOT}, optimal transport may be unrealistic in real-world scenarios where data distributions are noisy, incomplete, or unbalanced. The Unbalanced Optimal Transport (UoT) \cite{chizat2018scaling,liero2018optimal} addresses this challenge by relaxing the marginal constraints, allowing for partial matching through penalties on mass creation or destruction. In particular, UoT solves
\begin{eqnarray}
\bs{T}^{\ast} = \underset{\bs{T}\in \mathbb{R}^{MXN}}{\arg{\min}} \sum^{M}_{i=1}\sum^{N}_{j=1}\bs{T}_{ij}\bs{C}_{ij} - \lambda H(\bs{T})  \\ + \ \rho_{1}\mathrm{KL}(\bs{T}\bs{1}^{N}||\bs{\mu}) + \ \rho_{1}\mathrm{KL}(\bs{T}^{\top}\bs{1}^{M}||\bs{\nu}) \nonumber
\label{eq:Sinkhorn-uot}
\end{eqnarray}

here, $\rho_{1}$ and $\rho_{2}$ represent the marginal regularization parameters, and $\mathrm{KL}(\boldsymbol{P}||\boldsymbol{Q})$ denotes the Kullback-Leibler divergence between two positive vectors. Similar to the classical OT formulation, there are solvers based on the Sinkhorn algorithm that can address Eq.\,\eqref{eq:Sinkhorn-uot} \cite{pham2020unbalanced}. However, these solvers typically require more iteration steps to converge to optimal solutions due to the added complexity introduced by the relaxed marginal constraints.


\section{Unbalance Optimal Transport (UoT)}
\begin{table}[!hbt]
\centering
\setlength{\extrarowheight}{0pt}
\setlength{\tabcolsep}{3.pt}
\renewcommand{\arraystretch}{0.8}
\addtolength{\extrarowheight}{\aboverulesep}
\addtolength{\extrarowheight}{\belowrulesep}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\caption{\textsc{MGPath} performance and running time (in second) comparison between OT and UoT.}
\label{tab:UOT-OT}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{lcccc} 
\toprule
\multirow{2}{*}{\textbf{Methods}}  & \multicolumn{4}{c}{\textbf{TCGA-NSCLC}}        \\ 
\cline{2-5}  & \textbf{AUC}  $\uparrow$      & \textbf{F1}  $\uparrow $       & \textbf{ACC}  $\uparrow$   &\textbf{Time (s) $\downarrow$}    \\ 
\cline{1-5}
% \rowcolor{cyan!15}
\textsc{MGPath} (OT, 4 text prompts) &  76.2$\pm$2.2& 69.0$\pm$3.5 & 69.3$\pm$2.8 & 1482\\
\rowcolor{gray!15} \textsc{MGPath} (UoT, 4 text prompts) & 77.0$\pm$1.8 & 70.2$\pm 3.4$ & 70.4$\pm 3.3$ & 3260\\
% \textsc{MGPath} (Cosine, 4 text prompts) & 76.6$\pm$1.1 & 69.5$\pm$1.4 & 69.7$\pm$1.5 \\
% \textsc{MGPath} (Cosine, 2 text prompts) & 75.8$\pm$3.7 & 68.3$\pm$4.5 & 68.4$\pm$4.5 \\
\midrule
\multirow{2}{*}{}  & \multicolumn{4}{c}{\textbf{TCGA-RCC}}        \\ 
% \cline{2-4}  & \textbf{AUC}        & \textbf{F1}         & \textbf{ACC}         \\ 
\cline{1-5}
% \rowcolor{cyan!15}
\rowcolor{gray!15} \textsc{MGPath} (OT, 4 text prompts) &  92.1$\pm$2.8 & 76.5$\pm$5.2  &  81.7$\pm$2.9  & 1451\\
\textsc{MGPath} (UoT, 4 text prompts) & 92.8$\pm$2.4 & 76.8$\pm$4.7 & 82.4 $\pm$ 2.4  & 3049\\
\bottomrule
\end{tabular}}
\end{table}
To conduct a comparative evaluation of the performance of \textsc{MGPath} using optimal transport versus unbalanced optimal transport (Section \ref{sec:uot}) given the more flexible constraints in UoT, we conducted an additional experiment. To be specific, we test on the TCGA-NSCLC and TCGA-RCC datasets with the CLIP architecture (ResNet-50) using a 4-text-prompt setting. Table ~\ref{tab:UOT-OT} presents our findings where the running time is computed as seconds of average across five-folds. The results show that UoT outperforms OT with an approximate 1\% improvement across all metrics. However, UoT 
is approximately 2 times slower than OT. This increase is attributed to the added flexibility and complexity introduced by relaxing the marginal constraints in the UoT formulation. Given this trade-off, we choose OT as the main distance in \textsc{MGPath} and leave the UoT version for further evaluation. It is also important to know that our OT formulation leverages approximate solutions through the regularized formulation (Eq.,\eqref{eq:Sinkhorn}) and produces smoothed optimal mappings $\bs{T}^{\ast}$, which can implicitly help the model adapt to perturbations like UoT.

\clearpage

\bibliography{references}
\bibliographystyle{abbrv}
\end{document}




