
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:exps}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DATASETS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t]
  \caption{Characteristics of the datasets: size, smallest and largest distances, and number of unique distances. The total number of pairwise distances is $|U|(|U|-1)$.}
  \label{tab:datasets}
  \begin{tabular}{@{}lrrrrr@{}}
    \toprule
    Data  & type & $|U|$ & $\min d$ & $\max d$ & dists \\
    \midrule
    \emph{ft70} \cite{tsplib}  & infra & 70 & 331 & 2\,588 & 1\,441 \\
    \emph{kro124} \cite{tsplib} & infra & 100 & 81 & 4\,309 & 3\,297 \\
    \emph{celegans} \cite{kunegis2013konect} & bio & 297 & 1 & 24 & 24 \\
    \emph{rbg323} \cite{tsplib} & infra & 323 & 0 & 21 & 23 \\
    \emph{wiki-vote} \cite{snapnets} & social & 1\,300 & 1 & 9 & 9 \\
    \emph{US airports} \cite{toreopsal,kunegis2013konect} & flight & 1\,402 & 1 & 169\,685 & 27\,237 \\
    \emph{moreno} \cite{moody2001peer,kunegis2013konect} & social & 2\,155 & 1 & 52 & 52 \\
    \emph{openflights} \cite{toreopsal,kunegis2013konect} & flight & 2\,868 & 1 & 17 & 17 \\
    \emph{cora} \cite{vsubelj2013model,kunegis2013konect} & citation & 3\,991 & 1 & 45 & 45 \\
    \emph{bitcoin} \cite{kumar2016edge,kumar2018rev2} & trust & 4\,709 & 0 & 134 & 130 \\
    \emph{gnutella} \cite{snapnets} & P2P & 5\,153 & 1 & 21 & 21 \\
  \bottomrule
\end{tabular}
\end{table}


\begin{table}[] \centering
\caption{Performance comparison between our approximation algorithms (\textbf{\algbac{}}, \textbf{\algbacb{}}, and \textbf{\algbacf{}}), the greedy algorithm (\textbf{Greedy}), and the random baseline (\textbf{Rand.}), relative to the optimum solution value (Opt.) for fixed parameter value $k=10$.}
\label{tab:performsmallk}
\tcbset{colframe=black!5, colback=black!5, size=fbox, on line}
\setlength{\tabcolsep}{0pt}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lrrrrrr@{}}
\toprule
Data & \algbac{} & \algbacb{} & \algbacf{} & Greedy & Rand. & Opt. \\
\midrule
\emph{ft70} & 95\% & \tcbox{98\%} & 95\% & 95\% & 63\% & 786 \\
\emph{kro124p} & 88\% & \tcbox{89\%} & 88\% & 88\% & 45\% & 1 136 \\
\emph{celegans} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 29\% & 7 \\
\emph{rbg323} & 80\% & \tcbox{100\%} & 80\% & 80\% & 0\% & 15 \\
\emph{wiki-vote} & \tcbox{80\%} & \tcbox{80\%} & \tcbox{80\%} & \tcbox{80\%} & 40\% & 5 \\
\emph{US airports} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 0\% & 64 036 \\
\emph{moreno} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 27\% & 22 \\
\emph{openflights} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 33\% & 9 \\
\emph{cora} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 19\% & 27 \\
\emph{bitcoin} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & \tcbox{100\%} & 29\% & 77 \\
\emph{gnutella} & 85\% & \tcbox{92\%} & 85\% & 85\% & 38\% & 13 \\
\midrule
Average & 93.4\% & \tcbox{96.3\%} & 93.4\% & 93.4\% & 29.3\% & 100\% \\
\bottomrule
\end{tabular*}
\end{table}


\begin{figure*}[th!]

\centering
%\input{performance-fig}
\subfloat[\emph{ft70} dataset]{\includegraphics{performance_a.pdf}}
\subfloat[\emph{kro124} dataset]{\includegraphics{performance_b.pdf}}
\subfloat[\emph{rbg323} dataset]{\includegraphics{performance_c.pdf}}
\caption{The y-axis shows the diversity score of the solutions provided by the algorithms. The x-axis shows the parameter $k$, which is the required solution size. All solutions converge to the same set as $k$ approaches $n$.  \label{fig:experimentsk}}
\end{figure*}

Next we describe our experiments.
All experiments were performed on an Intel\,Core\,i5-8265U processor at~1.6 GHz with 16\,GB\,RAM. 
Our methods were implemented in Python~3.8 and are publicly available.\footnote{\url{https://version.helsinki.fi/dacs/ammd}}


\subsection{Setup}

\ptitle{Data}
For data, we used weighted digraphs that we converted into an asymmetric distance space by computing the metric closure, that is, we define the distance $d(u, v)$ to be 
the weighted shortest path length between $u$ and $v$. We only used the largest strongly connected component.
Table~\ref{tab:datasets} shows the data and several statistics used for evaluation.
The column $\min d$ (resp. $\max d$) denotes the smallest (resp.
largest) distance present. The last column shows the
number of unique distances, which heavily
influences the running time of our algorithms from Section~\ref{sec:approx}.
The datasets \emph{ft70}, \emph{kro124} and \emph{rgb323} are asymmetric traveling salesman instances from the public library TSPLIB \cite{tsplib}. The \emph{bitcoin} dataset originally has edge weights between $[-10,10]$, which we have rescaled to nonnegative weights between $[0,20]$.

\ptitle{Baselines} Besides our proposed algorithms \algbac{},
\algbacb{}, and \algbacf{}, we have also implemented
and tested the following baselines: 

\pttitle{Greedy} Iteratively pick vertices maximizing the $\dmin$ distance towards the already chosen set of vertices as in Algorithm~\ref{algo:greedydmin}. To improve the performance, we start with a vertex of an edge with the maximum $\dmin$ distance.

\pttitle{Random} Select a subset $S \subseteq U$ of size $|S|=k$ uniformly at random. Repeat 10 times and return the set with the highest score $\diver{S}$.

\pttitle{Optimal} Computes an optimal solution by reducing it to solving multiple $k$-clique problems. First, sort all the unique weights in the AMMD instance.
The optimum $R^*$ is equal to one of the unique weights.
Let $R$ be one of the unique weights. Create an undirected graph $G_R = (U, E)$ with the same nodes as our AMMD instance, and $ij \in E$ if and only if both $d(i,j) \geq R$ and $d(j, i) \geq R$.
For every $R \leq R^*$ the graph $G_R$ will have a clique of size $k$, and for every $R>R^*$ the graph $G_R$ does not contain a clique of size $k$, by the optimality of $R^*$.
A binary search on the sorted list of unique weights finds the optimum $R^*$, by solving at most $\bigO(\log n)$ $k$-clique problems.
Note that the $k$-clique problem can be solved optimally in $\bigO(n^{\omega k/3})$ time \cite{nevsetvril1985complexity}. 
A similar idea has been used to optimally solve symmetric MMD and related fairness variants \cite{akagi2018exact,wang2023max}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The diversity scores of the returned sets}
\label{exp:per}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ptitle{Small $k$ regime}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As mentioned in the introduction, in a typical AMMD setting one is interested in finding solutions of small size.
We compare the performance of our algorithms \algbac{}, \algbacb{} and \algbacf{} in Table~\ref{tab:performsmallk} with the aforementioned baselines Greedy, Random, and Optimal on all the datasets from Table~\ref{tab:datasets}, for the choice of $k=10$. 

Observe that \algbacb{} achieves the highest score in every experiment and finds the optimal value on seven datasets. In addition, \algbacf{} achieves the same solution quality as \algbac{} and Greedy while being significantly faster than \algbac{} and having the approximation guarantee discussed in Section~\ref{sec:speeding}. We also note that Random often performs very poorly, even with an increased number of repetitions. 

\algbacf{}, Greedy, and Random were very fast and took less than three seconds to run, whereas \algbacb{} took several minutes on some datasets. On \emph{wiki-vote} and \emph{gnutella} datasets, our baseline Optimal exceeded a time limit of two hours, but we were able to find the optimal values by manually checking that no cliques of size $k$ exist when only edges with higher weight are considered.  The running times on the real-world datasets are shown in Table~\ref{tab:running_times} in the Appendix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ptitle{Performance for any $k$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We were able to run all the algorithms for all values of $k$ on the three small traveling salesman instances of Table~\ref{tab:datasets} within a reasonable time.
Figure~\ref{fig:experimentsk} shows the results, which are consistent with our previous findings. \algbacb{} performs very well also for larger values of $k$ while \algbac{}, \algbacf{}, and Greedy are often slightly worse. Note that as $k$ increases, the output of all algorithms converges to the same set, which is the entire universe of points.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Running times of the algorithms}
\label{exp:rt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
%\input{times-fig}
\centering
\subfloat[Synthetic scale-free graphs.\label{fig:runtimesa}]{\includegraphics{runtimes_a.pdf}}
\hfill
\subfloat[Synthetic complete digraph.\label{fig:runtimesb}]{\includegraphics{runtimes_b.pdf}}
\caption{Running time of our algorithms as a function of the generated graph size and the size of the interval from which distances are sampled.
\label{fig:runtimes}}
\end{figure}

In this section, we compare the running time of our algorithms \algbac{}, \algbacb{}, and \algbacf{} on synthetically generated graph instances.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ptitle{Increasing graph size}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the first experiment, we generated directed (unweighted) scale-free networks of varying size
$n$ according to the model by~\citet{bollobas2003directed}. These synthetic
graphs are weakly connected, and we make them strongly connected by adding
directed edges in both directions. We set $k = 10$, and defined distances as the shortest path distances between pairs of
nodes (metric closure). The diameter of scale-free networks typically scales
about logarithmically with graph size~\cite{bollobas2004diameter}, so the
number of unique distances in our AMMD instances does not change that drastically when
increasing the graph size $n$. For example, when generating instances of size
$n=100$ and $n=3\,200$, the diameter roughly only doubled from 5 to 10.
Figure~\ref{fig:runtimesa} shows the results of the average running time over
several repeats. It shows that although the number of unique distances is
relatively low, the difference in speed between \algbacb{} and \algbacf{} is
still substantial. For $n=12\,000$ the running time of \algbacb{} was about
49s while \algbacf{} required less than 14s. As expected, the running time of Optimal scales exponentially while Greedy and Random are very fast.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ptitle{Unique distances}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In a second timing experiment, we kept both $n$ and $k$ fixed and increased the number of unique distances.
We generated complete weighted digraphs of size $n=400$, where the weights are random
positive integers below a variable upper bound. To guarantee the triangle
inequality, we again took the metric closure. We set $k=10$. 
Figure~\ref{fig:runtimesb} shows the results. The theoretical speed-up of
\algbacf{} when compared to \algbac{} and \algbacb{}, as discussed in
Section~\ref{sec:speeding}, is clearly visible in our implementations on
practical data as well. Note that the running times of Greedy and Random do not depend on the number of unique distances, while Optimal takes prohibitively long for these instances. 
