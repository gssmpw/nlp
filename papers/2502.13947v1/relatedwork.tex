\section{Background and Related Works}
\vspace{-3pt}
\subsection{QUBO Problem}



Equation (\ref{eq:1}) presents the general formulation of the QUBO problem. In this equation, the matrix $Q$ (i.e., weight matrix), sized $n\times n$, comprises integer values that encompass both positive and negative integers. The vector $x$, also of size $n$, consists of binary values, with each element $x_i$ being either one or zero. Thus, the objective is to determine the solution that minimizes the $x^T Q x$ problem, effectively solving the QUBO problem. Thus, the primary aim is to identify the vector $x$ that minimizes the function $f(x)$.

\vspace{-0.2cm}

\begin{equation}
\label{eq:1}
\small
f(x) = \sum_{i=1}^{n} Q_{ii} x_i + \sum_{i=1}^{n} \sum_{j \neq i}^{n} Q_{ij} x_i x_j\
\end{equation}

\vspace{-8pt}
\subsection{Ising formulation}
\vspace{-3pt}
Equation (\ref{eq:2}) shows the general Ising formulation, where $E$ represents the system's energy. $i$ and $j$ are indices for individual spins (nodes). $J_{ij}$ is a coupling constant determining the interaction strength between spins $i$ and $j$. It is typically a symmetric matrix describing the interactions between all spins. $s_i$ and $s_j$ are individual spin values, often taking values of $+1$ or $-1$. $h_i$ represents the strength of an external magnetic field acting on spin $i$. The transformation from Ising to QUBO is easy, and the $\{0,1\}$-valued variables of QUBO can be transformed to $\{-1,1\}$-valued variables of Ising formulation as shown in Equation (\ref{eq:3}). 

\vspace{-0.2cm}

\begin{equation}
\label{eq:2}
\small
E = -\sum_{i} \sum_{j} J_{ij} s_i s_j - \sum_{i} h_i s_i
\end{equation}

\vspace{-8pt}

\begin{equation}
\label{eq:3}
\small
s_i = 2 \times x_i - 1
\end{equation}

\vspace{-10pt}
\subsection{Related Work}
We review prior works on hybridizing classical processors and Ising machines to solve the QUBO problems. 
It should be stated that there are prior works on solving QUBO using only classical processors, such as \cite{ref1} and \cite{ref5}. 
In \cite{ref1}, a search algorithm denoted as D\textsuperscript{2}TS, has been proposed. This paper proposes a fast and efficient modified Tabu search while utilizing a unique perturbation-based diversification strategy. It shows promising results in solving large-size QUBO problems on a classical machine with few search epochs.

In \cite{ref2}, a subQUBO approach was presented to solve the QUBO optimization problem. This approach uses multiple Quantum annealer hardware to solve the subQUBO problems in combination with a Tabu search and a randomizer to find the solution for the QUBO. 
Another subQUBO approach for solving the Trip Planning Problems (TPP) using Ising machines has been introduced in \cite{ref3}. By partitioning the original QUBO model into smaller subQUBOs through combined variable selection, this paper finds near-optimal solutions to the QUBO problem. 

The authors of \cite{ref4} introduced a hybrid annealing method called subQUBO model extraction for solving QUBO models with Ising machines. This method successfully obtained multiple quasi-optimal solutions by leveraging a classical computer to identify and extract size-limited subQUBO models from selected solution instances based on variable variance. The iterative refinement of the solution set was a key aspect of this approach.
In \cite{ref10}, an algorithm for automatically planning touristic trips was introduced. This algorithm utilized Tabu search as a meta-heuristic and defined a specific set of operators to navigate the search space efficiently.
In \cite{ref28}, a metaheuristic is used in tandem with using the D-wave system quantum annealer to solve the QUBO problems. Finally, note that the Variational Quantum Eigensolver (VQE) method for hybrid usage of the classical and quantum machines for solving large-size problems has been proposed \cite{ref29}. While our proposed algorithm could be applied to any Ising machine platform (digital, analog, photonic, or quantum).


\vspace{-8pt}
\subsection{\textit{Tabu} search}
Tabu search is an optimization algorithm that explores solution spaces incrementally by modifying a current solution while avoiding recently explored solutions. This memory mechanism prevents the algorithm from getting stuck in local optima, making it effective for complex combinatorial optimization problems. 

The algorithm begins with a vector solution $x$ and runs for a fixed number of iterations ($\alpha$). In each iteration, it identifies the element of $x$ that, when inverted, yields the most significant improvement in the objective value, with a complexity of $O(n^2)$. To enhance efficiency, a data structure storing the 1-flip move value (change in $x_i$) for every possible move was proposed in \cite{ref1}. Additionally, an efficient method for updating this data structure after each iteration was suggested, reducing the time complexity from $O(n^2)$ to $O(n)$. To avoid local minima, Tabu search incorporates a memory mechanism. Specifically, a value flipped in one iteration within the $x$ vector cannot be flipped for the subsequent $c$ iterations unless it results in a better solution. This prevents immediate undoing of beneficial moves and promotes diversity by discouraging recent moves, aiding in escaping local optima.

Algorithm \ref{alg:Tabu} depicts the pseudo-code for the Tabu search algorithm used in this paper, based on \cite{ref1}. The algorithm initiates by computing all $\Delta x_{k}$ values, representing the potential change in the objective function if the $k$\textsuperscript{th} variable is flipped. Within the main loop, the $C$ vector dictates which elements can be flipped; a positive value in the $C$ vector indicates that the corresponding variable cannot be flipped unless it becomes negative in subsequent iterations. This serves as the memory function, and the allowable elements for flipping are stored in the $filtered\_vec$ at each iteration. The variable yielding the highest difference is selected for flipping, leading to a recalculation of the $\Delta x_{k}$ values for the next iteration. The chosen element of the $C$ vector has $c$ added to it, while all values in the $C$ vector are decremented by one. The objective value and $x_{min}$ values are then updated correspondingly.


\begin{algorithm}[t]
\caption{Tabu search}
\label{alg:Tabu}
\begin{algorithmic}[1]
\setlength{\itemsep}{0.2em} % Adjust the space between items
\setlength{\baselineskip}{0.8em} % Adjust the space between lines
\small
%\REQUIRE $Q$
%\ENSURE $x_{\text{min}}$ for $x^T Q x$ problem
\STATE OV = $x^TQx$ $\;\;\;\;\;\;$ \small \%OV: Object Value \normalsize
\vspace{-1.5pt}
\STATE $OV_{min} = OV$
\vspace{-1.5pt}
\FOR{$k$ in $1$ to $n$}
\vspace{-1.5pt}
    \STATE $\Delta x_k = (1 - 2x_k) \left(Q_{k,k} + \sum_{j \in n; j \neq k, x_j = 1} 
    Q_{j,k}\right)$
    \vspace{-1.5pt}
\ENDFOR
\vspace{-1.5pt}
\FOR{$\alpha$ iterations}
\vspace{-1.5pt}
    \FOR {each k}
    \vspace{-1.5pt}
        \IF {$C[k] <= 0$}
        \vspace{-1.5pt}
            \STATE $filtered\_vec[k] = \Delta x_k$
            \vspace{-1.5pt}
        \ELSE
        \vspace{-1.5pt}
            \STATE $filtered\_vec[k] = INF$
            \vspace{-1.5pt}
        \ENDIF
        \vspace{-1.5pt}
    \ENDFOR    
    \vspace{-1.5pt}
    \STATE $i \gets argmin(filtered\_vec)$ 
    \vspace{-1.5pt}
    \STATE $\Delta x_i \gets -\Delta x_i$
    \vspace{-1.5pt}
    \FOR{each $i \in n; j \neq i$}
    \vspace{-1.5pt}
        \IF{$x_{j} = 1$}
        \vspace{-1.5pt}
            \STATE $\Delta x_j = \Delta x_j + Q_{i,j}$
            \vspace{-1.5pt}
        \ELSE
        \vspace{-1.5pt}
            \STATE $\Delta x_j = \Delta x_j - Q_{i,j}$
            \vspace{-1.5pt}
        \ENDIF
        \vspace{-1.5pt}
    \ENDFOR
    \vspace{-1.5pt}
    \STATE $OV = OV + \Delta x_i$
    \vspace{-1.5pt}
    \STATE $C[i] += c$
    \vspace{-1.5pt}
    \STATE $x_i = 1 - x_i$
    \vspace{-1.5pt}
    \FOR {each i}
    \vspace{-1.5pt}
        \STATE $C[i]-=1 \text{ IF } C[i] > 0$
        \vspace{-1.5pt}
    \ENDFOR
    \vspace{-1.5pt}
    \IF {$OV < OV_{min}$}
    \vspace{-1.5pt}
        \STATE $x_{min} = x$
        \vspace{-1.5pt}
        \STATE $OV_{min} = OV$
        \vspace{-1.5pt}
    \ENDIF    
    \vspace{-1.5pt}
\ENDFOR
\vspace{-1.5pt}
\RETURN $x_{min}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Proposed \alg}\label{alg:alg2}
\begin{algorithmic}[1]
\small
\REQUIRE $Q$
\vspace{-1pt}
\ENSURE $x_{\text{min}}$ for $x^T Q x$ problem
\vspace{-1pt}
\STATE $S \gets$ Generate\_Initial\_Solution\_Set()
\vspace{-1pt}
\STATE $S \gets$ Call\_IM\_Solution\_Set($S$)
\vspace{-1pt}
\STATE $r \gets$ MRAnnealer.Initial()
\vspace{-1pt}
\STATE $\eta \gets$ Weight\_Effect\_Extraction()($Q$)
\vspace{-1pt}
\WHILE{Termination Condition is not met}
\vspace{-1pt}
    \STATE $S, \Delta \gets$ Tabusearch(Q, $\alpha$, $S$)
    \vspace{-1pt}
    \STATE $\gamma \gets$ \text{Deviation\_Extraction}($S$)
    \vspace{-1pt}
    \STATE $A \gets$ Control\_Parameters\_Aggregation($\eta, \Delta, \gamma$)
    \vspace{-1pt}
    \STATE $S, S' \gets$ Call\_IM\_Partial\_Solution\_Set($S, A, Q$)
    \vspace{-1pt}
    \STATE $S \gets$ Mutation($S, S', A, r$)
    \vspace{-1pt}
    \STATE $r \gets$ MRAnnealer.NextRate() 
    \vspace{-1pt}
\ENDWHILE
\vspace{-1pt}
\RETURN $x_{\text{min}}$
\vspace{-1pt}
\end{algorithmic}
\label{alg:proposed_heuristic}
\end{algorithm}

\vspace{-7pt}