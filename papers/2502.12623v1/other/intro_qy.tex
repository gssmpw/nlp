Different modalities are often interwoven. In the context of music, humans typically experience music alongside complementary textual and visual signals, such as lyrics, the composition of a piece, live performances, or the arrangement of instruments in a band. These additional modalities significantly influence how humans perceive and understand music. Therefore, it is reasonable to assume that training a model to incorporate other modalitiesâ€”such as images, videos, and textual music features can enhance its performance on music understanding tasks~\cite{DBLP:conf/ijcnn/MancoBQF21,DBLP:conf/icml/GardnerDSB24,DBLP:journals/corr/abs-2301-11325,DBLP:conf/icassp/LiuHSS24}.

In this work, we introduce the concept of multimodal music understanding, where multiple modality signals are leveraged to enhance the perception of music. We implement this concept through multi-way instruction tuning, inspired by code-switched~\cite{DBLP:conf/iclr/SongZQXCWL22} and multi-way multilingual translation~\cite{DBLP:journals/jmlr/FanBSMEGBCWCGBL21} that extend translation models beyond simple bilingual pairings. For multimodal music understanding, we establish music-centric relationships that go beyond the conventional pairing of music and text modalities commonly seen in existing music large language models (LLMs)~\cite{DBLP:conf/ismir/DohCLN23,DBLP:conf/icassp/LiuHSS24,DBLP:journals/corr/abs-2311-11255,DBLP:conf/icml/GardnerDSB24,DBLP:journals/corr/abs-2410-15573}.
% \mz{we may need a quick introduction about the music modality -- how people used to process it. otherwise I am feeling "music-focused" would be vague and confusing.}

We present \textbf{DeepResonance}, a multimodal music understanding LLM trained on music-centric multi-way data that integrates the music, text, image, and video modalities. Specifically, we construct two 4-way training datasets, \textbf{Music4way-MI2T} and \textbf{Music4way-MV2T}, where the source data comprises music, images/videos, and textual descriptions and instructions. The target data includes textual descriptions enriched with multimodal information, such as music, images/videos, and low-level music features, including tempo, chords, key, and downbeats. Using these datasets, we develop DeepResonance based on the NExT-GPT~\cite{DBLP:conf/icml/Wu0Q0C24} architecture. To enhance multimodal music understanding tasks, we propose two key modifications to the backbone model. The first involves \textbf{multi-sampled ImageBind embeddings}, which are designed to retain richer information of music, image, and video modalities from the ImageBind encoders~\cite{DBLP:conf/cvpr/GirdharELSAJM23}, thereby fostering deeper interaction with the music modality and improving multimodal music understanding. The second is a \textbf{pre-alignment Transformer}, a module aimed at pre-adapting different modalities to each other before feeding them into the text LLM module. This component is particularly effective in simultaneously handling multimodal inputs of music, text, images, and videos. These innovations collectively advance the model's ability to integrate and process diverse multimodal signals for enhanced music understanding.

We evaluate DeepResonance on three conventional music understanding tasks (i.e., music + text (instruction)$\xrightarrow{}$ text) and three multimodal music understanding tasks (i.e., music + image/video + text (instruction) $\xrightarrow{}$ text (multimodal-enriched) outputs). The former includes two existing benchmarks, MusicQA~\cite{DBLP:conf/icassp/LiuHSS24} and MusicCaps~\cite{DBLP:journals/corr/abs-2301-11325}, along with our newly constructed benchmark, Music4way-MusicCaps. These benchmarks cover both captioning and question-answering tasks for music understanding. The latter evaluation for multimodal music understanding uses the test splits of Music4way-MI2T and Music4way-MV2T, as well as the newly introduced \textbf{Music4way-Any2T} benchmark to assess the model's robustness. 

As shown in Fig.~\ref{fig:radar}, DeepResonance models with different configurations ($\alpha$ and $\beta$) consistently outperform related models across all six benchmarks in supervised settings, demonstrating the effectiveness of our proposed multi-way datasets and model architecture components for music understanding tasks. Additionally, we conduct zero-shot evaluations to assess the model's generalization to unseen music understanding datasets and perform ablation studies to evaluate the impact of each proposed component in different downstream task settings.

The contributions of this work can be summarized as follows:
\begin{itemize}
    \item We introduce the Music4way-MI2a and Music4way-MV2T datasets, enabling music, text, image, and video integration for music understanding. In addition, using the the newly curated Music4way-Any2T dataset along with out-of-domain benchmarks, we firstly assess the generalization of music LLMs to multi-way multimodal inputs in this paper. \qw{maybe specifying what music LLMs we evaluate would make it clearer for the contribution?}
    \item We propose DeepResonance, the first model applying instruction-tuning on music foundation models using multi-way data that integrates multiple modalities, shifting the paradigm from the traditional music + text $\xrightarrow{}$ text approach to the music + image/video + text $\xrightarrow{}$ text (multimodal-enriched) framework.
    \item We propose multimodal embedding multi-sampling and pre-alignment as two key adaptation on the exsiting multimodal architecture, to enhance the multimodal fusion for music LLMs. Benefit from it, our DeepResonance outperforms existing music LLMs across six benchmarks including various downstream tasks, demonstrating the effectiveness of our innovative datasets and model, which will be publicly available for the follow-up studies.
\end{itemize}
\qw{in this way, we could remove the last paragraph in the related work}
\qw{personally, I'm feeling that we don't have to specify the model name of ``next-gpt'' and ``imagebind'' in the introduction, as they're just about the architecture. To highlight our technical contribution as possible, I would recommend to use ``multi-modal foundation model'' and ``multi-modal encoders'', and specify them in the method section as an implementation choice. After all, our main contribution seems to be the first work shifting the way people train and evaluation music LLMs. I try this writing way the above contribution part and you can take a look. However, this is just a minor point, I am also totally comfortable with the current writing.}