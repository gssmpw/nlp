\section{Related Work}
\noindent \textbf{Music Understanding}
is an emerging topic that builds upon the foundational research efforts in music information retrieval (MIR), which traditionally focused on low-level music feature recognition tasks, such as identifying tempo, chords, keys, and instruments~\cite{DBLP:conf/ecir/FaraldoGJH16,DBLP:conf/ismir/PauwelsOGS19,DBLP:conf/ismir/GururaniSL19,DBLP:journals/tismir/SchreiberUM20}. Early work in this area was centered on basic tagging tasks, such as determining the genre or version of a piece of music~\cite{DBLP:conf/ismir/Tzanetakis01,DBLP:conf/icassp/WonONGS21,DBLP:journals/spm/YesilerDBTS21}. Over time, the focus shifted to high-level understanding tasks that require a more comprehensive interpretation of the content, sentiment, and insights conveyed by music. These tasks include captioning, reasoning, question answering, and tool using~\cite{DBLP:conf/ijcnn/MancoBQF21,DBLP:conf/icml/GardnerDSB24,DBLP:journals/corr/abs-2301-11325,DBLP:conf/icassp/LiuHSS24,deng-etal-2024-musilingo,DBLP:journals/corr/abs-2410-15573}.

% Our work advances the exploration of music understanding by uniquely integrating multiple modalities—image, video, and low- and high-level music features—to enhance performance on these tasks. To achieve this, we construct music-centric multi-way training datasets specifically designed for multimodal music understanding. Furthermore, we extend traditional music understanding tasks by introducing multimodal benchmarks that move beyond the conventional reliance on interpreting the music modality alone, shifting the paradigm from music + text $\xrightarrow{}$ text to music + image/video + text $\xrightarrow{}$ text (multimodal-enriched).

\noindent \textbf{Multimodal Instruction Tuning and Music Foundation Models:}
Recently, multimodal pre-training has successfully bridged image, audio, and video modalities to text LLMs~\cite{DBLP:conf/iclr/TangYSC000M024,DBLP:conf/icml/Wu0Q0C24,DBLP:conf/iclr/0001LLKG24,DBLP:conf/cvpr/TangYKLZB24} through multimodal instruction tuning~\cite{DBLP:conf/nips/LiuLWL23a,DBLP:journals/corr/abs-2307-08581} or universal multimodal embedding space encoders~\cite{DBLP:conf/cvpr/GirdharELSAJM23,DBLP:conf/iclr/ZhuLNYCWPJZLZ0024}. However, few studies have focused on the music modality. MU-LLaMA~\cite{DBLP:conf/icassp/LiuHSS24} was among the first to instruction-tune LLaMA models~\cite{DBLP:journals/corr/abs-2302-13971} for the music domain, while LLark~\cite{DBLP:conf/icml/GardnerDSB24} extended music LLMs to support a wide range of tasks, including captioning, reasoning, and low-level music feature recognition. M$^2$UGen~\cite{DBLP:journals/corr/abs-2311-11255} introduced music generation modules built on MU-LLaMA, leveraging newly constructed music-centric datasets for instruction fine-tuning. OpenMU~\cite{DBLP:journals/corr/abs-2410-15573} unified existing music understanding datasets, curating a comprehensive benchmark for evaluating music + text $\xrightarrow{}$ text tasks. Other models, such as MusCaps~\cite{DBLP:conf/ijcnn/MancoBQF21}, LP-MusicCaps~\cite{DBLP:conf/ismir/DohCLN23}, and MusiLingo~\cite{deng-etal-2024-musilingo}, were designed for task-specific purposes.

Our work differs from the previous studies on music understanding and music foundation models in three aspects: (1) We are the first to apply instruction-tuning on music foundation models using multi-way data that integrates multiple modalities, shifting the paradigm from the traditional music + text $\xrightarrow{}$ text approach to the music + image/video + text $\xrightarrow{}$ text (multimodal-enriched) framework. (2) We are the first to access the generalization of music LLMs to multi-way multimodal inputs using our newly curated Music4way-Any2T dataset and zero-shot evaluation on out-of-domain benchmarks. (3) We propose multi-sampled ImageBind embeddings and pre-alignment Transformer that significantly impact multimodal music understanding tasks, delivering state-of-the-art music LLMs optimized for different downstream tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{202502_DeepResonance_Fig2.pdf}
    \caption{\textbf{Multi-way Instruction Tuning Data Construction.} Based on AudioSet, we construct Music4way (M+T$\xrightarrow{}$T, I+T$\xrightarrow{}$T, V+T$\xrightarrow{}$T), Music4way-MI2T (M+I+T$\xrightarrow{}$T), Music4way-MV2T (M+V+T$\xrightarrow{}$T), and Music4way-Any2T (M+I/V+T$\xrightarrow{}$T) for instruction tuning and evaluation. (M: music; I: image; V: video; T: text)}
    \label{fig:data}
\end{figure*}