\section{Related Work}
\noindent \textbf{Music Understanding}
is an emerging topic that builds upon the foundational research efforts in music information retrieval (MIR), which traditionally focused on low-level music feature recognition tasks, such as identifying tempo, chords, keys, and instruments____. Early work in this area was centered on basic tagging tasks, such as determining the genre or version of a piece of music____. Over time, the focus shifted to high-level understanding tasks that require a more comprehensive interpretation of the content, sentiment, and insights conveyed by music. These tasks include captioning, reasoning, question answering, and tool using____.

% Our work advances the exploration of music understanding by uniquely integrating multiple modalities—image, video, and low- and high-level music features—to enhance performance on these tasks. To achieve this, we construct music-centric multi-way training datasets specifically designed for multimodal music understanding. Furthermore, we extend traditional music understanding tasks by introducing multimodal benchmarks that move beyond the conventional reliance on interpreting the music modality alone, shifting the paradigm from music + text $\xrightarrow{}$ text to music + image/video + text $\xrightarrow{}$ text (multimodal-enriched).

\noindent \textbf{Multimodal Instruction Tuning and Music Foundation Models:}
Recently, multimodal pre-training has successfully bridged image, audio, and video modalities to text LLMs____ through multimodal instruction tuning____ or universal multimodal embedding space encoders____. However, few studies have focused on the music modality. MU-LLaMA____ was among the first to instruction-tune LLaMA models____ for the music domain, while LLark____ extended music LLMs to support a wide range of tasks, including captioning, reasoning, and low-level music feature recognition. M$^2$UGen____ introduced music generation modules built on MU-LLaMA, leveraging newly constructed music-centric datasets for instruction fine-tuning. OpenMU____ unified existing music understanding datasets, curating a comprehensive benchmark for evaluating music + text $\xrightarrow{}$ text tasks. Other models, such as MusCaps____, LP-MusicCaps____, and MusiLingo____, were designed for task-specific purposes.

Our work differs from the previous studies on music understanding and music foundation models in three aspects: (1) We are the first to apply instruction-tuning on music foundation models using multi-way data that integrates multiple modalities, shifting the paradigm from the traditional music + text $\xrightarrow{}$ text approach to the music + image/video + text $\xrightarrow{}$ text (multimodal-enriched) framework. (2) We are the first to access the generalization of music LLMs to multi-way multimodal inputs using our newly curated Music4way-Any2T dataset and zero-shot evaluation on out-of-domain benchmarks. (3) We propose multi-sampled ImageBind embeddings and pre-alignment Transformer that significantly impact multimodal music understanding tasks, delivering state-of-the-art music LLMs optimized for different downstream tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{202502_DeepResonance_Fig2.pdf}
    \caption{\textbf{Multi-way Instruction Tuning Data Construction.} Based on AudioSet, we construct Music4way (M+T$\xrightarrow{}$T, I+T$\xrightarrow{}$T, V+T$\xrightarrow{}$T), Music4way-MI2T (M+I+T$\xrightarrow{}$T), Music4way-MV2T (M+V+T$\xrightarrow{}$T), and Music4way-Any2T (M+I/V+T$\xrightarrow{}$T) for instruction tuning and evaluation. (M: music; I: image; V: video; T: text)}
    \label{fig:data}
\end{figure*}