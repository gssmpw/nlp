%\vspace{-3mm}
\section{Introduction}
\vspace{-3mm}

%Large Language Models (LLMs) have achieved significant advancements with ChatGPT representing a particularly revolutionary milestone. It excels in producing human-like conversations, heralding a new era in human-AI interactions. Moreover, the latest LLMs demonstrated remarkable reasoning capabilities and exceptional generalization skills. However, their reliance on training with collections of textual content from the Internet leaves them considerably distant from achieving a profound understanding of the physical world. On the other hand, the ubiquitous Wi-Fi devices and the extensive coverage of Wi-Fi networks present an opportunity to expand Wi-Fi capabilities beyond communication, particularly in sensing the physical world~\cite{ma2019wifi, tan2022commodity}. As Wi-Fi signals traverse the physical environment, they interact with surrounding people and objects, causing reflection, diffraction, scattering, etc. Consequently, the received signals can carry a substantial amount of information about both people and the physical environment. This raises an essential and intriguing question: can we integrate LLMs with Wi-Fi sensing to complete tasks in the real physical world? We explore this question by examining how to extend the boundaries of LLMs through direct interaction with the physical world via Wi-Fi signals.

Large Language Models (LLMs) have achieved significant advancements with ChatGPT representing a particularly revolutionary milestone. Their ability to generate human-like conversations in a zero-shot manner signifies a transformative shift in human-AI interactions. Moreover, the latest LLMs demonstrated remarkable reasoning capabilities and exceptional generalization skills. However, their reliance on training with collections of textual content from the Internet leaves them considerably distant from achieving a profound understanding of the physical world. Meanwhile, the ubiquitous Wi-Fi devices and the extensive coverage of Wi-Fi networks present an opportunity to expand Wi-Fi capabilities beyond communication, particularly in sensing the physical world~\cite{tan2022commodity}. As Wi-Fi signals traverse the physical environment, they interact with surrounding people and objects, causing reflection, diffraction, scattering, etc. Consequently, the received signals can carry a substantial amount of information about both people and the environment. Conventional Wi-Fi-based sensing systems can achieve various sensing tasks in the physical environment, such as human activity recognition and localization~\cite{ma2019wifi}. However, these systems typically rely on complex signal processing techniques and the labor-intensive training of machine learning or deep learning models.
This raises a fundamental and compelling question: \emph{can we integrate LLMs with Wi-Fi sensing to comprehend the physical world without complex signal processing and in a zero-shot manner?}

%We investigate this question by exploring the understanding capabilities of LLMs with raw Wi-Fi signals as input while incorporating physical model guidance based on Wi-Fi sensing principles.

We investigate this question by exploring the capabilities of LLMs to understand raw Wi-Fi signals and incorporating physical model guidance based on Wi-Fi sensing. Specifically, we introduce Wi-Chat, an LLM-powered Wi-Fi sensing system for human activity recognition. Unlike existing LLMs that primarily analyze traditional textual and visual data, Wi-Chat can understand Wi-Fi signals which are real-world projections of the physical environment. We demonstrate that LLMs, having been trained on extensive human knowledge, when integrated with the physical models of Wi-Fi sensing, can be directly leveraged for Wi-Fi signal analysis. This approach can derive deep insights that traditionally require complex signal processing and machine learning or deep learning models trained on large volumes of labeled data.

Wi-Chat directly feeds textual or visual representations of raw Wi-Fi signals into well-known LLMs such as ChatGPT, DeepSeek, and LLama for human activity recognition. Additionally, we incorporate physical models of Wi-Fi sensing into LLMs via prompting, enabling a deep understanding of human activities in the physical world through Wi-Fi signals.
We conduct experiments using a self-collected human activity dataset. The benchmark systems for comparison include both conventional Wi-Fi-based human activity recognition systems and fundamental machine learning models. Our evaluations demonstrate that LLMs can perform zero-shot human activity recognition directly from raw Wi-Fi signals, achieving the best accuracy of $90\%$ on unseen data. The key contributions of our work include:

\begin{itemize}
  \item We propose Wi-Chat, the first LLM-powered Wi-Fi-based human activity recognition system, which integrates the reasoning capabilities of LLMs with the sensing capabilities of wireless signals to interpret human activities.
  \vspace{-3mm}
  \item We incorporate Wi-Fi sensing into prompts, providing physical model guidance to LLMs.
  \vspace{-3mm}
  \item The experiments show that Wi-Chat achieves promising performance in a zero-shot setting without complex signal processing.
\end{itemize}

%processes raw Wi-Fi signals by converting them into textual or visual representations, which are then directly fed into well-known LLMs such as ChatGPT, DeepSeek, and Llama for human activity recognition. These activities include walking, falling, breathing, and the absence of motion. Furthermore, we enhance the effectiveness of LLMs in interpreting Wi-Fi signals by incorporating prompts informed by physical models of Wi-Fi sensing. This approach enables LLMs to achieve a deeper understanding of human activities in the physical world through Wi-Fi-based sensing.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/systemflow02.pdf}
    \vspace{-3mm}
    \caption{The overall computing pipeline of three different paradigms for Wi-Fi-based human activity recognition.}
    \vspace{-4mm}
    \label{systemflow}
\end{figure*}