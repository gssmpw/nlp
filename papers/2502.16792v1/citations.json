[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhou_what_2023",
        "author": "Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum",
        "title": "What {Algorithms} can {Transformers} {Learn}? {A} {Study} in {Length} {Generalization}"
      },
      {
        "key": "huang_formal_2024",
        "author": "Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael",
        "title": "A {Formal} {Framework} for {Understanding} {Length} {Generalization} in {Transformers}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "weiss_thinking_2021",
        "author": "Weiss, Gail and Goldberg, Yoav and Yahav, Eran",
        "title": "Thinking {Like} {Transformers}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "huang_formal_2024",
        "author": "Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael",
        "title": "A {Formal} {Framework} for {Understanding} {Length} {Generalization} in {Transformers}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "sabbaghi_explicitly_2024",
        "author": "Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi",
        "title": "Explicitly {Encoding} {Structural} {Symmetry} is {Key} to {Length} {Generalization} in {Arithmetic} {Tasks}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ahuja_provable_2024",
        "author": "Ahuja, Kartik and Mansouri, Amin",
        "title": "On {Provable} {Length} and {Compositional} {Generalization}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sabbaghi_explicitly_2024",
        "author": "Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi",
        "title": "Explicitly {Encoding} {Structural} {Symmetry} is {Key} to {Length} {Generalization} in {Arithmetic} {Tasks}"
      },
      {
        "key": "ahuja_provable_2024",
        "author": "Ahuja, Kartik and Mansouri, Amin",
        "title": "On {Provable} {Length} and {Compositional} {Generalization}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zou_universal_2023",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "key": "wei_jailbroken_2023",
        "author": "Alexander Wei and Nika Haghtalab and Jacob Steinhardt",
        "title": "Jailbroken: How Does {LLM} Safety Training Fail?"
      },
      {
        "key": "andriushchenko_jailbreaking_2024",
        "author": "Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion",
        "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hou_universal_2024",
        "author": "Hou, Kaiying and Brandfonbrener, David and Kakade, Sham and Jelassi, Samy and Malach, Eran",
        "title": "Universal {Length} {Generalization} with {Turing} {Programs}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hahn_why_2024",
        "author": "Hahn, Michael and Rofin, Mark",
        "title": "Why are {Sensitive} {Functions} {Hard} for {Transformers}?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "marsden_provable_2024",
        "author": "Marsden, Annie and Dogariu, Evan and Agarwal, Naman and Chen, Xinyi and Suo, Daniel and Hazan, Elad",
        "title": "Provable {Length} {Generalization} in {Sequence} {Prediction} via {Spectral} {Filtering}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang_transformers_2024",
        "author": "Wang, Zixuan and Wei, Stanley and Hsu, Daniel and Lee, Jason D.",
        "title": "Transformers {Provably} {Learn} {Sparse} {Token} {Selection} {While} {Fully}-{Connected} {Nets} {Cannot}"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sanford_representational_2023",
        "author": "Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus",
        "title": "Representational {Strengths} and {Limitations} of {Transformers}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang_transformers_2024",
        "author": "Wang, Zixuan and Wei, Stanley and Hsu, Daniel and Lee, Jason D.",
        "title": "Transformers {Provably} {Learn} {Sparse} {Token} {Selection} {While} {Fully}-{Connected} {Nets} {Cannot}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "cho_position_2024",
        "author": "Cho, Hanseul and Cha, Jaeyoung and Awasthi, Pranjal and Bhojanapalli, Srinadh and Gupta, Anupam and Yun, Chulhee",
        "title": "Position {Coupling}: {Leveraging} {Task} {Structure} for {Improved} {Length} {Generalization} of {Transformers}"
      },
      {
        "key": "cho_arithmetic_2024",
        "author": "Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee",
        "title": "Arithmetic {Transformers} {Can} {Length}-{Generalize} in {Both} {Operand} {Length} and {Count}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "mcleish_transformers_2024",
        "author": "McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom",
        "title": "Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhu_pose_2024",
        "author": "Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian",
        "title": "{PoSE}: {Efficient} {Context} {Window} {Extension} of {LLMs} via {Positional} {Skip}-wise {Training}"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wu_never_2024",
        "author": "Wu, Tong and Zhao, Yanpeng and Zheng, Zilong",
        "title": "Never {Miss} {A} {Beat}: {An} {Efficient} {Recipe} for {Context} {Window} {Extension} of {Large} {Language} {Models} with {Consistent} \"{Middle}\" {Enhancement}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ruoss_randomized_2023",
        "author": "Ruoss, Anian and Del\u00e9tang, Gr\u00e9goire and Genewein, Tim and Grau-Moya, Jordi and Csord\u00e1s, R\u00f3bert and Bennani, Mehdi and Legg, Shane and Veness, Joel",
        "title": "Randomized {Positional} {Encodings} {Boost} {Length} {Generalization} of {Transformers}"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "roziere_code_2024",
        "author": "Baptiste Rozi\u00e8re and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and J\u00e9r\u00e9my Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre D\u00e9fossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve",
        "title": "Code Llama: Open Foundation Models for Code"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "chen_extending_2023",
        "author": "Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong",
        "title": "Extending {Context} {Window} of {Large} {Language} {Models} via {Positional} {Interpolation}"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "emozilla_dynamically_2023",
        "author": "emozilla",
        "title": "Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "peng_yarn_2023",
        "author": "Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico",
        "title": "{YaRN}: {Efficient} {Context} {Window} {Extension} of {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "chen_clex_2024",
        "author": "Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing",
        "title": "{CLEX}: Continuous  Length Extrapolation for Large Language Models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhu_pose_2024",
        "author": "Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian",
        "title": "{PoSE}: {Efficient} {Context} {Window} {Extension} of {LLMs} via {Positional} {Skip}-wise {Training}"
      },
      {
        "key": "wu_never_2024",
        "author": "Wu, Tong and Zhao, Yanpeng and Zheng, Zilong",
        "title": "Never {Miss} {A} {Beat}: {An} {Efficient} {Recipe} for {Context} {Window} {Extension} of {Large} {Language} {Models} with {Consistent} \"{Middle}\" {Enhancement}"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "han_lminfinite_2024",
        "author": "Chi Han and Qifan Wang and Hao Peng and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang",
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "jin_llm_2024",
        "author": "Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu",
        "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "lu_controlled_2024",
        "author": "Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush",
        "title": "A Controlled Study on Long Context Extension and Generalization in LLMs"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "anil_exploring_2022",
        "author": "Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam",
        "title": "Exploring {Length} {Generalization} in {Large} {Language} {Models}"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "kazemnejad_impact_2023",
        "author": "Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva",
        "title": "The {Impact} of {Positional} {Encoding} on {Length} {Generalization} in {Transformers}"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "lee_teaching_2023",
        "author": "Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D. and Lee, Kangwook and Papailiopoulos, Dimitris",
        "title": "Teaching {Arithmetic} to {Small} {Transformers}"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "lu_controlled_2024",
        "author": "Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush",
        "title": "A Controlled Study on Long Context Extension and Generalization in LLMs"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "ontanon_making_2022",
        "author": "Ontanon, Santiago and Ainslie, Joshua and Fisher, Zachary and Cvicek, Vaclav",
        "title": "Making {Transformers} {Solve} {Compositional} {Tasks}"
      },
      {
        "key": "dziri_faith_2023",
        "author": "Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin",
        "title": "Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}"
      },
      {
        "key": "hupkes_compositionality_2020",
        "author": "Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia",
        "title": "Compositionality decomposed: how do neural networks generalise?"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "nagarajan_understanding_2021",
        "author": "Nagarajan, Vaishnavh and Andreassen, Anders and Neyshabur, Behnam",
        "title": "Understanding the {Failure} {Modes} of {Out}-of-{Distribution} {Generalization}"
      },
      {
        "key": "abbe_generalization_2023",
        "author": "Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin",
        "title": "Generalization on the {Unseen}, {Logic} {Reasoning} and {Degree} {Curriculum}"
      },
      {
        "key": "kalavasis_transfer_2024",
        "author": "Kalavasis, Alkis and Zadik, Ilias and Zampetakis, Manolis",
        "title": "Transfer {Learning} {Beyond} {Bounded} {Density} {Ratios}"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "hupkes_compositionality_2020",
        "author": "Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia",
        "title": "Compositionality decomposed: how do neural networks generalise?"
      },
      {
        "key": "liu_exposing_2023",
        "author": "Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Exposing {Attention} {Glitches} with {Flip}-{Flop} {Language} {Modeling}"
      },
      {
        "key": "deletang_neural_2023",
        "author": "Del\u00e9tang, Gr\u00e9goire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.",
        "title": "Neural {Networks} and the {Chomsky} {Hierarchy}"
      },
      {
        "key": "zhang_unveiling_2023",
        "author": "Zhang, Yi and Backurs, Arturs and Bubeck, S\u00e9bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal",
        "title": "Unveiling {Transformers} with {LEGO}: a synthetic reasoning task"
      },
      {
        "key": "hsieh_ruler_2024",
        "author": "Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris",
        "title": "{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "shaw_self_2018",
        "author": "Shaw, Peter  and\nUszkoreit, Jakob  and\nVaswani, Ashish",
        "title": "Self-Attention with Relative Position Representations"
      },
      {
        "key": "dai_transformer_2019",
        "author": "Dai, Zihang  and\nYang, Zhilin  and\nYang, Yiming  and\nCarbonell, Jaime  and\nLe, Quoc  and\nSalakhutdinov, Ruslan",
        "title": "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context"
      },
      {
        "key": "huang_improve_2020",
        "author": "Huang, Zhiheng  and\nLiang, Davis  and\nXu, Peng  and\nXiang, Bing",
        "title": "Improve Transformer Models with Better Relative Position Embeddings"
      },
      {
        "key": "ke_rethinking_2021",
        "author": "Guolin Ke and Di He and Tie-Yan Liu",
        "title": "Rethinking Positional Encoding in Language Pre-training"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "press_train_2022",
        "author": "Press, Ofir and Smith, Noah A. and Lewis, Mike",
        "title": "Train {Short}, {Test} {Long}: {Attention} with {Linear} {Biases} {Enables} {Input} {Length} {Extrapolation}"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "jelassi_repeat_2024",
        "author": "Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran",
        "title": "Repeat {After} {Me}: {Transformers} are {Better} than {State} {Space} {Models} at {Copying}"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "li_functional_2024",
        "author": "Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh",
        "title": "Functional {Interpolation} for {Relative} {Positions} {Improves} {Long} {Context} {Transformers}"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "jelassi_length_2023",
        "author": "Jelassi, Samy and d'Ascoli, St\u00e9phane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, Fran\u00e7ois",
        "title": "Length {Generalization} in {Arithmetic} {Transformers}"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "shen_positional_2023",
        "author": "Shen, Ruoqi and Bubeck, S\u00e9bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi",
        "title": "Positional {Description} {Matters} for {Transformers} {Arithmetic}"
      },
      {
        "key": "hu_casebased_2024",
        "author": "Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan",
        "title": "Case-based or rule-based: how do transformers do the math?"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "lanchantin_learning_2023",
        "author": "Jack Lanchantin and Shubham Toshniwal and Jason E Weston and Arthur Szlam and Sainbayar Sukhbaatar",
        "title": "Learning to Reason and Memorize with Self-Notes"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "csordas_neural_2022",
        "author": "Csord\u00e1s, R\u00f3bert and Irie, Kazuki and Schmidhuber, J\u00fcrgen",
        "title": "The {Neural} {Data} {Router}: {Adaptive} {Control} {Flow} in {Transformers} {Improves} {Systematic} {Generalization}"
      },
      {
        "key": "fan_looped_2024",
        "author": "Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook",
        "title": "Looped {Transformers} for {Length} {Generalization}"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "csordas_devil_2022",
        "author": "Csord\u00e1s, R\u00f3bert and Irie, Kazuki and Schmidhuber, J\u00fcrgen",
        "title": "The {Devil} is in the {Detail}: {Simple} {Tricks} {Improve} {Systematic} {Generalization} of {Transformers}"
      },
      {
        "key": "liu_transformers_2023",
        "author": "Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Transformers {Learn} {Shortcuts} to {Automata}"
      },
      {
        "key": "zhou_transformers_2024",
        "author": "Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny",
        "title": "Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}"
      }
    ]
  },
  {
    "index": 43,
    "papers": [
      {
        "key": "lee_selfimproving_2025",
        "author": "Nayoung Lee and Ziyang Cai and Avi Schwarzschild and Kangwook Lee and Dimitris Papailiopoulos",
        "title": "Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges"
      }
    ]
  }
]