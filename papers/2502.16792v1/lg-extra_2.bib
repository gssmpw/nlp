@misc{andriushchenko_jailbreaking_2024,
 archiveprefix = {arXiv},
 author = {Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
 eprint = {2404.02151},
 primaryclass = {cs.CR},
 title = {Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks},
 year = {2024}
}

@inproceedings{chen_clex_2024,
 author = {Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {{CLEX}: Continuous  Length Extrapolation for Large Language Models},
 year = {2024}
}

@article{child_generating_2019,
 author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
 journal = {arXiv preprint arXiv:1904.10509},
 title = {Generating Long Sequences with Sparse Transformers},
 year = {2019}
}

@inproceedings{cohen_contextcite_2024,
 author = {Benjamin Cohen-Wang and Harshay Shah and Kristian Georgiev and Aleksander Madry},
 booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 title = {ContextCite: Attributing Model Generation to Context},
 year = {2024}
}

@inproceedings{dai_transformer_2019,
 abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
 address = {Florence, Italy},
 author = {Dai, Zihang  and
Yang, Zhilin  and
Yang, Yiming  and
Carbonell, Jaime  and
Le, Quoc  and
Salakhutdinov, Ruslan},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'i}s},
 month = {July},
 pages = {2978--2988},
 publisher = {Association for Computational Linguistics},
 title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
 year = {2019}
}

@misc{deepseekai_deepseek_2025,
 archiveprefix = {arXiv},
 author = {DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
 eprint = {2501.12948},
 primaryclass = {cs.CL},
 title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
 year = {2025}
}

@misc{emozilla_dynamically_2023,
 author = {emozilla},
 howpublished = {\url{https://github.com/emozilla/Dynamically-Scaled-RoPE}},
 note = {Accessed: 2025-01-28},
 title = {Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning},
 year = {2023}
}

@inproceedings{gehring_convolutional_2017,
 author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning},
 pages = {1243--1252},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {Convolutional Sequence to Sequence Learning},
 volume = {70},
 year = {2017}
}

@misc{gpt-neox-library,
 author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Phang, Jason and Purohit, Shivanshu and Schoelkopf, Hailey and Stander, Dashiell and Songz, Tri and Tigges, Curt and Thérien, Benjamin and Wang, Phil and Weinbach, Samuel},
 month = {9},
 title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},
 version = {2.0.0},
 year = {2023}
}

@misc{groeneveld_olmo_2024,
 archiveprefix = {arXiv},
 author = {Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
 eprint = {2402.00838},
 primaryclass = {cs.CL},
 title = {OLMo: Accelerating the Science of Language Models},
 year = {2024}
}

@misc{han_lminfinite_2024,
 archiveprefix = {arXiv},
 author = {Chi Han and Qifan Wang and Hao Peng and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
 eprint = {2308.16137},
 primaryclass = {cs.CL},
 title = {LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models},
 year = {2024}
}

@inproceedings{hu_casebased_2024,
 abstract = {Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12- digit addition with over 95\% accuracy, which is over 40\% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule.},
 articleno = {783},
 author = {Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 location = {Vienna, Austria},
 numpages = {37},
 publisher = {JMLR.org},
 series = {ICML'24},
 title = {Case-based or rule-based: how do transformers do the math?},
 year = {2024}
}

@inproceedings{huang_improve_2020,
 abstract = {The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.},
 address = {Online},
 author = {Huang, Zhiheng  and
Liang, Davis  and
Xu, Peng  and
Xiang, Bing},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 editor = {Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 month = {November},
 pages = {3327--3335},
 publisher = {Association for Computational Linguistics},
 title = {Improve Transformer Models with Better Relative Position Embeddings},
 year = {2020}
}

@misc{jin_llm_2024,
 archiveprefix = {arXiv},
 author = {Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
 eprint = {2401.01325},
 primaryclass = {cs.CL},
 title = {LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
 year = {2024}
}

@misc{ke_rethinking_2021,
 archiveprefix = {arXiv},
 author = {Guolin Ke and Di He and Tie-Yan Liu},
 eprint = {2006.15595},
 primaryclass = {cs.CL},
 title = {Rethinking Positional Encoding in Language Pre-training},
 year = {2021}
}

@misc{kimiteam_kimi_2025,
 archiveprefix = {arXiv},
 author = {Kimi Team and Angang Du and Bofei Gao and Bowei Xing and Changjiu Jiang and Cheng Chen and Cheng Li and Chenjun Xiao and Chenzhuang Du and Chonghua Liao and Chuning Tang and Congcong Wang and Dehao Zhang and Enming Yuan and Enzhe Lu and Fengxiang Tang and Flood Sung and Guangda Wei and Guokun Lai and Haiqing Guo and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haotian Yao and Haotian Zhao and Haoyu Lu and Haoze Li and Haozhen Yu and Hongcheng Gao and Huabin Zheng and Huan Yuan and Jia Chen and Jianhang Guo and Jianlin Su and Jianzhou Wang and Jie Zhao and Jin Zhang and Jingyuan Liu and Junjie Yan and Junyan Wu and Lidong Shi and Ling Ye and Longhui Yu and Mengnan Dong and Neo Zhang and Ningchen Ma and Qiwei Pan and Qucheng Gong and Shaowei Liu and Shengling Ma and Shupeng Wei and Sihan Cao and Siying Huang and Tao Jiang and Weihao Gao and Weimin Xiong and Weiran He and Weixiao Huang and Wenhao Wu and Wenyang He and Xianghui Wei and Xianqing Jia and Xingzhe Wu and Xinran Xu and Xinxing Zu and Xinyu Zhou and Xuehai Pan and Y. Charles and Yang Li and Yangyang Hu and Yangyang Liu and Yanru Chen and Yejie Wang and Yibo Liu and Yidao Qin and Yifeng Liu and Ying Yang and Yiping Bao and Yulun Du and Yuxin Wu and Yuzhi Wang and Zaida Zhou and Zhaoji Wang and Zhaowei Li and Zhen Zhu and Zheng Zhang and Zhexu Wang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Ziyao Xu and Zonghan Yang},
 eprint = {2501.12599},
 primaryclass = {cs.AI},
 title = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},
 year = {2025}
}

@inproceedings{lanchantin_learning_2023,
 author = {Jack Lanchantin and Shubham Toshniwal and Jason E Weston and Arthur Szlam and Sainbayar Sukhbaatar},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Learning to Reason and Memorize with Self-Notes},
 year = {2023}
}

@article{lewkowycz_solving_2022,
 author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
 journal = {arXiv preprint arXiv:2206.14858},
 title = {Solving Quantitative Reasoning Problems with Language Models},
 year = {2022}
}

@misc{lu_controlled_2024,
 archiveprefix = {arXiv},
 author = {Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush},
 eprint = {2409.12181},
 primaryclass = {cs.CL},
 title = {A Controlled Study on Long Context Extension and Generalization in LLMs},
 year = {2024}
}

@inproceedings{nye_show_2022,
 author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
 booktitle = {Proceedings of the 10th International Conference on Learning Representations},
 title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},
 year = {2022}
}

@inproceedings{peng_limitations_2024,
 author = {Binghui Peng and Srini Narayanan and Christos Papadimitriou},
 booktitle = {First Conference on Language Modeling},
 title = {On Limitations of the Transformer Architecture},
 year = {2024}
}

@article{raffel_exploring_2019,
 archiveprefix = {arXiv},
 author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
 eprint = {1910.10683},
 journal = {arXiv e-prints},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
 year = {2019}
}

@misc{roziere_code_2024,
 archiveprefix = {arXiv},
 author = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
 eprint = {2308.12950},
 primaryclass = {cs.CL},
 title = {Code Llama: Open Foundation Models for Code},
 year = {2024}
}

@book{shalev_understanding_2014,
 author = {Shalev-Shwartz, Shai and Ben-David, Shai},
 isbn = {978-1-107-05713-5},
 publisher = {Cambridge University Press},
 title = {Understanding Machine Learning: From Theory to Algorithms},
 year = {2014}
}

@article{shannon_prediction_1951,
 author = {Shannon, Claude E.},
 journal = {Bell System Technical Journal},
 number = {1},
 pages = {50--64},
 publisher = {American Telephone and Telegraph Co.},
 title = {Prediction and Entropy of Printed English},
 volume = {30},
 year = {1951}
}

@inproceedings{shaw_self_2018,
 abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
 address = {New Orleans, Louisiana},
 author = {Shaw, Peter  and
Uszkoreit, Jakob  and
Vaswani, Ashish},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
 editor = {Walker, Marilyn  and
Ji, Heng  and
Stent, Amanda},
 month = {June},
 pages = {464--468},
 publisher = {Association for Computational Linguistics},
 title = {Self-Attention with Relative Position Representations},
 year = {2018}
}

@article{tay_efficient_2022,
 abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed—Reformer, Linformer, Performer, Longformer, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
 address = {New York, NY, USA},
 articleno = {109},
 author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
 issn = {0360-0300},
 issue_date = {June 2023},
 journal = {ACM Comput. Surv.},
 keywords = {Transformers, attention, deep learning, neural networks},
 month = {December},
 number = {6},
 numpages = {28},
 publisher = {Association for Computing Machinery},
 title = {Efficient Transformers: A Survey},
 volume = {55},
 year = {2022}
}

@inproceedings{wei_chain_2022,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
 booktitle = {Proceedings of the 36th Conference on Neural Information Processing Systems},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 year = {2022}
}

@inproceedings{wei_jailbroken_2023,
 author = {Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Jailbroken: How Does {LLM} Safety Training Fail?},
 year = {2023}
}

@misc{zhang_pointer_2022,
 archiveprefix = {arXiv},
 author = {Chiyuan Zhang and Maithra Raghu and Jon Kleinberg and Samy Bengio},
 eprint = {2107.12580},
 primaryclass = {cs.LG},
 title = {Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization},
 year = {2022}
}

@article{zou_universal_2023,
 author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
 journal = {arXiv preprint arXiv:2307.15043},
 title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
 year = {2023}
}

@misc{lee_selfimproving_2025,
      title={Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges}, 
      author={Nayoung Lee and Ziyang Cai and Avi Schwarzschild and Kangwook Lee and Dimitris Papailiopoulos},
      year={2025},
      eprint={2502.01612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}