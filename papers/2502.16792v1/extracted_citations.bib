@misc{andriushchenko_jailbreaking_2024,
 archiveprefix = {arXiv},
 author = {Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion},
 eprint = {2404.02151},
 primaryclass = {cs.CR},
 title = {Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks},
 year = {2024}
}

@inproceedings{chen_clex_2024,
 author = {Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {{CLEX}: Continuous  Length Extrapolation for Large Language Models},
 year = {2024}
}

@inproceedings{dai_transformer_2019,
 abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
 address = {Florence, Italy},
 author = {Dai, Zihang  and
Yang, Zhilin  and
Yang, Yiming  and
Carbonell, Jaime  and
Le, Quoc  and
Salakhutdinov, Ruslan},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 editor = {Korhonen, Anna  and
Traum, David  and
M{\`a}rquez, Llu{\'i}s},
 month = {July},
 pages = {2978--2988},
 publisher = {Association for Computational Linguistics},
 title = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
 year = {2019}
}

@misc{emozilla_dynamically_2023,
 author = {emozilla},
 howpublished = {\url{https://github.com/emozilla/Dynamically-Scaled-RoPE}},
 note = {Accessed: 2025-01-28},
 title = {Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning},
 year = {2023}
}

@misc{han_lminfinite_2024,
 archiveprefix = {arXiv},
 author = {Chi Han and Qifan Wang and Hao Peng and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
 eprint = {2308.16137},
 primaryclass = {cs.CL},
 title = {LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models},
 year = {2024}
}

@inproceedings{hu_casebased_2024,
 abstract = {Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12- digit addition with over 95\% accuracy, which is over 40\% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length. Code is available at https://github.com/GraphPKU/Case_or_Rule.},
 articleno = {783},
 author = {Hu, Yi and Tang, Xiaojuan and Yang, Haotong and Zhang, Muhan},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 location = {Vienna, Austria},
 numpages = {37},
 publisher = {JMLR.org},
 series = {ICML'24},
 title = {Case-based or rule-based: how do transformers do the math?},
 year = {2024}
}

@inproceedings{huang_improve_2020,
 abstract = {The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.},
 address = {Online},
 author = {Huang, Zhiheng  and
Liang, Davis  and
Xu, Peng  and
Xiang, Bing},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 editor = {Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 month = {November},
 pages = {3327--3335},
 publisher = {Association for Computational Linguistics},
 title = {Improve Transformer Models with Better Relative Position Embeddings},
 year = {2020}
}

@misc{jin_llm_2024,
 archiveprefix = {arXiv},
 author = {Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
 eprint = {2401.01325},
 primaryclass = {cs.CL},
 title = {LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
 year = {2024}
}

@misc{ke_rethinking_2021,
 archiveprefix = {arXiv},
 author = {Guolin Ke and Di He and Tie-Yan Liu},
 eprint = {2006.15595},
 primaryclass = {cs.CL},
 title = {Rethinking Positional Encoding in Language Pre-training},
 year = {2021}
}

@inproceedings{lanchantin_learning_2023,
 author = {Jack Lanchantin and Shubham Toshniwal and Jason E Weston and Arthur Szlam and Sainbayar Sukhbaatar},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Learning to Reason and Memorize with Self-Notes},
 year = {2023}
}

@misc{lee_selfimproving_2025,
      title={Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges}, 
      author={Nayoung Lee and Ziyang Cai and Avi Schwarzschild and Kangwook Lee and Dimitris Papailiopoulos},
      year={2025},
      eprint={2502.01612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{lu_controlled_2024,
 archiveprefix = {arXiv},
 author = {Yi Lu and Jing Nathan Yan and Songlin Yang and Justin T. Chiu and Siyu Ren and Fei Yuan and Wenting Zhao and Zhiyong Wu and Alexander M. Rush},
 eprint = {2409.12181},
 primaryclass = {cs.CL},
 title = {A Controlled Study on Long Context Extension and Generalization in LLMs},
 year = {2024}
}

@misc{roziere_code_2024,
 archiveprefix = {arXiv},
 author = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
 eprint = {2308.12950},
 primaryclass = {cs.CL},
 title = {Code Llama: Open Foundation Models for Code},
 year = {2024}
}

@inproceedings{shaw_self_2018,
 abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
 address = {New Orleans, Louisiana},
 author = {Shaw, Peter  and
Uszkoreit, Jakob  and
Vaswani, Ashish},
 booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
 editor = {Walker, Marilyn  and
Ji, Heng  and
Stent, Amanda},
 month = {June},
 pages = {464--468},
 publisher = {Association for Computational Linguistics},
 title = {Self-Attention with Relative Position Representations},
 year = {2018}
}

@inproceedings{wei_jailbroken_2023,
 author = {Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
 booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
 title = {Jailbroken: How Does {LLM} Safety Training Fail?},
 year = {2023}
}

@article{zou_universal_2023,
 author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
 journal = {arXiv preprint arXiv:2307.15043},
 title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
 year = {2023}
}

