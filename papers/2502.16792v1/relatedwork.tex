\section{Related work}
\label{sec:related-work}

\subsection{Theoretical guarantees for length generalization}
\citet{zhou_what_2023,huang_formal_2024}
propose that transformers will length-generalize at tasks which can be solved using a short RASP program \cite{weiss_thinking_2021}. Theorem 7 of \citet{huang_formal_2024}, however, assumes that the transformer is chosen so as to minimize a certain regularizer tailored specifically for length generalization, and their result is asymptotic in nature (in contrast to ours).
\citet{sabbaghi_explicitly_2024} show that using  gradient flow on a 1-layer linear transformer model with relative position embeddings on a simple linear regression task will converge to a model which length-generalizes, whereas the use of absolute position embeddings will fail to length-generalize. \citet{ahuja_provable_2024} show that a model class defined as a simple abstraction of attention heads that sums up pairwise dependencies of tokens experiences length generalization. %
Unlike our theoretical results, those of   \citet{sabbaghi_explicitly_2024,ahuja_provable_2024} are asymptotic in nature and cannot capture the (most common) case of a \emph{softmax} attention head. Moreover, they proceed, roughly speaking, by showing that the learned model is equal to the ground-truth model \emph{on the entire domain}, which is generally \emph{not} the case with actual language models \cite{zou_universal_2023,wei_jailbroken_2023,andriushchenko_jailbreaking_2024}. As our theoretical setup incorporates distributional assumptions, it establishes length generalization in cases where the learned model is only accurate on in-distribution inputs, which we believe to be more realistic.
Moreover, all of the preceding works generally apply only to specific classes of \emph{transformers or linear classes}; in contrast, our framework, while being able to capture an attention head, is significantly more general in that we allow arbitrary, potentially nonlinear, function classes (see \cref{def:sparse-group-attn}), thus meaning our results may have significance for other (non-transformer) classes of models as well. 

\citet{hou_universal_2024} proposes to use \emph{Turing programs}, a scratchpad strategy inspired by Turing machines, to achieve length generalization on an array of tasks. Their  theoretical results, however, are only representational in nature, showing that transformers can represent Turing programs without  accounting for what models algorithms such as risk minimization will actually learn. \citet{hahn_why_2024} offer reasons why transformers struggle to length-generalize on parity, based on their sensitivity bias. 

Further from our own work, \citet{marsden_provable_2024} obtain provable guarantees for length generalization in the context of dynamical systems. \citet{wang_transformers_2024} prove that gradient descent on transformers can provably learn a task known as the \emph{sparse token selection task} \cite{sanford_representational_2023}, which bears resemblance to \cref{def:sparse-planted} (indeed, a slight modification of the task defined in Section 2.1 of \citet{wang_transformers_2024} is in fact a special case of a distribution with sparse planted correlations (\cref{def:sparse-planted})). 




\subsection{Modifying positional embeddings for length \generalization}
\label{sec:related-posids}
In addition to position coupling \cite{cho_position_2024,cho_arithmetic_2024} (and the closely related Abacus embeddings  \cite{mcleish_transformers_2024}), which is a focus in our paper, several other techniques have been developed to modify position embeddings during training and/or inference time to improve the length generalization performance of transformers. \citet{zhu_pose_2024} developed the positional skip-wise technique (PoSE; see \cref{sec:prelim-pose}), which was later refined by modifying the distribution of position IDs used at training time in \citet{wu_never_2024}. We remark that PoSE is conceptually similar to \emph{randomized position embeddings} \cite{ruoss_randomized_2023}, which trains using a random set of position IDs from the test-length context window (with no guarantee on the contiguity of these IDs, unlike PoSE).

Another popular strategy to extend the context length of transformers which has seen traction for models at larger scales, such as as Code Llama \cite{roziere_code_2024}, is \emph{position interpolation}. This technique \emph{scales down} (``interpolates'') the position IDs in the longer test-length context window to match the length of the shorter training-length context. Several such interpolation strategies have been proposed, including the canonical choice of \emph{linear interpolation} \cite{chen_extending_2023}, as well as \emph{NTK-RoPE} \cite{emozilla_dynamically_2023}, \emph{YaRN} \cite{peng_yarn_2023}, and \emph{CLEX} \cite{chen_clex_2024}; the latter strategies adjust the amount of interpolation done on a per-frequency basis, with different RoPE frequencies receiving different interpolation scales. One downside of these position interpolation strategies is that they generally require some fine-tuning on the longer test-length sequences in order to effectively use the longer context windows (e.g., to achieve decreased perplexity on longer sequences than those seen during training). Such fine-tuning complicates the theoretical setting of length generalization, where it is typically assumed that any amount of training on the longer test-length sequences is not allowed. We remark, however, that these position interpolation techniques can be combined with PoSE \cite{zhu_pose_2024,wu_never_2024}; exploring such combinations in the context of our experiments is an interesting direction for future work.

Finally, we remark that some strategies, such as LM-Infinite \cite{han_lminfinite_2024} and Self-Extend \cite{jin_llm_2024}, have been proposed to adjust the attention mechanism at inference time so as to achieve length generalization without any fine-tuning, though their performance lags somewhat \cite{lu_controlled_2024}. 





\subsection{Empirical evaluations \& explanations of length generalization}
A number of papers have offered empirical evaluations and comparisons of various techniques for length generalization. \citet{anil_exploring_2022} studied a few simple length generalization tasks, such as parity and variable assignment, and found that techniques such as finetuning and using a scratchpad did not lead to much length generalization. 
\citet{kazemnejad_impact_2023} compared the performance of various positional encoding techniques for length generalization, and found that NoPE performed best (though their analysis did not account for techniques such as position interpolation and PoSE which can significantly improve length generalization for, e.g., RoPE).  \citet{lee_teaching_2023} observed that length generalization is difficult for transformers on arithmetic tasks. 
Finally, \citet{lu_controlled_2024} performed a systematic study comparing various approaches to extend the context length of LLMs, including various types of position interpolation. 


\citet{ontanon_making_2022,dziri_faith_2023,hupkes_compositionality_2020} study the out-of-distribution performance of transformers by focusing on \emph{compositional generalization}, which refers to the ability of transformers to compose individual tasks found in the training data. There is also extensive work more broadly on out-of-distribution generalization \cite{nagarajan_understanding_2021,abbe_generalization_2023,kalavasis_transfer_2024}.

Finally, a number of works (e.g., \citet{hupkes_compositionality_2020,liu_exposing_2023,deletang_neural_2023,zhang_unveiling_2023,hsieh_ruler_2024}) introduce new benchmarks and datasets for studying length generalization and more broadly the performance of LLMs on long contexts. 




\paragraph{Additional techniques for length generalization.} Many papers have introduced new types of positional encoding schemes with the hope of improved length generalization, including relative positional embeddings \cite{shaw_self_2018,dai_transformer_2019,huang_improve_2020,ke_rethinking_2021}, ALiBi \cite{press_train_2022}, Hard-Alibi \cite{jelassi_repeat_2024}, and FIRE \cite{li_functional_2024}. 


Many other types of techniques have  been proposed, such as priming the training dataset with a few long sequences \cite{jelassi_length_2023}, %
modifying the format of the input \cite{shen_positional_2023,hu_casebased_2024} such as by using a scratchpad \cite{lanchantin_learning_2023}, architectural changes \cite{csordas_neural_2022,fan_looped_2024}, 
and combining several such techniques \cite{csordas_devil_2022,liu_transformers_2023,zhou_transformers_2024}. Moreover, it was recently shown that length generalization can be achieved via a \emph{self-improvement} framework \cite{lee_selfimproving_2025}, which trains the model on sequences of increasing length, which are labeled by previous versions of the model.  We remark that such techniques lie outside the scope of this paper, as we require that the context length be bounded above by $L$ throughout training. Moreover, such techniques still may suffer from the computational issues that plague longer context lengths.