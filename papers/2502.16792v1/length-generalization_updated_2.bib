@inproceedings{abbe_generalization_2023,
 abstract = {This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an ‘extrapolating’ or ‘reasoning’ learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length generalization problem (e.g., Anil et al. 2022); (2) we introduce a curriculum learning algorithm called Degree-Curriculum that learns monomials more efficiently by incrementing supports.},
 author = {Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
 booktitle = {International Conference on Machine Learning},
 file = {Abbe et al. - 2023 - Generalization on the Unseen, Logic Reasoning and .pdf:/Users/ngolowich/Zotero/storage/W738VJX7/Abbe et al. - 2023 - Generalization on the Unseen, Logic Reasoning and .pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2301.13105 [cs, stat]},
 publisher = {arXiv},
 title = {Generalization on the {Unseen}, {Logic} {Reasoning} and {Degree} {Curriculum}},
 urldate = {2024-05-07},
 year = {2023}
}

@inproceedings{abbe_staircase_2021,
 abstract = {This paper identiﬁes a structural property of data distributions that enables deep neural networks to learn hierarchically. We deﬁne the “staircase” property for functions over the Boolean hypercube, which posits that high-order Fourier coefﬁcients are reachable from lower-order Fourier coefﬁcients along increasing chains. We prove that functions satisfying this property can be learned in polynomial time using layerwise stochastic coordinate descent on regular neural networks –a class of network architectures and initializations that have homogeneity properties. Our analysis shows that for such staircase functions and neural networks, the gradient-based algorithm learns high-level features by greedily combining lower-level features along the depth of the network. We further back our theoretical results with experiments showing that staircase functions are learnable by more standard ResNet architectures with stochastic gradient descent. Both the theoretical and experimental results support the fact that the staircase property has a role to play in understanding the capabilities of gradient-based learning on regular networks, in contrast to general polynomial-size networks that can emulate any Statistical Query or PAC algorithm, as recently shown.},
 author = {Abbe, Emmanuel and Boix-Adsera, Enric and Brennan, Matthew and Bresler, Guy and Nagaraj, Dheeraj},
 booktitle = {Neural Information Processing Systems},
 file = {Abbe et al. - 2021 - The staircase property How hierarchical structure.pdf:/Users/ngolowich/Zotero/storage/DFQXLG7I/Abbe et al. - 2021 - The staircase property How hierarchical structure.pdf:application/pdf},
 keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2108.10573 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {The staircase property},
 title = {The staircase property: {How} hierarchical structure can guide deep learning},
 urldate = {2024-05-23},
 year = {2021}
}

@misc{ahuja_provable_2024,
 abstract = {Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.},
 author = {Ahuja, Kartik and Mansouri, Amin},
 file = {Ahuja and Mansouri - 2024 - On Provable Length and Compositional Generalizatio.pdf:/Users/ngolowich/Zotero/storage/RNIJHWS2/Ahuja and Mansouri - 2024 - On Provable Length and Compositional Generalizatio.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2402.04875 [cs, stat]},
 publisher = {arXiv},
 title = {On {Provable} {Length} and {Compositional} {Generalization}},
 urldate = {2024-05-25},
 year = {2024}
}

@inproceedings{anil_exploring_2022,
 abstract = {The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.},
 author = {Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
 booktitle = {Neural Information Processing Systems},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/9ZLF4MR7/Anil et al. - 2022 - Exploring Length Generalization in Large Language .pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/YIUS2RWL/2207.html:text/html},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 month = {November},
 note = {arXiv:2207.04901 [cs]},
 publisher = {arXiv},
 title = {Exploring {Length} {Generalization} in {Large} {Language} {Models}},
 urldate = {2024-05-23},
 year = {2022}
}

@misc{arora_theory_2023,
 abstract = {A driver of current AI research is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this slingshot generalization since, naively viewed, it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at “complex skills,” which involve t-tuples of basic skills, emerges essentially at very similar scaling as competence on the elementary skills themselves.},
 author = {Arora, Sanjeev and Goyal, Anirudh},
 file = {Arora and Goyal - 2023 - A Theory for Emergence of Complex Skills in Langua.pdf:/Users/ngolowich/Zotero/storage/TG6E78Z8/Arora and Goyal - 2023 - A Theory for Emergence of Complex Skills in Langua.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2307.15936 [cs, stat]},
 publisher = {arXiv},
 title = {A {Theory} for {Emergence} of {Complex} {Skills} in {Language} {Models}},
 urldate = {2024-06-14},
 year = {2023}
}

@inproceedings{bai_transformers_2023,
 abstract = {Neural sequence models based on the transformer architecture have demonstrated remarkable {\textbackslash}emph\{in-context learning\} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving {\textbackslash}emph\{in-context algorithm selection\}, akin to what a statistician can do in real life -- A {\textbackslash}emph\{single\} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.},
 author = {Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
 booktitle = {Neural Information Processing Systems},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/GUQI3CFV/Bai et al. - 2023 - Transformers as Statisticians Provable In-Context.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/AEEP5WSF/2306.html:text/html},
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Computer Science - Computation and Language, Computer Science - Machine Learning},
 month = {July},
 note = {arXiv:2306.04637 [cs, math, stat]},
 publisher = {arXiv},
 shorttitle = {Transformers as {Statisticians}},
 title = {Transformers as {Statisticians}: {Provable} {In}-{Context} {Learning} with {In}-{Context} {Algorithm} {Selection}},
 urldate = {2024-05-28},
 year = {2023}
}

@misc{bhattamishra_separations_2024,
 abstract = {Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality. For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size. Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of N nearly orthogonal vectors in O(log N ) dimensional space and our lower bounds are based on reductions from communication complexity problems. We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.},
 author = {Bhattamishra, Satwik and Hahn, Michael and Blunsom, Phil and Kanade, Varun},
 file = {Bhattamishra et al. - 2024 - Separations in the Representational Capabilities o.pdf:/Users/ngolowich/Zotero/storage/IWU2J7ZV/Bhattamishra et al. - 2024 - Separations in the Representational Capabilities o.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2406.09347 [cs, stat]},
 publisher = {arXiv},
 title = {Separations in the {Representational} {Capabilities} of {Transformers} and {Recurrent} {Architectures}},
 urldate = {2024-06-18},
 year = {2024}
}

@misc{chen_extending_2023,
 abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based (Su et al., 2021) pretrained LLMs such as LLaMA (Touvron et al., 2023) models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least ∼ 600× smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 file = {Chen et al. - 2023 - Extending Context Window of Large Language Models .pdf:/Users/ngolowich/Zotero/storage/D36NZLFQ/Chen et al. - 2023 - Extending Context Window of Large Language Models .pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2306.15595 [cs]},
 publisher = {arXiv},
 title = {Extending {Context} {Window} of {Large} {Language} {Models} via {Positional} {Interpolation}},
 urldate = {2024-07-07},
 year = {2023}
}

@inproceedings{chen_training_2024,
 abstract = {We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting “task allocation” phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases — a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.},
 author = {Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
 booktitle = {COLT},
 file = {Chen et al. - 2024 - Training Dynamics of Multi-Head Softmax Attention .pdf:/Users/ngolowich/Zotero/storage/BNAP4LT9/Chen et al. - 2024 - Training Dynamics of Multi-Head Softmax Attention .pdf:application/pdf},
 keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2402.19442 [cs, math, stat]},
 publisher = {arXiv},
 shorttitle = {Training {Dynamics} of {Multi}-{Head} {Softmax} {Attention} for {In}-{Context} {Learning}},
 title = {Training {Dynamics} of {Multi}-{Head} {Softmax} {Attention} for {In}-{Context} {Learning}: {Emergence}, {Convergence}, and {Optimality}},
 urldate = {2024-05-28},
 year = {2024}
}

@inproceedings{chi_attention_2023,
 abstract = {An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any fine-tuning. Such long-context utilization capability relies heavily on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings show improvement on the long-context utilization capability of T5 on language modeling, retrieval, multi-document question answering, and code completion tasks without any fine-tuning. This suggests that a flexible positional embedding design and attention alignment can go a long way toward Transformer length extrapolation.},
 author = {Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander I.},
 booktitle = {NAACL-HLT},
 file = {Chi et al. - 2023 - Attention Alignment and Flexible Positional Embedd.pdf:/Users/ngolowich/Zotero/storage/DXE5L98E/Chi et al. - 2023 - Attention Alignment and Flexible Positional Embedd.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2311.00684 [cs]},
 publisher = {arXiv},
 title = {Attention {Alignment} and {Flexible} {Positional} {Embeddings} {Improve} {Transformer} {Length} {Extrapolation}},
 urldate = {2024-07-09},
 year = {2023}
}

@article{chi_toward_nodate,
 author = {Chi, Ta-Chung},
 file = {Chi - Toward Length-Extrapolatable Transformers.pdf:/Users/ngolowich/Zotero/storage/CRL9B4AC/Chi - Toward Length-Extrapolatable Transformers.pdf:application/pdf},
 language = {en},
 title = {Toward {Length}-{Extrapolatable} {Transformers}}
}

@misc{cho_arithmetic_2024,
 abstract = {Transformers often struggle with length generalization, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (requiring generalization over both the number of operands and their lengths) and multiplication (requiring generalization over both operand lengths). In this work, we achieve approximately 2–3× length generalization on both tasks, which is the first such achievement in arithmetic Transformers. We design task-specific scratchpads enabling the model to focus on a fixed number of tokens per each next-token prediction step, and apply multi-level versions of Position Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers know the right position to attend to. On the theory side, we prove that a 1-layer Transformer using our method can solve multi-operand addition, up to operand length and operand count that are exponential in embedding dimension. All our experiments are reproducible based on the codebase in github.com/HanseulJo/position-coupling.},
 author = {Cho, Hanseul and Cha, Jaeyoung and Bhojanapalli, Srinadh and Yun, Chulhee},
 file = {Cho et al. - 2024 - Arithmetic Transformers Can Length-Generalize in B.pdf:/Users/ngolowich/Zotero/storage/ERVET73Y/Cho et al. - 2024 - Arithmetic Transformers Can Length-Generalize in B.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2410.15787 [cs]},
 publisher = {arXiv},
 title = {Arithmetic {Transformers} {Can} {Length}-{Generalize} in {Both} {Operand} {Length} and {Count}},
 urldate = {2024-11-02},
 year = {2024}
}

@misc{cho_position_2024,
 abstract = {Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more "relevant" tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.},
 author = {Cho, Hanseul and Cha, Jaeyoung and Awasthi, Pranjal and Bhojanapalli, Srinadh and Gupta, Anupam and Yun, Chulhee},
 file = {Cho et al. - 2024 - Position Coupling Leveraging Task Structure for I.pdf:/Users/ngolowich/Zotero/storage/WZUP8JFZ/Cho et al. - 2024 - Position Coupling Leveraging Task Structure for I.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2405.20671 [cs]},
 publisher = {arXiv},
 shorttitle = {Position {Coupling}},
 title = {Position {Coupling}: {Leveraging} {Task} {Structure} for {Improved} {Length} {Generalization} of {Transformers}},
 urldate = {2024-08-13},
 year = {2024}
}

@misc{cho_position_2024-1,
 abstract = {Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more “relevant” tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, our models trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67× of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as N × 2 multiplication and a two-dimensional task. Our codebase is available at github.com/HanseulJo/position-coupling.},
 author = {Cho, Hanseul and Cha, Jaeyoung and Awasthi, Pranjal and Bhojanapalli, Srinadh and Gupta, Anupam and Yun, Chulhee},
 file = {Cho et al. - 2024 - Position Coupling Improving Length Generalization.pdf:/Users/ngolowich/Zotero/storage/7QMZRLBD/Cho et al. - 2024 - Position Coupling Improving Length Generalization.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2405.20671 [cs]},
 publisher = {arXiv},
 shorttitle = {Position {Coupling}},
 title = {Position {Coupling}: {Improving} {Length} {Generalization} of {Arithmetic} {Transformers} {Using} {Task} {Structure}},
 urldate = {2024-11-02},
 year = {2024}
}

@misc{collins_-context_2024,
 abstract = {A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.},
 author = {Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
 file = {Collins et al. - 2024 - In-Context Learning with Transformers Softmax Att.pdf:/Users/ngolowich/Zotero/storage/Z2SRUFFC/Collins et al. - 2024 - In-Context Learning with Transformers Softmax Att.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2402.11639 [cs]},
 publisher = {arXiv},
 shorttitle = {In-{Context} {Learning} with {Transformers}},
 title = {In-{Context} {Learning} with {Transformers}: {Softmax} {Attention} {Adapts} to {Function} {Lipschitzness}},
 urldate = {2024-05-28},
 year = {2024}
}

@inproceedings{csordas_devil_2022,
 abstract = {Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model conﬁgurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on ﬁve popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50\% to 85\% on the PCFG productivity split, and from 35\% to 81\% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100\% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results1.},
 author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 file = {Csordás et al. - 2022 - The Devil is in the Detail Simple Tricks Improve .pdf:/Users/ngolowich/Zotero/storage/MJXXXAGX/Csordás et al. - 2022 - The Devil is in the Detail Simple Tricks Improve .pdf:application/pdf},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2108.12284 [cs]},
 publisher = {arXiv},
 shorttitle = {The {Devil} is in the {Detail}},
 title = {The {Devil} is in the {Detail}: {Simple} {Tricks} {Improve} {Systematic} {Generalization} of {Transformers}},
 urldate = {2024-06-06},
 year = {2021}
}

@inproceedings{csordas_neural_2022,
 abstract = {Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.},
 author = {Csordás, Róbert and Irie, Kazuki and Schmidhuber, Jürgen},
 booktitle = {International Conference on Learning Representations},
 file = {Csordás et al. - 2022 - The Neural Data Router Adaptive Control Flow in T.pdf:/Users/ngolowich/Zotero/storage/8PFRJSH7/Csordás et al. - 2022 - The Neural Data Router Adaptive Control Flow in T.pdf:application/pdf},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2110.07732 [cs]},
 publisher = {arXiv},
 shorttitle = {The {Neural} {Data} {Router}},
 title = {The {Neural} {Data} {Router}: {Adaptive} {Control} {Flow} in {Transformers} {Improves} {Systematic} {Generalization}},
 urldate = {2024-07-07},
 year = {2021}
}

@inproceedings{dai_transformer-xl_2019,
 abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a ﬁxed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without ﬁnetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorﬂow and PyTorch1.},
 address = {Florence, Italy},
 author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
 booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
 file = {Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a.pdf:/Users/ngolowich/Zotero/storage/2QSKM9VC/Dai et al. - 2019 - Transformer-XL Attentive Language Models beyond a.pdf:application/pdf},
 language = {en},
 pages = {2978--2988},
 publisher = {Association for Computational Linguistics},
 shorttitle = {Transformer-{XL}},
 title = {Transformer-{XL}: {Attentive} {Language} {Models} beyond a {Fixed}-{Length} {Context}},
 urldate = {2024-05-26},
 year = {2019}
}

@inproceedings{deletang_neural_2023,
 abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the ﬁeld. In this work, we conduct an extensive empirical study (20 910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-ofdistribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufﬁcient capacity to ﬁt the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
 author = {Delétang, Grégoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
 booktitle = {International Conference on Learning Representations},
 file = {Delétang et al. - 2023 - Neural Networks and the Chomsky Hierarchy.pdf:/Users/ngolowich/Zotero/storage/GSB3PEE7/Delétang et al. - 2023 - Neural Networks and the Chomsky Hierarchy.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Formal Languages and Automata Theory},
 language = {en},
 month = {February},
 note = {arXiv:2207.02098 [cs]},
 publisher = {arXiv},
 title = {Neural {Networks} and the {Chomsky} {Hierarchy}},
 urldate = {2024-07-07},
 year = {2022}
}

@inproceedings{deora_optimization_2023,
 abstract = {The training and generalization dynamics of the Transformer’s core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on singlehead attention. Inspired by the demonstrated benefits of overparameterization when training fullyconnected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.},
 author = {Deora, Puneesh and Ghaderi, Rouzbeh and Taheri, Hossein and Thrampoulidis, Christos},
 booktitle = {Trans. Mach. Learn. Res.},
 file = {Deora et al. - 2023 - On the Optimization and Generalization of Multi-he.pdf:/Users/ngolowich/Zotero/storage/W73JDPLM/Deora et al. - 2023 - On the Optimization and Generalization of Multi-he.pdf:application/pdf},
 keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2310.12680 [cs, math, stat]},
 publisher = {arXiv},
 title = {On the {Optimization} and {Generalization} of {Multi}-head {Attention}},
 urldate = {2024-05-28},
 year = {2023}
}

@inproceedings{dziri_faith_2023,
 abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks—multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations’ performance can rapidly decay with increased task complexity.},
 author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
 booktitle = {Neural Information Processing Systems},
 file = {Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:/Users/ngolowich/Zotero/storage/W4TDJEKS/Dziri et al. - 2023 - Faith and Fate Limits of Transformers on Composit.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2305.18654 [cs]},
 publisher = {arXiv},
 shorttitle = {Faith and {Fate}},
 title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
 urldate = {2024-05-25},
 year = {2023}
}

@misc{fan_looped_2024,
 abstract = {Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation—a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.},
 author = {Fan, Ying and Du, Yilun and Ramchandran, Kannan and Lee, Kangwook},
 file = {Fan et al. - 2024 - Looped Transformers for Length Generalization.pdf:/Users/ngolowich/Zotero/storage/MBWPDRAS/Fan et al. - 2024 - Looped Transformers for Length Generalization.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {September},
 note = {arXiv:2409.15647 [cs]},
 publisher = {arXiv},
 title = {Looped {Transformers} for {Length} {Generalization}},
 urldate = {2025-01-03},
 year = {2024}
}

@inproceedings{fu_what_2023,
 abstract = {Attention layers—which map a sequence of inputs to a sequence of outputs—are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a randomfeature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.},
 author = {Fu, Hengyu and Guo, Tianyu and Bai, Yu and Mei, Song},
 booktitle = {Neural Information Processing Systems},
 file = {Fu et al. - 2023 - What can a Single Attention Layer Learn A Study T.pdf:/Users/ngolowich/Zotero/storage/96G8V78H/Fu et al. - 2023 - What can a Single Attention Layer Learn A Study T.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Computer Science - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2307.11353 [cs, math, stat]},
 publisher = {arXiv},
 shorttitle = {What can a {Single} {Attention} {Layer} {Learn}?},
 title = {What can a {Single} {Attention} {Layer} {Learn}? {A} {Study} {Through} the {Random} {Features} {Lens}},
 urldate = {2024-05-26},
 year = {2023}
}

@article{hahn_theoretical_2020,
 abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of selfattention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of selfattention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, ﬁnding that it cannot model periodic ﬁnite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
 author = {Hahn, Michael},
 file = {Hahn - 2020 - Theoretical Limitations of Self-Attention in Neura.pdf:/Users/ngolowich/Zotero/storage/52YN9QAJ/Hahn - 2020 - Theoretical Limitations of Self-Attention in Neura.pdf:application/pdf},
 issn = {2307-387X},
 journal = {Transactions of the Association for Computational Linguistics},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Formal Languages and Automata Theory},
 language = {en},
 month = {December},
 note = {arXiv:1906.06755 [cs]},
 pages = {156--171},
 title = {Theoretical {Limitations} of {Self}-{Attention} in {Neural} {Sequence} {Models}},
 urldate = {2024-05-25},
 volume = {8},
 year = {2020}
}

@inproceedings{hahn_why_2024,
 abstract = {Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the inputspace sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding transformers’ inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.},
 author = {Hahn, Michael and Rofin, Mark},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 file = {Hahn and Rofin - 2024 - Why are Sensitive Functions Hard for Transformers.pdf:/Users/ngolowich/Zotero/storage/RGYGJVCT/Hahn and Rofin - 2024 - Why are Sensitive Functions Hard for Transformers.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2402.09963 [cs]},
 publisher = {arXiv},
 title = {Why are {Sensitive} {Functions} {Hard} for {Transformers}?},
 urldate = {2024-08-14},
 year = {2024}
}

@misc{hou_universal_2024,
 abstract = {Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models. While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose Turing Programs, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine. This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications. We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD. We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task. Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al. [53]) program that simulates an arbitrary Turing machine.},
 author = {Hou, Kaiying and Brandfonbrener, David and Kakade, Sham and Jelassi, Samy and Malach, Eran},
 file = {Hou et al. - 2024 - Universal Length Generalization with Turing Progra.pdf:/Users/ngolowich/Zotero/storage/LWJFIWDF/Hou et al. - 2024 - Universal Length Generalization with Turing Progra.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2407.03310 [cs]},
 publisher = {arXiv},
 title = {Universal {Length} {Generalization} with {Turing} {Programs}},
 urldate = {2024-07-07},
 year = {2024}
}

@misc{hsieh_ruler_2024,
 abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the “needle”) from long distractor texts (the “haystack”), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variatibons with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
 author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
 file = {Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-C.pdf:/Users/ngolowich/Zotero/storage/STJ2PAPW/Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-C.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language},
 language = {en},
 month = {August},
 note = {arXiv:2404.06654 [cs]},
 publisher = {arXiv},
 shorttitle = {{RULER}},
 title = {{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?},
 urldate = {2024-08-30},
 year = {2024}
}

@inproceedings{huang_-context_2023,
 abstract = {Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.},
 author = {Huang, Yu and Cheng, Yuan and Liang, Yingbin},
 booktitle = {International Conference on Machine Learning},
 file = {Huang et al. - 2023 - In-Context Convergence of Transformers.pdf:/Users/ngolowich/Zotero/storage/T8NB799P/Huang et al. - 2023 - In-Context Convergence of Transformers.pdf:application/pdf},
 keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2310.05249 [cs, math, stat]},
 publisher = {arXiv},
 title = {In-{Context} {Convergence} of {Transformers}},
 urldate = {2024-05-28},
 year = {2023}
}

@misc{huang_formal_2024,
 abstract = {A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers.},
 author = {Huang, Xinting and Yang, Andy and Bhattamishra, Satwik and Sarrof, Yash and Krebs, Andreas and Zhou, Hattie and Nakkiran, Preetum and Hahn, Michael},
 file = {Huang et al. - 2024 - A Formal Framework for Understanding Length Genera.pdf:/Users/ngolowich/Zotero/storage/XCQB4T8W/Huang et al. - 2024 - A Formal Framework for Understanding Length Genera.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2410.02140 [cs]},
 publisher = {arXiv},
 title = {A {Formal} {Framework} for {Understanding} {Length} {Generalization} in {Transformers}},
 urldate = {2024-10-11},
 year = {2024}
}

@inproceedings{hupkes_compositionality_2020,
 abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect diﬀerent interpretations of compositionality and translate them into ﬁve theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these ﬁve tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
 author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
 booktitle = {Journal of Artificial Intelligence Research},
 file = {Hupkes et al. - 2020 - Compositionality decomposed how do neural network.pdf:/Users/ngolowich/Zotero/storage/TYQ27XTK/Hupkes et al. - 2020 - Compositionality decomposed how do neural network.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:1908.08351 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {Compositionality decomposed},
 title = {Compositionality decomposed: how do neural networks generalise?},
 urldate = {2024-06-06},
 year = {2019}
}

@misc{jelassi_length_2023,
 abstract = {We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on 5-digit numbers can perform 15-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few (10 to 50) long sequences to the training set. We show that priming allows models trained on 5-digit × 3-digit multiplications to generalize to 35 × 3 examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.},
 author = {Jelassi, Samy and d'Ascoli, Stéphane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, François},
 file = {Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:/Users/ngolowich/Zotero/storage/PSVV87X8/Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2306.15400 [cs]},
 publisher = {arXiv},
 title = {Length {Generalization} in {Arithmetic} {Transformers}},
 urldate = {2024-05-25},
 year = {2023}
}

@misc{jelassi_length_2023-1,
 abstract = {We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on 5-digit numbers can perform 15-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few (10 to 50) long sequences to the training set. We show that priming allows models trained on 5-digit × 3-digit multiplications to generalize to 35 × 3 examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.},
 author = {Jelassi, Samy and d'Ascoli, Stéphane and Domingo-Enrich, Carles and Wu, Yuhuai and Li, Yuanzhi and Charton, François},
 file = {Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:/Users/ngolowich/Zotero/storage/GJCEB94K/Jelassi et al. - 2023 - Length Generalization in Arithmetic Transformers.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2306.15400 [cs]},
 publisher = {arXiv},
 title = {Length {Generalization} in {Arithmetic} {Transformers}},
 urldate = {2024-05-28},
 year = {2023}
}

@inproceedings{jelassi_repeat_2024,
 abstract = {Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as “generalized state space models” (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.},
 author = {Jelassi, Samy and Brandfonbrener, David and Kakade, Sham M. and Malach, Eran},
 booktitle = {International Conference on Machine Learning},
 file = {Jelassi et al. - 2024 - Repeat After Me Transformers are Better than Stat.pdf:/Users/ngolowich/Zotero/storage/GIN6MLMR/Jelassi et al. - 2024 - Repeat After Me Transformers are Better than Stat.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2402.01032 [cs]},
 publisher = {arXiv},
 shorttitle = {Repeat {After} {Me}},
 title = {Repeat {After} {Me}: {Transformers} are {Better} than {State} {Space} {Models} at {Copying}},
 urldate = {2024-05-27},
 year = {2024}
}

@misc{kalavasis_transfer_2024,
 abstract = {We study the fundamental problem of transfer learning where a learning algorithm collects data from some source distribution 𝑃 but needs to perform well with respect to a different target distribution 𝑄. A standard change of measure argument implies that transfer learning happens when the density ratio 𝑑𝑄\{\vphantom{\}}𝑑𝑃 is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet (COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where the ratio 𝑑𝑄\{\vphantom{\}}𝑑𝑃 is unbounded, but transfer learning is possible.},
 author = {Kalavasis, Alkis and Zadik, Ilias and Zampetakis, Manolis},
 file = {Kalavasis et al. - 2024 - Transfer Learning Beyond Bounded Density Ratios.pdf:/Users/ngolowich/Zotero/storage/23NSRF6Q/Kalavasis et al. - 2024 - Transfer Learning Beyond Bounded Density Ratios.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:2403.11963 [cs, math, stat]},
 publisher = {arXiv},
 title = {Transfer {Learning} {Beyond} {Bounded} {Density} {Ratios}},
 urldate = {2024-07-07},
 year = {2024}
}

@inproceedings{katharopoulos_transformers_nodate,
 abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
 author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
 booktitle = {International Conference on Machine Learning},
 file = {Katharopoulos et al. - Transformers are RNNs  Fast Autoregressive Transf.pdf:/Users/ngolowich/Zotero/storage/4GEYK35E/Katharopoulos et al. - Transformers are RNNs  Fast Autoregressive Transf.pdf:application/pdf},
 language = {en},
 title = {Transformers are {RNNs}:  {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
 year = {2020}
}

@inproceedings{kazemnejad_impact_2023,
 abstract = {Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5’s Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5’s Relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model’s performance. Overall, our work suggests that explicit position encodings are not essential for decoder-only Transformers to generalize well to longer sequences.},
 author = {Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
 booktitle = {Neural Information Processing Systems},
 file = {Kazemnejad et al. - 2023 - The Impact of Positional Encoding on Length Genera.pdf:/Users/ngolowich/Zotero/storage/YKMZ8TUD/Kazemnejad et al. - 2023 - The Impact of Positional Encoding on Length Genera.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2305.19466 [cs]},
 publisher = {arXiv},
 title = {The {Impact} of {Positional} {Encoding} on {Length} {Generalization} in {Transformers}},
 urldate = {2024-05-24},
 year = {2023}
}

@inproceedings{kong_new_2019,
 abstract = {We ask whether reinforcement learning can ﬁnd theoretically optimal algorithms for online optimization problems, and introduce a novel learning framework in this setting. To answer this question, we introduce a number of key ideas from traditional algorithms and complexity theory. Speciﬁcally, we introduce the concept of adversarial distributions (universal and high-entropy training sets), which are distributions that encourage the learner to ﬁnd algorithms that work well in the worst case. We test our new ideas on the AdWords problem, the online knapsack problem, and the secretary problem. Our results indicate that the models have learned behaviours that are consistent with the optimal algorithms for these problems derived using the online primal-dual framework.},
 author = {Kong, Weiwei and Liaw, Christopher and Mehta, Aranyak and Sivakumar, D},
 booktitle = {International Conference on Learning Representations},
 file = {Kong et al. - 2019 - A NEW DOG LEARNS OLD TRICKS RL FINDS CLASSIC OPTI.pdf:/Users/ngolowich/Zotero/storage/8EKNDJ24/Kong et al. - 2019 - A NEW DOG LEARNS OLD TRICKS RL FINDS CLASSIC OPTI.pdf:application/pdf},
 language = {en},
 title = {A {NEW} {DOG} {LEARNS} {OLD} {TRICKS}: {RL} {FINDS} {CLASSIC} {OPTIMIZATION} {ALGORITHMS}},
 year = {2018}
}

@inproceedings{lee_teaching_2023,
 abstract = {Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and model scale. Additionally, we discuss length generalization challenges. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction objective for rapidly eliciting arithmetic capabilities.},
 author = {Lee, Nayoung and Sreenivasan, Kartik and Lee, Jason D. and Lee, Kangwook and Papailiopoulos, Dimitris},
 booktitle = {ICLR},
 file = {Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:/Users/ngolowich/Zotero/storage/QJ8YFKCC/Lee et al. - 2023 - Teaching Arithmetic to Small Transformers.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2307.03381 [cs]},
 publisher = {arXiv},
 title = {Teaching {Arithmetic} to {Small} {Transformers}},
 urldate = {2024-05-25},
 year = {2024}
}

@inproceedings{li_context_nodate,
 abstract = {Context length expansion of transformer models is considered a key challenge, especially when handling context beyond the training length during inference stage. In this paper, we propose Generalized extrapolatioN scalE (GeNE), a straightforward and effective method applied to the interpolate function of positional embeddings to achieve training short, test long. Experimental results show that GeNE notably improves long context language modeling. By randomly scaling the extrapolation ratio during the finetuning, GeNE achieves stable extrapolation on 64k contexts by training on 16k length. Further, the instruction following Llama2 model based on GeNE achieved competitive results compared with other open-source models of the same parameter scale. Our code is available at https: //github.com/LhLi-QED/GeNE.},
 author = {Li, Linhan and Zhang, Huaping},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 file = {Li and Zhang - Context Length Extension via Generalized Extrapola.pdf:/Users/ngolowich/Zotero/storage/4TZLRZGK/Li and Zhang - Context Length Extension via Generalized Extrapola.pdf:application/pdf},
 language = {en},
 title = {Context {Length} {Extension} via {Generalized} {Extrapolation} {Scale}},
 year = {2024}
}

@inproceedings{li_functional_2024,
 abstract = {Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5’s RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.},
 author = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
 booktitle = {International Conference on Learning Representations},
 file = {Li et al. - 2024 - Functional Interpolation for Relative Positions Im.pdf:/Users/ngolowich/Zotero/storage/B7B4ANKG/Li et al. - 2024 - Functional Interpolation for Relative Positions Im.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:2310.04418 [cs]},
 publisher = {arXiv},
 title = {Functional {Interpolation} for {Relative} {Positions} {Improves} {Long} {Context} {Transformers}},
 urldate = {2024-05-25},
 year = {2024}
}

@inproceedings{li_transformers_2023,
 abstract = {In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-ﬂy. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We ﬁrst explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical veriﬁcation. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.},
 author = {Li, Yingcong and Ildiz, M. Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
 booktitle = {International Conference on Machine Learning},
 file = {Li et al. - 2023 - Transformers as Algorithms Generalization and Sta.pdf:/Users/ngolowich/Zotero/storage/EVB2K5FZ/Li et al. - 2023 - Transformers as Algorithms Generalization and Sta.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2301.07067 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {Transformers as {Algorithms}},
 title = {Transformers as {Algorithms}: {Generalization} and {Stability} in {In}-context {Learning}},
 urldate = {2024-05-28},
 year = {2023}
}

@inproceedings{liu_exposing_2023,
 abstract = {Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture’s inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.},
 author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
 booktitle = {Neural Information Processing Systems},
 file = {Liu et al. - 2023 - Exposing Attention Glitches with Flip-Flop Languag.pdf:/Users/ngolowich/Zotero/storage/6UWZNPSP/Liu et al. - 2023 - Exposing Attention Glitches with Flip-Flop Languag.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2306.00946 [cs]},
 publisher = {arXiv},
 title = {Exposing {Attention} {Glitches} with {Flip}-{Flop} {Language} {Modeling}},
 urldate = {2024-05-28},
 year = {2023}
}

@inproceedings{liu_transformers_2023,
 abstract = {Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We show that a low-depth Transformer can represent the computations of any ﬁnite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with o(T ) layers can exactly replicate the computation of an automaton on an input sequence of length T . We ﬁnd that polynomial-sized O(log T )-depth solutions always exist; furthermore, O(1)-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirically, we ﬁnd that Transformers converge to shortcut solutions with standard training, across a wide variety of automata. We further investigate the brittleness of these solutions and propose potential mitigations.},
 author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
 booktitle = {International Conference on Learning Representations},
 file = {Liu et al. - 2023 - Transformers Learn Shortcuts to Automata.pdf:/Users/ngolowich/Zotero/storage/MXJR7YAD/Liu et al. - 2023 - Transformers Learn Shortcuts to Automata.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Formal Languages and Automata Theory},
 language = {en},
 month = {May},
 note = {arXiv:2210.10749 [cs, stat]},
 publisher = {arXiv},
 title = {Transformers {Learn} {Shortcuts} to {Automata}},
 urldate = {2024-08-31},
 year = {2022}
}

@article{lu_controlled_nodate,
 abstract = {Broad textual understanding and in-context learning require language models that utilize full document contexts. Due to the implementation challenges associated with directly training long-context models, many methods have been proposed for extending models to handle long contexts. However, owing to differences in data and model classes, it has been challenging to compare these approaches, leading to uncertainty as to how to evaluate long-context performance and whether it differs from standard evaluation. We implement a controlled protocol for extension methods with a standardized evaluation, utilizing consistent base models and extension data. Our study yields several insights into long-context behavior. First, we reaffirm the critical role of perplexity as a general-purpose performance indicator even in longer-context tasks. Second, we find that current approximate attention methods systematically underperform across long-context tasks. Finally, we confirm that exact fine-tuning based methods are generally effective within their extension range, whereas extrapolation remains challenging. All codebases, models, and checkpoints are made available open-source via https://github.com/Leooyii/LCEG, promoting transparency and facilitating further research in this critical area of AI development.},
 author = {Lu, Yi and Yan, Jing Nathan and Yang, Songlin and Chiu, Justin T and Ren, Siyu and Yuan, Fei and Zhao, Wenting and Wu, Zhiyong and Rush, Alexander M},
 file = {Lu et al. - A Controlled Study on Long Context Extension and G.pdf:/Users/ngolowich/Zotero/storage/C7TS6NUM/Lu et al. - A Controlled Study on Long Context Extension and G.pdf:application/pdf},
 language = {en},
 title = {A {Controlled} {Study} on {Long} {Context} {Extension} and {Generalization} in {LLMs}}
}

@misc{marsden_provable_2024,
 abstract = {We consider the problem of length generalization in sequence prediction. We define a new metric of performance in this setting – the Asymmetric-Regret– which measures regret against a benchmark predictor with longer context length than available to the learner. We continue by studying this concept through the lens of the spectral filtering algorithm. We present a gradient-based learning algorithm that provably achieves length generalization for linear dynamical systems. We conclude with proof-of-concept experiments which are consistent with our theory.},
 author = {Marsden, Annie and Dogariu, Evan and Agarwal, Naman and Chen, Xinyi and Suo, Daniel and Hazan, Elad},
 file = {Marsden et al. - 2024 - Provable Length Generalization in Sequence Predict.pdf:/Users/ngolowich/Zotero/storage/VPK3NTIN/Marsden et al. - 2024 - Provable Length Generalization in Sequence Predict.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2411.01035 [cs]},
 publisher = {arXiv},
 title = {Provable {Length} {Generalization} in {Sequence} {Prediction} via {Spectral} {Filtering}},
 urldate = {2024-11-05},
 year = {2024}
}

@misc{marsden_provable_2024-1,
 abstract = {We consider the problem of length generalization in sequence prediction. We define a new metric of performance in this setting – the Asymmetric-Regret– which measures regret against a benchmark predictor with longer context length than available to the learner. We continue by studying this concept through the lens of the spectral filtering algorithm. We present a gradient-based learning algorithm that provably achieves length generalization for linear dynamical systems. We conclude with proof-of-concept experiments which are consistent with our theory.},
 author = {Marsden, Annie and Dogariu, Evan and Agarwal, Naman and Chen, Xinyi and Suo, Daniel and Hazan, Elad},
 file = {Marsden et al. - 2024 - Provable Length Generalization in Sequence Predict.pdf:/Users/ngolowich/Zotero/storage/N3SPQJXA/Marsden et al. - 2024 - Provable Length Generalization in Sequence Predict.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2411.01035 [cs]},
 publisher = {arXiv},
 title = {Provable {Length} {Generalization} in {Sequence} {Prediction} via {Spectral} {Filtering}},
 urldate = {2024-11-05},
 year = {2024}
}

@misc{mcleish_transformers_2024,
 abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.},
 author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
 file = {McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:/Users/ngolowich/Zotero/storage/QQLUI3P4/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2405.17399 [cs]},
 publisher = {arXiv},
 title = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
 urldate = {2024-05-29},
 year = {2024}
}

@misc{mcleish_transformers_2024-1,
 abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.},
 author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
 file = {McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:/Users/ngolowich/Zotero/storage/GF3X9XCW/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2405.17399 [cs]},
 publisher = {arXiv},
 title = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
 urldate = {2024-08-18},
 year = {2024}
}

@misc{mcleish_transformers_2024-2,
 abstract = {The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.},
 author = {McLeish, Sean and Bansal, Arpit and Stein, Alex and Jain, Neel and Kirchenbauer, John and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Geiping, Jonas and Schwarzschild, Avi and Goldstein, Tom},
 file = {McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:/Users/ngolowich/Zotero/storage/B7AZA6H8/McLeish et al. - 2024 - Transformers Can Do Arithmetic with the Right Embe.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2405.17399 [cs]},
 publisher = {arXiv},
 title = {Transformers {Can} {Do} {Arithmetic} with the {Right} {Embeddings}},
 urldate = {2024-11-02},
 year = {2024}
}

@inproceedings{merrill_parallelism_2023,
 abstract = {Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L = P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.},
 author = {Merrill, William and Sabharwal, Ashish},
 booktitle = {Transactions of the Association for Computational Linguistics},
 file = {Merrill and Sabharwal - 2023 - The Parallelism Tradeoff Limitations of Log-Preci.pdf:/Users/ngolowich/Zotero/storage/MLV9RBGZ/Merrill and Sabharwal - 2023 - The Parallelism Tradeoff Limitations of Log-Preci.pdf:application/pdf},
 keywords = {Computer Science - Computational Complexity, Computer Science - Computation and Language},
 language = {en},
 month = {April},
 note = {arXiv:2207.00729 [cs]},
 publisher = {arXiv},
 shorttitle = {The {Parallelism} {Tradeoff}},
 title = {The {Parallelism} {Tradeoff}: {Limitations} of {Log}-{Precision} {Transformers}},
 urldate = {2024-05-25},
 year = {2022}
}

@inproceedings{nagarajan_understanding_2021,
 abstract = {Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way \{{\textbackslash}em even\} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.},
 author = {Nagarajan, Vaishnavh and Andreassen, Anders and Neyshabur, Behnam},
 booktitle = {International Conference on Learning Representations},
 file = {Nagarajan et al. - 2021 - Understanding the Failure Modes of Out-of-Distribu.pdf:/Users/ngolowich/Zotero/storage/6KTR6L26/Nagarajan et al. - 2021 - Understanding the Failure Modes of Out-of-Distribu.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
 language = {en},
 month = {April},
 note = {arXiv:2010.15775 [cs, stat]},
 publisher = {arXiv},
 title = {Understanding the {Failure} {Modes} of {Out}-of-{Distribution} {Generalization}},
 urldate = {2024-05-25},
 year = {2020}
}

@article{nguyen_understanding_nodate,
 abstract = {Transformer based large-language models (LLMs) display extreme proﬁciency with language yet a precise understanding of how they work remains elusive. One way of demystifying transformer predictions would be to describe how they depend on their context in terms of simple template functions. This paper takes a ﬁrst step in this direction by considering families of functions (i.e. rules) formed out of simple N -gram based statistics of the training data. By studying how well these rulesets approximate transformer predictions, we obtain a variety of novel discoveries: a simple method to detect overﬁtting during training without using a holdout set, a quantitative measure of how transformers progress from learning simple to more complex statistical rules over the course of training, a model-variance criterion governing when transformer predictions tend to be described by N -gram rules, and insights into how well transformers can be approximated by N -gram rulesets in the limit where these rulesets become increasingly complex. In this latter direction, we ﬁnd that for 78\% of LLM next-token distributions on TinyStories, their top-1 predictions agree with those provided by our N -gram rulesets.},
 author = {Nguyen, Timothy},
 file = {Nguyen - Understanding Transformers via N -gram Statistics.pdf:/Users/ngolowich/Zotero/storage/5HRBCRZ7/Nguyen - Understanding Transformers via N -gram Statistics.pdf:application/pdf},
 language = {en},
 title = {Understanding {Transformers} via {N} -gram {Statistics}}
}

@misc{nye_show_2021,
 abstract = {Large pre-trained language models perform remarkably well on tasks that can be done “in one pass”, such as generating realistic text (Brown et al., 2020) or synthesizing computer programs (Chen et al., 2021; Austin et al., 2021). However, they struggle with tasks that require unbounded multi-step computation, such as adding integers (Brown et al., 2020) or executing programs (Austin et al., 2021). Surprisingly, we ﬁnd that these same models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations. In particular, we train Transformers to perform multi-step computations by asking them to emit intermediate computation steps into a “scratchpad”. On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
 author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
 file = {Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:/Users/ngolowich/Zotero/storage/8VH32FNT/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:application/pdf},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2112.00114 [cs]},
 publisher = {arXiv},
 shorttitle = {Show {Your} {Work}},
 title = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
 urldate = {2024-11-04},
 year = {2021}
}

@article{olsson_-context_nodate,
 author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
 file = {Olsson et al. - IInn-dcuocntitoenxtHLeeaadrsning and.pdf:/Users/ngolowich/Zotero/storage/D9JUBZMD/Olsson et al. - IInn-dcuocntitoenxtHLeeaadrsning and.pdf:application/pdf},
 language = {en},
 title = {In-context learning and induction heads}
}

@misc{olsson_incontext_2022,
 archiveprefix = {arXiv},
 author = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
 eprint = {2209.11895},
 primaryclass = {cs.LG},
 title = {In-context Learning and Induction Heads},
 year = {2022}
}

@inproceedings{ontanon_making_2022,
 abstract = {Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions signiﬁcantly impact compositional generalization. We identiﬁed Transformer conﬁgurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).},
 address = {Dublin, Ireland},
 author = {Ontanon, Santiago and Ainslie, Joshua and Fisher, Zachary and Cvicek, Vaclav},
 booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
 file = {Ontanon et al. - 2022 - Making Transformers Solve Compositional Tasks.pdf:/Users/ngolowich/Zotero/storage/WARXUM3F/Ontanon et al. - 2022 - Making Transformers Solve Compositional Tasks.pdf:application/pdf},
 language = {en},
 pages = {3591--3607},
 publisher = {Association for Computational Linguistics},
 title = {Making {Transformers} {Solve} {Compositional} {Tasks}},
 urldate = {2024-05-24},
 year = {2022}
}

@inproceedings{peng_yarn_2023,
 abstract = {Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn.},
 author = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
 booktitle = {International Conference on Learning Representations},
 file = {Peng et al. - 2023 - YaRN Efficient Context Window Extension of Large .pdf:/Users/ngolowich/Zotero/storage/J8HA3VP6/Peng et al. - 2023 - YaRN Efficient Context Window Extension of Large .pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2309.00071 [cs]},
 publisher = {arXiv},
 shorttitle = {{YaRN}},
 title = {{YaRN}: {Efficient} {Context} {Window} {Extension} of {Large} {Language} {Models}},
 urldate = {2024-08-31},
 year = {2023}
}

@misc{phuong_formal_2022,
 abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (not results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
 author = {Phuong, Mary and Hutter, Marcus},
 file = {Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf:/Users/ngolowich/Zotero/storage/SXDT6EAI/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf:application/pdf},
 keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2207.09238 [cs]},
 publisher = {arXiv},
 title = {Formal {Algorithms} for {Transformers}},
 urldate = {2024-03-21},
 year = {2022}
}

@inproceedings{press_train_2022,
 abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
 author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
 booktitle = {International Conference on Learning Representations},
 file = {Press et al. - 2022 - Train Short, Test Long Attention with Linear Bias.pdf:/Users/ngolowich/Zotero/storage/FDFPARXP/Press et al. - 2022 - Train Short, Test Long Attention with Linear Bias.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language},
 language = {en},
 month = {April},
 note = {arXiv:2108.12409 [cs]},
 publisher = {arXiv},
 shorttitle = {Train {Short}, {Test} {Long}},
 title = {Train {Short}, {Test} {Long}: {Attention} with {Linear} {Biases} {Enables} {Input} {Length} {Extrapolation}},
 urldate = {2024-05-23},
 year = {2021}
}

@inproceedings{raffel_exploring_2023,
 abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
 author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
 booktitle = {Journal of machine learning research},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/GUBR7RT6/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/JFZLQYWQ/1910.html:text/html},
 keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
 month = {September},
 note = {arXiv:1910.10683 [cs, stat]},
 publisher = {arXiv},
 title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
 urldate = {2024-05-24},
 year = {2019}
}

@inproceedings{ruoss_randomized_2023,
 abstract = {Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0\% on average).},
 author = {Ruoss, Anian and Delétang, Grégoire and Genewein, Tim and Grau-Moya, Jordi and Csordás, Róbert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 file = {Ruoss et al. - 2023 - Randomized Positional Encodings Boost Length Gener.pdf:/Users/ngolowich/Zotero/storage/SQIBR58T/Ruoss et al. - 2023 - Randomized Positional Encodings Boost Length Gener.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {May},
 note = {arXiv:2305.16843 [cs, stat]},
 publisher = {arXiv},
 title = {Randomized {Positional} {Encodings} {Boost} {Length} {Generalization} of {Transformers}},
 urldate = {2024-05-25},
 year = {2023}
}

@misc{sabbaghi_explicitly_2024,
 abstract = {Despite the success of Transformers on language understanding, code generation, and logical reasoning, they still fail to generalize over length on basic arithmetic tasks such as addition and multiplication. A major reason behind this failure is the vast difference in structure between numbers and text; For example, the numbers are typically parsed from right to left, and there is a correspondence between digits at the same position across different numbers. In contrast, for text, such symmetries are quite unnatural. In this work, we propose to encode these semantics explicitly into the model via modified number formatting and custom positional encodings. Empirically, our method allows a Transformer trained on numbers with at most 5-digits for addition and multiplication to generalize up to 50-digit numbers, without using additional data for longer sequences. We further demonstrate that traditional absolute positional encodings (APE) fail to generalize to longer sequences, even when trained with augmented data that captures task symmetries. To elucidate the importance of explicitly encoding structure, we prove that explicit incorporation of structure via positional encodings is necessary for out-of-distribution generalization. Finally, we pinpoint other challenges inherent to length generalization beyond capturing symmetries, in particular complexity of the underlying task, and propose changes in the training distribution to address them.},
 author = {Sabbaghi, Mahdi and Pappas, George and Hassani, Hamed and Goel, Surbhi},
 file = {Sabbaghi et al. - 2024 - Explicitly Encoding Structural Symmetry is Key to .pdf:/Users/ngolowich/Zotero/storage/DSYFG74D/Sabbaghi et al. - 2024 - Explicitly Encoding Structural Symmetry is Key to .pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2406.01895 [cs]},
 publisher = {arXiv},
 title = {Explicitly {Encoding} {Structural} {Symmetry} is {Key} to {Length} {Generalization} in {Arithmetic} {Tasks}},
 urldate = {2025-01-03},
 year = {2024}
}

@misc{sanford_one-layer_2024,
 abstract = {A simple communication complexity argument proves that no one-layer transformer can solve the induction heads task unless its size is exponentially larger than the size suﬃcient for a two-layer transformer.},
 author = {Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
 file = {Sanford et al. - 2024 - One-layer transformers fail to solve the induction.pdf:/Users/ngolowich/Zotero/storage/7LT4W2G5/Sanford et al. - 2024 - One-layer transformers fail to solve the induction.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {August},
 note = {arXiv:2408.14332 [cs, stat]},
 publisher = {arXiv},
 title = {One-layer transformers fail to solve the induction heads task},
 urldate = {2024-08-27},
 year = {2024}
}

@inproceedings{sanford_representational_2023,
 abstract = {Attention layers, as commonly used in transformers, form the backbone of modern deep learning, yet there is no mathematical description of their benefits and deficiencies as compared with other architectures. In this work we establish both positive and negative results on the representation power of attention layers, with a focus on intrinsic complexity parameters such as width, depth, and embedding dimension. On the positive side, we present a sparse averaging task, where recurrent networks and feedforward networks all have complexity scaling polynomially in the input size, whereas transformers scale merely logarithmically in the input size; furthermore, we use the same construction to show the necessity and role of a large embedding dimension in a transformer. On the negative side, we present a triple detection task, where attention layers in turn have complexity scaling linearly in the input size; as this scenario seems rare in practice, we also present natural variants that can be efficiently solved by attention layers. The proof techniques emphasize the value of communication complexity in the analysis of transformers and related models, and the role of sparse averaging as a prototypical attention task, which even finds use in the analysis of triple detection.},
 author = {Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
 booktitle = {Neural Information Processing Systems},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/HJL2YKXF/Sanford et al. - 2023 - Representational Strengths and Limitations of Tran.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/JXKGKJEK/2306.html:text/html},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 month = {November},
 note = {arXiv:2306.02896 [cs, stat]},
 publisher = {arXiv},
 title = {Representational {Strengths} and {Limitations} of {Transformers}},
 urldate = {2024-05-25},
 year = {2023}
}

@misc{shen_positional_2023,
 abstract = {Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities –which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).},
 author = {Shen, Ruoqi and Bubeck, Sébastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
 file = {Shen et al. - 2023 - Positional Description Matters for Transformers Ar.pdf:/Users/ngolowich/Zotero/storage/S4ISED3A/Shen et al. - 2023 - Positional Description Matters for Transformers Ar.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2311.14737 [cs]},
 publisher = {arXiv},
 title = {Positional {Description} {Matters} for {Transformers} {Arithmetic}},
 urldate = {2024-05-25},
 year = {2023}
}

@inproceedings{sinha_curious_2022,
 abstract = {Transformer language models encode the notion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that matters, but relative position, and the extent to which APEs can capture this type of information has not been investigated. In this work, we observe that models trained with APE over-rely on positional information to the point that they breakdown when subjected to sentences with shifted position information. Speciﬁcally, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero- to full-shot tasks, across a range of model families and model sizes. Our ﬁndings raise questions about the efﬁcacy of APEs to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models.},
 author = {Sinha, Koustuv and Kazemnejad, Amirhossein and Reddy, Siva and Pineau, Joelle and Hupkes, Dieuwke and Williams, Adina},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 file = {Sinha et al. - 2022 - The Curious Case of Absolute Position Embeddings.pdf:/Users/ngolowich/Zotero/storage/KFRQCCNH/Sinha et al. - 2022 - The Curious Case of Absolute Position Embeddings.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2210.12574 [cs]},
 publisher = {arXiv},
 title = {The {Curious} {Case} of {Absolute} {Position} {Embeddings}},
 urldate = {2024-05-24},
 year = {2022}
}

@inproceedings{su_roformer_2023,
 abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
 author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
 booktitle = {Neurocomputing},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/YD9NL9DP/Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/MYR48UT9/2104.html:text/html},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 month = {November},
 note = {arXiv:2104.09864 [cs]},
 publisher = {arXiv},
 shorttitle = {{RoFormer}},
 title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
 urldate = {2024-05-23},
 year = {2021}
}

@inproceedings{syrgkanis_efficient_2016,
 abstract = {We provide the ﬁrst oracle efﬁcient sublinear regret algorithms for adversarial versions of the contextual bandit problem. In this problem, the learner repeatedly makes an action on the basis of a context and receives reward for the chosen action, with the goal of achieving reward competitive with a large class of policies. We analyze two settings: i) in the transductive setting the learner knows the set of contexts a priori, ii) in the small separator setting, there exists a small set of contexts such that any two policies behave differently in one of the contexts in the set. Our algorithms fall into the follow the perturbed leader family (Kalai \& Vempala, 2005) and achieve regret O(T 3/4 K log(N )) in the transductive setting and O(T 2/3d3/4K log(N )) in the separator setting, where K is the number of actions, N is the number of baseline policies, and d is the size of the separator. We actually solve the more general adversarial contextual semi-bandit linear optimization problem, whilst in the full information setting we address the even more general contextual combinatorial optimization. We provide several extensions and implications of our algorithms, such as switching regret and efﬁcient learning with predictable sequences.},
 author = {Syrgkanis, Vasilis and Krishnamurthy, Akshay and Schapire, Robert E.},
 booktitle = {International Conference on Machine Learning},
 file = {Syrgkanis et al. - 2016 - Efficient Algorithms for Adversarial Contextual Le.pdf:/Users/ngolowich/Zotero/storage/WJV72YEC/Syrgkanis et al. - 2016 - Efficient Algorithms for Adversarial Contextual Le.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:1602.02454 [cs]},
 publisher = {arXiv},
 title = {Efficient {Algorithms} for {Adversarial} {Contextual} {Learning}},
 urldate = {2024-06-14},
 year = {2016}
}

@misc{tarzanagh_transformers_2024,
 abstract = {Since its inception in “Attention Is All You Need”, the transformer architecture has led to revolutionary advancements in natural language processing. The attention layer within the transformer admits a sequence of input tokens X and makes them interact through pairwise similarities computed as softmax(XQK⊤ X⊤), where (K, Q) are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent, as follows. (1) Optimizing the attention layer, parameterized by (K, Q), with vanishing regularization, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter W := KQ⊤. Instead, directly parameterizing by W minimizes a Frobenius norm SVM objective. We characterize this convergence, highlighting that it can occur in locally-optimal directions rather than global ones. (2) Complementing this, for W-parameterization, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias of 1-layer transformers with nonlinear heads/MLPs. Our findings apply to general datasets, trivially extend to cross-attention layer, and their practical validity is verified via thorough numerical experiments. We also introduce open problems and future research directions. We believe these findings inspire a new perspective, interpreting multilayer transformers as a hierarchy of SVMs that separates and selects optimal tokens.},
 author = {Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
 file = {Tarzanagh et al. - 2024 - Transformers as Support Vector Machines.pdf:/Users/ngolowich/Zotero/storage/PQAMAJKF/Tarzanagh et al. - 2024 - Transformers as Support Vector Machines.pdf:application/pdf},
 keywords = {Mathematics - Optimization and Control, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2308.16898 [cs, math]},
 publisher = {arXiv},
 title = {Transformers as {Support} {Vector} {Machines}},
 urldate = {2024-05-28},
 year = {2024}
}

@inproceedings{tian_joma_2024,
 abstract = {We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions from previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. The code is at1.},
 author = {Tian, Yuandong and Wang, Yiping and Zhang, Zhenyu and Chen, Beidi and Du, Simon},
 booktitle = {International Conference on Learning Representations},
 file = {Tian et al. - 2024 - JoMA Demystifying Multilayer Transformers via JOi.pdf:/Users/ngolowich/Zotero/storage/VGEX5CAS/Tian et al. - 2024 - JoMA Demystifying Multilayer Transformers via JOi.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {March},
 note = {arXiv:2310.00535 [cs]},
 publisher = {arXiv},
 shorttitle = {{JoMA}},
 title = {{JoMA}: {Demystifying} {Multilayer} {Transformers} via {JOint} {Dynamics} of {MLP} and {Attention}},
 urldate = {2024-05-28},
 year = {2023}
}

@inproceedings{trauger_sequence_2023,
 abstract = {This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.},
 author = {Trauger, Jacob and Tewari, Ambuj},
 booktitle = {International Conference on Artificial Intelligence and Statistics},
 file = {Trauger and Tewari - 2023 - Sequence Length Independent Norm-Based Generalizat.pdf:/Users/ngolowich/Zotero/storage/36THZFDX/Trauger and Tewari - 2023 - Sequence Length Independent Norm-Based Generalizat.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2310.13088 [cs, stat]},
 publisher = {arXiv},
 title = {Sequence {Length} {Independent} {Norm}-{Based} {Generalization} {Bounds} for {Transformers}},
 urldate = {2024-06-18},
 year = {2023}
}

@inproceedings{vardi_implicit_2022,
 abstract = {Gradient-based deep-learning algorithms exhibit remarkable performance in practice, but it is not well-understood why they are able to generalize despite having more parameters than training examples. It is believed that implicit bias is a key factor in their ability to generalize, and hence it was widely studied in recent years. In this short survey, we explain the notion of implicit bias, review main results and discuss their implications.},
 author = {Vardi, Gal},
 booktitle = {Communications of the ACM},
 file = {Vardi - 2022 - On the Implicit Bias in Deep-Learning Algorithms.pdf:/Users/ngolowich/Zotero/storage/QVZD46C6/Vardi - 2022 - On the Implicit Bias in Deep-Learning Algorithms.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
 language = {en},
 month = {November},
 note = {arXiv:2208.12591 [cs, stat]},
 publisher = {arXiv},
 title = {On the {Implicit} {Bias} in {Deep}-{Learning} {Algorithms}},
 urldate = {2024-05-26},
 year = {2022}
}

@inproceedings{vaswani_attention_2023,
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
 booktitle = {Neural Information Processing Systems},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/MUKGMR6L/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/D4P5684W/1706.html:text/html},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 month = {August},
 note = {arXiv:1706.03762 [cs]},
 publisher = {arXiv},
 title = {Attention {Is} {All} {You} {Need}},
 urldate = {2024-05-23},
 year = {2017}
}

@inproceedings{wang_transformers_2024,
 abstract = {The transformer architecture has prevailed in various deep learning settings due to its exceptional capabilities to select and compose structural information. Motivated by these capabilities, Sanford et al. [48] proposed the sparse token selection task, in which transformers excel while fully-connected networks (FCNs) fail in the worst case. Building upon that, we strengthen the FCN lower bound to an average-case setting and establish an algorithmic separation of transformers over FCNs. Specifically, a one-layer transformer trained with gradient descent provably learns the sparse token selection task and, surprisingly, exhibits strong out-ofdistribution length generalization. We provide empirical simulations to justify our theoretical findings.},
 author = {Wang, Zixuan and Wei, Stanley and Hsu, Daniel and Lee, Jason D.},
 booktitle = {International Conference on Machine Learning},
 file = {Wang et al. - 2024 - Transformers Provably Learn Sparse Token Selection.pdf:/Users/ngolowich/Zotero/storage/QHMAVHTT/Wang et al. - 2024 - Transformers Provably Learn Sparse Token Selection.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Machine Learning},
 language = {en},
 month = {June},
 note = {arXiv:2406.06893 [cs, math, stat]},
 publisher = {arXiv},
 title = {Transformers {Provably} {Learn} {Sparse} {Token} {Selection} {While} {Fully}-{Connected} {Nets} {Cannot}},
 urldate = {2024-06-18},
 year = {2024}
}

@inproceedings{weiss_thinking_2021,
 abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in ﬁnite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder—attention and feed-forward computation—into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difﬁculty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
 author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
 booktitle = {International Conference on Machine Learning},
 file = {Weiss et al. - 2021 - Thinking Like Transformers.pdf:/Users/ngolowich/Zotero/storage/5RBZML8F/Weiss et al. - 2021 - Thinking Like Transformers.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {July},
 note = {arXiv:2106.06981 [cs]},
 publisher = {arXiv},
 title = {Thinking {Like} {Transformers}},
 urldate = {2024-05-25},
 year = {2021}
}

@misc{wu_never_2024,
 abstract = {Recently, many methods have been developed to extend the context length of pre-trained large language models (LLMs), but they often require fine-tuning at the target length (≫ 4K) and struggle to effectively utilize information from the middle part of the context. To address these issues, we propose ContinuityRelativity indExing with gAussian Middle (CREAM), which interpolates positional encodings by manipulating position indices. Apart from being simple, CREAM is training-efficient: it only requires fine-tuning at the pre-trained context window (e.g., Llama 2-4K) and can extend LLMs to a much longer target context length (e.g., 256K). To ensure that the model focuses more on the information in the middle, we introduce a truncated Gaussian to encourage sampling from the middle part of the context during fine-tuning, thus alleviating the “Lost-in-the-Middle” problem faced by long-context LLMs. Experimental results show that CREAM successfully extends LLMs to the target length for both Base and Chat versions of Llama2-7B with “Never Miss A Beat”. Our code will be publicly available soon.},
 author = {Wu, Tong and Zhao, Yanpeng and Zheng, Zilong},
 file = {Wu et al. - 2024 - Never Miss A Beat An Efficient Recipe for Context.pdf:/Users/ngolowich/Zotero/storage/VYAQJRKA/Wu et al. - 2024 - Never Miss A Beat An Efficient Recipe for Context.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language},
 language = {en},
 month = {June},
 note = {arXiv:2406.07138 [cs]},
 publisher = {arXiv},
 shorttitle = {Never {Miss} {A} {Beat}},
 title = {Never {Miss} {A} {Beat}: {An} {Efficient} {Recipe} for {Context} {Window} {Extension} of {Large} {Language} {Models} with {Consistent} "{Middle}" {Enhancement}},
 urldate = {2024-08-18},
 year = {2024}
}

@misc{xiao_theory_2024,
 abstract = {Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on reasoning problems of smaller lengths or sizes, the resulting model struggles with problems of larger sizes or lengths. Although LG has been studied by many researchers, the challenge remains. This paper proposes a theoretical study of LG for problems whose reasoning processes can be modeled as DAGs (directed acyclic graphs). The paper first identifies and proves the conditions under which LG can be achieved in learning to reason. It then designs problem representations based on the theory to learn to solve challenging reasoning problems like parity, addition, and multiplication, using a Transformer to achieve perfect LG.},
 author = {Xiao, Changnan and Liu, Bing},
 file = {Xiao and Liu - 2024 - A Theory for Length Generalization in Learning to .pdf:/Users/ngolowich/Zotero/storage/TB7LFCIT/Xiao and Liu - 2024 - A Theory for Length Generalization in Learning to .pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence},
 language = {en},
 month = {March},
 note = {arXiv:2404.00560 [cs]},
 publisher = {arXiv},
 title = {A {Theory} for {Length} {Generalization} in {Learning} to {Reason}},
 urldate = {2024-05-25},
 year = {2024}
}

@inproceedings{yao_self-attention_2023,
 abstract = {Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as \${\textbackslash}mathsf\{Dyck\}\_k\$, the language consisting of well-nested parentheses of \$k\$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process \${\textbackslash}mathsf\{Dyck\}\_\{k, D\}\$, the subset of \${\textbackslash}mathsf\{Dyck\}\_\{k\}\$ with depth bounded by \$D\$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with \$D+1\$ layers and \$O({\textbackslash}log k)\$ memory size (per token per layer) that recognizes \${\textbackslash}mathsf\{Dyck\}\_\{k, D\}\$, and a soft-attention network with two layers and \$O({\textbackslash}log k)\$ memory size that generates \${\textbackslash}mathsf\{Dyck\}\_\{k, D\}\$. Experiments show that self-attention networks trained on \${\textbackslash}mathsf\{Dyck\}\_\{k, D\}\$ generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.},
 author = {Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
 booktitle = {Annual Meeting of the Association for Computational Linguistics},
 file = {Yao et al. - 2023 - Self-Attention Networks Can Process Bounded Hierar.pdf:/Users/ngolowich/Zotero/storage/MSVCJC79/Yao et al. - 2023 - Self-Attention Networks Can Process Bounded Hierar.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory},
 language = {en},
 month = {March},
 note = {arXiv:2105.11115 [cs]},
 publisher = {arXiv},
 title = {Self-{Attention} {Networks} {Can} {Process} {Bounded} {Hierarchical} {Languages}},
 urldate = {2024-05-25},
 year = {2021}
}

@inproceedings{yun_are_2020,
 abstract = {Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that ﬁxed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to selfattention layers and empirically evaluate them.},
 author = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
 booktitle = {International Conference on Learning Representations},
 file = {Yun et al. - 2020 - ARE TRANSFORMERS UNIVERSAL APPROXIMATORS OF SEQUEN.pdf:/Users/ngolowich/Zotero/storage/XYB52ZRQ/Yun et al. - 2020 - ARE TRANSFORMERS UNIVERSAL APPROXIMATORS OF SEQUEN.pdf:application/pdf},
 language = {en},
 title = {{ARE} {TRANSFORMERS} {UNIVERSAL} {APPROXIMATORS} {OF} {SEQUENCE}-{TO}-{SEQUENCE} {FUNCTIONS}?},
 year = {2019}
}

@misc{zhang_unveiling_2023,
 abstract = {We propose a synthetic reasoning task, LEGO (Learning Equality and Group Operations), that encapsulates the problem of following a chain of reasoning, and we study how the Transformer architectures learn this task. We pay special attention to data eﬀects such as pretraining (on seemingly unrelated NLP tasks) and dataset composition (e.g., diﬀering chain length at training and test time), as well as architectural variants such as weight-tied layers or adding convolutional components. We study how the trained models eventually succeed at the task, and in particular, we manage to understand some of the attention heads as well as how the information ﬂows in the network. In particular, we have identiﬁed a novel association pattern that globally attends only to identical tokens. Based on these observations we propose a hypothesis that here pretraining helps for LEGO tasks due to certain structured attention patterns, and we experimentally verify this hypothesis. We also observe that in some data regime the trained transformer ﬁnds “shortcut” solutions to follow the chain of reasoning, which impedes the model’s robustness, and moreover we propose ways to prevent it. Motivated by our ﬁndings on structured attention patterns, we propose the LEGO attention module, a drop-in replacement for vanilla attention heads. This architectural change signiﬁcantly reduces Flops and maintains or even improves the model’s performance at large-scale pretraining.},
 author = {Zhang, Yi and Backurs, Arturs and Bubeck, Sébastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
 file = {Zhang et al. - 2023 - Unveiling Transformers with LEGO a synthetic reas.pdf:/Users/ngolowich/Zotero/storage/Z6SSAEJY/Zhang et al. - 2023 - Unveiling Transformers with LEGO a synthetic reas.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2206.04301 [cs]},
 publisher = {arXiv},
 shorttitle = {Unveiling {Transformers} with {LEGO}},
 title = {Unveiling {Transformers} with {LEGO}: a synthetic reasoning task},
 urldate = {2024-07-07},
 year = {2023}
}

@inproceedings{zhao_length_2024,
 abstract = {Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position methods. Finally, several challenges and future directions in this area are highlighted. Through this survey, We aim to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research.},
 author = {Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Xu, Dongliang and Yang, Qing and Liu, Hongtao and Qin, Bing and Liu, Ting},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 file = {arXiv Fulltext PDF:/Users/ngolowich/Zotero/storage/T2MB93AQ/Zhao et al. - 2024 - Length Extrapolation of Transformers A Survey fro.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/99IRVC9F/2312.html:text/html},
 keywords = {Computer Science - Computation and Language},
 month = {April},
 note = {arXiv:2312.17044 [cs]},
 publisher = {arXiv},
 shorttitle = {Length {Extrapolation} of {Transformers}},
 title = {Length {Extrapolation} of {Transformers}: {A} {Survey} from the {Perspective} of {Positional} {Encoding}},
 urldate = {2024-05-24},
 year = {2023}
}

@inproceedings{zhao_length_2024-1,
 abstract = {Transformer has taken the field of natural language processing (NLP) by storm since its birth. Further, Large language models (LLMs) built upon it have captured worldwide attention due to its superior abilities. Nevertheless, all Transformer-based models including these powerful LLMs suffer from a preset length limit and can hardly generalize from short training sequences to longer inference ones, namely, they can not perform length extrapolation. Hence, a plethora of methods have been proposed to enhance length extrapolation of Transformer, in which the positional encoding (PE) is recognized as the major factor. In this survey, we present these advances towards length extrapolation in a unified notation from the perspective of PE. Specifically, we first introduce extrapolatable PEs, including absolute and relative PEs. Then, we dive into extrapolation methods based on them, covering position interpolation and randomized position methods. Finally, several challenges and future directions in this area are highlighted. Through this survey, We aim to enable the reader to gain a deep understanding of existing methods and provide stimuli for future research.},
 author = {Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Xu, Dongliang and Yang, Qing and Liu, Hongtao and Qin, Bing and Liu, Ting},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 file = {Zhao et al. - 2024 - Length Extrapolation of Transformers A Survey fro.pdf:/Users/ngolowich/Zotero/storage/TFH3BMGL/Zhao et al. - 2024 - Length Extrapolation of Transformers A Survey fro.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language},
 language = {en},
 month = {April},
 note = {arXiv:2312.17044 [cs]},
 publisher = {arXiv},
 shorttitle = {Length {Extrapolation} of {Transformers}},
 title = {Length {Extrapolation} of {Transformers}: {A} {Survey} from the {Perspective} of {Positional} {Encoding}},
 urldate = {2024-05-26},
 year = {2023}
}

@misc{zhou_transformers_2024,
 abstract = {Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.},
 author = {Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
 file = {Zhou et al. - 2024 - Transformers Can Achieve Length Generalization But.pdf:/Users/ngolowich/Zotero/storage/FUKZ4976/Zhou et al. - 2024 - Transformers Can Achieve Length Generalization But.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2402.09371 [cs]},
 publisher = {arXiv},
 title = {Transformers {Can} {Achieve} {Length} {Generalization} {But} {Not} {Robustly}},
 urldate = {2024-05-25},
 year = {2024}
}

@inproceedings{zhou_what_2023,
 abstract = {Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers’ abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021)— a programming language designed for the computational model of a Transformer—and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the “min-degree-interpolator” model of learning from Abbe et al. (2023) does not correctly predict Transformers’ out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.},
 author = {Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
 booktitle = {International Conference on Learning Representations},
 file = {Zhou et al. - 2023 - What Algorithms can Transformers Learn A Study in.pdf:/Users/ngolowich/Zotero/storage/6GHE32UL/Zhou et al. - 2023 - What Algorithms can Transformers Learn A Study in.pdf:application/pdf},
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {October},
 note = {arXiv:2310.16028 [cs, stat]},
 publisher = {arXiv},
 shorttitle = {What {Algorithms} can {Transformers} {Learn}?},
 title = {What {Algorithms} can {Transformers} {Learn}? {A} {Study} in {Length} {Generalization}},
 urldate = {2024-05-24},
 year = {2023}
}

@inproceedings{zhu_pose_2024,
 abstract = {Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Fulllength fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.},
 author = {Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
 booktitle = {International Conference on Learning Representations},
 file = {Zhu et al. - 2024 - PoSE Efficient Context Window Extension of LLMs v.pdf:/Users/ngolowich/Zotero/storage/LR97EIJC/Zhu et al. - 2024 - PoSE Efficient Context Window Extension of LLMs v.pdf:application/pdf},
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
 language = {en},
 month = {February},
 note = {arXiv:2309.10400 [cs]},
 publisher = {arXiv},
 shorttitle = {{PoSE}},
 title = {{PoSE}: {Efficient} {Context} {Window} {Extension} of {LLMs} via {Positional} {Skip}-wise {Training}},
 urldate = {2024-08-18},
 year = {2023}
}
