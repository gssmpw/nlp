\paragraph{Formal setup for results.} Let $\hshort(\bX_i \mid \bX_{1:i-1})$ denote the probability $\hshort$ assigns to token $\bX_i$ given the context $\bX_{1:i-1}$. For each test example $\bX = (\bX_1, \ldots, \bX_{\bar L})$, we first computed a set of $k=5$ ``influential'' tokens for predicting $\bX_{\bar L}$, chosen amongst the first $\bar L - L$ tokens, by finding the $k$ values of $j \in [\bar L - L-1]$ which minimize
\begin{align}
\hshort(\bX_{\bar L} \mid \bX_{1:j-1}, \bX_{j+1:\bar L - 1})\label{eq:choose-j},
\end{align}
i.e., where token $\bX_j$ is masked out in all attention computations. Let $J_1, \ldots, J_k$ denote these $k$ tokens.\footnote{This is essentially equivalent to the ``leave-one-out'' baseline in \citet{cohen_contextcite_2024}.}
We then compute the following three negative log-likelihoods:
\begin{align}
  \ML_{\mathsf{short}} :=& -\log \hshort(\bX_{\bar L} \mid \bX_{\bar L - L:\bar L - 1})\nonumber\\
  \ML_{\mathsf{long}} :=& -\log \hshort(\bX_{\bar L} \mid \bX_{1:\bar L - 1})\nonumber\\
  \ML_{\mathsf{short,sparse}} :=& -\log \hshort(\bX_{\bar L} \mid \bX_{J_{1:k}}, \bX_{\bar L - L: \bar L - 1})\nonumber,
\end{align}
where to compute $\ML_{\mathsf{short,sparse}}$ we mask out all tokens in the first $\bar L - L$ tokens except those at positions $J_1, \ldots, J_k$. In words, $\ML_{\mathsf{long}}, \ML_{\mathsf{short}}, \ML_{\mathsf{short,sparse}}$ denote the cross-entropy losses for predicting $\bX_{\bar L}$ when the (a) full context $\bX_{1:\bar L-1}$ is used, (b) the $L$ most recent tokens $\bX_{\bar L - L: \bar L - 1}$ are used, and (c) the $L$ most recent tokens as well as $\bX_{J_{1:k}}$ are used, respectively. We illustrate the quantities $\ML_{\mathsf{short}}, \ML_{\mathsf{long}}, \ML_{\mathsf{short,sparse}}$ for a particular choice of $L, \bar L, J_{1:k}$ in \cref{fig:lshortlong}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{tikz-diagram.pdf}
  \caption{Illustration of the computation of $\ML_{\mathsf{short}}, \ML_{\mathsf{long}}, \ML_{\mathsf{short,sparse}}$ with $\bar L = 8, L = 2$, $k = 2$, $J_1 = 2, J_2 = 4$. Tokens shaded gray are \emph{masked} (i.e., \emph{not} attended to) while those shaded blue are \emph{not masked} (i.e., are attended to).}
  \label{fig:lshortlong}
\end{figure}

\begin{table}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Train Context Length (Model)} & \textbf{Eval Length = 64} & \textbf{Eval Length = 128} \\ \midrule
64 ($\hshort$)                      & 17.59                          & 14.74                            \\
128    ($\hlong$)                  & 17.62                          & 14.49                            \\ \bottomrule
\end{tabular}
\caption{Perplexities for (a) model trained on context length of 64 (top); and (b) model trained on context length of 128 (bottom).}
\label{tab:training-ppl}
\end{table}
  


\paragraph{Results \& discussion.} \arxiv{\cref{tab:training-ppl} confirms that the perplexity  of $\hshort$ on a context window of $128$ is less than that on a context window of $64$, verifying that $\hshort$ is able to utilize information beyond the shorter length-64 training context window.} In \cref{fig:olmo-sparse}, we plotted the pairs $(\ML_{\mathsf{short}} - \ML_{\mathsf{long}}, \ML_{\mathsf{short}} - \ML_{\mathsf{short,sparse}})$ for each example $\bX$. %
As can be seen, the quantity $\ML_{\mathsf{short}} - \ML_{\mathsf{long}}$, which may be interpreted as the \emph{amount the model $\hshort$ uses tokens $\bX_{1:\bar L - L - 1}$ to improve the prediction of $\bX_{\bar L}$}, is roughly equal (up to noise) to $\ML_{\mathsf{short}} - \ML_{\mathsf{short,sparse}}$, which may be interpreted as the \emph{amount the model uses tokens $\bX_{J_1}, \ldots, \bX_{J_k}$ to improve the prediction of $\bX_{\bar L}$}. This provides evidence for the hypothesis that the model's prediction of the $\bar L$th token is sparse in its dependence on tokens ``far in the past'', and is thus consistent with \ref{it:ta-sparsity} which predicts that such sparsity allows the model $\hshort$ to length-generalize. %



It is natural to wonder whether the behavior seen in \cref{fig:olmo-sparse} occurs even absent the use of methods such as PoSE to extend the context length. Accordingly, we also trained a model $\hlong$ in the same manner as $\hshort$ but with context length $\bar L = 128$ (and without PoSE). 
In \cref{fig:olmo-sparse-appendix}, we observe that a similar positive correlation is seen (a) for the model $\hlong$ trained on full contexts of length $\bar L$ (\cref{fig:hlong-hlong}), as well as for (b) the model $\hlong$ trained on contexts of length only $L$ but where the indices $J_{1:k}$ are chosen as in \cref{eq:choose-j} with respect to $\hshort$ (as opposed to $\hshort$; \cref{fig:hlong-hshort}), and conversely, the model $\hshort$ when the indices $J_{1:k}$ are chosen as in \cref{eq:choose-j} with respect to $\hlong$ (\cref{fig:hshort-hlong}). These observations suggest that the property that a small number $k$ of tokens at positions in $[\bar L - L - 1]$ can nearly recover the cross-entropy loss at position $\bar L$ obtained by training on \emph{all} of the tokens at positions in $[\bar L - L - 1]$ may be more of a property of the \emph{data distribution}, as opposed to a particularity of any particular language model variant such as $\hshort, \hlong$. In particular, it indicates that some property like \cref{def:sparse-planted} indeed governs the structure of the task of predicting the next token on long contexts.

  \begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
      \centering
      \includegraphics[width=\textwidth]{plots/eval_store_actual_128_0/attention_ce_diff_evaluatorc4_methodloo_ablate_pos126_shift64.pdf}
      \caption{{$\hlong$ CE loss; $J_{1:k}$ chosen from $\hlong$.}}
      \label{fig:hlong-hlong}
      \end{subfigure}
      \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/eval_loaded_actual_128_2/attention_ce_diff_evaluatorc4_methodloo_ablate_pos126_shift64.pdf}
        \caption{{$\hlong$ CE loss;  $J_{1:k}$ chosen from $\hshort$.}}
        \label{fig:hlong-hshort}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
          \centering
          \includegraphics[width=\textwidth]{plots/eval_loaded_finetuned_64_128_3/attention_ce_diff_evaluatorc4_methodloo_ablate_pos126_shift64.pdf}
          \caption{ $\hshort$ CE loss; $J_{1:k}$ chosen from $\hlong$.}
          \label{fig:hshort-hlong}
        \end{subfigure}
        
        \caption{Modification of the plot of \cref{fig:olmo-sparse} where cross-entropy of $\hlong$ is used instead (\cref{fig:hlong-hlong,fig:hlong-hshort}), and where the opposite model from the one being evaluated is used to generate the indices $J_{1:k}$ (\cref{fig:hlong-hshort,fig:hshort-hlong}).}
    \label{fig:olmo-sparse-appendix}
  \end{figure*}


