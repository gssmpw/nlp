\subsection{Positional Skip-Wise (PoSE) Training}
\label{sec:prelim-pose}
The \emph{Positional skip-wise (PoSE)} technique \cite{zhu_pose_2024} (see also \cite{wu_never_2024}) aims to ensure that: %
(a) the position IDs used during training cover all posible  position IDs $1, \ldots, \Lmax$ that could be observed at test-time (where $\Lmax$ is the maximum length of a test-time sequence), and (b) the \emph{differences} between different position IDs seen in training sequences is of similar magnitude to that seen in testing sequences. To do so, we fix an integer $c$ denoting a number of \emph{chunks}, and given a sequence of tokens $\bX = (\bX_1, \ldots, \bX_\ell)$ during training, we partition $\bX$ into $c$ contiguous chunks (i.e., subsequences) and assign to each chunk a random contiguous sequence of position IDs so that the first position ID of each chunk is greater than the last position ID of the previous chunk. %
{At test  time, one simply uses the true position IDs, namely $(1, 2, \ldots, L)$, corresponding to a sequence $\bX$ of length $L$. }

The precise schemes to partition into chunks and assign position IDs that we use are as follows: in all of our experiments we take the number of chunks to be $c = 2$: during training, we split a sequence $\bX = (\bX_1, \ldots, \bX_\ell)$ into two parts by choosing a uniformly random position to split at. We then choose 2 integers $J_0, J_1 \sim \Unif([\bar L - \ell])$, and let the position ID of the first chunk begin at $\min\{J_0, J_1 \}$ and the position ID of the second chunk begin at $\max\{J_0, J_1 \} + \ell_1$, where $\ell_1$ is the length of the first chunk. This is essentially the same as the scheme used in \citet{zhu_pose_2024} with all $v_i = 0$. %

