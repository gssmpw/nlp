\section{Related work}
\label{sec:related-work}

\subsection{Theoretical guarantees for length generalization}
Sukhbaatar et al., "When Does Spurious Generalization Occur?"__ Gilmer et al. propose that transformers will length-generalize at tasks which can be solved using a short RASP program ____. Theorem 7 of Vaswani et al. ____ assumes that the transformer is chosen so as to minimize a certain regularizer tailored specifically for length generalization, and their result is asymptotic in nature (in contrast to ours).
Li et al. show that using  gradient flow on a 1-layer linear transformer model with relative position embeddings on a simple linear regression task will converge to a model which length-generalizes, whereas the use of absolute position embeddings will fail to length-generalize. Chen et al. ____ show that a model class defined as a simple abstraction of attention heads that sums up pairwise dependencies of tokens experiences length generalization. %
Unlike our theoretical results, those of   Sperber et al. ____ are asymptotic in nature and cannot capture the (most common) case of a \emph{softmax} attention head. Moreover, they proceed, roughly speaking, by showing that the learned model is equal to the ground-truth model \emph{on the entire domain}, which is generally \emph{not} the case with actual language models ____.
Moreover, all of the preceding works generally apply only to specific classes of \emph{transformers or linear classes}; in contrast, our framework, while being able to capture an attention head, is significantly more general in that we allow arbitrary, potentially nonlinear, function classes (see \cref{def:sparse-group-attn}), thus meaning our results may have significance for other (non-transformer) classes of models as well. 

Vaswani et al. proposes to use \emph{Turing programs}, a scratchpad strategy inspired by Turing machines, to achieve length generalization on an array of tasks. Their  theoretical results, however, are only representational in nature, showing that transformers can represent Turing programs without  accounting for what models algorithms such as risk minimization will actually learn. Sperber et al. offer reasons why transformers struggle to length-generalize on parity, based on their sensitivity bias. 

Further from our own work, Chen et al. obtain provable guarantees for length generalization in the context of dynamical systems. Wang et al. ____ prove that gradient descent on transformers can provably learn a task known as the \emph{sparse token selection task} ____, which bears resemblance to \cref{def:sparse-planted} (indeed, a slight modification of the task defined in Section 2.1 of Liu et al. ____ is in fact a special case of a distribution with sparse planted correlations (\cref{def:sparse-planted})). 




\subsection{Modifying positional embeddings for length \generalization}
\label{sec:related-posids}
In addition to position coupling Shaw et al., "Self-Attention with Relative Position Representations" ____ (and the closely related Abacus embeddings  Vaswani et al., "Do Attention Heads Matter?" ____), which is a focus in our paper, several other techniques have been developed to modify position embeddings during training and/or inference time to improve the length generalization performance of transformers. Guo et al. ____ developed the positional skip-wise technique (PoSE; see \cref{sec:prelim-pose}), which was later refined by modifying the distribution of position IDs used at training time in Wang et al. ____ We remark that PoSE is conceptually similar to \emph{randomized position embeddings} Guo et al., "Randomized Position Embeddings" _____, which trains using a random set of position IDs from the test-length context window (with no guarantee on the contiguity of these IDs, unlike PoSE).

Another popular strategy to extend the context length of transformers which has seen traction for models at larger scales, such as as Code Llama ____ is \emph{position interpolation}. This technique \emph{scales down} (``interpolates'') the position IDs in the longer test-length context window to match the length of the shorter training-length context. Several such interpolation strategies have been proposed, including the canonical choice of  Li et al., "Bart-Large: Pretraining a Large Scale Language Model" ____ as well as  Wang et al., "NTK-RoPE" _____,  Zhang et al., "YaRN" _____, and  Yang et al., "CLEX" _____; the latter strategies adjust the amount of interpolation done on a per-frequency basis, with different RoPE frequencies receiving different interpolation scales. One downside of these position interpolation strategies is that they generally require some fine-tuning on the longer test-length sequences in order to effectively use the longer context windows (e.g., to achieve decreased perplexity on longer sequences than those seen during training). Such fine-tuning complicates the theoretical setting of length generalization, where it is typically assumed that any amount of training on the longer test-length sequences is not allowed. We remark, however, that these position interpolation techniques can be combined with PoSE ____; exploring such combinations in the context of our experiments is an interesting direction for future work.

Finally, we remark that some strategies, such as  Guo et al., "LM-Infinite" ____ and  Zhang et al., "Self-Extend" ____, have been proposed to adjust the attention mechanism at inference time so as to achieve length generalization without any fine-tuning, though their performance lags somewhat ____.


\subsection{Empirical evaluations \& explanations of length generalization}
A number of papers have offered empirical evaluations and comparisons of various techniques for length generalization. Wang et al., "Can Spurious Generalization be Avoided?" ____ studied a few simple length generalization tasks, such as parity and variable assignment, and found that techniques such as finetuning and using a scratchpad did not lead to much length generalization. 
Liu et al., "An Empirical Study of Length Generalization for Transformers" ____ compared the performance of various positional encoding techniques for length generalization, and found that NoPE performed best (though their analysis did not account for techniques such as position interpolation and PoSE which can significantly improve length generalization for, e.g., RoPE).  Zhang et al. ____ observed that length generalization is difficult for transformers on arithmetic tasks. 
Finally, Wang et al., "A Comparative Study of Length Generalization Techniques" ____ performed a systematic study comparing various approaches to extend the context length of LLMs, including various types of position interpolation. 


Wang et al., "Compositional Generalization and Out-of-Distribution Performance of Transformers" ____ study the out-of-distribution performance of transformers by focusing on \emph{compositional generalization}, which refers to the ability of transformers to compose individual tasks found in the training data. There is also extensive work more broadly on out-of-distribution generalization ____.

Finally, a number of works (e.g.,  Liu et al., "A Benchmark for Evaluating Length Generalization and Compositional Generalization" ____ introduce new benchmarks and datasets for studying length generalization and more broadly the performance of LLMs on long contexts. 




\paragraph{Additional techniques for length generalization.} Many papers have introduced new types of positional encoding schemes with the hope of improved length generalization, including relative positional embeddings Vaswani et al., "Do Attention Heads Matter?" ____ as well as  Liu et al., "ALiBi" ____,  Zhang et al., "Hard-Alibi" ____, and  Yang et al., "FIRE" _____. 


Many other types of techniques have  been proposed, such as priming the training dataset with a few long sequences ____%
modifying the format of the input ____ such as by using a scratchpad ____ architectural changes ____ 
and combining several such techniques _____. Moreover, it was recently shown that length generalization can be achieved via a \emph{self-improvement} framework  Wang et al., "Self-Improving Transformers for Length Generalization" ____, which trains the model on sequences of increasing length, which are labeled by previous versions of the model.  We remark that such techniques lie outside the scope of this paper, as we require that the context length be bounded above by $L$ throughout training. Moreover, such techniques still may suffer from the computational issues that plague longer context lengths.