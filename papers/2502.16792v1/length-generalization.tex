\documentclass{article}
\usepackage{etoolbox}
\newcommand{\arxiv}[1]{\iftoggle{icml}{}{#1}}
\newcommand{\icml}[1]{\iftoggle{icml}{#1}{}}
\newtoggle{icml}
\global\togglefalse{icml}

\icml{\usepackage{icml2025}}
\usepackage{graphicx}	
\usepackage{amssymb}
\usepackage{amsfonts,latexsym,amsthm,amssymb,amsmath,amscd,euscript}
\usepackage{bbm}
\arxiv{\usepackage{framed}}
\arxiv{\usepackage{fullpage}}
\arxiv{\usepackage[round,authoryear]{natbib}}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[export]{adjustbox} %
\usepackage{subcaption}
\usepackage{booktabs}
\icml{\usepackage[subtle]{savetrees}}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=blue,
  citecolor = black,      
  urlcolor=cyan,
}
\usepackage{accents}
\usepackage{setspace}
\hypersetup{colorlinks=true,citecolor=blue,urlcolor=black,linkbordercolor={1 0 0}}
\usepackage{tikz-cd}
\newcount\Comments  %
\Comments=0

\ifnum\Comments=1
\usepackage[inline]{showlabels}
\renewcommand{\showlabelfont}{\small\color{blue}}
\fi
\usepackage{turnstile}
\usepackage{mathpartir}
\usepackage{tikz}
\usepackage{xspace}

\arxiv{
\usepackage{algorithm}
\usepackage{verbatim}
\usepackage[noend]{algpseudocode}
}

\captionsetup{font=small}

\input{commands}
\nc{\db}[1]{\ifnum\Comments=1 {\color{red} [DB: #1]}\fi}


\arxiv{
  \title{The Role of Sparsity for Length \Generalization in Transformers}
  \date{\today}
  \author{Noah Golowich \\ {\small \texttt{nzg@mit.edu}} \\ {\small MIT EECS} \and Samy Jelassi \\ {\small \texttt{sjelassi@fas.harvard.edu}} \\ {\small Harvard University} \and David Brandfonbrener \\ {\small \texttt{david.brandfonbrener@gmail.com}} \\ {\small Kempner Institute at Harvard University} \and Sham M.~Kakade \\ {\small \texttt{sham@seas.harvard.edu}} \\ {\small Kempner Institute at Harvard University} \and Eran Malach \\ {\small \texttt{eran.malach@gmail.com}}  \\ {\small Kempner Institute at Harvard University}}
}
\icml{
\icmltitlerunning{Length \Generalization in Transformers}
  }
\begin{document}
\arxiv{\maketitle}
\icml{
  \twocolumn[
  \icmltitle{The Role of Sparsity for Length \Generalization in LLMs}
  \icmlkeywords{Length \Generalization, Transformers}
  ]
  }
\begin{abstract}
  Training large language models to predict beyond their training context lengths has drawn much attention in recent years, yet the principles driving such behavior of \emph{length generalization} remain underexplored. We propose a new theoretical framework to study length generalization for the next-token prediction task, as performed by decoder-only transformers. Conceptually, we show that length generalization occurs as long as each predicted token depends on a small (fixed) number of previous tokens.\arxiv{ We formalize such tasks via a notion we call \emph{$k$-sparse planted correlation} distributions, and show that an idealized model of transformers which generalize attention heads successfully length-generalize on such tasks. As a bonus, }
    Our theoretical model justifies certain techniques to modify positional embeddings which have been introduced to improve length generalization, such as \emph{position coupling}.\arxiv{

  } We support our theoretical results with experiments on synthetic tasks and natural language, which confirm that a key factor driving length generalization is a ``sparse'' dependency structure of each token on the previous ones. Inspired by our theory, we introduce \emph{Predictive Position Coupling}, which trains the transformer to \emph{predict} the position IDs used in a positional coupling approach. Predictive Position Coupling thereby allows us to broaden the array of tasks to which position coupling can successfully be applied to achieve length generalization.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Enabling large language models (LLMs) to generalize to contexts longer than their training context length has emerged as a key problem in recent years. %
Indeed, many factors limit the context length of sequences which can be used during training, including the increased \emph{computational cost} of training on long sequences \cite{tay_efficient_2022} as well as the fact that longer sequences may be less numerous in the training \emph{dataset}. Nevertheless, many applications require LLMs to be accurate on extremely long context lengths at inference time: for instance, a popular technique recently has been to use \emph{scratchpads} or \emph{Chain-of-Thought (CoT)} to perform various logic and reasoning tasks, and the length of the scratchpad can become very large, especially when combined with search or reinforcement learning techniques \cite{deepseekai_deepseek_2025,kimiteam_kimi_2025}.

Unfortunately, transformers struggle to \emph{length generalize} on even very simple arithmetic and logic tasks, such as computing parities, integer addition, and variable assignment \cite{anil_exploring_2022,kazemnejad_impact_2023}. Indeed, only recently have transformer models successfuly been trained to length-generalize to contexts many times their training length on integer addition tasks, using a technique known as \emph{position coupling} (closely related to \emph{Abacus embeddings}) \cite{cho_position_2024,cho_arithmetic_2024,mcleish_transformers_2024}. For many other simple problems, comparable length generalization remains a challenge. In light of this uneven progress, a natural question is whether there is a more principled way of understanding length generalization. \emph{In particular, can we formally reason about what structural properties of data aid or inhibit length generalization? Moreover, can this suggest architectural modifications, such as to positional embeddings, which improve length generalization?}

\icml{\vspace{-0.2cm}}
\paragraph{Contributions.} In this paper we give a positive answer to the above questions: First, we introduce a class of data distributions, namely those with \emph{sparse planted correlations} (\cref{def:sparse-planted}), which, roughly speaking, captures the property observed in many synthetic and natural language tasks that each token to be predicted depends on a small number $k$ of previous tokens, which we call the \emph{sparsity}. We then establish the following: 
\begin{itemize}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.25cm,rightmargin=0cm]}%
\item As long as the sparsity $k$ does not grow with the sequence length, then a simple class of models generalizing attention heads (namely, \emph{sparse functional attention}; \cref{def:sparse-group-attn}) has provable length generalization (\cref{thm:length-extrap}). This result also relies on an additional assumption of \emph{locality} on the hypothesis class. 
\item While the locality assumption is typically violated in practice, we show that (a theoretical abstraction of) \emph{position coupling} can remove the locality requirement (\cref{prop:pc-length-extrap-informal}), thus offering a theoretical justification for this recent technique.
\item We perform experiments (\cref{sec:experiments}) on  \emph{synthetic} and \emph{natural language} data to support our theorical conclusions: for instance, our experiments adjust the sparsity of several synthetic tasks, and we observe that length generalization improves monotonically with decreasing sparsity. For natural language data, we provide evidence that length-generalizing transformers indeed make accurate predictions using a small number of past tokens. 
\item Inspired by our theory, we introduce a modification of positional coupling, \emph{\predpc}, which, unlike positional coupling, works on tasks for which the coupled position IDs are input-dependent. We show (\cref{sec:cot-pc}) that \predpc enables significant length generalization on such tasks. 
\end{itemize}

\paragraph{Related work.} A few prior works have attempted to give theoretical justifications for the length generalization abilities of transformers \cite{zhou_what_2023,huang_formal_2024,sabbaghi_explicitly_2024,ahuja_provable_2024}; we compare these works, as well as many others, to ours in \cref{sec:related-work}. 
The idea that \emph{sparsity} and \emph{locality} are important for length generalization has appeared in several of these works \cite{huang_formal_2024,sabbaghi_explicitly_2024}. However, none has provided the precise theoretical and empirical evidence which we believe are essential for isolating the importance of these notions. 

\section{Preliminaries}
Our focus in this paper is on decoder-only transformers trained to predict the next token. We review the architecture of such transformers in more detail in \cref{sec:transformers-prelim}. In this section, we discuss a few aspects of the \emph{position encodings} of transformers which have previously been proposed to improve length \generalization.

Initially, \citet{vaswani_attention_2023} proposed to introduce positional information in a transformer model using \emph{absolute positional embeddings (APE)} (see also \citet{gehring_convolutional_2017}). The APE assigns to each token a \emph{position ID} $i$ (typically the position of that token in the sequence) and adds an embedding vector $\vape_i$ depending on $i$ to the embedding vector for the corresponding token. %
Recently, it has become more common to use position encodings which encode \emph{relative positions}. In particular, a popular positional encoding technique in many large-scale open-source transformers is the \emph{rotary positional encoding (RoPE)} \cite{su_roformer_2023}, which adjusts the computation of attention scores as follows: it multiplies each key embedding vector with position ID $i$ by a rotation matrix depending on $i$, and each query embedding vector with position ID $j$ by a rotation matrix depending on $j$. The effect of these rotations is that the attention score for a $(\text{key}, \text{query})$ pair with position IDs $(i,j)$ depends only on $i-j$ (and not $i$ or $j$ individually). 

An obstacle to length \generalization in transformers is that the joint distribution of particular tokens and their position IDs seen in training sequences may not match that seen in the longer sequences at test-time. To account for this discrepancy, a common empirical technique is to modify the position IDs at training or test time. As discussed in \cref{sec:introduction}, the \emph{positional coupling} technique, which has recently paved the way for major improvements in length generalization in arithmetic and logic tasks, plays a key role in our theoretical and empirical results. \icml{We discuss it next}\arxiv{We discuss it as well as another tecnique which will play a role in our experiments, \emph{PoSE}, next}; additional techniques to modify position IDs to achieve length generalization are discussed in \cref{sec:related-posids}. 



\subsection{Position Coupling}
\label{sec:prelim-pc}
The technique of \emph{position coupling} \cite{cho_position_2024,cho_arithmetic_2024,mcleish_transformers_2024} (similar to \emph{Abacus} in \citet{mcleish_transformers_2024})  works specifically for problems with structured input where there is a clear one-to-one relationship between certain tokens. In particular, it assigns each token in a sequence a particular position ID in a way so that tokens assigned the same position ID should have a (task-dependent) ``one-to-one correspondence''. %
For example, to solve \emph{string reversal}, i.e., predict the last $L$ tokens of the sequence\arxiv{\footnote{We use $\SEP$ to denote a special separator token.}}
\arxiv{\begin{align}
\bX_1, \ldots, \bX_L, \SEP, \bX_L, \ldots, \bX_1 \label{eq:string-reversal}
       \end{align}}
     \icml{$\bX_1, \ldots, \bX_L, \SEP, \bX_L, \ldots, \bX_1$,}
\arxiv{for some token IDs $\bX_1, \ldots, \bX_L$, }
then since the $i$th-to-last reversed token is equal to the $i$th input  token, we feed the following position IDs:\arxiv{\footnote{The separator token $\SEP$ receives a default position ID of $0$.}}
\arxiv{
\begin{align}
1, 2, \ldots, L, 0, L, \ldots, 2, 1 \label{eq:string-reversal-posids}.
\end{align}}
\icml{$1, 2, \ldots, L, 0, L, \ldots, 2, 1$.}
\arxiv{We provide the precise position coupling schemes for our experiments with synthetic data in \cref{sec:cot-pc}.}

\arxiv{\input{pose-overview.tex}}



\section{Theoretical model}
\label{sec:model}
\arxiv{\paragraph{Overview.}} In this section, we formally define our theoretical model. Our primary inspiration is the class of decoder-only transformers, which are trained to predict each next token as a function of the preceding ones in a given sequence. %
Accordingly, our theoretical framework will focus on the \emph{next-token prediction task}, where we fix lengths $L < \bar L$, and attempt to show that models \emph{trained} on sequences of length $\leq L$ to predict a \emph{single token} (interpreted as the $(L+1)$th token in the sequence) also succeed at predicting a \emph{single token} (interpreted as the $(\bar L+1)$th token) when \emph{tested} on sequences of length $\bar L$. 
\arxiv{To do so, we will have to make two types of assumptions.  First, in \cref{sec:sparse-structure} we make the following assumption on the distributions of the sequences: roughly speaking, there is always a short subsequence which is sufficient for predicting the token in question. Second, in \cref{sec:hypothesis-classes}, we assume that the learning algorithm is performing risk minimization with respect to a certain hypothesis class generalizing the class of attention heads. }%

\arxiv{\paragraph{Basic setup.}} Fix a set $\MV$ which denotes the set of tokens. $\MV^\st = \bigcup_{\ell \geq 0} \MV^\ell$ denotes the set of all arbitrary-length sequences of tokens.  For technical reasons discussed below, we model the problem of predicting the \emph{embedding} (i.e., \emph{representation}) of the next token, as opposed to the token itself, given a sequence of previous tokens. Accordingly, we fix a label set $\MY$ which is a convex subset of $\BR^d$ for some $d \in \BN$ denoting the space of possible token embeddings, and a loss function $\Loss : \MY \times \MY \to [0,1]$ with $\Loss(Y, Y) = 0$ for all $Y \in \MY$. %
For simplicity, we fix an arbitrary norm $\| \cdot \|$ on $\BR^d$, and assume that  $\mathrm{diam}(\MY) \leq 1$ with respect to this norm and that $\Loss(Y, Y') = \| Y - Y'\|$. %
 For a set $\Omega$ and $k \in \BN$, we let $\Sets{\Omega}{k}$ be the set of size-$k$ subsets of $\Omega$. 


A \emph{distribution ensemble} $\MP$ is a sequence of distributions $\MP_1, \MP_2, \ldots$, where for each $\ell \in \BN$, $\MP_\ell \in \Delta(\MV^\ell \times \MY)$ represents a distribution over labeled input sequences of length $\ell$.  %
Let $\MH \subset \MY^{\MV^\st}$ denote a class of functions $h : \MV^\st \to \MY$. The \emph{length \generalization} problem is as follows: we aim to choose a hypothesis $\hat h \in \MH$ which enjoys small square loss $\Loss(\cdot)$ for inputs drawn from $\MP_{\bar L}$ for some $\bar L \in \BN$, if we are only allowed to choose $\hat h$ based off of inputs of lengths less than $\bar L$, i.e., those drawn from $\MP_\ell$ for $\ell < \bar L$.

How can we use inputs drawn from such $\MP_\ell$ to choose $\hat h$? A classic paradigm in learning theory is \emph{empirical risk minimization} (see e.g., \citet{shalev_understanding_2014}), which stipulates to choose a hypothesis in $\MH$ to minimize the average loss over \emph{samples} drawn from $\MP_\ell$ for some values of $\ell$. However, as our goal is specifically to understand the \emph{out-of-distribution} generalization behavior from  inputs of length $\ell < \bar L$ to inputs of length $\bar L$, we instead assume that $\hat h$ is chosen so as to minimize the \emph{population risk} for inputs of length $\ell < \bar L$. This choice allows us to avoid having to consider the (in-distribution) generalization error for samples drawn from $\MP_\ell$ ($\ell < \bar L$). 
Formally, we define length \generalization as follows: 
\begin{definition}[Length \generalization]
  \label{def:length-extrap}
  For $L,\bar L \in \BN$, $ \ep \in (0,1)$, we say that the class $\MH$ has \emph{$(L,\bar L,\ep)$-length \generalization with respect to the ensemble $\MP$} %
  if the hypothesis %
  \begin{align}
\hat h := \argmin_{h \in \MH} \E_{\ell \sim \Unif([L/2, L])} \E_{(\bX, \bY) \sim \MP_\ell} [\Loss(h(\bX), \bY)]\label{eq:define-hath} 
  \end{align}
  satisfies $\E_{(\bX, \bY) \sim \MP_{\bar L}}[\Loss(\hat h(\bX), \bY)] \leq \ep$.\footnote{The lower bound of $L/2$ in the interval $[L/2,L]$ from which $\ell$ is sampled is unimportant; any constant factor of $L$ will suffice.}
\end{definition}
For a hypothesis class $\MH \subset \MY^{\MV^\st}$ and $\delta \in (0,1)$, we say that the ensemble $\MP$ is \emph{$\delta$-approximately $\MH$-realizable} if there is some $h^\st \in \MH$ so that $\E_{(\bX, \bY) \sim \MP_\ell}[\Loss(h^\st(\bX), \bY)] \leq \delta$ for each $\ell \in \BN$. We say that $\MP$ is \emph{$\MH$-realizable} \icml{in the case $\delta = 0$.}\arxiv{if it is 0-approximately $\MH$-realizable.} %

\subsection{Distributional assumptions: sparse structure}
\label{sec:sparse-structure}
Without distributional assumptions on $\MP_\ell$, achieving length \generalization per \cref{def:length-extrap} becomes degenerate in the following manner, even if we assume $\MH$-realizability. For any fixed (realizable) choice of the distributions $\MP_\ell$ for $\ell < \bar L$, unless $\hat h$ is \emph{identically equal} to $h^\st$, we can choose $\MP_{\bar L}$ to have all its mass on some sequence $\bX \in \MV^{\bar L}$ for which $\Loss(h^\st(\bX), \hat h(\bX)) \gg 0$. This choice prevents the loss of $\hat h$ defined in \cref{eq:define-hath} from being small, thus ruling out length \generalization, as formalized \icml{in \cref{prop:noasm-nolg}}\arxiv{below}. %
\arxiv{
\begin{proposition}
  \label{prop:noasm-nolg}
Fix any $L, \bar L \in \BN$ with $L < \bar L$, and a hypothesis class $\MH \subset \MY^{\MV^\st}$. Let $\MP_1, \ldots, \MP_{L}$ be $\MH$-realizable distributions, realized by $h^\st$. Suppose that $\ep > 0$ and $\hat h$ defined in \cref{eq:define-hath} satisfies $\sup_{\bX \in \MV^{\bar L}} \Loss(h^\st(\bX), \hat h(\bX)) > \ep$. Then there is an ensemble $\MP$ extending $\MP_1, \ldots, \MP_L$ so that $\MH$ does not have $(L, \bar L, \ep)$-length \generalization with respect to $\MP$. 
\end{proposition}}

While some works (e.g., \citet{ahuja_provable_2024,huang_formal_2024}) have offered explanations for length \generalization by showing that in fact, $h^\st$ is learned \emph{exactly} under appropriate assumptions, there is ample empirical evidence \cite{zou_universal_2023,wei_jailbroken_2023,andriushchenko_jailbreaking_2024} that transformers can err on worst-case inputs. Accordingly we ask: \emph{Are there distributional assumptions which enable us to establish length \generalization in settings where the ground truth hypothesis $h^\st$ may \emph{not} be learned (nearly) exactly in the sense that $\sup_{\bX \in \MV^\st} \ML(h^\st(\bX), \hat h(\bX))$ is small?} %
  
The sparse attention patterns in many transformers trained on natural language (e.g., \citet{child_generating_2019,tay_efficient_2022}) suggest that  for modeling a wide spectrum of natural language, one needs to attend to only a small number of previous tokens  to predict each successive token. Inspired by this observation, we define the following class of distribution ensembles, namely those with \emph{sparse planted correlations}. Roughly speaking, sequences drawn from such distributions have most of their tokens drawn independently from some distribution $\mu \in \Delta(\MV)$ (which we think of as a ``background distribution'', representing tokens not relevant for the task at hand), and a small number $k$ of tokens drawn from some ``planted'' correlated distribution over $k$-tuples of tokens, denoted by $\Qvoc$ in \cref{def:sparse-planted} below. One should interpret these planted $k$ tokens as having ``relevant information'' for the task of predicting the label $\bY$. The particular location of these tokens is drawn independently (denoted $\Qpos_\ell$ below). Formally, we have: %
    \begin{definition}[$k$-sparse planted correlations]
    \label{def:sparse-planted}
    Fix a positive integer $k \in \BN$. %
    We say that a distribution ensemble $\MP = (\MP_\ell)_{\ell \in \BN}$ has \emph{$k$-sparse planted correlations} if there are distributions $\mu \in \Delta(\MV)$, $\Qpos_\ell \in \Delta(\Sets{[\ell]}{k})$ %
    for $\ell \in \BN$, $\Qvoc \in \Delta(\MV^k)$, and a function $g^\st : \MV^k \to \MY$ so that the following holds. For each $\ell \in \BN$, a sample $(\bX, \bY) \sim \MP_\ell$ may be drawn as follows: first, we draw $S^\st \sim \Qpos_\ell, \bZ \sim \Qvoc$, and we set:%
  \begin{align}
\bX_{S^\star} = \bZ, \qquad \bX_i \sim \mu \ \forall i \not \in S^\star, \qquad \bY = g^\st(\bZ)\label{eq:y-gstar-x}
  \end{align}
\end{definition}
While the assumptions in \cref{def:sparse-planted} that (a) the remaining tokens $\bX_i$ for $i \not \in S^\st$ are i.i.d.~from $\mu$ and (b) the tuple $S^\st$ which indexes the correlated tokens that ``matter'' is drawn from a fixed distribution $\Qpos_\ell$ are not realistic, we emphasize that they are made to simplify the proofs and ensure a simple model which captures the salient features that enable length \generalization. We leave generalizations, such as that the $\bX_i$ are drawn from a Markov chain or Hidden Markov Model, for future work.

\arxiv{\input{kgram.tex}}
\icml{An example of a distribution ensemble satisfying \cref{def:sparse-planted} is the distribution of tokens in the \emph{$k$-gram retrieval task}, of outputting the token following a certain $k$-gram in a given sequence $\bX$. In particular, the $k$-gram in question together with the following token form the ``planted sequence''; see \cref{sec:basic-setup-appendix} for a formal description. }


\subsection{Defining the hypothesis classes}
\label{sec:hypothesis-classes}
In this section, we define a hypothesis class which allows us to effectively model the salient aspects of transformers while permitting us to obtain provable length \generalization. In particular, we aim to satisfy the following two desiderata:
\begin{enumerate}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.25cm,rightmargin=0cm]}
\item \label{it:sim-correlations}First, we would like there to be many ensembles $\MP$ with sparse planted correlations \arxiv{(per \cref{def:sparse-planted})} which are realizable with respect to $\MH$.%
\item \label{it:model-ahead}Second, we would like the class $\MH$ to capture simple transformers (i.e., a single attention head). 
\end{enumerate}
To motivate how to arrive at our function class starting from the above criteria, we recall the structure of a single attention head (\arxiv{we give a slightly simplified version here; }see \cref{sec:transformers-prelim} for a more complete presentation). It takes as input \emph{embedding vectors} $\bh_1, \ldots, \bh_L \in \BR^d$ which could be, for instance, the results of an embedding matrix multiplied by one-hot encodings of tokens $\bX_1, \ldots, \bX_L \in \MV$. For some matrices $\bK, \bV \in \BR^{d \times d}$ and a query vector\footnote{In the case of decoder-only transformers, $\bq$ is commonly taken to be equal to be a linear function applied to $\bh_L$, though in this discussion we take it to be fixed along with the other parameters of the attention head.} $\bq \in \BR^d$, the attention head computes
\begin{align}
&   \Aheadsimp_{\bK, \bV, \bq}(\bh_1, \ldots, \bh_L) = \sum_{i=1}^L \alpha_i \cdot \bV \bh_i\label{eq:simple-ahead}\arxiv{\\
&   \mbox{ where } \alpha = \softmax(\langle \bq, \bK \bh_1 \rangle, \ldots, \langle \bq, \bK \bh_L \rangle)\nonumber.}
\end{align}
\icml{where $\alpha = \softmax(\langle \bq, \bK \bh_1 \rangle, \ldots, \langle \bq, \bK \bh_L \rangle)$.} 
The softmax over linear functions of $\bh_1, \ldots, \bh_L$ allows $\Aheadsimp$ to attend to individual positions $i \in [L]$ whose tokens satisfy some property{ (e.g., the embedding vector points in a particular direction of $\BR^d$, which could represent a certain meaning of the token)}. %
Suppose we were to take $\MH$ to be the class of attention heads and try to satisfy \cref{it:sim-correlations}: we would need attention heads to be able to apply some function ($g^\st$ in the context of \cref{def:sparse-planted}) to a certain \emph{sub-sequence of $k$ tokens} ($\bX_{S^\st}$ in \cref{def:sparse-planted}).%
\footnote{
  Notice that since $S^\st$ is not given explicitly, there are certainly cases where it is information-theoretically impossible to determine $S^\st$ from $\bX$. However, we give several examples in \arxiv{\cref{sec:lower-bounds}}\icml{\cref{sec:basic-setup-appendix,sec:lower-bounds}} where the planted sequence $\bX_{S^\st}$ is sufficiently ``distinguishable'' from the remaining tokens $\bX_i,\ i \not \in S^\st$, and in fact $\bY$ can indeed be computed with high probability.
 }

Unfortunately, a single attention head as in \cref{eq:simple-ahead} is not able to do so for large classes of $g^\st$, when $k > 1$ and even if $\bX_{S^\st}$ is sufficiently ``distinguishable'': for instance, $\Aheadsimp_{\bK, \bV, \bq}(\bh_1, \ldots, \bh_L)$ is invariant to the order of $\bh_1, \ldots, \bh_L$ (and thus of the tokens, in the typical case that $\bh_i$ is an embedding of $\bX_i$), whereas $g^\st(\bX_{S^\st})$ may not be.\arxiv{

} Decoder-only transformers correct for this deficiency by (amongst other factors, such as positional embeddings) stacking multiple layers of attention heads. When doing so, embedding vectors $\bh_i$ at individual positions of higher layers can contain information about \emph{multiple} tokens of the input sequence. %
Inspired by this property, we introduce the following model of an ``idealized transformer'' -- instead of stacking multiple attention heads, it directly applies attention on \emph{subsets} of $k$ tokens: 

\begin{definition}[Sparse functional attention class]
  \label{def:sparse-group-attn}
  Fix a positive integer $k$ (the sparsity) together with hypothesis classes $\Gpred$ consisting of functions mapping $ {(\BN \times \MV)^k} \to (\BR \cup \{-\infty \})$ and $\Gval$ mapping  $\MV^k \to \MY$. %
  We define the \emph{$k$-sparse functional attention class} $\Hpred = \Hpred(\Gpred, \Gval)$ to be the class of all $h_{g_0, g_1} : \MV^\star \to \MY$, indexed by $g_0 \in \Gpred, g_1 \in \Gval$, where\footnote{If $g_0(S, \bx_S) = -\infty$ for all $S \in \Sets{[L]}{k}$, then we set $h_{g_0, g_1}(\bx_1, \ldots, \bx_L)$ to be $\frac{1}{|\Sets{[L]}{k}|} \sum_S g_1(\bx_S)$.}
  \begin{align}
h_{g_0, g_1}(\bx_1, \ldots, \bx_L) := \sum_{S \in \Sets{[L]}{k}} \frac{\exp(g_0(S, \bx_S)) \cdot g_1(\bx_S)}{\sum_{S' \in \Sets{[L]}{k}} \exp(g_0(S', \bx_{S'}))}\nonumber.
  \end{align}
\end{definition}

Intuitively, one should think of the ``attention function'' $g_0(S, \bx_S)$ as computing attention scores based off of \emph{sets} of $k$ tokens. Sparse functional attention with $k = 1$ captures a single attention head with fixed query vector as discussed above: in particular, for $S = \{ i \}$, we choose $g_0(S, \bx_S) = %
\langle \bq, \bK \bh_i \rangle$ where $\bh_i$ is the embedding of $\bx_i$, and %
$g_1(\bx_i) = \bV \cdot \bh_i$. The fact that $g_0(S, \bx_S)$ can also depend on the position indices $S$ should be interpreted as allowing us to model positional embeddings (see \cref{sec:theory-pc} for more discussion). 
The following proposition formalizes these observations, verifying that the sparse group attention class indeed generalizes attention heads.
\begin{proposition}[Informal version of \cref{prop:model-attn-head-formal}]
  \label{prop:model-attn-head}
  The class of attention heads corresponding to a \arxiv{fixed} vocabulary $\MV$ and embedding dimension $d$ is equal to the 1-sparse functional attention class $\Hpred(\Gpred, \Gval)$ for appropriate choices of $\Gpred, \Gval$. 
\end{proposition}

\paragraph{Additional properties.} We believe that the abstraction of sparse functional attention defined in \cref{def:sparse-group-attn} may be of broader interest in obtaining a theoretical understanding of various properties of transformers. Nevertheless, in order to analyze length \generalization, we need to further restrict the class $\Hpred(\Gpred, \Gval)$ in the following ways: first, we assume that the class $\Gpred$ is \emph{local}, meaning that it only outputs finite (i.e., not negative infinity) scores on subsets $S \subset \BN$ for which $\max(S) - \min(S)$ is bounded. Second, we assume that $\Gpred$ only uses \emph{relative positional information}, meaning that shifting $S$ does not change $g_0(S, \bx)$ for all $g_0 \in \Gpred, \bx \in \MV^k$.

\begin{assumption}[$\Gpred$ is local and relative]
  \label{asm:local-relative}
  We introduce the following properties of the class $\Gpred$: 
  \begin{enumerate}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.25cm,rightmargin=0cm]}
  \item \label{it:local} Fix $\Llocal \in \BN$. We say that $\Gpred$ is \emph{$\Llocal$-local} if for all $S \in \Sets{\BN}{k}$ for which $\max_{i \in S} i - \min_{i \in S} i > \Llocal$, we have $g_0(S, \bx) = -\infty$ for all $g_0 \in \Gpred, \bx \in \MV^k$. %
  \item \label{it:relative} We say that $\Gpred$ is \emph{relative} if for any $i \in \BN$ and $S \in \Sets{\BN}{k}$, $\bx \in \MV^k$, it holds that $g_0(S, \bx) = g_0(S + i, \bx)$ for all $g_0 \in \Gpred$.\footnote{For $S = \{i_1, \ldots, i_k\}$, $S + i := \{i_1 + i, \ldots, i_k + i\}$\arxiv{ denotes the shift of $S$ by $i$}.}
  \end{enumerate}
\end{assumption}
The assumption that $\Gpred$ is relative is inspired by the fact that many common positional embeddings only use \emph{relative} positional information. While the assumption of locality is strong, we remark that it is provably necessary (\cref{sec:lower-bounds}) and can be relaxed using techniques such as positional coupling (\cref{sec:theory-pc}). Throughout the paper, we will assume that the parameters $k, \Llocal$ are chosen so that $\Llocal \geq k$\arxiv{: this allows us to have $g_0(S, \bx) > -\infty$ for $g_0 \in \Gpred$ and sets $S$ of size $k$}. 

\section{Theoretical results\arxiv{: provable length generalization for the sparse group attention class}}
\label{sec:theory}
In this section, we establish formal length generalization guarantees for sparse functional attention classes $\Hpred$  (\cref{def:sparse-group-attn}) for distribution ensembles $\MP$ with sparse planted correlation (\cref{def:sparse-planted}). To do so, we need to make a few assumptions on the ensemble $\MP$. 
The first assumption ensures that the ensemble $\MP$ is approximately $\Hpred$-realizable. %
\begin{assumption}[Realizability]
  \label{asm:unique-selection}
  We assume that $\MP$ is $\delta$-approximately $\Hpred(\Gpred, \Gval)$-realizable, i.e., there is $h^\st \in \Hpred(\Gpred, \Gval)$ so that $\E_{(\bX, \bY) \sim \MP_\ell}[\Loss(h^\st(\bX), \bY)] \leq \delta$ for all $\ell\arxiv{ \in \BN}$.
  \end{assumption}


Our second assumption states that the distributions $\Qpos_\ell$ in the context of \cref{def:sparse-planted} have bounded coverage at all locations in the sense that changing $\ell$ or shifting $S$ by $\Delta$ units does not significantly change $\Qpos_\ell(S)$. 
\begin{assumption}[Coverage of $\Qpos_\ell$]
  \label{asm:density-ratio-position}
  Fix a $k$-sparse distribution ensemble $\MP$ specified by $\mu, (\Qpos_\ell)_{\ell \in \BN}, \Qvoc$, as well as $\Llocal \in \BN$. 
  We assume that for each $\ell \in \BN$ with $\ell \geq \Llocal$, there is some positive value $\eta_\ell \in \BR$ so that for all $\ell' \in [\Llocal, \ell]$, $\Delta \geq 0$ and $S^\st \in \Sets{[\ell]}{k}$, %
  \arxiv{
  \begin{align}
 \frac{\Qpos_\ell(S^\st)}{\Qpos_{\ell'}(S^\st - \Delta)} \leq \eta_\ell, \qquad  \frac{\Qpos_{\ell'}(S^\st)}{\Qpos_\ell(S^\st)}\leq \eta_\ell\nonumber,
  \end{align}
}
\icml{$\frac{\Qpos_\ell(S^\st)}{\Qpos_{\ell'}(S^\st - \Delta)} \leq \eta_\ell$ and $\frac{\Qpos_{\ell'}(S^\st)}{\Qpos_\ell(S^\st)}\leq \eta_\ell$, } 
  where the first inequality is only required if $S^\st -\Delta \in \Sets{[\ell']}{k}$ and the second inequality is only required if  $S^\st \in \Sets{[\ell']}{k}$. 
\end{assumption}
\arxiv{To help interpret \cref{asm:density-ratio-position}, note first that in order for the distribution ensemble $\MP$ to be  realizable (\cref{asm:unique-selection}) by a class $\Hpred(\Gpred, \Gval)$ satisfying $\Llocal$-locality (\cref{asm:local-relative}), it will typically be the case that $\max_{i \in S^\st} i - \min_{i \in S^\st} i \leq \Llocal$ with probability at least  $1-\delta$ for $S^\st \in \Qpos_\ell$, for any $\ell \in \BN$. Thus, a natural choice for $\Qpos_\ell$ is to fix some distribution $\Qpos_{\Llocal}$ over sets in $\Sets{[\Llocal]}{k}$ and let $\Qpos_\ell$ be the distribution of a random shift of a sample from $\Qpos_{\Llocal}$. Formally, $\Qpos_\ell$ is the distribution of the shift $S + \Delta$, where $S \sim \Qpos_{\Llocal}$ and $\Delta \sim \Unif(\{ 0, 1, \ldots, \ell - \Llocal\})$. It is straightforward to see that such a construction ensures that \cref{asm:density-ratio-position} is satisfied with $\eta_\ell = \ell$. More broadly, any value $\eta_\ell \leq \poly(\ell)$ leads to interesting conclusions in the context of our results, so we interpret \cref{asm:density-ratio-position} as being fairly mild.
  }



Our main result states that for any classes $\Gpred, \Gval$ and distribution ensemble $\MP$ satisfying \cref{asm:density-ratio-position,asm:unique-selection,asm:local-relative}, the class $\Hpred(\Gpred, \Gval)$ enjoys length \generalization with respect to the ensemble $\MP$. 
\begin{theorem}[Provable length \generalization]
  \label{thm:length-extrap}
  Fix any $k \in \BN$, and consider any $k$-sparse functional attention class $\Hpred = \Hpred(\Gpred, \Gval)$ which is $\Llocal$-local and relative (\cref{asm:local-relative}) for some $\Llocal\in \BN$. 
  Then for any $k$-sparse planted correlations distribution ensemble $\MP$ (\cref{def:sparse-planted}) satisfying \cref{asm:density-ratio-position,asm:unique-selection}, and any integers $L, \bar L$ for which $\Llocal \mid \bar L - L$ and $L \geq 4\Llocal$, $\Hpred$ achieves $(L, \bar L, \eta_L \eta_{\bar L} \cdot L \bar L^2 \cdot \delta)$-length \generalization with respect to the ensemble $\MP$. %
\end{theorem}
\arxiv{Notice that there is no explicit dependence on $k$ in the error bound $\eta_L \eta_{\bar L} \cdot L\bar L^2 \cdot \delta$; however, we have assumed $k \leq \Llocal \leq L/4$, meaning that one should only expect \cref{thm:length-extrap} to ``kick in'' once the training maximum length $L$ is sufficiently large as as function of $L$ (and thus of $k$). 
The requirement that $\Llocal \mid \bar L - L$ is unimportant and is made solely for convenience in the proof. Moreover, we make no attempt to optimize the error term $\eta_L \eta_{\bar L} \cdot L\bar L^2 \cdot \delta$. }

\arxiv{\paragraph{On the necessity of the assumptions.}}
In \cref{sec:lower-bounds}, we show that if we remove any one of the main assumptions of \cref{thm:length-extrap}, then length \generalization can fail to hold. %

\subsection{Improving length generalization: positional coupling}
\label{sec:theory-pc}
As discussed above, one limitation of \cref{thm:length-extrap} is its reliance on \cref{it:local} of \cref{asm:local-relative}, \icml{which is not satisfied in actual transformers: indeed, transformers may attend to tokens which are very far apart, as such dependencies can occur in natural language. Is there a way to mitigate this limitation?}\arxiv{which leads to the following restriction on the $k$-sparse distribution ensemble $\MP$: in typical examples (modulo some degenerate ones where, e.g., all functions in $\Gval$ are constant), in order to satisfy realizability (\cref{asm:unique-selection}), we will need the low-loss hypothesis $h^\st \in \Hpred(\Gpred, \Gval)$ to be of the form $h^\st = h_{g_0^\st, g_1^\st}$ for some $g_0^\st \in \Gpred$ which ``selects out'' the planted set $S^\st$ and $g_1^\st \in \Gval$ which correctly evaluates the label $\bY$ given $\bX_{S^\st}$; formally, for each $\ell \in \BN$, with high probability under $(\bX, S^\st, \bY) \sim \MP_\ell$,
\begin{align}
  g_0^\st(S^\st, \bX_{S^\st}) > -\infty, \quad g^\st(S, \bX_S) = -\infty \ \  \forall S \neq S^\st,\nonumber
\end{align}
and $g_1^\st(\bX_{S^\st}) = \bY$. (We formally call this property \emph{strong realizability} in \cref{def:strong-realizability}.) But by \cref{it:local} of \cref{asm:local-relative}, this means that $\max\{ S^\st \} - \min\{S^\st\} \leq \Llocal$ with high probability over the draw from $\MP_\ell$. Ideally, we would like to establish results for planted $k$-sparse ensembles $\MP$ for which the planted set $S^\st$ is \emph{not} local in this sense.
}

We now show how a theoretical abstraction of position coupling as discussed in \cref{sec:prelim-pc} can allow us to remove this locality requirement.
Roughly speaking, this abstraction of position coupling states that there is a joint distribution over $(S^\st, \psi_\ell)$, where $\psi_\ell : [\ell]\to [\ell]$ gives a way of ``rewriting'' position indices, so that the ``rewritten'' set $\psi_\ell(S^\st)$ of indices in the planted set $S^\st$ satisfies the locality condition of \cref{asm:local-relative}. In particular, for each position ID $i \in [\ell]$, the value of $\psi_\ell(i)$ should be interpreted as its ``coupled position ID'' as discussed in \cref{sec:prelim-pc}. %
More precisely, we have\icml{ (see \cref{sec:appendix-pc} for the full definition)}: %
\icml{
\begin{definition}[Informal version of \cref{def:amenability}]
  \label{def:amenability-informal}
  Fix a distribution ensemble $\MP$  with $k$-sparse planted correlations per \cref{def:sparse-planted} (defined by $\mu, \Qpos, \Qvoc$). A \emph{$\Llocal$-local position coupling} of $\MP$ is defined by,  for each $\ell \in \BN$, %
  a joint distribution $\Qposc$ over tuples $(S^\st, \psi_\ell)$ with $S^\st \in \Sets{[\ell]}{k}$ and $\psi_{\ell} : [\ell] \to [\ell]$ so that the marginal of $S^\st$ under $\Qposc$ is $ \Qpos_\ell$ %
  and with probability 1 under the draw of $(S^\st, \psi_\ell) \sim \Qposc$: %
 \begin{enumerate}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.35cm,rightmargin=0cm]}
 \item \label{it:local-coupling} $\max\{ \psi_\ell(S^\st) \} - \min \{ \psi_\ell(S^\st)\} \leq \Llocal$. %
 \item $\psi_\ell, S^\st$ satisfy some additional \arxiv{technical} conditions which, e.g., prevent coupling of non-planted tokens (i.e., outside of $S^\st$; %
\cref{def:amenability}). 
 \end{enumerate}
\end{definition}
}
\arxiv{\input{pc-formal.tex}}


With \cref{def:amenability} in hand, we can now describe how hypotheses in a sparse functional attention class $\Hpred$, which \emph{may not satisfy locality}, can be transformed into new hypotheses which will use information from the output of the ``position coupling'' $\psi_\ell$ and \emph{will also satisfy locality}. Roughly speaking, given a sample $(\bX, S^\st, \bY) \sim \MP_\ell$, we will join (i.e., ``couple'') all tokens $\bX_i$ for which $\psi_\ell(i)$ is identical. We will denote the resulting distribution over sequences of ``coupled tokens'' by $\PC[\MP_\ell]$. Moreover, we define a ``position-coupled'' hypothesis class $\PC[\Hpred]$ whose hypotheses, when given sequences of coupled tokens as above, ``unpacks'' them and applies the corresponding hypothesis in $\Hpred$. In \cref{sec:appendix-pc}, we formally define these notions and prove the following proposition, which shows that this procedure of position coupling can remove the locality requirement from \cref{thm:length-extrap}:
\begin{proposition}[Informal version of \cref{prop:pc-length-extrap}]
  \label{prop:pc-length-extrap-informal}
If $\Hpred$ is a sparse functional attention class and $\MP$ is an ensemble with $k$-sparse planted correlations  which satisfies $\delta$-approximate strong realizability (\cref{def:strong-realizability}) and \cref{asm:density-ratio-position}, then for $L, \bar L$ satisfying the conditions of \cref{thm:length-extrap}, $\PC[\Hpred]$ achieves $(L, \bar L, \eta_L \eta_{\bar L} \cdot L\bar L^2 \cdot \delta)$-length \generalization with respect to the ensemble $\PC[\MP]$. 
\end{proposition}

\arxiv{
\begin{remark}[Theoretical justification for PoSE]
  \label{rmk:pose}
  It is natural to wonder if our theoretical framework allows us to justify other techniques used to induce length \generalization, such as PoSE (\cref{sec:prelim-pose}), in a sense akin to \cref{prop:pc-length-extrap-informal}. At a high level, PoSE is adjusting the {distribution} of position IDs during training, so that IDs always seen farther apart than the training context window at test time may nevertheless be observed in the same training instance. In other words, a model trained as such should ``interpret'' a greater range of sets $S$ as satisfying the locality requirement of \cref{asm:local-relative}. Thus, we conjecture that this adjustment allows us to remove the locality requirement in \cref{asm:local-relative} as well; we leave a formal proof of this fact as an intriguing direction for future work. 
\end{remark}
}


\section{Experiments}
\label{sec:experiments}
Conceptually, we view the main takeaways of \cref{thm:length-extrap} and \cref{prop:pc-length-extrap-informal} to be the following:
\begin{enumerate}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.35cm,rightmargin=0cm]}
\customitem{(T1)}\label{it:ta-sparsity} First, an important factor enabling length generalization is that the label $\bY$ depends on only $k$ tokens of the input (in the sense of \cref{def:sparse-planted} and in particular the relation $\bY = g^\st(\bX_{S^\st})$ in \cref{eq:y-gstar-x}). In particular, the parameter $k$ (which we refer to informally as the \emph{sparsity}) must be the same for both the lengths on which we train (namely, lengths $\ell \leq L$) and the length $\bar L$ to which we attempt to extrapolate, and sufficiently small compared to the maximum training length $L$. 
\customitem{(T2)}\label{it:ta-locality} Second, \emph{locality} of the hypothesis class (per \cref{it:local} of \cref{asm:local-relative}) plays an important role as well: the maximum distance $\Llocal$ between tokens which ``matter'' in predicting the label $\bY$ must be the same for lengths $\ell \leq L$ on which we train and the length $\bar L$ to which we extrapolate. Moreover, as this requirement is quite strong (and unrealistic for many problems of interest), one way to mitigate it is the technique of position coupling (per \cref{prop:pc-length-extrap-informal}). 
\end{enumerate}



In this section, we evaluate these conclusions for synthetic and natural language modeling data. %

\arxiv{
  \begin{remark}
  \label{rmk:local-relative}
  One might wonder why we emphasize \cref{it:local} but not \cref{it:relative} of \cref{asm:local-relative} in takeaway \ref{it:ta-locality}. In fact, the conceptual message of \cref{it:relative}, namely that position IDs only influence attention scores by their \emph{relative} information (i.e., the difference between different positions) is already captured by the fact that it is common to use \emph{relative positional embeddings} and variants (e.g., RoPE \cite{su_roformer_2023}, FIRE \cite{li_functional_2024}) in many open-source transformer architectures. Due to the success of such embeddings, in this sense the constraint imposed by \cref{it:relative} can ``come for free''.
\end{remark}}

\subsection{Length \generalization for sparse parity}
\label{sec:sparse-parity}


First, we discuss a simple setting which measures the degree to which length \generalization in transformers reflects the requirement from \ref{it:ta-sparsity} that the sparsity $k$ be sufficiently small and not grow as a function of the sequence's length. In particular, consider the following \emph{sparse parity} task:\footnote{This task is slightly different from standard formulations of sparse parity, where the sparsely chosen positions are not identified as part of the input. We use this version so as to allow a different set of $k$ positions to be chosen for each example.} given $k ,\ell\in \BN$, the input is drawn from a distribution $\Dsp_{k,\ell}$ over length-$2\ell$ sequences, where tokens at even-numbered positions are bits, and tokens at odd-numbered positions belong to some set $\Omega$. Exactly $k$ tokens at odd-numbered positions belong to some ``special subset'' $\Omega' \subset \Omega$, and the goal is to find the parity of the $k$ tokens immediately following them. See \cref{sec:data-format-sparse-parity} for precise details. 


\paragraph{Experimental setup: data.} For each value of ${\Ktrain} \in {\{4,6,8,10,12\}}$, we train a transformer $\hat h_{\Ktrain}$ to predict the last token of samples drawn from $\Dsp_{\ell, k}$, where $\ell$ is sampled uniformly subject to the length of the sample satisfying {$2\ell \in [20, 50]$} and $k \sim \Unif([{\Ktrain}])$. We then evaluate the performance of each of the trained transformers $\hat h_{\Ktrain}$ on samples drawn from $\MD_{\ell, \ktest}$ of length {$2\ell \in [20, 500]$} and with sparsities $\ktest \in \{4,6,8,10,12,14,16\}$. 


\paragraph{Experimental setup: model.} Our model is based off of the GPT-NeoX (decoder-only) transformer \cite{gpt-neox-library}, and uses rotary positional embeddings (RoPE). %
To ensure nontrivial length generalization performance, we combined RoPE with PoSE (\cref{sec:prelim-pose}).\footnote{This task is not well-suited for position coupling, since the output token depends on $2k$ input tokens, all of which have different position IDs.} Full training and evaluation details may be found in \cref{sec:synthetic-experimental-details}.  %

\begin{remark}
   Numerous other modifications to position IDs have been proposed for length generalization, such as \emph{position interpolation} and various enhancements, which typically modify the way the transformer uses the position IDs at \emph{inference time}, often after a small amount of fine-tuning (see \cref{sec:related-posids}). We stick with PoSE in this paper (when position coupling is not applicable) because of its simplicity and since (a) it does not require modifying the transformer's computations at inference time; and (b) the fine-tuning for position interpolation requires sequences of length given by the \emph{testing} context length, which fails to lie in our framework where we assume that any amount of training on such sequences is not allowed. Understanding the role of sparsity and locality for  length generalization in transformers which make these inference-time modifications to position IDs is left for future work. 
  \end{remark}


\paragraph{Results \& discussion.} The accuracy of the trained transformers $\hat h_{{\Ktrain}}$ for predicting the last (parity) token on samples of various lengths $\ell$ and sparsity values $k$ is shown in \cref{fig:sparse-parity}{ for $\Ktrain =\arxiv{8,}10$ and in \cref{fig:sparse-parity-extra} for the remaining values of $\Ktrain$}.
Two observations are notable: first, when the training sparsity ${\Ktrain}$ is small enough (i.e., $\Ktrain \leq 8$\icml{; see \cref{fig:sparse-parity-extra}}), then $\hat h_{\Ktrain}$ experiences near-perfect length \generalization up to lengths of 500, for all values of the test sparsity $\ktest\leq {\Ktrain}$. However, for test sparsity values $\ktest > {\Ktrain}$, the performance of $\hat h_{\Ktrain}$ deteriorates rapidly, for both in-distribution and out-of-distribution lengths. This behavior is consistent with \cref{thm:length-extrap}, which implies that good length \generalization occurs as long as the sparsity  does not change between the train and test distributions and is sufficiently small as a function of the training length. 

A corollary of this reasoning is that for a fixed maximum training length $L$, the length \generalization behavior with respect to the distribution ensemble $(\Dsp_{\ell,k})_{\ell \in \BN}$ should degrade at some point if the sparsity $k$ is allowed to increase enough. 
(In particular, our theoretical guarantee on length \generalization only holds for sufficiently small training sparsity, i.e., $\Ktrain \leq L/4$ in the case of \cref{thm:length-extrap}, though the particular threshold is likely different in the case of actual transformers). This behavior is mirrored in \cref{fig:sparse-parity}\icml{ as well as \cref{fig:sparse-parity-extra} in \cref{sec:sparse-parity-extra-results}}, where, for each fixed value of $\Ktrain \in \{10,12\}$, larger values of $k \in [\Ktrain]$ have worse length \generalization despite all having near-perfect in-distribution performance.\footnote{Recall that $\hat h_{\Ktrain}$ was trained by placing equal weights on all sparsities $k \leq \Ktrain$, so in this respect the distributions $\Dsp_{\ell,k}$ for $k \leq \Ktrain$ were treated ``on equal footing.''}

\icml{
  \begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
      \centering
\includegraphics[width=\textwidth]{plots/output_dir.model_sparse_parity-0-0.5_ngram10vtrain_FINALlgplots.pdf}   
      \caption{Sparse parity}
      \label{fig:sparse-parity}
    \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/output_dir.model_parity_cot-0-_-0.5__trial_v2_position_options-poscoupling-210FINALlgplots.pdf}
    \caption{Parity with scratchpad \& \PPC}
    \label{fig:parity-cot}
  \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/output_dir.model_mon_pointer_chasing_cot_posemb_trial_v_position_options-poscoupling-230FINALlgplots.pdf}
    \caption{Variable asst.~with scratchpad \& \PPC}
    \label{fig:pointer-cot}
  \end{subfigure}
  \caption{Length generalization for experiments in \cref{sec:sparse-parity,sec:cot-pc}. Shaded areas in {(b), (c)} represent $95\%$ CIs over multiple training runs.}
  \label{fig:icml-synthetic-main}
  \end{figure*}
}

  \arxiv{
\begin{figure}[h!]
    \centering
    \foreach \i in {8,10} {
        \includegraphics[width=0.45\textwidth]{plots/output_dir.model_sparse_parity-0-0.5_ngram\i vtrain_FINALlgplots.pdf}
    }
    \caption{Length generalization for the sparse parity task with $\Ktrain \in \{4,6,8,10,12\}$ for various values of $\ktest$. \db{Results for 12 seem worse than 10 across all sparsity levels. Should we just exclude these or try to train them better?}\noah{yes, good point. It's not an issue of not training long enough (test loss bounces around the same values after roughly 10000-20000 iterations, and I trained for 50000). My guess is that forcing it to fit the 12-sparse parities somehow makes it do something funny which means it length-generalizes worse. That said, I don't think this is necessarily a problem since the only comparison that really makes sense is amongst the different lines \emph{within} each plot: i.e., for a given model which performs well at multiple sparsities in-distribution, its length generalization behavior is worse for higher sparsities. (Or if that's too messy to explain we can just delete the sparsity=12 plot.)} \db{I also wonder if focusing on in lengths <150 might make things more clear for 8 and 10?}\noah{What do you mean? Which thing in particular do you want to make clear?}}
    \label{fig:sparse-parity}
  \end{figure}
  }



\subsection{Scratchpad with \predpc}
\label{sec:cot-pc}
Many tasks, unlike sparse parity, have the property that 
the sparsity of the next-token prediction task (as formalized by, e.g., \cref{def:sparse-planted}) can be quite large and in fact \emph{increase} from shorter to longer lengths. To help achieve length \generalization, one approach which has been suggested in numerous existing works \cite{anil_exploring_2022,hou_universal_2024} is the \emph{chain-of-thought/scratchpad} technique \cite{nye_show_2022,wei_chain_2022,lewkowycz_solving_2022}. It proceeds by computing tokens representing intermediate steps, such as intermediate parities when the task is to compute the parity of a sequence of $\ell$ bits. It has been observed that the scratchpad technique alone is insufficient to ensure length generalization \cite{anil_exploring_2022,dziri_faith_2023,hu_casebased_2024,kazemnejad_impact_2023,lanchantin_learning_2023}. However, a number of recent works \cite{cho_position_2024,cho_arithmetic_2024,mcleish_transformers_2024} have shown that the technique of \emph{position coupling} (see \cref{sec:prelim-pc}), when combined with a scratchpad (and even when used on its own, when appropriate), can allow significant length \generalization on arithmetic tasks such as parity, addition, and multiplication. This development parallels our theoretical results in \cref{sec:theory-pc}, where we showed that position coupling removes the stringent requirement of locality, which is not satisfied in practical settings. 

One downside of position coupling is that it requires significant structure in the format of the data sequence, including the scratchpad: in particular, once the length is fixed, the \emph{coupled position IDs} (see \cref{sec:prelim-pc}) in all tokens the transformer is trained to predict  must be determined, as they must be fed into the model to predict each successive token. (For instance, for the string reversal example in \cref{sec:prelim-pc}, when $L$ denotes the length of the string to be reversed, the sequence of position IDs is given \arxiv{by \cref{eq:string-reversal-posids}}\icml{as in \cref{sec:prelim-pc}}.) %
As a consequence, the applications of position coupling are currently limited to a small number of synthetic tasks where the computation of each token in the sequence (including the scratchpad) can be coupled with a fixed position of the input.

Towards relaxing this limitation to allow sequences where the coupled position IDs can be \emph{input-dependent}, we propose \emph{\predpc} (\PPC), which modifies the transformer architecture to instead \emph{predict} the coupled position ID for each next token. In particular, we add an additional output embedding module so that, at each step, the transformer predicts two IDs: the next token ID, as well as the coupled position ID for that token. At generation time, those two predicted tokens are fed in as the position ID and token ID at the following position. 
In our results, we report the fraction of examples on which the model correctly predicts \emph{all tokens and coupled position IDs}. %


\arxiv{\paragraph{Experimental goals.} In the experiments discussed below, we aim to: (a) show that \predpc can successfully be applied to improve length generalization on instances where position IDs in the scratchpad must be predicted; and (b) validate our theoretical takeaways emphasizing the importance of \emph{sparsity} \ref{it:ta-sparsity} and \emph{locality}  \ref{it:ta-locality} in controlling length \generalization for instances with a scratchpad. Towards the latter goal, %
we will evaluate the impact of (i) removing position coupling, which as discussed in \cref{sec:theory-pc} corresponds to decreasing locality; or (ii) (partially) removing the scratchpad, which for our scratchpad formats will decrease the sparsity. %
}

\paragraph{Experimental setup: model.} 
We use the same NeoX model as described in \cref{sec:sparse-parity}, with the following modifications to implement position coupling. Following \cite{cho_position_2024,cho_arithmetic_2024,mcleish_transformers_2024}, we use absolute position embeddings %
and fix some integer $\Lmax$ denoting the maximum possible length of test-time sequences. During training, we shift each sequence of coupled position IDs by a uniformly random offset subject to the constraint that all shifted position IDs be at most $\Lmax$. In particular, if the sequence of coupled position IDs ranges from $1$ to $\ell$, then we would shift by a uniform element of $\{0, 1, \ldots, \Lmax - \ell \}$. We remark that using RoPE with PoSE (as oposed to \PPC) performs significantly worse; see \cref{sec:synthetic-experimental-details}. 



\subsubsection{Warmup: parity with scratchpad.}
\label{sec:parity-scratchpad}


  
As a warmup, we first consider the task of evaluating the parity of a sequence of $\ell$ bits with use of a scratchpad (and \PPC) to compute intermediate parities (as in, e.g., \citet{anil_exploring_2022,cho_arithmetic_2024}).
The scratchpad is structured as follows: it has length $\ell$, and the $i$th token of it is the parity of the first $i$ tokens of the input; thus, the final ($\ell$th) token of the scratchpad is the desired answer, i.e., the parity of all $\ell$ input tokens. 
To measure the impact of sparsity, we consider the following modification of the standard scratchpad format, which we refer to as having a scratchpad with ``jumps''. For values of $k \in \{1, 2, 3, 4 \}$, we only write every $k$th bit in the scratchpad, meaning that the $i$th bit of the scratchpad is the parity of the first $ki$ tokens of the input. In particular, the case $k=1$ corresponds to the standard scratchpad format. %
We also modify the position coupling IDs so that each position ID is repeated $k$ times in the input sequence. Full details of the data and position coupling formats are in \cref{sec:parity-scratchpad-data-format}. 

\paragraph{Results \& discussion.} \cref{fig:parity-cot} shows the length \generalization behavior for our transformer model trained as discussed above, for ``jump lengths'' $k \in \{1,2,3,4\}$. Note that the jump length $k$ parametrizes the sparsity of the next-token prediction task: each token in the scratchpad depends on $k+1$ previous tokens (namely, the immediately preceding one together with the $k$ tokens in the input which correspond to that step of computation).
While all $4$ jump lengths attain near-perfect accuracy for in-distribution lengths, smaller values of $k$ have superior performance on longer out-of-distribution lengths, consistent with  our takeaway \ref{it:ta-sparsity}. This fact is particularly notable in light of the fact that for jump length $k$, the scratchpad is \emph{shorter} by a factor of $k$, which a priori could presumably have made it easier to correctly predict all tokens and position IDs correctly (the metric measured in \cref{fig:parity-cot}). Thus, at least in this task, \emph{sparsity (as measured here by $k$) is a more important determinant of length \generalization than the length of the output sequence}. 
\arxiv{
  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{plots/output_dir.model_parity_cot-0-_-0.5__trial_v2_position_options-poscoupling-210FINALlgplots.pdf}
    \caption{Parity with scratchpad and \predpc}
    \label{fig:parity-cot}
  \end{figure}
  }

  
\subsubsection{Variable assignment with scratchpad.}
\label{sec:va-scratchpad}
Next, we consider the more challenging task of \emph{variable assignment} with a scratchpad, of which several slight variants have been studied in numerous prior works \arxiv{(sometimes under different names, like ``pointer chasing'')} \arxiv{\cite{zhang_unveiling_2023,zhang_pointer_2022,lanchantin_learning_2023,anil_exploring_2022,hsieh_ruler_2024,peng_limitations_2024}.}
\icml{\cite{zhang_unveiling_2023,zhang_pointer_2022,lanchantin_learning_2023,peng_limitations_2024}.}

\paragraph{Experimental setup: data overview.} In the variable assignment problem, we fix a \emph{depth} $d$, denoting the number of ``hops'' it takes to compute the final value. Roughly speaking, the goal is as follows: given a sequence of ``variable assignments'' of the form $v \gets v'$ (where $v,v'$ are token IDs), together with a \emph{starting variable} $\vinit$, follow a sequence of $d$ assignments starting at $\vinit$.\arxiv{

}In more detail, a problem instance consists of some number of ``chains'', each of the form $v_1 \gets v_2; v_2 \gets v_3 ; \cdots; v_d \gets v_{d+1}$ for some \emph{depth parameter} $d$. The variable assignments in each of these chains are interleaved randomly; exactly one of the chains has $v_1 = \vinit$. The goal of the task is to find the final variable $v_d$ on the chain for which $v_1 = \vinit$. To do so, we use a scratchpad which writes out in full the chain starting with $\vinit$, together with position coupling, which matches each element of this chain on the scratchpad with its corresponding position in the input. The data format and position coupling scheme are presented in detail in \cref{sec:var-assign}. We mention here that, because the chains are interleaved randomly, the position IDs used in the position coupling scheme \emph{depend on the particular input instance}. Thus, the use of \predpc is crucial in order to be able to feed in the correct coupled position IDs in the scratchpad at inference time.\footnote{Simply feeding in the correct value as the next position ID would be ``cheating'' as it would be performing part of the relevant computation of the scratchpad.} We train on input sequences of length $\ell \in [20,60]$, and test on lengths $\bar L \in [20, 200]$.

\paragraph{Results.} \cref{fig:pointer-cot} shows our results for the variable assignment problem with position coupling: the technique of \predpc allows good length \generalization for test lengths up to 3 times the training sequence length, for depths $d \in \{2,3,4,5\}$. In \cref{fig:pointer-cot-absshift,fig:pointer-rope-pose,fig:pointer-cot-rope-pose}\icml{ (\cref{sec:var-assign-results})}, we present baselines that remove either just the position coupling or the scratchpad altogether; as predicted by takeaways \ref{it:ta-locality} and \ref{it:ta-sparsity}, respectively, these modifications significantly harm length \generalization. \emph{In particular, using RoPE with PoSE (with or without scratchpad) performs significantly worse than \PPC.}

\arxiv{While in this paper, we have discussed two examples in which \predpc can be effectively applied as a proof of concept, we believe that \PPC can be (a) combined with other approaches, including improved positional embeddings such as FIRE \cite{li_functional_2024} and (b) be applied to several other synthetic (and potentially natural language) tasks. We leave these investigations as exciting directions for future work.}




\arxiv{\input{baseline-va-plots.tex}}







  \subsection{Sparsity \& length generalization in natural language}
  \label{sec:experiments-natural-language}
Finally, we study the interplay between sparsity and length generalization in natural language. \arxiv{As is typical in experiments involving natural language, we use the \emph{perplexity} %
of a model on test sequences\footnote{The perplexity of a language model on a sequence is the exponential of the mean of the negative log likelihoods of the model's predictions of each token in the sequence.} to evaluate its performance.  Two challenges that arise in this setting are: 
(1) a trivial way to achieve reasonably good length \generalization from sequences of length $\leq L$ to those of length $\bar L > L$ is to simply ignore tokens more than $L$ units in the past when given a sequence of length $\bar L $, and 
(2) since there is not a ``correct'' functional form for the value of each token as a function of previous ones, it is unclear how to define ``sparsity'' of the next-token prediction task. %

In order to understand length \generalization in models beyond the trivial baseline suggested in (1) above, we ask:  \emph{given $L < \bar L$, can a transformer trained on natural language sequences of length $L$ achieve smaller perplexity on sequences of length $\bar L$ than on sequences of length $L$?} Previous works have shown a positive answer to this question (as will our experiments) when one uses appropriate modifications to the positional embeddings, namely PoSE \cite{zhu_pose_2024} or position interpolation \cite{chen_extending_2023,emozilla_dynamically_2023,peng_yarn_2023,chen_clex_2024}. In light of this, a natural way to address challenge (2) above is as follows: \emph{is there a way to select a small number $k$ of the tokens more than $L$ units in the past from the present token, so that, using those $k$ tokens together with the most recent $L$ tokens, we can nearly recover the perplexity of the model on \emph{full sequences} of length $\bar L$?} Informally, we are asking if the dependence on tokens more than $L$ units in the past is \emph{sparse}, in terms of beating the trivial baseline for length \generalization.
}\icml{
  A trivial way to obtain generalization from sequences of length $\leq L$ to those of length $\bar L > L$ is to simply ignore tokens more than $L$ units in the past. 
  To understand length \generalization in models beyond this baseline, we ask:  \emph{given $L < \bar L$, can a transformer trained on natural language sequences of length $L$ achieve smaller perplexity on sequences of length $\bar L$ than on sequences of length $L$?} Previous works have shown a positive answer to this question (as will our experiments) when one uses appropriate modifications to the positional embeddings, namely PoSE \cite{zhu_pose_2024} or position interpolation \cite{chen_extending_2023,emozilla_dynamically_2023,peng_yarn_2023,chen_clex_2024}. In light of this, a natural way to measure sparsity is as follows: \emph{is there a way to select a small number $k$ of the tokens more than $L$ units in the past from the present token, so that, using \emph{just} those $k$ tokens together with the most recent $L$ tokens, we can nearly recover the perplexity of the model on \emph{full sequences} of length $\bar L$?} Informally, we are asking if the dependence on tokens more than $L$ units in the past is \emph{sparse}, in terms of beating the trivial baseline for length \generalization.
}

\paragraph{Experimental setup.} We trained our models using the OLMo codebase \cite{groeneveld_olmo_2024} on the C4 dataset \cite{raffel_exploring_2019}. We trained a transformer model $\hshort$ %
using context length $L = 64$.\footnote{We chose the context length to be so short in order to decrease the performance of the trivial baseline for length \generalization which ignores all but the $L$ most recent tokens.} Our aim is to length extrapolate to a context length of $\bar L = 2L = 128$. 
We used the PoSE technique with $\Lmax = \bar L$ and $c = 2$ chunks. We found that training from scratch using PoSE did not yield good performance at length $\bar L$, so instead we first trained the model without PoSE for roughly 70\% of the iterations and on the final 30\% of the iterations used PoSE. Precise hyperparameters may be found in \cref{sec:additional-nlp}.

\begin{figure}[h]
    \centering    \includegraphics[width=0.4\textwidth]{plots/eval_store_finetuned_64_128_1/attention_ce_diff_evaluatorc4_methodloo_ablate_pos126_shift64.pdf}
    \caption{Cross-entropy loss differences obtained by unmasking $k$ influential tokens ($x$-axis) or all of the first 64 tokens ($y$-axis).}
    \label{fig:olmo-sparse}
  \end{figure}

\arxiv{\input{natural-language.tex}}

\icml{
  \paragraph{Results \& discussion.} \cref{tab:training-ppl} confirms that the perplexity  of $\hshort$ on a context window of $128$ is less than that on a context window of $64$, verifying that $\hshort$ is able to utilize information beyond the shorter length-64 training context window. Moreover, in \cref{fig:olmo-sparse}, we plot, for each of a total of $500$ evaluation data points $\bX_{1:\bar L}$ with context window $\bar L = 128$, the amount of decrease in cross-entropy loss for predicting the \emph{last} token $\bX_{\bar L}$ of the sequence one achieves by the following two strategies:
  \begin{enumerate}\icml{[itemsep=0.0em,topsep=0.0em,leftmargin=0.35cm,rightmargin=0cm]}
  \item[(a)] Unmasking $k = 5$ ``influential'' tokens at positions $i \leq L = 64$ compared to masking out all tokens at positions $i \leq 64$ (this difference is the $x$-coordinate);
    \item[(b)] Unmasking \emph{all} tokens at positions $i \leq 64$ compared to masking out all tokens at positions $i \leq 64$ (this difference is the $y$-coordinate).
    \end{enumerate}
    In particular, the $y$-coordinate of each point should be interpreted as the amount the model uses the first $64$ tokens to improve the prediction of the last token, and the $x$-coordinate of each point should be interpreted as the amount the model uses a select $k$ of the first $64$ coordinates (see \cref{fig:lshortlong} for an illustration). As can be seen, the data points are roughly clustered along the line $x = y$, which indicates that (up to noise) the model's prediction of the last token in the sequence is sparse in its dependence on tokens ``far in the past'', consistent with \cref{it:ta-sparsity} which predicts that such sparsity allows $\hshort$ to length-generalize. We discuss additional details and experiments (including how the $k$ influentail tokens are chosen) in \cref{sec:additional-nlp}. 
}

\section*{Acknowledgements}
NG was supported in part by a Fannie \& John Hertz Foundation Fellowship and an NSF Graduate Fellowship.

  
\icml{\newpage}
\appendix

\icml{
\section*{Impact Statement}
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
}

\arxiv{\bibliographystyle{abbrvnat}}
\icml{\bibliography{length-generalization_updated_2,lg-extra_2}}
\arxiv{{\small \bibliography{length-generalization_updated_2,lg-extra_2}}}
\icml{\bibliographystyle{icml2025}}


\icml{\onecolumn}

\section{Related work}
\label{sec:related-work}

\subsection{Theoretical guarantees for length generalization}
\citet{zhou_what_2023,huang_formal_2024}
propose that transformers will length-generalize at tasks which can be solved using a short RASP program \cite{weiss_thinking_2021}. Theorem 7 of \citet{huang_formal_2024}, however, assumes that the transformer is chosen so as to minimize a certain regularizer tailored specifically for length generalization, and their result is asymptotic in nature (in contrast to ours).
\citet{sabbaghi_explicitly_2024} show that using  gradient flow on a 1-layer linear transformer model with relative position embeddings on a simple linear regression task will converge to a model which length-generalizes, whereas the use of absolute position embeddings will fail to length-generalize. \citet{ahuja_provable_2024} show that a model class defined as a simple abstraction of attention heads that sums up pairwise dependencies of tokens experiences length generalization. %
Unlike our theoretical results, those of   \citet{sabbaghi_explicitly_2024,ahuja_provable_2024} are asymptotic in nature and cannot capture the (most common) case of a \emph{softmax} attention head. Moreover, they proceed, roughly speaking, by showing that the learned model is equal to the ground-truth model \emph{on the entire domain}, which is generally \emph{not} the case with actual language models \cite{zou_universal_2023,wei_jailbroken_2023,andriushchenko_jailbreaking_2024}. As our theoretical setup incorporates distributional assumptions, it establishes length generalization in cases where the learned model is only accurate on in-distribution inputs, which we believe to be more realistic.
Moreover, all of the preceding works generally apply only to specific classes of \emph{transformers or linear classes}; in contrast, our framework, while being able to capture an attention head, is significantly more general in that we allow arbitrary, potentially nonlinear, function classes (see \cref{def:sparse-group-attn}), thus meaning our results may have significance for other (non-transformer) classes of models as well. 

\citet{hou_universal_2024} proposes to use \emph{Turing programs}, a scratchpad strategy inspired by Turing machines, to achieve length generalization on an array of tasks. Their  theoretical results, however, are only representational in nature, showing that transformers can represent Turing programs without  accounting for what models algorithms such as risk minimization will actually learn. \citet{hahn_why_2024} offer reasons why transformers struggle to length-generalize on parity, based on their sensitivity bias. 

Further from our own work, \citet{marsden_provable_2024} obtain provable guarantees for length generalization in the context of dynamical systems. \citet{wang_transformers_2024} prove that gradient descent on transformers can provably learn a task known as the \emph{sparse token selection task} \cite{sanford_representational_2023}, which bears resemblance to \cref{def:sparse-planted} (indeed, a slight modification of the task defined in Section 2.1 of \citet{wang_transformers_2024} is in fact a special case of a distribution with sparse planted correlations (\cref{def:sparse-planted})). 




\subsection{Modifying positional embeddings for length \generalization}
\label{sec:related-posids}
In addition to position coupling \cite{cho_position_2024,cho_arithmetic_2024} (and the closely related Abacus embeddings  \cite{mcleish_transformers_2024}), which is a focus in our paper, several other techniques have been developed to modify position embeddings during training and/or inference time to improve the length generalization performance of transformers. \citet{zhu_pose_2024} developed the positional skip-wise technique (PoSE; see \cref{sec:prelim-pose}), which was later refined by modifying the distribution of position IDs used at training time in \citet{wu_never_2024}. We remark that PoSE is conceptually similar to \emph{randomized position embeddings} \cite{ruoss_randomized_2023}, which trains using a random set of position IDs from the test-length context window (with no guarantee on the contiguity of these IDs, unlike PoSE).

Another popular strategy to extend the context length of transformers which has seen traction for models at larger scales, such as as Code Llama \cite{roziere_code_2024}, is \emph{position interpolation}. This technique \emph{scales down} (``interpolates'') the position IDs in the longer test-length context window to match the length of the shorter training-length context. Several such interpolation strategies have been proposed, including the canonical choice of \emph{linear interpolation} \cite{chen_extending_2023}, as well as \emph{NTK-RoPE} \cite{emozilla_dynamically_2023}, \emph{YaRN} \cite{peng_yarn_2023}, and \emph{CLEX} \cite{chen_clex_2024}; the latter strategies adjust the amount of interpolation done on a per-frequency basis, with different RoPE frequencies receiving different interpolation scales. One downside of these position interpolation strategies is that they generally require some fine-tuning on the longer test-length sequences in order to effectively use the longer context windows (e.g., to achieve decreased perplexity on longer sequences than those seen during training). Such fine-tuning complicates the theoretical setting of length generalization, where it is typically assumed that any amount of training on the longer test-length sequences is not allowed. We remark, however, that these position interpolation techniques can be combined with PoSE \cite{zhu_pose_2024,wu_never_2024}; exploring such combinations in the context of our experiments is an interesting direction for future work.

Finally, we remark that some strategies, such as LM-Infinite \cite{han_lminfinite_2024} and Self-Extend \cite{jin_llm_2024}, have been proposed to adjust the attention mechanism at inference time so as to achieve length generalization without any fine-tuning, though their performance lags somewhat \cite{lu_controlled_2024}. 





\subsection{Empirical evaluations \& explanations of length generalization}
A number of papers have offered empirical evaluations and comparisons of various techniques for length generalization. \citet{anil_exploring_2022} studied a few simple length generalization tasks, such as parity and variable assignment, and found that techniques such as finetuning and using a scratchpad did not lead to much length generalization. 
\citet{kazemnejad_impact_2023} compared the performance of various positional encoding techniques for length generalization, and found that NoPE performed best (though their analysis did not account for techniques such as position interpolation and PoSE which can significantly improve length generalization for, e.g., RoPE).  \citet{lee_teaching_2023} observed that length generalization is difficult for transformers on arithmetic tasks. 
Finally, \citet{lu_controlled_2024} performed a systematic study comparing various approaches to extend the context length of LLMs, including various types of position interpolation. 


\citet{ontanon_making_2022,dziri_faith_2023,hupkes_compositionality_2020} study the out-of-distribution performance of transformers by focusing on \emph{compositional generalization}, which refers to the ability of transformers to compose individual tasks found in the training data. There is also extensive work more broadly on out-of-distribution generalization \cite{nagarajan_understanding_2021,abbe_generalization_2023,kalavasis_transfer_2024}.

Finally, a number of works (e.g., \citet{hupkes_compositionality_2020,liu_exposing_2023,deletang_neural_2023,zhang_unveiling_2023,hsieh_ruler_2024}) introduce new benchmarks and datasets for studying length generalization and more broadly the performance of LLMs on long contexts. 




\paragraph{Additional techniques for length generalization.} Many papers have introduced new types of positional encoding schemes with the hope of improved length generalization, including relative positional embeddings \cite{shaw_self_2018,dai_transformer_2019,huang_improve_2020,ke_rethinking_2021}, ALiBi \cite{press_train_2022}, Hard-Alibi \cite{jelassi_repeat_2024}, and FIRE \cite{li_functional_2024}. 


Many other types of techniques have  been proposed, such as priming the training dataset with a few long sequences \cite{jelassi_length_2023}, %
modifying the format of the input \cite{shen_positional_2023,hu_casebased_2024} such as by using a scratchpad \cite{lanchantin_learning_2023}, architectural changes \cite{csordas_neural_2022,fan_looped_2024}, 
and combining several such techniques \cite{csordas_devil_2022,liu_transformers_2023,zhou_transformers_2024}. Moreover, it was recently shown that length generalization can be achieved via a \emph{self-improvement} framework \cite{lee_selfimproving_2025}, which trains the model on sequences of increasing length, which are labeled by previous versions of the model.  We remark that such techniques lie outside the scope of this paper, as we require that the context length be bounded above by $L$ throughout training. Moreover, such techniques still may suffer from the computational issues that plague longer context lengths. 

\section{Additional Preliminaries}

\icml{
\subsection{Additional context on distributional assumptions}
\label{sec:basic-setup-appendix}
The following proposition rules out length \generalization in the absence of distributional assumptions. 
\begin{proposition}
  \label{prop:noasm-nolg}
Fix any $L, \bar L \in \BN$ with $L < \bar L$, and a hypothesis class $\MH \subset \MY^{\MV^\st}$. Let $\MP_1, \ldots, \MP_{L}$ be $\MH$-realizable distributions, realized by $h^\st$. Suppose that $\ep > 0$ and $\hat h$ defined in \cref{eq:define-hath} satisfies $\sup_{\bX \in \MV^{\bar L}} \Loss(h^\st(\bX), \hat h(\bX)) > \ep$. Then there is an ensemble $\MP$ extending $\MP_1, \ldots, \MP_L$ so that $\MH$ does not have $(L, \bar L, \ep)$-length \generalization with respect to $\MP$. 
\end{proposition}

Below, we give a simple example of a distribution ensemble having $k$-sparse planted correlations (per \cref{def:sparse-planted}).
\input{kgram.tex}

}
\subsection{Transformers}
\label{sec:transformers-prelim}
In this section, we review the basic definitions and notations regarding the transformers architecture. We refer the reader to \citet{phuong_formal_2022} for a more extensive overview. 

\paragraph{Attention heads.} We begin by defining \emph{attention heads.} Consider matrices $\Wk, \Wq, \Wv \in \BR^{h \times d}$ and $\Wo \in \BR^{d \times h}$, where $d$ denotes the \emph{embedding dimension} and $h$ denotes the \emph{attention dimension}.  Let $\theta := (\Wk, \Wq, \Wv, \Wo)$ denote the full set of parameters for an attention head. The corresponding attention head $\Ahead_\theta : (\BR^d)^{L} \to \BR^d$ takes as input a sequence $\bh = (\bh_1, \ldots, \bh_L)$ of $L$ embedding vectors corresponding to individual tokens, as well as a \emph{query embedding vector} $\hquery \in \BR^d$, and outputs the embedding of the next token, $\Ahead_\theta(\bh, \hquery)$ by first computing:
\begin{align}
  \bq \gets \Wq \cdot \hquery \nonumber\\
  \bk_t \gets \Wk \cdot \bh_t, \quad \bv_t \gets \Wv \cdot \bh_t, \quad \alpha_t \gets \sigma(\langle \bq, \bk_1 \rangle, \ldots, \langle \bq, \bk_L \rangle), \quad \forall t \in [L],\nonumber
\end{align}
where typically $\sigma = \softmax$. %
Then, the attention head outputs $\Ahead_\theta(\bh, \hquery) := \Wo \cdot \sum_{t=1}^L \alpha_t \cdot \bv_t$.

\paragraph{Masked self-attention.} We consider transformers which perform masked self-attention, meaning that they aim to predict each token in a sequence by attending to previous tokens in the sequence. Moreover, in doing so, multiple attention heads are combined into an attention layer, of which multiple are combined into a transformer. First, we describe an attention layer: for some $m \in \BN$, %
we consider $m$ attention heads parametrized by $\theta_1, \ldots, \theta_m$, together with a multilayer perceptron $\MLP : \BR^d \to \BR^d$, with 2 layers. Letting $\theta = (\theta_1, \ldots, \theta_h, \MLP)$, we let the attention layer $\Alayer_\theta : (\BR^d)^{L} \to (\BR^d)^L$ be the mapping defined as follows: for $i \in [L]$ and $\bh = (\bh_1, \ldots, \bh_L) \in (\BR^d)^L$, 
\[
\Alayer_\theta(\bh)_i := \MLP(\bh_i + \by) + \bh_i + \by, \quad \by := \sum_{j=1}^m \Ahead_{\theta_j}(\bh_{1:i}, \bh_i).
\]
Note that $\Alayer_\theta(\bh)_i$ depends only on the embeddings up to index $i$, i.e., $\bh_1, \ldots, \bh_i$. Finally, given a collection $\theta = ((\theta\^1, \ldots, \theta\^\Lambda), \Wembed)$ denoting parameters for each of $\Lambda \in \BN$ layers together with an \emph{embedding matrix} $\Wembed \in \BR^{d \times |\MV|}$, we define $\Transformer_\theta : \MV^L \to (\BR^d)^L$ by, for $\bh = (\bh_1, \ldots, \bh_L) \in (\BR^d)^L$,
\begin{align}
\Transformer_\theta(\bX) := \Alayer_{\theta\^\Lambda} \circ \cdots \circ \Alayer_{\theta\^1}(\Wembed \cdot \bX)\nonumber,
\end{align}
where $\bX$ is interpreted as a matrix in $\BR^{|\MV| \times L}$ whose columns are the one-hot encodings of each of the tokens of $\bX$. To produce a sequence of tokens, typically $\Transformer_\theta$ is composed with an \emph{unembedding matrix} $\Wunembed \in \BR^{|\MV| \times d}$ to each position, and a distribution for each position may be obtained by applying a softmax.

\icml{\input{pose-overview.tex}}

\subsection{Sparse functional attention captures a single attention head}
In this section, we prove \cref{prop:model-attn-head-formal}, which shows that the $k$-sparse functional attention class in \cref{def:sparse-group-attn} is sufficiently general so as to capture the class of single attention heads (as defined in \cref{sec:transformers-prelim}). First we formally specify the relevant class of attention heads; to ensure that the class maps from sequences of \emph{tokens} (as defined in \cref{def:sparse-group-attn}) as opposed to sequences of \emph{embedding vectors} (as defined in \cref{sec:transformers-prelim}), we include an embedding matrix in the definition:
\begin{definition}[Single attention heads]
  Fix $d,h \in \BN$ and a finite set $\MV$ denoting the vocabulary. We define the class $\Aheads_{d,h,\MV}$, which consists of mappings $h : \MV^\st \to \BR^d$, as follows. It is the set of all mappings from $\bX \in \MV^L$ to an embedding vector as follows:
  \begin{align}
\bX \mapsto \Ahead_\theta(\Wembed \cdot \bX, \hquery)\nonumber,
  \end{align}
  where $\bW$ ranges over all matrices $\bW \in \BR^{d \times |\MV|}$, $\hquery$ ranges over all vectors $\hquery \in \BR^d$,  and $\theta$ ranges over all tuples $\theta = (\bK, \bQ, \bV, \bO)$, where $\bK, \bQ, \bV \in \BR^{h \times d}$ and $\bO \in \BR^{d \times h}$. Moreover, we are slightly abusing notation by interpreting $\bX$ as the matrix $\bX \in \BR^{|\MV| \times L}$ whose columns are the one-hot vectors of the individual tokens of the sequence $\bX$. 
\end{definition}

\begin{proposition}[Formal version of \cref{prop:model-attn-head}]
  \label{prop:model-attn-head-formal}
  Fix any $d,h \in \BN$ and finite set $\MV$. Then there are function classes $\Gpred$ consisting of functions mapping $\BN \times \MV \to (\BR \cup \{ -\infty \})$ and $\Gval$ consisting of functions mapping $\MV \to \BR^d$ so that $\Aheads_{d,h,\MV} $ is equal to the 1-sparse functional attention class $\Hpred(\Gpred, \Gval)$. 
\end{proposition}
\begin{proof}
  We let $\Gpred$ be the set of all mappings $g_0 : \BN \times \MV \to \BR$ which only depend on the $\MV$-component of the input, and $\Gval$ be the set of all mappings $g_1 : \MV \to \BR^d$. 

  Now fix any $\theta = (\bK, \bQ, \bV, \bO), \hquery, \Wembed$ indexing some element of $\Aheads_{d,h,\MV}$. For any $L \in \BN$, given $\bX \in \BR^{|\MV| \times L}$ whose columns represent the one-hot encoding vectors for some length-$L$ sequence, we have
  \begin{align}
    \Ahead_\theta(\Wembed \cdot \bX, \hquery) = & \bO \cdot \sum_{i=1}^L \alpha_i \cdot \bV  \Wembed  \bX_i \nonumber\\
\mbox{ where } \quad     \alpha_i =&  \softmax(\langle \bQ \hquery, \bK \Wembed \bX_1 \rangle, \ldots, \langle \bQ \hquery, \bK \Wembed \bX_L \rangle), \quad i \in [L]\nonumber.
  \end{align}
  Define $g_0 \in \Gpred$ by, for $\bx \in \MV$,
  \begin{align}
g_0(i, \bx) = \langle \bQ \hquery, \bK \Wembed \bx \rangle\nonumber,
  \end{align}
(where we conflate $\bx \in \MV$ and its one-hot encoding vector in $\BR^{|\MV|}$),  and define $g_1 \in \Gval$ by
  \begin{align}
g_1(\bx) := \bO\bV \Wembed \cdot \bx\nonumber.
  \end{align}
  It is now clear that the mapping $\bX \to \Ahead_\theta(\Wembed \cdot \bX, \hquery)$ is identical to the 1-sparse functional attention mapping $h_{g_0, g_1}$ defined in \cref{def:sparse-group-attn}.

  Moreover, since we have assumed $h \geq d$, it is evident that any $h_{g_0, g_1}$, for $g_0 \in \Gpred, g_1 \in \Gval$ may be realized as $\bX \mapsto \Ahead_\theta(\Wembed \cdot \bX, \hquery)$ for some $\theta, \hquery, \Wembed$ as above. 
\end{proof}


\section{Proofs for length \generalization}
\icml{
First, we make a remark.
  \begin{remark}[On \cref{asm:density-ratio-position}]
    To help interpret \cref{asm:density-ratio-position}, note first that in order for the distribution ensemble $\MP$ to be  realizable (\cref{asm:unique-selection}) by a class $\Hpred(\Gpred, \Gval)$ satisfying $\Llocal$-locality (\cref{asm:local-relative}), it will typically be the case that $\max_{i \in S^\st} i - \min_{i \in S^\st} i \leq \Llocal$ with probability at least  $1-\delta$ for $S^\st \in \Qpos_\ell$, for any $\ell \in \BN$. Thus, a natural choice for $\Qpos_\ell$ is to fix some distribution $\Qpos_{\Llocal}$ over sets in $\Sets{[\Llocal]}{k}$ and let $\Qpos_\ell$ be the distribution of a random shift of a sample from $\Qpos_{\Llocal}$. Formally, $\Qpos_\ell$ is the distribution of the shift $S + \Delta$, where $S \sim \Qpos_{\Llocal}$ and $\Delta \sim \Unif(\{ 0, 1, \ldots, \ell - \Llocal\})$. It is straightforward to see that such a construction ensures that \cref{asm:density-ratio-position} is satisfied with $\eta_\ell = \ell$. More broadly, any value $\eta_\ell \leq \poly(\ell)$ leads to interesting conclusions in the context of our results, so we interpret \cref{asm:density-ratio-position} as being fairly mild.
    \end{remark}
  }
\subsection{Proof of \cref{thm:length-extrap}}
\begin{proof}[Proof of \cref{thm:length-extrap}]
  Write $\Hpred := \Hpred(\Gpred, \Gval)$. 
  By \cref{asm:unique-selection}, %
  we have $ \E_{(\bX, \bY) \sim \MP_\ell}[\Loss(h^\st(\bX), \bY)] \leq \delta$ for some $h^\st \in \Hpred$. Let $\hat h \in \Hpred$ be chosen according to \cref{eq:define-hath}, so that we can write $\hat h = h_{\hat g_0, \hat g_1}$ for some $\hat g_0 \in \Gpred, \hat g_1 \in \Gval$.  Thus, for each $\ell \in [L/2,L]$, we have
  \begin{align}
    L\delta  \geq & \E_{(\bX, \bY) \sim \MP_\ell}[\Loss(\hat h(\bX), \bY)] \nonumber\\
    = &    \E_{(\bX, \bY) \sim \MP_\ell} \left[ \norm{ \bY -  \frac{\sum_{S \in \Sets{[\ell]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[\ell]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right]\nonumber \\
      = &    \E_{(\bX, S^\st) \sim \MP_\ell} \left[ \norm{ g^\st(\bX_{S^\st}) -  \frac{\sum_{S \in \Sets{[\ell]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[\ell]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right]\label{eq:indist-l},  
  \end{align}
  where $g^\st : \MV^k \to \MY$ is the function guaranteed by \cref{def:sparse-planted}. 
  By \cref{asm:density-ratio-position}, for any $S^\st \in \Sets{[L]}{k}$, we have $\frac{\MQ_{L-2\Llocal}(S^\st)}{\MQ_L(S^\st)} \leq \eta_L$ and $\frac{\MQ_{L-\Llocal}(S^\st)}{\MQ_L(S^\st)} \leq \eta_L$. (Recall that if, e.g., $S^\st \not \in \Sets{[L-\Llocal]}{k}$, then we use the convention that $\MQ_{L-\Llocal}(S^\st) = 0$.)  %
  At various points later in the proof, we will need to use \cref{eq:indist-l} for $\ell = L - \Llocal$ or $\ell = L - 2\Llocal$; in order for this to be valid, we need $L - 2\Llocal \geq L/2$, i.e., $L \geq 4\Llocal$, which is ensured by the hypotheses of \cref{thm:length-extrap}. 
  
  Thus, by the definition of the $k$-sparse planted structure (\cref{def:sparse-planted}), %
  we have that, for any $\bX \in \MV^L, S^\st \in \Sets{[L]}{k}$, 
  \begin{align}
\frac{\MP_{L-2\Llocal}(\bX_{1:L-2\Llocal}, S^\st) \cdot \prod_{i=L-2\Llocal+1}^L \mu(\bX_i)}{\MP_L(\bX, S^\st)} \leq \eta_L, \qquad \frac{\MP_{L-2\Llocal}(\bX_{1:L-2\Llocal}, S^\st) \cdot \prod_{i=L-2\Llocal+1}^{L-\Llocal} \mu(\bX_i)}{\MP_{L-\Llocal}(\bX, S^\st)} \leq \eta_L\nonumber.
  \end{align}
  It follows that
  \begin{align}
    \eta_L \cdot (L\delta) \geq & \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_{L-2\Llocal+1:L} \sim \mu^{\otimes 2\Llocal}}} \left[\norm{ g^\st(\bX_{S^\st}) - \frac{ \sum_{S \in \Sets{[L]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right] \label{eq:L2L-sstar}\\
    \eta_L \cdot (L \delta) \geq & \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_{L-2\Llocal+1:L-\Llocal} \sim \mu^{\otimes \Llocal}}} \left[\norm{ g^\st(\bX_{S^\st}) -  \frac{\sum_{S \in \Sets{[L-\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L-\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right] \label{eq:ll-sstar}.
  \end{align}
  The triangle inequality then gives
  \begin{align}
2\eta_L \cdot  (L \delta) \geq & \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_{L-2\Llocal+1:L} \sim \mu^{\otimes 2\Llocal}}} \left[\norm{ \frac{\sum_{S \in \Sets{[L]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L]}{k}} \exp(\hat g_0(S', \bX_{S'}))}  -  \frac{\sum_{S \in \Sets{[L-\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L-\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right]\label{eq:l-triangle}. 
  \end{align}
  We define the following random variables, where  $(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal}$, $\bX_j \sim \mu$ for $j > L-2\Llocal$, and $i \geq 1$: 
  \begin{align}
    A_0  := \sum_{S \in \Sets{[L-2\Llocal]}{k}} {\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}, \qquad B_0 := \sum_{S' \in \Sets{[L-2\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))\nonumber\\
    A_i = \sum_{S \in \Sets{[L-2\Llocal+i\Llocal]}{k}\backslash \Sets{[L-2\Llocal+(i-1)\Llocal]}{k}} \exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S), \nonumber\\
    B_i := \sum_{S \in \Sets{[L-2\Llocal+i\Llocal]}{k} \backslash \Sets{[L-2\Llocal+ (i-1)\Llocal]}{k}} \exp(\hat g_0(S, \bX_S))\nonumber.
  \end{align}
  Note that $A_0, A_1, \ldots \in \BR^d$ are vectors, and $B_0, B_1, \ldots \in \BR$ are scalars.
  We have that
  \begin{align}
\E \left[ \norm{ \frac{A_0}{B_0} - \frac{A_0 + A_1 }{B_0 + B_1} }\right] \leq 2\eta_L \cdot (L \delta), \qquad \E\left[ \norm{ \frac{A_0 + A_1 + A_2}{B_0 + B_1 + B_2} - \frac{A_0 + A_1}{B_0 + B_1} } \right] \leq 2\eta_L \cdot (L\delta)\nonumber,
  \end{align}
  where the first inequality uses \cref{eq:indist-l} with $\ell = L - 2\Llocal$, \cref{eq:ll-sstar} and the triangle inequality, and the second inequality uses \cref{eq:l-triangle}. %
  Let $Z_0$ denote the collection of random variables $\bX_{1:L-2\Llocal}$, and for $i \geq 1$, let $Z_i$ denote the collection of random variables $\bX_{L-2\Llocal + (i-1)\Llocal : L-2\Llocal + i\Llocal}$. Then $Z_0, Z_1, \ldots$ are independent, and $Z_1, Z_2, \ldots$ are identically distributed (since we have $(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal}$).  Since $\hat g_0(S, \bX_S) = -\infty$ for any $S$ for which $\max(S) - \min(S) > \Llocal$ (\cref{it:local} of \cref{asm:local-relative}), $A_i, B_i$ can each be written as functions of $Z_i, Z_{i-1}$ for each $i \geq 0$. Moreover, by \cref{it:relative} of \cref{asm:local-relative}, this function does not depend on $i$ for $i \geq 1$. By \cref{lem:hybrid}, it follows that, for any $t \geq -2$, 
  {\small\begin{align}
          &  \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_j \sim \mu \ \forall j > L-2\Llocal}}\left[ \norm{  \frac{\sum_{S \in \Sets{[L-2\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L-2\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))} -  \frac{\sum_{S \in \Sets{[L+ t\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L + t\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))}}\right] \nonumber\\
           \leq  & 6\eta_L (L \delta)  (t+2)\nonumber.
 \end{align}}
       A symmetric argument establishes that for any $t \geq 0$,
{\small\begin{align}
         & \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_j \sim \mu \ \forall j \leq 0}}\left[ \norm{  \frac{\sum_{S \in \Sets{[L-2\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L-2\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))} -  \frac{\sum_{S \in \Sets{[-t\Llocal+1,L-2\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[-t\Llocal+1,L -2\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))}}\right] \nonumber\\
         \leq &  6\eta_L(L \delta)  \cdot t\nonumber.
       \end{align}}
     Combining the two displays above with \cref{lem:ab-pi-vi} (and again using \cref{it:local} of \cref{asm:local-relative}), we see that for any $t_0 \geq 0, t_1 \geq -2$,
     {\small\begin{align}
         & \E_{\substack{(\bX_{1:L-2\Llocal}, S^\st) \sim \MP_{L-2\Llocal} \\ \bX_j \sim \mu \ \forall j \leq 0, j > L-2\Llocal}}\left[ \norm{  \frac{\sum_{S \in \Sets{[L-2\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L-2\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))} -  \frac{\sum_{S \in \Sets{[-t_0 \Llocal+1,L + t_1\Llocal]}{k}}\exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[-t_0\Llocal+1,L +t_1\Llocal]}{k}} \exp(\hat g_0(S', \bX_{S'}))}}\right] \nonumber\\
         \leq &  6\eta_L(L \delta)  (t_0 + t_1+2)\label{eq:add-t0-t1}.
            \end{align}}
          Let $\MP_{L-2\Llocal, t_0, t_1}$ denote the distribution of $\tilde \bX_{1:L + \Llocal(t_0 + t_1)} \in \MV^{L + \Llocal(t_0 + t_1)}$ and $\tilde S^\st \in \Sets{[L + \Llocal(t_0 + t_1)]}{k}$ defined as follows: first, draw $(\bX_{1:L-\Llocal}, S^\st) \sim \MP_{L-2\Llocal}$, as well as $\bX_j \sim \mu$ for all $j \leq 0, \ j > L-2\Llocal$, and then set $\tilde \bX_j = \bX_{j - t_0\Llocal}$ for $j \in [L + \Llocal(t_0 + t_1)]$ and $\tilde S^\st = S^\st + t_0 \Llocal$. Then \cref{eq:add-t0-t1} together with \cref{eq:indist-l} (with $\ell = L- 2\Llocal$) gives that, for all $t_0 \geq 0, t_1 \geq -2$, 
          \begin{align}
            & \E_{(\bX_{1:L + \Llocal(t_0 + t_1)}, S^\st) \sim \MP_{L-2\Llocal, t_0, t_1}}\left[ \norm{ g^\st(\bX_{S^\st}) - \frac{\sum_{S \in \Sets{[L + \Llocal(t_0 + t_1)]}{k}} \exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[L + \Llocal(t_0 + t_1)]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right]\nonumber\\
            \leq & O(\eta_L \cdot  (L \delta) \cdot (t_0 + t_1 + 2))\label{eq:etal-t0-t1}.
          \end{align}
          By \cref{def:sparse-planted} and \cref{asm:density-ratio-position}, for any $\bX_{1:\bar L} \in \MV^{\bar L}$ and $S^\st \in \Sets{[\bar L]}{k}$ with $\max\{S^\st \} - \min\{ S^\st\} \leq \Llocal$, there are some $t_0 \geq 0, t_1 \geq -2$ satisfying $t_0 + t_1 + 2 = (\bar L - L) / \Llocal$\footnote{In particular, any choice of $t_0$ satisfying $S^\st - t_0 \cdot \Llocal \in \Sets{[L - 2\Llocal]}{k}$ suffices.}  %
          so that
          \begin{align}
\frac{\MP_{\bar L}(\bX_{1:\bar L}, S^\st)}{\MP_{L-2\Llocal, t_0, t_1}(\bX_{1:\bar L}, S^\st)} \leq \frac{\MQ_{\bar L}(S^\st)}{\MQ_{L-2\Llocal}(S^\st - t_0 \cdot \Llocal)} \leq \eta_{\bar L} \label{eq:eta-barl-ub}.
          \end{align}
Noting that we have $\bar L = L + \Llocal(t_0 + t_1)$, using \cref{eq:eta-barl-ub} and summing \cref{eq:etal-t0-t1} over all possible values of $t_0,t_1$ for which $t_0 + t_1 + 2 = (\bar L - L)/\Llocal$ gives that
          \begin{align}
            & \E_{(\bX_{1:\bar L}, S^\st) \sim \MP_{\bar L}}\left[ \norm{ g^\st(\bX_{S^\st}) - \frac{\sum_{S \in \Sets{[\bar L]}{k}} \exp(\hat g_0(S, \bX_S)) \cdot \hat g_1(\bX_S)}{\sum_{S' \in \Sets{[\bar L]}{k}} \exp(\hat g_0(S', \bX_{S'}))} } \right]\nonumber\\
            \leq & O(\eta_L \eta_{\bar L} \cdot (L \delta) \cdot (t_0 + t_1)^2)\nonumber.
          \end{align}
          Using that $g^\st(\bX_{S^\st}) = \bY$ under $(\bX_{1:\bar L}, S^\st, \bY) \sim \MP_{\bar L}$, we conclude that
          \begin{align}
\E_{(\bX, \bY) \sim \MP_{\bar L}} \left[ \Loss(\hat h(\bX), \bY)\right] \leq O(\eta_L \eta_{\bar L} \cdot L \bar L^2 \cdot \delta)\nonumber.
          \end{align}
\end{proof}


\begin{lemma}
  \label{lem:hybrid}
  Suppose that $Z_0, Z_1, Z_2, \ldots$ are independent random variables with sample space $\Omega$, and that $Z_1, Z_2, \ldots$ are identically distributed. Furthermore suppose $f, f_0 : \Omega \to \BR^d$ and $g,g_0 : \Omega\to \BR$ are measurable functions, and let $A_i = f(Z_i, Z_{i-1}), B_i = g(Z_i, Z_{i-1})$ for each $i \geq 1$, and $A_0 = f_0(Z_1, Z_0), B_0 = g_0(Z_1, Z_0)$.  Suppose $\ep > 0$ satisfies 
  \begin{align}
\E \left[ \norm{ \frac{A_0}{B_0} - \frac{A_0+A_1}{B_0+B_1} }\right] \leq \ep, \quad \E\left[ \norm{ \frac{A_0+A_1}{B_0+B_1} - \frac{A_0 + A_1 + A_2}{B_0 + B_1 + B_2} } \right] \leq \ep\label{eq:ep-asm}.
  \end{align}
  Then for any $i \geq 1$, we have
  \begin{align}
\E \left[ \norm{ \frac{A_0}{B_0} - \frac{A_0 + A_1 + \cdots + A_i}{B_0 + B_1 + \cdots + B_i} } \right] \leq 3i\ep\nonumber.
  \end{align}
\end{lemma}
\begin{proof}
  We use a hybrid argument. Fix any $i \geq 2$, and define $A_1' = f(Z_1, Z_i), B_1' = g(Z_1, Z_i)$. We have
  \begin{align}
    & \E\left[ \norm{ \frac{A_0}{B_0} - \frac{A_0 + A_1 + A_i}{B_0 + B_1 + B_i} } \right] \nonumber\\
    \leq & \E\left[ \norm{ \frac{A_0}{B_0} - \frac{A_0 + A_1'}{B_0 + B_1'} } \right] + \E\left[ \norm{ \frac{A_0+A_1'}{B_0+B_1'} - \frac{A_0 + A_1 + A_i}{B_0 + B_1 + B_i} } \right]\leq 2\ep\nonumber,
  \end{align}
  where the final inequality follows from \cref{eq:ep-asm} together with the fact that $Z_1, Z_2, Z_i$ are iid. By the triangle inequality, it follows that
  \begin{align}
 \E\left[ \norm{ \frac{A_0+A_1}{B_0+B_1} - \frac{A_0 + A_1 + A_i}{B_0 + B_1 + B_i} } \right]  \leq 3\ep\nonumber. 
  \end{align}
  Fix any $j \geq 2$. Summing over $2 \leq i \leq j$ and using \cref{lem:ab-pi-vi}, we see that
  \begin{align}
\E\left[ \norm{ \frac{A_0+A_1}{B_0 + B_1} - \frac{A_0 + A_1 + \cdots + A_j}{B_0 + B_1 + \cdots + B_j} } \right] \leq 3\ep \cdot (j-1)\nonumber.
  \end{align}
  A final application of the triangle inequality yields the conclusion of the lemma. 
\end{proof}

\begin{lemma}
  \label{lem:ab-pi-vi}
  Let $L \in \BN$ and $a,q_i \in \BR^d$ be vectors for $1 \leq i \leq L$ and $b, p_i \in \BR_{>0}$ be positive real numbers for $1 \leq i \leq L$. Suppose that for each $i \in [L]$, $\norm{ \frac{a}{b} - \frac{q_i + a}{p_i + b} } = \epsilon_i.$ Then
  \[
\norm{ \frac{a}{b} - \frac{\sum_{i=1}^L q_i + a}{\sum_{i=1}^L p_i + b} } \leq \sum_{i=1}^L \ep_i.
\]
\end{lemma}
\begin{proof}
 For each \( i \), we have:

\[
\norm{ \frac{p_i a - bq_i}{b (p_i + b)} } = \norm{ \frac{a}{b} - \frac{q_i + a}{p_i + b} } \leq \epsilon_i.
\]

Multiplying both sides by \( b (p_i + b) \) (which is positive), we see that
\[
\norm{ p_ia-q_ib } \leq \epsilon_i b (p_i + b).
\]

Summing both sides over \( i = 1 \) to \( L \):

\[
\norm{ a \sum_{i=1}^L p_i - b \sum_{i=1}^L q_i} = \norm{ \sum_{i=1}^L p_ia - q_ib } \leq  b \sum_{i=1}^L\ep_i (p_i + b).
\]


Let $\ep = \sum_{i=1}^L \ep_i$, \( S = \sum_{i=1}^L p_i \) and \( V = \sum_{i=1}^L q_i \), so $ 
\norm{ a S - b V } \leq \epsilon b \left( S +  b \right).$ 
We need to bound $
\norm{ \frac{a}{b} - \frac{V + a}{S + b} }.
$
To do so, we compute
\[
\norm{ \frac{a}{b} - \frac{V + a}{S + b} }=\norm{ \frac{a S - b V}{b (S + b)} } \leq \frac{\epsilon b \left( S +  b \right)}{b (S + b)} = \epsilon,
\]
which gives the desired bound. 
\end{proof}

\subsection{On necessity of the assumptions}
\label{sec:lower-bounds}
In the below proposition, we establish that each of the assumptions of \cref{thm:length-extrap} is necessary in that none of them can be individually removed.

\begin{proposition}
  \label{prop:necessity-asms}
There is a constant $c > 0$ so that the following holds. For any of the assumptions: (a) \cref{it:local} of \cref{asm:local-relative}, (b) \cref{it:relative} of \cref{asm:local-relative}, (c) \cref{asm:unique-selection} with $\delta = 0$, (d) \cref{asm:density-ratio-position} with $\eta_\ell = O(\ell)$, and any integer $L \in \BN$, there is a $2$-sparse functional attention class $\Hpred = \Hpred(\Gpred, \Gval)$ and a $2$-sparse planted correlations distribution ensemble $\MP$ which satisfy each of the other 3 assumptions with $\Llocal = 2$ but which does not have $(L, \bar L, c)$-length \generalization for any $\bar L \geq 2L$. 
\end{proposition}
\begin{proof}
  We show in turn that each of the assumptions cannot be removed. For all of the examples, we set $k = 2$, and $\MV = \{1,2\}$.  We let $\MY = [0,1]$ be the unit interval, and let $\Loss(\bY, \bY') := |\bY - \bY'|$.

  \paragraph{Removing locality.} We first show that $\Llocal$-locality (\cref{it:local} of \cref{asm:local-relative}) cannot be dropped.   We consider the sparse function attention class $\Hpred = \Hpred(\Gpred, \Gval)$ where $\Gpred, \Gval$ are defined as follows:
\begin{itemize}
\item $\Gpred= \{ g_{00}, g_{01} \}$, defined as follows:
  \begin{align}
    g_{00}(S, \bx) =& \begin{cases}
      0 &: \bx = (2,2) \mbox{ or } \max\{ S \} - \min\{ S \} \geq L \\
      -\infty &: \mbox{otherwise}
    \end{cases}\nonumber\\
    g_{01}(S, \bx) =& \begin{cases}
      0& : \bx = (2,2) \\
      -\infty &: \mbox{otherwise}.
      \end{cases}\nonumber
  \end{align}
\item $\Gval$ consists of the single function $g_1 : \MV^2 \to \MY$ defined as $g_1(2,2) = 1$ and $g_1(\bx_1, \bx_2) = 0$ for all $(\bx_1, \bx_2) \neq (2,2)$. 
\end{itemize}

Fix any integer $L$. We define a distribution ensemble $\MP$ with $k$-sparse planted correlations, as specified by distributions $\mu \in \Delta(\MV), \Qpos_\ell \in \Delta(\Sets{[\ell]}{k})$ for each $\ell \in \BN$, and $\Qvoc \in \Delta(\MV^k)$, as well as a function $g^\st : \MV^k \to \MY$, as follows:
  \begin{itemize}
  \item $\mu$ is the point mass on $\{1 \}$. 
  \item $\Qvoc$ is the point mass on $\{ (2,2) \}$. 
  \item $\Qpos_\ell$ is uniform over the sets $\{1, 2\}, \{2,3\}, \ldots, \{\ell-1, \ell \}$ if $\ell \leq L$; and is uniform over the sets $\{ 1, 2\}, \ldots, \{ \ell-1, \ell \}, \{ 1, L+1 \} , \{2, L+2\}, \ldots, \{\ell - L, \ell \}$ if $\ell > L$.
  \item We set $g^\st = g_{1} \in \Gval$ as defined above. 
  \end{itemize}

  It is immediate from the above definitions that $\Hpred$ is relative (i.e., \cref{it:relative} of \cref{asm:local-relative}). Next, realizability (\cref{asm:unique-selection}) is clearly satisfied as the function $h_{g_{00}, g_{1}}$ achieves $0$ loss. Finally, coverage (\cref{asm:density-ratio-position}) is readily verified by taking $\eta_\ell = 2\ell$ and $\Llocal = 2$. (In particular, for $S^\st = \{ a, a+L \} \in \Sets{[\ell]}{k}$ and $\ell' \leq L$, we have that $S^\st - \Delta\not \in \Sets{[\ell']}{k}$ for all choices of $\Delta$ and so we do not need to satisfy $\frac{\Qpos_\ell(S^\st)}{\Qpos_{\ell'}(S^\st - \Delta)} \leq \eta_\ell$ (such an inequality does not even make sense).

  Finally, note that the risk minimization procedure which returns $\hat h = h_{g_{00}, g_1}$ satisfies  \cref{eq:define-hath} with the chosen value of $L$ and error $\delta = 0$. However, it is readily checked that $\hat h$ experiences error $\Omega(1)$ on the distribution $\MP_\ell$ for any $\ell \geq 2L$.

  \paragraph{Removing the relative assumption.} Next we show that \cref{it:relative} of \cref{asm:local-relative} cannot be dropped. We consider the sparse attention class defined by the following $\Gpred, \Gval$:
  \begin{itemize}
\item $\Gpred= \{ g_{00}, g_{01} \}$, defined as follows:
  \begin{align}
    g_{00}(S, \bx) =& \begin{cases}
      0 &: \bx = (2,2) \mbox{ and } \max\{ S \} - \min\{S \} = 1 \\
      -\infty &: \mbox{otherwise}
    \end{cases}\nonumber\\
    g_{01}(S, \bx) =& \begin{cases}
      0& : \bx =(2,2) \mbox{ and }\max\{S \} - \min\{S \} = 1 \\
      0 &: \max\{ S \} - \min\{S \} = 2, \max\{S \} > L \\
      -\infty &: \mbox{otherwise}.
      \end{cases}\nonumber
  \end{align}
\item $\Gval$ consists of the single function $g_1 : \MV^2 \to \MY$ defined as $g_1(2,2) = 1$ and $g_1(\bx_1, \bx_2) = 0$ for all $(\bx_1, \bx_2) \neq (2,2)$. 
\end{itemize}
Fix an integer $L$. We define an ensemble $\MP$ specified by $\mu, \Qpos_\ell, \Qvoc, g^\st$ as follows:
\begin{itemize}
  \item $\mu$ is the point mass on $\{1 \}$. 
  \item $\Qvoc$ is the point mass on $\{ (2,2) \}$. 
  \item $\Qpos_\ell$ is uniform over the sets $\{1, 2\}, \{2,3\}, \ldots, \{\ell-1, \ell \}$. 
  \item $g^\st = g_{1} \in \Gval$. 
  \end{itemize}
  It is immediate from the above definitions that $\Hpred = \Hpred(\Gpred, \Gval)$ is $\Llocal$-local (\cref{it:local} of \cref{asm:local-relative}) with $\Llocal = 2$. Realizability (\cref{asm:unique-selection}) is satisfied with $h^\st = h_{g_{00}, g_{1}}$. Coverage (\cref{asm:density-ratio-position}) is satisfied with $\Llocal = 1$ and $\eta_\ell = \ell$.

  The risk minimization procedure which returns $\hat h = h_{g_{01}, g_1}$ satisfies \cref{eq:define-hath} with the chosen value of $L$ and error $\delta = 0$, but $\hat h$ experiences error $\Omega(1)$ on the distribution $\MP_\ell$ for any $\ell \geq L+1$.

  \paragraph{Removing realizability.} Next, we show that \cref{asm:unique-selection} cannot be dropped. We consider the sparse attention class defined by the following $\Gpred, \Gval$:
  \begin{itemize}
\item $\Gpred= \{ g_{00}, g_{01} \}$, defined as follows:
  \begin{align}
    g_{00}(S, \bx) =& \begin{cases}
      0 &: \max\{ S \} - \min\{S \} = 1 \mbox{ and } \bx = (2,2)\\
      -\infty &: \mbox{otherwise}
    \end{cases}\nonumber\\
    g_{01}(S, \bx) =& \begin{cases}
      0& : \max\{S \} - \min\{S \} = 2 \mbox{ and } \bx = (2,2) \\
      -\infty &: \mbox{otherwise}.
      \end{cases}\nonumber
  \end{align}
\end{itemize}
Fix an integer $L$. We define an ensemble $\MP$ specified by the following $\mu, \Qpos_\ell, \Qvoc, g^\st$:
\begin{itemize}
  \item $\mu$ is the point mass on $\{1 \}$. 
  \item $\Qvoc$ is the point mass on $\{ (2,2) \}$. 
  \item $\Qpos_\ell$ is uniform over the sets $\{1, 2\}, \{2,3\}, \ldots, \{\ell-1, \ell \}$ if $\ell \leq L$. For $\ell > L$, we define:
    \begin{align}
      \Qpos_\ell(\{ 1, 2\}) =  \cdots = \Qpos_\ell( \{ \ell-1, \ell \}) = \frac{1}{10(\ell-1)} \nonumber\\
      \Qpos_\ell(\{ 1, 3 \}) = \Qpos_\ell(  \{2, 4\}) =  \cdots = \Qpos_\ell(\{\ell - 2, \ell \})  = \frac{9}{10(\ell-2)}\label{eq:qpos-910}.
    \end{align}
    
    \item $g^\st = g_1 \in \Gval$. 
    \end{itemize}
    It is immediate from the above definitions that $\Hpred = \Hpred(\Gpred, \Gval)$ is $\Llocal$-local and relative (\cref{asm:local-relative}) with $\Llocal = 2$. Moreover, coverage (\cref{asm:density-ratio-position}) is satisfied with $\eta_\ell = \ell$.

    However, note that the hypothesis $\hat h = h_{g_{00}, g_1}$ achieves {0 expected loss} on the distributions $\MP_\ell$ for $\ell \leq L$, but $\Omega(1)$ expected loss on the distributions $\MP_\ell$ for $\ell > L$.\footnote{Note that for $\bX \sim \MP_\ell$ for $\ell > L$, we will have that $g_{00}(S, \bX_S) = -\infty$ for all $S$ with nonzero probability; here, recall from \cref{def:sparse-group-attn} that $h_{g_{00}, g_1}(\bX) = \frac{1}{|\Sets{[\ell]}{k}|} \sum_{S \in \Sets{[\ell]}{k}} g_1(\bX_S)$.} Moreover, this example shows that we cannot even hope for an agnostic guarantee whereby $\hat h$ is close to the best-in-class hypothesis, since for any $\ell > L$, we in fact have\footnote{Ensuring that \cref{eq:worst-in-class} holds is the reason that we have $\Qpos_\ell$ put more mass on sets $S$ with $\max\{S \} - \min\{S\}  = 2$ in \cref{eq:qpos-910}.}
    \begin{align}
E_{(\bX, \bY) \sim \MP_\ell}[\Loss(\hat h(\bX), \bY)] \geq \E_{(\bX, \bY) \sim \MP_\ell}[\Loss(h_{g_{01}, g_1}(\bX), \bY)] + \Omega(1)\label{eq:worst-in-class}.
    \end{align}

    \paragraph{Removing coverage.} Finally, we show that \cref{asm:density-ratio-position} cannot be dropped. We consider the sparse attention class defined by the following $\Gpred, \Gval$: 
      \begin{itemize}
\item $\Gpred= \{ g_{00}, g_{01} \}$, defined as follows:
  \begin{align}
    g_{00}(S, \bx) =& \begin{cases}
      0 &: \bx = (2,2) \mbox{ and } \max\{ S \} - \min\{S \} = 1 \\
      -\infty &: \mbox{otherwise}
    \end{cases}\nonumber\\
    g_{01}(S, \bx) =& \begin{cases}
      0& : \bx = (2,2) \mbox{ and }\max\{S \} - \min\{S \} \in \{1,2\}\\
      -\infty &: \mbox{otherwise}.
      \end{cases}\nonumber
  \end{align}
\item $\Gval$ consists of the single function $g_1 : \MV^2 \to \MY$ defined as $g_1(2,2) = 1$ and $g_1(\bx_1, \bx_2) = 0$ for all $(\bx_1, \bx_2) \neq (2,2)$.
\end{itemize}
Fix an integer $L$. We define an ensemble $\MP$ specified by the following $\mu, \Qpos_\ell, \Qvoc, g^\st$:
\begin{itemize}
  \item $\mu$ is the point mass on $\{1 \}$. 
  \item $\Qvoc$ is the point mass on $\{ (2,2) \}$. 
  \item $\Qpos_\ell$ is uniform over the sets $\{1, 2\}, \{2,3\}, \ldots, \{\ell-1, \ell \}$ if $\ell \leq L$, and uniform over the sets $\{ 1,3\}, \{2,4\}, \ldots, \{\ell-2, \ell \}$ if $\ell > L$. 
  \item $g^\st = g_1 \in \Gval$. 
  \end{itemize}
  It is immediate from the above definitions that $\Hpred = \Hpred(\Gpred, \Gval)$ is $\Llocal$-local and relative (\cref{asm:local-relative}) with $\Llocal = 2$. Moreover, realizability (\cref{asm:unique-selection}) is satisfied by noting that $h_{g_{01}, g_1}$ has $0$ loss.

  However, note that the hypothesis $\hat h = h_{g_{00}, g_1}$ achieves $0$ expected loss on the distributions $\MP_\ell$ for $\ell \leq L$, but $\Omega(1)$ expected loss on the distribution $\MP_\ell$ for $\ell > L$. 
\end{proof}


\section{Theoretical analysis for position coupling}
\label{sec:appendix-pc}

In this section we formally state and prove \cref{prop:pc-length-extrap-informal}. We begin with some notations. Given a set $\Omega \subset \BN$ and $i \in \Omega$, we let $\rank_\Omega(i)$ denote the rank of $i$ in $\Omega$, i.e., the position of $i$ in $\Omega$ when the elements of $\Omega$ are sorted.
Next for $S \in \Sets{[\ell]}{k}$, $S = (i_1, \ldots, i_k)$, and $\psi_\ell : [\ell] \to [\ell]$, we let $\psi_\ell(S) \in \Sets{[\ell]}{k}$ to denote the set $\{\psi_\ell(i) : i \in S \}$. %

\icml{First, we discuss some additional motivation for \cref{prop:pc-length-extrap-informal}.
  \begin{remark}[Limitations of \cref{asm:local-relative}]
    The combination of \cref{asm:local-relative,asm:unique-selection}  leads to the following restriction on the $k$-sparse distribution ensemble $\MP$: in typical examples (modulo some degenerate ones where, e.g., all functions in $\Gval$ are constant), in order to satisfy realizability (\cref{asm:unique-selection}), we will need the low-loss hypothesis $h^\st \in \Hpred(\Gpred, \Gval)$ to be of the form $h^\st = h_{g_0^\st, g_1^\st}$ for some $g_0^\st \in \Gpred$ which ``selects out'' the planted set $S^\st$ and $g_1^\st \in \Gval$ which correctly evaluates the label $\bY$ given $\bX_{S^\st}$; formally, for each $\ell \in \BN$, with high probability under $(\bX, S^\st, \bY) \sim \MP_\ell$,
\begin{align}
  g_0^\st(S^\st, \bX_{S^\st}) > -\infty, \quad g^\st(S, \bX_S) = -\infty \ \  \forall S \neq S^\st,\nonumber
\end{align}
and $g_1^\st(\bX_{S^\st}) = \bY$. (We formally call this property \emph{strong realizability} in \cref{def:strong-realizability}.) But by \cref{it:local} of \cref{asm:local-relative}, this means that $\max\{ S^\st \} - \min\{S^\st\} \leq \Llocal$ with high probability over the draw from $\MP_\ell$. Ideally, we would like to establish results for planted $k$-sparse ensembles $\MP$ for which the planted set $S^\st$ is \emph{not} local in this sense; this is accomplished by \cref{prop:pc-length-extrap}. 
\end{remark}
}

\icml{We now formally define the notion of a local positional coupling, which expands out the technical details omitted in the informal version in \cref{def:amenability-informal}.
  \input{pc-formal.tex}}


\iftrue
\paragraph{Position coupling operation on functional attention classes.}
We say that a mapping $g_0 : (\BN \times \MV)^k \to \BR \cup \{-\infty\}$ (e.g., an element of $\Gpred$ is \emph{position-independent} if $g_0(S, \bX)$ depends only on $\bX$ (and not $S$). For such $g_0$, we will write $g_0 : \MV^k \to \BR \cup \{-\infty \}$. We say that $\Gpred$ is \emph{position-independent} if all $g_0 \in \Gpred$ are position-independent.


We define a new vocabulary $\bar \MV_k := (\MV \sqcup [k])^{\leq k}$, and ``position-coupled'' versions, $\PC[g_0] : (\BN \times \bar \MV_k)^k \to \BR \cup \{ -\infty\}$ and $\PC[g_1] : \bar \MV_k^k \to \MY$ of mappings $g_0 : \MV^k \to \BR \cup \{ -\infty \}, g_1 : \MV^k \to \MY$, as follows. First, given $\bX \in \bar\MV_k^k$, let
\begin{align}
\expand( \bX) \in \MV^\st\nonumber
\end{align}
be defined as follows: %
for each $i \in [k]$, we can write $\bX_{i} = ( (j_{i1}, \bZ_{i1}), \ldots, (j_{in}, \bZ_{in}))$ for some $n \geq 0$, $j_{i1}, \ldots, j_{in} \in [\ell]$ and  $ \bZ_{i1} \ldots, \bZ_{in} \in \MV$. Then we let $\expand(S, \bX)$ be the tuple consisting of all pairs $(j_{i1}, \bZ_{i1}), \ldots, (j_{in}, \bZ_{in})$ across all $i \in [k]$, where the indices $j_{i1}, \ldots, j_{in}$ are sorted in increasing order across all $i$ and all duplicates in the indices are removed. %
We next define
\begin{align}
  \PC[g_0](S, \bX) := \begin{cases}
    -\infty &: \max\{ S \} - \min\{S \} > \Llocal \\
    -\infty &: \expand(\bX) \not \in \MV^k \\
    g_0(\expand(\bX)) &: \mbox{ otherwise},
    \end{cases} \label{eq:define-pc-g0}
\end{align}
so that $\PC[g_0]$ maps $(\BN \times \bar\MV_k)^k \to \BR \cup \{-\infty \}$, and
\begin{align}
  \PC[g_1](\bX) := \begin{cases}
    g_1(\expand(\bX)) &: \expand(S) \in \MV^k \\
    y_0 &: \mbox{otherwise},
  \end{cases}\label{eq:define-pc-g1}
\end{align}
where $y_0 \in \MY$ is an arbitrary element; thus $\PC[g_1]$ maps $\bar\MV_k^k \to \MY$. 
For a sparse functional attention class $\Hpred = \Hpred(\Gpred, \Gval)$, we define
\begin{align}
\PC[\Hpred] := \left\{ h_{\PC[g_0], \PC[g_1]} \ : \ g_0 \in \Gpred, g_1 \in \Gval \right\}\nonumber.
\end{align}
Note that hypotheses in $\PC[\Hpred]$ map $\bar\MV_k^\ell \to \MY$. 
\fi 
\paragraph{Position coupling operation on distribution ensembles.} 
Next, we define a ``position coupling'' operation which modifies a distribution ensemble equipped with a local position coupling (per \cref{def:amenability}) to produce sequences in $\bar \MV_k$ with tokens coupled between positions. 

Fix a distribution ensemble $(\MP_\ell)_{\ell \in \BN}$ with $k$-sparse planted correlations, which is specified by the tuple $(\mu, \Qpos_\ell, \Qvoc)$.  %
Given a sequence $\bX \in \MV^\ell$, a set $S \in \Sets{[\ell]}{k}$, and a function $\psi_\ell :[\ell] \to [\ell]$, we let $\PC[\bX, S]$ be the distribution over sequences $\bar\bX \in \bar \MV_k^\ell$ defined as follows: for $i \in [\ell]$, the distribution of $\bar \bX_i$ is given by %
\begin{align}
  \bar \bX_i \begin{cases}
     = ((\rank_{\psi_\ell^{-1}(S)}(j),\bX_j))_{j \in \psi_\ell^{-1}(i)} &: i \in \psi_\ell(S) \\
    = (I, \bX_{\min(\psi_\ell^{-1}(i))}), \  I \sim \Unif([\ell]) &: i \not \in \psi_\ell(S), \psi_\ell^{-1}(i) \neq \emptyset \\
   \sim \Unif([\ell]) \times \mu &: \mbox{otherwise},
  \end{cases}\label{eq:pc-sequence}
\end{align}
In words, we are ``rearranging'' the tokens of $\bX$ by grouping together (i.e., ``coupling'') tokens indexed by $S$ which are mapped by $\psi_\ell$ to the same index.  %
Moreover, for positions $i$ not in $\psi_\ell(S)$, we take the token at the smallest position of $\psi_\ell^{-1}(i)$, and for positions $i$ not in the image of $\psi_\ell$, we sample $\bar \bX_i$ from $\Unif([\ell]) \times \mu$. 
We let $\PC[\MP_\ell]$ denote the distribution of $\PC[\bX, S^\st]$ where $(\bX, S^\st) \sim \MP_\ell$.


\paragraph{Strong realizability.} Finally, we introduce the following slight strengthening of realizability:
\begin{assumption}[Strong realizability]
  \label{def:strong-realizability}
Write $\Hpred = \Hpred(\Gpred, \Gval)$. We say that an ensemble $\MP$ is \emph{$\delta$-strongly approximately $\Hpred$-realizable} if there are $g_0^\st \in \Gpred, g_1^\st \in \Gval$, for each $\ell \in \BN$, with probability $1-\delta$ under $(\bX, S^\st, \bY) \sim \MP_\ell$,
\begin{align}
  g_0^\st(S^\st, \bX_{S^\st}) > -\infty, \quad g_0^\st(S, \bX_S) = -\infty \ \  \forall S \neq S^\st,\nonumber
\end{align}
and $g_1^\st(\bX_{S^\st}) = \bY$.
\end{assumption}
Using the fact that the diameter of $\bY$ is at most 1, it is straightforward that $\delta$-strong approximate realizability implies $\delta$-approximate realizability. 

\paragraph{Position coupling removes the need for locality.} The below proposition confirms that applying the position-coupling operation to a distribution ensemble $\MP_\ell$ with $k$-sparse planted correlations (\cref{def:sparse-planted}), to yield $\PC[\MP_\ell]$, and applying the position coupling operation $\PC$ to a sparse functional attention class $\Hpred$, to yield $\PC[\Hpred]$, 
can effectively remove the requirement for locality to hold (per \cref{asm:local-relative}) in order to obtain provable length \generalization:
\begin{proposition}[Length \generalization without locality from position coupling]
  \label{prop:pc-length-extrap}
  Suppose that $\MP = (\MP_\ell)_{\ell \in \BN}$ is an ensemble with $k$-sparse planted correlations (\cref{def:sparse-planted}) defined by distributions $(\mu, \Qpos_\ell, \Qvoc)$, and for some $\Llocal \in \BN$ is amenable to $\Llocal$-position coupling (\cref{def:amenability}). Moreover suppose that the distribution of $\psi_\ell(S^\st)$ for $ (S^\st, \psi_\ell) \sim \Qposc_\ell$ satisfies the coverage assumption in \cref{asm:density-ratio-position} for some values of $\eta_\ell > 0$, $\ell \in \BN$. 

  Then for any sparse functional attention class $\Hpred = \Hpred(\Gpred, \Gval)$ for which $\Gpred$ is position-independent and $\MP$ is $\delta$-approximately strongly $\Hpred$-realizable, and any integers $L, \bar L \in [\Lmax]$ for which $\Llocal \mid \bar L - L$ and $L \geq 4\Llocal$, %
  the class $\PC[\Hpred]$ achieves $(L, \bar L, \eta_L \eta_{\bar L} \cdot L\bar L^2 \cdot \delta)$-length \generalization with respect to the ensemble $\PC[\MP]$. 
\end{proposition}
\begin{proof}[Proof of \cref{prop:pc-length-extrap}]
  We will verify that the sparse functional attention class $\PC[\Hpred]$ together with the ensemble $\PC_{\mu, \psi}[\MP]$ satisfy the requirements of \cref{thm:length-extrap} with respect to the vocabulary $\bar \MV$ defined above.

  First, it is straightforward to see that $\PC_{\mu, \psi}[\MP]$ is itself a $k$-sparse planted correlations distribution ensemble: for length $\ell$, the planted set is given by $\psi_\ell(S^\st)$ for $S^\st \sim \MP_\ell$, and for all $i \not \in S^\st$, the distribution of $\bar \bX_i$ (for $\bar\bX \sim \PC[\MP_\ell]$) is an independent sample from $\Unif([\ell]) \times \mu$. (Here we have used the second and third cases in \cref{eq:pc-sequence} together with \cref{it:collisions} of \cref{def:amenability}.) We have additionally used \cref{it:rank} of \cref{def:amenability}, which ensures that the distribution of $\bar\bX_{\psi_\ell(S^\st)}$ does not depend on $\ell$ (recall from \cref{def:sparse-planted} that $\Qvoc$ cannot depend on $\ell$). 

  
  Next we verify each of the three assumptions made in \cref{thm:length-extrap}:
  \begin{itemize}
  \item \cref{asm:local-relative} is satisfied by the first line of \cref{eq:define-pc-g0} (which ensures $\Llocal$-locality) and since $\PC[g_0](S, \bX)$ does not depend on $S$ for sets $S$ satisfying $\max\{S\} - \min\{S \} \leq \Llocal$ (again by \cref{eq:define-pc-g0}). 
  \item To show that \cref{asm:unique-selection} is satisfied in the sense that
    $\PC[\MP]$ is $\delta$-approximately $\PC[\Hpred]$-realizable, we use that $\MP$ is $\delta$-strongly approximately $\Hpred$-realizable. In particular, choose $g_0^\st \in \Gpred, g_1^\st \in \Gval$ per \cref{def:strong-realizability}. %
    Consider a sample $(\bar \bX^\st, \bar S^\st) \sim \PC[\MP_\ell]$. We can write $\bar S^\st = \psi_\ell(S^\st)$ and $\bar \bX$ as in \cref{eq:pc-sequence} for a sample $(\bX, S^\st, \psi_\ell) \sim \MP_\ell$. 
    Thus, with probability $1-\delta$ under $(\bar \bX, \bar S^\st) \sim \PC[\MP_\ell]$, we have from \cref{eq:pc-sequence,eq:define-pc-g0} that
    \begin{align}
\PC[g_0^\st](\bar S^\st, \bar \bX_{\bar S^\st})  = g_0^\st(S^\st, \bX_{S^\st}) > -\infty\nonumber.
    \end{align}
    Moreover, in order to have $\PC[g_0^\st](S, \bar \bX_S)> -\infty$ for some $S \in \Sets{[\ell]}{k}$, we need (under the $1-\delta$ probability event above) that $\expand(\bar\bX_{S}) =\bX_{S^\st}$; here we have used that $g_0^\st \in \Gpred$ is position-independent. %
    For any such set $S$, it holds also that $\PC[g_1^\st](\bar\bX_{S}) = g_1^\st(\expand(\bar \bX_S)) = g_1^\st(\bX_{S^\st})$, which is equal to the label $\bar \bY$ on this event. %
    
  \item \cref{asm:density-ratio-position} is satisfied on account of the assumption from the proposition statement on the distribution of $\psi_\ell(S^\st)$ for $(S^\st, \psi_\ell) \sim \Qposc_\ell$. 
  \end{itemize}
\end{proof}

\icml{
\begin{remark}[Theoretical justification for PoSE]
  \label{rmk:pose}
  It is natural to wonder if our theoretical framework allows us to justify other techniques used to induce length \generalization, such as PoSE (\cref{sec:prelim-pose}), in a sense akin to \cref{prop:pc-length-extrap-informal}. At a high level, PoSE is adjusting the {distribution} of position IDs during training, so that IDs always seen farther apart than the training context window at test time may nevertheless be observed in the same training instance. In other words, a model trained as such should ``interpret'' a greater range of sets $S$ as satisfying the locality requirement of \cref{asm:local-relative}. Thus, we conjecture that this adjustment allows us to remove the locality requirement in \cref{asm:local-relative} as well; we leave a formal proof of this fact as an intriguing direction for future work. 
\end{remark}
}

\section{Additional experimental details: synthetic data experiments}
\label{sec:synthetic-experimental-details}
For our experiments with synthetic data (\cref{sec:sparse-parity,sec:cot-pc}), we used the GPT-NeoX decoder-only transformer model with a causal attention mask. The transformer had 12 layers, 16 heads, and an embedding dimension of 1024. We used the AdamW optimizer with learning rate $5 \cdot 10^{-5}$ and weight decay parameter equal to $0.1$; moreover, the experiments use a linear learning rate scheduler with 300 warmup steps. All shaded areas in figures represent $95\%$ confidence intervals, computed with respect to multiple training runs (the precise number of training runs for each experiment is noted in the following subsections). %

\icml{
\begin{remark}
  \label{rmk:local-relative}
  One might wonder why we emphasize \cref{it:local} but not \cref{it:relative} of \cref{asm:local-relative} in takeaway \ref{it:ta-locality}. In fact, the conceptual message of \cref{it:relative}, namely that position IDs only influence attention scores by their \emph{relative} information (i.e., the difference between different positions) is already captured by the fact that it is common to use \emph{relative positional embeddings} and variants (e.g., RoPE \cite{su_roformer_2023}, FIRE \cite{li_functional_2024}) in many open-source transformer architectures. Due to the success of such embeddings, in this sense the constraint imposed by \cref{it:relative} can ``come for free''.
\end{remark}
}


\subsection{Sparse parity (\cref{sec:sparse-parity})}

\subsubsection{Hyperparameters and data format.}
\label{sec:data-format-sparse-parity}
\paragraph{Hyperparameters.} The hyperparameters specific to our sparse parity experiments are shown in \cref{tab:sparse-parity-params}. 
\begin{table}[ht]
\centering
\begin{tabular}{l c}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Minimum training length & 20 \\
Maximum training length & 50 \\
Minimum testing length  & 20 \\
Maximum testing length  & 500 \\
Training batch size     & 64 \\
Number of training steps & 50,000 \\
Number of testing points & 512 \\
Vocabulary size $N$        & 20 \\
  Maximum position ID ($\Lmax$) & 510 \\
  Positional embedding & RoPE \\
  Number of training runs & 1 \\
\hline
\end{tabular}
\caption{Hyperparameters for sparse parity experiments (\cref{sec:sparse-parity}).}
\label{tab:sparse-parity-params}
\end{table}

\paragraph{Data format.} For some positive integer $N$, we consider an alphabet of $\MV = [N] \cup \{ \mf{t}_0, \mf{t}_1, \BOS, \SEP \}$. Given integers $k,\ell$ satisfying $k < \ell$, we consider the task of evaluating the parity of a particular set of $k$ out of $\ell$ tokens at even-numbered positions. The particular set of $k$ tokens is defined as those tokens for which the directly preceding token (which must belong to $[N]$) is $\leq N/2$. We remark that this task makes sense even for $N = 2$. 

Formally, the task is defined as follows. For each $\ell, k \in \BN$, we let $\Dsp_{\ell, k}$ denote the distribution over sequences of the form 
\begin{align}
\BOS, J_1, \mf{t}_{b_1}, \ldots, J_\ell, \mf{t}_{b_\ell}, \SEP, \mf{t}_{b^\st}\label{eq:sparse-parity}
\end{align}
where $(J_1, \ldots, J_\ell) \in [N]^\ell$ is chosen uniformly subject to the constraint that exactly $k$ of the values $J_1, \ldots, J_\ell$ are $\leq N/2$ and the remaining $\ell - k$ values are $> N/2$. Moreover, $b_1, \ldots, b_\ell \sim \Unif\{0,1\}$, and $b^\star = \bigoplus_{i : J_i \leq N/2} b_i$. We consider the task of predicting the final token $\mf{t}_{b^\st}$.

This task fits into the framework discussed in \cref{sec:model} as follows: $\MP_\ell$ is the distribution over tuples $(\bX, \bY)$ distributed as in \cref{eq:sparse-parity} where $\bX$ consists of all tokens except the final one $\mf{t}_{b^\st}$, and $\bY = \mf{t}_{b^\st}$.\footnote{Technically, the length of $\bX$ is $2\ell$ instead of $\ell$, but this discrepancy is immaterial.} It is straightforward to verify that, under the additional assumption that the $k$ values of $i$ for which $J_i \leq N/2$ are all within $\Llocal$ of each other (for some choice of $\Llocal$), then \cref{asm:density-ratio-position} is satisfied for $\eta_\ell = O(\ell)$ as long as $\ell \geq \Omega(\Llocal)$, and 
\cref{asm:local-relative,asm:unique-selection} are both satisfied for natural choices of the sparse functional attention class $\Hpred$. %


\subsubsection{Additional Results}
The analogous plots to \cref{fig:sparse-parity} for $\Ktrain \in \{4,6,\icml{8,}12\}$ are shown in \cref{fig:sparse-parity-extra}. The same patterns for $\Ktrain = \arxiv{8,}10$ are evident for each of these other values of $\Ktrain$ (except that the values of $\Ktrain \in \{4,6\}$ are sufficiently small so that there is essentially perfect length generalization for each value of $k \leq \Ktrain$). 
\label{sec:sparse-parity-extra-results}
\begin{figure*}[t]
    \centering
    \foreach \i in {4,6,\icml{8,}12} {
        \includegraphics[width=0.49\textwidth]{plots/output_dir.model_sparse_parity-0-0.5_ngram\i vtrain_FINALlgplots.pdf}
      }
      \caption{Length generalization for the sparse parity task for values $\Ktrain \in \{4,6,\icml{8,}12\}$ (see \cref{fig:sparse-parity} for $\Ktrain = \arxiv{8,}10$).}
      \label{fig:sparse-parity-extra}
    \end{figure*}
   

\subsection{Parity with scratchpad (\cref{sec:parity-scratchpad})}
\subsubsection{Hyperarameters and data format}
\label{sec:parity-scratchpad-data-format}

\paragraph{Hyperarameters.} The hyperparameters specific to our parity-with-scratchpad experiments are shown in \cref{tab:parity-scratchpad-params}.

\paragraph{Data format.}  For each length $\ell \in \BN$, we train using data of the following format, which computes the parity of $\ell$ bits by successively computing the parity of the first $i$ bits for each $i = 1, 2, \ldots, \ell$: 
\begin{align}
\BOS, b_1, \ldots, b_\ell, \SEP, b_1', \ldots, b_\ell'\label{eq:parity-cot}
\end{align}
where $b_1, \ldots, b_\ell \sim \Unif\{0,1\}$ are independent and $b_i' = \bigoplus_{j=1}^i b_j$ for $i \in [\ell]$. The model is trained to predict all tokens after the $\SEP$ (namely, the scratchpad). For position coupling, we use position IDs of $0, 1, \ldots, \ell, 0, 1, \ldots, \ell$ (which are shifted by a random offset during training as discussed above).  Note that in this task, the position IDs used in position coupling in fact \emph{are} determined solely by the length of the sequence. 
 
We train on $\ell$ for which the combined length of input and scratchpad satisfies $2\ell \in {[10, 40]}$ and test for $\ell$ satisfying $2\ell \in {[10, 200]}$.

\subsubsection{Additional results}
In \cref{fig:parity-cot-appendix}, we show the length generalization behavior for our experimental setup with modifications that (a) remove \predpc and simply use absolute position embeddings with a random shift during training time, and (b) use RoPE with PoSE. Both of these modifications significantly harm length \generalization behavior. 
\begin{table}[ht]
\centering
\begin{tabular}{l c}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Minimum training length & 10 \\
Maximum training length & 40 \\
Minimum testing length  & 10 \\
Maximum testing length  & 200 \\
Training batch size     & 64 \\
Number of training steps & 50,000 \\
Number of testing points & 192 \\
Maximum position ID ($\Lmax$) & 210 \\
  Positional embedding & Absolute Position Embeddings (learned) \\
  Number of training runs & 10\footnotemark\\
\hline
\end{tabular}
\caption{Hyperparameters for parity-with-scratchpad experiments (\cref{sec:parity-scratchpad}). }
\label{tab:parity-scratchpad-params}
\end{table}
\footnotetext{For the ablation studies in \cref{fig:parity-cot-appendix}, we only used 3 training runs. }


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}  \includegraphics[width=\textwidth]{plots/output_dir.model_parity_cot-0-_-0.5__trial_v2_position_options-shiftabspos-210FINALlgplots.pdf}
      \caption{Absolute positional embeddings with random shift}
      \label{fig:parity-cot-absshift}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
      \includegraphics[width=\textwidth]{plots/output_dir.model_parity_cot-0-_-0.5__trial_v2_position_options-fakepositions-2-210FINALlgplots.pdf}
      \caption{RoPE with PoSE}
      \label{fig:parity-cot-rope-pose}
      \end{subfigure}
      \caption{Length generalization figure for parity-with-scratchpad experiments (\cref{sec:parity-scratchpad}), with modifications that replace \predpc. Length generalization behavior is significantly worse than that in \cref{fig:parity-cot}.}
    \label{fig:parity-cot-appendix}
  \end{figure}
\subsection{Variable assignment with scratchpad (\cref{sec:va-scratchpad})}
\label{sec:var-assign}


\begin{table}[ht]
\centering
\begin{tabular}{l c}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Minimum training length & 20 \\
Maximum training length & 60 \\
Minimum testing length  & 20 \\
Maximum testing length  & 200 \\
Training batch size     & 32 \\
Number of training steps & 50,000 \\
Number of testing points & 192 \\
Vocabulary size  (number of variables)        & 200 \\
Maximum position ID ($\Lmax$) & 230 \\
  Positional embedding & Absolute positional embedding (learned) \\
  Number of training runs & 3\footnotemark \\
\hline
\end{tabular}
\caption{Hyperparameters for variable assignment-with-scratchpad experiments (\cref{sec:va-scratchpad}).}
\label{tab:va-params}
\end{table}
\footnotetext{The ablation study in \cref{fig:pointer-rope-pose} uses 10 training runs.} 

\subsubsection{Hyperparameters and Data format}
\label{sec:data-format-va-scratchpad}
\paragraph{Hyperparameters.} The hyperparameters specific to our variable assignment-with-scratchpad experiments are shown in \cref{tab:va-params}. 

\paragraph{Data format.} We now describe in detail the distribution of data for the ``variable assignment with scratchpad'' problem discussed in \cref{sec:va-scratchpad}. 
To sample a problem instance, we first draw some number $w$ of \emph{chains}, where a \emph{chain} is a sequence of $d$ variable assignments of the form: $v_1 \gets v_2; v_2 \gets v_3; \cdots ; v_d \gets v_{d+1}$. We sample uniformly at random a sequence of $w$ such chains with distinct variable IDs, where chain $j$ is denoted $v_{j,1} \gets v_{j,2}; v_{j,2} \gets v_{j,3}; \cdots ; v_{j,d} \gets v_{j,d+1}$. We interleave these chains randomly (but keep the order within each chain preserved). We let $\vinit = v_{1,1}$, and begin the input sequence with $v_{1,1}$ (so that it is possible for the model to determine which chain to follow). Finally, the sequence concludes with a scratchpad of the form $v_{1,1} \gets v_{1,2}; v_{1,2} \gets v_{1,3}; \cdots ; v_{1,d} \gets v_{1,d+1}$, evaluating the variable assignments along the correct chain. Overall, an example sequence consists of the following token IDs (here, $\gets$, $\SEPo$, $\SEPt$ are separator tokens):
\begin{align}
  & \BOS, v_{1,1}, \SEPo, v_{\pi(1), \sigma(1)}, \gets, v_{\pi(1), \sigma(1) + 1}, v_{\pi(2), \sigma(2)}, \gets, v_{\pi(2), \sigma(2) + 1}, \ldots, \SEPt, \nonumber\\
  & v_{1,1}, \gets, v_{1,2}, \ldots, v_{1,d}, \gets, v_{1,d+1}\label{eq:example-pc-cot}.
\end{align}
Here we have used $\pi(1), \pi(2), \ldots \in [w]$ to denote the chain index corresponding to each step in the input sequence, and 
$\sigma(1), \sigma(2), \ldots \in [d]$ to denote the position within its chain corresponding to each step in the input sequence.
As an example with $v = 2$ and $w = 2$, we might have the sequence:
\begin{align}
  & \BOS, v_{1,1}, \SEPo, v_{2,1}, \gets, v_{2,2}, v_{1,1} \gets v_{1,2}, v_{1,2}, \gets v_{1,3}, v_{2,2} \gets v_{2,3}, \SEPt, v_{1,1}, \gets, v_{1,2}, v_{1,2}, \gets, v_{1,3}\nonumber.
\end{align}

The model is trained to predict all tokens after the $\SEPt$. For a fixed depth $d$ and desired length $\ell$ of the input, we choose $w$ as large as possible so that the length of  the sequence between $\SEPo$ and $\SEPt$ is at most $\ell$. 

For position coupling, the first part of the example (before $\SEPt$) uses the true position IDs $1, 2,3, \ldots, \ell$, where $\ell$ denotes the number of tokens before $\SEPt$. The final part of the example (after $\SEPt$) uses position IDs $I_1, I_1 + 1, I_1 + 2, I_2, I_2 + 1, I_2 + 2, \ldots$, where for $j \in [d]$, $I_j$ denotes the position ID assigned to $v_{1,j}$ in the first part of the example. Summarizing, the position IDs corresponding to the example \cref{eq:example-pc-cot} are:
\begin{align}
0, 1, 2, \ldots, \ell, 0, I_1, I_1 + 1, I_1 + 2, \ldots, I_d, I_d + 1, I_d + 2\nonumber. 
\end{align}
Note that, because $I_1, I_2, \ldots, I_d$ depend on the (random) functions $\pi, \sigma$, it is crucial that the model learns to predict these coupled position IDs, so that they can be passed as the next position ID. %
\icml{
\subsubsection{Additional Results}
\label{sec:var-assign-results}
\cref{fig:pointer-cot-appendix} shows the length generalization behavior for our experimental setup with modifications that (a) remove \predpc and use absolute positional embeddings with a random shift during training time, (b) use RoPE with PoSE (keeping the scratchpad), and (c) use RoPE with PoSE and remove the scratchpad. Notice that \predpc (\cref{fig:pointer-cot}) greatly outperforms all 3 of these modifications.

\input{baseline-va-plots.tex}
}



  \section{Additional experimental details: Natural Language Experiments}
  \label{sec:additional-nlp}

  \paragraph{Additional experimental details.} Hyperparameters used for our training procedure for the model $\hshort$, discussed in \cref{sec:experiments-natural-language}. The model $\hshort$ with context length $L = 64$ discussed in \cref{sec:experiments-natural-language} was trained for 34000 steps without PoSE, and 15000 steps with PoSE. We additionally trained a model $\hlong$ with context length $\bar L = 128$ for 48000 steps, whose hyperparameters are identical to those of $\hshort$ except the batch and microbatch sizes are halved. Altogether, both models $\hshort,\hlong$ were trained on roughly 25B tokens, and each has 1.3B parameters. 

\begin{table}[ht]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Hyperparameter} & $\hshort$ & $\hlong$  \\
\midrule
Model Dimension         & 2048 & 2048 \\
MLP Hidden Size         & 8192 & 8192 \\
Number of Heads         & 32& 32 \\
Number of Layers        & 24 &24\\
  Position Embedding      & RoPE (w/ PoSE) & RoPE \\
  Activation Type & GeLU & GeLU\\
  Vocabulary Size & 32000 & 32000 \\
  Tokenizer & Llama-2-7b & Llama-2-7b \\
  Batch Size & 8192 & 4096\\
  Microbatch (per-device) size & 256 & 128 \\
  Training Context Length & 64  & 128 \\ 
  Number of training steps & 49000 & 48000 \\
\arxiv{  Learning Rate Scheduler        & Cosine w/ Warmup of 5000 & Cosine w/ Warmup of 5000  \\}
  Optimizer & AdamW (w/o weight decay) & AdamW (w/o weight decay) \\
  Learning Rate & $10^{-3}$ & $10^{-3}$ \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for our natural language modeling experiments (based off of OLMo codebase with C4 dataset).}
\label{tab:olmo-params}
\end{table}

  

  \icml{\input{natural-language.tex}}


  

\end{document}