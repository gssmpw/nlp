%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{xcolor}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{balance}
\newcommand{\ourframework}{$\mathsf{NeuronLens}$}

\newcommand{\fix}[1]{\textcolor{red}{URGENT, Please FIX: {#1}}}
\newcommand{\fixed}[1]{\textcolor{blue}{Fixed, {#1}}}

% \newcommand{\hammad}[1]{\textcolor{hammad}{#1}}
% \newcommand{\umair}[1]{\textcolor{umair}{#1}}
\usepackage{multirow}
\usepackage{float}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% \documentclass[preprint]{icml2025}

% \usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\usepackage{dsfont}

\begin{document}

\twocolumn[
\icmltitle{Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Muhammad Umair Haider}{uky}
\icmlauthor{Hammad Rizwan}{dal}
\icmlauthor{Hassan Sajjad}{dal}
\icmlauthor{Peizhong Ju}{uky}
\icmlauthor{A.B. Siddique}{uky}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{uky}{Department of Computer Science, University of Kentucky, Kentucky, USA}
\icmlaffiliation{dal}{Department of Computer Science, Dalhousie University, Halifax, Canada}

\icmlcorrespondingauthor{Muhammad Umair Haider}{muhammadumairhaider@uky.edu}
\icmlcorrespondingauthor{Hammad Rizwan}{hammad.rizwan@dal.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Polysemanticity,Interpretability}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}

% Binary neuron-concept attribution methods assign semantic meaning to neural units through discrete mappings.


% OLD
%Recent advances in neural network interpretability have focused on identifying and manipulating individual neurons to understand and control model behavior. These approaches attribute neurons to semantic concepts in a discrete mapping. This discrete mapping exacerbates the challenge of polysemanticity, leading to suboptimal downstream interpretation and intervention.  Our analysis across both encoder and decoder transformer architectures on multiple text classification tasks shows that neurons exhibit distinct, Gaussian-like activation patterns for different concepts which can be exploited for precise interventions. We introduce a novel range-based interpretation and manipulation framework that localizes concept attribution within neuronal activation patterns, achieving more precise concept control while minimizing unintended interference. Empirical analysis shows the framework successfully reduces unintended interference while maintaining comparable effectiveness at targeted concept manipulation. 
% TLDR
% Neurons in LLMs exhibit Gaussian-like activation patterns for different concepts, we present a novel range-based attribution that allows precise concept control while minimizing interference with other concepts.




% Name
% \textbf{RANGE} (Range Attribution through Neuronal Gaussian Encodings)
%New
% Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. 
% Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. 
% However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. 
% This makes precise control challenging and complicates downstream interventions.
% %However, such mappings amplify the challenge of polysemanticity making precise control difficult and complicating downstream interventions. 
% Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. 
% %Our analysis of encoder and decoder transformer architectures on multiple text classification datasets reveals that while individual neurons capture multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. 
% Building on this insight, we introduce {\ourframework}, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. 
% %{\ourframework} enables targeted concept intervention without interfering with unrelated concepts, overcoming key limitations of discrete mappings. 
% Extensive empirical evaluations demonstrate that {\ourframework} significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.
%\hammad{Umair: add numbers. Upto 25\% improvement in the compliment}

% We provide theoretical analysis showing why activation ranges are fundamental to neural network behavior and demonstrate practical applications in controlled model editing and concept removal. These findings challenge the prevailing paradigm of treating neurons as atomic units and suggest a more nuanced approach to neural network interpretability and manipulation.
%last
Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce {\ourframework}, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that {\ourframework} significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.
\end{abstract}


\section{Introduction}




Large language models (LLMs) have shown unprecedented performance in various tasks involving natural language understanding, generation, and transformation~\cite{brown2020language,bommasani2022opportunities,touvron2023llama,raffel2019exploring}.
However, the inner workings of LLMs remain largely opaque~\citep{BurkartH21}, as their representations are distributed across billions of parameters. 
This lack of interpretability raises critical concerns about reliability, fairness, and trustworthiness, particularly in high-stakes domains such as healthcare, law, and education. To this end, neuron-level interpretability can address these concerns by enabling researchers to uncover how individual neurons encode semantic concepts and contribute to model outputs.
With this understanding, researchers can diagnose safety risks~\citep{WeiHHXQXMW024,he2024jailbreaklens}, refine model outputs~\citep{MengSABB23,rizwan2024resolving}, optimize efficiency through pruning~\citep{frankle2019lotterytickethypothesisfinding,9506252}, and steer models toward desired objectives~\citep{SubramaniSP22}.


Recent research efforts have made significant progress in neuron-level interpretation by identifying salient neurons that influence model behavior~\citep{DalviDSBBG19, AntvergB22, ConmyMLHG23, marks2024sparse}.
Approaches such as maximal activation analysis identify neurons that exhibit the highest activation values for specific concepts~\citep{foote2023n2g, frankle2019lotterytickethypothesisfinding}. Probe-based methods employ auxiliary classifiers to distinguish between concepts using hidden representations~\citep{DalviDSBBG19, grain:aaai19-1}, while probeless approaches bypass the need for classifiers by directly analyzing neurons, improving simplicity and interpretability~\citep{AntvergB22}.
Other techniques include circuit discovery, which attributes concepts to groups of interacting neurons~\citep{olah2020zoom, ConmyMLHG23}, and causal analysis, which intervenes on internal components to identify their role in model behavior~\citep{vig2020investigating}. 
\begin{figure}[t!]
    \centering
\includegraphics[width=0.48\textwidth]{figure1_ranges.png}
        \caption{{\ourframework} leverages distinct, Gaussian-like activation patterns of neurons to enable fine-grained concept attribution and selective concept manipulation with minimal interference with other concepts. %\fix{need more overlap in gaussians}
        }
        \label{fig:overview}
        \vspace{-10pt}
\end{figure}


While these methods have advanced our understanding, they often rely on discrete neuron-to-concept mappings, which assume that entire neurons encode single concepts. However, neurons frequently exhibit polysemanticity -- the ability to encode multiple, seemingly unrelated concepts~\citep{lecomte2024causes, marshall2024understandingpolysemanticityneuralnetworks}. 
Given this heterogeneous encoding of concepts, traditional approaches often lead to unintended consequences when manipulating neurons, as changes intended for one concept may inadvertently affect others encoded by the same neuron or suboptimal interpretations of concepts~\citep{sajjad2022neuronlevelinterpretationdeepnlp}.


Despite being traditionally viewed as a challenge, could polysemanticity instead provide a unique lens for advancing interpretability and model control? If individual neurons encode multiple concepts, might their activation spectrum reveal distinct and identifiable patterns for each concept? Could these patterns enable precise interventions that adjust one concept while minimizing interference with others, overcoming the limitations of coarse, monolithic neuron-to-concept mappings? 



This work seeks to address these questions by analyzing the activation patterns of neurons in both encoder-based and decoder-based LLMs. Through statistical and qualitative analysis, we find that neuronal activations for concepts follow \emph{Gaussian-like distributions}, with distinct patterns for different concepts. 
%Notably, salient neurons -- those critical for predicting specific concepts -- exhibit minimal overlap between activation patterns across concepts, enabling the possibility of separability across concepts.
Our key insight is that the true fundamental unit of interpretability lies at a level more fine-grained than the neuron itself. Within a neuron's activation spectrum, \emph{activation ranges corresponding to specific concepts emerge as the fundamental unit}. This novel perspective enables a more precise approach to neuron interpretation and manipulation, addressing the limitations of traditional, discrete neuron-to-concept mappings.

Building upon these insights, we introduce {\ourframework}, a novel range-based framework for neuron interpretation and manipulation. {\ourframework} maps specific activation ranges within a neuron's distribution to individual concepts, rather than attributing entire neurons to single concepts. For each concept, {\ourframework}calculates a range that covers approximately 98.76\% of the activation spectrum, capturing the concept-specific activations while excluding unrelated concepts. 
Through experiments on encoder-based and decoder-based LLMs across several text classification datasets, we show that {\ourframework}.  significantly reduces unintended interference by up to 25 percentage points, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods. 


% \fix{Umair: a line of text with numbers.}



%This lack of interpretability poses critical challenges: practitioners cannot reliably diagnose failures, identify biases, or ensure predictions are grounded in meaningful reasoning. 
%In high-stakes domains such as healthcare, law, and education, these limitations raise serious concerns about reliability, fairness, and trustworthiness.
%To this end, neuron-level interpretability can address these challenges. 
%By understanding how individual neurons or groups of neurons represent semantic concepts and contribute to model outputs, researchers can analyze and intervene in model behavior in meaningful ways, such as diagnosing safety risks~\citep{WeiHHXQXMW024,he2024jailbreaklens}, editing or refining specific model responses~\citep{MengSABB23,rizwan2024resolving}, pruning redundant components to improve efficiency~\citep{frankle2019lotterytickethypothesisfinding,9506252}, and steering models to align with desired objectives~\citep{SubramaniSP22}. 
%\hammad{Abu: compress: point: importance.}

% Recently, significant research efforts have sought to demystify the internal mechanisms of LLMs by focusing on neuron-level interpretation at the layer level~\cite{DalviDSBBG19, AntvergB22} and across layers~\cite{ConmyMLHG23,marks2024sparse}. These studies aim to identify salient neurons that encode specific concepts or influence model behavior. Several methodological approaches have emerged, including maximal activation analysis, probe-based methods, circuit discovery, and causal analysis.
% Maximal activation analysis attributes specific concepts to neurons that exhibit the highest activation values when exposed to concept-specific inputs~\cite{foote2023n2g, frankle2019lotterytickethypothesisfinding}. 
% This approach is grounded in the rationale that maximally activating neurons are critical for controlling the model's outputs, thus highlighting their relevance to a given concept.
% Probe-based methods train auxiliary classifiers, such as linear probes, to determine how well the hidden representations distinguish between concepts~\cite{DalviDSBBG19, grain:aaai19-1}. The weights of these classifiers can then be analyzed to identify salient neurons. However, these methods require additional training, which may introduce biases from the probe itself.
% To address these limitations, probeless approaches have been proposed, such as analyzing individual neurons directly without auxiliary classifiers~\cite{AntvergB22}, offering a simpler and more interpretable alternative.
% Circuit discovery shifts the focus from individual neurons to subgraphs within the model, attributing groups of neurons and their interactions to specific concepts~\cite{olah2020zoom, ConmyMLHG23, marks2024sparse}.
% Similarly, the causal analysis identifies salient neurons or substructures by directly intervening on the network, such as corrupting internal components and measuring their impact on the model's behavior~\cite{vig2020investigating}.
%\hammad{Abu: Compress; point: no point, related}

% A significant challenge in neuron-level interpretation lies in the phenomenon of polysemanticity in deep learning models, where individual neurons encode multiple, seemingly unrelated concepts~\cite{lecomte2024causes, marshall2024understandingpolysemanticityneuralnetworks}. Given this heterogeneous encoding of concepts by neurons, and the reliance of traditional approaches on discrete neuronal attribution which maps entire neurons to single concepts often leads to unintended consequences on other concepts encoded by the same neuron or suboptimal interpretation of concepts~\citep{sajjad2022neuronlevelinterpretationdeepnlp}.
% This oversimplification is rooted in the hypothesis that polysemanticity arises due to factors such as the high-dimensional feature space exceeding the number of neurons, biases in tokenization schemes, and the initialization of model weights~\citep{sparseautoencoderpaper}.
% The consequences of these limitations are particularly pronounced during model interventions. Discrete neuron-to-concept mappings necessitate coarse-grained adjustments across an entire neuron, often leading to unintended consequences on other concepts encoded by the same neuron. 
%\hammad{Umair: citation}



%Moreover, if salient neurons display additional activation patterns specific to critical features, how can we leverage these patterns to identify and manipulate these neurons effectively?


% This work addresses these questions by analyzing the activation patterns of neurons in both encoder-based and decoder-based LLMs, including BERT (fine-tuned)~\cite{devlin2019bertpretrainingdeepbidirectional}, GPT-2 (fine-tuned)~\cite{radford2019language}, and Llama-3.2-3B (pre-trained)~\cite{touvron2023llama}, across multiple text classification datasets such as DBPedia-14, IMDB, SST2, AG-NEWS, and Emotions.

% This work addresses these questions by analyzing the activation patterns of neurons in both encoder-based and decoder-based LLMs. We find through statistical and qualitative analysis that neuronal activations for concepts form a Gaussian-like distribution, with distinct patterns for different concepts. Notably, salient neurons -- those critical for predicting specific concepts -- exhibit limited overlap between activation patterns across concepts.
% This separability diminishes in non-salient neurons, where overlapping activations make disentanglement difficult.
% Our extensive quantitative analysis further supports these findings. For instance, statistical tests reveal that approximately 99\% of neurons exhibit distributions consistent with Gaussian-like properties (e.g., near-zero skewness, kurtosis close to 3.0, and Kolmogorov-Smirnov test results within acceptable thresholds). These results indicate that Gaussian-like patterns are not only prevalent but also separable for salient neurons, providing a foundation for more precise attribution and manipulation within a neuron. 
% \hammad{Umair: Add crux line here. Point: Gaussian-like, range-based mapping. Crux line:
% Fundamental unit is more fine grained than neuron. Activation ranges within the neuron activation spectrum is the fundamental unit in our work, this is novel}



%Building upon these insights, we introduce NeuronLens, a novel range-based framework for neuron interpretation and manipulation. NeuronLens maps specific activation ranges within a neuron's distribution to individual concepts, rather than attributing entire neurons to single concepts. This method calculates a range that covers 98.76\% of the activation spectrum for the concept of interest $c$. This fine-grained approach enables precise intervention and interpretation to ameliorate the constraints of monolithic mappings.

% We compute the mean and standard deviation of activation's and mask majority of gaussian.
% operates by e
% calculating the mean and standard deviation of neuronal activations and defining concept-specific activation ranges using a 2.5-standard-deviation threshold. This range captures approximately 97\% of concept-relevant activations, allowing for precise attribution. By focusing on activation ranges rather than entire neurons, NeuronLens , providing greater control and flexibility in concept-specific interventions.

In summary, this work makes the following contributions:

\begin{itemize}
    \item We uncover that neuronal activations in LLMs form Gaussian-like distributions, with salient neurons exhibiting limited overlap in their activation patterns across concepts.
    \item We show that activation ranges within a neuron’s activation spectrum are the true fundamental unit of interpretability, offering a precise framework for neuron-level analysis.
    \item We propose {\ourframework}, a range-based framework for interpreting and manipulating neuronal activations, which enables fine-grained concept attribution and reduces unintended interference.
%    \item Through extensive empirical evaluation on BERT, GPT-2, and Llama-3.2-3B across five text classification datasets, we demonstrate that NeuronLens significantly outperforms existing methods in maintaining precise control over targeted concept intervention while minimizing side effects.
\end{itemize}

%By understanding how individual neurons or groups of neurons represent semantic concepts and contribute to model outputs, we can identify which components of the network are responsible for specific behaviors. This insight enables targeted interventions, such as refining undesirable outputs or amplifying desirable behaviors, without the need for sweeping alterations to the entire model. Beyond enabling trust and accountability, neuron-level analysis provides a deeper understanding of how LLMs generalize, how they encode polysemantic concepts, and how we can leverage these patterns for precise model manipulation.

%OLD
%Deep learning based models are generally considered as black boxes \citep{BurkartH21}, which presents significant challenges for their deployment in real-world applications where transparency and accountability is paramount. This issue is particularly pronounced in Large Language Models (LLMs), which contain millions or billions of parameters and exhibit patterns that are difficult to predict or explain. 
%Recently research efforts have focused on understanding the underlying operations within these systems based on neuronal interpretation at the layer level \citep{DalviDSBBG19, AntvergB22} and across layers \citep{ConmyMLHG23,marks2024sparse}. %\hammad{add more citations 1 min and improve text if needed}. Understanding the internal operations of these systems can help uncover reasoning patterns and shared characteristics across various models \hammad{revisit}. 
%The implication of this line of work would allow for targeted interventions and analysis, such as allowing researchers to gauge the safety of LLMs \citep{WeiHHXQXMW024,he2024jailbreaklens}, model editing \citep{MengSABB23,rizwan2024resolving}, model pruning \citep{frankle2019lotterytickethypothesisfinding,9506252} these systems and steering \citep{SubramaniSP22}. %\hammad{add more if needed}


%\hammad{(Biases)}.

% To understand and potentially edit or steer the model, we need to understand how its internal components collectively implement its underlying computational approach.
% Elucidating the mechanistic relationships between a model's internal components and their computational dynamics is paramount for any downstream intervention(find word that also says analysis).

% Current approaches in relevant literature are 1)capturing the maximally activating neurons \citep{foote2023n2g} where maximally activating neurons on concept of interest are attributed to that concept. 2) Training a linear probe\citep{DalviDSBBG19, AntvergB22} on the internal representation to get the saliency of neurons is another set of methods 4) Corruption of internal components and observing the causal effect is another line of work\citep{vig2020investigating}.


%Prior work has explored several key methodological approaches for finding salient neurons for concepts of interest. These include \textit{maximal activation neurons, linear probes, circuit discovery and causal analysis}. Maximally activated neurons attribute specific concepts to neurons which exhibit the highest activation patterns (negative and positive) when exposed to samples that elucidate the concept of interest \citep{foote2023n2g}. Linear probes learn linear separable subspaces based on model representations for different concepts of interest, the learned model weights are then utilized for neuronal saliency \citep{DalviDSBBG19, AntvergB22}. Circuits \citep{olah2020zoom, ConmyMLHG23, marks2024sparse} attribute subgraphs of the neural network (group of neurons), within the model, to concept of interests. Causal analysis utilizes targeted corruption of internal network components and observing the resultant effects on the model's behavior \citep{vig2020investigating}.

% % \hammad{max, linear, probless, corruption} approaches. 
% Extraction of maximally activating neuronal units predominant analytical methodology involves the identification \hammad{tied to relevant approaches sentence.}   
% Another approach is attribution methods such as gradient based methods that aim to where neurons are attributed to various concepts within the models based on the magnitude of the gradients when trying to change information regarding those concepts. \hammad{todo: (recheck, add more knowledge)}. \textbf{Circuits}




% This phenomenon is observed in the analysis performed  which identify that positive and maximally activation neurons in the transformers pointwise feed forward layer acts as a key-value memories. This analysis is further extended by who show that not only high positive but high negative activations are also important as they explicitly remove the previously processed semantic from the representation. Thus highly activating neurons control which information is \textit{elucidated}.



% Taking max neurons has a rationale that these neurons are the ones that are \hammad{talk about causality, causal tracing} participating most in the model output for the given concept C. Then there are attribution methods that associate neurons to concepts with concept C. Gradient based method is one such example, where neurons are attributed to a concept C based on the magnitude of gradient for concept C(cite). \hammad{detail gradient based simpler and to the point. cite}.

% While the aforementioned methodologies for identifying and manipulating salient neurons enable downstream interventions, they exhibit significant limitations in their theoretical framework. 

%The fundamental constraint lies in the monolithic attribution paradigm which potentially oversimplifies the complex, distributed nature of neuronal activation as can be seen in polysemanticity \citep{lecomte2024causes, marshall2024understandingpolysemanticityneuralnetworks}  where single neuron learns multiple seemingly unrelated concepts and elucidates them at different activation values \hammad{(cite related work of polysemantic (sparse autoencoder paper)}), and traditional approaches attribute individual neuron to a single concept. This phenomenon is the consequence of superposition which is hypothesized to be caused by many factors: 1) The number of features is more than the number of neurons 2) Initialization of weights for a neuron is closer or farther for more than one concept 3) Tokenization function used could have its biases. Mapping entire neuronal units to discrete conceptual representations is a reductive approach which ignores the comprehensive distributional characteristics across the \textbf{full activation spectrum of neurons}. This coarse localization necessitates modifications across the complete activation spectrum of individual neurons leading to unintended consequences for other concepts in model interventions. 
%\hammad{approaches that attribute whole neurons to concepts also might do it in parallel while manipulating various concepts as there is overlap in the concepts....we can talk about the need for them to trained together to learn the proper new activation space}. 

% constrains \textit{the} analysis to \textit{peripheral} activation states thereby failing to capture the c
% These methods of discovering salient neurons and adapting them for downstream interventions are sub-optimal as they have a major limitation. They attribute entire neurons to specific concepts and any intervention requires  entire neuron activation range

%Polysemantic behavior in neural networks has traditionally been regarded as a challenge for interpretability and model intervention. However, this behavior implies: that either neurons encode consistent information across multiple concepts, or they provides distinct information for different concepts. In the latter case, if a neuron encodes multiple concepts, its activation spectrum may exhibit distinct and identifiable patterns corresponding to each concept. Thus allowing for precise interventions. Moreover, this can mean that salient neurons may exhibit additional patterns that can used to find them. \hammad{redo}

% We observed that polysemantic neurons exhibit distinct activation patterns when responding to different concepts, suggesting that the very multiplicity of their function could be leveraged for more accurate concept attribution.
% The fundamental insight stems from the observation that when a neuron demonstrates polysemantic properties—responding to multiple, seemingly unrelated concepts—it does not activate uniformly across these different stimuli. Instead, we found that each concept elicits a characteristic activation pattern, creating a potentially distinguishable "signature" for each semantic concept within the same neuron.

% (either previous paragraph or the paragraph above it)

% The prevalence polysementicity is the major hurdle in interpretability and model interventions.  of additional patterns with respect to magnitude of maximally neuronal activation  in neurons means that there could be additional patterns wit



% In this work we analyze the neuronal activation pattern with respect to particular concepts, our findings reveal that neuron activations exhibit additional patterns of sensitivity and consistency. 

% These neuron classification methods from the literature are too rigid, as they attribute whole neurons to a concept C. In practice, the knowledge in LLMs is scattered. Many studies(cite polysemantic) suggest that the same neurons are responsible for encoding multiple concepts. To tackle this challenge x approaches propose circuits in the network instead of individual neurons responsible for a concept C....

% Training a linear probe on the model's output is another method to find neuron salience for concept C. \hammad{intro mostly related thing.}
% Existing approaches in the literature, such as maximum activation neuron identification, primarily rely on activation magnitude (whether positive, negative, or absolute) to establish comprehensive neuron-concept mappings. However, these conventional methodologies exhibit two fundamental limitations:

%To examine this hypothesis of additional patterns in the activation, we explore neuronal \textbf{sensitivity}(sensitivity here mens that the neuron activation spread is wide and the neuron activation varies more for all s in c) and \textbf{consistency}(consistency here means that the activation spread of the neuron is tight and neuron shows similar activation patterns for all the in) with respect to given concepts $put sequences of s in cc \in C$, which extend beyond traditional activation magnitude assessment. Moreover, we also explore if these can be exploited for downstream interventions in regards to x. 

%We extract neurons identified as salient using the previously described algorithms and evaluate their performance in relation to model manipulation aimed at removing concept-related information. Our results reveal that the maximal activation neuron approach for identifying salient neurons significantly outperforms other methods, making it the most effective for information attribution. Following extraction, we examine activation patterns. we also explore trends in the saliency of neurons across different concepts, moreover, we explore how these can be utilized to improve information attribution and intervention. Our findings reveal that


%\begin{itemize}
%    \item Salient neurons exhibit a strong correlation with sensitivity, i.e., neurons deemed salient for a concept \( c \) are also highly sensitive to \( c \).
%    \item Neurons activate within bounded ranges for different concepts, indicating distinct activation patterns.
%    \item Distinct activation ranges enhance the disentanglement of polysemy in neurons associated with different concepts. Thus allowing for precise information removal.
%\end{itemize}




% we find that .

% Second, after finding that neurons activate within a bounded range for a concept c, we present a novel approach of manipulating with the bounded range within the full activation spectrum of the neuron. We find that doing so is performing same level of manipulation as manipulating the whole neuron but the unintended affect on other concepts is improved across all the settings. this suggests that distinct activation ranges can improve the disentanglement of polysementicity neurons linked to different concepts for salient neurons.


% The metric is based on the statistical distribution of neuronal activations across all instances of a concept. Specifically, for a given neuron $h^l_z$ and a specific concept $c \in C$, we compute the standard deviation $\sigma$ of activation values across all samples $s \in S$  for concept $c$. 


% This approach facilitates a nuanced understanding of neuronal activation patterns which yields a novel finding that different activation ranges are associated with different concepts for each neuron. Furthermore, this provides a natural approach to selectively suppress or manipulate information related to specific concepts within these neurons. This alleviates the loss of information associated with neuron-level interventions, enabling more precise control over information within the models.


% Subsequently, we implement conditional masking of these identified activation ranges to selectively suppress the representation of concept C. Rather than completely suppressing entire neurons associated with C, our approach targets specific activation ranges within individual neurons. This fine-grained manipulation enables more precise control over concept removal compared to traditional neuron-level interventions




% This analysis yields activation ranges that are associated of concept C for each neuron.
% Subsequently, we implement conditional masking of these identified activation ranges to selectively suppress the representation of concept C. Rather than completely suppressing entire neurons associated with C, our approach targets specific activation ranges within individual neurons. This fine-grained manipulation enables more precise control over concept removal compared to traditional neuron-level interventions.



% . This solves 1) constraints of magnitude-based analysis, which traditionally restricts examination to extremal activation states, by incorporating the complete distributional characteristics across the activation spectrum 2) A rigid neuron-concept mapping paradigm that assigns entire neuronal units to singular concepts C, enabling a more nuanced characterization of neuronal representations

% They are using magnitude of activation which restricts them to only analyze the extreme activations, they do not look into the whole activation spectrum. 2) They have the same limitation noted above of being rigid and classifying whole neuron to a concept C.  
% Our approach considers the metric of consistent neural activation in addition to magnitude.

% Our approach can be classified in the activation analysis methods. The activation analysis methods in literature like Max neurons only use the magnitude of activation (either in the positive or negative direction or absolute value) to associate complete neurons with any concept C. These activation analysis methods have two limitations 1) They are using magnitude of activation which restricts them to only analyze the extreme activations, they do not look into the whole activation spectrum. 2) They have the same limitation noted above of being rigid and classifying whole neuron to a concept C.  

% To tackle the first limitation our analysis question was:


% % \textbf{Q1: Is there more information in the activation pattern than just magnitude?}
% % We hypothesized that for input sequences \hammad{s contained in S} for a given concept C the neuron's activations are sensitive for input sequences in $C$(i.e: the activation pattern is sensitive in the neurons that are capturing concept C for all semantically equivalent variations of s) 

% % To test this hypothesis we introduced a new ranking metric for neurons associated with concept C. This new metric uses the spread of neuron activations as a metric to define if for concept C the neuron behaves sensitively, with the rationale that if a neuron's behavior is sensitive for all the input sequences on $c$, it is encoding information about $c$
% % % across all the input S in C then that neuron behavior is consistent for the concept C regardless of variation in S \hammad{we formalize this notion/metric in section \S}.  \hammad{Our study reveals that max are consistent that knowledge is shared between some neurons C and that these concepts can be disentangled in the neuron activation space.}

% % We compared the neurons selected by our metric with the Max metric and found a high overlap between the two. confirming that Max neurons which are salient neurons for concept C in the literature also behave sensitively on the given concept. \hammad{at the end of the introduction, in contributions section}


% % To tackle the second problem of literature approaches being rigid in classifying whole neuron to a concept C our second research question was:
% % \textbf{Q2: Can we look deeper than the neuron level?}

% % % After discovering that neurons have consistent behavior for a concept C, and STD is a good matric to understand neuron association we wanted to use this discovery. In literature the main theme is to associate neurons with any concept C using different heuristics, this approach is too rigid as according to these papers: a neuron could be associated with multiple concepts: this type of neurons are called polysemantic neurons. \hammad{motivation for why we should look deeper}

% % This problem arises from litrature approaches classifying whole neuron to a concept C because in literature the basic block of analysis is a single neuron for any concept C, this approach is rigid and polysementicity arises from it. 
% % We use activation ranges in the activation spectrum of the neuron as the basic block of analysis. We introduce a new approach using our previous finding of sensitive behavior using STD to find ranges within the neuron's activation spectrum related to concept C. This approach gives us a more fine grained unit to associate with the concept C. Experimentation shows that using this fine grained unit we can control the information for specific concepts with minimal effect on the model's general performance, in comparison to associating whole neurons to a concept.





% \hammad{abalation on polysemantic, concept analysis issues}

% \hammad{work}

% \hammad{how you compare max to ranges. and how is that relevant and why}

% \hammad{classes neuron overlap which are consistent. visualization}

% \hammad{image polysemantic ranges concept}
% \hammad{experiments to highlight this classwise}

% \hammad{Llama max vs ranges and then show generation performance.}


% \hammad{reproduction of information as the linear activations are processed within the model from shallow to deeper layers. (subsection ablation)}
% \hammad{neurons limited more information required to be stored thus polysemantic.}





\section{Neuron Interpretation Analysis}
\label{prelim}
%This section provides an overview of the neuron analysis, methods for extracting salient neurons, and the process of validating their saliency.


\subsection{Preliminaries}
\label{sec:activation_analysis}

\textbf{Neuron.}
%In this paper, we call 
We refer to the output of an activation as a neuron.
In a transformer model, we consider neurons of hidden state vectors of different transformer layers.
% a neuron $j \in N^l$ represents a single scalar value in the hidden state vector output where $N^l = \{1, \ldots, d\}$ be the set of indices of neurons in transformer layer $l$ with output dimension $d$. 
Formally, given a hidden state vector $\mathbf{h}^l \in \mathbb{R}^d$ of size $d$ produced by layer $l$, $h^l_j$ denotes its $j$-th neuron, i.e., the $j$-th component of $\mathbf{h}^l$. 

% When processing an input sequence $\mathbf{x} = (x_1,\ldots,x_T)$, each layer produces a sequence of hidden states $\mathbf{H}^l = (\mathbf{h}^l_1,\ldots,\mathbf{h}^l_T)$, where neuron $j$ captures a specific feature across all positions through values $(h^l_{1,j},\ldots,h^l_{T,j})$.

\textbf{Concept.}
A concept $c \in C$ is a high-level semantic category that groups each input instance (or components of every instance), where $C$ is the set of all concepts. For example, in a language task, a sentence can be categorized into 4 types: declarative, interrogative, imperative, and exclamatory, where each type is a concept. Words of a sentence can also have concepts like noun, verb, adjective, adverb, etc. In this study, we focus on the situation that all input samples are labeled with concepts.


% $X = {x_1, ..., x_n}$ sharing meaningful properties. $c$ defines an equivalence class over input instances such that $\forall x_i, x_j \in X$, $x_i \sim_c x_j$ if they belong to the same semantic category. 

% In this study, concepts refer to the classes that input sequences are labeled with during model training and evaluation.

\textbf{Saliency Ranking.}
\label{def:ranking}
% For a neural network and concept $c$, let $\mathcal{N}^l = \{1, \ldots, d\}$ be the set of indices of neurons in layer $l$. 
A saliency ranking orders the importance of neurons based on some saliency metric. 
For a hidden state vector $\mathbf{h}^l \in \mathbb{R}^d$, 
%For a given $d$-dim hidden state vector, we use 
$s_{j,c}$ 
%to 
denotes the value of the saliency metric for the $j$-th neuron with respect to a concept $c$. The saliency ranking $(r_c(1), r_c(2), \cdots, r_c(d))$ is a permutation of the indices of neurons $(1,2,\cdots,d)$, where $r_c(j) < r_c(i)$ if $s_{j, c} > s_{i, c}$. The saliency metric is usually predetermined, e.g., the absolute value of each neuron.

\textbf{Concept Learning.}
Given a hidden state vector $\mathbf{h}^l$ as 
%the 
 input, the associated concept can be the output of an appended neural network (e.g., several fully connected layers). The parameters of this appended neural network can be trained using training samples labeled with concepts. 
% {\color{blue} The whole model can be viewed as a concept-learning model that maps a sample (or part of a sample) to a concept.} \fix{above is unclear. what does the concept learning refer to?. if this is just about formalizing a neuron interpretatio method we can simply say that given a neuron interpretation method that assigns a neuron to a concept.}



% A saliency ranking is a bijective function $r_c: \mathcal{N}^l \mapsto \mathcal{N}^l$ that orders neurons based on their importance in representing concept $c$, determined by some saliency metric $s_{j,c}$. For any neurons $j_1, j_2 \in \mathcal{N}^l$, if $r_c(j_1) < r_c(j_2)$ then $s_{j_1,c} \geq s_{j_2,c}$, where a lower rank indicates higher importance.


% \hammad{cite other people}



% \begin{table*}
% \centering
% \scriptsize
% \begin{tabular}{l|rrrr|rrrr|rrrr}
% \toprule
% \multicolumn{13}{c}{\textbf{Model Performance Drops}} \\
% \midrule
% & \multicolumn{4}{c|}{Probeless} & \multicolumn{4}{c|}{Probe} & \multicolumn{4}{c}{Max} \\
% \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% \midrule
% Class 0 & -0.738 & -0.733 & -0.103 & -0.097 & -0.613 & -0.650 & -0.010 & -0.038 & -0.695 & -0.751 & -0.125 & -0.124 \\
% Class 1 & 0.045 & 0.041 & -0.113 & -0.112 & -0.014 & -0.015 & -0.010 & -0.034 & -0.879 & -0.882 & -0.019 & -0.009 \\
% Class 2 & -0.570 & -0.541 & -0.052 & -0.057 & 0.017 & 0.009 & -0.347 & -0.359 & -0.776 & -0.736 & -0.029 & -0.032 \\
% Class 3 & -0.164 & -0.166 & -0.035 & -0.038 & 0.078 & 0.061 & -0.047 & -0.104 & -0.713 & -0.714 & -0.006 & -0.007 \\
% Class 4 & -0.623 & -0.617 & -0.087 & -0.084 & -0.005 & -0.010 & -0.003 & -0.020 & -0.754 & -0.753 & -0.240 & -0.248 \\
% Class 5 & -0.817 & -0.714 & -0.101 & -0.105 & -0.206 & -0.127 & 0.003 & -0.010 & -0.587 & -0.601 & -0.301 & -0.308 \\
% \midrule
% \textbf{Trimmed Mean} & -0.524 & -0.510 & -0.086 & -0.086 & -0.052 & -0.036 & -0.018 & -0.049 & -0.735 & -0.738 & -0.103 & -0.103 \\
% \bottomrule
% \end{tabular}
% \caption{Performance drops relative to Baseline configuration(i.e: unultered model's performance) for three techniques: Probeless, Probe, and Max. All values show the difference from Base values. Results are for \textit{Emotions} dataset on the GPT-2 model. The trimmed mean excludes the top and bottom 10\% of values.}
% \label{tab:performance_drops_probeless_Probe_Max}
% \end{table*}




% old
% \begin{definition}[Saliency Ranking]
% \label{def:ranking}
% Saliency ranking quantifies how important/relevant a neuron is for representing a given concept.

% Given activation matrix $H^l_c$, the saliency score $s_{j,c}$ for neuron $j$ and concept $c$ is:
% \[s_{j,c} = \frac{1}{n}\sum_{i=1}^n |H^l_c[i,j]|\]
% where $n$ is the number of samples. Neurons are ranked by descending $s_{j,c}$ values.
% \end{definition}

%new
\begin{table*}[!t]
\caption{Performance drops relative to Baseline configuration(i.e: unaltered model's performance) for three techniques: Probeless, Probe, and Max. All values show the difference from Base values. Results are for \textit{Emotions} dataset on the GPT-2 model. 30\% neurons used. The trimmed mean excludes the top and bottom 10\% of values.}
\centering
\scriptsize
\begin{tabular}{l|rrrr|rrrr|rrrr}
\toprule
& \multicolumn{4}{c|}{Probeless} & \multicolumn{4}{c|}{Probe} & \multicolumn{4}{c}{Max} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
\textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
Class 0 &\textbf{ -0.738} & -0.733 & -0.103 & -0.097 & -0.613 & -0.650 & \textbf{-0.010} & \textbf{-0.038} & -0.695 & \textbf{-0.751} & -0.125 & -0.124 \\
Class 1 & 0.045 & 0.041 & -0.113 & -0.112 & -0.014 & -0.015 & \textbf{-0.010} & -0.034 & \textbf{-0.879} & \textbf{-0.882} & -0.019 & \textbf{-0.009} \\
Class 2 & -0.570 & -0.541 & -0.052 & -0.057 & 0.017 & 0.009 & -0.347 & -0.359 &\textbf{ -0.776} & \textbf{-0.736} & \textbf{-0.029} & \textbf{-0.032} \\
Class 3 & -0.164 & -0.166 & -0.035 & -0.038 & 0.078 & 0.061 & -0.047 & -0.104 & \textbf{-0.713} & \textbf{-0.714} & \textbf{-0.006} & \textbf{-0.007} \\
Class 4 & -0.623 & -0.617 & -0.087 & -0.084 & -0.005 & -0.010 & \textbf{-0.003} & \textbf{-0.020} & \textbf{-0.754} & \textbf{-0.753} & -0.240 & -0.248 \\
Class 5 & \textbf{-0.817} & \textbf{-0.714} & -0.101 & -0.105 & -0.206 & -0.127 & \textbf{0.003} & \textbf{-0.010} & -0.587 & -0.601 & -0.301 & -0.308 \\
\bottomrule
\end{tabular}
\vspace{-10pt}
\label{tab:performance_drops_probeless_Probe_Max}
\end{table*}

\subsection{Concept Erasure}
\label{sec:concept_erasure}
To assess the performance of neuronal ranking obtained using attribution methods, concept erasure acts as a critical diagnostic intervention to determine the causal effect of identified neurons for a given concept \citep{grain:aaai19-1,dai2021knowledge,dalvi2019neurox,morcos2018importancesingledirectionsgeneralization}. The core idea is that by manipulating the salient neurons, it is possible to precisely eliminate a specific identified concept while causing minimal disruption to the other concepts learned by the model. This can be formalized as follows: 
given a concept-learning model $M$ that maps any input instance
%sample 
$x$ (or part of an instance) to a concept $M(x)\in C$, an ideal intervened model $M'_{\text{ideal}}$ after erasing a target concept $c\in C$ should satisfy the following property:

\vspace{-10pt}
\begin{align*}
M'_{\text{ideal}}(x)
\begin{cases}
     \neq M(x) \quad \text{ if } M(x) = c, \\
    = M(x) \quad \text{ if } M(x)\neq c.
\end{cases}
\end{align*}
\vspace{-10pt}

% \fix{the equation is unclear. Does it mean that if Mx is not equal to c, Mideal will be same as Mx?}
% {\color{blue}Yes.}

% given a model \( M \), a downstream neuron-level intervention is applied to produce a new model \( M' \). Ideally the intervened model must satisfy the following properties.



% The idea is that manipulation of the salient neurons should allow for the precise removal of an identified concept, while all other concepts learned by the model are minimally affected. This can be formalized as given the model $M$ a downstream neuron level intervention is applied to create a new model $M^{'}$. The intervened model must satisfy the following properties.
% \begin{equation}
%    M^{'}(x) \neq M(x) \forall x \in X \rightarrow c \in C 
% \end{equation}
% \begin{equation}
%    M^{'}(x) = M(x) \forall x \in X \rightarrow c^{'} \in C \setminus \{c\}
% \end{equation}


A popular approach of concept erasure in neuronal analysis literature \citep{dai2021knowledge, AntvergB22} is zeroing out specific neurons that are ``important'' to the target concept.


% Relevant literature \citep{dai2021knowledge, AntvergB22} for neuronal analysis zeros out the neuron activations to achieve the desired effects.  


\subsection{Salient Neurons Extraction}
\label{sec:sailency}
%\fix{this section is largely not necessary. We can introduce these techniques but providing so much details is unnecessary given the limited space.}
%\fixed{Detail have been reduced (user pointed to the appendix for details)}


% There are several approaches to extracting saliency ranking defined in Section~\ref{def:ranking}. In this section we first describe how neural activations are recorded for analysis and the most prominent neural activation based techniques.

% We then detail three prominent techniques - maximum activation analysis, probe-based classification, and probeless comparison - and evaluate their effectiveness in identifying concept-relevant neurons.
% The hypothesis behind maximally activating neurons \citep{frankle2019lotterytickethypothesisfinding} is that these neurons contribute most significantly to the model's output, suggesting their importance for the concept c. To extract maximally activating neurons, the mean of absolute neuronal activations is computed which is then utilized as a neuronal ranking for a given concept. We utilize absolute values as \citep{voita2023neurons} show that a high negative values are also an important signal. Specifically, we take column wise mean $\mu$ of absolute values in the matrix $H^l_c$, we then rank neurons based on the magnitude of the mean. 

% \textbf{Recording activations for analysis.}
\textbf{Problem setup and preparation}:
We use a transformer model to process sentences. Before doing neuron interpretation, we record activations for training samples of different concepts as preparation. Specifically, if we want to interpret neurons of $\mathbf{h}^l$ (hidden state vector at layer $l$), we will traverse the training dataset and store the values of $\mathbf{h}^l$ and the associated concepts of all samples into a set $H^l$. The set $H^l$ is further partitioned into $H^l_c$ for all concepts $c\in C$. Such preparation is commonly used in the literature on neuron interpretation \citep{dalvi2019neurox, grain:aaai19-1, AntvergB22}.
% Let $h^l(x_t) \in \mathds{R}^d$ represent the hidden state activation vector at the output of layer $l$ for input token $x_t$ in input sequence $X = \{x_1, ..., x_n\}$, where $t$ is the token position index. For a given concept $c \in C$ with a set of input sequences $X = \{x_1, ..., x_n\}$, we extract 
% %model 
% activation vectors from models based on their sentence-level representations token i.e. CLS token for encoder-only models and the last token for decoder-only models.
% \begin{equation}
%    H^l_c = h^l_{\text{CLS}|\text{Last Token}}(x) \forall x \in X \rightarrow c \in C
% \end{equation}
% \fix{the above equation does not seem right. $x_t$ represents the t-th token and h should not have a subscript based on all previous definitions of h in the text.}
% where $H^l_c$ is the set of hidden state vectors for concept $c$, and each vector contains the activations for all neurons in that layer. 
% This approach is consistent with the methods used to record activations in related literature \citep{dalvi2019neurox, grain:aaai19-1, AntvergB22}. 


\textbf{Max activations.} \citet{frankle2019lotterytickethypothesisfinding} extract high neural activations as a saliency ranking metric relying upon the rationale that maximally activating neurons are salient as these neurons play a critical role in controlling the model's output, highlighting their importance for a concept $c$. \textbf{Probe analysis.} \citet{grain:aaai19-1} train a linear classifier on the hidden representations \( H^l_c \) to distinguish between concepts. The learned model weights are then utilized as a saliency ranking. \textbf{Probeless.} \citet{AntvergB22} examine individual neurons, without the need for auxiliary classifiers, using the element-wise difference between mean vectors. Details of these approaches are provided in Appendix~\ref{sec:sec_sailiency_details}.

Mutual comparison of these approaches is provided in Table~\ref{tab:performance_drops_probeless_Probe_Max}. From the table, we observed that irrespective of the method used to obtain saliency ranking, a single concept eraser using salient neurons causes deterioration in performance across several concepts. Max activation causes the highest degree of deterioration in the targeted concept while maintaining a comparable deterioration in auxiliary concepts, based on this finding, we adopt max activation ranking for saliency ranking. Moreover, we hypothesize that one reason for such deterioration in overall performance is due to the polysemantic nature of neurons.



% \textbf{} we analyze that the ratio of performance drop in targeted concept to performance drop in auxiliary concept is highest for Max. Based on this finding, we adopt max activation ranking for saliency ranking.

% Details of these methodologies and their mutual comparison of performance in concept erasure are provided in the appendix~\ref{sec_sailiency_details}. The results indicate that while max activation produces a higher drop in targeted concept performance, the
%while 
% performance deterioration of other concepts shows a similar trend.  

% \fix{it is unclear why did we select max activation ranking and what was the point of describing other methods if we are not summarizing their results. Can we say that, we observed that irrespective of the method used to obtain saliency ranking, a single concept eraser using salient neurons causes deterioration in performance across several concepts, with max activation showing the worst deterioration across concepts. We hypothesize that one reason of such deterioration in overall performance is due to the polysemantic nature of neurons.}
%\fixed{text updated}



% Given this finding , we use max activation ranking as the ranking for saliency.



 


% NA
% Probe analysis \citep{grain:aaai19-1}, trains a linear classifier on the hidden representations $H^l_c$ to distinguish between different concepts. This involves learning a weight matrix $W \in \mathbb{R}^{d \times k}$, where $d$ is the hidden dimension and $k$ is the number of concept classes. 
% The absolute magnitude of the trained model weights, specifically each row is then used as a ranking for each concept.

% After training convergence, the magnitude of weights in $W$ corresponding to each neuron provides a measure of that neuron's importance for concept detection. Specifically, the saliency score $s_n$ for neuron $n$ is computed as: $s_n = |W_{n,:}|_2$ where $W_{n,:}$ represents the row vector of weights connected to neuron $n$. Higher saliency scores indicate stronger association between the neuron and the target concept $c$. 
% To ensure robust saliency estimates, the probe is trained with L1 \& L2 regularization.


% For each concept $c$, $q(c)$ is defined as the mean vector of all word representations associated with that label.

% \umair{Max, consistencts sailency is high foy, experimental setup, problem definition}









% To achieve this, neural activations are manipulated to analyze the impact on model performance for concept \( c \) relative to other concepts in \( C \). When manipulating or blocking the neural activations for \( c \), the objective is twofold: 1) To determine whether the model's performance on concept \( c \), for which the neural activations are blocked, is degraded. 2) To assess whether the model's performance on other concepts is affected.


% \umair{max citations: https://arxiv.org/pdf/2304.12918 https://arxiv.org/pdf/2110.07483, mor geva: 1: https://aclanthology.org/2021.emnlp-main.446.pdf 2:https://arxiv.org/pdf/2203.14680}

% To do this literature manipulates the neural activations to analyse the effect on model performance on $c$ compared to other concepts in $C$.  If we manipulate/block the neural activations for $C$ our goal becomes two folds 1: is the model performance on concept $c$ for which we are blocking the neural activations deteriorated. 2: Is the performance of model 

\begin{figure}[!t]
    \centering
        \includegraphics[width=0.47\textwidth]{multi_dataset_overlap.pdf}
        % \vspace{-8pt}
        \caption{Illustrates the percentage overlap of salient neurons identified through maximal activation ranking across concepts (i.e., classes), in all combinations, for the GPT-2 model trained on different datasets. Percentage of salient neurons extracted is 30\%.}
\label{fig:percentage_overlap_sailient_neuron}
\vspace{-10pt}
\end{figure}






\section{Polysemanticity}
% \hammad{ a table for static for argument on turning off neuron. subsection superposition.}
% add what superposition is.

% We show in fig \ref{fig:percentage_overlap_max} that for even a model trained for specific tasks, there is a significant overlap between the salient neurons chosen. \hammad{50\%->10\% neuron selection. Percentage overlap between classes. 2 class overlap, 3 class overlap, 4 class overlap, 5 classes overlap} overlap decrease performance.
% 
%In this section we discuss about the primary challenge in Neural Network interpretation, and manipulation: t
The polysemanticity of neuronal units, salient neurons encode information about multiple concepts, pose a challenge to neural network interpretation and manipulation. In this section, we discuss the degree of polysemanticity in salient neurons in detail.
\begin{figure}[!t]
    \centering
        \includegraphics[width=0.45\textwidth]{multiple_neuron_dist_all_classes.png}
        % \vspace{-8pt}
        \caption{Neuronal Activation Patterns of six neurons on \textit{AG-News} dataset. Two on the top are the highest activating neurons, two in the middle are middle-ranked neurons, and two at the bottom are the lowest activating neurons for \textit{class 1}.}
        \label{fig:multi_neuron_hist}
        \vspace{-10pt}
\end{figure}

\textbf{Superposition.}
Polysemanticity (i.e., superposition of neurons) is primarily thought to arise when a model must represent significantly more features than the available capacity of its representational space or due to the training paradigms used. Limited representation space forces to encode multiple unrelated features within the same neuron to achieve high performance \citep{anthropic2023toy}. Additionally, training paradigms such as subtokenization in tokenization schemes, which aim to reduce vocabulary size to decrease the model size and improve generalization for tokens with the low natural frequency. This results in context-dependent token decomposition, where a single token may be split differently based on its surrounding context \citep{sennrich2016neural}. This contextual nature leads to a state of superposition, where multiple semantic meanings are encoded within the same neural activations \citep{elhage2022superposition,meng2022locating}. 
Weight initialization can also contribute to polysemanticity, as demonstrated empirically by \citep{lecomte2024causes}. Their findings reveal that neurons exhibiting activation patterns equidistant from or similarly close to multiple conceptual representations at initialization can lead to polysemanticity, even when the network's capacity (i.e., the number of neurons) exceeds the number of concepts to be represented.


% compactness with representational power, 
% models must develop efficient representations for a vast space of possible tokens while maintaining a manageable vocabulary size. Modern tokenization approaches



% In large language models, the main reason that is thought to be the reason for polysemanticity and superposition is  if a task contains many more features than neurons, then achieving high performance at
% the task might force the network to co-allocate unrelated features to the same neuron \citep{anthropic2023toy}




% In large language models (LLMs), tokenization strategies predominantly aim to minimize the vocabulary size while maximizing token generalizability.  Research has demonstrated that this approach enables more efficient token embedding representations and improved handling of out-of-vocabulary tokens during inference (check and add citations). The inherent complexity of tokenization emerges from the contextual nature of token decomposition, wherein a single token can be fragmented into multiple subtokens contingent upon linguistic context. This characteristic suggests representations for these tokens are likely encoded in a superposition state, a phenomenon substantiated (cite sparse probing).
\begin{figure}[!t]
    \centering
        \includegraphics[width=0.45\textwidth]{neurons_comparison.png}
        % \vspace{-8pt}
        \caption{Neuronal Activation Patterns comparison of neurons 480 and 675. The plots show class-specific activity patterns with fitted Gaussian curves. Both neurons were part of salient set of all classes when top 5\% salient selected on \textit{AG-News} dataset.}
        \label{fig:overlapping_neuron}
        \vspace{-10pt}
\end{figure}
\textbf{Polysemanticity in salient neurons.}
Given that salient neurons have a strong causal association with the concept of interest, their tendency to be mono-semantic should be high, but we find that there is a high degree of polysemanticity in salient neurons.

We investigate this by extracting 30\% salient neurons (i.e.: Max neurons) for different datasets on the 
%trained 
GPT-2 model. The results show that there is a considerable overlap of salient neurons between concepts (classes) as shown in Figure~\ref{fig:percentage_overlap_sailient_neuron}. %We see a considerable percentage of overlap of salient neurons between concepts(classes) from the same dataset. Even 
In the case of a two-class dataset IMDB, 
%on a trained model, 
the overlap of salient neurons, selected by max, is 
more than
%upwards of 
60\%. This shows 
%the
%at there is 
a 
high degree of polysemanticity.

Consequently, we extrapolate that salient neural representations may exist in a superimposed configuration, wherein a subset of the salient neurons encode information through intricate, overlapping activation patterns.
%
% \textbf{Problems with current approaches that invite polysemanticity}
%
%
The monolithic attribution paradigm potentially oversimplifies the complex, distributed nature of neuronal activation as can be seen in polysemanticity \citep{lecomte2024causes, marshall2024understandingpolysemanticityneuralnetworks}  where single neuron learns multiple seemingly unrelated concepts and elucidates them at different activation values

% \hammad{experiment back of mind: turning off multiple concepts to show ranges is better than zeroing}
% % The tokenization for LLMs is largely based on minimizing the number of unique tokens needed for the data vocabulary \hammamd{cite some papers that do specific research to tokenization}, which allows for generalization for unseen tokens during inference reduced embedding layer size(check and add more). However this in combination with the fact a single token can be tokenized into various subtokens depending on the context means that these are likely to be represented in superposition (cite sparse probing). Due to aforementioned property we posit that it could be that sailient neurons or a subset  of salient  neurons could superposition neurons




\section{Neuronal Activation Patterns}
In this section, we analyze the properties of neuronal activations of the salient neurons (including polysemantic) extracted via maximal activation. Our findings indicate that neuronal activations on a concept form a \textbf{Gaussian-like distribution}. 
We also find that salient neurons have a distinct Gaussian distribution of activations for different concepts with minimal overlap with other concept activations. 
% We then analyze the distribution of neuronal activations by extracting different sets of neurons from saliency ranking. these sets include the top maximally activating, the bottom maximally activating, and a pair of neurons from the middle of the ranking.




%KDE , a non-parametric method that estimates probability density by placing kernel functions at each data point and summing them to create a smooth continuous estimate.
\textbf{Qualitative Evaluation.}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\textwidth]{common_neurons_box_plot.pdf}
        % \vspace{-18pt}
        \caption{Box plot of neural activation of 11 polysemantic neurons (i.e: neurons in the salient group for all classes, percentage selected: 5\% top salient)  for 4 randomly selected classes out of 14 classes of\textit{ DBPedia-14} dataset.}
\label{fig:common_neurons_box_plot}
        \vspace{-10pt}
\end{figure*}
To visually demonstrate that neuron activations for a concept $c$ follow a Gaussian-like distribution, we extract model representations as described in Section~\ref{sec:activation_analysis}. Using saliency ranking $r_c$ for a single concept class, we examine neurons from different ranking positions in the GPT-2 model on the AG-News dataset: two top-ranked neurons ($r_c \leq 2$), two middle-ranked neurons ($r_c \approx d/2$), and two bottom-ranked neurons ($r_c \geq d-1$). In Figure~\ref{fig:multi_neuron_hist} and \ref{fig:overlapping_neuron} we use Kernel Density Estimation (KDE) to visualize these distributions. Figure~\ref{fig:multi_neuron_hist} reveals that while the activations are Gaussian-like for different concepts, salient neurons demonstrate distinct activation patterns with limited overlap, middle-ranked neurons show a higher degree of overlap than the top ones, whereas non-salient neurons (bottom two) exhibit the highest overlap in their activation distributions. On \textit{GPT-2} model.

% Additionally in Figure~\ref{fig:overlapping_neuron} we visualize two polysemantic neurons that were part of the salient set for all classes in the dataset. 

% From the visual, it can be analysed that there are two types of polysemantic neurons. 1) One's like neuron 480 that although are part of the polysemantic set but still have distinct partially separable activation patterns. 2) One's like neuron 675 that have overlapping activation patterns for all the classes, such neurons cant be disentangled by using Neuron Lens.

% In Figure~\ref{fig:common_neurons_box_plot} we show 11 neurons from the polysemantic subset when a 5\% saliency set was selected. We plot 4 randomly selected classes out of 14 classes for such neurons. It can be observed from the Figure that even though the neurons are polysemantic the activation range in which they activate is mostly separable.



Additionally in Figure~\ref{fig:overlapping_neuron}, we identify and visualize two distinct types of polysemantic neurons that appear in the salient sets across all classes, when 5\% salient set was selected, in the dataset. The first type, exemplified by neuron 480, maintains partially separable activation patterns despite being polysemantic, suggesting some degree of class-specific behavior. In contrast, the second type, represented by neuron 675, exhibits completely overlapping activation patterns across all classes, making it hard to disentangle.
%using Neuron Lens, though it can be observed that classes 2 and 4 exhibit some degree of separability. 
To further investigate this phenomenon, Figure~\ref{fig:common_neurons_box_plot} presents a broader analysis of neurons from the polysemantic subset, identified using a 5\% saliency threshold. By examining these neurons' behavior across four randomly selected classes (out of 14 total classes), we observe that most polysemantic neurons exhibit a high degree of separability, for some classes – while they respond to multiple classes, they tend to operate in partially separable activation ranges, supporting the possibility of meaningful disentanglement.


\begin{table}[t]
% \vspace{-10pt}
\caption{Statistical analysis of different datasets showing skewness, kurtosis, and Kolmogorov-Smirnov test results.}
\centering
\scriptsize
\begin{tabular}{l|rrr}
\toprule
\textbf{Dataset} & \textbf{Skewness} & \textbf{Kurtosis} & \textbf{KS-Test} \\
\midrule
stanfordnlp/imdb & 0.0014 & 3.6639 & 1.0000 \\
fancyzhx/dbpedia\_14 & -0.0007 & 3.9360 & 0.9782 \\
dair-ai/emotion & 0.0015 & 3.0198 & 0.9446 \\
fancyzhx/ag\_news & -0.0013 & 3.2060 & 0.9918 \\
stanfordnlp/sst2 & -0.0083 & 3.2038 & 1.0000 \\
\bottomrule
\end{tabular}

\label{tab:dataset_stats}
\vspace{-10pt}
\end{table}

\textbf{Quantitative Evaluation.}
To quantify the effect of Gaussian-like distribution of neurons, for $c \in C$, we perform statistical analysis of activations. We computed the skewness, kurtosis \citep{joanes1998comparing} and analyzed the normality of neuronal activations using Kolmogorov-Smirnov (KS) test \citep{massey1951kolmogorov}. The results for basic distributional properties across all neurons are presented in Table~\ref{tab:dataset_stats}. The average skewness is close to 0 across all datasets, indicating strong symmetry (ideal normal distribution: 0), and the average kurtosis is close to 3, nearly identical to the expected value for a normal distribution (3.0), on \textit{GPT-2} model.


To quantitatively assess normality, while accounting for practical significance, we employ the KS test with an effect size threshold of 10\%. This approach tests whether the distribution remains within a reasonable bound of normality, rather than testing for perfect normality, which is overly strict for real-world data. For each neuron, we normalize the activations to zero mean and unit variance, then compute the KS statistic against a standard normal distribution. The KS statistic represents the maximum absolute difference between the empirical and theoretical cumulative distribution functions. Using a threshold of 0.1 (allowing maximum 10\% deviation from normal), we find that close to 100\% of the neurons exhibit practically normal distributions. The combination of near-ideal skewness and kurtosis values, visual confirmation through KDEs, and our effect size-based KS tests provide strong evidence that the activations follow approximately normal distributions.

% We further expand on this experiment to gauge \textbf{whether these Gaussian-like activations occur in separable ranges for various concepts}. The results indicate that the higher the ranking of these neurons the more separable the activation ranges are. However, the separable effect drops sharply resulting in high-ranking neurons having minimal activation meaning that they provide the same information across concepts. To illustrate this we compute the overlap maximal activating neurons overlap among the concepts. When choosing $30\%$ of the neurons we find that their overall value although different for each neuron provides the same activation across classes. 

% We also analyse their role in overall prediction performance, results shown in Table \ref{}. The results indicate that removing such neurons results in xyz. \hammad{TODO AT END}


\begin{table*}[t]
\caption{Evaluation of selected models on  IMDB, SST2, AG-News, and DBPedia-14 datasets using activation range and neuron masking techniques. Performance metrics, calculated using trimmed means at the class level, include class accuracy (Acc), class prediction probability (Conf), average accuracy across other classes (CAcc), and average class prediction probability across other classes (CConf). Base Values represent baseline model performance, Activation Range Masking and Neuron Masking entries show the drop from these baselines. The trimmed mean is calculated by removing extreme values (top and bottom 10\%) before computing the average to reduce the impact of outliers. We only \textbf{bold} the drops in auxiliary concepts (CAcc, CConf). The Drops in targeted concepts are comparable with \textbf{Neuron Masking with slightly better} results overall. For \textit{Bert} and \textit{GPT-2} 50\% and for \textit{Llama-3.2-3B} 30\% neurons selected.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Dataset} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{5}{*}{BERT} & IMDB & 0.928 & 0.904 & 0.928 & 0.904 & -0.190 & -0.353 & 0.059 & -0.078& -0.184 & -0.360 & 0.058 & \textbf{0.030} \\
& SST2 & 0.910 & 0.903 & 0.910 & 0.903 & -0.051 & -0.313 & 0.031 & -0.046& -0.060 & -0.330 & 0.031 & \textbf{0.043} \\
& AG-NEWS  & 0.948 & 0.929 & 0.948 & 0.929 & -0.271 & -0.590 & 0.012 & -0.074 & -0.261 & -0.590 & \textbf{0.013} & \textbf{-0.009} \\
& Emotions  & 0.894 & 0.834 & 0.917 & 0.876 & -0.291 & -0.633 & 0.013 & -0.265 & -0.279 & -0.635 & \textbf{0.014} & \textbf{-0.069} \\
& DBPedia-14 & 0.992 & 0.991 & 0.990 & 0.989 & -0.028 & -0.786 & 0.000 & -0.017 & -0.015 & -0.766 & 0.000 & \textbf{-0.000} \\
\midrule
\multirow{5}{*}{GPT-2} & IMDB & 0.952 & 0.939 & 0.952 & 0.939 & -0.196 & -0.188 & \textbf{0.033} & \textbf{0.045} & -0.195 & -0.197 & 0.031 & 0.042 \\
& SST2 &  0.966 & 0.958 & 0.966 & 0.958 & -0.165 & -0.190 & 0.025 & 0.032 & -0.159 & -0.192 & 0.025 & \textbf{0.028} \\
& AG-NEWS & 0.945 & 0.933 & 0.945 & 0.933 & -0.871 & -0.877 & -0.155 & \textbf{-0.163}& -0.849 & -0.862 & \textbf{-0.063} & -0.223  \\
& Emotions & 0.905 & 0.892 & 0.930 & 0.919 & -0.735 & -0.738 & -0.103 & -0.103 & -0.737 & -0.739 & \textbf{-0.044} & \textbf{-0.046} \\
& DBPedia-14 & 0.993 & 0.990 & 0.990 & 0.988& -0.810 & -0.845 & -0.154 & -0.177 & -0.782 & -0.825 & \textbf{-0.015} & \textbf{-0.031}  \\
\midrule
\multirow{5}{*}{Llama-3.2-3B} & IMDB & 0.952 & 0.939 & 0.952 & 0.939& -0.196 & -0.188 & 0.033 & 0.045 & -0.195 & -0.197 & \textbf{0.031} & \textbf{0.042} \\
& SST2 &  1.000 & 0.559 & 1.000 & 0.559 & -0.760 & -0.429 & -0.394 & -0.295 & -0.756 & -0.427 & \textbf{-0.384} & \textbf{-0.291} \\
& AG-NEWS & 1.000 & 0.744 & 1.000 & 0.744 & -0.934 & -0.725 & -0.660 & -0.572 & -0.935 & -0.725 &\textbf{ -0.484} &\textbf{ -0.454}  \\
& Emotions & 0.815 & 0.472 & 0.823 & 0.477 & -0.795 & -0.470 & -0.696 & -0.429 & -0.797 & -0.469 & \textbf{-0.594} & \textbf{-0.404}  \\
& DBPedia-14 & 1.000 & 0.533 & 1.000 & 0.563 & -0.992 & -0.528 & -0.912 & -0.445 & -0.986 & -0.527 & \textbf{-0.663} & \textbf{-0.354}  \\
\bottomrule
\end{tabular}
\label{tab:performance_drops_all}
\vspace{-10pt}
\end{table*}









% \begin{table*}[t]
% \centering
% \scriptsize
% \begin{tabular}{l|l|rrrr|rrrr|rrrr}
% \toprule
% \multicolumn{14}{c}{\textbf{Model Performance}} \\
% \midrule
% \textbf{Model} & \textbf{Dataset} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Activation Range Masking}} & \multicolumn{4}{c}{\textbf{Neuron Masking}} \\
% \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
% &  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% \midrule
% \multirow{4}{*}{BERT} & IMDB & 0.928 & 0.904 & 0.928 & 0.904 & -0.184 & -0.360 & 0.058 & 0.030 & -0.190 & -0.353 & 0.059 & -0.078 \\
% & SST2 & 0.910 & 0.903 & 0.910 & 0.903 & -0.060 & -0.330 & 0.031 & 0.043 & -0.051 & -0.313 & 0.031 & -0.046 \\
% & AG-NEWS & 0.948 & 0.929 & 0.948 & 0.929 & -0.261 & -0.590 & 0.013 & -0.009 & -0.271 & -0.590 & 0.012 & -0.074 \\
% & DB-14 & 0.992 & 0.991 & 0.990 & 0.989 & -0.015 & -0.766 & 0.000 & -0.000 & -0.028 & -0.786 & 0.000 & -0.017 \\
% \midrule
% \multirow{5}{*}{GPT-2} & IMDB & 0.952 & 0.939 & 0.952 & 0.939 & -0.195 & -0.197 & 0.031 & 0.042 & -0.196 & -0.188 & 0.033 & 0.045\\
% & SST2 &  0.966 & 0.958 & 0.966 & 0.958 & -0.159 & -0.192 & 0.025 & 0.028 & -0.165 & -0.190 & 0.025 & 0.032 \\
% & AG-NEWS & 0.945 & 0.933 & 0.945 & 0.933 & -0.849 & -0.862 & -0.063 & -0.223 & -0.871 & -0.877 & -0.155 & -0.163 \\
% & DB-14 & 0.992 & 0.991 & 0.990 & 0.989 & -0.001 & -0.008 & -0.001 & -0.009 & -0.004 & -0.027 & -0.004 & -0.031 \\
% & Banking-77 & 0.944 & 0.932 & 0.936 & 0.926 & -0.157 & -0.850 & 0.001 & -0.005 & -0.291 & -0.894 & 0.001 & -0.129 \\
% \midrule
% \multirow{3}{*}{Llama-3.2-3B} & IMDB & 0.952 & 0.939 & 0.952 & 0.939 & -0.195 & -0.197 & 0.031 & 0.042 & -0.196 & -0.188 & 0.033 & 0.045\\
% & SST2 &  1.000 & 0.559 & 1.000 & 0.559 & -0.756 & -0.427 & -0.384 & -0.291 & -0.760 & -0.429 & -0.394 & -0.295 \\
% & AG-NEWS & 1.000 & 0.744 & 1.000 & 0.744 & -0.935 & -0.725 & -0.484 & -0.454 & -0.934 & -0.725 & -0.660 & -0.572 \\
% %add here
% \bottomrule
% \end{tabular}
% \caption{Evaluation of selected models on  IMDB, SST2, AG-News, and DB-14 datasets using activation range and neuron masking techniques. Performance metrics, calculated using trimmed means at the class level, include class accuracy (Acc), class prediction probability (Conf), average accuracy across other classes (CAcc), and average class prediction probability across other classes (CConf). Base Values represent baseline model performance, Activation Range Masking and Neuron Masking entries show the drop from these baselines. The trimmed mean is calculated by removing extreme values (top and bottom 10\%) before computing the average to reduce the impact of outliers.}
% \label{tab:all_datasets_all_models}
% \end{table*}














% \begin{table*}
% \centering
% \scriptsize
% \begin{tabular}{l|rrrr|rrrr|rrrr}
% \toprule
% \multicolumn{13}{c}{\textbf{Model Performance}} \\
% \midrule
% & \multicolumn{4}{c|}{Base Values} & \multicolumn{4}{c|}{Range Drop} & \multicolumn{4}{c}{MAX Drop} \\
% \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% \midrule
% Class 0 & 0.913 & 0.527 & 0.821 & 0.474 & -0.742 & -0.453 & -0.229 & -0.169 & -0.760 & -0.458 & -0.256 & -0.197 \\
% Class 1 & 0.947 & 0.491 & 0.836 & 0.493 & -0.367 & -0.277 & -0.334 & -0.228 & -0.349 & -0.284 & -0.336 & -0.266 \\
% Class 2 & 0.739 & 0.370 & 0.860 & 0.500 & -0.638 & -0.327 & -0.334 & -0.254 & -0.638 & -0.328 & -0.351 & -0.287 \\
% Class 3 & 0.854 & 0.521 & 0.852 & 0.481 & 0.076 & -0.010 & -0.434 & -0.310 & 0.067 & -0.015 & -0.465 & -0.324 \\
% Class 4 & 0.779 & 0.477 & 0.860 & 0.494 & -0.779 & -0.472 & -0.308 & -0.191 & -0.779 & -0.472 & -0.396 & -0.257 \\
% Class 5 & 0.507 & 0.314 & 0.876 & 0.504 & -0.408 & -0.242 & -0.236 & -0.183 & -0.437 & -0.248 & -0.253 & -0.208 \\
% \midrule
% \textbf{Trimmed Mean} & 0.821 & 0.465 & 0.852 & 0.492 & -0.539 & -0.325 & -0.303 & -0.214 & -0.546 & -0.330 & -0.335 & -0.254 \\
% \bottomrule
% \end{tabular}
% \caption{Model performance: Base values and performance drops in Range and MAX configurations. The trimmed mean excludes the top and bottom 10\% of values to provide a more robust central tendency measure.}
% \label{tab:performance_drops_gpt2}
% \end{table*}

% GPT-2
% \begin{table*}
% \centering
% \scriptsize
% \begin{tabular}{l|rrrr|rrrr|rrrr}
% \toprule
% \multicolumn{13}{c}{\textbf{Model Performance}} \\
% \midrule
% & \multicolumn{4}{c|}{Base Values} & \multicolumn{4}{c|}{Range Drop} & \multicolumn{4}{c}{MAX Drop} \\
% \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% \midrule
% Class 0 & 0.969 & 0.956 & 0.913 & 0.903 & -0.693 & -0.742 & -0.005 & -0.007 & -0.695 & -0.751 & -0.125 & -0.124 \\
% Class 1 & 0.939 & 0.938 & 0.925 & 0.908 & -0.875 & -0.876 & -0.016 & -0.021 & -0.879 & -0.882 & -0.019 & -0.009 \\
% Class 2 & 0.902 & 0.872 & 0.932 & 0.923 & -0.776 & -0.743 & -0.019 & -0.027 & -0.776 & -0.736 & -0.029 & -0.032 \\
% Class 3 & 0.910 & 0.905 & 0.932 & 0.921 & -0.710 & -0.716 & -0.003 & -0.006 & -0.713 & -0.714 & -0.006 & -0.007 \\
% Class 4 & 0.869 & 0.854 & 0.938 & 0.927 & -0.747 & -0.746 & -0.101 & -0.104 & -0.754 & -0.753 & -0.240 & -0.248 \\
% Class 5 & 0.857 & 0.798 & 0.932 & 0.923 & -0.579 & -0.593 & -0.261 & -0.268 & -0.587 & -0.601 & -0.301 & -0.308 \\
% \midrule
% \textbf{Trimmed Mean} & 0.905 & 0.892 & 0.930 & 0.919 & -0.732 & -0.737 & -0.035 & -0.040 & -0.735 & -0.738 & -0.103 & -0.103 \\
% \bottomrule
% \end{tabular}
% \caption{Model performance: Base values and performance drops in Range and MAX configurations. The trimmed mean excludes the top and bottom 10\% of values to provide a more robust central tendency measure.}
% \label{tab:performance_drops_gpt2_emotions}
% \end{table*}

% \begin{proposition}[Range-based Information Separation]
% For a salient neuron $j$ and concept $c$, {\color{red} if SOMETHING follows Gaussian distribution, then} the probability of activation falling within concept-relevant bounds is:
% \[\Pr(|\frac{H^l_c[i,j] - \mu^l_{c,j}}{\sigma^l_{c,j}}| \leq \tau) = \text{erf}(\frac{\tau}{\sqrt{2}}) \approx 0.97\]
% for $\tau = 2.5$, where erf is the error function.
% \end{proposition}

% \begin{corollary}[Masking Function Optimality]
% The masking function $f$ defined in Equation (4) with $\tau = 2.5$ optimizes the tradeoff between concept preservation and interference minimization:
% \[\text{lb} = \mu[j] - \tau\sigma[j] \text{ and } \text{up} = \mu[j] + \tau\sigma[j]\]
% capturing approximately 97\% of concept-relevant activations while minimizing overlap with other concepts.
% \end{corollary}

\section{Activation Ranges-guided Intervention} 
\label{sec:Ranges}


Given that the neuronal activations are Gaussian-like with separable means, 
% interpreting and manipulating the entire neuron activation space, 
we can interpret and manipulate the neurons in a more precise manner instead of zeroing out the neurons. 
Specifically, we only zero out a salient neuron (by saliency ranking) when its value is within a certain range. The core part of this design is selecting a suitable range that is highly correlated to the target concept $c$ that we want to remove. Intuitively, this range-based method conducts fine grained manipulation of neurons and thus alleviates the negative impact on other non-target concepts.

In this work, we adopt a straightforward choice of the aforementioned range, which is easy to calculate and implement. Specifically, we first calculate the empirical average $\mu\in \mathds{R}$ and standard deviation $\sigma\geq 0$ of the values of the salient neuron for all samples associated with the target concept $c\in C$. After that, we assign the range as $[\mu-\tau\times \sigma,\ \mu+\tau \times \sigma]$, where $\tau>0$ is a hyperparameter to make a tradeoff between erasing the target concept $c$ (using larger $\tau$) and smaller impact on other concepts (using smaller $\tau$). For this work, we use $\tau=2.5$. We chose this value to cover the majority of the target concept activations to produce a similar deterioration of the targeted concept. 

Mathematically, our range-based method can be described as follows. To erase a target concept $c$ for a sample $x$ (e.g., a sentence or a word), we manipulate the salient neuron $h_j^l(x)$ ($j$-th element of the hidden state vector $\mathbf{h}^l(x)\in \mathds{R}^d$ at layer $l$).
\begin{align*}
    h_j^l(x)
    \begin{cases}
        =0  &\text{ if }h_j^l(x) \in \text{CorrelatedRange}(l, j, c),\\
        =h_j^l(x)  &\text{ otherwise},
    \end{cases}
\end{align*}

where in this work we let
\begin{align*}
    &\text{CorrelatedRange}(l, j, c)= [\mu-2.5\sigma,\ \mu+2.5 \sigma],\\
    &\mu = \frac{1}{\left|H^l_c\right|}\sum_{\mathbf{h}^l\in H^l_c}h_j^l,\ \quad\sigma = \sqrt{\frac{1}{\left|H^l_c\right|}\sum_{\mathbf{h}^l\in H^l_c} \left(h_j^l-\mu\right)^2}.
\end{align*}
Notice that $H^l_c$ was defined in problem setup and preparation of \cref{sec:sailency}, which denotes the set of hidden state vector $\mathbf{h}^l(x_c)$ at layer $l$ for all training samples $x_c$ associated with concept $c$. Here $|\cdot|$ denotes the cardinality of a set.


\begin{table*}[t]
\caption{Evaluation of selected models on the \textit{AG-News} dataset using neuron and range masking techniques. \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance. For \textit{Bert} and \textit{GPT-2} 50\% and for \textit{Llama-3.2-3B} 30\% neurons selected.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{4}{*}{BERT} 
&Class 0 & 0.945 & 0.936 & 0.949 & 0.927 & -0.205 & -0.587 & \textbf{0.004} & -0.076 & -0.198 & -0.589 & 0.007 & \textbf{-0.010} \\
&Class 1 & 0.993 & 0.988 & 0.933 & 0.910 & -0.225 & -0.659 & 0.004 & -0.077 & -0.194 & -0.650 & \textbf{0.003} &\textbf{ -0.012} \\
&Class 2 & 0.905 & 0.881 & 0.962 & 0.945 & -0.300 & -0.536 & 0.014 & -0.079 & -0.298 & -0.542 & 0.014 & \textbf{-0.009} \\
&Class 3 & 0.949 & 0.913 & 0.948 & 0.935 & -0.354 & -0.577 & 0.026 & -0.065 & -0.353 & -0.579 & \textbf{0.025} & \textbf{-0.005 } \\
\midrule
% \textbf{Trimmed Mean} & 0.894 & 0.834 & 0.917 & 0.876 & -0.279 & -0.635 & 0.014 & -0.069 & -0.291 & -0.633 & 0.013 & -0.265 \\
\multirow{4}{*}{GPT-2} 
&Class 0 & 0.955 & 0.951 & 0.941 & 0.928 & -0.920 & -0.926 & -0.231 & -0.224 & -0.919 & -0.925 & \textbf{-0.019} & \textbf{-0.008} \\
&Class 1 & 0.986 & 0.981 & 0.931 & 0.917 & -0.926 & -0.931 & -0.253 & -0.257 & -0.912 & -0.916 & \textbf{-0.054} & \textbf{-0.069} \\
&Class 2 & 0.897 & 0.886 & 0.960 & 0.949 & -0.696 & -0.737 & -0.110 & \textbf{-0.132} & -0.678 & -0.725 & \textbf{-0.097} & -0.306 \\
&Class 3 & 0.940 & 0.916 & 0.946 & 0.939 & -0.940 & -0.916 & -0.024 & \textbf{-0.037} & -0.887 & -0.882 & \textbf{-0.080} & -0.510 \\
\midrule\textbf{}
\multirow{4}{*}{Llama-3.2-3B} 
&Class 0 & 1.000 & 0.936 & 1.000 & 0.680 & -0.995 & -0.934 & -0.530 & -0.427 & -0.995 & -0.934 & \textbf{-0.345} & \textbf{-0.306} \\
&Class 1 & 1.000 & 0.742 & 1.000 & 0.744 & -0.870 & -0.680 & -0.615 & -0.599 & -0.875 & -0.681 & \textbf{-0.515} & \textbf{-0.503} \\
&Class 2 & 1.000 & 0.655 & 1.000 & 0.773 & -0.895 & -0.646 & -0.795 & -0.634 & -0.895 & -0.646 & \textbf{-0.655} & \textbf{-0.549} \\
&Class 3 & 1.000 & 0.642 & 1.000 & 0.778 & -0.975 & -0.641 & -0.698 & -0.630 & -0.975 & -0.640 & \textbf{-0.420} & \textbf{-0.459}\\
\bottomrule
\end{tabular}

\label{tab:ag_news_on_all}
\vspace{-10pt}
\end{table*}
% Concretely, the partial activation spectrum of the neuronal activation can be mapped to a concept $c$, using the mean and standard deviation of a sub-range of neuronal activation.
%can be mapped to a concept $c$. 


% \begin{theorem}[Activation Distribution Properties]
% For a neuron $n$ and concept $c$, given activation matrix $H^l_c$ as defined in Equation (1), the empirical distribution of activations approximates a Gaussian:
% \[P(H^l_c[i,j]) \approx \mathcal{N}(\mu^l_{c,j}, {\sigma^l_{c,j}}^2)\]
% where $\mu^l_{c,j}$ and $\sigma^l_{c,j}$ are the mean and standard deviation for neuron $j$ in layer $l$ for concept $c$.


% Following from the Central Limit Theorem and the linear combination of inputs in the feed-forward computation:
% \[h^l_{CLS|LastToken}(x) = W^l h^{l-1} + b^l\]
% As the number of inputs grows large, the distribution approaches normal regardless of input distribution.
% \end{theorem}


%To gauge the performance of neuronal range manipulation, 



% We compare our range-based neuron manipulation method with monolithic attribution of salient neurons (i.e., masking maximally activating neurons) using the 



%. We use 


% concept erasure as outlined in Section~\ref{sec:concept_erasure}.


%as a diagnostic tool to compare both attribution techniques.
%
%
% We compute max ranking as explained in \ref{sec:activation_analysis}, and manipulate top $p$ neurons under both the settings 1) Manipulating ranges as explained in \ref{sec:Ranges}. 2) Manipulating whole neurons as explained in \ref{sec:concept_erasure}. 
%
%For the attribution and masking of activation ranges, 


% Given an activation matrix $H^l_c$, we compute neuron-specific statistics. For each salient neuron $j$, we calculate the column-wise standard deviation 



%for an activation matrix $H^l_c$, the column-wise standard deviation 


% $\sigma[j] = \sqrt{\frac{1}{n}\sum_{i=1}^n (X[i,j] - \mu[j])^2}$ and mean $\mu[j] = \frac{1}{n}\sum_{i=1}^n X[i,j]$ using


%for an 


% the 
% activation matrix $H^l_c$.

%for each salient neuron
% We define our 
%The 
% range-bound mask based on the activation values as follows:
%and chosen $\tau$ is formalized below:
% For a threshold $\tau$, we apply a conditional mask to each activation value:

% \vspace{-12pt}
% \begin{equation}
% \centering
%     f(X[i,j]) = \begin{cases}
%     0 & \text{if }  lb \leq  X[i,j] \leq ub \\
%     X[i,j] & \text{otherwise}
%     \end{cases}
% \end{equation}
% \vspace{-8pt}


% where the lower bound $(lb)$ is $\mu[j] - \tau * \sigma[j]$, the upper bound $(ub)$ is $\mu[j] + \tau * \sigma[j]$ and $\tau$ is a threshold to balance the coverage of Gaussian-like space and the tail end of the distribution. We use $\tau=2.5$ i.e. the span of activation range 2.5 times the standard deviation.
%
%
%threshold $\tau$
%To cover most of the Gaussian-like space and handle noisy activations at the tail end of the distribution, the empirical rule can be applied to calculate the span of the activation range, which is $2.5$ times the standard deviation.
%
% The range-bound mask $f(X[i,j]) =0$ sets 
%0 as 
% the activation value 
%for activations 
% that fall outside the bounds to zero. Thus removing their influence on downstream computations.

% We use $\tau$ as a hyperparameter that controls the \textbf{range bound} using the assumption that the activations are normally distributed. Under this assumption, we set the value of $\tau$ to 2.5 as it would cover $98.76\%$ spread of the activation for the concept $c$.



% Our findings reveal that salient neurons operate within a bounded range for a given concept Figure \ref{fig:nueron_activation_std}. 


% We further experiment to test the overlap of salient neurons between different concepts, this is shown in Figure \ref{fig:percentage_overlap_sailient_neuron}, which indicates that there is considerable sharing of salient neurons thus these neurons are polysemantic in nature \hammad{experiument with intersectional neurons only}.


% This observation raises regarding downstream interventions that aim to change the model behavior regarding those particular concepts as zeroing out the neurons and training them to manipulate behavior about a particular concept or concepts would result in losing information about other concepts \hammad{zeroing out downstream effect summation change, training output activation space neurons}.  We propose that instead of employing zero-out masking or fine-tuning entire neurons for downstream computations, we instead map a specific range within the activation spectrum of these neurons, this would ameliorate the aforementioned issues as only a subset of the neuronal activation space is shifted or changed.

% Moreover, salient neurons have a limited overlapping activation range between different concepts

% \hammad{need an image to show that box plot size is different for max vs random neurons}

% To map the partial activation spectrum to concept c, we compute the standard deviation of activations on concept c. We use $\tau$ as a hyperparameter that controls the \textbf{range bound} using the naive assumption that the activations are normally distributed. Under this assumption, we set the value of $\tau$ to 2.5 as it would cover $97\%$ spread of the activation for the concept $c$.
% Specifically, we perform this operation using the activation matrix $H^l_c$, we calculate the column wise standard deviation $\sigma[j] = \sqrt{\frac{1}{n}\sum_{i=1}^n (X[i,j] - \mu[j])^2}$ and mean $\mu[j] = \frac{1}{n}\sum_{i=1}^n X[i,j]$ for each salient neuron.



\subsection{Experimental Setup}
\textbf{Models.} This study employs a range of transformer-based architectures, including encoder and decoder models such as \textbf{fine-tuned} BERT \citep{devlin2019bertpretrainingdeepbidirectional}, DistilBERT \citep{sanh2020distilbertdistilledversionbert}, GPT-2 \citep{radford2019language}, and \textbf{pretrained} Llama-3.2-3B \citep{grattafiori2024llama3herdmodels}. We incorporate our methodology at the penultimate layer, abiliation for layer selection is provided in the Appendix~\ref{sec:layer_abiliation}.
%to investigate knowledge removal in large language models (LLMs). 
The training details for the models are provided in appendix~\ref{sec_experimental_appendix}.





% (if they already were not single token)

\textbf{Datasets.} We consider 
%The datasets utilized encompass 
various classification based tasks; sentiment analysis (IMDB, \citet{maas2011learning}), (SST2, \citet{socher2013recursive}), emotion detection (Dair-Ai/Emotions \citet{saravia-etal-2018-carer}), news classification (AG-News \citep{NIPS2015_250cf8b5}) and article content classification tasks (DBPedia-14 \citep{NIPS2015_250cf8b5}).

\textbf{Metrics.}
For performance evaluation, we employ two quantitative metrics: prediction accuracy and the model's confidence score for the ground truth. Initially, we established baseline measurements of both accuracy and confidence for all concepts $C$ in the unmodified model.
Post-intervention measurements are recorded for both the target concept $c$ and auxiliary concepts $c'$. The effectiveness and precision of neuron identification can be assessed through two key metrics: (1) the magnitude of performance degradation for concept $c$, and (2) the extent of unintended impact on auxiliary concepts $c'$. Throughout our analysis, we denote the accuracy and confidence metrics for concept $c$ as \textbf{Acc} and \textbf{Conf} respectively, while corresponding measurements for auxiliary concepts $c'$ are represented as \textbf{CAcc} and \textbf{CConf}.



% \begin{figure}[!t]
%     \centering
%         \includegraphics[width=0.47\textwidth]{accuracy_comparison_ag_news.pdf}
%         \caption{Accuracy comparison between Neuronal Range manipulation (blue) and Maximally activation monolithic attribution (red) methods. Both approaches show similar primary task degradation (solid lines), but differ significantly in complement task stability (dashed lines). The Neuronal Range method maintains more stable complement task performance compared to Maximal Activation's exponential degradation. Dataset \textit{AG-NEWS}.}
%         \label{fig:max_vs_range_10-90_AG_NEWS}
%         % \vspace{-0.4cm}
% \end{figure}

% \begin{figure}[!t]
%     \centering
%         \includegraphics[width=0.47\textwidth]{accuracy_comparison_DB_14.pdf}
%         \caption{Accuracy comparison between Neuronal Range manipulation (blue) and Maximally activation monolithic attribution (red) methods. Both approaches show similar primary task degradation (solid lines), but differ significantly in complement task stability (dashed lines). The Neuronal Range method maintains more stable complement task performance compared to Maximal Activation's exponential degradation. Dataset \textit{DBPedia-14}.}
%         \label{fig:max_vs_range_10-90_DB14}
%         % \vspace{-0.4cm}
% \end{figure}

% \begin{figure}[!t]
%     \centering
%         \includegraphics[width=0.47\textwidth]{accuracy_comparison_Banking_77.pdf}
%         \caption{Accuracy comparison between Neuronal Range manipulation (blue) and Maximally activation monolithic attribution (red) methods. Both approaches show similar primary task degradation (solid lines), but differ significantly in complement task stability (dashed lines). The Neuronal Range method maintains more stable complement task performance compared to Maximal Activation's exponential degradation. Dataset \textit{Banking-77}.}
%         \label{fig:max_vs_range_10-90_Banking-77}
%         % \vspace{-0.4cm}
% \end{figure}

% \subsection{Results}

% Our main finding is that manipulation using neuronal ranges is more precise than manipulating the whole neuron. Results of averaging and clipping 10\% to calculate trimmed means for all datasets are in Table \ref{tab:performance_drops_all}. It is clear from the results that the drop in performance in the concept of interest $c$ are comparable, and the drop in other concepts are better in the range masking. Complete results tables are provided in the appendix x. Full results for the dataset on AG-News are provided in the table~\ref{tab:emotions_on_all}.


\subsection{Results and Analysis}
\label{sec:results}

Table~\ref{tab:performance_drops_all} presents comprehensive results across five datasets, demonstrating the effectiveness of our range-based masking approach compared to traditional neuron masking. Range-based masking better preserves auxiliary concepts across all settings performance (CAcc, CConf), suggesting more precise concept isolation. 

Multi-class classification tasks (AG-News, Emotions, DBPedia-14) demonstrate more pronounced intervention effects. Range masking achieves substantial primary task degradation while maintaining auxiliary concept performance, particularly evident in AG-News results (Table~\ref{tab:ag_news_on_all}). The class-level analysis reveals asymmetric impacts: Classes 0 and 1 show larger performance drops (-0.919, -0.912 for GPT-2) compared to Classes 2 and 3 (-0.678, -0.887), suggesting varying degrees of concept distribution across neurons.

Model architecture and training methodology significantly influence intervention outcomes. BERT, trained directly on the classification tasks with a dedicated classification head, demonstrates remarkable resilience with minimal accuracy degradation (-0.015 on DBPedia-14) overall, under range masking it maintains auxiliary concept performance. This robustness likely stems from its task-specific fine-tuning, which enables more distributed and redundant concept representations. In contrast, GPT-2, despite being fine-tuned but trained in an autoregressive manner, shows substantially higher vulnerability with major drops in AG-NEWS ($\Delta_{acc} = -0.849$) and DBPedia-14 ($\Delta_{acc} = -0.782$). This increased sensitivity may be attributed to its autoregressive training objective, which potentially leads to more sequential and less redundant concept encodings. Llama-3.2-3B, evaluated in a few-shot setting without task-specific training, experiences the most severe degradation across all datasets (often exceeding -0.900), suggesting that pre-trained representations without task-specific fine-tuning are more vulnerable to targeted neuron interventions. These patterns indicate that task-specific training, particularly with classification-oriented objectives, may foster more robust and distributed concept representations that are more resilient to neuron interventions.

The effectiveness of range-based masking in preserving auxiliary concepts while achieving targeted performance degradation suggests that concept encoding in neural networks may follow activation range patterns rather than being strictly neuron-specific. This is particularly evident in the AG-News results, where range masking maintains better auxiliary concept scores (CAcc, CConf) compared to neuron masking across all classes and models. 

\begin{figure*}[!t]
    \centering
        \includegraphics[width=0.94\textwidth]{accuracy_comparison_multiple.pdf}
        \caption{Accuracy comparison between Neuronal Range manipulation (green) and complete neuron manipulation (orange) methods. Both approaches show similar primary task degradation (solid lines) but differ significantly in auxiliary task stability (dashed lines). The Neuronal Range method maintains more stable auxiliary task performance than Maximal Activation's steep degradation after 50\%. Model \textit{GPT-2}}
        \label{fig:max_vs_range_10-90_Emotions}
        \vspace{-10pt}
\end{figure*}

On binary classification tasks (IMDB, SST2), both masking approaches show moderate performance drops in targeted concepts, with BERT experiencing accuracy decreases of -0.184 and -0.060 respectively under range masking. The relatively modest impact on primary task performance, coupled with minimal auxiliary concept interference, indicates significant neuronal redundancy in binary classification tasks multiple neurons appear to encode similar sentiment-related features.

% These findings have important implications for neural network interpretability: (1) concept encoding appears to be more closely tied to activation ranges than to specific neurons, (2) architectural scale correlates with intervention vulnerability, potentially due to more specialized neuron functions in larger models, and (3) task complexity significantly influences concept distribution, with binary tasks showing higher redundancy compared to multi-class tasks. Our range-based approach provides a more precise tool for concept isolation, particularly valuable for understanding complex multi-class architectures where traditional neuron masking may cause unintended concept interference.




\textbf{Percentage Masking Effect}
As the proportion of masked neurons increases, the performance improvement over the maximum activation baseline becomes more pronounced. Beyond a critical masking threshold, the baseline performance deteriorates exponentially, while our proposed method maintains stable performance up to 100\% masking. This phenomenon can be explained by two 
key factors. First, as we ablate a higher percentage of neurons, the probability of encountering polysemantic neurons increases as a statistical consequence. Second, neurons identified through maximum activation are more likely to exhibit discriminatory behavior between concepts or demonstrate strong conceptual associations.
when analyzing neurons with lower activation patterns (not strongly associated with concepts), discrete classification becomes an increasingly crude metric, as evident from Figure~\ref{fig:max_vs_range_10-90_Emotions}. After 50\% masking, we see a steep drop in the general performance of the model, in the case of neuron masking. This suggests the need for a more nuanced approach to mapping information as to neural activations as blocking/manipulating a higher percentage of the model's representation creates a significant deviation from the original model's behavior. Our range-based methodology provides such precision, enabling more accurate characterization of neuronal behavior across the activation spectrum. 
%This also highlights an inherent flaw of complete neuron masking approaches, as the number of neurons being ablated grows the model's general performance deteriorates steeply because blocking/manipulating more percentage of the model's representation creates a significant deviation from the original model's behavior and computational capacity.

% \section{Key Takeaways}




\section{Related Work}

While we have discussed closely related approaches in Section~\ref{prelim}, here, we briefly review additional relevant techniques.
Circuit discovery identifies groups of neurons that jointly encode concepts, providing a structured view of model behavior~\citep{marks2024sparse, ConmyMLHG23, olah2020zoom}. However, extracting circuits is computationally intensive and lacks fine-grained neuron-level attribution.
Gradient-based methods attribute predictions to input features by tracking gradients through the network, with integrated gradients~\citep{sundararajan2017axiomaticattributiondeepnetworks,dai2021knowledge} being a widely used approach. However, they struggle with polysemanticity, as they do not disentangle overlapping concepts within neurons.
Causal analysis methods intervene on internal components to assess their role in encoding concepts. Causal tracing measures the effect of corrupting activations on model performance~\citep{vig2020investigating, meng2022locating}, while causal mediation analysis quantifies information propagation through neurons~\citep{vig2020investigating}. Although effective, these techniques require costly perturbation experiments.
Beyond neuron-level analysis, representation-level methods examine hidden states and their relationship to model outputs. Logit lens~\citep{logitlens} and tuned lens~\citep{belrose2023elicitinglatentpredictionstransformers} project intermediate representations into the model’s vocabulary space to track information flow, while sparse probing~\citep{gurnee2023findingneuronshaystackcase} compresses hidden representations into sparse, interpretable subspaces.
While prior work has advanced interpretability, most methods rely on discrete neuron-to-concept mappings, which fail to account for polysemanticity. Our work extends activation-based approaches by introducing activation ranges as the fundamental unit of interpretability to enable more precise concept attribution and intervention.

%OLD
% \fix{Need to know the detail of RW needed. We already discuss RW wherever we are using it in theh
% sections}
% Machinistic Interpretability(MI) is an emerging field concerned with reverse engineering models' underlying feature mappings for human-understandable concepts. This section presents major research directions currently under consideration by researchers in the domain.

% \citet{geva2020transformer} posit that factual knowledge in the models is stored in the Feed Forward Layers of the model. They identify concepts associated with neurons in the fully connected layers by analyzing neurons' activation, extracting get top k input triggers that produce the highest activation in any neuron under analysis. \citep{voita2023neurons} a recent work discovers that in addition to high positive activation, the high negative activation is also useful in analysis because high positive activation adds a concept and high negative removes a concept. 

% Probe-based methods train an auxiliary neural network on the hidden space of the model. 


% \citep{marks2024sparse, ConmyMLHG23, he2024jailbreaklens, olah2020zoom, anthropic2023toy} works finds circuits in the network instead of finding individual neurons, that are responsible for any phenomenon in the network








\section{Conclusion}

In this work, we challenged traditional assumptions about neuron interpretability by reframing polysemanticity as a resource rather than a limitation in interpreting neurons. Through an in-depth analysis of encoder- and decoder-based large language models across multiple text classification datasets, we uncovered that neuronal activations for individual concepts exhibit distinct, Gaussian-like distributions. This discovery allows for a more precise understanding of how neurons encode multiple concepts, enabling us to move beyond coarse, monolithic neuron-to-concept mappings.
Building upon these insights, we proposed \ourframework, a novel range-based framework for neuron interpretation and manipulation. \ourframework{} offers fine-grained control that reduces interference with unrelated concepts by attributing specific activation ranges within neurons to individual concepts. Extensive empirical evaluations demonstrated that $\mathsf{NeuronLens}$  outperforms existing methods in maintaining concept-specific precision while minimizing unintended side effects. 
The hyperparameter $\tau$ was set to ensure fair comparison across methods. Future work could explore mathematically deriving optimal $\tau$ values for individual neurons. 
Additionally, our work 
%this work 
proved 
that the fundamental unit of interpretation can be a range within the activation space, which can be 
%used 
exploited
for better interpretability. % in future work.

\balance
\newpage

\section{Impact Statement}
This work advances neural network interpretability by providing a fine-grained understanding of concept encoding in language models. The proposed $\mathsf{NeuronLens}$ framework enables 
%more 
precise control of model behavior, benefiting research in model safety and reliability. While this improved understanding could potentially be misused, the work's theoretical nature and focus on interpretability methods makes immediate harmful applications unlikely. %We encourage the research community to develop appropriate guidelines for applying these techniques in practice, particularly for high-stakes applications. 
%Our methodology promotes transparency in AI systems while providing tools for better model control and safety assessment.

% \section{Sensitive Activations}
% Neuron activation analysis focuses on analyzing the magnitude of neuron activation across sequences of inputs for a given concept $c$ using the methodology described in Section \S \ref{sec:activation_analysis}. 
% These approaches analyze the extremes of the magnitude of neuron activation (i.e., max in positive, negative, or absolute). We hypothesize that neuron activations encode additional inferential patterns beyond magnitude, specifically \textbf{sensitive activations}. We define sensitive activations as a pattern of neuronal activation outputs that exhibit significant variation across samples for a given concept $c$. To test this hypothesis we utilize median absolute deviation (MAD), a robust statistical measure resistant to outliers compared to standard deviation and variance:
% \[
% \text{MAD} = \text{median}\left( \lvert X_i - \text{median}(X) \rvert \right)
% \]
% Our analysis reveals a novel finding that neurons with high positive and negative activations (i.e. salient neurons for the discerning model's output information) also exhibit sensitivity in the activation pattern. To analyze this pattern we visualize MAD scores with maximally activating neurons using the mean of neuronal activations shown in Figure \ref{fig:nueron_activation_std}. The Figure shows a positive trend with maximally activating neurons for GPT-2 trained on Emotions(cite), the activation pattern for GPT2 (cite), the pattern for BERT and Llama along with a pattern for the other 2 classes is shown in Appendix \S\ref{}. 

% \begin{figure*}[!t]
%     \centering
%         \includegraphics[width=\textwidth]{class_statistics.pdf}
%         \caption{Neuron Ranges for x classes}
%         \label{fig:nueron_activation_std}
%         \vspace{-0.4cm}
% \end{figure*}
% \begin{figure}[!t]
%     \centering
%         \includegraphics[width=0.5\textwidth]{common_zeros_plot.pdf}
%         \caption{Percentage overlap of MAD ranking and Max ranking. \textit{Emotions} Dataset.}
%         \label{fig:percentage_overlap}
%         % \vspace{-0.4cm}
% \end{figure}


% % \begin{table*}[t]
% %     \centering
% %     \scriptsize  % Reduce font size
% %     % \setlength{\tabcolsep}{}  % Reduce column spacing
% %     \begin{tabular}{l|cccc|cccc|cccc}  % @{} removes padding at edges
% %     \toprule
% %     & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{MAD} & \multicolumn{4}{c}{Maximum} \\
% %     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% %     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
% %     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
% %     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% %     \midrule
% % Class 0 & .969 & .956 & .913 & .903 & .688 & .662 & .853 & .845 & .274 & .205 & .788 & .779 \\
% % Class 1 & .939 & .938 & .925 & .908 & .994 & .994 & .595 & .582 & .059 & .057 & .906 & .899 \\
% % Class 2 & .902 & .872 & .932 & .923 & .752 & .745 & .715 & .709 & .126 & .136 & .902 & .890 \\
% % Class 3 & .910 & .905 & .932 & .921 & .912 & .906 & .903 & .894 & .197 & .191 & .926 & .914 \\
% % Class 4 & .869 & .854 & .938 & .927 & .549 & .516 & .869 & .863 & .115 & .101 & .697 & .680 \\
% % Class 5 & .857 & .798 & .932 & .923 & .167 & .193 & .885 & .876 & .270 & .197 & .631 & .615 \\
% % \midrule
% % & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{$\text{MAD} \cap \text{Maximum}$} & \multicolumn{4}{c}{Maximum - $\text{MAD}$} \\
% % \midrule

% % Class 0 & .969 & .956 & .913 & .903 & .678 & .653 & .845 & .839 & .983 & .914 & .847 & .793 \\
% % Class 1 & .939 & .938 & .925 & .908 & .878 & .873 & .928 & .923 & .941 & .925 & .910 & .882 \\
% % Class 2 & .902 & .872 & .932 & .923 & .790 & .786 & .762 & .758 & .703 & .685 & .902 & .827 \\
% % Class 3 & .910 & .905 & .932 & .921 & .470 & .467 & .936 & .928 & .891 & .895 & .930 & .901 \\
% % Class 4 & .869 & .854 & .938 & .927 & .747 & .704 & .902 & .896 & .851 & .851 & .925 & .887 \\
% % Class 5 & .857 & .798 & .932 & .923 & .254 & .209 & .904 & .893 & .960 & .847 & .917 & .878 \\

% %     \bottomrule
% %     \end{tabular}
% %     \caption{Max vs Mad High on GPT 2 model on \textit{Emotions} dataset}
% %     \label{tab:classification_results_gpt_mad_max_emotions}
% % \end{table*}



% \begin{table*}[t]
%     \centering
%     \scriptsize  % Reduce font size
%     % \setlength{\tabcolsep}{}  % Reduce column spacing
%     \begin{tabular}{l|cccc|cccc|cccc}  % @{} removes padding at edges
%     \toprule
%     & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Maximum} & \multicolumn{4}{c}{Maximum - MAD} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .969 & .956 & .913 & .903 & .274 & .205 & .788 & .779 & .983 & .914 & .847 & .793 \\
% Class 1 & .939 & .938 & .925 & .908 & .059 & .057 & .906 & .899 & .941 & .925 & .910 & .882\\
% Class 2 & .902 & .872 & .932 & .923 & .126 & .136 & .902 & .890 & .703 & .685 & .902 & .827 \\
% Class 3 & .910 & .905 & .932 & .921 & .197 & .191 & .926 & .914 & .891 & .895 & .930 & .901 \\
% Class 4 & .869 & .854 & .938 & .927 & .115 & .101 & .697 & .680 & .851 & .851 & .925 & .887 \\
% Class 5 & .857 & .798 & .932 & .923 & .270 & .197 & .631 & .615 & .960 & .847 & .917 & .878\\

%     \bottomrule
%     \end{tabular}
%     \caption{Max vs Mad High on GPT 2 model on \textit{Emotions} dataset}
%     \label{tab:classification_results_gpt_mad_max_emotions}
% \end{table*}




% \hammad{move to appendix: Analysing the graph we can see that most neurons have a MAD of 5 while several neurons have a high MAD. We hypothesize that neurons that have more sensitive activations are also salient.}

% % Discuss the results

% %  paragraph on max vs intersection.
 
 
% %  We follow the same approach of extracting model activations using the aforementioned process and perform 
% %  median absolute deviation 
% % serving as an inherent measure of the activation spread for each neuron. Our analysis reveals a novel finding that neurons that have high positive and negative activations (i.e. salient neurons for the discerning model's output information) also exhibit consistency in the activation pattern. 


% \textcolor{blue}{here we have to show that the bounded range in max neurons is somewhat lower this is not a hard check. hammad visualization figure x}.

% \textbf{\hammad{layers experiment to show that performing zeroing out on earlier layers does not work well. point to discuss.}}

% \subsection{Experimental Setup}
% \label{sec:experimental_setup_sensitivity}
% To test the hypothesis we rank neurons based on MAD, maximal activation, linear probe weights and probeless as described in Section \S\ref{sec:activation_analysis}, for each concept $c$ in a given dataset. We then extract x percentage of neurons from the ranking sets.  We perform experimentation in two settings: 1) We check for the importance of neurons extracted in the aforementioned approaches and 2) We experiment with utilizing the intersection of neurons extracted (write in math max intersection sensitivity, probe intersection max, sensitivity intersection probe) that there  overlap in the.




% % (max intersection sensitive, sentsitive vs max, non intersection max and sensitive)

% \subsection{Metrics}

% % To test the goodness of rankings and find how much each neuron is actually causally affecting the models performance on concept $c$, we take the base accuracy and confidence of the concept $c$ on the base model. We also note the model's performance on all other concepts $c'$. We then mask the selected neurons for concept $c$ by zeroing out their activations. We then note the accuracy and confidence of the masked model on $c$ and $c'$. The change in accuracy and confidence on $c$ tells us the effectiveness of the selected neurons. The change in accuracy and confidence on $c'$ tell us the localization and preservation of the knowledge in the model when neurons for $c$ are masked. From both these matrics of drop in accuracy and confidence on $c$ and $c'$ we can judge the effectiveness and precision of the neuron discovery. In all the tables Acc and conf are the concept $c$ accuracy and confidence respectively. CAcc and CConf are the accuracy and confidence of $c'$.



% To evaluate the causal relationship between identified neurons and model performance on concept $c$, we employed a systematic masking approach. For performance evaluation, we employ two quantitative metrics: prediction accuracy and the model's confidence score for the ground truth. Initially, we established baseline measurements of both accuracy and confidence for all concepts $C$ in the unmodified model.
% % , while simultaneously recording performance metrics across all other concepts ($c'$). 
% Subsequently, we conducted targeted neuronal intervention by setting the activations of the selected salient neurons associated with concept $c$ to zero.
% Post-intervention measurements are recorded for both the target concept $c$ and auxiliary concepts $c'$. The disparity in accuracy and confidence scores for concept $c$ between baseline and masked conditions serves as a quantitative measure of the identified neurons contribution. Concurrently, performance variations across auxiliary concepts $c'$ provide insights into both knowledge localization and preservation within the neural network.

% The effectiveness and precision of neuron identification can be assessed through two key metrics: (1) the magnitude of performance degradation for concept $c$, and (2) the extent of unintended impact on auxiliary concepts $c'$. Throughout our analysis, we denote the accuracy and confidence metrics for concept $c$ as \textbf{Acc} and \textbf{Conf} respectively, while corresponding measurements for auxiliary concepts $c'$ are represented as \textbf{CAcc} and \textbf{CConf}.






% % \begin{table*}[t]
% %     \centering
% %     \footnotesize  % Reduce font size
% %     % \setlength{\tabcolsep}{}  % Reduce column spacing
% %     \begin{tabular}{l|cccc|cccc|cccc}  % @{} removes padding at edges
% %     \toprule
% %     & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Top Mad} & \multicolumn{4}{c}{Maximum} \\
% %     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% %     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
% %     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
% %     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
% %     \midrule
% % Class 0 & .969 & .956 & .913 & .903 & .678 & .653 & .845 & .839 & .983 & .914 & .847 & .793 \\
% % Class 1 & .939 & .938 & .925 & .908 & .878 & .873 & .928 & .923 & .941 & .925 & .910 & .882 \\
% % Class 2 & .902 & .872 & .932 & .923 & .790 & .786 & .762 & .758 & .703 & .685 & .902 & .827 \\
% % Class 3 & .910 & .905 & .932 & .921 & .470 & .467 & .936 & .928 & .891 & .895 & .930 & .901 \\
% % Class 4 & .869 & .854 & .938 & .927 & .747 & .704 & .902 & .896 & .851 & .851 & .925 & .887 \\
% % Class 5 & .857 & .798 & .932 & .923 & .254 & .209 & .904 & .893 & .960 & .847 & .917 & .878 \\

% %     \bottomrule
% %     \end{tabular}
% %     \caption{}
% %     \label{tab:classification_results_gpt_mad_max_emotions}
% % \end{table*}


% \subsection{Results}
% Using the experimental setup outlined in Section \S\ref{sec:experimental_setup_sensitivity}, we compare the causal role of neurons for prediction corruption. The results for the experiments are shown in Table \ref{tab:classification_results_gpt_mad_max_emotions}.

% For \textbf{sensitive vs. max}, we observe that maximal activation neurons exhibit a higher rate of class corruption compared to sensitivity; however, they also lead to greater corruption of other concepts. This suggests that sensitivity, as a pattern of salient neurons, shows a weaker correlation with the information utilized for prediction compared to max. Building on the observed overlap between max and sensitive neurons, as shown in Figure \ref{fig:percentage_overlap}, we investigate the effectiveness of neurons in the intersection of max and sensitive compared to those exclusive to max. We find that removing sensitive neurons leads to very low corruption in the 

    

% \textbf{Intersection} We observe that salient neurons(max neurons) overlap more than random chance, with the sensitive neurons. We show the results of the overlap in (table x). This shows that salient neurons also have the property of being highly sensitive to the concept $c$ input sequences.



% \textbf{non intersection max and sensitive}
% To test how important sensitive neurons are to the max set, we take the intersection of neurons from both the rankings and then compare the performance degradation of this intersection set and compare it with the max ranking omitting the intersection neurons. The results of this experiment are shown in (table x) here we can see that intersection neurons outperform max neurons, indicating that sensitive neurons within the max neuron set are more important.


% %NEW








% \newpage
% \clearpage











% For normalization, let $X \in R^{n \times d}$ be our activation matrix where:
 % and their ability to elucidate information about concepts $C$
% $n = |S|$ is the number of input sequences
% $d$ is the hidden dimension size
% Each row $X[i,:]$ is the hidden state vector for sequence $i$
% Each column $X[:,j]$ represents the activations of neuron $j$ across all sequences

% We apply Min-Max normalization per neuron (column-wise):
% \[X_{normalized}[i,j] = \frac{X[i,j] - \min_{i}(X[:,j])}{\max_{i}(X[:,j]) - \min_{i}(X[:,j])}\]

% This normalization ensures the activations for each neuron are scaled to [0,1], allowing for fair comparison of standard deviations across different neurons regardless of their original scales.

% behave in a similar pattern for all the input sequences for given $c$

% In literature, activation analysis is usually done by analyzing the magnitude of the neuron activations on the concept of interest c(cite). These approaches analyze the extremes of the magnitude of neuron activation on c(i.e., max in positive, negative, or absolute). We posit that there is more in the activation pattern than absolute magnitude. To test this we used the spread of the neuron activation as a ranking metric. Concretely we take the std of neuron activations on all the input sequences s in c.  


% normalize->std $(90\%)$->ranking 


% We find a high overlap between the std neurons and max neurons. This proves that salient neurons(max neurons) also exhibit this consistency behaviour for concept c.






% Using our original activation matrix $X \in R^{n \times d}$, we compute the standard deviation $\sigma \in R^d$ and mean $\mu \in R^d$ for each neuron (column-wise):
% $\mu[j] = \frac{1}{n}\sum_{i=1}^n X[i,j]$
% $\sigma[j] = \sqrt{\frac{1}{n}\sum_{i=1}^n (X[i,j] - \mu[j])^2}$
% For a threshold $\tau$, we apply a conditional mask to each activation value:

% \[f(X[i,j]) = \begin{cases}
% 0 & \text{if } \mu[j] - \tau\sigma[j] \leq X[i,j] \leq \mu[j] + \tau\sigma[j] \\[2pt]

% X[i,j] & \text{otherwise}
% \end{cases}\]
% This masks out (sets to zero) any activation values that fall within $\tau$ standard deviations of the mean for each neuron.


% \section{Experimental Setup}

% We use 4 Models, Distilbert, Bert, GPT2, Llama-3.2-3B. We used 7 datasets, SST2, IMDB, AG News, Emotions, DBPedia 14, Banking-77, Clinic-150. We use 3090 GPU for experimentation.

% \section{Results}

% \subsection{overlap} 
% % $max&std > randon\&max$ 
% \subsection{Max without std vs max without random} max-50\%std > max-50\%random
% \subsection{Nonmax neurons std} spread of max neurons vs non max neurons



% \section{Ranges}

% From the x above
% \hammad{
% Sections:
% Knowledge localization:
% 1) Consistency
% 2) Ranges}
% \section{Related Work}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{TODO:}

% Introduction:
% Motivate the use case and intro. Explain why other work is not optimal or restrictive. Explain what we do and how it improves the other work. 

% Problem Setup:



% Approach:
% Detail in computation
% \textbf{Abalation} to show lower layers are not optimal for such operation. Prove Max and Consistency.

% We find the value we want to generalize. Motivate the using std. we find that this 

% We find that a minimum of x neurons are able to achieve high performance. \textbf{max vs consistency (number less) you can claim our approach is not intrusive}. pattern in model size variation.

% max they are also consistent at 60\%. For some tasks, consistency performs on par with max discussion in ablation

% consistency (discovery,usecase) appendix.

% experimental setup 
% tasks definition outcomes expected blah blah.
% We use x y z details in Appendix

% for model x we use y layer etc for all experiments.

% Central table for experiments.

% explanation of experiments max 3 paragraph or 3/4 of one page.

% Abaltions




% How many 50 10 ablation performance based on number of neurons.



% \section{Methodology}

% \textbf{Problem Definition}:
% As to why: Deep Learning models are treated as black boxes, we want to understand how the internal components of the DL network works, By understanding this we can have a lot of applications, from safety to editing to interpretability itself.

% As to what: 

% As to how:
% For any concept C, we want to get the activation field A responsible for that concept. The literature picks neurons N, which represent the activation field associated with C. 



% ranges:
% We conditionally mask the range based on 3 STD around the mean



% \subsection{Recording The activations for analysis}
% Let $h^l(x) \in R^d$ represent the hidden state activation vector in layer $l$ for input sequence $x$, where $d$ is the hidden dimension size. For a concept $C$ with a set of input sequences $S = {x_1, ..., x_n}$ we extract model activation vectors from models based on their sentence-level representations tokens. For BERT-based models:
% $A_C = {h^l_{CLS}(x) \in R^d | x \in S}$, for GPT-based models:
% $A_C = {h^l_{last}(x) \in R^d | x \in S}$
% where $A_C$ is the set of hidden state vectors for concept $C$, and each vector contains the activations for all neurons in that layer.
% \subsection{Max}
% Given the activation set $A_C$, we first take the absolute value of each element in the hidden states, then compute the mean activation vector $\mu_C \in R^d$ across all sequences:
% $\mu_C = \frac{1}{|S|} \sum_{x \in S} |h^l(x)|$
% where $|h^l(x)|$ denotes element-wise absolute value of the hidden state vector.
% For a given percentage threshold $p$, we select the top $k = \lceil p \cdot d \rceil$ dimensions (neurons) from $\mu_C$:
% $TopNeurons_C = {i | \mu_C[i] \text{ is among the top } k \text{ values in } \mu_C}$
% This ensures we capture neurons that have high absolute activation values, regardless of whether they are strongly positive or strongly negative in response to the concept $C$. \textbf{(Cite top negative meta work)}
% \subsection{Consistency Neurons}
% For normalization, let $X \in R^{n \times d}$ be our activation matrix where:

% $n = |S|$ is the number of input sequences
% $d$ is the hidden dimension size
% Each row $X[i,:]$ is the hidden state vector for sequence $i$
% Each column $X[:,j]$ represents the activations of neuron $j$ across all sequences

% We apply Min-Max normalization per neuron (column-wise):
% \[X_{normalized}[i,j] = \frac{X[i,j] - \min_{i}(X[:,j])}{\max_{i}(X[:,j]) - \min_{i}(X[:,j])}\]

% This normalization ensures the activations for each neuron are scaled to [0,1], allowing for fair comparison of standard deviations across different neurons regardless of their original scales.

% \subsection{Ranges}
% Using our original activation matrix $X \in R^{n \times d}$, we compute the standard deviation $\sigma \in R^d$ and mean $\mu \in R^d$ for each neuron (column-wise):
% $\mu[j] = \frac{1}{n}\sum_{i=1}^n X[i,j]$
% $\sigma[j] = \sqrt{\frac{1}{n}\sum_{i=1}^n (X[i,j] - \mu[j])^2}$
% For a threshold $\tau$, we apply a conditional mask to each activation value:

% \[f(X[i,j]) = \begin{cases}
% 0 & \text{if } \mu[j] - \tau\sigma[j] \leq X[i,j] \leq \mu[j] + \tau\sigma[j] \\[2pt]

% X[i,j] & \text{otherwise}
% \end{cases}\]
% This masks out (sets to zero) any activation values that fall within $\tau$ standard deviations of the mean for each neuron.



% \section{Findings}

% \subsection{Consistent Max Neurons}
% The intersection of max neurons and std neurons is higher the random chance

% \subsection{Ranges perform}

% Ranges perform much better than masking the whole neuron across the board.


% \section{Abilliations}

% \subsection{Percent of neurons}

% Percent has no significant affect on the general trends of approach outcomes
% \subsection{Consistency neurons}
% Consistency neurons does not perform same as max neurons but at the same time they do not affect other classes as much 
% \subsection{Max without consistency}

% max without consistency does not perform as good as max without random neurons
% \subsection{Intersection neurons}
% intersection perform good on some datasets and models and does not perform as good for others
% \subsection{100 \% ranges}
% for trained models 100 percent neurons can be used for masking the ranges, does not work in the llama 
% \subsection{Tao}

% we select tao = 2.5 as it covers ~95\% of the activation spectrum
% \subsection{Layers}

% we perform our whole approach at the last later's output because that layer has the maximum info about the class to predict 



% \subsection{Recording The activations for analysis}

% Record all the activations for any particular concept C. In the case of Bert-based models, record the activation of the CLS token, In the case of GPT-based models, record the last token output when the class token is being predicted.


% \subsection{Max}

% In the case of Max we take the mean $\mu$ of the activations across all the input sequences $S$ for concept $C$. We then select the top X percent of neurons from the mean set. This gives us the neurons that are highly activating on the concept $C$ across examples $S$.

% \subsection{Consistency Neurons}

% For computing standard deviation and comparing individual neurons STD amongst each other we first normalize the recorded activation using Min-Max normalization
% In the case of consistency neurons, we take the standard deviation $\mu$ of the activations(we choose STD because it returns the values in the same units as the original values). We then normalize the STD $\mu$ values by using min and max of the values so that the neurons can be compared with each other. After taking the normalized values we take the ranking of the neurons based on the spread of the activation.

% \subsection{Ranges}
% In this approach we take the standard deviation $\sigma$ and mean $\mu$ of the recorded activations, we then conditionally mask $\tau$ standard deviation $\sigma$ around the mean for the selected neurons


% \[
% f(x) = \begin{cases} 
% 0 & \text{if } \mu - \tau\sigma \leq x \leq \mu + \tau\sigma \\[2pt]
% x & \text{otherwise}
% \end{cases}
% \]


% \section{Experimentation}

% \subsection{Results}

%\subsection{References}


\nocite{langley00}

\bibliography{main_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\include{appendix}



% \include{appendix}
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.



