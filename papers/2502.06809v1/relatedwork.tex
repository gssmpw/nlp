\section{Related Work}
While we have discussed closely related approaches in Section~\ref{prelim}, here, we briefly review additional relevant techniques.
Circuit discovery identifies groups of neurons that jointly encode concepts, providing a structured view of model behavior~\citep{marks2024sparse, ConmyMLHG23, olah2020zoom}. However, extracting circuits is computationally intensive and lacks fine-grained neuron-level attribution.
Gradient-based methods attribute predictions to input features by tracking gradients through the network, with integrated gradients~\citep{sundararajan2017axiomaticattributiondeepnetworks,dai2021knowledge} being a widely used approach. However, they struggle with polysemanticity, as they do not disentangle overlapping concepts within neurons.
Causal analysis methods intervene on internal components to assess their role in encoding concepts. Causal tracing measures the effect of corrupting activations on model performance~\citep{vig2020investigating, meng2022locating}, while causal mediation analysis quantifies information propagation through neurons~\citep{vig2020investigating}. Although effective, these techniques require costly perturbation experiments.
Beyond neuron-level analysis, representation-level methods examine hidden states and their relationship to model outputs. Logit lens~\citep{logitlens} and tuned lens~\citep{belrose2023elicitinglatentpredictionstransformers} project intermediate representations into the modelâ€™s vocabulary space to track information flow, while sparse probing~\citep{gurnee2023findingneuronshaystackcase} compresses hidden representations into sparse, interpretable subspaces.
While prior work has advanced interpretability, most methods rely on discrete neuron-to-concept mappings, which fail to account for polysemanticity. Our work extends activation-based approaches by introducing activation ranges as the fundamental unit of interpretability to enable more precise concept attribution and intervention.

%OLD
% \fix{Need to know the detail of RW needed. We already discuss RW wherever we are using it in theh
% sections}
% Machinistic Interpretability(MI) is an emerging field concerned with reverse engineering models' underlying feature mappings for human-understandable concepts. This section presents major research directions currently under consideration by researchers in the domain.

% \citet{geva2020transformer} posit that factual knowledge in the models is stored in the Feed Forward Layers of the model. They identify concepts associated with neurons in the fully connected layers by analyzing neurons' activation, extracting get top k input triggers that produce the highest activation in any neuron under analysis. \citep{voita2023neurons} a recent work discovers that in addition to high positive activation, the high negative activation is also useful in analysis because high positive activation adds a concept and high negative removes a concept. 

% Probe-based methods train an auxiliary neural network on the hidden space of the model. 


% \citep{marks2024sparse, ConmyMLHG23, he2024jailbreaklens, olah2020zoom, anthropic2023toy} works finds circuits in the network instead of finding individual neurons, that are responsible for any phenomenon in the network