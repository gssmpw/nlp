%\section{RW}
% The rationale for using maximally activating neurons is that these neurons are highly influential in determining the model's response to user inputs. Recent analyses by \citep{geva2020transformer,geva2022transformer} demonstrate that neurons exhibiting maximal positive activation in transformer architecture's feedforward layers function as key-value memory stores. \citet{voita2023neurons}  expanded upon these findings, revealing that both strong positive and negative neuronal activations play crucial roles: while positive activations store information, strong negative activations serve to suppress previously processed semantic content from the representation. These highly activated neurons thus regulate information flow through selective enhancement and inhibition.


%-activation based, probe based, gradient based, circuits


% Our analysis is positioned in the activation analysis category of approaches. The fundamental constraint of activation analysis methods is the exclusive dependence on activation magnitude (whether positive, negative, or absolute) to establish comprehensive neuron-concept mappings inherently constrains \textit{the} analysis to \textit{peripheral} activation states thereby failing to capture the comprehensive distributional characteristics across the \textbf{full activation spectrum of neurons}. 
% \textbf{Representation Level} approaches investigate neural representations by examining neuronal outputs across multilayered model architectures. cite(https://arxiv.org/pdf/1608.04207) uses a classifier to classifier to predict information about sentences such as ordering, length, and word inclusion, the intuition behind the approach is to verify if such information can be elucidated from the model representations. (add more papers)

% logit analysis (cite https://www.lesswrong.com/posts/
% AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) enables comprehensive mapping of token-level representations from hidden layers to the model's vocabulary space. This analytical technique facilitates the elucidation of information propagation and transformation mechanisms throughout the neural network's structure for the response generation process. cite(Eliciting Latent Predictions from Transformers with the Tuned Lens) enhances the approach by adding trainable affine transformation to match the unembedding layer better.

% There are advanced versions of probing like sparse probing(https://arxiv.org/pdf/2305.01610) (compression my work??)
% % Methods like logit lense are applied to the whole hidden state of the network to understand what the models have learned on the representation level. 

% \textbf{Neuron Attribution} methods delve one step deeper and investigate the role of neurons in interpreting individual dimensions. The core idea is the ranking of neurons that elucidate various linguistic concepts.  use For example, cite ((https://arxiv.org/pdf/1812.09355) train linear \textbf{probes} to find salient neurons that correspond to various linguistic concepts, to demonstrate significant correlations between neuron activations and conceptual encoding they utilize concept erasure to remove key concepts which would have a direct impact on machine translation. Using probes for salient neuron extraction has certain limitations concerning sub-optimal ranking as the quality of the classifier learned varies and limited guarantees that the representation being analyzed is utilized by the model's task at hand. cite(https://arxiv.org/pdf/2110.07483) provides an alternative approach that uses the mean of token representations that correspond to certain concepts, after extraction of means the representations are further processed by removing taking the element-wise difference between them and summing for a single representation. This provides a more stable ranking improving upon linear and Gaussian-based methods.  


% Attribution
% Next granularity and then the whole representation are introduced by this paper here instead of the whole representation of the token they interpret individual dimensions(called neurons). There are many methods in the literature to analyze the neurons associated with concepts to get the ranking of the neuron's importance. First is Probing

% \subsubsection{Probing}

% In probing you train a separate binary model on the representation of the model (first paper: https://arxiv.org/pdf/1608.04207, Dr. Hassan probe https://arxiv.org/pdf/1812.09355) the magnitude of weights of the trained probe point to the importance of neurons in the hidden dimension for the given concept c. 




% \subsection{Attribution Methods}

% There are several methods to take the ranking of neuron units in the hidden dimension 1) mean sensitivity(probless: https://arxiv.org/pdf/2110.07483) takes the sensitivity of the mean as a metric of neuron attribution) 2) Gradient based methods(Intigrated gradients: https://arxiv.org/pdf/1703.01365)


% \subsection{Causal Tracing??}

% In causal tracing, the salient components of the model for the given concept c are discovered by corrupting different components of the model and observing the effect of corruption on the networks performance. The degree to which corruption effect the models performance points to the saliency of the component under observation.

% https://arxiv.org/pdf/2106.02997, causal mediation analysis: https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf



% \subsection{Maximal activation associating method}

% Maximally activating neurons are associated with saliency through out literature(Lottery ticket compression, magnitude based compression)

% In the transformers literature (mor geva 1, mor geve 2, https://arxiv.org/pdf/2304.12918) find the input triggers that produce highest activation in the neuron under analysis. 

% We use the inverse of this approach where we find salient neurons associated to a concept by taking max in the mean activation on concept c.

% feed-forward layers
\section{Limitations}
While \ourframework{} can disentangle polysemanticity to a degree using \textbf{Gaussian Like Distribution}, it is unable to completely disentangle concepts encoded in the polysemantic neurons, because there still is a significant overlap in the distributions of concepts in activations. Additionally, in this work, we use $\tau$ to be a fixed value of 2.5 to make the comparison of approaches fair, but $\tau$ selection can be optimized to be more sophisticated. We also get results primarily from the penultimate layer, and not the intermediate or earlier layers, however, we do give ablation and rationale for this choice in Appendix~\ref{sec:layer_abiliation}


\section{Training Details}
\label{sec_experimental_appendix}
% The experiments focus on evaluating knowledge removal in both pretrained large language models (LLMs) and LLMs fine-tuned for downstream tasks. The goal of this process is to remove specific knowledge related to a concept $c$, thereby reducing the model's predictability for that concept while maintaining its performance on all other concepts within a given task.

For BERT, DistilBERT, and Llama, we utilize pretrained models. Since BERT, and DistilBert are not inherently trained as a conversational agent, we use top-performing fine-tuned models from the Hugging Face repository. For the Llama model, few-shot prompt completion is employed to predict class labels. This involves providing a small number of training samples from the dataset to guide the model’s predictions.

For GPT-2, we fine-tune the pretrained model across all datasets for three epochs. The input sequence is constructed by concatenating the text with a \textless{}sep\textgreater{} token, followed by the class label, and ending with an \textless{}eos\textgreater{} token. During training, the loss is back-propagated only for the class label token, while all other tokens are assigned a skip label (-100). Additionally, all class labels are added to the model’s dictionary as special single-token entries.

\textbf{Dataset Preprocessing for Llama}
For Llama we process whole datasets in few shout settings and only curate 2000 samples per class, where the model prediction was correct.
% \onecolumn

\section{Saliency details}
\label{sec:sec_sailiency_details}

\textbf{Max activations.} \citet{frankle2019lotterytickethypothesisfinding} extract high neural activations as a saliency ranking metric relying upon the rationale that maximally activating neurons are salient as these neurons play a critical role in controlling the model's output, highlighting their importance for a concept $c$.To identify them, the column-wise mean of absolute neuronal activations in $H^l_c$, $H^l_c$ is defined in Section\ref{sec:sailency}, is computed, given that high negative activations also carry significant signals \citep{voita2023neurons}. The magnitude of the means is then considered as a ranking for concept $c$.

\textbf{Probe analysis.} \citet{grain:aaai19-1} train a linear classifier on the hidden representations \( H^l_c \) to distinguish between concepts. The learned model weights are then utilized as a saliency ranking. This process involves learning a weight matrix \( W \in \mathbb{R}^{d \times |c|} \), where \( d \) is the hidden dimension and \( |c| \) is the number of concept classes. The absolute weight values of each row in the weight matrix are used as a ranking for the importance of each neuron for a given concept. To prevent the emergence of redundant solutions characterized by minimal variations in the weights, the probe is trained using the elastic regularization technique.

 \textbf{Probeless.} \citet{AntvergB22} examine individual neurons, without the need for auxiliary classifiers, using the element-wise difference between mean vectors. The element-wise difference between mean vectors is computed as $r = \sum_{c,c'\in C} |q(c) - q(c')|$, where $r \in \mathbb{R}^d$ and $d$ is the hidden dimension. The final neuron saliency ranking is obtained by sorting $r$ in descending order.


 % In Table \ref{tab:performance_drops_probeless_Probe_Max} we compare all of the aforementioned ranking techniques. We show the results of the Emotions classification dataset on the GPT-2 model. The table represents the drop in performance metrics from the baseline unaltered model. From the table, we observe that ranking based on maximally activating neurons outperforms other approaches in terms of producing a higher drop in targeted concept performance while keeping comparable deterioration in maintaining a comparable drop in other concepts. For the rest of this study, we use max activation ranking as the ranking for saliency.



% \newpage
\section{Full Results}
Here we provide the complete results for the datasets shown in Table~\ref{tab:performance_drops_all}. In Table~\ref{tab:imdb_on_all} we provide results on \textit{IMDB} dataset on all selected models. In Table~\ref{tab:sst_on_all} we provide results on \textit{SST2} dataset on all selected models. In Table~\ref{tab:emotions_on_all} we provide results on \textit{Emotions} dataset on all selected models. In Table~\ref{tab:db_14_on_all} we provide results on \textit{DBPedia-14} dataset on all selected models.

\begin{table*}[t]
\caption{Evaluation of selected models on the \textit{IMDB} dataset using neuron and range masking techniques. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{2}{*}{BERT} 
&Class 0 & 0.930 & 0.908 & 0.926 & 0.901 & -0.169 & -0.352 & 0.061 & -0.066 & -0.163 & -0.359 & 0.059 & 0.035 \\
&Class 1 & 0.926 & 0.901 & 0.930 & 0.908 & -0.211 & -0.355 & 0.057 & -0.091 & -0.206 & -0.361 & 0.056 & 0.025 \\
\midrule
\multirow{2}{*}{GPT-2} 
&Class 0 & 0.965 & 0.941 & 0.940 & 0.922 & -0.935 & -0.922 & 0.050 & 0.057 & -0.905 & -0.901 & 0.055 & 0.046 \\
&Class 1 & 0.940 & 0.922 & 0.965 & 0.941 & -0.620 & -0.667 & 0.005 & 0.018 & -0.610 & -0.657 & 0.015 & 0.027 \\
\midrule
\multirow{2}{*}{Llama-3.2-3B} 
&Class 0 & 1.000 & 0.619 & 1.000 & 0.500 & -0.643 & -0.448 & -0.515 & -0.287 & -0.640 & -0.446 & -0.502 & -0.278 \\
&Class 1 & 1.000 & 0.500 & 1.000 & 0.619 & -0.877 & -0.410 & -0.273 & -0.304 & -0.873 & -0.409 & -0.265 & -0.303 \\

\bottomrule
\end{tabular}

\label{tab:imdb_on_all}
\end{table*}

\begin{table*}[t]
\caption{Evaluation of selected models on the \textit{SST2} dataset using neuron and range masking techniques. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{2}{*}{BERT} 
&Class 0 & 0.890 & 0.882 & 0.930 & 0.925 & -0.058 & -0.308 & 0.029 & -0.047 & -0.075 & -0.329 & 0.031 & 0.036 \\
&Class 1 & 0.930 & 0.925 & 0.890 & 0.882 & -0.043 & -0.318 & 0.033 & -0.045 & -0.045 & -0.330 & 0.030 & 0.050 \\
\midrule
\multirow{2}{*}{GPT-2} 
&Class 0 & 0.950 & 0.937 & 0.981 & 0.978 & -0.142 & -0.158 & 0.010 & 0.012 & -0.142 & -0.167 & 0.009 & 0.010 \\
&Class 1 & 0.981 & 0.978 & 0.950 & 0.937 & -0.187 & -0.223 & 0.041 & 0.053 & -0.176 & -0.216 & 0.041 & 0.046 \\
\midrule
\multirow{2}{*}{Llama-3.2-3B} 
&Class 0 & 1.000 & 0.620 & 1.000 & 0.690 & -0.532 & -0.459 & -0.420 & -0.424 & -0.532 & -0.456 & -0.404 & -0.415 \\
&Class 1 & 1.000 & 0.690 & 1.000 & 0.620 & -0.289 & -0.379 & -0.326 & -0.315 & -0.284 & -0.376 & -0.306 & -0.301 \\

\bottomrule
\end{tabular}

\label{tab:sst_on_all}
\end{table*}

\begin{table*}[t]
\caption{Evaluation of selected models on the \textit{Emotions} dataset using neuron and range masking techniques. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{6}{*}{BERT} 
&Class 0 & 0.960 & 0.935 & 0.901 & 0.851 & -0.241 & -0.718 & 0.013 & -0.266 & -0.222 & -0.718 & 0.012 & -0.055 \\
&Class 1 & 0.942 & 0.904 & 0.905 & 0.861 & -0.223 & -0.691 & 0.028 & -0.254 & -0.213 & -0.692 & 0.032 & -0.064 \\
&Class 2 & 0.824 & 0.723 & 0.926 & 0.889 & -0.371 & -0.533 & 0.016 & -0.284 & -0.352 & -0.534 & 0.018 & -0.115 \\
&Class 3 & 0.927 & 0.873 & 0.916 & 0.876 & -0.247 & -0.664 & 0.010 & -0.256 & -0.240 & -0.667 & 0.012 & -0.057 \\
&Class 4 & 0.884 & 0.837 & 0.922 & 0.880 & -0.406 & -0.646 & 0.012 & -0.251 & -0.402 & -0.648 & 0.012 & -0.066 \\
&Class 5 & 0.591 & 0.566 & 0.929 & 0.886 & -0.303 & -0.392 & 0.004 & -0.299 & -0.303 & -0.397 & 0.005 & -0.090 \\
\midrule
\multirow{6}{*}{GPT-2} 
&Class 0 & 0.969 & 0.956 & 0.913 & 0.903 & -0.695 & -0.751 & -0.125 & -0.124 & -0.698 & -0.749 & -0.009 & -0.009 \\
&Class 1 & 0.939 & 0.938 & 0.925 & 0.908 & -0.879 & -0.882 & -0.019 & -0.009 & -0.879 & -0.880 & -0.016 & -0.015 \\
&Class 2 & 0.902 & 0.872 & 0.932 & 0.923 & -0.776 & -0.736 & -0.029 & -0.032 & -0.780 & -0.739 & -0.023 & -0.028 \\
&Class 3 & 0.910 & 0.905 & 0.932 & 0.921 & -0.713 & -0.714 & -0.006 & -0.007 & -0.715 & -0.716 & -0.002 & -0.001 \\
&Class 4 & 0.869 & 0.854 & 0.938 & 0.927 & -0.754 & -0.753 & -0.240 & -0.248 & -0.754 & -0.753 & -0.127 & -0.133 \\
&Class 5 & 0.857 & 0.798 & 0.932 & 0.923 & -0.587 & -0.601 & -0.301 & -0.308 & -0.587 & -0.601 & -0.280 & -0.289 \\
\midrule
\multirow{6}{*}{Llama-3.2-3B} 
&Class 0 & 0.950 & 0.550 & 0.782 & 0.455 & -0.950 & -0.547 & -0.655 & -0.408 & -0.945 & -0.547 & -0.571 & -0.378 \\
&Class 1 & 0.905 & 0.498 & 0.804 & 0.473 & -0.855 & -0.495 & -0.743 & -0.433 & -0.867 & -0.494 & -0.607 & -0.404 \\
&Class 2 & 0.785 & 0.421 & 0.827 & 0.483 & -0.785 & -0.420 & -0.771 & -0.454 & -0.785 & -0.420 & -0.658 & -0.436 \\
&Class 3 & 0.790 & 0.482 & 0.833 & 0.476 & -0.760 & -0.477 & -0.635 & -0.423 & -0.755 & -0.476 & -0.544 & -0.402 \\
&Class 4 & 0.780 & 0.487 & 0.829 & 0.476 & -0.780 & -0.486 & -0.534 & -0.365 & -0.780 & -0.486 & -0.444 & -0.324 \\
&Class 5 & 0.536 & 0.296 & 0.855 & 0.498 & -0.417 & -0.284 & -0.751 & -0.465 & -0.429 & -0.282 & -0.653 & -0.434 \\

\bottomrule
\end{tabular}

\label{tab:emotions_on_all}
\end{table*}


\begin{table*}[t]
\caption{Evaluation of selected models on the \textit{DBPedia-14} dataset using neuron and range masking techniques. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{14}{*}{BERT} 
&Class 0 & 0.972 & 0.966 & 0.992 & 0.991 & -0.082 & -0.702 & 0.001 & -0.014 & -0.076 & -0.698 & 0.001 & -0.000 \\
&Class 1 & 0.987 & 0.986 & 0.991 & 0.990 & -0.030 & -0.778 & 0.000 & -0.017 & -0.018 & -0.770 & 0.000 & -0.000 \\
&Class 2 & 0.987 & 0.985 & 0.991 & 0.990 & -0.239 & -0.814 & 0.001 & -0.018 & -0.217 & -0.806 & 0.001 & -0.000 \\
&Class 3 & 0.997 & 0.997 & 0.990 & 0.989 & -0.008 & -0.766 & 0.000 & -0.019 & -0.001 & -0.731 & 0.000 & -0.000 \\
&Class 4 & 0.984 & 0.983 & 0.991 & 0.990 & -0.058 & -0.777 & 0.001 & -0.018 & -0.032 & -0.761 & 0.000 & -0.000 \\
&Class 5 & 0.995 & 0.995 & 0.990 & 0.989 & -0.007 & -0.795 & 0.000 & -0.017 & -0.001 & -0.771 & 0.000 & -0.000 \\
&Class 6 & 0.975 & 0.974 & 0.992 & 0.991 & -0.121 & -0.807 & 0.000 & -0.015 & -0.112 & -0.803 & 0.000 & -0.001 \\
&Class 7 & 0.994 & 0.994 & 0.990 & 0.989 & -0.028 & -0.789 & 0.000 & -0.017 & -0.010 & -0.767 & 0.000 & -0.000 \\
&Class 8 & 1.000 & 1.000 & 0.990 & 0.989 & -0.001 & -0.808 & 0.000 & -0.022 & 0.000 & -0.772 & 0.000 & -0.000 \\
&Class 9 & 0.999 & 0.998 & 0.990 & 0.989 & -0.004 & -0.837 & 0.000 & -0.019 & -0.001 & -0.811 & 0.000 & -0.000 \\
&Class 10 & 0.994 & 0.993 & 0.990 & 0.989 & -0.025 & -0.846 & 0.000 & -0.016 & -0.005 & -0.831 & 0.000 & -0.000 \\
&Class 11 & 0.997 & 0.997 & 0.990 & 0.989 & -0.013 & -0.751 & 0.000 & -0.017 & -0.001 & -0.726 & 0.000 & -0.000 \\
&Class 12 & 0.990 & 0.990 & 0.990 & 0.989 & -0.018 & -0.772 & 0.000 & -0.017 & -0.005 & -0.755 & 0.000 & -0.000 \\
&Class 13 & 0.994 & 0.994 & 0.990 & 0.989 & -0.009 & -0.740 & 0.001 & -0.017 & -0.001 & -0.721 & 0.000 & -0.000 \\
\midrule
\multirow{14}{*}{GPT-2} 
&Class 0 & 0.985 & 0.977 & 0.990 & 0.989 & -0.860 & -0.877 & -0.133 & -0.136 & -0.850 & -0.869 & -0.002 & -0.017 \\
&Class 1 & 0.995 & 0.992 & 0.990 & 0.988 & -0.500 & -0.567 & -0.180 & -0.192 & -0.460 & -0.544 & -0.023 & -0.024 \\
&Class 2 & 0.985 & 0.980 & 0.990 & 0.989 & -0.890 & -0.904 & -0.189 & -0.213 & -0.880 & -0.902 & -0.004 & -0.010 \\
&Class 3 & 0.995 & 0.995 & 0.990 & 0.987 & -0.900 & -0.933 & -0.145 & -0.143 & -0.900 & -0.927 & -0.008 & -0.017 \\
&Class 4 & 0.970 & 0.969 & 0.992 & 0.989 & -0.715 & -0.773 & -0.224 & -0.260 & -0.695 & -0.750 & -0.042 & -0.062 \\
&Class 5 & 0.995 & 0.993 & 0.990 & 0.988 & -0.315 & -0.446 & -0.127 & -0.192 & -0.290 & -0.432 & -0.013 & -0.025 \\
&Class 6 & 0.965 & 0.964 & 0.992 & 0.990 & -0.925 & -0.932 & -0.052 & -0.062 & -0.910 & -0.928 & -0.006 & -0.007 \\
&Class 7 & 1.000 & 0.998 & 0.989 & 0.987 & -0.815 & -0.865 & -0.003 & -0.008 & -0.775 & -0.846 & -0.026 & -0.057 \\
&Class 8 & 1.000 & 1.000 & 0.989 & 0.987 & -0.995 & -0.990 & -0.148 & -0.188 & -0.900 & -0.932 & -0.026 & -0.055 \\
&Class 9 & 1.000 & 1.000 & 0.989 & 0.987 & -0.975 & -0.979 & -0.250 & -0.268 & -0.955 & -0.958 & -0.020 & -0.049 \\
&Class 10 & 0.995 & 0.993 & 0.990 & 0.988 & -0.595 & -0.685 & -0.045 & -0.053 & -0.590 & -0.675 & -0.005 & -0.011 \\
&Class 11 & 0.985 & 0.984 & 0.990 & 0.988 & -0.210 & -0.453 & -0.094 & -0.118 & -0.135 & -0.396 & -0.015 & -0.034 \\
&Class 12 & 0.990 & 0.988 & 0.990 & 0.988 & -0.930 & -0.938 & -0.293 & -0.309 & -0.855 & -0.880 & -0.013 & -0.029 \\
&Class 13 & 1.000 & 0.999 & 0.989 & 0.987 & -0.985 & -0.986 & -0.393 & -0.416 & -0.945 & -0.981 & -0.018 & -0.044 \\
\midrule
\multirow{14}{*}{Llama-3.2-3B} 
&Class 0 & 1.000 & 0.586 & 1.000 & 0.559 & -0.990 & -0.584 & -0.949 & -0.473 & -0.990 & -0.584 & -0.823 & -0.441 \\
&Class 1 & 1.000 & 0.533 & 1.000 & 0.563 & -1.000 & -0.528 & -0.870 & -0.446 & -0.970 & -0.528 & -0.706 & -0.371 \\
&Class 2 & 1.000 & 0.467 & 1.000 & 0.568 & -0.995 & -0.462 & -0.963 & -0.477 & -0.995 & -0.461 & -0.838 & -0.432 \\
&Class 3 & 1.000 & 0.460 & 1.000 & 0.569 & -0.995 & -0.459 & -0.981 & -0.486 & -0.995 & -0.459 & -0.815 & -0.420 \\
&Class 4 & 1.000 & 0.828 & 1.000 & 0.539 & -0.965 & -0.809 & -0.981 & -0.454 & -0.955 & -0.808 & -0.852 & -0.412 \\
&Class 5 & 1.000 & 0.349 & 1.000 & 0.568 & -1.000 & -0.348 & -0.882 & -0.429 & -0.989 & -0.347 & -0.585 & -0.346 \\
&Class 6 & 1.000 & 0.809 & 1.000 & 0.541 & -1.000 & -0.787 & -0.972 & -0.449 & -1.000 & -0.787 & -0.736 & -0.366 \\
&Class 7 & 1.000 & 0.599 & 1.000 & 0.558 & -0.855 & -0.588 & -0.918 & -0.410 & -0.860 & -0.586 & -0.489 & -0.274 \\
&Class 8 & 1.000 & 0.420 & 1.000 & 0.572 & -1.000 & -0.420 & -0.957 & -0.467 & -1.000 & -0.420 & -0.660 & -0.335 \\
&Class 9 & 1.000 & 0.527 & 1.000 & 0.563 & -1.000 & -0.524 & -0.842 & -0.435 & -0.995 & -0.523 & -0.552 & -0.320 \\
&Class 10 & 1.000 & 0.505 & 1.000 & 0.565 & -0.995 & -0.503 & -0.907 & -0.464 & -1.000 & -0.503 & -0.589 & -0.322 \\
&Class 11 & 1.000 & 0.505 & 1.000 & 0.565 & -0.975 & -0.501 & -0.862 & -0.416 & -0.970 & -0.501 & -0.579 & -0.313 \\
&Class 12 & 1.000 & 0.560 & 1.000 & 0.561 & -0.980 & -0.545 & -0.812 & -0.417 & -0.975 & -0.544 & -0.496 & -0.310 \\
&Class 13 & 1.000 & 0.587 & 1.000 & 0.559 & -0.990 & -0.584 & -0.722 & -0.406 & -0.985 & -0.584 & -0.588 & -0.337 \\

\bottomrule
\end{tabular}

\label{tab:db_14_on_all}
\end{table*}





\section{Layer Ablation}
\label{sec:layer_abiliation}

In Table~\ref{tab:layer_abiliation_emotions_1} and Table~\ref{tab:layer_abiliation_emotions_2} we provide results of applying both approaches on all layers of \textit{GPT-2} model on \textit{Emotions} dataset. From the results we can see that: Early layers (1-3) show highly variable and often severe impacts: Layer 1 exhibits minimal effects ($\Delta Acc = -0.113$, $\Delta CAcc = -0.064$), while Layers 2-3 show extreme degradation ($\Delta Acc \approx -0.7$, $\Delta CAcc > -0.5$). Middle layers (4-8) demonstrate inconsistent behavior with high variance in impacts. Layer 12, however, achieves an optimal balance: it maintains substantial primary task impact ($\Delta Acc = -0.571$) while minimizing auxiliary concept interference ($\Delta CAcc = -0.060$). This pattern holds true for both neuron masking and range masking techniques, with range masking showing slightly better preservation of auxiliary concepts ($\Delta CAcc = -0.045$). The mid-range primary task degradation combined with minimal auxiliary impact makes Layer 12 the most suitable for targeted interventions, offering better control and specificity compared to earlier layers.
\begin{table*}[t]
\caption{Evaluation of layer selection on \textit{GPT-2} model on the \textit{Emotions} dataset using neuron and range masking techniques. 20\% Neurons selected. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
\midrule
\multirow{6}{*}{1} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.029 & -0.071 & -0.074 & -0.100 & 0.006 & 0.002 & -0.004 & -0.005 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.011 & -0.056 & -0.090 & -0.116 & 0.001 & -0.003 & -0.004 & -0.004 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.206 & -0.195 & -0.052 & -0.092 & -0.019 & -0.015 & -0.001 & -0.002 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.128 & -0.152 & -0.051 & -0.090 & -0.004 & -0.005 & -0.001 & -0.002 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.055 & -0.084 & -0.061 & -0.093 & -0.016 & -0.009 & 0.002 & -0.001 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.249 & -0.217 & -0.055 & -0.094 & 0.016 & 0.013 & -0.004 & -0.005 \\
\midrule
\multirow{6}{*}{2} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.804 & -0.808 & -0.389 & -0.386 & -0.061 & -0.133 & -0.077 & -0.096 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & 0.053 & -0.003 & -0.819 & -0.781 & -0.011 & -0.049 & -0.110 & -0.145 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.868 & -0.737 & -0.515 & -0.519 & -0.365 & -0.337 & -0.077 & -0.126 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.870 & -0.805 & -0.498 & -0.501 & -0.215 & -0.248 & -0.096 & -0.153 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.729 & -0.707 & -0.461 & -0.463 & -0.042 & -0.077 & -0.076 & -0.116 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.845 & -0.769 & -0.511 & -0.508 & -0.229 & -0.188 & -0.106 & -0.163 \\
\midrule
\multirow{6}{*}{3} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.896 & -0.904 & -0.824 & -0.832 & -0.647 & -0.688 & -0.517 & -0.544 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.901 & -0.916 & -0.835 & -0.832 & -0.568 & -0.607 & -0.609 & -0.630 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.868 & -0.845 & -0.838 & -0.851 & -0.605 & -0.600 & -0.589 & -0.619 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.868 & -0.896 & -0.830 & -0.840 & -0.567 & -0.605 & -0.567 & -0.596 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.800 & -0.811 & -0.849 & -0.857 & -0.502 & -0.522 & -0.513 & -0.544 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & 0.022 & 0.081 & -0.865 & -0.881 & -0.155 & -0.124 & -0.561 & -0.596 \\

\midrule
\multirow{6}{*}{4} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.650 & -0.703 & -0.698 & -0.764 & -0.608 & -0.621 & -0.499 & -0.510 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.845 & -0.884 & -0.667 & -0.725 & -0.491 & -0.519 & -0.480 & -0.491 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.858 & -0.824 & -0.772 & -0.809 & -0.488 & -0.497 & -0.506 & -0.523 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.700 & -0.808 & -0.663 & -0.739 & -0.534 & -0.546 & -0.512 & -0.528 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.239 & -0.514 & -0.754 & -0.797 & -0.304 & -0.307 & -0.452 & -0.471 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.612 & -0.463 & -0.692 & -0.765 & -0.047 & -0.038 & -0.525 & -0.541 \\
\midrule
\multirow{6}{*}{5} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.838 & -0.852 & -0.492 & -0.630 & -0.695 & -0.688 & -0.554 & -0.555 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.387 & -0.563 & -0.683 & -0.714 & -0.552 & -0.564 & -0.605 & -0.599 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.702 & -0.700 & -0.634 & -0.690 & -0.472 & -0.470 & -0.607 & -0.605 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.361 & -0.507 & -0.615 & -0.692 & -0.567 & -0.575 & -0.538 & -0.539 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.873 & -0.844 & -0.525 & -0.650 & -0.668 & -0.653 & -0.594 & -0.594 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.637 & -0.573 & -0.588 & -0.681 & -0.069 & -0.022 & -0.548 & -0.553 \\
\midrule
\multirow{6}{*}{6} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.720 & -0.775 & -0.829 & -0.830 & -0.484 & -0.499 & -0.318 & -0.322 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.871 & -0.887 & -0.750 & -0.768 & -0.176 & -0.195 & -0.499 & -0.499 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.895 & -0.860 & -0.735 & -0.773 & -0.680 & -0.638 & -0.335 & -0.348 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.863 & -0.884 & -0.772 & -0.793 & -0.418 & -0.431 & -0.379 & -0.381 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.621 & -0.669 & -0.743 & -0.784 & -0.430 & -0.435 & -0.247 & -0.262 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.143 & -0.086 & -0.808 & -0.831 & -0.114 & -0.070 & -0.474 & -0.478 \\

\bottomrule
\end{tabular}

\label{tab:layer_abiliation_emotions_1}
\end{table*}

\begin{table*}[t]
\caption{Evaluation of layer selection on \textit{GPT-2} model on the \textit{Emotions} dataset using neuron and range masking techniques. 20\% Neurons selected. Here, \textbf{Acc} represents class accuracy, \textbf{Conf} denotes class prediction probability, and \textbf{CAcc} and \textbf{CConf} refer to average accuracy and average class prediction probability across other classes, respectively. The \textit{Base Values} indicate the baseline model performance, while \textit{Activation Range Masking} and \textit{Neuron Masking} show deviations from the baseline performance.}
\centering
\scriptsize
\begin{tabular}{l|l|rrrr|rrrr|rrrr}
\toprule
\textbf{Model} & \textbf{Class} & \multicolumn{4}{c|}{\textbf{Base Values}} & \multicolumn{4}{c|}{\textbf{Neuron Masking}} & \multicolumn{4}{c}{\textbf{Activation Range Masking}} \\
\cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-14}
&  & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\

\midrule
\multirow{6}{*}{7} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.908 & -0.901 & -0.752 & -0.753 & -0.527 & -0.538 & -0.492 & -0.498 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.884 & -0.895 & -0.743 & -0.729 & -0.484 & -0.509 & -0.330 & -0.338 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.866 & -0.835 & -0.767 & -0.765 & -0.451 & -0.442 & -0.336 & -0.355 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.786 & -0.819 & -0.641 & -0.666 & -0.445 & -0.457 & -0.331 & -0.346 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.626 & -0.618 & -0.810 & -0.817 & -0.341 & -0.335 & -0.521 & -0.532 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & 0.106 & 0.147 & -0.810 & -0.811 & 0.102 & 0.107 & -0.547 & -0.553 \\
\midrule
\multirow{6}{*}{8} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.776 & -0.791 & -0.209 & -0.291 & -0.191 & -0.312 & -0.082 & -0.114 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.585 & -0.667 & -0.412 & -0.441 & -0.591 & -0.644 & -0.199 & -0.227 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.692 & -0.716 & -0.469 & -0.496 & -0.560 & -0.562 & -0.468 & -0.486 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.657 & -0.714 & -0.415 & -0.464 & -0.468 & -0.503 & -0.230 & -0.266 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.501 & -0.509 & -0.531 & -0.569 & -0.201 & -0.234 & -0.258 & -0.290 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.092 & -0.050 & -0.634 & -0.647 & 0.065 & 0.058 & -0.279 & -0.308 \\
\midrule
\multirow{6}{*}{9} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.759 & -0.768 & -0.311 & -0.351 & -0.610 & -0.661 & -0.307 & -0.328 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.570 & -0.713 & -0.319 & -0.346 & -0.906 & -0.910 & -0.267 & -0.298 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.424 & -0.520 & -0.504 & -0.531 & -0.635 & -0.643 & -0.579 & -0.595 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.810 & -0.834 & -0.501 & -0.502 & -0.759 & -0.772 & -0.502 & -0.516 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.358 & -0.357 & -0.476 & -0.481 & -0.587 & -0.566 & -0.519 & -0.527 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.133 & -0.101 & -0.546 & -0.554 & 0.106 & 0.104 & -0.450 & -0.462 \\

\midrule
\multirow{6}{*}{10} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.733 & -0.741 & -0.105 & -0.126 & -0.624 & -0.659 & -0.146 & -0.163 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.389 & -0.671 & -0.178 & -0.209 & -0.899 & -0.911 & -0.254 & -0.285 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.230 & -0.513 & -0.116 & -0.224 & -0.699 & -0.735 & -0.409 & -0.451 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.434 & -0.687 & -0.081 & -0.133 & -0.898 & -0.905 & -0.401 & -0.455 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.489 & -0.506 & -0.188 & -0.256 & -0.140 & -0.186 & -0.063 & -0.102 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.306 & -0.243 & -0.157 & -0.240 & 0.063 & 0.010 & -0.095 & -0.127 \\
\midrule
\multirow{6}{*}{11} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.358 & -0.496 & -0.382 & -0.414 & -0.301 & -0.441 & -0.121 & -0.148 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.800 & -0.857 & -0.078 & -0.123 & -0.858 & -0.875 & -0.128 & -0.162 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.897 & -0.861 & -0.416 & -0.450 & -0.901 & -0.864 & -0.464 & -0.500 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.923 & -0.921 & -0.427 & -0.470 & -0.913 & -0.914 & -0.354 & -0.393 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.152 & -0.212 & -0.039 & -0.075 & -0.210 & -0.239 & -0.181 & -0.204 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & 0.047 & -0.028 & -0.131 & -0.173 & 0.053 & 0.002 & -0.142 & -0.159 \\
\midrule
\multirow{6}{*}{12} 
&Class 0 & 0.970 & 0.957 & 0.915 & 0.904 & -0.550 & -0.603 & -0.013 & -0.003 & -0.542 & -0.594 & 0.005 & 0.012 \\
&Class 1 & 0.933 & 0.932 & 0.931 & 0.913 & -0.526 & -0.545 & 0.001 & 0.012 & -0.521 & -0.538 & -0.005 & -0.004 \\
&Class 2 & 0.901 & 0.865 & 0.934 & 0.924 & -0.416 & -0.402 & 0.002 & 0.006 & -0.419 & -0.407 & 0.007 & 0.006 \\
&Class 3 & 0.926 & 0.924 & 0.932 & 0.919 & -0.561 & -0.576 & -0.007 & 0.003 & -0.561 & -0.572 & 0.000 & 0.005 \\
&Class 4 & 0.885 & 0.867 & 0.938 & 0.927 & -0.655 & -0.658 & -0.042 & -0.034 & -0.657 & -0.659 & -0.011 & -0.003 \\
&Class 5 & 0.851 & 0.786 & 0.934 & 0.924 & -0.718 & -0.672 & -0.300 & -0.297 & -0.718 & -0.672 & -0.267 & -0.266 \\



\bottomrule
\end{tabular}

\label{tab:layer_abiliation_emotions_2}
\end{table*}








% \section{Theoretical Framework}
% \subsection{Properties of Neural Activation Spaces}

% \begin{theorem}[Activation Space Decomposition]
% Given a neuron $n$ in layer $l$ of a neural network, its complete activation space $\mathcal{A}_n \subset \mathbb{R}$ can be decomposed into a direct sum of concept-specific subspaces and a null space:
% \[\mathcal{A}_n = \bigoplus_{c \in C} \mathcal{A}_{n,c} \oplus \mathcal{N}\]
% where:
% - $\mathcal{A}_{n,c}$ represents the activation subspace for concept $c$
% - $\mathcal{N}$ is the null space (activations not strongly associated with any concept)
% - $\bigoplus$ denotes the direct sum of spaces
% \end{theorem}

% \begin{proof}
% Consider the pre-activation function of neuron $n$:
% \[f_n(x) = W_n^T x + b_n\]

% 1) First, we observe that the weight matrix $W_n$ can be decomposed using Singular Value Decomposition (SVD):
% \[W_n = U\Sigma V^T = \sum_{i=1}^r \sigma_i u_i v_i^T\]
% where $r$ is the rank of $W_n$, $\sigma_i$ are singular values, and $u_i$, $v_i$ are left and right singular vectors.

% \begin{theorem}[Activation Manifold Structure]
% For a neuron $n$, its activation patterns for different concepts form separable sub-manifolds $\mathcal{M}_c$ in activation space, where each manifold:
% \[\mathcal{M}_c = \{f_n(x) | x \in X_c\}\]
% exhibits local geometric structure preserving concept-relevant features.
% \end{theorem}

% 2) For inputs $x$ associated with concept $c$, we can show that their representations cluster in the input space:
% \[\forall x \in X_c: x = \mu_c + \epsilon\]
% where $\mu_c$ is the concept centroid and $\epsilon$ is a small perturbation.

% 3) This clustering property in input space induces corresponding clusters in activation space. For each concept $c$:
% \[\mathcal{A}_{n,c} = \{f_n(x) | x \in X_c\}\]

% 4) The key insight is that these activation clusters form approximately disjoint ranges:
% \[\mathbb{P}(\mathcal{A}_{n,c_1} \cap \mathcal{A}_{n,c_2} \neq \emptyset) \leq \delta\]
% for some small $\delta$ and any $c_1 \neq c_2$.

% 5) We can prove this by showing that the activation distributions for different concepts are approximately Gaussian with separated means:
% \[f_n(x) | x \in X_c \sim \mathcal{N}(\mu_{n,c}, \sigma_{n,c}^2)\]

% 6) The null space $\mathcal{N}$ captures activations that don't strongly correlate with any concept:
% \[\mathcal{N} = \{a \in \mathcal{A}_n | \forall c: \|a - \mu_{n,c}\| > k\sigma_{n,c}\}\]
% where $k$ is a threshold parameter (e.g., $k=2.5$ in our implementation).

% Therefore, any activation $a \in \mathcal{A}_n$ can be uniquely decomposed into:
% \[a = \sum_{c \in C} a_c + a_N\]
% where $a_c \in \mathcal{A}_{n,c}$ and $a_N \in \mathcal{N}$.
% \end{proof}

% \begin{corollary}[Basis for Range Masking]
% The decomposition theorem provides the theoretical foundation for why range-based masking is more precise than neuron-level masking. When we mask a specific range:
% \[\tilde{f}_n(x) = \begin{cases} 
% 0 & \text{if } f_n(x) \in \mathcal{A}_{n,c} \\
% f_n(x) & \text{otherwise}
% \end{cases}\]
% we primarily affect a single concept subspace while preserving others.
% \end{corollary}

% \begin{theorem}[Information Preservation Bound]
% For a neuron $n$ with range-masked activation $\tilde{a}_n$, the information loss $\mathcal{L}$ relative to the original activation $a_n$ is bounded by:
% \[\mathcal{L}(a_n, \tilde{a}_n) \leq H(X|\tau) - H(X)\]
% where $H(X)$ is the entropy of the original activation distribution and $H(X|\tau)$ is the conditional entropy after range masking.
% \end{theorem}




% $f_n(x) = \phi(W_n^T x + b_n)$
% \begin{table*}[t]
% \centering
% \scriptsize
% \begin{tabular}{c|cccc|cccc|cccc}
% \toprule
% & \multicolumn{4}{c|}{Base Values} & \multicolumn{4}{c|}{Activation Range Masking } & \multicolumn{4}{c}{Neuron Masking} \\
% \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
% \% & Acc & Conf & CAcc & CConf & Acc & Conf & CAcc & CConf & Acc & Conf & CAcc & CConf \\
% \midrule
% 10 & 0.908 & 0.887 & 0.929 & 0.917 & -0.332 & -0.324 & -0.038 & -0.032 & -0.329 & -0.321 & -0.036 & -0.028 \\
% 20 & 0.908 & 0.887 & 0.929 & 0.917 & -0.552 & -0.556 & -0.047 & -0.044 & -0.555 & -0.559 & -0.060 & -0.054 \\
% 30 & 0.908 & 0.887 & 0.929 & 0.917 & -0.730 & -0.736 & -0.068 & -0.072 & -0.734 & -0.739 & -0.120 & -0.121 \\
% 40 & 0.908 & 0.887 & 0.929 & 0.917 & -0.771 & -0.776 & -0.100 & -0.116 & -0.772 & -0.781 & -0.183 & -0.186 \\
% 50 & 0.908 & 0.887 & 0.929 & 0.917 & -0.829 & -0.835 & -0.110 & -0.130 & -0.837 & -0.841 & -0.170 & -0.188 \\
% 60 & 0.908 & 0.887 & 0.929 & 0.917 & -0.855 & -0.851 & -0.151 & -0.180 & -0.867 & -0.860 & -0.227 & -0.268 \\
% 70 & 0.908 & 0.887 & 0.929 & 0.917 & -0.830 & -0.840 & -0.188 & -0.220 & -0.841 & -0.848 & -0.282 & -0.363 \\
% 80 & 0.908 & 0.887 & 0.929 & 0.917 & -0.859 & -0.858 & -0.250 & -0.284 & -0.876 & -0.869 & -0.490 & -0.565 \\
% 90 & 0.908 & 0.887 & 0.929 & 0.917 & -0.884 & -0.864 & -0.305 & -0.339 & -0.892 & -0.871 & -0.715 & -0.760 \\
% 100 & 0.908 & 0.887 & 0.929 & 0.917 & -0.852 & -0.865 & -0.364 & -0.398 & -0.741 & -0.830 & -0.892 & -0.877 \\
% \bottomrule
% \end{tabular}
% \caption{Performance metrics across different ablation percentages (10\% trimmed means), Emotions dataset and GPT-2 model}
% \label{tab:percentage10-100_emotions_gpt}
% \end{table*}


% \section{Theoretical Framework}

% \subsection{Information-Theoretic Foundations}

% \begin{theorem}[Range Information Separation]
% For a neuron $n$ with activation distribution $p(a)$, the mutual information between input $X$ and activation $A$ in range $R=[l,u]$ is bounded by:

% \begin{equation}
% I(X; A|R) \leq \frac{1}{2}\log\left(1 + \frac{\text{Var}(A|R)}{\sigma^2}\right)
% \end{equation}

% where $\sigma^2$ is the noise variance.
% \end{theorem}

% \begin{proof}
% Consider the channel $Y = X + N$ where $N \sim \mathcal{N}(0,\sigma^2)$:
% 1. The differential entropy $h(A|R)$ satisfies:
%    \[ h(A|R) \leq \frac{1}{2}\log(2\pi e\text{Var}(A|R)) \]
% 2. The conditional entropy $h(A|X,R)$ equals:
%    \[ h(A|X,R) = \frac{1}{2}\log(2\pi e\sigma^2) \]
% 3. Therefore:
%    \[ I(X;A|R) = h(A|R) - h(A|X,R) \leq \frac{1}{2}\log\left(1 + \frac{\text{Var}(A|R)}{\sigma^2}\right) \]
% \end{proof}

% \begin{theorem}[Optimal Range Boundaries]
% The optimal range boundaries $[l^*,u^*]$ for concept $c$ satisfy:

% \begin{equation}
% [l^*,u^*] = argmax_{l,u} \left(I(X_c;A|[l,u]) - \beta\sum_{c'\neq c}I(X_{c'};A|[l,u])\right)
% \end{equation}

% where $\beta$ controls the trade-off between concept isolation and information preservation.
% \end{theorem}



% \section{Theoretical Validation}

% \subsection{Information-Theoretic Analysis}
% We empirically validate our theoretical bounds through the following experiments:

% 1. Mutual Information Measurement:
%    \begin{equation}
%    \hat{I}(X;A|R) = \sum_{x,a\in R} \hat{p}(x,a)\log\frac{\hat{p}(x,a)}{\hat{p}(x)\hat{p}(a)}
%    \end{equation}

% 2. Range Optimization:
%    \begin{equation}
%    \mathcal{L}(l,u) = \hat{I}(X_c;A|[l,u]) - \beta\sum_{c'\neq c}\hat{I}(X_{c'};A|[l,u])
%    \end{equation}





% \section{Theoretical Framework}
% \subsection{Properties of Neural Activation Spaces}

% \begin{theorem}[Activation Space Decomposition]
% For any neuron $n$ in layer $l$, its activation space $\mathcal{A}_n$ can be decomposed into concept-specific subspaces:
% \[\mathcal{A}_n = \bigoplus_{c \in C} \mathcal{A}_{n,c} \oplus \mathcal{N}\]
% where $\mathcal{A}_{n,c}$ represents the activation subspace for concept $c$, and $\mathcal{N}$ is the null space.
% \end{theorem}

% \begin{proof}
% Given the pre-activation function $f_n(x) = W_n^T x + b_n$, we can show through singular value decomposition that:
% \[W_n = \sum_{c \in C} U_c \Sigma_c V_c^T + U_N \Sigma_N V_N^T\]
% where each $(U_c, \Sigma_c, V_c)$ corresponds to concept $c$'s contribution.
% \end{proof}

% \begin{corollary}[Activation Range Separation]
% For any two concepts $c_1, c_2$, the probability of activation overlap decreases exponentially with the distance between their means:
% \[P(\mathcal{A}_{n,c_1} \cap \mathcal{A}_{n,c_2} \neq \emptyset) \leq e^{-\frac{(\mu_{c_1} - \mu_{c_2})^2}{2(\sigma_{c_1}^2 + \sigma_{c_2}^2)}}\]
% \end{corollary}












% \textbf{Table GPT2 Emotions Layer abilation}
% \begin{table*}
%     \centering
%     \footnotesize  % Reduce font size
%     % \setlength{\tabcolsep}{}  % Reduce column spacing
%     \begin{tabular}{l|cccc|cccc|cccc}  % @{} removes padding at edges
%     \toprule
%      \multicolumn{13}{c}{\textbf{Layer 1}} \\ 
%     \midrule
%     & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .975 & .959 & .911 & .899 & .941 & .886 & .842 & .804 \\
% Class 1 & .933 & .932 & .931 & .913 & .934 & .928 & .927 & .909 & .921 & .876 & .841 & .797 \\
% Class 2 & .901 & .865 & .934 & .924 & .883 & .850 & .933 & .922 & .696 & .670 & .882 & .833 \\
% Class 3 & .926 & .924 & .932 & .919 & .922 & .918 & .931 & .917 & .798 & .772 & .881 & .829 \\
% Class 4 & .885 & .867 & .938 & .927 & .868 & .857 & .940 & .926 & .830 & .782 & .877 & .834 \\
% Class 5 & .851 & .786 & .934 & .924 & .867 & .799 & .931 & .920 & .602 & .569 & .880 & .831 \\
% \midrule
%  \multicolumn{13}{c}{\textbf{Layer 2}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .908 & .824 & .838 & .808 & .166 & .149 & .526 & .518 \\
% Class 1 & .933 & .932 & .931 & .913 & .922 & .883 & .821 & .768 & .986 & .928 & .111 & .133 \\
% Class 2 & .901 & .865 & .934 & .924 & .536 & .527 & .857 & .798 & .033 & .128 & .419 & .405 \\
% Class 3 & .926 & .924 & .932 & .919 & .711 & .675 & .836 & .766 & .056 & .118 & .434 & .418 \\
% Class 4 & .885 & .867 & .938 & .927 & .843 & .789 & .862 & .811 & .156 & .160 & .477 & .464 \\
% Class 5 & .851 & .786 & .934 & .924 & .622 & .599 & .828 & .761 & .006 & .017 & .423 & .417 \\
% \midrule
%  \multicolumn{13}{c}{\textbf{Layer 3}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .323 & .270 & .398 & .361 & .074 & .054 & .092 & .072 \\
% Class 1 & .933 & .932 & .931 & .913 & .364 & .325 & .322 & .284 & .032 & .016 & .096 & .081 \\
% Class 2 & .901 & .865 & .934 & .924 & .296 & .265 & .345 & .305 & .033 & .020 & .096 & .073 \\
% Class 3 & .926 & .924 & .932 & .919 & .359 & .318 & .365 & .323 & .058 & .027 & .102 & .079 \\
% Class 4 & .885 & .867 & .938 & .927 & .383 & .345 & .425 & .383 & .085 & .056 & .089 & .070 \\
% Class 5 & .851 & .786 & .934 & .924 & .696 & .662 & .373 & .328 & .873 & .867 & .069 & .043 \\
%  \midrule
%  \multicolumn{13}{c}{\textbf{Layer 4}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .362 & .336 & .417 & .394 & .320 & .255 & .218 & .140 \\
% Class 1 & .933 & .932 & .931 & .913 & .442 & .412 & .451 & .423 & .088 & .048 & .263 & .188 \\
% Class 2 & .901 & .865 & .934 & .924 & .413 & .368 & .428 & .401 & .043 & .041 & .162 & .115 \\
% Class 3 & .926 & .924 & .932 & .919 & .391 & .378 & .420 & .391 & .226 & .116 & .269 & .180 \\
% Class 4 & .885 & .867 & .938 & .927 & .581 & .559 & .485 & .456 & .646 & .353 & .184 & .130 \\
% Class 5 & .851 & .786 & .934 & .924 & .804 & .749 & .409 & .384 & .239 & .323 & .243 & .160 \\
% \midrule
%  \multicolumn{13}{c}{\textbf{Layer 5}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .275 & .270 & .361 & .349 & .132 & .105 & .424 & .275 \\
% Class 1 & .933 & .932 & .931 & .913 & .380 & .368 & .326 & .315 & .546 & .369 & .247 & .199 \\
% Class 2 & .901 & .865 & .934 & .924 & .429 & .395 & .327 & .319 & .200 & .165 & .300 & .235 \\
% Class 3 & .926 & .924 & .932 & .919 & .359 & .348 & .394 & .380 & .565 & .416 & .317 & .228 \\
% Class 4 & .885 & .867 & .938 & .927 & .216 & .213 & .344 & .333 & .012 & .023 & .412 & .277 \\
% Class 5 & .851 & .786 & .934 & .924 & .782 & .764 & .387 & .372 & .214 & .214 & .346 & .243 \\


% \midrule

% \multicolumn{13}{c}{\textbf{Layer 6}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .485 & .458 & .598 & .582 & .250 & .182 & .087 & .074 \\
% Class 1 & .933 & .932 & .931 & .913 & .756 & .737 & .432 & .414 & .062 & .045 & .181 & .145 \\
% Class 2 & .901 & .865 & .934 & .924 & .222 & .227 & .599 & .576 & .006 & .005 & .200 & .152 \\
% Class 3 & .926 & .924 & .932 & .919 & .507 & .492 & .554 & .538 & .063 & .039 & .160 & .126 \\
% Class 4 & .885 & .867 & .938 & .927 & .455 & .432 & .691 & .665 & .264 & .197 & .194 & .143 \\
% Class 5 & .851 & .786 & .934 & .924 & .737 & .717 & .460 & .446 & .708 & .701 & .126 & .093 \\
% \midrule 

% \bottomrule
%     \end{tabular}
%     \label{tab:GPT2_abiliaton_layers1}
% \end{table*}

% \begin{table*}
%     \centering
%     \footnotesize  % Reduce font size
%     % \setlength{\tabcolsep}{}  % Reduce column spacing
%     \begin{tabular}{l|cccc|cccc|cccc}  % @{} removes padding at edges
%     \toprule

% \multicolumn{13}{c}{\textbf{Layer 7}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .443 & .419 & .423 & .406 & .062 & .056 & .163 & .151 \\
% Class 1 & .933 & .932 & .931 & .913 & .448 & .423 & .600 & .576 & .049 & .037 & .188 & .184 \\
% Class 2 & .901 & .865 & .934 & .924 & .450 & .423 & .598 & .570 & .036 & .030 & .167 & .160 \\
% Class 3 & .926 & .924 & .932 & .919 & .481 & .467 & .602 & .573 & .140 & .105 & .291 & .253 \\
% Class 4 & .885 & .867 & .938 & .927 & .544 & .532 & .416 & .395 & .259 & .248 & .128 & .110 \\
% Class 5 & .851 & .786 & .934 & .924 & .953 & .894 & .387 & .372 & .957 & .934 & .124 & .113 \\
% \midrule


% \multicolumn{13}{c}{\textbf{Layer 8}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .779 & .645 & .834 & .790 & .194 & .167 & .706 & .613 \\
% Class 1 & .933 & .932 & .931 & .913 & .342 & .287 & .732 & .687 & .347 & .264 & .519 & .472 \\
% Class 2 & .901 & .865 & .934 & .924 & .342 & .303 & .466 & .438 & .210 & .149 & .465 & .429 \\
% Class 3 & .926 & .924 & .932 & .919 & .458 & .421 & .702 & .653 & .269 & .209 & .517 & .455 \\
% Class 4 & .885 & .867 & .938 & .927 & .684 & .632 & .679 & .637 & .383 & .358 & .406 & .358 \\
% Class 5 & .851 & .786 & .934 & .924 & .916 & .845 & .655 & .616 & .759 & .736 & .301 & .278 \\
% \midrule
% \multicolumn{13}{c}{\textbf{Layer 9}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .360 & .296 & .609 & .576 & .211 & .189 & .605 & .553 \\
% Class 1 & .933 & .932 & .931 & .913 & .027 & .021 & .664 & .615 & .362 & .219 & .612 & .567 \\
% Class 2 & .901 & .865 & .934 & .924 & .267 & .222 & .355 & .329 & .477 & .345 & .430 & .393 \\
% Class 3 & .926 & .924 & .932 & .919 & .167 & .152 & .430 & .403 & .116 & .090 & .431 & .417 \\
% Class 4 & .885 & .867 & .938 & .927 & .298 & .300 & .419 & .400 & .527 & .510 & .462 & .446 \\
% Class 5 & .851 & .786 & .934 & .924 & .957 & .890 & .484 & .462 & .718 & .685 & .388 & .370 \\
% \midrule

% \multicolumn{13}{c}{\textbf{Layer 10}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .346 & .298 & .769 & .742 & .237 & .216 & .810 & .778 \\
% Class 1 & .933 & .932 & .931 & .913 & .033 & .020 & .677 & .628 & .543 & .261 & .753 & .704 \\
% Class 2 & .901 & .865 & .934 & .924 & .202 & .130 & .525 & .474 & .671 & .352 & .818 & .700 \\
% Class 3 & .926 & .924 & .932 & .919 & .028 & .019 & .531 & .464 & .491 & .237 & .851 & .786 \\
% Class 4 & .885 & .867 & .938 & .927 & .745 & .680 & .875 & .825 & .395 & .361 & .749 & .671 \\
% Class 5 & .851 & .786 & .934 & .924 & .914 & .796 & .839 & .798 & .545 & .543 & .777 & .684 \\
% \midrule

% \multicolumn{13}{c}{\textbf{Layer 11}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .669 & .516 & .794 & .756 & .612 & .461 & .533 & .490 \\
% Class 1 & .933 & .932 & .931 & .913 & .074 & .056 & .802 & .752 & .133 & .075 & .853 & .790 \\
% Class 2 & .901 & .865 & .934 & .924 & .000 & .001 & .470 & .424 & .004 & .004 & .518 & .474 \\
% Class 3 & .926 & .924 & .932 & .919 & .012 & .010 & .578 & .526 & .003 & .003 & .505 & .449 \\
% Class 4 & .885 & .867 & .938 & .927 & .675 & .628 & .757 & .722 & .732 & .655 & .898 & .852 \\
% Class 5 & .851 & .786 & .934 & .924 & .904 & .788 & .792 & .765 & .898 & .758 & .804 & .751 \\
% \midrule

% \multicolumn{13}{c}{\textbf{Layer 12}} \\
%  \midrule

%  & \multicolumn{4}{c|}{Base} & \multicolumn{4}{c|}{Range} & \multicolumn{4}{c}{Maximum} \\
%     \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule{10-13}
%     \textbf{Class} & \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} & 
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} &
%     \textbf{Acc} & \textbf{Conf} & \textbf{CAcc} & \textbf{CConf} \\
%     \midrule
% Class 0 & .970 & .957 & .915 & .904 & .427 & .363 & .921 & .916 & .420 & .354 & .903 & .901 \\
% Class 1 & .933 & .932 & .931 & .913 & .411 & .393 & .926 & .909 & .406 & .387 & .931 & .925 \\
% Class 2 & .901 & .865 & .934 & .924 & .483 & .458 & .941 & .930 & .485 & .463 & .936 & .931 \\
% Class 3 & .926 & .924 & .932 & .919 & .365 & .351 & .933 & .924 & .365 & .347 & .926 & .922 \\
% Class 4 & .885 & .867 & .938 & .927 & .228 & .208 & .927 & .924 & .230 & .208 & .896 & .892 \\
% Class 5 & .851 & .786 & .934 & .924 & .133 & .115 & .667 & .659 & .133 & .114 & .635 & .627 \\
% \midrule


%     \bottomrule
%     \end{tabular}
%     \label{tab:GPT2_abiliaton_layers2}
% \end{table*}

