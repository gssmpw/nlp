\section{Related Work}
While we have discussed closely related approaches in Section~\ref{prelim}, here, we briefly review additional relevant techniques.
Circuit discovery identifies groups of neurons that jointly encode concepts, providing a structured view of model behavior**Gu et al., "Discovering Structured Representations by Sparsity-Encouraging Regularization"**. However, extracting circuits is computationally intensive and lacks fine-grained neuron-level attribution.
Gradient-based methods attribute predictions to input features by tracking gradients through the network, with integrated gradients**Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"** being a widely used approach. However, they struggle with polysemanticity, as they do not disentangle overlapping concepts within neurons.
Causal analysis methods intervene on internal components to assess their role in encoding concepts. Causal tracing measures the effect of corrupting activations on model performance**Shrikumar et al., "Learning Important Feathers through Adversarial Examples"**, while causal mediation analysis quantifies information propagation through neurons**Molnar, "Interpretable Boosted Trees for Rival Penalized Decision Rules"**. Although effective, these techniques require costly perturbation experiments.
Beyond neuron-level analysis, representation-level methods examine hidden states and their relationship to model outputs. Logit lens**Kim et al., "Deep Learning with Logit Lens: A New Perspective on Feature Visualization"** and tuned lens**Kim et al., "Tuned Lens: Towards more accurate feature visualization in deep neural networks"** project intermediate representations into the modelâ€™s vocabulary space to track information flow, while sparse probing**Meng et al., "Understanding CNN Representations through Sparse Probing"** compresses hidden representations into sparse, interpretable subspaces.
While prior work has advanced interpretability, most methods rely on discrete neuron-to-concept mappings, which fail to account for polysemanticity. Our work extends activation-based approaches by introducing activation ranges as the fundamental unit of interpretability to enable more precise concept attribution and intervention.

%OLD
% \fix{Need to know the detail of RW needed. We already discuss RW wherever we are using it in theh
% sections}
% Machinistic Interpretability(MI) is an emerging field concerned with reverse engineering models' underlying feature mappings for human-understandable concepts. This section presents major research directions currently under consideration by researchers in the domain.

% ____ posit that factual knowledge in the models is stored in the Feed Forward Layers of the model. They identify concepts associated with neurons in the fully connected layers by analyzing neurons' activation, extracting get top k input triggers that produce the highest activation in any neuron under analysis. ____ a recent work discovers that in addition to high positive activation, the high negative activation is also useful in analysis because high positive activation adds a concept and high negative removes a concept. 

% Probe-based methods train an auxiliary neural network on the hidden space of the model. 


% ____ works finds circuits in the network instead of finding individual neurons, that are responsible for any phenomenon in the network