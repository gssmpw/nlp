@article{ConmyMLHG23,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@article{anthropic2023toy,
    title={A Toy Model of Double Descent from Sparsely-Gated Routing},
    author={{Anthropic}},
    year={2023},
    journal={Transformer Circuits},
    url={https://transformer-circuits.pub/2023/toy-double-descent/index.html}
}

@misc{belrose2023elicitinglatentpredictionstransformers,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.08112}, 
}

@inproceedings{dai2021knowledge,
    title = "Knowledge Neurons in Pretrained Transformers",
    author = "Dai, Damai  and
      Dong, Li  and
      Hao, Yaru  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.581/",
    doi = "10.18653/v1/2022.acl-long.581",
    pages = "8493--8502",
    abstract = "Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers."
}

@article{geva2020transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@misc{gurnee2023findingneuronshaystackcase,
      title={Finding Neurons in a Haystack: Case Studies with Sparse Probing}, 
      author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
      year={2023},
      eprint={2305.01610},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.01610}, 
}

@article{he2024jailbreaklens,
  title={JailbreakLens: Interpreting Jailbreak Mechanism in the Lens of Representation and Circuit},
  author={He, Zeqing and Wang, Zhibo and Chu, Zhixuan and Xu, Huiyu and Zheng, Rui and Ren, Kui and Chen, Chun},
  journal={arXiv preprint arXiv:2411.11114},
  year={2024}
}

@misc{logitlens,
author = {nostalgebraist},
  title = {interpreting GPT: the logit lens â€” LessWrong},
  url = "https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens",
month = {8},
year = {2020},
  note = "[Online; accessed 2025-01-29]"
}

@article{marks2024sparse,
  author       = {Samuel Marks and
                  Can Rager and
                  Eric J. Michaud and
                  Yonatan Belinkov and
                  David Bau and
                  Aaron Mueller},
  title        = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal
                  Graphs in Language Models},
  journal      = {CoRR},
  volume       = {abs/2403.19647},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.19647},
  doi          = {10.48550/ARXIV.2403.19647},
  eprinttype    = {arXiv},
  eprint       = {2403.19647},
  timestamp    = {Wed, 10 Apr 2024 17:37:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-19647.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{meng2022locating,
    title={Locating and Editing Factual Associations in {GPT}},
    author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
    journal={Advances in Neural Information Processing Systems},
    year={2022}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@misc{sundararajan2017axiomaticattributiondeepnetworks,
      title={Axiomatic Attribution for Deep Networks}, 
      author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
      year={2017},
      eprint={1703.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.01365}, 
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@article{voita2023neurons,
  title={Neurons in large language models: Dead, n-gram, positional},
  author={Voita, Elena and Ferrando, Javier and Nalmpantis, Christoforos},
  journal={arXiv preprint arXiv:2309.04827},
  year={2023}
}

