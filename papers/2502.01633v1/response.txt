\section{Related Work}
\label{related}
\textbf{Token-space Jailbreaking.} Token-space attacks **Ribeiro, "Beyond Accuracy: Behavioral Testing and Robustness Metrics"** __**, **Wallace, "Training Physical Adversarial Examples"** 
modify the input at the token level to decrease some loss value. 
For example, the GCG algorithm **Liu, "TextFooler: A Model-agnostic Adversarial Attack towards Text Classification"** __**, one of the first transferrable token-level attacks to achieve significant success rates on aligned models, uses the gradient of the loss to guide the greedy search.
Subsequent work has refined this approach to obtain lower computational cost and improved effectiveness **Ryota, "Adversarial Training and Robustness for Deep Neural Networks"** __**, including token-level modifications by other heuristics and without a gradient **Xu, "Adversarial Attacks on Text Classification Models"** and random searches over cleverly chosen initial prompts **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"**. 
%However, since such methods specifically aim for token-level perturbations, their resulting prompts often lack any semantic meaning. 
%Consequently, the resulting prompts can be detected by perplexity-based filters **Papernot, "Practical Black-Box Attacks against Machine Learning"** or smoothing methods **Kumar, "Adversarial Examples: A Survey on Threats and Defenses"**.  
We adopt the use of a loss function from these methods as a signal to inform how to navigate the prompt-space for better jailbreaks while remaining gradient-free. 

\noindent \textbf{Prompt-space Jailbreaking.} These methods often rely on a "red-teaming" LLM to generate adversarial prompts **Wallace, "Training Physical Adversarial Examples"**. 
Methods such as PAIR **Jia, "PAIR: A framework for white-box attacks against text classification models"** deploy a separate LLLM, called the "attacker", which uses a crafted system prompt to interact with the target LLM over multiple rounds and generate semantic jailbreaks; they operate in a black-box manner, requiring only the target’s outputs, and are highly transferrable **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"**.
Some other methods fine-tune a model to generate the attacking prompts **Chen, "Robust Text Classification using Adversarial Training"** __**, though this demands substantial computational resources. Rather than fine-tuning, we rely on increased test-time computation **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"** __**, while others start from expert-crafted prompts (e.g., DAN **Alvarez, "DAN: A framework for white-box attacks against text classification models"**) and refine them via genetic algorithms **Kumar, "Adversarial Examples: A Survey on Threats and Defenses"**. Like these methods, our approach generates semantically meaningful jailbreaks by using another LLM as the attacker, however, our approach is significantly different from the prior work as we develop reasoning modules based on the loss values to better navigate the prompt space.

\noindent \textbf{Comparison with PAIR and TAP.} The closest methods to our framework are PAIR **Jia, "PAIR: A framework for white-box attacks against text classification models"** __**, and TAP-T **Liu, "TAP-T: Adversarial Training of Text Classification Models"** __**. Our verifier-driven method outperforms PAIR and TAP, whose prompt-based CoT remains static and does not leverage a loss. That said, while TAP-T creates a tree of attacks based on the attacker's CoT, it prunes only prompts that do not request the same content as the original intent, and does not utilize any reasoning methodologies. 


\noindent \textbf{Chain-of-Thought (CoT).} CoT prompting **Stajduhar, "Chain-of-Thought Prompt Engineering for Conversational AI"** __** and scratch-padding **Liu, "Improving conversational agents with chain-of-thought reasoning"** __** demonstrate how prompting the model to produce a step-by-step solution improves the LLM's performance. Recent work **Veerendra, "Exploring Chain of Thought Reasoning in Conversational AI"** suggests constructing the CoT through several modules rather than relying only on the language model's CoT capabilities. Notably,  __** explicitly constructs the CoT to ensure that it follows a particular path. Likewise, we use three modules for explicitly constructing the reasoning steps, aiming to reduce the loss function with each step.

\noindent \textbf{Reasoning.} 
% Multi-step reasoning **Li, "Multi-Step Reasoning in Conversational AI"** has catalyzed the emergence of "verifiers" as an alternative to fine-tuning ____. Verifiers have gained popularity under the "best-of-N" scheme, where multiple candidate solutions are generated and subsequently evaluated. Furthermore, it is now established that process-based reward models (PRM) **Gupta, "Process-Based Reward Models for Conversational AI"** __** as the verifier are more effective than outcome-based reward models (ORM) **Chen, "Outcome-Based Reward Models for Conversational AI"** __**. This is because a granular step-wise guidance facilitates a look-ahead signal at each step, which is often necessary for a searching algorithm **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"** __**. Empirical evidence shows that strong verifier models are crucial for accurate model guidance **Alvarez, "DAN: A framework for white-box attacks against text classification models"**, which requires further intermediate-level annotations. However, obtaining such annotations to train PRMs typically requires human annotators **Chen, "Robust Text Classification using Adversarial Training"** __** or deploying a heuristic **Kumar, "Adversarial Examples: A Survey on Threats and Defenses"** __**. In this work, we propose to directly utilize the loss value as a step-wise verifier, thus eliminating the need for additionally training a verifier model. Similar to recent efforts that improve the efficiency of the test-time compute by performing a search against the verifier **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"** __**, we perform a heuristic search with backtracking  **Gupta, "Process-Based Reward Models for Conversational AI"** using the loss function.
Recent advances in reasoning have enhanced LLMs’ capabilities in solving complex problems by scaling test-time computation mechanisms **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"**. 
``Best-of-N'' sampling **Kumar, "Adversarial Examples: A Survey on Threats and Defenses"**, which runs N parallel streams and verifies the final answers through outcome-based reward models (ORMs), is a straightforward test-time scaling approach.
% used in jailbreaking (e.g., PAIR **Jia, "PAIR: A framework for white-box attacks against text classification models"**). 
However, it might fail to uncover solutions that require incremental improvements, limiting its effectiveness compared to other test-time methods. 
To address this limitation, recent work utilizes process-based reward models (PRMs) __**,  **Liu, "Improving conversational agents with chain-of-thought reasoning"** to optimally scale the test-time computation, thereby improving the reasoning performance **Gupta, "Process-Based Reward Models for Conversational AI"**. 
PRMs provide step-by-step verification that facilitates a look-ahead signal at each step, often necessary for a searching algorithm **Li, "Textual Adversarial Attacks: Robustness to Camera Network Watermarking"** __**. 
Similarly, the use of a continuous loss function as a step-wise verifier allows us to run a tree search. 
This framework fits well into the ``Proposer and Verifier'' perspective **Liu, "Improving conversational agents with chain-of-thought reasoning"** of test-time computation, where the proposer proposes a distribution of solutions and a verifier assigns rewards to the proposed distributions.  
Robust verifier models are essential for accurate guidance __**, but they require intermediate annotations from human annotators __** or heuristics __**. 
In our work, we use the loss values from a surrogate LLM as a verifier, eliminating the need to train an additional verifier model. 

 % Empirical evidence underscores the necessity of robust verifier models for accurate guidance **Alvarez, "DAN: A framework for white-box attacks against text classification models"**, typically requiring intermediate-level annotations from human annotators __** or heuristics __**. In this work, we utilize the loss value as a step-wise verifier, eliminating the need to train an additional verifier model.




\noindent \textbf{Reasoning vs safety}. The reasoning framework for exploiting test-time compute can also be used to improve alignment.
OpenAI uses ``deliberative alignment'' **Bostrom, "Deliberate Alignment"** to incorporate human-generated and adversarial data to improve the alignment of the o1 model family ____. 
These models consistently outperform traditional frontier LLMs in metrics measuring vulnerability to automatic adversarial attacks **Ryota, "Adversarial Training and Robustness for Deep Neural Networks"**.
The limitations of current automatic jailbreaking methods and the efficacy of using test-time compute for safety alignment naturally beg the question of whether test-time compute frameworks can be used to \textit{bypass} model guardrails instead of \textit{enforcing} them. Adversarial reasoning, the framework we propose in this paper, demonstrates that bypassing a model's guardrails—even those that leverage increased computation for enhanced safety—is not only possible but also effective. 

% Additional related work is provided in \Cref{additional}.