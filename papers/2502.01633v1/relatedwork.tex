\section{Related Work}
\label{related}
\textbf{Token-space Jailbreaking.} Token-space attacks \cite{shin2020autopromptelicitingknowledgelanguage,wen2023hardpromptseasygradientbased,zou2023universaltransferableadversarialattacks,hayase2024querybasedadversarialpromptgeneration,andriushchenko2024jailbreakingleadingsafetyalignedllms} modify the input at the token level to decrease some loss value. 
For example, the GCG algorithm \cite{zou2023universaltransferableadversarialattacks}, one of the first transferrable token-level attacks to achieve significant success rates on aligned models, uses the gradient of the loss to guide the greedy search.
Subsequent work has refined this approach to obtain  lower computational cost and improved effectiveness \cite{liao2024amplegcglearninguniversaltransferable,jia2024improvedtechniquesoptimizationbasedjailbreaking}, including token-level modifications by other heuristics and without a gradient \cite{hayase2024querybasedadversarialpromptgeneration} and random searches over cleverly chosen initial prompts \cite{andriushchenko2024jailbreakingleadingsafetyalignedllms}. 
%However, since such methods specifically aim for token-level perturbations, their resulting prompts often lack any semantic meaning. 
%Consequently, the resulting prompts can be detected by perplexity-based filters \cite{alon2023detectinglanguagemodelattacks} or smoothing methods \cite{robey2024smoothllmdefendinglargelanguage}.  
We adopt the use of a loss function from these methods as a signal to inform how to navigate the prompt-space for better jailbreaks while remaining gradient-free. 

\noindent \textbf{Prompt-space Jailbreaking.} These methods often rely on a "red-teaming" LLM to generate adversarial prompts \cite{perez2022redteaminglanguagemodels,wei2023jailbrokendoesllmsafety,sadasivan2024fastadversarialattackslanguage,chao2024jailbreakingblackboxlarge,liu2024autodangeneratingstealthyjailbreak,mehrotra2024treeattacksjailbreakingblackbox,zeng2024johnnypersuadellmsjailbreak,samvelyan2024rainbowteamingopenendedgeneration, liu2024autodanturbolifelongagentstrategy}. 
Methods such as PAIR \cite{chao2024jailbreakingblackboxlarge} deploy a separate LLM, called the "attacker", which uses a crafted system prompt to interact with the target LLM over multiple rounds and generate semantic jailbreaks; they operate in a black-box manner, requiring only the target’s outputs, and are highly transferrable \cite{chao2024jailbreakingblackboxlarge}.
Some other methods fine-tune a model to generate the attacking prompts \cite{perez2022redteaminglanguagemodels,ge2023martimprovingllmsafety,zeng2024johnnypersuadellmsjailbreak,paulus2024advprompterfastadaptiveadversarial,beetham2024liarleveragingalignmentbestofn}, though this demands substantial computational resources. Rather than fine-tuning, we rely on increased test-time computation \cite{snell2024scalingllmtesttimecompute}, while others start from expert-crafted prompts (e.g., DAN \cite{ChatGPTDAN2024}) and refine them via genetic algorithms \cite{liu2024autodangeneratingstealthyjailbreak,samvelyan2024rainbowteamingopenendedgeneration,lapid2024opensesameuniversalblack}. Like these methods, our approach generates semantically meaningful jailbreaks by using another LLM as the attacker, however, our approach is significantly different from the prior work as we develop reasoning modules based on the loss values to better navigate the prompt space.

\noindent \textbf{Comparison with PAIR and TAP.} The closest methods to our framework are PAIR \cite{chao2024jailbreakingblackboxlarge} and TAP-T \cite{mehrotra2024treeattacksjailbreakingblackbox}. Our verifier-driven method outperforms PAIR and TAP, whose prompt-based CoT remains static and does not leverage a loss. That said, while TAP-T creates a tree of attacks based on the attacker's CoT, it prunes only prompts that do not request the same content as the original intent, and does not utilize any reasoning methodologies. 


\noindent \textbf{Chain-of-Thought (CoT).} CoT prompting \cite{wei2023chainofthoughtpromptingelicitsreasoning} and scratch-padding \cite{nye2021workscratchpadsintermediatecomputation} demonstrate how prompting the model to produce a step-by-step solution improves the LLM's performance. Recent work \cite{zheng2024criticcotboostingreasoningabilities,wang2024strategicchainofthoughtguidingaccurate,xiang20252reasoningllmslearning} suggests constructing the CoT through several modules rather than relying only on the language model's CoT capabilities. Notably,  \cite{xiang20252reasoningllmslearning} explicitly constructs the CoT to ensure that it follows a particular path. Likewise, we use three modules for explicitly constructing the reasoning steps, aiming to reduce the loss function with each step.

\noindent \textbf{Reasoning.} 
% Multi-step reasoning \cite{hendrycks2021measuringmathematicalproblemsolving,wei2023chainofthoughtpromptingelicitsreasoning} has catalyzed the emergence of "verifiers" as an alternative to fine-tuning \cite{cobbe2021trainingverifierssolvemath}. Verifiers have gained popularity under the "best-of-N" scheme, where multiple candidate solutions are generated and subsequently evaluated. Furthermore, it is now established that process-based reward models (PRM) \cite{uesato2022solvingmathwordproblems,lightman2023letsverifystepstep,wang2024mathshepherdverifyreinforcellms} as the verifier are more effective than outcome-based reward models (ORM) \cite{cobbe2021trainingverifierssolvemath,yu2024ovmoutcomesupervisedvaluemodels}. This is because a granular step-wise guidance facilitates a look-ahead signal at each step, which is often necessary for a searching algorithm \cite{xie2024montecarlotreesearch}. Empirical evidence shows that strong verifier models are crucial for accurate model guidance \cite{zhang2024smalllanguagemodelsneed,zhang2024generativeverifiersrewardmodeling,stechly2024selfverificationlimitationslargelanguage}, which requires further intermediate-level annotations. However, obtaining such annotations to train PRMs typically requires human annotators \cite{uesato2022solvingmathwordproblems,lightman2023letsverifystepstep} or deploying a heuristic \cite{wang2024mathshepherdverifyreinforcellms}. In this work, we propose to directly utilize the loss value as a step-wise verifier, thus eliminating the need for additionally training a verifier model. Similar to recent efforts that improve the efficiency of the test-time compute by performing a search against the verifier \cite{snell2024scalingllmtesttimecompute} rather than a vanilla best-of-N approach, we perform a heuristic search with backtracking  \cite{gandhi2024streamsearchsoslearning,openai2024learning} using the loss function.
Recent advances in reasoning have enhanced LLMs’ capabilities in solving complex problems by scaling test-time computation mechanisms \cite{hendrycks2021measuringmathematicalproblemsolving,RomeraParedes2023MathematicalDF,ahn2024largelanguagemodelsmathematical,shao2024deepseekmathpushinglimitsmathematical,rein2023gpqagraduatelevelgoogleproofqa,openai_system_card_2024}. 
``Best-of-N'' sampling \cite{cobbe2021trainingverifierssolvemath,yu2024ovmoutcomesupervisedvaluemodels}, which runs N parallel streams and verifies the final answers through outcome-based reward models (ORMs), is a straightforward test-time scaling approach.
% used in jailbreaking (e.g., PAIR \cite{chao2024jailbreakingblackboxlarge}). 
However, it might fail to uncover solutions that require incremental improvements, limiting its effectiveness compared to other test-time methods. 
To address this limitation, recent work utilizes process-based reward models (PRMs) \cite{uesato2022solvingmathwordproblems,lightman2023letsverifystepstep,wang2024mathshepherdverifyreinforcellms} to optimally scale the test-time computation, thereby improving the reasoning performance \cite{snell2024scalingllmtesttimecompute,xie2024montecarlotreesearch,gandhi2024streamsearchsoslearning,openai2024learning}. 
PRMs provide step-by-step verification that facilitates a look-ahead signal at each step, often necessary for a searching algorithm \cite{xie2024montecarlotreesearch}. 
Similarly, the use of a continuous loss function as a step-wise verifier allows us to run a tree search. 
This framework fits well into the ``Proposer and Verifier'' perspective \cite{snell2024scalingllmtesttimecompute} of test-time computation, where the proposer proposes a distribution of solutions and a verifier assigns rewards to the proposed distributions.  
Robust verifier models are essential for accurate guidance \cite{zhang2024smalllanguagemodelsneed, zhang2024generativeverifiersrewardmodeling, stechly2024selfverificationlimitationslargelanguage}, but they require intermediate annotations from human annotators or heuristics \cite{uesato2022solvingmathwordproblems, lightman2023letsverifystepstep, wang2024mathshepherdverifyreinforcellms}. 
In our work, we use the loss values from a surrogate LLM as a verifier, eliminating the need for training a verifier model. 

 % Empirical evidence underscores the necessity of robust verifier models for accurate guidance \cite{zhang2024smalllanguagemodelsneed, zhang2024generativeverifiersrewardmodeling, stechly2024selfverificationlimitationslargelanguage}, typically requiring intermediate-level annotations from human annotators \cite{uesato2022solvingmathwordproblems, lightman2023letsverifystepstep} or heuristics \cite{wang2024mathshepherdverifyreinforcellms}. In this work, we utilize the loss value as a step-wise verifier, eliminating the need to train an additional verifier model.




\noindent \textbf{Reasoning vs safety}. The reasoning framework for exploiting test-time compute can also be used to improve alignment.
OpenAI uses ``deliberative alignment'' \cite{Guan2024-sv} to incorporate human-generated and adversarial data to improve the alignment of the o1 model family \cite{openai_system_card_2024, openai2024learning}.
These models consistently outperform traditional frontier LLMs in metrics measuring vulnerability to automatic adversarial attacks \cite{openai_system_card_2024, Hughes2024-te}.
The limitations of current automatic jailbreaking methods and the efficacy of using test-time compute for safety alignment naturally beg the question of whether test-time compute frameworks can be used to \textit{bypass} model guardrails instead of \textit{enforcing} them. Adversarial reasoning, the framework we propose in this paper, demonstrates that bypassing a model's guardrails—even those that leverage increased computation for enhanced safety—is not only possible but also effective. 

% Additional related work is provided in \Cref{additional}.