\section{Related Work}
\label{related}
\textbf{Token-space Jailbreaking.} Token-space attacks ____ modify the input at the token level to decrease some loss value. 
For example, the GCG algorithm ____, one of the first transferrable token-level attacks to achieve significant success rates on aligned models, uses the gradient of the loss to guide the greedy search.
Subsequent work has refined this approach to obtain  lower computational cost and improved effectiveness ____, including token-level modifications by other heuristics and without a gradient ____ and random searches over cleverly chosen initial prompts ____. 
%However, since such methods specifically aim for token-level perturbations, their resulting prompts often lack any semantic meaning. 
%Consequently, the resulting prompts can be detected by perplexity-based filters ____ or smoothing methods ____.  
We adopt the use of a loss function from these methods as a signal to inform how to navigate the prompt-space for better jailbreaks while remaining gradient-free. 

\noindent \textbf{Prompt-space Jailbreaking.} These methods often rely on a "red-teaming" LLM to generate adversarial prompts ____. 
Methods such as PAIR ____ deploy a separate LLM, called the "attacker", which uses a crafted system prompt to interact with the target LLM over multiple rounds and generate semantic jailbreaks; they operate in a black-box manner, requiring only the target’s outputs, and are highly transferrable ____.
Some other methods fine-tune a model to generate the attacking prompts ____, though this demands substantial computational resources. Rather than fine-tuning, we rely on increased test-time computation ____, while others start from expert-crafted prompts (e.g., DAN ____) and refine them via genetic algorithms ____. Like these methods, our approach generates semantically meaningful jailbreaks by using another LLM as the attacker, however, our approach is significantly different from the prior work as we develop reasoning modules based on the loss values to better navigate the prompt space.

\noindent \textbf{Comparison with PAIR and TAP.} The closest methods to our framework are PAIR ____ and TAP-T ____. Our verifier-driven method outperforms PAIR and TAP, whose prompt-based CoT remains static and does not leverage a loss. That said, while TAP-T creates a tree of attacks based on the attacker's CoT, it prunes only prompts that do not request the same content as the original intent, and does not utilize any reasoning methodologies. 


\noindent \textbf{Chain-of-Thought (CoT).} CoT prompting ____ and scratch-padding ____ demonstrate how prompting the model to produce a step-by-step solution improves the LLM's performance. Recent work ____ suggests constructing the CoT through several modules rather than relying only on the language model's CoT capabilities. Notably,  ____ explicitly constructs the CoT to ensure that it follows a particular path. Likewise, we use three modules for explicitly constructing the reasoning steps, aiming to reduce the loss function with each step.

\noindent \textbf{Reasoning.} 
% Multi-step reasoning ____ has catalyzed the emergence of "verifiers" as an alternative to fine-tuning ____. Verifiers have gained popularity under the "best-of-N" scheme, where multiple candidate solutions are generated and subsequently evaluated. Furthermore, it is now established that process-based reward models (PRM) ____ as the verifier are more effective than outcome-based reward models (ORM) ____. This is because a granular step-wise guidance facilitates a look-ahead signal at each step, which is often necessary for a searching algorithm ____. Empirical evidence shows that strong verifier models are crucial for accurate model guidance ____, which requires further intermediate-level annotations. However, obtaining such annotations to train PRMs typically requires human annotators ____ or deploying a heuristic ____. In this work, we propose to directly utilize the loss value as a step-wise verifier, thus eliminating the need for additionally training a verifier model. Similar to recent efforts that improve the efficiency of the test-time compute by performing a search against the verifier ____ rather than a vanilla best-of-N approach, we perform a heuristic search with backtracking  ____ using the loss function.
Recent advances in reasoning have enhanced LLMs’ capabilities in solving complex problems by scaling test-time computation mechanisms ____. 
``Best-of-N'' sampling ____, which runs N parallel streams and verifies the final answers through outcome-based reward models (ORMs), is a straightforward test-time scaling approach.
% used in jailbreaking (e.g., PAIR ____). 
However, it might fail to uncover solutions that require incremental improvements, limiting its effectiveness compared to other test-time methods. 
To address this limitation, recent work utilizes process-based reward models (PRMs) ____ to optimally scale the test-time computation, thereby improving the reasoning performance ____. 
PRMs provide step-by-step verification that facilitates a look-ahead signal at each step, often necessary for a searching algorithm ____. 
Similarly, the use of a continuous loss function as a step-wise verifier allows us to run a tree search. 
This framework fits well into the ``Proposer and Verifier'' perspective ____ of test-time computation, where the proposer proposes a distribution of solutions and a verifier assigns rewards to the proposed distributions.  
Robust verifier models are essential for accurate guidance ____, but they require intermediate annotations from human annotators or heuristics ____. 
In our work, we use the loss values from a surrogate LLM as a verifier, eliminating the need for training a verifier model. 

 % Empirical evidence underscores the necessity of robust verifier models for accurate guidance ____, typically requiring intermediate-level annotations from human annotators ____ or heuristics ____. In this work, we utilize the loss value as a step-wise verifier, eliminating the need to train an additional verifier model.




\noindent \textbf{Reasoning vs safety}. The reasoning framework for exploiting test-time compute can also be used to improve alignment.
OpenAI uses ``deliberative alignment'' ____ to incorporate human-generated and adversarial data to improve the alignment of the o1 model family ____.
These models consistently outperform traditional frontier LLMs in metrics measuring vulnerability to automatic adversarial attacks ____.
The limitations of current automatic jailbreaking methods and the efficacy of using test-time compute for safety alignment naturally beg the question of whether test-time compute frameworks can be used to \textit{bypass} model guardrails instead of \textit{enforcing} them. Adversarial reasoning, the framework we propose in this paper, demonstrates that bypassing a model's guardrails—even those that leverage increased computation for enhanced safety—is not only possible but also effective. 

% Additional related work is provided in \Cref{additional}.