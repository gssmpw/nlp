\section{Examples}\label{app: examples}
\Cref{o1_example,Claude_example} show the details of producing the jailbreaking prompt, including the feedback strings. In \Cref{o1_example}, the algorithm runs for 6 iterations and expands multiple strings (only the expanded ones are shown here), whereas in \Cref{Claude_example}, it finds a jailbreaking prompt with only one iteration. \Cref{compare_pair} shows that PAIR sometime deviates from the original intent. \Cref{llama2_example,llamaRR_example} show that our algorithm finds jailbreaks that the target LLM begins with refusal but then complies.

\begin{figure}[t!]
     \centering
     \includegraphics[width=0.9\linewidth]{figs/JB_example_o1.pdf}
     \vspace{-0.1in}
     \caption{A full example of exploring the prompt-space for 6 iterations and jailbreaking OpenAI o1. Note that each node has $n/k = 8$ children, where only the explored ones are shown in this graph for simplicity.}
     \label{o1_example}
w\end{figure}


\begin{figure}[t!]
     \centering
     \includegraphics[width=0.9\linewidth]{figs/JB_example_Claude.png}
     \vspace{-0.2in}
     \caption{An example of exploring the prompt-space for only 1 iteration that jailbreaks Claude. Note that the initial node has 8 children where only the explored one is shown here for simplicity.}
     \label{Claude_example}
\end{figure}



\begin{figure}[t!]
     \centering
     \begin{subfigure}[b]{0.95\textwidth}
         \centering
         % Linewidth is the now the unit for half a page.
         \includegraphics[width=1.0\linewidth]{figs/pair_example.pdf}
         \caption{PAIR}
         \label{goal60_llama2_pair}
     \end{subfigure}
     \\
     \begin{subfigure}[b]{0.95\textwidth}
         \centering
         % Linewidth is the now the unit for half a page.
         \includegraphics[width=1.0\linewidth]{figs/perc_example.pdf}
         \caption{Ours}
         \label{goal60_llama2_perc}
     \end{subfigure}
        \caption{We ran our algorithm in a comparison with PAIR both for 7 iterations. While our method reaches a jailbreak for Llama-2-7B by the end of this, PAIR's output deviates from original intent. The last prompt of PAIR is not about extracting the instructions for locating the chop shops, but rather about roleplay with someone who knows about them. In this case, Llama-2's answer does not give any instructions. This happens because PAIR solely focuses on avoiding the target's refusals without any further guidance.}
        \label{compare_pair}
\end{figure}

\begin{figure}[t!]
     \centering
     \includegraphics[width=0.9\linewidth]{figs/Refuse_llama.png}
     \vspace{-0.1in}
     \caption{A jailbreaking example where Llama-2-7B initially refuses to comply.}
     \label{llama2_example}
\end{figure}


\begin{figure}[t!]
     \centering
     \includegraphics[width=0.9\linewidth]{figs/Refuse-RR.png}
     \vspace{-0.2in}
     \caption{A jailbreaking example where Llama-3-8B-RR initially refuses to comply.}
     \label{llamaRR_example}
\end{figure}
