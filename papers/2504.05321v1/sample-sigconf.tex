
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%

%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\let\Bbbk\relax
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage[titletoc]{appendix}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
%\acmDOI{XXXXXXX.XXXXXXX}

% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD '2025]{Make sure to enter the correct
 conference title from your rights confirmation emai}{TBD}{TBD}

\acmISBN{978-1-4503-XXXX-X/18/06}



\begin{document}
\title{VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted Trie in Sponsored Search
}


\author{Boyang Zuo$^{\dagger, \ast}$}
\email{zuoby22@mails.tsinghua.edu.cn}
\affiliation{%
  \institution{Tsinghua University}
  \city{Beijing}
  \country{China}
}

\author{Xiao Zhang$^{\dagger}$}
\email{zx142853@alibaba-inc.com}
\affiliation{%
  \institution{Taobao and Tmall Group}
  \city{Beijing}
  \country{China}
}

\thanks{$^\dagger$These authors contributed equally to this work and should be considered co-first authors.}
\thanks{$^\ast$This work was done when the first author was an intern at Alimama.}


\author{Feng Li}
\email{adam.lf@alibaba-inc.com}
\affiliation{%
  \institution{Taobao and Tmall Group}
  \city{Beijing}
  \country{China}
}

\author{Pengjie Wang}
\email{pengjie.wpj@alibaba-inc.com}
\affiliation{%
  \institution{Taobao and Tmall Group}
  \city{Beijing}
  \country{China}
}

\author{Jian Xu}
\email{xiyu.xj@alibaba-inc.com}
\affiliation{%
  \institution{Taobao and Tmall Group}
  \city{Beijing}
  \country{China}
}

\author{Bo Zheng}
\email{bozheng@alibaba-inc.com}
\affiliation{%
  \institution{Taobao and Tmall Group}
  \city{Beijing}
  \country{China}
}


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.





\begin{abstract}
   In the realm of sponsored search advertising, matching advertisements with the search intent of a user's query is crucial. Query-to-bidwords(i.e. bidding keywords) rewriting, which involves transforming user queries into keywords for bidding, is a vital technique that has garnered significant attention from both industry and academia. Recently, with the prevalence of large language models (LLMs), generative retrieval methods have proven effective in producing high-relevance rewrites. However, we have identified a significant limitation in existing approaches: While fine-tuning LLMs for specific domains enhances semantic relevance, these models have no perception of the intrinsic value of their generated outputs, such as commercial value. Therefore, after supervised fine-tuning (SFT), a reinforcement learning from human feedback (RLHF) phase is often employed to address this issue. Nevertheless, traditional preference alignment methods often face challenges in aligning fine-grained values and are susceptible to overfitting, which diminishes the effectiveness and quality of the generated results. To address these challenges, we propose \textbf{VALUE} (\textbf{V}alue-\textbf{A}ware \textbf{L}arge language model for q\textbf{U}ery rewriting via w\textbf{E}ighted trie), the first framework that ensures the generation of high-value and highly relevant bidwords. Our approach utilizes weighted trie, an innovative modification of the traditional trie data structure. By modulating the LLM's output probability distribution with value information from the trie during decoding process, we constrain the generation space and guide the trajectory of text production. Our method not only addresses fine-grained value alignment but also effectively reduces the hallucination issues often encountered with LLMs. Offline experiments demonstrate the effectiveness of our method in semantic matching and preference alignment, showing a remarkable improvement in the value attribute by more than fivefold. Online A/B tests further revealed that our Revenue Per Mille (RPM) metric increased by 1.64\%. VALUE has been deployed on our advertising system since October 2024 and served the Double Eleven promotions, the biggest shopping carnival in China.
\end{abstract}

%%% DPO/PPO无法对细粒度的价值，这块怎么描述。 Furthermore, traditional preference alignment methods like PPO and DPO struggle with aligning fine-grained value in this scenario and are prone to overfitting, further reducing hit rates and value attributes of the generated results.

%%% high-value的解释，取得更高平台收益。(abbr. high revenue)。

%%% keyword、bidword、bid-word。 所有行文都使用bidword，统一。



%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003325.10003330</concept_id>
       <concept_desc>Information systems~Query reformulation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003346</concept_id>
       <concept_desc>Information systems~Top-k retrieval in databases</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Query reformulation}
\ccsdesc[500]{Information systems~Top-k retrieval in databases}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\renewcommand{\shortauthors}{Boyang Zuo and Xiao Zhang, et al}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Query Rewrite; Semantic Matching; Generative Retrieval; Large Language Model;}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{WTLLMQR}
%   \caption{Weighted Trie based LLM for Query Rewrite.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{pic1_value_system_overview.pdf}
    \caption{System Overview of our proposed VALUE Framework includes three parts: "SFT and Alignment", "Weighted Trie Construction and Weight Momentum Update", and "Collaborative Inference". During the LLM decoding process, we derive a value distribution from a weighted trie to constrain the generation space, and subsequently merge this with another distribution to produce the final output distribution.}
%%    \caption{System Overview of our proposed VALUE Framework when generating the \(k+1\)-th token. We use the prefix (the generated \(k\) tokens) to obtain the available candidates for the \(k+1\)-th token and their value information from the weighted trie. We then constrain the output space of the LLM according to these candidates, adjust the output probabilities of these tokens given by the LLM using the value information, and sample the next token from the new distribution.}


%%% system overview + 推理合起来
%%% prompt 用一句话
%%%

    \Description{}
    \label{fig:sysoverview}
\end{figure}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1\textwidth]{system_overview_flat.jpg}
%     \caption{System Overview of our VALUE Framework. When generating the next token, we obtain the available tokens and their value information from the weighted trie. We then constrain the output space of the LLM, adjust the output probabilities of these tokens give by the LLM and sample the next token from the new distribution.}
%     \Description{}  
%     \label{fig:overview}
% \end{figure*}


%%% 改：
%%% 第一段，写搜索广告业务，里面有一个改写，目标是受限空间。 需要考虑相关性与价值。
%%% 第二段，目前业界主要做法是什么，怎么分类，参考RL-based
%%% 第三段，这些改写的主要问题，无法感知价值、无法解决受限生成
%%% 第四段，提出我们的方法，哪里好。


%%% 改：
%%% prefix -> generated tokens，下文增加描述这是对 t -> t+1 的生成的。对这个树增加的层数描述，对WeightedTrie把细节补足。把算法晚自习的图拿来用，和逻辑上来讲。讲整个框架图。
%%% 树怎么构建，和伪代码，把图放上来。
%%% 协同推理过程也需要放进来。


Sponsored search is a crucial element of modern search engines, where advertisers bid on relevant bidwords to display their ads alongside original search results. Upon receiving user's query, it is essential to match the query with relevant bidwords \cite{fain2006sponsored}, and subsequently identify suitable ads associated with those bidwords. This process is a critical component in the ad retrieval pipeline, driving the major revenue of search engines \cite{jansen2008sponsored}. The success of sponsored search depends on the search engine's ability to accurately identify bidwords that closely align with user intent. However, users often face challenges when formulating queries, leading to suboptimal search experiences. These challenges are intensified when users do not use precise or correct terminology, employ synonyms, or mix languages in their search phrases due to varying levels of language proficiency. Additionally, search terms might be misspelled or overly general, complicating the retrieval of relevant ads. For example, a user might search for "spring fashion," which is broad and could correspond to bidwords like "2024 spring collection" or "women's spring trends". Each query reflects different intents but lacks clarity without additional context. As e-commerce platforms continue to grow in both scale and diversity, ensuring accurate and relevant ad retrieval becomes increasingly challenging, necessitating the need for advanced query rewriting and bidword matching techniques to better align user queries with advertiser bidwords.

% The landscape of search engine advertising has evolved significantly. Initially, search engines employed an exact match strategy, requiring a user’s query to precisely match a bidword for an ad to be displayed. This approach was not only cumbersome, requiring advertisers to bid on numerous keyword variations, but also inefficient, often missing potential revenue from slight query modifications \cite{diversity}. To address these limitations, search engines introduced close-variant match types, which align search queries with bidwords sharing the same intent, even if expressed differently. This innovation broadens the scope of relevant queries, enhancing ad display opportunities and revenue potential. Despite these advancements, manual rewriting remains unscalable due to its susceptibility to errors and the extensive expertise required. Traditional automated query rewriting algorithms also exhibit considerable limitations. Rule-based techniques, relying on predefined syntactic transformations, cannot generalize to novel query patterns, failing to optimize queries deviating from established norms. Synthesis-based approaches, attempting to construct new queries from scratch, often struggle with the intricate structures of complex queries, resulting in suboptimal performance. Accurately matching search queries to their close variants remains a formidable challenge due to the complexities inherent in interpreting search intent. The heterogeneity in user expression of product preferences frequently results in semantic gaps between user queries and keywords. This issue is further exacerbated with long-tail queries, where retrieval systems often fail to yield pertinent results \cite{taobaowww}.


In query rewriting, two primary paradigms exist: discriminative methods, which reformulate queries by the retrieval of similar terms and generative methods, which utilize language models for direct query-to-bidword transformation. Discriminative methods leverage sparse and dense retrieval techniques to find relevant bidwords. Sparse retrieval techniques, such as BM25 \cite{robertson2009probabilistic}, employ exact lexical matching through high-dimensional term-frequency vectors. Their reliance on exact lexical matching leads to semantic brittleness. This rigidity prevents them from recognizing semantic equivalences between different expressions, resulting in significant retrieval failures when faced with semantically similar but lexically divergent queries. Dense retrieval approaches overcome the limitations of sparse methods by utilizing low-dimensional semantic embeddings generated by neural encoders like sBERT \cite{reimers2019sentence}. Dense retrieval, while effective in capturing semantic relationships, suffers from significant limitations, particularly the Matthew Effect and long-tail challenges. High-frequency queries dominate model training due to abundant click logs, leading to over-optimized embeddings for head queries while neglecting tail queries with sparse data. Furthermore, dense retrieval models struggle with distribution shifts and require costly strategies like meta-learning or data augmentation to address long-tail issues \cite{taobaowww}.

Recently, the rise of generative retrieval has garnered widespread attention. Wang \cite{baiduwww} utilizes LLMs for one-step query-to-keyword rewriting, using a trie during the decoding process. Despite improvements over traditional information retrieval (IR) baselines, existing generative approaches using LLMs for query-to-bidword rewriting tasks have notable limitations. A key limitation is their inability to recognize the value of the generated results, which, in our scenario, pertains to the outcomes they can deliver. Models trained solely through supervised fine-tuning on online logs lack awareness of the value of the results. Due to the training methodology and the SFT loss function design, the model treats each query-bidword pair in the training samples equally, failing to learn any preference or value information. Although there is a great deal of work on Reinforcement Learning from Human Feedback (RLHF), such as PPO, DPO, KTO, and ORPO\cite{ouyang2022training, rafailov2024direct, ethayarajh2024kto, hong2024orpo}, which can partially mitigate the value perception issues of SFT models, these methods also have significant drawbacks. Firstly, they require substantial computational resources for training. However, the effective Cost Per Mille (eCPM) value of bidwords is a statistical measure that frequently fluctuates. When these fluctuations occur, the previous alignment efforts become obsolete, and the model needs to be retrained. Secondly, these training methods are prone to overfitting \cite{meng2024simpo}, especially in our scenario. Unlike traditional alignment objectives for LLMs, such as harmlessness and usefulness, which can be achieved using a small dataset, the value of a bidword is completely independent of its textual content. Therefore, the model must be trained individually for each bidword. Our experiment on DPO show that models trained in this manner tend to overfitting. There is an urgent need for a new paradigm to address the challenges above.

Through our observations, we have identified an opportunity in the sponsored search advertising scenario to enhance the value of bidwords without compromising their relevance. The value of a bidword does not necessarily correlate with its literal meaning. Two bidwords might differ by only a symbol or character, yet exhibit substantial differences in value. Therefore, there is an opportunity to increase the value of the top \(K\) bidwords while minimally affecting relevance by prioritizing those with the same level of relevance but higher value over others. Thereby, we present VALUE, an innovative reward-guided inference framework that integrates value-awareness into the decoding process via a novel weighted trie structure. We design algorithms for the construction and update of the value of trie node. During decoding process, VALUE obtain reward information from for the next tokens the weighted trie. This information is used to adjust the output distribution, thereby sampling the subsequent token. By this reward-guided search, the framework aims to maximize the reward of the outputs. We used a multi-task fine-tuning strategy to equip the model with essential query rewriting skills. To achieve better value alignment, we designed an optional post-training alignment phase. To evaluate the effectiveness of our proposed method, we conducted offline experiments using historical online logs. Compared to the current online service model, our method demonstrated a \textbf{9.3\%} improvement in \textbf{relevance}. Additionally, online A/B tests showed increases in key profit metrics, with \textit{Cost} rising by \textbf{1.53\%} and \textit{Revenue Per Mille (RPM)} increasing by \textbf{1.64\%}. Given these significant improvements, our method has been fully deployed in the online service. Our contributions can be summarized as follows:

\begin{enumerate}
    % \item \textbf{Incorporating Value Optimization in Query Rewriting:} We are the first to propose injecting value-awareness into query rewriting, achieving this through a framework based on weighted trie collaborative inference with LLMs.
    \item \textbf{Pioneering Query Rewriting Framework via Collaborative Inference:} We present a groundbreaking query rewriting framework that integrates LLMs with weighted trie, uniquely designed to simultaneously optimize for both high value and semantic relevance. This framework represents the first of its kind to effectively balance these two critical aspects.
    \item \textbf{Renovation of Trie:} We have reinvented the trie data structure, by seamlessly integrating value information directly into its nodes. This advancement is further supported by our proposed algorithms for trie construction and momentum update of node value.
    \item \textbf{Post-Training Alignment for Fine-Grained Reward Optimization:} We propose an post-training alignment algorithm that fine-tunes the model to better align with fine-grained reward attributes. 
\end{enumerate}



%%% 贡献点1再好好想想





% In this work, we propose a novel framework to address the limitations of existing approaches by incorporating value awareness into the ret generation process. Our approach utilizes a weighted trie structure to guide the model's token selection with value information during the LLM decoding process. The whole framework comprises four stages: LLM SFT, weighted trie construction, post-training trie alignment, and momentum-based trie update. Firstly, we use online logs scored by a relevance model to obtain highly relevant \textit{<query, bidword>} pairs for SFT, enable the model with basic query rewriting capabilities. Secondly, we obtain the eCPM value of each bidword and insert them into our weighted trie. Then we jointly optimize the entire inference system to align with eCPM value. Since eCPM is a statistical value, it will fluctuate over time. In order to provide better performance without retraining our system, we use momentum updates to modify node values of our trie. Our approach significantly enhances the eCPM values of the top-k generated bidwords, achieving an impressive increase of 1076\% without compromising relevance. Furthermore, our method demonstrates a substantial 400\% increase in the Spearman correlation metric, which further validates the effectiveness of our value-aware framework. Our contributions can be summarized as follows:




% \begin{enumerate}
%     \item \textbf{Introduction of a Weighted Trie Structure}: We introduce a novel weighted trie structure. This structure allows us to inject eCPM value information directly into the generated bidwords.

%     \item \textbf{Post-Training Trie Alignment}: We design a post-training scheme for the trie structure. This post-training alignment leads to more valuable bidwords by incorporating real-time value information into the generation process.

%     \item \textbf{Momentum-Based Trie Update}: To accommodate dynamic changes in bidword values, we propose a momentum-based update scheme for the trie structure, enhancing the robustness and adaptability of our framework.
% \end{enumerate}




\section{Related Works}

\subsection{Query Rewriting}
The process of query rewriting, often termed query expansion or reformulation, is essential in enhancing e-commerce search technologies. It profoundly affects the shopping experience of users and the financial performance of e-commerce platforms. This approach can be generally classified into two classes: discriminative methods and generative methods.

\textbf{Discriminative methods} approach query rewriting as a retrieval task. Pseudo-relevance feedback methods \cite{xu,t6,t31,t40} enhance queries by identifying expansion terms from the top-ranked documents of an initial query, combining global corpus analysis with local feedback. Although these methods effectively address word mismatches, they are susceptible to semantic drift due to noisy or irrelevant top results. To address these challenges, researchers suggest utilizing a well-constructed thesaurus as a candidate set for query rewriting. \cite{t5,t22} However, they caution that the effectiveness of these methods is highly dependent on the thesaurus's quality. If the thesaurus is inadequate, it may lead to query semantic drift, where the intended meaning of the query is compromised. Additionally, some approaches \cite{t3,t8,t18} focus on generating candidate rewrites by leveraging search logs, using similar terms from users' search histories as extensions. However, these search logs inherently display a bias towards popular queries due to the Matthew effect. Consequently, the training data derived from this method may fall short in optimizing for less frequently searched long-tail queries.



\textbf{Generative methods}, on the other hand, focus on the direct transformation of queries into keywords \cite{baiduwww, lee2018rare, qi2020prophetnet, mohankumar2023unified}. These methods involve transforming the input query directly into multiple bidwords, utilizing auto-regressive language models combined with trie-based decoding techniques to restrict the output space during inference. However, while previous methods ensure semantic relevance between input and output, they fall short in addressing value—a critical attribute in sponsored search advertising—which remains an open challenge. Our approach, as detailed in the following sections, effectively tackles this challenge by introducing innovative techniques that not only ensure semantic relevance but also optimize for value. By addressing this critical gap, our research has the potential to revolutionize sponsored search advertising, leading to more effective ad targeting and increased user satisfaction.


%%% TF-IDF那一段删掉

%%% 参考RL-based的方法来写，把 Query Rewriting 的相关工作都重述一遍。

\subsection{Preference Alignment}
Reinforcement Learning from Human Feedback (RLHF) represents a pivotal technique for aligning LLMs with human preferences and values \cite{christiano2017deep, bai2022training, ouyang2022training}, commonly applying the Bradley-Terry model \cite{bradley1952rank} to estimate the probability of a pairwise competition between two independently evaluated instances. The conventional RLHF pipeline is structured into three distinct phases: supervised fine-tuning \cite{zhou2024lima, taori2023stanford,conover2023free,ding2023enhancing}, reward model training \cite{gao2023scaling, chen2024odin, havrilla2024glore, leike2018scalable}, and policy optimization \cite{schulman2017proximal, anthony2017thinking}. Notably, Proximal Policy Optimization (PPO) \cite{schulman2017proximal} is extensively employed during the policy optimization phase. The RLHF framework has found broad applicability across various domains, including the mitigation of toxicity \cite{amini2024direct, korbak2023pretraining}, the assurance of safety \cite{dai2023safe}, the enhancement of helpfulness \cite{wei2024long} and the improvement of model reasoning capabilities \cite{havrilla2024teaching}. Recent studies \cite{casper2023open} have underscored the challenges inherent in the entire RLHF pipeline, spanning from the collection of preference data to the training of models. Furthermore, subsequent research has illuminated potential biases introduced by RLHF, such as the generation of verbose outputs \cite{dubois2024length, singhal2023long, wang2023far}.

% In contrast, alignment methods such as Direct Preference Optimization (DPO) \cite{rafailov2024direct}, Kahneman-Tversky Optimization (KTO) \cite{ethayarajh2024kto}, Simple Preference Optimization (SimPO) \cite{meng2024simpo}, and Monolithic preference optimization (ORPO) \cite{hong2024orpo} offer more streamlined approaches by avoiding the complexities of training reward models. Notably, both SimPO and ORPO further simplify the alignment process by eliminating the need for a reference model. While these methods lack an explicit reward model, which limits their ability to sample preference pairs from the optimal policy, researchers have proposed augmenting preference data using a trained or refined Supervised Fine-Tuning (SFT) policy. This allows the policy to learn from data generated by the optimal policy. Further research has extended these approaches into iterative frameworks, continuously updating the reference model for optimization. Alignment as Reward-Guided Search (ARGS) \cite{khanov2024args} presents a novel approach by integrating alignment directly into the decoding process. However, these alignment methods generally consume significant computational resources, and when the value standards shift, they necessitate retraining the models to adapt to the new criteria.


\section{Methodology}
Traditional methods, such as standard beam search or top-k sampling, prioritize candidates with higher probabilities, often neglecting their value attributes. Although these approaches are effective in numerous contexts, they may fall short in applications where the value attributes are important. VALUE addresses this limitation by incorporating a Weighted Trie that allows for value-aware token selection. At each step of the decoding process, VALUE modifies the model's output probabilities based on the value obtained from the trie, prioritizing tokens of higher value among those already highly relevant.

%%\begin{figure*}[t]
%%    \centering
%%    \includegraphics[width=\textwidth]{pic2_VALUE_SFT_Alignment.pdf}
%%    \caption{Multi-task Fine-Tuning and Post-Training Alignment for Value-Aware Query Rewriting}
%%    \label{fig:task}
%%\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{pic3_VALUE_VALUE_Collaborative_Inference.pdf}
    \caption{Collaborative Inference of VALUE framework. When generating the next token, we obtain two output distribution from the LLM and weighted trie. We exclude tokens not present in the value distribution and adjust the probabilities of the remaining tokens. Then, we sample the next token from the modified distribution.} 
    \label{fig:col}
\end{figure*}



\subsection{System Overview}
As can be seen from Figure~\ref{fig:sysoverview}, our entire system comprises the following key modules: "LLM SFT and Alignment," "Weighted Trie Construction and Weight Momentum Update," and "Collaborative Inference." For each generated token, we obtain the probability distribution from the LLM. Concurrently, we retrieve all possible candidates under the current prefix from the weighted trie along with their respective values. By integrating these values with the probability distribution, we derive a new probability distribution and sample the next token from it.

\subsection{Weighted Trie}

\subsubsection{Definition and Construction of Weighted Trie}
Our Weighted Trie is an renovation of the traditional trie data structure. Each node in the Weighted Trie, referred to as a \textbf{WeightedTrieNode}, contains additional attributes to store weight-related information. The primary attributes of a WeightedTrieNode are:

\begin{itemize}
    \item \textbf{children}: A dictionary where the keys are integers representing tokens, and the values are child nodes of type \textit{WeightedTrieNode}.
    \item \textbf{mean}: A floating-point number representing the average weight of the node's children.
    \item \textbf{max}: A floating-point number representing the maximum weight among the node's children.
    \item \textbf{is\_word}: A boolean value indicating whether the node represents a complete word.
\end{itemize}

% \renewcommand{\shortauthors}{Zuo et al.}
\noindent \textbf{Construction of Weighted Trie:} the construction of the Weighted Trie involves initializing the root node, inserting bidwords with their corresponding values, and updating the weights of the nodes. The pseudocode in Algorithm 1 demonstrates the construction process.

\begin{algorithm}
\caption{Construct Weighted Trie}
\begin{algorithmic}
\STATE \textbf{Input:} List of pairs $(\textit{bidword}, \textit{ecpm})$
\STATE \textbf{Output:} Weighted Trie
\STATE Initialize weighted trie: $\mathit{weighted\_trie}$
\FOR{each $(\textit{bidword}, \textit{ecpm})$ \textbf{in} list}
    \STATE $\mathit{token\_ids} \leftarrow \text{tokenizer}(\textit{bidword})$
    \STATE Insert $\mathit{token\_ids}$ into the trie:
    \FOR{each $\mathit{token\_id}$ \textbf{in} $\mathit{token\_ids}$}
        \STATE Traverse or create node for $\mathit{token\_id}$
    \ENDFOR
    \STATE Set $\mathit{leaf\_node.mean} \leftarrow \textit{ecpm}$
    \STATE Set $\mathit{leaf\_node.max} \leftarrow \textit{ecpm}$
\ENDFOR
\STATE Update weights in the weighted trie using post-order traversal:
\FOR{each \textit{node} \textbf{in} post-order}
    \STATE $\mathit{node.mean} \leftarrow \frac{1}{N} \sum_{\mathit{child} \in \mathit{children}} \mathit{child.mean}$ \\
    \STATE $\mathit{node.max} \leftarrow \max_{\mathit{child} \in \mathit{children}} \mathit{child.max}$
\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsubsection{Weighted Trie Momentum Update}
Momentum updates are essential due to the frequent changes in eCPM and the evolving bidword space. The process is delineated as follows:

\paragraph{Tokenization and Node Insertion:} Each bidword undergoes tokenization, converting it into a sequence of token IDs. These IDs facilitate the insertion or updating of nodes within the trie. The update process for a node is governed by:

% \begin{equation}
% \text{leaf\_node.max}_{\text{new}} = \max(\text{eCPM}_{\text{new}}, \text{leaf\_node.max}_{\text{old}})
% \end{equation}

\begin{equation}
V_{\text{new}} = \max(eCPM_{\text{new}}, V_{\text{old}})
\end{equation}

% \begin{equation}
% % \text{leaf\_node.mean}_{\text{new}} = \alpha \cdot \text{leaf\_node.mean}_{\text{old}} + \beta \cdot \text{eCPM}_{\text{new}}
% \end{equation}

\begin{equation}
W_{\text{new}} = \alpha \cdot eCPM_{\text{new}} \ + \  \beta \cdot W_{\text{old}}
\end{equation}

where $\alpha$ and $\beta$ are hyperparameters that control the update rate, with the constraint $\alpha + \beta = 1$. V is the max attribute of the node and W is the mean attribute of the node. If the node is absent, it is initialized as:

\begin{equation}
% \text{leaf\_node.mean} = \text{eCPM}_{\text{new}}, \quad \text{leaf\_node.max} = \text{eCPM}_{\text{new}}
W_{\text{new}} = eCPM_{\text{new}},\  V_{\text{new}} = eCPM_{\text{new}}
\end{equation}

\paragraph{Bottom-up Update:} Following node insertion, a bottom-up iteration is executed to update all intermediate nodes, ensuring that the entire trie structure accurately reflects the most recent eCPM values.

This method enables the trie to dynamically adjust to variations in the bidword space and eCPM values over time, preserving the framework's efficacy without necessitating frequent retraining, maintaining stability and performance in a dynamic e-commerce environment.



%%% 改：
%%% methodology部分删除冗余描述。
%%% 3.2和3.5合并，按照算法晚自习行文来写。
%%% 大图包括所有模块，整个前向推理的逻辑。
%%% 3.2 树的创建与更新，把树的内容美化放过来 、  3.3 LLM的训练、   3.4 LLM的对齐、  3.5 推理过程、 3.6 部署，不暴露top500那么多细节。

\subsection{Collaborative Inference}
As shown in Figure~\ref{fig:col}, when generating the \( t \)-th token, we insert the query into the prompt template and concatenate it with the bidword prefix to form the input of LLM to get the output probabilities. We use generated bidword prefix to retrieve all child nodes from the trie. The value of these child nodes is computed using the following formula:
\begin{equation}
\mathcal{V}_k = \alpha \cdot \text{mean}_k + \beta \cdot \text{max}_k 
\end{equation}
Here, \(\alpha\) and \(\beta\) are tunable hyperparameters, both set to 0.5, allowing us to prioritize paths with higher average values or to explore bidwords with the maximum potential value, depending on our specific needs.

We then apply the softmax function to these values to obtain the normalized value \(\hat{\mathcal{V}}_k\). 
Next, we mask out all probabilities in the LLM-generated distribution that are not children of the current node. For the remaining probabilities, we adjust them by weighting with the normalized value \(\hat{\mathcal{V}}_k\) and a depth-dependent factor \(\theta\). The adjusted probability for each token \( k \) is given by:

\begin{equation}
p(k \mid x_{<t}) = p_{\text{LLM}}(k \mid x_{<t}) \cdot (1 + \mathcal{V}_k \cdot \theta)
\end{equation}

where \(\theta\) is designed to vary with the depth of the trie, playing a crucial role in balancing relevance and value in our model's output. At shallower levels of the trie, \(\theta\) is set to smaller values to minimize the influence of value on the output, thereby preventing the model from favoring high-value but less relevant options. As we delve deeper into the trie, \(\theta\) increases, allowing us to prioritize higher-value candidates from a pool of already highly relevant choices. Consequently, we derive a modified probability distribution \(p(k \mid x_{<t})\), from which the next token is sampled as usual.

\subsection{Multi-task Fine-Tuning}
As no open-source LLMs are trained to perform query rewriting for the e-commerce scenario, they lack an awareness of the bidword space. Consequently, they are unable to effectively generate bidwords. To address this, we need to perform supervised fine-tuning (SFT) on these models to inject domain-specific knowledge into the model and enhancing their capability to understand and effectively rewrite e-commerce queries for improved user experience and search accuracy. 

\noindent \textbf{Query Rewriting Dataset:}
To construct our training dataset, we initially obtain query-bidword pairs from online logs. Specifically, when a user searches for a query \( x \) on our e-commerce platform, the system logs the corresponding bidwords \( Y = \{y^1, y^2, \ldots, y^n\} \) that are associated with the advertisements the user interacts with. From this list, we select the top-ranked bidword \( y^1 \) as the gold standard candidate to construct our initial dataset \( D \) with \( N \) samples:
\begin{equation}
D = \{(x_i, y_{i}^1)\}_{i=1}^{N} \quad \text{where} \quad x_i \sim p(x)
\end{equation}
where \( p(x) \) denotes the query distribution in our search engine.


We apply a relevance filter to \( D \) using a representation model fine-tuned from an open-source embedding model. The relevance score \( \text{rel}(x, y) \) is computed as the cosine similarity between the embeddings of \( x \) and \( y \):

\begin{equation}
\text{rel}(x, y) = \frac{\text{emb}(x) \cdot \text{emb}(y)}{\|\text{emb}(x)\| \|\text{emb}(y)\|}
\end{equation}

We then filter the dataset \( D \) to obtain \( D_r \) ensuring that the relevance score exceeds a threshold \( \tau_{\text{rel}} \):

\begin{equation}
D_r = \{(x_i, y_i) \mid (x_i, y_i) \in D, \, \text{rel}(x_i, y_i) > \tau_{\text{rel}}\}
\end{equation}

For head queries, the number of corresponding bidwords can be in the tens of thousands. To ensure class balance, we truncate the bidwords for each query based on their value.

\noindent \textbf{Fine-tuning Tasks:}
We employ two types of supervised fine-tuning (SFT) tasks to enhance the model's understanding and performance in query rewriting for e-commerce.

\textbf{Task 1: \texttt{<prompt + query + bidword>}} \\
This task aims to enhance the model's spatial awareness of the bidword space and the association between queries and bidwords. Subsequently, the model will develop fundamental rewriting skills.

% By training the model with pairs of queries and their corresponding bidwords, we enable the LLM to develop a robust understanding of the bidword space and how different bidwords relate to specific queries. This foundational knowledge is crucial for generating relevant and effective bidwords during the inference phase.

\textbf{Task 2: \texttt{<prompt + query + bidword\_list>}} \\
In this task, the model is trained with lists of bidwords sorted in descending order based on their eCPM values. The bidword\_list for each query is structured as follows:

\[    
\text{bidword\_list} = [\text{bidword}_1, \text{bidword}_2, \ldots, \text{bidword}_k]
\]

% We have validate that this ordering allows the model to learn value preferences, as higher eCPM bidwords are prioritized. During beam search inference, the model will assign higher values to bidwords that appear earlier in the list compared to model trained with only task 1. By incorporating these two SFT tasks, we ensure that the model not only understands the bidword space and the relevance of bidwords to queries but also learns to prioritize bidwords based on their economic value.

\subsection{LLM Post-training Alignment with Weighted Trie}
Although conventional RLHF approaches can effectively integrate with our framework, we propose a more tailored solution. Our Weighted DPO (WDPO) approach directly incorporates the economic value of bidwords into the alignment process, leveraging the eCPM values for bidwords. It is important to note that WDPO is \textbf{optional} as the performance improvements it offers are not as significant relative to computational cost when compared to SFT and the varying nature of bidwords. Therefore, WDPO can be considered when resources permit, providing an additional layer of refinement to the training process.

The alignment process proceeds as follows:
\begin{enumerate}
\item Identify relevant bidwords from online logs and calculate their corresponding eCPM values for each query in our dataset.
\item Randomly sample two bidwords $(y_w, y_l)$ with differing eCPM values, ensuring:
\begin{equation}
|eCPM(y_w) - eCPM(y_l)| > \tau
\end{equation}
where $\tau$ is a predefined threshold, $y_w$ represents the bidword with higher eCPM, and $y_l$ the lower.
\item Process the query and selected bidwords through both policy and reference models to obtain probabilities:
$\pi_\theta(y_w|x)$, $\pi_\theta(y_l|x)$, $\pi_{ref}(y_w|x)$, and $\pi_{ref}(y_l|x)$.
\item Apply the modified DPO loss function:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{WDPO}(\pi_\theta) = -\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}} \bigg[ & \log \sigma\bigg(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} \\
        & - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\bigg) \cdot w\bigg]
    \end{aligned}
\end{equation}
where $w$ is a weight function based on KL divergence:
\begin{equation}
w = \exp(-KL([\pi_\theta(y_w|x), \pi_\theta(y_l|x)] \parallel [P(y_w), P(y_l)]))
\end{equation}
and $P(y_i)$ represents normalized eCPM values:
\begin{equation}
P(y_i) = \frac{eCPM(y_i)}{\sum_{j \in \{w,l\}} eCPM(y_j)}
\end{equation}
\end{enumerate}

% This Weighted DPO (WDPO) approach overcomes the limitations inherent in traditional RLHF methods by integrating the eCPM value as a reward signal directly within the loss function. This innovation obviates the necessity for a separate reward model or human labeling, thereby diminishing computational overhead and simplifying the training process. The post-training alignment phase offers a powerful mechanism to further refine the model's performance, enhancing its ability to generate high-value, contextually relevant bidwords for e-commerce query rewriting tasks.



\subsection{Online Depolyment}
In the context of deploying LLMs for online serving, the balance between computational efficiency and latency is paramount. Our approach leverages the Qwen2-7B model for offline inference, targeting head queries that constitute 50\% of the page views (PV) in our system. This offline process generates the top 500 results per query, which are subsequently cached for rapid access. For mid-tail and long-tail queries that do not benefit from the cached results, we employ the Qwen2-1.5B model in an online serving capacity. This model is optimized to deliver a latency of 50 milliseconds, efficiently producing the top 50 results per query. In the future, we plan to integrate user behavior sequence data into our online serving framework to enhance personalized recommendation systems and gradually remove the offline part.


% \subsubsection{Performance Analysis}

% We use Qwen2-7B as the base model for training and collaborative reasoning. However, the reasoning part needs to use LLM and WT collaborative path reasoning to increase a certain amount of calculation, and the goal is to retrieve Topk through beam search. The performance is unacceptable in the initial brute force method. When batch size is 1, the reasoning time of a single token will surge by more than 30\%. At the same time, as K increases, the latency also increases nonlinearly, which will become unacceptable for tens of millions of query reasoning.

% So we 1) integrate the vllm framework, 2) use tensor to replace the original float in the calculation of nodes in the tree, and 3) heap optimize the BeamHypotheses class.

% In the Top500 reasoning task, it is nearly 20 times improved, making it possible for tasks with tens of millions of reasoning to be updated at the daily level.

% On the other hand, in terms of WT update, we are inspired by the plug-and-play language model (PPLM), which calculates the output score based on the attribute model, back-propagates the gradient to adjust the model, and then recalculates the token probability distribution. Another recent study, ARGS, uses a reward model to score each possible sentence, which may consume a lot of resources. Compared with the alignment task, in our scenario, each bidding word has an objective value, which can be regarded as the business preference for alignment. By storing these preferences and utilizing them at each node of the Trie, we can effectively create a reward model with a time complexity (compared to ARGS), namely the update of WT.


% \subsubsection{Deployment Detail}
% In the launch plan of VALUE Phase 1, we adopt a divide-and-conquer plan -

% 1. Sort the request query in descending order according to the display frequency, and regard the top 10 million level queries covering nearly 95\% of pv as the head query, and the rest as the tail.
% 2. Since the head query is relatively stable and does not change much on a daily basis, we can use offline reasoning and cache the results, and serve them in real time online in the form of igraph, which has very good "reasoning" performance.
% 3. The tail query changes greatly on a daily basis and cannot be covered as much as possible through caching. We use methods such as reducing Topk and reducing base parameters to perform llm real-time reasoning. This part has not been launched yet, but due to the complexity and variability of the tail query, we have still achieved certain results in the experimental part. In the future, we will gradually improve the personalized capabilities in VALUE 2, launch it with real-time reasoning capabilities, and strive to cover more head queries.

\section{Experiments} 
\subsection{Datasets}
\noindent \textbf{Training Dataset:} For multi-task fine-tuning, we extracted 150 billion records from 30 days of online logs, followed by several rounds of data cleaning. Initially, we employed manual observation and regular expressions to filter out the majority of noisy queries. In the subsequent screening phase, we calculated the click-through rate (CTR) based on page views (PV) and clicks, retaining only those $\langle\texttt{query},\texttt{bidword}\rangle$ pairs associated with purchase behavior that meet the click-through rate requirements. We then applied a relevance model for further filtering. This relevance model is a fine-tuned version of BAAI/bge-large-zh-v1.5\cite{xiao2024c}, optimized for e-commerce scenarios. We truncated the number of bidwords for each query, retaining a maximum of 50 bidwords per query. After third round of filtering, we were left with 110 million records.



\noindent \textbf{Test Dataset:} Our test dataset was constructed by randomly sampling 30,000 queries from online logs. These queries were stratified into three categories: 40\% head queries, 40\% torso queries, and 20\% tail queries. 

% We manually annotated the relevance of all generated bidwords for 3,000 of these queries, assigning binary labels (1 for relevant, 0 for irrelevant). For the evaluation of QRCHR, we utilized the next day's online logs to find all click records related to these 30,000 queries and calculated the hit rate. The QRECPM value was also computed using the next day's data.

\subsection{Evaluation Metrics}
\noindent \textbf{Offline Metrics:} 
To comprehensively evaluate our query rewrite approach, we utilize several offline metrics:

\begin{enumerate}


\item \textbf{Hit Rate:} A precision-oriented metric measuring the alignment between generated rewrites and user-clicked bidwords. For a query set $\mathcal{Q}$, it is defined as:
\begin{equation}
    \text{hitrate@}K = \frac{\sum_{q \in \mathcal{Q}} | \mathcal{R}_q^K \cap \mathcal{C}_q |}{\sum_{q \in \mathcal{Q}} | \mathcal{C}_q |}
\end{equation}
where $\mathcal{R}_q^K$ represents the top-$K$ rewrite candidates for query $q$, and $\mathcal{C}_q$ denotes the set of clicked bidwords for $q$. The numerator counts total hits across all queries, while the denominator normalizes by the total clickable bidwords.


\item \textbf{Relevance Score:} A semantic alignment measure between rewritten queries and the original query intent, typically quantified via pretrained language model similarity (e.g., BERTScore).



    

\item \textbf{Spearman Rank Correlation ($\rho$):} A non-parametric measure of rank consistency between model-generated and ground-truth bidword rankings. For $n$ observations:
\begin{equation}
    \rho = 1 - \frac{6 \sum_{i=1}^n d_i^2}{n(n^2 - 1)}
\end{equation}
where $d_i$ is the rank difference for the $i$-th bidword pair. The coefficient $\rho \in [-1, 1]$ reflects the monotonicity of the predicted rankings.

\end{enumerate}



\noindent \textbf{Online Metrics:} To evaluate the model's performance, we use three key metrics: \textbf{cost}, which is the total advertiser expenditure for clicks within a specific traffic segment, \textbf{revenue per mille (RPM)}, which measures the revenue generated per thousand ad impressions and \textbf{Page View Relevance (PV rele)}, 



\subsection{Implementation Details}

\noindent \textbf{Multi-task Fine-tuning:} We fine-tuned the Qwen2-7B model using a learning rate of 1e-5 and a cosine learning rate scheduler. The process was optimized with AdamW ($\beta_1 = 0.9$, $\beta_2 = 0.999$) and a weight decay of 0.001. Gradient accumulation was set to 4. We used a batch size of 128 and applied the DeepSpeed ZeRO Stage 2 parallelism strategy with bfloat16 precision. The training was performed on 32 NVIDIA H20 GPUs for a total of 12 hours. 

\noindent \textbf{Post-training Alignment:} For the post-training alignment phase, we use our fine-tuned Qwen2-7B model. The alignment phrase was carried out with a learning rate of 1e-6 and a batch size of 64. We set the $\beta$ parameter to 0.1. The training was executed on 16 NVIDIA H20 GPUs for a duration of 3 hours. 


\subsection{Offline Experiments}

\begin{table*}[t]
\centering
\caption{Comparative Performance Analysis of Various Models. The eCPM metric is used as an approximate estimation of reward, serving as a reference for the effectiveness of weighted trie collaborative inference.}
\label{table:performance}
\begin{tabular}{lccccccc}
\toprule
Model & hitrate@50 & hitrate@500 & Spearman's $\rho$ & Relevance & OOVR & eCPM \\ 
\midrule
OnlineModel & 59.93\% & 85.41\% & 0.02 & 68.15\% & 0\% & 8676 \\
DeepNB & 44.16\% & 79.82\% & 0.13 & 46.26\% & 0\% & 12852 \\
BEQUE & 39.26\% & 62.85\% & 0.08 & 74.37\% & 38.41\% & 7582 \\
raw Qwen2-7B & 12.88\% & 18.37\% & -0.03 & 59.81\% & 68.63\% & 2937 \\
SFT Qwen2-7B & 38.18\% & 63.71\% & 0.02 & 76.33\% & 34.85\% & 7439 \\
SFT+Trie & 59.11\% & 86.38\% & -0.01 & 75.31\% & 0\% & 7270 \\
SFT+Trie+DPO & 38.61\% & 62.19\% & 0.05 & 69.48\% & 0\% & 28391 \\
\textbf{SFT+WeightedTrie(\textit{VALUE})} & 60.37\% & 91.52\% & 0.46 & 74.55\% & 0\% & 59803 \\
SFT+WeightedTrie+DPO & 50.35\% & 77.73\% & 0.23 & 70.18\% & 0\% & 38301 \\
\textbf{SFT+WeightedTrie+WDPO(\textit{VALUE+WDPO})} & \underline{62.67\%} & \underline{91.91\%} & \underline{0.56} & 73.49\% & 0\% & \underline{63775} \\
\bottomrule
\end{tabular}
\end{table*}





In this section, we present the results of offline experiments conducted to evaluate the performance of our proposed models against several baseline models. The comparative performance analysis is summarized in Table \ref{table:performance}. We compared our models with the following baselines: first, the \textbf{Online Model}, which represents the current state-of-the-art multi-channel recall system deployed online; second, the \textbf{DeepNB} model \cite{chen2020rpm}, which employs a vector-based approach focusing solely on value; and finally, the \textbf{BEQUE} model \cite{taobaowww}, which utilizes a LLM fine-tuned with multi-task SFT and PRO alignment, representing the latest query rewriting approach. The performance metrics used for evaluation include hitrate, eCPM, Spearman's $\rho$, Relevance, and OOVR(Out-Of-Vocabulary Rate. To a certain extent, it is also equivalent to the hallucination rate of LLMs.). These metrics provide a comprehensive assessment of the models' effectiveness in terms of query rewriting, relevance, and OOVR. 


\subsubsection{Comprehensive Evaluation of the VALUE Framework}
The VALUE framework we propose is designed to optimize online RPM revenue by utilizing offline eCPM values as a reference for learning effectiveness, rather than as direct revenue indicators. Our subsequent online experiments have demonstrated the validity and rationality of this modeling approach.The results presented in Table \ref{table:performance} clearly illustrate the significant effectiveness of our VALUE framework. Notably, the eCPM metric shows a remarkable improvement, serving as a strong indicator of offline learning success. Compared to the model trained with Supervised Fine-Tuning (SFT) on the same dataset, our VALUE model achieves a 722.6\% increase in eCPM. Furthermore, when compared to the current online service model, our framework exhibits a 589.3\% enhancement in eCPM. Importantly, the relevance metric for our VALUE model does not decline when compared to the SFT model; in fact, it demonstrates an improvement of 6.4 percentage points over the online service model. This observation substantiates our earlier hypothesis that relevance and value are decoupled in this domain, allowing us to reorder the generation sequence of bidwords to enhance both metrics simultaneously without trade-offs.

Compared to the DeepNB model, which focuses on efficiency-oriented rewriting using click data to maximize value, our VALUE framework demonstrates superior performance across multiple dimensions. While the DeepNB model effectively maximizes value, it suffers from poor relevance metrics. In contrast, our VALUE framework not only maintains high relevance but also significantly outperforms DeepNB in terms of value attributes. Specifically, the eCPM metric for our VALUE model is substantially higher than that of DeepNB. When compared to the BEQUE model, our framework achieves comparable relevance metrics but excels in generating high-value bidwords. This is evidenced by the eCPM metric, where VALUE achieves a value of 59803, far surpassing BEQUE's 7582. Additionally, our VALUE framework demonstrates a superior hit rate, with hitrate@500 reaching 91.52\%, compared to BEQUE's 62.85\%. This significant improvement in hit rate underscores the effectiveness of our approach in generating bidwords that are not only relevant but also highly likely to be clicked by users. It is noteworthy that the BEQUE method suffers from a high Out-Of-Vocabulary Rate (OOVR) of 38.41\%, which adversely affects its hit rate.

Moreover, our VALUE framework outperforms the online service model across all evaluated metrics, further validating the efficacy of our approach. The comprehensive improvements in eCPM, relevance, and other metrics underscore the robustness and effectiveness of our method in generating high-value and contextually relevant bidwords for e-commerce query rewriting tasks.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{theta_tradeoff.png}
    \caption{Trade-off between RPM and PV rele through $\theta$.}
    \Description{}
    \label{fig:overview}
\end{figure}




\begin{table}[t]
\centering
\caption{Comparative Performance Analysis of Hyperparameter.}
\label{table:thetaperformance}
\begin{tabular}{lcc}
\toprule
Hyperparameter($\theta$) & hitrate@500 & Relevance \\ 
\midrule
$\theta_1$ & 86.38\% & 75.31\% \\
$\theta_2$ & 89.65\% & 75.04\% \\
$\theta_3$ & 91.52\% & 74.55\% \\
$\theta_4$ & 88.63\% & 69.85\% \\
$\theta_5$ & 73.71\% & 60.53\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Enhanced Performance in Fine-Grained Value Alignment}
Compared to traditional alignment methods such as DPO and Preference Ranking Optimization (PRO) used in the BEQUE model, our VALUE framework demonstrates a significant lead in the Spearman rank correlation metric. This indicates that conventional value alignment methods struggle with fine-grained value alignment tasks. Specifically, the Spearman's $\rho$ for our VALUE model is 0.46, which is substantially higher than the 0.05 achieved by DPO model and 0.08 by BEQUE. 

Additionally, our framework seamlessly integrates with conventional model alignment methods. When we combined our proposed WDPO with the VALUE framework, we achieved the highest Spearman's $\rho$ of 0.56. 



DPO suffers from significant overfitting issues, which severely impact its performance in terms of hit rate and relevance. In our experiments, the SFT + DPO + Trie model exhibited a hitrate@500 of only 62.19\% and a relevance score of 69.48\%, both of which are considerably lower than those achieved by our \textbf{VALUE} framework. DPO faces inherent challenges when dealing with positive and negative samples that are lexically very similar. The overfitting problem in DPO arises because the method tends to excessively optimize for the specific preference pairs seen during training, leading to a decline in generalization capability. DPO enhances the model's belief in specific events. Therefore, when we use DPO in conjunction with our VALUE framework, the improvement in eCPM is not significant. Specifically, the eCPM for the VALUE + DPO model is 38301, which is only marginally higher than the 28391 achieved by the SFT + DPO + WeightedTrie model.

Furthermore, the improvement in the Spearman rank correlation metric with DPO is not significant. The SFT + DPO + Trie model achieved a Spearman's $\rho$ of only 0.05, which is marginally better than the baseline but far from satisfactory. This limited improvement underscores the inherent limitations of DPO in handling fine-grained value alignment tasks.




% \subsubsection{Summary}
% To sum up, our experimental results unequivocally demonstrate the superiority of the VALUE framework in query rewriting, achieving excellent preference alignment while maintaining relevance. The integration of VALUE with WDPO presents a robust solution for fine-grained value alignment in query rewriting tasks.


\subsection{Exploration of Hyperparameter Configurations}
In this section, we provide a concise analysis of the impact of the hyperparameter $\theta$ on system performance. As shown in Table \ref{table:thetaperformance}, an increase in $\theta$ results in a decrease in relevance. Figure \ref{fig:overview} visually represents the dynamics of this trade-off. Initially, optimizing for value leads to an increase in RPM; however, when relevance becomes significantly low, RPM ultimately decreases. This underscores the critical importance of meticulous hyperparameter tuning to ensure alignment with specific application objectives. For a detailed exploration of the hyperparameter $\theta$, please refer to Appendix \ref{appendix:thetadetails}.




\subsection{Scaling Laws in Model Performance}
In our investigation of model performance across varying scales, we conducted experiments using models ranging from 0.5 billion to 65 billion parameters. Our findings consistently demonstrate that larger models exhibit superior performance across all evaluated metrics. Specifically, as model size increases, there is a notable improvement in hitrate, indicating enhanced capability in generating high-value and relevant bidwords. This trend aligns with the theoretical expectations of scaling laws, which suggest that larger models can capture more complex patterns and nuances in data, thereby improving overall effectiveness.


\begin{table}[t]
\centering
\caption{Comparative Performance Analysis of Base Model Size.}
\label{table:basemodelsizeperformance}
\begin{tabular}{lcc}
\toprule
Model Size & hitrate@500 & Relevance \\ 
\midrule
0.5B & 68.32\% & 61.85\% \\
1.5B & 87.49\% & 71.29\% \\
7B & 91.52\% & 74.55\% \\
14B & 92.69\% & 76.68\% \\
65B & 93.98\% & 77.99\% \\
\bottomrule
\end{tabular}
\end{table}







\subsection{Online Experiments}

To further validate the effectiveness of our proposed VALUE framework, we conducted 14-day online A/B tests on our system, focusing on cost and RPM. Results are summarized in Table \ref{table:online} and Table \ref{table:length}.

\begin{table}[t]
\centering
\caption{Online A/B Test Results}
\label{table:online}
\begin{tabular}{lccc}
\toprule
Query Type & Cost & RPM & PV rele \\
\midrule
VALUE  all queries &  +1.53\% & +1.64\% & +0.32pt \\
VALUE  head queries & +0.97\% & +1.03\% & +0.10pt \\
VALUE torso queries & +1.68\% & +1.74\% & +0.33pt \\
VALUE tail queries & +2.35\% & +2.66\%  & +0.75pt \\
% VALUE($\theta_3$)  all queries && -0.01\% \\
% VALUE($\theta_3$)  head queries & +0.97\% & +1.03\% & +0.01\% \\
% VALUE($\theta_3$)  torso queries & +1.68\% & +1.74\% & -0.01\% \\
% VALUE($\theta_3$)  tail queries & +2.35\% & +2.66\% & -0.02\% \\
\bottomrule
\end{tabular}
\end{table}

The online A/B tests corroborate the offline findings, consistently demonstrating improvements in both cost and RPM across all query types. Our framework exhibits particularly notable enhancements for torso and tail queries, which are typically characterized by greater variability and complexity. This underscores the framework's robust generalization capabilities. As illustrated in Table \ref{table:length}, our model excels in discerning the intent behind very long and complex queries. The observed improvements in RPM and relevance for medium to super long queries underscore the model's proficiency in managing intricate and challenging query structures.


% \subsubsection{Query Layering Effect Analysis}

% We conducted query hierarchical analysis on 2 dimensions, using the online effect RPMi indicator (the better the increment of this value, the better the increment of eCPM, hitrate and other indicators under this layer) and the display relevance indicator (because there are many relevance threshold logics in the online link, its increment is not as significant as the offline increment, but the larger the increment of this value, the larger the increment of Query Rewriting Relevance under this layer) to measure the performance of each layer of query.



% 1. Query frequency heat stratification, we divide the query into 3 categories according to the frequency share: head, middle and tail. 


% \begin{table}[t]
% \centering
% \caption{Comparative Performance Analysis of Query Frequency Layer.}
% \label{table:basemodelsizeperformance}
% \begin{tabular}{lccccccc}
% \toprule
% Query Frequency & $\Delta$RPMi & $\Delta$Relevance \\ 
% \midrule
% Head Query & +0.52\% & +0.24\% \\
% Middle Query & +1.14\% & +0.43\% \\
% Tail Query & +1.28\% & +0.56\% \\
% \bottomrule
% \end{tabular}
% \end{table}



% 2. Query length stratification, we divide the query into short query (len < 5), medium query (5 $\leq$ len < 10), long query (10 $\leq$ len < 20), and super long query (len $\geq$ 20) according to the length.

\begin{table}[t]
\centering
\caption{Comparative Performance Analysis of Query Length Layer.}
\label{table:length}
\begin{tabular}{lccccccc}
\toprule
Query Length& Cost & RPM & Relevance \\ 
\midrule
Short Query &+0.41\% & +0.56\% & +0.02pt \\
Medium Query &+0.88\% & +0.98\% & +0.30pt  \\
Long Query &+1.26\% &+1.56\% & +0.79pt  \\
Super Long Query&+1.17\% & +1.55\% & +0.54pt  \\
\bottomrule
\end{tabular}
\end{table}





% \subsection{Ablation Study}

% \subsubsection{Scaling Law}
% \begin{table}[h!]
% \centering
% \begin{tabular}{lcccc}
% \toprule
% Model & QRCHR @500 & QRECPM @500 & Relevance @500 \\
% \midrule
% Qwen2.5-0.5B & 9.51\% & 12013 & 59.38\% \\
% Qwen2.5-1.5B & 22.03\% & 18467 & 61.16\% \\
% Qwen2.5-3B & 41.82\% & 35668 & 64.98\% \\
% Qwen2.5-7B & 78.66\% & 53182 & 69.12\% \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of Different Model Sizes}
% \label{tab:scaling}
% \end{table}

% To elucidate the impact of scaling laws on our query rewriting task, we conducted an ablation study focusing on the Qwen2.5 series models. This experiment utilized a relatively small training set and the same test set as employed in the offline experiments. All models were implemented using the VALUE framework and utilized the $\theta_2$ weighted trie configuration. We evaluated four different model sizes and observed that, with the same training dataset, larger models consistently achieved improvements across all metrics.

% As illustrated in Table \ref{tab:scaling}, the QRCHR @500 metric increased from 9.51\% for the smallest model (Qwen2.5-0.5B) to 78.66\% for the largest model (Qwen2.5-7B). Similarly, the QRECPM @500 metric exhibited a significant rise from 12013 to 53182. These results underscore that increasing the model size leads to enhanced performance in query rewriting tasks, thereby demonstrating the scaling law.

% \subsubsection{Comparison of Different LLMs}
% \begin{table}[h!]
% \centering
% \begin{tabular}{lcccc}
% \toprule
% Model & QRCHR @500 & QRECPM @500 & Relevance @500 \\
% \midrule
% Baichuan2-7B & 75.51\% & 52113 & 70.11\% \\
% ChatGLM3-6B & 74.18 \% & 46153 & 65.86\% \\
% Qwen2-7B & 77.82\% & 54381 & 68.47\% \\
% Qwen2.5-7B & 78.66\% & 53182 & 69.12\% \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of Different LLMs}
% \label{tab:llm}
% \end{table}

% In this section, we present the comparison results of different LLMs as shown in Table \ref{tab:llm}. The models evaluated include Baichuan2-7B\cite{baichuan2023}, ChatGLM3-6B\cite{du2021glm}, Qwen2-7B, and Qwen2.5-7B. The variability in the performance of these models can be attributed to differences in their pre-training data and model architectures. While each model exhibits unique strengths, it is evident that Qwen2.5-7B and Qwen2-7B excel in query reformulation tasks, as indicated by their high QRCHR @500 scores. This suggests that these models are particularly effective in generating high-quality reformulated queries, which can be crucial for applications requiring precise query handling. Baichuan2-7B, although slightly lower in QRCHR @500, demonstrates a strong relevance score, making it a viable option for tasks where maintaining the contextual integrity of the input queries is essential. The differences in performance among these models highlight the significant impact that pre-training data and model architecture can have on downstream tasks. Selecting an appropriate pre-trained model is crucial for enhancing performance in query reformulation tasks.

\subsection{Case Study}  
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{pic4_value_case.pdf}
    \caption{Case Study of VALUE Framework}
    \Description{}
    \label{fig:case}
\end{figure}
In this section, as illustrated in Figure \ref{fig:case}, we examine a specific case involving the user query "fatty's drink." Previously, without the use of LLMs, our system could only match the query at the keyword level, specifically "drink." With the integration of LLMs, we gained a deeper understanding of the user's intent, identifying a preference for low-sugar, low-calorie beverages. Additionally, by employing a weighted trie, we ensured that the advertisements presented were not only closely aligned with the user's intent but also of high value.

\section{Conclusion}
In this study, we introduced \textbf{VALUE}, an innovative framework designed to enhance query rewriting in sponsored search advertising by integrating value-awareness into the generation process. Our approach addresses the significant challenge of producing rewrites that are not only semantically relevant but also economically valuable—a gap that existing methods often fail to bridge. By embedding value information directly into the LLM's decoding process through a novel weighted trie structure, we effectively steer generation towards high-value bidwords without substantially compromising relevance.

Comprehensive experiments underscore the effectiveness of \textbf{VALUE} in enhancing the economic attributes of bidword generation. Offline evaluations demonstrate a significant increase in eCPM, outperforming traditional methods while preserving high relevance scores. The improvement in Spearman rank correlation confirms the superior ranking quality achieved through our fine-grained value alignment. Furthermore, online A/B testing validates the real-world efficacy of \textbf{VALUE}, highlighting notable improvements in key performance indicators such as cost and RPM. Our framework's versatility also suggests potential applications beyond sponsored search advertising, wherever the generation space is fixed and reward signals are present.
% While this paper demonstrates its effectiveness in the field of search advertising, the framework holds promise for broader applications across various domains requiring optimized generation within constrained spaces. 








\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-sigconf}

\begin{appendices}
\section{Hyperparameter Details}
\label{appendix:thetadetails}

In this appendix, we delve into the specifics of the hyperparameter $\theta_d$, which is designed to vary with the depth of the trie. The selection of $\theta_d$ is critical in modulating the influence of value relative to relevance as the trie depth changes. The configurations explored in our research are as follows:

\begin{itemize}
    \item \textbf{$\theta_1$}: [0, 0, 0, 0, 0, 0, 0, …], indicating that the weighted trie is ineffective and the original LLM reasoning result is used.
    \item \textbf{$\theta_2$}: [1, 1, 1, 1, 1, 1, 1, …], implying equal weighting of relevance and value at each depth level.
    \item \textbf{$\theta_3$}: [1, 2, 3, 4, 5, 6, 7, …], where the weight increases linearly with depth, emphasizing value as the trie deepens.
    \item \textbf{$\theta_4$}: [1, 2, 4, 8, 16, 32, 64, …], representing an exponential increase in weight with depth, further prioritizing value.
    \item \textbf{$\theta_5$}: [2, 4, 8, 16, 32, 64, 128, …], offering a more pronounced exponential tendency compared to $\theta_4$.
\end{itemize}

These configurations allow for nuanced control over the relevance-value trade-off, enabling the system to adapt its focus based on the hierarchical structure of the weighted trie. The choice of $\theta_d$ thus plays a pivotal role in optimizing the overall performance of the inference system.
% Add detailed parameter tables or descriptions here

\end{appendices}



\end{document}