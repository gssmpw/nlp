\section{Related Works}
\subsection{Query Rewriting}
The process of query rewriting, often termed query expansion or reformulation, is essential in enhancing e-commerce search technologies. It profoundly affects the shopping experience of users and the financial performance of e-commerce platforms. This approach can be generally classified into two classes: discriminative methods and generative methods.

\textbf{Discriminative methods} approach query rewriting as a retrieval task. Pseudo-relevance feedback methods \cite{xu,t6,t31,t40} enhance queries by identifying expansion terms from the top-ranked documents of an initial query, combining global corpus analysis with local feedback. Although these methods effectively address word mismatches, they are susceptible to semantic drift due to noisy or irrelevant top results. To address these challenges, researchers suggest utilizing a well-constructed thesaurus as a candidate set for query rewriting. \cite{t5,t22} However, they caution that the effectiveness of these methods is highly dependent on the thesaurus's quality. If the thesaurus is inadequate, it may lead to query semantic drift, where the intended meaning of the query is compromised. Additionally, some approaches \cite{t3,t8,t18} focus on generating candidate rewrites by leveraging search logs, using similar terms from users' search histories as extensions. However, these search logs inherently display a bias towards popular queries due to the Matthew effect. Consequently, the training data derived from this method may fall short in optimizing for less frequently searched long-tail queries.



\textbf{Generative methods}, on the other hand, focus on the direct transformation of queries into keywords \cite{baiduwww, lee2018rare, qi2020prophetnet, mohankumar2023unified}. These methods involve transforming the input query directly into multiple bidwords, utilizing auto-regressive language models combined with trie-based decoding techniques to restrict the output space during inference. However, while previous methods ensure semantic relevance between input and output, they fall short in addressing value—a critical attribute in sponsored search advertising—which remains an open challenge. Our approach, as detailed in the following sections, effectively tackles this challenge by introducing innovative techniques that not only ensure semantic relevance but also optimize for value. By addressing this critical gap, our research has the potential to revolutionize sponsored search advertising, leading to more effective ad targeting and increased user satisfaction.


%%% TF-IDF那一段删掉

%%% 参考RL-based的方法来写，把 Query Rewriting 的相关工作都重述一遍。

\subsection{Preference Alignment}
Reinforcement Learning from Human Feedback (RLHF) represents a pivotal technique for aligning LLMs with human preferences and values \cite{christiano2017deep, bai2022training, ouyang2022training}, commonly applying the Bradley-Terry model \cite{bradley1952rank} to estimate the probability of a pairwise competition between two independently evaluated instances. The conventional RLHF pipeline is structured into three distinct phases: supervised fine-tuning \cite{zhou2024lima, taori2023stanford,conover2023free,ding2023enhancing}, reward model training \cite{gao2023scaling, chen2024odin, havrilla2024glore, leike2018scalable}, and policy optimization \cite{schulman2017proximal, anthony2017thinking}. Notably, Proximal Policy Optimization (PPO) \cite{schulman2017proximal} is extensively employed during the policy optimization phase. The RLHF framework has found broad applicability across various domains, including the mitigation of toxicity \cite{amini2024direct, korbak2023pretraining}, the assurance of safety \cite{dai2023safe}, the enhancement of helpfulness \cite{wei2024long} and the improvement of model reasoning capabilities \cite{havrilla2024teaching}. Recent studies \cite{casper2023open} have underscored the challenges inherent in the entire RLHF pipeline, spanning from the collection of preference data to the training of models. Furthermore, subsequent research has illuminated potential biases introduced by RLHF, such as the generation of verbose outputs \cite{dubois2024length, singhal2023long, wang2023far}.

% In contrast, alignment methods such as Direct Preference Optimization (DPO) \cite{rafailov2024direct}, Kahneman-Tversky Optimization (KTO) \cite{ethayarajh2024kto}, Simple Preference Optimization (SimPO) \cite{meng2024simpo}, and Monolithic preference optimization (ORPO) \cite{hong2024orpo} offer more streamlined approaches by avoiding the complexities of training reward models. Notably, both SimPO and ORPO further simplify the alignment process by eliminating the need for a reference model. While these methods lack an explicit reward model, which limits their ability to sample preference pairs from the optimal policy, researchers have proposed augmenting preference data using a trained or refined Supervised Fine-Tuning (SFT) policy. This allows the policy to learn from data generated by the optimal policy. Further research has extended these approaches into iterative frameworks, continuously updating the reference model for optimization. Alignment as Reward-Guided Search (ARGS) \cite{khanov2024args} presents a novel approach by integrating alignment directly into the decoding process. However, these alignment methods generally consume significant computational resources, and when the value standards shift, they necessitate retraining the models to adapt to the new criteria.