
\section{LLM-based AL Goals}\label{app:al_setting}

In this section, we highlight the various ways LLM-based AL can be used by providing an intuitive categorization based on the LLM-based AL goals.
In particular, LLM-based AL techniques can focus on 
selecting or generating specific data points $\mX$,
learning an optimal prompt $P$,
selecting contexts $C$ to include in the prompt,
deciding on the LLM model $\mathcal{M}_i \in M$,
features to leverage $x_j$,
hyperparameters $\theta$,
model architectures $\mathcal{A}$,
data for annotation $\mX_{S}$,
and
evaluation data $\mX_{\rm eval}$.



\begin{itemize}[left=0pt]
    \item \textbf{Data Points} ($X$): 
    LLM-based active learning can be used to select specific data points $x \in X$ from a larger pool for different purposes, such as training, budget-constrained training, and domain adaptation. This also includes selecting data for annotation, $x_{\text{annot}}$, which maximizes learning gains, and choosing evaluation subsets, $x_{\text{eval}}$, that ensure comprehensive model assessment.
    Furthermore, we can also leverage LLMs to generate entirely novel data points that lie outside the set of unlabeled data.
    This 
    
    \item \textbf{Prompts} ($P$): 
    The technique can be applied to select the most effective prompts $p \in P$ and prompt variations that optimize LLM outputs for specific tasks.
    
    \item \textbf{Contexts} ($C$): 
    LLM-based active learning can be utilized to select the most relevant contexts $c \in C$ or contextual inputs that enhance model performance on context-dependent tasks.
    
    \item \textbf{LLM Model Variants} ($\mathcal{M}$): 
    The method can be used to select the best LLM model variant $\mathcal{M}_i \in \mathcal{M}$ from a set of available models for a given input.
    Alternatively, we can also use LLM-based AL to identify models that contribute effectively to ensembles.
    
    \item \textbf{Features} ($x_j$): 
    LLM-based active learning can be used to select important features $x_j \in \vx$ or identify new features that can be integrated to improve model accuracy and explainability.
    Furthermore, it can also be leveraged to estimate missing features, e.g., if there are several missing values in the feature vector of a specific instance.
    
    \item \textbf{Hyperparameters} ($\theta$): 
    The technique is useful for selecting optimal hyperparameter values $\theta_i \in \theta$, such as learning rate or batch size, and adjusting them dynamically during training to optimize performance.
    
    \item \textbf{Model Architectures} ($\mathcal{A}$): 
    LLM-based active learning can assist in selecting the most appropriate model architecture $\mathcal{A}_i \in \mathcal{A}$ or choosing between different versions of a model for specific use cases.
    
    \item \textbf{Data for Annotation} ($\mX_{S}$): 
    The technique can be used to select specific data subsets $\mX_{S} \subset \mX$ that should be labeled by human annotators, focusing on those that provide the greatest potential improvement in model learning.
    
    \item \textbf{Evaluation Data Subsets} ($\mX_{\text{eval}}$): 
    LLM-based active learning is beneficial for selecting evaluation data points $\mX_{\text{eval}} \subset X$ that maximize the effectiveness of model evaluation, ensuring coverage of edge cases and comprehensive testing.
\end{itemize}
\begin{comment}
% \section{LLM-based Active Learning Overview} \label{llm-based-AL-overview}
\section{Detailed LLM-based AL Formulation} \label{llm-based-AL-formulation}
% \subsection{Preliminaries}\label{sec:problem-prelim}


\todo{complete this part, add equations for various parts, and discuss trade-offs at high-level, can also reference the two taxonomy tables, etc...}

\todo{significantly improve this, or comment it out if we run out of time. We will need something formal for this. Would also be great to have this also in Section 2, even if it is much more condensed.}


Let $\mathcal{M}$ be an LLM parameterized by $\theta$ that takes a text sequence $X = (x_1, \cdots, x_m) \in \X$ as input 
and produces an output $\yhat \in \hat{\Y}$, where $\yhat = \mathcal{M}(X; \theta)$; the form of $\yhat$ is task-dependent. The inputs may be drawn from a labeled dataset $\D = \{ (X^{(1)}, Y^{(1)}), \cdots, (X^{(N)}, Y^{(N)})\}$, or an unlabeled dataset of prompts for sentence continuations and completions $\D = \{ X^{(1)}, \cdots, X^{(N)} \}$.
% For this and other notation, see Table~\ref{table:notation}.

% \begin{Definition}[\sc Large Language Model]\label{def:LLM}
% A \emph{large language model (LLM)} $\mathcal{M}$ parameterized by $\theta$ is 
% a model with an autoregressive, autoencoding, or encoder-decoder architecture that has been 
% trained on a large corpus of hundreds of millions to trillions of tokens, with some optional post-training for instruction following or reasoning.
% \end{Definition}


% \subsection{LLM-based Active Learning Overview} \label{llm-based-AL-overview}
We summarize a basic framework for LLM-based active learning in Algorithm~\ref{alg:llm-based-AL}.
Notice that an LLM $\mathcal{M}$ is provided as input to Algorithm~\ref{alg:llm-based-AL}, and can be leveraged in both the querying and annotation steps.
Furthermore, in the querying step, the LLM can be used to generate entirely new instances (or even new labels) as well as select existing instances from the unlabeled data $\mathcal{U}$. 
% \todo{Fix this...}


Traditionally, active learning is a paradigm that seeks to efficiently label a subset of a large dataset to improve a model's performance while minimizing labeling costs. 
LLM-based active learning is a new paradigm with a similar goal, though is not bound by simply labeling a subset of a large dataset, but can also label new instances that are generated by an LLM.
% Furthermore, the cost of annotation
The formulation is typically represented as a sequential decision-making process.
% 
More formally, let $\mathcal{U} = \{\vx_i\}_{i=1}^N$ be a pool of $N$ unlabeled instances, where $\vx_i \in \mathcal{X}$ are feature vectors in the input space $\mathcal{X}$.
However, we also have a potentially infinite set of unlabeled instances $\mathcal{U}_g$ that can be generated by an LLM $\mathcal{M}$.
Furthermore, let $\mathcal{L} = \{(\vx_i, y_i)\}_{i=1}^M$ be a labeled dataset, where $y_i \in \mathcal{Y}$ are the corresponding labels from the label space $\mathcal{Y}$ where $|\mathcal{L}| = M \ll N$.
Let $k$ denote the labeling budget that limits the number of instances that can be labeled by the oracle/annotator.
One key difference in the LLM-based AL setting is the oracle/annotator can be either human or a large generative model.
This leads to additional complexity as the labeling budget that is a proxy for cost is more complex as well.
Given this, we have a predictive model $f_\theta : \mathcal{X} \to \mathcal{Y}$ parameterized by $\theta$, trained using the labeled data $\mathcal{L}$.
The goal of LLM-based AL is to iteratively select or generate the most informative instances $\vx \in \mathcal{U} \cup \mathcal{U}_g$ for labeling by an oracle/annotator, such that the model $f_\theta$ achieves the highest possible performance with the smallest labeling cost.
\end{comment}







%SUBHO added
% \section{Discussions on Traditional Strategies}
\section{Traditional Selection Strategies}
\label{app:trad-strategy}
We provide additional discussions on traditional selection strategies from Section~\ref{sec:querying-selection}.
The \texttt{Coreset} is a pure diversity-based strategy where unlabeled examples are selected using a greedy furthest-first traversal conditioned on all labeled examples \citep{sener2017active, geifman2017deep, citovsky2021batch}. Similarly \citet{agarwal2020contextual} uses a coreset-based strategy on features to select unlabeled examples using CNN. The \texttt{Least confidence} is an uncertainty-based active learning algorithm where the uncertainty score of an unlabeled example is its predicted class probability and the algorithm then samples unlabeled examples with the smallest uncertainty score \citep{settles2009active, settles2011theories, wang2014new}. The \texttt{Margin}-based selection strategy is also an uncertainty-based strategy \citep{tong2001support, balcan2009agnostic, settles2009active}. Margin first sorts unlabeled examples according to their multiclass margin score and then selects examples that are the hardest to discriminate and can be thought of as examples closest to their class margin. The \texttt{Max-entropy} strategy \cite{wang2014new, kremer2014active, diao2023active} is an uncertainty-based strategy that selects unlabeled examples according to the entropy of the example's predictive class probability distribution. The \texttt{Badge} algorithm combines both uncertainty and diversity sampling \citep{ash2019deep, ash2021gone}. Badge chooses a batch of unlabeled examples by applying $k$-Means++ \citep{arthur2006k} on the gradient embeddings computed from the penultimate layer of the model. The value of the gradient embedding captures the uncertainty score of the examples. Finally, \texttt{Bald} (Bayesian Active Learning by Disagreements)  \citep{kirsch2019batchbald, pmlr-v70-gal17a} chooses unlabeled examples that are expected to maximize the information gained from the model parameters i.e. the mutual information between predictions and model posterior.


















% \subsection{Budget}
% % one facet of the problem
% In LLM-based AL, one may also have a fixed budget on the cost of generation (\eg, input/output tokens).

% Budget:
% - Number of samples
% - LLM cost of generation (input/output tokens)
% - etc


% % % \todo{revise text}
% % Budget in terms of the monetary cost of generation via LLMs:
% % Note that there are two costly elements, e.g., we may have a budget for the cost of obtaining the output $Y_{ij}$ for a given model $M_i \in \mathcal{M}$ and input prompt $X_j \in \mathcal{X}$.
% % This is difficult as the cost is based on the number of input tokens (which can be computed apriori, and the number of tokens of the output from the model, which is unknown, but can be somewhat estimated or upper bounded).

% % Alternatively, the problem may be formulated with respect to a budget $B$ on the human feedback for a specific model $M_i$, input prompt $X_j$, and output $Y_{ij} = M_i(X_j)$, forming a tuple $(M_i, X_j, Y_{ij}=M_i(X_j))$.



% \subsection{Add other facets}
% Discuss other unique facets to this problem, key differences, new challenges, and opportunities.









\eat{%}
\todo{%}
In work by~\citet{atsidakou2024contextual} it is shown that contextual Pandoras box can be reduced to contextual bandits and further reduced to online regression.
}%
\todo{cite other works, etc.}
% From Contextual Bandits to bandits to online Regression


\todo{Add unification and show that these are recovered (certain instantiations)}

\ryan{cite relevant papers for known connections, bandits--pandoras, contextual bandits online linear regression,} 
}%





% \ryan{This section can be short 1/2 page, and just mathematically show various connections, and discuss them. Would be good to show unifying equation, then show equations, and how they fit under it, etc.}
% \ryan{I also created an initial table, see Table~\ref{tab:AL-connections-overview} in Appendix. Likely can be improved, etc}
% \end{comment}





























\begin{comment}

% \eat{%}
\begin{table*}[ht]
\centering
\small
\begin{tabular}{p{3.5cm}p{3.5cm}p{2cm}p{2cm}p{4.5cm}}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multirow{2}{*}{\textbf{Description}} & \textbf{Search} & \textbf{Aspect} & \multirow{2}{*}{\textbf{Space of Techniques}} \\
& & \textbf{Space} & \textbf{Focus} & \\
\midrule

\multirow{3}{*}{\textcolor{googleblue}{\textbf{Data Selection for Training and Evaluation}}} 
& \textbf{Data Points ($X$)} & Discrete, Large & Multi & Active sampling, uncertainty-based selection \\
& \textbf{Data for Annotation ($x_{\text{annot}}$)} & Discrete, Large & Single & Human-in-the-loop annotation, informativeness-driven selection \\
& \textbf{Evaluation Data Subsets ($x_{\text{eval}}$)} & Discrete, Small/Medium & Single & Coverage-based subset selection, edge case identification \\
\midrule

\multirow{2}{*}{\textcolor{googlered}{\textbf{Prompt and Context Selection}}} 
& \textbf{Prompts ($P$)} & Discrete, Large & Single & Prompt engineering, prompt ranking, reinforcement learning \\
& \textbf{Contexts ($C$)} & Discrete, Medium & Multi & Contextual relevance scoring, information retrieval-based filtering \\
\midrule

\multirow{2}{*}{\textcolor{googlegreen}{\textbf{Feature and Parameter Optimization}}} 
& \textbf{Features ($\mathbf{v}$)} & Discrete, Large & Multi & Feature selection algorithms (e.g., forward selection, LASSO), attention mechanisms \\
& \textbf{Hyperparameter Values ($\theta$)} & Continuous, Infinite & Single & Bayesian optimization, grid search, hyperparameter tuning via gradient-based methods \\
\midrule

\multirow{2}{*}{\textcolor{googlepurple}{\textbf{Model and Architecture Selection}}} 
& \textbf{LLM Model Variants ($\mathcal{M}$)} & Discrete, Medium & Multi & Ensemble learning, model comparison \\
& \textbf{Model Architectures ($\mathcal{A}$)} & Discrete, Large & Single & Architecture search algorithms, neural architecture search (NAS) \\
\bottomrule
\end{tabular}
\caption{\todo{remove and add the final} Taxonomy of LLM-based Active Learning Techniques}
\label{tab:active-learning-extended}
\end{table*}
% }%









% \renewcommand{\checkmark}{\faCheck}

\definecolor{googleblue}{RGB}{66,133,244}
\definecolor{googlered}{RGB}{219,68,55}
\definecolor{googlegreen}{RGB}{15,157,88}
\definecolor{googlepurple}{RGB}{138,43,226}
\definecolor{lightred}{RGB}{255, 220, 219}
\definecolor{lightblue}{RGB}{204, 243, 255}
\definecolor{lightgreen}{RGB}{200, 247, 200}
\definecolor{lightpurple}{RGB}{230,230,250}
\definecolor{lightyellow}{RGB}{242, 232, 99}
\definecolor{lighterblue}{RGB}{197, 220, 255}
\definecolor{lighterred}{RGB}{253, 249, 205}
\definecolor{lightyellow}{RGB}{207, 161, 13}
\definecolor{darkpurple}{RGB}{218, 210, 250}
\definecolor{darkred}{RGB}{255,198,196}
\definecolor{darkblue}{RGB}{172, 233, 252}

\definecolor{grey}{RGB}{163, 163, 163}

% \newcolumntype{A}[2]{%
%     >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
%     l%
%     <{\egroup}%
% }
% % \newcommand*\rot{\multicolumn{1}{A{90}{1em}}}
% \newcommand*\rotbar{\multicolumn{1}{|A{90}{1em}}}
\newcommand*\rot{\rotatebox{90}}

\begin{table*}[!ht]
\centering
\caption{%
\textbf{Taxonomy of LLM-based Active Learning}
We summarize the unique properties and key advantages of each category of LLM-based Active Learning approaches defined in our proposed taxonomy.
Note that $\downarrow$ and $\uparrow$ indicate whether the costs are low or high, respectively.
Further, $\checkmark$ indicates that the class of methods in the proposed taxonomy has that specific property, whereas \textcolor{grey}{$\checkmark$} indicates ``somewhat''.
In terms of cost, we indicate the cost of each class of methods with regards to the human cost, compute cost (\eg, cost of computing or input/output tokens), and the cost in terms of time which may be the runtime.
% 
\todo{Fill in the checkmarks, and other intuitive symbols, and write text/caption to clarify meaning.}
% 
% 
% \textbf{Taxonomy of Datasets for Bias Evaluation in LLMs.}
% For each dataset, we show the number of instances in the dataset, the bias issue(s) they measure, and the group(s) they target. Black checks indicate explicitly stated issues or groups in the...
}
\vspace{-1.5mm}
\label{tab:taxonomy}
\renewcommand{\arraystretch}{1.10} 
% \renewcommand{\arraystretch}{0.92} 
\small
% \footnotesize
% \tiny
% \setlength{\tabcolsep}{3.7pt} 
\begin{tabularx}{1.0\linewidth}{l H ccc cccc ccccc cc c c c H H}
\toprule

& 
& \multicolumn{3}{c}{\textbf{\textcolor{googleblue}{Cost}}}
& \multicolumn{4}{c}{\textbf{\textcolor{googlegreen}{Quality}}}
& \multicolumn{5}{c}{\textbf{\textcolor{googlered}{Complexity}}}
& \multicolumn{3}{c}{\textbf{\textcolor{googlepurple}{Settings}}}
\\
\cmidrule(lr){2-5}
\cmidrule(lr){6-9}
\cmidrule(lr){10-14}
\cmidrule(l){15-17}


& 
& \rot{\textbf{\textcolor{googleblue}{Human Effort}}}
& \rot{\textbf{\textcolor{googleblue}{Compute Cost}}} % (in/output tokens)}}
& \rot{\textbf{\textcolor{googleblue}{Time}}}
% QUALITY
& \rot{\textbf{\textcolor{googlegreen}{Accuracy}}}
& \rot{\textbf{\textcolor{googlegreen}{Diversity}}}
& \rot{\textbf{\textcolor{googlegreen}{Robustness}}}
& \rot{\textbf{\textcolor{googlegreen}{Noise}}}
% 
& \rot{\textbf{\textcolor{googlered}{General}}} 
& \rot{\textbf{\textcolor{googlered}{Heterogeneous}}}
& \rot{\textbf{\textcolor{googlered}{Multi-modal}}}
& \rot{\textbf{\textcolor{googlered}{Continuous}}}
& \rot{\textbf{\textcolor{googlered}{Discrete}}}
% 
& \rot{\textbf{\textcolor{googlepurple}{Multi-objective}}}
& \rot{\textbf{\textcolor{googlepurple}{Multi-label}}}
& \rot{\textbf{\textcolor{googlepurple}{Feature gen.}}}
% & \rot{\textbf{\textcolor{googlepurple}{...}}}
\\


\hboldline
% \TTT

\rowcolor{darkblue} 
\textsc{\textcolor{googleblue}{Query Approach} (\S~\ref{sec:query})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{darkblue} 
\quad \textsc{\textcolor{googleblue}{Selection} (\S~\ref{sec:query-selection})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightblue} 
% \quad \quad \textbf{Sampling, etc}
\quad \quad \textbf{Traditional, etc}
& TODO
& $\downarrow$
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$}
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\

\rowcolor{lightblue} 
\quad \quad \textbf{LLM-based}
& TODO
& $\downarrow$
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\


\rowcolor{darkblue} 
\quad \textsc{\textcolor{googleblue}{Generation} (\S~\ref{sec:generation})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightblue} 
\quad \quad \textbf{Traditional, etc}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\



\rowcolor{lightblue} 
\quad \quad \textbf{LLM-based}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\




\rowcolor{darkblue} 
\quad \textsc{\textcolor{googleblue}{Hybrid Approach} (\S~\ref{sec:query-hybrid})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightblue} 
\quad \quad \textbf{Select \& Adapt}
& Select top-k instances via a traditional approach, and then use LLM-based approach to finalize selection to maximize criterion provided to LLM (diversity, representativeness, accuracy, etc...). Further, can use LLM-based approach to adapt the top-k instances selected, to create even more diverse examples, etc.
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\



\hline



\rowcolor{darkred}
\textsc{\textcolor{googlered}{Oracle} (\S~\ref{sec:oracle})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{darkred}
\quad \textsc{\textcolor{googlered}{Human Feedback} (\S~\ref{sec:oracle-human-feedback})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightred} 
\quad \quad \textbf{Single Expert}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\

\rowcolor{lightred} 
\quad \quad \textbf{Multiple Experts}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\



\rowcolor{lightred} 
\quad \quad \textbf{Human + LLM Verify}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\




\rowcolor{darkred}
\quad \textsc{\textcolor{googlered}{llm-based Feedback} (\S~\ref{sec:oracle-llm-based-feedback})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightred} 
\quad \quad \textbf{Single LLM}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\



\rowcolor{lightred} 
\quad \quad \textbf{Multiple LLMs}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\


\rowcolor{lightred} 
% \quad \quad \textbf{Human Verify}
\quad \quad \textbf{LLM + Human Verify}
& LLMs can generate a lot of feedback fast, and in many tasks especially natural language where it is very time-consuming and costly for humans to write the text, we can instead leverage LLMs to generate text, and then the human verifies the correctness, which is much faster and cheaper.
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\




\rowcolor{darkred}
\quad \textsc{\textcolor{googlered}{Hybrid Approach} (\S~\ref{sec:oracle-hybrid})}
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
& 
\\

\rowcolor{lightred} 
\quad \quad \textbf{Adaptive}
& TODO
& \textcolor{grey}{$\checkmark$} 
& $\checkmark$ 
& \textcolor{grey}{$\checkmark$} 
& 
& \textcolor{grey}{$\checkmark$} 
& 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
& $\checkmark$ 
\\




\boldbottomline
% \multicolumn{17}{l}{*...} \\


\end{tabularx}
\end{table*}



\end{comment}


\section{AL Paradigms with LLMs \writer{Yu Xia}} \label{app:setting}
%\ryan{%
%Active learning for LLMs vs. LLM for Active Learning
%}%

%\subsection{Active Learning for LLMs}
With the rise of LLMs, the active learning has evolved to address new challenges and opportunities across various learning paradigms. 
%In the following sections, we discuss several key areas where active learning is reimagined for LLM-centric workflows.

%\subsection{Active Prompt Engineering}
%Traditional prompt design, especially when incorporating few-shot examples, requires extensive manual effort to identify most informative demonstrations.
%To address this, \citet{qian2024ape} propose APE (Active Prompt Engineering), a human-in-the-loop tool that actively selects ambiguous examples for human feedback, which are then converted into few-shot exemplars to improve LLM performance.
%To further reduce the human effort, \citet{bayer2024activellm} introduce ActiveLLM, an approach leveraging directly LLMs for selecting informative examples in textual few-shot scenarios.

%There have also been work using AL techniques for finding the best few-shot examples.
%\citet{qian2024ape} develops an active learning approach for prompt engineering where at each iteration, a set of prompts are derived, and then evaluated by a committee of models, where the best is selected, and then the approach repeats.
%\cite{bayer2024activellm}



\subsection{Active In-Context Learning}
Recent advances have recast demonstration selection for in-context learning as an active learning problem, where the goal is to identify and annotate the most informative examples under stringent labeling budgets. For example, \citet{margatina2023active} demonstrate that similarity-based sampling can consistently outperform traditional uncertainty-based methods when selecting single-round demonstrations for LLMs. Building on these insights, \citet{mavromatis2024covericl} propose CoverICL—a graph-based algorithm that integrates uncertainty sampling with semantic coverage to enhance both performance and budget efficiency across diverse tasks and LLM architectures. In a complementary approach, \citet{qian2024ape} introduce APE, a human-in-the-loop tool that iteratively pinpoints ambiguous examples for few-shot prompts, thus progressively refining LLM performance through active learning principles.


\subsection{Active Supervised Finetuning}
Active learning strategies have also been adapted to reduce the labeling cost associated with supervised finetuning. For instance, \citet{yu-etal-2022-actune} present AcTune, which actively queries high-uncertainty instances while simultaneously leveraging self-training on low-uncertainty unlabeled data. This dual strategy, further refined by region-aware sampling, effectively mitigates redundancy in the training data. Similarly, \citet{xia-etal-2024-hallucination} propose an active learning framework tailored for text summarization that systematically identifies and annotates diverse instances exhibiting various types of hallucinations, thereby improving both data efficiency and factual correctness. Extending these ideas to overcome cold-start challenges, \citet{bayer2024activellm} develop ActiveLLM, which employs state-of-the-art LLMs (e.g., GPT-4, Llama 3) to select informative instances, significantly boosting finetuning performance in both few-shot and iterative settings.

\subsection{Active Preference Alignment}
Active preference alignment targets label-efficient methods for refining LLMs via human or AI feedback.
\citet{ji2024reinforcement} frame reinforcement learning from human feedback (RLHF) as a contextual dueling bandit problem and develop an active-query-based algorithm (ADPO) that drastically cuts down the number of preference queries needed for LLM alignment.
Building on simpler, more stable methods, \citet{muldrew2024active} propose an active learning extension to Direct Preference Optimization (DPO), showing notable gains in both convergence speed and final quality through selective preference labeling.
Complementarily, \citet{chen2024cost} introduce a cost-effective approach for constructing reward models, combining on-policy querying and active data selection to maximize the impact of limited human feedback, and achieving strong performance improvements in DPO with minimal expert annotation.

%\citet{ji2024reinforcement} proposes an active learning-based DPO method called ADPO for active querying.
%They show that ADPO can achieve the same level of performance compared to DPO while using only about half of the queries for human preference.
%More recently, \citet{muldrew2024active} developed another active learning strategy for DPO that has an iterative data acquisition and fine-tuning loop called active preference learning where at each step, the approach selects a batch of prompt/completion pairs for the oracle to label, which are then used to improve the model via a cycle of fine-tuning. The process is repeated until the budget is reached.
%The acquisition function for prompt/completion pairs is based on predictive entropy and a measure of certainty of the implicit preference model optimized via DPO.
% 
%\citet{guo2024direct} introduces an approach for direct alignment of LLMs via online feedback.
%
%\citet{chen2024cost}


%\subsection{LLMs for Active Learning}

%\paragraph{Few-shot Ex} \cite{bayer2024activellm}

%\ryan{One way to organize it might be a subsection for AL for LLMs, and another for LLMs for Active Learning, then under each, mention more granular settings, just a thought, I'm sure there are better ways to do it}

%\ryan{This section can either be combined into techniques (copied to appropriate place), or we can maybe add another taxonomy around the specific setting (few-shot, fine-tuning, etc). For instance:}
%In this section, we categorize LLM-based AL approaches by whether the specific setting they focus on, that is, few-shot setting, fine-tuning, prompt learning, model pruning.

%\ryan{Note another way to organize it might be to categorize methods by whether they operate pre-processing (prior to training), in-training, or post-training. See fairness survey which we did something similar too}





%\subsection{Fine-Tuning Setting}



%\subsection{Active Prompt Learning Setting}
% \subsection{Active Prompt Engineering}
%\citet{qian2024ape} develops an active learning approach for prompt engineering where at each iteration, a set of prompts are derived, and then evaluated by a committee of models, where the best is selected, and then the approach repeats.

%\todos{I can write this section a bit as well. One of my recent paper with Brano is on active prompt selection for few shot learning, so i know some relevant literature.}

\subsection{Active Knowledge Distillation}
Active knowledge distillation has emerged to reduce the computational costs of LLMs by selectively transferring their knowledge into smaller models. 
\citet{zhang2024elad} propose ELAD, which leverages reasoning-step uncertainties to guide sample selection and employs teacher-driven explanation revisions to optimize the distillation process. 
\citet{liu-etal-2024-evolving} introduce EvoKD, an iterative strategy that identifies student model weaknesses and dynamically generates labeled data, continuously refining the student’s capabilities through LLM feedback. 
Meanwhile, \citet{palo-etal-2024-performance} present PGKD, an active feedback loop that uses performance signals such as hard-negative mining to inform new data creation, yielding substantial efficiency gains and reducing inference costs for text classification at scale.


% \begin{comment}
% \subsection{Unifying Framework: From AL to Bandits and Beyond}
\section{From AL to Bandits and Beyond}
% : A Unification}
\label{sec:unifying-settings}
%, Online Learning, RLHF, Pandoras Box, and Beyond}
% \ryan{add table sketched in notebook}

%Also fine-tuning, unlearning, dynamical systems etc
In this section, we briefly discuss a unifying view of the LLM-based AL problem through the lens of bandits, online learning, RLHF, pandoras box, among others.
We provide an intuitive summary of such connections in Table~\ref{tab:comparison-problem-settings} across a variety of problem settings.
%We also provide a more fine-grained analysis in Table~\ref{tab:AL-connections-overview}.
Intuitively, they overlap in goals, that is, 
% Overlap in Goals: 
many of these techniques aim to optimize resource use (e.g., labels, feedback, compute) while improving learning. 
They nearly all operate sequentially, but they differ in what drives decisions (e.g., reward, uncertainty, cost).
% Thus, they are all sequent
% 
However, they may have different constraints, for instance, 
% Differences in constraints: 
some settings assume explicit costs (e.g., pandora's box), while others lack such constraints (e.g., online learning).
% 
% Sequential Learning: Nearly all approaches operate sequentially, but they differ in what drives decisions (e.g., reward, uncertainty, cost).
These insights allow practitioners to choose the right framework depending on the data, task, and constraints of their problem.


\renewcommand{\arraystretch}{1.5} % Increase row height for readability

\begin{table}[t!]
\centering
\footnotesize
% \rowcolors{2}{gray!10}{white}
\begin{tabular}{p{2.5cm} *{4}{>{\centering\arraybackslash}m{0.3cm}}
H
H
*{2}{>{\centering\arraybackslash}m{0.5cm}}
}
\toprule
% \textbf{Technique} 
& 
\rotatebox{90}{\textbf{Sequential Learning}} & 
\rotatebox{90}{\textbf{Active Querying}} & 
\rotatebox{90}{\textbf{Human Feedback}} & 
\rotatebox{90}{\textbf{Exploration-Exploitation}} & 
\rotatebox{90}{\textbf{Cost-Awareness}} & 
\rotatebox{90}{\textbf{Pre-trained Models}} & 
\rotatebox{90}{\textbf{Statistical Foundations}} & 
\rotatebox{90}{\textbf{Dynamic Environments}} \\
\midrule

\textbf{Active Learning} & 
\cmark & \cmark &  & \cmark &  &  & \cmark & \cmark \\

\textbf{RLHF} & 
\cmark & \cmark & \cmark & \cmark &  &  &  & \cmark \\

\textbf{Pandora’s Box} & 
\cmark & \cmark &  & \cmark & \cmark &  & \cmark & \cmark \\

\textbf{Bandits} & 
\cmark &  &  & \cmark &  &  &  & \cmark \\

\textbf{Optimal Design} & 
 &  &  &  & \cmark &  & \cmark &  \\

\textbf{Online Learning} & 
\cmark &  &  & \cmark &  &  & \cmark & \cmark \\

% \textbf{Fine-tuning} & 
%  &  &  &  &  & \cmark &  & \cmark \\

\bottomrule
\end{tabular}

\caption{%
%\ryan{this needs to be carefully checked, for instance, all use human feedback right (under the def we have below)? Also some other factors seem to be missing, and others may not be that meaningful/useful.}
Comparison of problem settings across a range of important properties.
\textit{Sequential Learning}: Learning incrementally over time, rather than all at once. 
\textit{Active Querying}: Actively selecting or querying data points to improve the model. 
\textit{Human Feedback}: Utilizing human input or feedback to guide and improve learning. 
\textit{Exploration-Exploitation}: Balancing the need to explore new options versus exploiting known ones. 
\textit{Cost-Awareness}: Considering the costs of acquiring data or feedback during the learning process. 
% \textbf{Pre-trained Models}: Using pre-existing models as a starting point for further learning or adaptation. 
\textit{Statistical Foundations}: Grounding the technique in statistical principles or experimental design. 
\textit{Dynamic Environments}: Adapting to environments that change over time or where new information continuously emerges.
% \eat{%}
% \todo{Fix, add more factors, add grey checkmark, and define the properties more formally/intuitively that are used to summarize, etc}
% \todo{Still need to fix some stuff and improve. Should add math formulation, etc.}
% }%
}
\label{tab:comparison-problem-settings}
\end{table}



\begin{comment}
% \renewcommand{\cmark}{\textcolor{green!50!black}{✔}}
\renewcommand{\arraystretch}{1.5} % Increase row height for readability

\begin{table*}[h!]
\centering
\footnotesize
% \rowcolors{2}{gray!10}{white}

\begin{tabular}{p{2.5cm} p{6.0cm} p{2.9cm} p{3.1cm} 
HHHHHHHH
% *{8}{>{\centering\arraybackslash}m{1.0cm}}
}
\toprule
% \textbf{Technique} 
& \textbf{Problem Definition} & 
% \textbf{Mathematical Formulation} &
\textbf{Constraints} & 
\textbf{Key Aspect} & 
% \textbf{Unique Aspects} & 
\rotatebox{90}{\textbf{Sequential Learning}} & 
\rotatebox{90}{\textbf{Active Querying}} & 
\rotatebox{90}{\textbf{Human Feedback}} & 
\rotatebox{90}{\textbf{Exploration-Exploitation}} & 
\rotatebox{90}{\textbf{Cost-Awareness}} & 
\rotatebox{90}{\textbf{Pre-trained Models}} & 
\rotatebox{90}{\textbf{Statistical Foundations}} & 
\rotatebox{90}{\textbf{Dynamic Environments}} \\
\midrule

\textbf{Active Learning} & 
Iteratively selects the most informative data points to query and label for model improvement. 
$$\min_{\mathbf{X}} \mathbb{E}[L(f(\mathbf{X}))] \quad \text{s.t.} \quad |\mathbf{X}| \leq k$$ & 
Budget constraint ($|\mathbf{X}| \leq k$), Uncertainty measure. & 
Maximizes information gain by selecting uncertain or diverse samples for labeling. & 
\cmark & \cmark &  & \cmark &  &  & \cmark & \cmark \\
\midrule


\textbf{Optimal Design} & 
Selects experimental designs to maximize statistical efficiency, often for model fitting. 
$$\max_{\mathbf{X}} \det(\mathbf{X}^T \mathbf{X})$$ & 
Budget on experiments ($|\mathbf{X}| \leq k$). & 
Ensures that the selected data points provide maximal information for the design of the study. & 
 &  &  &  & \cmark &  & \cmark &  \\
 \midrule


\textbf{Pandora’s Box} & 
Selects which data to collect based on both information gain and cost. Balances exploration with the cost of queries. 
$$\min_{\mathbf{X}} \mathbb{E}[L(f(\mathbf{X}))] + \lambda  \text{cost}(\mathbf{X}) \;\; \text{s.t.} \;\; |\mathbf{X}| \leq k$$ & 
Budget constraint ($|\mathbf{X}| \leq k$), Cost-aware optimization. & 
Cost is factored directly into the decision-making process. & 
\cmark & \cmark &  & \cmark & \cmark &  & \cmark & \cmark \\
\midrule

\textbf{Bandits} & 
Learns the best action by balancing exploration and exploitation over time.  
$$\max_{a_t} \mathbb{E}[r_t(a_t)] \quad \text{s.t.} \quad \sum_t \mathbb{P}(a_t) = 1$$ & 
Action constraints (distribution over actions must sum to 1). & 
Focus on balancing short-term rewards (exploitation) and long-term learning (exploration). & 
\cmark &  &  & \cmark &  &  &  & \cmark \\
\midrule

\textbf{RLHF} & 
Learns from human feedback to improve the performance of reinforcement learning agents. 
$$\max_{\pi} \mathbb{E}[\sum_t \gamma^t r_t(\pi(s_t))] \quad \text{s.t.} \;\text{HF} \quad$$ 
% feedback from human. 
& 
Feedback constraint (human input required at each step). & 
Incorporates human preferences or corrections into policy learning. & 
\cmark & \cmark & \cmark & \cmark &  &  &  & \cmark \\
\midrule



\textbf{Online Learning} & 
Learns incrementally from data arriving over time, adjusting the model after each data point. 
$$\min_{\theta} \sum_{t=1}^{T} L(y_t, f(\theta, x_t))$$ & 
No strict budget, but data must be processed in real-time. & 
Adapts to new data in a sequential manner. & 
\cmark &  &  & \cmark &  &  & \cmark & \cmark \\
% \midrule

% \textbf{Fine-tuning} & 
% Refines a pre-trained model on new data to improve performance in a specific domain. 
% $\min_{\theta} \sum_{t=1}^{T} L(y_t, f(\theta, x_t))$ & 
% No additional budget; starts from pre-trained weights. & 
% Adaptation of general models to specific tasks or domains. & 
%  &  &  &  &  & \cmark &  & \cmark \\

\bottomrule
\end{tabular}

\caption{
Overview of related problem settings, constraints, and key aspects.
For a unifying framework of these, see Section~\ref{sec:unifying-settings}.
% We discuss a unified framework 
% that can be unified in a single framework.
% Unifying 
% Comparison of Techniques Based on Key Properties: 
% \textbf{Sequential Learning}: Learning incrementally over time, rather than all at once. 
% \textbf{Active Querying}: Actively selecting or querying data points to improve the model. 
% \textbf{Human Feedback}: Utilizing human input or feedback to guide and improve learning. 
% \textbf{Exploration-Exploitation}: Balancing the need to explore new options versus exploiting known ones. 
% \textbf{Cost-Awareness}: Considering the costs of acquiring data or feedback during the learning process. 
% \textbf{Pre-trained Models}: Using pre-existing models as a starting point for further learning or adaptation. 
% \textbf{Statistical Foundations}: Grounding the technique in statistical principles or experimental design. 
% \textbf{Dynamic Environments}: Adapting to environments that change over time or where new information continuously emerges.
}
\label{tab:AL-connections-overview}
\end{table*}
\end{comment}


\section{Applications \writer{Ryan Aponte}} \label{app:apps}
LLM-based active learning offers many promising applications; from LLM-to-SLM knowledge distillation  to low-resource language translation, to entity matching. Active learning reduces the cost of data supervision, broadening the tasks machine learning can be applied to. Active learning, combined with causal learning, may also be useful for reducing measured bias (Sec.~\ref{apps-active-debiasing}).
Table~\ref{tab:taxonomy-techniques} offers a taxonomy of active learning techniques. 
Overall, active learning is likely to continue to offer benefits in data-constrained environments. 
In particular, active learning may increase the utility of LLMs in domains requiring expert knowledge, such as medicine, law, and engineering; a historical weakness~\cite{yao2023beyond}. Finally, Table~\ref{tab:AL-datasets}, we include a pairing of active learning applications and datasets used.


% \ryan{I added initial text (currently a big larger than a page, but much to do) and created an initial application table (which needs updating to be consistent with the subsections, and we can add Sec.~\ref{} to every application subsection in Table}

% \ryan{Ryan Aponte: Can you polish the table, add to it, add Sec, and make it consistent with subsections, then add any other subsections I missed with work, and then add intro paragraph to this section, and final discussion subsection that discusses other applications, or novel aspects that arise.}

% \ryan{Ryan Aponte: when you are going through the works, please add the datasets used in various works, the application, etc. Please create a new table for this, as it will help with discussion, and can be used later for datasets section too}


\subsection{Text Classification}\label{apps-text-classification}

\citet{rouzegar2024enhancing} apply active learning to identify the most relevant samples for labeling text. The framework is applied to, among other datasets, Fake News for document authenticity~\cite{avrv-tp46-24}. The method reduces the cost of obtaining supervision and also enables a trade-off between cost efficiency and performance. The authors use GPT-3.5 and require some human supervision, unlike some more recent work~\cite{du2024causal}. 
ActiveLLM~\cite{bayer2024activellm} uses LLMs for selecting instances for few-shot text classification with model mismatch, in which the selection, or query, model is different from the model used for the final task. ActiveLLM use an LLM to estimate data point uncertainty and diversity without external supervision, improving few-shot learning.



\subsection{Text Summarization}\label{apps-text-summarization}

\citet{li2024active} proposes LLM-Determined Curriculum Active Learning (LDCAL) that improves the stability of the active learner by selecting instances from easy to hard by using LLMs to determine the difficulty of a document. LDCAL also leverages a new AL technique termed Certainty Gain Maximization that captures how well the unselected instances are represented by the selected ones, which is then used to select instances that maximize the certainty gain for the unselected ones.
More formally, the certainty gain (CG) measures the gain in representation certainty for an unlabeled instance $\vx_u$ when a candidate instance $\vx_s$ is selected for annotation, that is,
\begin{align}
\text{CG}(\vx_{s}, \vx_{u}) \!= \!\max \!\left(\! f(\vx_{s}, \vx_u) \!- \!\max_{\vx_{i} \in \mathcal{D}_{\ell}} \!f(\vx_{u}, \vx_{i}), 0 \!\right)\nonumber
\end{align}
It is derived as the maximum of the difference between $f(\vx_s, \vx_u)$, which measures the similarity between $\vx_s$ and $\vx_u$, and the current maximum similarity $\max_{\vx_i \in \mathcal{D}_\ell} f(\vx_u, \vx_i)$, where $\vx_i$ represents instances in the labeled set $\mathcal{D}_\ell$. To ensure non-negative values, any negative difference is clipped to 0. This formulation ensures that the selection of $\vx_s$ positively contributes to the representational coverage of the unlabeled instances, balancing the sampling process across high-density and low-density regions in the data distribution.
Finally, the Average Certainty Gain (ACG) for a candidate instance $\vx_s$ is
\[
\text{ACG}(\vx_s) = \frac{1}{L} \sum_{\vx_u \in \mathcal{D}_{\text{u}}} \text{CG}(\vx_s, \vx_u)
\]
where $L$ is the total number of unlabeled instances and $\mathcal{D}_{\text{u}}$ is the set of all unlabeled instances.
A key advantage of LDCAL over uncertainty-based acquisition strategies is that the acquisition model does not need to be trained after each AL iteration.


\subsection{Non-Textual Classification}\label{apps-non-text-classification}
~\citet{margatina2023active} apply active learning to several areas, including multiple-choice question answering and test on 15 different models from the GPT and OPT families. The selection of similar in-context samples for multiple-choice questions was the most effective sampling criterion. For classification, diversity was more effective than similarity as selection criterion. The authors also found larger models to have higher performance.

\subsection{Question Answering}\label{apps-question-answering}
\citet{diao2023active}, in recognition of the utility of chain-of-thought prompting, ActivePrompt uses active learning to design more effective prompts for LLMs based on human-designed chains of thought. With only eight exemplars made by humans, the method achieves higher performance on complex reasoning tasks. Uncertainty is estimated by querying an LLM with the same prompt repeatedly to and response disagreement is measured.

\subsection{Entity Matching}\label{apps-entity-matching}

An approach called APE~\cite{qian2024ape} focuses on an active prompt engineering approach for entity matching where at each iteration, a set of prompts are derived, and then evaluated by a committee of models, where the best is selected, and then the approach repeats. APE selects the most informative samples of a dataset reduces the cost to humans of identifying samples in most need of human feedback. 
%In a demo video, DBLP-Scholar is sampled from~\cite{}.
% 
\citet{ming2024autolabel} proposed AutoLabel that starts by selecting the most representative seed data using traditional techniques such as density clustering and sampling. Then uses an LLM with chain-of-thought prompting to obtain labels, and then leverages human feedback to rectify the labeled results for entity recognition.
% 
\citet{zhang2023llmaaa} proposed LLMaAA that leverages LLMs for annotation in an active learning loop. LLMaAA is used for both named entity recognition and relation extraction.

\input{table-datasets}

\subsection{Active Debiasing of LLMs}\label{apps-active-debiasing}

\citet{du2024causal} proposed a causal-guided AL approach for debiasing LLMs by leveraging the LLMs to select data samples that contribute bias to the dataset. The method works by applying active learning to identify the most important samples within the dataset and the model looks for causally invariant relationships; this approach is less compute-intensive than fine-tuning a model on a debiasing dataset.

\subsection{Translation}\label{apps-translation}

\citet{kholodna2024llms} apply active learning for annotations in low-resource languages and find near-SOTA performance, with a reduction in annotation cost (relative to human annotators) of 42.45 times. Like other active learning methods, samples with the highest prediction uncertainty are selected. The authors test on 20 low-resource languages spoken in Sub-Saharan Africa. The authors found larger LLMs like GPT-4-Turbot and Claude 3 Opus had more consistent performance than Llama 2-70B and Mistral 7B. 
FreeAL use active learning to collect data for task-specific knowledge, such as translation~\cite{xiao2023freeal}, without requiring human annotation. Tested on eight benchmarks, FreeAL achieves near-human-supervised performance, without requiring any human annotation. With additional feedback rounds, FreeAL was able to improve performance. In FreeAL, an LLM and SLM are paired and the LLM provides annotation, while the SLM is a weak learner. The authors propose the use of limited human supervision to further improve performance.


\subsection{Sentiment Analysis}\label{apps-sentiment-analysis}
~\citet{xiao2023freeal} use active learning for movie sentiment analysis with the Movie Review dataset Seeing Stars~\cite{pang2005seeingstarsexploitingclass} in the method ActivePrune, which uses ordinal (movie stars, 1-5) labels.
ActivePrune outperforms other pruning methods and with its increased compute efficiency, they reduce end-to-end active learning time by 74\%. ActivePrune works by reducing dataset size and increasing representation of underrepresented data, based on perplexity.


\subsection{Other}\label{sec-apps-other}
% \todo{move to appropriate place in paper}
% RyanA - there is an "other" section in the table, so one is included here
In this section, we describe additional noteworthy applications of active learning.

\subsubsection{Question Generation}\label{sec-apps-other-question-generation}
\citet{piriyakulkij2023active} proposed an active preference inference approach that infers the preferences of individual users by minimizing the amount of questions to ask the user to obtain their preferences.
The approach uses more informative questions to improve the user experience of such systems.



\subsubsection{System Design}\label{sec-apps-other-system-design}
\citet{taneja2024can} leverages an approach for actively correcting labels to enhance LLM-based systems that have multiple components.
\citet{astorga2024partially} introduced a partially observable cost-aware AL method focused on the setting where features and/or labels may be partially observed.


\section{LLM-based AL Advantages}\label{sec:appendix-AL-objectives}
By iteratively selecting and generating instances (to label) for training, LLM-based active learning can have the following advantages:

\begin{itemize}[left=0pt]
    \item \textbf{Better Accuracy:} LLM-based active learning can achieve better accuracy with fewer instances by selecting and generating the most informative instances.

    \item \textbf{Reduced Annotation Costs:} LLM-based active learning techniques can reduce labeling and other annotation costs by selecting or generating the most informative samples to use for model training and fine-tuning.

    \item \textbf{Faster Convergence:} Using LLM-based active learning often leads to faster convergence as the model can be learned more quickly by iteratively selecting and generating the most informative examples.

    \item \textbf{Improved Generalization:} By leveraging LLM-based active learning techniques that iteratively select and generate the most informative and diverse examples, the model can generalize better to new data.

    \item \textbf{Robustness:} Iteratively selecting or generating the best examples for training can often improve the robustness to noise, as the model is learned from a set of high-quality representative examples.
\end{itemize}

\begin{comment}
\section{Open Problems \& Challenges \writer{Zhouhang Xie}} \label{app:open-problems-challenges}
In this section, we discuss open problems and highlight important challenges for future work.

\subsection{Heterogeneous Annotation Costs}
While LLMs give rise to opportunities to develop better and more cost-effective AL methods, a challenge that arises is that the complex and heterogeneous costs for acquiring labels.
% For instance, an LLM-based AL method that is hybrid, and utilizes LLMs to generate new examples, and for annotation may leverage both LLMs and humans. 
Traditionally, AL methods assume a fixed budget
%$k$ 
representing the amount of examples that human annotators can label.
However, this assumption relied on the fact that the cost of each human annotation was the same.
However, in LLM-based AL, the costs are often complex and heterogeneous.
For instance, the cost can be the number of human annotations, the LLM costs for generation during the query step and the LLM costs during the annotation step.
% (which can be associated with the input/output tokens).
% In the above approach, the fixed budget $k$ is
%Instead new methods should be developed and investigated that consider all such costs during the optimization process.
To this end, novel algorithms that take into account these heterogeneous costs during the optimization process are a important future direction.
%\ryan{I added the above, please polish and improve. Would be nice to state this more formally, and add more discussion on it}



\subsection{Multi-LLM AL algorithms}
%\ryan{Does this still fit with the above?}
% While LLMs give rise to many opportunities to develop better and more cost-effective active learning methods that utilize the generation and annotation capabilities of LLMs in the active learning framework, a challenge that arises is calls for LLMs can be costly.
Furthermore, while there has been success in leveraging LLMs to annotate data and generate labels in AL algorithms, different LLMs have different performance and computational cost trade-off.
To this end, drawing from conventional wisdom of combining models with different strength-cost trade-offs in machine learning (such as retrieve-and-rerank~\cite{huang2024comprehensivesurveyretrievalmethods}, multi-oracle clusting~\cite{DBLP:conf/iclr/SilwalANMRK23}, \textit{inter alia}), there are abundant opportunities in combining cheap-weak and expensive-strong models for efficient and scalable inference.
%(related prior works such as retrieve-and-rerank scheme in information retrieval, combining light-weight classifier and LLM calls for data clustering~\cite{DBLP:conf/iclr/SilwalANMRK23}, inter alia)

\subsection{LLM Agents and Active Learning}
Following current success in combining LLM with active learning, a fruitful future direction is to explore the interaction between LLM agents and active learning.
On the one hand there are numerous methods for incorporating AL into LLM agents and its typical related methodologies, such as RAG~\cite{xu2024activeragautonomouslyknowledgeassimilation} and prompt design and in-context example selection~\cite{mukherjee2024experimental}.
On the other hand, LLM agents have shown good performance in various fields~\cite{Wang_2024_surveyllmagents}, and it would be worth investigating whether LLM agents can serve a role in enhancing existing AL algorithms, for example, as better drop-in replacement for expensive annotation efforts.


\subsection{Multimodality}
While this survey covers recent progress in various fields in combining LLM and active learning, these applications typically are in text-only domains.
To this end, a promising future direction is to explore LLMs for active learning in tasks involve other modalities, such as image, audio, and human behavioral data such as recommender systems.
These new modalities open up new unique challenges for applying LLMs in AL settings.
For example, unlike in text-based tasks, it is unclear whether LLMs can understand other data modalities such as audio and users' behavioral sequence as well as human annotators, leaving spaces for future exploration.

\subsection{Unstable Performance in LLM-based Annotation for AL Algorithms}
Finally, we note that there is an active line of work in improving LLMs in order to to better simulator human judgments, such as \textit{fine-tuning} LLMs into reward models~\cite{lambert2024rewardbenchevaluatingrewardmodels} and utilizing LLMs to simulate participants and annotators~\cite{zheng2023judgingllmasajudgemtbenchchatbot}, the results remain mixed~\cite{tan2024largelanguagemodelsdata}.
However, system deployer often-times will not have an estimation of how well LLMs can simulate human annotation when dealing with an unknown domain.
To this end, algorithms that dynamically route instances to human and LLM annotators based on the competency of LLMs remains an open problem.

\end{comment}
