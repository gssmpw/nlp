% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt,dvipsnames]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


\usepackage{colortbl}

\input{preamble}
\usepackage{subfigure}
\usepackage{float}
\usepackage{graphbox}
\usepackage{svg}
\usepackage{nicefrac}
\usepackage{xcolor}

\usepackage{bold-extra}
\usepackage[T1]{fontenc}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{graynode}{RGB}{20,20,20}
\definecolor{crimsonred}{RGB}{220,20,60}
\definecolor{darkgraynode}{gray}{0.5}
\definecolor{lightgraynode}{gray}{0.8}

\usepackage{rotate}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{capt-of}
\usepackage{tabulary}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{pifont} 

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{\X}{\mathbb{X}}
\newcommand{\Y}{\mathbb{Y}}
\newcommand{\Sent}{\mathbb{S}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\yhat}{\hat{Y}}
\newcommand{\Yhat}{\hat{\Y}}

\definecolor{gray}{RGB}{20,20,20}
\definecolor{gray}{RGB}{0.7,0.7,0.7}
\definecolor{greencm}{RGB}{0,153,0}

\newcommand{\cm}{ {\color{greencm}\normalsize\cmark}}
\newcommand{\cmgray}{ {\color{gray}\normalsize\cmark}}
\newcommand{\xm}{ {\color{red}\normalsize\xmark}}
\newcommand{\boldline}{{{\vrule width 0.3mm}}}
\newcommand{\hboldline}{\noalign{\hrule height 0.3mm}}
\newcommand{\boldbottomline}{\noalign{\hrule height 0.3mm}}

\definecolor{plotblue}{RGB}	{30,144,255}
\definecolor{plotgreen}{RGB}	{50,205,50}
\definecolor{plotred}{RGB}	{220,20,60}

\definecolor{myyellow}{RGB}{255,255,204}
\definecolor{myred}{RGB}{255,204,204}
\definecolor{myblue}{RGB}{0,200,255}
\definecolor{mygreen}{RGB}{80,220,80}

\newcommand{\sz}[1]{\mathlarger{\mathlarger{#1}}}
\newcommand{\mdot}[0]{{\mathsmaller{ \mathsmaller{\scriptsize \bullet}}}}
\newcommand{\szs}[0]{{\cdot}}
\newcommand{\onormof}[2]{\|#1\|_{#2}}
\newcommand*\hrulefillvar[1][0.4pt]{\leavevmode\leaders\hrule height#1\hfill\kern0pt}

% \usepackage{algorithm}
% \usepackage{algpseudocode}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\usepackage{subfigure}
\usepackage{nicefrac}



\DeclareMathAlphabet{\mathbcal}{OMS}{cmsy}{b}{n}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{comment}
\newcommand\bmmax{2}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amssymb}

\usepackage[fixed]{fontawesome5}

\usepackage{paralist}
% \setlength{\leftmargini}{10pt}   % No left margin for first-level items
\setlength{\itemsep}{4pt}        % No space between items
\setlength{\topsep}{6pt}         % No space above and below the list
% \setlength{\leftmargin}{0pt}   % No left margin for first-level items

\usepackage{enumitem}

\definecolor{thedarkblue}{RGB}{0,0,120} %104} % 180
\definecolor{mydarkblue}{rgb}{0,0.08,0.45} %ICML dark blue

\usepackage{hyperref}
\hypersetup{%
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}




\definecolor{googleblue}{HTML}{4285F4}
\definecolor{googlered}{HTML}{DB4437}
\definecolor{googlepurple}{HTML}{A142F4} % New purple color
\definecolor{googlegreen}{HTML}{0F9D58}


\definecolor{googleyellow}{HTML}{F4B400} % Bright yellow color 
\definecolor{googleorange}{HTML}{FBBC05} % Orange shade 
\definecolor{googlecyan}{HTML}{34A853} % Light cyan-green 
\definecolor{googlegray}{HTML}{9AA0A6} % Medium gray for text or subtle elements 
\definecolor{googlepink}{HTML}{EA4335} % Reddish-pink hue 
\definecolor{googlelightblue}{HTML}{7BAAF7} % Lighter blue for highlights or backgrounds

\usepackage{cleveref} %%added by subho

\usepackage{makecell}
\newcommand{\algcolor}[1]{\text{\textcolor{mydarkblue}{#1}}}

% \newcommand{\writer}[1]{\textcolor{purple}{\small \bf (#1)}}
%\newcommand{\writer}[1]{\textcolor{purple}{\bf (#1)}}
\newcommand{\writer}[1]{}


\newcommand{\hanjia}[1]{{\color{black}#1\color{black}}}


\newcommand{\todo}[1]{\textcolor{red}{~TODO:~#1}}
\newcommand{\SP}[1]{\textcolor{red}{~Soumya:~#1}}
\newcommand{\ryan}[1]{\textcolor{blue}{~Ryan:~#1}}
\newcommand{\hanieh}[1]{\textcolor{purple}{~Hanieh:~#1}}
\newcommand{\hongjie}[1]{\textcolor{blue}{~Hongjie:~#1}}
\newcommand{\roy}[1]{\textcolor{blue}{~Ruiyi:~#1}}
\newcommand{\jw}[1]{\textcolor{brown}{[Jack:~#1]}}
\newcommand{\todos}[1]{\textcolor{magenta}{~Subho:~#1}}
\newcommand{\namyong}[1]{\textcolor{Cerulean}{~Namyong:~#1}}
\newcommand{\kenneth}[1]{\textcolor{Lavender}{~Kenneth:~#1}}
\newcommand{\todob}[1]{\textcolor{blue}{~Brano:~#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{13\baselineskip}
%
% and set <dim> to something 5cm or larger.

% \title{Active Learning using LLMs: A Survey}
% \title{LLM-based Active Learning: A Survey}
\title{From Selection to Generation: A Survey of LLM-based Active Learning}
% \title{Active Learning in the era of LLMs: A Survey}




% Yu Xia*, Subhojyoti Mukherjee*, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joseph Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao 'Kenneth' Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian Mcauley


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% Yu Xia*, Subhojyoti Mukherjee*, Zhouhang Xie, Junda Wu, Xintong Li, Ryan Aponte, Hanjia Lyu, Joseph Barrow, Hongjie Chen, Franck Dernoncourt, Branislav Kveton, Tong Yu, Ruiyi Zhang, Jiuxiang Gu, Nesreen K. Ahmed, Yu Wang, Xiang Chen, Hanieh Deilamsalehy, Sungchul Kim, Zhengmian Hu, Yue Zhao, Nedim Lipka, Seunghyun Yoon, Ting-Hao 'Kenneth' Huang, Zichao Wang, Puneet Mathur, Soumyabrata Pal, Koyel Mukherjee, Zhehao Zhang, Namyong Park, Thien Huu Nguyen, Jiebo Luo, Ryan A. Rossi, Julian Mcauley

\author{
{\bf Yu Xia$^{1}$\thanks{~~Equal contributions}\hspace{1.5mm},
Subhojyoti Mukherjee$^{2\hspace{0.2mm}*}$,
Zhouhang Xie$^1$,
Junda Wu$^1$,
Xintong Li$^1$,} \\
{\bf
Ryan Aponte$^3$, Hanjia Lyu$^4$, Joe Barrow$^5$,
Hongjie Chen$^6$,
Franck Dernoncourt$^5$,
Branislav Kveton$^5$,} \\
{\bf
Tong Yu$^5$, Ruiyi Zhang$^5$, Jiuxiang Gu$^5$,
Nesreen K. Ahmed$^7$,
Yu Wang$^{8}$,
Xiang Chen$^5$,} \\
{\bf
Hanieh Deilamsalehy$^5$, Sungchul Kim$^{5}$,
Zhengmian Hu$^5$,
Yue Zhao$^9$, Nedim Lipka$^5$,
Seunghyun Yoon$^5$,} \\
{\bf Ting-Hao `Kenneth' Huang$^{10}$, Zichao Wang$^5$, Puneet Mathur$^5$, Soumyabrata Pal$^5$, Koyel Mukherjee$^5$,} \\
{\bf Zhehao Zhang$^{11}$, Namyong Park$^{12}$, Thien Huu Nguyen$^8$, Jiebo Luo$^4$, Ryan A. Rossi$^5$, Julian McAuley$^1$}\vspace{1mm}\\
$^1$University of California San Diego,
$^2$University of Wisconsin Madison, \\
$^3$Carnegie Mellon University,
$^4$University of Rochester,
$^5$Adobe Research,
$^6$Dolby Labs,\\
$^7$Intel AI Research,
$^8$University of Oregon,
$^9$University of Southern California,\\
$^{10}$Pennsylvania State University,
$^{11}$Dartmouth College,
$^{12}$Meta AI
}

\begin{document}
\maketitle

\begin{abstract}
Active Learning (AL) 
%with Large Language Models (LLMs) has emerged as 
has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training.
% With LLM-based AL, # RyanA- assuming this line typo
In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations.
% \kenneth{As an opening sentence, it might be useful to mention the intuition: what's special about active learning *with LLM*?}\ryan{good point, added above sentence}
Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning.
%focusing on its use with LLMs, the various aspects that can be optimized,
% by LLM-based active learning, 
%its methods, applications, and integration with LLMs.
% for efficient data annotation. 
We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop.
%how each category of techniques can be used for selecting various aspects for LLMs.
% We propose an intuitive taxonomy to classify the key approaches to AL in the context of LLMs, such as uncertainty sampling, query-by-committee, and diversity-based strategies, and explore their strengths and limitations. 
%Furthermore, we examine how LLMs are fine-tuned in AL settings, investigating both supervised and semi-supervised learning paradigms. 
We further examine the impact of AL on LLM learning paradigms and its applications across various domains. 
%various domains, such as natural language processing, question-answering, and dialogue systems. 
%Additionally, we discuss evaluation metrics and benchmark datasets commonly employed in the field, providing a comprehensive overview of their role in assessing AL performance with LLMs. 
Finally, we identify open challenges and propose future research directions.
%, such as model interpretability, scalability, and the integration of user feedback. 
This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.
% enhance the deployment of LLMs in active learning scenarios.
% This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.
\end{abstract}




\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{graphics/fig-LLM-based-AL-overview-v2.pdf}
\caption{Overview of LLM-based active learning. 
We start with initial data, including unlabeled instances $\mathcal{U}$.
There are two main steps.
First, LLM-based selection and/or generation leverages an LLM $\mathcal{M}$ to select unlabeled instances $\vx_i \in \mathcal{U}$ for annotation or \emph{generate entirely new instances} $\vx^{\prime} \not\in \mathcal{U}$. 
Next, given the LLM-based selected or generated instances $\vx_i$ or $\vx^{\prime}$, LLM-based annotation uses an LLM $\mathcal{M}$ to generate labels $y_i$ and $y^{\prime}$ for the instances.
Note that for intuition we show how LLMs can be leveraged for both steps; however, we may also use traditional techniques for selecting unlabeled instances, use humans for annotation, or both.
\eat{%}
Hence, an LLM-based AL approach is one that leverages LLMs for either of these steps, or both.
}%
% Note that in either step, one may also leverage 
% Note the above is an oversimplification
}
\label{fig:LLM-based-AL-overview}
\vspace{-2mm}
\end{figure}





\section{Introduction \writer{Ryan}}
Active Learning (AL) has been a widely studied technique that aims to reduce data annotation efforts by actively selecting most informative data samples for labeling and subsequent model training \cite{cohn1994improving,cohn1996active, settles2009active,olsson2009literature, fu2013survey,ren2021survey,zhan2022comparative}.
With an effective data selection strategy, this process helps to efficiently improve model performance with fewer labeled data instances, which can be particularly valuable when data annotation is expensive or time-consuming \cite{aggarwal2014active,hino2020active,schroder2020survey}.

%\todob{Add older references when you cite. You have them in \cref{sec:querying-selection}.}
\input{table-technique-taxonomy}


Despite the success of traditional active learning methods, the advent of Large Language Models (LLMs) with remarkable reasoning and generation capabilities creates a new paradigm of active learning.
For example, as illustrated in Figure~\ref{fig:LLM-based-AL-overview}, instead of solely relying on a predefined metric to query data instances, such as uncertainty \cite{kirsch2019batchbald, diao2023active} or diversity \cite{agarwal2020contextual, citovsky2021batch}, LLMs can now be used to select most informative instances after reasoning or even generate entirely new instances that are better suited for efficient model training \cite{bayer2024activellm, parkar2024selectllm, yang2024rewards}.
Moreover, with the collected informative data instances, LLMs also enable new data annotation schemes by collaborating with a human labeler or directly simulating a human labeler \cite{xiao2023freeal, kholodna2024llms, wang2024human}, which further reduces manual annotation efforts.

%\todob{I love the above paragraph because it talks about non-traditional approaches that would not be possible before.}



However, in spite of the immense potential of LLMs for active learning---particularly in high-quality data acquisition and annotation for efficient model training---existing surveys primarily focus on traditional active learning techniques, necessitating an up-to-date review of how LLMs have advanced AL in recent years. In this paper, we address this gap by presenting the first comprehensive survey of LLM-based AL techniques, which introduces a unifying taxonomy centered on the two main components of active learning: 
\textit{Querying} (selecting or generating unlabeled instances) and 
\textit{Annotation} (assigning labels).
\Cref{tab:taxonomy-techniques} and \Cref{fig:LLM-based-AL} provide an overview of the proposed taxonomy.
\Cref{table:qual-and-quant-comparison} further provide an intuitive comparison of existing LLM-based AL methods from the aspects of taxonomy and applications.
% Specifically, our survey systematically reviews relevant recent works as follows.
Guided by this taxonomy, our survey organizes and systematically reviews recent works across key aspects of LLM-based active learning as follows.
\begin{itemize}[nosep,leftmargin=0.8em]
	\item \textbf{Preliminaries} (\S\ref{sec:problem}):  We begin by introducing and formulating LLM-based active learning.
	\item \textbf{Querying} (\S\ref{sec:querying}): We describe querying strategies, including LLM-based selection and generation.
	\item \textbf{Annotation} (\S\ref{sec:ann}): We detail various annotation schemes, ranging from human annotation to LLM-based and hybrid approaches.
	\item \textbf{Termination} (\S\ref{sec:termination}): 
%	We discuss how recent works incorporate LLM costs into the termination of the active learning loop.
	We discuss how recent works consider LLM costs for terminating the AL loop.
	\item \textbf{Active Learning Paradigms} (\S\ref{sec:setting}): 
	We examine how AL influences LLMs' learning paradigms.
	\item \textbf{Applications} (\S\ref{sec:apps}): We highlight the diverse applications of LLM-based active learning.
	\item \textbf{Open Problems} (\S\ref{sec:open-problems-challenges}): We discuss open problems and present future research directions.
\end{itemize}



\paragraph{Survey Scope}
This survey focuses mainly on recent works leveraging LLMs for AL, 
which creates a new paradigm driven by LLMs' reasoning and generation capabilities.
While some works use traditional AL for LLMs, 
we may not cover them thoroughly as they use techniques reviewed by prior surveys 
\cite{zhan2022comparative, perlitz2023active}.

% \paragraph{Motivation}
% Given its immense potential in high-quality data acquisition and annotation as well as efficient model training, we provide a structured overview of LLM-based Active Learning techniques with a unifying taxonomy as in Figure \ref{fig:LLM-based-AL} and Table~\ref{tab:taxonomy-techniques} focusing on the two main components of active learning, i.e., Querying and Annotation.

% \paragraph{Scope of the Survey}
% % It is worth clarifying that 
% In this survey, we mainly focus on recent works that leverage LLMs for active learning, which creates a new paradigm with LLMs' reasoning and generation abilities.
% While there are also works applying traditional active learning methods on LLM finetuning, we might not to go into details as they share similar techniques already covered by previous surveys \cite{zhan2022comparative, perlitz2023active}.

% \paragraph{Overview of the Survey}
% We first define and formulate LLM-based active learning in Section \ref{sec:problem}.
% Then we describe in detail how querying strategies evolve from selection to LLM-based generation in Section \ref{sec:querying}, how annotation schemes evolve from human to LLMs in Section \ref{sec:ann}, and how recent works consider LLM costs in the termination of the active learning loop in Section \ref{sec:termination}.
% We further discuss the impact of active learning on the learning paradigms of LLMs in Section \ref{sec:setting} and its broad applications in Section \ref{sec:apps}.
% Lastly, we discuss the open problems and future research directions in Section \ref{sec:open-problems-challenges}.

%extensively over the past two decades~\citep{lewis1994sequential,lewis1994heterogeneous,cohn1994improving,cohn1996active} with several earlier surveys~\citep{settles2009active,olsson2009literature,fu2013survey,aggarwal2014active,hino2020active,schroder2020survey,ren2021survey,zhan2022comparative}.
% \SP{The advent of powerful LLM's with remarkable zero-shot capabilities creates a new paradigm of active learning. To obtain state of the art performance, it is important to identify and manually label queries on which the LLM is not confident. 
% Fine-tuning or in-context learning with these domain-specific examples can improve the generation quality significantly. 
% However, LLM's bring novel challenges to the area of Active Learning.}
%However, 
% \SP{to the best of our knowledge, there does not exist a survey} 
%there is still not a survey focused on using LLMs (and other types of generative models) in the active learning paradigm.
%In this work, we propose a unifying taxonomy for LLM-based active learning that leverages LLMs for selection and generation in the querying phase, and also for generating annotations in the annotation phase (Table~\ref{tab:taxonomy-techniques}). 
% in both the selection/generation
% \SP{
%T%o obtain state of the art performance, it is important to identify and manually label queries on which the LLM is not confident. 
%Fine-tuning or in-context learning with these domain-specific examples can improve the generation quality significantly. 
% However, LLM's bring novel challenges to the area of Active Learning.}
% \kenneth{Similarly, I'd sell why AL is important and why AL *with LLM* is important/interesting first, then say no one did a survey paper for it.}

% \SP{Elaborate on the definition of cost. In many enterprise settings, it is expensive to manually label ground truth answers to complex queries.}
%Active learning (AL) aims to iteratively select the best instances to label to maximize performance while minimizing 
% \SP{labeling} 
%cost~\cite{hino2020active,schroder2020survey}.
%AL techniques that select the most informative instances can (i) reduce labeling costs, (ii) improve model accuracy, (iii) enhance data efficiency, and (iv) adapt to changing data while considering cost.
% \SP{being data-efficient}. 
%This process helps to efficiently improve model performance with fewer labeled examples, which can be particularly valuable when labeling data is expensive or time-consuming.  
%A key observation is that not all examples are equally important to label~\cite{margatina2023active}, and therefore, if we have a good function for selecting the important instances to label, we can achieve nearly as good accuracy with significantly fewer examples, and reduce the effort required for labeling.
%We note that while active learning is typically framed in terms of labels for instances, we can also leverage active learning to carefully select the next best prompt or model and hyperparameters to sample. 
% This is the difference between a discrete labeling space and a continuous one. \SP{This line needs to be rewritten}




%In the context of Large Language Models (LLMs), we focus on leveraging the principles of active learning to improve the model’s performance by selecting and labeling the most informative examples from a potentially large pool of unlabeled data. This is especially important for LLMs, as they often require vast amounts of training data to perform well, and labeling this data can be resource-intensive.
%For LLMs, the active learning process can be applied in several ways, given their focus on tasks like text generation, classification, translation, and summarization. Active learning can help identify the most informative text samples, sentences, or paragraphs that, when labeled, would improve the model’s ability to generalize across tasks.
%\kenneth{Many NLP readers know the high-level idea of AL. So the questions they might be thinking at this point is: Why do we need to select data instances to label in the LLM era? One, isn't the cost of labeling data very low if we're using LLMs (as compared to using humans)? Two, isn't it that LLM can label data in few-shot ways? Are we creating datasets for fine-tuning LLMs? What exactly are we optimizing for? It'd be useful to make these more clear.}
%\ryan{Agree, this paragraph actually should probably go deeper into our proposed taxonomy perhaps, I know it somewhat comes next, but given its importance might be good to repeat it here. }
%\ryan{Also how this reads, maybe}



%An active learning approach consists of two main components:
%\begin{compactenum}
    % Query selection and generation
    %\item[\textbf{1.}\!] \textbf{Querying:} This component decides on queries to send to the oracle, \eg, labels of specific instances. Traditionally, the query selection component selects examples based on uncertainty or some other useful metric to improve performance.
    %\emph{However, LLMs can now be used to better select examples, and even generate examples that may maximize performance. These are examples that may not even be in the dataset.} \kenneth{Max whose performance? Another LLM, or a small-ish model, or both?}
    
    %\item[\textbf{2.}\!] \textbf{Annotation (Oracle):} The oracle/annotation returns the responses to the underlying query. Traditionally, the oracle is typically a human labeler, and most previous work prior to LLMs treats this component as a black-box that is used directly and defined a priori.
    %\emph{However, with the rise of generative models, we now have the ability to develop more interesting and effective techniques for the oracle. For instance, one can leverage LLMs directly as the oracle, as opposed to simulating the oracle as a human labeler.
    %We can also develop hybrid approaches for the oracle that leverage both LLMs and humans.}
%\end{compactenum}
%\medskip
%\ryan{reference figure 2, table 2, etc, and make connections.}

%The rise of LLMs offers significant changes in how both of these components can now be developed and improved upon.
%In terms of leveraging generation capabilities in the query formulator components, LLMs can now \emph{generate} examples, labels, and other aspects that can then be selected. 
%Furthermore, LLMs can also \emph{select} the next examples to be given to the oracle to improve performance.
%For the oracle/annotation component, LLMs can be used to label and provide a wide range of feedback on the instances provided by the query formulator component. Hence, LLMs can serve as the evaluators, and generate many different types of feedback that can then be used by the models. \SP{I think we should write this in the following way: LLM's have compressed world knowledge and have awesome zero-shot abilities for any task. However in enterprise specific settings which comprise data that most likely the LLM has not seen during pre-training, its abilities will be limited. To bridge this gap, we still need Active Learning - even more so because such settings are usually label expensive.}


%In this work, we survey existing work focused on applying active learning techniques to generative models \jw{not vice versa (applying generative models to active learning)? or both ways?}. 
%We then propose an intuitive taxonomy that categorizes LLM-based active learning approaches by what they focus on optimizing and selecting. 
%The proposed taxonomy highlights important and novel directions for future research.
%In particular, the proposed taxonomy categorizes work by whether they select data instances for training or fine-tuning, prompts, LLMs, hyperparameters, among many other important aspects that may be of interest to both researchers and practitioners for leveraging LLM-based active learning techniques.

%\eat{%
%We discuss and highlight new aspects that arise from the various LLM-based active learning settings.
% This includes 


%The key contributions of this work are as follows:

%\begin{compactenum}
%    \item \textbf{A formalization of LLM-based active learning.}
%    In \S\ref{sec:query}, we ...
    
%    \item \textbf{A survey and taxonomy of TODO}
%    We categorize ...

    
%    \item \textbf{TODO}
%    In \S\ref{sec:query}, we ...
    
%    \item \textbf{An overview of key open problems and challenges that future work should address.}
%    TODO
%\end{compactenum}
%}%


%is not the direct applications of traditional active learning for LLM training or finetuning.
%\SP{Provide citations}
%We instead focus more on recent works that leverage LLMs for active learning.
%However, sometimes we do include some of the prior work whenever it makes sense, and improves the readability and intuitiveness of our survey.
% \hanieh{Should we swap the first two sentences of this paragraph? For example first mention we intent to ... followed by we do not attempt?}

% \todos{This actually nicely clears up the scope.}

%\eat{%
%\medskip\noindent\textbf{Organization of this Article.}
%\todo{Add overview and reference figure/table for it}
%}%









% \section{LLM-based Active Learning Overview} \label{llm-based-AL-overview}
% We summarize a basic framework for LLM-based active learning in Algorithm~\ref{alg:llm-based-AL}.
% Notice that an LLM $\mathcal{M}$ is provided as input to Algorithm~\ref{alg:llm-based-AL}, and can be leveraged in both the querying and annotation steps.
% Furthermore, in the querying step, the LLM can be used to generate entirely new instances (or even new labels) as well as select existing instances from the unlabeled data $\mathcal{U}$. 
% % \todo{Fix this...}


% Traditionally, active learning is a paradigm that seeks to efficiently label a subset of a large dataset to improve a model's performance while minimizing labeling costs. 
% LLM-based active learning is a new paradigm with a similar goal, though is not bound by simply labeling a subset of a large dataset, but can also label new instances that are generated by an LLM.
% % Furthermore, the cost of annotation
% The formulation is typically represented as a sequential decision-making process.
% % 
% More formally, let $\mathcal{U} = \{\vx_i\}_{i=1}^N$ be a pool of $N$ unlabeled instances, where $\vx_i \in \mathcal{X}$ are feature vectors in the input space $\mathcal{X}$.
% However, we also have a potentially infinite set of unlabeled instances $\mathcal{U}_g$ that can be generated by an LLM $\mathcal{M}$.
% Furthermore, let $\mathcal{L} = \{(\vx_i, y_i)\}_{i=1}^M$ be a labeled dataset, where $y_i \in \mathcal{Y}$ are the corresponding labels from the label space $\mathcal{Y}$ where $|\mathcal{L}| = M \ll N$.
% Let $k$ denote the labeling budget that limits the number of instances that can be labeled by the oracle/annotator.
% One key difference in the LLM-based AL setting is the oracle/annotator can be either human or a large generative model.
% This leads to additional complexity as the labeling budget that is a proxy for cost is more complex as well.
% Given this, we have a predictive model $f_\theta : \mathcal{X} \to \mathcal{Y}$ parameterized by $\theta$, trained using the labeled data $\mathcal{L}$.
% The goal of LLM-based AL is to iteratively select or generate the most informative instances $\vx \in \mathcal{U} \cup \mathcal{U}_g$ for labeling by an oracle/annotator, such that the model $f_\theta$ achieves the highest possible performance with the smallest labeling cost.



% \todo{complete this part, add equations for various parts, and discuss trade-offs at high-level, can also reference the two taxonomy tables, etc...}

% \todo{Need to also slightly modify general algorithm, since $\mathcal{U}~\leftarrow~\mathcal{U} - \mathcal{I},~~\mathcal{L}~\leftarrow~\mathcal{L} \cup \mathcal{I}'$ is not entirely correct when generating, ...}


% % \ryan{In some tasks, there may not necessarily be labels.}












\definecolor{googleblue}{RGB}{66,133,244}
\definecolor{googlered}{RGB}{219,68,55}
\definecolor{googlegreen}{RGB}{15,157,88}
\definecolor{googlepurple}{RGB}{138,43,226}
\definecolor{lightred}{RGB}{255, 220, 219}
\definecolor{lightblue}{RGB}{204, 243, 255}
\definecolor{lightgreen}{RGB}{200, 247, 200}
\definecolor{lightpurple}{RGB}{230,230,250}
\definecolor{lightyellow}{RGB}{242, 232, 99}
\definecolor{lighterblue}{RGB}{197, 220, 255}
\definecolor{lighterred}{RGB}{253, 249, 205}
\definecolor{lightyellow}{RGB}{207, 161, 13}
\definecolor{darkpurple}{RGB}{218, 210, 250}
\definecolor{darkred}{RGB}{255,198,196}
\definecolor{darkblue}{RGB}{172, 233, 252}

% \begin{table*}[!ht]
% \centering
% \caption{
% \textbf{Taxonomy of LLM-based Active Learning Problem Settings.} 
% Note here we define the problem settings for LLM-based Active Learning, many of which are novel or only recently explored; and also provide intuitive examples.
% % Further, we do not mention the objective type...
% Further, each specific problem setting can support many different tasks/applications, and can even optimize many different data types for which one would apply LLM-based AL methods such as to select the top-k labels, prompts, and so on while minimizing cost.
% \todo{
% See role survey for an example of this table, and importance...
% }
% % Note a specific problem settings that can support many different tasks/applications.
% \ryan{%
% CATEGORIES:
% Data-based (including both generation of entire features, generating new label types, selecting instances, and even generating new instances, etc)
% - Features
% - Instances/Labels
% - 
% % 
% % 
% % Architecture-based 
% Meta-Learning
% - Architecture/model
% - LLM selection
% - Hyperparameter optimization (can generate hyperparameters, overcoming discrete search)
% % 
% % 
% Prompt-Learning
% - Instructions
% - Few-shot examples/ranking
% % 
% % 
% Model-based
% - either during training
% - pre-training 
% - post-process (reduce model size)
% }
% }
% \vspace{2.5mm}
% \label{table:taxonomy-of-problem-settings}
% \renewcommand{\arraystretch}{1.10} 
% % \renewcommand{\arraystretch}{0.92} 
% % \small
% \footnotesize
% % \setlength{\tabcolsep}{3.7pt} 
% \vspace{-3mm}
% \begin{tabularx}{1.0\linewidth}{H l X H}
% \toprule
% \TTT
% % \textbf{Bias Issue}
% & \textbf{LLM-based AL Setting} 
% & \textbf{Definition and Example}
% &
% \\
% \hboldline
% % \TTT

% \rowcolor{darkblue}
% &  & 
% & \\

% % Data-based (including both generation of entire features, generating new label types, selecting instances, and even generating new instances, etc)
% % - Features
% % - Instances/Labels
% \rowcolor{darkblue}
% {} & % bias issue
% \textsc{\textcolor{googleblue}{\text{Data-based}}} & 
% {\bf The data LLM-based AL settings focus on data such as generating or selection of labels, features, graphs/sequential data such as text, among others.
% } % definition
% & \\


% \rowcolor{darkblue}
% &  & 
% & \\
% \hline


% \rowcolor{lightblue}
% {} & % bias issue
% \qquad 
% \multirow{3}{*}{\textbf{\textcolor{black}{\text{Feature-based}}}} & % harm
% \textrm{In this setting, labels may be known, but features may be either unknown, partially observed for some instances, and not others, or specific feature-values missing.} % definition
% & \\
% % ---
% \rowcolor{lightblue}
% {} & 
% {} & 
% {%
% $-$ \textit{In LLM feature-based AL, the features themselves may be costly to obtain, \eg, customers often do not provide much information when signing up for a service or making a purchase, and most of it may be entirely missing, such as the customers gender, age, interests, and so on. Obtaining such information is both costly as users have to manually provide it, and depending on the type of features (high cardinality, numerical, etc), they may be very noisy, and may benefit from normalization.}
% % \eg, \textit{\emph{\texttt{"Both genders"}} excludes non-binary gender identities} \citep{bender2021dangers}
% } % definition
% & \\
% \hline

% \rowcolor{lightblue}
% {} & % bias issue
% \qquad 
% \multirow{3}{*}{\textbf{\textcolor{black}{\text{Labels}}}} & 
% {Selection or generation of labels for instances (query), as well as the evaluation and feedback on such instance-label pairs (oracle).
% }
% % {Reinforced normativity of the dominant social group and implicit exclusion or devaluation of other groups} % definition
% & \\
% % ---
% \rowcolor{lightblue}
% {} & 
% {} & 
% {%
% $-$ \textit{%
% In this setting, LLM-based AL is used to select or generate labels for existing or novel generated instances. It can also be used to evaluate and provide complex feedback on such instance-label pairs (oracle).
% % In LLM-based feature-based AL, the features themselves may be costly to obtain, \eg, customers often do not provide much information when signing up for a service or making a purchase, and most of it may be entirely missing, such as the customers gender, age, interests, and so on. Obtaining such information is both costly as users have to manually provide it, and depending on the type of features (high cardinality, numerical, etc), they may be very noisy, and may benefit from normalization.
% }
% % \eg, \textit{\emph{\texttt{"Both genders"}} excludes non-binary gender identities} \citep{bender2021dangers}
% } % definition
% % {\eg, \textit{\emph{\texttt{"Both genders"}} excludes non-binary gender identities} \citep{bender2021dangers}} % definition
% & \\
% % \cline{2-4}
% \hboldline




% \rowcolor{darkpurple}
% &  & 
% & \\

% % Data-based (including both generation of entire features, generating new label types, selecting instances, and even generating new instances, etc)
% % - Features
% % - Instances/Labels
% \rowcolor{darkpurple}
% {} & % bias issue
% \textsc{\textcolor{googlepurple}{\text{Meta-Learning}}} & 
% {\bf The data LLM-based AL settings focus on data such as generating or selection of labels, features, graphs/sequential data such as text, among others.
% } % definition
% & \\


% \rowcolor{darkpurple}
% &  & 
% & \\
% \hline


% \rowcolor{lightpurple}
% {} & % bias issue
% \qquad 
% \multirow{1}{*}{\textbf{\textcolor{black}{\text{Architecture}}}} & % harm
% \textrm{TODO} % definition
% & \\
% % ---
% \rowcolor{lightpurple}
% {} & 
% {} & 
% {%
% $-$ \textit{
% TODO
% }
% } % definition
% & \\
% \hline

% \rowcolor{lightpurple}
% {} & % bias issue
% \qquad 
% \multirow{1}{*}{\textbf{\textcolor{black}{\text{Hyperparameters}}}} & 
% {TODO
% }
% % {Reinforced normativity of the dominant social group and implicit exclusion or devaluation of other groups} % definition
% & \\
% % ---
% \rowcolor{lightpurple}
% {} & 
% {} & 
% {%
% $-$ \textit{%
% TODO
% % In this setting, LLM-based AL is used to select or generate labels for existing or novel generated instances. It can also be used to evaluate and provide complex feedback on such instance-label pairs (oracle).
% }
% } % definition
% & \\
% % \cline{2-4}
% \hboldline





% \rowcolor{darkred}
% &  & 
% & \\


% \rowcolor{darkred}
% {} & % bias issue
% % In-training
% \textsc{\textcolor{googlered}{\text{Model-based}}} & 
% {\bf In this setting, Model LLM-based Active Learning focuses on optimizing model aspects such as attention weights (either during training, or post-processing)} % definitiondefinition
% & \\


% \rowcolor{darkred}
% &  & 
% & \\
% \hline


% \rowcolor{lightred}
% {} & % bias issue
% \qquad 
% \multirow{1}{*}{\textbf{\textcolor{black}{\text{TODO see above}}}} & % harm
% {TODO} % definition
% % {TODO} % definition
% & \\
% % ---
% \rowcolor{lightred}
% {} & 
% {} & 
% {$-$ \textit{TODO}} % definition
% & \\
% \hline

% \rowcolor{lightred}
% {} & % bias issue
% \qquad 
% \multirow{1}{*}{\textbf{\textcolor{black}{\text{TODO see above}}}} & % harm
% {TODO}
% & \\
% % ---
% \rowcolor{lightred}
% {} & 
% {} & 
% {%
% $-$ \textit{%
% TODO
% }
% }% definition
% & \\

% \boldbottomline

% \end{tabularx}
% \end{table*}


























% % \renewcommand{\checkmark}{\faCheck}

% \definecolor{googleblue}{RGB}{66,133,244}
% \definecolor{googlered}{RGB}{219,68,55}
% \definecolor{googlegreen}{RGB}{15,157,88}
% \definecolor{googlepurple}{RGB}{138,43,226}
% \definecolor{lightred}{RGB}{255, 220, 219}
% \definecolor{lightblue}{RGB}{204, 243, 255}
% \definecolor{lightgreen}{RGB}{200, 247, 200}
% \definecolor{lightpurple}{RGB}{230,230,250}
% \definecolor{lightyellow}{RGB}{242, 232, 99}
% \definecolor{lighterblue}{RGB}{197, 220, 255}
% \definecolor{lighterred}{RGB}{253, 249, 205}
% \definecolor{lightyellow}{RGB}{207, 161, 13}
% \definecolor{darkpurple}{RGB}{218, 210, 250}
% \definecolor{darkred}{RGB}{255,198,196}
% \definecolor{darkblue}{RGB}{172, 233, 252}

% \definecolor{grey}{RGB}{163, 163, 163}

% % \newcolumntype{A}[2]{%
% %     >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
% %     l%
% %     <{\egroup}%
% % }
% % % \newcommand*\rot{\multicolumn{1}{A{90}{1em}}}
% % \newcommand*\rotbar{\multicolumn{1}{|A{90}{1em}}}
% \newcommand*\rot{\rotatebox{90}}

% \begin{table*}[!ht]
% \centering
% \caption{%
% \textbf{Taxonomy of LLM-based Active Learning}
% We summarize the unique properties and key advantages of each category of LLM-based Active Learning approaches defined in our proposed taxonomy.
% Note that $\downarrow$ and $\uparrow$ indicate whether the costs are low or high, respectively.
% Further, $\checkmark$ indicates that the class of methods in the proposed taxonomy has that specific property, whereas \textcolor{grey}{$\checkmark$} indicates ``somewhat''.
% In terms of cost, we indicate the cost of each class of methods with regards to the human cost, compute cost (\eg, cost of computing or input/output tokens), and the cost in terms of time which may be the runtime.
% % 
% \todo{Fill in the checkmarks, and other intuitive symbols, and write text/caption to clarify meaning.}
% % 
% % 
% % \textbf{Taxonomy of Datasets for Bias Evaluation in LLMs.}
% % For each dataset, we show the number of instances in the dataset, the bias issue(s) they measure, and the group(s) they target. Black checks indicate explicitly stated issues or groups in the...
% }
% \vspace{-1.5mm}
% \label{tab:taxonomy}
% \renewcommand{\arraystretch}{1.10} 
% % \renewcommand{\arraystretch}{0.92} 
% \small
% % \footnotesize
% % \tiny
% % \setlength{\tabcolsep}{3.7pt} 
% \begin{tabularx}{1.0\linewidth}{l H ccc cccc ccccc cc c c c H H}
% \toprule

% & 
% & \multicolumn{3}{c}{\textbf{\textcolor{googleblue}{Cost}}}
% & \multicolumn{4}{c}{\textbf{\textcolor{googlegreen}{Quality}}}
% & \multicolumn{5}{c}{\textbf{\textcolor{googlered}{Complexity}}}
% & \multicolumn{3}{c}{\textbf{\textcolor{googlepurple}{Settings}}}
% \\
% \cmidrule(lr){2-5}
% \cmidrule(lr){6-9}
% \cmidrule(lr){10-14}
% \cmidrule(l){15-17}


% & 
% & \rot{\textbf{\textcolor{googleblue}{Human Effort}}}
% & \rot{\textbf{\textcolor{googleblue}{Compute Cost}}} % (in/output tokens)}}
% & \rot{\textbf{\textcolor{googleblue}{Time}}}
% % QUALITY
% & \rot{\textbf{\textcolor{googlegreen}{Accuracy}}}
% & \rot{\textbf{\textcolor{googlegreen}{Diversity}}}
% & \rot{\textbf{\textcolor{googlegreen}{Robustness}}}
% & \rot{\textbf{\textcolor{googlegreen}{Noise}}}
% % 
% & \rot{\textbf{\textcolor{googlered}{General}}} 
% & \rot{\textbf{\textcolor{googlered}{Heterogeneous}}}
% & \rot{\textbf{\textcolor{googlered}{Multi-modal}}}
% & \rot{\textbf{\textcolor{googlered}{Continuous}}}
% & \rot{\textbf{\textcolor{googlered}{Discrete}}}
% % 
% & \rot{\textbf{\textcolor{googlepurple}{Multi-objective}}}
% & \rot{\textbf{\textcolor{googlepurple}{Multi-label}}}
% & \rot{\textbf{\textcolor{googlepurple}{Feature gen.}}}
% % & \rot{\textbf{\textcolor{googlepurple}{...}}}
% \\


% \hboldline
% % \TTT

% \rowcolor{darkblue} 
% \textsc{\textcolor{googleblue}{Query Approach} (\S~\ref{sec:query})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{darkblue} 
% \quad \textsc{\textcolor{googleblue}{Selection} (\S~\ref{sec:query-selection})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightblue} 
% % \quad \quad \textbf{Sampling, etc}
% \quad \quad \textbf{Traditional, etc}
% & TODO
% & $\downarrow$
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$}
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\

% \rowcolor{lightblue} 
% \quad \quad \textbf{LLM-based}
% & TODO
% & $\downarrow$
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\


% \rowcolor{darkblue} 
% \quad \textsc{\textcolor{googleblue}{Generation} (\S~\ref{sec:generation})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightblue} 
% \quad \quad \textbf{Traditional, etc}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\



% \rowcolor{lightblue} 
% \quad \quad \textbf{LLM-based}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\




% \rowcolor{darkblue} 
% \quad \textsc{\textcolor{googleblue}{Hybrid Approach} (\S~\ref{sec:query-hybrid})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightblue} 
% \quad \quad \textbf{Select \& Adapt}
% & Select top-k instances via a traditional approach, and then use LLM-based approach to finalize selection to maximize criterion provided to LLM (diversity, representativeness, accuracy, etc...). Further, can use LLM-based approach to adapt the top-k instances selected, to create even more diverse examples, etc.
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\



% \hline



% \rowcolor{darkred}
% \textsc{\textcolor{googlered}{Oracle} (\S~\ref{sec:oracle})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{darkred}
% \quad \textsc{\textcolor{googlered}{Human Feedback} (\S~\ref{sec:oracle-human-feedback})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightred} 
% \quad \quad \textbf{Single Expert}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\

% \rowcolor{lightred} 
% \quad \quad \textbf{Multiple Experts}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\



% \rowcolor{lightred} 
% \quad \quad \textbf{Human + LLM Verify}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\




% \rowcolor{darkred}
% \quad \textsc{\textcolor{googlered}{llm-based Feedback} (\S~\ref{sec:oracle-llm-based-feedback})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightred} 
% \quad \quad \textbf{Single LLM}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\



% \rowcolor{lightred} 
% \quad \quad \textbf{Multiple LLMs}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\


% \rowcolor{lightred} 
% % \quad \quad \textbf{Human Verify}
% \quad \quad \textbf{LLM + Human Verify}
% & LLMs can generate a lot of feedback fast, and in many tasks especially natural language where it is very time-consuming and costly for humans to write the text, we can instead leverage LLMs to generate text, and then the human verifies the correctness, which is much faster and cheaper.
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\




% \rowcolor{darkred}
% \quad \textsc{\textcolor{googlered}{Hybrid Approach} (\S~\ref{sec:oracle-hybrid})}
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% & 
% \\

% \rowcolor{lightred} 
% \quad \quad \textbf{Adaptive}
% & TODO
% & \textcolor{grey}{$\checkmark$} 
% & $\checkmark$ 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & \textcolor{grey}{$\checkmark$} 
% & 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% & $\checkmark$ 
% \\




% \boldbottomline
% % \multicolumn{17}{l}{*...} \\


% \end{tabularx}
% \end{table*}





















% \definecolor{categoryblue}{HTML}{4285F4}
% \definecolor{categoryred}{HTML}{DB4437}
% \definecolor{categorypurple}{HTML}{A142F4} 
% \definecolor{categorygreen}{HTML}{0F9D58}


% \begin{table*}[h]
% \centering
% \small
% \begin{tabular}{cr l}
% \toprule
% \textbf{Category} & \textbf{Active Learning Strategy} & \textbf{Example Approaches} \\ 
% \midrule

% \multirow{3}{*}{\textcolor{categoryblue}{\textbf{{Uncertainty-based Methods}}}} 
% & Pool-based Sampling (Sec.~\ref{sec:pool-sampling}) & ... \\ 
% \multirow{4}{*}{(\textbf{Section~\ref{sec:uncertainty}})}
% & Query-by-Committee (Sec.~\ref{sec:qbc}) & ... \\ 
% & Entropy Measures (Sec.~\ref{sec:entropy}) & ... \\ 
% & Ensemble-based (Sec.~\ref{sec:ensemble}) & ... \\ 
% \midrule

% \multirow{3}{*}{\textcolor{categorygreen}{\textbf{{Diversity-based Approaches}}}} 
% & Core-set Selection (Sec.~\ref{sec:core-set}) & ... \\ 
% \multirow{4}{*}{(\textbf{Section~\ref{sec:diversity}})}
% & Clustering Techniques (Sec.~\ref{sec:clustering}) & ... \\ 
% & Representative Sampling (Sec.~\ref{sec:representative}) & ... \\ 
% & Hybrid Diversity (Sec.~\ref{sec:hybrid-diversity}) & ... \\ 
% \midrule

% \multirow{2}{*}{\textcolor{categorypurple}{\textbf{{Model-based Strategies}}}} 
% & Reinforcement Learning (Sec.~\ref{sec:rl}) & ... \\ 
% \multirow{2}{*}{(\textbf{Section~\ref{sec:model-based}})}
% & Meta-Learning (Sec.~\ref{sec:meta-learning}) & ... \\ 
% & Unsupervised Pretraining (Sec.~\ref{sec:pretrain}) & ... \\ 
% \midrule

% \multirow{3}{*}{\textcolor{categoryred}{\textbf{{User-Centric Methods}}}} 
% & Human-in-the-Loop (Sec.~\ref{sec:hitl}) & ... \\ 
% \multirow{4}{*}{(\textbf{Section~\ref{sec:user-centric}})}
% & Personalized Feedback (Sec.~\ref{sec:feedback}) & ... \\ 
% & Annotation Strategies (Sec.~\ref{sec:annotation}) & ... \\ 
% & Crowd-sourced Annotation (Sec.~\ref{sec:crowd}) & ... \\ 

% \bottomrule
% \end{tabular}
% \caption{%
% \todo{probably to be removed, if we can't use it for anything... needs to be actually completed with techniques.}
% Overview of LLM-based Active Learning Strategies (Sections~\ref{sec:uncertainty}-\ref{sec:user-centric}).
% \todo{add actual details, etc}
% }
% \label{tab:active-learning-overview}
% \end{table*}
\section{What is LLM-based Active Learning? \writer{Joe Barrow}} \label{sec:problem}
In this section, we introduce some preliminaries.
We start with basic notations of active learning and then introduce the LLM-based active learning loop consisting of five main steps.

\paragraph{Active Learning} Let $\mathcal{U} = \{\vx_i\}_{i=1}^N$ be a pool of $N$ unlabeled instances, where $\vx_i \in \mathcal{X}$ are feature vectors in the input space $\mathcal{X}$. 
%\todob{This is confusing. Why would $\vx_i$ be a feature vector? It is a sequence of tokens. The labels $y_i$ below are also sequences of tokens.}
%However, we also have a potentially infinite set of unlabeled instances $\mathcal{U}_g$ that can be generated by an LLM $\mathcal{M}$.
Furthermore, let $\mathcal{L} = \{(\vx_i, y_i)\}_{i=1}^M$ be a labeled dataset, where $y_i \in \mathcal{Y}$ are the corresponding labels from the label space $\mathcal{Y}$ where the size of the labeled set $|\mathcal{L}| = M$ grows as more data instances are labeled.
We are also given an annotation budget $k$ that limits the number of instances that can be labeled by the annotator.
We have a predictive model $f_\theta : \mathcal{X} \to \mathcal{Y}$ parameterized by $\theta$ to be iteratively trained using the labeled data $\mathcal{L}$.
The objective of traditional AL is to efficiently select and label a subset of the unlabeled data instances $\vx_i \in \mathcal{U}$ to maximize the performance of model~$f_\theta$ before reaching the annotation budget $k$. 
%\todob{I am confused why we introduce both $k$ and $M$. In traditional active learning, we want to label $k \ll N$ examples $\vx_i \in \mathcal{U}$. Then, as we say later, one new trend is that the labeled examples may not belong to $\mathcal{U}$ when synthesized.}

\paragraph{LLM-based Active Learning} LLM-based AL shares a similar goal. 
However, it is not bounded by the unlabeled set $\mathcal{U}$, but can also use an LLM $\mathcal{M}$ to generate entirely new data instances denoted as $\vx^{\prime} \not\in \mathcal{U}$ as well as generating corresponding labels $y^{\prime}$ by simulating a human annotator.
We formulate in details the LLM-based AL loop as follows.



\begin{itemize}[left=0pt]
    \item \textbf{Initialize}: For a good starting point of the active learning loop, an LLM $\mathcal{M}$ can be used to annotate an initial set of labels or generate an initial dataset $\mathcal{L}_\text{init}$ to warm up the target model $f_\theta$. This approach overcomes the cold start problem that traditional AL methods face when there is no labeled data instance available and the initial model $f_\theta$ does not offer sufficient information for selecting informative data instances.
    \item \textbf{Query}: With an initialized model $f_\theta$, the \textbf{Querying} (\S\ref{sec:querying}) module is implemented to acquire the most informative data instances. Extending traditional AL methods that select from the unlabeled set $\mathcal{U}$ with certain uncertainty or diversity metrics, the LLM $\mathcal{M}$ can now be used to select instances $\vx_i \in \mathcal{U}$ either by scoring or directly choosing, augment existing examples such as generating paraphrases or contrast examples, or even generate entirely new labeled examples $\vx^{\prime} \not\in \mathcal{U}$.
    \item \textbf{Annotate}: The acquired data instances $\vx_i$ or $\vx^{\prime}$ are then passed to the \textbf{Annotation} (\S\ref{sec:ann}) module to obtain corresponding labels $y_i$ or $y^{\prime}$. The LLM $\mathcal{M}$ can be used either in conjunction with, to augment, or even to entirely replace the human labeler. 
    The labeled instances $\vx_i$ are then excluded from the unlabeled set $\mathcal{U}$ and added to the labeled set $\mathcal{L}$ with their labels $(\vx_i, y_i)$ or $(\vx^{\prime}, y^{\prime})$.
    \item \textbf{Train}: With newly annotated data instances added to the labeled dataset $\mathcal{L}$, the target model $f_\theta$ is trained with a step update on its parameters $\theta$ with respect to the loss $\ell((\vx_i, y_i), {f_\theta})$.
    The updated model $f_{{\theta}}$ is then used to provide information for the querying module in the next iteration before a termination criterion is met.
    \item \textbf{Terminate}: The active learning loop reaches \textbf{Termination} (\S\ref{sec:termination}) when a fixed annotation budget $k$ is reached, or when some property of the model $f_\theta$ being optimized, such as convergence, is satisfied.
    With LLMs, the budget can be not only based on the availability or cost of human annotators, but also the cost of prompting the LLM or a combination of both.
\end{itemize}
We also summarize the LLM-based active learning loop in Algorithm \ref{alg:llm-based-AL}.
The goal of LLM-based AL in the general setting is to iteratively select or generate with an LLM most informative instances $\vx_i$ or $\vx^{\prime}$ for human or LLM labeling and subsequent model training. We discuss other variants of LLM-based AL goals in Appendix \ref{app:al_setting}.

\begin{algorithm}[!t]
\captionsetup{font=small}
    \caption{\; LLM-based Active Learning}
    % Framework}
	\label{alg:llm-based-AL}
	\begin{algorithmic}[1]
		\small
		\Require Unlabeled dataset $\mathcal{U}$,
        LLM $\mathcal{M}$,
        Annotation budget $k$
        
        % \!\!\!\!\!\!\!large language model $\mathcal{M}$
        
        %\!\!\!\!\!\!\!prompts $\mathcal{P}$
        
        % \!\!\!\!\!\!\!budget $k$

        
        % \Require Unlabeled data $\mathcal{U}$, large language model $\mathcal{M}$, 
        
        % budget $k$

        \smallskip
		\Ensure Trained model $f_\theta$, Labeled dataset $\mathcal{L}$
        \medskip
		
		\State $\mathcal{L}_\text{init},~\mathcal{U}~\leftarrow~\texttt{Initialize}(\mathcal{U},~\mathcal{M})$ 
        % \Comment{\algcolor{\textbf{Initialize/Seed} (\S\ref{sec:seed-initialize})}}
		

        \State $f_\theta~\leftarrow~\texttt{Train}(\mathcal{L}_\text{init})$ 
        % \Comment{\algcolor{\textbf{Model Training} (\S\ref{sec:model-training})}}
		
        \While{\textbf{not} \texttt{Terminate}$(k, f_\theta, \mathcal{M})$} 
        % \Comment{\algcolor{\textbf{Stopping} (\S\ref{sec:terminate})}}
        
		\State $\vx~\leftarrow~\texttt{Query}(f_\theta,~\mathcal{U},~\mathcal{M})$ 
        \Comment{\algcolor{\textbf{Querying} (\S\ref{sec:querying})}}
		
        \State $(\vx, y)~\leftarrow~\texttt{Annotate}(\vx,~\mathcal{M})$ 
        \Comment{\algcolor{\textbf{Annotation} (\S\ref{sec:ann})}}
		
        \State $\mathcal{U}~\leftarrow~\mathcal{U} \;\backslash\; \{\vx\},~~\mathcal{L}~\leftarrow~\mathcal{L} \cup \{(\vx, y)\}$
		
        \State $f_\theta~\leftarrow~\texttt{Train}(\mathcal{L})$ 
        % \Comment{\algcolor{\textbf{Updating Model} (\S\ref{sec:model-training})}}
		
        \EndWhile
        \State \textbf{return} $~f_{{\theta}^{*}},~\mathcal{L}$
	\end{algorithmic}
\end{algorithm}


%As illustrated in Figure \ref{fig:LLM-based-AL}, the LLM-based Active Learning framework then consists of the following steps.



%that takes a text sequence $X \in \X$ as input and produces an output $\yhat \in \hat{Y}$, where $\yhat = \mathcal{M}(X)$; the form of $\yhat$ is task-dependent.
%The inputs are a LLM $\mathcal{M}$, an unlabeled dataset $\mathcal{U} = \{(X_1, ..., X_m)\}$, a budget $k$, and a set of prompts $\mathcal{P}$, and the outputs of the algorithm are a labeled dataset $\mathcal{L} = \{((X_1, \hat{Y}_1), ..., (X_n, \hat{Y}_n))\}$, and a trained model or an optimized prompt, $f$.


%Traditionally, active learning is a paradigm that seeks to efficiently label a subset of a large dataset to improve a model's performance while minimizing labeling costs.
%This is done by \textit{querying} for the samples whose labels would be most informative, then having the queried points \textit{annotated} by an oracle.
%Using the model to query a only subset of the unlabeled data can improve convergence and accuracy, while simultaneously reducing annotation costs.
%Using unlabeled data from real inputs can improve robustness by reducing train-test domain mismatch.

%LLM-based active learning is a new paradigm with a similar goal, but where an LLM can be used to query, annotate, augment, or even generate data.
%We summarize the algorithm for LLM-based active learning in Algorithm~\ref{alg:llm-based-AL}.
%The algorithm provides a general framework with which to compare and contrast various LLM-based active learning approaches, but is not meant to be comprehensive.


%The algorithm relies on five programs, \textsc{INIT}, \textsc{QUERY}, \textsc{ANNOTATE}, \textsc{TRAIN}, and \textsc{TERMINATE}, the first three of which can use an LLM to achieve their aims.
%In the following paragraphs we describe these functions as well as how they differ from their counterparts in traditional active learning.

%In this survey, we give additional attention to how the querying and annotation steps in Algorithm~\ref{alg:llm-based-AL} have been augmented with LLMs.
%Many LLM-based active learning methods can be categorized based on how they choose to implement the $\textsc{QUERY}$ or $\textsc{ANNOTATE}$ functions, as well as the task(s) they are focused on.
%We use them as a central part of our taxonomy in \Cref{table:qual-and-quant-comparison}, and structure Section~\ref{sec:querying} (Querying) and Section~\ref{sec:ann} (Annotation) based on this taxonomy.

%\paragraph{Initialization}
%The \textsc{INIT} program is used to acquire an initial dataset to start the active learning loop.

%Traditional active learning often faces a "cold start problem" when there are no labeled data instances available, making it challenging to select the first batch of informative samples for annotation. 
%In LLM-based active learning, this issue can be addressed by using the LLM $\mathcal{M}$ to generate an initial dataset, or annotate an initial set of labels, thereby providing a starting point for optimizing the target model.
%, which are used either as in-context examples for the LLM as an annotator, or to train a smaller model, $f$, for querying.

%\paragraph{Querying}
%In traditional active learning, querying involves selecting data points from an unlabeled pool using strategies like uncertainty-based sampling. 
%LLM-based active learning extends this by using the model to select instances either by scoring or directly choosing, order examples for curriculum learning, augment existing examples such as generating paraphrases or contrast examples, or generate entirely new labeled examples.

\begin{comment}
\subsection{\textsc{QUERY}}
The \textsc{QUERY} program is used to select the most informative points to send to an oracle for labeling.
In pool-based active learning, you query from a pool of unlabeled data with a strategy such as uncertainty-based sampling.
LLM-based active learning extends this in several ways, by using the model to:
\begin{itemize}
    \item \textit{select instances}, in some cases directly selecting a set or in others assigning a score
    \item \textit{order examples}, a problem known as curriculum learning, to improve the training of $f$
    \item \textit{augment existing examples}, for instance by generating paraphrases or opposite examples
    \item \textit{generate entirely new examples} with labels
\end{itemize}
In the latter cases, the set of queried points $\mathcal{I}$ is no longer a subset of $\mathcal{U}$.
Hence, only the points that are members of U are removed in Algorithm~\ref{alg:llm-based-AL}.

We structure our discussion of querying in Section~\ref{sec:querying} into the 4 categories, reflecting Table~\ref{tab:taxonomy-techniques}.
\begin{itemize}
    \item \textit{Traditional Selection}~(Section~\ref{sec:querying-selection}), where a method that uses the model $f$ to sample based on uncertainty or expected gain is used to select points for annotation
    \item \textit{LLM-Based Selection}~(Section~\ref{sec:querying-llm-based-selection}), where an LLM is used to select points, which can happen either by using the LLM to assign a score as its output~\citep{li2024active,azeemi2024language}, directly sampling the LLM for a subset of points~\citep{bayer2024activellm}, or using the LLM to estimate uncertainty~\citep{melo2024deep}.
    \item \textit{LLM-Based Generation}~(Section~\ref{sec:querying-llm-based-gen}) are all cases where points aren't \textit{selected} or \textit{sampled} in querying, but rather \textit{generated}. This can either be based on augmenting existing data points in the dataset, such as by generating paraphrases, or generating entirely new examples with labels~\citep{yao2023beyond,yuan2024hide,qian2024ape,diao2023active}.
    \item \textit{Hybrid}~(Section~\ref{sec:querying-llm-based-selection-and-gen}), hybrid approaches make use of both LLMs and small models to sample points for annotation, in some cases generating demonstrations with an LLM~\cite{xiao2023freeal} or using a co-prediction network~\cite{yuan2024hide}.
\end{itemize}
\end{comment}

%\paragraph{Annotation}
%The \textsc{ANNOTATE} program uses an oracle to return a set of labels for unlabeled points.
%Traditional active learning most often considers a human labeler as the oracle; given an instance of unlabeled data, the human assigns some label.
%Recently, LLMs have been used either in conjunction with, to augment, or even to entirely replace human labelers.
%In other settings, humans are used as an oracle but for non-traditional labels, such as writing a chain-of-thought given a selected input and label.

\begin{comment}
\textsc{ANNOTATE} is implemented with either:
\begin{itemize}
    \item \textit{Human Annotation}~(Section~\ref{sec:ann-human}), where a human serves as an oracle to provide labels, chains of thought, or explanations.
    \item \textit{LLM-Based Annotation}~(Section~\ref{sec:ann-llm-based}) where an LLM is used as the oracle~\citep{qian2024ape,zhang2023llmaaa,yuan2024hide,kholodna2024llms,xiao2023freeal}.
    \item \textit{Hybrid}~(Section~\ref{sec:ann-hybrid-human-and-llm}), which typically involves a multistage setup, where LLMs label some subset and humans label another subset, based on factors such as LLM confidence~\citep{ming2024autolabel,rouzegar2024enhancing}.
\end{itemize}
\end{comment}

%\paragraph{Model Training}
%The \textsc{TRAIN} program uses the labeled data to produce a trained model or optimized prompt, $f$.
%In LLM-based active learning, training often follows the same paradigm as traditional active learning: train a machine learning model on the labeled pairs.
%However, there are also instances where the result of training is not a learned model, but rather a prompt.
%In these cases, training can be selecting labeled instances that lead to the highest improvement, selecting a prompt from a list of prompts, or generating a prompt de novo.

%\paragraph{Termination}
%The active learning loop ends when a fixed annotation budget $k$ is reached, or when some property of the model being optimized, such as convergence, is satisfied.
%In LLM-based active learning, the budget can be based on the availability or cost of human annotators, the cost of prompting the LLM, or a combination of the two.


\begin{comment}
\subsection{Settings}

In this section, we highlight the various ways LLM-based AL can be used by providing an intuitive categorization based on the LLM-based AL goal.
In particular, LLM-based AL techniques can focus on
selecting or generating specific data points $\mX$,
learning an optimal prompt $P$,
selecting contexts $C$ to include in the prompt,
deciding on the LLM model $\mathcal{M}_i \in M$,
features to leverage $x_j$,
hyperparameters $\theta$,
model architectures $\mathcal{A}$,
data for annotation $\mX_{S}$,
and
evaluation data $\mX_{\rm eval}$.

\begin{compactitem}
    \item \textbf{Data Points} ($X$):
    LLM-based active learning can be used to select specific data points $x \in X$ from a larger pool for different purposes, such as training, budget-constrained training, and domain adaptation. This also includes selecting data for annotation, $x_{\text{annot}}$, which maximizes learning gains, and choosing evaluation subsets, $x_{\text{eval}}$, that ensure comprehensive model assessment.
    Furthermore, we can also leverage LLMs to generate entirely novel data points that lie outside the set of unlabeled data.
    This

    \item \textbf{Prompts} ($P$):
    The technique can be applied to select the most effective prompts $p \in P$ and prompt variations that optimize LLM outputs for specific tasks.

    \item \textbf{Contexts} ($C$):
    LLM-based active learning can be utilized to select the most relevant contexts $c \in C$ or contextual inputs that enhance model performance on context-dependent tasks.

    \item \textbf{LLM Model Variants} ($\mathcal{M}$):
    The method can be used to select the best LLM model variant $\mathcal{M}_i \in \mathcal{M}$ from a set of available models for a given input.
    Alternatively, we can also use LLM-based AL to identify models that contribute effectively to ensembles.

    \item \textbf{Features} ($x_j$):
    LLM-based active learning can be used to select important features $x_j \in \vx$ or identify new features that can be integrated to improve model accuracy and explainability.
    Furthermore, it can also be leveraged to estimate missing features, e.g., if there are several missing values in the feature vector of a specific instance.

    \item \textbf{Hyperparameters} ($\theta$):
    The technique is useful for selecting optimal hyperparameter values $\theta_i \in \theta$, such as learning rate or batch size, and adjusting them dynamically during training to optimize performance.

    \item \textbf{Model Architectures} ($\mathcal{A}$):
    LLM-based active learning can assist in selecting the most appropriate model architecture $\mathcal{A}_i \in \mathcal{A}$ or choosing between different versions of a model for specific use cases.

    \item \textbf{Data for Annotation} ($\mX_{S}$):
    The technique can be used to select specific data subsets $\mX_{S} \subset \mX$ that should be labeled by human annotators, focusing on those that provide the greatest potential improvement in model learning.

    \item \textbf{Evaluation Data Subsets} ($\mX_{\text{eval}}$):
    LLM-based active learning is beneficial for selecting evaluation data points $\mX_{\text{eval}} \subset X$ that maximize the effectiveness of model evaluation, ensuring coverage of edge cases and comprehensive testing.
\end{compactitem}
\end{comment}

\begin{comment}
\subsection{Preliminaries}\label{sec:problem-prelim}
Let $\mathcal{M}$ be an LLM parameterized by $\theta$ that takes a text sequence $X = (x_1, \cdots, x_m) \in \X$ as input 
and produces an output $\yhat \in \hat{\Y}$, where $\yhat = \mathcal{M}(X; \theta)$; the form of $\yhat$ is task-dependent. The inputs may be drawn from a labeled dataset $\D = \{ (X^{(1)}, Y^{(1)}), \cdots, (X^{(N)}, Y^{(N)})\}$, or an unlabeled dataset of prompts for sentence continuations and completions $\D = \{ X^{(1)}, \cdots, X^{(N)} \}$.
% For this and other notation, see Table~\ref{table:notation}.

% \begin{Definition}[\sc Large Language Model]\label{def:LLM}
% A \emph{large language model (LLM)} $\mathcal{M}$ parameterized by $\theta$ is 
% a model with an autoregressive, autoencoding, or encoder-decoder architecture that has been 
% trained on a large corpus of hundreds of millions to trillions of tokens, with some optional post-training for instruction following or reasoning.
% \end{Definition}


\subsection{LLM-based Active Learning Overview} \label{llm-based-AL-overview}
We summarize a basic framework for LLM-based active learning in Algorithm~\ref{alg:llm-based-AL}.
Notice that an LLM $\mathcal{M}$ is provided as input to Algorithm~\ref{alg:llm-based-AL}, and can be leveraged in both the querying and annotation steps.
Furthermore, in the querying step, the LLM can be used to generate entirely new instances (or even new labels) as well as select existing instances from the unlabeled data $\mathcal{U}$. 
% \todo{Fix this...}


Traditionally, active learning is a paradigm that seeks to efficiently label a subset of a large dataset to improve a model's performance while minimizing labeling costs. 
LLM-based active learning is a new paradigm with a similar goal, though is not bound by simply labeling a subset of a large dataset, but can also label new instances that are generated by an LLM.
% Furthermore, the cost of annotation
The formulation is typically represented as a sequential decision-making process.
% 
More formally, let $\mathcal{U} = \{\vx_i\}_{i=1}^N$ be a pool of $N$ unlabeled instances, where $\vx_i \in \mathcal{X}$ are feature vectors in the input space $\mathcal{X}$.
However, we also have a potentially infinite set of unlabeled instances $\mathcal{U}_g$ that can be generated by an LLM $\mathcal{M}$.
Furthermore, let $\mathcal{L} = \{(\vx_i, y_i)\}_{i=1}^M$ be a labeled dataset, where $y_i \in \mathcal{Y}$ are the corresponding labels from the label space $\mathcal{Y}$ where $|\mathcal{L}| = M \ll N$.
Let $k$ denote the labeling budget that limits the number of instances that can be labeled by the oracle/annotator.
One key difference in the LLM-based AL setting is the oracle/annotator can be either human or a large generative model.
This leads to additional complexity as the labeling budget that is a proxy for cost is more complex as well.
Given this, we have a predictive model $f_\theta : \mathcal{X} \to \mathcal{Y}$ parameterized by $\theta$, trained using the labeled data $\mathcal{L}$.
The goal of LLM-based AL is to iteratively select or generate the most informative instances $\vx \in \mathcal{U} \cup \mathcal{U}_g$ for labeling by an oracle/annotator, such that the model $f_\theta$ achieves the highest possible performance with the smallest labeling cost.



\todo{complete this part, add equations for various parts, and discuss trade-offs at high-level, can also reference the two taxonomy tables, etc...}

\todo{Need to also slightly modify general algorithm, since $\mathcal{U}~\leftarrow~\mathcal{U} - \mathcal{I},~~\mathcal{L}~\leftarrow~\mathcal{L} \cup \mathcal{I}'$ is not entirely correct when generating, ...}


% \ryan{In some tasks, there may not necessarily be labels.}




\subsection{Objectives}
By iteratively selecting and generating instances (to label) for training, LLM-based active learning can have the following advantages:

\begin{itemize}
    \item \textbf{Better Accuracy:} LLM-based active learning can achieve better accuracy with fewer instances by selecting and generating the most informative instances.

    \item \textbf{Reduced Annotation Costs:} LLM-based active learning techniques can reduce labeling and other annotation costs by selecting or generating the most informative samples to use for model training and fine-tuning.

    \item \textbf{Faster Convergence:} Using LLM-based active learning often leads to faster convergence as the model can be learned more quickly by iteratively selecting and generating the most informative examples.

    \item \textbf{Improved Generalization:} By leveraging LLM-based active learning techniques that iteratively select and generate the most informative and diverse examples, the model can generalize better to new data.

    \item \textbf{Robustness:} Iteratively selecting or generating the best examples for training can often improve the robustness to noise, as the model is learned from a set of high-quality representative examples.
\end{itemize}




% \subsection{Problem}

% \todo{Move this to appropriate place, and then add a more general formulation and discussion of the various facets involved, and how they differ from traditional AL.}

% \eat{%}
% Given a set of prompts (text sequence) $\mathcal{X} = \{X_1, X_2, \ldots, X_n\}$\footnote{Note the term prompt used in this work refers to the actual task instructions and question/text given as input by the user.}, a set of large language models $\mathcal{M} = \{M_1, M_2, \ldots, M_k\}$, and a set of human feedback scores $\mathcal{H} = \{h(X, Y, M)\}$ for each prompt $X \in \mathcal{X}$, output $Y=M(X)$ generated by model $M \in \mathcal{M}$, the objective is to learn a function $f: \mathcal{M} \times \mathcal{X} \rightarrow \mathbb{R}$ that predicts the expected performance of each model $M$ over the set of input prompts $\mathcal{X}$.

% Formally, let $\mathcal{D} = \{(X, Y, h) \mid X \in \mathcal{X}, Y = M(X), M \in \mathcal{M}\}$ represent the dataset of prompts, outputs, and feedback. We aim to find the model $M^* \in \mathcal{M}$ that maximizes the expected human feedback score over the set of prompts:
% \begin{align} \label{eq:basic-obj-func}
% M^* = \arg\max_{M \in \mathcal{M}} \mathbb{E}_{X \sim \mathcal{X}}[f(M, X)]
% \end{align}
% where $f(M, X)$ is the predicted feedback score for model $M$ on prompt $X$.
% In practice, the goal is to train the function $f$ using the available data $\mathcal{D}$ such that it generalizes well to unseen prompts, allowing us to identify the best-performing model $M^*$ for a given distribution of input prompts $\mathcal{X}$.
% }%







% \subsection{Selected Aspects via LLM-based Active Learning} 
% \subsection{Selected/Generated Aspects}

% \todo{rewrite}

\subsection{LLM-based AL Settings}

In this section, we highlight the various ways LLM-based AL can be used by providing an intuitive categorization based on the LLM-based AL goal.
In particular, LLM-based AL techniques can focus on 
selecting or generating specific data points $\mX$,
learning an optimal prompt $P$,
selecting contexts $C$ to include in the prompt,
deciding on the LLM model $\mathcal{M}_i \in M$,
features to leverage $x_j$,
hyperparameters $\theta$,
model architectures $\mathcal{A}$,
data for annotation $\mX_{S}$,
and
evaluation data $\mX_{\rm eval}$.



\begin{compactitem}
    \item \textbf{Data Points} ($X$): 
    LLM-based active learning can be used to select specific data points $x \in X$ from a larger pool for different purposes, such as training, budget-constrained training, and domain adaptation. This also includes selecting data for annotation, $x_{\text{annot}}$, which maximizes learning gains, and choosing evaluation subsets, $x_{\text{eval}}$, that ensure comprehensive model assessment.
    Furthermore, we can also leverage LLMs to generate entirely novel data points that lie outside the set of unlabeled data.
    This 
    
    \item \textbf{Prompts} ($P$): 
    The technique can be applied to select the most effective prompts $p \in P$ and prompt variations that optimize LLM outputs for specific tasks.
    
    \item \textbf{Contexts} ($C$): 
    LLM-based active learning can be utilized to select the most relevant contexts $c \in C$ or contextual inputs that enhance model performance on context-dependent tasks.
    
    \item \textbf{LLM Model Variants} ($\mathcal{M}$): 
    The method can be used to select the best LLM model variant $\mathcal{M}_i \in \mathcal{M}$ from a set of available models for a given input.
    Alternatively, we can also use LLM-based AL to identify models that contribute effectively to ensembles.
    
    \item \textbf{Features} ($x_j$): 
    LLM-based active learning can be used to select important features $x_j \in \vx$ or identify new features that can be integrated to improve model accuracy and explainability.
    Furthermore, it can also be leveraged to estimate missing features, e.g., if there are several missing values in the feature vector of a specific instance.
    
    \item \textbf{Hyperparameters} ($\theta$): 
    The technique is useful for selecting optimal hyperparameter values $\theta_i \in \theta$, such as learning rate or batch size, and adjusting them dynamically during training to optimize performance.
    
    \item \textbf{Model Architectures} ($\mathcal{A}$): 
    LLM-based active learning can assist in selecting the most appropriate model architecture $\mathcal{A}_i \in \mathcal{A}$ or choosing between different versions of a model for specific use cases.
    
    \item \textbf{Data for Annotation} ($\mX_{S}$): 
    The technique can be used to select specific data subsets $\mX_{S} \subset \mX$ that should be labeled by human annotators, focusing on those that provide the greatest potential improvement in model learning.
    
    \item \textbf{Evaluation Data Subsets} ($\mX_{\text{eval}}$): 
    LLM-based active learning is beneficial for selecting evaluation data points $\mX_{\text{eval}} \subset X$ that maximize the effectiveness of model evaluation, ensuring coverage of edge cases and comprehensive testing.
\end{compactitem}



\subsection{Budget}
% one facet of the problem
In LLM-based AL, one may also have a fixed budget on the cost of generation (\eg, input/output tokens).

Budget:
- Number of samples
- LLM cost of generation (input/output tokens)
- etc


% % \todo{revise text}
% Budget in terms of the monetary cost of generation via LLMs:
% Note that there are two costly elements, e.g., we may have a budget for the cost of obtaining the output $Y_{ij}$ for a given model $M_i \in \mathcal{M}$ and input prompt $X_j \in \mathcal{X}$.
% This is difficult as the cost is based on the number of input tokens (which can be computed apriori, and the number of tokens of the output from the model, which is unknown, but can be somewhat estimated or upper bounded).

% Alternatively, the problem may be formulated with respect to a budget $B$ on the human feedback for a specific model $M_i$, input prompt $X_j$, and output $Y_{ij} = M_i(X_j)$, forming a tuple $(M_i, X_j, Y_{ij}=M_i(X_j))$.



\subsection{Add other facets}
Discuss other unique facets to this problem, key differences, new challenges, and opportunities.


\subsection{Overview of Taxonomy 1}

\subsection{Overview of Taxonomy 2}





\end{comment}



\begin{comment}
\subsection{Unifying Framework: From AL to Bandits and Beyond}
\label{sec:unifying-settings}
%, Online Learning, RLHF, Pandoras Box, and Beyond}
% \ryan{add table sketched in notebook}

%Also fine-tuning, unlearning, dynamical systems etc

Add unification and show that these are recovered (certain instantiations)

\ryan{cite relevant papers for known connections, bandits--pandoras, contextual bandits online linear regression,} 



In this section, we aim to provide a unifying view of the problem through the lens of bandits, online learning, RLHF, pandoras box, among others.
We provide an intuitive summary of such connections in Table~\ref{tab:comparison-problem-settings} across a variety of problem settings.
We also provide a more fine-grained analysis in Table~\ref{tab:AL-connections-overview}.

Overlap in Goals: Many of these techniques aim to optimize resource use (e.g., labels, feedback, compute) while improving learning.
% 
Differences in Constraints: Some approaches assume explicit costs (e.g., Pandora’s Box), while others lack such constraints (e.g., online learning).
% 
Sequential Learning: Nearly all approaches operate sequentially, but they differ in what drives decisions (e.g., reward, uncertainty, cost).

% These insights allow practitioners to choose the right framework depending on the data, task, and constraints of their problem.


% \subsection{Bandits}

% \subsection{RLHF}

% \subsection{Pandoras Box}

In work by~\citet{atsidakou2024contextual} it is shown that contextual Pandoras box can be reduced to contextual bandits and further reduced to online regression.
% From Contextual Bandits to bandits to online Regression

% \subsection{Online Learnimg}

% \subsection{Fine-Tuning}

% \subsection{OD}


\ryan{This section can be short 1/2 page, and just mathematically show various connections, and discuss them. Would be good to show unifying equation, then show equations, and how they fit under it, etc.}
\ryan{I also created an initial table, see Table~\ref{tab:AL-connections-overview} in Appendix. Likely can be improved, etc}



\renewcommand{\arraystretch}{1.5} % Increase row height for readability

\begin{table*}[h!]
\centering
\footnotesize
% \rowcolors{2}{gray!10}{white}
\begin{tabular}{p{2.5cm} *{5}{>{\centering\arraybackslash}m{0.5cm}}
H
*{2}{>{\centering\arraybackslash}m{0.5cm}}
}
\toprule
% \textbf{Technique} 
& 
\rotatebox{90}{\textbf{Sequential Learning}} & 
\rotatebox{90}{\textbf{Active Querying}} & 
\rotatebox{90}{\textbf{Human Feedback}} & 
\rotatebox{90}{\textbf{Exploration-Exploitation}} & 
\rotatebox{90}{\textbf{Cost-Awareness}} & 
\rotatebox{90}{\textbf{Pre-trained Models}} & 
\rotatebox{90}{\textbf{Statistical Foundations}} & 
\rotatebox{90}{\textbf{Dynamic Environments}} \\
\midrule

\textbf{Active Learning} & 
\cmark & \cmark &  & \cmark &  &  & \cmark & \cmark \\

\textbf{RLHF} & 
\cmark & \cmark & \cmark & \cmark &  &  &  & \cmark \\

\textbf{Pandora’s Box} & 
\cmark & \cmark &  & \cmark & \cmark &  & \cmark & \cmark \\

\textbf{Bandits} & 
\cmark &  &  & \cmark &  &  &  & \cmark \\

\textbf{Optimal Design} & 
 &  &  &  & \cmark &  & \cmark &  \\

\textbf{Online Learning} & 
\cmark &  &  & \cmark &  &  & \cmark & \cmark \\

% \textbf{Fine-tuning} & 
%  &  &  &  &  & \cmark &  & \cmark \\

\bottomrule
\end{tabular}

\caption{%
Comparison of problem settings across a range of important properties.
\textit{Sequential Learning}: Learning incrementally over time, rather than all at once. 
\textit{Active Querying}: Actively selecting or querying data points to improve the model. 
\textit{Human Feedback}: Utilizing human input or feedback to guide and improve learning. 
\textit{Exploration-Exploitation}: Balancing the need to explore new options versus exploiting known ones. 
\textit{Cost-Awareness}: Considering the costs of acquiring data or feedback during the learning process. 
% \textbf{Pre-trained Models}: Using pre-existing models as a starting point for further learning or adaptation. 
\textit{Statistical Foundations}: Grounding the technique in statistical principles or experimental design. 
\textit{Dynamic Environments}: Adapting to environments that change over time or where new information continuously emerges.
% \eat{%}
% \todo{Fix, add more factors, add grey checkmark, and define the properties more formally/intuitively that are used to summarize, etc}
% \todo{Still need to fix some stuff and improve. Should add math formulation, etc.}
% }%
}
\label{tab:comparison-problem-settings}
\end{table*}





\end{comment}






% \subsection{LLM Selection and Output Generation}

% \begin{algorithm}
% \caption{LLM-based Active Learning Framework for LLM Selection and Output Generation}
% \label{alg:LLM_selection_active_learning}
% \begin{algorithmic}[1]
% \Require Set of LLMs $\{\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_N\}$, unlabeled task dataset $D_T$, labeled dataset $D_L$, query strategy $\mathcal{Q}$, evaluation metric $\mathcal{E}$, oracle for verification
% \Ensure Model $\mathcal{S}^*$ trained for LLM and generation selection
% \State Initialize $D_L \leftarrow \text{Seed labeled data with known LLM outputs}$
% \State Initialize $D_T \leftarrow \text{Task-specific data requiring LLM selection and generation analysis}$
% \State Train initial selector model $\mathcal{S}$ on $D_L$
% \While{stopping criteria not met}
%     \State $\text{Select a subset of tasks } D_Q \subset D_T \text{ using } \mathcal{Q}(\mathcal{S}, D_T)$
%     \For{each task $t \in D_Q$}
%         \State Generate outputs $\{g_1, g_2, \dots, g_N\}$ using LLMs $\{\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_N\}$
%         \State Evaluate outputs $\{g_1, g_2, \dots, g_N\}$ using $\mathcal{E}$ and rank them
%         \State Annotate $t$ with the best LLM and its output using the oracle
%     \EndFor
%     \State Add annotated tasks in $D_Q$ to $D_L$
%     \State Remove $D_Q$ from $D_T$
%     \State Retrain $\mathcal{S}$ on the updated $D_L$
% \EndWhile
% \State \Return Trained selector model $\mathcal{S}^*$
% \end{algorithmic}
% \end{algorithm}

% We summarize a general framework for LLM-based model selection and output generation in Algorithm~\ref{alg:LLM_selection_active_learning}.
% Let ${\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_N}$ represent a set of large language models (LLMs) available for task generation, where each model may produce distinct outputs based on its unique strengths and capabilities. The unlabeled task dataset, $D_T$, comprises tasks requiring evaluation to determine which LLM and output are most suitable for each task. The labeled dataset, $D_L$, initially contains a subset of tasks with known mappings to the best LLM and its optimal output, serving as a seed for training. The selector model, $\mathcal{S}$, is the model being actively trained to learn how to select the most appropriate LLM and output for each task. The query strategy, $\mathcal{Q}$, is a method for identifying the most informative tasks in $D_T$ to prioritize for labeling. The evaluation metric, $\mathcal{E}$, is used to score and rank the outputs from each LLM based on criteria relevant to the task, such as accuracy, relevance, or other task-specific measures. Finally, the oracle represents a reliable source of annotations, either a human expert or a robust automated system, that confirms the best LLM-output pairing for each task selected by $\mathcal{Q}$. Together, these components enable iterative training of $\mathcal{S}$ to improve its ability to select the most effective LLM and output for a variety of tasks in $D_T$.










% \section{From Querying to Generation} \label{sec:annotation}
\section{Query: From Selection to Generation \writer{Subho}} \label{sec:querying}
Active learning (AL) fundamentally seeks to maximize model performance with minimal annotation cost by carefully selecting the most informative examples. 
Traditionally, this process has relied on uncertainty-based and diversity-based metrics \cite{settles2011theories, wang2014new, geifman2017deep, citovsky2021batch}. 
%\todob{Cite. You have the references in \cref{sec:querying-selection}.}
With the advent of large language models (LLMs), however, the paradigm is evolving from merely selecting examples from a fixed unlabeled pool to also generating new, high-value queries on demand.


\begin{figure*}[t!]
\centering
\includegraphics[width=0.88\linewidth]{graphics/fig-LLM-based-AL.pdf}
% \includegraphics[width=1.0\linewidth]{graphics/fig-LLM-based-AL-v1.pdf}
\caption{%
Our proposed taxonomy classifies LLM-based active learning (AL) methods by their querying and annotation processes, the two key components of AL. 
Beyond selection,  LLMs enable the querying module to also generate unlabeled instances, while the annotation module assigns labels using LLMs, human annotators, or both.
% Overview of the proposed LLM-based active learning taxonomy.
}
\label{fig:LLM-based-AL}
%\vspace{4mm}
\vspace{-3mm}
\end{figure*}

\subsection{Traditional Selection} \label{sec:querying-selection}

In this section, we briefly survey some of the key existing active learning query selection strategies. 
%All active learning query selection strategies try to balance uncertainty and diversity in selecting unlabeled examples for annotation. 
The uncertainty metric captures how uncertain the model is in predicting the label of the example, whereas the diversity metric captures how different the chosen example is from the already selected examples in the labeled pool. 
Uncertainty-based methods, such as \text{Least Confidence}~\citep{settles2009active, settles2011theories, wang2014new}, \text{Margin Sampling}~\citep{tong2001support, balcan2009agnostic, settles2009active}, and \text{Max-Entropy}~\citep{wang2014new, kremer2014active, diao2023active}, quantify how unsure a model is about its predictions. Complementary to these, diversity-based strategies—like \text{Coreset}~\citep{sener2017active, geifman2017deep, citovsky2021batch} and \text{CDAL}~\citep{agarwal2020contextual}—ensure that selected examples cover varied regions of the input space. Hybrid approaches, including \text{Badge}~\citep{ash2019deep, ash2021gone} and \text{Bald}~\citep{kirsch2019batchbald, pmlr-v70-gal17a}, strike a balance between these two aspects. For a comprehensive discussion of these traditional strategies, see \Cref{app:trad-strategy} and recent surveys in \citet{ren2021survey, li2024survey}.
%Some of the diversity-based selection strategies include \texttt{Coreset} \citep{sener2017active, geifman2017deep, citovsky2021batch}, \texttt{CDAL} \citep{agarwal2020contextual}. Some uncertainty-based methods include \texttt{Least confidence} \citep{settles2009active, settles2011theories, wang2014new}, \texttt{Margin} \citep{tong2001support, balcan2009agnostic, settles2009active}, \texttt{Max-entropy} \cite{wang2014new, kremer2014active, diao2023active}. Some methods try to trade-off between the two strategies like \texttt{Badge} \citep{ash2019deep, ash2021gone}, \texttt{Bald} \citep{kirsch2019batchbald, pmlr-v70-gal17a}. We discuss all these methods briefly in \Cref{app:trad-strategy}. Interested reader can read more recent works on deep active learning in \citet{ren2021survey, li2024survey}.






% \todos{This is just surveying traditional LLM selection strategies? I can write this up. 1. Coreset, 2. Least confidence, 3. Margin, 4. Max entropy, 5. Badge, 6. Bald. There are a few recent ones also based on optimal design I can briefly talk about that. There is also rejection sampling methods.}

% \subsubsection{Traditional Strategies}


\subsection{LLM-based Selection} \label{sec:querying-llm-based-selection}
% Selection via LLMs

With the emergence of LLMs, active learning strategies are being reimagined to exploit their powerful in-context reasoning and few-shot capabilities. 
In LLM-based selection, the model itself plays a dual role—both as a predictor and as a selector of informative queries. 
For instance, {ActiveLLM} \cite{bayer2024activellm} leverages an LLM to assess uncertainty and diversity in a completely unsupervised manner, making it particularly suitable in few-shot and model mismatch settings.
%
%bayer2024activellm, parkar2024selectllm
%\texttt{ActiveLLM}~\cite{bayer2024activellm} uses LLMs to select instances for the few-shot and model mismatch setting. Importantly, ActiveLLM can estimate uncertainty and diversity without any annotated data, and does not require training during the annotation process.
% 
%
Similarly, \text{ActivePrune}\cite{azeemi2024language} applies an LLM-driven approach to prune large unlabeled pools, reducing the computational burden of traditional acquisition functions for tasks such as translation, sentiment analysis, topic classification, and summarization.
%
%
%In contrast, \texttt{SelectLLM}~\citep{parkar2024selectllm} directly uses an LLM to select unlabeled examples by prompting LLMs. Specifically, \texttt{SelectLLM} uses LLMs to estimate
%the usefulness and impact of each example in the unlabeled pool and then refines these examples into subsets using k-NN to use them for few-shot learning with LLMs. 
In another line of work, \text{SelectLLM}~\citep{parkar2024selectllm} prompts LLMs directly to evaluate and rank the usefulness of unlabeled examples; the ranked instances are then refined using k-NN clustering to form effective few-shot demonstrations.
Recent work by \citet{jeong2024llm} further demonstrates that LLMs can generate meaningful rankings of examples, which in turn can inform fine-tuning for downstream tasks.
%\citet{jeong2024llm} show that LLMs can generate ranks of examples (based on their features), and then use the features of these ranked examples to fine-tune LLMs for downstream tasks.

% \todos{The above selection strategies roughly work for LLMs as well. There are two ways: If few shot/in-context learning then just pick 1-4, get their contextual representation from the LLM (I am considering an LLM that is more than a black box) and plug in the method to get the next most informative sample(s). If we also have the freedom to iteratively train the LLM with samples or batches of samples then use 5-6 as well. These require gradient computation information.}
% \ryan{LLMs are also used to select directly whatever it is we are doing active learning on}

\subsection{LLM-based Generation} \label{sec:querying-llm-based-gen}
Beyond selection from a fixed unlabeled set, LLMs also enable the generation of entirely new examples and labels, thereby extending the AL paradigm to an effectively infinite search space. In the following, we distinguish between generation strategies that remain within the confines of an existing unlabeled pool and those that extend beyond it.
%In this section, we discuss generating new examples using LLMs, and/or annotating these examples with LLMs and finally using them for few shot learning. 



% Generating new examples entirely

% Generating new label types

% Generating model

% \todos{This is actually the most interesting one and least studied somehow (I feel). We have to look around for this. I am guessing you are saying that generate both $x$ and $y$.}

% \ryan{I wrote below. This is one example where they actually do generation which is then used for selection, but there are many other works, and probably more so, on that actually generate new examples not in the "unlabeled set", and then label it, etc. This is probably more interesting (and what I focused on in the figures), at least to me, as it changes the AL paradigm and problem entirely. The AL problem is usually selecting from an unlabeled set, but LLMs make it that we are selecting from a possibly infinite set, since they can generate us new examples}

\paragraph{Generation for Selection within Unlabeled Set}
Several recent works integrate traditional selection metrics with LLM capabilities to improve query selection for few-shot learning. For example, \citet{margatina2023active} employ a combination of k-NN and perplexity-based strategies to balance uncertainty and diversity, demonstrating that uncertainty sampling may become an emergent property in larger models for in-context learning. In a similar vein, \citet{mukherjee2024experimental} highlight the effectiveness of experimental design techniques, such as G-optimal design, for selecting high-impact unlabeled examples. Additionally, \citet{diao2023active} harness LLMs to generate multiple answers to a given question, using the variability among these answers as a proxy for uncertainty—albeit without venturing outside the initial dataset. Meanwhile, \text{EAGLE}~\citep{bansal2023large} first samples examples based on a conditional informativeness criterion and then leverages LLMs to generate in-context labels, streamlining the annotation process.

%\todob{There are many methods for active learning with pairwise and $K$-wise active human feedback elicitation. They formulate the problem as identifying a subset of prompts with candidate responses, either online or offline, where preferential feedback would improve policy learning by RLHF, either through a reward model or DPO \citep{rafailov23direct}. These works differ in how the prompts are selected: \citet{mehta23sample,ji2024reinforcement,muldrew2024active} choose prompts based on differences of estimated rewards to their responses; \citet{mukherjee24optimal,scheid24optimal,thekumparampil24comparing} derive optimal policies for offline exploration using D-optimal designs \citep{pukelsheim06optimal}; and \citet{das24active,liu24dual} solve D-optimal designs online using a greedy algorithm. Most works prove that the errors in learned reward models diminish with more feedback.}

%We can divide this approach into two parts, in the first part we will look into how query selection strategies are used in \Cref{sec:querying-selection} are incorporated with an LLM to select examples for annotation, and in the second part we discuss how some recent works have used an LLM to directly select unlabeled examples for annotation. 
%
%Several recent works integrate traditional selection metrics with LLM capabilities to improve query selection for few-shot learning. 
%For example, \citet{margatina2023active} use K-NN, and perplexity-based query selection strategies to balance diversity and uncertainty and select examples for annotation for few-shot learning with LLMs. However, they show that uncertainty sampling approaches alone do not generalize well to in-context learning and larger models might perform better with uncertain demonstrations, hinting that uncertainty might be an emerging LLM ability. Similarly, \citet{mukherjee2024experimental} show that experimental design like G-optimal design can be used to select unlabeled examples for few-shot learning with LLMs.
%
%\citet{diao2023active} uses LLMs to generate $k$ answers to a question that are then used to measure the uncertainty for selection.
%In this work, they simply leverage LLMs to generate answers which are used to estimate the uncertainty of a given question, however, they do not generate new questions not in the initial dataset.
%
%\citet{bansal2023large} proposes the \texttt{EAGLE} algorithm that first samples existing unlabeled examples using a conditional informativeness condition and then uses the LLM to generate the label in in-context learning setting. 



\paragraph{Generation for Selection outside Unlabeled Set}

A more radical departure from traditional AL involves generating new examples that are not present in the original unlabeled pool. The \text{APE} framework~\citep{qian2024ape} uses a Query-by-Committee strategy combined with chain-of-thought prompting to synthesize new prompts that are then sent to human annotators. Other works, such as those by \citet{yang2024rewards} and \citet{mukherjee2024multi}, generate both new examples and their labels using a trained LLM. These generated examples are then subjected to a rejection sampling process, ensuring that only those meeting predefined accuracy thresholds are retained. Similarly, \citet{yao2023beyond} use an explanation-generation model to produce human-guided rationales, which not only enhance label quality but also inform a novel diversity-based selection strategy akin to coreset sampling.

%A more radical departure from traditional AL involves generating new examples that are not present in the original unlabeled pool. 
%\text{APE}~\citep{qian2024ape} generates new prompts using a Query-by-Committee sampling strategy and chain-of-thought prompting style technique and then uses humans to label these prompts.
%
%\citet{yang2024rewards} and \cite{mukherjee2024multi} generate both new examples and their labels from a trained LLM, then uses rejection sampling to remove examples that fall below the expected threshold of accuracy for some objectives and then again finetune the LLM to align it better for multiple objectives simultaneously. 
%
%Similarly, \citet{yao2023beyond} implement an explanation generation model to produce explanations that are 
%guided by human explanations which are obtained using a diversity-based AL sampling strategy similar to coreset for obtaining the explanation annotations. Then they use a prediction
%model that utilizes those explanations for few shot predictions, and a novel data
%diversity-based AL sampling strategy similar to coreset for obtaining the explanation annotations.


% \ryan{An alternative way to organize this, and other sections is to focus on what LLMs are being used to actively optimize (models, data points, features, etc).}


\subsection{Hybrid} \label{sec:querying-llm-based-selection-and-gen}
Recognizing that neither pure selection nor generation can fully address all challenges in AL, recent work has begun to explore hybrid strategies that combine both. For instance, \texttt{NoiseAL} \citep{yuan2024hide} employs a two-stage process: small LLMs first identify promising unlabeled examples, which are then passed to an annotator LLM for labeling. Similarly, the Causal-guided Active Learning (CAL) framework \citep{du2024causal} integrates density-based clustering with LLM-driven query selection (e.g., via GPT-4) to autonomously identify and correct bias patterns in unlabeled data. Such hybrid methods seek to leverage the complementary strengths of LLMs and traditional human-in-the-loop strategies, ultimately pushing the boundaries of active learning toward more efficient and robust systems.
%In this section, we discuss hybrid LLM-based strategies for querying that leverage both LLM-based selection and LLM-based generation.
%
%For instance, \text{NoiseAL}~\citep{yuan2024hide} uses two small LLMs to select unlabelled examples and then uses another annotator LLMs to obtain labels of the samples. 
%Similarly, \text{CAL}~\citep{du2024causal} combines active learning with the causal mechanisms
%and proposes a casual-guided active learning
%(CAL) framework. They use a density-based clustering method and utilize LLMs like GPT4 to automatically and autonomously identify informative unlabeled samples and induce bias patterns in the examples.
%Such hybrid methods seek to leverage the complementary strengths of LLMs and traditional human-in-the-loop strategies, ultimately pushing the boundaries of active learning toward more efficient and robust systems.


% Joe Barrow to help
\section{Annotation: From Human to LLMs \writer{Xintong Li \& Hanjia}}
% LLM-based} 
\label{sec:ann}
Annotation has traditionally relied on human experts for high-quality labels. 
Recently, leveraging LLMs as annotators has further reduced annotation expenses, though challenges such as bias and label inconsistency. 
A hybrid approach that integrates human expertise with LLM-based annotation offers a promising solution, balancing efficiency and accuracy through dynamic task routing, verification mechanisms, and prompt engineering strategies.

\input{table-techniques}

\subsection{Human Annotation \writer{Xintong Li \& Hanjia}} \label{sec:ann-human}
Traditional human annotation involves sending selected or generated instances to annotators for labeling, which remains the most accurate approach but is often costly. Several recent works have explored active learning strategies to optimize the annotation process. 
ActivePrune~\cite{azeemi2024language} and CAL~\cite{du2024causal} reduce annotation costs by actively selecting the most valuable instances for labeling. 
\hanjia{For instance, in the case of imbalanced data, enhancing the model's performance on the minority class can not only improve overall accuracy but also mitigate biases. One effective approach to achieving this is by increasing the size of the minority class. Providing human annotators with data that includes more minority samples has been shown to be effective~\cite{lyu2022social}. }
Active-Prompt~\cite{diao2023active} and AL-Principle~\cite{margatina2023active} focus on guiding LLMs by selecting instances where annotators verify final answers or prompt examples before model predictions. 
Furthermore, Beyond-Labels~\cite{yao2023beyond} extends traditional annotation by collecting short rationales or natural language explanations alongside labels, improving both model interpretability and performance. 
Additionally, APL~\cite{muldrew2024active} and BAL-PM~\cite{melo2024deep} incorporate human preference learning by asking annotators to compare or rank multiple model outputs. 
\hanjia{However, there are still several challenges in human annotation. Human annotator variability, as differences in expertise, cognitive biases, and annotation consistency can lead to label noise and disagreements, ultimately affecting model performance. Moreover, bias and fairness considerations in active learning are also an active area of research, as human annotations can inadvertently reflect societal biases, leading to skewed model predictions.}

\subsection{LLM-based Annotation \writer{Hanjia}} \label{sec:ann-llm-based}
\hanjia{To address the challenges in human annotation, ongoing research explores leveraging LLMs as annotators in active learning, primarily to reduce annotation costs. 
FreeAL~\cite{xiao2023freeal}, which distills task-specific knowledge with the help of a downsteam small language model, demonstrates improved performance without any human supervision.
Similarly, by employing {\tt GPT-4-Turbo} for annotating low-resource languages, \citet{kholodna2024llms} reported substantial reductions in estimated annotation costs compared to human annotation.
However, a key challenge in LLM-based annotation is ensuring high-quality labels. To mitigate quality issues, LLMaAA~\cite{zhang2023llmaaa} incorporates in-context examples, demonstrating improved annotation reliability.
Despite such advancements, LLMs, like human annotators, are susceptible to biases inherited from their training data. Research has shown that LLMs' responses to cognitive psychological tasks often resemble those of individuals from western, educated, industrialized, rich, and democratic societies~\cite{atari2023humans}. Biases in LLM-based annotations also persist in certain domains, such as political science~\cite{zhang2024electionsim}. For example, \citet{qi2024representation} identified three dimensions of bias in LLM-generated political samples: societal and cultural contexts, demographic groups, and political institutions.
A particular concerning case arises when LLMs are used to annotate content that has also been generated by LLMs. In such scenarios, there is a risk of \textit{self-reinforcement bias}, where the model's inherent tendencies are amplified rather than corrected. This could create a feedback loop where the model's performance may appear artificially inflated.
LLM-based annotations can also be sensitive to input variations~\cite{mizrahi-etal-2024-state}. Slight changes in prompt phrasing, context, or model sampling parameters can lead to inconsistent annotations~\cite{10.1145/3689217.3690621}. Ensuring consistency in LLM-generated annotations remains an ongoing challenge, requiring further research into prompt engineering, calibration techniques, and hybrid human-LLM validation strategies.
}
\begin{table*}[!t]
\centering
\small
\begin{tabular}{rlH}
\toprule
\multicolumn{1}{c}{\textbf{Task}} & \textbf{Description} & \textbf{Use Case} \\ 
\midrule
\textbf{Text Classification } (Sec.\ref{apps-text-classification}) & Selects uncertain or ambiguous texts for classification. & Sentiment analysis, topic categorization. \\ 
% \midrule

\textbf{Text Summarization } (Sec.\ref{apps-text-summarization}) & Chooses diverse or complex document types to improve summarization. & Summarizing news articles, scientific papers, etc. \\

\textbf{Non-Text Classification  } (Sec.\ref{apps-non-text-classification}) & The pairing of non-text samples, such as images, with labels. & TODO. \\ 

\textbf{Question Answering }  (Sec.\ref{apps-question-answering})  & Chooses ambiguous or difficult questions for QA systems to refine. & Answering questions in specialized domains like law or medicine. \\ 

\textbf{Entity Modeling }  (Sec.\ref{apps-entity-matching})  & A binary classification task to pair entities with one of two labels. & TODO. \\ 

\textbf{Debiasing }  (Sec.\ref{apps-active-debiasing})  & The process of reducing measured bias in machine-generated output. & TODO. \\ 

\textbf{Translation } (Sec.\ref{apps-translation})  & Selects sentences or phrases with high uncertainty in translation. & Improving accuracy of translations, especially for low-resource languages. \\ 

\textbf{Sentiment Analysis } (Sec.\ref{apps-sentiment-analysis})  & Focuses on sentences where the model is uncertain about sentiment. & Determining sentiment in reviews, social media posts, etc. \\ \bottomrule

% \midrule
\end{tabular}
\caption{
% \todo{Ryan A. to update, make consistent with subsections in application section, and add references for each} 
% \todo{Ryan Aponte: please also reorder subsections, to be same order as in Table 2, etc.}
Applications of LLM-based Active Learning.
%\ryan{This table needs to highlight the unique aspects for each application offered by LLM AL}
}
\label{tab:applications}
\vspace{-3mm}
\end{table*}

\subsection{Hybrid \writer{Hanjia \& Xintong}} \label{sec:ann-hybrid-human-and-llm}
% In this section, we discuss techniques that leverage both humans and LLMs to obtain better annotations.
\hanjia{While LLM-based annotation has significantly reduced costs, it remains prone to errors, particularly in complex or domain-specific tasks~\cite{lu2023human}. To address this issue, researchers have developed methods to evaluate annotation quality and dynamically route data to either LLMs or human annotators, balancing efficiency and accuracy. For example, \citet{wang2024human} proposed a multi-step human-LLM collaborative approach where LLMs first generate labels and explanations. A verifier then assesses the quality of these labels, and human annotators re-annotate a subset of low-quality cases. Similarly, }
\citet{rouzegar2024enhancing} investigates using LLMs with human annotations for text classification to achieve lower costs while maintaining accuracy \hanjia{based on confidence thresholds}. 
\hanjia{Another approach to combining human expertise with LLMs involves having humans curate a set of annotated examples, which are then incorporated into LLM prompts to enable annotation in a few-shot learning manner. While this method is both intuitive and effective, selecting optimal examples remains challenging. \citet{qiu2024semantics} show that examples included in prompts can sometimes overly constrain LLM decision-making, leading models to favor labels that align closely with provided examples rather than considering a broader range of possibilities. Refining strategies for selecting and structuring prompt examples is therefore an important direction for improving LLM-assisted annotation.}



\section{Termination: From Criterion to LLMs \writer{Junda}} \label{sec:termination}
Termination criterion in active learning loop is crucial for balancing model performance improvements with annotation costs. 
In LLM-based AL, termination criteria must consider not only traditional factors such as annotation budget $k$, model performance gains or uncertainty reduction, 
but also the variable costs associated with LLMs.

%\todob{I do not understand this section. When we formulated the problem, we clearly said that the stopping criterion is the budget $k$. Why do we need other termination criteria?}

\subsection{Traditional Approaches} \label{sec:termination-traditional}
In traditional active learning, 
one widely adopted strategy is to stop querying when performance improvements on a validation set fall below a predefined threshold \cite{settles2009active}.
Other approaches monitor the stability of the model’s predictions or the uncertainty estimates, terminating annotation once these metrics indicate that the model has sufficiently converged \cite{tong2001active}.
In addition, several theoretical frameworks provide sample complexity bounds as termination criteria, ensuring that marginal returns are decreasing \cite{zhu2003combining}.
However, such criteria assume that the cost per annotation is uniform and homogeneous, which simplifies the stopping rule.


 
\subsection{Cost-Aware Termination} \label{sec:termination-hybrid}
In LLM-based AL, estimating the annotation cost can be challenging and complex.
While traditional AL relies on a discrete budget $k$ to represent the number of human annotations, 
LLM-based AL may combine both human and LLM annotations. 
In such cases, the cost of an annotation depends not only on the source (human vs. LLM) but also on variable factors like input and output token counts.
Even in the case that LLM-based annotations are used without any human annotations, the cost cannot be easily approximated as the discrete budget that often represents the amount of examples that can be labeled.
It is straightforward to see that since the cost depends on the input and more so the output tokens, then the budget may be better represented as a real-valued amount that pertains directly to a monetary cost.
Recent exploration in hybrid termination criteria \cite{akins2024cost,pullar2024hitting} including cost-aware termination criterion that integrates token-level cost analysis and performance plateau detection. 
Other works have suggested a combined cost-performance metric that balances the fixed costs of human annotations with the variable costs of LLM-based annotations  \cite{xia2024llm, zhang2024interactive}.


% amount of  may be a real-valued is no longer discrete, nor can be easily approximated with the budget $b$, as $b$ in both cases 

% annotating making the cost 
% is cost of labeling data via humans is is defined by  is usually a proxy for the budget $b$




% todo...


% \section{LLMs for Active Learning vs. Active learning for LLMs \writer{Yu Xia}}
%\ryan{%
%Active learning for LLMs vs. LLM for Active Learning
%}

\section{AL Paradigms with LLMs \writer{Yu Xia}} \label{sec:setting}

With the rise of LLMs, AL has evolved to address new challenges and opportunities across various learning paradigms. In this section, we briefly outline four LLM-based AL paradigms, where more details can be found at Appendix \ref{app:setting}. 
We also take a step further and briefly discuss in Appendix \ref{sec:unifying-settings} a unifying view of the LLM-based AL problem through the lens of bandits, online learning, RLHF, pandoras box, among others.

\paragraph{Active In-Context Learning} Recent studies frame few-shot demonstration selection as an active learning problem, leveraging semantic coverage and ambiguity-driven sampling to optimize LLM performance~\citep{margatina2023active, mavromatis2024covericl, qian2024ape}. 

\paragraph{Active Supervised Finetuning} To reduce labeling costs, active learning has been integrated into supervised finetuning via uncertainty-based querying, self-training on low-uncertainty data, and strategic sample selection~\citep{yu-etal-2022-actune, xia-etal-2024-hallucination, bayer2024activellm}. 

\paragraph{Active Preference Alignment} Efficient label selection is critical in reinforcement learning from human feedback (RLHF). Recent approaches employ targeted preference queries to accelerate alignment and improve data efficiency~\citep{ji2024reinforcement, muldrew2024active, chen2024cost}. 




\paragraph{Active Knowledge Distillation} Selective knowledge transfer from LLMs to smaller models reduces computational costs. Methods using uncertainty-based sample selection and iterative student feedback improve distillation efficiency while maintaining performance~\citep{zhang2024elad, liu-etal-2024-evolving, palo-etal-2024-performance}.




%\citet{zhang2024elad} developed an LLM-based active knowledge distillation approach called ELAD that performs efficient sample selection via an explanation-guided approach to identify samples by utilizing the uncertainty in the explanation steps.
%\cite{liu-etal-2024-evolving}
%\cite{palo-etal-2024-performance}


%ActivePrune~\cite{azeemi2024language} introduces a language model approach for pruning unlabeled instances for active learning settings where the unlabeled instance pool is large, and thus, computationally costly for an acquisition function to search over.






% \section*{Techniques}
% % \writer{TODO}} 
% \label{sec:techniques}

%\todo{revise into more general/intuitive categories, and merge}
%\ryan{Note that see Section~\ref{sec:setting} for another possibility. Likely this section, and Section~\ref{sec:setting} will be merged.}




%\subsection{Active DPO}





%\subsection{RLHF}
%Reward models are designed to align LLMs with human preferences through reinforcement learning with human feedback (RLHF)~\citep{ouyang-etal-2022-training, korbak-etal-2023-pretraining}. 
%Traditionally, RLHF leverages human preference data in two primary ways: it is either used to train independent discriminative RMs or integrated directly into LLMs using methods such as DPO \citep{rafailov-etal-2024-direct} or SLiC-HF \citep{zhao-etal-2023-slic}.



%\subsection{Automatic Evaluation}
%autoraters for automatic evaluation using LLMs~\cite{vu2024foundational}
% 
%few-shot ordinal classification~\cite{qin2024lampo}
% \Yu Xia: I did not find any mentioning of Active Learning these two papers so removed this subsection.



%\subsection{Active Learning in LLMs (general)}
% 
% 
%low-resource languages~\cite{kholodna2024llms}











%%% SUBHO: I moved activLLM to 3.2
% ActiveLLM~\cite{bayer2024activellm} uses LLMs to select instances for the few-shot and model mismatch setting. Importantly, ActiveLLM can estimate uncertainty and diversity without any annotated data, and does not require training during the annotation process.


%%% SUBHO: I moved activeprune to 3.2
% ActivePrune~\cite{azeemi2024language} introduces a language model approach for pruning unlabeled instances for active learning settings where the unlabeled instance pool is large, and thus, computationally costly for an acquisition function to search over. They showed the utility of ActivePrune for translation, sentiment analysis, topic classification, and summarization.
%
%
% Experiments on translation, sentiment analysis, topic classification, and summarization tasks on four diverse datasets and four active learning strategies demonstrate that ActivePrune outperforms existing data pruning methods.









% \section{Evaluation Methodology \writer{TODO}} \label{sec:eval}

% \todo{Discuss various methodologies for evaluation, and settings, etc}

% \subsection{Methodology}
% % \subsubsection{}

% \subsubsection{Budget}

% - Compute cost incurred by using the LLM $\mathcal{M}$ (\eg, input/output tokens, etc)
% - Number of examples
% - Time


% \subsection{Metrics}



% \section{Datasets \writer{TODO}} \label{sec:datasets}
% \hongjie{
% look for datasets that are specific to LLM-based AL tasks.
% checkout the datasets in this paper~\cite{yuan2024hide}.
% }
% % \todo{this section may not be needed, unless dataset is specific to AL, since most can be used}
% In this section, we provide a summary of the useful datasets that either have been used or can be used for various LLM-based AL tasks.
% As an aside, we do not include the abundance of traditional datasets, and focus entirely on datasets that are unique and interesting for an LLM-based AL task or setting.
% \todo{provide example, and create a table, and categorize the datasets accordingly}








\section{Applications \writer{Ryan Aponte}} \label{sec:apps}

LLM-based active learning (AL) has been applied across diverse tasks, reducing annotation costs and improving performance in data-scarce settings. 
We summarize in Table~\ref{tab:applications} key applications and specific use cases.
Table~\ref{table:qual-and-quant-comparison} also bridges these applications with our taxonomy of techniques, which provides an intuitive comparison of the state-of-the-art LLM-based AL methods.
These applications include text classification~\citep{rouzegar2024enhancing}, text summarization~\citep{li2024active}, non-text classification~\citep{margatina2023active}, question answering~\citep{diao2023active}, entity matching~\citep{qian2024ape}, debiasing~\citep{du2024causal}, translation~\citep{kholodna2024llms}, and sentiment analysis~\citep{xiao2023freeal}. Beyond these, AL has also been used for optimizing system design~\citep{taneja2024can} and question generation~\citep{piriyakulkij2023active}. 
For a more detailed discussion of the AL applications on each task, please refer to Appendix~\ref{app:apps}, where we also include a pairing of active learning applications and datasets in Table~\ref{tab:AL-datasets}.
In Appendix~\ref{sec:appendix-AL-objectives}, we also discuss in further detail the benefits of applying LLM-based AL.



\section{Open Problems \& Challenges \writer{Zhouhang Xie}} \label{sec:open-problems-challenges}

In this section, we discuss open problems and challenges of LLM-based AL for future works.

\paragraph{Heterogeneous Annotation Costs} Traditional AL assumes a fixed annotation budget, but LLM-based AL introduces complex cost structures, including human labeling, LLM query costs, and annotation expenses. Future work should develop algorithms that optimize selection strategies while accounting for these heterogeneous costs.

\paragraph{Multi-LLM AL Algorithms} Different LLMs present trade-offs in performance and cost, suggesting opportunities for hybrid approaches that combine weak-cheap and strong-expensive models. Inspired by retrieval-augmented models~\citep{huang2024comprehensivesurveyretrievalmethods} and multi-oracle clustering~\citep{DBLP:conf/iclr/SilwalANMRK23}, optimizing multi-LLM frameworks for AL remains an open challenge.

\paragraph{LLM Agents and Active Learning} Integrating AL into LLM agents presents new possibilities, such as improving retrieval-augmented generation (RAG)~\citep{xu2024activeragautonomouslyknowledgeassimilation} and in-context example selection~\citep{mukherjee2024experimental}. Conversely, exploring LLM agents as annotation tools could enhance existing AL pipelines by reducing human labeling costs.

\paragraph{Multimodal Active Learning} Most LLM-based AL applications focus on text, leaving open questions on extending these methods to images, audio, and behavioral data. Future work should investigate how well LLMs generalize to non-text domains and whether they can match human-level annotation quality in these settings.

\paragraph{Unstable Performance in LLM-based Annotation} While LLMs are increasingly used to simulate human annotations~\citep{zheng2023judgingllmasajudgemtbenchchatbot, lambert2024rewardbenchevaluatingrewardmodels}, their reliability varies across domains~\citep{tan2024largelanguagemodelsdata}. Future research should explore adaptive routing mechanisms that dynamically allocate annotation tasks between LLMs and human annotators based on model competency.




%\subsection{Multi-objective Active Learning}


%\subsection{Hierarchical Active Learning}


%\subsection{Active Multi-LLM and Multi-Agent Techniques}



% \subsection{Theoretical Foundations}
% We need to develop better theoretical foundations for LLM+AL, especially when a black-box oracle is involved...



\section{Conclusion \writer{TODO}} \label{sec:conc}
% \todo{}
In this survey, we present an intuitive taxonomy of LLM-based Active Learning, detailing how LLMs can act as sample selectors, data generators, and annotators within the AL loop. 
We show how these techniques are reshaping traditional AL paradigms and enabling more efficient data acquisition and model training across various applications.
By reviewing existing methods, highlighting emerging trends and discussing open challenges, we aim to offer a useful foundation for researchers and practitioners looking to incorporate LLM-based AL techniques into their applications.




\section*{Limitations}
% Our work has several limitations.
% 
%In this paper, we do not summarize datasets as most approaches can leverage a wide variety of datasets.
%Many examples provided throughout the paper are based on classification for simplicity, though LLM-based AL techniques are more widely applicable for other tasks.
% First, ...
% Second, ...
% Third, ...
% These are left to future work.
% \todo{please improve}
LLM-based Active Learning (AL) gives rise to many important applications with critical advantages over traditional AL techniques. Despite the fundamental importance of LLM-based AL, there remain some challenges. The reliance on high-quality initial labeled data may introduce biases, while the computational overhead, cost, and robustness of iterative query generation and selection may limit its application in practice. Query generation and selection techniques via LLMs remain sensitive to model uncertainty, often lacking theoretical guarantees and may  lead to inconsistent performance. Furthermore, such techniques may also suffer from reproducibility and robustness issues. Ethical risks such as bias amplification through generation of examples and labels outside known set remains important to handle when deployed in practice. 



\bibliography{main}
\bibliographystyle{acl_natbib}

% \newpage
\appendix
\input{appendix}



\end{document}
