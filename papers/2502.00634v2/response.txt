\section{Related Work}
\noindent\textbf{Simultaneous Translation}
Various SiMT methods introduce different read/write policies. Some approaches propose rule-based fixed policies **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, while others focus on adaptive policies that adjust dynamically based on the context. These adaptive policies are modeled in various forms, such as multi-head monotonic attention **Vaswani, "Attention Is All You Need"**__, Transducer **Prabhavalkar, "A Stream-Informed Transducer Model for Streaming End-To-End Speech Recognition"**__, information transport model **Chiu, "State-of-the-art speech recognition with raw waveforms: A study on noised-training and other irregularities in deep networks"**__, Hidden Markov model **Rabiner, "A tutorial on hidden semi-Markov models and their use for speech recognition"**__, and self-modifying process ____ . More recently, some studies __**Krause, "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"**, Liang, "Pre-training Text Sequences for Task-Agnostic Transfer Learning" ** have also demonstrated the promising performance of large language models in SiMT tasks. However, these efforts are predominantly validated on OMT datasets. __**Stoyanov, "Constructing Monotonic Pseudo-References with a Single Pass through the Translation"**, _**Song, "Monotonic References Generation for Simultaneous Machine Translation with Two-Stage Beam Search"**_ constructed monotonic pseudo-references to reduce unnecessary reorderings. __**Li, "Efficient Monotonic Reference Generation with Monotonic Attention and Pseudo-References"** generated monotonic references with two-stage beam search. ____ employed RL to balance monotonicity and quality of translations. However, existing work fails to account for real SiMT scenarios and alignment with human preferences.

\noindent\textbf{LLM Alignment} Aligning LLMs with human preference has become a crucial research challenge recently. Reinforcement Learning from Human Feedback (RLHF) is one of the key approaches **Jiang, "Reinforcement Learning from Human Feedback for Neural Machine Translation"**. For stable training and less memory costs, ____ proposed Direct Preference Optimization (DPO), which directly optimizes LLMs without relying on a reward model. Similarly, methods such as CPO __**Cheng, "Constrained Policy Optimization for Safe Reinforcement Learning"**, KTO ____ were introduced to improve DPO. Besides, preference alignment is also widely applied to enhance specific downstream tasks __**Wang, "Reinforcement Learning from Human Feedback with Preference Alignment for Neural Machine Translation"**_. ____ explored using RLHF to improve the translation quality. ____ proposed utilizing automated evaluation metrics as feedback to enhance translation performance. Nevertheless, existing methods neglect latency preference in the SiMT task and do not improve the read/write policy during the optimization process, both of which negatively impact the alignment in the SiMT task.

\begin{figure}[t]
	\centering
	\small
        \includegraphics[width=1\textwidth]{framework-v2.pdf}
        \vspace{-0.2in}
	\caption{Overview of our proposed SimulPL Framework. With the first four preferences, we construct the human preference prompts to guide GPT-4/4o generating human-preferred translations. The latency preference is integrated into the preference optimization process.}
 \vspace{-0.2in}
	\label{simulpl-framework}
    \end{figure}