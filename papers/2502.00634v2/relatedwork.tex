\section{Related Work}
\noindent\textbf{Simultaneous Translation}
Various SiMT methods introduce different read/write policies. Some approaches propose rule-based fixed policies \citep{ma2019stacl,elbayad2020efficient}, while others focus on adaptive policies that adjust dynamically based on the context. These adaptive policies are modeled in various forms, such as multi-head monotonic attention \cite{mamonotonic}, Transducer \citep{liu2021cross}, information transport model \citep{zhang-feng-2022-information}, Hidden Markov model \citep{zhang2023hidden}, and self-modifying process \citep{yu-etal-2024-self}. More recently, some studies \citep{wang2023simultaneous,agostinelli-etal-2024-simul,wang2024conversational} have also demonstrated the promising performance of large language models in SiMT tasks. However, these efforts are predominantly validated on OMT datasets. \citet{chen2020improving} constructed monotonic pseudo-references to reduce unnecessary reorderings. \citet{wang2023better} generated monotonic references with two-stage beam search. \citet{guo2023simultaneous} employed RL to balance monotonicity and quality of translations. However, existing work fails to account for real SiMT scenarios and alignment with human preferences.

\noindent\textbf{LLM Alignment} Aligning LLMs with human preference has become a crucial research challenge recently. Reinforcement Learning from Human Feedback (RLHF) is one of the key approaches \citep{ouyang2022training,bai2022training,yuan2023rrhf}. For stable training and less memory costs, \citet{rafailov2024direct} proposed Direct Preference Optimization (DPO), which directly optimizes LLMs without relying on a reward model. Similarly, methods such as CPO \citep{xu2024contrastive} and KTO \citep{ethayarajh2024kto} were introduced to improve DPO. Besides, preference alignment is also widely applied to enhance specific downstream tasks \citep{stiennon2020learning}. \citet{xu2024advancing} explored using RLHF to improve the translation quality. \citet{he2024improving} proposed utilizing automated evaluation metrics as feedback to enhance translation performance. Nevertheless, existing methods neglect latency preference in the SiMT task and do not improve the read/write policy during the optimization process, both of which negatively impact the alignment in the SiMT task.

\begin{figure}[t]
	\centering
	\small
        \includegraphics[width=1\textwidth]{framework-v2.pdf}
        \vspace{-0.2in}
	\caption{Overview of our proposed SimulPL Framework. With the first four preferences, we construct the human preference prompts to guide GPT-4/4o generating human-preferred translations. The latency preference is integrated into the preference optimization process.}
 \vspace{-0.2in}
	\label{simulpl-framework}
    \end{figure}