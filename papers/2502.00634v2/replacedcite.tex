\section{Related Work}
\noindent\textbf{Simultaneous Translation}
Various SiMT methods introduce different read/write policies. Some approaches propose rule-based fixed policies ____, while others focus on adaptive policies that adjust dynamically based on the context. These adaptive policies are modeled in various forms, such as multi-head monotonic attention ____, Transducer ____, information transport model ____, Hidden Markov model ____, and self-modifying process ____. More recently, some studies ____ have also demonstrated the promising performance of large language models in SiMT tasks. However, these efforts are predominantly validated on OMT datasets. ____ constructed monotonic pseudo-references to reduce unnecessary reorderings. ____ generated monotonic references with two-stage beam search. ____ employed RL to balance monotonicity and quality of translations. However, existing work fails to account for real SiMT scenarios and alignment with human preferences.

\noindent\textbf{LLM Alignment} Aligning LLMs with human preference has become a crucial research challenge recently. Reinforcement Learning from Human Feedback (RLHF) is one of the key approaches ____. For stable training and less memory costs, ____ proposed Direct Preference Optimization (DPO), which directly optimizes LLMs without relying on a reward model. Similarly, methods such as CPO ____ and KTO ____ were introduced to improve DPO. Besides, preference alignment is also widely applied to enhance specific downstream tasks ____. ____ explored using RLHF to improve the translation quality. ____ proposed utilizing automated evaluation metrics as feedback to enhance translation performance. Nevertheless, existing methods neglect latency preference in the SiMT task and do not improve the read/write policy during the optimization process, both of which negatively impact the alignment in the SiMT task.

\begin{figure}[t]
	\centering
	\small
        \includegraphics[width=1\textwidth]{framework-v2.pdf}
        \vspace{-0.2in}
	\caption{Overview of our proposed SimulPL Framework. With the first four preferences, we construct the human preference prompts to guide GPT-4/4o generating human-preferred translations. The latency preference is integrated into the preference optimization process.}
 \vspace{-0.2in}
	\label{simulpl-framework}
    \end{figure}