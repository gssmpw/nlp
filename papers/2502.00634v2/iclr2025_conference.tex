
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{graphicx}%图片插入
\usepackage{amsfonts}%指示函数
\usepackage{subfigure}%子图安排
\usepackage{float}%是否浮动
\usepackage{wrapfig}%插入非跨栏图片
\usepackage{multirow}%用于表格的列单元格合并
\usepackage{array}%表格垂直居中
\usepackage{booktabs} % 表格上下居中，三线法
\usepackage{caption}   % 用于表格和图形的标题控制
\usepackage{tabularx}
\usepackage{xcolor} %用于标记新修改的文本部分，使用指定颜色显示。
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}%里面有关于公式{solit}的环境


\title{SimulPL: Aligning Human Preferences in Simultaneous Machine Translation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Donglei Yu\textsuperscript{\rm 1 2}, Yang Zhao \textsuperscript{\rm 1 2}, Jie Zhu\textsuperscript{\rm 3}, Yangyifan Xu\textsuperscript{\rm 1 2}, Yu Zhou\textsuperscript{\rm 1 2}\thanks{Corresponding author.}, Chengqing Zong\textsuperscript{\rm 1 2} \\
\textsuperscript{\rm 1} School of Artificial Intelligence, University of Chinese Academy of Sciences\\
\textsuperscript{\rm 2} State Key Laboratory of Multimodal Artificial Intelligence Systems, \\ Institute of Automation, Chinese Academy of Sciences, Beijing, China\\
\textsuperscript{\rm 3} Graduate School of Translation and Interpretation, Beijing Foreign Studies University \\
\texttt{\{yudonglei2021,zhaoyang2015\}@ia.ac.cn} \\
\texttt{jojo-josephine@bfsu.edu.cn} \\
\texttt{\{yangyifanxu2021,yu.zhou,chengqing.zong\}@ia.ac.cn}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: \textbf{translation quality preference}, \textbf{monotonicity preference}, \textbf{key point preference}, \textbf{simplicity preference}, and \textbf{latency preference}. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates \textbf{latency preference} into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. Our data and code will be available at \url{https://github.com/EurekaForNLP/SimulPL}.
\end{abstract}

\section{Introduction}

    Simultaneous Machine Translation (SiMT)  \citep{grissom2014don,gu2017learning,ma2019stacl} generates translations while receiving the streaming source inputs. Therefore, the SiMT model needs to learn not only the translation ability but also a read/write policy during training to decide whether to wait for the next incoming source token (READ) or to generate a new target token (WRITE) \citep{grissom2014don,alinejad2021translation}.
    
    The real-time nature of SiMT scenarios leads to unique human preferences from audiences, which has been demonstrated by relevant linguistic studies \citep{kurz2001conference,zwischenberger2010quality}. On one hand, the audiences prefer translations that are accurate and easy to understand \citep{moser1996expectations,sridhar2013corpus,dayter2020strategies}; on the other hand, they also prefer translations to be delivered without unnecessary latency. Fulfilling these preferences is an important goal for interpreters \citep{amini2013quality,kurz2001conference} and should also be considered in SiMT. However, how to make SiMT models align with human preferences remains unexplored. Existing SiMT methods \citep{ma2019stacl,alinejad2021translation} are primarily trained and evaluated on corpora from the normal offline machine translation (OMT) task, which do not reflect real SiMT scenarios. Some studies \citep{chen2020improving,guo2023simultaneous} have proposed constructing monotonic references to avoid hallucinations, but they still fail to comprehensively consider human preferences.

    Furthermore, aligning preferences in the SiMT task presents its own challenges. Existing preference alignment methods \citep{rafailov2024direct,xu2024contrastive,ethayarajh2024kto} are designed for tasks such as OMT and question answering, which focus solely on optimizing the model's generated responses. In contrast, these methods have limitations in the SiMT context: they do not account for human preferences regarding latency in the SiMT task and fail to consider enhancing the read/write policy of SiMT models during the preference optimization phase. As a result, these current preference alignment methods are unsuitable for the SiMT task.
    
    To address these issues, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, based on existing research in linguistics and computational linguistics \citep{moser1996expectations,zwischenberger2010quality,he2016interpretese,cho2016can,chen2020improving,guo2023simultaneous}, we categorize human preferences in SiMT scenarios and focus on five aspects: \textbf{translation quality preference}, \textbf{monotonicity preference}, \textbf{key point preference}, \textbf{simplicity preference}, and \textbf{latency preference}. Based on the first four preferences, SimulPL constructs human preference prompts to effectively guide GPT-4/4o in generating preference data for the SiMT task. During the fine-tuning phase, SimulPL proposes Multi-task Supervised Fine-tuning (MSFT) to jointly train the translation ability and read/write policy of the SiMT model for initial preference alignment. Subsequently, SimulPL employs SimulDPO for further preference optimization. During the SimulDPO phase, SimulPL integrates \textbf{latency preference} into the optimization objective and enables the SiMT model to further adjust its read/write policy, thereby facilitating more effective alignment with human preferences. We evaluate SimulPL on test sets with references that we manually revised to align with human preferences. Experimental results demonstrate that SimulPL achieves higher translation quality across all latency levels. Furthermore, our manual assessment and multi-aspect evaluation indicate that SimulPL exhibits better alignment with human preferences from both the overall perspective and across the categorized five aspects. 

    To the best of our knowledge, SimulPL is the first preference learning framework for simultaneous tasks like SiMT. Our contributions can be summarized as follows:

    \begin{itemize}
    \item {Our work addresses a critical gap in the study of human preferences for SiMT scenarios. We categorize SiMT human preferences into five aspects: translation quality, monotonicity, key points, simplicity, and latency. This categorization enables the construction of human preference prompts to efficiently guide LLMs in generating preference data for SiMT.}

    \item {We propose SimulPL, a preference learning framework tailored for SiMT scenarios. Unlike existing preference learning methods, SimulPL integrates latency preference into the optimization objective and allows the SiMT model to improve its read/write policy during the preference optimization process, enabling better alignment with human preferences.}

    \item {Experimental results demonstrate that SimulPL effectively enhances the translation quality across various latency levels. Furthermore, our preference evaluation indicates that SimulPL exhibits better alignment with human preferences.}
    
   \end{itemize}

\section{Related Work}
\noindent\textbf{Simultaneous Translation}
Various SiMT methods introduce different read/write policies. Some approaches propose rule-based fixed policies \citep{ma2019stacl,elbayad2020efficient}, while others focus on adaptive policies that adjust dynamically based on the context. These adaptive policies are modeled in various forms, such as multi-head monotonic attention \cite{mamonotonic}, Transducer \citep{liu2021cross}, information transport model \citep{zhang-feng-2022-information}, Hidden Markov model \citep{zhang2023hidden}, and self-modifying process \citep{yu-etal-2024-self}. More recently, some studies \citep{wang2023simultaneous,agostinelli-etal-2024-simul,wang2024conversational} have also demonstrated the promising performance of large language models in SiMT tasks. However, these efforts are predominantly validated on OMT datasets. \citet{chen2020improving} constructed monotonic pseudo-references to reduce unnecessary reorderings. \citet{wang2023better} generated monotonic references with two-stage beam search. \citet{guo2023simultaneous} employed RL to balance monotonicity and quality of translations. However, existing work fails to account for real SiMT scenarios and alignment with human preferences.

\noindent\textbf{LLM Alignment} Aligning LLMs with human preference has become a crucial research challenge recently. Reinforcement Learning from Human Feedback (RLHF) is one of the key approaches \citep{ouyang2022training,bai2022training,yuan2023rrhf}. For stable training and less memory costs, \citet{rafailov2024direct} proposed Direct Preference Optimization (DPO), which directly optimizes LLMs without relying on a reward model. Similarly, methods such as CPO \citep{xu2024contrastive} and KTO \citep{ethayarajh2024kto} were introduced to improve DPO. Besides, preference alignment is also widely applied to enhance specific downstream tasks \citep{stiennon2020learning}. \citet{xu2024advancing} explored using RLHF to improve the translation quality. \citet{he2024improving} proposed utilizing automated evaluation metrics as feedback to enhance translation performance. Nevertheless, existing methods neglect latency preference in the SiMT task and do not improve the read/write policy during the optimization process, both of which negatively impact the alignment in the SiMT task.

\begin{figure}[t]
	\centering
	\small
        \includegraphics[width=1\textwidth]{framework-v2.pdf}
        \vspace{-0.2in}
	\caption{Overview of our proposed SimulPL Framework. With the first four preferences, we construct the human preference prompts to guide GPT-4/4o generating human-preferred translations. The latency preference is integrated into the preference optimization process.}
 \vspace{-0.2in}
	\label{simulpl-framework}
    \end{figure}

\section{Preliminaries}
\noindent\textbf{Reward Modeling} Existing preference alignment methods typically involve reward modeling and preference optimization. For reward modeling, a human-annotated preference dataset $(\mathrm{x},\mathrm{y}^w,\mathrm{y}^l)$ is first constructed, where $\mathrm{x}$ represents the input, $\mathrm{y}^w$ is preferred over $\mathrm{y}^l$, which is denoted as $\mathrm{y}^w \succ \mathrm{y}^l$. Subsequently, existing methods \citep{christiano2017deep,kim2023preference} often train a reward model based on the Bradley-Terry model \citep{bradley1952rank}, which is formulated as:

\begin{equation}
    \begin{split}
    p(\mathrm{y}^w \succ \mathrm{y}^l \mid \mathrm{x})=\frac{\exp(r(\mathrm{x},\mathrm{y}^w))}{\exp(r(\mathrm{x},\mathrm{y}^w))+\exp(r(\mathrm{x},\mathrm{y}^l))}=\sigma(\exp(r(\mathrm{x},\mathrm{y}^w))-\exp(r(\mathrm{x},\mathrm{y}^l)))
    \end{split}
\end{equation}

where $r(\mathrm{x},\mathrm{y}^w)$ is the score estimated by the reward model, and $\sigma(\cdot)$ is the logistic sigmoid function. 

\noindent\textbf{Preference Optimization} Reinforcement learning (RL) is widely used for preference optimization. Using signals from a reward model, the LLM can be optimized with the following objective:

\begin{equation}
    \begin{split}
     \max_{\pi_\theta} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{x})]
    \end{split}
    \label{equ:rlhf}
\end{equation}

Additionally, several methods, such as DPO \citep{rafailov2024direct}, directly conduct preference alignment without a reward model.
However, existing preference alignment methods cannot be directly applied to the SiMT task, as their optimization objectives do not account for the latency preference and do not adjust the read/write policy in the optimization process.
    
\begin{figure}[t]
\centering
\small
\begin{minipage}{0.45\textwidth}
\captionof{table}{Statics of our constructed datasets. We present the reference-free COMET scores of our annotated target sentences with GPT-4/4o and the original target sentences. }
\label{table:static-data}
\begin{tabular}{crrrr}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Dataset}} & \multicolumn{2}{c}{Size}                             & \multicolumn{2}{c}{Ref-free COMET}                           \\
\cmidrule(r{0.2cm}l{0.2cm}){2-3} \cmidrule(r{0.2cm}l{0.2cm}){4-5}
\multicolumn{1}{l}{}                         & \multicolumn{1}{c}{train} & \multicolumn{1}{c}{test} & \multicolumn{1}{c}{GPT-4/4o} & \multicolumn{1}{c}{Origin} \\
\midrule
Zh$\rightarrow$En                                        & 13,491                     & 2,000                     & 79.13                          & 73.72                             \\
De$\rightarrow$En                                        & 15,717                     & 2,168                     & 78.93                          & 75.02                             \\
En$\rightarrow$Zh                                        & 19,967                     & 2,841                     & 80.30                           & 76.97       \\
\bottomrule
\end{tabular}
\end{minipage}%
\vspace{-0.05in}
\hfill  % 在两者之间插入间隙
\begin{minipage}{0.45\textwidth}
    \centering
 \includegraphics[width=\textwidth]{data-win-rate.pdf}
   \vspace{-0.2in} 
  \caption{Human evaluation between our annotated target references and origin target references. Our newly annotated references are more preferred.}
  \vspace{-0.2in}
  	\label{fig:data-win-rate}
\end{minipage}
\end{figure}

\section{Method: SimulPL}
We propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. The overview of SimulPL is shown in Figure \ref{simulpl-framework}. In this framework, we construct human preference prompts based on our categorization of SiMT human preferences to guide GPT-4/4o in generating preference data. During the fine-tuning phase, SimulPL introduces Multi-task Supervised Fine-tuning (MSFT) to jointly learn translation ability and the read/write policy for initial preference alignment. During the preference optimization phase, SimulPL proposes Simultaneous Direct Preference Optimization (SimulDPO), which takes latency preference into account and further improves the read/write policy. The details are discussed in the following.

\subsection{Categorization of Human Preference}
In real-time SiMT scenarios, the audience exhibits unique human preferences \citep{kurz2001conference,zwischenberger2010quality,amini2013quality}. Based on existing research in linguistics and computational linguistics, we categorize SiMT human preferences into five aspects: 
\begin{itemize}
    \item \textbf{Translation Quality Preference}: Similar to OMT, faithful and fluent translations are also preferred in SiMT \citep{ma2019stacl,miao2021generative}.
    \item  \textbf{Monotonicity Preference}:  In the SiMT process, translating monotonically in accordance with the source word order allows for the delivery of translations with minimal pauses \citep{yang2023impacts,chen2020improving}, which is favored by the audience \citep{macias2006probing}.
    \item \textbf{Key Point Preference}: According to existing research \citep{moser1996expectations,he2016interpretese}, concise translations that highlight important information points are more appealing than those that provide complete information in the SiMT scenarios.
    \item \textbf{Simplicity Preference}: In real-time SiMT scenarios, the audience prefers sentences with simpler syntactic structures, which are easier to follow \citep{sridhar2013corpus,dayter2020strategies}. 
     \item \textbf{Latency Preference}: In real-time settings, the audience prefers translations to be delivered without unnecessary latency \citep{rennert2010impact,cho2016can}.
\end{itemize}
It is important to note that latency preference differs from the other four preferences, as it focuses not on the translation content but rather on reducing delays. Therefore, SimulPL aligns with the first four preferences by improving translation ability, and with the latency preference by enhancing the read/write policy.
\subsection{Data Construction}

\paragraph{Annotation of Human-preferred Translation} In our categorization, the first four preferences are reflected in the translation content. Therefore, we utilize them as prior knowledge to construct human preference prompts and leverage GPT-4/4o \citep{achiam2023gpt} to efficiently generate human-preferred translations, denoted as $\mathrm{Y}^w$. The original references, not fully aligned with human preferences, are denoted as $\mathrm{Y}^l$. For the training data, we select subsets from three datasets—WMT15 De$\rightarrow$En, WMT22 Zh$\rightarrow$En, and MUST-C En$\rightarrow$Zh for annotation. The complete prompt used for annotation is provided in Appendix \ref{appendix:data-prompt}.
 Correspondingly, newstest2015 De$\rightarrow$En, newstest2021 Zh$\rightarrow$En, and tst-COMMON are annotated for evaluation. To ensure the accuracy of the test set, we first use GPT-4/4o to generate drafts, and then manually revise them to produce human-preferred references. Our annotators are all qualified in simultaneous interpretation, ensuring reliable and trustworthy revisions. The statistics for our constructed dataset, along with ref-free COMET scores, are shown in Table \ref{table:static-data}. Notably, we calculate reference-free COMET scores for both kinds of references, showing that our annotated references match the quality of the originals.

To verify that our constructed translation data aligns with human preference, we randomly sample 100 sentences from each of the three language pairs and conducted a manual evaluation by professional simultaneous interpreters. The results in Figure \ref{fig:data-win-rate} show that our annotated data achieves a higher win rate, indicating stronger alignment with human preference. To further validate our annotated data quality, we conduct additional comparisons between GPT-generated translations and manually revised translations through human evaluation, along with automatic evaluation from the perspective of the first four preferences. These results are available in Appendix \ref{appendix:data-analysis}.

\paragraph{Prefix Pairs Extraction} To enable the SiMT model in learning translation based on source prefixes instead of complete source sentences, we extract prefix pairs from our annotated sentence pairs using word alignment and add them to the training data. For each sentence pair $(\mathrm{X},\mathrm{Y}^w)$, we use awesome-align \citep{dou2021word} to get the word alignment. For target token $y_t$, we denote the corresponding source token as $x_{a_t}$, and the set of extracted prefix pairs are denoted as:
\begin{equation}
D_{p}^w=\{(\mathrm{x},\mathrm{y}^w)\mid \mathrm{if}\ 0 < t \leq |\mathrm{y}^w|, \mathrm{then}\ 0 < a_t \leq |x|\mathrm{;}\ a_{|\mathrm{y}^w|+1} > |x| \}
\end{equation}
Intuitively, for the given source prefix $\mathrm{x}$, the target prefix $\mathrm{y}^w$ includes all the translatable content. Similarly, we can extract prefix pairs from sentence pairs $(\mathrm{X}, \mathrm{Y}^l)$ to obtain $D_p^l$. Then, we merge $D_p^w$ and $D_p^l$ to create the prefix-level preference dataset:
\begin{equation}
D_p = \{(\mathrm{x}, \mathrm{y}^w, \mathrm{y}^l) \mid (\mathrm{x}, \mathrm{y}^w) \in D_p^w, (\mathrm{x}, \mathrm{y}^l) \in D_p^l\}    
\end{equation}


\subsection{Multi-task Supervised Fine-tuning}
Based on a pre-trained language model $\pi_{\mathrm{pre}}$, SimulPL introduces Multi-task Supervised Fine-tuning (MSFT) to jointly learn translation ability and read/write policy on $D_p^w$ for initial preference alignment. For translation ability, the model learns to generate the target prefix $\mathrm{y}^w$ from the source prefix $\mathrm{x}$. For read/write policy, SimulPL adds an extra confidence layer, consisting of a linear layer and a sigmoid layer, to make read/write decisions. Specifically, when predicting $y^w_t$, an additional confidence  $c^w_t$ is estimated by the confidence layer. If $t < |\mathrm{y}^w|$, the model should predict $c^w_t = 1$, indicating the WRITE decision. Otherwise, if $t > |\mathrm{y}^w|$, the model should estimate $c^w_t = 0$, which means it should stop translating and choose the READ decision. The complete training loss for the MSFT phase is calculated as:
\begin{equation}
    \begin{split}
    \mathcal{L}_{\mathrm{MSFT}}=-\prod_{t=1}^{|\mathrm{y}^w|}y^w_t\log{\pi_{\mathrm{sft}}(y^w_t\mid \mathrm{x},\mathrm{y}^w_{\leq \ t-1})}-\prod_{t=1}^{|\mathrm{y}^w|+1}[\mathbb{I}(t\leq|\mathrm{y}^w|)\log{c^w_t}+\mathbb{I}(t > |\mathrm{y}^w|)\log{(1-c^w_t)}]
    \end{split}
\end{equation}
where $\pi_{\mathrm{sft}}$ is initialized with the parameters of $\pi_{\mathrm{pre}}$, and $\mathbb{I}(\cdot)$ denotes the indicator function. It is noted that we train the model to predict $c^w_{|\mathrm{y}^w|+1} = 0$, allowing the SiMT model to learn to stop translating at the appropriate position.
\subsection{Simultaneous Direct Preference Optimization}
After the MSFT phase, SimulPL introduces Simultaneous Direct Preference Optimization (SimulDPO) to further align with human preferences. In the SimulDPO phase, SimulPL integrates the latency preference into the optimization objective and allows the SiMT model to further improve its read/write policy during preference optimization.

First, we modify the optimization objective from Equation \ref{equ:rlhf} to encourage the SiMT model to generate human-preferred translations while satisfying additional latency preference. Specifically, we add an output length constraint, which can be expressed as follows:
\begin{equation}
    \begin{split}
    \max_{\pi_\theta} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]+\alpha\mathbb{E}[|\mathrm{y}|]
    \end{split}
    \label{equ:simul-dpo-object}
\end{equation}
where $\alpha$ is a hyper-parameter introduced to control the output length constraint. Unlike $\pi_{\theta}$ which only accesses the source prefix $\mathrm{x}$, $\pi_{\mathrm{ref}}$ is provided with the complete source sentence $\mathrm{X}$ to generate a more accurate prediction, thus effectively preventing $\pi_{\theta}$ from diverging. Intuitively, based on the received $\mathrm{x}$, the SiMT model is encouraged to translate as much content as possible to minimize unnecessary latency. Theoretically, we prove that the constraint $\mathbb{E}[|\mathrm{y}|]$ aligns with the objective of reducing latency, which is provided in Appendix \ref{appendix:proof-latency-equ-length}. Therefore, although similar in form to R-DPO \citep{park2024disentangling}, our goals and methods are entirely opposite, as detailed in Appendix \ref{appendix:diff-with-length-work}.

Based on Equation \ref{equ:simul-dpo-object}, we can derive the following reward function concerning $\pi_\theta$ and $|\mathrm{y}|$:
\begin{equation}
    \begin{split}
r(\mathrm{x},\mathrm{y})=\beta \log{\frac{\pi_{\theta}(\mathrm{y}\mid \mathrm{x})}{\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X})}}+\beta \log{Z(\mathrm{x})}-\alpha|\mathrm{y}|
    \end{split}
    \label{equ:reward}
\end{equation}
where $Z(\mathrm{x})$ is the partition function. The detailed derivation is provided in Appendix \ref{appendix:proof-optimal-solution}.

Since both $\pi_{\theta}$ and $\pi_{\text{ref}}$ are initialized from $\pi_{\text{sft}}$, they also possess the ability to estimate confidence. We denote the confidence estimated by $\pi_{\theta}$ when predicting $y_t$ as $c_t$, leading to the following expression: $\pi_\theta(\mathrm{y}\mid \mathrm{x})=\prod_{t=1}^{|\mathrm{y}|+1}[\pi_{\theta}(y_t\mid \mathrm{x},\mathrm{y}_{\leq \ t-1})]^{\mathbb{I}(t\leq |\mathrm{y}|)}=\prod_{t=1}^{|\mathrm{y}|+1}[\pi_{\theta}(y_t\mid \mathrm{x},\mathrm{y}_{\leq \ t-1})]^{c_t}$. Intuitively, we can also gain that:$|y|=\sum_{t=1}^{|\mathrm{y}|+1} \mathbb{I}(t\leq |\mathrm{y}|)=\sum_{t=1}^{|\mathrm{y}|+1} c_t$.
 Substituting these two equations into Equation \ref{equ:reward}, we obtain the following representation of $r(\mathrm{x},\mathrm{y})$:
\begin{equation}
    \begin{split}
         r(\mathrm{x},\mathrm{y})=\beta \sum_{t=1}^{|\mathrm{y}|+1} c_t \log \frac{\pi_\theta(y_t\mid \mathrm{x},\mathrm{y}_{\leq \ t-1})}{\pi_{\text{ref}}(y_t\mid \mathrm{X},\mathrm{y}_{\leq \ t-1})}+\beta\log Z(\mathrm{x})-\alpha\sum_{t=1}^{|\mathrm{y}|+1} c_t
    \end{split}
\end{equation}

Then, with the Bradley-Terry model, we can derive the training objective of SimulDPO as follows:
\begin{equation}
    \begin{split}
         \mathcal{L}_\mathrm{SimulDPO}&= -\log \sigma(\sum_t^{|\mathrm{y}^w|+1} r^{w}_t - \sum_t^{|\mathrm{y}^l|+1} r^l_t ) \\
         r^{\ast}_t & = c^{\ast}_t\cdot(\beta \log\frac{\pi_{\theta}(y^{\ast}_t\mid \mathrm{x},\mathrm{y}^{\ast}_{\leq t-1})}{\pi_{\text{ref}}(y^{\ast}_t\mid \mathrm{X},\mathrm{y}^{\ast}_{\leq t-1})}-\alpha), \quad \ast \in \{w,l\}
    \end{split}
    \label{equ:loss-sidpo}
\end{equation}
This training loss enables the SiMT model to further improve its read/write policy during the preference alignment phase, and takes the latency preference into account. During training, if the SiMT model can accurately predict a token that aligns with human preferences (i.e.,  $\log\frac{\pi_{\theta}(y^{w}_t\mid \mathrm{x},\mathrm{y}^{w}_{\leq t-1})}{\pi_{\text{ref}}(y^{w}_t\mid \mathrm{X},\mathrm{y}^{w}_{\leq t-1})} > \frac{\alpha}{\beta}$), then the SiMT model will learn to predict $c^{w}_t$ close to 1 to avoid latency. Conversely, if the prediction does not align well, $c^{w}_t$ should be predicted close to 0. Additionally, since the read/write decisions for $\mathrm{y}^{l}$ are not the focus of the optimization, we directly set $c^{l}_t=\mathbb{I}(t\leq |\mathrm{y}^l|)$, instead of using the predictions from the SiMT model during training.

\subsection{Confidence-based Policy During Inference}
During inference, SimulPL makes decisions based on estimated $c_t$. If $c_t > 0.5$, it indicates that the SiMT model can generate a target token $y_t$ aligned with human preferences and should choose WRITE; otherwise, it chooses READ. Following \citet{wang2023simultaneous}, we introduce the reading length $n$ to control the latency level of SimulPL. Specifically, when the SiMT model chooses READ, it needs to wait for $n$ new source words before making further decisions. In Appendix \ref{appendix:conf-based-policy}, we provide more details with Algorithm \ref{policy-algorithm} and analyze the impact of the confidence threshold.

\section{Experiments}
\subsection{Experimental Details}
\begin{figure}
    \centering
        \subfigure[Zh$\rightarrow$En]{
            \includegraphics[width=0.31\textwidth]{zh-en-bleu-nature.pdf} 
        }
        \subfigure[De$\rightarrow$En]{
            \includegraphics[width=0.31\textwidth]{de-en-bleu-nature.pdf} 
        }
        \subfigure[En$\rightarrow$Zh]{
            \includegraphics[width=0.31\textwidth]{en-zh-bleu-nature.pdf} 
        }
        \vspace{-0.05in}
    \caption{SacreBLEU against LAAL on Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks.}
    \label{fig:main-bleu-scores}
        \vspace{-0.1in}
\end{figure}
\begin{figure}

    \centering
        \subfigure[Zh$\rightarrow$En]{
            \includegraphics[width=0.31\textwidth]{zh-en-comet-nature.pdf} 
        }
        \subfigure[De$\rightarrow$En]{
            \includegraphics[width=0.31\textwidth]{de-en-comet-nature.pdf} 
        }
        \subfigure[En$\rightarrow$Zh]{
            \includegraphics[width=0.31\textwidth]{en-zh-comet-nature.pdf} 
        }
     \vspace{-0.05in}
    \caption{COMET against LAAL on Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks.}
    \label{fig:main-comet-scores}
    \vspace{-0.1in}
\end{figure}

\noindent\textbf{Datasets}
We validate our method on text-to-text SiMT tasks using our annotated datasets with human-preferred references. For Transformer-based SiMT models, we first pre-train them on the complete OMT training sets and then fine-tune on our annotated data. For LLM-based SiMT models, we begin with SFT on a subset of the OMT training data with the size of 100k, followed by additional SFT on our annotated data, so that LLMs can initially learn the translation ability.

\noindent\textbf{Baselines} Existing SiMT models primarily include Transformer-based and LLM-based architectures. We reproduce both types of SiMT models, as detailed in the following:

\begin{itemize}
    \item \textbf{wait-$k$} \citep{ma2019stacl}: This Transformer-based SiMT model first waits for $k$ source tokens. It then repeatedly generates a target token and waits for a source token.
    \item \textbf{SM$^2$} \citep{yu-etal-2024-self}: This Transformer-based SiMT model learns the confidence of current prediction during training. During inference, it decides whether to wait for an additional source token or output a target token based on the confidence.
    \item \textbf{LLM-PFX-SFT} \citep{wang2023simultaneous}: This LLM-based SiMT model utilizes prefix-pairs data for SFT and utilizes incremental decoding during inference.

    \item \textbf{LLM-PFX-SFT+DPO}: Building on LLM-PFX-SFT, we further align preferences using our constructed prefix-level preference data through a standard DPO method.
\end{itemize}

\noindent\textbf{Implementation Details}
We implement Transformer-based SiMT models using Fairseq \citep{ott2019fairseq}. For the Zh$\rightarrow$En and En$\rightarrow$Zh SiMT tasks, we use vocabularies of 45,000 for Chinese and 35,000 for English respectively. The De$\rightarrow$En task is applied with a shared vocabulary of 32,000. For LLM-based SiMT models, following, we choose Llama2-7B-chat \citep{touvron2023llama} as the base model and use Simuleval \citep{simuleval2020} for evaluation. To address vocabulary inconsistencies among SiMT models, we allow word-level read/write operations during testing to facilitate accurate latency comparison. More implementation details are available in Appendix \ref{appendix:implentation-details}.

\noindent\textbf{Evaluation Metrics} We use specific metrics to measure our categorized human preferences respectively. For \textit{Translation Quality Preference}, we utilize SacreBLEU \citep{post2018call} and COMET as the metrics. For \textit{Latency Preference}, we chose Length-Adaptive Average Lagging (LAAL) \citep{papi2022over} to avoid misjudging the over-generation phenomena. A higher LAAL indicates greater latency. For \textit{Monotonicity Preference}, we define Normalized Inversion Rate (NIR) to measure the monotonicity. Specifically, we first use awesome-align to obtain word alignment between the model's output  $\mathrm{\hat{Y}}$ and the source sentence $\mathrm{X}$. We denote the source position corresponding to $\hat{y}_t$ as $\hat{a}_t$. These source positions form a sequence $\mathrm{\hat{A}} = [\hat{a}_1, \hat{a}_2, \dots]$. The inversion number \citep{mannila1985measures} of $\mathrm{\hat{A}}$, denoted as $I_{{\mathrm{\hat{A}}}}$, reflects the monotonicity of $I_{{\mathrm{\hat{A}}}}$. A smaller $I_{{\mathrm{\hat{A}}}}$ indicates a more monotonic translation, while a larger $I_{{\mathrm{\hat{A}}}}$ suggests more reordering in the translation. Since $I_{{\mathrm{\hat{A}}}}$ is affected by $|\mathrm{\hat{A}}|$, we define $\mathrm{NIR} = \tfrac{2I_{\mathrm{\hat{A}}}}{|\mathrm{\hat{A}}|(|\mathrm{\hat{A}}|-1)} \times 100\%$ as the metric for monotonicity preference. For \textit{Key Point Preference}, we use the Sentence Length Ratio (SLR) as the metric, defined as $\mathrm{SLR} = \tfrac{|\mathrm{\hat{Y}}|}{|\mathrm{X}|}$. A smaller SLR indicates that the SiMT model completes the translation with a shorter sentence, aligning better with key point preference. For \textit{Simplicity Preference}, we use Stanza \citep{qi2020stanza} to convert $|\mathrm{\hat{Y}}|$ into a dependency tree $\hat{T}$, and define the depth of $\hat{T}$ as Dependency Depth (DD) for evaluation. A smaller DD indicates simpler syntax, which is easier for audiences to follow.

\begin{figure}[t]

    \centering
        \subfigure[Human evaluation between SimulPL and LLM-PFX-SFT.]{
            \includegraphics[width=0.9\textwidth]{winrate-sft.pdf} 
        }
        \subfigure[Human evaluation between SimulPL and LLM-PFX-SFT+DPO.]{
            \includegraphics[width=0.9\textwidth]{winrate-dpo.pdf} 
        }
        \vspace{-0.1in}
    \caption{Human Evaluation in differenct latency groups on Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. We divide the latency into three levels: low latency (0 $\leq$ LAAL $<$ 4), medium latency (4 $\leq$ LAAL $<$ 8), and high latency (LAAL $\geq$ 8). We report the independent evaluation results at each latency group and the overall results across all latency groups.}
    \vspace{-0.1in}
    \label{fig:main-win-rate}
\end{figure}
\begin{table}[t]
\caption{Multi-aspect Evaluation in different latency groups on Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. We define Normalized Inversion Rate (NIR), Sentence Length Ratio (SLR) and Dependency Depth (DD) for respectively measuring monotonicity preference, key point preference and simplicity preference. COMET and SacreBLEU are used to measure translation quality preference.}
\vspace{-0.05in}
\label{table:multi-aspect-evaluation}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrrrrrrrrrr}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{Models}}} & \multicolumn{15}{c}{\textbf{Preference Analysis}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \\ \cmidrule(l){2-16} 
\multicolumn{1}{c|}{}                                 & \multicolumn{5}{c|}{\textbf{Zh$\rightarrow$En}}                                                                                                                                      & \multicolumn{5}{c|}{\textbf{De$\rightarrow$En}}                                                                                                                                      & \multicolumn{5}{c}{\textbf{En$\rightarrow$Zh}}                                                                                                                                      \\
\cmidrule{2-16}
\multicolumn{1}{c|}{}                                 & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}} & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c}{\textbf{COMET}} & \multicolumn{1}{c|}{\textbf{BLEU}} & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}} & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c}{\textbf{COMET}} & \multicolumn{1}{c|}{\textbf{BLEU}} & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}} & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c}{\textbf{COMET}} & \multicolumn{1}{c}{\textbf{BLEU}} \\ 
\midrule
\multicolumn{16}{c}{\textit{Low Latency}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\
\textbf{LLM-PFX-SFT}                                 & 5.24                             & 7.60                            & 0.90                             & 74.25                              & 12.05                                  & 3.03                             & 5.95                            & 1.32                             & 81.21                              & 29.61                                  & 5.21                             & 5.72                            & 2.62                             & 74.74                              & 19.23                                  \\
\textbf{LLM-PFX-SFT+DPO}                             & 3.39                             & 7.97                            & 0.80                             & 72.67                              & 14.79                                  & 2.97                             & \textbf{5.76}                   & 1.29                             & 81.19                              & 29.29                                  & 5.13                             & 5.68                            & 2.55                             & 74.55                              & 19.62                                  \\
\textbf{SimulPL}                                    & \textbf{2.95}                    & \textbf{6.97}                   & \textbf{0.64}                    & \textbf{76.32}                     & \textbf{23.73}                         & \textbf{2.69}                    & 6.02                            & \textbf{1.01}                    & \textbf{82.34}                     & \textbf{42.69}                         & \textbf{4.81}                    & \textbf{5.46}                   & \textbf{0.96}                    & {\textbf{80.52}} & \textbf{38.99}                         \\
\midrule
\multicolumn{16}{c}{\textit{medium latency}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\
\textbf{LLM-PFX-SFT}                                 & 4.65                             & 7.02                            & 0.65                             & 78.76                              & 24.13                                  & 3.80                             & 5.97                            & 1.01                             & 84.86                              & 46.31                                  & 7.65                             & 5.43                            & 0.97                             & \textbf{82.42}                     & 41.97                                  \\
\textbf{LLM-PFX-SFT+DPO}                             & \textbf{3.49}                    & 7.09                            & 0.63                             & 79.14                              & 25.90                                  & 3.88                             & \textbf{5.94}                   & 1.00                             & 84.79                              & 46.39                                  & 7.55                             & 5.44                            & 0.97                             & 81.87                              & 41.58                                  \\
\textbf{SimulPL}                                    & 4.21                             & \textbf{6.91}                   & \textbf{0.62}                    & \textbf{79.29}                     & \textbf{27.37}                         & \textbf{3.60}                    & 5.96                            & \textbf{0.99}                    & \textbf{84.93}                     & \textbf{48.03}                         & \textbf{7.11}                    & \textbf{5.38}                   & \textbf{0.95}                    & 82.29                              & \textbf{43.08}                         \\
\midrule
\multicolumn{16}{c}{\textit{High Latency}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\
\textbf{LLM-PFX-SFT}                                 & 5.66                             & 6.82                            & 0.61                             & 80.64                              & 29.81                                  & 4.53                             & 5.93                            & 0.98                             & \textbf{86.48}                     & 51.41                                  & 9.48                             & 5.40                            & 0.91                             & 83.56                              & 44.47                                  \\
\textbf{LLM-PFX-SFT+DPO}                             & \textbf{4.60}                    & 6.85                            & 0.62                             & \textbf{81.25}                     & 29.47                                  & 4.70                             & \textbf{5.90}                   & 0.97                             & 86.23                              & \textbf{51.60}                                  & 9.34                             & 5.37                            & \textbf{0.91}                    & 83.08                              & 44.10                                  \\
\textbf{SimulPL}                                    & 5.59                             & \textbf{6.78}                   & \textbf{0.60}                    & 81.00                              & \textbf{30.00}                         & \textbf{4.44}                    & 5.92                            & \textbf{0.97}                    & 86.40                              & 51.58                         & \textbf{9.10}                    & \textbf{5.37}                   & 0.92                             & \textbf{83.63} & \textbf{44.96}                         \\ \bottomrule
\end{tabular}
}
\vspace{-0.1in}
\end{table}

\subsection{Translation Quality}
The SacreBLEU and COMET scores for different SiMT methods are shown in Figure \ref{fig:main-bleu-scores} and Figure \ref{fig:main-comet-scores}. These results show that our proposed SimulPL achieves higher translation quality across all latency levels on three language pairs, particularly in low latency level. This indicates that SimulPL better meets the translation quality preference in SiMT scenarios. Since the test set we used includes human-aligned references, we argue this can also reflect the effectiveness of SimulPL in aligning with other human preferences. We provide detailed numerical results and values for other latency metrics in Appendix \ref{appendix:num-results}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}

\subsection{Preference Evaluation}
\label{section:human-prefer}

To validate the effectiveness of SimulPL in preference alignment, we conduct a further human preference evaluation for LLM-PFX-SFT, LLM-PFX-SFT+DPO, and SimulPL. This evaluation includes the overall human evaluation and multi-aspect evaluation. Specifically, we first divide the models' outputs into three latency groups: low latency (0 $\leq$ LAAL $<$ 4), medium latency (4 $\leq$ LAAL $<$ 8), and high latency (LAAL $\geq$ 8). For human evaluation, we manually assess 100 sentences sampled from each latency group. Our evaluators are all qualified in simultaneous interpretation and can provide accurate assessments. For multi-aspect evaluation, we measure the performance of these SiMT models in terms of translation quality preference, monotonicity preference, key point preference, and simplicity preference in different latency groups. Detailed analyses are provided as follows.

\noindent\textbf{Human Evaluation} The results of the human evaluation are shown in Figure \ref{fig:main-win-rate}, which show that SimulPL achieves higher win rates in all latency groups for these three language pairs. This indicates that SimulPL can generate translations more aligned with human preferences. We attribute this performance improvement to the joint optimization of translation ability and read/write policy during the preference alignment process.

\noindent\textbf{Multi-aspect Evaluation} The results of muti-aspect evaluation are shown in Table \ref{table:multi-aspect-evaluation}. SimulPL achieves better alignment across all latency groups for the three language pairs. SimulPL not only maintains high translation quality but also effectively manages monotonicity, key points, and simpler syntactic structures. Under low latency conditions, SimulPL achieves a better trade-off between latency preference and other preferences and generates better translations.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}

\subsection{Ablation Studies}
We conduct ablation studies on SimulPL for the Zh$\rightarrow$En SiMT task, with detailed analyses in the following. Additional results and other analyses on De$\rightarrow$En SiMT task are shown in Appendix \ref{appendix:analysis-de-en}.

\noindent\textbf{Effect of MSFT} To verify the role of MSFT, we evaluate the performance of a SiMT model trained with regular SFT using prefix pairs data, similar to LLM-PFX-SFT. The results in Figure \ref{fig:ablation-bleu-laal} show that MSFT outperforms SFT, especially in the low latency level. This shows that by explicitly modeling the multi-task of translation ability and read/write policy, MSFT improves the SiMT performance more effectively and provides better initialization parameters for SimulDPO.

\noindent\textbf{Effect of SimulDPO} As shown in Figure \ref{fig:ablation-bleu-laal}, SimulPL, which introduces SimulDPO after MSFT phase, achieves higher SacreBLEU scores across various latency levels compared to Only MSFT. This indicates SimulPL further enhances the translation ability and read/write policy during the SimulDPO phase, leading to better performance.

\noindent\textbf{Effect of ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{X})$} To verify whether the predicted ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{X})$ in OMT setting can provide a more accurate constraint for the training objective, we replace ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{X})$ in SimulPL with the probability ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{x})$ and evaluate the performance in this setting. As shown in Figure \ref{fig:ablation-bleu-laal}, the performance of SimulPL trained with ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{x})$ obviously declines. We argue this is due to the inaccurate prediction of ${\pi_{\mathrm{ref}}}(\mathrm{y}\mid\mathrm{x})$ negatively impacts the preference optimization.
\subsection{Impact of $\alpha$ on Balancing Alignment and Latency During Training}
In SimulDPO, $\alpha$ is introduced as a hyper-parameter into the training loss. As shown in Equation \ref{equ:loss-sidpo}, $\alpha$ functions as a token-level threshold. Since the gradient of $\alpha c^{l}_t$ does not propagate, we only analyze the impact of $\alpha c^{w}_t$. Specifically, during training, if $\log\frac{\pi_{\theta}(y^{w}_t\mid \mathrm{x},\mathrm{y}^{w}_{\leq t-1})}{\pi_{\text{ref}}(y^{w}_t\mid \mathrm{X},\mathrm{y}^{w}_{\leq t-1})}>\frac{\alpha}{\beta}$, we consider that $\pi_{\theta}(y^{w}_t\mid \mathrm{x},\mathrm{y}^{w}_{\leq t-1})$ presents a prediction aligning human preferences well, and the SiMT model should learn to predict a higher $c^{w}_t$. Conversely, the model should learn a lower $c^{w}_t$ when this condition is not met. Thus, appropriately increasing the value of $\alpha$ can enhance the model's ability to learn better alignment quality. However, if $\alpha$ is set too high, the SiMT model could become overly cautious in translation, leading to $c^{w}_t$ failing to accurately balance between latency preference and other preferences. To validate the effect of $\alpha$, we train SimulPL with different $\alpha$ values and compare their performance on Zh$\rightarrow$En task. The results are shown in Figure \ref{fig:alpha-bleu-laal}. When $\alpha=10$, the SiMT model always estimates a low confidence and tends to wait for more tokens before generating translation. On the other hand, when $\alpha$ is too small ($\alpha=0$), the model’s performance decreases to some extent. We will further explore the impact of $\alpha$ in future work.
\begin{figure}[t]
\centering
\small
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{ablation-bleu-nature.pdf}
   \vspace{-0.1in} 
  \caption{Ablation Studies of SimulPL framework on Zh$\rightarrow$En SiMT task.}
  \vspace{-0.2in}
  \label{fig:ablation-bleu-laal}
\end{minipage}
\hfill  % 在两者之间插入间隙
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{alpha-bleu-nature.pdf}
   \vspace{-0.1in} 
  \caption{The comparison of SimulDPO with difference $\alpha$ values on Zh$\rightarrow$En SiMT task.}
  \vspace{-0.2in}
  	\label{fig:alpha-bleu-laal}
\end{minipage}
\hfill  % 在两者之间插入间隙
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{pos-bleu-nature.pdf}
   \vspace{-0.1in} 
  \caption{The comparison between different preference optimization methods.}
  \vspace{-0.2in}
  \label{fig:pos-bleu-laal}
  \end{minipage}
\end{figure}
\subsection{Generalization to Other Preference Optimization Methods}
% \begin{wrapfigure}{r}{0.3\textwidth}
% \begin{center}
% \advance\leftskip+1mm
%   \vspace{-0.3in} 
%  \includegraphics[width=0.3\textwidth]{pos-bleu-nature.pdf}
%    \vspace{-0.2in} 
%   \caption{The comparison between Simul-*PO methods and their origin versions on Zh$\rightarrow$En SiMT task.}
%   	\label{fig:data-win-rate}
% \vspace{-0.6in}
% \end{center}
% \end{wrapfigure}
In the SimulPL framework, SimulDPO is introduced for further preference optimization, which is adapted from DPO. Theoretically, SimulPL has generalization to any preference optimization methods, making them applicable to the SiMT scenarios. To validate this generality, we integrate CPO and KTO into the SimulPL framework, deriving SimulCPO and SimulKTO. Their respective training losses are provided in the following:
\begin{equation}
\small
\begin{split}
    &\mathcal{L}_{\mathrm{SimulCPO}} = -\log \sigma(\sum_t^{|\mathrm{y}^w|+1} u^{w}_t - \sum_t^{|\mathrm{y}^l|+1} u^l_t ) - \log \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) \\
    &u^{\ast}_t = \beta c^{\ast}_t\log\pi_{\theta}(y^{\ast}_t\mid \mathrm{x},\mathrm{y}^{\ast}_{\leq t-1})-\alpha c^{\ast}_t,  \quad \ast \in \{w,l\}
\end{split}
\end{equation}

\begin{equation}
\small
\begin{split}
    &\mathcal{L}_{\mathrm{SimulKTO}} = \lambda_{\mathrm{y}}-v(\mathrm{x},\mathrm{y}) \\
    &v(\mathrm{x},\mathrm{y}) = \begin{cases}  \lambda_{w}\sigma(\sum_t^{|\mathrm{y}|+1} r^{w}_t-z_0), & \text{if } \mathrm{y}\sim \mathrm{y}^w \mid \mathrm{x} \\ \lambda_{l}\sigma(z_0-\sum_t^{|\mathrm{y}|+1} r^l_t) , & \text{if } \mathrm{y} \sim \mathrm{y}^{l}\mid \mathrm{x}  \end{cases}
\end{split}
\end{equation}
where $\lambda_{\mathrm{y}}$ denotes $\lambda_{w}$ ($\lambda_{l}$) when $\mathrm{y}$ is desirable (undesirable), and $z_0=\mathbb{D}_{\mathrm{KL}}[\pi_{\theta}(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]$.

We evaluate their performances on the Zh$\rightarrow$En SiMT task. As shown in Figure \ref{fig:pos-bleu-laal}, both SimulCPO and SimulKTO achieve higher performance compared to CPO and KTO, particularly in low-latency levels. These results indicate the generalization of SimulPL. Additionally, SimulDPO, SimulCPO, and SimulKTO exhibit similar performance, making it difficult to determine which is most suitable for SiMT task. Besides preference optimization methods, SimulPL may also generalize to other tasks like simultaneous inference \citep{chen2024livemind}. We will explore this in future work. 

\section{Conclusion}
We bridge the gap in the study of SiMT human preferences and propose SimulPL, a preference learning framework tailored for SiMT task. Drawing from existing research, we categorize preferences in SiMT scenarios into five aspects: translation quality, monotonicity, key points, simplicity, and latency. By leveraging the first four preferences, SimulPL constructs human preference prompts to efficiently guide LLMs in generating preference data for SiMT. During the fine-tuning phase, SimulPL introduces MSFT for initial preference alignment. During the preference optimization phase, SimulPL proposes SimulDPO, integrating latency preference into the optimization objective and further improving the read/write policy. Our experiments indicate that SimulPL achieves better preference alignment both overall and across each aspect. Additionally, our analysis shows that SimulPL has a generalization to other preference optimization methods. 

\section*{Acknowledgments}
The research work has been supported by the Natural Science Foundation of China under Grant No. 62336008 and  No. 62476271.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Dataset Construction and Analysis}
\subsection{Human Preference Prompts}
\label{appendix:data-prompt}

Based on our categorized SiMT human preferences, we construct human preference prompts, which account for translation quality preference, monotonicity preference, key point preference, and simplicity preference, to efficiently guide GPT-4/4o in generating preference data for the SiMT task. Taking Zh$\rightarrow$En SiMT task as an example, our complete human preferences prompts are shown in Figure \ref{fig:prompt}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{prompt.pdf}
    \caption{Our constructed human preferences prompts on Zh$\rightarrow$En SiMT task.}
    \label{fig:prompt}
\end{figure}

\subsection{Further Evaluation of Our Annotated Datasets.}
\label{appendix:data-analysis}
We conduct both automated muti-aspect evaluation and additional human evaluation to validate the quality of our constructed dataset further. The details are described in the following.

\paragraph{Multi-aspect Evaluation.} Similar to Section \ref{section:human-prefer}, we also use our defined NIR, SLR, and DD to conduct multi-aspect evaluation on the GPT-generated references and the original references. We use Ref-free COMET to assess the translation quality here. The results in Table \ref{table:data-prefer} indicate that GPT-4/4o aligns better with human preferences. 

Then, we compare the multi-aspect evaluation results of GPT-generated translations and those manually revised by interpreters on the test sets. To facilitate comparison, we also provide the results for the original references of the test sets. As shown in Table \ref{table:data-prefer-test}, the translations generated by GPT-4/4o are either superior to or comparable with the original references in terms of monotonicity, key points, simplicity, and translation quality. Moreover, these results are very close to the manually revised translations. This indicates that the quality of the GPT-generated data is both reliable and aligned well with human preferences.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}

\begin{table}[H]
\centering
\caption{Multi-aspect evaluation on our annotated references and original references.}
\label{table:data-prefer}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccrrrcrrrcrrr}
\toprule
\multirow{2}{*}{\textbf{References}} & \multicolumn{4}{|c}{\textbf{Zh$\rightarrow$En}}                                        & \multicolumn{4}{|c}{\textbf{De$\rightarrow$En}}                                        & \multicolumn{4}{|c}{\textbf{En$\rightarrow$Zh}}                                        \\
\cmidrule{2-13}
                           & \multicolumn{1}{|c}{\textbf{NIR $\downarrow$}}                      & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c|}{\textbf{COMET}} & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}}                       & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c|}{\textbf{COMET}}        & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}} & \multicolumn{1}{c}{\textbf{DD $\downarrow$}}  & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c}{\textbf{COMET}}  \\
\midrule
GPT-4/4o                      & \multicolumn{1}{r}{11.39} & 5.48                   & 0.55      &  79.13          & \multicolumn{1}{r}{9.99}  & 5.61                   & 0.92      &    78.93          & \multicolumn{1}{r}{25.20} & 5.52                   & 0.89    &    80.30            \\
Origin                      & \multicolumn{1}{r}{45.41} & 6.34                   & 0.56       &  73.72         & \multicolumn{1}{r}{34.37} & 6.31                   & 1.09       &   75.02          & \multicolumn{1}{r}{44.05} & 6.20                   & 0.94   &     76.97           \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[H]
\centering
\caption{Multi-aspect evaluation on test sets.}
\label{table:data-prefer-test}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccrrrcrrrcrrr}
\toprule
\multirow{2}{*}{\textbf{References}} & \multicolumn{4}{|c}{\textbf{Zh$\rightarrow$En}}                                        & \multicolumn{4}{|c}{\textbf{De$\rightarrow$En}}                                        & \multicolumn{4}{|c}{\textbf{En$\rightarrow$Zh}}                                        \\
\cmidrule{2-13}
                            & \multicolumn{1}{|c}{\textbf{NIR $\downarrow$}}                      & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c|}{\textbf{COMET}} & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}}                       & \multicolumn{1}{c}{\textbf{DD $\downarrow$}} & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c|}{\textbf{COMET}}        & \multicolumn{1}{c}{\textbf{NIR $\downarrow$}} & \multicolumn{1}{c}{\textbf{DD $\downarrow$}}  & \multicolumn{1}{c}{\textbf{SLR $\downarrow$}} & \multicolumn{1}{c}{\textbf{COMET}}  \\
\midrule
GPT-4/4o                      & \multicolumn{1}{r}{6.48} & 6.55 & 0.49 & 78.32 & \multicolumn{1}{r}{5.49} & 5.71 & 0.92 & 80.86 & \multicolumn{1}{r}{10.65} & 5.63 & 0.88 & 81.24     \\
Manually Revised      & \multicolumn{1}{r}{6.71} & 6.69 & 0.52 & 79.05 & \multicolumn{1}{r}{5.52} & 5.84 & 0.95 & 81.89 &\multicolumn{1}{r}{10.47}  & 5.63 & 0.88 & 81.42 \\
Origin                      & \multicolumn{1}{r}{13.21} &  7.53 & 0.74 & 79.05 &  \multicolumn{1}{r}{7.80} & 6.29 & 1.09 & 80.78 &  \multicolumn{1}{r}{12.89} & 6.19 & 0.89 & 75.77   \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Human evaluation.} Following \citet{kocmi-etal-2022-findings} and \citet{xu2024contrastive}, we randomly sample 200 sentences from the test set and employ professional interpreters, who are not involved in the revision process, to manually assess their GPT-generated translations and manually-revised translations. This evaluation is conducted on our test sets. The evaluation criteria are as follows:

\begin{itemize}
\item \textbf{0:} The translation is poor and fails to convey any meaningful information.
\item \textbf{2:} The translation conveys some of the speaker’s information but misses key points. It also includes unnecessary reordering, leading to unnecessary delays in real-time scenarios, and uses less common expressions that make it harder for the audience to quickly understand.
\item \textbf{4:} The translation conveys all the important information with minimal unnecessary reordering. It uses simple expressions that generally meet the audience's needs, though there are still some minor issues.
\item \textbf{6:} The translation is a perfect interpretation, accurately conveying the speaker's key points while omitting unnecessary details. It uses expressions that align with spoken language conventions, significantly reflecting human preferences in simultaneous interpretation.
\end{itemize}

 The results are shown in Table \ref{table:human-score-test}. The average scores of GPT-generated translations are close to those are manually revised. Besides, the score distribution shows that most of the GPT-generated translations scored 4 or higher. These results suggest that the GPT-generated data aligns well with SiMT human preferences.

 \begin{table}[H]
\centering
\caption{Human evaluation on GPT-generated translations and manually revised translations.}
\label{table:human-score-test}
\resizebox{\textwidth}{!}{
\begin{tabular}{clcrrrrrr}
\toprule
     \multicolumn{1}{c|}{}                                   &     \multicolumn{1}{c|}{}                       & \multicolumn{3}{c|}{\textbf{Win-Tie-Lose}}                                                                                                                           & \multicolumn{4}{c}{\textbf{Distribution of Scores}}                                                                                                                                       \\
\cmidrule{3-9}
\multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{References}}}   & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{Average Score}}}        & { \textbf{win ratio}}                   & \multicolumn{1}{c}{{ \textbf{lose ratio}}} & \multicolumn{1}{c|}{{\textbf{tie ratio}}} & \multicolumn{1}{c}{{ \textbf{0}}} & \multicolumn{1}{c}{{ \textbf{2}}} & \multicolumn{1}{c}{{ \textbf{4}}} & \multicolumn{1}{c}{{ \textbf{6}}} \\
\midrule
\multicolumn{9}{c}{\textit{Chinese-English}}  \\
{ GPT-4/4o}         & \multicolumn{1}{c}{{ 5.37}} & \multicolumn{1}{r}{{ 6.00\%}}  & { 12.00\%}                        & { 82.00\%}                       & { 0.50\%}                & { 5.50\%}                & { 19.00\%}               & { 75.00\%}               \\
{ Manually Revised} & \multicolumn{1}{c}{{ 5.57}} & \multicolumn{1}{r}{{ 12.00\%}} & { 6.00\%}                         & { 82.00\%}                       & { 0.00\%}                & { 1.00\%}                & { 19.50\%}               & { 79.50\%}            \\
\midrule
\multicolumn{9}{c}{\textit{German-English}}  \\
{ GPT-4/4o}         & \multicolumn{1}{c}{{ 4.47}} & \multicolumn{1}{r}{{ 2.00\%}}  & { 80.50\%}                        & { 17.50\%}                       & { 5.50\%}                & { 19.50\%}                & { 21.00\%}               & { 54.00\%}               \\
{ Manually Revised} & \multicolumn{1}{c}{{5.01 }} & \multicolumn{1}{r}{{ 17.50\%}} & { 80.50\%}                         & { 2.00\%}                       & { 3.00\%}                & { 10.00\%}                & { 20.50\%}               & { 66.50\%}            \\
\midrule
\multicolumn{9}{c}{\textit{English-Chinese}}  \\
{ GPT-4/4o}         & \multicolumn{1}{c}{{ 4.61}} & \multicolumn{1}{r}{{ 1.00\%}}  & { 94.00\%}                        & { 5.00\%}                       & { 3.50\%}                & { 10.50\%}                & { 38.00\%}               & { 48.00\%}               \\
{ Manually Revised} & \multicolumn{1}{c}{{ 4.73}} & \multicolumn{1}{r}{{ 5.00\%}} & { 94.00\%}                         & { 1.00\%}                       & { 3.50\%}                & { 8.50\%}                & { 36.00\%}               & { 52.00\%}            \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Proofs and Derivations}
\subsection{Proof of Equivalence Between Output Length Constraint and Latency Optimization}
\label{appendix:proof-latency-equ-length}
To incorporate the goal of reducing latency into the optimization objective, we can directly include a latency evaluation metric. Specifically, we integrate Average Lagging (AL) \citep{ma2019stacl}, a commonly used metric for measuring SiMT latency, into the optimization objective as follows:
\begin{equation}
   \max_{\pi_{\theta}} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]-\mathbb{E}[\mathrm{AL}]
\label{equ:al-object}
\end{equation}
Given the sample $(\mathrm{X},\mathrm{Y})$, when receiving source prefix $\mathrm{x}$, the SiMT model needs to output the target prefix $\mathrm{y}$. We assume that the SiMT model can accept a source sentence with a maximum length of $M$ and generate a target sentence with a maximum length of $N$. Therefore, the following relationship holds:
\begin{equation}
    0 < |\mathrm{x}| \leq \mathrm{|X|} \leq M,\quad 0 < |\mathrm{y}| \leq \mathrm{|Y|} \leq N,
\end{equation}
During simultaneous translation, the prefix pair $(\mathrm{x},\mathrm{y})$ evolves from $(x_0,\emptyset )$ to $(\mathrm{X},\mathrm{Y})$. For the SiMT process passing by $(\mathrm{x},\mathrm{y})$, the maximum possible latency occurs when the SiMT model waits for the full input of $\mathrm{x}$ before starting to generate $\mathrm{y}$, then waits for the complete $\mathrm{X}$ before outputting the remaining part of $\mathrm{Y}$. In this situation,  AL can be computed as:
\begin{equation}
    \text{AL}=\frac{1}{\mathrm{y}}\sum_{t=1}^{|y|}(|\mathrm{x}|-\frac{t-1}{\frac{\mathrm{|Y|}}{\mathrm{|X|}}})=|\mathrm{x}|-\frac{|\mathrm{X}|}{2|\mathrm{Y}|}(\mathrm{|y|}-1) \leq (-\frac{1}{2N}|\mathrm{y}|+\frac{1}{2N}+1)|\mathrm{X}| = -C_1 \mathrm{|y|}+C_2
\end{equation}
where $C_1=\frac{\mathrm{|X|}}{2N}$,$C_2=\frac{\mathrm{|X|}}{2N}+|\mathrm{X}|$.
Therefore, we can derive the following relationship:
\begin{equation}
    \begin{split}
    & \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]-\mathbb{E}[\mathrm{AL}] \\
    \leq \  & \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})] + C_1\mathbb{E}[\mathrm{|y|}]-C_2
    \end{split}
\end{equation}
Based on this upper bound, we can optimize the objective in Equation \ref{equ:al-object} by optimizing the following one: 
\begin{equation}
    \max_{\pi_{\theta}} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})] + C_1\mathbb{E}[\mathrm{|y|}]-C_2
\end{equation}
Since $C_1$  and $C_2$ are constants, this objective is equivalent to:
\begin{equation}
    \begin{split}
    \max_{\pi_\theta} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]+\alpha\mathbb{E}[|\mathrm{y}|]
    \end{split}
\end{equation}
where $\alpha$ is the added hyper-parameter.
\subsection{Derivation of the Optimal Solution for Reward Maximization Constrained by KL Divergence and Latency}
\label{appendix:proof-optimal-solution}
Starting from Equation \ref{equ:simul-dpo-object}, we can conduct the derivation as follows:
\begin{equation}
    \begin{split}
            &\max_{\pi_\theta} \mathbb{E}_{\mathrm{x} \sim D, \mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})]-\beta\mathbb{D}_{\text{KL}}[\pi_\theta(\mathrm{y}\mid \mathrm{x})||\pi_{\mathrm{ref}}(\mathrm{y}\mid \mathrm{X})]+\alpha\mathbb{E}[|\mathrm{y}|] \\
            =& \max_{\pi_\theta} \mathbb{E}_{\mathrm{x} \sim D }\mathbb{E}_{\mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[r(\mathrm{x},\mathrm{y})-\beta \log{\frac{\pi_{\theta}(\mathrm{y}\mid \mathrm{x})}{\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X})}} +\alpha |\mathrm{y}|] \\
            = & -\min_{\pi_\theta}\mathbb{E}_{\mathrm{x} \sim D }\mathbb{E}_{\mathrm{y} \sim \pi_{\theta}(\mathrm{y}\mid \mathrm{x}) }[\log \frac{\pi_{\theta}(\mathrm{y}\mid \mathrm{x})}{\frac{1}{Z(\mathrm{x})}\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X}) \exp (\frac{1}{\beta}(r(\mathrm{x},\mathrm{y})+\alpha |\mathrm{y}|))}-\log Z(\mathrm{x})] \\
            = &-\min_{\pi_\theta}\mathbb{E}_{\mathrm{x} \sim D }[\mathbb{D}_{\text{KL}}[\pi_{\mathrm{ref}}||\frac{1}{Z(\mathrm{x})}\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X}) \exp (\frac{1}{\beta}(r(\mathrm{x},\mathrm{y})+\alpha |\mathrm{y}|))]-\log Z(\mathrm{x})] 
    \end{split}
\end{equation}
Where $Z(\mathrm{x})$ is the partition function, which is calculated as:
\begin{equation}
    Z(\mathrm{x}) = \sum_{\mathrm{y}}\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X}) \exp (\frac{1}{\beta}(r(\mathrm{x},\mathrm{y})+\alpha |\mathrm{y}|))
\end{equation}
Based on the property of KL divergence, we can gain the optimal solution for Equation \ref{equ:simul-dpo-object} as:
\begin{equation}
    \pi^{\ast}=\frac{1}{Z(\mathrm{x})}\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X}) \exp (\frac{1}{\beta}(r(\mathrm{x},\mathrm{y})+\alpha |\mathrm{y}|))
\end{equation}
Thus, we can derive the expression of $r(\mathrm{x},\mathrm{y})$ concerning $\pi_{\theta}$ and $|\mathrm{y}|$:
\begin{equation}
    r(\mathrm{x},\mathrm{y})=\beta \log{\frac{\pi_{\theta}(\mathrm{y}\mid \mathrm{x})}{\pi_{\mathrm{ref}} (\mathrm{y}\mid \mathrm{X})}}+\beta \log{Z(\mathrm{x})}-\alpha|\mathrm{y}|
\end{equation}

\section{Distinctions between SimulDPO and R-DPO}
\label{appendix:diff-with-length-work}
Firstly, the objectives and methods of R-DPO are entirely opposite to SimulDPO. In R-DPO, \citet{park2024disentangling} aim to prevent models from generating too long responses and use a regularization term of "$-\alpha |\mathrm{y}|$" to achieve this. In contrast, for the SiMT task, audiences prefer translations with low latency, which requires the SiMT model to translate as much content as possible based on the already received source prefix. To achieve this, SimulDPO introduces "$+\alpha |\mathrm{y}|$" as an additional constraint. It is important to note that the goal of SimulDPO is not to optimize for the length itself, but rather to optimize for latency preferences. 

Secondly, as shown in Equation \ref{equ:loss-sidpo}, we use $|\mathrm{y}|=\sum_{t=1}^{|\mathrm{y}|+1} c_t$ to make "$+\alpha |\mathrm{y}|$" differentiable, allowing gradient signals to be directly propagated to the parameters through backpropagation. In contrast, \citet{park2024disentangling} treats the "$-\alpha |\mathrm{y}|$" as a margin without further processing.

\section{Confidence-based Policy During Inference}
\label{appendix:conf-based-policy}

Algorithm \ref{policy-algorithm} further illustrates the confidence-based policy adopted by SimulPL during inference. As shown in Algorithm 1, in the confidence-based policy, we set the confidence threshold to 0.5 as the basis for read/write decisions. To examine its impact, we compare the performance of SimulPL on the Zh$\rightarrow$En task with different threshold values ($\gamma$). The results are presented in Table \ref{table:conf-thre-effect}. When $\gamma$
 is set to a small value ($\gamma$=0.1), the model is allowed to output tokens with low confidence, which results in a decline in translation quality, especially in low latency levels (0 $\leq$ LAAL $<$ 4). When $\gamma$ is set to a higher value ($\gamma$=0.9), the model imposes stricter constraints on token quality, leading to unnecessary delays. We plan to further explore the impact of $\gamma$ in our future work.
\begin{algorithm}
\caption{Confidence-based Policy In Inference}
\label{policy-algorithm}
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Streaming source prefix $\mathrm{x}$,$t=1$,read length $n$, $y_0 \leftarrow \left \langle  \mathrm{BOS} \right \rangle$}
\Output{Target outputs $\mathbf{Y}$}
\While{$y_{t-1} \neq \left \langle  \mathrm{EOS} \right \rangle$}{
    estimate confidence $c_{t}$;
    \eIf{$c_{t} \geq 0.5 $}{
        generate $y_t$ with $\mathbf{x},\mathrm{y}_{\leq t-1}$\;
        $t \leftarrow t+1$\;
        }{
        wait for next $n$ source words\;
        update $\mathrm{x}$;
    }
}
\end{algorithm}

\begin{table}[H]
\centering
\caption{Human evaluation on GPT-generated translations and manually revised translations.}
\label{table:conf-thre-effect}
\resizebox{\textwidth}{!}{
\begin{tabular}{r|rr|rr|rr|rr|rr}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{$n$}}} & \multicolumn{2}{c|}{\textbf{$\gamma$=0.1}}                                  & \multicolumn{2}{c|}{\textbf{$\gamma$=0.3}}                                  & \multicolumn{2}{c|}{\textbf{$\gamma$=0.5}}                                     & \multicolumn{2}{c|}{\textbf{$\gamma$=0.7}}                                     & \multicolumn{2}{c}{\textbf{$\gamma$=0.9}}                                     \\
\multicolumn{1}{c|}{}                              & \multicolumn{1}{c}{\textbf{LAAL}} & \multicolumn{1}{c|}{\textbf{SacreBLEU}} & \multicolumn{1}{c}{\textbf{LAAL}} & \multicolumn{1}{c|}{\textbf{SacreBLEU}} & \multicolumn{1}{c}{\textbf{LAAL}} & \multicolumn{1}{c|}{\textbf{SacreBLEU}} & \multicolumn{1}{c}{\textbf{LAAL}} & \multicolumn{1}{c|}{\textbf{SacreBLEU}} & \multicolumn{1}{c}{\textbf{LAAL}} & \multicolumn{1}{c}{\textbf{SacreBLEU}} \\
\midrule
3                                                 & 2.03                              & 20.76                                  & 1.99                              & 21.51                                  & 1.73                              & 22.51                                  & 1.99                              & 22.36                                  & 2.04                              & 22.51                                  \\
5                                                 & 3.21                              & 24.25                                  & 3.19                              & 24.89                                  & 2.90                              & 24.94                                  & 3.11                              & 25.24                                  & 3.29                              & 24.97                                  \\
7                                                 & 4.36                              & 26.48                                  & 4.36                              & 26.58                                  & 4.04                              & 26.76                                  & 4.28                              & 26.94                                  & 4.45                              & 26.78                                  \\
10                                                & 6.27                              & 28.17                                  & 6.25                              & 28.10                                  & 5.87                              & 27.97                                  & 6.18                              & 28.45                                  & 6.36                              & 27.81                                  \\
15                                                & 9.42                              & 29.86                                  & 9.40                              & 29.80                                  & 9.08                              & 29.94                                  & 9.22                              & 30.13                                  & 9.46                              & 30.20                                  \\
20                                                & 12.37                             & 30.17                                  & 12.35                             & 30.20                                  & 12.01                             & 30.05                                  & 12.15                             & 30.52                                  & 12.46                             & 30.47             \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Additional Implementation Details}
\label{appendix:implentation-details}
The hyper-parameters of the Transformer-based SiMT models used in our experiments are shown in Table \ref{table:transformer-hyper}. The hyper-parameters used by SimulPL during the MSFT and SimulDPO phases are listed in Table \ref{table:llm-hyper}. In the training process, we share the LoRA in the MSFT and SimulDPO phases. We use the instruction-following format to guide the LLM in completing the SiMT task. Our used prompt template is shown in Figure \ref{fig:prompt-simul}. For a fair comparison between Transformer-based and LLM-based SiMT models, we following \citet{mamonotonic} to apply greedy search during inference.

\section{Numerical Results}
\label{appendix:num-results}
Tables \ref{table:numerical-zh-en}, \ref{table:numerical-de-en}, and \ref{table:numerical-en-zh} respectively present the numerical results of different SiMT models on the Zh$\rightarrow$En, De$\rightarrow$En, and En$\rightarrow$Zh SiMT tasks. In addition to LAAL, we also recorded other common latency metrics such as AL \citep{ma2019stacl}, DAL \citep{mamonotonic}, and AP \citep{cho2016can}.

\section{Additional Analyses on De$\rightarrow$En SiMT Task}
\label{appendix:analysis-de-en}
In this section, we conduct our analysis experiments on De$\rightarrow$En task. The results are respectively shown in Figure \ref{fig:ablation-bleu-laal-de-en}, \ref{fig:alpha-bleu-laal-de-en}, and \ref{fig:pos-bleu-laal-de-en}. 
Through these experiments, we further validate our findings: Both SimulDPO and MSFT improve model performance, with a more pronounced effect at low latency levels; although the effect is less pronounced in the De$\rightarrow$En task compared to the Zh$\rightarrow$En task, SimulPL's performance is still influenced by $\alpha$; SimulPL also generalizes well to other preference optimization methods on the De$\rightarrow$En task.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{prompt-simulpl.pdf}
    \caption{Our prompt template in the SimulPL framework and other LLM-based SiMT models.}
    \label{fig:prompt-simul}
\end{figure}

\begin{figure}[h]
\centering
\small
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{ablation-bleu-nature-de-en.pdf}
   \vspace{-0.1in} 
  \caption{Ablation Studies of SimulPL framework on De$\rightarrow$En SiMT task.}
  \vspace{-0.2in}
  \label{fig:ablation-bleu-laal-de-en}
\end{minipage}
\hfill  % 在两者之间插入间隙
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{alpha-bleu-nature-de-en.pdf}
   \vspace{-0.1in} 
  \caption{The comparison of SimulDPO with difference $\alpha$ values on De$\rightarrow$En SiMT task.}
  \vspace{-0.2in}
  	\label{fig:alpha-bleu-laal-de-en}
\end{minipage}
\hfill  % 在两者之间插入间隙
\begin{minipage}{0.3\textwidth}
    \centering
 \includegraphics[width=\textwidth]{pos-bleu-nature-de-en.pdf}
   \vspace{-0.1in} 
  \caption{The comparison between different preference optimization methods on De$\rightarrow$En SiMT task.}
  \vspace{-0.2in}
  \label{fig:pos-bleu-laal-de-en}
  \end{minipage}
\end{figure}

\begin{table}[h]
\centering
\begin{minipage}{0.45\textwidth}
\caption{Hyper-parameters of Transformer-based SiMT models in our experiments.}
\label{table:transformer-hyper}
\begin{tabular}{l c }
\toprule
\multicolumn{2}{c}{\textbf{Transformer Hyper-parameter}} \\
\midrule[1pt]
encoder layers          & 6                              \\
encoder attention heads & 8                             \\
encoder embed dim       & 512                      \\
encoder ffn embed dim   & 1024                     \\
decoder layers          & 6                        \\
decoder attention heads & 8                    \\
decoder embed dim       & 512                     \\
decoder ffn embed dim   & 1024                     \\
dropout                 & 0.1                 \\
optimizer               & adam  \\
adam-$\beta$          & (0.9, 0.98)    \\
clip-norm               & 1e-7                \\
lr                      & 5e-4                \\
lr scheduler        & inverse sqrt  \\
warmup-updates          & 4000          \\
warmup-init-lr          & 1e-7     \\
weight decay            & 0.0001        \\
label-smoothing         & 0.1                \\
max tokens              & 8192 \\
\bottomrule[1pt]
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\caption{Hyper-parameters of SimulPL in our experiments.}
\centering
\label{table:llm-hyper}
\begin{tabular}{clc}
\toprule[1pt]
\multicolumn{3}{c}{\textbf{SimulPL Hyper-parameter}} \\
\midrule[1pt]
\multirow{3}{*}{LoRA}     & lora\_r            & 64   \\
                          & lora\_alpha        & 16   \\
                          & lora\_dropout      & 0.1  \\
\midrule
\multirow{4}{*}{MSFT}     & batch\_size           & 64   \\
                          & micro\_batch\_size & 32   \\
                          & learning\_rate     & 2e-4 \\
                          & training steps     & 1000 \\
\midrule
\multirow{4}{*}{SimulDPO} & $\alpha$            & 0.1  \\
                          & $\beta$            & 0.1  \\
                          & batch\_size           & 64   \\
                          & micro\_batch\_size & 16   \\
                          & learning\_rate     & 2e-6 \\
                          & training steps     & 400  \\

\bottomrule[1pt]
\end{tabular}
\end{minipage}
\end{table}

\begin{table}[t]
\centering
\caption{Numerical results on Zh$\rightarrow$En SiMT task.}
\label{table:numerical-zh-en}
\begin{tabularx}{\textwidth}{>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X}
\toprule
\multicolumn{7}{c}{\textbf{Chinese$\rightarrow$English}}                                \\
\midrule
\multicolumn{7}{c}{wait-$k$}                                \\
\midrule
\multicolumn{1}{c}{$k$}        & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
1                            & 9.70                      & 56.42                     & 2.02                     & 1.04                   & 0.48                   & 2.53                    \\
3                            & 17.88                    & 68.80                      & 4.22                     & 2.92                   & 0.69                   & 4.93                    \\
5                            & 21.93                    & 73.90                      & 6.46                     & 4.70                    & 0.88                   & 7.27                    \\
7                            & 24.79                    & 75.96                     & 8.48                     & 6.60                    & 1.00                      & 9.34                    \\
9                            & 25.73                    & 76.48                     & 10.26                    & 8.54                   & 1.05                   & 11.11                   \\
\midrule
\multicolumn{7}{c}{SM$^2$}       \\                           \midrule                                                  
\multicolumn{1}{c}{$\gamma$} & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
0.3                          & 21.96                    & 75.43                     & 3.99                     & 2.41                   & 0.75                   & 7.29                    \\
0.4                          & 25.87                    & 77.82                     & 5.44                     & 3.65                   & 0.86                   & 9.51                    \\
0.5                          & 27.94                    & 78.75                     & 6.80                      & 5.06                   & 0.93                   & 11.73                   \\
0.55                         & 28.60                     & 79.06                     & 7.55                     & 5.95                   & 0.97                   & 13.07                   \\
0.6                          & 28.83                    & 79.33                     & 8.64                     & 7.17                   & 1.01                   & 14.99                   \\
0.65                         & 29.04                    & 79.44                     & 10.23                    & 8.98                   & 1.07                   & 17.33                   \\
0.7                          & 29.10                     & 79.55                     & 12.41                    & 11.45                  & 1.13                   & 20.69                   \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT}
\\
\midrule
\multicolumn{1}{c}{$n$}        & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 10.07                    & 71.98                     & 3.96                    & -13.82                & 1.24                  & 7.81                   \\
5                            & 14.02                    & 76.51                     & 4.30                    & -14.45                & 1.12                  & 8.12                   \\
7                            & 22.01                    & 78.13                     & 4.59                    & -4.10                 & 0.85                  & 8.99                   \\
10                           & 26.24                    & 79.39                     & 6.11                    & 0.57                  & 0.84                  & 11.39                  \\
15                           & 29.54                    & 80.45                     & 9.21                    & 5.99                  & 0.88                  & 15.20                  \\
20                           & 30.08                    & 80.83                     & 12.06                   & 9.53                  & 0.93                  & 18.51                  \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT + DPO}
\\
\midrule
\multicolumn{1}{c}{$n$}        & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 12.08                   & 69.39                     & 3.39                     & -10.22                & 1.03                  & 6.54                    \\
5                            & 17.49                    & 75.95                     & 3.56                    & -9.87                 & 0.93                  & 7.16                    \\
7                            & 24.44                    & 78.44                     & 4.27                    & -2.35                 & 0.79                   & 8.48                   \\
10                           & 27.35                    & 79.84                     & 5.96                     & 2.59                  & 0.80                  & 11.08                  \\
15                           & 29.12                    & 81.06                     & 9.14                    & 6.10                  & 0.87                  & 15.12                  \\
20                           & 29.82                    & 81.44                     & 12.04                   & 9.45                   & 0.93                  & 18.49                  \\
\midrule
\multicolumn{7}{c}{SimulPL}
\\
\midrule
\multicolumn{1}{c}{$n$}        & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 22.51                    & 75.20                      & 1.73                    & -3.25                 & 0.68                  & 4.50                   \\
5                            & 24.94                    & 77.44                     & 2.90                    & -1.31                 & 0.70                  & 6.43                   \\
7                            & 26.76                    & 78.79                     & 4.04                    & 0.42                  & 0.73                  & 8.32                   \\
10                           & 27.97                    & 79.78                     & 5.87                    & 2.42                  & 0.79                  & 11.05                  \\
15                           & 29.94                    & 80.85                     & 9.08                    & 6.21                  & 0.86                   & 15.11                  \\
20                           & 30.05                    & 81.14                     & 12.01                   & 9.65                  & 0.91                  & 18.49    
\\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[]
\caption{Numerical results on De$\rightarrow$En SiMT task.}
\label{table:numerical-de-en}
\begin{tabularx}{\textwidth}{>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X}
\toprule
\multicolumn{7}{c}{\textbf{German$\rightarrow$English}}                                                                                                                                                    \\
\midrule
\multicolumn{7}{c}{wait-$k$}                                                                                                                                                                        \\
\midrule
\multicolumn{1}{c}{$k$}      & \multicolumn{1}{c}{BLEU}  & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL}  & \multicolumn{1}{c}{AL}   & \multicolumn{1}{c}{AP}    & \multicolumn{1}{c}{DAL}   \\
1                            & 12.77                     & 56.08                     & 0.68                      & 0.06                     & 0.46                      & 1.54                      \\
3                            & 31.11                     & 69.52                     & 2.98                      & 1.40                     & 0.87                      & 4.11                      \\
5                            & 39.24                     & 76.62                     & 4.98                      & 3.28                     & 1.04                      & 6.14                      \\
7                            & 41.42                     & 79.41                     & 7.21                      & 5.05                     & 1.23                      & 8.25                      \\
9                            & 42.59                     & 80.36                     & 9.02                      & 6.95                     & 1.32                      & 9.99                      \\
\midrule
\multicolumn{7}{c}{SM$^2$}     \\                                                                                                                     \midrule
\multicolumn{1}{c}{$\gamma$} & \multicolumn{1}{c}{BLEU}  & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL}  & \multicolumn{1}{c}{AL}   & \multicolumn{1}{c}{AP}    & \multicolumn{1}{c}{DAL}   \\
0.3                          & 38.76                     & 80.25                     & 4.06                      & 2.43                     & 1.05                      & 7.62                      \\
0.4                          & 43.63                     & 82.27                     & 5.21                      & 3.30                     & 1.18                      & 9.27                      \\
0.5                          & 44.99                     & 83.22                     & 6.49                      & 4.52                     & 1.29                      & 10.94                     \\
0.55                         & 45.79                     & 83.63                     & 7.23                      & 5.25                     & 1.35                      & 11.95                     \\
0.6                          & 46.05                     & 83.77                     & 8.06                      & 6.21                     & 1.40                      & 12.97                     \\
0.65                         & 46.47                     & 83.99                     & 9.05                      & 7.40                     & 1.45                      & 14.23                     \\
0.7                          & 46.82                     & 84.18                     & 10.27                     & 8.85                     & 1.52                      & 15.76                     \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT}  \\                                                                                                                \midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU}  & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL}  & \multicolumn{1}{c}{AL}   & \multicolumn{1}{c}{AP}    & \multicolumn{1}{c}{DAL}   \\
3                            & 19.05                     & 79.46                     & 2.58                      & -11.35                   & 1.32                      & 4.98                      \\
5                            & 40.16                     & 82.95                     & 3.37                      & -0.16                    & 0.85                      & 6.31                      \\
7                            & 44.04                     & 84.25                     & 4.48                      & 1.56                     & 0.86                      & 7.98                      \\
10                           & 48.58                     & 85.47                     & 6.24                      & 3.98                     & 0.90                      & 10.34                     \\
15                           & 50.68                     & 86.30                     & 9.22                      & 7.36                     & 0.97                      & 13.65                     \\
20                           & 52.13                     & 86.66                     & 12.11                     & 10.56                    & 1.02                      & 16.13                     \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT+DPO}                                                                                                                                                                 \\
\midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU}  & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL}  & \multicolumn{1}{c}{AL}   & \multicolumn{1}{c}{AP}    & \multicolumn{1}{c}{DAL}   \\
3                            & 19.82                     & 79.52                     & 2.46                      & -9.20                    & 1.25                      & 4.79                      \\
5                            & 38.75                     & 82.86                     & 3.16                      & -0.43                    & 0.85                      & 6.28                      \\
7                            & 43.88                     & 84.16                     & 4.49                      & 1.82                     & 0.85                      & 8.03                      \\
10                           & 48.90                     & 85.41                     & 6.24                      & 4.16                     & 0.88                      & 10.34                     \\
15                           & 50.78                     & 86.18                     & 9.25                      & 7.57                     & 0.95                      & 13.67                     \\
20                           & 52.41                     & 86.27                     & 12.13                     & 10.77                    & 1.00                      & 16.14                     \\
\midrule
\multicolumn{7}{c}{SimulPL}                                                                                                                                                                        \\
\midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU}  & \multicolumn{1}{c}{LAAL}  & \multicolumn{1}{c}{AL}    & \multicolumn{1}{c}{AP}   & \multicolumn{1}{c}{DAL}   & \multicolumn{1}{c}{COMET} \\
3                            & \multicolumn{1}{r}{40.81} & \multicolumn{1}{r}{2.05}  & \multicolumn{1}{r}{-1.43} & \multicolumn{1}{r}{0.74} & \multicolumn{1}{r}{4.22}  & \multicolumn{1}{r}{81.15} \\
5                            & \multicolumn{1}{r}{44.57} & \multicolumn{1}{r}{3.27}  & \multicolumn{1}{r}{0.42}  & \multicolumn{1}{r}{0.78} & \multicolumn{1}{r}{6.14}  & \multicolumn{1}{r}{83.53} \\
7                            & \multicolumn{1}{r}{46.84} & \multicolumn{1}{r}{4.45}  & \multicolumn{1}{r}{2.00}  & \multicolumn{1}{r}{0.82} & \multicolumn{1}{r}{7.90}  & \multicolumn{1}{r}{84.44} \\
10                           & \multicolumn{1}{r}{49.23} & \multicolumn{1}{r}{6.28}  & \multicolumn{1}{r}{4.12}  & \multicolumn{1}{r}{0.88} & \multicolumn{1}{r}{10.32} & \multicolumn{1}{r}{85.41} \\
15                           & \multicolumn{1}{r}{50.86} & \multicolumn{1}{r}{9.24}  & \multicolumn{1}{r}{7.49}  & \multicolumn{1}{r}{0.95} & \multicolumn{1}{r}{13.65} & \multicolumn{1}{r}{86.27} \\
20                           & \multicolumn{1}{r}{52.30} & \multicolumn{1}{r}{12.15} & \multicolumn{1}{r}{10.69} & \multicolumn{1}{r}{1.00} & \multicolumn{1}{r}{16.13} & \multicolumn{1}{r}{86.53} \\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[]
\caption{Numerical results on En$\rightarrow$Zh SiMT task.}
\label{table:numerical-en-zh}
\begin{tabularx}{\textwidth}{>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X>{\raggedleft\arraybackslash}X}
\toprule
\multicolumn{7}{c}{\textbf{English$\rightarrow$Chinese}}                                                                                                                                                      \\
\midrule
\multicolumn{7}{c}{wait-$k$}                                                                                                                                                               \\
\midrule
\multicolumn{1}{c}{$k$}      & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
1                            & 6.37                     & 57.00                     & 0.59                     & 0.36                   & 0.33                   & 1.17                    \\
3                            & 26.09                    & 70.28                     & 2.77                     & 1.83                   & 0.75                   & 3.54                    \\
5                            & 35.52                    & 76.39                     & 4.80                     & 3.59                   & 0.96                   & 5.52                    \\
7                            & 38.46                    & 78.91                     & 6.73                     & 5.45                   & 1.08                   & 7.35                    \\
9                            & 40.76                    & 80.00                     & 8.37                     & 7.32                   & 1.12                   & 8.94                    \\
\midrule
\multicolumn{7}{c}{SM$^2$}                                                                                                                                                                 \\
\midrule
\multicolumn{1}{c}{$\gamma$} & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
0.3                          & 39.46                    & 79.83                     & 4.52                     & 3.38                   & 0.91                   & 6.81                    \\
0.4                          & 41.96                    & 81.01                     & 5.31                     & 4.19                   & 0.97                   & 7.77                    \\
0.5                          & 42.81                    & 81.64                     & 6.13                     & 5.13                   & 1.01                   & 8.82                    \\
0.6                          & 43.27                    & 81.97                     & 7.38                     & 6.53                   & 1.08                   & 10.52                   \\
0.65                         & 43.63                    & 82.14                     & 8.30                     & 7.61                   & 1.12                   & 11.92                   \\
0.7                          & 43.90                    & 82.22                     & 9.62                     & 9.08                   & 1.16                   & 13.60                   \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT}                                                                                                                                                            \\
\midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 7.58                     & 69.86                     & 4.43                     & -57.94                 & 4.02                   & 7.43                    \\
5                            & 30.88                    & 79.61                     & 4.40                     & -1.91                  & 1.14                   & 7.19                    \\
7                            & 40.31                    & 81.87                     & 5.46                     & 3.92                   & 0.92                   & 8.45                    \\
10                           & 43.62                    & 82.96                     & 7.29                     & 6.47                   & 0.91                   & 10.55                   \\
15                           & 44.47                    & 83.54                     & 10.00                    & 9.48                   & 0.96                   & 13.26                   \\
20                           & 44.47                    & 83.58                     & 12.23                    & 11.84                  & 1.00                   & 15.17                   \\
\midrule
\multicolumn{7}{c}{LLM-PFX-SFT+DPO}                                                                                                                                                      \\
\midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 7.85                     & 69.28                     & 4.42                     & -54.99                 & 3.87                   & 7.41                    \\
5                            & 31.39                    & 79.81                     & 4.26                     & -0.77                  & 1.04                   & 6.92                    \\
7                            & 39.95                    & 81.41                     & 5.47                     & 3.88                   & 0.92                   & 8.48                    \\
10                           & 43.21                    & 82.33                     & 7.26                     & 6.45                   & 0.91                   & 10.52                   \\
15                           & 44.08                    & 83.05                     & 10.01                    & 9.49                   & 0.96                   & 13.28                   \\
20                           & 44.12                    & 83.10                     & 12.23                    & 11.83                  & 0.99                   & 15.17                   \\
\midrule
\multicolumn{7}{c}{SimulPL}                                                                                                                                                               \\
\midrule
\multicolumn{1}{c}{$n$}      & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{COMET} & \multicolumn{1}{c}{LAAL} & \multicolumn{1}{c}{AL} & \multicolumn{1}{c}{AP} & \multicolumn{1}{c}{DAL} \\
3                            & 37.26                    & 79.53                     & 2.64                     & 1.24                   & 0.73                   & 4.87                    \\
5                            & 40.72                    & 81.51                     & 3.97                     & 2.86                   & 0.79                   & 6.58                    \\
7                            & 42.30                    & 81.77                     & 5.32                     & 4.28                   & 0.87                   & 8.33                    \\
10                           & 43.86                    & 82.81                     & 7.18                     & 6.37                   & 0.90                   & 10.46                   \\
15                           & 44.83                    & 83.58                     & 9.89                     & 9.33                   & 0.96                   & 13.21                   \\
20                           & 45.09                    & 83.68                     & 12.18                    & 11.75                  & 1.00                   & 15.16           \\
\bottomrule
\end{tabularx}
\end{table}


\end{document}
