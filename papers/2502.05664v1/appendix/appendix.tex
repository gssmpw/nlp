% \vspace{20mm}
\newpage

\vspace*{\textheight}
\newpage

\noindent {\LARGE \textbf{Appendix}} \\

\section{Algorithm of \ourapproach}
\label{alg:codesim}

Algorithm~1 shows the pseudo-code of our prompting technique.

\begin{algorithm}[h]
\small
\caption{\tool}
\begin{algorithmic}[1]

\State $p \gets$ maximum number of planning steps
\State $d \gets$ maximum number of debugging steps \\
% \State $problem \gets$ problem to be solved\\

\For{$i \gets 1$ to $p$}
    \State \# Start of Planning Agent
    \State $plan \gets$ GeneratePlan($problem$)
    \State $feedback \gets$ ValidatePlan($problem$, $plan$)
    \If{$feedback$ is negative}
        \State $plan \gets$ RefinePlan($problem$, $plan$, $feedback$)
    \EndIf
    \State \# End of Planning Agent
    \\
    \State \# Start of Coding Agent
    \State $code \gets$ GenerateCode($problem$, $plan$) %\Comment{Coding Agent}
    
    \State $passed,\space log \gets$ test($code$, $sample\_io$) %Test code using sample IO\\
    \If{$passed$}
        \State Return $code$
    \Else
        % \State Pass $code$ to Debugging Agent
        \State \# Start of Debugging Agent
        \For{$j \gets 1$ to $d$}
            \State $code \gets$ DebugCode(\\
                \quad \quad \quad \quad \quad $problem$,\\
                \quad \quad \quad \quad \quad $plan$, \\
                \quad \quad \quad \quad \quad $code$, \\
                \quad \quad \quad \quad \quad $log$ \\
            \quad \quad \quad \quad) \\ %\Comment{Debugging Agent}
            \State $passed,\space log \gets$ test($code$, $sample\_io$)
            \If{$passed$}
                \State Return $code$
            \EndIf
        \EndFor
        \State \# End of Debugging Agent
    \EndIf
    \State \# End of Coding Agent
    \\
\EndFor
\State Return $code$
\end{algorithmic}
\end{algorithm}

\section{Exclusion of AgentCoder}
\label{app:elimination-of-agentcoder}
We have not included AgentCoder \cite{huang2023agentcoder} in our comparison due to reproducibility issues which undoubtedly plays a critical role in fair comparison as indicted in \citet{laskar-etal-2024-systematic}, as we were unable to replicate their results. In our attempts to reproduce their work on the HumanEval benchmark using ChatGPT, we achieved $56.7$\% accuracy after four iterations, consuming $11.9$ million tokens. When using GPT-4, we attained only $17.7$\% accuracy after two iterations, with $10.4$ million tokens consumed. The token consumption in both cases is significantly higher compared to MapCoder ($1.7$ million tokens with ChatGPT and $2.1$ million with GPT-4) and \toolnospace($0.89$ million tokens in ChatGPT and $0.85$ million in GPT-4). These two experiments resulted in a cost of approximately \$$500$ USD, but we were still unable to come close to AgentCoder's reported claims of $79.9$\% accuracy with ChatGPT and $96.3$\% with GPT-4.

Furthermore, we found unaddressed issues on their GitHub page (\href{https://github.com/huangd1999/AgentCoder/issues/8}{link}) related to reproducibility. Additionally, for the MBPP dataset, they used all test cases as public test cases (\href{https://github.com/huangd1999/AgentCoder/issues/3}{link}), which deviates from standard practices. As a result, we did not consider those results in our comparison either.

\section{Details Promptings of \tool}
\label{app:prompts}

The \emph{Planning Agent} interacts with the LLM three times to generate a plan. In the first API call, it instructs the LLM to comprehend the problem, generate an example problem, recommend a suitable algorithm, and finally produce the plan (Figure \ref{prompt:plan-generation}). In the second API call, the LLM is instructed to verify the plan through simulation (Figure \ref{prompt:plan-verification}). If the plan is satisfactory, it is returned by the agent. Otherwise, the LLM is called again to refine the plan based on the feedback from the simulation (Figure \ref{prompt:plan-refinement}).

The next step involves the \emph{Coding Agent}, which receives the plan from the \emph{Planning Agent} and uses the prompt outlined in Figure \ref{prompt:code-generation} to generate code.

If the code fails to pass the sample input/output, \tool activates its final agent, the \emph{Debugging Agent}, using the prompt shown in Figure \ref{prompt:debugging}.

These figures also include the rationale behind the inclusion of each sentence in the prompt.

\section{Example Problem}
We present a complete example of problem solving using \tool below:

% \onecolumn
% \newpage

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.90\textwidth]{figures/steps/Plan-Generation.pdf}
    \caption{\emph{Planning Agent}: Prompt for Plan Generation. }
    \label{prompt:plan-generation}
\end{figure*} 

\begin{figure*}[h]
    \centering
    % \hspace*{-0.5cm}
    \includegraphics[width=0.90\textwidth]{figures/steps/Plan-Verification.pdf}
    \caption{\emph{Planning Agent}: Prompt for Plan Verification with the help of Simulation.}
    \label{prompt:plan-verification}
\end{figure*} 

\begin{figure*}[h]
    \centering% \hspace*{-0.5cm}
    \includegraphics[width=0.90\textwidth]{figures/steps/Plan-Refinement.pdf}
    \caption{\emph{Planning Agent}: Prompt for Plan Refinement.}
    \label{prompt:plan-refinement}
\end{figure*} 

\begin{figure*}[h]
    \centering% \hspace*{-0.5cm}
    \includegraphics[width=0.90\textwidth]{figures/steps/Code-Generation.pdf}
    \caption{\emph{Coding Agent}: Prompt for Code Generation.}
    \label{prompt:code-generation}
\end{figure*} 
\onecolumn

\begin{figure*}[h]
    \centering% \hspace*{-0.5cm}
    \includegraphics[width=0.90\textwidth]{figures/steps/Debugging.pdf}
    \caption{\emph{Debugging Agent}: Prompt for Debugging.}
    \label{prompt:debugging}    
\end{figure*}

% \onecolumn

\begin{tcolorbox}[breakable]
\label{app:example-problem}
    \input{appendix/example}
\end{tcolorbox}
\twocolumn  % Switch back to two-column mode