\section{Results}
\label{sec:results}
% \tool demonstrates outstanding performance in both basic and competitive-level programming tasks. The following subsections detail the results of our experiments.

\subsection{Basic Code Generation}
% We observed exceptional results using ChatGPT, GPT-4, and GPT-4o across HumanEval, HumanEval-ET, EvalPlus-HumanEval, MBPP, and MBPP-ET datasets. In all cases, \tool surpassed the baseline models. A notable performance decline was seen in GPT-4o when compared to strong baselines such as Reflexion, LATS, and MapCoder, indicating that these methods did not scale well with the newer models. However, \tool consistently enhanced performance across all models.


% \rizwan{lik table here}
% In Table \ref{tab:basic-dataset-result} we evaluate the model performances on simple code geneartion tasks. Overall, \tool demonstrates consistent superior performance than all other baselines across all the datasets using all the LLMs. Notably, \tool achieves top scores with GPT4o, reaching 95.1\% on HumanEval, 87.2\% on EvalPlus, and 90.7\% on MBPP, resulting in an impressive 82.7\% overall average. This represents a significant improvement over the next best method, MapCoder, which scores 79.0\% on average with GPT4o. \toolnospace's effectiveness is consistent across different model variants, outperforming other approaches with ChatGPT (75.1\% avg.) and GPT4 (81.3\% avg.) as well. The method's robust performance across diverse datasets, including the challenging MBPP-ET where it achieves 61.5\% with GPT4, underscores its versatility in handling various programming tasks. These results strongly indicate that \toolnospace's simulation-driven planning and debugging approach marks a substantial advancement in code generation and problem-solving capabilities.

In Table \ref{tab:basic-dataset-result}, we evaluate the model performances on simple code generation tasks. Overall, \tool demonstrates consistently superior performance compared to all other baselines across all datasets and LLMs. Notably, \tool achieves top scores with GPT-4o, reaching {\bf 95.1\%} on HumanEval, {\bf87.2\%} on EvalPlus, and {\bf90.7\%} on MBPP, resulting in an impressive {\bf82.7\%} overall average and their new state-of-the-art (SoTA)  results. This represents a significant improvement over the next best method, MapCoder, which scores 79.0\% on average with GPT-4o. \toolnospace's effectiveness is consistent across different model variants, outperforming other approaches with ChatGPT ({\bf 75.1\% avg}) and GPT-4 ({\bf 81.3\% avg}) as well. The method's robust performance across diverse datasets, including the challenging MBPP-ET where it achieves {\bf61.5\%} with GPT-4, underscores its versatility in handling various programming tasks. These results strongly indicate that \toolnospace's simulation-driven planning and debugging approach marks a substantial advancement in code generation and problem-solving capabilities, as it consistently outperformed other baselines.



\subsection{ Competitive Problem Solving}

In Table \ref{tab:cc-dataset-results}, we evaluate performance on complex, contest-level code generation tasks. \tool delivers significant improvements over other baselines in solving complex contest-level code generation tasks. With GPT-4, \tool reaches a strong {\bf 29.1\%} on CodeContests and {22.0\%} on APPS, marking a consistent edge over MapCoder’s 25.3\% average. %\rizwan{maybe add few words on AlphaCodium}
The performance gains are even more pronounced with ChatGPT, where \tool achieves a {\bf 16.4\%} on CodeContests, and {12.0\%} on APPS resulting {\bf 14.2\%} overall, outperforming MapCoder’s 12.0\%. These results highlight \toolnospace's ability to handle the complexity of contest-level problems more effectively, especially through its simulation-driven approach. 


% \tool also achieved strong results in competitive problem-solving datasets. In the CodeContest dataset, \tool outperformed the strong baseline, MapCoder, when using both ChatGPT and GPT-4. Similarly, in the APPS dataset, \tool surpassed MapCoder with ChatGPT and matched its performance with GPT-4. Additionally, \tool consumed fewer tokens compared to MapCoder (Table \ref{tab:ablation-token-counts}).

\input{tables/contest-results}
\input{tables/open-llm-results}

\subsection{Performance Across Open-source LLMs}
\label{sec:exp:open-source}
% To demonstrate the generalizibility, we also evaluate \tool’s performance with open-source LLMs such as Gemma2-9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. In all cases, \tool outperformed other benchmarks by a significant margin (Table \ref{tab:open-llm-results}). On LLaMa3.1-70B, \tool achieves an accuracy of 90.2\%, which is comparable to GPT-4o, OpenAI's latest model. In contrast, Reflexion \cite{shinn2023reflexion} showed no improvement in accuracy. This demonstrates \tool’s generalization capability across various LLMs.

To further demonstrate \toolnospace’s generalization capability, we evaluate its performance with open-source LLMs, including Gemma2-9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. As shown in Table \ref{tab:open-llm-results}, \tool consistently outperforms all other methods across these models. On LLaMa3.1-70B, \tool achieves an accuracy of {\bf 90.2\%} on HumanEval and {\bf 76.2\%} on EvalPlus, with an average of {\bf 80.1\%}, closely matching GPT-4o's performance. Due to the complex prompting scheme of MapCoder, open-source LLMs often struggle to generate output in the correct format. Therefore, we exclude MapCoder from this experiment. On the other hand, Reflexion shows minimal improvement in accuracy. These results highlight \toolnospace's strong generalization ability across various LLM architectures, even on smaller models like Gemma2-9B that achieves a notable avg accuracy of {\bf 75.8\%}.


