
\section{\tool}
\label{sec:mapcoder}
\label{sec:codesim}
% The main focus of this work is to develop a multi-agent system where the agents will interact with one another and generate correct code. The agents should be designed in such a way so that they can help each other to build a complete environment that produce correct code as much as possible. For this reason we have taken three agents in our system imitating human programming cycle. They are planning, coding, and debugging agent. Then using a traversal plan we have accommodate all the agents. All of them are described below:

Our goal is to develop a multi-agent code generation approach capable of complex problem solving. Drawing inspiration from recent works like MapCoder and ChatDev (in a different context), we devise the agents in \tool for planning, coding, and debugging. 
While these existing approaches focus primarily on expanding steps without verifying underlying hypotheses, we address this limitation by introducing a novel verification approach. Our approach simulates input/output step-by-step, verifying generated plans and performing internal debugging, mirroring how humans understand, visualize, and refine in algorithm development. Below, we present our proposed model.




% We devise a pipeline sequence for \toolnospace, intelligently cascading the agents in a structured way and enhancing each agent's capability by augmenting in-context learning signals from previous agents in the pipeline. However, not all the agent responses/outputs are equally useful. Therefore, additionally, \tool features an adaptive agent traversal schema to interact among corresponding agents dynamically, iteratively enhancing the generated code by, for example, fixing bugs, while maximizing the usage of the LLM agents.  In this section, we first discuss the agents (as per the pipeline), their prompts, and interactions, followed by the dynamic agent traversal protocol in \tool towards code generation for competitive problem-solving. 

\input{tables/basic-datasets-result}

\subsection{Planning Agent}
\label{subsec:planning-agent}
% This is our first agent. This agent receives a problem description and generate a detailed plan for solving the problem. After receiving a problem if we tell the LLM to generate a plan directly without any further knowledge that results in wrong plan in most of the cases. So, we replicate a programmers' behavior here. A human programmer when receives a problem it first recall a similar problem that he/she has solved before. This recalling is important because it helps to get a direction for starting the plan generation of the original problem. Then he/she generate a plan. After plan generation he/she might not start coding, in fact it is not good practice. Common practice is to simulate the sample input using the plan. After following the steps if someone cannot get expected output then it revice the plan again and then start coding. We have imitated these steps to generate a plan. For generating relevant example problems we have utilizes the inherent knowledge of the LLM that is gained while training. So, first we instruct the LLM to think about the problem and understand it, then give a similar problem, and finally generate a planning for the original problem. After generating a plan we instruct the LLM to follow that plan and make a simulation with the sample input. After completing this simulation if the simulation results are not matched with the expected output then LLM respond us to modify the plan otherwise LLM response that the plan is ok. If we get a negative response then we instruct the LLM to revice the plan and return the revised plan from this step.

% The first agent in \tool is a \emph{Planning Agent}. Given a problem description, the \emph{Planning Agent} first generate an exemplar-a relevant problem (description), its plan and and solution. This step steps reflects a human programmer when receives a problem it first recall a similar problem that he/she has solved before.  This exemplar or recalling is important because it helps to get a direction for starting the plan generation of the original problem. However, instead of generating $k$ ungrounded exemplars, we only generate one exemplar at a time. We instruct the LLM to think about the original problem and understand it, then leveraging the exemplar to generate a planning for the original problem. After generating a plan we instruct the LLM to follow that plan and make a simulation with the sample input. After completing this simulation if the simulation results are not matched with the expected output then LLM respond us to modify the plan otherwise LLM response that the plan is ok. If we get a negative response then we instruct the LLM to revise the plan and return the revised plan from this step.

The first component of \tool is the \emph{Planning Agent}. Given a problem description, the \emph{Planning Agent} generates a single exemplar—a relevant problem along with its plan and solution. This mimics the behavior of human programmers, who, when faced with a new problem, first recall a similar problem they’ve previously solved. This exemplar-based recall is crucial as it provides a starting point for constructing a solution plan. Instead of generating multiple ungrounded exemplars as in MapCoder, our agent focuses on only one at a time. We then instruct the LLM to %thoroughly understand the original problem and use the exemplar as a guide for generating
generate an appropriate plan. Once the plan is created, the LLM simulates (step-by-step) the solution with a sample input. If the simulation result does not match the expected output, the agent prompts the LLM to revise the plan. Otherwise, the plan is deemed valid. In the case of failure, the \emph{Planning Agent} refines the plan. The complete prompts for the Planning Agent—including plan generation, verification, and refinement—are provided in the Appendix (Figure \ref{prompt:plan-generation}, 
\ref{prompt:plan-verification}, \ref{prompt:plan-refinement}).
% \smallskip
\subsection{Coding Agent}
\label{subsec:coding-agent}
% Next is the \emph{Coding Agent}. It takes the problem description, and a plan from the \emph{Planning Agent} as input and implements the corresponding planning into code to solve the problem. After generating code \tool evaluate it with the sample I/O. If the code passed all the sample test cases then we return this code as a solution otherwise pass it to the following agent.

Next component is the \emph{Coding Agent}, which takes the problem description and the plan generated by the \emph{Planning Agent} as input. The role of this agent is to translate the plan into executable code that solves the given problem. Once the code is generated, \tool evaluates it using sample input/output test cases. If the code passes all sample tests, it is returned as the final solution. Otherwise, the code is handed over to the next agent for further refinement. Figure \ref{prompt:code-generation} in the Appendix provides the complete prompt used by the \emph{Coding Agent}.

% \smallskip
\subsection{Debugging Agent}
\label{subsec:debugging-agent}
% The last agent, \emph{Debugging Agent} takes the original problem, plan from the \emph{Planning Agent}, generated code and the execution (unit testing) log  from the \emph{Coding Agent}, as input and try to debug the code. For debugging we again utilize the power of the simulation. We instruct the LLM to simulate the code with sample input where it fails to generate expected outcome. In this way the LLM can go through each step of the generated code and can detect where the bug is. Then we instruct the LLM to solve that bug by giving a modified code. We have shown the complete prompt in Appendix (Figure \ref{prompt:debugging}). Note that, different from the other techniques such as LATS \cite{zhou2023lats}, AgentCoder \cite{huang2023agentcoder}, Reflexion \cite{shinn2023reflexion}, our \emph{Debugging Agent} does not require any additional test case generation in the pipeline. We have discussed the reason of excluding additional I/O generation phase from our approach on Ablation Study \ref{subsec:impact-of-additional-io}. 

The final component, the \emph{Debugging Agent}, receives the original problem, the plan from the \emph{Planning Agent}, the code generated by the \emph{Coding Agent}, and the execution (unit testing) log as input to debug the code. To identify bugs, instead of directly prompting the LLMs, we uniquely leverage the simulation once again. The LLM is instructed specifically to simulate the code on inputs where it fails to produce the expected output, allowing it to trace the execution step by step and locate the error. Once the bug is identified, the LLM modifies the code to resolve the issue. The complete prompt for the \emph{Debugging Agent} is shown in the Appendix (Figure \ref{prompt:debugging}). Unlike other approaches such as LATS \cite{zhou2023lats}, AgentCoder \cite{huang2023agentcoder}, and Reflexion \cite{shinn2023reflexion}, our \emph{Debugging Agent} does not require additional test case generation. The rationale behind excluding this phase is discussed in the Ablation Study \ref{subsec:impact-of-additional-io}.

% \begin{figure}
%     \centering
%     \hspace*{-0.3cm}
%     \includegraphics[width=0.49\textwidth]{figures/steps/step-2-short.pdf}
%     \vspace{-6mm}
%     \caption{Prompt for \emph{Planning Agent}.} 
%     % The example problem mentioned in this figure will come from the Self-retrieval Agent.}
%     \label{fig:prompt-agent-2-short}
%     \vspace{-4mm}
% \end{figure}

% \vspace{-mm}
% \begin{figure}[h]
%     \centering
%     \hspace*{-0.3cm}
%     \includegraphics[width=0.49\textwidth]{figures/steps/step-4-short.pdf}
%     \vspace{-6mm}
%     \caption{Prompt for \emph{Debugging Agent}.}
%     \label{fig:prompt-agent-4-short}
%     \vspace{-4mm}
% \end{figure} 



\smallskip
\subsection{Adaptive Iteration}
\label{sec:agent-traverse}


\tool employs an adaptive iteration starting with the \emph{Planning Agent}, which generates plans for the given problem. These plans are passed to the \emph{Coding Agent}, which translates them into code and tests against sample I/Os. If all tests pass, the code is returned; otherwise, it's sent to the \emph{Debugging Agent}. The \emph{Debugging Agent} attempts to fix the code for up to $d$ attempts. If unsuccessful after $d$ attempts, the process returns to the \emph{Planning Agent}, restarting the cycle. Once code passing all sample I/Os is obtained, the cycle ends, returning the code as the final output solution for evaluation against hidden test cases. This entire process repeats for a maximum of $p$ cycles if needed. Algorithm \ref{alg:codesim} in the Appendix summarizes our adaptive agent traversal. The algorithm's complexity is $O(pd)$. Appendix \ref{app:example-problem} provides a comprehensive example of how \tool solves a problem. 
% comparing \toolnospace's problem-solving approach to others (e.g., Direct, Chain-of-thought, and Reflexion).


% \tool employs an adaptive iteration starting with the \emph{Planning Agent}, which generates plans for the given problem. These plans are then passed to the \emph{Coding Agent}, which translates them into code and tests against sample I/Os. If all tests pass, the code is returned; otherwise, it's sent to the \emph{Debugging Agent}. The \emph{Debugging Agent} attempts to fix the code for up to $d$ iterations. If unsuccessful, the process returns to the \emph{Planning Agent}, restarting the cycle. Once code passing all sample I/Os is obtained, the iteration ends returning the code as the final output solution  which then are evaluated against the hidden test cases. However, this process repeats for maximum $p$ iterations. Algorithm \ref{alg:codesim} in the Appendix summarizes our adaptive agent traversal. The algorithm's complexity is $O(pd)$. Appendix \ref{app:example-problem} provides a comprehensive example comparing \toolnospace's problem-solving approach to Direct, Chain-of-thought, and Reflexion methods.

% The dynamic traversal in \tool begins with the \emph{Planning Agent}, which outputs the plans for the original problem. Then the generated plan is pushed to the \emph{Coding Agent}. \emph{Coding Agent} translates the plan into code, tested with sample I/Os. If all pass, the code is returned; otherwise, it's passed to \emph{Debugging Agent}. They attempt to fix the code iteratively up to $d$ times. If Debugging agent failed to generate a code that passes all the sample I/Os then we will again go to the \emph{Planning Agent} and repeat the above cycle again. As soon as it gets a code that passes all the sample I/Os the code is returned for evaluation with hidden test cases. This iterative process continues for $p$ iterations. We summarize our dynamic agent traversal in Algorithm \ref{alg:codesim} in Appendix. Our algorithm's complexity is $O(pd)$. A complete example illustrating \toolnospace's problem-solving compared to Direct, Chain-of-thought, and Reflexion approaches is shown in Appendix \ref{app:example-problem}.

%in Figure %\ref{fig:qualitative-example}.
%Detailed prompts for each agent are in Appendix \ref{app:prompts}.