\begin{abstract}
\begin{nobreakwords}
Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use  compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce {\bf \toolnospace}, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis—planning, coding, and debugging—through a human-like perception approach.  As human verifies their understanding of any algorithms through visual simulation, \tool uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate \toolnospace's remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results—({HumanEval \bf 95.1\%}, {MBPP \bf 90.7\%}, {APPS \bf 22\%}, and {CodeContests \bf 29.1\%}). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (\codebase).
\end{nobreakwords}
\end{abstract}