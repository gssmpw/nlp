

\section{Experimental Setup}
% \noindent\textbf{Datasets:} 
\subsection{Datasets}
Following MapCoder, we evaluate \tool on five basic programming benchmarks i.e., \textbf{HumanEval}~\cite{chen2021codex}, \textbf{HumanEval-ET}~\cite{dong2023codescore}, \textbf{EvalPlus}~\cite{evalplus}, \textbf{MBPP})~\cite{austin2021program}, and \textbf{MBPP-ET}~\cite{dong2023codescore} and two competitive programming datasets i.e., \textbf{APPS} \cite{hendrycks2021apps}, and \textbf{CodeContest} \cite{li2022codecontest}. For fair comparison, we collect all the datasets from the repository of the MapCoder. 

% \smallskip
% \noindent\textbf{Baselines:} 
\subsection{Baselines and Metric}
\label{sec:baselines-metrics}
To evaluate \toolnospace, we compare it against state-of-the-art code generation approaches, including {\bf MapCoder}, as well as several notable methods: {\bf Direct}, Chain of Thought  (\textbf{CoT}) \cite{wei2022chain}, \textbf{Self-Planning}  \cite{jiang2023self}, \textbf{Analogical Reasoning}  \cite{yasunaga2023large}, and \textbf{Self-collaboration} \cite{dong2023selfcollaboration}. For simpler programming tasks, we include strong baselines such as Reflexion \cite{shinn2023reflexion} and LATS \cite{zhou2023lats}. We exclude AgentCoder \cite{huang2023agentcoder} due to reproducibility issues (discussed in Appendix \ref{app:elimination-of-agentcoder}). 
%For complex problem solving we consider additional baselines AlphaCodium \citep{ridnik2024code}.
For fair comparison, our evaluation utilizes \href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{ChatGPT (gpt-3.5-turbo-1106)}, \href{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo}{GPT-4 (gpt-4-1106-preview)} from OpenAI, alongside open-source LLMs such as Gemma2-9B, Mixtral8x7B, LLaMa3.1-8B, and LLaMa3.1-70B. For basic programming tasks, we report next-generation performance with additional evaluations using  \href{https://platform.openai.com/docs/models/gpt-4o}{GPT-4o (gpt-4o-2024-08-06)}. We adopt the widely used $pass@1$ metric, where a model is deemed successful if its sole predicted solution is correct.

\subsection{Reproducibility}
We aim to contribute to the NLP community by open-sourcing all of our code along with result logs, enabling others to reproduce our findings. For simple programming, we set the maximum number of planning tries to $p = 5$ and debugging tries to $d = 5$. For the competitive problem solving, we used $p = 3$ and $d = 3$ by default except for the CodeContest with GPT-4 where $p = 3$, $d = 5$.  

% To evaluate \tool,  along with the state-of-the-art code generation approach  \textbf{MapCoder} \cite{islam-etal-2024-mapcoder}, we also consider 
% such as \textbf{Direct}, Chain of Thought Prompting (\textbf{CoT}) \cite{wei2022chain}, \textbf{Self-Planning} Prompting \cite{jiang2023self}, \textbf{Analogical Reasoning} Prompting \cite{yasunaga2023large}, \textbf{Self-collaboration} \cite{dong2023selfcollaboration}. For simple programing tasks, we also consider the other strong baselines \textbf{Reflexion} \cite{shinn2023reflexion}, and \textbf{LATS} \cite{zhou2023lats}. We exclude AgentCoder \cite{huang2023agentcoder} due to its reproducibility issues (discussed in Appendix \ref{app:elimination-of-agentcoder}). We  evaluate all the datasets using \href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{ChatGPT (gpt-3.5-turbo-1106)}, \href{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo}{GPT-4 (gpt-4-1106-preview)}, \href{https://platform.openai.com/docs/models/gpt-4o}. We have also evaluated our method using open-source LLM such as Gemma2-9B, Mixtral8x7B, LLaMa3.1-8B, LLaMa3.1-70B. For basic programming, which benchmarks are more widely evaluated on, we provided the next gen performance using additional {GPT4o (gpt-4o-2024-08-06)}.  We  use the widely adopted $Pass@1$ evaluation metric, where the model is considered successful if the only predicted solutions is correct (modify this definiition if wrong).  

% % \smallskip
% % \noindent
% % \textbf{Foundation Models,  Evaluation Metric, $k$, $t$:} 
% % \subsection{Foundation Models, Evaluation Metric, $k$, and $t$}
% With  $k = t = 5$ in HumanEval, and $k = t = 3$ for others, we evaluate all the datasets using \href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{ChatGPT (gpt-3.5-turbo-1106)}, \href{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo}{GPT-4 (gpt-4-1106-preview)} from OpenAI and \href{https://deepmind.google/technologies/gemini/#gemini-1.0}{Gemini Pro} from Google. We have also evaluated our method using an open-source LLM, Mistral-7B-instruct. We have used the Pass@k evaluation metric, where the model is considered successful if at least one of the $k$ generated solutions is correct.
