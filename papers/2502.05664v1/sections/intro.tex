\section{Introduction}
\label{sec:intro}


\begin{figure*}
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.99\textwidth]{figures/CodeSim-Overview.pdf}
    \vspace{-4mm}
    \caption{Overview of \toolnospace: It consists of three agents—planning, coding, and debugging. The \emph{Planning Agent} first generates an exemplar problem-solution (i.e., via self-retrieval) and devises a plan, which is then verified and refined through simulation. Next, the \emph{Coding Agent} implements the plan. Finally, the \emph{Debugging Agent} addresses potential bugs through step-wise simulation across $d$ trials. The entire process iterates $p$ times.
    }
    \label{fig:overview}
    \vspace{-4mm}
\end{figure*}


In recent years, the rise of Large Language Models (LLMs) has made significant advances in AI-assisted coding and reshaped the domain of code generation and problem-solving \citep{LLMSurvey}. Code generation assistants built on GPT-4 \citep{openai2024gpt4technicalreport}, Mistral \citep{jiang2023mistral}, and Llama \citep{dubey2024llama3herdmodels}, inter alia, have demonstrated unprecedented ability to understand, generate, and manipulate code across various programming languages and problem domains. However, despite these advancements, significant challenges persist in generating code for complex programming tasks. 

% These challenges encompass the need for deep comprehension of natural language problem descriptions, multi-step reasoning beyond memorization, and expertise in algorithms and data structures. Additionally, the ability to generate substantial, functional code that aligns with comprehensive test cases remains a formidable hurdle in the field of AI-assisted programming and problem-solving \cite{khan2023xcodeeval}.


Current state-of-the-art approaches in code generation typically employ a \emph{dual-pass} process \cite{shi2024code-mgdebug, jin2024rgd, zhong-etal-2024-debug, levin2024chatdbg}. In the \emph{first pass}, they use LLMs to generate an initial,  fully/partially correct version of the program. Then accordingly in the \emph{second pass}, they apply external tool-based iterative debuggers that leverage runtime compiler feedback or other diagnostic tools to refine and correct the generated code. While this approach has shown promise, it necessitates numerous iterations of LLM-tool interactions, and importantly its effectiveness is heavily dependent on the quality of the initial code generation—a process that continues to present substantial difficulties. Therefore, in this paper, we present {\bf \toolnospace}, a novel multi-agent code generation framework that seamlessly synthesizes complex code solutions without external resources, while offering potential for further enhancement through minimal external debugging. 
 

Synthesizing programs even in the \emph{first pass}, however, is fundamentally challenging, requiring a deep understanding of natural language processing, computer algorithms, data structures, and problem-solving strategies. These challenges are further compounded when attempting to generate code for competitive programming problems or advanced software engineering tasks, where adherence to specific constraints or passing unit tests are paramount \cite{khan2023xcodeeval}. 


While earlier code generation methods employed direct approaches \cite{chen2021codex}, chain-of-thought reasoning \citep{CoT}, synthesized test-case guidance \cite{codet}, retrieval-augmented generation \cite{parvez2021retrieval}, and various in-context exemplar-based strategies \citep{shum-etal-2023-automatic, zhang2022automatic}, recent paradigms have shifted toward plan-based \cite{jiang2023self}, sampling or tree-searching \cite{zhou2023lats}, self-retrieval \cite{yasunaga2023large}, and diverse agent-based approaches \cite {zhang-etal-2024-codeagent, qian-etal-2024-chatdev, shinn2023reflexion, huang2023agentcoder, dong2023selfcollaboration}.

% Early approaches utilizing LLMs for code generation employ a direct prompting approach, where LLMs generate code directly from problem descriptions and sample I/O . Recent methods like chain-of-thought \citep{CoT} advocates modular or pseudo code-based generation to enhance planning and reduce errors, while retrieval-based approaches such as  \citet{parvez2021retrieval} leverage relevant problems and solutions to guide LLMs' code generations. A promising solution to the above challenge is self-reflection~\cite{shinn2023reflexion, chen2022codet}, which iteratively evaluates the generated code against test cases, reflects on mistakes and modifies accordingly. However, such approaches have limitations too. Firstly, while previous studies indicate that superior problem-solving capabilities are attained when using in-context exemplars \citep{shum-etal-2023-automatic, zhang2022automatic, CoT} or plans \citep{jiang2023self}, these approaches, during both code generation and debugging, only leverage the problem description itself in a zero-shot manner. Consequently, their gains can be limited. 

Most recently, MapCoder \cite{islam-etal-2024-mapcoder} proposes a multi-agent framework that implements agents emulating different stages of program synthesis such as recalling relevant examples, designing/planning, code generation, and testing/debugging. While this approach mimics a real developer's code generation cycle and shows improvements, it focuses solely on expanding steps without verifying the underlying hypotheses, with tests being performed only during the debugging phase. Consequently, the resulting gains are limited and it also requires larger number of iterations (i.e., LLM API calls).

To address these limitations, \toolnospace—built upon planning, coding, and debugging agents—introduces a novel verification approach inspired by human problem-solving. By simulating input/output step-by-step, \tool verifies both the generated plans and performs internal debugging, mirroring how humans understand, visualize, and refine algorithms. This simulation-driven planning and debugging process ensures that each step is thoroughly evaluated, significantly enhancing both solution quality and efficiency. Figure \ref{fig:overview} shows an overview of our proposed approach, \tool and in Figure \ref{fig:qualitative-example}, we demonstrate how simulation assists in both plan verification and debugging, highlighting its crucial role in improving problem-solving accuracy. 


% , their performance in code generation tasks remains limited.


% as planning, coding, and debugging. There is one crucial step that is missing in this domain that is simulation. Common practice of a human programmer is to validate the generated plan by applying it on sample IO. If the plan successfully generate the expected output then he/she start coding. And at the time of debugging simulation also helps to detect the bug.







% Moreover, we design a novel structured pipeline schema that intelligently cascades the LLM agents and incorporates a dynamic iteration protocol to enhance the generation procedure at every step. We made our prompting scheme simple and make it lightweight so that it did not take too much tokens. 

% Additionally, existing iterative self-reflection methods rely on extra test cases generated by LLM agents (e.g., AgentCoder \cite{huang2023agentcoder}, LATS \cite{zhou2023languageLATS}, self-reflection \cite{shinn2023reflexion}) or external tools, compounding the challenges. Test case generation is equally challenging as code generation \cite{pacheco2007feedback}, %with success reliant on the model and the dataset. 
% and incorrect test cases can lead to erroneous code. Blindly editing code based on these test cases can undermine problem-solving capabilities. For instance, while self-reflection boosts GPT-4's performance on the HumanEval dataset, it drops by 3\% on the MBPP dataset \cite{shinn2023reflexion}. Upon identification, to validate this, on the HumanEval dataset itself, we replace their GPT-4 with ChatGPT, and see that model performance drops by 26.3\%. Therefore, our debugging agent performs unit tests and bug fixing using only the sample I/O, without any artifact-more plausible for real-world widespread adoption. 

% draws inspiration from human cognition and perception of problem-solving process. 


% Our approach uniquely addresses the entire spectrum of program synthesis, encompassing planning, coding, and debugging stages, through a human-like perception model.

% A key innovation of \tool is its incorporation of visual simulation techniques for plan verification and internal debugging. This approach mirrors the human process of algorithm comprehension and verification, where step-by-step visualization plays a crucial role. By simulating input/output processes, \tool can internally verify and debug generated code, leading to more robust and accurate results.

% Our framework leverages multiple specialized LLM agents that collaborate throughout the code generation process. This multi-agent approach allows for a more comprehensive and nuanced handling of complex programming tasks, with different agents focusing on specific aspects of the problem-solving process.


% We evaluate \tool on eight popular programming synthesis benchmarks including both basic programming like HumanEval, MBPP and challenging competitive program-solving benchmarks like APPS, CodeContests and xCodeEval. With multiple different LLMs including ChatGPT, GPT-4, GPT4o, LLaMa, Gemma, and Mixtral, our approach significantly enhances their problem-solving capabilities - consistently achieving new SOTA performances, often outperforming strong baselines like MapCoder \cite{islam2024mapcoder}. In addition, our experiments indicate that \tool's performance can be further enhanced when combined with external debugging tools such as LDB \cite{zhong-etal-2024-debug}--suggesting a promising direction for future research in hybrid code generation and debugging systems. Furthermore, with detailed ablation studies, we analyze \tool to provide more insights. By open-sourcing our framework, we aim to contribute to the broader research community and facilitate further advancements in AI-assisted programming and problem-solving.


We evaluate \tool on seven popular programming synthesis benchmarks, including foundational tasks like HumanEval and MBPP, as well as challenging competitive problem-solving benchmarks such as APPS, and CodeContest. Our experiments leverage multiple LLMs, including ChatGPT, GPT-4, GPT-4o, LLaMa, Gemma, and Mixtral, showcasing significant improvements in their program synthesis capabilities. \tool consistently achieves state-of-the-art performances, often surpassing strong baselines like MapCoder. Additionally, our findings suggest that \toolnospace's performance can be further improved when integrated with external debugging tools, such as LDB \cite{zhong-etal-2024-debug}, highlighting a promising direction for future research in hybrid code generation and debugging systems. Through detailed ablation studies, we provide valuable insights into \toolnospace's functionality. We will open-source our framework to support future research in AI-assisted programming and problem-solving.


% To evaluate the effectiveness of \tool, we conducted extensive experiments across eight challenging competitive problem-solving and program synthesis benchmarks. Our results demonstrate significant improvements over existing methods, achieving new state-of-the-art performance on several key metrics. Notably, \tool achieved a pass@1 rate of 95.1\% on the HumanEval benchmark and 45.3\% on xCodeEval, with results pending for MBPP, APPS, and CodeContests.





% %%%%%%%%%%%%%%%%%%%%%%%% 






\iffalse

The remainder of this paper is organized as follows: Section 2 provides an overview of related work in the field of AI-driven code generation and debugging. Section 3 details the architecture and methodology of our \tool framework. Section 4 presents our experimental setup and results, followed by a comprehensive discussion in Section 5. Finally, we conclude in Section 6 with a summary of our contributions and directions for future work.






% Ashraful Comments: The following copied from Mapcoder (Slight change) needs to be changed whole sentences I think 
Computer Programming has emerged as an ubiquitous problem-solving tool that brings tremendous benefits to every aspects of our life \cite{li2022competition, parvez-etal-2018-building}. To maximize programmers’ productivity, and enhance accessibility, automation in program synthesis is paramount. 


% With the growth of LLMs, significant advancements have been made in program synthesis—driving us in an era where we can generate fully executable code, requiring no human intervention~\cite{chowdhery2022palm, nijkamp2022codegen, islam2024mapcoder}.
\fi



