\section{Ablation Studies and Analyses}
\label{sec:ablation-study}

\subsection{Impact of Different Agents}
\label{sec:agent-impact-ablation}
Our primary contributions are two folds: (i) the simulation-guided plan verification step within the \emph{Planning Agent} and (ii) the bug fixing process through simulation in \emph{Debugging Agent}. To evaluate the significance of these components, we ablate these two parts of our approach and present the results in Table \ref{tab:agent-disabling}. The findings confirm that both components contribute significantly. 

\input{tables/ablationwa}



\subsection{Fine-grained Analysis of the Impact of Simulation}
\label{subsec:impact-simulation}
Table \ref{tab:simulation-impact} presents the impact of incorporating \textit{Simulation} in \toolnospace. The results show that \tool consistently outperforms other approaches across both simple and multi-agent settings, demonstrating superior performance with both open-source and proprietary LLMs. This highlights the effectiveness of \textit{Simulation} in enhancing problem-solving efficiency within our pipeline. 

\input{tables/ablation-simulation-impact}

\subsection{Impact of Varying Programming Languages}
\label{subsec:various-programming-languages}
To evaluate the performance of \tool across various programming languages, we utilized the xCodeEval \cite{khan2023xcodeeval} dataset. The experimental results, presented in Table \ref{tab:veriying-PL}, demonstrate that \tool maintains strong performance across different programming languages, highlighting its versatility and effectiveness.

\input{tables/ablation-vpl}


\subsection{Use of External Debugger}
\label{subsec:ldb-debugger}

\input{tables/ldb-results}

\noindent
The performance of \tool can be further enhanced by incorporating an external debugger in the \emph{second pass}. We experiment with LDB as the external debugger on HumanEval dataset in Table \ref{tab:ldb-results}. We use the output code from the most competitive \emph{first-pass} generation methods, including \toolnospace, Reflexion, and MapCoder, using GPT-4o as the backbone. These seed programs are then passed to LDB, which was tested with two different LLMs: ChatGPT and GPT-4o. As can be seen, \tool achieves $95.1$\% accuracy in the \emph{first pass} with GPT-4o, surpassing Reflexion's \emph{second pass} performance of $94.5$\%.  By utilizing LDB with GPT-4o, \tool achieves a \emph{second pass} accuracy of $97.6$\%, setting a new state-of-the-art result for a \emph{dual-pass} approach. In addition, we note that the \emph{second pass} with LDB consumes 39K more tokens in Reflexion compared to our approach, highlighting the efficiency  of \toolnospace.
% LDB GPT4o + CodeSim GPT4o = 219.8k
% LDB GPT4o + Reflexion GPT4o = 258.8k

% However, this \emph{dual-pass} process requires approximately 51.6 thousand more tokens than the \emph{single-pass} approach of \toolnospace.


\subsection{Qualitative Example}
We also conduct a qualitative analysis to better understand how \tool improves performance across various datasets. Figure \ref{fig:qualitative-example} demonstrates how \tool enhances the plan through simulation and assists in debugging the code using the same technique. A complete example, including LLM output, is provided in Appendix \ref{app:example-problem}.

\subsection{Impact of $p$ and $d$}
\tool includes two key hyperparameters: the maximum number of planning steps ($p$) and the maximum number of debugging steps ($d$). By varying these parameters, we plot the results in Figure \ref{tab:ablation-p-d-results}, which shows a proportionate improvement in performance. It is important to note that higher values of $p$ and $d$ lead to more API calls and increased token consumption, allowing users to adjust these parameters to balance between accuracy and cost.


\input{tables/ablationpd}

\subsection{Impact of Number of Sample I/Os}
\label{subsec:impact-of-sample-io}
The HumanEval dataset has an average of only $2.82$ sample I/Os per example, which is a relatively small number for deriving meaningful insights. In this ablation, we augment the dataset by adding 5 more sample I/Os from the HumanEval-ET dataset. This augmentation increases performance notably, leading to \textbf{$89$\%} accuracy with ChatGPT, a \textbf{$3.5$\%} improvement over previous results, \textbf{$86$\%}.


\input{tables/ablation-token-counts}


\subsection{Impact of Synthesizing Additional I/O}
\label{subsec:impact-of-additional-io}
Increasing the number of sample I/Os for testing can enhance the overall performance of our approach, as indicated in \ref{subsec:impact-of-sample-io}. Based on this insight, we use a self-consistency \cite{wang2023selfconsistency} method to generate additional test cases. We instruct the LLM to generate five more test cases for each problem, covering both basic and edge cases. The LLM is called twice, and we select the test cases that are present in both responses. However, this approach results in a performance decline. With ChatGPT we achieve \textbf{$78$\%} accuracyâ€”a \textbf{$9.3$\%} decrease from the original \textbf{$86$\%}. This indicates that generating additional I/Os is a non-trivial task that may negatively impact final outcomes.

\subsection{API Call and Token Analysis}
\label{subsec:api-time-analysis}
We compare the API calls and token consumption of our approach with the previous state-of-the-art method, MapCoder \cite{islam-etal-2024-mapcoder}, as shown in Table \ref{tab:ablation-token-counts}. The results reveal that \tool not only improves performance but also reduces token consumption. On average, \tool uses $4.13$ thousand fewer tokens while achieving a $7.1$\% increase in accuracy, proving that \tool is more efficient in both accuracy and token usage compared to MapCoder.

\subsection{Error Analysis and Challenges}
\begin{nobreakwords}
Although \tool demonstrates strong performance compared to other methods, it faces challenges in specific algorithmic domains. The APPS dataset \cite{hendrycks2021apps} includes problems with three levels of difficulty: (i) Introductory, (ii) Interview, and (iii) Competition. Figure \ref{fig:difficulty-levels} illustrates the performance of different approaches based on difficulty level. The results indicate that for introductory and interview-level problems, \tool does not surpass MapCoder when using ChatGPT. Additionally, when using GPT-4, \tool struggles to outperform MapCoder on interview-level problems. Upon manual review, we observe that for more complex issues, such as dynamic programming (DP), \tool encounters difficulties in constructing the DP table.
\end{nobreakwords}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/results/apps-difficulty-levels.png}
    \vspace{-3mm}
    \caption{Performance of different approaches across different difficulty levels on the APPS dataset.}
    \label{fig:difficulty-levels}
    \vspace{-4mm}
\end{figure}  

