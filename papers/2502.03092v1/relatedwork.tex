\section{Related Work}
\label{sec:related-work}
Works related to communication-efficient FL~\cite{chen2021communication} can be mainly categorized into the following four topics: sparsification, quantification, distillation, and others:

\noindent\textbf{Sparsification}: 
Gradients are sparse and only portions of gradients are actually useful for optimization~\cite{aji2017sparse}.
Consequently, sparsifiers including top-$k$ or random-$k$ are introduced in~\cite{aji2017sparse,lin2017deep,wangni2018gradient} to enable a compression rate of 1/100 and thereby greatly reducing communication costs.
Theoretical analyses in~\cite{sahu2021rethinking} also support these methods by formally showing that top-$k$ is the communication-optimal sparsifier under a limited communication budget. 
Nevertheless, no prior knowledge is used during sparsification, leading to high compressing errors~\cite{alistarh2018convergence} and affecting the convergence rate severely.

\noindent\textbf{Quantification}: 
It is well-known that full-precision (32-bit) data types are redundant in machine learning~\cite{denil2013predicting}.
Thus, transmitted data are cast into smaller data types before communicating~\cite{bernstein2018signsgd,alistarh2017qsgd,seide20141}, thereby reducing communication overhead.
However, since the 1-bit data type is the smallest, gradient quantification has limited flexibility and owns a compression rate of up to 1/32.

\noindent\textbf{Distillation}:
Wu~\cite{wu2022communication} used knowledge distillation to distill the local model to a smaller model before communicating. 
Nevertheless, it requires an extra public dataset for alignments between the teacher and the student, which not only incurs additional computational costs but also introduces domain gaps between the public dataset and the clients' local dataset.
Goetz~\cite{goetz2020federated} and Hu~\cite{hu2022fedsynth} propose to generate a synthetic dataset from the full dataset that has a smaller size and can be used to approximate gradients.
Still, the synthesis process requires simulating the multi-step optimization of model weights for multiple epochs, leading to not only high time and space complexity (\textit{i.e.}, gradients and intermediate model weights are calculated and stored many times), but also great instability and possible collapse (Figure~\ref{fig:pre-exp-one}) due to excessively deep back-propagation paths and the gradient explosion, especially for large models and datasets.

\noindent\textbf{Others}:
In~\cite{chen2019communication,zhou2021communication}, researchers alter the FL workflow to allow parallelism in the communication and the computation, without modifying the transmitted data.
Therefore, this parallelism can be used with other data-compressing algorithms combinedly to further accelerate the training process. Li~\cite{li2021communication} proposed utilizing compressed sensing in data compressing and decompressing.
However, the optimization of parameters in compressed sensing can be non-trivial. 

The proposed E-3SFC is quite different from the previous works mentioned above.
First, the proposed E-3SFC is more privacy-preserving than others, since the transmitted information is not even the model itself.
Under the same communication budget, E-3SFC empirically converges faster than the sparsification method (see Figure.~\ref{fig:accuracy-loss}).
Then, unlike distillation, the proposed E-3SFC imposes a lower computational burden since it does not employ public datasets and only utilizes single-step model optimization.
Additionally, E-3SFC possesses the merits of higher communication efficiency and flexibility.
The proposed E-3SFC can reduce the overall communication overhead during the uploading and downloading phases of FL with flexible communication budget schedulers.
In terms of flexibility, it can be seamlessly integrated into communication-computation parallelism and compressed sensing to to enhance its usability.


\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/3sfc-arch.pdf}
    \caption{The general workflow of 3SFC. When compressing in \ding{185}, a set of trainable parameters and labels (i.e., synthetic features) will first be fed into the frozen local model to calculate model gradients. Then, calculated model gradients will be compared with real model gradients to optimize the synthetic features. When decompressing in \ding{182}, simply feed the local model with the received synthetic features and use the generated gradients to update the global model.}
    \label{fig:3sfc-arch}
\end{figure*}