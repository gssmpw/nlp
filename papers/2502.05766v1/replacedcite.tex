\section{Related works}
\subsection{Knowledge distillation}
% This approach involves extracting latent knowledge from a teacher network to guide the learning process of a student network.
%, has emerged as an important technique for model compress and transfer learning. 
%Cross-modal knowledge distillation constitutes a subfield within the broader paradigm of knowledge distillation, which 
Knowledge distillation (KD) has emerged as a crucial technique in model compression and transfer learning.
Cross-modal knowledge distillation (CMKD), an extension of KD, focuses on transferring knowledge between different modalities____.
% This area has been extensively studied, with applications ranging from
% transferring knowledge between different views of images____, from videos to sound____, and from audio and images to video____.
% , is extensively studied, such as from one view to another view of images____, from videos to sound____, from audio and images to video____ and so on. 
Many studies have explored cross-modal distillation for audio-visual speech data, leveraging the inherent synergy and correspondence between audio signals and lip movements. 
% Wei et al.____ propose a cross-modal distillation method from an ASR model to an AVSR model. 
Afouras et al.____ train a lipreading model by distillation from an ASR model, combining connectionist temporal classification with frame-wise cross entropy loss. 
LIBS____ enhances lipreading by distilling multi-scale knowledge from speech recognizers. 
%Ren et al.____ propose distilling cross-modal advanced knowledge for lipreading using a master model. 
% Those works are based on supervised learning, while another line of work employs cross-modal knowledge distillation under the framework of self-supervised learning. 
Another line of research employs CMKD within a self-supervised learning framework. 
% LiRA____ adopts PHASE+____ based features for learning visual speech representations. 
RAVEn____ utilizes an asymmetric distillation scheme for audio-visual representation learning with modality-specific student. 
Both AV-data2vec____ and our previously proposed AV2vec____ adopt a multimodal teacher for distilling a student that accepts audio, visual, and audio-visual inputs.
%Compared to previous studies focusing solely on lipreading, our work addresses general audio-visual representation learning. Compared to other self-supervised methods, our model leverages speech foundation models trained on vast audio corpus, enabling richer knowledge transfer beyond audio-visual data.

%Our study is also related to researches that exploit an ensemble of features or models for KD.
%An ensemble knowledge distillation method____ is proposed to distill from large self-supervised speech models.
%Recent studies____ also propose a dynamic ensemble teacher-student distillation framework for fake audio detection.
%Our work employs an aggregation of multi-layers representations from multi-teachers to boost the performance of KD.

Our work is also related to research exploring ensemble-based approaches in knowledge distillation. 
%Recent studies have demonstrated the effectiveness of ensemble methods in various speech-related tasks. 
For instance, Huang et al.____ propose an ensemble knowledge distillation framework to transfer knowledge from large self-supervised speech models. 
Similarly, recent studies____ develop a dynamic ensemble teacher-student architecture for fake audio detection. 
Building upon these works, our approach leverages aggregated representations from multiple teacher networks across different layers to enhance knowledge distillation.

 


%Compared to other self-supervised learning methods, our model enables the transfer of rich knowledge from speech foundation models that absorb from a much larger audio corpus beyond audio-visual data.

\subsection{Visual and audio-visual speech recognition}

% Early works follows a two-step paradigm for visual and audio-visual speech recognition____, including a feature extraction step and classification step. 
Nowadays, end-to-end frameworks have become the predominant approach for
%most works adopt an end-to-end framework for 
visual and audio-visual speech recognition____, where a single network consumes raw lip video pixels and audio features to generate text outputs. The end-to-end paradigm relies on sequence-to-sequence model architectures and their corresponding loss function, including encoder-decoder with attention (ED)____, connectionist temporal classification (CTC)____, and neural transducer____. 
Under this framework, researchers elaborate on designing neural network architecture for extracting visual representations, such as
 temporal convolutional network (TCN)____, Transformer____, visual Transformer pooling____, and Conformer____.
Concurrently, another line of research explores effective audio-visual fusion strategy for AVSR, such as V-CAFE____, and unified cross-modal attention____. 

%gated-based fusion____, 
%attention-based fusion____, 



Recently, there has been a growing trend in leveraging large-scale ASR models for visual and audio-visual speech recognition. 
Djilali et al.____ pioneered the reprogramming of ASR models for VSR by introducing a prior network that transforms visual representations into audio representations. 
Building on a similar concept, Prajwal et al.____ proposed a VSR approach with a simplified training objective. 
In the AVSR domain, Simic et al.____ enhanced the Whisper model, originally designed for ASR, by incorporating a fusion module for audio and visual inputs. 
Similarly, Whisper-flamingo____ extended the Whisper model's capability through additional cross-attention components that process visual representations for AVSR tasks. 
More recently, the integration of large language models with pretrained audio and visual encoders has achieved state-of-the-art performance in AVSR tasks____. 
Our work distinguishes itself by learning general audio-visual representations applicable beyond conventional lipreading and AVSR tasks. 
Moreover, our approach can be seamlessly integrated with these existing techniques to further enhance 
visual and audio-visual speech recognition performance in future work.

%Recently, a predominant trend emerges that utilizing
%large-scale ASR models for visual and audio-visual speech recognition.
%Djilali et al.____  proposed to reprogramming ASR model for VSR by
%using a prior network to convert visual representations to audio representations.
%Prajwal et al.'s research____ on VSR share the similar idea with this work with a simplified training objective.
%For AVSR, Simic et al.____. proposed to tackle AVSR task by insert a module that fusing  audio and visual inputs
%to a whisper model, which is originally trained for ASR. Whisper-flamingo____ also proposed to
%modify the ASR model using additional component that consuming visual representations for AVSR. Recently, large lanaguage models
%with pretrained audio and visual encoders are employed for AVSR task____ and achieves state-of-the-art performance.
%Our work distinguishes itself by learning general audio-visual representations that can be applied to tasks beyond lipreading and AVSR.
%Furthermore, our method can be potentially integrated with those techniques for further boosting performance on the visual and audio-visual speech recognition in future work.

 
\subsection{Speech foundation models}

Foundation models, a term first coined by Bommasani et al.____, are defined as large models trained on broad data that adapts to a wide range of downstream tasks. Various speech foundation models have emerged, broadly categorized into two types, i.e. supervised learning-based and self-supervised learning (SSL)-based models.
Among SSL-based speech foundation models, 
% wav2vec 2.0____ leverages a contrastive loss as pretext task, encouraging the Transformer encoder to discriminate  between  ground truth quantized vectors from a set of distractors. 
HuBERT____  utilizes k-means clustering to generate pseudo-class labels for model training using a mask prediction loss,
with iterative refinement through subsequent clustering and mask prediction steps.
% And the model is refined by further iteration of cluster and mask prediction step. 
WavLM____ improves HuBERT by using a more diverse pretraining dataset and performs speech denoising modeling during pretraining.
%, significantly enhancing the robustness of the learnt features. 
On the other hand, Whisper family____ exemplifies  supervised learning-based speech foundation model. Trained on large-scale multilingual and multitask labeled data, it demonstrates robust performance in ASR and speech translation tasks. 
In our experiments, we evaluated both a self-supervised speech foundation model, WavLM, and a supervised one, iFLYTEK-speech.
%Recent work also proved the cross tasks transferability of whisper-based representations [On the Transferability of Whisper-based Representations for “In-the-Wild” Cross-Task Downstream Speech Applications]. 
%In this study, two speech foundation models are selected including a self-supervised model, i.e., WavLM____, and a in-house supervised model developed by iFLYTEK company, i.e., iFLYTEK-speech.
% We notices that speech foundation model is usually trained with much larger audio data than that for most audio-visual model, because audio-visual data is more difficult to collect. Therefore, leveraging speech foundation models to distill audio-visual representation learning is promising especially in scenarios with limited paired audio-visual data. Our work adopts two speech foundation models, including WavLM and a supervised learning based model Iflytek-ASR trained by ifLYTEK company.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure1.pdf}
    \caption{Illustration of  overall scheme of our proposed method during the pretraining and finetuning phases.}
    \label{fig:figure1}
\end{figure}