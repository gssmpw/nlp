% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{andriushchenko2024agentharm,
  title={{AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents}},
  author={Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and others},
  journal={arXiv preprint arXiv:2410.09024},
  year={2024}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@misc{MosaicML2023Introducing,
    author    = {MosaicML NLP Team},
    title     = {Introducing {MPT-7B}: A New Standard for Open-Source, Commercially Usable {LLM}s},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-04-28}
    }% change this date
    urldate   = {2023-03-28} % change this date
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@misc{krippendorff11,
    author = "Klaus Krippendorff",
    title = "Computing {K}rippendorff's Alpha-Reliability",
    year = "2011",
    note = "University of Pennsylvania
Scholarly Commons, \url{https://repository.upenn.edu/asc\_papers/43}"
}

@inproceedings{lin-etal-2022-gendered,
    title = "Gendered Mental Health Stigma in Masked Language Models",
    author = "Lin, Inna  and
      Njoo, Lucille  and
      Field, Anjalie  and
      Sharma, Ashish  and
      Reinecke, Katharina  and
      Althoff, Tim  and
      Tsvetkov, Yulia",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.139",
    doi = "10.18653/v1/2022.emnlp-main.139",
    pages = "2152--2170",
    abstract = "Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental health stigma by developing a framework grounded in psychology research: we use clinical psychology literature to curate prompts, then evaluate the models{'} propensity to generate gendered words. We find that masked language models capture societal stigma about gender in mental health: models are consistently more likely to predict female subjects than male in sentences about having a mental health condition (32{\%} vs. 19{\%}), and this disparity is exacerbated for sentences that indicate treatment-seeking behavior. Furthermore, we find that different models capture dimensions of stigma differently for men and women, associating stereotypes like anger, blame, and pity more with women with mental health conditions than with men. In showing the complex nuances of models{'} gendered mental health stigma, we demonstrate that context and overlapping dimensions of identity are important considerations when assessing computational models{'} social biases.",
}
@inproceedings{field-etal-2021-survey,
    title = "A Survey of Race, Racism, and Anti-Racism in {NLP}",
    author = "Field, Anjalie  and
      Blodgett, Su Lin  and
      Waseem, Zeerak  and
      Tsvetkov, Yulia",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.149",
    doi = "10.18653/v1/2021.acl-long.149",
    pages = "1905--1925",
    abstract = "Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",
}

@inproceedings{gupta-etal-2024-sociodemographic,
    title = "Sociodemographic Bias in Language Models: A Survey and Forward Path",
    author = "Gupta, Vipul  and
      Narayanan Venkit, Pranav  and
      Wilson, Shomir  and
      Passonneau, Rebecca",
    editor = "Fale{\'n}ska, Agnieszka  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta  and
      Goldfarb-Tarrant, Seraphina  and
      Nozza, Debora",
    booktitle = "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.gebnlp-1.19",
    doi = "10.18653/v1/2024.gebnlp-1.19",
    pages = "295--322",
    abstract = "Sociodemographic bias in language models (LMs) has the potential for harm when deployed in real-world settings. This paper presents a comprehensive survey of the past decade of research on sociodemographic bias in LMs, organized into a typology that facilitates examining the different aims: types of bias, quantifying bias, and debiasing techniques. We track the evolution of the latter two questions, then identify current trends and their limitations, as well as emerging techniques. To guide future research towards more effective and reliable solutions, and to help authors situate their work within this broad landscape, we conclude with a checklist of open questions.",
}

@article{deshpande2023toxicity,
  title={{Toxicity in ChatGPT: Analyzing persona-assigned language models}},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335},
  year={2023}
}
@inproceedings{ousidhoum-etal-2021-probing,
    title = "Probing Toxic Content in Large Pre-Trained Language Models",
    author = "Ousidhoum, Nedjma  and
      Zhao, Xinran  and
      Fang, Tianqing  and
      Song, Yangqiu  and
      Yeung, Dit-Yan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.329",
    doi = "10.18653/v1/2021.acl-long.329",
    pages = "4262--4274",
    abstract = "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.",
}

@inproceedings{ocampo-etal-2023-playing,
    title = "Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection",
    author = "Ocampo, Nicol{\'a}s Benjam{\'\i}n  and
      Cabrio, Elena  and
      Villata, Serena",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.173",
    doi = "10.18653/v1/2023.findings-acl.173",
    pages = "2758--2772",
    abstract = "Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages in complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a {``}build it, break it, fix it{''}, training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models{'} performances on implicit HS benchmarks.",
}

@inproceedings{wen-etal-2023-unveiling,
    title = "Unveiling the Implicit Toxicity in Large Language Models",
    author = "Wen, Jiaxin  and
      Ke, Pei  and
      Sun, Hao  and
      Zhang, Zhexin  and
      Li, Chengfei  and
      Bai, Jinfeng  and
      Huang, Minlie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.84",
    doi = "10.18653/v1/2023.emnlp-main.84",
    pages = "1322--1338",
    abstract = "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04{\%} on BAD and 62.85{\%} on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language.",
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@inproceedings{brown2022does,
  title={What does it mean for a language model to preserve privacy?},
  author={Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  booktitle={Proceedings of the 2022 ACM conference on fairness, accountability, and transparency},
  pages={2280--2292},
  year={2022}
}

@article{yukhymenko2024synthetic,
  title={A Synthetic Dataset for Personal Attribute Inference},
  author={Yukhymenko, Hanna and Staab, Robin and Vero, Mark and Vechev, Martin},
  journal={arXiv preprint arXiv:2406.07217},
  year={2024}
}

@article{kim2024propile,
  title={Propile: Probing privacy leakage in large language models},
  author={Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2024llm,
  title={Llm-pbe: Assessing data privacy in large language models},
  author={Li, Qinbin and Hong, Junyuan and Xie, Chulin and Tan, Jeffrey and Xin, Rachel and Hou, Junyi and Yin, Xavier and Wang, Zhun and Hendrycks, Dan and Wang, Zhangyang and others},
  journal={arXiv preprint arXiv:2408.12787},
  year={2024}
}

@article{ziems2024can,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={237--291},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{he2024if,
  title={{If in a Crowdsourced Data Annotation Pipeline, a GPT-4}},
  author={He, Zeyu and Huang, Chieh-Yang and Ding, Chien-Kuang Cornelia and Rohatgi, Shaurya and Huang, Ting-Hao Kenneth},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--25},
  year={2024}
}

@article{gilardi2023chatgpt,
  title={{ChatGPT outperforms crowd workers for text-annotation tasks}},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={30},
  pages={e2305016120},
  year={2023},
  publisher={National Acad Sciences}
}


@article{zhu2023can,
  title={{Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks}},
  author={Zhu, Yiming and Zhang, Peixian and Haq, Ehsan-Ul and Hui, Pan and Tyson, Gareth},
  journal={arXiv preprint arXiv:2304.10145},
  year={2023}
}


@inproceedings{korre-etal-2023-harmful,
    title = "Harmful Language Datasets: An Assessment of Robustness",
    author = "Korre, Katerina  and
      Pavlopoulos, John  and
      Sorensen, Jeffrey  and
      Laugier, L{\'e}o  and
      Androutsopoulos, Ion  and
      Dixon, Lucas  and
      Barr{\'o}n-cede{\~n}o, Alberto",
    editor = {Chung, Yi-ling  and
      R{{\textbackslash}"ottger}, Paul  and
      Nozza, Debora  and
      Talat, Zeerak  and
      Mostafazadeh Davani, Aida},
    booktitle = "The 7th Workshop on Online Abuse and Harms (WOAH)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.woah-1.24",
    doi = "10.18653/v1/2023.woah-1.24",
    pages = "221--230",
    abstract = "The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.",
}

@inproceedings{dev-etal-2022-measures,
    title = "On Measures of Biases and Harms in {NLP}",
    author = "Dev, Sunipa  and
      Sheng, Emily  and
      Zhao, Jieyu  and
      Amstutz, Aubrie  and
      Sun, Jiao  and
      Hou, Yu  and
      Sanseverino, Mattie  and
      Kim, Jiin  and
      Nishi, Akihiro  and
      Peng, Nanyun  and
      Chang, Kai-Wei",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-aacl.24",
    pages = "246--267",
    abstract = "Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP{---}both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications{---}can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.",
}

@inproceedings{banko-etal-2020-unified,
    title = "A Unified Taxonomy of Harmful Content",
    author = "Banko, Michele  and
      MacKeen, Brendon  and
      Ray, Laurie",
    editor = "Akiwowo, Seyi  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Fourth Workshop on Online Abuse and Harms",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.alw-1.16",
    doi = "10.18653/v1/2020.alw-1.16",
    pages = "125--137",
    abstract = "The ability to recognize harmful content within online communities has come into focus for researchers, engineers and policy makers seeking to protect users from abuse. While the number of datasets aiming to capture forms of abuse has grown in recent years, the community has not standardized around how various harmful behaviors are defined, creating challenges for reliable moderation, modeling and evaluation. As a step towards attaining shared understanding of how online abuse may be modeled, we synthesize the most common types of abuse described by industry, policy, community and health experts into a unified typology of harmful content, with detailed criteria and exceptions for each type of abuse.",
}


@article{mazeika2024harmbench,
  title={{HarmBench: A standardized evaluation framework for automated red teaming and robust refusal}},
  author={Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and others},
  journal={arXiv preprint arXiv:2402.04249},
  year={2024}
}

@misc{openai2024gpt4technicalreport,
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
}

@article{anthropic2024claude,
  title={Claude 3.5 sonnet model card addendum},
  author={Anthropic, AI},
  journal={Claude-3.5 Model Card},
  year={2024}
}

@article{dubey2024llama,
  title={{The Llama 3 herd of models}},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{webber2010similarity,
  title={A similarity measure for indefinite rankings},
  author={Webber, William and Moffat, Alistair and Zobel, Justin},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={28},
  number={4},
  pages={1--38},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@inproceedings{pavlovic-poesio-2024-effectiveness,
    title = "The Effectiveness of {LLM}s as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation",
    author = "Pavlovic, Maja  and
      Poesio, Massimo",
    editor = "Abercrombie, Gavin  and
      Basile, Valerio  and
      Bernadi, Davide  and
      Dudy, Shiran  and
      Frenda, Simona  and
      Havens, Lucy  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.nlperspectives-1.11",
    pages = "100--110",
    abstract = "Recent studies focus on exploring the capability of Large Language Models (LLMs) for data annotation. Our work, firstly, offers a comparative overview of twelve such studies that investigate labelling with LLMs, particularly focusing on classification tasks. Secondly, we present an empirical analysis that examines the degree of alignment between the opinion distributions returned by GPT and those provided by human annotators across four subjective datasets. Our analysis supports a minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.",
}
@article{zhang2023llmaaa,
  title={{LLMaAA: Making Large Language Models as Active Annotators}},
  author={Zhang, Ruoyu and Li, Yanzeng and Ma, Yongliang and Zhou, Ming and Zou, Lei},
  journal={arXiv preprint arXiv:2310.19596},
  year={2023}
}

@article{tan2024large,
  title={Large language models for data annotation: A survey},
  author={Tan, Zhen and Li, Dawei and Wang, Song and Beigi, Alimohammad and Jiang, Bohan and Bhattacharjee, Amrita and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
  journal={arXiv preprint arXiv:2402.13446},
  year={2024}
}


@article{kim2024meganno+,
  title={{MEGAnno+: A Human-LLM Collaborative Annotation System}},
  author={Kim, Hannah and Mitra, Kushan and Chen, Rafael Li and Rahman, Sajjadur and Zhang, Dan},
  journal={arXiv preprint arXiv:2402.18050},
  year={2024}
}


@article{he2023annollm,
  title={Annollm: Making large language models to be better crowdsourced annotators},
  author={He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, Alex and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2303.16854},
  year={2023}
}

@article{zheng2023judging,
  title={{Judging LLM-as-a-Judge with MT-Bench and Chatbot- Arena}},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}
@article{lin2024wildbench,
  title={{WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild}},
  author={Lin, Bill Yuchen and Deng, Yuntian and Chandu, Khyathi and Brahman, Faeze and Ravichander, Abhilasha and Pyatkin, Valentina and Dziri, Nouha and Bras, Ronan Le and Choi, Yejin},
  journal={arXiv preprint arXiv:2406.04770},
  year={2024}
}

@article{dubois2024length,
  title={{Length-controlled AlpacaEval: A simple way to debias automatic evaluators}},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}
@article{shakhadri2024shakti,
  title={{SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments}},
  author={Shakhadri, Syed Abdul Gaffar and KR, Kruthika and Aralimatti, Rakshit},
  journal={arXiv preprint arXiv:2410.11331},
  year={2024}
}

@article{xu2024device,
  title={On-device language models: A comprehensive review},
  author={Xu, Jiajun and Li, Zhiyuan and Chen, Wei and Wang, Qun and Gao, Xin and Cai, Qi and Ling, Ziyuan},
  journal={arXiv preprint arXiv:2409.00088},
  year={2024}
}

@article{schwartz2020green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}


@inproceedings{davidson2017automated,
  title={Automated hate speech detection and the problem of offensive language},
  author={Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={11.1},
  pages={512--515},
  year={2017}
}
@inproceedings{basile2019semeval,
  title={Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter},
  author={Basile, Valerio and Bosco, Cristina and Fersini, Elisabetta and Nozza, Debora and Patti, Viviana and Pardo, Francisco Manuel Rangel and Rosso, Paolo and Sanguinetti, Manuela},
  booktitle={Proceedings of the 13th international workshop on semantic evaluation},
  pages={54--63},
  year={2019}
}
@article{jahan2023systematic,
  title={A systematic review of hate speech automatic detection using natural language processing},
  author={Jahan, Md Saroar and Oussalah, Mourad},
  journal={Neurocomputing},
  volume={546},
  pages={126232},
  year={2023},
  publisher={Elsevier}
}
@article{john2000hate,
  title={Hate speech},
  author={John, T Nockleby},
  journal={Encyclopedia of the American Constitution (2nd ed., edited by Leonard W. Levy, Kenneth L. Karst et al., New York: Macmillan, 2000)},
  pages={1277--1279},
  year={2000}
}

@article{wang2024detoxifying,
  title={Detoxifying Large Language Models via Knowledge Editing},
  author={Wang, Mengru and Zhang, Ningyu and Xu, Ziwen and Xi, Zekun and Deng, Shumin and Yao, Yunzhi and Zhang, Qishen and Yang, Linyi and Wang, Jindong and Chen, Huajun},
  journal={arXiv preprint arXiv:2403.14472},
  year={2024}
}
@article{niu2024parameter,
  title={Parameter-Efficient Detoxification with Contrastive Decoding},
  author={Niu, Tong and Xiong, Caiming and Yavuz, Semih and Zhou, Yingbo},
  journal={arXiv preprint arXiv:2401.06947},
  year={2024}
}
@inproceedings{caselli-etal-2021-hatebert,
    title = "{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish",
    author = "Caselli, Tommaso  and
      Basile, Valerio  and
      Mitrovi{\'c}, Jelena  and
      Granitzer, Michael",
    editor = "Mostafazadeh Davani, Aida  and
      Kiela, Douwe  and
      Lambert, Mathias  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.3/",
    doi = "10.18653/v1/2021.woah-1.3",
    pages = "17--25",
    abstract = "We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena."
}

@inproceedings{lees2022new,
  title={A new generation of perspective api: Efficient multilingual character-level transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  booktitle={Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining},
  pages={3197--3207},
  year={2022}
}

@inproceedings{khondaker2024detoxllm,
  title={{DetoxLLM: A Framework for Detoxification with Explanations}},
  author={Khondaker, Md Tawkat Islam and Abdul-Mageed, Muhammad and Lakshmanan, Laks},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={19112--19139},
  year={2024}
}
@article{ko2024large,
  title={Large Language Models can be Strong Self-Detoxifiers},
  author={Ko, Ching-Yun and Chen, Pin-Yu and Das, Payel and Mroueh, Youssef and Dan, Soham and Kollias, Georgios and Chaudhury, Subhajit and Pedapati, Tejaswini and Daniel, Luca},
  journal={arXiv preprint arXiv:2410.03818},
  year={2024}
}

@article{guo2024hey,
  title={{Hey GPT, Can You be More Racist? Analysis from Crowdsourced Attempts to Elicit Biased Content from Generative AI}},
  author={Guo, Hangzhi and Venkit, Pranav Narayanan and Jang, Eunchae and Srinath, Mukund and Zhang, Wenbo and Mingole, Bonam and Gupta, Vipul and Varshney, Kush R and Sundar, S Shyam and Yadav, Amulya},
  journal={arXiv preprint arXiv:2410.15467},
  year={2024}
}

@inproceedings{kumar-etal-2023-language,
    title = "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
    author = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Njoo, Lucille  and
      Anastasopoulos, Antonios  and
      Tsvetkov, Yulia",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.241/",
    doi = "10.18653/v1/2023.eacl-main.241",
    pages = "3299--3321",
    abstract = "Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works' taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different strategies' motivations, their limitations, and open problems for future research."
}

@article{li2024precision,
  title={Precision Knowledge Editing: Enhancing Safety in Large Language Models},
  author={Li, Xuying and Li, Zhuo and Kosuga, Yuji and Yoshida, Yasuhiro and Bian, Victor},
  journal={arXiv preprint arXiv:2410.03772},
  year={2024}
}
