\section{Related Work}
Alongside a discussion of what is meant by LLM harmfulness,
this section covers two distinct strands of related work: measuring types of harm in LLMs, and LLMs for diverse annotation tasks. %First,

%Different kinds of 
Diverse undesirable LLM outputs, from toxic language to privacy invasion, have been discussed in the observed \cite{banko-etal-2020-unified}. Here we review the ones we include in our definition of ``harm.'' %definition. Plus, we review LLMs as judges. 
Toxic content can be elicited from both generative  \cite{deshpande2023toxicity} and masked LLMs \cite{ousidhoum-etal-2021-probing}. 
%Among ways 
To measure toxic or hateful language, some use APIs such as PerspectiveAPI \cite{lees2022new} or HateBERT \cite{caselli-etal-2021-hatebert}. \citet{openai2024gpt4technicalreport} report that GPT4 produces toxic content 0.78\% of the time, versus 6.48\% in GPT3.5.
%as opposed to GPT3.5 with 6.48\%. On the other hand,
\citet{dubey2024llama} report that llama3-70B produces harmful content 5\% of the time, %whereas the 405B model generates harm 3\% of the time. 
compared to 3\% in the 405B model.
Instead of %single value classifiers to measure harm, 
reporting an absolute rate, we focus on relative harmfulness of different LLMs. %, so we point to recent work on LLMs for annotation.

The first category of harm we consider is social stereotyping and bias. %discrimination. It has been shown that 
LLMs can perpetuate social bias based on gender, race, religion etc. \cite{lin-etal-2022-gendered,bender2021dangers,field-etal-2021-survey,gupta-etal-2024-sociodemographic,andriushchenko2024agentharm,mazeika2024harmbench}. This can marginalize these groups more, and results in less fair model performance. \citet{guo2024hey} designed a competition to elicit biased output from LLMs to assess the perception of bias from non-expert users. %The first part of our work is similar to this analysis, but 
We also intentionally elicit harmful output, going %we look at other types of harms besides bias.
beyond social bias.

%When the models become stronger, they become more robust to jailbreaking attacks to elicit harmful content. However, there are datasets that can still jailbreak models to produce harmful content \cite{andriushchenko2024agentharm,mazeika2024harmbench}.

Our second category of harm is offensiveness and toxicity, which %. As opposed to stereotyping or social discrimination, this harm 
%is more subjective and harder to define than the previous category, so there 
lacks an established definition due to its greater subjectivity \cite{dev-etal-2022-measures,korre-etal-2023-harmful}. We include hate speech (HS) and abusive language as toxic content. HS can be defined as expressions of offensive and discriminatory discourse towards a group or an individual based on characteristics such as race, religion, nationality, or other group characteristics \cite{john2000hate,jahan2023systematic,basile2019semeval,davidson2017automated}. It includes racism, negative stereotyping, and sexist language. On the other hand, abusive language is content with inappropriate words such as profanity or disrespectful terms. It also includes psychological threats such as humiliation. %or constant criticism. %Toxic content can be elicited from both generative models \cite{deshpande2023toxicity} and masked language models \cite{ousidhoum-etal-2021-probing}.

%In addition to obvious toxic content, LLMs can generate diverse implicit toxic outputs using reinforcement learning with favoring toxic content in the reward function \cite{wen-etal-2023-unveiling}.  Regarding the subjectivity of this task, \cite{korre-etal-2023-harmful} reannotate the existing datasets with different definitions of toxicity and show that broader definitions result in more robust annotations, but interannotator agreements are still lower than 0.5. \cite{dev-etal-2022-measures} also point out the lack of definition for bias and harm in general and propose a framework to guide researchers during the development of bias measures.

Harm can be implicit, such as privacy invasion
%We are also interested in privacy invasion,
where there is 
leakage of personal information. %leakage from the model. 
%LLMs can memorize details of the training data and then leak private information such as 
This includes social security numbers, phone numbers, or bank account information \cite{carlini2021extracting,brown2022does}. 
%There are several frameworks to test the privacy of LLMs \cite{li2024llm} and generate data for personal attribute inference \cite{yukhymenko2024synthetic,kim2024propile}.

%Our definition of harm includes hate speech (HS) as well. HS can be defined as \textcolor{red}{expressions of} hatred towards a social group, the humiliation of the members of a group, or %communication disparaging  extreme disparagement of a person or a group based on race, color, ethnicity, gender, sexual orientation, nationality, religion, or other group characteristics .

For data annotation, LLMs
%Besides text generation, 
%LLMs have been used to annotate data because they 
can %be comparable to 
replace humans for some tasks, %and make the annotation process faster and cheaper 
with gains in efficiency and economy \cite{tan2024large}. They have been used for sociological annotations such as for classification of stance, bots or humor  \cite{ziems2024can,zhu2023can}. For tasks such as topic and frame detection or sentence segmentation they can surpass crowd-workers
%Some works show that they can surpass crowd-workers for some tasks such as topic and frame detection or sentence segmentation %into research aspects 
\cite{he2024if,gilardi2023chatgpt}. Some have argued that human-LLM collaboration results in more reliable annotation \cite{he2024if,zhang2023llmaaa,kim2024meganno+}. In addition to more objective tasks,
%LLMs have been used to annotate data %even 
they have been applied to subjective annotations such as offensiveness and abusiveness \cite{pavlovic-poesio-2024-effectiveness,zhu2023can,he2023annollm}, %. For example, LLMs are used as judges to rank responses from different LLMs 
or to rank outputs from different LLMs based on helpfulness, accuracy, or relevance \cite{zheng2023judging,lin2024wildbench,dubois2024length}. These works tend to focus on human-large LLM interactions, whereas we focus on single-turn responses from smaller LLMs. We inspire from \citet{zheng2023judging} but we only measure harm instead of overall performance. Plus, we use 3 LLMs to evaluate smaller LLMs.