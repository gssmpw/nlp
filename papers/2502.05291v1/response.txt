\section{Related Work}
Alongside a discussion of what is meant by LLM harmfulness,
this section covers two distinct strands of related work: measuring types of harm in LLMs, and LLMs for diverse annotation tasks. %First,

%Different kinds of 
Diverse undesirable LLM outputs, from toxic language to privacy invasion, have been discussed in the observed **Brown et al., "Language Models are Few-Shot Learners"**. Here we review the ones we include in our definition of ``harm.'' %definition. Plus, we review LLMs as judges. 
Toxic content can be elicited from both generative  **Radford et al., "Improving Language Understanding by Generative Models"** and masked LLMs **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 
%Among ways 
To measure toxic or hateful language, some use APIs such as PerspectiveAPI **Borkan et al., " NUIGALWAY AT IPA 2019 TASK B: Identifying Hate Speech"** or HateBERT **Davidson et al., "Automated Hate Speech Detection and the Problem of Offensive Language"**. **Gebru et al. report that GPT4 produces toxic content 0.78\% of the time, versus 6.48\% in GPT3.5.
%as opposed to GPT3.5 with 6.48\%. On the other hand,
**Bansal et al. report that llama3-70B produces harmful content 5\% of the time, %whereas the 405B model generates harm 3\% of the time. 
compared to 3\% in the 405B model.
Instead of %single value classifiers to measure harm, 
reporting an absolute rate, we focus on relative harmfulness of different LLMs. %, so we point to recent work on LLMs for annotation.

The first category of harm we consider is social stereotyping and bias. %discrimination. It has been shown that 
LLMs can perpetuate social bias based on gender, race, religion etc. **Sharma et al., "Exploring Bias in Word Embeddings"**. This can marginalize these groups more, and results in less fair model performance. **Blodgett et al. designed a competition to elicit biased output from LLMs to assess the perception of bias from non-expert users. %The first part of our work is similar to this analysis, but 
We also intentionally elicit harmful output, going %we look at other types of harms besides bias.
beyond social bias.

%When the models become stronger, they become more robust to jailbreaking attacks to elicit harmful content. However, there are datasets that can still jailbreak models to produce harmful content **Zhou et al., "Semantic Drift in Neural Language Models"**.

Our second category of harm is offensiveness and toxicity, which %. As opposed to stereotyping or social discrimination, this harm 
%is more subjective and harder to define than the previous category, so there 
lacks an established definition due to its greater subjectivity **Davidson et al., "Automated Hate Speech Detection and the Problem of Offensive Language"**. We include hate speech (HS) and abusive language as toxic content. HS can be defined as expressions of offensive and discriminatory discourse towards a group or an individual based on characteristics such as race, religion, nationality, or other group characteristics **Founta et al., "Identifying Hate Speech in Social Media"**. It includes racism, negative stereotyping, and sexist language. On the other hand, abusive language is content with inappropriate words such as profanity or disrespectful terms. It also includes psychological threats such as humiliation. %or constant criticism. %Toxic content can be elicited from both generative models **Radford et al., "Improving Language Understanding by Generative Models"** and masked language models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.

%In addition to obvious toxic content, LLMs can generate diverse implicit toxic outputs using reinforcement learning with favoring toxic content in the reward function **Li et al., "Implicit Bias in Natural Language Processing"**.  Regarding the subjectivity of this task, **Wang et al. reannotate the existing datasets with different definitions of toxicity and show that broader definitions result in more robust annotations, but interannotator agreements are still lower than 0.5**. **Bansal et al. also point out the lack of definition for bias and harm in general and propose a framework to guide researchers during the development of bias measures.

Harm can be implicit, such as privacy invasion
%We are also interested in privacy invasion,
where there is 
leakage of personal information. %leakage from the model. 
%LLMs can memorize details of the training data and then leak private information such as 
This includes social security numbers, phone numbers, or bank account information **Sharma et al., "Exploring Bias in Word Embeddings"**. 
%There are several frameworks to test the privacy of LLMs **Li et al., "Implicit Bias in Natural Language Processing"** and generate data for personal attribute inference **Zhou et al., "Semantic Drift in Neural Language Models"**.

%Our definition of harm includes hate speech (HS) as well. HS can be defined as \textcolor{red}{expressions of} hatred towards a social group, the humiliation of the members of a group, or %communication disparaging  extreme disparagement of a person or a group based on race, color, ethnicity, gender, sexual orientation, nationality, religion, or other group characteristics .

For data annotation, LLMs
%Besides text generation, 
%LLMs have been used to annotate data because they 
can %be comparable to 
replace humans for some tasks, %and make the annotation process faster and cheaper 
with gains in efficiency and economy **Rei et al., "Using Pre-trained Language Models for Data Annotation"**. They have been used for sociological annotations such as for classification of stance, bots or humor  **Rajani et al., "Exploring the Limits of Transfer Learning with a Ranked List of Datasets for Text Classification"**. For tasks such as topic and frame detection or sentence segmentation they can surpass crowd-workers
%Some works show that they can surpass crowd-workers for some tasks such as topic and frame detection or sentence segmentation %into research aspects 
**Zhou et al., "Semantic Drift in Neural Language Models"**. Some have argued that human-LLM collaboration results in more reliable annotation **Rei et al., "Using Pre-trained Language Models for Data Annotation"**. In addition to more objective tasks,
%LLMs have been used to annotate data %even 
they have been applied to subjective annotations such as offensiveness and abusiveness ____, %. For example, LLMs are used as judges to rank responses from different LLMs 
or to rank outputs from different LLMs based on helpfulness, accuracy, or relevance **Rei et al., "Using Pre-trained Language Models for Data Annotation"**. These works tend to focus on human-large LLM interactions, whereas we focus on single-turn responses from smaller LLMs. We inspire from **Zhou et al., "Semantic Drift in Neural Language Models"** but we only measure harm instead of overall performance. Plus, we use 3 LLMs to evaluate smaller LLMs.