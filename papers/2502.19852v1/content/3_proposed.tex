\section{\ours: Reproducible Feedback Environments}
\label{sec:convcodeworld}
\input{table/categorization}

In real-world settings of interactive code generation, the types and combinations of feedback can vary significantly due to factors such as the availability of feedback from code execution (e.g., error messages, output) and the expertise of the feedback provider. These variations, particularly the provider's expertise, can strongly influence the quality of the verbal feedback when it is offered.

% In real-world settings of interactive code generation, the types and combinations of feedback available can vary significantly due to diverse constraints such as the availability of execution and verbal feedback, and the expertise of the feedback provider---which affects the quality of the verbal feedback when it is provided. 

To effectively evaluate LLMs under these varying conditions, we propose \ours, a novel and reproducible environment designed to simulate a wide range of interactive code generation scenarios. 
Two key features of \ours are as follows: (a) \textbf{Encompassing Diverse Real-World Scenarios:} \ours covers nine distinct feedback combinations that occur in practical settings; (b) \textbf{Ensure the Reproducibility of Evaluation:} \ours provides a consistent and repeatable framework for assessing the performance of LLMs. 


% In the following sections, we will categorize the types of feedback (\S\ref{convcodeworld:categorization}), present the feedback combinations offered by \ours (\S\ref{convcodeworld:combination}), and explain how to ensure reproducible evaluation (\S\ref{convcodeworld:reproducibility}). 

% % \subsection{Motivation}
% The goal of \ours is to evaluate the performance of a code generation model while interacting with user in diverse real-world scenarios. 
% Two challenges to achieve this goal are: 
% \begin{itemize}
%     \item Cover diverse real-world scenarios
%     \item Ensure reproducibility
%     \item 
% \end{itemize}





\begin{comment}    
\subsection{Conversational Code Generation as a Partially Observable Markov Decision Process}
The goal of \ours is to evaluate the performance of a code generation model while interacting with user in diverse real-world scenarios. 
%We model this task as a 
Inspired by dialogue generation and evaluation~\cite{young2013pomdp}, where
partially observable markov decision process (POMDP), 
%as it effectively captures the 
%to capture uncertainty and 
was widely adopted to capture
sequential decision-making of utterances
%inherent in conversational code generation, where the agent must make decisions
based on incomplete information, 
we formalize conversational code generation process.


\sw{functionality?o and f?}
Formally, we define a POMDP as a tuple $\mathcal{M}= \langle \mathbb{S}, \mathbb{A}, T, \Omega, O, R \rangle$: 
$\mathbb{S}$ is the state space, and each state $s_t \in \mathbb{S}$ represents the functionality of the code at turn $t$.\footnote{This aligns with the partial observability aspect because the functionality is not directly measurable--it can only be inferred through observations like code execution with full test coverage or code reviews by expert programmers.} 
$\mathbb{A}$ is the action space, and each action $a_t \in \mathbb{A}$ represents the code snippet generated or refined by the agent at turn $t$. 
$T: \mathbb{S}\times\mathbb{A} \rightarrow \Delta(\mathbb{S})$ is a state transition probability function where $\Delta(\cdot)$ is a probability distribution of the set $(\cdot)$. 
$\Omega$ is the set of possible observations where $o_t \in \Omega$ is a set of feedback received from the environment at turn $t$. 
$O:\mathbb{S} \times \mathbb{A} \rightarrow \Delta(\Omega)$ is an observation function. 
$R: \mathbb{S}\times\mathbb{A} \rightarrow \mathbb{R}$ is an immediate reward function returning a real number. % signifying the proba. %, 1 if the code satisfies the desired functionality. 
% $h$ is the maximum number of turns (\textit{horizon}). 

% \begin{itemize}
%     \item $\mathbb{S}$ is the state space, and each state $s_t \in \mathbb{S}$ represents the functionality of the code at turn $t$. 
%     \item $\mathbb{A}$ is the action space, and each action $a_t \in \mathbb{A}$ represents the code snippet generated or refined by the agent at turn $t$. 
%     \item $T: \mathbb{S}\times\mathbb{A} \rightarrow \Delta(\mathbb{S})$ is a state transition probability function where $\Delta(\cdot)$ is a probability distribution of the set $(\cdot)$. 
%     \item $\Omega$ is the set of observations where $o_t \in \Omega$ is a set of feedback received from the environment at turn $t$. 
%     \item $O:\mathbb{S} \times \mathbb{A} \rightarrow \Delta(\Omega)$ is an observation function. 
%     \item $R: \mathbb{S}\times\mathbb{A} \rightarrow \mathbb{R}$ is an immediate reward function returning a real number. %, 1 if the code satisfies the desired functionality. 
%     \item $\gamma$ is a discount factor. 
% \end{itemize}



The conversational code generation task is conducted as follows. For each turn $t$, the agent receives an observation $o_{t} \in \Omega$ from the environment with the observation probability $O(s_t, a_t)$ representing $Pr(o_t|s_t,a_t)$, where $s_t \in \mathbb{S}$ and $a_t \in \mathbb{A}$. 
Then, the agent conducts an action $a_{t+1} \in \mathbb{A}$ from $o_{t}$ by a policy $\pi$, generating code from the current observation. 
The transition function $T$ then updates $s_t$ by the probability distribution $T(s_t,a_{t+1})$ representing probability $Pr(s_{t+1}|s_t,a_{t+1})$, where $s_{t+1} \in \mathbb{S}$. The iteration terminates in two conditions: (1) When the turn $t$ reaches to the horizon $h$; (2) When the agent reaches the goal state, i.e., the code's functionality matches the desired specification provided in the problem description. 
\end{comment}

\subsection{Feedback Categorization}
\label{convcodeworld:categorization}

To accurately simulate real-world feedback in interactive code generation, we focus on two critical components:
% In conversational code generation, the observations $\Omega$ correspond to different types of feedback the agent receives. 
% Each feedback encompasses two key types of information:
(a) \textbf{Fault Localization:} Identifying the specific parts of the code where issues or errors occur; (b) \textbf{Guidance for Refinement:} Offering suggestions or instructions on how to correct the identified issues.
% \begin{itemize}
%     \item \textbf{Fault Localization:} Identifying the specific parts of the code where issues or errors occur.
%     \item \textbf{Guidance for Refinement:} Offering suggestions or instructions on how to correct the identified issues.
% \end{itemize}

As means of obtaining such information,  we consider three different types of feedback: compilation feedback, execution feedback, and verbal feedback. 
\paragraph{Compilation Feedback ($f_c$)}
Originated from the compiler, this feedback identifies syntax and type-checking errors but cannot localize logical or runtime errors. 
As a result, Table~\ref{tab:categorization} marks this with $\triangle$  for partial fault localization. 
Additionally, compilation errors do not offer refinement guidance.
% This feedback Originated from the compiler and localizes syntax errors and type-checking issues. 
% However, compiler cannot localize where
% logical or runtime errors,
% because of which
% Table~\ref{tab:categorization}
% marks $\triangle$
% to denote a partial Fault Localization.
% Also compilation errors
% do not provide refinement guidance.

%While compiler can localize where syntax errors occur, it
%fails to provide such information on  
%of $f_c$ to be partial, and as 
%error messages do not
% for correction, 
%is not provided.
% Compilation feedback uses a compiler to check for syntax errors. This feedback is cost-effective as it does not require execution conditions or test cases; however, it fails to assess the code's alignment with user intent in terms of functionality. %Due to its cost-efficiency, we can expect that 

\paragraph{Execution Feedback}
Derived from code execution, this feedback includes runtime errors and test run results. Full or partial fault localization is provided, depending on test coverage (TC): 
(a) \textbf{Full TC ($f_e^*$):} Inspired by Test-Driven Development (TDD;~\citealp{beck2022test}), complete test cases allow precise fault localization, identifying where and under what conditions the code fails. This provides details on the failure's location and triggering inputs;
% When complete test cases are available, precise fault localization identifies exactly where and under what conditions the code fails. This includes detailed information on both the location of the failure and its triggering inputs.
(b) \textbf{Partial TC ($f_e$):} In more realistic settings with partial test coverage, fault localization is limited to tested code lines, potentially leaving faults in untested sections undetected. This type of feedback simulates incomplete real-world test suites, where only a subset of possible execution paths is covered. 
% In more realistic settings with partial test coverage, fault localization is limited to areas of the code that are tested, potentially leaving faults in untested sections undetected or ambiguous. This type of feedback simulates incomplete real-world test suites, where only a subset of possible execution paths is covered. 
% \begin{itemize}
%     \item \textbf{Full TC:} Fault localization can be precise, identifying exactly where and under what conditions the code fails. 
%     \item \textbf{Partial TC:} Fault localization is limited to the areas covered by the test cases, potentially missing faults in untested portions of the code.
% \end{itemize}
%Throughout the test coverage, execution feedback lacks guidance for 
Refinement guidance is not provided in either full or partial test coverage executions.


% Throughout test coverage, execution feedback lacks guidance for refinement, there is no explicit corrective suggestions. % as may be implicit in the nature of the failures but 
% often lacks explicit corrective suggestions.
% It involves running the code to identify runtime errors or to check if the output matches expected results based on given inputs. 
% To utilize execution feedback, we need an (secured) environment that all the dependencies regarding the code are prepared~\citep{codex}, and test cases of input and expected output pairs. 
% We further divide this feedback based on test coverage (TC) such as branch coverage:
% \begin{itemize}
%     \item \textbf{Partial TC:} Limited number of test cases is available, reducing the burden of preparing them. 
%     \item \textbf{Full TC:} Maximize coverage to improve feedback quality. 
% \end{itemize}

\paragraph{Verbal Feedback} Verbal feedback in our benchmark is generated by LLMs simulating human feedback, ranging from novice to expert levels. This feedback could emulate responses from humans, such as experts guiding LLMs to generate code, or novices without coding expertise. Both fault localization and refinement guidance are provided verbally, but the extent and accuracy of this feedback depend on the simulated provider: (a) \textbf{Novice-Level ($f_v$):} At this level, the LLM simulates novice feedback, which tends to rely heavily on other feedback types (e.g., compilation or execution feedback) and often restates observed errors without deeper understanding. Refinement guidance may be incorrect or absent, due to the novice's limited expertise simulated by the LLM's potential hallucinations. (b) \textbf{Expert-Level ($f_v^*$):} 
Expert feedback reflects scenarios where expert programmers use LLMs to automate simpler tasks, allowing them to concentrate on more complex coding challenges. 
This feedback is simulated by the LLM to provide detailed fault localization and code refinement guidance. 
It generates the feedback an expert programmer might give, focusing on resolving issues with a deep understanding of programming concepts and the expected functionality. %This feedback is generally precise and actionable, offering clear steps to correct errors.


% In contrast, expert feedback is simulated by the LLM to provide fault localization independently, drawing on a deeper understanding of programming concepts and desired functionality. Refinement guidance is generally accurate, offering clear, actionable steps to resolve identified issues. This feedback simulates scenarios where expert programmers might use LLMs to automate repetitive tasks and focus on more complex coding challenges.


\subsubsection{Verbal Feedback Generation}
We generate $f_v$ and $f_v^*$ by GPT-4o with in-context learning~\citep{dong2022survey}. We chose GPT-4o as we found it to be best at following instructions and minimizing risks such as ground truth code leakage, as discussed in Appendix~\ref{appendix:gt_code_leakage}. 
% Specifically, we conduct in-context learning~\citep{dong2022survey} for each $f_v$ and $f_v^*$:\footnote{In-context examples are provided in Appendix~\ref{appendix:icl_examples}.}
% To ensure reproducibility and enable extensive comparisons, we obtain verbal feedback through systematic procedures inspired by~\citet{wang2024mint} and~\citet{yao2024tau}, utilizing GPT-4o via in-context learning~\citep{dong2022survey}.\footnote{See Appendix~\ref{appendix:simulator_comparison} for comparative analysis of verbal feedback across different LLMs.}
% This approach avoids the challenges associated with inconsistent feedback quality and the impracticality of sourcing real-time human input:
\begin{itemize}[left=4pt]
    \item \textbf{Generation of $f_v$:} Novice-level verbal feedback is constructed by verbalizing outputs from compilation and/or execution feedback, possibly supplemented with language model predictions. %Fault localization relies on restating compiler or runtime errors, and guidance for refinement may be incorrect or vague.\footnote{An in-context example for novice-level verbal feedback generation is provided in Appendix~\ref{appendix:novice_icl}.}
    \item \textbf{Generation of $f_v^*$:} Expert-level verbal feedback is produced by showing the agent's code with the correct reference code~\citep{wang2024mint}, enabling a comparison and subsequent detailed feedback on required modifications. We perform extensive analysis to ensure no ground truth code is leaked during $f_v^*$ generation (see Appendix~\ref{appendix:gt_code_leakage} for analysis on this).
\end{itemize}
See Appendices~\ref{appendix:simulator_comparison} for comparative analysis of verbal feedback using different LLMs, ~\ref{appendix:icl_examples} for the in-context examples, and ~\ref{appendix:case_study} for a generated example of $f_v^*$.
{For the detailed prompting methods for the $f_v$ and $f_v^*$ construction, please refer to our codebase.\footnote{\footnotesize\href{https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld}{ \texttt{https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld}}}
% ~\footnote{\footnotesize\href{https://github.com/sfc-gh-hhan/convcodeworld}{ \texttt{https://github.com/sfc-gh-hhan/convcodeworld}}}
% \subsection{Ensuring Reproducible Evaluation}
\paragraph{Reproducibility and Cost-Efficiency Compared to Human Annotation}
\label{convcodeworld:reproducibility}
Manual annotation of verbal feedback is costly and lacks reproducibility. Instead, we use GPT-4o, as supported by prior studies demonstrating the effectiveness of LLM-generated feedback in benchmarks~\citep{wang2024mint,yao2024tau}. This approach improves reproducibility by using a consistent feedback provider and reduces annotation costs to about 1.5\% (Appendix~\ref{appendix:efficiency}) of those for human annotators. 
%\footnote{In the worst-case scenario, CodeLlama-7B-Instruct, which requested the most verbal feedback due to its low performance, incurred a total cost of \$215 (26.4M input tokens and 5.5M output tokens) for 15,905 turns using GPT-4o-2024-05-13 pricing (\$5/1M input tokens and \$15/1M output tokens). By comparison, assuming human annotation takes 96 seconds per turn~\citep{wang2024mint} and the average U.S. private non-farmer worker's hourly wage is \$35.04 according to \cite{uswage}, the human annotation cost would be approximately \$14,792. } 

% One challenge in ensuring reproducibility and enabling extensive comparisons is obtaining consistent verbal feedback for each turn of LLM-generated code. 
% Manual annotation is costly and prone to inconsistency, as feedback style can vary between annotators.
% To address this, we use GPT-4o for verbal feedback generation, leveraging previous studies that have demonstrated the efficacy of LLM-generated feedback in evaluation benchmarks~\citep{wang2024mint,yao2024tau}. Using GPT-4o reduces annotation costs to approximately 1.5\% of the cost of human annotators.\footnote{In the worst-case scenario, CodeLlama-7B-Instruct, which requested the most verbal feedback due to its low performance, incurred a total cost of \$215 (26.4M input tokens and 5.5M output tokens) for 15,905 turns using GPT-4o-2024-05-13 pricing (\$5/1M input tokens and \$15/1M output tokens). By comparison, assuming human annotation takes 96 seconds per turn~\citep{wang2024mint} and the average U.S. private non-farmer worker's hourly wage is \$35.04 according to \cite{uswage}, the human annotation cost would be approximately \$14,792. } 
% Additionally, by standardizing the feedback provider to a single model, we ensure consistent feedback across all evaluated code generation models.


% \begin{itemize}
%     \item \textbf{Novice-Level:} At this level, fault localization relies heavily on other feedback types, such as compilation or execution feedback. Novice feedback may restate observed errors without deeper insight. Guidance for refinement may be incorrect or absent due to limited expertise.\footnote{Feedback generated by LLMs, including self-corrections that might contain hallucinations, falls into this category.}
%     % At this expertise level, fault localization heavily relies on other forms of feedback, such as compilation errors or execution results. Novice-level feedback may simply echo these errors without deeper insight. Guidance for refinement may be incorrect or entirely absent, as the provider may lack the necessary knowledge to suggest effective corrections. This type of feedback often includes general expressions of dissatisfaction or confusion and may contain inaccuracies or lack specificity.\footnote{Feedback generated by LLMs, including self-corrections that might contain hallucinations, falls into this category.}
%     \item \textbf{Expert-Level:} Experts provide fault localization independently of other feedback, based on their understanding of programming concepts and desired functionality. Guidance for refinement is mostly correct and offers actionable suggestions for improvement on how to fix the identified problems.\footnote{This setting is practical as expert programmers may leverage LLMs to reduce repetitive tasks and focus on more critical coding.} 
%     % At the expert level, fault localization is provided independently of other feedback types. Experts can pinpoint issues within the code based on their understanding of programming concepts and the desired functionality. Guidance for refinement is mostly correct, offering actionable suggestions on how to fix the identified problems. This feedback provides high-level directions for necessary corrections without revealing the exact solution, ensuring that the refinement process remains a learning opportunity. 
%     % \item Expert users, who are proficient in generating code themselves, may leverage LLMs to reduce repetitive tasks and focus on more critical coding. Unlike novice users, experts not only can determine whether the generated code is correct or identify errors, but they can also provide correct guidance on how to rectify these issues. However, as providing excessively specific feedback, e.g., ground truth itself in one extreme,  it is more practical to offer high-level directions for necessary corrections. 
% \end{itemize}




% \subsection{Observations}
% To ensure reproducibility and enable extensive comparisons, we obtain verbal feedback through systematic procedures inspired by~\citet{wang2024mint} and~\citet{yao2024tau}, utilizing GPT-4o via in-context learning~\citep{dong2022survey}.\footnote{See Appendix~\ref{appendix:simulator_comparison} for comparative analysis of verbal feedback across different LLMs.}
% This approach avoids the challenges associated with inconsistent feedback quality and the impracticality of sourcing real-time human input:
% \begin{itemize}
%     \item \textbf{Novice-Level NL Feedback Generation:} This feedback is constructed by verbalizing outputs from compilation and/or execution feedback, possibly supplemented with language model predictions. Fault localization relies on restating compiler or runtime errors, and guidance for refinement may be incorrect or vague.\footnote{An in-context example for novice-level feedback generation is provided in Appendix~\ref{appendix:novice_icl}.}
%     \item \textbf{Expert-Level Verbal Feedback Generation:} This feedback is produced by showing the agent's code with the correct reference code~\citep{wang2024mint}, enabling a comparison and subsequent detailed feedback on required modifications.\footnote{An in-context example for expert-level feedback generation is provided in Appendix~\ref{appendix:expert_icl}.}
% \end{itemize}

% The human-in-the-loop setup lacks reproducibility for extensive comparisons because it involves providing immediate and consistent-quality human feedback after each code generation instance by an LLM. Moreover, this approach is cost-prohibitive.

% Instead, inspired by~\citet{wang2024mint} and~\citet{yao2024tau}, we build a reproducible environment by simulating user feedback with GPT-4o via in-context learning~\citep{dong2022survey}.\footnote{See Appendix~\ref{appendix:simulator_comparison} for comparative analysis of user feedback simulation across different LLMs. }

% \paragraph{Novice Feedback Simulation}
% We simulate novice users by verbalizing compilation and/or execution feedback and optionally including LLM-predicted information, which, like real novice feedback, can sometimes be incorrect.\footnote{The in-context example for novice feedback simulation is provided in Appendix~\ref{appendix:novice_icl}. }

% \paragraph{Expert Feedback Simulation}
% To simulate expert users, who have a clear understanding of the intended final code structure, our simulator LLM takes both the code generated by another LLM and the correct code~\citep{wang2024mint}, enabling a comparison and subsequent detailed feedback on required modifications.\footnote{The in-context example for expert feedback simulation is provided in Appendix~\ref{appendix:expert_icl}. }

% \subsection{Verbal Reinforcement Learning}


% In our POMDP framework, the agent faces partial observability because it cannot directly observe the true functionality state $s_t$ of the code at turn $t$. Instead, it must rely on observations--feedback from the environment--to update its belief state $b_t \in \mathbb{S}$, thereby the next action $a_{t+1}$ is chosen by the belief state $b_t$, i.e., $a_{t+1} = \pi(b_t)$. 



% Depending on real-world constraints, different scenarios offer varying feedback combinations, leading to different levels of the lower bound of belief accuracy:




% $\mathbb{S}$ is a state space where $s_t \in \mathbb{S}$ represents the functionality of code at turn $t$. The goal state is when the functionality of the code matches the desired specification, explained in the given problem description $x$. Formulating state as the functionality of code fits, as the functionality is not directly measurable but only through observations like code execution with full test coverage or code review by expert-level programmers, fitting the partial observability aspect.



% is not directly observable, 
% represents the underlying goal the agent is trying to achieve. Not directly observable, fitting the partial observability aspect.

% \begin{itemize}
%     \item State ($\mathbb{S}$): The true functionality of the desired code, encompassing all requirements and constraints. The rationale is that it represents the underlying goal the agent is trying to achieve. Not directly observable, fitting the partial observability aspect.
%     \item Action ($\mathbb{A}$): Code snippets generated or refined by the agent.
%     \item Observations ($\Omega$): Feedback received after submitting code, including: 
% \end{itemize}

% $\mathbb{S}$ is a state space, 
% and $\mathbb{A}$ is an action space.  
% We define a state $s \in \mathbb{S}$ as the functionality of the generated code, and $s$ is the goal state if the code has the desired functionality that $u$ requested. $a_m \in \mathbb{A}_m$ is $m$'s action of code generation or refinement. $a_u \in \mathbb{A}_u$ is an action that the user $u$ provides feedback of the current version of code. 
% $T: \mathbb{S}\times\mathbb{A} \rightarrow \Delta(\mathbb{S})$ is a state transition probability function where $\Delta(\cdot)$ is a probability distribution of the set $(\cdot)$. 
% $\Omega=\Omega_m \times \Omega_u$ is a joint set of each agent's observations $\Omega_m$ and $\Omega_u$.  
% $O:\mathbb{S} \times \mathbb{A} \rightarrow \Delta(\Omega)$ is an observation function. 
% $R: \mathbb{S}\times\mathbb{A} \rightarrow \mathbb{R}$ is an immediate reward function returning a real number. 
% $h$ is the maximum number of turns (\textit{horizon}), and $b_0 \in \Delta(\mathbb{S})$ is a distribution of initial state at turn $t=0$. 

\begin{comment}
Given a generated code and corresponding feedback through interaction, this task satisfies following properties:
\begin{itemize}
    \item \textbf{Markov Decision Process:} To generate the next version of the code, $m$ only needs to consider the current version of the code and the corresponding feedback. %The functionality (and whether it matches to the desired functionality from $u$) of the currently generated code by $m$ is solely decided by the previous version of code, feedback, and the refinement action. 
    \item \textbf{Partially Observable:} $m$ might not be able to specify the functionality of the currently generated code (and thus whether it matches to the desired functionality from $u$) due to limited or noisy information in feedback, e.g., partial test coverage in execution feedback, incorrect feedback by novice user.   %Feedback corresponding to the generated code may not fully explain the status of the code, e.g., the correctness, how far the code is from the correct code, etc. 
    % \item \textbf{Decentralized:} Heterogeneous agents--$m$ and $u$--are cooperative to return code with the desired functionality. 
\end{itemize}
Therefore, we can formulate this conversational code generation task as a decentralized partially observable markov decision process (Dec-POMDP;~\cite{oliehoek2016concise}). 

Formally, we define a Dec-POMDP by a tuple $\mathcal{M}= \langle \mathbb{D}, \mathbb{S}, \mathbb{A}, T, \Omega, O, R, h, b_0 \rangle$. 
$\mathbb{D}=\{m,u\}$ is a set of agents consists of a code generation model $m$ and a human user $u$.\footnote{For simplicity, we assume a single code generation model and user, but this can be extended to multiple models and users.} 
$\mathbb{S}$ is a state space, and $\mathbb{A}=\mathbb{A}_m \times \mathbb{A}_u$ is a joint action space of each agent's action space $\mathbb{A}_m$ and $\mathbb{A}_u$. 
We define a state $s \in \mathbb{S}$ as the functionality of the generated code, and $s$ is the goal state if the code has the desired functionality that $u$ requested. $a_m \in \mathbb{A}_m$ is $m$'s action of code generation or refinement. $a_u \in \mathbb{A}_u$ is an action that the user $u$ provides feedback of the current version of code. 
$T: \mathbb{S}\times\mathbb{A} \rightarrow \Delta(\mathbb{S})$ is a state transition probability function where $\Delta(\cdot)$ is a probability distribution of the set $(\cdot)$. 
$\Omega=\Omega_m \times \Omega_u$ is a joint set of each agent's observations $\Omega_m$ and $\Omega_u$.  
$O:\mathbb{S} \times \mathbb{A} \rightarrow \Delta(\Omega)$ is an observation function. 
$R: \mathbb{S}\times\mathbb{A} \rightarrow \mathbb{R}$ is an immediate reward function returning a real number. 
$h$ is the maximum number of turns (\textit{horizon}), and $b_0 \in \Delta(\mathbb{S})$ is a distribution of initial state at turn $t=0$. 

The conversational code generation task is conducted as follows. For each turn $t$, the code generation model $m$ receives an observation $o_{t,m} \in \Omega_m$ from the state $s_t \in \mathbb{S}$ with the observation probability $O(s_t, a_t')$ representing $Pr(o_t|s_t,a_t')$, where $o_t = (o_{t,m}, o_{t,u})\in\Omega$ and $a_t' \in \mathbb{A}$. Then, $m$ conducts an action $a_{t+1,m} \in \mathbb{A}_m$ from $o_{t,m}$ by a policy $\pi_m: \Omega_m \rightarrow \mathbb{A}_m$, generating code from the current observation. 
The transition function $T$ then updates $s_t$ by the probability distribution $T(s_t,a_{t+1})$ representing probability $Pr(s_t'|s_t,a_{t+1})$, where $s_t' \in \mathbb{S}$ is a temporal state and $a_{t+1} = (a_{t+1,m},\phi) \in \mathbb{A}$ is a joint action of the model's action $a_{t,m}$ and the user's action $\phi$, signifying no user action is taken yet. 
Then, the user $u$ receives an observation $o_{t,u}' \in \Omega_u$ with the observation probability $O(s_t', a_{t+1})$ representing $Pr(o_t'|a_{t+1}, s_t')$, where $o_t'=(o_{t,m}', o_{t,u}')\in \Omega$. Next, $u$ takes an action $a_{t+1,u} \in \mathbb{A}_u$ from $o_{t,u}'$ by a policy $\pi_u:\Omega_u \rightarrow \mathbb{A}_u$, providing user feedback to $m$. Finally, the transition function $T$ updates $s_t'$ into $s_{t+1}$ by $T(s_t', a_{t+1}')$ representing $Pr(s_{t+1}|s_t', a_{t+1}')$, where $a_{t+1}'=(\phi,a_{t+1,u}') \in \mathbb{A}$. 
% The initial condition 

%$o_t,m= \langle x, a_{t,m} \rangle$

As $m$ cannot directly fetch the state from the environment, it has a \textit{belief}
\begin{align}
\label{eq:belief}
b_m(s_t)\delequal Pr(s_t|o_{t,m}, a_{t-1,m}, o_{t-1,m},..., o_{1,m}, a_{0,m}),
\end{align}
for each $s_t \in \mathbb{S}$ to estimate the current state. 
Though the accuracy of $b_m$ varies to which $m$ we employ, the upper bound of this accuracy is limited by the degree of information loss in observations, which is determined by the feedback available from each scenario's configuration. 
We define $o_{t,m}$ the observation of $m$ at turn $t$ as a tuple:
\begin{align}
\label{eq:o_m}
o_{t,m}\delequal \langle x, \hat{y}_t, F_t \rangle,
\end{align}



How the belief state is likely to be the current state? 

The degree of observability is diverse according to the available feedback. 

\end{comment}




% We categorize three different types of feedback in conversational code generation scenario: Compilation Feedback, Execution Feedback, and User Feedback.

\begin{comment}    
In conversational code generation, we categorize the types of feedback that can be provided to LLMs.
First, we examine the possible signals that feedback can comprise.

\paragraph{Feedback Signals}
\sw{maybe we unify the category pillars? hard to match these three with abstract}
The feedback signals can be divided into three categories:
\begin{itemize}
    \item \textbf{User Intent Satisfaction:} Assessing whether the generated code aligns with user intentions. %assessing whether the generated code aligns with the user's intentions.
    \item \textbf{Error Notification:} Identifying errors within the code. % alerting errors in the code when present.
    \item \textbf{Refinement Guidance:} Proposing guidance to rectify identified issues.
\end{itemize}


Based on the defined feedback signal categories, Table~\ref{tab:categorization} categorizes the feedback types for conversational code generation. 
\paragraph{Compilation Feedback}
Compilation feedback uses a compiler to check for syntax errors. This feedback is cost-effective as it does not require execution conditions or test cases; however, it fails to assess the code's alignment with user intent in terms of functionality.
% This feedback provides whether the current code predictions is: 1) Syntactically correct (knows the target programming language’s grammar) and 2) Compilable (no undefined variables/functions/classes, type mismatch for type-sensitive languages, etc.)
% Pros: Cheap \& Easy to obtain (easily scalable to large scale)
% Cons: Weak Feedback (do not tell whether the code is functionally correct)

\paragraph{Execution Feedback}
It involves running the code to identify runtime errors or to check if the output matches expected results based on given inputs. 
% It provides whether the execution of the code prediction is as expected.
% Two subtypes of errors: 
% 1) Runtime errors: Errors only produced when the code runs. Abnormal termination such as DivisionByZero, IndexError, etc.
% 2) Logical errors: While the code runs and terminates normally, the execution result is different from the expected one. 
To utilize execution feedback, we need an (secured) environment that all the dependencies regarding the code are prepared~\citep{codex}, and test cases of inputs and expected outputs. 
% We further break down this feedback based on the coverage of test cases used, distinguishing between Public TCs settings, which offer limited test case coverage with relaxed burden of preparing test cases, and Full TCs settings, which aim to maximize coverage to enhance feedback quality.
% \sw{TCs can be rigorously annotated with coverage in mind-- We repurpose human annotated TCs from BigCodeBench,  5.6 test cases with an average branch coverage of 99\%. However, in more realistic settings, some subset of such TCs will be annotated. To cover both scenarios, we consider high/low coverage scenarios} hj: I paraphrased this in Sec 4.1
We further divide this feedback based on test coverage from low, which user provides limited number of test cases to reduce the burden of preparing them, to high, which aims to maximize coverage to improve feedback quality.  %\yh{is there a quantitative way to differentiate full TCs vs public TCs.  Also does public TCs have anything to do with "public"?  If not, maybe it should have a different name.}
% Pros: Rich Feedback. With sufficient test cases, this feedback can give the functional correctness of the code prediction. 
% Cons: For execution, it requires: 1) Sufficient amount of test cases, and 2) (Secured) Environment that all the dependencies regarding the code are prepared. 

% Regarding user feedback, we divide based on their level of expertise into two groups. 
% We divide user feedback into two groups based on the level of expertise. 
We divide user feedback into two groups based on expertise level, recognizing that the correctness of guidance may vary for lower expertise levels. 
\paragraph{Novice User Feedback}
Novice users, who are not familiar with programming, can build feedback on tool feedback, such as
%after compiling or 
compilation and execution of the model-generated code. They report either their satisfaction with the outcome or, if dissatisfied, identify where the issues arose, such as syntax errors or stack trace results. 
Consequently, feedback from novice users often resembles %the information provided in compilation and execution feedback, although it generally offers similar content. Optionally, this feedback 
but may contain refinement directions, which might include erroneous information or lack specific corrective suggestions.\footnote{Feedback from LLMs, including self-corrections, is categorized here, as such feedback may contain hallucinations.} 


\paragraph{Expert User Feedback}
Expert users, who are proficient in generating code
themselves, may leverage
%often utilize LLMs to generate code, aiming 
LLMs to reduce repetitive tasks and focus on more critical coding. Unlike novice users, experts not only can determine whether the generated code is correct or identify errors, but they can also provide correct guidance on how to rectify these issues. However, as providing excessively specific feedback, e.g., ground truth itself in one extreme,  
%could be as involved as refining the code themselves; thus, 
it is more practical to offer high-level directions for necessary corrections. 

\subsection{User Feedback Simulation as Reproducible Environments}
The human-in-the-loop setup lacks reproducibility for extensive comparisons because it involves providing immediate and consistent-quality human feedback after each code generation instance by an LLM. Moreover, this approach is cost-prohibitive.

Instead, inspired by~\citet{wang2024mint} and~\citet{yao2024tau}, we build a reproducible environment by simulating user feedback with GPT-4o via in-context learning~\citep{dong2022survey}.\footnote{See Appendix~\ref{appendix:simulator_comparison} for comparative analysis of user feedback simulation across different LLMs. }

\paragraph{Novice Feedback Simulation}
We simulate novice users by verbalizing compilation and/or execution feedback and optionally including LLM-predicted information, which, like real novice feedback, can sometimes be incorrect.\footnote{The in-context example for novice feedback simulation is provided in Appendix~\ref{appendix:novice_icl}. }

\paragraph{Expert Feedback Simulation}
To simulate expert users, who have a clear understanding of the intended final code structure,  
our simulator LLM takes both the code generated by another LLM and the correct code~\citep{wang2024mint}, enabling a comparison and subsequent detailed feedback on required modifications.\footnote{The in-context example for expert feedback simulation is provided in Appendix~\ref{appendix:expert_icl}. }
\end{comment}

\subsection{Feedback Combinations}
\label{convcodeworld:combination}
In each of our turns, we simulate different real-world interactive code generation scenarios by combining representative feedback combinations. We represent feedback settings by taking a Cartesian product across compilation feedback settings, execution feedback settings, and verbal feedback settings. In particular, we formalize a feedback combination $\Omega$ as a tuple of feedback expressed by regular expression notation:
\begin{align}
    \label{eq:observation}
    \Omega = 
    \langle f_c, [\phi | f_e | f_e^*], [\phi | f_v | f_v^*] \rangle.
    % \Omega = \{\langle x,y,z \rangle | f_c^t \in \{f_c\}, f_e^t \in \{\phi, f_e, f_e^*\}, f_v^t \in \{\phi, f_v, f_v^*\} \}\} 
\end{align} 
The choices of feedback settings is simply dictated by these observations: (a) Compilation feedback $f_c$ is always present since it is cheap and universally available; (b) Execution feedback varies among being unavailable ($\phi$), available with partial test coverage ($f_e$), or with full test coverage ($f_e^*$); (c) Verbal feedback can be also unavailable ($\phi$), available with novice-level ($f_v$), or with expert-level ($f_v^*$). By combining these options---1 for compilation feedback, 3 for execution feedback, and 3 for verbal feedback---we obtain 9 distinct feedback combinations.

% (1) $f_c$ is always available due to the easy accessibility of compilers; 
% (2) Execution feedback varies between being unavailable ($\phi$), available with partial test coverage ($f_e$), or with full test coverage ($f_e^*$);
% (3) Verbal feedback can be also unavailable ($\phi$), available with novice-level ($f_v$), or with expert-level ($f_v^*$). 

% We interpret these diverse feedback combinations as observability in the framework of partially observable Markov decision processes (POMDPs). In this setting, observability refers to the information available to the LLM at each turn, which influences its decision-making process.

% where we formally redefine each feedback symbol ($f_c$, $f_e$, $f_e^*$, $f_v$, and $f_v^*$) as the set of each feedback type. 
Each feedback combination $\Omega$ reflects a unique real-world scenario, allowing us to comprehensively evaluate LLMs under diverse conditions as listed in Table~\ref{tab:partial_observability}.

% Given a problem description $x$, the target code generation model $\mathcal{M}$ iteratively generates the next version of code from feedback:
% \begin{align}
%     \label{eq:convcodeworld}
%     \mathcal{C}_{t+1}^{\mathcal{M}} = \mathcal{M}(x;\mathcal{C}_{t}^{\mathcal{M}};\Omega_t), 
% \end{align}
% where code $\mathcal{C}_t^{\mathcal{M}}$ is generated code by $\mathcal{M}$ at turn $t$ and $\Omega_t$ is the tuple of feedback at turn $t$. 

Now it is easy to formalize the interactive code generation in \ours: For each turn $t$, the target code generation model $\mathcal{M}$ iteratively generates the next version of code $\mathcal{C}_{t+1}^{\mathcal{M}}$ from the problem description $x$, the generated code $\mathcal{C}_{t}^{\mathcal{M}}$, and the corresponding tuple of feedback $\Omega_t$:
\begin{align}
    \label{eq:convcodeworld}
    \mathcal{C}_{t+1}^{\mathcal{M}} = \mathcal{M}(x;\mathcal{C}_{t}^{\mathcal{M}};\Omega_t).
\end{align}
% where $x$ is the problem description.








\section{\oursstatic: A Static Benchmark for Efficient Evaluation}
\label{sec:convcodebench}
While \ours provides a comprehensive live benchmark for evaluating LLMs in interactive code generation scenarios, it requires access to an LLM for verbal feedback generation. Although this approach is more efficient and reproducible than using human annotators, it still introduces additional overhead, cost, and potential reproducibility issues, especially when using closed API models like GPT-4o. To address these challenges, we propose \oursstatic, a static benchmark designed to complement \ours. 

\oursstatic leverages feedback logs generated by a fixed reference model interacting with GPT-4o. The benchmark presents pre-generated conversations---including the code produced by the reference model and the corresponding feedback, such as verbal feedback by GPT-4o---and tasks the target code model with refining the code. We revise Equation~\ref{eq:convcodeworld} to formalize \oursstatic as follows.
For each turn $t$, the target code generation model $\mathcal{M}$ is provided generated code $\mathcal{C}_{t}^{\widebar{\mathcal{M}}}$ from a reference model $\widebar{\mathcal{M}}$, and the corresponding tuple of feedback $\widebar{\Omega}_t$ provided to outputs generated by $\widebar{\mathcal{M}}$. Given the model and feedback corresponding to a reference model, the target model $\mathcal{M}$ generates the next version of code $\mathcal{C}_{t+1}^{\mathcal{M}}$:
\begin{align}
    \label{eq:convcodebench}
    \mathcal{C}_{t+1}^{\mathcal{M}} = \mathcal{M}(x;\mathcal{C}_{t}^{\widebar{\mathcal{M}}};\widebar{\Omega}_t). 
\end{align}


This approach offers several advantages:
\begin{itemize}[left=4pt]
    \item \textbf{Elimination of Dependency on  External LLMs or APIs for Verbal Feedback Generation:} By using static feedback logs, \oursstatic reduces costs and latency associated with real-time LLM interactions.
    % Elimination of reliance on separate LLMs or APIs for user feedback simulation, reducing costs and latency.
    \item \textbf{Parallel Processing of Inference Calls:} The static nature of the benchmark allows for batched evaluation requests across all turns, enabling faster turnaround times.
    % Parallel processing of inference calls for all turns, enabling batched evaluation requests and faster turnaround times.
    \item \textbf{Enhanced Reproducibility:} Utilizing fixed logs ensures consistent evaluations, further increasing reproducibility.
    % Increased reproducibility due to the use of static logs.
\end{itemize}



% where $x$ is . 

% \begin{table}[t]
% \begin{wraptable}{r}{0.52\textwidth}
\begin{wraptable}{h!}{0.46\textwidth}
\centering
\caption{Performance of three different LLMs at turn 0 (i.e. the initial code generation without feedback) and at turn 10 on \ours where $\Omega = \langle f_c, \phi, f_v^* \rangle$. }
\scriptsize
% \small
\begin{tabular}{lcc}\thickhline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Pass@$1$}} \\
& \textbf{Turn 0} & \textbf{Turn 10} \\ \hline
CodeLlama-7B-Instruct & 21.8 & 55.2 \\ 
DeepSeek-Coder-6.7B-Instruct & 35.2 & 83.1 \\ 
GPT-4-0613 & 46.0 & 92.5 \\\thickhline
\end{tabular}
\normalsize
\label{tab:model_select_for_statis}
% \end{table}
\end{wraptable}

One key concern when using \oursstatic is the bias introduced by pre-generated interaction logs prompting the question: \textit{Can we ensure high correlation between static and live benchmarks by an appropriate choice of reference model?} 

We hypothesize that using logs from a weaker model, where the generated code still requires refinement even after multiple turns, allows for better differentiation among models based on their ability to improve unsolved code.

Based on this rationale, we used CodeLlama-7B-Instruct as a reference model, as it is worse than many other models at both turns 0 and 10 (see Table~\ref{tab:model_select_for_statis}). %\footnote{One may consider a weaker model than CodeLlama-7B-Instruct, such as DeepSeek-Coder-}
We find that creating \oursstatic with this model yields a very strong correlations with live settings. When comparing models on \ours and \oursstatic, we obtained Spearman's rank correlations between 0.82 and 0.99. We find that using CodeLlama-7B-Instruct as the base model outperforms both DeepSeek-Coder-6.7B-Instruct (a stronger code model) and GPT-4 (one of the state-of-the-arts) as reference models (\S\ref{sec:static_results}). 

In summary, we find that \oursstatic is a great way of comparing code models within the framework of \ours despite relying on logs from a reference model because of strong rank correlations across the two setups.

\begin{comment}
Key points to address
1. \ours is a live benchmark. Needs access to an LLM for simulating user feedback. Therefore has an extra overhead (either running a separate model or calling a closed API) and cost. Close apis additionally have reproducibility issues.
2. idea: can we use a feedback logs with a fixed code model (reference model) and gpt4-o, and then use these static logs. In particular, we can take frozen conversation between reference model and gpt4-o until turn i (for all values i), and then ask our target code model to refine the outcome and check if we our target model can get it right. This will have 3 benefits: we do not need to rely on a separate LLM/Api for user feedback simulation which will drive costs and latency down; furthermore, we can make inference calls for all turns in parallel (unlike the live setting where LLM calls need to happen in sequence), which allows batching eval requests for a faster turn around time.
3. Of course, the risk here is that we are introducing a bias against the live setting where the code and feedback corresponds to the target model. Therefore the key design constraint here is: can we choose a reference model in such a way such that performance of code models on the static benchmark correlates strongly with the performance on the live benchmarks. If we choose a relatively weak model, we risk biasing the target model with very noisy interaciton logs. If we choose a model with strong turn 1 performance, we risk not exploring the space of all turns. We need a \textit{goldilocks model} -- a model whose turn 1 performance is not very high but that also manages to climb up with more feedback so we know that the reference model output and corresponding user feedback is not noisy. Indeed we find that deepseek-coder 6.7B-instruct fits this criteria -- it's turn 1 performance on bigcodebench is 35\%, but it climbs up to 82\% by turn 10 (matching gpt4-o). We hypothesize that using Deepseek Coder 6.7B as reference model will create a static benchmark that will correlate strongly with live performance. And indeed we find in section HOJAE link section here, that this way of creating \oursstatic leads to strong correlation with live benchmarks (Spearman rank correlation between 0.78 and 0.99) outperforming both codellama-34B (weaker code model) and gpt4-o (strong code model) as reference models.

To address the cost of environment API calls from \ours,
 we introduce \oursstatic, a  static benchmark not requiring expensive environments, yet offering closely correlated evaluation results with \ours.
 
 \oursstatic~replaces environments with uses logs from \ours, generated by a reference LLM, along with simulated user feedback, based on which we can evaluate the target LLMs, tasked to refine reference LLM generation.
 
The advantage of this approach is having previous interactions logged, such that it is efficient, and reproducible without re-geerating user feedbacks.
Desirably, reference LLMs should correlate sufficiently with target LLMs to test, for which we hypothesized 
with reference models with high generation accuracy, diverse accuracy, and low accuracy. 

Among hypotheses test, we found DeepSeek-Coder-6.7B-Instruct as the reference LLM \sw{HOJAEelaborate this decision} and found a high Spearman's rank correlation (0.79 to 0.99) between the rankings in \oursstatic and \ourslive.
\end{comment}
