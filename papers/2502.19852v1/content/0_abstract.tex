\begin{abstract}
Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold:
\textbf{First}, we introduce \ours, a novel and reproducible environment for benchmarking interactive code generation. \ours simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback:
(a) compilation feedback;
(b) execution feedback with varying test coverage;
(c) verbal feedback generated by GPT-4o with different levels of expertise. 
%
\textbf{Second}, we introduce \oursstatic, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with \ours. 
% \textbf{Second}, we create a fast and API-free version of our benchmark \oursstatic, a \textit{static} benchmark that leverages pre-generated feedback logs, eliminating the need for costly dynamic (and expensive) verbal feedback generation while maintaining a strong Spearman rank correlation (0.82 to 0.99) with corresponding settings in \ours. 
%
\textbf{Third}, extensive evaluations of both closed-source and open-source LLMs {including R1-Distill} on \ours reveal key insights: 
(a) LLM performance varies significantly based on the feedback provided;
(b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; 
(c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; 
(d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa.
% (c) Interactive training may limit LLMs' ability to utilize unseen feedback;
% (d) Different LLMs excel at different optimization goals: those effective at minimizing the number of turns to solve a problem may not be the best at maximizing the total number of problems solved, and vice versa.
% (d) Different LLMs excel at different goals: some minimize the number of turns to solve a problem, while others solve more problems overall.
% Based on these findings, future works can leverage our benchmarks to focus on generalization, or pursue diverse multi-turn evaluation goals such as minimizing the number of turns to solve a problem or solving more problems overall. 
%
All implementations and benchmarks are publicly available at \href{https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld}{\small \texttt{https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld}}.


\begin{comment}    
Large language models (LLMs) are immensely useful for code generation and are frequently used in interactive settings across diverse scenarios
with varying feedback combination.
However, existing code generation benchmarks do not adequately capture the diversity of feedback in multi-turn interactions, limiting the development and evaluation of LLMs in these settings.
To address this challenge, we make the following contributions:
\textbf{First}, we model multi-turn code generation as a partially observable markov decision process (POMDP), as a unified framework of diverse scenarios.
%and enabling future research to employ reinforcement learning for performance improvements.
\textbf{Second}, we introduce \ours, a novel and reproducible \textit{world} that systematically combines three types of feedback--(1) compilation feedback, (2) execution feedback with varying test coverage, and (3) verbal feedback differentiated by expertise level--into nine distinct combinations that broadly cover diverse multi-turn code generation scenarios. 
Extensive evaluations of both closed-source and open-source LLMs on \ours reveal that: (1) \textbf{Weaker LLMs with sufficient feedback loops can surpass state-of-the-art LLMs} in single-turn code generation without feedback; (2) \textbf{An LLM's performance varies significantly depending on the feedback provided};
(3) \textbf{Different LLMs excel at different optimization goals}--an LLM effective at minimizing the number of turns to solve a problem may not be the best at maximizing the total number of problems solved, and vice versa.
\textbf{Third}, we propose \oursstatic, a \textit{static} benchmark that leverages pre-generated feedback logs, eliminating the need for costly LLM-based verbal feedback generation while maintaining a strong Spearman rank correlation (0.82 to 0.99) with corresponding settings in \ours. 
All implementations and benchmarks will be made publicly available at \href{}{\texttt{https://link\_available\_in\_camera-ready\_version}}.
\end{comment}
% Large language models (LLMs) are immensely useful for code generation and are frequently used in interactive settings by individuals across diverse scenarios with varying feedback combination.
% However, existing code generation benchmarks fail to capture the diversity of feedback combination in multi-turn setting. 
% To address this gap, we present \ours, a novel and reproducible \textit{world} that provides diverse feedback combination, where each feedback is categorized into three pillars: compilation feedback, execution feedback with varying test coverage, and verbal feedback differentiated by expertise level.  
% To effectively express different feedback combination into a single formulation, we formulate multi-turn code generation by partially observable markov decision processes (POMDPs), encouraging possible future research of employing reinforcement learning to improve the multi-turn code generation performance. 
% Furthermore, we propose  a \textit{static} benchmark \oursstatic, that leverages pre-generated feedback logs, eliminating the need for costly LLM-based user feedback generation, while maintaining
% a strong correlation (spearman rank correlation from 0.82 to 0.99) with corresponding settings from \ours. We release all implementations and benchmarks at \href{}{\texttt{https://link\_available\_in\_camera-ready\_version}}. %\href{https://github.com/sfc-gh-hhan/convcodeworld}{\texttt{https://github.com/sfc-gh-hhan/convcodeworld}}. 
\end{abstract}


