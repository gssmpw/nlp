\section{Introduction}
\label{intro}
Human-AI pair programming has become a promising approach to boost software development productivity, where large language models (LLMs) iteratively refine the code from developers' feedback.
% Human-AI pair programming has emerged as a promising approach to enhance software development productivity, where developers provide feedback towards large language models (LLMs) to iteratively refine code.
% Human-AI pair programming has emerged as a promising approach to enhance software development productivity by enabling collaboration between developers and large language models (LLMs) on coding tasks. 
However, most existing benchmarks focus on single-turn scenarios, where LLMs are expected to generate executable code in one attempt~\cite{codex,DBLP:conf/nips/HendrycksBKMAGB21,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}.
% However, most existing benchmarks focus on single-turn scenarios, where LLMs are expected to generate compilable and executable code in a single attempt~\cite{codex,DBLP:conf/nips/HendrycksBKMAGB21,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}.

% Human-AI pair programming has emerged as a promising approach to enhance software development productivity, by allowing developers and AI models like large language models (LLMs) to collaborate on coding tasks. Although this collaboration involves iterative, multi-turn interactions with developer feedback, most existing benchmarks are limited to single-turn scenarios where LLMs are expected to produce compilable and executable code in a single attempt~\cite{codex,DBLP:conf/nips/HendrycksBKMAGB21,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}.


\begin{figure*}[t!]
    \centering  
    \includegraphics[width=1.0\linewidth]{fig/convcodeworld_fig1_v5.pdf}
    % Overview of \ours and \oursstatic: 
    \caption{\textbf{(Left)} \ours is a dynamic, reproducible environment that simulates nine distinct feedback scenarios by combining three types of feedback. \textbf{(Right)} \oursstatic is a static version of the benchmark that uses pre-generated logs and strongly correlates with \ours. 
    Together, these frameworks provide a comprehensive, cost-effective approach for evaluating LLMs in multi-turn, feedback-driven code generation, enabling scalable and consistent benchmarking across diverse feedback combinations. 
    }
    \label{fig:overview}
\end{figure*}



% Some recent benchmarks, such as InterCode~\citep{yang2023intercode} and MINT~\citep{wang2024mint}, have introduced multi-turn evaluation frameworks, but they are limited in the types and combinations of feedback they support (\S\ref{related}). 
% These benchmarks typically emphasize specific forms of feedback, such as execution feedback and verbal feedback by GPT-4, 
% but lack the breadth of feedback combinations needed to fully assess LLM performance in diverse, realistic scenarios.


% % However, in real-world scenarios, the combination of feedback that a user can provide may vary, and this variation can significantly affect the performance of LLMs in interactive code generation. 
% To effectively evaluate LLMs' code generation abilities in realistic scenarios, we present \ours, a novel and reproducible \textit{world} designed to support diverse feedback combinations, assembled by
% % This enables models to formulate the multi-turn conversational code generation task as partially observable markov decision processes (POMDPs), and
% three feedback categories: 
To address these gaps, we introduce \ours (\S\ref{sec:convcodeworld}; left panel in Figure~\ref{fig:overview}), a novel environment for benchmarking 
interactive multi-turn code generation across diverse feedback combinations. 
\ours features nine scenarios by combining three feedback types: 
(a) compilation feedback, (b) execution feedback with partial and full test coverage, and (c) novice and expert level verbal human feedback. 
We simulate human feedback using GPT-4o~\citep{gpt-4o} to generate verbal responses, ensuring reproducibility and cost-efficiency at only 1.5\% of the cost of human annotation (Appendix~\ref{appendix:efficiency}).
% We simulate human feedback using the high-performing closed-source LLM GPT-4o~\citep{gpt-4o} to generate verbal responses, ensuring reproducibility and cost-efficiency requiring only 1.5\% of financial cost compared to human intervention (\S\ref{convcodeworld:reproducibility}).
% See left side of Figure~\ref{fig:overview} for an illustration.

% To address these gaps, we introduce \ours, a novel, reproducible environment designed to benchmark interactive multi-turn code generation across a wide range of feedback combinations (\S\ref{sec:convcodeworld}).
% \ours offers nine distinct scenarios by systematically combining three key types of feedback:
% (a) compilation feedback, (b) execution feedback with partial and full test coverage, and (c) simulated human feedback at novice and expert levels. To simulate human feedback, we generate natural language feedback with varying levels of expertise by GPT-4o (\cite{gpt-4o}; \S\ref{convcodeworld:reproducibility}). See left side of Figure~\ref{fig:overview} for an illustration.


% Execution feedback is further subdivided into full and partial test coverage, where the former represents annotating sufficient test cases to achieve near-perfect branch coverage, e.g., BigCodeBench~\citep{zhuo2024bigcodebench}, and the latter is a more realistic setting when annotating its subspace, e.g., public test cases in competition-level programming~\citep{DBLP:conf/nips/HendrycksBKMAGB21,li2022competition}. 
% Similarly, we divide verbal feedback into two expertise levels---novice and expert---employing GPT-4o~\citep{gpt-4o} for reproducible evaluation. 
% Specifically, novice-level feedback is generated without providing the answer code, 
% while expert-level is modeled to know the code, but does not leak the code in feedback.
% Fault localization is performed by verbalizing compilation and execution errors. As we do not provide any ground truth knowledge during the feedback generation process, this would lead to potential inaccuracies or hallucinations in their refinement suggestions, similar to the guidance a novice might provide. Expert users, in contrast, are experienced developers who know how to write the target code but prefer not to do so themselves, choosing instead to focus on more challenging code that requires their expertise. Expert feedback is simulated by comparing the current version of the code with the ground truth code, injecting the knowledge that experts would possess. We ensure that the ground truth code is not directly exposed in the feedback and that the feedback remains high-level, similar to code reviews, which requires less effort than writing the correct code from scratch. 



\begin{comment}    
%the AI's ability to respond effectively to different types of
Meanwhile, in real-world scenarios, iterative feedback from developers in diverse levels
%ranging from error messages to user-provided hints—can 
significantly impact the overall productivity and quality of the generated code.
%However, existing code benchmarks typically focus on two main limitations: (1) single-turn interactions where models are expected to generate compilable and executable code in a single attempt~\citep{codex,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}, and (2) restricted coverage that do not fully capture the diversity feedbacks found in real-world programming scenarios~\citep{wang2024mint}.}
To properly capture this setting, we present two key contributions. First, we introduce \ours, a novel and reproducible environment designed to support diverse feedback combinations that encompass comprehensive and realistic coding scenarios. \ours offers three feedback categories: (1) compilation feedback, (2) execution feedback, and (3) user feedback. Execution feedback is further subdivided into full and partial coverage,
%based on test coverage. Full test coverage represents an extensive setting 
where the former represents annotating  sufficient test cases to achieve near-perfect branch coverage, e.g., BigCodeBench~\citep{zhuo2024bigcodebench}, 
and the latter a more realistic setting annotating its subspace, e.g., 
public test cases in competition-level programming.
Similarly, we simulate 
%To ensure reproducibility, user feedback is simulated using GPT-4o. We distinguish 
user feedback using GPT-4o, enabling reproducible evaluation, covering diverse expertise levels,
%: novice and expert.}
Specifically, novice users 
%can recognize whether the current version of the code functions correctly but may
are simulated to lack the knowledge to fix errors,
%We simulate novice user feedback by 
using the generated code, compilation feedback, and execution feedback to generate feedback. 
\end{comment}


% A downside of replacing costly human intervention with LLMs in \ours is that it can be expensive (due to computational overhead or API costs), slow (waiting for LLM or API responses), and reliant on external APIs, which may expire.
% To address these concerns, we introduce \oursstatic, a static, external API-free benchmark that uses pre-generated feedback logs (\S\ref{sec:convcodebench}). 
% \ours eliminates the need for real-time feedback generation while achieving a strong correlation (Spearman’s rank 0.82-0.99) with the live interactions in \ours (\S\ref{sec:static_results}). 
% This makes it a cost-effective and scalable solution for large-scale benchmarking of LLMs. 

% A downside of replacing expensive human intervention with LLMs in \ours is the potential for costs (computational overhead or API fees), latency (waiting for LLM or API responses), and reliance on external APIs, which may expire. 

While replacing expensive human intervention with LLMs in \ours reduces costs, it can still be expensive due to computational overhead or API fees, and latency due to LLM response.
To address these issues, we introduce \oursstatic (\S\ref{sec:convcodebench}; right panel in Figure~\ref{fig:overview}), a static benchmark using pre-generated feedback logs. \oursstatic eliminates the need for real-time feedback generation while maintaining strong correlation with \ours (Spearman’s rank 0.82-0.99; \S\ref{sec:static_results}), offering a cost-effective and scalable solution for large-scale LLM benchmarking.


% \ours sends generated conversations and code over to GPT-4o to generate next round of input in a dynamic way. However, this can be expensive (requires paying for API calls), slow (we need to wait for the API calls), and creates dependence on an external API. To address these concerns, we introduce \oursstatic, a static, external API-free benchmark that uses pre-generated feedback logs (\S\ref{sec:convcodebench}). 
% % a static benchmark closely correlated with \ours results. 
% \ours eliminates the need for real-time feedback generation while achieving a strong correlation (Spearman’s rank 0.82-0.99) with the live interactions in \ours (\S\ref{sec:static_results}). 
% This makes it a cost-effective and scalable solution for large-scale benchmarking of LLMs. 

Existing benchmarks like InterCode~\citep{yang2023intercode} and MINT~\citep{wang2024mint} lack the variety feedback combinations needed for comprehensive LLM performance assessment (\S\ref{related}). Additionally, their reliance on LLM calls for verbal feedback increases costs.
%
% Our study distinguishes itself by (a) offering a reproducible environment with nine unique feedback combinations, and (b) providing a cost-effective benchmark that maintains high correlation with live environments using pre-generated logs, eliminating the need for expensive LLM calls to generate verbal feedback.
Our study stands out by (a) offering a reproducible environment with \textbf{9 unique feedback combinations}, and (b) providing a \textbf{cost-effective benchmark} using pre-generated logs, avoiding costly LLM calls for verbal feedback while maintaining strong correlation with live results. 


% From generated code by a reference model and corresponding feedback, \oursstatic approximates the interactive code generation as if an evaluated LLM produced the code and refines it using the feedback. 
% % \oursstatic approximates the interactive code generation problem as a code repair problem using logs from \ours. Given pairs of generated code 
% A key criterion is selecting which reference model's logs to use. We found that logs from CodeLlama-7BInstruct~\citep{roziere2023code}, which showed the lowest performance in \ours, yielded the highest correlation between \oursstatic and \ours. This selection enables enough room for code improvements, making \oursstatic a robust proxy for interactive conversational code generation evaluation.

\begin{comment}    
Fault localization is performed by verbalizing compilation and execution errors. As we do not provide any ground truth knowledge during the feedback generation process, this would lead to potential inaccuracies or hallucinations in their refinement suggestions, similar to the guidance a novice might provide. Expert users, in contrast, are experienced developers who know how to write the target code but prefer not to do so themselves, choosing instead to focus on more challenging code that requires their expertise. Expert feedback is simulated by comparing the current version of the code with the ground truth code, injecting the knowledge that experts would possess. We ensure that the ground truth code is not directly exposed in the feedback and that the feedback remains high-level, similar to code reviews, which requires less effort than writing the correct code from scratch. 
\end{comment}


% {\ours's three feedback categories enable the simulation of various realistic scenarios, such as: (i) code compilation and execution are available, but the user provides only partial test coverage without textual feedback (Figure~\ref{fig:live_cf_ef_public}); (ii) compilation is available, and the code is not executable, but the user is an expert (Figure~\ref{fig:live_cf_sef}); and (iii) both compilation and execution are available with full test coverage, but the user is a novice who may not provide refinement directions or may suggest incorrect directions (Figure~\ref{fig:live_cf_ef_full_snf}). This diversity is critical, as our experimental results demonstrate that LLMs perform significantly differently across scenarios, underscoring the need for evaluation across various feedback combinations. }



\begin{comment}
Human-AI pair programming has emerged as a promising approach to boost software development productivity. While large language models (LLMs) have shown remarkable capabilities in code generation, existing code benchmarks often fall short in evaluating these models in realistic scenarios. Most benchmarks  focus on single-turn interactions, where models are expected to produce compilable and executable code in one attempt~\citep{codex,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}. 

However, these benchmarks fail to capture two critical aspects of real-world programming scenarios:
1) The interactive nature of human-AI collaboration in code development;
2) The various granularities of feedback that occur during the programming process.

Addressing these gaps is crucial to better assess the capabilities of AI-assisted programming tools and their readiness for real-world deployment.

\begin{figure*}[t!]
    \centering  
    \includegraphics[width=1.0\linewidth]{fig/convcodeworld_fig1_v3.pdf}
    \caption{Overview of \ours and \oursstatic.}
    \label{fig:overview}
\end{figure*}

Our work makes two primary contributions to address these challenges. 
First, we introduce \ours, novel and reproducible environments designed to assess the multi-turn code generation capabilities of LLMs. \ours incorporates a comprehensive categorization of feedback types that reflect diverse real-world programming scenarios:
\begin{itemize}
    \item \textbf{Compilation Feedback} indicates whether the code compiles successfully or provides error messages.
    \item \textbf{Execution Feedback} assesses the code's runtime behavior with test cases from users. 
    \item \textbf{User Feedback} simulates interactions with users who can identify issues and might provide suggestions for code improvement. 
\end{itemize}
% In order to simulate both users, we use GPT4-o via in-context learning~\citep{dong2022survey}.\footnote{See Appendix~\ref{appendix:icl_examples} for the in-context examples. } 
% We use gpt4-o to simulate two different kinds of user feedback:

\sw{why this is meaningful and comprehensive}

Second, to address potential cost and reproducibility challenges of API-based evaluations in some scenarios, 
 we also propose \oursstatic, a complementary static benchmark. \oursstatic uses logs from \ours generated by a reference LLM alongside corresponding simulated user feedback, to assess
 the target LLMs' ability to refine code at each turn, while keeping the previous interactions frozen. This approach is more cost-effective, efficient, and reproducible, as it eliminates the need to re-generate user feedback at each turn. 
\sw{i tried to hide details}
We choose DeepSeek-Coder-6.7B-Instruct as the reference LLM (see Section~\ref{sec:convcodebench} for how we arrived at this choice), we get fairly high Spearman's rank correlation between the rankings in \oursstatic and \ourslive (between 0.79 to 0.99).
\sw{
From our validation using DeepSeek- as the reference LLM (see Section~\ref{sec
} for rationales on this choice), we observed a strong Spearman's rank correlation (0.79 to 0.99) between the rankings in \oursstatic and \ourslive.}

\sw{let's change to iclr, see if we can summarize these bullets}
\end{comment}
Through extensive experiments using both \ours and \oursstatic across 21 different open and closed-source models {including R1-Distill (\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}; Appendix~\ref{appendix:r1_results})}, we have gathered several key insights: 
(\S\ref{exp:convcodeworld}):
\begin{itemize}[left=6pt]
    \item \textbf{Feedback Combinations Diversifying Evaluation:} LLM performance varies across feedback settings, with feedback combinations affecting model rankings, highlighting the need for evaluation across diverse scenarios.
    \item \textbf{Weaker Models with Feedback Surpassing Single-Turn SOTA:} Weaker LLMs, with sufficient multi-turn feedback, can surpass state-of-the-art models in single-turn scenarios without feedback. This emphasizes the importance of interactive multi-turn code generation.
    \item \textbf{Generalization Challenges:} Models trained on limited feedback struggle to generalize to unseen combinations, highlighting the difficulty of adapting LLMs to new scenarios.
   \item \textbf{MRR and Recall Trade-off:} LLMs that efficiently solve problems in fewer turns (high MRR) may not solve as many problems in total (high Recall), highlighting a trade-off between efficiency and problem coverage.
    %Interactive training may limit an LLM’s ability to generalize to unseen feedback, suggesting that reliance on specific feedback types can reduce adaptability.
   % \item \textbf{New Design Space:}  This benchmark highlights a gap in current solution space and points to the need for future models to optimize for diverse multi-turn evaluation objectives.
   %Based on these findings, future models can leverage our benchmarks to focus on generalization, or pursue diverse multi-turn evaluation goals. Some may aim to minimize the number of turns required to solve a problem, while others optimize for solving a broader range of problems given number of turns.
\end{itemize}

\begin{comment}    
\begin{itemize}
\item \textbf{Impact of Multi-Turn on  GPT Models:} While GPT-4o demonstrates higher performance per iteration compared to GPT-4, the latter solves more problems given sufficient iterations. This highlights the importance of assessing models over multiple turns.
\item \textbf{Impact of Training Data on Model Performance:} ReflectionCoder~\citep{ren2024reflectioncoder}, trained on non-expert feedback, underperforms its base model (DeepSeek-Coder-6.7B-Instruct~\citep{deepseek-coder}) when presented with expert feedback. This suggests that the nature of training data significantly influences a model's ability to utilize different types of feedback.
% \item \textbf{Feedback utilization capabilities:} GPT-4 and GPT-4o show remarkable ability to match the performance achieved with novice user feedback by solely analyzing compilation and execution feedback, such as stack traces and error messages.
\item \textbf{Impact of Test Coverage:} When provided with high-coverage test cases instead of just low-coverage ones, GPT-4 and GPT-4-Turbo consistently improved their performance with each iteration, a pattern not observed in other models.
\item \textbf{Impact of Verbal Feedback:} Expert feedback effectively reduces the performance disparity between closed-source and open-source LLMs. Notably, DeepSeek-Coder-6.7B-Instruct excels in utilizing expert feedback among open-source LLMs, even surpassing GPT-4o in recall given sufficient iterations.

\end{itemize}
\end{comment}


% By offering a comprehensive, cost-effective, and reproducible evaluation environment \ours and an even more affordable API-free benchmark \oursstatic, this study sets a new standard for assessing LLMs in interactive, multi-turn code generation tasks, addressing key gaps in existing benchmarks. 


