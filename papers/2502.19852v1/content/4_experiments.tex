\section{Experiments}
Using \ours and \oursstatic, we conduct comprehensive experiments to evaluate LLMs' interactive code generation capabilities across diverse feedback combinations. 
This section outlines our experimental setup (\S\ref{exp:setup}), results on \ours  (\S\ref{exp:convcodeworld}), and results on \oursstatic (\S\ref{sec:static_results}).

\subsection{Setup}
\label{exp:setup}
To implement \ours, we extended BigCodeBench{-Full}-Instruct~\citep{zhuo2024bigcodebench}, a single-turn Python code generation benchmark, into an interactive framework using a custom prompt pipeline built using DSPy~\citep{khattab2024dspy} (see Appendix~\ref{appendix:impl} for the implementation details).  
BigCodeBench was selected for three key reasons: (a) its highly challenging problem sets (as of the writing of this paper, the highest performance on this data is 51.1\% of Pass@$1$); (b) its large scale, with 1,140 problems, offering higher generalizability than smaller benchmarks like HumanEval (\citealp{codex}; 164 problems) and MBPP-sanitized (\citealp{DBLP:journals/corr/abs-2108-07732}; 399-427 problems); and (c) its comprehensive test coverage---an average of 5.6 cases per problem with 99\% branch coverage---enabling the evaluation of a wide spectrum  of execution feedback scenarios, ranging from partial to full test coverage.
% The implementation of \ours is based on DSPy~\citep{khattab2024dspy}.
% To create \ours, we extended BigCodeBench~\citep{zhuo2024bigcodebench}, a rigorous single-turn code generation benchmark, for its complexity that challenges advanced models like GPT-4o and GPT-4. Its extensive dataset of 1,140 problems offers superior generalizability compared to smaller benchmarks like HumanEval (\citealp{codex}; 164 problems) and MBPP-sanitized (\citealp{DBLP:journals/corr/abs-2108-07732}; 399-427 problems). Additionally, BigCodeBench's detailed test coverage--5.6 cases per problem with 99\% branch coverage--enables diverse execution feedback scenarios, from partial to full test coverage.
% The implementation details of \ours is described in Appendix~\ref{appendix:impl}. 
% Throughout the evaluation, the maximum token length for all code generation models is set to 8K. For models with a smaller maximum length, we use their respective maximum token length. 
% For verbal feedback generation, we use GPT-4o with the maximum token length as 2K. %, which demonstrated the highest utility and the lowest ground truth code leakage in expert-level feedback simulations.\footnote{See Appendix~\ref{appendix:verbal_feedback} for details.}


\paragraph{Evaluation Metrics}
In the interactive scenario, where code is iteratively refined based on feedback, we focus on two aspects for evaluation: (a) the number of turns it takes to produce correct code, with fewer turns being preferable, and (b) whether the model can eventually solve the problem within a set number of turns $n$. In our experiments, we set $n=10$. 


To capture these aspects, we use Pass@$1$~\citep{codex} as the core metric to assess code correctness at each turn and adapt two complementary metrics from information retrieval: %, namely Mean Reciprocal Rank (MRR) and Recall: 
(a) \textbf{Mean Reciprocal Rank (MRR):} $\frac{1}{k}$ where $k$ is the turn at which the model produces correct code. If no correct code is generated within $n$ turns, the score is set to 0; 
(b) \textbf{Recall:} 1 if the model produces correct code within $n$ turns. 
% \begin{itemize}
% \item \textbf{MRR:} $\frac{1}{k}$ where $k$ is the turn at which the model produces correct code. If no correct code is generated within $n$ turns, the score is set to 0.
% \item \textbf{Recall:} 1 if the model produces correct code within $n$ turns. 
% \end{itemize}


% Pass@$k$
% MRR
% Recall
% \yh{please define those evaluation metrics before using them.}


\paragraph{Baseline LLMs}
We extensively evaluated 3 closed-source and 18 open-source LLMs ranging from 7B to 70B:\footnote{While we attempted smaller models like DeepSeek-Coder-1.3B-Instruct,  it failed to follow interactive code generation format, resulting degeneration.} 
(a) \textbf{Closed-Source:} We select three OpenAI LLMs---GPT-4-0613, GPT-4-Turbo-2024-04-09, and GPT-4o-2024-05-13;
(b) \textbf{Open-Source:} Llama-3.1-70B-Instruct~\citep{dubey2024llama}, Llama-3.1-8B-Instruct, DeepSeek-Coder-V2-Lite-Instruct (\cite{zhu2024deepseek}; an MoE model; total params: 16B; active params: 2.4B), DeepSeek-Coder-33B-Instruct~\citep{deepseek-coder}, DeepSeek-Coder-6.7B-Instruct, ReflectionCoder-DS-33B~\citep{ren2024reflectioncoder}, ReflectionCoder-DS-6.7B, Qwen1.5-72B-Chat~\citep{qwen}, Qwen1.5-32B-Chat, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1,\footnote{\href{https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1}{\texttt{https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1}}} CodeLlama-34B-Instruct~\citep{roziere2023code},\footnote{We excluded CodeLlama-70B-Instruct due to its 4K token length limitation, which is too short for interactive code generation.} CodeLlama-13B-Instruct, and CodeLlama-7B-Instruct. {In Appendix~\ref{appendix:r1_results}, we further included the results of two recent R1-Distill~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} models and their base models on \ours.}

% \begin{itemize}
%     \item \textbf{Closed-Source:} We select three OpenAI LLMs---GPT-4-0613, GPT-4-Turbo-2024-04-09, and GPT-4o-2024-05-13. 
%     \item \textbf{Open-Source:} Llama-3.1-70B-Instruct~\citep{dubey2024llama}, Llama-3.1-8B-Instruct, DeepSeek-Coder-V2-Lite-Instruct (\cite{zhu2024deepseek}; an MoE model; total params: 16B; active params: 2.4B), DeepSeek-Coder-33B-Instruct~\citep{deepseek-coder}, DeepSeek-Coder-6.7B-Instruct, ReflectionCoder-DS-33B~\citep{ren2024reflectioncoder}, ReflectionCoder-DS-6.7B, Qwen1.5-72B-Chat~\citep{qwen}, Qwen1.5-32B-Chat, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1,\footnote{\href{https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1}{https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1}} CodeLlama-34B-Instruct~\citep{roziere2023code},\footnote{We excluded CodeLlama-70B-Instruct due to its 4K token length limitation, while our experiments require 8K.} and CodeLlama-7B-Instruct.
%     %Following the suggestion in~\citet{deepseek-coder}, we chose 7B as the default scale of LLMs in our experiments, considering the balance between performance and serving efficiency for real-world deployment. Specifically, we employ DeepSeek-Coder-V2-Lite-Instruct~\citep{zhu2024deepseek} (an MoE model; total params: 16B; active params: 2.4B), CodeQwen1.5-7B-Chat~\citep{qwen}, CodeLlama-7B-Instruct~\citep{roziere2023code}, ReflectionCoder-DS-6.7B~\citep{ren2024reflectioncoder}, and DeepSeek-Coder-6.7B-Instruct~\citep{deepseek-coder}. 
% \end{itemize}


\subsection{Results on \ours}
\label{exp:convcodeworld}
% \paragraph{Overview of Results}
Tables~\ref{tab:convcodebench_mrr} and~\ref{tab:convcodebench_recall} present MRR and Recall scores, respectively, for both closed-source and open-source LLMs across various feedback combinations. These results provide a comprehensive view of model performance in \ours.

% \subsubsection{Overview of Results}
\paragraph{Overview of Results}
While closed-source models generally outperformed most open-source models, Llama-3.1-70B-Instruct demonstrated competitive Recall performance, surpassing both GPT-4-Turbo and GPT-4o in certain scenarios like $\langle f_c, [f_e|f_e^*], f_v \rangle$ and $\langle f_c, [\phi|f_e|f_e^*], f_v^* \rangle$. 

Notably, this Recall gap between closed-source and open-source models narrows significantly under specific feedback settings, particularly when expert-level verbal feedback $f_v^*$ is provided. For instance, in the $\langle f_c, \phi, f_v^* \rangle$ setting, DeepSeek-Coder6.7B-Instruct (82.8) outperformed GPT-4o (82.3), and DeepSeek-Coder33B-Instruct (85.4) outperformed GPT-4-Turbo (84.7).

Another key observation is that, among open-source models smaller than 30B, no clear winner emerges across all feedback combinations. This emphasizes the importance of selecting models based on the specific type of feedback available.



% Unsurprisingly, we observe that closed-source models from the GPT-4 family outperformed selected open-source models. However, the Recall gap narrows significantly under certain conditions, especially when expert-level verbal feedback is incorporated.
% \subsubsection{Key Observations}






% \paragraph{Recall}
% This trend is even stronger in Recall, where weaker models were often comparable to or exceeded the performance of GPT-4 (46.0), GPT-4-Turbo (48.0), and GPT-4o (50.8):
% \begin{itemize}
%     \item $\langle f_c, f_e, \phi \rangle$: DeepSeek-Coder-33B-Instruct (45.5); ReflectionCoder-DS-33B (45.3).
%     \item $\langle f_c, f_e^*, \phi \rangle$: DeepSeek-Coder-33B-Instruct (46.1); Qwen1.5-72B-Chat (47.5), Qwen1.5-32B-Chat (45.3); DeepSeek-Coder-V2-Lite-Instruct (46.1).
%     \item $\langle f_c, f_e, f_v \rangle$: DeepSeek-Coder-33B-Instruct (50.4); ReflectionCoder-DS-33B (51.4); Qwen1.5-72B-Chat (47.5); DeepSeek-Coder-V2-Lite-Instruct (47.0); CodeQwen1.5-7B-Chat (45.8).
%     \item $\langle f_c, f_e^*, f_v \rangle$: All models except for CodeLlama-34B-Instruct (44.6), CodeLlama-13B-Instruct (41.1), and CodeLlama-7B-Instruct (30.5).
%     \item $\langle f_c, [\phi|f_e|f_e^*], f_v^* \rangle$: All models.
% \end{itemize}
\input{table/live}

\subsubsection{Feedback Combinations:  Diversified Evaluation}
\label{exp:5.2.1}
% \subsubsection{LLM Performance Varies by Feedback Combinations}
% We observed significant performance variation within the same model across different feedback combinations.
% This highlights the necessity of \ours, which enables the evaluation of code generation models across diverse feedback combinations. 

We observed significant performance variation within the same model across different feedback combinations, emphasizing the necessity of \ours for evaluating code generation models under diverse feedback conditions.

Specifically, we summarize the effect of providing different feedback combinations:


% In general, incorporating richer feedback unsurprisingly led to better performance. For example, performance improved progressively with the feedback combinations $\langle f_c, f_e, \phi \rangle$, $\langle f_c, f_e, f_v \rangle$, and $\langle f_c, f_e, f_v^* \rangle$. However, each model responded differently to these feedback combinations, revealing important variations in feedback utilization as summarized below.



\paragraph{Impact of Novice-Level Verbal Feedback on Execution Feedback Utilization}
% \paragraph{Verbal Feedback Reliance}\

Without novice-level verbal feedback ($f_v$), some models—DeepSeek-Coder-33B-Instruct, DeepSeek-Coder-6.7B-Instruct, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1, CodeLlama-13B-Instruct, and CodeLlama-7B-Instruct—showed minimal performance differences between partial ($\langle f_c, f_e, \phi \rangle$) and full ($\langle f_c, f_e^*, \phi \rangle$) test coverage in execution feedback.
% Some models---DeepSeek-Coder-33B-Instruct, DeepSeek-Coder-6.7B-Instruct, CodeQwen1.5-7B-Chat, StarCoder2-15B-Instruct-v0.1, CodeLlama-13B-Instruct, and CodeLlama-7B-Instruct---showed minimal performance differences, without verbal feedback, or, between $\langle f_c, f_e, \phi \rangle$ and $\langle f_c, f_e^*, \phi \rangle$. 
However, these models showed greater reliance on $f_v$, 
 especially in $\langle f_c, f_e^*, f_v \rangle$ compared to $\langle f_c, f_e, f_v \rangle$, indicating that they need $f_v$ to fully leverage $f_e^*$.
In contrast, high-performing models---GPT-4, GPT-4-Turbo, GPT-4o, and Llama-3.1-70B---demonstrated a larger performance boost from $\langle f_c, f_e, \phi \rangle$ to $\langle f_c, f_e^*, \phi \rangle$ compared to the boost from $\langle f_c, f_e, \phi \rangle$ to $\langle f_c, f_e, f_v \rangle$. This suggests these models can infer refinement strategies directly from raw execution feedback without heavily relying on $f_v$.

\paragraph{Impact of Expert-Level Verbal Feedback on Execution Feedback Utilization}
% \paragraph{Balancing Complex Feedback}
Most models demonstrated performance improvements with richer execution feedback, progressing through the sequences $\langle f_c, \phi, f_v^* \rangle$, $\langle f_c, f_e, f_v^* \rangle$, and $\langle f_c, f_e^*, f_v^* \rangle$. 
However, exceptions arise: (a) DeepSeek-Coder family and ReflectionCoder-DS-6.7B exhibited no performance difference with the inclusion of execution feedback; (b) Llama-3.1-8B-Instruct, ReflectionCoder-DS-33B, and CodeQwen1.5-7B-Chat  showed no significant difference between $\langle f_c, \phi, f_v^* \rangle$ and $\langle f_c, f_e, f_v^* \rangle$, but performance improved when full test coverage ($\langle f_c, f_e^*, f_v^* \rangle$) was ensured; (c) In some weaker models---Qwen1.5-32B-Chat and StarCoder2-15B-Instruct-v0.1---increasing the test coverage from $\langle f_c, f_e, f_v^* \rangle$
to $\langle f_c, f_e^*, f_v^* \rangle$ resulted in negative performance impacts.
Additionally, the highest performance of Qwen1.5-32B-Chat was observed with $\langle f_c, \phi, f_v^* \rangle$, while adding execution feedback ($f_e$ or $f_e^*$) led to decreased performance. 
% We hypothesize that weak models struggle to effectively utilize when feedback is complex, leading to lower performance.
We hypothesize that weaker models struggle to utilize complex feedback effectively, resulting in lower performance. {We further discuss the possible reasons for these exceptions in Appendix~\ref{appendix:struggle_to_utilize_feedback}. }









% \paragraph{Effect of Incorporating Expert-Level Verbal Feedback}
% \hj{stale}
% The comparative analysis of Figures~\ref{fig:live_cf} to~\ref{fig:live_cf_ef_full_snf} and~\ref{fig:live_cf_sef} to~\ref{fig:live_cf_ef_full_sef} indicates that incorporating expert-level feedback can significantly narrow the performance disparity between closed-source and open-source LLMs.  In scenarios where verbal feedback is absent or user expertise is low, it may be more beneficial to employ closed-source LLMs over open-source ones. Conversely, for expert users, relying solely on open-source LLMs may be sufficiently effective. An illustrative example is the DeepSeek-Coder-6.7B-Instruct, depicted in brown in Figure~\ref{fig:live_cf_sef}. This model initially scores a Pass@1 score of 35.2, but after refining code using combined feedback from compilation and expert-level feedback, it achieves a Pass@1 score of 82.8 in the tenth iteration, surpassing the 82.3 score of GPT-4o colored as green.



% \paragraph{Effect of Incorporating Novice-Level Verbal Feedback}
% \hj{stale}
% Figures~\ref{fig:gpt-4} and~\ref{fig:gpt-4o} indicate that the performance of GPT-4 and GPT-4o, when provided with novice-level feedback (CF + EF + SNF), does not significantly improve compared to their performance without it (CF + EF).
% Novice-level feedback is essentially a verbalized version of compilation feedback and execution feedback, which can be considered as a natural language-digested reasoning chain. This suggests that GPT-4 and GPT-4o are capable of understanding raw results such as stack traces and knowing how to correct code accordingly.
% In contrast, GPT-4-Turbo in Figure~\ref{fig:gpt-4-turbo} shows more significant performance improvements with novice-level feedback compared to the aforementioned models, and this effect is notably prominent across all open-source LLMs (Figures~\ref{fig:deepseekv2} to~\ref{fig:deepseek}).

\subsubsection{Multi-Turn Feedback:  Weaker Models Outperforming Single-Turn SOTA }
% \subsubsection{Feedback Enables Weaker LLMs to Surpass State-of-the-Art Models}

Weaker LLMs with sufficient feedback outperformed the single-turn, no-feedback performance ($\langle \phi, \phi, \phi \rangle$) of state-of-the-art models like GPT-4 and GPT-4-Turbo.

\paragraph{MRR} When expert-level verbal feedback ($f_v^*$) was incorporated, most weaker models, including DeepSeek-Coder-6.7B-Instruct and Llama-3.1-8B-Instruct, surpassed the single-turn code generation performance of state-of-the-art single-turn models such as GPT-4, GPT-4-Turbo, and GPT-4o. Additionally, with the inclusion of novice-level verbal feedback ($f_v$) and either partial or full execution feedback ($f_e$ or $f_e^*$), DeepSeek-Coder-33B-Instruct and ReflectionCoder-DS-33B matched or exceeded the single-turn performance of GPT-4 and GPT-4-Turbo. 

\input{table/live_recall}


\paragraph{Recall} 
Most open-source models exhibited significant improvements when novice-level verbal feedback with execution feedback ($\langle f_c, [f_e|f_e^*], f_v \rangle$) or expert-level verbal feedback ($\langle f_c, [\phi|f_e|f_e^*], f_v \rangle$) was provided. %, with a few exceptions for models like Qwen1.5-32B-Chat and CodeLlama-7B. 
Remarkably, providing execution feedback with full test coverage while omitting any verbal feedback ($\langle f_c, f_e^*, \phi \rangle$) enabled some models, such as DeepSeek-Coder-V2-Lite-Instruct, DeepSeek-Coder-33B-Instruct, and Qwen1.5-72B-Chat, to achieve or even exceed GPT-4's single-turn performance. 





% Notably ReflectionCoder-DS-6.7B and ReflectionCoder-DS-33B were initialized with parameters of DeepSeek-Coder-6.7B-Instruct and eepSeek-Coder-33B-Instruct, trained explicitly to refine code excluding $f_v^*$---using only $f_c$, $f_e$, and $f_v$ that has undergone one round of reflection from $f_c$ and $f_e$. Both Tables~\ref{tab:convcodebench_mrr} and~\ref{tab:convcodebench_recall} indicate that, as expected, ReflectionCoder-DS performs better on $\langle f_c, [f_e|f_e^*], f_v \ranlge$.
%  However, the dichotomy between the two series of models changes with $f_v^*$---the difference in MRR is negligible and DeepSeek-Coder generally performs better in Recall. This suggests that when training models for interactive code generation, we need to include feedback that is available in the target evaluation scenario. 
 % ReflectionCoder-DS-6.7B, which is trained exclusively with non-expert-level feedback, appears to have diminished capability in leveraging expert-level verbal feedback.

\subsubsection{Generalization: Unseen Feedback Combination}
\label{exp:5.2.3}
ReflectionCoder-DS family were initialized from DeepSeek-Coder-Instruct, and trained to refine code on a specific scenario of $\langle f_c, f_e^*, f_v \rangle$. As a result, ReflectionCoder-DS-6.7B outperformed DeepSeek-Coder-6.7B-Instruct on $\langle f_c, [f_e|f_e^*], f_v \rangle$. However, with unseen feedback like expert-level verbal feedback ($f_v^*$), the performance gap narrows significantly, with minimal MRR difference and DeepSeek-Coder-Instruct generally outperforming in Recall. 
This tendency is more pronounced in ReflectionCoder-DS-33B; 
except for $\langle f_c, [f_e|f_e^*], f_v \rangle$, ReflectionCoder-DS-33B consistently performed at or below the level of DeepSeek-Coder-33B-Instruct across all feedback combinations in both MRR and Recall. 
This indicates that training on a specific feedback combination can reduce the performance on the other combinations. 

% DeepSeek-Coder-33B-Instruct,  showed comparable performance to ReflectionCoder-DS-33B throughout feedback combinations in both MRR and Recall, and outperformed on $\langle f_c, [\phi|f_e|f_e^*], f_v^* \rangle$. 

% This indicates that training models for interactive code generation should integrate all feedback relevant to the target evaluation scenario.

% the target evaluation scenario

% (1) limiting feedback 



\subsubsection{Trade-Off: Multi-Turn MRR and Recall }
We observed that an LLM requiring fewer turns to solve a problem (high MRR) may not excel at solving as many problems as possible (high Recall), and vice versa: (a) \textbf{Closed-Source Models:} GPT-4o achieved the highest MRR, while GPT-4 had the best Recall;\footnote{This quantitatively confirms what some accounts observed on  \href{https://x.com/voooooogel/status/1793782669970706433}{\texttt{x.com}}} (b) \textbf{Open-Source Models $\geq$ 30B:} Llama-3.1-70B led in both MRR and Recall. DeepSeek-Coder-33B-Instruct and ReflectionCoder-DS-33B followed in MRR. However, with $f_e^*$ or $f_v^*$ feedback, Qwen1.5-72B-Chat generally outperformed them in Recall, despite having a lower MRR; (c) \textbf{Open-Source Models $<$ 30B:} MRR and Recall tendencies were similar  without verbal feedback. With verbal feedback, CodeQwen1.5-7B-Chat excelled in MRR, while DeepSeek-Coder-V2-Lite-Instruct ($\langle f_c, [f_e|f_e^*], f_v \rangle$), and DeepSeek-Coder-6.7B-Instruct ($\langle f_c, [\phi|f_e|f_e^*], f_v^* \rangle$) led in Recall. 





% \begin{itemize}
%     \item \textbf{Closed-Source Models:} 
%     \item \textbf{Open-Source Models $\geq$ 30B:} 
%     \item \textbf{Open-Source Models $<$ 30B:} 
% \end{itemize}

\begin{comment}    
\paragraph{GPT-4 vs GPT-4o} GPT-4o consistently achieves the highest MRR across all feedback combinations, however, GPT-4 achieved higher Recall. In other words, GPT-4o is good at getting solutions right in the first go, but in case it makes a mistake, it is worse at getting to the right answer than GPT4.\footnote{This quantitatively confirms what some accounts observed on twitter https://x.com/voooooogel/status/1793782669970706433}

Similarly, when comparing Figures~\ref{fig:live_cf_ef_public} and~\ref{fig:live_cf_ef_full}, GPT-4 demonstrates additional improvements when transitioning from low test coverage to high coverage, whereas GPT-4-Turbo and GPT-4o do not benefit from full test scenarios. This agrees with the observation above in that GPT-4 might be better at incorporating more contextual feedback and the smaller GPT-4o model.

\paragraph{Recall of Open-Source Models}
Among open-source LLMs, DeepSeek-Coder-V2-Lite-Instruct exhibited the highest Recall when utilizing either no verbal feedback or novice-level feedback. However, with expert-level verbal feedback, DeepSeek-Coder-6.7B-Instruct demonstrated superior Recall, which was comparable to, or in some cases (CF + SEF), even surpassed that of GPT-4o. Considering that the initial performance of DeepSeek-Coder-6.7B-Instruct was about 15\%p lower than that of GPT-4o, this underscores two points: 1) the steering ability of open-source LLMs may approach that of closed-source LLMs, and 2) it is challenging to infer the ultimate performance after iterations from single turn performance.
\end{comment}

% \yh{point out which figure}

% \paragraph{MRR}
% Table~\ref{tab:convcodebench_mrr} demonstrates that among closed-source LLMs, GPT-4o exhibits the highest MRR across all feedback combinations, indicating that it generally requires the fewest iterations. Open-source LLMs displayed varying trends depending on the type of feedback. Specifically, when user feedback was not utilized or when only novice feedback was applied, DeepSeek-Coder-V2-Lite-Instruct generally performed the best. In contrast, when expert feedback was incorporated, CodeQwen1.5-7B-Chat and ReflectionCoder-DS-6.7B achieved superior MRR performance.


% \paragraph{Recall}
% Table~\ref{tab:convcodebench_recall} indicates that, unlike the MRR results, GPT-4 achieved the highest Recall scores. This suggests that when provided with sufficient iterations, GPT-4 is more likely to utilize feedback effectively to generate correct code. 



% \input{table/static_deepseek}
% \input{table/static_deepseek_cumul}
% \input{fig/convcodebench_mrr_rank_correlation_deepseek}

% \input{table/static_codellama_cumul}







\input{fig/convcodebench_mrr_rank_correlation_codellama}


\subsection{Results on \oursstatic}
\label{sec:static_results}
While \ours provides valuable insights into interactive code generation across various feedback combinations, \oursstatic offers a faster, cheaper, and more reproducible alternative. 
As discussed in \S\ref{sec:convcodebench}, we chose CodeLlama-7B-Instruct as the reference model, and excluded scenarios without verbal feedback, as they do not require LLM intervention. 
Additionally, $\langle f_c, \phi, f_v \rangle$ scenario was omitted as CodeLlama-7B-Instruct achieved a 100\% compilation success rate in the initial generation, eliminating the need for novice-level verbal feedback on compilation.

% see Appendix~\ref{appendix:convcodebench} for \oursstatic results with different reference models.

\paragraph{\oursstatic as a Reliable Proxy }
We conducted a comparative analysis of \oursstatic and \ours to validate \oursstatic as a proxy, comparing the MRR (Figure~\ref{fig:static_mrr_rank_correlation_codellama}) and Recall (Appendix~\ref{appendix:static_recall_corr_codellama}) results across target models and feedback combinations 
Spearman’s rank correlations ranged from 0.82–0.99, indicating that \oursstatic is a reliable, efficient, and cost-effective proxy for \ours.
  % for Recall correlations (0.82-0.91) between \oursstatic and \ours.


Additionally, Table~\ref{tab:convcodebench_static_codellama} presents the results on \oursstatic, showing that MRR ranking trends closely aligned with \ours (Table~\ref{tab:convcodebench_mrr}), with  minor deviations.
While absolute recall and MRR scores are slightly lower compared to \ours, the rankings amongst models remained roughly consistent between \oursstatic and \ours. Based on approximately consistent rankings across \ours and \oursstatic, \textbf{we recommend code LLMs use \oursstatic as a solid alternative to compare against other baselines.}


% where MRR trends are consistent across different feedback combinations. 
% Among closed-source language models, GPT-4o consistently achieved the highest MRR and Recall scores, followed by GPT-4-Turbo and GPT-4. In the open-source category, DeepSeek-Coder-V2-Lite-Instruct exhibited the best performance, with CodeQwen1.5-7B-Chat and ReflectionCoder-DS-6.7B following closely.
% Meanwhile, in Recall, 
% in the open-source domain, DeepSeek-Coder-V2-Lite-Instruct consistently excelled Recall, followed by CodeQwen1.5-7B-Chat and ReflectionCoder-DS-6.7B under novice-level feedback conditions. 
% When expert-level verbal feedback was provided, DeepSeek-Coder-6.7B-Instruct and CodeQwen1.5-7B-Chat exhibited comparable performance. Notably, when only compilation and expert-level verbal feedback were provided, DeepSeek-Coder-6.7B-Instruct outperformed all other open-source models, consistent with the results observed in \ours. 



\input{table/static_codellama}


