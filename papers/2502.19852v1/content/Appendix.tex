\clearpage

\section{{DeekSeek-R1-Distill Results on \ours}}
\label{appendix:r1_results}
\input{table/live_r1}
\input{table/live_recall_r1}

\begin{table}[h!]
\centering
\caption{{Hyperparameter tuning results of DeepSeek-R1-Distill-Qwen-32B on BigCodeBench-Hard-Instruct.}}
% \small
\scriptsize
\begin{tabular}{cccc}\thickhline
 & \textbf{Temperature} & \textbf{Max Token Length} & \textbf{Pass@1} \\\hline
Reported & - & - & 43.9 \\\hdashline
& 0.0 & 8K & 43.7 \\
& 0.2 & 8K & 44.6 \\ 
Reproduced & 0.2 & 16K & \textbf{45.9} \\ 
& 1.0 & 8K & 44.6 \\ 
& 1.0 & 16K & 45.1 \\\thickhline
\end{tabular}
\normalsize
\label{tab:r1_distill_hyperparameters}
\end{table}

{We present the \ours results for the DeepSeek-R1-Distill~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} models. These models, which are trained to handle more complex reasoning, required the following hyperparameter adjustments for inference: 
\begin{itemize}
    \item \textbf{Increased Token Length}: The maximum token length was increased from 8K to 16K (see Table~\ref{tab:r1_distill_hyperparameters}) to support longer reasoning steps.
    \item \textbf{Temperature Adjustment}: The temperature was changed from 0.0 to 0.2. The 0.0 setting resulted in degeneration, causing repetitive sentences in reasoning. We also experimented with a temperature of 1.0, as o1 models only support this value,\footnote{\href{https://community.openai.com/t/why-is-the-temperature-and-top-p-of-o1-models-fixed-to-1-not-0/938922}{\texttt{https://community.openai.com/t/why-is-the-temperature-and-top-p-of-o1-\\models-fixed-to-1-not-0/938922}}} but 0.2 provided the best performance.
\end{itemize}}

{Tables~\ref{tab:convcodebench_mrr_r1} and~\ref{tab:convcodebench_recall_r1} extend the results presented in Tables~\ref{tab:convcodebench_mrr} and~\ref{tab:convcodebench_recall}, including two DeepSeek-R1-Distill models (Llama-70B and Qwen-32B) and their base models (Llama-3.3-70B-Instruct and Qwen2.5-32B).
We summarize the key observations on the impact of R1-Distillation:
\begin{itemize}
    \item \textbf{Lack of Significant Improvement}: R1-Distilled models do not demonstrate a significant improvement over other models.
    \item \textbf{Reduced Expert Feedback Utilization}: R1-Distilled models face challenges in effectively utilizing expert feedback over their base models. 
    \item \textbf{DeepSeek-R1-Distill-Llama-70B vs. Llama-3.3-70B-Instruct}: 
        \begin{itemize}
            \item \textbf{MRR}: R1-Distillation results in a decrease in MRR performance.
            \item \textbf{Recall}: R1-Distillation generally improves the utilization of execution and novice feedback but hurts expert feedback.
        \end{itemize}
    \item \textbf{DeepSeek-R1-Distill-Qwen-32B vs. Qwen2.5-32B}: 
        \begin{itemize}
            \item \textbf{MRR}: R1-Distillation improves the utilization of execution and novice feedback but slightly hurts expert feedback.
            \item \textbf{Recall}: R1-Distillation improves feedback utilization across all types of feedback in most feedback combinations.
        \end{itemize}
\end{itemize}}





\section{{Distinction of \ours}}
\label{appendix:our_distinction}
{We elaborate distinctive implications from existing works such as InterCode~\cite{yang2023intercode} and MINT~\cite{wang2024mint}: 
\begin{itemize}
    \item Comparative analyses of partial to full test coverage in execution feedback enables to evaluate both:
        \begin{itemize}
            \item \textbf{Test generalization:} A model's ability to produce code that passes full tests even when only partial tests are provided.
            \item \textbf{Test utilization:} A model's capability to leverage given test results for code refinement.
        \end{itemize}
    InterCode----which uses full test only----evaluates test utilization only, and
    MINT----which uses partial test only----provides an entangled evaluation of test generalization and test utilization. In contrast, \ours, by providing both partial and full test, enables \textbf{isolated evaluation of each test generalization and test utilization} as we illustrate below. 
    For instance, in Table~\ref{tab:convcodebench_recall}, DeepSeek-Coder-6.7B-Instruct shows modest test generalization ($\langle f_c, \phi, \phi \rangle \rightarrow \langle f_c, f_e, \phi \rangle$: 35.2 $\rightarrow$ 37.7), but shows limited test utilization ($\langle f_c, f_e, \phi \rangle \rightarrow \langle f_c, f_e^*, \phi \rangle$: 37.7 $\rightarrow$ 37.5). Meanwhile, Llama-3.1-8B-Instruct exhibits capabilities in both test generalization ($\langle f_c, \phi, \phi \rangle \rightarrow \langle f_c, f_e, \phi \rangle$: 31.8 $\rightarrow$ 38.4) and test utilization ($\langle f_c, f_e, \phi \rangle \rightarrow \langle f_c, f_e^*, \phi \rangle$: 38.4 $\rightarrow$ 40.0). 
    \item \ours simulates an \textbf{``engaged''} user, offering verbalized explanations of test results, as illustrated in Figure~\ref{fig:gpt4_ef_unit_snf_case_study_2}. In contrast, InterCode lacks verbal feedback, and MINT provides only generic feedback for $f_v$----\textit{``Your answer is wrong."} To evaluate how verbalized explanations enhance models' test utilization capabilities, an exemplar scenario in \ours is when both full execution feedback and novice feedback are available. In Table~\ref{tab:convcodebench_recall}:
        \begin{itemize}
            \item Without the inclusion of novice feedback ($\langle f_c, f_e^*, \phi \rangle$): Llama-3.1-8B-Instruct's test utilization capabilities (40.0) are weaker compared to CodeQwen1.5-7B-Chat (\textbf{41.1}).
            \item With the inclusion of novice feedback ($\langle f_c, f_e^*, f_v \rangle$): significantly improves Llama-3.1-8B-Instruct's performance, surpassing CodeQwen1.5-7B-Chat (\textbf{51.8} vs. 49.5).
        \end{itemize}
    \item Covering comprehensive combinations of feedback types, \ours analyzes previously underexplored cases, such as:
        \begin{itemize}
            \item Full execution feedback vs. partial execution and novice feedback
            \item Partial execution and expert feedback vs. full execution and expert feedback
            \item Full execution and novice feedback vs. expert feedback
        \end{itemize}
    \item Cost-effective static benchmark (\oursstatic): \oursstatic correlates strongly with online evaluation while reducing costs. Neither MINT nor InterCode provide such a static benchmark.
\end{itemize}
}



\section{Verbal Feedback}
\label{appendix:verbal_feedback}
\subsection{Discussion on Employing LLMs for Verbal Feedback Generation}
A key challenge in creating \ours is generating verbal feedback. 
Human annotation is both impractical and inconsistent (\S\ref{convcodeworld:reproducibility}), which led us to employ GPT-4o for this task. 
While GPT-4o may not fully replicate the nuances of human feedback, it ensures reproducibility and affordability, both critical for maintaining consistency across benchmark evaluations.
As demonstrated by direct comparisons between LLM-generated and human feedback in prior studies \citep{wang2024mint}, we find this method sufficiently effective for our benchmarking purposes.

\subsection{Cost-Efficiency of \ours Compared to Human Annotation}
\label{appendix:efficiency}
In the worst-case scenario, CodeLlama-7B-Instruct, which requested the most verbal feedback due to its low performance, incurred a total cost of \$215 (26.4M input tokens and 5.5M output tokens) for 15,905 turns using GPT-4o-2024-05-13 pricing (\$5/1M input tokens and \$15/1M output tokens). By comparison, assuming human annotation takes 96 seconds per turn~\citep{wang2024mint} and the average U.S. private non-farmer worker's hourly wage is \$35.04 according to \cite{uswage}, the human annotation cost would be approximately \$14,792. 

\subsection{{Human Evaluation of Generated Verbal Feedback}}
\begin{table}[h!]
\centering
\caption{Human evaluation of simulated expert-level user feedback by GPT-4o and real user feedback by ShareGPT.}
% \small
\scriptsize
\begin{tabular}{lccc}\thickhline
\textbf{Expert Feedback by} & \textbf{Is Helpful} & \textbf{Is Human-Expert-Like} \\ \hline
ShareGPT & 35\% & 30\% \\
\ours & 55\% & 25\% \\
\thickhline
\end{tabular}
\normalsize
\label{tab:human_evaluation}
\end{table}

{We conducted human evaluation to validate the realism of simulated expert-level user feedback, noting that in-context examples might lead to unrealistic responses. 
Specifically, two human evaluators rated randomly assigned feedback samples from either real user feedback from ShareGPT~\footnote{\href{https://huggingface.co/datasets/anon8231489123/ShareGPT\_Vicuna\_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json}{\small \texttt{https://huggingface.co/datasets/anon8231489123/ShareGPT\_Vicuna\_unfilter\\ed/blob/main/ShareGPT\_V3\_unfiltered\_cleaned\_split\_no\_imsorry.json}}} logs or expert feedback generated by \ours using GPT-4o (see Figure~\ref{fig:expert_human_eval} for the annotation platform). As shown in Table~\ref{tab:human_evaluation}, our generated feedback was found to be comparable to authentic logs in terms of expert-human-likeness and was rated higher for helpfulness, consistent with MINT's findings.}

\subsection{Possible Reasons for the Observed ``Struggle to Utilize Feedback''}
\label{appendix:struggle_to_utilize_feedback}
{From Section~\ref{exp:5.2.1}, we further discuss two possible reasons for models when they struggle to utilize complex feedback:
\begin{itemize}
    \item \textbf{Limited Model Size}: Smaller models, such as ReflectionCoder-DS-6.7B, may lack the capacity to process and integrate complex information effectively, which could limit performance improvements even when execution feedback is included ($35.2 \rightarrow 37.7$).
    In contrast, their bigger versions like ReflectionCoder-DS-33B demonstrated performance gains with execution feedback ($41.6 \rightarrow 45.3$).
    Mixed feedback types may distract small models further. When comparing Expert feedback only vs. Expert feedback + execution feedback. For Qwen1.5-Chat, the 72B model's performance improved with execution feedback, while the 32B model's performance deteriorated, which suggests that smaller models might become distracted when faced with multiple feedback signals simultaneously~\citep{liu-etal-2024-lost}. However, this distraction may be mitigated with well-designed training data, as even smaller models like Llama-3.1-8B-Instruct show improvements when provided with more execution feedback.
    \item \textbf{Limited Generalization Training}: ReflectionCoder models were trained on a specific feedback combination, $\langle f_c, f_e^*, f_v \rangle$, limiting their adaptability to other feedback types (Section~\ref{exp:5.2.3}).
    For example, with expert feedback, ReflectionCoder-DS-33B scores lower (81.4) than its base model DeepSeekCoder-33B-Instruct (85.4).
\end{itemize}}







% As annotating verbal feedback by human is impractical and might be inconsistent (\S\ref{convcodeworld:reproducibility}), 
% we employ GPT-4o to produce this verbal feedback. 
% While we acknowledge that while it may not perfectly replicate human feedback, it serves as a practical and reasonable approximation, offering the advantages of reproducibility and affordability which are essential for consistent benchmark evaluations. 
% As supported by direct comparison between LLM-generated feedback and actual feedback from existing research\citep{wang2024mint}, 
% we consider this method sufficient for the purposes of our benchmark. 
% Although we have not conducted direct comparisons between GPT-4o-generated feedback and actual human feedback, 

\subsection{Analysis of Ground Truth Code Leakage in Generated Expert-Level Verbal Feedback}
\label{appendix:gt_code_leakage}
\begin{table}[h]
% \begin{wraptable}{r}{0.46\textwidth}
\centering
\caption{Pass@$1$ results of various LLMs with expert-level verbal feedback $f_v^*$ generated by GPT-4o compared to direct ground truth code feedback. The total number of turns $n=1$. 
% turn 0 (i.e. the initial code generation without feedback) and at turn 1 on \ours among single-turn generation without any feedback, $\Omega = \langle f_c, \phi, f_v^* \rangle$, and directly providing the ground truth code as feedback. In the latter two settings, max turn $n=1$. 
For each column, bold and underscore indicate $1$st and $2$nd place performance while keeping the code generation model fixed.
}
\scriptsize
\begin{tabular}{lccc}\thickhline
% \multirow{2}{*}{Feedback} & \multicolumn{3}{c}{\textbf{Pass@$1$}} \\
% & \textbf{w/o Feedback} & \textbf{$\Omega = \langle f_c, \phi, f_v^* \rangle$} & \textbf{Ground Truth Code as Feedback} \\ \hline
\multirow{2}{*}{Feedback}  & \multicolumn{3}{c}{Code Generation}  \\ 
& GPT-4-0613 & GPT-4-Turbo-2024-04-09 & GPT-4o-2024-05-13 \\\hline
\textbf{w/o Feedback} & 46.0 & 48.0 & 50.8 \\\hdashline
\textbf{+ Expert-Level Verbal Feedback} & \underline{70.0} & \underline{69.0} & \underline{68.5} \\
\textbf{+ Ground Truth Code} & \textbf{97.9} & \textbf{88.2} & \textbf{79.7} \\ 
% & \textbf{$\langle \phi, \phi, \phi \rangle$} & \textbf{$\langle f_c, \phi, f_v^* \rangle$} & \textbf{$\langle f_c, \phi, f_{gt}^* \rangle$} \\ \hline
% GPT-4-0613 & 46.0 & 70.0 & 97.9 \\
% GPT-4-Turbo-2024-04-09 & 48.0 & 69.0 & 88.2 \\
% GPT-4o-2024-05-13 & 50.8 & 68.5 & 79.7 \\
\thickhline
\end{tabular}
\normalsize
\label{tab:cheating}
\end{table}
% \end{wraptable}

\begin{table}[h!]
\centering
\caption{Ground truth code leakage ratio (\%) by incorporating different models for expert-level verbal feedback generation. The lower the better. }
% \small
\scriptsize
\begin{tabular}{lcc}\thickhline
\multirow{2}{*}{$f_v^*$ Generation} & Mentioning & Including \\ 
& \texttt{ground\_truth\_code} ($\downarrow$) & Refined Code ($\downarrow$)\\ \hline
GPT-4-0613 & 51.1 & 0.0 \\
GPT-4-Turbo-2024-04-09 & 31.4 & 0.0 \\ 
\rowcolor{gray!10}
GPT-4o-2024-05-13 &\phantom{0}2.5 & 0.1 \\\thickhline
\end{tabular}
\normalsize
\label{tab:gt_code_leak}
\end{table}

The generation of expert-level verbal feedback $f_v^*$ involves comparing the generated code with the ground truth code to provide modification suggestions. This process could raise concerns about potential code leakage. 
As shown in Table~\ref{tab:cheating}, providing the ground truth code significantly outperforms providing $f_v^*$, empirically confirming that $f_v^*$ is unlikely to be a direct copy of the ground truth code.

{To detect leakage, we use a \textbf{canary sequence} approach, commonly used to test for training data or prompt leakage in LLMs ~\citep{team2024gemini,openai2023gpt4,greshake2023not,perez2022ignore,agarwal-etal-2024-prompt}. 
Specifically, we consider leakage if the feedback simulator includes a canary sequence within the feedback. This sequence contains the term \textit{ground truth code}, which is given in the prompt (see Figure~~\ref{fig:expert_prompt}). 
As shown in Table~\ref{tab:gt_code_leak}, leakage rates are estimated by how often a model references the ground truth code in $f_v^*$. 
For example, a leakage might be detected if the feedback contains phrases such as, \textit{``Unlike the ground truth code, the current code omits exception handling of DivideByZero..."} (see Figures~\ref{fig:desirable} and~\ref{fig:undesirable} for comparisons of desirable vs. leaked cases).}

{Notably, GPT-4o shows the lowest leakage rate at 2.5\%, indicating its ability to generate $f_v^*$ with minimal leakage. 
This suggests that when $f_v^*$ generated by GPT-4o is provided, the observed performance improvement is not driven by exposure to the correct code. }

% Furthermore, Table~\ref{tab:gt_code_leak} estimates leakage rates, based on how often a model referenced ground truth code in $f_v^*$ (e.g., \textit{"Unlike the ground truth code, the current code omits exception handling of DivideByZero...", etc.}; see Figures~\ref{fig:desirable} and~\ref{fig:undesirable} to compare the desirable and leaked cases), with GPT-4o showing the lowest at 2.5\%, indicating its ability to generate $f_v^*$ with minimal leakage. 
% This suggests that, when $f_v^*$ generated by GPT-4o is provided, the performance improvement is not driven by exposure to correct code. 

\subsection{Comparative Analysis of Verbal Feedback Across Different LLMs}
\label{appendix:simulator_comparison}
\begin{table}[h!]
\centering
\caption{Pass@$1$ results over different model combinations of expert-level verbal feedback $f_v^*$ generation and code generation on \ourslive  where $\Omega = \langle f_c, \phi, f_v^* \rangle$ and the total number of turns $n=1$. 
{Each row represents a model used to provide verbal feedback.
Each column represents a model that utilizes this feedback to refine code.
}
For each column, bold and underscore indicate $1$st and $2$nd place performance while keeping the code generation model fixed.}
% \small
\scriptsize
\begin{tabular}{lcccc}\thickhline
\multirow{2}{*}{$f_v^*$ Generation} & \multicolumn{3}{c}{Code Generation}  \\ 
& GPT-4-0613 & GPT-4-Turbo-2024-04-09 & GPT-4o-2024-05-13 \\\hline
% GPT-4-0613 & \cellcolor[HTML]{86CEAB} \underline{65.1} & \cellcolor[HTML]{DDF1E8} \underline{61.4} & \cellcolor[HTML]{AEDFC7} \underline{63.4} \\
% GPT-4-Turbo-2024-04-09 & \cellcolor[HTML]{B9E4CF} 62.9 &  59.9 & \cellcolor[HTML]{C3E7D5} 62.5 \\ 
% GPT-4o-2024-05-13 & \cellcolor[HTML]{56BC8A} \textbf{67.1} & \cellcolor[HTML]{7FCCA7} \textbf{65.4} & \cellcolor[HTML]{9CD7BA} \textbf{64.2} \\\thickhline
w/o Feedback & 46.0 & 48.0 & 50.8 \\\hdashline
GPT-4-0613 & \underline{65.1} &  \underline{61.4} &  \underline{63.4} \\
GPT-4-Turbo-2024-04-09 &  62.9 &  59.9 &  62.5 \\ 
% \rowcolor{gray!10}
GPT-4o-2024-05-13 &  \textbf{67.1} &  \textbf{65.4} & \textbf{64.2} \\
\thickhline
\end{tabular}
\normalsize
\label{tab:plug_and_play}
\end{table}

% \begin{figure}[h!]
%     \centering  
%     \includegraphics[width=0.7\linewidth]{fig/plug_and_play.pdf}
%     \caption{Pass@$1$ results over different combination of code generation and expert-level verbal feedback $f_v^*$ generation on \ourslive  where $\Omega = \langle f_c, \phi, f_v^* \rangle$ and the total number of turns $n=1$. The greener the color, the higher the performance.}
%     \label{fig:plug_and_play}
% \end{figure}

In our main experiments, we utilized GPT-4o for verbal feedback generation and investigated its performance in comparison to other models.
To see the effect of using other LLMs for verbal feedback generation, we conducted a single iteration of code generation using three closed-source LLMs as both code generators and expert-level verbal feedback generators, examining the Pass@$1$ performance. {Table~\ref{tab:plug_and_play} evaluates different models as potential verbal feedback simulators. The effectiveness of the feedback provided by each simulator is assessed by comparing the performance across columns, showing consistent superior performance when employing GPT-4o for feedback generation. }



% \section{Approximate Recall}
% \input{table/static_deepseek_cumul}
% Table~\ref{tab:convcodebench_cumul_pass_gain_static} delineates the performance of both closed-source and open-source models in terms of C-Recall. GPT-4o and GPT-4 perform very close to each other. In the open-source domain, DeepSeek-Coder-V2-Lite-Instruct consistently excelled, followed by CodeQwen1.5-7B-Chat and ReflectionCoder-DS-6.7B under novice feedback conditions. When expert feedback was provided, DeepSeek-Coder-6.7B-Instruct and CodeQwen1.5-7B-Chat exhibited comparable performance. Notably, when only compilation and expert feedback were provided, DeepSeek-Coder-6.7B-Instruct outperformed all other open-source models, consistent with the results observed in \ours.

% \section{Implementation Detail of \ours}
% \label{appendix:impl}

\section{Verbal Feedback by Open-Source LLMs}
\begin{table}[h!]
\centering
\caption{Pass@$1$ results over different model combinations of expert-level verbal feedback $f_v^*$ generation and code generation on \ourslive  where $\Omega = \langle f_c, \phi, f_v^* \rangle$ and the total number of turns $n=1$. For each column, bold and underscore indicate $1$st and $2$nd place performance while keeping the code generation model fixed. }
% \small
\scriptsize
\begin{tabular}{lcccc}\thickhline
% \multirow{2}{*}{Code Generation} & \multicolumn{3}{c}{$f_v^*$ Generation}  \\ 
\multirow{2}{*}{$f_v^*$ Generation} & \multicolumn{2}{c}{Code Generation}  \\ 
& GPT-4o-2024-05-13 & {Llama-3.1-70B-Instruct} \\\hline
% GPT-4-0613 & \cellcolor[HTML]{86CEAB} \underline{65.1} & \cellcolor[HTML]{DDF1E8} \underline{61.4} & \cellcolor[HTML]{AEDFC7} \underline{63.4} \\
% GPT-4-Turbo-2024-04-09 & \cellcolor[HTML]{B9E4CF} 62.9 &  59.9 & \cellcolor[HTML]{C3E7D5} 62.5 \\ 
% GPT-4o-2024-05-13 & \cellcolor[HTML]{56BC8A} \textbf{67.1} & \cellcolor[HTML]{7FCCA7} \textbf{65.4} & \cellcolor[HTML]{9CD7BA} \textbf{64.2} \\\thickhline
w/o Feedback & 50.8 & 45.4 \\\hdashline
GPT-4o-2024-05-13 & \underline{64.2} & {\textbf{65.1}} \\
{Llama-3.1-70B-Instruct} & {\textbf{65.8}} & {\underline{62.1}} \\
\thickhline
\end{tabular}
\normalsize
\label{tab:verbal_feedback_by_llama}
\end{table}

% {In Table~\ref{tab:verbal_feedback_by_llama}, Llama-3.1-70B-Instruct serves comparable performance to GPT-4o, confirming the feasibility of using open-source models as verbal feedback simulators.}

{Table~\ref{tab:verbal_feedback_by_llama} supports the feasibility of using Llama-3.1-70B-Instruct as a verbal feedback simulator, replacing GPT-4o-2024-05-13.}

 
\section{Implementation Details}
\label{appendix:impl}
\begin{table}[h]
\centering
\caption{Pass@$1$ results over different implementation for initial code generation without feedback. \ours chose Direct Generation by BigCodeBench implementation, which showed the highest performance. For each column, bold and underscore indicate $1$st and $2$nd place performance while keeping the code generation model fixed.}
% \small
\scriptsize
\begin{tabular}{lccc}\thickhline
Implementation & DeepSeek-Coder-6.7B-Instruct & GPT-4o-2024-05-13\\\hline
\multicolumn{3}{c}{\textbf{w/o Feedback ($\Omega = \langle \phi, \phi, \phi \rangle$)}}  \\
Reported & 35.5 & 51.1 \\\hdashline
\rowcolor{gray!10}
Direct Generation (BigCodeBench impl.) & \textbf{35.2} & \textbf{50.8} \\
\texttt{DSPy.Predict} & \underline{33.6} & \phantom{0}1.8 \\
\texttt{DSPy.ChainOfThought} & 20.2 & \underline{49.3} \\\hdashline
\multicolumn{3}{c}{\textbf{Compilation Feedback only} ($\Omega = \langle f_c, \phi, \phi \rangle$; $n=1$)} \\
\rowcolor{gray!10}
Direct Generation (BigCodeBench impl.) & \textbf{35.2} & \textbf{50.8} \\
\texttt{DSPy.Predict} & \underline{33.7} & 50.1 \\
\texttt{DSPy.ChainOfThought} & 32.8 & \underline{50.5} \\
\thickhline
\end{tabular}
\normalsize
\label{tab:init_gen}
\end{table}

We utilize DSPy~\citep{khattab2024dspy}\footnote{\texttt{\href{https://github.com/stanfordnlp/dspy}{https://github.com/stanfordnlp/dspy}}} manage the interactive code generation flow for \ours and \oursstatic. 
For both code and verbal feedback generation follow DSPy's default prompt format, incorporating ChaingOfThought (CoT)~\citep{wei2022chain} reasoning by \texttt{DSPy.ChainOfThought} function. 
The exception is initial code generation, where we adopt BigCodeBench's~\citep{zhuo2024bigcodebench} implementation,\footnote{\texttt{\href{https://github.com/bigcode-project/bigcodebench}{https://github.com/bigcode-project/bigcodebench}}} without CoT reasoning.
As shown in Table~\ref{tab:init_gen}, we attribute this choice to the observation that, for initial code generation (without prior feedback), models tend to perform better without additional reasoning steps like CoT (\texttt{DSPy.ChainOfThought}) or prompting (both in \texttt{DSPy.Predict} and \texttt{DSPy.ChainOfThought}).


% we conjecture the reason for this as, for the initial code generation, without previous code and feedback, code generation models generally shows higher performance when additional reasoning steps like CoT (DSPy.ChainOfThought) or prompting (both in DSPy.Predict and DSPy.ChainOfThought) was not involved.  

Hyperparameters are set as follows: 
{We used greedy decoding (temperature = 0) in all experiments, following~\cite{chen2023codet}.}
The total number of turns $n=10$, with a maximum token length of 8K for all code generation models. For models with a lower token limit, we use their respective maximum length. For verbal feedback generation, we use GPT-4o-2024-05-13 with a token limit of 2K.
Regarding the partial test coverage of execution feedback, we utilize the first three test cases, {which aligns with benchmarks like HumanEval \citep{codex} and CodeContests~\citep{li2022competition} providing up to three public test cases.}



% Regarding the hyperparameters, throughout the evaluation, we set the total number of turns $n$ as 10. 
% The maximum token length for all code generation models is set to 8K. For models with a smaller maximum length, we use their respective maximum token length. 
% For verbal feedback generation, we use GPT-4o with the maximum token length as 2K. 
%, which demonstrated the highest utility and the lowest ground truth code leakage in expert-level feedback simulations.\footnote{See Appendix~\ref{appendix:verbal_feedback} for details.}


\clearpage
\input{fig/convcodebench_live_pass_at_1}
\input{fig/convcodebench_live_pass_at_1_per_model}



\clearpage
\input{table/static_deepseek}
\input{table/static_gpt4}
% \input{table/static_recall}

\input{fig/convcodebench_cumul_rank_correlation_codellama}
\input{fig/convcodebench_mrr_rank_correlation_deepseek}
\input{fig/convcodebench_cumul_rank_correlation_deepseek}
\input{fig/convcodebench_mrr_rank_correlation_gpt4}
\input{fig/convcodebench_cumul_rank_correlation_gpt4}


\clearpage


\begin{figure*}[h!]
\section{In-Context Learning Examples}
\label{appendix:icl_examples}
\subsection{Novice-Level Verbal Feedback}
\label{appendix:novice_icl}
    \centering
    \begin{WideMinted}[fontsize=\small, frame=lines, linenos]
Example Input:
Sort a list of integers in ascending order. The function should take a list of integers and return a sorted list. Ensure that the function handles negative numbers and zeros correctly. Check if the function's output is a sorted list.
```python
>>> sorted_list = sort_func([3, -1, 0, 5, -10, 2])
>>> sorted_list
[-10, -1, 0, 2, 3, 5]
```
You should write self-contained code starting with:
```python
def sort_func(int_list):
```

Ground Truth Code:
```python
def sort_func(int_list):
    return sorted(int_list)
```

Previous Code:
```python
def sort_func(int_list):
    for i in range(len(int_list)):
        for j in range(len(int_list) - 1):
            if int_list[j] < int_list[j + 1]:
                int_list[j], int_list[j + 1] = int_list[j + 1], int_list[j]
   return int_list

test_list = [3, -1, 0, 5, -10, 2]
print(sort_func(test_list))
```

Compilation Feedback:
Traceback (most recent call last):
  File "tmp.py", line 6
    return int_list
                   ^
IndentationError: unindent does not match any outer indentation level

Reasoning:
Let's think step by step in order to produce the novice-level `user_feedback`. 

User Feedback:
I got this error: 
```
Traceback (most recent call last):
  File "tmp.py", line 6
    return int_list
                   ^
IndentationError: unindent does not match any outer indentation level
```
I think there might be something wrong with the indentation of the `return int_list` line. Also, I think there exists a simpler way to sort a list in Python. 
    \end{WideMinted}
    \caption{The in-context example of $f_v$ generation where $\Omega = \langle f_c, \phi, f_v \rangle$.}
    \label{fig:novice_ICL}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{WideMinted}[fontsize=\small, frame=lines, linenos]
Example Input:
Sort a list of integers in ascending order. The function should take a list of integers and return a sorted list. Ensure that the function handles negative numbers and zeros correctly. Check if the function's output is a sorted list.
```python
>>> sorted_list = sort_func([3, -1, 0, 5, -10, 2])
>>> sorted_list
[-10, -1, 0, 2, 3, 5]
```
You should write self-contained code starting with:
```python
def sort_func(int_list):
```

Previous Code:
```python
def sort_func(int_list):
    for i in range(len(int_list)):
        for j in range(len(int_list) - 1):
            if int_list[j] < int_list[j + 1]:
                int_list[j], int_list[j + 1] = int_list[j + 1], int_list[j]
    return int_list

test_list = [3, -1, 0, 5, -10, 2]
print(sort_func(test_list))
```

Compilation Feedback:
No syntax errors

Execution Feedback:
TEST_CASE_1
Traceback (most recent call last):
  File "__test__.py", line 78, in test_case_1
AssertionError: sort_func([3, -1, 0, 5, -10, 2]) != [-10, -1, 0, 2, 3, 5]

Reasoning:
Let's think step by step in order to produce the novice-level `user_feedback`.

User Feedback:
I got this error: 
```
Traceback (most recent call last):
  File "__test__.py", line 78, in test_case_1
AssertionError: sort_func([3, -1, 0, 5, -10, 2]) != [-10, -1, 0, 2, 3, 5]
```
It seems like the function didn't sort the list correctly. The list is in descending order instead of ascending order. Also, I think there exists a simpler way to sort a list in Python. 
    \end{WideMinted}
    \caption{The in-context example of $f_v$ generation where $\Omega = \langle f_c, f_e[^*], f_v \rangle$.}
    \label{fig:ef_vovice_ICL}
\end{figure*}



\begin{figure*}[h!]
\subsection{Expert-Level Verbal Feedback}
\label{appendix:expert_icl}
    \centering
    \begin{WideMinted}[fontsize=\scriptsize, frame=lines, linenos]
Example Input:
Sort a list of integers in ascending order. The function should take a list of integers and return a sorted list. Ensure that the function handles negative numbers and zeros correctly. Check if the function's output is a sorted list.
```python
>>> sorted_list = sort_func([3, -1, 0, 5, -10, 2])
>>> sorted_list
[-10, -1, 0, 2, 3, 5]
```
You should write self-contained code starting with:
```python
def sort_func(int_list):
```

Ground Truth Code:
```python
def sort_func(int_list):
    return sorted(int_list)
```

Previous Code:
```python
def sort_func(int_list):
    for i in range(len(int_list)):
        for j in range(len(int_list) - 1):
            if int_list[j] < int_list[j + 1]:
                int_list[j], int_list[j + 1] = int_list[j + 1], int_list[j]
    return int_list

test_list = [3, -1, 0, 5, -10, 2]
print(sort_func(test_list))
```

Reasoning:
Let's think step by step in order to produce the user_feedback. We need to compare the `previous_code` with the `ground_truth_code` and identify specific areas where the `previous_code` deviates from the expected behavior. We should focus on the sorting algorithm, handling of edge cases like negative numbers and zeros, and the overall performance of the code.

User Feedback:
1. **Sorting Algorithm**: The `previous_code` implements a bubble sort algorithm, but it sorts the list in descending order due to the condition `if int_list[j] < int_list[j + 1]`. The correct approach should use `if int_list[j] > int_list[j + 1]` to sort in ascending order.

2. **Edge Case Handling**: While the `previous_code` attempts to handle all integers, it incorrectly sorts the list in the reverse order. Ensure that the sorting condition correctly compares elements to achieve ascending order.

3. **Code Simplicity and Readability**: The bubble sort implementation in the `previous_code` is more complex and less readable compared to using the built-in `sorted()` function. Simplifying the code to use `sorted()` will enhance readability and maintainability.

4. **Performance**: The bubble sort algorithm used in the `previous_code` has a time complexity of O(n^2), which can be inefficient for large lists. The `sorted()` function, on the other hand, uses Timsort, which has a time complexity of O(n log n) and is more suitable for sorting large datasets.

5. **Test Case Verification**: The `previous_code` includes a print statement to verify the function's output. While this is useful for debugging, consider using assertions or test functions to automate the verification process in a more scalable way.

Overall, the `previous_code` functions incorrectly by sorting the list in descending order. Using the correct comparison in the bubble sort condition or utilizing Python's built-in sorting capabilities would improve the function's correctness and performance.

    \end{WideMinted}
    \caption{The in-context example of $f_v^*$ generation where $\Omega = \langle f_c, \phi, f_v^* \rangle$.}
    \label{fig:expert_ICL}
\end{figure*}




\begin{figure*}[h!]
\vspace{-1cm}
    \centering
    \begin{WideMinted}[fontsize=\scriptsize, frame=lines, linenos]
Example Input:
Sort a list of integers in ascending order. The function should take a list of integers and return a sorted list. Ensure that the function handles negative numbers and zeros correctly. Check if the function's output is a sorted list.
```python
>>> sorted_list = sort_func([3, -1, 0, 5, -10, 2])
>>> sorted_list
[-10, -1, 0, 2, 3, 5]
```
You should write self-contained code starting with:
```python
def sort_func(int_list):
```

Ground Truth Code:
```python
def sort_func(int_list):
    return sorted(int_list)
```

Previous Code:
```python
def sort_func(int_list):
    for i in range(len(int_list)):
        for j in range(len(int_list) - 1):
            if int_list[j] < int_list[j + 1]:
                int_list[j], int_list[j + 1] = int_list[j + 1], int_list[j]
    return int_list

test_list = [3, -1, 0, 5, -10, 2]
print(sort_func(test_list))
```

Execution Feedback:
TEST_CASE_1
Traceback (most recent call last):
  File "__test__.py", line 78, in test_case_1
AssertionError: sort_func([3, -1, 0, 5, -10, 2]) != [-10, -1, 0, 2, 3, 5]

Reasoning:
Let's think step by step in order to produce the user_feedback. We need to compare the `previous_code` with the `ground_truth_code` and identify specific areas where the `previous_code` deviates from the expected behavior. We should focus on the sorting algorithm, handling of edge cases like negative numbers and zeros, and the overall performance of the code.

User Feedback:
1. **Sorting Algorithm**: The `previous_code` implements a bubble sort algorithm, but it sorts the list in descending order due to the condition `if int_list[j] < int_list[j + 1]`. The correct approach should use `if int_list[j] > int_list[j + 1]` to sort in ascending order.

2. **Edge Case Handling**: While the `previous_code` attempts to handle all integers, it incorrectly sorts the list in the reverse order. Ensure that the sorting condition correctly compares elements to achieve ascending order.

3. **Code Simplicity and Readability**: The bubble sort implementation in the `previous_code` is more complex and less readable compared to using the built-in `sorted()` function. Simplifying the code to use `sorted()` will enhance readability and maintainability.

4. **Performance**: The bubble sort algorithm used in the `previous_code` has a time complexity of O(n^2), which can be inefficient for large lists. The `sorted()` function, on the other hand, uses Timsort, which has a time complexity of O(n log n) and is more suitable for sorting large datasets.

5. **Test Case Verification**: The `previous_code` includes a print statement to verify the function's output. While this is useful for debugging, consider using assertions or test functions to automate the verification process in a more scalable way.

Overall, the `previous_code` functions incorrectly by sorting the list in descending order. Using the correct comparison in the bubble sort condition or utilizing Python's built-in sorting capabilities would improve the function's correctness and performance.

    \end{WideMinted}
    \caption{The in-context example of $f_v^*$ generation where $\Omega = \langle f_c, f_e[^*], f_v^* \rangle$.}
    \label{fig:ef_expert_ICL}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \begin{WideMinted}[fontsize=\scriptsize, frame=lines, linenos]
You are given input, previous_code, execution_feedback to simulate user feedback that compares the `previous_code` and the `ground_truth_code`.
Your task is to provide the simulated `user_feedback` that highlights specific areas where the `previous_code` deviates from the `ground_truth_code` and suggests improvements or corrections.
- You SHOULD NOT leak `ground_truth_code` in the simulated user feedback.
- Do not generate updated code.
- Do not reveal that you can access the `ground_truth_code`. Only indirect information is allowed.
    \end{WideMinted}
    \caption{Prompt used for expert feedback generation in the feedback combination $\Omega = \langle f_c, [f_e|f_e^*], f_v^* \rangle$.}
    \label{fig:expert_prompt}
\end{figure*}





\begin{figure*}[h!]
    \section{Case Study}
    \label{appendix:case_study}
    \centering
    \textbf{Input Code}
    \begin{lstlisting}
import smtplib
from email.message import EmailMessage
import getpass
import socket

SERVER_ADDRESS = "localhost"
SERVER_PORT = 25
BUFFER_SIZE = 1024
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587

def task_func(client_socket):
    # Ensure the socket is connected
    if not client_socket:
        print("Client socket is not connected.")
        return

    # Receive the message from the client
    try:
        message = client_socket.recv(BUFFER_SIZE).decode()
    except socket.error as e:
        print(f"Failed to receive message from client: {e}")
        return

    # If the message is empty, do not send the email
    if not message:
        print("Received an empty message. Not sending email.")
        return

    # Ask for the sender's email, recipient's email, and sender's email password for authentication
    sender_email = input("Enter sender's email: ")
    recipient_email = input("Enter recipient's email: ")
    password = getpass.getpass("Enter sender's email password: ")

    # Create the email message
    email_message = EmailMessage()
    email_message.set_content(message)
    email_message["Subject"] = "Message from client"
    email_message["From"] = sender_email
    email_message["To"] = recipient_email

    # Connect to the SMTP server and send the email
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:
            smtp.starttls()
            smtp.login(sender_email, password)
            smtp.send_message(email_message)
            print("Email sent successfully.")
    except Exception as e:
        print(f"Failed to send email: {e}")
\end{lstlisting}

    \textbf{Execution Feedback with Partial Test Coverage ($f_e$)}
    \begin{WideMinted}[fontsize=\tiny, frame=lines, linenos]
TEST_SMTP_SERVER_CONNECTION_ERROR
Traceback (most recent call last):
  File "/notebooks/miniconda/envs/bigcodebench/lib/python3.9/unittest/mock.py", line 1336, in patched
    return func(*newargs, **newkeywargs)
  File "__test__.py", line 125, in test_smtp_server_connection_error
  File "__test__.py", line 39, in task_func
  File "/tmp/utils.py", line 251, in readline
    raise IOError
OSError


TEST_SUCCESSFUL_EMAIL_SEND
Traceback (most recent call last):
  File "/notebooks/miniconda/envs/bigcodebench/lib/python3.9/unittest/mock.py", line 1336, in patched
    return func(*newargs, **newkeywargs)
  File "__test__.py", line 81, in test_successful_email_send
  File "__test__.py", line 39, in task_func
  File "/tmp/utils.py", line 251, in readline
    raise IOError
OSError
    \end{WideMinted}

    \caption{Case study of GPT-4 on \ours ($\Omega =\langle f_c, f_e, f_v \rangle$; $t=7$): Input Code and $f_e$.}
    \label{fig:gpt4_ef_unit_snf_case_study}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \textbf{Novice-Level Verbal Feedback ($f_v$)}    
    \begin{WideMinted}[fontsize=\tiny, frame=lines, linenos]
I got this error:
```
Traceback (most recent call last):
  File "/notebooks/miniconda/envs/bigcodebench/lib/python3.9/unittest/mock.py", line 1336, in patched
    return func(*newargs, **newkeywargs)
  File "__test__.py", line 125, in test_smtp_server_connection_error
  File "__test__.py", line 39, in task_func
  File "/tmp/utils.py", line 251, in readline
    raise IOError
OSError

Traceback (most recent call last):
  File "/notebooks/miniconda/envs/bigcodebench/lib/python3.9/unittest/mock.py", line 1336, in patched
    return func(*newargs, **newkeywargs)
  File "__test__.py", line 81, in test_successful_email_send
  File "__test__.py", line 39, in task_func
  File "/tmp/utils.py", line 251, in readline
    raise IOError
OSError
```
It seems like there is an issue with the socket connection or the way the code is handling the socket. The `OSError` exceptions are being raised during the execution of the `task_func` function.
    \end{WideMinted}

    \textbf{Refined Code by Feedback}    
    \begin{lstlisting}
import smtplib
from email.message import EmailMessage
import getpass
import socket

SERVER_ADDRESS = "localhost"
SERVER_PORT = 25
BUFFER_SIZE = 1024
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587

def task_func(client_socket):
    # Ensure the socket is connected
    if not client_socket:
        print("Client socket is not connected.")
        return

    # Receive the message from the client
    try:
        message = client_socket.recv(BUFFER_SIZE).decode()
    except socket.error as e:
        print(f"Failed to receive message from client: {e}")
        return

    # If the message is empty, do not send the email
    if not message:
        print("Received an empty message. Not sending email.")
        return

    # Ask for the sender's email, recipient's email, and sender's email password for authentication
    sender_email = input("Enter sender's email: ")
    recipient_email = input("Enter recipient's email: ")
    password = getpass.getpass("Enter sender's email password: ")

    # Create the email message
    email_message = EmailMessage()
    email_message.set_content(message)
    email_message["Subject"] = "Message from client"
    email_message["From"] = sender_email
    email_message["To"] = recipient_email

    # Connect to the SMTP server and send the email
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:
            smtp.starttls()
            smtp.login(sender_email, password)
            smtp.send_message(email_message)
            print("Email sent successfully.")
    except Exception as e:
        print(f"Failed to send email: {e}")
    \end{lstlisting}
    \caption{Case study of GPT-4-0613 on \ours ($\Omega =\langle f_c, f_e, f_v \rangle$; turn 7): $f_v$ and Refined Code.}
    \label{fig:gpt4_ef_unit_snf_case_study_2}
\end{figure*}



\begin{figure*}[t]
    \centering
    \textbf{Input Code}
    \begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
def task_func(n, seed=0):
    np.random.seed(seed)
    points = [(x, y) for x, y in np.random.rand(n, 2)]

    fig, ax = plt.subplots()
    ax.scatter([x for x, y in points], [y for x, y in points])
    ax.set_title("Scatter plot of random points")
    ax.set_xlabel("X")
    ax.set_ylabel("Y")

    return fig, points
    \end{lstlisting}

    \textbf{Expert-Level Verbal Feedback ($f_v^*$)}    
    \begin{WideMinted}[fontsize=\scriptsize, frame=lines, linenos]
1. **Random Points Generation**: The `previous_code` generates random points using a list comprehension with `np.random.rand(n, 2)`, which is a valid approach. However, it can be simplified by directly using `np.random.rand(n)` for both x and y coordinates separately. This makes the code more readable and aligns with common practices.

2. **Plotting Process**: The `previous_code` correctly plots the scatter plot using `ax.scatter()`. However, it extracts x and y coordinates from the `points` list using list comprehensions. This step can be avoided by directly using the arrays generated for x and y coordinates, which simplifies the code.

3. **Return Values**: The `previous_code` correctly returns the figure and the list of points. However, the generation of points can be streamlined by using `zip(x, y)` directly on the generated x and y arrays, which is more efficient and concise.

4. **Code Simplicity and Readability**: The `previous_code` is functional but can be simplified for better readability. Using direct array operations for x and y coordinates and avoiding unnecessary list comprehensions will make the code cleaner and easier to understand.

5. **Reproducibility**: Both the `previous_code` and the expected implementation correctly set the random seed for reproducibility, which is good practice.

Overall, while the `previous_code` is functional, it can be improved for simplicity and readability by directly using the generated x and y arrays for plotting and point generation. This will make the code more concise and maintainable.
    \end{WideMinted}

    \textbf{Refined Code by Feedback}    
    \begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
def task_func(n, seed=0):
    np.random.seed(seed)
    x, y = np.random.rand(2, n)
    points = list(zip(x, y))

    fig, ax = plt.subplots()
    ax.scatter(x, y)
    ax.set_title("Scatter plot of random points")
    ax.set_xlabel("X")
    ax.set_ylabel("Y")

    return fig, points
    \end{lstlisting}
    \caption{Case study of DeepSeek-Coder-6.7B-Instruct on \ours ($\Omega =\langle f_c, \phi, f_v^* \rangle$; turn 1).}
    \label{fig:deepseek_sef_case_study}
\end{figure*}


\begin{figure*}[h!]
    
    \includegraphics[width=1.0\linewidth]{fig/annotate.pdf}
    \caption{{Annotation platform~\citep{pei-etal-2022-potato} for collecting human evaluation of expert verbal feedback.}}
    \label{fig:expert_human_eval}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \begin{WideMinted}[fontsize=\small, frame=lines, linenos]
1. **Configuration File Reading**: The `previous_code` correctly reads the configuration file using `configparser`. However, ensure that the configuration file path is valid and exists before attempting to read it. This is not explicitly checked in the `previous_code`.

2. **Directory Existence Check**: The `previous_code` uses `os.path.exists(project_dir)` to check if the project directory exists. While this works, it is more appropriate to use `os.path.isdir(project_dir)` to specifically check for directory existence, as it is more semantically correct.

3. **ZIP Archive Creation**: The `previous_code` attempts to create the ZIP archive using `shutil.make_archive(project_dir, 'zip', archive_dir)`. This is incorrect because `shutil.make_archive` expects the base name of the archive and the root directory to archive. The correct usage should be `shutil.make_archive(base_name=os.path.splitext(zip_file_path)[0], format='zip', root_dir=project_dir)`.

4. **Exception Handling**: The `previous_code` raises a generic `Exception` if the ZIP archive creation fails. While this is acceptable, it is better to provide a more specific error message indicating the failure reason. Additionally, ensure that the ZIP file is actually created by checking its existence after the `shutil.make_archive` call.

5. **Return Value**: The `previous_code` correctly returns `True` if the ZIP archive is successfully created. However, it should also ensure that the ZIP file exists before returning `True`.

6. **Code Simplicity and Readability**: The `previous_code` includes a detailed docstring, which is good practice. However, the actual implementation can be simplified and made more readable by following the correct usage of `shutil.make_archive` and ensuring proper exception handling.

Overall, the `previous_code` has the right structure but needs corrections in the directory existence check, ZIP archive creation, and exception handling to function correctly.
    \end{WideMinted}
    \caption{An example case that the feedback guides toward the ground truth without explicitly referencing it.}
    \label{fig:desirable}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \begin{WideMinted}[fontsize=\small, frame=lines, linenos]
1. **Class Name**: The class name in the `previous_code` is `EmailHandler`, but it should be `EmailRequestHandler` to match the `ground_truth_code`.

2. **Content-Type Check**: Instead of directly checking the `Content-Type` header, use `cgi.parse_header` to parse the header and then check if `ctype != 'application/json'`.

3. **Error Handling for Content-Type**: When the `Content-Type` is not `application/json`, simply send a 400 response and end headers without writing a message to the response body.

4. **Reading Content-Length**: Use `length = int(self.headers.get('content-length'))` instead of `content_length = int(self.headers.get('Content-Length', 0))`.

5. **JSON Decoding**: When catching `json.JSONDecodeError`, send a 400 response and end headers without writing a message to the response body.

6. **Missing Fields Check**: When required fields are missing, send a 400 response and end headers without writing a message to the response body.

7. **SMTP Authentication Error Handling**: When catching `smtplib.SMTPAuthenticationError`, send a 535 response and end headers without writing a message to the response body.

8. **General Exception Handling**: Remove the general exception handler that sends a 500 response, as it is not present in the `ground_truth_code`.

By making these changes, the `previous_code` will align more closely with the `ground_truth_code`.
    \end{WideMinted}
    \caption{An example case that the feedback directly references ground truth, leading to ``leakage''.}
    \label{fig:undesirable}
\end{figure*}