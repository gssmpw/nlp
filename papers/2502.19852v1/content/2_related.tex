\section{Related Work}
\label{related}

Code generation benchmarks have traditionally focused on single-turn generation from natural language problem descriptions~\citep{codex,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}.
More recently, LLM performance has improved through interactions with external tools, such as interpreters for compiling, executing test cases, and verbal feedback, resulting in more accurate outputs~\citep{shinn2023reflexion,madaan2024self,chen2024teaching,olausson2024is}. 
This shift has led to the development of multi-turn benchmarks like InterCode~\citep{yang2023intercode} and MINT~\citep{wang2024mint}.

\input{table/partial_observability}

% While these benchmarks advance real-world coding simulation, 
However, existing multi-turn benchmarks remain limited in feedback diversity. InterCode focuses on compilation and partial execution feedback but lacks full test coverage and verbal feedback. MINT generates verbal feedback via GPT-4, reducing human-in-the-loop evaluation costs, but its feedback scope is narrow and requires costly LLM calls for each evaluation.



% Code generation benchmark has focused on one-shot generation scenario from problem description in natural language~\citep{codex,DBLP:journals/corr/abs-2108-07732,li2022competition,zhuo2024bigcodebench}. Meanwhile, 
% LLM code generation capabilities are improved through interactions with external tools, such as interpreters for compiling and executing test cases, and verbal feedback, leading to more accurate and reliable output~\citep{shinn2023reflexion,madaan2024self,chen2024teaching}. 
% This has motivated benchmark efforts, such as InterCode~\citep{yang2023intercode} and \textsc{Mint}~\citep{wang2024mint}, evaluating LLMs' multi-turn interaction capabilities.

% While both InterCode and MINT advance the simulation of real-world coding scenarios, they remain limited in scope. 
% InterCode focuses only on compilation and execution feedback with partial test coverage, but lacks other critical forms of feedback such as full test coverage and verbal guidance. 
% MINT employs GPT-4 to generate verbal feedback, reducing the need for human-in-the-loop evaluations, yet its feedback coverage remains narrow.  
% Consequently, neither fully captures the diversity of feedback combinations that occur in real-world development. Furthermore, MINT requires calling the LLM for each evaluation, making it potentially costly. 


 Our study presents (a) \ours, a reproducible environment with \textbf{nine unique feedback combinations} (Table~\ref{tab:partial_observability}), and (b) \oursstatic, a \textbf{cost-effective benchmark} that maintains high correlation with live environment by using pre-generated logs, eliminating the need for costly LLM calls to provide verbal feedback. {We further discuss the distinction of \ours in Appendix~\ref{appendix:our_distinction}.}

 