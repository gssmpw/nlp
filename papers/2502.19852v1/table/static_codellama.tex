% \section{\oursstatic}
% \subsection{Reference Model: CodeLlama-7B-Instruct}
\begin{table}[t]
    \centering
    \caption{MRR and Recall results on \oursstatic using logs of CodeLlama-7B-Instruct in \ourslive. {\protect\xmark} indicates that no feedback of that type is provided ($\phi$). For each column, bold and underscore indicate $1$st and $2$nd place performance within the same model group. }
    % \begin{center}
    \scriptsize
    \begin{tabular}{lccccccccccccccc}
        \thickhline
        & \multicolumn{5}{c}{\multirow{1.4}{*}{MRR}} & \multicolumn{5}{c}{\multirow{1.4}{*}{Recall}} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-11}
        Compilation Feedback  & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ \\
        Execution Feedback & $f_e$ & $f_e^*$ & \xmark & $f_e$ & $f_e^*$ & $f_e$ & $f_e^*$ & \xmark & $f_e$ & $f_e^*$ \\ 
        % Test Coverage & \scriptsize part. &\scriptsize full &  &\scriptsize part. &\scriptsize full \\ 
        Verbal Feedback & $f_v$ & $f_v$ & $f_v^*$ & $f_v^*$ & $f_v^*$ & $f_v$ & $f_v$ & $f_v^*$ & $f_v^*$ & $f_v^*$ \\\hline
        % User Expertise & \scriptsize novice & \scriptsize novice & \scriptsize expert & \scriptsize expert & \scriptsize expert \\\hline
        \multicolumn{11}{c}{Closed-Source Models}\\
        GPT-4-0613 & 53.0 & 55.8 & 63.1 & 62.7 & 63.4 & 59.5 & 65.7 & 85.9 & \textbf{82.3} & 83.1 \\
        GPT-4-Turbo-2024-04-09 & \underline{55.7} & \underline{58.3} & \underline{65.4} & \underline{64.0} & \underline{65.3} & \underline{61.8} & \textbf{68.2} & \textbf{86.8} & 81.4 & \underline{84.2} \\
        GPT-4o-2024-05-13 & \textbf{57.4} & \textbf{59.9} & \textbf{66.4} & \textbf{65.7} & \textbf{66.8} & \textbf{62.1} & \underline{68.1} & \underline{86.2} & \underline{81.9} & \textbf{84.7} \\\hdashline
        \multicolumn{11}{c}{Open-Source Models ($\geq 30\textrm{B}$)}\\
        Llama-3.1-70B-Instruct & \textbf{54.2} & \textbf{56.6} & \textbf{63.7} & \textbf{63.1} & \textbf{63.9} & \textbf{60.2} & \textbf{65.7} & \textbf{85.9} & \textbf{81.5} & \textbf{84.0} \\
        DeepSeek-Coder-33B-Instruct & \underline{48.2} & \underline{50.6} & \underline{60.1} & 58.8 & \underline{59.8} & \underline{51.9} & \underline{58.0} & \underline{83.2} & \underline{78.2} & \underline{79.7} \\
        ReflectionCoder-DS-33B & 47.9 & 49.9 & 59.5 & \underline{59.1} & 59.6 & 51.2 & 56.2 & 82.2 & 77.8 & 79.6 \\
        Qwen1.5-72B-Chat & 42.6 & 45.7 & 54.7 & 54.1 & 55.2 & 47.8 & 55.7 & 80.3 & 76.8 & 78.7 \\
        Qwen1.5-32B-Chat & 41.1 & 43.2 & 52.2 & 48.7 & 48.5 & 45.7 & 51.4 & 76.2 & 67.2 & 66.8 \\
        CodeLlama-34B-Instruct & 36.1 & 37.6 & 50.2 & 49.2 & 49.7 & 40.2 & 43.9 & 78.3 & 72.4 & 73.8 \\\hdashline
        \multicolumn{11}{c}{Open-Source Models ($< 30\textrm{B}$)}\\
        Llama-3.1-8B-Instruct & 42.6 & 45.3 & 54.7 & 54.0 & 54.9 & \underline{47.9} & \underline{54.6} & 80.9 & \underline{75.9} & \underline{78.0} \\
        DeepSeek-Coder-V2-Lite-Instruct & \textbf{46.3} & \textbf{48.4} & \textbf{58.2} & \textbf{56.0} & \textbf{57.1} & \textbf{51.1} & \textbf{55.6} & \textbf{82.0} & 74.7 & 77.9\\
        DeepSeek-Coder-6.7B-Instruct & 43.0 & 45.4 & \underline{56.5} & \underline{55.6} & 56.0 & 46.8 & 52.9 & \underline{81.3} & \textbf{77.5} & \textbf{78.7} \\
        ReflectionCoder-DS-6.7B & 43.4 & 45.4 & 55.7 & 55.1 & 55.2 & 46.7 & 51.6 & 79.3 & 74.8 & 75.9\\
        CodeQwen1.5-7B-Chat & \underline{45.8} & \underline{47.4} & 56.3 & \underline{55.6} & \underline{56.3} & 49.1 & 53.2 & 78.0 & 74.1 & 76.3\\
        % \rowcolor{gray!10}
        % CodeLlama-7B-Instruct (ref.) & 23.5 & 25.2 & 35.0 & 33.4 & 33.9\\
        StarCoder2-15B-Instruct-v0.1 & 43.1 & 44.2 & 54.1 & 53.0 & 53.3 & 45.8 & 49.0 & 78.0 & 72.2 & 72.7 \\
        CodeLlama-13B-Instruct & 34.8 & 36.9 & 47.2 & 46.9 & 47.2 & 37.8 & 43.2 & 73.1 & 68.9 & 68.9 \\
        \thickhline
    \end{tabular}
    % \end{center}
    \label{tab:convcodebench_static_codellama}
\end{table}





% \begin{table}[h]
%     \centering
%     \caption{Recall on \oursstatic logs of CodeLlama-7B-Instruct in \ourslive. The dashed line separates the closed-source (above) and open-source (below) LLMs. For each column, Bold and underscore mean $1$st and $2$nd performance among the same source.\footnotemark[7]}
%     \small
%     \begin{tabular}{lccccccccccccccc}
%         \thickhline
%         Compilation Feedback  & $f_c$ & $f_c$ & $f_c$ & $f_c$ & $f_c$ \\
%         Execution Feedback & $f_e$ & $f_e^*$ & \xmark & $f_e$ & $f_e^*$ \\ 
%         % Test Coverage & \scriptsize part. &\scriptsize full &  &\scriptsize part. &\scriptsize full \\ 
%         Simulated User Feedback & $f_v$ & $f_v$ & $f_v^*$ & $f_v^*$ & $f_v^*$ \\\hline
%         % User Expertise & \scriptsize novice & \scriptsize novice & \scriptsize expert & \scriptsize expert & \scriptsize expert \\\hline
%         GPT-4-0613 & 59.5 & 65.7 & 85.9 & 82.3 & 83.1 \\
%         GPT-4-Turbo-2024-04-09 & 61.8 & 68.2 & 86.8 & 81.4 & 84.2 \\
%         GPT-4o-2024-05-13 & 62.1 & 68.1 & 86.2 & 81.9 & 84.7 \\\hdashline
%         DeepSeek-Coder-V2-Lite-Instruct & 51.1 & 55.6 & 82.0 & 74.7 & 77.9\\
%         CodeQwen1.5-7B-Chat & 49.1 & 53.2 & 78.0 & 74.1 & 76.3\\
%         % \rowcolor{gray!10}
%         % CodeLlama-7B-Instruct (ref.) & \phantom{00}4.1 & \phantom{00}8.1 & \phantom{0}39.9 & \phantom{0}32.1 & \phantom{0}33.2\\
%         ReflectionCoder-DS-6.7B & 46.7 & 51.6 & 79.3 & 74.8 & 75.9\\
%         DeepSeek-Coder-6.7B-Instruct & 46.8 & 52.9 & 81.3 & 77.5 & 78.7 \\
%         DeepSeek-Coder-33B-Instruct & 51.9 & 58.0 & 83.2 & 78.2 & 79.7 \\
%         StarCoder2-15B-Instruct-v0.1 & 45.8 & 49.0 & 78.0 & 72.2 & 72.7 \\
%         CodeLlama-13B-Instruct & 37.8 & 43.2 & 73.1 & 68.9 & 68.9 \\
%         CodeLlama-34B-Instruct & 40.2 & 43.9 & 78.3 & 72.4 & 73.8 \\
%         ReflectionCoder-DS-33B & 51.2 & 56.2 & 82.2 & 77.8 & 79.6 \\
%         Qwen1.5-32B-Chat & 45.7 & 51.4 & 76.2 & 67.2 & 66.8 \\
%         Qwen1.5-72B-Chat & 47.8 & 55.7 & 80.3 & 76.8 & 78.7 \\
%         Llama-3.1-8B-Instruct & 47.9 & 54.6 & 80.9 & 75.9 & 78.0 \\
%         Llama-3.1-70B-Instruct & 60.2 & 65.7 & 85.9 & 81.5 & 84.0 \\
%         \thickhline
%     \end{tabular}
%     \normalsize
%     \label{tab:convcodebench_cumul_pass_gain_static_codellama}
% \end{table}




