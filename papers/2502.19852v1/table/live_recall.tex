\begin{table*}[t]
    \centering
    \caption{Recall results on \ourslive. {\protect\xmark} indicates that no feedback of that type is provided ($\phi$). The leftmost results, with three {\protect\xmark}, represent $\Omega = \langle  \phi, \phi, \phi \rangle$, corresponding to single-turn code generation without any feedback. For each column, bold and underscore indicate $1$st and $2$nd place performance within the same model group. 
    } 
    \scriptsize
    \begin{tabular}{lccccccccccccccc}
        \thickhline
         Compilation Feedback & \xmark &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$  \\
         Execution Feedback & \xmark &  \xmark &  $f_e$ &  $f_e^*$ & \xmark &  $f_e$ &  $f_e^*$ & \xmark &  $f_e$ &  $f_e^*$ \\ 
         % Test Coverage & & &\scriptsize  part.&\scriptsize full & &\scriptsize  part.&\scriptsize full &  &\scriptsize  part.&\scriptsize full \\ 
         Verbal Feedback & \xmark & \xmark & \xmark & \xmark &  $f_v$ &  $f_v$ &  $f_v$ &  $f_v^*$ &  $f_v^*$ &  $f_v^*$ \\\hline
         % Expertise & & & & & \scriptsize novice & \scriptsize novice & \scriptsize novice & \scriptsize expert & \scriptsize expert & \scriptsize expert \\\hline
         \multicolumn{11}{c}{Closed-Source Models}\\
         GPT-4-0613 & 46.0 & 46.0 & \underline{60.3} & \textbf{70.5} & 46.0 & \textbf{61.9} & \textbf{72.5} & \textbf{89.7} & \textbf{91.1} & \textbf{92.5} \\
         GPT-4-Turbo-2024-04-09 & \underline{48.0} & \underline{48.0} & 56.7 & 63.8 & \underline{48.0} & 58.6 & 68.1 & \underline{84.7} & \underline{87.5} & \underline{88.5} \\
         GPT-4o-2024-05-13 & \textbf{50.8} & \textbf{50.8} & \textbf{60.5} & \underline{67.6} & \textbf{50.8} & \underline{60.8} & \underline{69.6} & 82.3 & 84.9 & 86.2 \\\hdashline
         \multicolumn{11}{c}{Open-Source Models ($\geq 30\textrm{B}$)}\\
         Llama-3.1-70B-Instruct & 45.4 & \textbf{45.4} & \textbf{56.2} & \textbf{64.8} & \textbf{45.4} & \textbf{59.5} & \textbf{70.8} & \textbf{86.7} & \textbf{88.9} & \textbf{91.8} \\
         DeepSeek-Coder-33B-Instruct & 41.6 &  \underline{41.6} &  \underline{45.5} &  46.1 &  \underline{41.6} &  50.4 &  56.6 &  \underline{85.4} &  84.6 &  85.6 \\
         ReflectionCoder-DS-33B & 41.6 & \underline{41.6} & 45.3 & 44.9 & \underline{41.6} & \underline{51.4} & 57.2 & 81.4 & 81.8 & 84.2 \\
         Qwen1.5-72B-Chat & 32.9 & 33.2 & 39.9 & \underline{47.5} & 33.2 & 47.5 & \underline{57.9} & 84.4 & \underline{86.1} & \underline{87.2} \\
         Qwen1.5-32B-Chat & 32.0 & 32.0 & 41.1 & 45.3 & 32.0 & 44.6 & 54.3 & 75.9 & 61.8 & 57.1 \\
         CodeLlama-34B-Instruct & 28.8 & 28.8 & 33.7 & 35.8 & 28.8 & 37.5 & 44.6 & 80.0 & 82.0 & 82.3 \\\hdashline
         \multicolumn{11}{c}{Open-Source Models ($< 30\textrm{B}$)}\\
         Llama-3.1-8B-Instruct & 31.4 & 31.8 & 38.4 & 40.0 & 31.7 & 43.2 & \textbf{51.8} & \underline{80.9} & \underline{80.2} & \textbf{83.7} \\
         DeepSeek-Coder-V2-Lite-Instruct & \underline{38.3} & \underline{38.3} & \textbf{43.4} & \textbf{46.1} & \underline{38.3} & \textbf{47.0} & \underline{51.4} & 76.3 & 75.8 & 76.9 \\
         DeepSeek-Coder-6.7B-Instruct & 35.2 & 35.2 & 37.7 & 37.5 & 35.2 & 43.3 & 48.2 & \textbf{82.8} & \textbf{82.5} & \underline{83.1} \\
         ReflectionCoder-DS-6.7B & 37.4 & 37.4 & 39.6 & 40.7 & 37.4 & 44.7 & 50.4 & 79.1 & 79.6 & 78.9 \\
         CodeQwen1.5-7B-Chat & \textbf{39.3} & \textbf{39.6} & \underline{40.1} & \underline{41.1} & \textbf{39.5} & \underline{45.8} & 49.5 & 74.4 & 74.7 & 77.4 \\
         StarCoder2-15B-Instruct-v0.1 & 37.1 & 37.1 & 39.3 & 40.0 & 37.1 & 42.6 & 46.3 & 76.9 & 76.8 & 75.6 \\
         CodeLlama-13B-Instruct & 28.4 & 28.4 & 29.7 & 30.0 & 28.4 & 35.1 & 41.1 & 69.0 & 70.7 & 71.6 \\
         CodeLlama-7B-Instruct & 21.8 & 21.8 & 22.9 & 23.0 & 21.8 & 26.2 & 30.5 & 61.7 & 53.9 & 55.2 \\
        \thickhline
    \end{tabular}
    \normalsize
    \label{tab:convcodebench_recall}
\end{table*}
