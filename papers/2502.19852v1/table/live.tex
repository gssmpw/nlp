\begin{table*}[t]
    \centering
    \caption{MRR results on \ourslive. 
    {\protect\xmark} indicates that no feedback of that type is provided ($\phi$). The leftmost results, with three {\protect\xmark}, represent $\Omega = \langle  \phi, \phi, \phi \rangle$, corresponding to single-turn code generation without any feedback. For each column, bold and underscore indicate $1$st and $2$nd place performance within the same model group.}
    \scriptsize
    \begin{tabular}{lccccccccccccccc}
        \thickhline
          Compilation Feedback & \xmark &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$ &  $f_c$  \\
         Execution Feedback & \xmark &  \xmark &  $f_e$ &  $f_e^*$ & \xmark &  $f_e$ &  $f_e^*$ & \xmark &  $f_e$ &  $f_e^*$ \\ 
         % Test Coverage & & &\scriptsize  part.&\scriptsize full & &\scriptsize  part.&\scriptsize full &  &\scriptsize  part.&\scriptsize full \\ 
         Verbal Feedback & \xmark & \xmark & \xmark & \xmark &  $f_v$ &  $f_v$ &  $f_v$ &  $f_v^*$ &  $f_v^*$ &  $f_v^*$ \\\hline
         % Expertise & & & & & \scriptsize novice & \scriptsize novice & \scriptsize novice & \scriptsize expert & \scriptsize expert & \scriptsize expert \\\hline
         \multicolumn{11}{c}{Closed-Source Models}\\
         GPT-4-0613 & 46.0 & 46.0 & \underline{52.1} & \underline{56.1} & 46.0 & 52.4 & \underline{56.4} & \underline{63.1} & \underline{64.3} & \underline{64.8} \\
         GPT-4-Turbo-2024-04-09 & \underline{48.0} & \underline{48.0} & 51.8 & 54.8 & \underline{48.0} & \underline{52.6} & \underline{56.4} & 62.4 & \underline{64.3} & 64.5 \\
         GPT-4o-2024-05-13 & \textbf{50.8} & \textbf{50.8} & \textbf{55.0} & \textbf{57.9} & \textbf{50.8} & \textbf{55.1} & \textbf{58.6} & \textbf{63.3} & \textbf{64.7} & \textbf{65.3} \\\hdashline
         \multicolumn{11}{c}{Open-Source Models ($\geq 30\textrm{B}$)}\\
         Llama-3.1-70B-Instruct & \textbf{45.4} & \textbf{45.4} & \textbf{49.9} & \textbf{53.4} & \textbf{45.4} & \textbf{50.8} & \textbf{55.2} & \textbf{60.7} & \textbf{62.6} & \textbf{63.3} \\
         DeepSeek-Coder-33B-Instruct & \underline{41.6} & \underline{41.6} & \underline{43.4} & \underline{43.6} & \underline{41.6} & 45.5 & 48.0 & \underline{58.6} & \underline{58.5} & 58.8 \\
         ReflectionCoder-DS-33B & \underline{41.6} & \underline{41.6} & 42.9 & 42.9 & \underline{41.6} & \underline{45.6} & \underline{48.1} & 57.7 & 58.2 & \underline{58.9} \\
         Qwen1.5-72B-Chat & 32.9 & 33.0 & 35.8 & 38.3 & 33.0 & 38.6 & 41.4 & 50.6 & 52.0 & 52.7 \\
         Qwen1.5-32B-Chat & 32.0 & 32.0 & 35.3 & 36.7 & 32.0 & 36.6 & 39.7 & 47.4 & 42.6 & 40.8 \\
         CodeLlama-34B-Instruct & 28.8 & 28.8 & 31.0 & 31.9 & 28.8 & 32.5 & 35.1 & 48.7 & 49.2 & 49.8 \\\hdashline         
         \multicolumn{11}{c}{Open-Source Models ($< 30\textrm{B}$)}\\
         Llama-3.1-8B-Instruct & 31.4 & 31.5 & 34.0 & 34.6 & 31.5 & 36.1 & 39.1 & 49.4 & 49.8 & 51.3 \\
         DeepSeek-Coder-V2-Lite-Instruct & \underline{38.3} & \underline{38.3} & \textbf{40.5} & \textbf{41.7} & \underline{38.3} & \textbf{42.0} & \textbf{43.8} & 52.7 & 52.9 & 53.3 \\
         DeepSeek-Coder-6.7B-Instruct & 35.2 & 35.2 & 36.2 & 36.1 & 35.2 & 38.8 & 40.5 & \underline{53.3} & 53.2 & \underline{53.9} \\
         ReflectionCoder-DS-6.7B & 37.4 & 37.4 & 38.3 & 38.7 & 37.4 & 40.4 & 42.4 & \underline{53.3} & \textbf{53.8} & 53.6 \\
         CodeQwen1.5-7B-Chat & \textbf{39.3} & \textbf{39.4} & \underline{39.7} & \underline{40.1} & \textbf{39.3} & \textbf{42.0} & \underline{43.7} & \textbf{53.7} & \underline{53.5} & \textbf{54.8} \\
         StarCoder2-15B-Instruct-v0.1 & 37.1 & 37.1 & 37.9 & 38.3 & 37.1 & 39.4 & 40.5 & 52.7 & 52.8 & 52.1 \\
         CodeLlama-13B-Instruct & 28.4 & 28.4 & 29.0 & 29.0 & 28.4 & 31.2 & 33.0 & 43.9 & 44.3 & 44.8 \\
         CodeLlama-7B-Instruct & 21.8 & 21.8 & 22.3 & 22.3 & 21.8 & 23.5 & 25.2 & 35.0 & 33.4 & 33.9 \\
        \thickhline
    \end{tabular}
    \normalsize
    \label{tab:convcodebench_mrr}
\end{table*}




