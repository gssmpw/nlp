[
  {
    "index": 0,
    "papers": [
      {
        "key": "chapman2021risk",
        "author": "Chapman, Margaret P and Bonalli, Riccardo and Smith, Kevin M and Yang, Insoon and Pavone, Marco and Tomlin, Claire J",
        "title": "Risk-sensitive safety analysis using conditional value-at-risk"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chow2018risk",
        "author": "Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco",
        "title": "Risk-constrained reinforcement learning with percentile risk criteria"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "tang2019worst",
        "author": "Tang, Yichuan Charlie and Zhang, Jian and Salakhutdinov, Ruslan",
        "title": "Worst cases policy gradients"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "moldovan2012risk",
        "author": "Moldovan, Teodor and Abbeel, Pieter",
        "title": "Risk aversion in Markov decision processes via near optimal Chernoff bounds"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2021learning",
        "author": "Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao",
        "title": "Learning policies with zero or bounded constraint violation for constrained mdps"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "achiam2017constrained",
        "author": "Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter",
        "title": "Constrained policy optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chow2018lyapunov",
        "author": "Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad",
        "title": "A lyapunov-based approach to safe reinforcement learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "perkins2002lyapunov",
        "author": "Perkins, Theodore J and Barto, Andrew G",
        "title": "Lyapunov design for safe reinforcement learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ma2021model",
        "author": "Ma, Haitong and Chen, Jianyu and Eben, Shengbo and Lin, Ziyu and Guan, Yang and Ren, Yangang and Zheng, Sifa",
        "title": "Model-based constrained reinforcement learning using generalized control barrier function"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "anderson2020neurosymbolic",
        "author": "Anderson, Greg and Verma, Abhinav and Dillig, Isil and Chaudhuri, Swarat",
        "title": "Neurosymbolic reinforcement learning with formally verified exploration"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "qin2021density",
        "author": "Qin, Zengyi and Chen, Yuxiao and Fan, Chuchu",
        "title": "Density constrained reinforcement learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zanon2020safe",
        "author": "Zanon, Mario and Gros, S{\\'e}bastien",
        "title": "Safe reinforcement learning using robust MPC"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "sui2015safe",
        "author": "Sui, Yanan and Gotovos, Alkis and Burdick, Joel and Krause, Andreas",
        "title": "Safe exploration for optimization with Gaussian processes"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ghasemipour2022so",
        "author": "Ghasemipour, Kamyar and Gu, Shixiang Shane and Nachum, Ofir",
        "title": "Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jiang2024importance",
        "author": "Jiang, Yiding and Kolter, J Zico and Raileanu, Roberta",
        "title": "On the importance of exploration for generalization in reinforcement learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024entropy",
        "author": "Zhang, Ruoqi and Luo, Ziwei and Sj{\\\"o}lund, Jens and Sch{\\\"o}n, Thomas B and Mattsson, Per",
        "title": "Entropy-regularized diffusion policy with q-ensembles for offline reinforcement learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "rigter2024one",
        "author": "Rigter, Marc and Lacerda, Bruno and Hawes, Nick",
        "title": "One risk to rule them all: A risk-sensitive perspective on model-based offline reinforcement learning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "bozkus2024multi",
        "author": "Bozkus, Talha and Mitra, Urbashi",
        "title": "Multi-timescale ensemble Q-learning for Markov decision process policy optimization"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "wu2024ocean",
        "author": "Wu, Fan and Zhang, Rui and Yi, Qi and Gao, Yunkai and Guo, Jiaming and Peng, Shaohui and Lan, Siming and Han, Husheng and Pan, Yansong and Yuan, Kaizhao and others",
        "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning"
      }
    ]
  }
]