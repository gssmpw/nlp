\section{related work}
\subsection{Safe RL}

In safe RL, model-based and model-free approaches differ primarily in how they handle environment knowledge. Model-based methods leverage a detailed model of the environment, enabling more precise safety guarantees and efficient planning. Model-free methods, lacking this model, rely on empirical data and adaptation, which can offer flexibility in unknown environments but may require more data for reliability. 

Recent advances in RL and control have focused on integrating risk-sensitive metrics to manage uncertainties in the environment\cite{chapman2021risk}\cite{chow2018risk}. A popular approach is based on Conditional Value at Risk (CVaR), which aims to make agents more robust to low-probability, high-impact events. CVaR is particularly effective in addressing aleatoric uncertainty, which arises from inherent randomness in the environment. For example, \cite{tang2019worst} proposes a CVaR-based objective that enables agents to be resilient against this type of uncertainty by focusing on the worst-case scenarios within a specified confidence level, helping to safeguard against risky outcomes. This framework allows the agent to be cautious under uncertainty while still optimizing performance in high-confidence regions.

Policy Optimization-Based Approaches involve adjusting policies to optimize rewards under safety constraints. Model-based methods use risk-aware optimizations like Chernoff bounds \cite{moldovan2012risk} and advanced projections (e.g., OptPress-PrimalDual \cite{liu2021learning}), which depend on knowing the environment’s dynamics to calculate risks and enforce constraints accurately. Model-free methods, such as Constrained Policy Optimization (CPO) \cite{achiam2017constrained}, adapt policies based solely on real-time data and empirical feedback, often requiring more samples but providing flexibility in dynamic or partially known environments.

Control Theory-Based Approaches apply control functions to regulate actions within safe limits. Model-based methods, such as Lyapunov functions \cite{chow2018lyapunov}\cite{perkins2002lyapunov} and Control Barrier Functions (CBFs) \cite{ma2021model}, can define safety constraints based on known dynamics, ensuring stability and safety for complex systems like robotics. In contrast, model-free methods approximate Lyapunov functions or use adaptive CBFs, estimating safety boundaries from data, which is useful when exact dynamics are unknown but can be less precise.

Formal Methods-Based Approaches  use logical verification to ensure policies meet safety requirements. Model-based techniques rely on symbolic policy verification or neurosymbolic methods \cite{anderson2020neurosymbolic} to rigorously assess all possible actions against safety constraints, benefiting from a known environment model. Model-free formal methods, like Dynamic Constraint RL (DCRL) \cite{qin2021density}, use probabilistic checks to approximate verification, providing conservative safety assurances based on sample data when a full model is unavailable.

Gaussian Process (GP)-Based Approaches use probabilistic modeling to handle uncertainty in decision-making. Model-based methods integrate GPs with Model Predictive Control (MPC) \cite{zanon2020safe} to quantify uncertainty within a known framework, optimizing actions based on both expected outcomes and associated risks. Model-free techniques, on the other hand, use GPs for safe exploration by sampling high-uncertainty areas to avoid risky actions, allowing the agent to adaptively balance exploration and exploitation in unknown settings \cite{sui2015safe}.


\subsection{Ensemble RL}
One common approach in offline RL is to use Q-ensembles—multiple Q-value estimators that together produce conservative estimates by backing up lower-confidence bounds across ensemble members. This ensemble technique helps reduce overconfidence in unseen or poorly represented actions in the dataset, ensuring that the policy is less likely to select risky actions during deployment. The proposed Model Standard-deviation Gradient (MSG) algorithm refines this approach by independently training each Q-network in the ensemble, using separate target values to promote diversity among networks. This setup helps capture a wider range of uncertainty and enhances the ensemble's conservatism, effectively mitigating the distributional shift problem, where the policy may encounter states or actions absent in the dataset\cite{ghasemipour2022so}.

Exploration via Distributional Ensemble (EDE) is an ensemble-based RL approach designed to improve exploration in contextual Markov Decision Processes (CMDPs) by targeting epistemic uncertainty—areas where the agent has limited knowledge and can learn more\cite{jiang2024importance}. EDE combines distributional Q-learning with multiple Q-networks, allowing it to estimate the full distribution of potential rewards for each state-action pair. By calculating the variance across the ensemble's Q-value predictions, EDE identifies high-uncertainty areas that are prime for exploration, thereby guiding the agent to gather data in less-understood parts of the environment. This targeted exploration strategy helps the agent generalize better across similar but unseen environments, reduces overfitting, and improves adaptability by focusing on informative areas rather than random exploration.


In RL, a mean-reverting stochastic differential equation (SDE) is utilized to control the distribution of actions for improved stability and exploration in policy training\cite{zhang2024entropy}. Specifically, within ensemble RL methods, the SDE can aid in diffusing the action distribution into a standard Gaussian, allowing the policy to generate a wide range of actions, which are then sampled based on the environmental state. This mean-reverting property ensures that actions do not drift too far from a baseline level, helping to stabilize the training process by balancing exploration and exploitation. In this setup, entropy regularization is often combined with the SDE framework, encouraging the policy to sample diverse actions while still focusing on reliable, data-grounded actions.

 \cite{rigter2024one} addresses the need for risk-averse decision-making in offline RL by targeting both epistemic and aleatoric uncertainty. It uses an ensemble of models to capture epistemic uncertainty, where disagreement among models highlights areas of limited data coverage. To make the policy conservative, the method introduces adversarial perturbations that adjust the transition distribution toward low-value outcomes, effectively simulating worst-case scenarios for each action. Synthetic rollouts are generated from this modified distribution and added to the dataset, enabling the policy to learn from potentially adverse outcomes. The policy is then optimized using a risk-sensitive objective, such as Conditional Value at Risk (CVaR), which prioritizes avoiding low-value actions. This combined approach helps the agent make safer, conservative choices in uncertain environments without needing direct exploration. 

\cite{bozkus2024multi} proposed method enhances traditional Q-learning by leveraging an ensemble of Q-functions, where multiple Q-learning agents operate in parallel across unique, synthetically generated Markovian environments. Each agent independently estimates Q-values based on its environment, capturing different perspectives and structural variations that provide a richer set of data points. This ensemble approach is key to managing the inherent complexity and unpredictability in large-scale networks by enabling the agent to learn from a diverse set of scenarios, thus improving generalization and robustness. The OCEAN framework for model-based offline RL leverages an ensemble of dynamics models to predict transition probabilities, helping the agent simulate and assess future state transitions conservatively\cite{wu2024ocean}. Technically, each model in the ensemble predicts the probability distribution of reaching a next state given a current state-action pair. Disagreement among the models is used as an uncertainty measure to indicate regions of the state-action space where the data is sparse or out-of-distribution.