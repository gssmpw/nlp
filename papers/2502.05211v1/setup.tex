\section{Experimental setup}\label{sec:setup}
In this section, we provide the details of our experimental setup.

\subsection{FEMNIST~\cite{caldas2018leaf,cohen2017emnist}}\label{setup:femnist}
FEMNIST (Figure~\ref{fig:femnist_hist}) is a character recognition classification task with 3,400 clients, 62 classes (52 for upper and lower case letters and 10 for digits), and 671,585  grayscale images. 
Each client has data of their own handwritten digits or letters. We use 300 randomly selected clients with their original data in a cross-silo fashion, as FedRecover uses the cross-silo setting in its implementation.
We use the CNN used by ~\cite{cao2022fedrecover} and use the Xavier weight initialization.


\begin{figure}[t]
\centering
\includegraphics[width=0.5\linewidth]{figures/femnist_hist.png}
\vspace{-0.55cm}
\caption{FEMNIST histogram from leaf.cmu.edu}
\label{fig:femnist_hist}
\vspace{-0.3cm}
\end{figure}

\noindent\textbf{Hyperparameters for re-eval:} For FEMNIST, we run over 200 epochs with 300 clients. In the attack setting, 60 clients are malicious. The results in Figure~\ref{fig:fdr_recovery_benign_attack} use $T_w=10$, and $T_c=10$. The FL algorithm used here is FedAVG with a local learning rate of $0.05$ and a global learning rate of $1$. We keep the batch size to 32. The number of local epochs is kept at $1$. For Figure~\ref{fig:fedrec_fnr_fpr_twarm20}, we consider the possibility of benign clients being misclassified as malicious or malicious clients being misclassified as benign, so we vary the false negative and false positive rates between 0.1 and 0.5. 
% For Figure~\ref{fig:fedrec_Tw_Tc} we vary $T_c$ between 1 and 10, where $T_c=1$ means all updates are exact updates and use 10 and 20 for $T_w$.

\subsection{CIFAR10~\cite{Krizhevsky2009learning}}\label{setup:cifar10}
CIFAR10 is a 10-class classification task with 60,000 total RGB images, each of size 32 $\times$ 32. We divide all the data among 100 clients using either Dirichlet~\cite{reddi2020adaptive} or \emph{FCJ}~\cite{fang2020local} distributions, which are the two most popular synthetic strategies to generate the FL dataset. We use a Resnet20 model with the CIFAR dataset.

\noindent\textbf{Hyperparameters for re-eval:} We run over 100 epochs with 100 clients. In the attack setting, 20 clients are malicious. The FL algorithm used here is FedAVG, with a local learning rate of 0.01 and a global learning rate of 1. We keep the batch size to 16. The number of local epochs is kept at $2$. The results in Figure~\ref{fig:fdr_recovery_benign_attack} use $T_w=10$, and $T_c=5$ and the fang distribution. Contrary to the rest of the datasets used, we use $T_c=5$ because CIFAR10 was a much more challenging learning task.



% We notice that the overall accuracy makes the system seem falsely robust under imbalanced conditions. To analyze this hypothesis, we choose a skewed distribution to represent imbalance by removing $90\%$ of the samples of all classes except class zero. This produces 5000 samples of class zero and 500 samples each for other classes in the training set.\\
% In the imbalanced setting, the overall accuracy in no-attack is $68\%$ which is close to its balanced accuracy of $70.82\%$. However, this accuracy is biased towards class 0, which has the highest per-class accuracy of $86.2\%$($75.8\%$ in the balanced scenario), since it has ten times more samples than the rest. The mean per-class accuracy is $52\%$ as it ignores the class imbalance. This result is expected because a model with reduced samples should lose some utility.


\subsection{MNIST~\cite{lecunmnist}}\label{setup:mnist}
MNIST is a 10-class digit image classification dataset, which contains 70,000 grayscale images of size 28 $\times$ 28. We consider 100 FL clients and divide all data using Dirichlet or FCJ distributions. We use the same CNN as the FEMNIST dataset.

\noindent\textbf{Hyperparameters for re-eval:} For MNIST, we run over 2000 epochs with 100 clients, a learning rate of $0.03$, and a batch size of 32. In the attack setting, 20 clients are malicious. We set $T_w=20$, and $T_c=10$. The FL algorithm used here is FedSGD. The results reported in Figure~\ref{fig:fdr_recovery_benign_attack} use the FCJ distribution.

\subsection{Fashion-MNIST~\cite{xiao2017fashion}}\label{setup:fashion}
Fashion-MNIST is a 10-class image classification dataset with grayscale images of clothing of size 28 $\times$ 28. It contains 70,000 total images. 
We consider 100 FL clients and divide all 70,000 images using Dirichlet or FCJ distributions.
For CIFAR10, MNIST, and FashionMNIST, we divide each client's data in train/test/validation splits in the ratio of $10:1:1$. We combine clients' validation data and use it for validation and hyperparameter tuning and report accuracy on test data. We use the same CNN as the FEMNIST dataset.

\noindent\textbf{Hyperparameters for re-eval:} We run over 2000 epochs with 100 clients, a learning rate of $3\times10^{-3}$~\footnote{We could not achieve the same accuracy reported in~\cite{cao2022fedrecover} using their reported $3\times10^{-4}$ learning rate, hence we use $3\times10^{-3}$.}, and a batch size of $32$. In the attack setting, 20 clients are malicious. We set $T_w=20$, and $T_c=10$. The FL algorithm used here is FedSGD. The results reported in Figure~\ref{fig:fdr_recovery_benign_attack} use the FCJ distribution.

\subsection{StackOverflow~\cite{stackoverflow2019}}\label{setup:stackoverflow}
StackOverflow is a language-modeling dataset that is used for tag prediction and next-word prediction. It consists of 342,477 users who are used as clients, and the training data consists of 135,818,730 examples. We use an RNN with a 96-dimensional embedding and a 10000-word vocabulary. The complete network consists of an input layer followed by an embedding layer, an LSTM layer, and two dense layers.
We use the cross-device setting to obtain the baseline in~\cite{reddi2020adaptive} by using the fedjax~\cite{fedjax2021} framework. The FL algorithm is $FedAdam$, and the training consists of 1500 rounds with 50 clients chosen every round with one local epoch. We keep the batch size to 16, the client optimizer as SGD with a learning rate of $10^{-3}$, and Adam as the server optimizer with a learning rate of $10^{-2}$.