%\vspace{-0.2cm}
\section{Background}\label{sec:background}
% \yasra{maybe after each category (attack, defense), we add our choices at the end instead of making a separate subsection?}
% \begin{itemize}
%     \item Federated Learning
%     \item FL Attacks
    
%     \item Attacks used in our study
%     \item Threat model in our study
%     \item FL Defenses
%     \item Defenses used in our study
%     \item Justify every choice over here
% \end{itemize}
%\vspace{-0.1cm}
\subsection{Federated Learning (FL)}\label{background:FL}
In FL~\cite{kairouz2019advances, mcmahan2017communication, konevcny2016federated}, a service provider, called \emph{server}, trains a \emph{global model}, $\theta^g$, on the private data from multiple collaborating clients, all without directly collecting their individual data.
During the $t^{th}$ FL round, the server selects $n$ out of total $N$ clients and shares the most recent global model ($\theta^t_g$) with them. 
Then, a client $k$ uses their local data $D_k$ to compute an update $\nabla^t_k$ and shares it with the server. These updates serve as a client's contribution towards refining a global model. 
Depending on how a client computes their update, FL algorithms can be broadly divided~\cite{mcmahan2017communication} into \emph{FedSGD} and \emph{FedAvg}. 
In \emph{FedSGD}, a client computes the update by sampling a subset $b$ from their local data and calculating a gradient of loss $\ell(b;\theta^t_g)$ of the global model on the subset, i.e., $\nabla^t_k = {\partial \ell(b; \theta^t_g)}/{\partial \theta^t_g}$. In \emph{FedAvg}, a client $k$ \emph{fine-tunes} $\theta^t_g$ on their local data using stochastic gradient descent (SGD) for a fixed number of local epochs $E$, resulting in an updated local model $\theta^t_k$. The client then computes their update as the difference $\nabla^t_k= \theta^t_k-\theta^t_g$ and shares $\nabla^t_k$ with the server.
Next, the server computes an aggregate of client updates using an AGR, $f_\mathsf{agg}$(such as mean), i.e., using $\nabla^t_\mathsf{agg}= f_\mathsf{agr}(\nabla^t_{\{k\in[n]\}})$. Finally, the server updates the global model of the $(t+1)^{th}$ round using SGD as $\theta^{t+1}_g\leftarrow \theta^{t}_g+\eta\nabla^t_\mathsf{agg}$ with server's learning rate $\eta$. Due to these differences, FedAvg achieves faster convergence and attains higher accuracy than FedSGD~\cite{mcmahan2017communication}.

After discussing the update process of FL models and the collaboration between servers and clients, we present the FL applications (\emph{where it is deployed}) and setups (\emph{how it is used}). There are two main types of deployments: \textbf{cross-device} and \textbf{cross-silo}, as explained in~\cite{kairouz2019advances}.
In \emph{cross-device FL}, $N$ is very large (ranging from a few thousand to billions)~\cite{reddi2020adaptive}, and only a tiny fraction of them are chosen in each FL training round ($n\ll N$). These clients are typically resource-constrained devices such as mobile phones, smartwatches, and other IoT devices~\cite{gboard, zhao2022multimodal}. Contrastingly, in \emph{cross-silo FL}, $N$ is moderate (up to 100)~\cite{li2021model}, and all clients are selected in each round ($n=N$). These clients are typically large corporations, including banks~\cite{webankcredit} and hospitals~\cite{nguyen2022federated}.
% \vspace{-0.3cm}
\subsection{Poisoning Attacks in FL}\label{background:attacks}
\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/defenses_systemization2.pdf}
% \vspace{-0.35cm}
\caption{Systemizing FL Defenses: Categorization of defenses based on their \emph{defense phases}. Processing operations for client updates are color-coded: filtering (blue), re-weighing (green), and modification (purple). Defenses belonging to the same category of processing may have different phases, e.g., FedRecover performs update modification at the server, while Ditto performs update modification at the client. Dependencies are also highlighted, such as FedRecover utilizing stored local and global models for estimation.}
\label{fig:defenses_systemization}
% \vspace{-0.6cm}
\end{figure}

\begin{table*}[]
\caption{A systematic overview of FL defenses, helping practitioners in 1) selecting defenses aligned with their use cases, 2) combining multiple defenses for heightened performance, and 3) designing new defense approaches by gaining insights into the FL defenses.}
% \vspace{-0.3cm}
\label{tab:systemization}
\scriptsize
\resizebox{1\textwidth}{!}{%
\begin{tabular}{|l|l|l|p{0.38\linewidth}|l|}
\hline
% \tiny
% small % Set the font size to small
\textbf{Dimension} & \textbf{Types} & \textbf{Attributes} & \textbf{Description} & \textbf{Defenses} \\ \hline
\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Processing \\ of Client \\ Updates\end{tabular}}} & \multirow{2}{*}{Filtering} & Update-based & Filters updates by comparing update values with each other. & TrMean~\cite{yin2018byzantine}, Krum~\cite{blanchard2017machine}, GeometricMean~\cite{pillutla2019robust} \\ \cline{3-5} 
 &  & Metric-based & Filters updates by comparing metrics associated with each update. & MST-AD \& Density-AD~\cite{ranjan2022securing}, LF-Fighter~\cite{jebreel2022defending}, ERR \& LFR~\cite{fang2020local}, FLDetector~\cite{zhang2022fldetector} \\ \cline{2-5} 
 & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Update\\  re-weighing\end{tabular}} & Similarity-based & Filters updates by comparing their similarity with a reference. & CONTRA~\cite{awan2021contra}, FLTrust~\cite{cao2020fltrust} \\ \cline{3-5} 
 &  & Loss-based & Applies weights based on loss for an update. & Sageflow~\cite{park2021sageflow}, Anomaly Detection~\cite{li2020learning} \\ \cline{3-5}
 &  & Optimization-based & Applies weights based on an optimization problem. & SmartFL~\cite{xie2022robust} \\ \cline{2-5} 
 & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Update \\ modification\end{tabular}} & Scaling/Clipping & Scales or clips an update if it exceeds a certain threshold. & Norm Bounding~\cite{sun2019can}, Signguard~\cite{xu2021signguard} \\ \cline{3-5} 
 &  & Distillation & Distills an update into a low-dimensional vector. & Cronus~\cite{chang2019cronus}, Auror~\cite{shen2016auror} \\ \cline{3-5} 
 &  & Regularization & Introduces regularization to control robustness and privacy. & Ditto~\cite{li2021ditto} \\ \cline{3-5} 
 &  & Estimation & Estimates a benign update from historical information. & FedRecover~\cite{cao2022fedrecover} \\ \hline
 % &  &  &  \\ 
 \hline
\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Server's \\ Knowledge\end{tabular}}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Knowledge \\ of data\end{tabular}} & No-knowledge & Server has no knowledge of data or its distribution at the client side. & TrMean~\cite{yin2018byzantine}, FedRecover~\cite{cao2022fedrecover}, FLDetector~\cite{zhang2022fldetector}, Cronus~\cite{chang2019cronus} \\ \cline{3-5} 
 &  & Partial knowledge & The server uses a small auxiliary dataset. & Sageflow~\cite{park2021sageflow}, SmartFL~\cite{xie2022robust}, FLTrust~\cite{cao2020fltrust} \\ \cline{2-5} 
 & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Knowledge \\ of updates\end{tabular}} & No-knowledge & Server does not have knowledge of local model updates. & Auror~\cite{shen2016auror}, Cronus~\cite{chang2019cronus} \\ \cline{3-5} 
 &  & Full knowledge & Server has complete knowledge of local model updates. & FLCert~\cite{cao2022flcert}, Krum~\cite{blanchard2017machine}, Anomaly Detection~\cite{li2020learning} \\ \hline
 % &  &  &  \\ 
 \hline
\multirow{4}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Defense \\ Phase\end{tabular}}} & \multirow{2}{*}{Aggregation} & Pre-aggregation & Processing of client updates is done before updates are aggregated. & MST-AD \& Density-AD~\cite{ranjan2022securing}, Bulyan~\cite{mhamdi2018the}, TrMean~\cite{yin2018byzantine}, Krum~\cite{blanchard2017machine}, Sageflow~\cite{park2021sageflow} \\ \cline{3-5} 
 &  & Post-aggregation & Processing of client updates is done after aggregation. & FLCert~\cite{cao2022flcert}, ERR \& LFR~\cite{fang2020local} \\ \cline{2-5} 
 & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Non-\\ aggregation\end{tabular}} & Local training & The defense component is part of the local training. & FLIP~\cite{zhang2022flip}, Ditto~\cite{li2021ditto}, Cronus~\cite{chang2019cronus} \\ \cline{3-5} 
 &  & Post-training & The defense is applied after training. & FedRecover~\cite{cao2022fedrecover} \\ \hline
\end{tabular}}
% \vspace{-0.5cm}
\end{table*}
There are various poisoning attacks in literature~\cite{blanchard2017machine, baruch2019a, bhagoji2019analyzing, bagdasaryan2018how, mhamdi2018the, fang2020local, mahloujifar2019universal, xie2019fall, munoz2017towards, shejwalkar2021manipulating}.
An \emph{untargeted} poisoning attack aims to lower the test accuracy for all test inputs indiscriminately~\cite{fang2020local, baruch2019a, mhamdi2018the, mahloujifar2019universal, xie2019fall}. A \emph{targeted} poisoning attack~\cite{bhagoji2019analyzing, bagdasaryan2018how} lowers the accuracy on specific test inputs.
For instance, in \emph{backdoor attacks}~\cite{bagdasaryan2018how} (a sub-category of targeted attacks), the goal is to misclassify only those test inputs that have an embedded \emph{backdoor trigger}. Since these attacks only affect a subset of inputs, they are much weaker than untargeted attacks.




We can also divide the attacks based on the adversary's capabilities. In model poisoning attacks~\cite{fang2020local, baruch2019a, mhamdi2018the, xie2019fall, bhagoji2019analyzing, bagdasaryan2018how, shejwalkar2021manipulating}, the adversary is strong enough to access and perturb the model gradients on malicious devices before they are sent to the server in every training round. A data-poisoning adversary~\cite{munoz2017towards} is much weaker than the model poisoning adversary~\cite{baruch2019a, fang2020local, shejwalkar2022back, shejwalkar2021manipulating} as it can only poison the datasets on malicious devices. 
% On the other hand, a model poisoning adversary can directly manipulate model gradients and have a much more substantial impact on the global model accuracy.
%\vspace{-0.3cm}
\subsubsection{Attacks used in our Study}\label{background:attacks_study}
In our evaluation, we focus on untargeted model poisoning attacks \emph{as they are stronger}~\cite{shejwalkar2022back}.
% , as they have demonstrated superior performance compared to targeted attacks~\cite{shejwalkar2022back}. \yasra{why we are just citing Virat's work? we should also cite some other work here and in other places. Reviewers will focus heavily on this and may ask to draw a distinction between this and Virat's work. }
% The selected attacks for our study include the following: We use the following untargeted model poisoning attacks for evaluations in this work, as they generally outperform other attacks such as targeted attacks~\cite{shejwalkar2022back}.

\noindent\textbf{Stat-Opt~\cite{fang2020local}:} provides a generic model poisoning method and tailors it to specific AGRs such as TrMean~\cite{yin2018byzantine}, Median~\cite{yin2018byzantine}, and Krum~\cite{blanchard2017machine}. The adversary first calculates the mean of the benign clients' updates, $\nabla^b$, and finds the \emph{static} malicious direction $w = -sign(\nabla^b)$. 
It directs the benign average along the calculated direction and scales it with $\gamma$ to obtain the final poisoned update, $-\gamma w$.

\noindent\textbf{Dyn-Opt~\cite{shejwalkar2021manipulating}:} proposes a general poisoning method and tailors it to specific AGRs, similar to Stat-Opt. The key distinction lies in the \emph{dynamic} and \emph{data-dependent} nature of the perturbation. The attack starts by computing the mean of benign updates, 
% The difference is that the perturbation is \emph{dynamic} and \emph{data-dependent}. The attack starts by computing the benign updates' mean, 
$\nabla^b$, and a data-dependent direction, $w$. The final poisoned update is calculated as $\nabla^` = \nabla^b + \gamma w$, where the attack finds the largest $\gamma$ that can bypass the AGR. They compare their attack with Stat-Opt and show that the dataset-tailored $w$ and optimization-based scaling factor $\gamma$ make their attack a much stronger one.
% \vspace{-0.3cm}
\subsubsection{Threat Model in Our Study}\label{background:threat_model}
Below, we detail the threat models for poisoning attacks used in our study.

\noindent\textbf{Goal:}
Our adversary is \emph{untargeted}, crafts malicious updates and sends them to the server, where, upon aggregation with other updates, it indiscriminately lowers the accuracy of the global model for all test inputs.



\noindent\textbf{Knowledge:} In line with most defense approaches, we assume that the adversary knows the AGR used by the server.
Unless explicitly specified, the adversary has complete knowledge of the gradients of both malicious and benign clients. In some cases, we employ a partial knowledge adversary, where the adversary knows the server's AGR but not the gradients of benign clients.

\noindent\textbf{Capabilities:} Our untargeted model poisoning adversary controls $m$ out of $N$ clients. The adversary is strong enough to manipulate model updates of the malicious clients it controls and has access to the global model parameters shared every round. 
We set the proportion of malicious clients at 20\% (unless stated otherwise), a common benchmark in prior studies~\cite{shejwalkar2021manipulating, fang2020local, cao2020fltrust, cao2022fedrecover}, which also examined how varying this percentage impacts the severity of attacks. To ensure consistency and comparability with these works, we adhere to the same 20\% setting in our implementation.
% We assume $20\%$ malicious clients unless stated otherwise since several prior works~\cite{shejwalkar2021manipulating, fang2020local, cao2020fltrust, cao2022fedrecover} use $20\%$ malicious clients. Each of them also performs an ablation over the percentage of malicious clients to show that it directly correlates with the attack impact. Therefore, to align and compare our implementation with previous works, we match their original setting of $20\%$ malicious clients.

% While practical FL settings may not have such high percentages of malicious clients~\cite{shejwalkar2022back}\yasra{CITE multiple}, the use of 20\% aligns with prior defense works aiming to demonstrate efficacy even in highly adversarial conditions.

% Most defense works assume very high percentages of malicious clients to demonstrate that their defenses work even in highly adversarial settings. Hence, although unreasonable in practical FL settings~\cite{shejwalkar2022back}, we follow prior defense works and use 20\% malicious clients to stay consistent with the assumptions made by prior works.} 
 


% \begin{figure}[t]
% \centering
% \includegraphics[scale=.4]{figures/choice_of_defense.pdf}
% \caption{FL Defense Categories}
% \label{fig:choice_of_defense}
% \vspace{-0.4cm}
% \end{figure}




% \begin{table*}[]
% \caption{Systemization of FL Defenses. \yasra{Cite paper or two for each attribute.}}
% \vspace{-0.3cm}
% \label{tab:systemization}
% \begin{tabular}{|l|l|l|p{0.4\linewidth}|} % Adjust the width as needed
% \hline
% \textbf{Dimension} & \textbf{Types} & \textbf{Attributes} & \textbf{Description} \\ \hline
% \multirow{7}{*}{Update Operation} & \multirow{2}{*}{Filtering} & Update-based & Filters updates by comparing update values with each other. \\ \cline{3-4} 
%  &  & Metric-based & Filters updates by comparing loss/error for each update. \\ \cline{2-4} 
%  & \multirow{3}{*}{Re-weighing updates} & Similarity-based & Filters updates by comparing their similarity with a reference. \\ \cline{3-4} 
%  &  & Loss-based & Applies weights based on loss for an update. \\ \cline{3-4} 
%  &  & Optimization-based & Applies weights based on an optimization problem. \\ \cline{2-4} 
%  & \multirow{2}{*}{Update modification} & Scaling/clipping & Scales or clips an update if it exceeds a certain threshold. \\ \cline{3-4} 
%  &  & Distillation & Distills an update into a low-dimensional vector. \\ \hline
%  \hline
%  % &  &  &  \\ \hline
% \multirow{4}{*}{Server's Knowledge} & \multirow{2}{*}{Knowledge of data} & No-knowledge & Server has no knowledge of data or its distribution at the client side. \\ \cline{3-4} 
%  &  & Partial knowledge & Server has some knowledge of data and its distribution on the client side. Uses its own small dataset to check the performance of local model updates by comparing their outputs or losses. \\ \cline{2-4} 
%  & \multirow{2}{*}{Knowledge of updates} & No-knowledge & Server does not have knowledge of local model updates. Instead, it receives distilled information from the clients. \\ \cline{3-4} 
%  &  & Full knowledge & Server has complete knowledge of local model updates because it collects them from every client. \\ \hline
%  % &  &  &  \\ \hline
%  \hline
% \multirow{4}{*}{Defense Phase} & \multirow{2}{*}{Aggregation} & Pre-aggregation & Update operations are performed; for example, update-based filtering, and then the updates are aggregated. \\ \cline{3-4} 
%  &  & Post-aggregation & Updates are compared after aggregation. For example, in metric-based filtering, updates are filtered after aggregation by applying some metric-based technique. \\ \cline{2-4} 
%  & \multirow{2}{*}{Non-aggregation} & Local training & The defense component is part of the local training. For example, in Ditto, a regularization term is added to the local training objective that acts as a knob to control model drift and robustness. \\ \cline{3-4} 
%  &  & Post-training & The defense is applied after one set of training because it needs all the past information; for example, FedRecover. \\ \hline
% \end{tabular}
% \end{table*}




% \subsection{Defenses in Federated Learning}\label{background:defenses}
% Many defensive techniques are proposed in the literature to counter the threat of poisoning in Federated Learning. These defensive techniques come in different flavors and operate differently, but they have one common goal: to make FL robust against poisoning attacks. Dimension-wise filtering removes malicious components of client updates and then performs aggregation, e.g., Trimmed-mean~\cite{xie2018generalized, yin2018byzantine} and Median~\cite{xie2018generalized, yin2018byzantine}. Vector-wise filtering mechanisms such as Krum~\cite{blanchard2017machine} and Bulyan~\cite{mhamdi2018the} remove entire malicious updates before aggregation based on their Euclidean distances from each other. Malicious client detection techniques such as FLDetector~\cite{zhang2023fldetector} detect and remove entire clients from the training process. A scaling defense such as Norm-bounding~\cite{sun2019can} scales the norms of malicious updates to reduce their impact on the global model. Cronus~\cite{chang2019cronus} is a knowledge-transfer-based defense as it reduces the dimensionality of the client updates shared with the server because lower dimensionality leads to higher robustness. Personalization techniques such as Ditto~\cite{li2021ditto} fine-tune the poisoned global model on the client's data to optimize its performance for each client separately. Distinct from all these defenses, recently, a recovery technique FedRecover~\cite{cao2022fedrecover} was proposed, where they recover a poisoned global model from historical information.

% \subsubsection{Defenses Used in Our Study}\label{background:defenses_study}
% Though the FL defense literature is rich, we choose three representative defense techniques in our studies as they are distinct in their nature of defending against poisoning attacks, as shown in
% Figure~\ref{fig:defenses_systemization}: Trimmed mean is a filtering mechanism, removing malicious components of client updates dimension-wise \textit{during training}. FLDetector is designed to \textit{detect} malicious clients \textit{during training} by analyzing their updates. It is important to note that although Trimmed-mean and FLDetector seem similar in their filtering mechanisms, they are very different and have distinct approaches. Trimmed-mean filters \emph{components} of updates before aggregating them. FLDetector removes entire \emph{clients} from the training process if they deviate too much from an estimated reference model calculated using historical information. FedRecover focuses on \textit{recovery} from a previously poisoned global model \textit{after} training has been done. Below are their details.

% \noindent \textbf{Trimmed Mean (\emph{TrMean})}~\cite{yin2018byzantine}
% \label{background:trmean}
% is a foundational defense used in advanced AGRs~\cite{cao2022fedrecover,zhang2022fldetector,shejwalkar2021manipulating}. It sorts each input dimension $j$ of the client updates, discards the $m$ largest and smallest values (where $m$ indicates compromised clients), and averages the rest.

% \paragraphb{FLDetector~\cite{zhang2022fldetector}}\label{background:fld}
% is designed to \emph{detect} and eliminate malicious clients, ensuring a byzantine-robust FL system obtains a precise global model. FLDetector operates on the principle that malicious updates, tainted by adversaries, differ statistically from benign ones.\\
% For discerning these updates, the server estimates a global model update for client $k$ at round $t$ using the L-BFGS algorithm: ($\hat{\nabla}_{t}^{k} = \nabla_{t-1}^{k} + \hat{H}^{t}\cdot(\theta_{t} - \theta_{t-1})$). Here, $\nabla_{t-1}^{k}$. The server retains past $N$ global model differences ($\Delta \theta_{t}$) and updates differences ($\Delta \nabla_{t-1}$) to compute the \emph{HVP(Hessian Vector Product)} with \emph{L-BFGS}. It then gauges a client's \emph{suspicious score} by comparing actual and predicted updates through their Euclidean distance. With scores from the last $N$ rounds, clients are clustered via Gap statistics~\cite{tibshirani2001estimating} and K-means. The group with higher average scores is deemed malicious. Upon detecting a rogue client, the server ceases training, removes the offender, and restarts training to achieve enhanced accuracy.

% \paragraphb{FedRecover~\cite{cao2022fedrecover}}\label{background:fdr}
% aims to \emph{recover} an FL global model compromised by a poisoning attack. In the \emph{original training phase}, for each round $t$, FedRecover saves $\nabla^k_t$ from client $k$ and global models $\theta^t_g$. This data is used as \emph{historical information} in the \emph{recovery phase}, which consists of the following stages.
% In the \emph{warmup phase}, the server requests clients' \emph{exact updates} for the initial $T_w$ rounds. In the \emph{estimation phase}, the server \emph{estimates} client updates each round, with $\hat{\nabla}^t_k$\ representing client $k$'s estimated update at round $t$. This estimate is derived using the L-BFGS algorithm~\cite{nocedal1980updating} based on the original global model, client update, and recovered global model. The model's estimated update is defined as $\hat{\nabla}^k_t = \overline{\nabla}^t_k + \Tilde{H}^t_k(\hat{\theta}^t_g - \overline{\theta}^t_g)$, where the latter term is the HVP(Hessian Vector Product).

% Every $T_c$ rounds, to ensure the estimated global model $\hat{\theta}^t_g$ aligns closely with the accurate model, the server initiates a \emph{periodic correction} by requesting \emph{exact updates}. If any component of a client's estimated update surpasses the \emph{abnormality threshold} $\tau$, that client is prompted for an exact update.
% In the final \emph{fine-tuning phase}, spanning $T_f$ rounds, clients are asked to provide their exact updates,$\nabla^t_k$, to refine the global model by eliminating potential estimation errors.
