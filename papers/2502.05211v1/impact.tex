% \vspace{-0.2cm}
\section{Impact Analysis of Pitfalls}\label{sec:impact}
% \begin{itemize}
%     \item Go through each pitfall and explain its impact on Trimmed Mean, FedRecover, and FLDetector. Since you use trimmed mean for human activity recognition, next-word prediction, and image classification, include evaluation for these with trimmed mean where applicable.
%     \item Explain why you needed to perform experiments on different modalities and tasks.
%     \item Justify why you performed the experiment whose results you have reported. What do those results signify?
%     \item Provide concrete recommendations with every pitfall.
% \end{itemize}
In this section, we 
% comprehensively 
analyze the impact of each identified pitfall. We test representative defenses across diverse setups by following the recommendations outlined in \S\ref{sec:pitfalls} and scrutinizing their implications. This systematic exploration is intended to help researchers make informed decisions about the robustness of FL defenses.

\begin{figure}[t]
\centering
\hspace*{-.5cm}
\includegraphics[width=.9\columnwidth]{figures/FedRecover_recovery_from_benign_and_attack.pdf}
% \vspace{-0.3cm}
\caption{Performance of FedRecover on four datasets, both with and without trim attack in the original training.}
% \vspace{-0.4cm}
\label{fig:fdr_recovery_benign_attack}
\end{figure}
% \vspace{-0.25cm}
\subsection{Intrinsically Robust Datasets}\label{impact:datasets}
First, we evaluate how the selection of datasets impacts the robustness of our three representative FL defenses: TrMean, FedRecover, and FLDetector.
% \vspace{-0.25cm}
\subsubsection{TrMean is only robust with MNIST}\label{impact:datasets:trmean}
% \noindent\textbf{1) TrMean is robust with MNIST only:}\label{pitfalls:datasets:trmean}
% To observe the effects of different datasets on TrMean, we implement FedAvg and FedSGD with four datasets, MNIST, FashionMNIST, CIFAR10, and FEMNIST, under no-attack and Stat-Opt attack~\cite{fang2020local} attack.
% We plot our results in Figure~\ref{fig:fl_baseline}, where the intrinsic robustness of MNIST is evident from Figure~\ref{fig:baseline_mnist}. Stat-Opt attack with a very high ($20\%$) percentage of malicious clients reduces the accuracy of the MNIST-trained FL model by less than $1\%$ for the FedAvg setting. The accuracy drop is much higher for the other three datasets, with the highest being about $50\%$ for CIFAR10.
To assess TrMean's sensitivity to different datasets, we employ FedAvg and FedSGD on MNIST, FashionMNIST, CIFAR10, and FEMNIST, both without attacks and under Stat-Opt~\cite{fang2020local}. Results in Figure~\ref{fig:fl_baseline}a highlight MNIST's intrinsic robustness. Despite a high ($20\%$) amount of malicious clients, MNIST-trained FL model accuracy drops less than $1\%$ in FedAvg, while other datasets experience more substantial declines, peaking at $50\%$ for CIFAR10.

% The reason for this performance variation lies in the complexity of these tasks. Although widely used, MNIST is a very old and basic dataset that consists of ten digits, which are somewhat easier to distinguish when compared to other datasets. On the other hand, FashionMNIST consists of clothing articles and is a slightly tougher task than digits. FEMNIST is the extended version of MNIST as it incorporates lower-case and upper-case alphabets in addition to the digits. These three datasets are single-channel grayscale images, but CIFAR10 consists of RGB images with a more complex visual task consisting of animals and vehicles. This complexity is evident from the baseline accuracies in Figure~\ref{fig:fl_baseline}, where we can see that the no-attack accuracy for MNIST is higher than the other three, and CIFAR10 has the lowest.
The variation in performance can be attributed to task complexity. 
% MNIST, being a basic dataset of digits, is easier to distinguish\yasra{cite}. FashionMNIST, featuring clothing articles, poses a slightly more challenging task\yasra{cite, this can not just be observational, anyone else claims that? If yes, cite them here for all the next three sentences.}. FEMNIST extends MNIST with alphabets and has a very large number (805263) of samples\yasra{cite}. In contrast, CIFAR10 involves RGB images and a more intricate task of identifying animals and vehicles\yasra{cite}. 
Baseline accuracies in Figure~\ref{fig:fl_baseline} reflect this complexity, with MNIST having the highest no-attack accuracy and CIFAR10 the lowest.
From this set of experiments, we can conclude that \emph{TrMean is highly robust using MNIST-based evaluations, but not with other datasets as the evaluations of TrMean using other three datasets, FashionMNIST, FEMNIST, and CIFAR10 show.}
% \vspace{-0.25cm}
\subsubsection{FedRecover works better with simple datasets}\label{impact:datasets:fdr}
% \noindent\textbf{2) FedRecover works better with simple datasets:}\label{impact:datasets:fdr}
We evaluate FedRecover on FashionMNIST, FEMNIST, and CIFAR10, in addition to MNIST, since  MNIST is heavily evaluated in ~\cite{cao2022fedrecover}. We test FedRecover under \emph{recovery-from-benign} and \emph{recovery-from-attack} scenarios. In the former, no attack occurs during original training, while in the latter, Stat-Opt attack is applied during original training but not during recovery (we discuss attack during recovery in \S\ref{impact:attacks:fdr}). Consistent results with~\cite{cao2022fedrecover} on MNIST and FashionMNIST validate our implementation.

Figure~\ref{fig:fdr_recovery_benign_attack} shows that even without attacks, FedRecover does not fully recover for complex datasets like FEMNIST and CIFAR10. In recovery from an attack for MNIST (Fashion-MNIST), FedRecover achieves 91\% (72.7\%) recovery accuracy from a post-attack accuracy of 80\% (64\%), where the original training accuracy is 92\% (75.2\%). For FEMNIST and CIFAR10, differences between the baseline accuracy and recovery accuracies are 11\% and 19\%, respectively, indicating an incomplete recovery in the attack setting. This performance variation stems from dataset complexity, with estimation errors higher for complex tasks such as FEMNIST. The impact of \emph{periodic correction} and \emph{warmup} phase on estimation error and recovery performance is discussed in \S\ref{impact:algorithm}, \S\ref{impact:scalability}, and \S\ref{impact:attacks}.

\begin{table}[]
\caption{Impact of data-level perturbations on FLDetector.}
% \vspace{-0.3cm}
\scriptsize
\resizebox{.88\columnwidth}{!}{%
\begin{tabular}{|c|c|cc|cc|}
\hline
\multirow{2}{*}{dataset} & \multirow{2}{*}{pertubation} & \multicolumn{2}{c|}{FedSGD} & \multicolumn{2}{c|}{FedAvg} \\ \cline{3-6} 
                         &                & \multicolumn{1}{c|}{FPR} & FNR & \multicolumn{1}{c|}{FPR} & FNR \\ \hline
\multirow{2}{*}{MNIST}   & Noisy-features & \multicolumn{1}{c|}{0}   & 0   & \multicolumn{1}{c|}{0}   & 0   \\ \cline{2-6} 
                         & Noisy-label    & \multicolumn{1}{c|}{0}   & 0   & \multicolumn{1}{c|}{0}   & 0   \\ \hline
\multirow{2}{*}{Fashion} & Noisy-features & \multicolumn{1}{c|}{0}   & 0   & \multicolumn{1}{c|}{0}   & 0   \\ \cline{2-6} 
                         & Noisy-label    & \multicolumn{1}{c|}{0}   & 0   & \multicolumn{1}{c|}{0}   & 0   \\ \hline
\multirow{2}{*}{FEMNIST} & Noisy-features & \multicolumn{1}{c|}{0}   & 1   & \multicolumn{1}{c|}{0}   & 1   \\ \cline{2-6} 
                         & Noisy-label    & \multicolumn{1}{c|}{0}   & 1   & \multicolumn{1}{c|}{0}   & 1   \\ \hline
\end{tabular}%
}
\label{tab:fld_data_distribution}
% \vspace{-0.3cm}
\end{table}
% \vspace{-0.5cm}
% \vspace{-0.25cm}
\subsubsection{FLDetector's performance varies with task complexity}\label{impact:datasets:fld}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{1.06\linewidth}
        \hspace{-.6cm}
        \includegraphics[width=\linewidth]{figures/num_samples_per_user_new.pdf}
        % \vspace*{-3mm}
        \caption{Histogram of number of samples per client.}
        \label{fig:num_samples_per_user}
    \end{subfigure}
    % \hspace{0.5mm}
    \begin{subfigure}[b]{1.06\linewidth}
        \hspace{-.6cm}
        \includegraphics[width=\linewidth]{figures/num_classes_per_user.pdf}
        % \vspace*{-3mm}
        \caption{Histogram of number of classes per client.}
        \label{fig:num_classes_per_user}
    \end{subfigure}
    % \vspace{-0.4cm}
    \caption{Comparison of Sample and Class Distribution in FL Datasets: Histograms illustrating (a) the number of samples and (b) the number of classes per client, generated using FCJ and Dir distributions. 
    % Histograms of (a) no. of samples and (b) no. of classes per client for FL datasets generated using FCJ and Dirichlet (Dir) distributions. 
    From left to right, the non-i.i.d. degree of generated datasets increases, reflecting the impact of higher FCJ (or Dir) parameters in generating more (or less) non-i.i.d. datasets. We note that all FCJ client datasets are almost the same size, while Dir client datasets are widely varying. Similarly, with FCJ, all clients have all the classes, while with Dir, the number of classes varies widely.}
    % \vspace{-0.5cm}
    \label{fig:num_samples_classes_per_user}
\end{figure}

% \noindent\textbf{3) FLDetector's performance varies with task complexity:}\label{pitfalls:datasets:fld}
% We want to evaluate the performance of FLDetector with varying task complexities. We assess the performance of FLDetector over three datasets: MNIST, FashionMNIST, and FEMNIST. We also introduce some minor perturbations in the features and labels of the datasets to increase the task complexity as we already know the results of these datasets from Table 2 in the unperturbed setting~\cite{zhang2022fldetector}. We can see in Table \ref{tab:fld_data_distribution} that the MNIST and FashionMNIST datasets are robust under both perturbations and FLDetector can perfectly detect malicious clients and achieves a zero FNR and FPR.
% However, we achieve an FPR of 0 and FNR of 1 in all conditions for FEMNIST, i.e., no attack is detected, and all the malicious clients are classified as benign and continue to be part of the training process.
We assess FLDetector's performance across varying task complexities using MNIST, FashionMNIST, and FEMNIST. To enhance task complexity, we introduce minor perturbations in features and labels, as we already know the results of these datasets in the unperturbed setting~\cite{zhang2022fldetector}. Table~\ref{tab:fld_data_distribution} shows that MNIST and FashionMNIST datasets remain robust, with FLDetector achieving perfect detection, i.e., zero FNR (False Negative Rate) and FPR (False Positive Rate). However, for FEMNIST, FLDetector shows an FPR of 0 and FNR of 1 across all conditions, indicating that it fails to detect any attacks, classifying all malicious clients as benign and allowing them to remain in the training process.
% The reason for this performance variation across datasets also lies within the task complexity, which we have discussed extensively in \S\ref{impact:datasets:trmean} and \S\ref{impact:datasets:fdr}.
% Since MNIST and FashionMNIST are less affected by the Stat-Opt attack (Figure~\ref{fig:fl_baseline}), their benign updates are close together, and it is difficult for a malicious update to hide among them. Consequently, it becomes easier for FLDetector to differentiate between malicious and benign updates. \emph{On the other hand, FEMNIST, a naturally distributed dataset, is a much more complex task than MNIST and FashionMNIST. This makes client updates far apart from each other, enabling a malicious update to blend in easily without being detected by FLDetector}.

Performance variations across datasets stem from task complexity, extensively discussed in \S\ref{impact:datasets:trmean} and \S\ref{impact:datasets:fdr}. MNIST and FashionMNIST, less affected by the Stat-Opt attack (Figure~\ref{fig:fl_baseline}), have closely clustered benign updates, making it difficult for malicious updates to be stealthy. This proximity aids FLDetector in distinguishing between malicious and benign updates. Conversely, FEMNIST, a naturally distributed and more heterogeneous dataset than MNIST and FashionMNIST (\S\ref{impact:distribution:statistical_analyses}), results in client updates being further apart. This increased distance enables a malicious update to blend in seamlessly, evading detection by FLDetector.
% This evaluation also relates to \emph{choice of attacks} in~\S\ref{impact:attacks:fld} where we show that FLDetector's performance lowers further under the influence of strong adaptive attacks as compared to the naive label-flipping attack used in Table~\ref{tab:fld_data_distribution}.
\subsection{Homogeneous Data Distribution}\label{impact:distribution}
In this section, we first show that the Dir (Dirichlet) distribution is more \emph{real-world} than the FCJ distribution through statistical analyses and visualization. Subsequently, we show the effect of these distributions and their varying levels of heterogeneity on the robustness of TrMean and FedRecover. The original work on FLDetector~\cite{zhang2022fldetector} already analyzed different levels of heterogeneity, so we skip this here.
% in their Figure 2, so we skip this here.
% \vspace{-0.15cm}
\subsubsection{Statistical analyses of FCJ and Dir distributions}\label{impact:distribution:statistical_analyses}
% \noindent\textbf{1) Statistical analyses of FCJ and Dir distributions:}\label{impact:distribution:statistical_analysis}
We consider a classification task with a total of C classes; we generate client datasets using Dir and FCJ for 100 clients with varying degrees of non-i.i.d. We provide analyses for CIFAR10, but it applies to other datasets. We then plot the following three statistics of the datasets:
% of the client datasets. 
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \hspace{-.6cm}
        \includegraphics[scale=.6]{figures/iid_femnist_tsne-2.pdf}
        % \vspace*{-.5cm}
        \caption{(Left) i.i.d. client datasets, (right) real-world FEMNIST datasets.}
        \label{fig:iid_femnist_tsne}
    \end{subfigure}
    % \hspace{0.5mm}
    \begin{subfigure}[b]{1\columnwidth}
        % \hspace{-.6cm}
        \includegraphics[width=\columnwidth]{figures/fcj_tsne.pdf}
        % \vspace*{-3mm}
        \caption{FCJ distributed client datasets.}
        \label{fig:fcj_tsne}
    \end{subfigure}
    % \hspace{0.5mm}
    \begin{subfigure}[b]{1\columnwidth}
        % \hspace{-.6cm}
        \includegraphics[width=\columnwidth]{figures/dir_tsne.pdf}
        % \vspace*{-3mm}
        \caption{Dir distributed client datasets.}
        \label{fig:dir_tsne}
    \end{subfigure}
    % \vspace{-0.6cm}
    \caption{T-SNE projections of class frequency vectors of client datasets generated using FCJ (b) and Dir (c) distributions. From left to right, non-i.i.d. degree increases.}
    % \vspace{-0.5cm}
    \label{fig:tsne_dir_fcj}
\end{figure}

\noindent \textbf{(Stat-1):} We plot the number of samples per user, which is motivated by the visualizations of real-world FL datasets in Leaf (Figure~\ref{fig:femnist_hist}).
% \footnote{https://leaf.cmu.edu} 
Figure~\ref{fig:num_samples_per_user} shows the results for three degrees of non-i.i.d. For Dir we use $\alpha\in\{0.1, 0.3, 0.5\}$ and for FCJ we use bias $b\in\{0.9, 0.5, 0.1\}$.
We note that Dir produces client datasets with heterogeneous sizes; the histograms are similar to real-world datasets in the Leaf repository (Figure~\ref{fig:femnist_hist}). However, FCJ makes client datasets with almost equal size; note that the FCJ histograms are always concentrated around 500 (the total number of samples in the CIFAR10 dataset / total number of clients).

\noindent \textbf{(Stat-2):} In Figure~\ref{fig:num_classes_per_user}, we show the number of classes each client has when we use Dir or FCJ. We observe that for FCJ, all clients have all the classes except when the bias is 0.9. In the real-world datasets, all clients generally do not have all the classes~\cite{mcmahan2017communication}. Similar to these real-world datasets, the clients have a widely varying number of classes in Dir distribution.

\noindent\textbf{(Stat-3):} For each client, we compute a C-dimensional vector where the $i^{th}$ dimension represents the number of samples from class $i$; here, we use CIFAR10 with 100 clients; hence we get 100 10-dimensional vectors. Then, we plotted T-SNE projections of these 100 vectors; we plotted them for both Dir and FCJ distributed client datasets. Figures~\ref{fig:fcj_tsne} and~\ref{fig:dir_tsne} show the projections for Dir and FCJ, respectively. 
For reference, we also show in Figure~\ref{fig:iid_femnist_tsne} how the T-SNE projections look like for (1) 100 clients with perfectly i.i.d. datasets and (2) the real-world FEMNIST dataset.

For FCJ distribution, we note that, for bias values greater than 0.2 (Figure~\ref{fig:fcj_tsne} center and right), the client datasets form local clusters, i.e., within these clusters, the clients have highly i.i.d. datasets. This is expected because FCJ forms C groups of clients where $i^{th}$ group gets the bias fraction of data from $i^{th}$ class and bias/($C-1$) fraction of data from other classes. This data is then randomly assigned to clients within the $i^{th}$ group, which makes these clients' datasets i.i.d. On the other hand, globally,  
these clusters form potentially non-i.i.d. structures. However, for a bias of 0.1, we observe almost i.i.d. datasets as expected; Figure~\ref{fig:iid_femnist_tsne}-left shows a perfectly i.i.d. dataset. To summarize, \emph{although both are globally non-i.i.d., we have shown that the FCJ distribution is locally-i.i.d., while Dir is locally non-i.i.d.}. Next, we study their impact.
% \vspace{-0.2cm}
\subsubsection{TrMean is more robust with lower heterogeneity}\label{impact:distribution:trmean}
% \noindent\textbf{2) TrMean is more robust with lower heterogeneity:}
We demonstrate the impact of FCJ and Dir on TrMean's robustness for FashionMNIST\footnote{We observe similar trends for other datasets, but for brevity, we only include FashionMNIST here.} in Figure~\ref{fig:trmean_distributions_fmnist}. In the no-attack setting, FCJ (DIR) shows little difference, with accuracy going from $89\%$ ($88\%$) at $0.1$ ($0.5$) bias to $84\%$ ($86\%$) at $0.9$ ($0.1$) bias. However, the distinction emerges in the attack setting where FCJ is more robust. For the Stat-Opt attack, FCJ (DIR) accuracy decreases from $84\%$ ($80\%$) at $0.1$ ($0.5$) bias to $75\%$ ($59\%$) at $0.9$ ($0.1$) bias. A similar trend is observed for the Dyn-Opt attack. The performance change is rooted in the nature of the distributions (\S\ref{impact:distribution:statistical_analyses}). FCJ is locally i.i.d. but globally non-i.i.d., while Dir is non-i.i.d. both locally and globally. Due to Dir's greater heterogeneity, benign updates are more dispersed, making it easier for a malicious update to hide. Consequently, TrMean struggles to detect malicious updates, leading to lower global model performance.

% By varying their distribution parameters, we can produce varying levels of non-i.i.d. datasets; for consistency, we use the parameters used in prior works~\cite{fang2020local,cao2022fedrecover,shejwalkar2022back,khan2023pitfalls}.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/trmean_distributions_fmnist.pdf}
        \caption{TrMean}
        \label{fig:trmean_distributions_fmnist}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/fedrecover_distributions_fmnist.pdf}
        \caption{FedRecover}
        \label{fig:fedrecover_distributions_fmnist}
    \end{subfigure}
    % \vspace{-0.3cm}
    \caption{The effect of varying heterogeneity levels for FCJ and Dir distributions on the FashionMNIST dataset.}
    % \vspace{-0.5cm}
    \label{fig:distributions_trmean_fedrecover}
\end{figure}
% \vspace{-0.15cm}
\subsubsection{FedRecover performs better under lower heterogeneity}\label{impact:distributions:fdr}
% \noindent\textbf{3) FedRecover performs better under lower heterogeneity:}\label{impact:distribution:fdr}
We show the impact of FCJ and Dir distributions on FedRecover for the FashionMNIST dataset 
% and show the results 
in Figure~\ref{fig:fedrecover_distributions_fmnist}.
We observe that the performance of FedRecover lowers when we increase the level of heterogeneity and that FCJ performs slightly better than Dir. Specifically, the difference between no-attack accuracies for FCJ at bias 0.3 and 0.7 is 7\% and 12.6\%, respectively, whereas, for Dir, that difference is slightly higher; 8.2\% and 13.3\% corresponding to non-i.i.d. parameters of 0.7 and 0.3 respectively\footnote{For FCJ, a higher value of the i.i.d. parameter means higher heterogeneity, but it is the opposite case for Dir.}.

The reason for FedRecover's performance reduction under high heterogeneity is that a higher heterogeneity means that client updates are far apart and drift away from the global model. Since \emph{the update estimation in FedRecover uses knowledge of the current and past global models, the HVP calculation step (\S\ref{background:fdr}) incurs significant estimation errors if the local model drifts away from the global model.}
% \vspace{-0.3cm}
\subsection{Slow-converging Algorithms}\label{impact:algorithm}
% \subsubsection{TrMean}\label{pitfalls:algorithm:trmean}
Here, we will study the impact of the choice of the two widely used FL algorithms, FedSGD and FedAvg, on the robustness of TrMean and FedRecover. We combine our analysis of algorithms with the choice of attacks for FLDetector in \S\ref{impact:attacks:fld}; therefore, we do not discuss it in this section.
% \noindent\textbf{1) Fast baselines make TrMean more robust.}\label{pitfalls:algorithm:trmean}
% \vspace{-0.3cm}
\subsubsection{Fast algorithms make TrMean more robust}\label{impact:algorithm:trmean}
In Figure~\ref{fig:fl_baseline}, we show the accuracy of FedSGD and FedAvg with TrMean in benign and Stat-Opt attack scenarios. For FedSGD, we align our implementation with recent defenses~\cite{cao2022fedrecover,fang2020local}. In contrast, FedAvg is optimized for greater accuracy with only $5\%$ to $20\%$ of FedSGD's communication (the number of rounds).

Under both benign and adversarial conditions, FedAvg greatly surpasses FedSGD in performance, convergence, and communication for all datasets, as shown in Figure~\ref{fig:fl_baseline}. 
% For example, with MNIST(CIFAR10) it achieves a \textcolor{red}{$x\%$($x\%$)} accuracy compared to the \textcolor{red}{$x\%$($x\%$)} accuracy with FedSGD. We perform the Stat-Opt attack and show the results in Figure~\ref{fig:fl_baseline} where the accuracy drop for MNIST(CIFAR10) is only \textcolor{red}{$x\%$($\%$)} compared to the \textcolor{red}{$x\%$($\%$)} accuracy drop using FedSGD.
% From this set of experiments, 
We can conclude here that in adversarial situations, \emph{FedAvg proves more resilient to untargeted poisoning due to its rapid convergence}, giving adversaries minimal time for poisoning.
% \vspace{-0.15cm}
\subsubsection{Fast algorithms lower estimation errors in FedRecover}\label{impact:algorithm:fdr}
% \noindent\textbf{2) Fast baselines provide a much higher starting point to FedRecover.}\label{pitfalls:algorithm:fdr}
We examine FedRecover's performance under different algorithms, FedSGD and FedAvg. The original study~\cite{cao2022fedrecover} applies the Stat-Opt attack to MNIST with FedSGD over 2000 rounds, reducing accuracy from 96\% to 81\%. By employing FedAvg with carefully chosen hyperparameters (Appendix~\ref{sec:setup}), as shown in Figure~\ref{fig:baseline_mnist}, we achieve 98\% accuracy in 50 rounds, with the Stat-Opt attack lowering it to only 96\%. This allows for perfect recovery, as depicted in Figure~\ref{fig:acc_vs_comm_mnist_fashion}, where both MNIST and FashionMNIST (no-attack accuracy: 87\%) show perfect recovery under the fast-converging FedAvg. This perfect recovery holds even with variations in the periodic correction periodicity $T_c$.
FedRecover excels in this scenario because \emph{FedAvg's rapid convergence (\S\ref{background:FL}) provides a high starting accuracy during FedRecover's \emph{warmup phase}.} Throughout periodic correction and abnormality fixing phases, \emph{client updates computed over multiple local epochs (in contrast to FedSGD's single local epoch) lead to lower estimation errors, ensuring perfect recovery.}
% \emph{Observing the performance of FedRecover and TrMean (\S\ref{impact:algorithm:trmean}) with FedAvg and the slower-converging FedSGD, we hope that future evaluations will use fast-converging FL algorithms, as they greatly reduce the impact of attacks.}
% \vspace{-0.2cm}
\subsection{Limited FL Settings}\label{impact:scalability}
To show the effect of \emph{scale} on FL defenses, we first assess the impact of a scale-constrained threat model on TrMean. We then show why FedRecover and FLDetector are incompatible with cross-device settings and discuss the practicality of storage, computation, and communication for FedRecover (or, any mechanism requiring stored historical information).
% As explained in \S\ref{background:FL}, the cross-device setting is generally preferred on a larger scale where the number of devices ranges from a few thousand to billions. In Pitfall 4 in \S\ref{pitfalls:4}, we report that $24\%$ of the works use the cross-device setting for evaluation.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/stackoverflow_trim.pdf}
        \caption{Trim Attack}
        \label{fig:stackoverflow_trim}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/stackoverflow_percentage_malicious.pdf}
        \caption{Varying \% malicious}
        \label{fig:stackoverflow_percentage_malicious}
    \end{subfigure}
    % \vspace{-0.2cm}
    \caption{Stackoverflow in practical settings.}
    % \vspace{-0.5cm}
    \label{fig:stackoverflow}
\end{figure}
% \vspace{-0.2cm}
\subsubsection{Mean AGR is robust on a large scale}\label{impact:scalability:stackoverflow}
In this section, we showcase the robustness of the mean AGR, typically not robust in cross-silo settings~\cite{yin2018byzantine}, within a large-scale, cross-device environment. \emph{Given the growing popularity of language models~\cite{touvron2023llama, brown2020language}, although underrepresented in our defense survey (Figure~\ref{fig:defense_survey}a), we leverage the StackOverflow~\cite{stackoverflow2019} dataset for large-scale evaluation} (setup details in Appendix~\ref{sec:appendix}). The no-attack baselines in Figure~\ref{fig:stackoverflow_trim} are obtained following the settings in~\cite{reddi2020adaptive}. We can see that with the standard amount of 20\% malicious clients, the Stat-Opt attack significantly impacts StackOverflow in cross-device.


The StackOverflow FL setting has about 300,000 clients and $20\%$ amounts to 60,000 malicious clients. Accessing and modifying such a vast number of devices is considered impractical, factoring in operational and financial costs~\cite{shejwalkar2022back}. Figure~\ref{fig:stackoverflow_percentage_malicious} shows a decline in attack performance as the percentage of malicious clients decreases. Notably, \emph{\textbf{with less than 5\% malicious clients, the Mean AGR remains unaffected}} by the Stat-Opt attack.
\emph{It is important to highlight the difference between our findings and~\cite{shejwalkar2022back}} where the attack impact with Mean AGR is $>0\%$ even for $0.01\%$ of malicious clients for FEMNIST, CIFAR10, and Purchase, while in our Stack Overflow case, the attack impact is $0\%$ for $<2\%$ malicious clients. With our results, \emph{\textbf{we strongly emphasize using datasets especially designed for FL and evaluating defenses under constraints imposed by scaling up the system in addition to small-scale experimentation.}} We do not dismiss the significance of small-scale experiments. Cross-silo
% We are not labeling small-scale experimentations as impractical. Cross-silo 
FL is indeed widely used. Instead, we emphasize the critical need for evaluating FL defenses within scaled-up settings.
 % as well. Rather, we are emphasizing the importance of evaluating FL defenses in scaled-up settings.
% \noindent\textbf{Reason for choosing Stackoverflow:}
% Our surveyed works primarily focus on image tasks like MNIST, FashionMNIST, and CIFAR10 (Figure~\ref{fig:defense_survey}). Although language models like GPT-3~\cite{brown2020language} and LLaMA~\cite{touvron2023llama} are gaining popularity, language tasks are under-represented in FL defenses literature (Figure~\ref{fig:defense_survey}). In line with our goal of re-evaluating FL defenses and highlighting experimental pitfalls, we aim to incorporate language tasks, specifically a \emph{next-word prediction} task using Stack Overflow's naturally distributed data~\cite{stackoverflow2019}.
% \noindent\textbf{Stackoverflow and TrMean with a high percentage of malicious clients:}
% With the setup for Stack Overflow mentioned in~\S\ref{appdx:setup}, we can achieve the baseline accuracy of $25.44\%$(FedAdam no-attack) as shown in Figure~\ref{fig:stackoverflow_trim}. 
% This matches the $25.2\%$ accuracy in the original paper's~\cite {reddi2020adaptive} Table 1, and we verified our implementation. To set up another baseline for comparing different algorithms, we also use the SGD optimizer for the server instead of Adam and obtain an accuracy of $22.99\%$ (FedAvg no-attack). Next, we perform the Stat-Opt attack with the same percentage of malicious clients in Figure~\ref{fig:fl_baseline}, i.e., $20\%$, to match the settings for comparison. The corresponding attack accuracies for the two algorithms, $13\%$ and $13.1\%$, are also shown in Figure~\ref{fig:stackoverflow_trim}. We find that the Stat-Opt attack lowers the accuracy in both cases by a significant amount, which is consistent with the trend for other datasets (except MNIST) in Figure~\ref{fig:fl_baseline}.
% \noindent\textbf{Practical considerations make TrMean and Mean extremely robust on Stack Overflow:}
% Practically, a malicious agent would need to hack into a device to poison the model parameters. However, most research simulates small-scale cross-silo FL settings  for security evaluations. For example, $20\%$ malicious clients would amount to $<=20$ clients, and we argue that it is possible to hack into that many devices.
% \vspace{-0.15cm}
\subsubsection{Resource overheads for FedRecover}\label{impact:scalability:fdr_fld}
Here, we first explain the reasons behind our conviction that FedRecover and FLDetector are incompatible in the cross-device setting. Consequently, 
% why we believe FedRecover and FLDetector are incompatible in the cross-device setting. Therefore, 
we do not evaluate these two defenses under the cross-device setting. Nevertheless, in this section, we comment on some of the \emph{practical} aspects of FedRecover, such as computation, communication, and storage costs, to assess the feasibility of scaling up such systems.
% \noindent\textbf{Compatibility of Fedrecover and FLDetector with the cross-device setting:}
% We believe FedRecover is incompatible with the cross-device FL setting, as it requires historical information on the client's past model updates to estimate the next round's update. In cross-device FL, a client participates in very few rounds, which gives FedRecover little information for recovery. Since FedRecover uses the LBFGS algorithm to estimate the next update using the client's past updates and global models, in the cross-device setting, FedRecover would be unable to estimate a good update if a client gets selected after several rounds. Similarly, due to a lack of consistent historical information, we believe FLDetector to be incompatible with the cross-device setting as well.

\noindent\textbf{Compatibility of Fedrecover and FLDetector with the cross-device FL:}
FedRecover's reliance on historical information from clients' past updates is hindered in cross-device, where clients participate in a few rounds, limiting available historical updates. Similarly, FLDetector faces challenges in the cross-device setting due to a lack of consistent historical information. Therefore, \emph{we find FedRecover and FLDetector incompatible with the cross-device setting}.

\noindent\textbf{Communication-accuracy tradeoff for FedRecover:}
In Figure~\ref{fig:acc_vs_comm_femnist}, we show the tradeoff between the recovery accuracy and communication of FedRecover.
% FedRecover's 
The recovery accuracy increases as it relies more on exact updates (locally computed updates instead of estimated ones) from the clients, where the \emph{minimum number of exact updates} is $T_w$ + $\frac{T_{total} - T_w - T_f}{T_c}$ + $T_{f}$. Interestingly, using the same amount of exact updates (effectively the same amount of communication as the server would communicate with clients for the same number of rounds) gives us more accuracy by training from scratch, i.e., not using FedRecover but restarting training with benign clients only. For instance, with only $20\%$ exact updates and $T_w=20$, FedRecover achieves $\approx76\%$ accuracy while training from scratch with an equivalent 40 rounds achieves $\approx80\%$.

In Figure~\ref{fig:acc_vs_comm_mnist_fashion}, we can see that with a small percentage of exact updates, FedRecover completely recovers for simpler datasets like MNIST and FashionMNIST with FedAvg and is unaffected by the variation in periodicity $T_c$. With the same amount of exact updates, we can achieve the same results without FedRecover as well. This is because a fast algorithm gives less time for the adversary to attack and a higher starting point for FedRecover to recover, leading to lower estimation errors (\S\ref{impact:algorithm:fdr}).

Based on these observations, we conclude that with fast converging algorithms and proper settings, we might not need to use FedRecover, especially when it comes with an additional computational and storage cost. In our experiments, for the slow baseline that uses FedSGD over 1000 epochs for MNIST, we require \textbf{\emph{$\approx200GB$}} of storage for saving the client model updates every round. By extension, this applies to any defense that requires knowledge of past updates. This cost would significantly increase with the number of clients and by using larger models for more complex tasks.
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/acc_vs_comm.pdf}
        % \vspace{-0.3cm}
        \caption{FEMNIST}
        \label{fig:acc_vs_comm_femnist}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\columnwidth}
        \includegraphics[width=\columnwidth]{figures/acc_vs_comm_mnist_fashion.pdf}
        % \vspace{-0.3cm}
        \caption{MNIST and Fashion}
        \label{fig:acc_vs_comm_mnist_fashion}
    \end{subfigure}
    % \vspace{-0.3cm}
    \caption{Communication-accuracy tradeoff for FedRecover. Despite the presence of four lines, their overlap is discernible as we can achieve baseline accuracy with exact updates.}
    % there are four lines in (b), but since we can achieve the baseline accuracy with the exact updates, the lines overlap.}
    % \vspace{-0.7cm}
    \label{fig:acc_vs_comm}
\end{figure}
% \vspace{-0.2cm}
\subsection{Naive Attacks}\label{impact:attacks}
% This component is crucial in our research due to the prevalent use of simple and naive attacks, as highlighted in \S\ref{pitfalls:5}. We first demonstrate that TrMean struggles against potent model poisoning attacks using Stat-Opt and Dyn-Opt attacks. We then design an adaptive attack against FLDetector, revealing high \emph{imperfect client detection} rates and show how this impacts FedRecover's performance, as it relies on a detection mechanism such as FLDetector to remove malicious clients before recovery is performed.
This component is critical in our research, given the prevalence of simple attacks highlighted in \S\ref{pitfalls:5}. We demonstrate TrMean's vulnerability to powerful model poisoning attacks such as Stat-Opt and Dyn-Opt. We also test our adaptive attack on FLDetector, revealing high rates of \emph{imperfect client detection} and showcasing its impact on FedRecover, which relies on FLDetector to identify malicious clients before recovery.
% \vspace{-0.15cm}
\subsubsection{Choice of attacks greatly impact TrMean's performance}\label{impact:attacks:trmean}
% \noindent\textbf{1) Choice of attacks greatly impact TrMean's performance:}\label{impact:attacks:trmean}
We conduct Stat-Opt~\cite{fang2020local} and Dyn-Opt~\cite{shejwalkar2021manipulating} attacks on FashionMNIST, varying the heterogeneity for both FCJ and Dir distributions. The results are depicted in Figure~\ref{fig:trmean_distributions_fmnist}.
Generally, Dyn-Opt is stronger than Stat-Opt, particularly noticeable at higher heterogeneity levels. For instance, at Dir bias level $0.1$, the accuracy drops to $51\%$ for Dyn-Opt compared to $59\%$ for Stat-Opt. Dyn-Opt's strength lies in finding optimal perturbations tailored for the dataset at every FL round, making it more potent compared to the static nature of perturbation in Stat-Opt, as discussed in \S\ref{background:attacks_study}.
% \vspace{-0.15cm}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.44\columnwidth}
        \includegraphics[width=\columnwidth]{figures/RQ3_fnr_fpr_femnist_twarm20.pdf}
        % \vspace{-7mm}
        \caption{$T_w=10$}
        \label{fig:fedrec_fnr_fpr_twarm20}
    \end{subfigure}
    \hspace{0.5mm}
    \begin{subfigure}[b]{0.44\columnwidth}
        \includegraphics[width=\columnwidth]{figures/RQ3_fnr_fpr_femnist_twarm10.pdf}
        % \vspace{-7mm}
        \caption{$T_w=20$}
        \label{fig:fedrec_fnr_fpr_twarm10}
    \end{subfigure}
    % \vspace{-0.3cm}
    \caption{Performance of FedRecover, represented by the difference between no-attack and post-recovery accuracies, under non-zero FPRs and FNRs caused by adaptive attacks.}
    % \vspace{-0.7cm}
    \label{fig:fedrec_fnr_fpr}
\end{figure}
\subsubsection{Overcoming FLDetector with our adaptive attack}\label{impact:attacks:fld}
% \noindent\textbf{2) Circumventing FLDetector with our adaptive attack:}\label{pitfalls:attacks:fld}
% We design a novel approach to evaluate FLDetector's robustness against \emph{adaptive attacks}. Specifically, the malicious clients send updates perturbed by our adaptive attack to the server, thereby evading detection by FLDetector, causing non-zero False Negative Rates (FNR) and False Positive Rates (FPR). This leads to a lower accuracy as these malicious clients are part of the subsequent training process. Here, we first thoroughly explain the adaptive attack we have developed and then perform its impact analysis on FLDetector.
We introduce a novel attack to assess FLDetector's resilience against \emph{adaptive attacks}. Malicious clients craft updates using our attack and evade detection by FLDetector, resulting in non-zero FNRs and FPRs.

Our attack adds a carefully crafted perturbation vector to client model updates, so they are close enough to the estimated model updates, thereby bypassing FLDetector's detection. Here we describe our attack formulation in detail.

% We defer the \textbf{attack formulation} steps to Appendix~\ref{appdx:attack_formulation} due to space constraints. Following those steps, an adversary is able to craft a malicious update using Equations~\ref{eqn:fld_mal_update},~\ref{eqn:fld_good_distance}, and~\ref{eqn:fld_perturbation}.


% \subsection{Our adaptive attack \momin{bring this to the main paper}}\label{appdx:attack_formulation}
\noindent\textbf{\em Attack Formulation:}
FLDetector computes the estimated update (\S\ref{background:fld}) for client $k$ as:
\begin{equation}\label{eqn:fld_update}
    \hat{\nabla}_{t}^{k} = \nabla_{t-1}^{k} + \hat{H}^{t}\cdot(\theta_{t} - \theta_{t-1})
\end{equation}
% $\hat{\nabla}_{t}^{k} = \nabla_{t-1}^{k} + \hat{H}^{t}\cdot(\theta_{t} - \theta_{t-1})$
In this attack, we introduce a perturbation vector, $\mathcal{P}$, that modifies the updates sent from malicious clients. A malicious client computes its update so that its final update is computed as the sum of its previous update, the $HVP$ or $Hessian\ Vector\ Product$ ($\hat{H}^{t}\cdot(\theta_{t} - \theta_{t-1})$), and the perturbation vector. This can be written as:
\begin{equation}\label{eqn:fld_mal_update}
    \hat{\nabla}_{t}^{k} = \nabla_{t-1}^{k} + \hat{H}^{t}\cdot(\theta_{t} - \theta_{t-1}) + \mathcal{P}
\end{equation}

The server estimates an update by adding the $HVP$ to the last round's exact update and compares it with the actual update in that round (\S\ref{background:fdr}). The malicious update, therefore, deviates from the estimated update by $\mathcal{P}$. We proceed to detail the calculation of this perturbation vector $\mathcal{P}$. To calculate $\mathcal{P}$, we first calculate a \emph{good distance range}, $\mathcal{R}$, that is a safe perturbation distance for the perturbation vector by taking the norm between the old and new client updates. The good distance range, $\mathcal{R}_{t}^{k}$, for client $k$, at round $t$ is given by:


\begin{equation}\label{eqn:fld_good_distance}
    \mathcal{R}_{t}^{k} = ||\hat{\nabla}_{t}^{k} - \nabla_{t}^{k}||
\end{equation}

Here, $\hat{\nabla}_{t}^{k}$ is the estimated update for client $k$ at round $t$, and $\nabla_{t}^{k}$ is the actual update for client $k$ at round $t$.
The deviation, $\mathcal{D}$, for the perturbation vector is calculated by following deviation strategies in~\cite{shejwalkar2021manipulating}. It can either be \textit{unit vector}, \textit{sign}, or \textit{std}. Finally, $\mathcal{P}$ is computed by taking the average of all the \textit{good distance ranges}, $\mathcal{R}$, and directing it in the direction of the deviation, $\mathcal{D}$:


\begin{equation}\label{eqn:fld_perturbation}
    \mathcal{P} = \frac{\mathcal{D}}{||\mathcal{D}||}\cdot\frac{1}{N}\sum_{k=1}^{N} \mathcal{R}_{t}^{k}
\end{equation}


The impact of our adaptive attack on the performance of FLDetector is shown in Table~\ref{tab:fld_adaptive_attacks} and across all cases (different combinations of dataset, FL algorithm, and percentage of compromised clients), except one, we find that the FNR is non-zero, which means that malicious clients have not been detected, and they continue to be part of the training process. Since the attack is designed to craft malicious updates that are statistically close to the benign ones, it leads to a non-zero FPR as well, which means that many benign clients are falsely detected as malicious and are removed from the training process.
Since \emph{we achieve a 0 FPR and a 1 FNR for FEMNIST, this means that the attack never gets detected, no benign or malicious client is removed, and all malicious clients seem benign to FLDetector on the server.} We discuss the impact of non-zero FNR and FPR on further training and recovery in \S\ref{impact:attacks:fdr}.
% Additionally, an interesting observation is that FedAvg outperforms FedSGD by having very low FPRs, but has higher FNRs.

\begin{table}
\caption{Impact of our adaptive attack on FLDetector. Here $\% m$ represents the percentage of malicious clients.}
% \vspace{-0.2cm}
% \scriptsize % Increase font size slightly
\centering
\setlength{\tabcolsep}{3pt} % Reduce the space between columns
\begin{tabular}{|c|l|cc|cc|cc|cc|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{\%}\\ \textbf{m}\end{tabular}}} & \multirow{2}{*}{\textbf{Baseline}} & \multicolumn{2}{l|}{\textbf{MNIST}} & \multicolumn{2}{l|}{\textbf{Fashion}} & \multicolumn{2}{l|}{\textbf{CIFAR10}} & \multicolumn{2}{l|}{\textbf{FEMNIST}} \\ \cline{3-10} 
\multicolumn{1}{|l|}{} &  & \multicolumn{1}{l|}{\textit{FPR}} & \multicolumn{1}{l|}{\textit{FNR}} & \multicolumn{1}{l|}{\textit{FPR}} & \multicolumn{1}{l|}{\textit{FNR}} & \multicolumn{1}{l|}{\textit{FPR}} & \multicolumn{1}{l|}{\textit{FNR}} & \multicolumn{1}{l|}{\textit{FPR}} & \multicolumn{1}{l|}{\textit{FNR}} \\ \hline
 \hline
\multirow{2}{*}{5} & FedSGD & \multicolumn{1}{c|}{0.4} & 1 & \multicolumn{1}{c|}{0.52} & 0.4 & \multicolumn{1}{c|}{0.21} & 0 & \multicolumn{1}{c|}{0} & 1 \\ \cline{2-10} 
 & FedAvg & \multicolumn{1}{c|}{0.02} & 1 & \multicolumn{1}{c|}{0} & 1 & \multicolumn{1}{c|}{0.13} & 1 & \multicolumn{1}{c|}{0} & 1 \\ \hline
  \hline
\multirow{2}{*}{10} & FedSGD & \multicolumn{1}{c|}{0.39} & 1 & \multicolumn{1}{c|}{0.56} & 0.6 & \multicolumn{1}{c|}{0.37} & 0 & \multicolumn{1}{c|}{0} & 1 \\ 
\cline{2-10} 
 & FedAvg & \multicolumn{1}{c|}{0.02} & 1 & \multicolumn{1}{c|}{0.02} & 1 & \multicolumn{1}{c|}{0.23} & 0.67 & \multicolumn{1}{c|}{0} & 1 \\ \hline
  \hline
\multirow{2}{*}{15} & FedSGD & \multicolumn{1}{c|}{0.48} & 0.8 & \multicolumn{1}{c|}{0.54} & 0.3 & \multicolumn{1}{c|}{0.35} & 0.35 & \multicolumn{1}{c|}{0} & 1 \\ \cline{2-10} 
 & FedAvg & \multicolumn{1}{c|}{0.02} & 1 & \multicolumn{1}{c|}{0.05} & 1 & \multicolumn{1}{c|}{0.18} & 0.67 & \multicolumn{1}{c|}{0} & 1 \\ \hline
 \hline
\multirow{2}{*}{20} & FedSGD & \multicolumn{1}{c|}{0.45} & 1 & \multicolumn{1}{c|}{0.62} & 0.3 & \multicolumn{1}{c|}{0.35} & 0.33 & \multicolumn{1}{c|}{0} & 1 \\ \cline{2-10} 
 & FedAvg & \multicolumn{1}{c|}{0.01} & 1 & \multicolumn{1}{c|}{0.06} & 1 & \multicolumn{1}{c|}{0.17} & 0.67 & \multicolumn{1}{c|}{0} & 1 \\ \hline
\end{tabular}
% \vspace{-0.5cm}
\label{tab:fld_adaptive_attacks}
\end{table}
% \vspace{-0.25cm}
\subsubsection{Imperfect detection leads to lower recovery performance}\label{impact:attacks:fdr}
% \noindent\textbf{3) Imperfect detection leads to lower recovery performance:}\label{pitfalls:attacks:fdr}
% We have shown that our adaptive attack leads to imperfect client detection (\S\ref{impact:attacks:fld}), and these clients stay in the training process. Here, we show the effect of these malicious clients when they are present during recovery as they have \emph{escaped} FLDetector. These escaped clients are represented by the FNR (False Negative Rate). We also show the effect of benign clients incorrectly classified as malicious and removed from the training process. These clients are represented by the FPR (False Positive Rate). In Figure \ref{fig:fedrec_fnr_fpr}, we show results when we vary the FNR and FPR for FEMNIST with $T_w=20$ and $T_w=10$ warmup rounds with $20\%$ malicious clients (setup details in Appendix~\ref{appdx:setup}).
% From these figures, we can see that the presence of malicious clients and the absence of benign clients during recovery makes it difficult for FedRecover to achieve accuracy similar to the no-attack baseline of $82\%$, e.g., at $FNR=FPR=0.3$ for $T_w=10$, the recovery accuracy is only $67\%$. This is because the \emph{malicious updates in the recovery process lead to higher estimation errors, deviating the recovery model from the benign one}. We also observe that we get slightly better performance using more warmup rounds, as we achieve a post-recovery accuracy of $70\%$ for $T_w=20$. This is because \emph{more warmup rounds give a higher starting point to the model and lead to lower estimation errors overall.}
Our adaptive attack results in imperfect client detection (\S\ref{impact:attacks:fld}), allowing escaped clients into the recovery process. The escaped clients are denoted by the FNR, while benign clients, incorrectly classified as malicious and subsequently removed from training, are represented by the FPR.

Figure~\ref{fig:fedrec_fnr_fpr} shows that non-zero FNRs and FPRs challenge FedRecover's ability to reach the no-attack accuracy of $82\%$. For instance, at $FNR=FPR=0.3$ for $T_w=10$, the recovery accuracy is only $67\%$. This is because malicious updates in the recovery process cause higher estimation errors and deviate the recovery model from the benign one. A marginal improvement is observed with more warmup rounds, achieving a post-recovery accuracy of $70\%$ for $T_w=20$, as increased warmup rounds provide a higher starting point for the model, resulting in lower overall estimation errors.
% \vspace{-0.2cm}
\subsection{Unfair Metrics}\label{impact:evaluation}
A key feature of FL is heterogeneity (\S\ref{impact:distribution:statistical_analyses}), which makes testing and reporting individual client's performance essential. Unfortunately, almost all of the surveyed works only report the global model accuracy (\S\ref{pitfalls:6}). To show that this is unfair, we show how personalized performance differs from that of the global model using TrMean and FedRecover under benign and adversarial settings.
% \noindent\textbf{1) Different clients have different levels of robustness under TrMean:}\label{impact:evaluation:trmean}
% \vspace{-0.15cm}
\subsubsection{Different clients have different levels of robustness under TrMean}\label{impact:evaluation:trmean}
% We perform training over FEMNIST for 200 rounds to obtain a global model accuracy of $82\%$ and log the accuracy of the global model on each client's test data separately. The global model accuracy is represented by the line \emph{global w/o attack} in Figure~\ref{fig:trmean_per_client_acc} and the line \emph{per-client w/o attack} shows the variation in the test accuracies on a per-client basis. We show that \emph{per-client w/o attack} is roughly centered around the global accuracy of $82\%$, which means that overall, the global model learns the features of combined data of all the clients due to aggregation of model updates. However, due to heterogeneity, it performs \emph{poorly for some clients, well enough for most clients, and exceptionally well for a few clients}.
% This is evident from the \emph{per-client w/o attack} line, which is very steep at either end. We plot similar curves \emph{global w/ attack} ($66\%$) and \emph{per-client w/ attack} in the attack scenario and note that it follows the same trend as the no-attack scenario. However, a key difference in the attack scenario is that most clients lie below the global attack accuracy.

We train on FEMNIST for 200 rounds, achieving a global model accuracy of $82\%$. Figure~\ref{fig:trmean_per_client_acc} displays the accuracy trends, where \emph{global w/o attack} represents the global model accuracy, and \emph{per-client w/o attack} shows individual client test accuracies. \emph{Per-client w/o attack} clusters around the global accuracy, indicating the model learns from the combined data. Due to heterogeneity, we see a lot of variation in the performance of individual clients. In the attack scenario (\emph{global w/ attack} at $66\%$ and \emph{per-client w/ attack}), a similar trend persists, but most clients now fall below the global attack accuracy.
% \textcolor{red}{comment}
% Another interesting thing to note is that although the global model accuracy has dropped from $82\%$ to $66\%$, the average drop in client model accuracy is $x\%$(\textcolor{red}{calculate and report this metric here}). 
% This strengthens our point about calculating and reporting per-client metrics in addition to global ones.
\emph{Based on our observations due to heterogeneity across clients, we strongly advocate using per-client metrics in future evaluations.}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
        \includegraphics[width=\columnwidth]{figures/trmean_per_client_accuracy.pdf}
        % \vspace{7mm}
        \caption{TrMean}
        \label{fig:trmean_per_client_acc}
    \end{subfigure}
    % \hspace{0.5mm}
    \begin{subfigure}[b]{0.43\columnwidth}
        \includegraphics[width=\columnwidth]{figures/fedrecover_per_client_accuracy.pdf}
        % \vspace{-7mm}
        \caption{FedRecover}
        \label{fig:fedrecover_per_client_acc}
    \end{subfigure}
    % \vspace{-0.3cm}
    \caption{Personalized evaluations, i.e., per-client accuracy for FEMNIST with TrMean and FedRecover. Note that the accuracy does not increase monotonically with the client number, rather we plot it in an ascending order here since order does not matter when we want to show variation in per-client accuracy. This should not be perceived as a relationship between accuracy and client ID/number. We are simply showing the range of individual client accuracies and have plotted them in a monotonic fashion for visual comparison.}
    % \vspace{-0.4cm}
    \label{fig:per_client_acc}
\end{figure}

% \noindent\textbf{2) FedRecover's performance greatly varies on a  per-client basis:}\label{impact:evaluation:fdr}
\subsubsection{FedRecover's performance greatly varies on a per-client basis}\label{impact:evaluation:fdr}
% With an objective similar to the one with personalized evaluations for TrMean in \S\ref{impact:evaluation:trmean}, we perform personalized evaluations on FedRecover with the FEMNIST dataset. Performing FedRecover on the global model in \S\ref{impact:evaluation:trmean} with $66\%$ accuracy, we achieve a global post-recovery accuracy of $74\%$ (\emph{post-recovery}) and show it on a per-client basis with the lines \emph{$fnr0.1\_fpr0.1$}, \emph{$fnr0.3\_fpr0.3$}, \emph{$fnr0.5\_fpr0.5$} in Figure~\ref{fig:fedrecover_per_client_acc}. The post-recovery accuracies for different levels of FNRs and FPRs show that most client models lie below the global post-recovery accuracy. With an FNR and FPR of 0.5, all clients lie below the global post-recovery accuracy and even below the post-attack accuracy. This means that none of the models have been able to achieve recovery. We have explained in \S\ref{impact:attacks:fdr} that the presence of malicious clients in recovery leads to higher estimation errors, which is also reflected in our per-client evaluations. We find our observation of per-client performance variation consistent with TrMean's (\S\ref{impact:evaluation:trmean}) and emphasize the need for personalized evaluations, especially when the data is non-i.i.d.
Similar to the personalized evaluations for TrMean in \S\ref{impact:evaluation:trmean}, we apply personalized evaluations to FedRecover for the FEMNIST dataset. After performing FedRecover on the global model with $66\%$ accuracy (\S\ref{impact:evaluation:trmean}), we achieve a post-recovery accuracy of $74\%$ (\emph{post-recovery} in Figure~\ref{fig:fedrecover_per_client_acc}). This is illustrated on a per-client basis with the lines \emph{$fnr0.1fpr0.1$}, \emph{$fnr0.3fpr0.3$}, \emph{$fnr0.5fpr0.5$} in Figure~\ref{fig:fedrecover_per_client_acc}. The post-recovery accuracies for various FNRs and FPRs indicate that most client models fall below the global post-recovery accuracy. At an FNR and FPR of 0.5, all clients lie below both the post-recovery accuracy and the post-attack accuracy, signifying a failure in achieving recovery. This aligns with our findings in \S\ref{impact:attacks:fdr} that \emph{the presence of malicious clients during recovery leads to higher estimation errors, as mirrored in our per-client evaluations. This consistency underscores the necessity for personalized evaluations, particularly in non-i.i.d. datasets.}

\subsubsection{Impact of class-imbalance on per-class accuracies}\label{impact:evaluation:imbalance}
% Here, we show the impact of class imbalance on the performance of TrMean with CIFAR10. For brevity, we show our experimental settings for imbalanced and balanced results in Appendix~\ref{appdx:setup:imbalanced_cifar10}, and here we only discuss the imbalanced scenario.
% We perform the Stat-Opt in the imbalanced setting (Figure~\ref{fig:imbalanced_cifar}) and obtain an overall accuracy of $48\%$, which is higher than its balanced counterpart, i.e., only $42.66\%$. This overall accuracy is heavily influenced by class 0's performance under attack, where the accuracy drops from $86.2\%$ to only $64.3\%$ compared to dropping from $75.8\%$ to $17.3\%$ in the balanced scenario. The mean per-class accuracy for the imbalanced attack scenario turns out to be $27\%$, as shown in the figure. From this, we can conclude that \emph{when a class is dominant in the train and test datasets, it is less affected by an attack}, thereby influencing the overall accuracy to a higher value close to its own.
% While overall accuracy is essential and widely used, as it captures the holistic performance of a system, we advocate also to report per-class and mean per-class accuracies. Real-world datasets often have a class imbalance, and our study reveals that ignoring this aspect may lead to misjudgments about a system's robustness against attacks.

\begin{figure}
\centering
\includegraphics[scale=.5]{figures/cifar10_balanced_attack.pdf}
\vspace*{-0.3cm}
\caption{Overall and per-class accuracies for balanced CIFAR10 before and after Stat-Opt attack.}
\label{fig:balanced_cifar}
\vspace*{-0.5cm}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=.5]{figures/cifar10_unbalanced_attack.pdf}
% \vspace*{-0.5cm}
\caption{Overall, per-class, and mean per-class accuracies for imbalanced CIFAR10 before and after Stat-Opt attack. Class 0 has the most samples.}
\label{fig:imbalanced_cifar}
% \vspace*{-0.6cm}
\end{figure}

We examine the impact of class imbalance in CIFAR10 on the performance of TrMean under the no-attack and Stat-Opt attack scenarios. To set up our baseline, we train an Alexnet model with 100 clients over 100 epochs. We use the standard CIFAR10 dataset that has 50000 training samples and 10000 test samples.
Figure~\ref{fig:balanced_cifar} shows that we achieve an overall accuracy of $70.82\%$ without attack and $42.66\%$ with attack in the balanced setting. It also shows the respective per-class accuracies. We define the \emph{overall accuracy} as the total number of correct samples out of the total number of samples in the test dataset and the \emph{mean per-class accuracy} as the mean of all the per-class accuracies in the test dataset. In this case, since the test dataset is perfectly balanced, i.e., each class has the same number of test samples, the overall accuracy and the mean accuracy are the same.


Next, we create an imbalance in the CIFAR10 dataset by removing $90\%$ of the samples of all classes except class zero. This produces 5000 samples of class zero and 500 samples each for other classes in the training set.
Figure~\ref{fig:imbalanced_cifar} shows that in the imbalanced setting, the overall accuracy in no-attack is $68\%$, which is close to its balanced accuracy of $70.82\%$. However, this accuracy is biased towards class 0, which has the highest per-class accuracy of $86.2\%$($75.8\%$ in the balanced scenario) since it has ten times more samples than the rest. The mean per-class accuracy is $52\%$ as it ignores the class imbalance. This indicates that \emph{the mean per-class accuracy is a better metric than overall accuracy in this case, as it shows that the model loses utility with reduced samples.}

The imbalanced Stat-Opt scenario yields an overall accuracy of $48\%$, surpassing the balanced counterpart at $42.66\%$. Figure~\ref{fig:imbalanced_cifar} shows that the overall accuracy is significantly influenced by class 0, dropping from $86.2\%$ to $64.3\%$ under attack, compared to a drop from $75.8\%$ to $17.3\%$ in the balanced scenario. The mean per-class accuracy for the imbalanced attack is $27\%$. This highlights that \emph{dominant classes are less affected by attacks}, influencing the overall accuracy to reflect their performance. \emph{While overall accuracy is crucial, we stress the importance of reporting per-class and mean per-class accuracies, especially in real-world datasets with class imbalances, as neglecting this aspect can lead to misjudgments about a system's robustness.}