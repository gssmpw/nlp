\section{Related Work}
\label{sec:related}
\noindent\textbf{Previous systemizations: }
In prior research, various taxonomies
% Previous works have designed taxonomies 
in adversarial ML have been developed, extending beyond
% that are not just limited to 
FL~\cite{goldblum2020dataset, jere2020taxonomy, barreno2010the, biggio2018wild, huang2011adversarial, rodriguez2023survey} and their main focus is on the attacks.
~\cite{rodriguez2023survey}  presents a taxonomy specifically for FL defenses, categorizing them based on their occurrence at the client, server, or communication channel. Our contribution extends beyond this by providing a more comprehensive and multidimensional systemization of the existing defenses in the literature. Additionally, we leverage our systemization to identify representative defenses for in-depth pitfall analyses.
% provide a taxonomy of FL defenses that categorizes defenses based on where they occur (client, server, or communication channel). On the other hand, we perform a more comprehensive and multidimensional systemization of the defenses in literature. We also use our systemization to select representative defenses for pitfall analyses.


Shejwalkar et al.~\cite{shejwalkar2022back} have performed the systemization of FL attacks, highlighting misconceptions about the robustness of FL systems that may arise from overlooking practical considerations about the threat model in deployment scenarios. Their conclusion emphasizes the high robustness of FL with simple and cost-effective defenses in practical threat models. Our work complements this perspective by focusing on defenses. We systematically categorize and re-evaluate representative defenses, uncovering common pitfalls, and providing actionable recommendations to address each.
% have performed a systemization of FL attacks and have shown several misconceptions about the robustness of FL systems that may arise due to ignoring practical considerations about the threat model in deployment scenarios. They conclude that FL is highly robust with simple and low-cost defenses in practical threat models. Our work is done from the perspective of defenses, where we perform their systemization, re-evaluate representative defenses to uncover common pitfalls, and provide actionable recommendations to counter every pitfall.

\noindent\textbf{Pitfalls and guidelines: }
Arp et al.~\cite{arp2022and} have extensively studied the pitfalls associated with ML in computer security, identifying and providing recommendations for several challenges in this domain. It is crucial to emphasize the distinctions between our work and theirs. While we may identify similar pitfalls, such as Inappropriate Baseline (\S\ref{impact:algorithm}), Inappropriate Performance Measures (\S\ref{impact:evaluation}), and Lab-only Evaluation (\S\ref{impact:scalability}), the underlying reasons for these pitfalls and their impact are specific to FL. 
% . They identify several pitfalls in this domain and provide concrete recommendations against each pitfall. It is very crucial to highlight our differences with their work. We may identify similar pitfalls with their work, such as P6-Inappropriate Baseline, P7-Inappropriate Performance Measures, and P9-Lab-only Evaluation, but the underlying reasons for the existence of those pitfalls and their impact are specific to FL. 
For example, a pitfall, ``inappropriate performance measures,'' addresses inappropriate performance measures like overall accuracy due to its limited capture of information about false positives and false negatives. In our context (\S\ref{impact:evaluation}), we discuss inappropriate performance measures, particularly in relation to fairness, and advocate for the use of personalized metrics, especially when dealing with non-i.i.d. data.
 % P7 in \cite{arp2022and} is about inappropriate performance measures such as overall accuracy because it does not capture the information about false positives and false negatives. We also discuss inappropriate performance measures in the context of fairness(\S\ref{impact:evaluation}) and recommend using personalized metrics, especially with non-i.i.d. data.

In the space of adversarial ML, Carlini et al.~\cite{carlini2019evaluating} identified numerous pitfalls associated with evaluating defenses against adversarial learning. While there are some commonalities with our work, such as considerations regarding the choice of attacks and testing against adaptive attacks, the primary distinction lies in the fact that our evaluation is specifically conducted in FL. Certain evaluation components are exclusive to FL, such as dealing with client heterogeneity and personalized evaluations.
% have identified a plethora of pitfalls associated with evaluating defenses against adversarial learning. They also have some points in common with our work, such as choice of attacks and testing against adaptive attacks. Still, the main difference is that our evaluation is performed over Federated Learning, and some evaluation components are exclusive to FL only, e.g., heterogeneity across clients and personalized evaluations.
% Our work is inspired by ~\cite{khan2023pitfalls} that started the work of identifying FL pitfalls and used FedRecover as a case study to show the impact of some of the pitfalls. We have expanded the analysis by exploring a variety of pitfalls and studying their impact. We have also broadened the evaluation spectrum via additional representative defenses such as FLDetector and TrMean and tested FL on a large scale with the language modality using the StackOverflow dataset.

\noindent\textbf{Improvements over~\cite{khan2023pitfalls}:} We take inspiration from~\cite{khan2023pitfalls}, which initiated the identification of FL pitfalls and used FedRecover as a case study to showcase the impact of some of these pitfalls. Before building upon their work, we set out to perform a systemization of FL defenses to understand the FL defenses space. This helps us to select representative defenses for pitfall analysis. We have expanded \cite{khan2023pitfalls}'s analysis by exploring additional pitfalls and studying their impact. Additionally, we broadened the evaluation spectrum by incorporating additional representative defenses like FLDetector and TrMean. Our study extends to a large-scale evaluation of FL with the language modality using StackOverflow~\cite{stackoverflow2019}.