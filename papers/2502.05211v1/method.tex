\section{Methodology}\label{sec:method}

\begin{figure*}
\centering
\includegraphics[scale=.4]{figures/defenses_survey.pdf}
% \vspace*{-0.8cm}
\caption{Frequency of choices of the six key components (Dataset, distribution of clients' data, FL algorithm, FL type, attacks, and evaluation) of robustness evaluation setup. \S~\ref{sec:impact} discusses the impacts of choices on the robustness of FL poisoning defenses.}
\label{fig:defense_survey}
% \vspace*{-.5cm}
\end{figure*}

\textcolor{red}{describe why this section was needed. basically you are providing the descriptions for some of the components being used later so the reader can know them early on. For example, you will describe FedSGD/FedAvg, cross-silo/cross-device here.\\
Here you are explaining the components of FL workflow. In the pitfalls section you are explaining the pitfalls in the workflow. There is a big difference. Then later on you are explaining the impact. These three need to be separately explained. Make sure you explain this properly for the reader!\\
Also decide the order of components/pitfalls 3 and 4}
\begin{itemize}
    \item Papers selection
    \item FL components selection
    \item Pitfall identification
    \item Re-evaluation methodology
    \item Also mention that you use many tasks and modalities, i.e., image classification, next-word prediction, and human activity recognition using multimodal sensor data.
\end{itemize}

% \noindent\textbf{The Federated Learning workflow:}
\subsection{The Federated Learning workflow:}\label{method:workflow}
Federated Learning comes in many variants, but we divide a typical FL defense evaluation workflow into six distinct components. Below, we briefly explain the components in the order they typically appear in the workflow(~\ref{fig:fl_flowchart}):
\begin{itemize}
    \item Dataset: The first step in any learning-based system, including FL, is to select or collect a dataset for evaluation. We report the prevalent datasets in ~\S\ref{pitfalls:1}, and through a thorough evaluation, assess the impact of the different choices of these datasets in ~\S\ref{impact:datasets}.
    \item Data Distribution: If a dataset such as MNIST or FashionMNIST is used for evaluation, it must first be distributed among the clients participating in the FL training using some distribution technique such as Dirichlet Distribution~\ref{}. On the other hand, some datasets are specifically designed for FL evaluations, e.g., FEMNIST(~\cite{caldas2018leaf}). In ~\ref{pitfalls:2}, we report the prevalence of such techniques, and in ~\S\ref{impact:distribution}, we show the difference between these distributions and their impact on FL robustness.
    \item FL Type: Once data has been distributed among clients, we have to decide the policy for client selection in every round of FL, which we refer to as the \emph{FL Type}.
    The FL type for practical deployments can be either \textbf{cross-device} or \textbf{cross-silo}~\cite{kairouz2019advances}. In \emph{cross-device FL}, the total number of clients, $N$, is very large (from a few thousand to billions), and only a tiny fraction of them is chosen in each FL training round, i.e., $n\ll N$. Clients are resource-constrained devices such as mobile phones, smartwatches, IoT devices, etc. While, in \emph{cross-silo FL}, $N$ is moderate (up to 100), and all clients are selected in each round, i.e., $n=N$, where clients are large corporations, e.g., banks, hospitals, etc. \textcolor{red}{prevalence and implications}
    \item FL Algorithm: Another important step in the FL workflow is to decide how local and global updates will be computed. Based on how these updates are computed, we can classify FL Algorithms into two types: FedSGD and FedAvg~\cite{mcmahan2017communication}, whose update computation we have described in~\S\ref{background:FL}. We report the prevalence of these FL algorithms in~\S\ref{pitfalls:3} and the impact of FedAvg over FedSGD in~\S\ref{impact:algorithm}.
    \item Attack: Once the above four components have been selected, the attack needs to be chosen. There are different kinds of attacks designed for FL, which we thoroughly explain in~\S\ref{background:attacks}. We find in our study that many naive attacks are commonly used~\S\ref{pitfalls:5}, and stronger adaptive attacks lead to different conclusions on the robustness of FL defenses~\S\ref{impact:attacks}.
    \item Evaluation Metrics: The final step in the FL evaluation workflow is to choose the metrics to be computed and reported. Since there are multiple parties collaborating in FL, in addition to a server, the metrics can be different for each of them. We show in~\S\ref{pitfalls:6} that most of our surveyed works report the server's metrics, and in~\S\ref{impact:evaluation}, we compare the global metrics with local ones and find drastic differences between the two.
\end{itemize}

% \noindent\textbf{Papers selection:}
\subsection{Papers Selection:}\label{method:selection}
We select 50 FL defense papers from top-tier security venues: Usenix Security, IEEE S\&P, ACM CCS, and NDSS, and identify the six components(~\S\ref{method:workflow}) of FL workflow for each of them. We list the surveyed papers in Table~\ref{tab:defenses}, and mention the choices for each of the six components. We then plot the frequency of these choices to get a better understanding of the prevalent choices in Figure~\ref{fig:defense_survey}.

\subsection{Re-evaluation Methodology:}\label{method:reevaluation}
\textcolor{red}{Write here how you will perform the re-evaluation. In the background section you justified the choice of attacks and defenses used. Here explain how you will perform the reevaluation. i.e., re-implementation, baselines, experimental setup. All of that goes here. Also mention limitations of your reevaluation here. Such as not being able to perform all settings on all defenses. Some due to practical limitations. Some due to space limitations.}

