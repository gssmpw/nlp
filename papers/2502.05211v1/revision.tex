\section{\textit{\textbf{REVIEWER 1}
}}

1. In Section 2.2.3, it is said that "three representative defense techniques" are chosen since "they are distinct in their nature." However, it is not explained what the "nature" of these defense techniques is. More specifically, there is a lack of overview of existing defense works, thus it is difficult to judge whether the three chosen defense techniques are representative or not. As far as I am concerned, both Trimmed Mean and FLDetector try to detect and remove malicious participants (Don't they belong to the same type of defense?) and FedRecover tries to recover the correct model without detection. Additionally, TrMean is an early work (2018), thus its limitations may not apply to more recent defense works.

\textbf{\textit{Response:}}
Trimmed Mean removes malicious components of updates dimension-wise and the training continues til completion i.e., until the end of specified global rounds. FLDetector, on the other hand, detects malicious clients, removes them from the client pool entirely, and then restarts the training process.
Trimmed Mean is an early work but it has been used in recent works (cite FedRecover, Back to the Drawing Board,  FLDetector), hence we include it in our study as a representative 'filtering' defense.

2. In Pitfall-2, the highlighted statement "high heterogeneity can make us believe that a system is robust when it is not" is confusing. It is shown by experiments in the paper that high heterogeneity makes a system less robust. I suggest clarifying this statement.

\textbf{\textit{Response:}}
Changed statement to "low heterogeneity can make us believe that a system is robust when it is not".

3. In Pitfall-3, the title is "Incorrect Baselines." I find it inappropriate since the authors did not refer to baselines (i.e., other defense works that a proposed defense work is to be compared with), but rather the federated learning algorithms.

\textbf{\textit{Response:}}

4. In Pitfall-4, the statement "Considering only hypothetical scenarios, while ignoring practical limitations..." is too vague. It can refer to almost anything. In this part, the authors actually want to explain that existing defenses usually assume a cross-silo rather than a cross-device FL setting. This is not "hypothetical scenarios" but only "limited scenarios." Additionally, it has already been found that poisoning attacks are less effective in a cross-device FL setting than in a cross-silo FL setting, thus evaluating defenses against stronger attacks (i.e., the cross-silo setting) may not be faulty, in my humble opinion.

\textbf{\textit{Response:}}
Consider changing this to:
Pitfall-4: Limited FL Setting. Considering limited settings, such as using only small-scale settings for evaluations, may not lead to conclusions that apply to large-scale settings as well.
In its explanation, write that with scaling up the system, the threat model changes (cite back to the drawing board). And since the threat model changes, e.g., it becomes more stricter about the percentage of malicious clients in cross-device, the attacks naturally become less effective and defenses naturally become more effective.

5. In Section 4.2 (Stat-1), the statement "We note that Dir produces client datasets ... very similar to those of the real-world datasets in Leaf repository" is unsupported. The authors need to give a reference to the Leaf repository, or better still, show the distribution of real-world datasets in the Leaf repository.

\textbf{\textit{Response:}}
Include the FEMNIST data distribution figure from the leaf website.

6. In Section 5.1, it is concluded that on a large scale, poisoning attacks are ineffective and simple AGRs are unaffected. This conclusion has already been reached in existing works [1].
[1] Shejwalkar, Virat, et al. "Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning." 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 2022.

\textbf{\textit{Response:}}
Our experiments differ from [1] in a way that we evaluate using an actual real-world dataset that is naturally distributed with 130 million training samples and 300,000 users.

7. In Section 5.2, I am confused by the Recommendation "we propose to balance data..." It seems that a lot has been argued for an imbalanced dataset setting. Did the authors mean that we should balance the data only for the multimodal sensing datasets or imbalanced datasets? I also find that the two questions raised in this section "(1) How does imbalance among classes in train and test data affect the robustness of a system? and (2) How do choice of evaluation metrics impact our assessment of the robustness of a system" unanswered.

\textbf{\textit{Response:}}

8. There are many typos and non-standard expressions in the paper, including but not limited to:

- I suggest referring to [74] by another name rather than "NDSS attack".
- "L-BFGS" or "LBFGS", please be consistent.
- replace "iid" with "i.i.d."
- replace "Fcj" with "FCJ"
- What is hvp in page 9?
- What is %m in Table 2?
- The title of Section 5.2 "Mumltimodal" should be "Multimodal"

\textbf{\textit{Response:}}
-
-done
-done
-done
-
-
-done


\section{\textbf{\textit{REVIEWER 2}}}

(i) Premise of paper: When I set out to read the paper, I originally thought that this would be an SoK-style one; however, after reading, it seemed the first half was more of an SoK, and second half was about the authors proposing a new adaptive attack. I found the storyline of the paper to be confusing and could not easily follow the logical coherence between each section. I would suggest the authors to either stick to an SoK paper and provide more details about the analysis of the 50 top-tier papers they selected.

\textbf{\textit{Response:}}
Change writing in a way that it appears as an SoK/survey/analysis paper. But present the adaptive attack as a byproduct of your re-evaluation. Show that you needed the adaptive attack to prove the point about choice of attacks, and that it is not the standalone contribution of your paper.

(ii) Paper title: currently, the title is too broad/generic and I would recommend the authors to use a title that is more representative of the work they present in the paper. “En Route Robust Federated Learning” does not make any sense and “A Critical Analysis of Experimental Practices in Security Evaluation” is also misleading as the authors only focus on defense papers against poisoning attacks.

\textbf{\textit{Response:}}
The title should mention something about the experimental setups of defenses, and that we are performing a re-evaluation of existing defenses with different settings.

(iii) Introduction: on page 2, column 1, the authors do mention the existing works that identify pitfalls in machine-learning based security; however I miss a justification for why the authors think that the pitfalls that they identified and proposed in this work are different from the ones that have been already put forward by previous work. A justification on what makes FL unique would help the reader to better understand why such a study is required. At the moment, my impression is that there is almost a 100\% overlap between the pitfalls mentioned in this paper and the ones from the literature.

(iv) Set of 50 top-tier papers: There is no clarification as to how these papers were selected. As the authors pointed out, there exists a large body of work on defense papers for FL; however, I miss an explanation as to why these specific papers were selected. An explanation on the selection method and criteria would be much appreciated.

\textbf{\textit{Response:}}
Write a section explaining the paper selection methodology.

(v) Section 2.2: The sub-heading could again be a bit more descriptive. You wrote “Poisoning attacks against FL”, but you also present three defenses in section 2.2.3. 

Regarding section 2.2.1: I do not follow the justification for the percentage of malicious clients you considered in your threat model. In the footnote, you wrote “Most defense works assume very high percentages of malicious clients to demonstrate that their defenses work even in highly adversarial settings. Hence, although unreasonable in practical FL settings [73], we follow prior defense works and use 20\% malicious clients.” If the assumption made by prior work is unreasonable in practical FL, why did you then follow prior work? Or did you mean to phrase this assumption differently and intended to stay consistent with assumptions made by prior works?

Section 2.2.2: please define what is NDSS attack.

\textbf{\textit{Response:}}
Change the footnote to add we choose 20\% malicious clients to stay consistent with the assumptions made by prior works.

Change the NDSS attack to Dyn-Opt(Dynamic Optimization)


(vi) Section 3: This whole section seems disconnected form the rest of the paper. I did not understand why would the authors state the pitfalls, explain their prevalence and implications, and immediately present recommendations. The observations made in the Prevalence and Implications seem unfounded and I could not follow how the authors came to the conclusions they presented. Moreover, I miss an explanation on how the pitfalls were identified (this further strengthens my point regarding the overlap with existing works that also studied pitfalls). There was also no connection with the content of section 3 to what was presented in section 2.2. How did the authors, after considering 2 attacks and 3 defenses, already came up with the 6 pitfalls they presented?

\textbf{\textit{Response:}}
Convert the paper to SoK style. And explain clearly in the beginning why you are structuring the paper in a particular way so that the reader knows clearly in the beginning what the purpose of this structure is.

(vii) Section 4: This section gets even more confusing. After having already provided recommendations for the pitfalls, the authors now evaluate the impact of the proposed pitfalls on FL security. What is the connection between the recommendations in section 3 and the evaluation in section 4?

\

(viii) Section 5: The section heading says “Analysis of Hidden Pitfalls in FL Security” - how were the hidden pitfalls identified or discovered? Is there any connection between the six pitfalls and hidden pitfalls in section 5? What role did the set of 50 top-tier paper play in identifying the hidden pitfalls?

\textbf{\textit{Response:}}
Explain that section 5 is different from section 3 and 4. The 6 pitfalls mentioned are related to different experimental settings that are already known and used in literature. For example, the FEMNIST dataset has still been used in the top-tier 50 papers. But there is no paper in those 50 that discusses multimodal sensor data. Or a large-scale NLP task. Therefore we set aside a separate category.
Think of a reason for why they imbalanced data case is something different from the 6 pitfalls and why you wrote it separately.

(ix) Discussion: Given that this paper evaluated a large body of defense papers for FL systems, I would have expected a section discussing the findings and impact on future research challenges. It could be that since the authors got side-tracked into implementing a new adaptive attack, took away the primary focus of what this paper should have been about.

\textbf{\textit{Response:}}
Make changes to the writing so that the paper does not seem like it is centered around the adaptive attack. Explain clearly that the adaptive attack is the byproduct of your re-evaluation methodology, and it was necessary to design an adaptive attack to show that FLDetector does not perform up to the mark in the presence of adaptive attacks.