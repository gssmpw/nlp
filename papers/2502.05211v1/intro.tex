\section{Introduction}\label{sec:introduction}
Federated learning (FL)~\cite{mcmahan2017communication} is an emerging approach in machine learning (ML) where multiple data owners, called \emph{clients}, collaboratively train a shared model, known as the \emph{global model}, while keeping their individual training data private.
The central \emph{server} (service provider) iteratively aggregates \emph{model updates} from each client, which are generated based on their local data. The server merges these updates using an \emph{aggregation rule} (AGR) and uses them to update the global model. Following each training iteration (also known as \emph{round}), the refined global model is distributed to the clients participating in the next round. Prominent distributed platforms such as Google's Gboard~\cite{gboard} for next-word prediction, Apple's Siri~\cite{technologyreviewApplePersonalizes} for automatic speech recognition~\cite{paulik2021federated}, and WeBank~\cite{webankcredit} for credit risk predictions, have adopted this FL mechanism. Its intrinsic characteristic of promoting collaboration while preserving privacy has rendered it indispensable in critical applications, notably in medical diagnosis~\cite{feki2021federated, ku2022privacy, qayyum2022collaborative}, activity recognition~\cite{zhao2022multimodal, ek2020evaluation, ouyang2021clusterfl, sozinov2018human}, and next-character prediction~\cite{sun2022fedsea}.

% \begin{figure}
% \hspace{1cm}
% \centering
% \includegraphics[width=0.7\columnwidth]{figures/FL_intro3.pdf}
% \vspace{-0.2cm}
% \caption{Basic flow of FL. In a poisoning attack, one or more clients could be malicious.}
% \label{fig:FL_intro}
% \vspace{-0.8cm}
% \end{figure}

\begin{figure}
\centering
\includegraphics[width=0.89\textwidth]{figures/flowchart8.pdf}
% \vspace*{-0.3cm}
\caption{
FL defense evaluation pipeline. We display common choices for each stage in the pipeline, e.g., FedSGD or FedAvg as the FL algorithm, and highlight the associated pitfall, i.e., using a slow-converging algorithm. We also indicate interdependencies between stages, e.g., large-scale and cross-device in FL type limit the number of malicious clients in the attack.
}
\label{fig:fl_flowchart}
% \vspace*{-0.5cm}
\end{figure}

FL is gaining popularity due to its privacy-preserving and collaborative nature, yet it faces vulnerabilities to \emph{poisoning attacks}~\cite{fang2020local, shejwalkar2021manipulating, shejwalkar2022back, wang2020attack}, where malicious or \emph{compromised clients} intentionally corrupt FL training and \emph{poison} the global model. This can result in a poisoned model that performs poorly on all inputs in \emph{untargeted poisoning} attacks or on specific inputs in \emph{targeted poisoning} attacks. To address these threats, the community has developed various defense mechanisms. Robust AGRs such as Multi-krum~\cite{blanchard2017machine} and Trimmed-mean~\cite{yin2018byzantine}, detect and discard malicious updates. Certified defenses like CRFL~\cite{xie2021crfl} and Ensemble FL~\cite{cao2021provably} provide robustness certifications. Tools like FLDetector~\cite{zhang2022fldetector} proactively identify and remove malicious clients during training. Meanwhile, FedRecover~\cite{cao2022fedrecover} focuses on post-poisoning recovery after an attack, aiming to restore the global model's performance. Ditto~\cite{li2021ditto} integrates fairness and robustness by regularizing the local training objective, and Cronus~\cite{chang2019cronus} enhances security and privacy through knowledge distillation.

\noindent\textbf{Systemizing FL Defenses:}
Besides these few defenses, the variety of available options (Table~\ref{tab:defenses}) poses a challenge for practitioners, i.e., determining the right defense for a specific use case or integrating multiple defenses for enhanced robustness becomes complex without a clear understanding of where a defense and its dependencies fit in the FL pipeline.

To address these research gaps, we conduct a comprehensive systemization (\S\ref{sec:systemization}) of FL defenses, organizing them along three crucial dimensions: processing of client updates, server's knowledge, and defense phase.
While existing works~\cite{shejwalkar2022back, jere2020taxonomy, barreno2010the, biggio2018wild, huang2011adversarial, rodriguez2023survey} have designed taxonomies primarily focused on adversarial ML, including those that guide the selection of the appropriate attacks and settings for FL, a dedicated systemization for FL defenses has been lacking.

To the best of our knowledge, we are the first to propose such a systemization.
Our framework simplifies the selection and integration of defenses, clarifying when and where each defense is applied (\S\ref{systemization:classification}). For example, Figure~\ref{fig:defenses_systemization} shows that FLDetector operates in the pre-aggregation phase(``when") at the server(``where"). Moreover, the systemization highlights underrepresented defense types, encouraging further exploration and innovation. 
For example, 

our analysis identified FedRecover as the only post-training, update-modification technique using estimation, prompting exploration of not only other estimation-based but also post-training defenses. From our systemization in Table~\ref{tab:systemization}, we can see that most defenses are on the server side, operate during pre-aggregation, and employ metric-based processing of client updates. Importantly, our systemization is designed to be expandable to incorporate more attributes in the future.

\noindent\textbf{Pitfalls in experimental setups of FL defenses: }
During our systemization, we find that defenses are evaluated across various experimental setups with different choices of datasets, data distributions, attacks, etc. We thoroughly survey 50 top-tier FL defense works (\S\ref{sec:pitfalls}) and report frequently used choices of six experimental setup components: \emph{data and distribution, FL type and algorithm, attack, and evaluation type}. We find questionable trends in these choices, e.g., most works use the intrinsically robust MNIST dataset and resort to simple attacks such as label flipping. This motivates the need to uncover \emph{pitfalls} in the experimental setups of existing FL defense works and provide guidelines for future evaluations.


Existing literature provides guidelines for robustness evaluations. For instance, \cite{arp2022and} offers insights and recommendations, focusing on pitfalls in centralized ML-based security system evaluations. Similarly, \cite{carlini2019evaluating} identifies pitfalls in evaluating adversarial robustness and suggests mitigation guidelines. In~\cite{shejwalkar2022back}, authors question trends in existing attack threat models, demonstrating FL robustness under practical limitations. However, a comprehensive exploration of FL defenses and their experimental setups is lacking.

% \noindent\textbf{Analysis of pitfalls in literature}
To fill this gap in the literature, we identify six distinct pitfalls in the evaluation setups of existing works based on our survey in \S\ref{sec:pitfalls}. We take inspiration from \cite{khan2023pitfalls} in identifying pitfalls in FL defenses and building on top of their work. Not only do we explore additional pitfalls and analyze their impact in much more diverse settings, but we also first perform an extensive systemization to justify the choices we make in pitfall analysis. For example, Figure~\ref{fig:defenses_systemization} and Table~\ref{tab:systemization} help us to select and justify three distinct defenses for the impact analysis of pitfalls in \S\ref{sec:impact}.


Figure~\ref{fig:fl_flowchart} shows the pitfalls at the point of their occurrence in the FL training pipeline. For instance, choosing an intrinsically robust dataset is a pitfall that occurs in the dataset selection stage. We also show the common choices for each stage of the pipeline, such as global or personalized evaluation in the last stage. Figure~\ref{fig:fl_flowchart} also highlights the interdependencies between stages. For example, non-i.i.d. distribution in the distribution stage influences the choice of evaluation metrics (\S\ref{impact:evaluation}), while large-scale limits the threat model in the attack stage (\S\ref{impact:scalability}). We thoroughly explain each pitfall and its prevalence in \S\ref{sec:pitfalls}, and provide actionable recommendations to overcome them. Finally, in \S\ref{sec:impact}, we perform a thorough impact analysis of the identified pitfalls using three representative defenses and show how we can avoid them by following our recommendations.
These pitfalls \emph{can also be applied to attacks}, e.g., only evaluating an attack in cross-silo settings and not considering its efficacy in the cross-device setting is a pitfall. Similarly, other pitfalls can arise from attacks, as attacks and defenses are inherently interconnectedâ€”essentially two sides of the same coin working together. \textbf{\emph{However, for the purpose of this paper, we will analyze the pitfalls from the lens of defenses.}}

\noindent In summary, we make the following contributions:

\noindent\textbf{1. Systemization of FL Defenses:} We perform a comprehensive systemization of FL defenses along three dimensions: processing of client updates, server's knowledge, and the defense phase (\S\ref{sec:systemization}).

\noindent\textbf{2. Identifying Major Pitfalls:} We comprehensively review 50 top-tier FL defense works and identify six prevalent pitfalls. In response to each pitfall, we provide actionable recommendations to guide future research efforts (\S\ref{sec:pitfalls}).

\noindent\textbf{3. Dissecting the Impact of Pitfalls:} Guided by our systemization, we choose three representative FL defenses and use them to perform a thorough impact analysis of our identified pitfalls. We show how the pitfalls lead to a false sense of security, and by following our recommendations, the research community can overcome them (\S\ref{sec:impact}).

% \vspace{-0.2cm}
\begin{mybox}
    This work is not meant as finger-pointing, particularly to the defenses under evaluation. We have chosen them as representative contributions to the field, humbly employing them as testbeds to offer constructive guidelines for future research.
\end{mybox}
% \vspace{-0.3cm}