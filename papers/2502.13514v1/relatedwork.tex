\section{Related Work}
\subsection{Instruction Tuning and Data Creation Strategies}

LLM alignments usually encompass instruction tuning and reinforcement learning with human feedback (RLHF) \cite{ouyang2022training}. Instruction tuning, often referred to as supervised fine-tuning (SFT),  trains LLMs on a dataset consisting of (QUERY, RESPONSE) pairs in a supervised fashion. This effectively bridges the gap between the next-word prediction pertaining objective of LLMs and
the usersâ€™ objective of having LLMs adhere to human instructions \cite{zhang2023instruction}.

In recent years, many heuristic strategies have been explored for creating instruction-tuning data \cite{sprague2024cot}. \citet{wang2023self} proposed to generate instruction tuning data using ChatGPT, prompting ChatGPT to generate both the QUERY and RESPONSE. In the following, quite a few strategies have been proposed to improve the quality of instruction-tuning data. \citet{wang2023noise,weber2024donkii} proposed to remove low-quality data from instruction tuning data with the consideration that low-quality data may mislead LLMs. In another direction, some work found that increasing data quality and diversity instead of quantity can effectively induce instruction following abilities \cite{zhou2024lima,liumakes,xialess}. Thereby, they proposed different strategies to create, or select from existing datasets, high-quality and diverse instruction tuning data. 
For instance, \citet{shen2024rethinking} found out through experimental studies that long responses are surprisingly more effective for instruction tuning. 
More recently, \citet{ding2024mitigating} proposed an innovative Socratic-guided sampling strategy to select instruction-tuning data, improving the sampling efficiency for challenging heavy-tailed data.

However, there lack of a principled understanding of what makes good instruction tuning data for alignment. The building of a data creation strategy predominantly relies on developers' instruction, followed by extensive data creation and experimental validation.
This process is often time-consuming and labor-cost. In this work, we design a swift method to study a strategy's effectiveness using only a handful of examples. This can greatly reduce the cost of strategy validation. In addition, this method may be combined with existing data sampling strategies, e.g., that proposed by \citet{ding2024mitigating}, to achieve further improvement. We leave this topic for future work.

\subsection{Influence Estimation by Gradient Descent}

Data influence estimation using gradient descent has been used in many tasks, such as identifying mislabeled examples \cite{pruthi2020estimating}, analyzing memorization effects \cite{feldman2020neural}, and deriving various interpretability insights \cite{madsen2022post}. In the LLM setting, this kind of method was mainly used for training data selection. For instance, \cite{han2023understanding} used model gradients to select a subset of pretraining data
that supports ICL and accordingly analyze the characteristics of the supportive pertaining data. \cite{engstromdsdm,xialess} traced the projections of training examples' gradients onto those of evaluation examples, and accordingly selected the most effective training examples for evaluation performance. In this work, we employ the idea of gradient-based influence estimation to study the effectiveness of particular data creation strategies on target tasks efficiently.