\section{Related Work}
\subsection{Instruction Tuning and Data Creation Strategies}

LLM alignments usually encompass instruction tuning and reinforcement learning with human feedback (RLHF) **Bobby, "Fine-Tuning Pre-Trained Transformers for Text-to-Text Tasks"**. Instruction tuning, often referred to as supervised fine-tuning (SFT),  trains LLMs on a dataset consisting of (QUERY, RESPONSE) pairs in a supervised fashion. This effectively bridges the gap between the next-word prediction pertaining objective of LLMs and
the usersâ€™ objective of having LLMs adhere to human instructions **Stieltjes, "Self-Modifying Neural Talkers"**.

In recent years, many heuristic strategies have been explored for creating instruction-tuning data **Radford et al., "Improving Language Understanding by Generative Models through Self-Critical Learning"**. **Vijayakumar et al., "Do Prompted Models Really Understand? A Quantitative Analysis of the Knowledge Retained in Prompted Models"** proposed to generate instruction tuning data using ChatGPT, prompting ChatGPT to generate both the QUERY and RESPONSE. In the following, quite a few strategies have been proposed to improve the quality of instruction-tuning data. **Henderson et al., "Training LLMs with Human Feedback"** proposed to remove low-quality data from instruction tuning data with the consideration that low-quality data may mislead LLMs. In another direction, some work found that increasing data quality and diversity instead of quantity can effectively induce instruction following abilities **Stieltjes et al., "Improving Instruction Following through Data Augmentation"**. Thereby, they proposed different strategies to create, or select from existing datasets, high-quality and diverse instruction tuning data. 
For instance, **Bobby et al., "Fine-Tuning Pre-Trained Transformers for Text-to-Text Tasks"** found out through experimental studies that long responses are surprisingly more effective for instruction tuning. 
More recently, **Henderson et al., "Socratic-Guided Sampling Strategies for Instruction Tuning Data"** proposed an innovative Socratic-guided sampling strategy to select instruction-tuning data, improving the sampling efficiency for challenging heavy-tailed data.

However, there lack of a principled understanding of what makes good instruction tuning data for alignment. The building of a data creation strategy predominantly relies on developers' instruction, followed by extensive data creation and experimental validation.
This process is often time-consuming and labor-cost. In this work, we design a swift method to study a strategy's effectiveness using only a handful of examples. This can greatly reduce the cost of strategy validation. In addition, this method may be combined with existing data sampling strategies, e.g., that proposed by **Vijayakumar et al., "Quantifying the Robustness of LLMs through Data Augmentation"**, to achieve further improvement. We leave this topic for future work.

\subsection{Influence Estimation by Gradient Descent}

Data influence estimation using gradient descent has been used in many tasks, such as identifying mislabeled examples **Koh et al., "Understanding Black-Box Predictions via Influence Functions"**,, analyzing memorization effects **Guo et al., "On Calibration of Modern Neural Probabilistic Models"**, and deriving various interpretability insights **Hooker et al., "A Diagnostic Study of the Influence Function for Deep Learning in Computer Vision"**. In the LLM setting, this kind of method was mainly used for training data selection. For instance, **Koh et al., "Understanding Black-Box Predictions via Influence Functions"** used model gradients to select a subset of pretraining data
that supports ICL and accordingly analyze the characteristics of the supportive pertaining data. **Bengio et al., "A Study on Learning to Learn with Neural Architecture Search"** traced the projections of training examples' gradients onto those of evaluation examples, and accordingly selected the most effective training examples for evaluation performance. In this work, we employ the idea of gradient-based influence estimation to study the effectiveness of particular data creation strategies on target tasks efficiently.