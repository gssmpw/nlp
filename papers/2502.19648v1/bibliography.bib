% -----------------------------------------------------------------------
% File: CogSci_Template.bib
% -----------------------------------------------------------------------

% Modified : Eli M. Silk (esilk at pitt.edu)            05/24/2005
% Modified : David Noelle (dnoelle at ucmerced.edu)     11/19/2014


@inproceedings{cadena2019well,
  title={How well do deep neural networks trained on object recognition characterize the mouse visual system?},
  author={Cadena, Santiago A and Sinz, Fabian H and Muhammad, Taliah and Froudarakis, Emmanouil and Cobos, Erick and Walker, Edgar Y and Reimer, Jake and Bethge, Matthias and Tolias, Andreas and Ecker, Alexander S},
  booktitle={Real Neurons $\{$$\backslash$\&$\}$ Hidden Units: Future directions at the intersection of neuroscience and artificial intelligence@ NeurIPS 2019},
  year={2019}
}

@article{cadena2022diverse,
  title={Diverse task-driven modeling of macaque V4 reveals functional specialization towards semantic tasks},
  author={Cadena, Santiago A and Willeke, Konstantin F and Restivo, Kelli and Denfield, George and Sinz, Fabian H and Bethge, Matthias and Tolias, Andreas S and Ecker, Alexander S},
  journal={bioRxiv},
  pages={2022--05},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{cadena2019deep,
  title={Deep convolutional models improve predictions of macaque V1 responses to natural images},
  author={Cadena, Santiago A and Denfield, George H and Walker, Edgar Y and Gatys, Leon A and Tolias, Andreas S and Bethge, Matthias and Ecker, Alexander S},
  journal={PLoS computational biology},
  volume={15},
  number={4},
  pages={e1006897},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}


@article{klindt2017neural,
  title={Neural system identification for large populations separating “what” and “where”},
  author={Klindt, David and Ecker, Alexander S and Euler, Thomas and Bethge, Matthias},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}



@article{zipser1988back,
  title={A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons},
  author={Zipser, David and Andersen, Richard A},
  journal={Nature},
  volume={331},
  number={6158},
  pages={679--684},
  year={1988},
  publisher={Nature Publishing Group UK London}
}

@article{karoui2010spectrum,
author = {Noureddine El Karoui},
title = {{The spectrum of kernel random matrices}},
volume = {38},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {1 -- 50},
keywords = {concentration of measure, Covariance matrices, eigenvalues of covariance matrices, Hadamard matrix functions, high-dimensional inference, kernel matrices, machine learning, multivariate statistical analysis, Random matrix theory},
year = {2010},
doi = {10.1214/08-AOS648},
URL = {https://doi.org/10.1214/08-AOS648}
}


@article{bahri2024explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={27},
  pages={e2311878121},
  year={2024},
  publisher={National Academy of Sciences}
}


@book{widom2016lectures,
  title={Lectures on integral equations},
  author={Widom, Harold},
  year={2016},
  publisher={Courier Dover Publications}
}


@article{nassar20201,
  title={On 1/n neural representation and robustness},
  author={Nassar, Josue and Sokol, Piotr and Chung, SueYeon and Harris, Kenneth D and Park, Il Memming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6211--6222},
  year={2020}
}

@inproceedings{miller2022divisive,
  title={Divisive Feature Normalization Improves Image Recognition Performance in AlexNet},
  author={Miller, Michelle and Chung, SueYeon and Miller, Kenneth D},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{santurkar2019computer,
    title={Computer Vision with a Single (Robust) Classifier},
    author={Shibani Santurkar and Dimitris Tsipras and Brandon Tran and Andrew Ilyas and Logan Engstrom and Aleksander Madry},
    booktitle={ArXiv preprint arXiv:1906.09453},
    year={2019}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@inproceedings{
geirhos2020on,
title={On the surprising similarities between supervised and self-supervised models},
author={Robert Geirhos and Kantharaju Narayanappa and Benjamin Mitzkus and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
booktitle={NeurIPS 2020 Workshop SVRHM},
year={2020},
url={https://openreview.net/forum?id=q2ml4CJMHAx}
}

@article{zhuang2021unsupervised,
  title={Unsupervised neural network models of the ventral visual stream},
  author={Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C and DiCarlo, James J and Yamins, Daniel LK},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={3},
  pages={e2014196118},
  year={2021},
  publisher={National Acad Sciences}
}
@article{konkle2022self,
  title={A self-supervised domain-general learning framework for human ventral stream representation},
  author={Konkle, Talia and Alvarez, George A},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={491},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{muttenthaler2023improving,
  title={Improving neural network representations using human similarity judgments},
  author={Muttenthaler, Lukas and Linhardt, Lorenz and Dippel, Jonas and Vandermeulen, Robert A and Hermann, Katherine and Lampinen, Andrew K and Kornblith, Simon},
  journal={arXiv preprint arXiv:2306.04507},
  year={2023}
}

@article{gulcu2018dnn,
	author = {Umut Gulcehre and Marcel A. J. van Gerven},
	title = {Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream},
	volume = {35},
	number = {27},
	pages = {10005--10014},
	year = {2015},
	doi = {10.1523/JNEUROSCI.5023-14.2015},
	publisher = {Society for Neuroscience},
	abstract = {Converging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream areas. We quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human brain. This was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural network. Our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. Furthermore, it allowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the quality of the developed approach. Stimulus features that successfully explained neural responses indicate that population receptive fields were explicitly tuned for object categorization. This provides strong support for the hypothesis that object categorization is a guiding principle in the functional organization of the primate ventral stream.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/35/27/10005},
	eprint = {https://www.jneurosci.org/content/35/27/10005.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{saxe_if_2021,
	title = {If deep learning is the answer, what is the question?},
	volume = {22},
	issn = {1471-0048},
	url = {https://doi.org/10.1038/s41583-020-00395-8},
	doi = {10.1038/s41583-020-00395-8},
	abstract = {Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.},
	number = {1},
	journal = {Nature Reviews Neuroscience},
	author = {Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
	month = jan,
	year = {2021},
	pages = {55--67},
}


@article{sorscher2022neural,
  title={Neural representational geometry underlies few-shot concept learning},
  author={Sorscher, Ben and Ganguli, Surya and Sompolinsky, Haim},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={43},
  pages={e2200800119},
  year={2022},
  publisher={National Acad Sciences}
}

@article{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@Article{Cohen2020,
author={Cohen, Uri
and Chung, SueYeon
and Lee, Daniel D.
and Sompolinsky, Haim},
title={Separability and geometry of object manifolds in deep neural networks},
journal={Nature Communications},
year={2020},
month={Feb},
day={06},
volume={11},
number={1},
pages={746},
abstract={Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an `object manifold'. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with `classification capacity', a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds' radius, dimensionality and inter-manifold correlations.},
issn={2041-1723},
doi={10.1038/s41467-020-14578-5},
url={https://doi.org/10.1038/s41467-020-14578-5}
}


@article{jazayeri2021interpreting,
  title={Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity},
  author={Jazayeri, Mehrdad and Ostojic, Srdjan},
  journal={Current opinion in neurobiology},
  volume={70},
  pages={113--120},
  year={2021},
  publisher={Elsevier}
}

@article{bashivan2019neural,
  title={Neural population control via deep image synthesis},
  author={Bashivan, Pouya and Kar, Kohitij and DiCarlo, James J},
  journal={Science},
  volume={364},
  number={6439},
  pages={eaav9436},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{chung2021neural,
  title={Neural population geometry: An approach for understanding biological and artificial neural networks},
  author={Chung, SueYeon and Abbott, LF},
  journal={Current opinion in neurobiology},
  volume={70},
  pages={137--144},
  year={2021},
  publisher={Elsevier}
}

@article{gao2017theory,
  title={A theory of multineuronal dimensionality, dynamics and measurement},
  author={Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  journal={BioRxiv},
  pages={214262},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}

@article{cristianini2001kernel,
  title={On kernel-target alignment},
  author={Cristianini, Nello and Shawe-Taylor, John and Elisseeff, Andre and Kandola, Jaz},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019},
  organization={PMLR}
}

@article{raghu2017svcca,
  title={Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{majaj2015simple,
  title={Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance},
  author={Majaj, Najib J and Hong, Ha and Solomon, Ethan A and DiCarlo, James J},
  journal={Journal of Neuroscience},
  volume={35},
  number={39},
  pages={13402--13418},
  year={2015},
  publisher={Soc Neuroscience}
}

@inproceedings{bordelon2020spectrum,
  title={Spectrum dependent learning curves in kernel regression and wide neural networks},
  author={Blake Bordelon and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={International Conference on Machine Learning},
  pages={1024--1034},
  year={2020},
  organization={PMLR}
}

@article{canatar2021spectral,
  title={Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={2914},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{elmoznino2022high,
  title={High-performing neural network models of visual cortex benefit from high latent dimensionality},
  author={Elmoznino, Eric and Bonner, Michael F},
  journal={bioRxiv},
  pages={2022--07},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{schrimpf2018brain,
  title={Brain-score: Which artificial neural network for object recognition is most brain-like?},
  author={Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J and Rajalingham, Rishi and Issa, Elias B and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and others},
  journal={BioRxiv},
  pages={407007},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@article{Schrimpf2020integrative,
  title={Integrative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence},
  author={Schrimpf, Martin and Kubilius, Jonas and Lee, Michael J and Murty, N Apurva Ratan and Ajemian, Robert and DiCarlo, James J},
  journal={Neuron},
  year={2020},
  url={https://www.cell.com/neuron/fulltext/S0896-6273(20)30605-X}
}


@article{khaligh2014deep,
  title={Deep supervised, but not unsupervised, models may explain IT cortical representation},
  author={Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
  journal={PLoS computational biology},
  volume={10},
  number={11},
  pages={e1003915},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}


@article{yamins2014performance,
  title={Performance-optimized hierarchical models predict neural responses in higher visual cortex},
  author={Yamins, Daniel LK and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},
  journal={Proceedings of the national academy of sciences},
  volume={111},
  number={23},
  pages={8619--8624},
  year={2014},
  publisher={National Acad Sciences}
}

@article{kell2018task,
  title={A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy},
  author={Kell, Alexander JE and Yamins, Daniel LK and Shook, Erica N and Norman-Haignere, Sam V and McDermott, Josh H},
  journal={Neuron},
  volume={98},
  number={3},
  pages={630--644},
  year={2018},
  publisher={Elsevier}
}

@article{richards2019deep,
  title={A deep learning framework for neuroscience},
  author={Richards, Blake A and Lillicrap, Timothy P and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and others},
  journal={Nature neuroscience},
  volume={22},
  number={11},
  pages={1761--1770},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{lindsay2021convolutional,
  title={Convolutional neural networks as a model of the visual system: Past, present, and future},
  author={Lindsay, Grace W},
  journal={Journal of cognitive neuroscience},
  volume={33},
  number={10},
  pages={2017--2031},
  year={2021},
  publisher={MIT Press}
}

@Article{Freeman2013,
        author={Freeman, Jeremy
        and Ziemba, Corey M.
        and Heeger, David J.
        and Simoncelli, Eero P.
        and Movshon, J. Anthony},
        title={A functional and perceptual signature of the second visual area in primates},
        journal={Nature Neuroscience},
        year={2013},
        month={Jul},
        day={01},
        volume={16},
        number={7},
        pages={974-981},
        abstract={The authors examined neuronal responses in V1 and V2 to synthetic texture stimuli that replicate higher-order statistical dependencies found in natural images. V2, but not V1, responded differentially to these textures, in both macaque (single neurons) and human (fMRI). Human detection of naturalistic structure in the same images was predicted by V2 responses, suggesting a role for V2 in representing natural image structure.},
        issn={1546-1726},
        doi={10.1038/nn.3402},
        url={https://doi.org/10.1038/nn.3402}
        }

@InProceedings{ChalnickBillman1988a,
  author =	 {A. Chalnick and D. Billman},
  title =	 {Unsupervised learning of correlational structure},
  booktitle =	 {Proceedings of the Tenth Annual Conference of the
                  Cognitive Science Society},
  pages =	 {510--516},
  year =	 1988,
  address =	 {Hillsdale, NJ},
  publisher =	 {Lawrence Erlbaum Associates}
}

@InCollection{Feigenbaum1963a,
  author =	 {E. A. Feigenbaum},
  title =	 {The simulation of verbal learning behavior},
  booktitle =	 {Computers and thought},
  publisher =	 {McGraw-Hill},
  year =	 1963,
  editor =	 {E. A. Feigenbaum and J. Feldman},
  address =	 {New York}
}

@Article{Hill1983a,
  author =	 {J. A. C. Hill},
  title =	 {A computational model of language acquisition in the
                  two-year old},
  journal =	 {Cognition and Brain Theory},
  year =	 1983,
  volume =	 6,
  pages =	 {287--317}
}

@TechReport{OhlssonLangley1985a,
  author =	 {S. Ohlsson and P. Langley},
  title =	 {Identifying solution paths in cognitive diagnosis},
  institution =	 {Carnegie Mellon University, The Robotics Institute},
  year =	 1985,
  address =	 {Pittsburgh, PA},
  number =	 {CMU-RI-TR-85-2}
}

@PhdThesis{Matlock2001,
  author =	 {Teenie Matlock},
  title =	 {How real is fictive motion?},
  school =	 {Psychology Department, University of California,
                  Santa Cruz},
  year =	 2001,
  type =	 {Doctoral dissertation}
}

@PhdThesis{Lewis1978a,
  author =	 {C. Lewis},
  title =	 {Production system models of practice effects},
  school =	 {Department of Psychology, University of Michigan,
                  Ann Arbor},
  year =	 1978,
  type =	 {Doctoral dissertation}
}

@Book{NewellSimon1972a,
  author =	 {A. Newell and H. A. Simon},
  title =	 {Human problem solving},
  publisher =	 {Prentice-Hall},
  year =	 1972,
  address =	 {Englewood Cliffs, NJ}
}

@Book{ShragerLangley1990a,
  editor =	 {J. Shrager and P. Langley},
  title =	 {Computational models of scientific discovery and
                  theory formation},
  publisher =	 {Morgan Kaufmann},
  year =	 1990,
  address =	 {San Mateo, CA}
}

@article{van2017primer,
  title={A primer on encoding models in sensory neuroscience},
  author={van Gerven, Marcel AJ},
  journal={Journal of Mathematical Psychology},
  volume={76},
  pages={172--183},
  year={2017},
  publisher={Elsevier}
}

@article{naselaris2011encoding,
  title={Encoding and decoding in fMRI},
  author={Naselaris, Thomas and Kay, Kendrick N and Nishimoto, Shinji and Gallant, Jack L},
  journal={Neuroimage},
  volume={56},
  number={2},
  pages={400--410},
  year={2011},
  publisher={Elsevier}
}

@article{carandini2005we,
  title={Do we know what the early visual system does?},
  author={Carandini, Matteo and Demb, Jonathan B and Mante, Valerio and Tolhurst, David J and Dan, Yang and Olshausen, Bruno A and Gallant, Jack L and Rust, Nicole C},
  journal={Journal of Neuroscience},
  volume={25},
  number={46},
  pages={10577--10597},
  year={2005},
  publisher={Soc Neuroscience}
}

@article{enroth1966contrast,
  title={The contrast sensitivity of retinal ganglion cells of the cat},
  author={Enroth-Cugell, Christina and Robson, John G},
  journal={The Journal of physiology},
  volume={187},
  number={3},
  pages={517--552},
  year={1966},
  publisher={Wiley Online Library}
}

@article{adelson1985spatiotemporal,
  title={Spatiotemporal energy models for the perception of motion},
  author={Adelson, Edward H and Bergen, James R},
  journal={Josa a},
  volume={2},
  number={2},
  pages={284--299},
  year={1985},
  publisher={Optica Publishing Group}
}

@article{feather2022model,
  title={Model metamers reveal divergent invariances between biological and artificial neural networks},
  author={Feather, Jenelle and Leclerc, Guillaume and Madry, Aleksander and McDermott, Josh H},
  journal={Nature Neuroscience},
  pages={1--18},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{dapello2021neural,
  title={Neural population geometry reveals the role of stochasticity in robust perception},
  author={Dapello, Joel and Feather, Jenelle and Le, Hang and Marques, Tiago and Cox, David and McDermott, Josh and DiCarlo, James J and Chung, SueYeon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15595--15607},
  year={2021}
}
@article{tuckute2022many,
  title={Many but not all deep neural network audio models capture brain responses and exhibit hierarchical region correspondence},
  author={Tuckute, Greta and Feather, Jenelle and Boebinger, Dana and McDermott, Josh H},
  journal={bioRxiv},
  pages={2022--09},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{golan2020controversial,
  title={Controversial stimuli: Pitting neural networks against each other as models of human cognition},
  author={Golan, Tal and Raju, Prashant C and Kriegeskorte, Nikolaus},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={47},
  pages={29330--29337},
  year={2020},
  publisher={National Acad Sciences}
}

@article{geirhos2021partial,
  title={Partial success in closing the gap between human and machine vision},
  author={Geirhos, Robert and Narayanappa, Kantharaju and Mitzkus, Benjamin and Thieringer, Tizian and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23885--23899},
  year={2021}
}

@article{geirhos2018generalisation,
  title={Generalisation in humans and deep neural networks},
  author={Geirhos, Robert and Temme, Carlos RM and Rauber, Jonas and Sch{\"u}tt, Heiko H and Bethge, Matthias and Wichmann, Felix A},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{geirhos2018imagenet,
  title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}


@misc{simon2022eigenlearning,
      title={The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks}, 
      author={James B. Simon and Madeline Dickens and Dhruva Karkada and Michael R. DeWeese},
      year={2022},
      eprint={2110.03922},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{conwell2022can,
  title={What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?},
  author={Conwell, Colin and Prince, Jacob S and Kay, Kendrick N and Alvarez, George A and Konkle, Talia},
  journal={BioRxiv},
  pages={2022--03},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{stringer2019high,
  title={High-dimensional geometry of population responses in visual cortex},
  author={Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D},
  journal={Nature},
  volume={571},
  number={7765},
  pages={361--365},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{canatar2021out,
  title={Out-of-distribution generalization in kernel regression},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12600--12612},
  year={2021}
}

@article{jacot2020kernel,
  title={Kernel alignment risk estimator: Risk prediction from training data},
  author={Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15568--15578},
  year={2020}
}

@inproceedings{guo2022adversarially,
  title={Adversarially trained neural representations are already as robust as biological neural representations},
  author={Guo, Chong and Lee, Michael and Leclerc, Guillaume and Dapello, Joel and Rao, Yug and Madry, Aleksander and Dicarlo, James},
  booktitle={International Conference on Machine Learning},
  pages={8072--8081},
  year={2022},
  organization={PMLR}
}

@misc{robustness_python,
   title={Robustness (Python Library)},
   author={Logan Engstrom and Andrew Ilyas and Hadi Salman and Shibani Santurkar and Dimitris Tsipras},
   year={2019},
   url={https://github.com/MadryLab/robustness}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{dosovitskiyimage,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year=2021
}

@inproceedings{zbontar2021barlow,
  title={Barlow twins: Self-supervised learning via redundancy reduction},
  author={Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  booktitle={International Conference on Machine Learning},
  pages={12310--12320},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2021empirical,
  title={An empirical study of training self-supervised vision transformers},
  author={Chen, Xinlei and Xie, Saining and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9640--9649},
  year={2021}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}


@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{salman2020adversarially,
  title={Do adversarially robust ImageNet models transfer better?},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  booktitle={Proceedings of the 34th International Conference on Neural Information Processing Systems},
  pages={3533--3545},
  year={2020}
}


@article{kriegeskorte2008representational,
  title={Representational similarity analysis-connecting the branches of systems neuroscience},
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A},
  journal={Frontiers in systems neuroscience},
  pages={4},
  year={2008},
  publisher={Frontiers}
}


@article{hermann2020shapes,
  title={What shapes feature representations? exploring datasets, architectures, and training},
  author={Hermann, Katherine and Lampinen, Andrew},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9995--10006},
  year={2020}
}

@article{hermann2020origins,
  title={The origins and prevalence of texture bias in convolutional neural networks},
  author={Hermann, Katherine and Chen, Ting and Kornblith, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19000--19015},
  year={2020}
}

@article{dapello2020simulating,
  title={Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations},
  author={Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David and DiCarlo, James J},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13073--13087},
  year={2020}
}

@article{nonaka2021brain,
  title={Brain hierarchy score: Which deep neural networks are hierarchically brain-like?},
  author={Nonaka, Soma and Majima, Kei and Aoki, Shuntaro C and Kamitani, Yukiyasu},
  journal={IScience},
  volume={24},
  number={9},
  pages={103013},
  year={2021},
  publisher={Elsevier}
}

@article{el2010spectrum,
  title={The spectrum of kernel random matrices},
  author={El Karoui, Noureddine},
  journal={Ann. Statist.},
  volume={38},
  number={1},
  pages={1--50},
  year={2010}
}

@article{kubilius2018cornet,
  title={Cornet: Modeling the neural mechanisms of core object recognition},
  author={Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel LK and DiCarlo, James J},
  journal={BioRxiv},
  pages={408385},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{canatar2022kernel,
  title={A Kernel Analysis of Feature Learning in Deep Neural Networks},
  author={Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}

@article{williams1995gaussian,
  title={Gaussian processes for regression},
  author={Williams, Christopher and Rasmussen, Carl},
  journal={Advances in neural information processing systems},
  volume={8},
  year={1995}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{kobak2020optimal,
  title={The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization},
  author={Kobak, Dmitry and Lomond, Jonathan and Sanchez, Benoit},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={6863--6878},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}

@article{liang2020just,
author = {Tengyuan Liang and Alexander Rakhlin},
title = {{Just interpolate: Kernel “Ridgeless” regression can generalize}},
volume = {48},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1329 -- 1347},
keywords = {data-dependent bounds, high dimensionality, implicit regularization, kernel methods, Minimum-norm interpolation, reproducing kernel Hilbert spaces, spectral decay},
year = {2020},
doi = {10.1214/19-AOS1849},
URL = {https://doi.org/10.1214/19-AOS1849}
}

@inproceedings{jacot2020implicit,
  title={Implicit regularization of random feature models},
  author={Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  booktitle={International Conference on Machine Learning},
  pages={4631--4640},
  year={2020},
  organization={PMLR}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@ARTICLE{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2:3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{hofmann2008kernel,
  title={Kernel methods in machine learning},
  author={Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  journal={The Annals of Statistics},
  volume={36},
  number={3},
  pages={1171},
  year={2008},
  publisher={Institute of Mathematical Statistics}
}

@article{sollich1998learning,
  title={Learning curves for Gaussian processes},
  author={Sollich, Peter},
  journal={Advances in neural information processing systems},
  volume={11},
  year={1998}
}

@article{seung1992statistical,
  title={Statistical mechanics of learning from examples},
  author={Seung, Hyunjune Sebastian and Sompolinsky, Haim and Tishby, Naftali},
  journal={Physical review A},
  volume={45},
  number={8},
  pages={6056},
  year={1992},
  publisher={APS}
}

@book{bai2010spectral,
  title={Spectral analysis of large dimensional random matrices},
  author={Bai, Zhidong and Silverstein, Jack W},
  volume={20},
  year={2010},
  publisher={Springer}
}

@article{loureiro2021learning,
  title={Learning curves of generic features maps for realistic datasets with a teacher-student model},
  author={Loureiro, Bruno and Gerbelot, Cedric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and Mezard, Marc and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18137--18151},
  year={2021}
}

@article{ding2021grounding,
  title={Grounding representation similarity with statistical testing},
  author={Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2108.01661},
  year={2021}
}


@article{canatar2024spectral,
  title={A spectral theory of neural prediction and alignment},
  author={Canatar, Abdulkadir and Feather, Jenelle and Wakhloo, Albert and Chung, SueYeon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bun2018overlaps,
  title={Overlaps between eigenvectors of correlated random matrices},
  author={Bun, Jo{\"e}l and Bouchaud, Jean-Philippe and Potters, Marc},
  journal={Physical Review E},
  volume={98},
  number={5},
  pages={052145},
  year={2018},
  publisher={APS}
}

@article{baik2005phase,
author = {Jinho Baik and G{\'e}rard Ben Arous and Sandrine P{\'e}ch{\'e}},
title = {{Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices}},
volume = {33},
journal = {The Annals of Probability},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1643 -- 1697},
keywords = {Airy kernel, limit theorem, Random matrix, Sample covariance, Tracy–Widom distribution},
year = {2005},
doi = {10.1214/009117905000000233},
URL = {https://doi.org/10.1214/009117905000000233}
}

@inproceedings{pospisilestimating,
  title={Estimating Shape Distances on Neural Representations with Limited Samples},
  author={Pospisil, Dean A and Larsen, Brett W and Harvey, Sarah E and Williams, Alex H},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@misc{kornblith2019similarityneuralnetworkrepresentations,
      title={Similarity of Neural Network Representations Revisited}, 
      author={Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey Hinton},
      year={2019},
      eprint={1905.00414},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.00414}, 
}


@misc{bloemendal2015principalcomponentssamplecovariance,
      title={On the principal components of sample covariance matrices}, 
      author={Alex Bloemendal and Antti Knowles and Horng-Tzer Yau and Jun Yin},
      year={2015},
      eprint={1404.0788},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/1404.0788}, 
}



@misc{baik2004phasetransitionlargesteigenvalue,
      title={Phase transition of the largest eigenvalue for non-null complex sample covariance matrices}, 
      author={Jinho Baik and Gerard Ben Arous and Sandrine Peche},
      year={2004},
      eprint={math/0403022},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/math/0403022}, 
}


@article{Bun_2017,
   title={Cleaning large correlation matrices: Tools from Random Matrix Theory},
   volume={666},
   ISSN={0370-1573},
   url={http://dx.doi.org/10.1016/j.physrep.2016.10.005},
   DOI={10.1016/j.physrep.2016.10.005},
   journal={Physics Reports},
   publisher={Elsevier BV},
   author={Bun, Joël and Bouchaud, Jean-Philippe and Potters, Marc},
   year={2017},
   month=jan, pages={1–109} }



@misc{pospisil2023estimatingshapedistancesneural,
      title={Estimating Shape Distances on Neural Representations with Limited Samples}, 
      author={Dean A. Pospisil and Brett W. Larsen and Sarah E. Harvey and Alex H. Williams},
      year={2023},
      eprint={2310.05742},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2310.05742}, 
}


@book{Potters_Bouchaud_2020, place={Cambridge}, title={A First Course in Random Matrix Theory: for Physicists, Engineers and Data Scientists}, publisher={Cambridge University Press}, author={Potters, Marc and Bouchaud, Jean-Philippe}, year={2020}} <div></div>



@article{Monasson_2015,
   title={Estimating the principal components of correlation matrices from all their empirical eigenvectors},
   volume={112},
   ISSN={1286-4854},
   url={http://dx.doi.org/10.1209/0295-5075/112/50001},
   DOI={10.1209/0295-5075/112/50001},
   number={5},
   journal={EPL (Europhysics Letters)},
   publisher={IOP Publishing},
   author={Monasson, Rémi and Villamaina, Dario},
   year={2015},
   month=dec, pages={50001} }



@misc{benaychgeorges2021optimalcleaningsingularvalues,
      title={Optimal cleaning for singular values of cross-covariance matrices}, 
      author={Florent Benaych-Georges and Jean-Philippe Bouchaud and Marc Potters},
      year={2021},
      eprint={1901.05543},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/1901.05543}, 
}


@misc{atanasov2024scalingrenormalizationhighdimensionalregression,
      title={Scaling and renormalization in high-dimensional regression}, 
      author={Alexander Atanasov and Jacob A. Zavatone-Veth and Cengiz Pehlevan},
      year={2024},
      eprint={2405.00592},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.00592}, 
}

@article {Stringer374090,
	author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
	title = {High-dimensional geometry of population responses in visual cortex},
	elocation-id = {374090},
	year = {2018},
	doi = {10.1101/374090},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {A neuronal population encodes information most efficiently when its activity is uncorrelated and high-dimensional, but correlated lower-dimensional codes provide robustness against noise. Here, we analyzed the correlation structure of natural image coding, in large visual cortical populations recorded from awake mice. Evoked population activity was high dimensional, with correlations obeying an unexpected power-law: the nth principal component variance scaled as 1/n. This was not inherited from the 1/f spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that the variance spectrum must decay at least this fast if a population code is smooth, i.e. if small changes in input cannot dominate population activity. The theory also predicted larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness represents a fundamental constraint governing correlations in neural population codes.},
	URL = {https://www.biorxiv.org/content/early/2018/07/22/374090},
	eprint = {https://www.biorxiv.org/content/early/2018/07/22/374090.full.pdf},
	journal = {bioRxiv}
}

@article{Bun_2016,
   title={Rotational Invariant Estimator for General Noisy Matrices},
   volume={62},
   ISSN={1557-9654},
   url={http://dx.doi.org/10.1109/TIT.2016.2616132},
   DOI={10.1109/tit.2016.2616132},
   number={12},
   journal={IEEE Transactions on Information Theory},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Bun, Joel and Allez, Romain and Bouchaud, Jean-Philippe and Potters, Marc},
   year={2016},
   month=dec, pages={7475–7490} }

@misc{ghosh2022investigatingpowerlawsdeep,
      title={Investigating Power laws in Deep Representation Learning}, 
      author={Arna Ghosh and Arnab Kumar Mondal and Kumar Krishna Agrawal and Blake Richards},
      year={2022},
      eprint={2202.05808},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.05808}, 
}

@misc{bordelon2022selfconsistentdynamicalfieldtheory,
      title={Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks}, 
      author={Blake Bordelon and Cengiz Pehlevan},
      year={2022},
      eprint={2205.09653},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.09653}, 
}

@article{Li_2021,
   title={Statistical Mechanics of Deep Linear Neural Networks: The Backpropagating Kernel Renormalization},
   volume={11},
   ISSN={2160-3308},
   url={http://dx.doi.org/10.1103/PhysRevX.11.031059},
   DOI={10.1103/physrevx.11.031059},
   number={3},
   journal={Physical Review X},
   publisher={American Physical Society (APS)},
   author={Li, Qianyi and Sompolinsky, Haim},
   year={2021},
   month=sep }

@article{Deutsch:1991msp,
    author = "Deutsch, J. M.",
    title = "{Quantum statistical mechanics in a closed system}",
    doi = "10.1103/PhysRevA.43.2046",
    journal = "Phys. Rev. A",
    volume = "43",
    number = "4",
    pages = "2046",
    year = "1991"
}


@misc{raghu2017svccasingularvectorcanonical,
      title={SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability}, 
      author={Maithra Raghu and Justin Gilmer and Jason Yosinski and Jascha Sohl-Dickstein},
      year={2017},
      eprint={1706.05806},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1706.05806}, 
}

@inproceedings{
williams2024equivalence,
title={Equivalence between representational similarity analysis, centered kernel alignment, and canonical correlations analysis},
author={Alex H Williams},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=zMdnnFasgC}
}

@article{PhysRevA.43.2046,
  title = {Quantum statistical mechanics in a closed system},
  author = {Deutsch, J. M.},
  journal = {Phys. Rev. A},
  volume = {43},
  issue = {4},
  pages = {2046--2049},
  numpages = {0},
  year = {1991},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.43.2046},
  url = {https://link.aps.org/doi/10.1103/PhysRevA.43.2046}
}

@article{BAIK20061382,
title = {Eigenvalues of large sample covariance matrices of spiked population models},
journal = {Journal of Multivariate Analysis},
volume = {97},
number = {6},
pages = {1382-1408},
year = {2006},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X0500134X},
author = {Jinho Baik and Jack W. Silverstein},
keywords = {Eigenvalues, Sample covariance matrices, Spiked population models, Almost sure limits, Non-null case},
abstract = {We consider a spiked population model, proposed by Johnstone, in which all the population eigenvalues are one except for a few fixed eigenvalues. The question is to determine how the sample eigenvalues depend on the non-unit population ones when both sample size and population size become large. This paper completely determines the almost sure limits of the sample eigenvalues in a spiked model for a general class of samples.}
}

@misc{aggarwal2023mobilityedgelevymatrices,
      title={Mobility Edge for L\'evy Matrices}, 
      author={Amol Aggarwal and Charles Bordenave and Patrick Lopatto},
      year={2023},
      eprint={2210.09458},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/2210.09458}, 
}


@article{hotelling1936relations,
  title={Relations between two sets of variates.},
  author={Hotelling, H},
  journal={Biometrika},
  year={1936}
}


@article{bjorck1973numerical,
  title={Numerical methods for computing angles between linear subspaces},
  author={Bjorck, Ake and Golub, Gene H},
  journal={Mathematics of computation},
  volume={27},
  number={123},
  pages={579--594},
  year={1973}
}

@book{golub1995canonical,
  title={The canonical correlations of matrix pairs and their numerical computation},
  author={Golub, Gene H and Zha, Hongyuan},
  year={1995},
  publisher={Springer}
}

@article{knowles2017anisotropic,
  title={Anisotropic local laws for random matrices},
  author={Knowles, Antti and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={169},
  pages={257--352},
  year={2017},
  publisher={Springer}
}

@article{marchenko1967distribution,
  title={Distribution of eigenvalues for some sets of random matrices},
  author={Marchenko, Vladimir Alexandrovich and Pastur, Leonid Andreevich},
  journal={Matematicheskii Sbornik},
  volume={114},
  number={4},
  pages={507--536},
  year={1967},
  publisher={Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}

@article{ledoit2011eigenvectors,
  title={Eigenvectors of some large sample covariance matrix ensembles},
  author={Ledoit, Olivier and P{\'e}ch{\'e}, Sandrine},
  journal={Probability Theory and Related Fields},
  volume={151},
  number={1},
  pages={233--264},
  year={2011},
  publisher={Springer}
}


@misc{kong2017spectrumestimationsamples,
      title={Spectrum Estimation from Samples}, 
      author={Weihao Kong and Gregory Valiant},
      year={2017},
      eprint={1602.00061},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.00061}, 
}

@article {Pospisil2024.02.16.580726,
	author = {Pospisil, Dean A. and Pillow, Jonathan W.},
	title = {Revisiting the high-dimensional geometry of population responses in visual cortex},
	elocation-id = {2024.02.16.580726},
	year = {2024},
	doi = {10.1101/2024.02.16.580726},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Recent advances in large-scale recording technology have spurred exciting new inquiries into the high-dimensional geometry of the neural code. However, characterizing this geometry from noisy neural responses, particularly in datasets with more neurons than trials, poses major statistical challenges. We address this problem by developing new tools for the accurate estimation of high-dimensional signal geometry. We apply these tools to investigate the geometry of representations in mouse primary visual cortex. Previous work has argued that these representations exhibit a power law, in which the n{\textquoteright}th principal component falls off as 1/n. Here we show that response geometry in V1 is better described by a broken power law, in which two different exponents govern the falloff of early and late modes of population activity. Our analysis reveals that later modes decay more rapidly than previously suggested, resulting in a substantially larger fraction of signal variance contained in the early modes of population activity. We examined the signal representations of the early population modes and found them to have higher fidelity than even the most reliable neurons. Intriguingly there are many population modes not captured by classic models of primary visual cortex indicating there is highly redundant yet poorly characterized tuning across neurons. Furthermore, inhibitory neurons tend to co-activate in response to stimuli that drive the early modes consistent with a role in sharpening population level tuning. Overall, our novel and broadly applicable approach overturns prior results and reveals striking structure in a population sensory representation.Significance Statement The nervous system encodes the visual environment across millions of neurons. Such high-dimensional signals are difficult to estimate{\textemdash}and consequently{\textemdash}to characterize. We address this challenge with a novel statistical method that revises past conceptions of the complexity of encoding in primary visual cortex. We discover population encoding is dominated by approximately ten features while additional features account for much less of the representation than previously thought. Many dominant features are not explained by classic models indicating highly redundant encoding of poorly characterized nonlinear image features. Interestingly, inhibitory neurons respond in unison to dominant features consistent with a role in sharpening population representation. Overall, we discover striking properties of population visual representation with novel, broadly applicable, statistical tools.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2024/02/21/2024.02.16.580726},
	eprint = {https://www.biorxiv.org/content/early/2024/02/21/2024.02.16.580726.full.pdf},
	journal = {bioRxiv}
}



@article{bru1989diffusions,
  title={Diffusions of perturbed principal component analysis},
  author={Bru, Marie-France},
  journal={Journal of multivariate analysis},
  volume={29},
  number={1},
  pages={127--136},
  year={1989},
  publisher={Elsevier}
}

@article{bru1991wishart,
  title={Wishart processes},
  author={Bru, Marie-France},
  journal={Journal of Theoretical Probability},
  volume={4},
  pages={725--751},
  year={1991},
  publisher={Springer}
}

@article{silverstein1995analysis,
  title={Analysis of the limiting spectral distribution of large dimensional random matrices},
  author={Silverstein, Jack W and Choi, Sang-Il},
  journal={Journal of Multivariate Analysis},
  volume={54},
  number={2},
  pages={295--309},
  year={1995},
  publisher={Elsevier}
}

@article{lahiri2016random,
  title={Random projections of random manifolds},
  author={Lahiri, Subhaneil and Gao, Peiran and Ganguli, Surya},
  journal={arXiv preprint arXiv:1607.04331},
  year={2016}
}

@misc{chun2024estimatingspectralmomentskernel,
      title={Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices}, 
      author={Chanwoo Chun and SueYeon Chung and Daniel D. Lee},
      year={2024},
      eprint={2410.17998},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.17998}, 
}

@InProceedings{10.1007/978-1-4612-4228-4_3,
author="Golub, Gene H.
and Zha, Hongyuan",
editor="Bojanczyk, Adam
and Cybenko, George",
title="The Canonical Correlations of Matrix Pairs and their Numerical Computation",
booktitle="Linear Algebra for Signal Processing",
year="1995",
publisher="Springer New York",
address="New York, NY",
pages="27--49",
abstract="This paper is concerned with the analysis of canonical correlations of matrix pairs and their numerical computation. We first develop a decomposition theorem for matrix pairs having the same number of rows which explicitly exhibits the canonical correlations. We then present a perturbation analysis of the canonical correlations, which compares favorably with the classical first order perturbation analysis. Then we propose several numerical algorithms for computing the canonical correlations of general matrix pairs; emphasis is placed on the case of large sparse or structured matrices.",
isbn="978-1-4612-4228-4"
}

@InProceedings{10.1007/11564089_7,
author="Gretton, Arthur
and Bousquet, Olivier
and Smola, Alex
and Sch{\"o}lkopf, Bernhard",
editor="Jain, Sanjay
and Simon, Hans Ulrich
and Tomita, Etsuji",
title="Measuring Statistical Dependence with Hilbert-Schmidt Norms",
booktitle="Algorithmic Learning Theory",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="63--77",
abstract="We propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.",
isbn="978-3-540-31696-1"
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}




@misc{ma2022samplecanonicalcorrelationcoefficients,
      title={Sample canonical correlation coefficients of high-dimensional random vectors with finite rank correlations}, 
      author={Zongming Ma and Fan Yang},
      year={2022},
      eprint={2102.03297},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url={https://arxiv.org/abs/2102.03297}, 
}


@misc{bykhovskaya2025highdimensionalcanonicalcorrelationanalysis,
      title={High-Dimensional Canonical Correlation Analysis}, 
      author={Anna Bykhovskaya and Vadim Gorin},
      year={2025},
      eprint={2306.16393},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      url={https://arxiv.org/abs/2306.16393}, 
}




@inproceedings{Cai2016,
    author = {Cai, Mingbo and Schuck, Nicolas W and Pillow, Jonathan W and Niv, Yael},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {A Bayesian method for reducing bias in neural representational similarity analysis},
    url = {https://proceedings.neurips.cc/paper/2016/file/b06f50d1f89bd8b2a0fb771c1a69c2b0-Paper.pdf},
    volume = {29},
    year = {2016}
}

@article{Walther2016,
	author = {Alexander Walther and Hamed Nili and Naveed Ejaz and Arjen Alink and Nikolaus Kriegeskorte and J{\"o}rn Diedrichsen},
	journal = {NeuroImage},
	pages = {188-200},
	title = {Reliability of dissimilarity measures for multi-voxel pattern analysis},
	volume = {137},
	year = {2016}
}



@article{Schutt2023,
    article_type = {journal},
    title = {Statistical inference on representational geometries},
    author = {Schütt, Heiko H and Kipnis, Alexander D and Diedrichsen, Jörn and Kriegeskorte, Nikolaus},
    editor = {Serences, John T and Behrens, Timothy E},
    volume = 12,
    year = 2023,
    month = {aug},
    pub_date = {2023-08-23},
    pages = {e82566},
    citation = {eLife 2023;12:e82566},
    doi = {10.7554/eLife.82566},
    url = {https://doi.org/10.7554/eLife.82566},
    keywords = {statistical inference, representational similarity analysis, toolbox, human, mouse},
    journal = {eLife},
    issn = {2050-084X},
    publisher = {eLife Sciences Publications, Ltd},
}

@article{PhysRevE.50.1810,
  title = {Theory of L\'evy matrices},
  author = {Cizeau, P. and Bouchaud, J. P.},
  journal = {Phys. Rev. E},
  volume = {50},
  issue = {3},
  pages = {1810--1822},
  numpages = {0},
  year = {1994},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.50.1810},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.50.1810}
}


@misc{ledoit2016numericalimplementationquestfunction,
      title={Numerical Implementation of the QuEST Function}, 
      author={Olivier Ledoit and Michael Wolf},
      year={2016},
      eprint={1601.05870},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1601.05870}, 
}