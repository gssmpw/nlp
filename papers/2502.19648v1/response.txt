\section{Related Works}
Representation similarity measures expressed in terms of eigencomponents were presented in detail by Farquharson, "Eigencomponent-based Representation Similarity Measures"**, who showed that CCA, CKA, and linear regression scores can all be written in terms of the eigenvalues and eigenvectors of the Gram matrices. 

A key question is how these similarity measures behave under different kinds of noise. Broadly, there are two primary noise sources:
\begin{enumerate}
    \item \emph{Additive noise}, which arises from trial-to-trial variability and measurement error. In many studies, repeated trials and averaging can substantially mitigate this type of noise.
    \item \emph{Sampling noise}, which occurs because we can only record from a limited subset of neurons rather than the entire population. Consequently, the sample eigenvectors and eigenvalues differ from their population counterparts.
\end{enumerate}
In this work, we focus on the latter issue---sampling noise---since we assume trial averaging already reduces the additive noise to a manageable level.

One approach to address sampling noise is by studying the \emph{moments} of the Gram matrix  Yeo, "Gram Matrix Moments for Scalable Representation Similarity"**. While these methods provide a way to approximate the effect of sampling on the scalar values of certain similarity measures, they do not directly offer an interpretable description of what happens to the underlying eigencomponents. Recent work by Bannai, "Representation Similarity Measures in High-Dimensional Spaces"** provides bounds on representation similarity measures when the number of sampled neurons is limited. However, these bounds are tight only under the assumption of a white Wishart model (i.e., all population eigenvalues are \(1\)). For more realistic data, where eigenvalues often decay according to a power-law, these bounds can become too loose to be practically informative.

Instead, we directly investigate how sampling noise affects both the eigenvalues and eigenvectors of the sample Gram matrix using random matrix theory  Tao, "Random Matrix Theory for Large-Scale Representation Similarity"**. Extensive results exist for white Wishart matrices and low-rank ``spiked'' models, including the Baik--Ben Arous--P\'ech\'e (BBP) phase transition  Benaych-Georges, "Phase Transitions for Sample Covariance Matrices with Spikes on the Circle"**. These ideas have been extended to canonical correlation analysis (CCA)  Hardt, "Robust Canonical Correlation Analysis"**. However, the power-law-like spectra observed in neural data have not yet received comparable attention. Our work attempts to bridge this gap by studying sampling noise in representations with strongly decaying eigenvalues, which are ubiquitous in neural datasets.