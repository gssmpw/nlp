\UseRawInputEncoding
\documentclass{article}
\usepackage{custom-definitions}

% If submitted for review:
% \usepackage{icml2025/icml2025}

% If preprint:
\usepackage[preprint]{icml2025/icml2025}

% If accepted:
% \usepackage[accepted]{icml2025/icml2025}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}
\usepackage[all]{nowidow} % prevent few word lines
% Short \icmltitle
\icmltitlerunning{Spectral Analysis of Representational Similarity
}

\begin{document}

\twocolumn[
\icmltitle{Spectral Analysis of Representational Similarity with Limited Neurons}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hyunmo Kang}{equal,flatiron}
\icmlauthor{Abdulkadir Canatar}{equal,flatiron,nyu}
\icmlauthor{SueYeon Chung}{flatiron,nyu}
\end{icmlauthorlist}

\icmlaffiliation{flatiron}{Center for Computational Neuroscience, Flatiron Institute, New York, NY, 10010}
\icmlaffiliation{nyu}{Center for Neural Science, New York University, New York, NY, 10003, USA}
\icmlcorrespondingauthor{SueYeon Chung}{schung@nyu.edu}
% the "keywords" metadata
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}

Measuring representational similarity between neural recordings and computational models is challenging due to constraints on the number of neurons that can be recorded simultaneously. In this work, we investigate how such limitations affect similarity measures, focusing on Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA). Leveraging tools from Random Matrix Theory, we develop a predictive spectral framework for these measures and demonstrate that finite neuron sampling systematically underestimates similarity due to eigenvector delocalization. To overcome this, we introduce a denoising method to infer population-level similarity, enabling accurate analysis even with small neuron samples. Our theory is validated on synthetic and real datasets, offering practical strategies for interpreting neural data under finite sampling constraints.
\end{abstract}

\section{Introduction}
Understanding how artificial neural networks relate to biological neural activity remains one of the central challenges in computational neuroscience \cite{carandini2005we, van2017primer, naselaris2011encoding}. As deep learning models become increasingly sophisticated at matching human-level performance on complex tasks, there is growing interest in whether these models actually learn representations that mirror those found in the brain \cite{yamins2014performance, khaligh2014deep, kell2018task, richards2019deep, lindsay2021convolutional}. However, a fundamental obstacle stands in the way of making this comparison: while artificial networks can be analyzed in their entirety, neuroscientists can only record from a small subset of neurons in any given brain region \cite{Cai2016,Walther2016,Schutt2023}. This sampling limitation poses a critical challenge for the field. When we measure the similarity between model and neural representations using standard techniques like Canonical Correlation Analysis (CCA) or Centered Kernel Alignment (CKA), how much does our limited neural sample size distort the true relationship? The stakes for answering this question are high - these similarity metrics are increasingly used to evaluate competing neural network architectures and training approaches based on their match to brain data. 


Our work provides the first rigorous theoretical framework for understanding how neuron sampling affects representational similarity measures. Our analysis reveals that measuring CCA and CKA with a limited number of recorded neurons systematically underestimates the true population-level similarity.
% as demonstrated in Fig.\ref{fig:sample_cka_w_pop_eig}. 
This underestimation stems primarily from eigenvector delocalization \cite{aggarwal2023mobilityedgelevymatrices, PhysRevE.50.1810, baik2004phasetransitionlargesteigenvalue}—a phenomenon where sample eigenvectors become increasingly misaligned with their population counterparts as the number of recorded neurons decreases. Understanding and accounting for this effect is crucial for accurate interpretation of neural representational similarities, particularly in experimental settings where only a subset of neurons can be recorded.

Our analysis proceeds in two parts. First, in the forward problem, we investigate how neuron sub-sampling from the full underlying population distorts the population eigencomponents and how this distortion affects the computed similarity measures. Second, in the backward problem, we ask whether observations from a finite number of neurons can be used to reliably infer the population representational similarity.

\subsection{Our Contributions}
\begin{itemize}
    \item \textbf{Eigencomponent-wise Analysis of Representation Similarity:} We show how neuron sub-sampling alters the eigenvalues and eigenvectors of the Gram matrix, leading to a systematic underestimation of CCA/CKA due to eigenvector delocalization.

    \item \textbf{Backward Inference via Denoising Eigenvectors:} We introduce a denoising method that leverages population eigenvalue priors (e.g., power law) to infer the true population similarity from limited data, substantially correcting the sampling bias.

    \item \textbf{Validation on Real Neural Data:} Applying our framework to primate visual cortex recordings confirms that even modest neuron counts can lead to severe underestimation of model–brain similarity and that our method effectively recovers the missing signal.
\end{itemize}


\subsection{Related Works}
Representation similarity measures expressed in terms of eigencomponents were presented in detail by \cite{kornblith2019similarity}, who showed that CCA, CKA, and linear regression scores can all be written in terms of the eigenvalues and eigenvectors of the Gram matrices. 

A key question is how these similarity measures behave under different kinds of noise. Broadly, there are two primary noise sources:
\begin{enumerate}
    \item \emph{Additive noise}, which arises from trial-to-trial variability and measurement error. In many studies, repeated trials and averaging can substantially mitigate this type of noise.
    \item \emph{Sampling noise}, which occurs because we can only record from a limited subset of neurons rather than the entire population. Consequently, the sample eigenvectors and eigenvalues differ from their population counterparts.
\end{enumerate}
In this work, we focus on the latter issue---sampling noise---since we assume trial averaging already reduces the additive noise to a manageable level.

One approach to address sampling noise is by studying the \emph{moments} of the Gram matrix \cite{kong2017spectrumestimationsamples, chun2024estimatingspectralmomentskernel}. While these methods provide a way to approximate the effect of sampling on the scalar values of certain similarity measures, they do not directly offer an interpretable description of what happens to the underlying eigencomponents. Recent work by \cite{Pospisil2024.02.16.580726} provides bounds on representation similarity measures when the number of sampled neurons is limited. However, these bounds are tight only under the assumption of a white Wishart model (i.e., all population eigenvalues are \(1\)). For more realistic data, where eigenvalues often decay according to a power-law, these bounds can become too loose to be practically informative.

Instead, we directly investigate how sampling noise affects both the eigenvalues and eigenvectors of the sample Gram matrix using random matrix theory \cite{Potters_Bouchaud_2020, bun2018overlaps, Bun_2017}. Extensive results exist for white Wishart matrices and low-rank ``spiked'' models, including the Baik--Ben Arous--P\'ech\'e (BBP) phase transition \cite{baik2004phasetransitionlargesteigenvalue}, which reveals that sample eigenvectors often serve as poor estimators of their population counterparts. These ideas have been extended to canonical correlation analysis (CCA) \cite{ma2022samplecanonicalcorrelationcoefficients, bykhovskaya2025highdimensionalcanonicalcorrelationanalysis}. However, the power-law-like spectra observed in neural data have not yet received comparable attention. Our work attempts to bridge this gap by studying sampling noise in representations with strongly decaying eigenvalues, which are ubiquitous in neural datasets.



\section{Notation \& Problem Setup}\label{sec:notation_problem_setup}

We use bold fonts for matrices and bracket notation for vectors. We use a tilde to denote quantities related to their population values. 

We consider two centered population activations $\tilde \X \in \bR^{P \times \tilde N_x}$ and $\tilde \Y \in \bR^{P \times \tilde N_y}$ with $\tilde N_x$ and $\tilde N_y$ neurons, recorded in response to a fixed set of stimuli of size $P$. Centered means that we subtracted column-wise mean. Their corresponding population Gram matrices are given by $\tilde\bSigma_x = \tilde\X \tilde\X^\top$ and $\tilde\bSigma_y = \tilde\Y \tilde\Y^\top$ with eigendecomposition:
\begin{align}
    \tilde\bSigma_x &= \sum_{i=1}^{P} \tilde\lambda_i \ketbra{\tilde u_i}{\tilde u_i}, \quad \tilde\bSigma_y = \sum_{a=1}^{P} \tilde\mu_a \ketbra{\tilde{w}_a}{\tilde w_a}.
\end{align}
The sample activations $\X \in \bR^{P \times N_x}$ and  $\Y \in \bR^{P \times N_y}$ are assumed to be generated from the population ones by a random projection $\X = \tilde\X \R$ where $\R \in \bR^{\tilde N \times N}$ is a random matrix with Gaussian i.i.d entries. Their Gram matrices are defined as $\bSigma_x = \X \X^\top$ and $\bSigma_y = \Y\Y^\top$ with eigendecomposition:
\begin{align}
    \bSigma_x &= \sum_{i=1}^{P} \lambda_i \ketbra{u_i}{u_i}, \quad \bSigma_y = \sum_{a=1}^{P} \mu_a \ketbra{w_a}{w_a}.
\end{align}
Random projections serve as an effective approach for sampling high-dimensional data due to their geometry-preserving properties \cite{lahiri2016random} and a popular method in analyzing neural dynamics from limited recordings \cite{gao2017theory}. It also reduces our problem when we consider sample Gram matrices as structured random Wishart matrices (see SI.\ref{sec:SI.A_sample_gram_matrix}).
% Specifically, the sample Gram matrix can be expressed as $\bSigma = \sqrt{\tilde\bSigma} \W \sqrt{\tilde\bSigma}$ where $\W$ is a Wishart matrix with aspect ratio $q = P/N$ (see SI.\ref{sec:SI.A_sample_gram_matrix}).

% Note that all our eigenvectors reside in $\mathbb{R}^P$.
We define the overlap as the squared inner product of two unit vectors. The overlap matrices are:
\begin{alignat}{2}
    Q^{x}_{ij} &:= \mathbb{E}[\braket{u_i|\tilde u_j}^2] \quad &&\text{(brain\_1 sample vs population)}\nonumber\\
    Q^{y}_{ab} &:= \mathbb{E}[\braket{w_a|\tilde w_b}^2] \quad &&\text{(brain\_2 sample vs population)}\nonumber\\
    \\
    M_{ia} &:= \mathbb{E}[\braket{u_i|w_a}^2] \quad &&\text{(brain\_1 vs brain\_2 sample)}\nonumber\\
    \tilde{M}_{ia} &:= \braket{\tilde u_i|\tilde w_a}^2 \quad &&\text{(brain\_1 vs brain\_2 population)}
    \label{eq:overlap_matrices}
\end{alignat}
where the expectations are over different instances of neuron samplings. Here, the matrices $\Q$ represent the self-overlap between sample and population eigenvectors, $\M$ represents the cross-overlap between two sample eigenvectors, and $\tilde\M$ between two population eigenvectors.

\subsection{Common Representational Similarity Measures}
Here, we review common representational similarity measures and show that these measures can be expressed in terms of the average quantities presented above.

\textbf{Canonical Correlation Analysis (CCA)} is an algorithm that sequentially finds a set of orthonormal vectors $\{\bv_\alpha\}$ for which the correlation coefficients $\rho_\alpha = \text{corr}(\X\bv_\alpha,\Y\bv_\alpha)$ for two matrices $\X, \Y$ are maximized \cite{hotelling1936relations}. The squared sum of these coefficients gives the CCA similarity $\text{CCA} = \sum_\alpha \rho_\alpha^2$, and can be expressed in terms of the overlap matrix $M_{ia}$ \cite{bjorck1973numerical,kornblith2019similarity}
\begin{align}\label{eq:cca_spectral}
    \text{CCA} &= \sum_{a=1}^{N_y}\frac{\braket{u_i | w_a}^2}{\min(N_x,N_y)} = \sum_{i=1}^{N_x} \sum_{a=1}^{N_y} \frac{M_{ia}}{\min(N_x,N_y)}.
\end{align}
CCA has emerged as a popular tool in deep learning to compare neural representations \cite{raghu2017svccasingularvectorcanonical}.

Canonical Correlation Analysis (CCA) is sensitive to perturbations when the condition number of $\mathbf{X}$ or $\mathbf{Y}$ is large \cite{golub1995canonical}. To enhance robustness, Singular Value CCA (SVCCA) performs CCA on truncated singular values of $\mathbf{X}$ and $\mathbf{Y}$. In this approach, the sum of the overlap matrix $\mathbf{M}$ is truncated to include only the first few components. To avoid confusion, from now on, we will refer to SVCCA truncated to the top ten components for both $\mathbf{X}$ and $\mathbf{Y}$ as CCA, i.e $\text{(SV)CCA} = \frac{1}{10}\sum_{i=1}^{10} \sum_{a=1}^{10} {M_{ia}}.$


\textbf{Centered Kernel Alignment (CKA)} is a summary statistic of whether two representations agree on the (dis)similarity between a pair of examples based on their dot products \cite{cristianini2001kernel}. CKA is defined as $\frac{\Tr \bSigma_x \bSigma_y}{\sqrt{\Tr\bSigma_x^2 \Tr\bSigma_y^2}}$ and essentially measures the angle between two Gram matrices. In terms of spectral components, it can be expressed as:
\begin{align}\label{eq:cka_spectral}
    \text{CKA} &= \sum_{i=1}^{P}\sum_{a=1}^{P}
    \frac{\lambda_i}{\sqrt{\sum_{j=1}^P \lambda_j^2}}
    \frac{\mu_a}{\sqrt{\sum_{b=1}^P \mu_b^2}}
    M_{ia}.
\end{align}
Note that CKA is very similar to CCA but with additional (normalized) eigenvalue terms. CKA will be the main focus of our work.

\textbf{Representational Similarity Analysis (RSA)} is a popular method in neuroscience used to compare different brain regions in response to the same set of stimuli \cite{kriegeskorte2008representational}. It is similar to CKA, except RSA compares pair-wise Euclidean distances instead of dot products. Recent work has established its equivalence to CKA when RSA is combined with an extra centering step \cite{williams2024equivalence}. Therefore, our analyses are directly applicable to (centered-)RSA.
    

\section{Theoretical Background}

Treating $\bSigma_x$ and $\bSigma_y$ as random matrices described in Sec.\ref{sec:notation_problem_setup}, we leverage results from random matrix theory \cite{Potters_Bouchaud_2020} to compute deterministic equivalents of average CCA and CKA in the asymptotic limit. Defining $q_x = P/N_x$ and $q_x = P/N_y$, we consider the limit $P, N_x, N_y \to \infty$ by keeping $q_x, q_y \sim \cO(1)$.

Both similarity measures depend on the cross-overlap between sample eigenvectors $M_{ia}$ defined in \eqref{eq:overlap_matrices}. Asymptotically $M_{ia}$ decouples as \cite{bun2018overlaps}
\begin{align}\label{eq:cross_overlap_formula}
    M_{ia} = \sum_{j, b} Q^x_{ij} \tilde M_{jb} Q^y_{ba},
\end{align}
where the self-overlaps $Q^x_{ij}$ and $Q^y_{ab}$ can be computed analytically \cite{ledoit2011eigenvectors}.  The self-overlap matrix for $\X$ can be expressed in terms of the resolvent matrix $\G(z) = (z-\bSigma)^{-1}$ given by:
\begin{align}\label{eq:self_overlap_formula}
    Q_{ij} = \text{const.} \lim_{\eta\to 0^+}\Im \G_{jj}(\lambda_i - i\eta),
\end{align}
where the resolvent $\G(z)$ has a deterministic equivalent given by the following self-consistent equation
\begin{align}\label{eq:subordination_eq}
    \G_{ij}(z) = \frac{\delta_{ij}}{z - \tilde\lambda_j(1+q(z\mathfrak{g}(z)-1))},\; \mathfrak{g}(z) = \frac{1}{P}\Tr\G(z).
\end{align}
We provide a detailed derivation of these results in SI.\ref{sec:SI.A}. Here, we note that the complex function $\mathfrak{g}(z)$ and \eqref{eq:self_overlap_formula} can be solved numerically (see SI.\ref{sec:SI_experimental_details} for details). Plugging in the formula for expected $M_{ia}$ in \eqref{eq:cca_spectral} and \eqref{eq:cka_spectral}, we get an analytical formula for CCA and CKA, respectively. Several remarks are in order:

-- While our theory is applicable to the general cases where observations from both models are sampled, henceforth, we fix one of the models to be deterministic for practical reasons. Often, neural similarity measures are applied to compare biological data with limited neuron recordings to an artificial model where the entire population is available. For example fixing model $\Y$ implies that its self-overlap $\Q^y$ is just an identity matrix, hence simplifying \eqref{eq:cross_overlap_formula} to $\M = \Q^{x} \tilde\M$.

-- The analytical formula for CCA and CKA depends only on the population quantities. However, since the self-overlap matrix $Q_{ij}$ in \eqref{eq:self_overlap_formula} explicitly depends on individual eigencomponents, its deterministic equivalent specifically depends on the population eigenvalue for the $j^\text{th}$ component ($\tilde\lambda_j$), and the expected value of the sample eigenvalue for the $i^\text{th}$ component ($\bE[\lambda_i]$).


\noindent \textbf{Sample Eigenvalues:}~ Theoretical values of sample eigenvalues $\bE[\lambda_i]$, in principle, can be computed by solving the following integral equation \cite{Potters_Bouchaud_2020}
\begin{align}\label{eq:sample_eigenvalue}
    \int_{\bE[\lambda_i]}^{\infty} \rho(\lambda) \, d\lambda = \frac{i}{P}, \;\; \rho(\lambda) = \frac{1}{\pi}\lim_{\eta\to 0^+} \Im \mathfrak{g}(\lambda - i\eta).
\end{align}
Here, $\rho(\lambda)$ is the deterministic equivalent of the empirical eigenvalue density and depends only on the population eigenvalues (see SI.\ref{sec:SI.A_eigenvalue_statistics}). This may be problematic due to numerical instabilities.

However, we know that each single-trial eigenvalue concentrates around this mean with trial-to-trial fluctuations of $\cO(1/\sqrt{P})$ \cite{Potters_Bouchaud_2020}. In large $P$ limit, we can neglect these fluctuations and replace $\bE[\lambda_i]$ with a single-trial observation. We provide a detailed account of this approximation with supporting numerical experiments in SI.\ref{sec:SI.A_sample_eigenvalue_statistics}.


\noindent \textbf{Sample Eigenvectors:}~ Unlike eigenvalues, the sample eigenvectors $\braket{u_i|\tilde u_j}^2$ exhibit trial-to-trial fluctuations that does not vanish even as $P \to \infty$ (see SI.\ref{sec:SI.A_sample_eigenvector_statistics}). Instead, we need to compute the mean value of the overlap represented by the squared overlap $Q_{ij}$ in \eqref{eq:overlap_matrices} \cite{bun2018overlaps}.


\noindent \textbf{BBP Phase Transition:}~ Despite the inevitable fluctuation in the sample eigenvectors, their mean behavior can still differ markedly from that of the population eigenvectors.  
A classic example is the Baik--Ben Arous--P\'ech\'e (BBP) phase transition \cite{baik2004phasetransitionlargesteigenvalue}.  
Consider a population Gram matrix with one large ``spike'' eigenvalue and the rest equal to 1.  
If the spike strength exceeds a critical threshold determined by \(P/N\),  
then the sample eigenvector associated with that spike has an overlap on the order of \(O_P(1)\) with the population spike eigenvector.  
If the spike strength is below that threshold, however, the corresponding sample eigenvector becomes delocalized,  
and its overlap with the spike is of order \(O\bigl(1/P\bigr)\). We provide a detailed discussion of this in SI.\ref{sec:SI.A_sample_eigenvector_statistics}).
\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{figs/bbp_plot.pdf}
    \caption{Illustration of BBP phase transition. On the left is the case where the spike is bigger than the critical value, and thus, the sample eigenvector related to the spike is close to the actual spike, lying at the cone with a small angle. On the right is the case where the spike is smaller than the critical value; in this case, sample eigenvector related to the spike mixes with bulk eigenvectors, ending up completely delocalized.}
    \label{fig:evec_deloc}
\end{figure}

\noindent \textbf{Numerical Confirmation:}~ Finally, we numerically test the theoretical prediction for self-overlap given by~\eqref{eq:self_overlap_formula} on the eigenvectors of deep neural network activations. We extract layer activations from a pre-trained ResNet18 on CIFAR-10 images and subsample $N$ neurons through random projection. In Fig.~\ref{fig:overlap_and_cka_resnet18}a, we show the self-overlap $Q_{ii}$ for the first few eigenvectors of the layer activations and demonstrate a perfect match with theory. As the number of neurons decreases, the number of delocalized eigenvectors increases since fewer eigenvectors have self-overlap $Q_{ii} \approx 1$.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/overlap_and_cka_resnet18.pdf}
    \caption{\textbf{a)} Self-overlap $Q_{ii}$ between sample and population eigenvectors for ResNet18 activations. \textbf{b)} CKA between population and sample activations when $N$ neurons are sampled. The gray-shaded region represents the standard deviation of empirical CKA across different random samplings.}
    \label{fig:overlap_and_cka_resnet18}
\end{figure}

The effect of eigenvector delocalization is reflected in the CKA between the sampled and population layer activations as shown in Fig.~\ref{fig:overlap_and_cka_resnet18}b. The alignment is completely misleading when small amounts of neurons are sampled and pose a significant problem for practical purposes.

\section{Applying Theory to Representation Similarity}

\subsection{Forward Problem: Impact of Neuron Sampling on Similarity}
In the forward problem, we assume that the population eigenvalues and eigenvectors are known. The first step is to obtain the typical sample eigenvalues by running a single-trial numerical simulation. We then move on to the eigenvectors by computing \(\mathbf{Q}^{(x)}\) using~\eqref{eq:self_overlap_formula}. Finally, we calculate the overlap between the two systems, \(\mathbf{M}\), using~\eqref{eq:cross_overlap_formula}. Having these components allows us to evaluate both CCA and CKA as functions of the number of neurons \(N\).

As illustrated in Fig.~\ref{fig:cka_plot_true1}, the theoretical predictions obtained from this eigen-decomposition match the observed CCA and CKA across different values of \(N\). Notice that CKA decreases when the number of neurons \(N\) is reduced. As discussed above, both of these effects can be explained by the delocalization of eigenvectors.

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figs/cka_plot_true1.pdf}
    \end{minipage}\hfill%
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figs/cca_plot_true1.pdf}
    \end{minipage}
     \caption{\textbf{Comparison of sample vs population measures for CKA and CCA:} Error bars represent empirical sample similarity and dotted lines the theoretical predictions. The black dotted line marks the true population similarity which is set to 1 for both measures. Solid lines indicate inferred true similarity from samples. Sample similarity is lower due to eigenvector delocalization, while our method consistently provides a closer estimate of the true value.
     % Error bars are empirical sample representation similarity, and dotted ones are theoretical predictions of it. Black horizontal dotted line is population representation similarity, in this case we set both CKA and CCA to be $1$. Solid lines are inferred true similarity only from sample observation. Note that sample representation similarity is smaller than population one due to eigenvector delocalization, and our inference method consistently gives a bigger estimate of population similarity, closer to the actual value, which is $1$.
     % Left: CKA as a function of $N$, showing how the theoretical predictions align with empirical samples and the population value. Right: Similar analysis for CCA, demonstrating parallel behavior in both similarity metrics.
     }
     \label{fig:cka_plot_true1}
\end{figure*}

\subsection{Backward Problem: Inferring Population Similarity from Limited Neurons}
Just like in our earlier analysis, inferring the population representational similarity begins with estimating the eigenvalues of the underlying population. In general, this is difficult because sample eigenvalues can deviate substantially from their population counterparts. Moreover, if \(N < P\), there are \(P - N\) zero eigenvalues in the sample covariance matrix, further complicating the problem.

However, if we adopt a parametric form, we can often achieve significant improvements in accuracy~\cite{Pospisil2024.02.16.580726}. Here, we assume a power-law spectrum of the form \(\tilde{\lambda}_i = i^{-1-\delta}\)~\cite{stringer2019high}. We develop a numerical method based on random matrix theory that reliably infers the true decay rate of population eigenvalues based on only the sample eigenvalues (see SI.\ref{sec:SI_power_law_theory} for detailed analysis). More sophisticated approaches---such as allowing a broken power law and minimizing the error using unbiased moment estimators---are also possible~\cite{Pospisil2024.02.16.580726}.

After estimating the population eigenvalues $\tilde\lambda_i$, we address the eigenvectors by computing \(\mathbf{Q}^{(x)}\) using \eqref{eq:self_overlap_formula}. Since every population eigenbasis produces the same mean self-overlap, having \(\tilde{\lambda}_i\) is sufficient to find \(\mathbf{Q}^{(x)}\). 

Our final goal is to estimate the population cross-overlaps \(\tilde{\mathbf{M}}\), which are required to infer the true population similarity between two systems. A straightforward way to do this is to invert the forward relationship \(\mathbf{M} = \mathbf{Q}^{(x)}\,\mathbf{\tilde{M}}\). Two challenges arise in this naive approach. First, eigenvector statistics do not self-average~\cite{Potters_Bouchaud_2020}, so the empirical cross-overlap differs from \(\mathbf{M}\), its expected value. This discrepancy can be partially mitigated by trial averaging or statistical bootstrapping. 

Even if we obtain the mean cross-overlap, a second issue emerges: \(\mathbf{Q}^{(x)}\) is not invertible unless \(P \ll N\). As a result, it is impossible to recover the entire matrix \(\tilde{\mathbf{M}}\). Intuitively, only the first few eigenvectors are well-localized; the rest delocalize and lose information, so we can only reliably retrieve the corresponding columns of \(\tilde{\mathbf{M}}\).

To handle this practically, we use a constrained optimization:
\begin{align}
    \min_{\mathbf{\tilde{M}}} \bigl\| \mathbf{M} - \mathbf{Q}^{(x)}\,\mathbf{\tilde{M}} \bigr\|_F
\end{align}

where each element of \(\mathbf{\tilde{M}}\) is restricted to lie in \([0,1]\), as these entries represent squared inner products of eigenvectors. We provide the pseudo-code for this denoising algorithm in Alg.\ref{alg:infer_population_similarity}

\begin{algorithm}[h]
\caption{Inference of Population Cross-Overlap $\tilde{\mathbf{M}}$}
\label{alg:infer_population_similarity}
\begin{algorithmic}[1]
\REQUIRE $\{\lambda_i\}_{i=1}^P$: Sample eigenvalues \\
\hspace*{2.5em} $P$: Number of stimuli \\
\hspace*{2.5em} $N$: Number of sampled neurons \\
\hspace*{2.5em} $\mathbf{M} \in \mathbb{R}^{P \times P}$: sample cross-overlap matrix
\vspace{0.5em}
\STATE \textbf{Step 1: Estimate Population Eigenvalues}
    \STATE \hspace{2em} Assume power-law ansatz: $\tilde{\lambda}_i \propto i^{-1 - \delta}$
    \STATE \hspace{2em} Find $\delta$ that best explains observed $\{\lambda_i\}_{i=1}^P$
\vspace{0.5em}
\STATE \textbf{Step 2: Compute Self-overlap matrix}
    \STATE \hspace{2em} $\mathbf{Q}^{(x)} \leftarrow function(\{\tilde{\lambda}_i\}, P, N)$ 
\vspace{0.5em}
\STATE \textbf{Step 3: Optimize Population Similarity}
    \STATE \hspace{2em} Solve constrained optimization problem:
    \STATE \hspace{2em} $\min_{\tilde{\mathbf{M}}} \|\mathbf{M} - \mathbf{Q}^{(x)} \cdot \tilde{\mathbf{M}}\|_F$
    \STATE \hspace{2em} subject to $\tilde{M}_{ij} \in [0,1]$ for $\forall i,j$
\vspace{0.5em}
\RETURN $\tilde{\mathbf{M}}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Up to How Many Eigenvectors Can We Resolve for Given \(N,P\)?}
Consider a power-law spectrum, which decays relatively quickly. Under such a spectrum, only the leading sample eigenvectors tend to be well-localized, as shown in Fig.~\ref{fig:effective_dimension_varying_P_varying_N}. If we run the backward algorithm, we observe that for a given \(N,P\), we can reliably recover only those initial components that remain localized.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/effective_dimension_varying_P_varying_N.pdf}
    \caption{Participation ratio (P.R.) of self-overlap ($1/\sum_j Q_{ij}^2$), indicating the onset of eigenvector delocalization, for a power-law spectrum $\tilde{\lambda}_i \sim i^{-1.2}$. For fixed \(N\), increasing \(P\) marginally affects the leading eigenvectors. By contrast, for fixed \(P\), increasing \(N\) makes more eigenvectors localized. Only sample eigenvectors below the black horizontal line are localized (P.R. $\approx 1$). Heuristically, $\tilde{M}_{ia}$ can be recovered reliably for only indices below this line.}
    \label{fig:effective_dimension_varying_P_varying_N}
\end{figure}

We can explicitly truncate these eigenvectors by taking a partial inverse of \(\mathbf{Q}^{(x)}\) (see SI.~\ref{sec:details of backward}). However, this approach can be numerically unstable and might produce values of \(M_{ij}\) outside the \([0,1]\) range. 

Additionally, Fig.~\ref{figs/M_tilde_M_comparison.pdf} demonstrates that, under a power-law of the same exponent, varying \(P\) has a subtler effect on these leading indices than varying \(N\), which significantly affects localization.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/effective_dimension_varying_P_varying_N.pdf}
%     \caption{Effective dimension into which the \(i\)-th sample eigenvector delocalizes, given by $ 1/\sum_jQ_{ij}^2$, for a power-law spectrum $\tilde{\lambda}_i \sim i^{-1.2}$. For a fixed \(N\), increasing \(P\) only slightly affects the leading eigenvectors. By contrast, for fixed \(P\), increasing \(N\) makes the sample eigenvectors more localized, bringing them closer to the population eigenvectors. We plot $y=2$ for convenience. Qualitatively, before this horizontal line, sample eigenvectors are localized (effective dimension $\approx 1$), while after it they rapidly delocalize. Heuristically, for a given \(N\), we can recover $\tilde{M}_{ia}$ reliably for indices below this line.}
%     \label{fig:effective_dimension_varying_P_varying_N}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/M_tilde_M_comparison.pdf}
    \caption{\textbf{Recovering population overlaps:} Each column shows the single-trial empirical $\mathbf{M}$, the theoretical prediction of $\mathbf{M}$, the inferred population overlap $\tilde{\hat{\mathbf{M}}}$, and the actual population overlap $\tilde{\mathbf{M}}$.With fewer neurons $N$, sample eigenvectors become delocalized, causing large discrepancies. Nevertheless, our inference method successfully recovers the dominant overlaps, which are enough for global similarity measures such as CKA and CCA.}\label{figs/M_tilde_M_comparison.pdf}
\end{figure}

\subsubsection{Why This Is Sufficient for Inferring Population Similarity}
Although our denoising approach only manages to recover the leading few eigencomponents (those that remain localized), it is precisely these components that matter most for similarity measures like CKA and (SV)CCA. As shown in Fig.~\ref{fig:cka_plot_true1}, these metrics are governed primarily by the initial eigenvalues and eigenvectors. Thus, even with a very limited number of neurons, estimating those leading components is sufficient for practical purposes.

To estimate population CKA from sample observations, one could focus solely on the denominator, as the sample numerator’s expected value matches the population value~\cite{10.1007/11564089_7}. This allows applying methods from~\cite{kong2017spectrumestimationsamples, chun2024estimatingspectralmomentskernel}, exploiting the fact that the changes in eigenvalues and eigenvectors offset each other. However, this approach estimates CKA using only eigenvalues, ignoring eigenvector statistics. In contrast, our method incorporates both inferred eigenvalues and eigenvectors for a more complete estimation.

% Finally, note that to estimate the population CKA from sample observations, one could focus solely on the denominator since the expected value of the sample numerator already matches the population value~\cite{10.1007/11564089_7}. This yields a direct application of the methods in~\cite{kong2017spectrumestimationsamples, chun2024estimatingspectralmomentskernel}, exploiting the fact that the changes in eigenvalues and eigenvectors offset each other, leaving the numerator unchanged on average. However, that approach does not provide estimates for the population eigenvector statistics; it only requires eigenvalues to estimate CKA. Our method, in contrast, expresses the estimated CKA in terms of both the inferred population eigenvalues and eigenvectors, thereby offering a more complete estimation framework.

% (commented-out material preserved)

\section{Experiments}

\subsection{Synthetic Data with a Known Population Gram Matrix}
We first evaluate our approach on a synthetic dataset where the population Gram matrix is fully specified, allowing us to directly compare our estimated similarity measures against the ground-truth population values. For simplicity, we set the two population Gram matrices to be identical, i.e., \(\tilde\bSigma_x = \tilde\bSigma_y\). Under this setup, the population CKA and CCA should both be \(1\).

Fig.~\ref{fig:cka_plot_true1} illustrates that our forward and backward procedures work well. In the forward approach, we show that the eigencomponent-based analysis matches the empirical results closely. In the backward approach, even with an extremely limited number of neurons (\(N\approx 20\)), our method infers a population similarity close to \(1\), despite the observed sample similarity being substantially lower.

Since the population eigenvectors (and hence the population cross-overlaps \(\tilde{M}\)) are known, we can also verify how well the inferred overlaps \(\hat{\tilde{M}}\) match the true overlaps \(\tilde{M}\). Specifically, Fig.~\ref{figs/M_tilde_M_comparison.pdf} displays the top-left \(10 \times 10\) block of each matrix: the empirical \(\mathbf{M}\), the theoretical \(\mathbf{M}\) (second column), the inferred population overlap \(\hat{\tilde{\mathbf{M}}}\) (third column), and the actual population overlap \(\tilde{\mathbf{M}}\) (fourth column). Because we set \(\tilde\bSigma_x = \tilde\bSigma_y\), the actual population cross-overlap \(\tilde{\mathbf{M}}\) should be the identity matrix. However, with fewer neurons, the sample eigenvectors become more delocalized, as evident in the first column. Our theoretical prediction of this phenomenon (second column) aligns closely with the empirical observation. Notably, even with severely limited neurons, our backward-inference method recovers a cross-overlap matrix \(\hat{\tilde{\mathbf{M}}}\) (third column) much closer to the true identity than the naive observed \(\mathbf{M}\).

\subsubsection{Sampling Neurons Can Change Representation Similarity Ranking}
Next, we showcase a synthetic example in which \emph{sampling} can lead to a reversal in the similarity rankings of models. Specifically, we construct two models:
\begin{itemize}
    \item \textbf{Model 1} has significant overlap with the ``Brain'' representation on its first 3 population eigenvectors.
    \item \textbf{Model 2} has significant overlap with the Brain on the next 3 eigenvectors.
\end{itemize}
We set the total population (SV)CCA of Model 2 to be higher than that of Model 1. However, as neurons are sampled, eigenvectors corresponding to larger indices (smaller eigenvalues) tend to delocalize more. Hence, the empirical cross-overlap \(\mathbf{M}\) for Model 2 deteriorates faster, causing its (SV)CCA to drop more than that of Model 1. Eventually, Model 1 overtakes Model 2 in the sample-based (SV)CCA ranking, as illustrated in Fig.~\ref{fig:cca_flip}.

Fig.~\ref{fig:cca_flip_M} presents the empirical and population cross-overlaps of the two models (each compared to the Brain). We set \(P=200\) and \(N=30\), and all population eigenvalues follow a power-law with exponent \(-1.2\). Model 2’s higher-dimensional overlaps delocalize more strongly, producing an apparent discrepancy that flips their observed ranking once neuron sampling is taken into account.

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{figs/v2_cka_scatter_plot_N20.pdf}
    \end{minipage}\hfill%
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{figs/v2_cca_scatter_plot_N20.pdf}
    \end{minipage}
    \caption{Scatter plots of observed sample similarity vs.\ inferred population similarity for multiple models compared to V2 cortex, using only \(N=20\) neurons (out of a larger set). (\textbf{Left}) CKA results; (\textbf{Right}) CCA results. The dotted line \(y=x\) indicates equality. Notice that the inferred population similarity is consistently higher than the naive sample-based measure, demonstrating how limited neuron sampling can lead to underestimation of the true model-brain correspondence.}
    \label{figs:cca_cka_scatter}
\end{figure*}

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\linewidth]{figs/cca_flip.pdf}
    \caption{Sample-based CCA ranking flips despite Model 2 having a larger \emph{population} CCA than Model 1. The decrease in Model 2’s CCA is more pronounced due to its stronger reliance on higher-indexed eigenvectors, which become more delocalized with limited neuron sampling.}
    \label{fig:cca_flip}
\end{figure}

\begin{figure}[h]
    \centering\includegraphics[width=.8\linewidth]{figs/cca_flip_M.pdf}
    \caption{Empirical vs. population cross-overlaps for Model 1 vs.\ Brain and Model 2 vs.\ Brain. Here, \(P=200\) and \(N=30\). All three population eigenvalue spectra follow a power law with exponent \(-1.2\). Although Model 2’s true overlap is higher at the population level, it relies on higher-indexed (smaller eigenvalue) components, which delocalize more severely in the sample.}
    \label{fig:cca_flip_M}
\end{figure}





\subsection{Brain Data}
Finally, we apply our denoising framework to real neural recordings in the primate visual cortex, comparing them against various computational model predictions. (for experimental details see SI.\ref{sec:SI_experimental_details})

In Fig.~\ref{figs:cca_cka_scatter}, we illustrate a scatter plot of the representation similarity for different models compared to neural responses from V2 cortex \cite{Freeman2013, schrimpf2018brain}, given an artificially limited neuron count of \(N=20\) out of $103$ neurons. The \(x\)-axis corresponds to the observed \emph{sample} CKA or CCA, while the \(y\)-axis is our \emph{inferred population} measure. Observe that our inference method consistently produces higher population similarity estimates than the naive sample estimates. In particular, certain models that appear to have lower similarity (when judged by the raw, sample-based metric) can actually exhibit higher \emph{true} similarity to the brain once sampling effects are taken into account.

\section{Conclusion and Outlook}
We have presented an eigencomponent-based analysis of how sampling a finite number of neurons affects representational similarity measures, including CCA and CKA. By applying methods from Random Matrix Theory, we established that this limited sampling systematically underestimates similarity because of eigenvector delocalization in the sample Gram matrices. Our framework provides:
\begin{itemize}
    \item \textbf{Forward Analysis:} A procedure to predict how population eigenvalues and eigenvectors will manifest under neuron sampling, thus explaining the observed drop in representation similarity.
    \item \textbf{Backward Inference:} A denoising algorithm capable of inferring the \emph{population} representation similarity from limited data, overcoming the biases introduced by sampling noise.
\end{itemize}

We validated our approach on both synthetic and real datasets. In the synthetic experiments, where the population Gram matrices were fully known, we showed that our method reliably recovers the true population overlaps and similarity values, even in regimes with very few neurons. Importantly, we highlighted a striking effect of sampling: under certain configurations, the ranking of two models with respect to the brain can be inverted when only a limited set of neurons is recorded. In real datasets from primate visual cortex, our method consistently produced higher \emph{population} similarity estimates than naive sample-based methods, underscoring that the observed decrease in similarity is largely a sampling artifact.

\paragraph{Future Directions.}
There are several promising avenues for extending our work. First, it would be valuable to explore more sophisticated spectral priors—such as broken power-law spectra—to account for multiple functional subpopulations in the data, each contributing a distinct spectral structure. Second, while we have focused on sampling noise, future work should incorporate explicit models of additive noise that arises in real-time neurophysiological recordings, relaxing the assumption that trial averaging eliminates most of it. Third, improved denoising methods could be developed by adopting Bayesian approaches to model the joint distribution of sample eigenvectors and population eigenvectors \cite{Monasson_2015}, thus allowing more accurate recovery of the population eigenspaces. Finally, as we outline in SI.\ref{sec: appendix regression}, our framework naturally extends to regression settings, where sampling-induced distortions in eigencomponents can adversely affect regression scores, much like their impact on representational similarity measures.


Overall, our results suggest that practical neuroscience studies must account for sampling-induced eigenvector delocalization when interpreting representational similarity. By unveiling the intrinsic biases introduced by limited neuron sampling and proposing a systematic solution, we aim to provide neuroscientists and machine learning researchers with more reliable tools for comparing computational models and neural data.


\section{Impact Statement}
Our work may be highly impactful in providing reliable, robust similarity measures and check the reliability of existing studies based on neural recordings. Furthermore our method may help comparing similarities of artificial networks and may potentially be used in distilling large models.



\bibliography{bibliography}
\bibliographystyle{icml2024/icml2024}

\input{appendix}

\end{document}



% Measuring the representational similarity between neural recordings and computational models is challenging due to the limited number of neurons that can be recorded simultaneously. In this study, we examine how this limitation impacts representational similarity measures, specifically Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA). By decomposing these measures into their eigen-components and utilizing tools from Random Matrix Theory, we demonstrate that sampling a finite number of neurons leads to an underestimation of similarity measures because of the delocalization of eigenvectors. We provide an eigen-component-wise analysis of how neuron sampling affects CCA and CKA. Additionally, we introduce a systematic method to denoise eigenvectors, allowing us to infer the population representation similarity (as if an infinite number of neurons were available) even when only a small sample of neurons is used. Our findings offer practical guidelines for interpreting representational similarity in neural data with limited sampling.




% In this section, we explore the relationship between sample eigenvalues and eigenvectors and their population counterparts. For both the forward and backward problems, addressing eigenvalues is the first step. Details are in appendix.

% \subsection{Sample Eigenvalues}
% Given the population eigenvalues $\{\tilde{\lambda}_i\}$ and the aspect ratio $\alpha = \frac{P}{N}$, we begin by calculating the resolvent of the sample Gram matrix using the Marčenko-Pastur equation \cite{Bun_2017}. This yields the sample eigenvalue distribution $\rho_{\Sigma}(\lambda)$. In the high-dimensional limit, the typical $i$-th sample eigenvalue $\lambda_i$ satisfies:
% \begin{align}
%     \int_{\lambda_i}^{\infty} \rho_{\Sigma}(\lambda) \, d\lambda = \frac{i}{P}.
%     \label{eq:sample_eigenvalue}
% \end{align}
% In high-dimensional limits, eigenvalue fluctuations around their typical values are known to be small \cite{Potters_Bouchaud_2020}. As a result, when taking the expectation over eigenvalues and eigenvectors, we treat the eigenvalues as deterministic. The resulting $i$-th sample eigenvalue will then be used in the subsequent equations involving eigenvectors.

% \subsection{Sample Eigenvectors}
% The overlap between the $i$-th sample and $j$-th population eigenvectors is given by the eigenvector overlap formula \cite{Bun_2017}:
% % \begin{align}
% % &PQ_{ij} = P\,\mathbb{E}\left[ |\braket{u_i | \tilde{u}_j}|^2 \right] \\&= 
% % \frac{ \alpha \lambda_i \tilde{\lambda}_j }{ \left[ \tilde{\lambda}_j (1 - \alpha) - \lambda_i + \alpha \lambda_i \tilde{\lambda}_j h_{\Sigma}(\lambda_i) \right]^2 + \left[ \alpha \lambda_i \tilde{\lambda}_j \pi \rho_{\Sigma}(\lambda_i) \right]^2 }, \label{eq:overlap}
% % \end{align}
% where $h_{\Sigma}(\lambda)$ is phi times Hilbert transform of the $\rho_{\Sigma}(\lambda)$. This formula allows us to compute the matrix $\mathbf{Q}$. Note that Equation \ref{eq:overlap} involves both $\lambda_i$ and $\tilde{\lambda}_j$.

% This formula is for bulk eigenvectors, in the case of outliers(meaning $\rho_{\Sigma}(\lambda)=0$, we need different approach, as is shown in \cite{BAIK20061382}

% \subsection{Analytic Formula for $\mathbb{E}[\mathbf{M}]$}
% Given that we know the average squared component of a sample eigenvector in the entire population eigenbasis (matrix $\mathbf{Q}$), we can express the sample eigenvectors of the brain and model using the following ansatz.
% \begin{align}
%     \ket{u_i} &= \sum_{j=1}^{P} \epsilon^{(1)}_{ij} \sqrt{Q^{(1)}_{ij}} \ket{\tilde{u}_j}, \\
%     \ket{w_a} &= \sum_{b=1}^{P} \epsilon^{(2)}_{ab} \sqrt{Q^{(2)}_{ab}} \ket{\tilde{w}_b},
%     \label{eq: noise independent}
% \end{align}
% where $\epsilon^{(1)}_{ij}$ and $\epsilon^{(2)}_{ab}$ are standard normal variables. We assume each $\epsilon^{(1)}_{ij}, \epsilon^{(2)}_{ab}$ are independent, by using ergodic hypothesis\cite{Deutsch:1991msp, bun2018overlaps}. This can be theoretically shown in dyson brownian motion framework, and numerically be shown by off-diagonal element of sample resolvent in population eigenbasis is close to $0$. 

% Then the mean squared overlap between the brain and model eigenvectors can be calculated using a simple convolution formula \cite{bun2018overlaps, Bun_2017}:
% \begin{align}
%     \mathbb{E} \left[ M_{ia} \right] &= \sum_{j=1}^{P} \sum_{b=1}^{P} Q^{(1)}_{ij} \tilde{M}_{jb} Q^{(2)}_{ab}.
%     \label{eq:expected M}
% \end{align}
% In matrix notation, this is expressed as:
% \begin{align}
%     \mathbb{E}[ \mathbf{M} ] = \mathbf{Q}^{(1)} \tilde{\mathbf{M}} \left( \mathbf{Q}^{(2)} \right)^\top. \label{eq:double_disorder}
% \end{align}

% If we have access to all neurons in the model (i.e., $P/N_y \to 0$), then $\mathbf{Q}^{(2)} = \mathbf{I}_{P \times P}$, simplifying the formula to:
% \begin{align}
%     \mathbb{E}[ \mathbf{M} ] = \mathbf{Q}^{(1)} \tilde{\mathbf{M}}. \label{eq:single_disorder}
% \end{align}






% \section{Eigenvector statistics for realistic spectrum}
% \label{sec:power_law_spectrum}

% From here on we focus on data with power-law behavior. Power-law spectra have been observed in a variety of neural data~\cite{stringer2019high} and in deep learning models \cite{kaplan2020scaling, bahri2024explaining}. 
% We consider the situation where the population eigenvalues follow a power law, \(\tilde{\lambda}_i \propto i^{-1-\delta}\). 
% Our analysis shows that, as the eigenvalue index \(i\) increases, the corresponding sample eigenvector becomes progressively delocalized in the basis of the population eigenvectors.  

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figs/overlap_single_P_200_N1_100_N2_400_a_0.1.pdf}
%     \caption{(\textbf{Left}) A $10 \times 10$ submatrix of $\mathbf{Q}$ for the power-law spectrum $\tilde{\lambda}_i \sim i^{-1.2}$, 
%     illustrating how higher-index sample eigenvectors spread out in the population eigenbasis. 
%     (\textbf{Right}) The effective dimension of each column of $\mathbf{Q}$, defined by the inverse participation ratio 
%     $\displaystyle 1 \bigl/ \sum_{j} Q_{ij}^2$, which indicates how many population eigenvectors each sample eigenvector ``delocalizes'' into.}
%     \label{figs:typcal_Q}
% \end{figure}


% \subsection{For Power-Law Eigenspectrum, Eigenvector Delocalization Drives the Drop in CKA}
% \label{subsec:cka_drop_powerlaw}

% Recall from \eqref{eq:cka_spectral} that the Centered Kernel Alignment (CKA) involves a weighted sum of the (normalized) eigenvalues and the corresponding eigenvector overlaps, denoted \(M_{ia}\).  
% When the population eigenvalues follow a power law, \(\tilde{\lambda}_i \sim i^{-1-\delta}\), we show in the Appendix that the sample eigenvalues \(\lambda_i\) also follow a similar decay at leading order up to rank. 
% In this regime, only the first few eigencomponents make a dominant contribution to CKA, as the initial terms of a power-law series tend to dominate the sum.  
% Surprisingly, due to the normalization factor, these first few sample eigenvalues turn out to be very close to their population counterparts.  
% Hence, changes in CKA with respect to \(N\) are primarily driven by shifts in the eigenvector overlaps, \(M_{ia}\), rather than changes in the eigenvalues themselves. This is shown in Fig.\ref{fig:overlap_and_cka_resnet18}, where we compare naive sample CKA with CKA with population eigenvalues and sample eigenvectors.

% % \subsection{A Model with Higher Population Similarity to the Brain Can Have Lower Sample Similarity}
% % \label{subsec:cka_flip_example}

% % Because eigenvector delocalization predominantly causes the drop in CKA under a power-law spectrum, we can anticipate when that drop will be most pronounced.  
% % Consider two models, Model~1 and Model~2, which have different overlaps with the brain's population eigenvectors.  
% % Model~1 aligns more strongly with the higher-index eigenvectors of the brain, whereas Model~2 aligns more strongly with the lower-index eigenvectors.  
% % As \(N\) decreases, the higher-index sample eigenvectors (those associated with smaller eigenvalues) delocalize more quickly, leading to a sharper decline in the CKA for Model~1.  
% % Thus, even if Model~1 originally exhibits a higher \emph{population-level} CKA with the brain, its \emph{empirical} (sample-level) CKA may actually become smaller than that of Model~2.  
% % This phenomenon is illustrated in Fig.~\ref{fig:cka_flip}.

% % \begin{figure}[htbp]
% %     \centering
% %     \includegraphics[width=0.9\linewidth]{figs/cka_flip_tildeM.pdf}
% %     \caption{Population overlap structure of model 1 and model 2 with brain}
% %     \label{fig:enter-label}
% % \end{figure}
% % \begin{figure}[htbp]
% %     \centering
% %     \includegraphics[width=0.9\linewidth]{figs/cka_flip_M_N20.pdf}
% %     \caption{Sample overlap structure of model 1 and model 2 with brain when $N=20$}
% %     \label{fig:enter-label}
% % \end{figure}

% % \begin{figure}[htbp]
% %     \centering
% %     \includegraphics[width=0.9\linewidth]{figs/cka_flip.pdf}
% %     \caption{A toy example demonstrating that Model~1 (with higher population-level CKA to the brain) can exhibit a lower sample-level CKA than Model~2 once sample eigenvectors delocalize.}
% %     \label{fig:cka_flip}
% % \end{figure}



% \subsection{For Power-Law Eigenspectrum, It Is \texorpdfstring{$N$}{N}, Not \texorpdfstring{$P/N$}{P/N}, That Governs Eigenvector Delocalization}
% \label{subsec:eigenvector_deloc_powerlaw}

% Suppose \(\tilde{\lambda}_i \sim i^{-1.2}\) for \(i = 1,\dots,P\).  
% Since only the first few eigenvectors ultimately determine CCA and CKA, we focus on these leading components and track their overlap, \(Q_{ii}\), where \(Q_{ii}\) measures how much the \(i\)-th sample eigenvector aligns with the \(i\)-th population eigenvector.  
% Fig.~\ref{fig:effective_dimension_varying_P_varying_N} shows how \(Q_{ii}\) changes for different values of \(N\) and \(P\).  
% For a fixed \(P\), as \(N\) increases, the sample eigenvectors become increasingly similar to the population eigenvectors, leading to higher \(Q_{ii}\).  
% Conversely, holding \(N\) fixed and varying \(P\) produces a subtler effect, at least for these first few dominant terms.  
% In other words, for power-law spectra, it is primarily \(N\)—rather than the ratio \(P/N\)—that determines how sharply the eigenvectors delocalize.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/effective_dimension_varying_P_varying_N.pdf}
%     \caption{Effective dimension that $i$-th sample eignvector delocalizes into,  $\sum_j1/Q_{ij}^2$ for a power-law spectrum with \(\tilde{\lambda}_i \sim i^{-1.2}\).  
%     For a fixed \(N\), varying \(P\) has only a minor effect on the leading indices.  In contrast, for a fixed \(P\), increasing \(N\) makes the sample eigenvectors more localized and closer to the population eigenvectors. We plot $y=2$ for convenience. Note that qualitatively, before this line sample eignevector was localized(effective dimenison $\approx 1$, but after this line sample eigenvector quickly delocalizes. Heuristically, we can define for given $N$, we can recover recover cross overlap $\tilde{M_{ia}}$ reliably up to sample indicies below this horizontal line.}
%     \label{fig:effective_dimension_varying_P_varying_N}
% \end{figure}





% \subsection{For power-law eigenspectrum, eignevector delocalization drives drop of CKA}
% In \eqref{eq:cka_spectral}, CKA was weights some of normalized eigenvalue with overlap $M_{ia}$. For population eigenvalue follwing $\tilde{\lambda_i}\sim i^{-1-\delta}$, sample eigenvalue also follows $\lambda_i \sim i^{-1-\delta}$ at leading order, as we show in appendix. In this case, only first few eigencomponents matters for CKA since in power law case first few term dominates the sum. Surprisngly, due to normalization factor, first few terms of sample eigenvalues are very close to population one. Thus change of CKA w.r.t. $N$ is mainly driven by change of $M_{ia}$ terms.


% \subsection{Model with higher population similarity with brain could have lower sample similarity}
% Since in power-law spectrum eigenvector delocalization is mainly driving drop of CKA, we can predict when will this drop be more abrupt. Consider the case where model 1 has more overlap at bigger indicies of brain's population eigenvector. Model 2 ahs more overlap with smaller indicies of brain's population eignevector. Then, as we decrease $N$, CKA of Model 1 and brain will drop more abrupt than Model 2, since brain's sample eigenvector with bigger indices will become delocalized more quickly. This leads to interesting effect, even Model 1 had higher CKA with brain's population, empirical CKA measured with brain's sample could be actually smaller.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/cka_flip.png}
%     \caption{Toy case when Model 1 had higher CKA with brain's population but could have smaller CKA with brain's sample than Model 2}
%     % \label{fig:enter-label}
% \end{figure}

% \subsection{For power-law eigenspectrum, it is $N$, not $P/N$, that governs eigenvector delocalization}
% Assume $\tilde{\lambda}_i \sim i^{-1.2} \ , i=1,...P$. Let's focus on first few eigenvectors which will eventually govern CKA.  Then as we change $N$, we measure $Q_{ii}$, which is how much $i$-th sample eigenvector overlaps with $i$-th population eigenvector. On Fig.\ref{fig:overlap_with_P}, we can see that for fixed $P$, increasing $N$ makes more and more sample eigenvector close to population one. Interestingly, fixing $N$ and increasing $P$ has much subtler effect than opposite at least for first few terms of $Q_{ii}$. 


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/overlap_varying_N_varying_P.pdf}
%     \caption{$Q_{ii}$ for power-law eigenspectrum with $\tilde{\lambda_i} \sim i^{-1.2}$. For fixed $N$, changing $P$ has only subtle effect.}
%     \label{fig:overlap_with_P}
% \end{figure}

% We can see this from Eq.\ref{eq:overlap}. For first few components, we can show that $\mathfrak{h}_{\Sigma}(\lambda_i) \approx \frac{1}{\lambda_i}$. Then, for diagonal elements of $Q$,  first term in denominator is much smaller than second term. In this case,
% \begin{align}
% \braket{u_i | \tilde{u_i}}^2 &= Q_{ii} \\
% &\approx \frac{1}{P}\frac{q\lambda_i \tilde{\lambda_i}}{[q\lambda_i \tilde{\lambda_i} \pi \rho(\lambda_i)]^2} \\
% &= \frac{N}{P^2\lambda_i \tilde{\lambda_i}} \frac{1}{[\pi \rho(\lambda_i)]^2} 
% \end{align}

% As we show in appendix, when population eigenvalue density was $\rho(\tilde{\lambda})=\gamma \tilde{\lambda}^{-1-\gamma}$ for $\gamma =1-\epsilon$ for small $\epsilon >0$, then we can approximate sample eigenvalue density $\rho({\lambda}) \approx \gamma c^{\gamma}{\lambda}^{-1-\gamma}$ for $\lambda \gg 1$, where $c=\frac{\sum_{i=1}^P i^{-1/\gamma}}{\sum_{i=1}^N i^{-1/\gamma}}$ Also $\lambda_i \approx c \tilde{\lambda_i}$ for first few terms. If we plug this approximation to our overlap formula, 
% \begin{align}
% \braket{u_i | \tilde{u_i}}^2 & \approx \frac{c N}{\pi^2 i^2} = \frac{\sum_{i=1}^P i^{-1/\gamma}}{\sum_{i=1}^N i^{-1/\gamma}}\frac{N}{\pi^2 i^2}
% \label{eq:approx Q_ii}
% \end{align}
% which only has subtle dependence on $P$, since both numerator and denominator of $c$ quickly converges to $\sum_{i=1}^{\infty} i^{-1/\gamma}$.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/Q_ii_approx.png}
%     \caption{Theoretical $Q_{ii}$ with approximate Eq.\ref{eq:approx Q_ii} when $P=100, \tilde{\lambda_i} = i^{-1.2}$ }
%     \label{fig:enter-label}
% \end{figure}
% \section{Sample Eigenvectors as Poor Estimators of population Eigenvectors}


% \subsection{$\mathbf{Q}$ for Real Data}
% In real neural network activations, eigenvalues rarely show significant spectral gaps as in the spiked tensor model. Instead, they typically exhibit continuous heavy-tailed spectra \cite{stringer2019high}. In such cases, there is no phase transition in $Q_{ii}$ as described above. However, as the index $i$ increases, the eigenvectors quickly delocalize. As we will demonstrate in the next section, this delocalization leads to an underestimation of CKA for smaller $N$. Additionally, it becomes challenging to infer information about population eigenvectors from observed sample eigenvectors except for the first few. Look at Fig.\ref{figs/typcal_Q.png}



% \addplot for CCA phase transition here






% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{figs/IT_experiment.pdf}
%     \caption{\textbf{Empirical vs.\ inferred CKA:} We compare the responses of inferior temporal (IT) cortex to those of a Wide-ResNet on 3200 visual stimuli. The original dataset contains $168$ simultaneously recorded neurons. We artificially limit the neuron count to $N$ via random projection and measure the sample CKA (error bars). Our theoretical prediction (dotted line) matches this drop with smaller $N$. The solid line indicates our inferred population CKA (from backward inference), which largely exceeds the naive sample estimate at low $N$.}
%     \label{fig:IT_experiment}
% \end{figure}

% In \cref{fig:IT_experiment}, we show the results for Wide-ResNet activations against inferior temporal (IT) cortex recordings on the same 3200 images. The original dataset had $168$ neural recordings. We randomly projected these 168 neurons into lower-dimensional subspaces (various $N$) and applied our analysis. As expected, the observed sample CKA decreases with smaller $N$, but our theory (dotted lines) accurately predicts this trend by modeling the neural population with a power-law spectrum. Moreover, our inference method consistently estimates the population CKA to be around $0.4$—well above the naive sample estimate for small $N$.


% \section{Estimating population Eigenvector from entire sample Eigenbasis}

% \subsection{Determining Resolvable Eigenvectors}
% Another question that practitioners might have is: how many neurons are needed to reliably infer similarity or to achieve consistency in uncertainty levels across different experiments? To address this, we propose a simple method that establishes a consistent metric across different dimensions $P$. This allows us to determine, at a given uncertainty level, how many eigenvectors are resolvable based on the number of neurons $N$.

% As demonstrated in the previous section, in the case where the population eigenvalues follow a rank-one spike model(BBP phase transition), the answer is straightforward. If the spike exceeds the critical threshold, one eigenvector is resolvable; otherwise, none are. However, for real data where the eigenvalue spectrum is heavy-tailed without a significant spectral gap, such a phase transition does not occur. Therefore, we need a metric that consistently accounts for the uncertainty in eigenvector estimation.

% From the sample activations, and in cases where inferring eigenvalues is straightforward, we can calculate $\mathbf{Q}$ as noted earlier. This matrix contains information about the mean squared projection of a sample eigenvector onto the entire population eigenbasis. For inference, we want mean squared projection of population eigenvector with entire sample eigenbasis. Using the ansatz in Equation \ref{eq: noise independent}, we can write the joint probability distribution of the observed sample eigenbasis and the $i$-th population eigenvector. We then calculate the mutual information $I(\ket{\tilde{u}_i}; \{\ket{u_j}\})$ using the equations from \cite{MonassoN_y015}.

% When all $P$ mean overlaps on the sample eigenbasis are $\frac{1}{P}$, we can calculate that $I/P=0.5$. By setting a threshold level on $I/P$ that is greater than $0.5$, we can consistently determine how many eigenvectors are resolvable at this threshold level as a function of the number of neurons $N$. This makes it possible to provide practitioners with the number of eigenvectors that are resolvable at this level across different experiments. Note that since eigenvectors quickly delocalize, the number of resolvable eigenvectors is significantly smaller than the rank $N$.



% \subsection{Bigger sample size $P$ is better}
% Until now, we fixed sample size $P$ and observed effect of changing neuron size $N$. Here, we consider the opposite, for fixed $N$, how would varying $P$ effect inference of eigenvectors? We focus on power-law eigenspectrum, where $\tilde{\lambda_i} \sim i^{-1}$, $<\tilde{\lambda}>=1$, for different $P$. 

% In the Equation for $Q_{ii}$, Eq.\ref{eq:overlap}, for power-law spectrum latter term in numerator dominates. In this case, for different $P$, we can say that $\lambda_i$ is similar, since $<\tilde{\lambda}>=1$ and eigenvalue follows levy-statistics. Then, we can observed that$\frac{Q_{ii}(P_1)}{Q_{ii}(P_2)} \approx 1$  for different $P_1, P_2$, . This is remarkable result, since eventhough we increase number of dimension $P$ significant, overlap of between $i$-th population eigenvector and $i$-th sample eigenvector remains similar. Thinknig in terms of mutual information, this means if we increase $P$, we have more information about population eigenvector in sample eigenbasis.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/mutual_info_P_1000_N_x00_a_0.2.png}
%     \caption{Mutual information between first population eigenvector and entire sample eigenbasis, varying $P$. Information per dimension increase with $P$. Here $N=100$ fixed.
%     }
%     \label{fig:enter-label}
% \end{figure}

% Thus in the case of infering first few eigenvector in the case of power-law eigenspectrum, bigger $P$ is better different from BBP phase trnasition case.