\renewcommand{\theequation}{S\arabic{equation}}
% \renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
% \renewcommand{\thesection}{S\arabic{section}}
\setcounter{equation}{0}
% \setcounter{table}{0}
\setcounter{figure}{0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\onecolumn

\section{Detailed Derivation of the Main Result}\label{sec:SI.A}

\subsection{The Sample Gram Matrix}\label{sec:SI.A_sample_gram_matrix}

Let $\tilde\X \in \bR^{P\times\tilde N_x}$ denote the true population matrix with $P$ samples and $\tilde N_x$ neurons. We consider sampling only in the neuron/feature axis. The sample data $\X \in \bR^{P\times N_x}$ is obtained by applying an $\tilde N_x \times N_x$ random projection matrix $\R_x$ on $\tilde\X$
\begin{align}
    \X = \tilde \X \R_x, \quad (\R_x)_{ij} \sim \cN\lrpar{0, \frac{1}{N_x}}.
\end{align}
The population and sample Gram matrices and their corresponding eigen-components are denoted as
\begin{align}
    \tilde\bSigma_x &= \tilde\X\tilde\X^\top = \sum_{i=1}^P \tilde\lambda_i \ketbra{\tilde u_i}{\tilde u_i},\nonumber\\
    \bSigma_x &= \X\X^\top = \sum_{i=1}^P \lambda_i \ketbra{u_i}{u_i}.
\end{align}
In Random Matrix Theory (RMT), it is often convenient to consider matrices of the form $\M = \sqrt{\C} \W \sqrt{\C}$, where $\W = \R\R^\top$ is a random Wishart matrix and $\C$ is a deterministic square matrix. We first put $\bSigma_x$ into this form to simplify our calculations \cite{knowles2017anisotropic}. The sample Gram matrix can be written in terms of the SVD components of $\tilde \X = \U \tilde\bLambda_x^{1/2} \V^\top$
\begin{align}
    \bSigma_x = \tilde\X \R_x\R_x^\top \tilde\X^\top = \U \tilde\bLambda_x^{1/2} \lrpar{\V^\top \R_x\R_x^\top \V }\tilde\bLambda_x^{1/2} \U^\top,
\end{align}
where $\tilde\bLambda_x \in \bR^{P\times \tilde N_x}$ is a diagonal matrix, and $U \in \bR^{P\times P}$ and $V \in \bR^{\tilde N_x\times \tilde N_x}$ orthogonal matrices. Since deterministic orthogonal transformations of Wishart matrices are again Wishart matrices, we get:
\begin{align}
    \bSigma_x = \U \tilde\bLambda_x^{1/2} \W_x \tilde\bLambda_x^{1/2} \U^\top,
\end{align}
where $\W_x = \V^\top \R_x\R_x^\top \V$ is a random Wishart matrix with aspect ratio $\phi_x = {\tilde N_x/N_x}$. We divide our discussion into two cases:
\begin{itemize}
    \item When $P \geq \tilde N_x$, the eigenvalue matrix can be completed to a $P\times P$-matrix by zero padding and replacing $\W_x$ with a Wishart matrix with $q_x = P/N_x$. Using the orthogonality of $\U$, this allows us to express $\bSigma_x$ as
    \begin{align}
        \bSigma_x = (\U \tilde\bLambda_x^{1/2}  \U^\top) (\U \W_x \U^\top) (\U\tilde\bLambda_x^{1/2} \U^\top) = \sqrt{\tilde\bSigma_x} \W_x \sqrt{\tilde\bSigma_x},
    \end{align}
    where $\W_x$ is a Wishart matrix with aspect ratio $q_x = P/N_x$.

    \item  When $P < N_x$, the eigenvalue matrix and the Wishart matix can be written as 
    \begin{align}
        \tilde\bLambda_x = \begin{pmatrix}
        \tilde\bLambda'_x & \0
        \end{pmatrix}, \quad \W_x = \begin{pmatrix} \R_1 \\ \R_2
        \end{pmatrix} \begin{pmatrix} \R_1^\top & \R_2^\top
        \end{pmatrix},
    \end{align}
    where the $P\times P$ matrix $\tilde\bLambda'_x$ is the non-zero part of $\tilde\bLambda_x$ and $\R_1 \in \bR^{P\times N_x}$, $\R_2 \in \bR^{(\tilde N_x - P)\times N_x}$ are two projection matrices. Plugging these back in, we arrive at the same form as the previous case.
\end{itemize}
In both cases, the statistics of $\bSigma_x$ does not depend explicitly on $\tilde N_x$.

% Finally, we absorb the factor $\phi_x$ into the definition of population Gram matrix by rescaling $\tilde\bSigma_x \to \tilde\bSigma_x/\phi_x$ to simplify notation.

\subsection{Eigenvalue statistics of sample Gram matrices}\label{sec:SI.A_eigenvalue_statistics}

One of the main objectives of RMT is to understand the eigenvalue distribution of random matrices in terms of deterministic quantities \cite{Potters_Bouchaud_2020}. Here, we review some classical results on the eigenvalue statistics of random matrices of the form $\bSigma = \sqrt{\tilde \bSigma} \W \sqrt{\tilde\bSigma}$ where $\W$ is a $P\times N$ Wishart matrix with ratio $q= \frac{P}{N}$. Here, $\bSigma$ and $\tilde\bSigma$ are the sample and population Gram matrices, and they have the following eigendecompositions
\begin{align}
    \bSigma = \sum_{i=1}^P \lambda_i \ketbra{u_i}{u_i},\qquad \tilde\bSigma = \sum_{i=1}^P \tilde\lambda_i \ketbra{\tilde u_i}{\tilde u_i}.
\end{align}
We denote their (discrete-)eigenvalue distribution by $\rho(\lambda)$ and $\tilde\rho(\tilde\lambda)$:
\begin{align}\label{eq:SI.A_sample_pop_eigen_densities}
    \rho(\lambda) = \frac{1}{P}\sum_{i=1}^P \delta(\lambda - \lambda_i), \qquad \tilde\rho(\tilde\lambda) = \frac{1}{P}\sum_{i=1}^P \delta(\tilde\lambda - \tilde\lambda_i).
\end{align}
We define the resolvent of the random matrix $\X$ and its trace as
\begin{align}
    \G(z) = (z - \bSigma)^{-1} = \sum_{i=1}^P \frac{\ketbra{u_i}{u_i}}{z - \lambda_i}.
\end{align}
The Stieltjes transform of the empirical spectral distribution is defined as
\begin{align}
    \mathfrak{g}^P(z) := \int \frac{\rho(\lambda)}{z - \lambda} d\lambda = \frac{1}{P} \Tr \G(z).
\end{align}
In large $P$ limit, this quantity is self-averaging and there is a deterministic equivalent $\mathfrak{g}(z) \sim \mathfrak{g}^P(z)$ given by the self-consistent equation
\begin{align}\label{eq:SI.A_self_consistent_equation}
    \mathfrak{g}(z) = \int \frac{\tilde\rho(\tilde\lambda)}{z - \tilde\lambda(1 - q + q z \mathfrak{g}(z))}d\tilde\lambda,
\end{align}
which only depends on the deterministic eigenvalues $\tilde\rho_x(\tilde\lambda)$ and the ratio $q = P/N$ \cite{Potters_Bouchaud_2020}. In practical applications, $\tilde\rho_x(\tilde\lambda)$ is often replaced by with the uniform measure over the population eigenvalues $\{\tilde\lambda_i\}$ as defined in \eqref{eq:SI.A_sample_pop_eigen_densities}. This remarkable result was first obtained in \cite{marchenko1967distribution} for white Wishart matrices (for which $\tilde\bSigma = \I$). 

Due to the equivalence $\mathfrak{g}(z) \sim \mathfrak{g}^P(z)$ in large $P$ limit, these two integrals are equivalent
\begin{align}
    \int \frac{\rho(\lambda)}{z - \lambda} d\lambda \underset{P\to\infty}{\rightarrow} \int \frac{\tilde\rho(\tilde\lambda)}{z - \tilde\lambda(1 - q + q z \mathfrak{g}(z))}d\tilde\lambda,
\end{align}
from which one can obtain the density of the limiting spectral density using the inversion formula \cite{Bun_2017}
\begin{align}\label{eq:SI.A_limiting_spectral_density}
    \rho(\lambda) = \frac{1}{\pi}\lim_{\eta\to 0^+}\Im \mathfrak{g}(\lambda - i\eta).
\end{align}
The Stieltjes transform also connects to the effective regularization in ridge regression \cite{bordelon2020spectrum, jacot2020implicit, canatar2021spectral, atanasov2024scalingrenormalizationhighdimensionalregression}. We define a new function $\kappa(z)$ as
\begin{align}\label{eq:SI.A_kappa_definition}
    \kappa(z) := -\frac{z}{1-q + q z \mathfrak{g}(z)},\quad \mathfrak{g}(z) = z^{-1} - q^{-1}(z^{-1} + \kappa(z)^{-1})
\end{align}
and express \eqref{eq:SI.A_self_consistent_equation} in terms of this quantity:
\begin{align}
    \mathfrak{g}(z) = \frac{\kappa(z)}{z} \int \frac{\tilde\rho(\tilde\lambda)}{\tilde\lambda + \kappa(z)}d\tilde\lambda = z^{-1} - q^{-1}(z^{-1} + \kappa(z)^{-1}).
\end{align}
Then, we obtain a new self-consistent equation for $\kappa$
\begin{align}
    \kappa(z) = -z + \kappa(z)\int \frac{q \tilde\lambda}{\tilde\lambda + \kappa(z)} \tilde\rho(\tilde\lambda)d\tilde\lambda,
\end{align}
which is also known as the Silverstein equation \cite{silverstein1995analysis}. Expressing this in terms of the discrete population eigenvalues, and evaluating it at $z = -\lambda$, we get
\begin{align}
    \kappa = \lambda + \kappa \frac{1}{N} \sum_{i=1}^P \frac{\tilde\lambda_i}{\tilde\lambda_i + \kappa},
\end{align}
which is exactly the equation for the renormalized ridge parameter in \cite{canatar2021spectral, atanasov2024scalingrenormalizationhighdimensionalregression} with the scaling $\tilde\lambda_i \to N\tilde\lambda_i$.

\subsection{Eigenvector statistics of sample Gram matrices and the self-overlap matrix}

This result from \eqref{eq:SI.A_self_consistent_equation} can also be generalized to the resolvent matrix itself \cite{knowles2017anisotropic, Bun_2017}, which becomes diagonal in the population eigenbasis:
\begin{align}
    \G(z) = \sum_{i=1}^P \frac{\ketbra{u_i}{u_i}}{z - \lambda_i} \sim \sum_{i=1}^P \frac{\ketbra{\tilde u_i}{\tilde u_i}}{z - \tilde\lambda_i(1 - q + q z \mathfrak{g}(z))},
\end{align}
where the integral over eigenvalues is replaced by the discrete measure over population eigenvalues. This allows us to study the eigenvector statistics by analyzing the quantity 
\begin{align}\label{eq:SI.A_resolvent_matrix_elements}
    \braket{\tilde u_j|\G(z)|\tilde u_j} = \sum_{i=1}^P \frac{\braket{u_i|\tilde u_j}^2}{z - \lambda_i} \sim \frac{1}{z - \tilde\lambda_j(1 - q + q z \mathfrak{g}(z))}.
\end{align}
In large $P$ limit, the sum over empirical eigenvalues become an integral:
\begin{align}
    \braket{\tilde u_j|\G(z)|\tilde u_j} \underset{P\to\infty}{\longrightarrow} \int \frac{Q(\lambda, \tilde\lambda_j)}{z - \lambda}\rho(\lambda)d\lambda,
\end{align}
where we defined $Q(\lambda_i,\tilde\lambda_j) := P \braket{u_i|\tilde u_j}^2$ is the overlap between the $i^{\text{th}}$ sample eigenvector and the $j^{\text{th}}$ population eigenvector. Now, we can obtain $Q(\lambda_i, \tilde\lambda_j)$ using the following inversion formula
\begin{align}
    Q(\lambda_i, \tilde\lambda_j) = \frac{1}{\pi \rho(\lambda_i)} \lim_{\eta\to 0^+} \Im \braket{\tilde u_j|\G(\lambda_i - i\eta)|\tilde u_j}.
    \label{Eq: self overlap appendix}
\end{align}
Using the equivalence in \eqref{eq:SI.A_resolvent_matrix_elements} and evaluating this expression explicitly:
\begin{align}\label{eq:SI.A_sample_true_overlap}
    Q(\lambda_i, \tilde\lambda_j) = \frac{q \lambda_i \tilde\lambda_j}{\lrsqpar{\tilde\lambda_j (1-q) - \lambda_i + q \lambda_i\tilde\lambda_j \mathfrak{h}(\lambda_i)}^2 + \lrsqpar{q\lambda_i\tilde\lambda_j\pi\rho(\lambda_j)}^2},
\end{align}
we get an explicit formula for eigenvector overlaps \cite{ledoit2011eigenvectors, Bun_2017}, where $\rho(\lambda_i)$ is given by \eqref{eq:SI.A_limiting_spectral_density} and $\mathfrak{h}(z)$ is its Hilbert transform:
\begin{align}
    \mathfrak{h}(z) = \text{p.v.} \int \frac{\rho(\lambda)}{z - \lambda} d\lambda.
\end{align}
and can be obtained from the Stieltjes transform via
\begin{align}
    \lim_{\eta\to 0^+}\mathfrak{g}(z-i\eta) = \mathfrak{h}(z) + i\pi \rho(z).
\end{align}

\subsection{Overlap formula for two Gram matrices}

Here, we provide a short review of the work by \citet{bun2018overlaps} which derives an overlap formula between eigenvectors from random matrices. We consider observations from two representations $\X \in \bR^{P\times N_x}$ and $\Y \in \bR^{P\times N_y}$ in response to a common set of inputs of size $P$. Their sample Gram matrices have decompositions:
\begin{align}
    \bSigma_x &= \X\X^\top = \sum_{i=1}^P \lambda_i \ketbra{u_i}{u_i},\qquad \bSigma_y = \Y\Y^\top = \sum_{a=1}^P \mu_a \ketbra{w_a}{w_a}.
\end{align}

We assume that $\X$ and $\Y$ are observations sampled from the underlying population features $\tilde\X \in \bR^{P\times \tilde N_x}$ and $\tilde\Y \in \bR^{P\times \tilde N_y}$ through independent random projections. The corresponding population Gram matrices are decomposed as:
\begin{align}
    \tilde\bSigma_x &= \tilde\X\tilde\X^\top = \sum_{i=1}^P \tilde\lambda_i \ketbra{\tilde u_i}{\tilde u_i},\qquad \tilde\bSigma_y = \tilde\Y \tilde \Y^\top = \sum_{a=1}^P \tilde\mu_a \ketbra{\tilde w_a}{\tilde w_a}.
\end{align}
We consider two sample data matrices $\X \in \bR^{P\times N_x}$ and $\Y \in \bR^{P\times N_y}$. In \secref{sec:SI.A_sample_gram_matrix}, we showed that the sample Gram matrices can be expressed in terms of the population ones as:
\begin{align}\label{eq:SI.A_sample_gram_matrices}
    \bSigma_x &= \sqrt{\tilde\bSigma_x} \W_x \sqrt{\tilde\bSigma_x},\nonumber\\
    \bSigma_y &= \sqrt{\tilde\bSigma_y} \W_y \sqrt{\tilde\bSigma_y},
\end{align}
where the Wishart matrices $\W_x$ and $\W_y$ have aspect ratios $q_x = P/N_x$ and $q_y = P/N_y$, respectively. Resolvents of the sample Gram matrices are
\begin{align}
    \G_x(z) &\equiv (z - \bSigma_x)^{-1} = \sum_{i=1}^P \frac{\ketbra{u_i}{u_i}}{z - \lambda_i}, \qquad \G_y(z') \equiv (z' - \bSigma_y)^{-1} = \sum_{a=1}^P \frac{\ketbra{w_a}{w_a}}{z' - \mu_a}.
\end{align}
% We define $\rho_x^{P}(\lambda) := P^{-1}\sum_i \delta(\lambda - \lambda_i)$ to be the empirical eigenvalue density of $\bSigma_x$. The Stieltjes transform of this density is defined as
% \begin{align}
%     \mathfrak{g}^P_x(z) &:= \int \frac{\rho^P_x(\lambda)}{z-\lambda}d\lambda = \frac{1}{P}\Tr\G_x(z)\nonumber\\
%     \mathfrak{g}_x(z) &:= \lim_{P\to\infty}\mathfrak{g}^P_x(z) = \int \frac{\rho_x(\lambda)}{z-\lambda}d\lambda.
% \end{align}
% where $\mathfrak{g}_x(z)$ is the limiting value of the empirical $\mathfrak{g}^P_x(z)$, and $\rho_x(\lambda)$ is the continuous density of the empirical eigenvalues. It is a well-known result in RMT that $\mathfrak{g}_x(z)$ concentrates around a deterministic function that only depends on $q_x$ and the population Gram matrix \cite{Bun_2017}. In our case, this deterministic function is defined by the self-consistent equation
% \begin{align}\label{eq:SI.A_self_consistent_eq}
%     \mathfrak{g}_x(z) = \int \frac{\tilde\rho_x(\tilde\lambda)}{z - \tilde\lambda (1 - q_x + q_x z \mathfrak{g}_x(z))}d\tilde\lambda,
% \end{align}
% where $\tilde\rho_x(\tilde\lambda)$ is the eigenvalue density of the population Gram matrix. In practical applications, we often replace $\tilde\rho_x(\tilde\lambda)$ with the uniform measure over the population eigenvalues $\{\tilde\lambda_i\}$.

% Once $\mathfrak{g}_x(z)$ is solved self-consistently, one can \textit{invert} it to obtain the limiting spectral density $\rho_x(\lambda)$ through the relation
% \begin{align}\label{eq:SI.A_density_inversion_formula}
%     \lim_{\eta\to 0^+}\mathfrak{g}_x(z-i\eta) = \mathfrak{h}_x(z) + i\pi \rho_x(z),\quad \rho_x(z) = \frac{1}{\pi}\lim_{\eta\to 0}\Im \mathfrak{g}_x(z-i\eta)
% \end{align}
% where $\mathfrak{h}_x(z)$ is the Hilbert transform of $\rho_x(\lambda)$ \cite{Bun_2017}. 
We want to compute
\begin{align}\label{eq:SI.A_psi_empirical}
    \psi_P(z, z') = \bE \lrsqpar{\frac{1}{P}\Tr \lrsqpar{\G_x(z)\G_y(z')}} = \bE \lrsqpar{\frac{1}{P^2}\sum_{i, a = 1}^P \frac{P \braket{u_i | w_a}^2}{(z - \lambda_i)(z' - \mu_a)}},
\end{align}
where the expectation is over random realizations of sample Gram matrices \cite{bun2018overlaps}. In the limit $P\to\infty$, as empirical eigenvalues become continuous, this object approaches a deterministic function
\begin{align}
    \psi_P(z, z') \sim \psi(z, z') = \int \frac{\rho_x(\lambda)\rho_y(\mu)}{(z - \lambda)(z' - \mu)}  M(\lambda,\mu) d\lambda \,d\mu, \quad M(\lambda_i,\mu_a) \sim \bE\lrsqpar{P\braket{u_i|w_a}^2}
\end{align}
Here, $\rho_x(\lambda)$, $\rho_y(\mu)$ are the eigenvalue densities of $\bSigma_x$, $\bSigma_y$ given by \eqref{eq:SI.A_limiting_spectral_density}. The function $M(\lambda_i,\mu_a) \sim \bE\lrsqpar{P\braket{u_i|w_a}^2}$ denotes the expected overlap between two eigenvectors associated with eigenvalues $\lambda_i$ and $\mu_a$, and it is the central object for our analysis since it directly appears in CCA and CKA. This quantity can be obtained by computing $\psi(\lambda_i - i\eta, \mu_a + i\eta')$, collecting the term proportional to $\eta\eta'$ and taking the limit $\eta,\eta'\to 0$ \cite{bun2018overlaps}:
\begin{align}\label{eq:SI.A_inversion_formula}
    \psi(\lambda_i - i\eta, \mu_a + i\eta') &= \int \frac{(\lambda_i - \lambda + i\eta)\rho_x(\lambda)}{(\lambda_i - \lambda)^2 + \eta^2} \frac{(\mu_a - \mu - i\eta')\rho_y(\mu)}{(\mu_a - \mu)^2 + {\eta'}^2} M(\lambda,\mu) d\lambda \,d\mu\nonumber\\
    &= \int \frac{\eta\rho_x(\lambda)}{(\lambda_i - \lambda)^2 + \eta^2} \frac{\eta'\rho_y(\mu)}{(\mu_a - \mu)^2 + {\eta'}^2} M(\lambda,\mu) d\lambda \,d\mu + (\dots)\nonumber\\
    &\underset{\eta,\eta'\to 0}{=} \pi^2\rho_x(\lambda_i)\rho_y(\mu_a) M(\lambda_i,\mu_a)  + (\dots)
\end{align}

To simplify, we will assume that the population eigenvectors form a complete set of basis:
\begin{align}
    \I = \sum_{i=1}^P \ketbra{\tilde u_i}{\tilde u_i} = \sum_{a=1}^P \ketbra{\tilde w_a}{\tilde w_a}.
\end{align}
Then each resolvent in \eqref{eq:SI.A_psi_empirical} can be expressed in these bases:
\begin{align}
    \G_x(z) &= \sum_{i,j} \ketbra{\tilde u_i}{\tilde u_j} \Phi_{ij}^x(z), \quad \Phi_{ij}^x(z): \braket{\tilde u_i|\G_x(z)|\tilde u_j},\nonumber\\
    \G_y(z) &= \sum_{a,b} \ketbra{\tilde w_a}{\tilde w_b} \Phi_{ab}^y(z'), \quad \Phi_{ab}^y(z'):= \braket{\tilde w_a|\G_y(z)|\tilde w_b},
\end{align}
where $\Phi_{ij}^x$ and $\Phi_{ab}^y$ are the matrix elements of resolvents $\G_x(z)$ and $\G_y(z')$ in their respective deterministic bases. Then, \eqref{eq:SI.A_psi_empirical} simplifies to
\begin{align}\label{eq:SI.A_psi_limiting}
    \psi_P(z, z') = \bE \lrsqpar{\frac{1}{P}\sum_{i,j,a,b} \Phi_{ij}^x(z) \tilde C_{ja} \Phi_{ab}^y(z') \tilde C^\top_{bi}} = \frac{1}{P}\sum_{i,j,a,b} \bE [\Phi_{ij}^x(z)] \tilde C_{ja} \bE [\Phi_{ab}^y(z')] \tilde C^\top_{bi},
\end{align}
where we defined the deterministic overlap matrix elements $\tilde C_{ia}:= \braket{\tilde u_i| \tilde w_a}$. In the second equality, we assumed that two resolvents are independent, reducing the problem to computing the expected resolvent of a single Gram matrix.

As discussed around \eqref{eq:SI.A_resolvent_matrix_elements}, the resolvent $\G_x$ has a limiting value for $P\to\infty$ that is diagonal in the corresponding deterministic basis \cite{Bun_2017}, and its matrix elements are given by:
\begin{align}\label{eq:SI.A_determinstic_resolvent}
    \Phi_{ij}^x(z) = \frac{\delta_{ij}}{z - \tilde\lambda_i (1 - q_x + q_x z \mathfrak{g}_x(z))} + \cO(P^{-1/2}),
\end{align}
where $\mathfrak{g}_x(z)$ satisfies the self-consistency condition in \eqref{eq:SI.A_self_consistent_equation}.

In order to compute the overlap $M(\lambda_i, \mu_a)$, we use \eqref{eq:SI.A_inversion_formula} and collect the term proportional to $\eta\eta'$. Thanks to \eqref{eq:SI.A_psi_limiting} and \eqref{eq:SI.A_determinstic_resolvent}, this term simplifies to:
\begin{align}
    \pi^2\rho_x(\lambda_i)\rho_y(\mu_a) M(\lambda_i,\mu_a) = \frac{1}{P} \sum_{j,b} \lrpar{\lim_{\eta\to 0}\Im\Phi_{jj}^x(\lambda_i -i\eta)}\tilde C_{jb}^2 \lrpar{\lim_{\eta'\to 0}\Im\Phi_{bb}^y(\mu_a - i\eta')}.
\end{align}
Defining
\begin{align}
    Q_x(\lambda_i, \tilde\lambda_j) &:= \frac{1}{\pi\rho_x(\lambda_i)}\lim_{\eta\to 0}\Im\Phi_{jj}^x(\lambda_i -i\eta),\quad 
    Q_y(\mu_a, \tilde\mu_b) := \frac{1}{\pi\rho_y(\eta_a)}\lim_{\eta'\to 0}\Im\Phi_{bb}^y(\mu_a - i\eta')
\end{align}
we get an equation for $M$ as
\begin{align}
    M(\lambda_i,\mu_a) = \frac{1}{P} \sum_{j,b} Q_x(\lambda_i, \tilde\lambda_j)\tilde C_{jb}^2 Q_y(\mu_a, \tilde\mu_b).
\end{align}
Here, $Q_x$ and $Q_y$ were already calculated in \eqref{eq:SI.A_sample_true_overlap}. Identifying the following quantities
\begin{align}
    Q^{x}_{ij} &\equiv \bE\braket{u_i|\tilde u_j}^2 = \frac{1}{P} Q_x(\lambda_i, \tilde\lambda_j),\quad Q^{y}_{ab} \equiv \bE\braket{w_a|\tilde w_b}^2 = \frac{1}{P} Q_y(\mu_a, \tilde\mu_b),\nonumber\\
    M_{ia} &\equiv \bE\braket{u_i|w_a}^2 = \frac{1}{P} M(\lambda_i,\mu_a),\quad \tilde M_{ia} := \braket{\tilde u_i|\tilde w_a}^2 = \tilde C_{ia}^2,
    \label{eq: definition overlap matrices appendix}
\end{align}
we get our main result \cite{bun2018overlaps}:
\begin{align}
    \M &= \Q^{x} \tilde\M {\Q^{y}}^\top,\nonumber\\
    Q^{x}_{ij} &= \frac{1}{P}\frac{q_x \lambda_i \tilde\lambda_j}{\lrsqpar{\tilde\lambda_j (1-q_x) - \lambda_i + q_x \lambda_i\tilde\lambda_j \mathfrak{h}_x(\lambda_i)}^2 + \lrsqpar{q_x\lambda_i\tilde\lambda_j\pi\rho_x(\lambda_i)}^2},\nonumber\\
    Q^{y}_{ab} &= \frac{1}{P}\frac{q_y \mu_a \tilde\mu_b}{\lrsqpar{\tilde\mu_b (1-q_y) - \mu_a + q_y \mu_a\tilde\mu_b \mathfrak{h}_y(\mu_a)}^2 + \lrsqpar{q_y\mu_a\tilde\mu_b\pi\rho_y(\mu_a)}^2}.
    \label{eq:final overlap formula}
\end{align}


\subsection{Statistics of sample eigenvalues and its concentration properties}\label{sec:SI.A_sample_eigenvalue_statistics}

As we discussed in the main text, the practical usage of \eqref{eq:SI.A_sample_true_overlap} requires computing the expectation value of individual sample eigenvalues. Eq.\ref{eq:final overlap formula}, treating \( i^{\text{th}} \) biggest sample eigenvalue as deterministic and plugging $\eta=1/\sqrt{P}$

\noindent
For sufficient conditions, we can show that the sample resolvent $\mathfrak{g}(z)$ self-averages. In this case, sample eigenvalue density $\rho(\lambda)$ converges in law. Here, we show that for practical use of Eq.\ref{Eq: self overlap appendix}, Eq.\ref{eq:final overlap formula}, we can treat \( i^{\text{th}} \) biggest eigenvalue as effectively deterministic in its most probable position. 

Specifically, we demonstrate that for a large number of eigenvalues \( P \), the most probable \( i \)-th largest eigenvalue \( \lambda_i \) satisfies
\begin{align}\label{sec:SI_eigenvalue_integral_formula}
    \int_{\lambda_i}^{\infty} \rho(\lambda) \, d\lambda = \frac{i}{P},
\end{align}
and that the fluctuations around this most probable is $O(1/\sqrt{P})$.

Consider a set of \( P \) eigenvalues \( \{\lambda_1, \lambda_2, \dots, \lambda_P\} \) drawn independently from the probability density \(\rho(\lambda)\). We order these eigenvalues in descending order:
\[
\lambda_{(1)} \;\ge\; \lambda_{(2)} \;\ge\; \dots \;\ge\; \lambda_{(P)},
\]
where \(\lambda_{(i)}\) denotes the \( i^{\text{th}} \) largest eigenvalue. To find the most probable value \(\lambda_i\) for the \( i^{\text{th}} \) largest eigenvalue, we focus on the probability that \emph{exactly} \#\( i \) eigenvalues exceed a threshold \(\bar{\lambda}\). If we define
\[
F(\bar{\lambda}) \;=\; \int_{\bar{\lambda}}^{\infty} \rho(\lambda') \, d\lambda',
\]
then the probability that exactly \( i \) out of \( P \) samples exceed \(\bar{\lambda}\) is given by the binomial expression
\[
F(\bar{\lambda}, P, i)
\;=\;
\binom{P}{i}\, \bigl[ F(\bar{\lambda}) \bigr]^{i}\,
\bigl[ 1 - F(\bar{\lambda}) \bigr]^{\,P - i}.
\]
We determine the threshold \(\bar{\lambda}_i\) that maximizes \(F(\bar{\lambda}, P, i)\) by setting its derivative (with respect to \(\bar{\lambda}\)) to zero. From this calculation, one obtains the simple condition
\[
F(\bar{\lambda}_i) \;=\; \frac{i}{P}.
\]
Equivalently, since \(F(\lambda) = \int_{\lambda}^{\infty}\rho(\lambda')\,d\lambda'\), the most probable \( i^{\text{th}} \) largest eigenvalue \(\lambda_i\) satisfies
\[
\int_{\lambda_i}^{\infty} \rho(\lambda)\, d\lambda
\;=\;
\frac{i}{P}.
\]

Now we calculate approximations for fluctuation around this most probable position. Let's  analyze \( F(\bar{\lambda}, P, i)\) near \(\bar{\lambda}_i\). Write \(\bar{\lambda} = \lambda_i + \delta\lambda\) and expand \( F(\bar{\lambda}) \) in a Taylor series about \(\lambda_i\):
\[
F(\bar{\lambda}) 
\;=\;
F(\lambda_i + \delta\lambda)
\;\approx\;
F(\lambda_i)
\;+\;
\left.\frac{dF}{d\lambda}\right|_{\lambda_i}\,\delta\lambda
\;+\;
\frac12 \,\left.\frac{d^2F}{d\lambda^2}\right|_{\lambda_i}\,(\delta\lambda)^2 
\;+\;\dots
\]
Since \(F(\lambda_i) = \tfrac{i}{P}\) and \(\lambda_i\) is determined by maximizing \(F(\bar{\lambda}, P, i)\), the first derivative of \(F\) at \(\lambda_i\) vanishes:
\[
\left.\frac{dF}{d\lambda}\right|_{\lambda_i} = 0,
\]
thus
\[
F(\bar{\lambda}) 
\;\approx\;
\frac{i}{P}
\;+\;
\frac12\,F''(\lambda_i)\,(\delta\lambda)^2.
\]
(We expect \(F''(\lambda_i) < 0\) since \(F(\lambda)\) decreases with \(\lambda\).)

Substituting this expansion back into
\(\binom{P}{i}\,\bigl[F(\bar{\lambda})\bigr]^i\bigl[1-F(\bar{\lambda})\bigr]^{P-i}\),
we find that the dominant dependence on \(\delta\lambda\) appears in a Gaussian-like factor
\[
\exp\!\Bigl(-\tfrac12\,|F''(\lambda_i)|\,P\,(\delta\lambda)^2\Bigr).
\]
This indicates that \(\bar{\lambda}\) (the threshold that yields exactly \( i \) exceedances) is peaked sharply around \(\lambda_i\) with a variance
\[
\sigma_i^2
\;=\;
\frac{1}{-\,F''(\lambda_i)\,P}.
\]



In summary, most probable \( i \)-th largest eigenvalue \(\lambda_{(i)}\) is determined by
\[
\int_{\lambda_i}^{\infty} \rho(\lambda)\, d\lambda
\;=\;
\frac{i}{P},
\]
with fluctuation $O(1/\sqrt{P})$.

\subsection{Statistics of sample eigenvalues and its concentration properties}\label{sec:SI.A_sample_eigenvector_statistics}

Note that unlike eigenvalue density converges in law, eigenvector statistics \eqref{Eq: self overlap appendix} is noisy even when $P \rightarrow \infty$ \cite{Potters_Bouchaud_2020}. In this case, we define $Q$ matrix as expectation over different trials as in Eq.\ref{eq: definition overlap matrices appendix}. Equivalently, this could be obtained by averaging over small eigenvalue interval, which could be done by plugging small $\eta = 1/\sqrt{P}$ to extract pole. Note that this $1/\sqrt{P}$ is also obtained by analyzing fluctuation around most probable $i$-th biggest eigenvalue as in above. This is essentially averaging over cauchy distribution centered in $\lambda$ with width $\eta$. Thus for practial usage of Eq.\ref{eq:final overlap formula}, we simply plug this most likely $i$-th eigenvalue \cite{Bun_2017}, with $\eta=1/\sqrt{P}$.


% \section{Effect of the Normalization Term in the Eigenvalues}
% \label{appendix: normalizing e-vale}
% As noted above, the Central Kernel Alignment (CKA) is a weighted sum of \( M \), where the weights are normalized eigenvalues:
% \begin{align}
% \text{CKA} &= \sum_{i=1}^{P} \sum_{a=1}^{P} \left( \frac{\lambda_X^i}{\sqrt{\sum_{k=1}^{P} (\lambda_X^k)^2}} \right) \left( \frac{\lambda_Y^a}{\sqrt{\sum_{b=1}^{P} (\lambda_Y^b)^2}} \right) M_{ia}.
% % \label{eq:cka_spectral}
% \end{align}

% In \figref{fig:SI_sample_eigenvalues}, we show that the normalized sample eigenvalues closely follow the true ones up to rank \( N \). Here, we present the sample eigenvalues without normalization.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/sample_evalue_without_normalization.png}
%     \includegraphics[width=0.7\linewidth]{figs/how_sample_eigenvalue_changes.png}
%     \caption{Unnormalized sample eigenvalues when the true eigenvalues follow \( \tilde{\lambda}_X^i \sim i^{-1.2} \). Here, \( P=1000 \) is fixed. The left plot is in log-log scale, and the right plot is the same data in linear-log scale.}
%     \label{fig:SI_sample_eigenvalues}
% \end{figure}

% We observe why normalization sets a consistent scale across different \( N \). Empirically, when \( \tilde{\lambda}_i \sim i^{-1-\gamma} \), the sample eigenvalues approximately follow \( \lambda_i \approx c(N) i^{-1-\gamma} \) for some constant \( c(N) \) depending on \( N \). Then,
% \[
% \sqrt{\sum_j \lambda_j^2} = c(N) \sqrt{\sum_{j=1}^N j^{-2(1+\gamma)}} \approx c(N) \sqrt{\sum_{j=1}^{\infty} j^{-2(1+\gamma)}}.
% \]
% Thus, normalizing \( \lambda_i \) by \( \sqrt{\sum_j \lambda_j^2} \) eliminates the dependency on \( c(N) \).

% \section{Additional Details for \eqref{eq:overlap}}
% \label{appendix: details of overlap formula}
% The resolvent contains information about the eigenvalues and eigenvectors of the Gram matrix:
% \begin{align}
% G_{\Sigma}(z) = (z\mathbf{I}-\Sigma)^{-1} = \sum_{i=1}^P \frac{\ket{u_i}\bra{u_i}}{z-\lambda_i},
% \end{align}
% where \( \Sigma \) is the covariance matrix, \( \lambda_i \) are its eigenvalues, and \( \ket{u_i} \) are the corresponding eigenvectors.

% The Stieltjes transform is the trace of the resolvent:
% \begin{align}
% g_{\Sigma}(z) = \frac{1}{P}\operatorname{Tr}\left[(z\mathbf{I}-\Sigma)^{-1}\right] = \frac{1}{P}\sum_{i=1}^P \frac{1}{z-\lambda_i}.
% \end{align}

% It is known that $g_\bSigma(z)$ can be obtained from $g_{\tilde{\Sigma}}(z)$ using the Mar\v{c}enko-Pastur equation. We will deal with a discretized version of this equation, which is known to be more numerically stable for heavy-tailed eigen spectra such as power laws~\cite{Bun_2017}.

% Assuming we know the true eigenvalues $\tilde\lambda_i$, $g_{\Sigma}(z)$ satisfies the self-consistent equation (discretized Mar\v{c}enko-Pastur equation)~\cite{Potters_Bouchaud_2020}:
% \begin{align}
% g_{\Sigma}(z) = \frac{1}{P} \sum_{i=1}^{P} \frac{1}{z - \tilde{\lambda}_i [1 - \alpha + \alpha z g_{\Sigma}(z)]},
% \end{align}
% where \( \alpha = P/N \).

% At first glance, it seems we are dealing with \( g_{\Sigma}(z) \), the trace of the resolvent, so it appears to retain only information about the eigenvalues, losing the eigenvectors. However, recent work using the replica method has shown how to calculate the entire \( G_{\Sigma}(z) \) from \( g_{\Sigma}(z) \) in the high-dimensional limit~\cite{bun2018overlaps}:
% \begin{align}
% G_{\Sigma}(z) = \frac{Z(z)}{z} G_{\tilde{\Sigma}}(Z(z)), \quad \text{where} \quad Z(z) = \frac{z}{1-\alpha + \alpha z g_{\Sigma}(z)}. \label{Eq:resolvent}
% \end{align}
% For alternative derivation using diagrammatic method, have a look at \cite{atanasov2024scalingrenormalizationhighdimensionalregression}. 
% From this, we can obtain \eqref{eq:overlap}, as shown in~\cite{bun2018overlaps}. Since $G_{\Sigma}(z)$ is simply in the basis of true eigenvectors, it is easier to first deal with first the self-overlap of the sample and true eigenvectors \( Q \), and then using it to compute \( \mathbb{E}[M] = Q\tilde{M} \).

% \begin{align}
% \bra{\tilde{u}_j} G_{\Sigma}(z) \ket{\tilde{u}_j} &= \sum_{i=1}^P \frac{|\braket{u_i| \tilde{u}_j}|^2}{z-\lambda_i}
% \end{align}

% However, as shown in \cite{Bun_2016}, unlike eigenvalue the eigenvectors keep fluctuates in limit $P\rightarrow \infty$. To deal with this, we deal with eigenvector averaged for interval size $\eta$ w.r.t eigenvalue $1> \eta > 1/P$. In terms of index, we can think it as averaging over different trials(if we think in terms of index not eigenvalues, typical $\lambda_i$ be in $\int_{\lambda_i}^{\infty} d\lambda \rho_{\Sigma}(\lambda) = \frac{i}{N}$ with small fluctuation).

% \begin{align}
% \operatorname{Im} \left[ \bra{\tilde{u}_j} G_{\Sigma}(\lambda_i - i\eta) \ket{\tilde{u}_j} \right] &\approx \pi \rho_{\Sigma}(\lambda_i) P \mathbb{E} [|\braket{u_i| \tilde{u}_j}|^2] \\
% &= \pi \rho_{\Sigma}(\lambda_i) P  Q_{ij}.
% \end{align}

% Rearranging terms, we obtain \eqref{eq:overlap}. Note that \( h_{\Sigma}(\lambda) \) and \( \rho_{\Sigma}(\lambda) \) in \eqref{eq:overlap} are related to the real and imaginary parts of \( g_{\Sigma}(z) \):
% \begin{align}
% \lim_{\eta \rightarrow 0} g_{\Sigma}(\lambda - i\eta) &= h_{\Sigma}(\lambda) - i\pi \rho_{\Sigma}(\lambda),
% \end{align}
% where \( h_{\Sigma}(\lambda) \) is the Hilbert transform and \( \rho_{\Sigma}(\lambda) \) is the density of states. We set \( \eta = 1/\sqrt{P} \) as suggested in~\cite{Potters_Bouchaud_2020}.


% The analysis above was overlap of sample eigenvector $\ket{u_i}$ with true eigenvector $\ket{\tilde{u_i}}$. However, since $G_{\Sigma}(z)$ is diagnoal on true eigenvector basis from \eqref{Eq:resolvent}, its simple to extend to arbitrary vector $\ket{w}$. Say that $\ket{w}= (w_1, w_2, ... w_P)$ in the true eigenvector basis. Then $\operatorname{Im} \left[ \bra{w} G_{\Sigma}(\lambda_i - i\eta) \ket{w} \right] = \sum_{j=1}^P w_j^2 \operatorname{Im} \left[ \bra{\tilde{u_j}} G_{\Sigma}(\lambda_i - i\eta) \ket{\tilde{u_j}} \right]$. 
% % This leads us to \eqref{eq:expected_M}.

% \begin{section}{Details of eignevalue density when population eigenvalue followed power-law}
% To used \eqref{eq: definition overlap matrices appendix}, we first need to deal with eigenvalues, for both forward and backward problem. For forward case, we need to determine sample eigenavlue from population eigenvalues. For backward case, we need to infer true eigenvalue from population eigenvalues. For forward case, we can numerically solve marchenko-pastur equation and calculate sample eigenvalue density, but for power-law case we will show simple approximation works well. For backward case, 
% there is numerous literature, estimating density from moment\cite{kong2017spectrumestimationsamples}, using same method but puting broken power-law anstaz\cite{Pospisil2024.02.16.580726}. Here we use similar idea, putting even simpler power-law anstaz and determing mulitplicative constant and exponent using 2 identity. In this sense, we aremainly focus on eigenvector statistics, assuming inferring true eigenvalue is not hard.

% \begin{subsection}{Sample Eigenvalues behavior(Forward Problem)}

% \subsubsection{Tail behavior of sample eigenvalue density}
% Consider a population eigenvalue distribution following a power law:
% \begin{equation}
%     \rho_{\tilde{\Sigma}}(\tilde{\lambda}) = \frac{1}{\gamma} \tilde{\lambda}^{-(1+\gamma)}, \quad \tilde{\lambda} \in [1,\infty]
% \end{equation}
% This corresponds to ordered eigenvalue scaling:
% \begin{equation}
%     \tilde{\lambda}_i \sim i^{-\frac{1}{\gamma}}
% \end{equation}

% Expanding marchenko-pastur equation (\textcolor{red}{Add Abdul's Expansion}), we can demonstrate that the sample eigenspectrum exhibits a similar tail behavior at leading order:
% \begin{equation}
%     \rho_{\Sigma}(\lambda) \sim \lambda^{-(1+\gamma)}, \quad \text{for } \lambda \gg 1
% \end{equation}

% \subsubsection{Determining constant factor}
% We propose the ansatz:
% \begin{equation}
%     \lambda_i = c(N)\tilde{\lambda}_i, \quad \text{for } i=1,\ldots,N
% \end{equation}

% To determine $c(N)$, we utilize the identity $\mathbb{E}[\sum_i \lambda_i] = \sum_i \tilde{\lambda}_i$. This can be proven as follows:

% Assume sample gram matrix$\mathbf{\Sigma}$ was population gram matrix multiplied by white-Wishart matrix:
% \begin{equation}
%     \mathbf{\Sigma} = \frac{1}{N}\sqrt{\mathbf{\tilde{\Sigma}}} \mathbf{Z} \mathbf{Z}^T \sqrt{\mathbf{\tilde{\Sigma}}}, 
%     \quad \text{where } Z_{ij} \sim \mathcal{N}(0,1), \mathbf{Z}\in \mathbb{R}^{P\times N}
% \end{equation}

% Then:
% \begin{align}
%     \mathbb{E} [\Tr \mathbf{\Sigma}] &= \mathbb{E} \left[\sum_{i=1}^P \Sigma_{ii} \right] \\
%     &= \mathbb{E} \left[\frac{1}{N} \sum_{j,k,l}(\sqrt{\mathbf{\tilde{\Sigma}}})_{ij} Z_{jk} Z_{lk} (\sqrt{\mathbf{\tilde{\Sigma}}})_{li} \right] \\
%     &= \frac{1}{N} \sum_{j,k,l}(\sqrt{\mathbf{\tilde{\Sigma}}})_{ij} \mathbb{E} [Z_{jk} Z_{lk}] (\sqrt{\mathbf{\tilde{\Sigma}}})_{li} \\
%     &= \sum_{j,l}(\sqrt{\mathbf{\tilde{\Sigma}}})_{ij} \delta_{jl} (\sqrt{\mathbf{\tilde{\Sigma}}})_{li} \\
%     &= \Tr \mathbf{\tilde{\Sigma}}
%     \label{eq: mean eigenvalue identity}
% \end{align}

% Therefore, the scaling factor $c(N)$ is given by:
% \begin{equation}
%     c(N) = \frac{\sum_{i=1}^P i^{-\frac{1}{\gamma}}}{\sum_{i=1}^N i^{-\frac{1}{\gamma}}}
% \end{equation}

% \subsubsection{Empirical Validation}
% Figure \ref{fig:powerlaw_approx} shows the comparison between empirical sample eigenvalues and our approximation using $\lambda_i = c(N)\tilde{\lambda}_i$.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.6\linewidth]{figs/powerlaw_approx.png}
%     \caption{Comparison of empirical sample eigenvalues with power-law ansatz approximation}
%     \label{fig:powerlaw_approx}
% \end{figure}

% The close agreement between the ansatz-based overlap formula and empirical results suggests that numerical SVD calculations can be avoided for this analysis.

% \end{subsection}



% \begin{subsection}{Inferring Population eigenvalue (Backward Problem)}
% As we showed above, when population eigenvalue density has power-law tail, then sample eigenvalue density also has power-law tail with same exponent. Then simplest thing to infer population eigenvalue is putting same exponent as observed from sample eigenvalue, and plug constnat determined by \eqref{eq: mean eigenvalue identity}. However, this method brings question of how to determine the power-law exponent of sample eigenvalue, and for practical purpose, we use another identity to first determine exponent. This is identity regarding inverse partipation ratio of eigenvalue, derived by chanwoo chun.
% \begin{align}
%     \text{IPR}(\{\lambda_i\}))= \text{IPR}(\{\tilde{\lambda_i}\}))+1/N
%    \quad \text{\textcolor{red}{(Add Proof)}}
% \end{align}

% Uinsg this identity with \eqref{eq: mean eigenvalue identity}, we can infer population eigenvalue density with powerlaw anstaz, $\tilde{\lambda_i} = (const) i^{-1/\gamma}$. We did simulation, and indeed when population eigenvalue actually followed powerlaw, this method works well.
% \end{subsection}
% \end{section}





% \begin{section} {For Power-law eigenspectrum, drop of CKA w.r.t to $N$ is mainly driven by eigenvector delocalization}


% Recall CKA could be expressed from eigencomponents,
% \begin{align}\label{eq:SI.A_cka_spectral}
%     \text{CKA} &= \frac{\Tr \mathbf{\Sigma_X} \mathbf{\Sigma_Y}}{\sqrt{\Tr\mathbf{\Sigma_X}^2} \sqrt{\Tr\mathbf{\Sigma_Y}^2}}  = \sum_{i=1}^{P} \sum_{a=1}^{P} 
%     \frac{\lambda_i}{\sqrt{\sum_{j=1}^{P} \lambda_j^2}}
%     \frac{\mu_a}{\sqrt{\sum_{b=1}^{P} \mu_b^2}}
%     M_{ia}.
% \end{align}
% Note that CKA is sum of overlap $M_{ia}$ weightbed by \textbf{normalized} eigenvalues. 

% In the case when population eigenvalue followed $\tilde{\lambda_i} = i^{-1/\gamma}, i=1,...,P$, we showed sample eigenvalue approximately followed ${\lambda_i} = c(N)i^{-1/\gamma}, i=1,...,N$, where $c(N)=\frac{\sum_{i=1}^{P}i^{-1/\gamma}}{\sum_{i=1}^{N}i^{-1/\gamma}}$. Then for sufficiently large $N$, 
% \begin{align}
%     \frac{\lambda_i}{\sqrt{\sum_{j=1}^{N} \lambda_j^2}} \approx \frac{c(N)i^{-1/\gamma}}{\sqrt{\sum_{j=1}^{N} (c(N) j^{-1/\gamma})^2}} \approx \frac{i^{-1/\gamma}}{\sqrt{\sum_{j=1}^{\infty} (j^{-1/\gamma})^2}}
% \end{align}
% For $\gamma \approx 1$. This means that for first few terms that dominates CKA, at least eigenvalue terms has negligible change. We thus conlcude that eigenvector delocalization is what mainly driving drop of CKA.

% \textcolor{red}{Add drop of CKA show empirical curve, and also emprical CKA with population eignevlaue and smaple eigenvector, to emphasize drop is mainly by eignevector term}
% \end{section}






% \section{For Power-law eigenspectrum, $N$ itself, not $P/N$ governs eignevector delocalization}

% Assume $\tilde{\lambda}_i \sim i^{-1} \ , i=1,...P$. Let's focus on first few eigenvectors which will eventually govern CKA. For each P, put $\lambda_i = i^{-1}, i=1,...,P$. Then as we change $N$, we measure $Q_{ii}$, which is how much $i$-th sample eigenvector overlaps with $i$-th true eigenvector. On Fig.\ref{fig:SI_overlap_with_P}, we can see that for fixed $P$, increasing $N$ makes more and more sample eigenvector close to population one. Interestingly, fixing $N$ and increasing $P$ has much subtler effect than opposite at least for first few terms of $Q_{ii}$. 
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/hil_E_N20.png}
%         % \caption{$Q_{ii}$ for power-law eigenspectrum with $\tilde{\lambda_i} \sim i^{-1}$. For fixed $N$, changing $P$ has only subtle effect.}
%         \label{fig:SI_hilbert_transform}
%     \end{minipage}%
%     \hspace{0.04\textwidth}%
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/hil_E_N1000.png}
%         % \caption{Theoretical $Q_{ii}$ with approximate Eq.\ref{eq:approx Q_ii} when $P=100, \tilde{\lambda_i} = i^{-1.2}$}
%         % \label{fig:Q_ii_approx}
%     \end{minipage}
% \end{figure}
% We can see this from Eq.\ref{eq:SI.A_sample_true_overlap}. For first few components, we can show that $\mathfrak{h}_{\Sigma}(\lambda_i) \approx \frac{1}{\lambda_i}$.
%  Then, for diagonal elements of $Q$,  first term in denominator is much smaller than second term. In this case,
% \begin{align}
% \braket{u_i | \tilde{u_i}}^2 &= Q_{ii} \\
% &\approx \frac{1}{P}\frac{q\lambda_i \tilde{\lambda_i}}{[q\lambda_i \tilde{\lambda_i} \pi \rho(\lambda_i)]^2} \\
% &= \frac{N}{P^2\lambda_i \tilde{\lambda_i}} \frac{1}{[\pi \rho(\lambda_i)]^2} 
% \end{align}

% As we show in appendix, when population eigenvalue density was $\rho(\tilde{\lambda})=\gamma \tilde{\lambda}^{-1-\gamma}$ for $\gamma =1-\epsilon$ for small $\epsilon >0$, then we can approximate sample eigenvalue density $\rho({\lambda}) \approx \gamma c^{\gamma}{\lambda}^{-1-\gamma}$ for $\lambda \gg 1$, where $c=\frac{\sum_{i=1}^P i^{-1/\gamma}}{\sum_{i=1}^N i^{-1/\gamma}}$ Also $\lambda_i \approx c \tilde{\lambda_i}$ for first few terms. If we plug this approximation to our overlap formula, 
% \begin{align}
% \braket{u_i | \tilde{u_i}}^2 & \approx \frac{c N}{\pi^2 i^2} = \frac{\sum_{i=1}^P i^{-1/\gamma}}{\sum_{i=1}^N i^{-1/\gamma}}\frac{N}{\pi^2 i^2}
% \label{eq:approx Q_ii}
% \end{align}
% which only has subtle dependence on $P$, since both numerator and denominator of $c$ quickly converges to $\sum_{i=1}^{\infty} i^{-1/\gamma}$.
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/overlap_varying_N_varying_P.pdf}
%         \caption{$Q_{ii}$ for power-law eigenspectrum with $\tilde{\lambda_i} \sim i^{-1}$. For fixed $N$, changing $P$ has only subtle effect.}
%         \label{fig:SI_overlap_with_P}
%     \end{minipage}%
%     \hspace{0.04\textwidth}%
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figs/Q_ii_approx.png}
%         \caption{Theoretical $Q_{ii}$ with approximate Eq.\ref{eq:approx Q_ii} when $P=100, \tilde{\lambda_i} = i^{-1.2}$}
%         \label{fig:Q_ii_approx}
%     \end{minipage}
% \end{figure}

% \begin{subsection}{Alternative view: Infinitesimal Noise(just to share nothing conclusive)}
%     Here we show alternative view point of sample eigenvalue and eigenvectors, as perturbating populatian gram matrix. Recall that by assuming each neuron is sampled from population gram matrix $\tilde{\mathbf{\Sigma}}$, we modeld sample gram matrix as adding multiplicative noise $\mathbf{\Sigma} = \frac{1}{N} \sqrt{\tilde{\mathbf{\Sigma}}}\mathbf{W}_{P/N}\sqrt{\tilde{\mathbf{\Sigma}}}$, where $W$ stands for white wishart matrix. Now we can think of adding infinitesimal mulitplicative noise, known as Wishart Process\cite{bru1989diffusions}\cite{bru1991wishart}.

%     An analogous process for additive noise is known as dyson brownian motion, which is very similar to basic perturabtion theory for undegraduate quantum mechanics.
%     \begin{figure}
%         \centering
%         \includegraphics[width=0.4\linewidth]{figs/dyson_full.png}
%         \caption{Diffusion of eigenvalue and eigenvector in additive noise, $dB_{ij}$ are Brownian motion,  from \cite{Potters_Bouchaud_2020}}
%         \label{fig:dyson_full}
%     \end{figure}
%     This is quite intuitive, where adjacent eignevectors are mixed where level spacing $(\lambda_i - \lambda_j)^{-1}$ is governing. Eignevectors having closer eigenvalues are mixed more. In this sense, eigenvalue density itself governs mixing of eigenvector.

%     In the case of mulitplicative noise, we can also show stochastic diffential equation for eigenvalue and eignevectors, where level spacing also matters. Note that when we assume $\lambda_i = i^{-1}/\sum_{j=1}^P j^{-1}$, then level spcacing of first few terms doesn't change much for different $P$.(just to share another viewpoint haha)
% \end{subsection}

\section{Relation to regression based similarity measures}
\label{sec: appendix regression}
Regression Score is not an representational similarity measure but commonly used for scoring model closeness to brain \cite{schrimpf2018brain, canatar2024spectral}. Here, we discuss how our theoretical analysis for the overlap matrix $\mathbf{M}$ also can be applied to the regression setting. Regression score measures how well a model's activations $\mathbf{X}$ predict neural responses $\mathbf{Y}$ via a linear probe. Concretely, one performs ridge regression on a training subset $(\mathbf{X}_{1:p}, \mathbf{Y}_{1:p})$ of size $p<P$, obtaining:
\begin{align}
    &\hat{\mathbf{X}}(p) 
\;=\;
\mathbf{Y}\,\hat\beta(p). \\
&\hat\beta(p) 
\;=\; 
\arg\min_\beta 
\bigl\|
  \mathbf{Y}_{1:p}\beta 
  \;-\;
  \mathbf{X}_{1:p}
\bigr\|_F^2 
\;+\;
\alpha_{\mathrm{reg}}\|\beta\|_F^2,
\end{align}
Then regression score gives the neural prediction error, 
\begin{align}
E_g(p) 
\;=\; 
\frac{\|\hat{\mathbf{X}}(p) - \mathbf{X}\|_F^2}{\|\mathbf{X}\|_F^2},
\end{align}
Note that this error can be decomposed to each error mode, where $E_g(p) = \sum_i \widetilde W_i(p)$ where $\widetilde W_i(p) := \frac{\kappa^2}{1-\gamma}\frac{W_i}{(p\lambda_i + \kappa)^2} $.

The quantity $W_i$ denotes the projection of target labels on the $i^{\text{th}}$-model eigenvalue end hence can be expressed in terms of eigencomponents, $W_i = \sum_j \frac{\lambda_j}{\sum_k \lambda_k}M_{ij}$. However, calculating $W_i$ assumes that there is access to population level eigenvalues and poses a problem with limited data. In future work, we would like to test if our analyses help improve the reliability of regression based similarity methods.



\section{Theory of Power Law Spectrum}\label{sec:SI_power_law_theory}

Here, we consider the case where population spectrum obeys a power law:
\begin{align}
    \tilde\lambda_k = \lrpar{\frac{k}{P}}^{-s},\quad k= 1,\dots,P, \quad s > 1
\end{align}
where we normalized eigenvalue indices explicitly by $P$. For large $P$, the population density becomes:
\begin{align}
    \tilde\rho(\tilde\lambda) = \frac{1}{P}\sum_{k=1}^P  \delta(\tilde\lambda - \tilde\lambda_k) \sim \frac{1}{P}\int_1^P \delta(\tilde\lambda - \tilde\lambda_k) dk,
\end{align}
We change the variables to $\mu := \tilde\lambda_k$ for which we get:
\begin{align}
    d\mu = -s P^{s} k^{-s-1} dk = -\frac{s}{P}\, {\mu}^{1+1/s}\, dk.
\end{align}
In the limit $P\to\infty$, the density becomes
\begin{align}
    \tilde\rho(\tilde\lambda) = \frac{1}{s}\int_1^{\infty} \mu^{-1-1/s} \, \delta(\tilde\lambda - \mu) \, d\mu = \gamma \, {\tilde\lambda}^{-1-\gamma}, \quad \tilde\lambda \in [1, \infty], \quad \gamma = s^{-1},
\end{align}
where we defined $\gamma \in [0, 1]$ for notational convenience. Note that, in this definition, the expectation value of $\tilde\lambda$ diverges.

Next, we solve the self-consistent equation for the Stieltjes transform \eqref{eq:SI.A_self_consistent_equation} which reads:
\begin{align}
    \mathfrak{g}(z) = \int \frac{\tilde\rho(\tilde\lambda)}{z - \tilde\lambda(1 - q + q z \mathfrak{g}(z))}d\tilde\lambda = \frac{\kappa}{z}\int_1^\infty \frac{\gamma \tilde\lambda^{-1-\gamma}}{\kappa - \tilde\lambda}d\tilde\lambda,\quad \kappa := \frac{z}{z - \tilde\lambda(1 - q + q z \mathfrak{g}(z))}
\end{align}
where we defined $\kappa$ to simplify notation. This integral has an analytical solution expressed in terms of hypergeometric functions \cite{bahri2024explaining}:
\begin{align}
    \int_1^\infty \frac{ \tilde\lambda^{-1-\gamma}}{\kappa - \tilde\lambda}d\tilde\lambda = -\frac{1}{1+\gamma}\,{}_2 F_1(1, 1+\gamma, 2+\gamma, \kappa).
\end{align}
In order to solve the self-consistent equation analytically, we need to expand the ${}_2 F_1$ :
\begin{align}
    \int_1^\infty \frac{ \tilde\lambda^{-1-\gamma}}{\kappa - \tilde\lambda}d\tilde\lambda = -\pi  \kappa ^{-\gamma-1} (\cot (\pi  \gamma )-i) - \kappa^{-1}\sum_{n=0}^\infty \frac{1}{n-\gamma}\kappa^{-n}
\end{align}
which can be truncated. Here, we keep all terms and arrange the self-consistent equation in the form
\begin{align}\label{eq:SI_stieltjes_expansion}
    \frac{\mathfrak{g}'(z)}{\gamma} = -\pi  \kappa ^{-\gamma} (\cot (\pi  \gamma )-i) - \sum_{n=0}^\infty \frac{1}{n-\gamma}\kappa^{-n}, \quad \mathfrak{g}'(z) := z\mathfrak{g}(z)
\end{align}
where we defined $\mathfrak{g}'(z)$ in terms of which $\kappa$ becomes
\begin{align}
    \kappa = \frac{z}{z - \tilde\lambda(1+q(\mathfrak{g}'(z)-1))}.
\end{align}
We expand the r.h.s. of \eqref{eq:SI_stieltjes_expansion} in terms of $\mathfrak{g}'(z)$:
\begin{align}
    \frac{\mathfrak{g}'(z)}{\gamma} =& \frac{1}{\gamma }- \pi  (\cot (\pi  \gamma )-i)\left(\frac{z}{1-q}\right)^{-\gamma } - \sum_{n=1}^\infty \frac{1}{(n-\gamma )}\left(\frac{z}{1-q}\right)^{-n}\nonumber\\
    &- \mathfrak{g}'(z)\frac{q }{1-q}\left(\pi (\cot (\pi  \gamma )-i) \gamma\left(\frac{z}{1-q}\right)^{-\gamma }+\sum_{n=1}^\infty \frac{n}{(n-\gamma )}\left(\frac{z}{1-q}\right)^{-n}\right) + \cO(\mathfrak{g}'(z)^2).
\end{align}
In order to obtain an analytical solution, we must assume $\mathfrak{g}'(z) \gg 1$ and keep only the linear term above. Then, the solution to self-consistent equation becomes:
\begin{align}
    \mathfrak{g}(z) =& z^{-1} \frac{\frac{1}{\gamma }- \pi  (\cot (\pi  \gamma )-i)\left(\frac{z}{1-q}\right)^{-\gamma } - \sum_{n=1}^\infty \frac{1}{(n-\gamma )}\left(\frac{z}{1-q}\right)^{-n}}{\frac{1}{\gamma} + \frac{q}{1-q}\left(\pi (\cot (\pi  \gamma )-i) \gamma\left(\frac{z}{1-q}\right)^{-\gamma }+\sum_{n=1}^\infty \frac{n}{(n-\gamma )}\left(\frac{z}{1-q}\right)^{-n}\right)}.
\end{align}
Next, we compute the sample eigenvalue density $\rho(\lambda)$ and its Hilbert transform $\mathfrak{h}(\lambda)$ by computing
\begin{align}
    \lim_{\eta\to 0^+} \mathfrak{g}(\lambda-i\eta) = \mathfrak{h}(\lambda) + i\pi\rho(\lambda).
\end{align}
This is an extremely tedious calculation which we perform using Mathematica\footnote{We will provide the Mathematica file.}. Furthermore, we expand the results in $q$ and, assuming $q \ll 1$, keep only the linear term. In this regime, the leading order behavior of $\rho(\lambda)$ and $\mathfrak{h}(\lambda)$ looks like:
\begin{align}\label{sec:SI_density_hilbert_formula}
    \rho(\lambda) &= \gamma \lambda^{-1-\gamma}\lrpar{1 - q\gamma\lrpar{2\pi\gamma\cot(\pi\gamma)\lambda^{-\gamma} + \sum_{n=1}^\infty \frac{n+\gamma}{n-\gamma}\lambda^{-n}}} + \cO(q^2) \nonumber\\
    \mathfrak{h}(\lambda) &= \lambda^{-1}\lrpar{1-  \lambda ^{-\gamma }\pi  \gamma\cot (\pi  \gamma )-\lambda^{-1}\frac{\gamma }{1-\gamma}}\nonumber\\
    &+\pi  \gamma ^2 q \left(\pi  \gamma  \lambda ^{-2 \gamma -1} \left(\cot ^2(\pi  \gamma )-1\right)+\lambda ^{-\gamma -2}\frac{ (\gamma +1) \cot (\pi  \gamma )}{1-\gamma }\right) + \cO(q^2, \lambda^3).
\end{align}
Here, we did not include higher order terms for $\mathfrak{h}(\lambda)$ to avoid clutter.

Finally, we use the formula for estimating sample eigenvalues \eqref{sec:SI_eigenvalue_integral_formula} for which we obtain an explicit formula:
\begin{align}
    \mathfrak{F}(\lambda,q;\gamma):= \int_{\lambda}^\infty \rho(\lambda) d\lambda = \lambda^{-\gamma}\lrpar{1-q \gamma^2 \lrpar{\lambda^{-\gamma}\pi\cot{\pi\gamma} + \sum_{n=1}^\infty\frac{1}{n-\gamma}\lambda^{-n}}}.
\end{align}
Here, semi-column separates sample related arguments that we have access empirically ($\lambda_i$, $q$) and the only population related quantity $\gamma$. Hence, using the following relation \cite{ledoit2016numericalimplementationquestfunction, Bun_2017}
\begin{align}\label{eq:SI_estimating_eigenvalues}
    \mathfrak{F}(\lambda_i,q;\gamma) = \frac{i}{P}
\end{align}

we can either predict the shape of empirical eigenvalues given the decay rate of population spectrum (forward), or infer the population decay rate given the empirical observations of eigenvalues (backward). Finally, we numerically test our theory and obtain perfect agreement with empirical data in Fig.\ref{fig:SI_eigenvalue_prediction}.

\begin{figure*}[h]
    \centering
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=.99\linewidth]{figs/power_law_prediction_1.1.pdf}
    \end{minipage}\hfill%
    \begin{minipage}{0.47\textwidth}
        \centering
    \includegraphics[width=.99\linewidth]{figs/power_law_prediction_2.4.pdf}
    \end{minipage}
    \caption{For a population spectrum with $\tilde\lambda_k = k^{-1.1}$ (Left) and $\tilde\lambda_k = k^{-2.4}$ (Right), we show the spectra of the empirical eigenvalues for different $N$. Black solid line indicates the true eigenvalue decay. The numbers in parentheses in the legend indicate the inferred true decay rate from a population of $N$. In the regime $s<2$ ($\gamma > 0.5$), the empirical eigenvalues are always overestimated (Left), and in the regime $s\geq2$ ($\gamma < 0.5$) they are always underestimated (Right).}
\label{fig:SI_eigenvalue_prediction}
\end{figure*}

% \begin{section}{Kong \& Valiants Unbiased estimator of Moments}
%     Regarding estimating population CKA, there is well known method introduced by \cite{kong2017spectrumestimationsamples}. This shows algorithm to estimated moments regarding population gram matrix, from observation of moments of sample gram matrix. In this case, the numerator of CKA is already unbiased, and this method could be applied to denomiator. 

%     Although this method is proven to be unbiased, our method stands out in different viewpoint, that we are trying to estimate population CKA with infering population eigencomponents, which conveys much more moments that self-moment.
% \end{section}


% \begin{align}
%     \boxed{ \sum_{n=1}^{\infty}\frac{\gamma}{n-\gamma}\left(\frac{z}{1-q}\right)^{-n} =\frac{\gamma}{1-\gamma}\,\frac{1-q}{z}\;_2F_1\!\left(1,1-\gamma;2-\gamma;\frac{1-q}{z}\right). }
% \end{align}





% \begin{section}{Sample CKA can be flipped w.r.t to Population CKA}
%     Note that since $\mathbb{E}[\mathbf{M}] = \mathbf{Q}\mathbf{\tilde{M}}$, when $\mathbf{\tilde{M}}$ had more values on higher indices w.r.t to brain, than value will drop more quickly when we decrease $N$. This effect leads to although true CKA is bigger than other model, sample CKA is actually smaller.
%     \begin{figure}[htbp]
%         \centering
%         \includegraphics[width=0.7\linewidth]{figs/cka_flip.png}
%         % \caption{Caption}
%         \label{fig:SI_cka_flip}
%     \end{figure}
% \end{section}


\section{Experimental Details}\label{sec:SI_experimental_details}
Code for all experiments are provided with supplemental material.
% is publicly available at 
% \href{https://github.com/canatara/spectral_cka/tree/main/ICML_code}{https://github.com/canatara/spectral\_cka/tree/main/ICML\_code}.

\subsection{Synthetic Data}
For the synthetic experiments, we generate a population activation matrix in \(\mathbb{R}^{P \times \tilde{N}}\) whose Gram matrix follows a chosen spectral distribution (e.g., a power-law). We then form the sample activation matrix by projecting onto a random subset (or random linear subspace) of size \(N\), yielding \(\mathbb{R}^{P \times N}\). This procedure enables us to directly control the underlying population eigenvalues and eigenvectors, facilitating clean comparisons between sample-level and population-level similarity measures.

\subsection{Brain Data}
We employ a set of publicly available neural recordings from primate visual cortex (e.g., V2) and compare these against the representations of various vision models, similarly to the methodology in \cite{canatar2024spectral}. In total, we evaluate 32 models spanning supervised, self-supervised, and adversarially trained architectures, including well-known families such as ResNet, DenseNet, MobileNet, EfficientNet, and Vision Transformers. We extract intermediate-layer activations for each model on the same set of visual stimuli used in the neural recordings, applying the standard preprocessing routines (e.g., image resizing, ImageNet normalization).

Within each model, we select one or more representative layers (e.g., post-ReLU or transformer blocks). We then compute Gram matrices from those activations, matching the dimensionality of the neural dataset. In scenarios where the dataset contains more neurons than we wish to analyze, we project the data into a lower-dimensional subspace of size \(N\). Finally, we compute representational similarity (e.g., CKA or (SV)CCA) between these model-derived Gram matrices and the neural Gram matrices, both in their raw (sample) forms and using our denoising procedure for backward inference.
 


\section{Another denoising method: truncated inverse}
\label{sec:details of backward}
We utilize a truncated Singular Value Decomposition (SVD) to obtain a regularized estimate of \(\tilde{\mathbf{M}}\):

\begin{align}
    \tilde{\mathbf{M}} = \mathbf{V} \mathbf{\Sigma}^{-1}_{\text{trunc}} \mathbf{U}^\top \mathbf{M},
\end{align}

where \(\mathbf{Q}^{(x)} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top\) is the SVD of \(\mathbf{Q}^{(x)}\), and \(\mathbf{\Sigma}^{-1}_{\text{trunc}}\) is the truncated inverse of the singular values, defined as:

\begin{align}   \left(\mathbf{\Sigma}^{-1}_{\text{trunc}}\right)_{ii} = 
    \begin{cases}
        \frac{1}{\sigma_i} & \text{if } i \leq \tau, \\
        0 & \text{otherwise},
    \end{cases}
\end{align} 