\section{Related work}
Our work stands at the intersection of two lines of research: logical reasoning with synthetic simplified English data, and context length evaluation. Li et al., "Synthetic Data for Logical Reasoning" and Zhang et al., "Context Length Evaluation for NLP Models" explored the intersection of the two, but they imported the padding technique from previous work on LLM stress-texting, while we scale the dataset's original generation process instead of padding it with other text.
Human annotated long context benchmarks are very valuable but hard to annotate Wang et al., "Human Annotated Long Context Benchmarks", which causes current language models to saturate them.

\paragraph{Synthetic datasets for reasoning}

%\paragraph{Neuro-symbolic methods and externalization} Our work 

Numerous works investigate the logical capabilities of NLP models using textual datasets and symbolic reasoning Xu et al., "Logical Reasoning with Textual Datasets". We focus on the grammar-derived synthetic datasets. RuleTaker, LogicNLI , FLD,  FOL-NLI, address different subsets of first-order logic with English translations. Other works also explore non-standard logic with synthetic datasets, notably probabilistic Leitgeb et al., "Probabilistic Logics", paraconsistent Avron et al., "Paraconsistent Logics", epistemic Koons, "Epistemic Logics".

These approaches focus on input sizes typically suitable to a standard BERT encoder (<512 tokens). Here, we push the number of expressions in the input while avoiding paradoxes. This is related to the satisfiability problem which was explored by Zhang et al., "Satisfiability Checking for NLP" who use a solver to study the satisfiability in natural language using the Z3 solver and dedicated generation logic on constrained problems.  However, they also focus on relatively moderate text size while we use satisfiability checking as a stepping stone to generate large text and not only as a task in itself.

\paragraph{LLM context length stress tests}  Our work is also related to context window stress testing. The Long-Range Arena , provides the first systematic analysis of the long-range processing capabilities of text encoders, focusing mainly on algorithmic reasoning and retrieval tasks. Needle in Haystack benchmarks  test longer window sizes with simple retrieval tasks and use Paul Graham essays as padding. BABILong   uses bAbi  reasoning tasks and interleaves relevant text with irrelevant input from BookCorpus . FlenQA  applies a similar process to the RuleTaker  deductive logical reasoning task and uses Paul Graham essays as padding. Ruler uses simple algorithmic tasks like variable tracking and word counting. They also use Paul Graham essays as noise, or repetitions of sentences such as The sky is blue, following . %____ evaluates missing item prediction. % and also shows that the context window is shortened when the input is homogeneous, but they use a simplistic missing item prediction task.
The MuSR dataset uses GPT-4 generated  problems, which makes it hard to verify the problems' integrity at scale.