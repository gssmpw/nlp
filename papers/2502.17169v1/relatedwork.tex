\section{Related work}
Our work stands at the intersection of two lines of research: logical reasoning with synthetic simplified English data, and context length evaluation. \citet{levy2024same} and \citet{kuratov2024babilong} explored the intersection of the two, but they imported the padding technique from previous work on LLM stress-texting, while we scale the dataset's original generation process instead of padding it with other text.
Human annotated long context benchmarks are very valuable but hard to annotate \cite{bowman2022quality,wang2024novelqa,bai2024longbench}, which causes current language models to saturate them.

\paragraph{Synthetic datasets for reasoning}

%\paragraph{Neuro-symbolic methods and externalization} Our work \cite{olausson-etal-2023-linc}

Numerous works investigate the logical capabilities of NLP models using textual datasets and symbolic reasoning \cite{helwe2022logitorch}. We focus on the grammar-derived synthetic datasets. RuleTaker \citep{ruletaker} LogicNLI \citep{tian-etal-2021-diagnosing}, FLD \cite{fld23} and FOL-NLI \cite{sileo2024scaling} address different subsets of first-order logic with English translations. Other works also explore non-standard logic with synthetic datasets, notably probabilistic \citep{jin2023cladder,sileo2022probing}, paraconsistent \cite{kazemi2024boardgameqa}, epistemic \cite{sileo-lernould-2023-mindgames} logics.

These approaches focus on input sizes typically suitable to a standard BERT \cite{devlin2018bert} encoder (<512 tokens). Here, we push the number of expressions in the input while avoiding paradoxes. This is related to the satisfiability problem which was explored by \citet{fragmentsaaai,richardson2022pushing} who use a solver to study the satisfiability in natural language using the Z3 solver and dedicated generation logic on constrained problems.  However, they also focus on relatively moderate text size while we use satisfiability checking as a stepping stone to generate large text and not only as a task in itself.

\paragraph{LLM context length stress tests}  Our work is also related to context window stress testing. The Long-Range Arena \cite{tay2021long} provides the first systematic analysis of the long-range processing capabilities of text encoders, focusing mainly on algorithmic reasoning and retrieval tasks. Needle in Haystack benchmarks \cite{kamradt2023needlenih,li2024needlebench} test longer window sizes with simple retrieval tasks and use Paul Graham essays as padding. BABILong   \cite{kuratov2024babilong} uses bAbi \cite{babi} reasoning tasks and interleaves relevant text with irrelevant input from BookCorpus \cite{Zhu_2015_ICCV}. FlenQA \cite{levy2024same} applies a similar process to the RuleTaker \cite{ruletaker} deductive logical reasoning task and uses Paul Graham essays as padding. Ruler uses simple algorithmic tasks like variable tracking and word counting. They also use Paul Graham essays as noise, or repetitions of sentences such as \textit{The sky is blue}, following \citet{mohtashami2024random}. %\citet{sileo2024attention} evaluates missing item prediction. % and also shows that the context window is shortened when the input is homogeneous, but they use a simplistic missing item prediction task.
The MuSR dataset uses GPT-4 generated \cite{sprague2023musr} problems, which makes it hard to verify the problems' integrity at scale.