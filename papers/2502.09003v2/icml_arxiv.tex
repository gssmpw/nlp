%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\usepackage{subcaption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RoSTE: An Efficient QA-SFT Approach for LLMs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{stackengine}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{adjustbox}

\usepackage{array}
\usepackage{multirow}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


\def\prm{\mathbf{w}}
\def\loss{\widehat{\mathcal{L}}}
\def\gram{\mathbf{G}}
\def\model{\mathbf{m}}
\def\modelR{\mathbf{m}_{Q, \mathbf{R}}}
\def\lambmax{\lambda_{\rm max}}
\def\lambmin{\lambda_{\rm min}}
\def\a{\mathbf{a}}
\def\x{\mathbf{x}}
\def\X{\mathbf{X}}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\def\R{\mathbf{R}}
\def\exppythia{\texttt{Exp.1}}
\def\expllama{\texttt{Exp.2}}
\def\basecolor{\cellcolor{gray!25}}
\def\goodcolor{\cellcolor{green!35}}
\newcommand{\expec}[1]{\mathbb{E}\left[ {#1} \right]}
\newcommand{\dotp}[2]{\left\langle{#1}\ \middle|\ {#2}\right\rangle}
\DeclareMathOperator*{\argmin}{\arg\!\min}

% fix to ICML template (copied from stackexchange)
\newcommand{\alglinelabel}{%
  \addtocounter{ALC@line}{-1}% Reduce line counter by 1
  \refstepcounter{ALC@line}% Increment line counter with reference capability
  \label% Regular \label
}

\allowdisplaybreaks
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand*{\htwai}[1]{\textbf{\textcolor{purple}{To: #1}}}
\newcommand*{\mh}[1]{\textbf{\textcolor{red}{Mingyi: #1}}}
\newcommand*{\oscar}[1]{\textbf{\textcolor{cyan}{Oscar: #1}}}
\newcommand*{\htwaiupdate}[1]{{\textcolor{purple}{#1}}}
\newcommand*{\qw}[1]{\textbf{\textcolor{blue}{Quan: #1}}}
\newcommand*{\dk}[1]{\textbf{\textcolor{blue}{DK: #1}}}


\begin{document}

\twocolumn[
\icmltitle{RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Quan Wei}{equal,umn}
\icmlauthor{Chung-Yiu Yau}{equal,cuhk}
\icmlauthor{Hoi-To Wai}{cuhk}
\icmlauthor{Yang (Katie) Zhao}{umn}
\icmlauthor{Dongyeop Kang}{umncs}
\icmlauthor{Youngsuk Park}{aws}
\icmlauthor{Mingyi Hong}{umn}
\end{icmlauthorlist}

\icmlaffiliation{umn}{Department of Electrical and Computer Engineering, University of Minnesota, USA.}
\icmlaffiliation{umncs}{Department of Computer Science and Engineering, University of Minnesota, USA.}
\icmlaffiliation{cuhk}{Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong SAR of China.}
\icmlaffiliation{aws}{Amazon Web Services, USA}

\icmlcorrespondingauthor{}{}
% \icmlcorrespondingauthor{Hoi-To Wai}{htwai@se.cuhk.edu.hk}
% \icmlcorrespondingauthor{Mingyi Hong}{mhong@umn.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Supervised fine-tuning is a standard method for adapting pre-trained large language models (LLMs) to downstream tasks. Quantization has been recently studied as a post-training technique for efficient LLM deployment. To obtain quantized fine-tuned LLMs, conventional pipelines would first fine-tune the pre-trained models, followed by post-training quantization. This often yields suboptimal performance as it fails to leverage the synergy between fine-tuning and quantization. To effectively realize low-bit quantization of weights, activations, and KV caches in LLMs, we propose an algorithm named Rotated Straight-Through-Estimator (RoSTE), which combines quantization-aware supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that identifies an effective rotation configuration to reduce activation outliers. We provide theoretical insights on RoSTE by analyzing its prediction error when applied to an overparameterized least square quantized training problem. Our findings reveal that the prediction error is directly proportional to the quantization error of the converged weights, which can be effectively managed through an optimized rotation configuration. Experiments on Pythia, Qwen and Llama models of different sizes demonstrate the effectiveness of RoSTE. Compared to existing post-SFT quantization baselines, our method consistently achieves superior performances across various tasks and different LLM architectures. Our code is available at \url{https://github.com/Hong-Lab-UMN-ECE/RoSTE}.
\end{abstract} 

\begin{figure}[h] 
    \centering
    \includegraphics[width=0.75\linewidth]{fig/scatter_exp1.pdf} \\ \vspace{\baselineskip}
    \includegraphics[width=0.75\linewidth]{fig/scatter_exp2.pdf}
    \caption{RoSTE surpasses the performance of SOTA quantization methods on fine-tuning benchmark. Horizontal axis represents the total amount of hours needed to fine-tune pre-trained LLMs on a server of 8 $\times $ A100 NVIDIA GPUs.}  \vspace{-\baselineskip}
    \label{fig:acc-time-plot}
\end{figure}


\section{Introduction}

LLMs have shifted a significant step toward achieving artificial general intelligence \cite{bubeck2023sparks} and exhibit remarkable capabilities across different domains, including text generation \cite{anil2023palm,touvron2023llama,thoppilan2022lamda}, code generation \cite{chen2021evaluating,austin2021program,li2022competition}, and mathematical problem-solving \cite{cobbe2021training,trinh2024solving,wei2022chain,lewkowycz2022solving}. To adapt LLMs to various applications and scenarios, supervised fine-tuning (SFT) is a standard approach, enabling models to leverage diverse training data and align with specific tasks based on pre-trained models.

While fine-tuned models excel in domain-specific tasks, their substantial computational and storage demands present challenges for efficient deployment, particularly in resource-constrained environments \cite{xu2024survey}. To address these limitations, various model compression techniques have been developed, including quantization \cite{lin2023awq,frantar2022gptq}, pruning \cite{ma2023llm,sun2023simple}, distillation \cite{xu2024survey-kd}, and low-rank approximation \cite{wang2024svd,yuan2023asvd}. Among these, quantization is particularly effective for compressing LLMs, as it significantly reduces memory consumption, inference latency, and power usage. Additionally, its compatibility with specialized hardware accelerators enhances its practical deployment across diverse devices. Quantization techniques generally fall into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ is well-suited for quick deployment with minimal resources but often sacrifices accuracy in low-bit settings. In contrast, QAT achieves effective compression with minimal performance loss but requires retraining the entire LLM on a large corpus, incurring substantial computational costs.

For efficient deployment of task-specific LLMs, combining quantization with fine-tuning techniques offers a promising solution. A straightforward approach to obtaining quantized fine-tuned LLMs involves a two-step process: first fine-tune the pre-trained models, then apply quantization. However, applying quantization through PTQ in the second step often degrades the performance of the fine-tuned models, while QAT introduces an additional training phase, substantially increasing computational costs. Treating fine-tuning and quantization as separate steps can lead to suboptimal results, as it fails to exploit the synergy between these processes.

This work presents one of the first studies on \textit{quantization-aware fine-tuning (QA-SFT)} to obtain effective fine-tuned and quantized LLM as a single training phase. To maximize the hardware capability of modern GPUs, we concentrate on designs utilizing the 4-bit quantization of weights, activations, and KV cache in LLMs. However, low-bit quantization presents a major challenge: performance degradation due to the weight and activation outliers, which expand the quantization range and increase the quantization error. Notice that even the high-performance data-free QAT methods \cite{liu2023llm} fail with 4-bit activation quantization.

The first key aspect of our work is to leverage rotation-based quantization in QA-SFT. Our work is inspired by recent findings on rotation-based PTQ methods \cite{ashkboos2024quarot,liu2024spinquant}, which demonstrate that applying offline and online rotations to linear projection layers and KV caches in LLMs effectively mitigates weight and activation outliers in post-trained models. However, when directly applied to QA-SFT, these PTQ methods fail to prevent outliers from re-emerging within layers during fine-tuning, resulting in performance degradation.
To address this, we propose a joint training method combining the adaptive selection of rotation matrices and QA-SFT. The second key aspect of our work is to utilize a bilevel optimization formulation that simultaneously tackles QA-SFT and selects the rotation matrices based on the weights.

This paper proposes the Rotated Straight-Through-Estimator (RoSTE) algorithm that integrates the aforementioned ingredients. Our contributions are summarized as:
\begin{itemize}[itemsep=0mm, topsep=0mm, leftmargin=*]
    \item We introduce a novel SFT training problem that directly optimizes quantized weights and rotation matrices within a single model architecture. To tame the non-smooth manifold optimization, we propose a bilevel optimization formulation, where \emph{upper level subproblem} optimizes weight matrices, while \emph{lower level subproblem} employs a surrogate loss to guide the selection of rotation matrix. 
    \item To tackle the bilevel QA-SFT optimization, we propose the RoSTE algorithm which alternates between (i) a QAT subroutine incorporating a rotation-enabled straight-through-estimator (STE) update, and (ii) a low complexity heuristic for selecting rotation matrices based on the random Walsh-Hadamard matrix.
    \item We provide a theoretical analysis of the benefits of rotation-enabled quantization in QA-SFT by examining the prediction error resulted from the QAT stage of RoSTE. This analysis directly motivates the use of quantization error based surrogate loss and justifies the adoption of the low-complexity Walsh-Hadamard rotation.
\end{itemize}
We conduct experiments on fine-tuning Pythia, Qwen and Llama models, demonstrating the effectiveness of RoSTE. An accuracy-vs-training-time plot in Figure \ref{fig:acc-time-plot} illustrates the practical benefit of RoSTE for obtaining a fine-tuned quantized LLM. {Overall, we believe this is the first work that develops an efficient quantization algorithm for the SFT process, with theoretical justification and practical effectiveness.}




\subsection{Related Works}
The quest on quantizing modern LLMs for efficient deployment started with weight-only quantization. GPTQ \cite{frantar2022gptq} stood out as a robust postprocessing algorithm that matches the layer output between a quantized model and a full-precision target model. Soon after, a new trend turned to tackling outlier values in weight matrices and activations due to their incompatibility with quantization \cite{lin2023awq,lee2024owq,chee2024quip,tseng2024quip}, pushing the limit of accurately quantized models below 2-bits.
While the memory consumption of storing the model parameters is reduced by weight-only quantization, their activations remain in full precision during inference which prohibits the application of long context LLMs on consumer-grade accelerators with limited memory storage.

This motivates the development of weight-activation quantization. It allows weights and activations to be directly multiplied using discrete arithmetic units that accelerate inference and reduce the inference memory requirement.
Existing methods can be categorized as follows: 
(1) mixed-precision quantization  \cite{dettmers2022gpt3,zhao2024atom} that assigns extra bit-widths to outlier values;
(2) scaling-based quantization \cite{xiao2022smoothquant,shao2023omniquant} that employs scaling to balance the representation range between activations and weights;
(3) rotated quantization \cite{ashkboos2024quarot,liu2024spinquant} that utilizes orthogonal transformation to remove activation outliers; 
(4) knowledge distillation \cite{liu2023llm,du2024bitdistiller,xu2024onebit} that re-trains a quantized model to match the behavior of a full-precision target model.
Among these methods, rotated quantization methods demonstrate superior performance in 4-bit weight-activation quantization.



\section{Preliminary} \label{sec:prelim}

This section introduces ingredients that are essential to the proposed {RoSTE} algorithm through overviewing two major approaches for achieving efficient quantized LLMs.

\subsection{Post-Training Quantization (PTQ)} \label{sec:quant}
The main objective of post-training quantization is to find a quantized model that preserves the behavior of the original model. While sophisticated quantizer designs such as vector quantization  \cite{tseng2024quip,egiazarian2024extreme} can maintain a rich representation of weight values using $\leq 2$ bits on average, most existing works are limited to weight-only quantization. In contrast, for computationally efficient designs with quantized weights and activation, we focus on uniform quantization that compresses a full-precision tensor into one floating point scaling factor and a set of bit-width limited integers. This scheme is known for its practical efficiency across different modern hardware \cite{jacob2018quantization,ashkboos2024quarot}. 

Formally, the $b$-bits \textit{uniform quantizer} can be expressed as
\begin{equation} \label{eq:def_quant}
Q(\X) = \Bigg( \underbrace{ {\rm clamp}_b\left( \left\lfloor \frac{\X}{s(\X)}  \right\rceil \oplus z(\X) \right) }_{\text{$b$-bits integer tensor}} \ominus ~z(\X) \Bigg) s(\X)
\end{equation}
where $\X$ is a high-precision floating-point tensor; $\lfloor \cdot \rceil$ denotes an element-wise nearest rounding; ${\rm clamp}_b(\cdot)$ projects the values to the range of $b$-bits representable integers; $\oplus, \ominus$ represent element-wise addition/subtraction between tensor and scalar. The choice of scaling $s(\X) \in \mathbb{R}$ and shifting $z(\X) \in \mathbb{Z}$ determines the range of which the $b$-bits integer tensor represents. For \textit{symmetric} quantization, we adopt
\begin{equation} \label{eq:sym-q-a}
    s(\X) = \frac{\max(|\X_{:}|)}{2^{b-1}-1} \, c,\quad z(\X) =0,
\end{equation}
\begin{equation} \label{eq:sym-q-b}
    {\rm clamp}_b(\X) = \max\{ -2^{b-1}, \min\{ \X, 2^{b-1}-1 \}  \}
\end{equation}
with $c \in (0,1]$ a \textit{clipping factor} used to scale down the representation range so as to mitigate the impact of outlier values. To take advantage of the representation range in tensor with value distribution skewed away from 0, we can adopt \textit{asymmetric} quantization by choosing
\begin{equation*}
    s(\X) = \frac{\max(\X_:) - \min(\X_:)}{2^{b}-1} \, c, ~ z(\X) = \left\lfloor \frac{-\min(\X)}{s(\X) } \right\rceil,
\end{equation*}
\begin{equation} \label{eq:asym_quant}
 {\rm clamp}_b(\X) =\max\{ 0, \min\{ \X, 2^{b}-1 \}  \} 
\end{equation}
% \mh{'our' sounds we proposed.} 
The above uniform quantization scheme reduces the memory consumption from storing a $d$-elements 32-bit floating-point tensor $\X$ using $32d$ bits, to storing an integer tensor with its shifting and scaling scalars using $bd + b + 32$ bits.

In practice, we partition a tensor into quantization groups such that each group has its own scaling and shifting $s(\X), z(\X)$. Further description of quantizer hyperparameters used in our work will be provided in Appendix \ref{app:implem}.


\noindent {\bf Incoherence Processing via Rotation.}  The precision of uniform quantization degrades as the representation range increases, especially when there are outlier values in the full-precision tensor. To reduce the effects of outlier values, incoherence processing was proposed in \cite{chee2024quip} which pre-multiplies an orthogonal matrix to the full-precision tensor prior to quantization, and post-multiplies the transposed orthogonal matrix to recover the original tensor.
Later in \cite{ashkboos2024quarot,tseng2024quip}, incoherence processing by Walsh-Hadamard rotation is shown to be effective in both uniform quantization and vector quantization for 4-bits weight-activation quantization in LLMs.

We illustrate the idea of incoherence processing by constructing a multi-layer feedforward neural network with activation quantizer $Q_x$ and weight quantizer $Q_w$. The output of the $i$-th linear layer is given by
\begin{equation} \label{eq:def_quant_lin}
    {\tt LIN}_{i}(\X; {\bf W}_i^\star, \R_i) = \sigma( Q_x( \X \R_i)  Q_w(\R_i^\top {\bf W}_i^\star ) )
\end{equation}
where $\X$ is the input activation; ${\bf W}_i^\star$ is the pre-trained weight matrix;  $\R_i$ denotes rotation matrix and $\sigma$ is any activation function.  Notice that as $\R_i \R_i^\top = {\bf I}$, the architecture in \eqref{eq:def_quant_lin} is \emph{invariant} to any choice of rotation matrix $\R_i$ when both quantizers $Q_w, Q_x$ are error-free, i.e., $Q_w(\cdot), Q_x(\cdot)$ are the identity map.
In general when $Q(x) \neq x$,  it has been observed that these rotation matrices suppressed outliers within each quantizer and preserved the pre-trained model behavior during inference.
On the downside, they impose extra memory and computation overhead since rotation is performed during inference within the activation quantizer. Thankfully, these overheads do not counteract the benefits of incoherence processing due to the fast Hadamard CUDA kernels \cite{ashkboos2024quarot,tseng2024quip}.


\subsection{Quantization-Aware Training (QAT)}

The main objective of quantization-aware training (QAT) is to directly optimize a quantized neural network using gradient-based methods. From an optimization perspective, this is challenging as the quantization operator is not differentiable. To this regard, we focus on a popular remedy that has been studied before the era of LLMs is the straight-through estimator (STE) \cite{courbariaux2015binaryconnect,bai2018proxquant} which approximates the Jacobian of quantizer by the identity matrix. During the backward calculation, the derivative of quantizer $Q$ in the chain rule is replaced by
\begin{equation}
    \frac{\partial Q(g(\X))}{\partial \X} \approx \frac{\partial g(\X)}{\partial \X} , \label{eq:ste}
\end{equation}
for any differentiable function $g$. This approximation utilizes the insight that a quantizer behaves like an identity function in low resolution, while tolerating a gradient bias since quantization error persists in high resolution. In practice, STE is known to work well in training quantized neural network models \cite{li2017training,yin2019understanding} as well as LLMs \cite{liu2023llm,panferov2025quest}.


These QAT techniques are useful for the sceneario when we consider obtaining a quantized LLM that minimizes the \emph{fine-tuning} objective, as introduced below.

\subsection{Supervised Fine-Tuning (SFT)}

Foundation models that were pre-trained on large unstructured text corpus require fine-tuning to adapt their output behavior for specialized applications such as coding assistants \cite{chen2021evaluating} and instruction-following conversational chatbot \cite{ouyang2022training}. 
Towards this goal, Supervised fine-tuning (SFT) resumes the training of a given (pre-trained) model with the data distribution replaced by an application-specific curated dataset \citep{chung2024scaling}.

In specific, let $ \mathcal{D} := \{(\x_i, \y_i)\}_{i=1}^N $ denote the SFT dataset with $N$ samples. For each $i \in [N]$, $\x_i \in {\cal X}$ is a sequence of input prompt and $ \y_i = (y_{i,0}, \dots, y_{i,T-1})$ with $y_{i,t} \in {\cal Y}$ is a sequence of preferred output tokens. To fine-tune the model with ${\cal D}$, we consider minimizing the following SFT loss:
\begin{align}
\hspace{-0.2cm} \mathcal{L}_{\text{SFT}} (\model(\cdot) ) 
:= \mathbb{E}_i \left[ - 
\sum_{t=0}^{T-1} \log {\rm P}(y_{i,t} | \x_i, y_{i,<t}; \model(\cdot) ) \right]
\label{eq:qa-sft}
\end{align}
where the expectation is taken w.r.t.~$i \in \{0,\ldots,N-1\}$ with a uniform distribution, $ y_{i,<t} $ denotes the sequence of tokens preceding $ y_{i,t} $. The likelihood ${\rm P}(y_{i,t} | \x_i, y_{i,<t}; \model) $ is the probability of the target token $ y_{i,t} $ given the input $ \x_i $ and prior context $y_{i,<t}$, as predicted by the model $\model$. 

Although it is a natural idea to apply QAT on fine-tuning tasks, existing works such as \citep{dettmers2023qlora, xu2023qa} (also see \citep{lee2024improving, bondarenko2024low} for similar ideas applied to training LLMs) only considered quantization aware adaptation utilizing an additional LoRA architecture. Moreover, they focused on direct quantization without incoherence processing whose performance can be sensitive to outliers. 
This motivates us to consider integrating QAT with incoherence processing for SFT.



\section{Proposed Algorithm: RoSTE} 
This section presents the Rotated Straight-Through-Estimator (RoSTE) algorithm through jointly optimizing the rotated quantizer and model parameters. 
To fix the idea, we parameterize the quantized LLM by the weight matrices $\{ {\bf W}_i \}_{i=0}^{\ell - 1}$ and rotation matrices $\{ \R_i \}_{i=0}^{\ell - 1}$. Consider the abstraction of an LLM with $\ell$ layers/modules as $\model_Q: \overline{\cal X} \to \mathbb{R}^{|{\cal T}|}$, where $\overline{\cal X}$ is the set of sequences with variable context length, ${\cal T}$ is the set of vocabulary, we denote
\begin{align}
    &\model_Q( \overline{\x}; \{ {\bf W}_i, \R_i \}_{i=0}^{\ell - 1}) := {\tt NN}( \overline{\x} ; \{ {\tt LIN}_i(\cdot; {\bf W}_i, {\bf R}_i ) \}_{i=0}^{\ell-1} ), 
    \notag
\end{align}
for any  $\overline{\x} \in \overline{\cal X}$,
where ${\tt LIN}$ was defined in \eqref{eq:def_quant_lin}, and ${\tt NN}$ denotes the neural network architecture such as transformers\footnote{For simplicity, we excluded the self-attention layers from our notation but the same idea applies to all the layers in the transformer architecture, as demonstrated in \cite{liu2024spinquant}. See Appendix~\ref{app:implem} for the implementation details.}. 

With the above parameterization, an ideal strategy is to consider the optimization problem: 
\begin{align}
    \min_{ \{ {\bf W}_i, \R_i \}_{i=0}^{\ell - 1} }~~ & {\cal L}_{\rm SFT} \big( \model_Q( \, \cdot \, ; \{ {\bf W}_i, \R_i \}_{i=0}^{\ell - 1} ) \big) \label{eq:joint-train-ideal} \\
    {\rm s.t.}~~ & \R_i \R_i^\top = {\bf I},~i=0,\ldots,\ell-1, \notag
\end{align}
which {\it simultaneously} selects the weight and rotation matrices under quantization and directly optimizes the SFT objective\footnote{Our approach can be applied on other fine-tuning objectives as well, e.g., \citep{chen2024self, li2024getting}.} of the quantized-and-rotated model. 


\begin{algorithm}[t] 
\caption{RoSTE Algorithm} 
\begin{algorithmic}[1]\label{alg:roste}
    \STATE {\bf Input:} Pre-trained model parameters $\{ {\bf W}^{\rm pt}_i \}_{i = 0}^{\ell - 1}$, step size $\eta > 0$. 
    \STATE Initialize ${\cal W}^0 = \{ {\bf W}^{\rm pt}_i \}_{i = 0}^{\ell - 1}$.
    \FOR{$k = 0,\ldots,K-1$}
        \STATE \texttt{/* Rotation configuration */}
        \STATE Find an approximate lower level solution \alglinelabel{line:ll}
        \begin{equation} \label{eq:bin_ll}
            {\cal R}^k = \argmin_{\R_0, ..., \R_{\ell-1} \in \{ {\bf H}, {\bf I}\} } {\cal E}( {\cal W}^{kT}, \{  \R_i\}_{i=0}^{\ell -1 }), 
        \end{equation} 
        where ${\bf H}$ is a random Walsh-Hadamard matrix generated according to \eqref{eq:gen_random_had}.
        \STATE \texttt{/* QAT Stage via STE */}
        \FOR{$t = 0,\ldots,T-1$}  \alglinelabel{line:ul-start}
            \STATE Draw a mini-batch of training samples $\xi^{kT+t} \subseteq \{ 0, ..., N-1\}$ uniformly at random and update 
            \begin{align}
                &{\cal W}^{kT+t+1} = {\cal W}^{kT+t} \\
                &\quad- \eta \stackon{$\nabla$}{{\scriptsize s.t.e.}}_{{\cal W}} {\cal L}_{\rm SFT}(\model_Q(\cdot; {\cal W}^{kT+t}, {\cal R}^k); \xi^{kT+t}) \notag
            \end{align} 
            \vspace{-0.5cm}  \alglinelabel{line:ste}
        \ENDFOR \alglinelabel{line:ul-end}
    \ENDFOR 
    \STATE {\bf Output:} Quantized fine-tuned $\model_Q(~\cdot~; {\cal W}^{KT}, {\cal R}^{K-1})$.
\end{algorithmic}
\end{algorithm}

However, tackling \eqref{eq:joint-train-ideal} can be challenging even with approaches such as alternating optimization. 
This is because, upon fixing the weight matrices, minimizing the objective function w.r.t.~the rotation matrices $\{ \R_i \}_{i=0}^{\ell-1}$ involves a non-tractable manifold optimization while the objective function is non-differentiable due to quantization.
Meanwhile, when the rotation matrices are fixed, the minimization problem w.r.t.~the weight matrices $\{ {\bf W}_i \}_{i=0}^{\ell-1}$ is similar to standard QAT; see \citep{liu2023llm}. 

The above obstacle motivated us to consider an alternative formulation (albeit somewhat {heuristic}) that simplifies the search for $\{ \R_i \}_{i=0}^{\ell-1}$ adapted to the weight matrices. This formulation explicitly separates the process of (quantized) model training and the rotation matrix optimization, and leverages a simpler loss function for the rotation matrix design. More specifically, we consider:
\begin{align}
    \min_{\{ {\bf W}_i \}_{i=0}^{\ell - 1}} ~ & {\cal L}_{\rm SFT}(\model_Q(~\cdot~; \{ {\bf W}_i , \R^\star_i \}_{i=0}^{\ell - 1})) \label{eq:bilevel-form} \\
    {\rm s.t.} ~ & \textstyle \{\R^\star_i\}_{i=0}^{\ell -1} \in \argmin_{\{\R_i\}_{i=0}^{\ell -1}} ~ {\cal E}(\{ {\bf W}_i , \R_i\}_{i=0}^{\ell -1 }) \notag \\
    & \qquad \qquad \quad  {\rm s.t.}~\R_i \R_i^\top = {\bf I},~i=0,\ldots, \ell-1, \notag
\end{align}
which is a {bilevel} optimization problem
where the lower level optimal rotation matrices $\{ \R_i^\star \}_{i=0}^{\ell - 1}$ minimize the weight-activation quantization error:\vspace{-.1cm}
\begin{align}
    {\cal E}(\{ {\bf W}_i , \R_i \}_{i=0}^{\ell - 1}) & = \sum_{i=0}^{\ell - 1} \| Q_w(\R_i^\top {\bf W}_i) - \R_i^\top {\bf W}_i \|^2 \label{eq:quant-error} \\
    & + \frac{1}{n} \sum_{i=0}^{\ell-1} \sum_{j=0}^{n-1}\| Q_x(\X_{i,j} \R_i) - \X_{i,j} \R_i \|^2 , \notag \vspace{-.1cm}
\end{align}
for $\X_{i,j}$ representing the input activation of layer $i$ on the $j$-th calibration data sample, e.g., by drawing a subset of size $n$ from ${\cal D}$ the fine-tuning dataset.

Notice that now the optimal lower level variable aims at \emph{assisting} the upper level weights so that an STE gradient approximation on ${\cal L}_{\rm SFT}$ w.r.t. $\{{\bf W}_i\}_{i=0}^{\ell -1}$ has a smaller bias. However, it remains challenging for us to access the optimal rotation matrices for every iteration of the upper level minimization as solving the lower level problem can still be computationally expensive. In this regard, we propose a {\it lazy} lower level approximation where the rotation matrices are updated after $T$ iterations of optimizing the weight matrices.


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{fig/roste-flow-chart.pdf}\vspace{-.3cm}
    \caption{The RoSTE algorithm alternates between tackling the lower level problem for rotation configuration and the upper level problem of SFT training using rotation-aware STE.}\vspace{-.4cm}
    \label{fig:roste}
\end{figure}

The RoSTE algorithm is now summarized in Algorithm~\ref{alg:roste} and Fig.~\ref{fig:roste}.
The algorithm is akin to alternating optimization and consists of two parts. The first part {(cf.~line~\ref{line:ul-start}--\ref{line:ul-end})} pertains to the QAT stage with SFT objective for selecting the weight matrices $\{ {\bf W}_i \}_{i=0}^{\ell-1}$ under the {rotation matrices}. Notice that STE gradient can be computed efficiently when $\{ \R_i \}$ are chosen as the Walsh-Hadamard matrices; also see \eqref{eq:ste-toy}.
{To our best knowledge, the subroutine used in this stage is the first direct application of STE to SFT training as well as its adaptation to the possibly varying rotation matrices.}
The second part {(cf.~line~\ref{line:ll})} pertains to the selection of rotation matrices in the lower level optimization which is a non-smooth problem on the manifold. Compared to \eqref{eq:joint-train-ideal}, its objective function can be easily computed as the latter only represents the quantization error. 
Furthermore, as we will show in Sec.~\ref{sub:theory}, the random Walsh-Hadamard matrix ${\bf H}$ yields an approximate-but-universal solution to minimize ${\cal E}(\cdot)$. As such, we propose to approximate the subproblem by limiting the search space to $\R_i \in \{ {\bf H}, {\bf I} \}$ using a random Hadamard matrix ${\bf H}$ \citep{tseng2024quip} and perform a (low-complexity) combinatorial search to obtain an approximate lower level solution that decides if the rotation matrix should be applied on each layer. Details of this heuristic implementation can be found in Appendix~\ref{app:implem}.



\section{Theoretical Insights of RoSTE} \label{sub:theory}

This section aims at providing theoretical insights on the RoSTE algorithm that tackles the bilevel problem \eqref{eq:bilevel-form}. In particular, we show that the quantization error \eqref{eq:quant-error} is a reasonable surrogate loss for optimizing the rotation matrices, provided that the weight matrices are optimized using the STE method as in Algorithm~\ref{alg:roste}. Notice that the SFT objective is complicated and possibly untractable for analysis. To concentrate on the insights pertaining to using rotation in the quantized LLMs, we shall introduce a few approximations. 
We will use $\dotp{\cdot}{\cdot}$ to denote inner products of vectors, and $\| \x \|_{\bf K}^2 = \dotp{\x}{{\bf K}\x}$ to denote a ${\bf K}$-weighted squared norm of vector $\x$ for any square matrix ${\bf K}$.

Our setup follows from the literature on analyzing the convergence of SGD for neural networks under the interpolation regime \citep{ma2018power, vaswani2019painless}. 
To describe it, let us fix the rotation matrices $\{ \R_i \}_{i=0}^{\ell-1}$ and consider the QAT stage (cf.~line~\ref{line:ul-start}--\ref{line:ul-end}) in the RoSTE algorithm. Instead of analyzing ${\cal L}_{\rm SFT} ( \model_Q(\cdot) )$ directly, we consider the quadratic loss function as a simplified objective to draw insights for RoSTE. Moreover, the training dataset consists of samples $(\x_\xi, {\bf y_\xi})$ with a single output token in $\mathbb{R}$ such that $\y_\xi \in {\cal Y} \equiv \mathbb{R}$.
For any $\model: \overline{\cal X} \to \mathbb{R}^{|{\cal T}|}$, we now consider the {squared prediction error}:
\begin{equation} \label{eq:quadratic-loss}
    \widehat{\cal L} ( \model(\cdot) ) := \frac{1}{2} \mathbb{E}_\xi \left[ ( o(\model(\x_\xi))  - {\bf y}_{\xi} )^2 \right],
\end{equation}
in lieu of ${\cal L}_{\rm SFT}(\cdot)$,
where $o: \mathbb{R}^{|{\cal T}|} \to {\cal Y}$ maps the {probability distribution over ${\cal T}$ to a token}.

We further assume that the composite map $o(\model_Q(\cdot))$ is a linear quantized model given by
\begin{equation} \label{eq:modelr}
    o( \model_Q(\x; \prm, \R ) ) = \dotp{Q_x(\R\x)}{Q_w(\R \prm)}, 
\end{equation}
where $\R$ is a rotation matrix satisfying $\R \R^\top = {\bf I}$ and $Q_x, Q_w: \mathbb{R}^d \rightarrow \mathbb{R}^d$ are the quantization functions [see Sec.~\ref{sec:quant}]. 
Let $\x_t, \y_t$ be the sample drawn at iteration $t$ in the inner loop update of line~\ref{line:ste}, Algorithm~\ref{alg:roste}, we have
\begin{align}
    \prm^{t+1} & = \prm^t - \eta \, {\bm g}^t_{\rm s.t.e.} \label{eq:ste-toy} \\
    {\bm g}^t_{\rm s.t.e.}  & = ( \dotp{Q_x(\R \x_t )}{ Q_w(\R \prm^t) } - {\bf y}_t )\R^\top Q_x(\R \x_t), \notag
\end{align}
where $\eta>0$ is the step size and we have used the STE approximation $\partial(Q_w(\R \prm)) / \partial (\prm) \approx \R$ when computing the stochastic gradient ${\bm g}^t_{\rm s.t.e.}$ at $\prm^t$.

Our next endeavor is to study an upper bound on the loss value of quantized model, $\widehat{\cal L} ( \model_Q( \, \cdot \, ; \prm^T, \R ) )$, after running the recursion \eqref{eq:ste-toy} for $T \geq 1$ steps. 
Define the Gram matrix of the quantized-rotated features by 
\begin{equation}
    \gram := \expec{Q_x(\R \x_\xi) Q_x(\R \x_\xi)^\top}
\end{equation}
and make the following assumptions accordingly:
\begin{assumption}[Gram Matrix] \label{assm:gram}
    There exists constants $\lambmin, \rho > 0$ such that
    \begin{equation} \label{eq:gram_lb}
        \gram^2 \succeq \lambmin \gram ,~~
    \textstyle \sup_{0 \leq t \leq T-1} \| Q_x(\R \x_t) \|_{\gram}^2 \leq \rho.
    \end{equation}
\end{assumption}
The above conditions are mild as $\lambmin$ is only the smallest non-zero eigenvalue of the Gram matrix $\gram$ and $\rho$ exists when the input prompts $\x_t$ are bounded. 
\begin{assumption}[Interpolation] \label{assm:interpolation}
    For any orthogonal matrix $\R$, there exists $\prm^\star_{\R} \in \mathbb{R}^d$ such that ${\bf y}_\xi = \dotp{Q_x(\R \x_\xi)}{\prm^\star_{\R}}$ for any $\xi$.
\end{assumption}
The above assumption requires that the quantized-rotated features $(\x_\xi, \y_\xi)$ are interpolatable by a full-precision model $\prm^\star_{\R}$. This assumption is closely related to the standard interpolation assumption that appeared in the literature on training with over-parameterized models \cite{ma2018power,vaswani2019painless}. It is worth noticing that Assumption \ref{assm:interpolation} does not require the interpolator $\prm_\R^\star$ to be in the quantized model parameter space \eqref{eq:modelr}. 

Define the shorthand notation $\modelR^t := \model_Q( \cdot; \prm^t, \R)$, we observe the following convergence results for the QAT stage during the RoSTE algorithm:
\begin{theorem} \label{thm:main}
    Under Assumptions \ref{assm:gram}, \ref{assm:interpolation} and the step size $\eta = \lambmin / (6 \rho)$, the objective value of the quantized model produced by the recursion \eqref{eq:ste-toy} is bounded by
    \begin{align}
    & \mathbb{E} [ \loss(\modelR^{t+1}) ] \leq \left(1- \mu \right)^{t+1} \loss(\modelR^{0}) \label{eq:convergence_bound} \\
    &\quad \textstyle + (6 + 2\mu^{-1} ) \sum_{s=0}^{t+1} \left(1- \mu \right)^{t-s} \expec{\| \e(\R \prm^{s}) \|_{\gram}^2} \notag
    \end{align}
    for any $t \geq 0$,
    where $\mu = \frac{\lambmin^2}{12\rho}$ and ${\bf e}(\x) = Q_w(\x) - \x$.
\end{theorem}
% \oscar{I think the definition of $\loss$ already included $\mathbb{E}$ so we dont need $\mathbb{E}$ in \eqref{eq:convergence_bound}?} \htwai{No. The definition of $\loss$ does not include  the expectation w.r.t. randomness in SGD.}
See Appendix \ref{proof:main} for the proof.
Our result shows that STE only converges to an inexact solution, which is consistent with previous findings on STE training.
For instance, when training models with activation-only quantization, \citep[Lemma 10]{yin2019understanding} proved that the STE gradient is non-vanishing near local minima. For models with weight-only quantization, \citep[Corollary 1]{li2017training} only showed a convergence guarantee for the full-precision weights but not the quantized weights.
In comparison to the prior findings, our result demonstrates the convergence of prediction error with quantized model. 

Suppose that the QAT stage of RoSTE is run with $T \gg 1$ inner-loop iterations. Applying the theorem shows that given $\overline{\R}$, the resultant prediction error of the intermediate model $\prm^T$ will be bounded by ${\cal O} (\sum_{s=0}^{T} \left(1- \mu \right)^{T-s} \expec{\| Q_w( \overline{\R} \prm^{s}) - \overline{\R} \prm^{s} \|_{\gram}^2} )$, i.e., a weighted sum of the weight quantization errors during the QAT process. Due to the exponential weighting $\left(1- \mu \right)^{T-s}$, the prediction error is dominated by the weight quantization error of recent iterates.
Crucially, the above analysis shows that the rotation matrices play a pivoting role in the performance of QAT. This inspires us to apply ${\cal E}(\cdot)$ in \eqref{eq:quant-error} to guide us in the selection for optimal rotation matrices. 

{\bf Randomized Rotation Matrices.}
Now as we demonstrated that the quantization error is crucial to the prediction performance with the quantized model, we turn our focus to tackling the lower-level subproblem in \eqref{eq:bilevel-form}. Notice that minimizing ${\cal E}(\cdot)$ w.r.t.~the rotation matrix remains challenging.
Instead of directly tackling the manifold optimization, our strategy is to apply the random Walsh-Hadamard matrix \citep{tseng2024quip} design as an approximate-yet-universal solution. Consider the random rotation matrix:
\begin{equation} \label{eq:gen_random_had}
    \R(\zeta) = {\bf H} {\rm Diag}({\bf r}(\zeta))
\end{equation}
where ${\bf H} \in \mathbb{R}^{d \times d}$ is a Walsh-Hadamard matrix \citep{fino1976unified} and ${\bf r}(\zeta) \in \{-1, 1\}^d$ is a random sign vector.
{Notice that $\R(\zeta)$ is a binary matrix which favors efficient implementation on GPUs.}

We observe the following proposition adapted from \protect{\citep[Lemma 3.1]{tseng2024quip}}:
\begin{proposition} \label{prop:rot_quant_error}
Consider a $b_w$-bits symmetric quantizer $Q_w: \mathbb{R}^{d} \rightarrow \mathbb{R}^d$ [cf.~\eqref{eq:sym-q-a}, \eqref{eq:sym-q-b} with $c=1$].
For any $\prm \in \mathbb{R}^d$,
\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]
\item with $\R = {\bf I}$, it holds that
\begin{equation} \label{eq:quant_error_ub_simple}
        \| Q_w(\prm) - \prm \|^2  \leq \frac{d \, \max_{i} \prm_i^2 }{4(2^{b_w - 1} -1)^2} .
\end{equation}
\item with ${\R} = \R(\zeta)$ from \eqref{eq:gen_random_had}, with probability $1-\delta$ we have
    \begin{equation}
        \| Q_w({\bf R}(\zeta) \prm) - {\bf R}(\zeta) \prm \|^2 \leq \frac{\log(4d/\delta)}{2 (2^{b_w - 1} -1)^2} \| \prm \|^2 . \label{eq:modelr_quant_error_ub}
    \end{equation}
\end{itemize}
\end{proposition}
See Appendix \ref{app:rot} for the proof.


Observe that the quantization error is $\mathcal{O}(d \max_i \prm_i^2)$ without rotation, and is  $\mathcal{O}(\|\prm \|^2)$  with rotation. Note that the former bound is more sensitive to weight vectors with outliers.
In particular, the worst case prediction error in the QAT stage with $\R$ chosen as \eqref{eq:gen_random_had} is strictly better than that for the case with $\R = {\bf I}$ (no rotation) if
\begin{equation} \label{eq:compare-rotate}
    \frac{\log(4d/\delta)}{2} \| \overline{\prm}_\R \|^2 \leq \frac{\max_{i} ({\overline{\prm}_{\bf I}}_i)^2 d}{4},
\end{equation}
where $\overline{\prm}_\R$, $\overline{\prm}_{\bf I}$ are the respective converged solutions of \eqref{eq:ste-toy}.
It demonstrates that applying the random rotation matrix in \eqref{eq:gen_random_had} suffices to reduce the quantization error of weight matrices that contain outlier values.
To obtain the best performance, we design the RoSTE algorithm such that at the outer loop, it chooses between ${\bf H}$ or ${\bf I}$ (i.e., no rotation) according to the current weight matrices. 

\begin{remark}
The analysis in \eqref{eq:compare-rotate} enables a novel interpretation of the bit-widths in $Q_x$ and $Q_w$ during STE training. On one hand, it is beneficial to increase the bit-width of activation quantization $Q_x$ until Assumption \ref{assm:interpolation} is satisfied, and further increasing its bit-width would not improve the prediction performance as the bound \eqref{eq:convergence_bound} only depends on \emph{weight quantization error}. On the other hand, increasing the bit-width of weight quantization always reduces the prediction error as seen in \eqref{eq:convergence_bound}, \eqref{eq:modelr_quant_error_ub}. It is also interesting to see that despite adopting low-bit activation quantizers, increasing the dimension $d$ may still allow us to satisfy the interpolation condition Assumption \ref{assm:interpolation}. This is due to the intuition that kernelized high dimensional features are more likely to be separable \citep{liang2020just}. In other words, a neural network with high-dimensional hidden representations can tolerate low-bit quantized activations because the information about $\x_\xi$ retains in the high-dimensional discrete vector $Q_x(\R \x_\xi)$.
\end{remark}

\begin{table*}[tb]

\centering
\caption{{\bf Results on {\exppythia}}. Accuracies of the 4-bit quantized Pythia 6.9B and Qwen 2.5 7B models fine-tuned using the Reddit TL;DR dataset. {\tt FP16} and {\tt BF16} refer to using 16-bit half-precision floating points and 16-bit brain floating points formats, respectively, and {\tt W4A4KV4} refers to using 4-bit quantizations on weights, activation, and KV cache.}\vspace{.2cm}
\label{tab:main_pythia}

\begin{adjustbox}{width=.7\linewidth}
\begin{tabular}{ccccccc}
\toprule 
Bit-width & Method & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-LSum & ROUGE (Avg.)\\
\midrule 
\midrule
&  \multicolumn{6}{c}{Pythia-6.9B}\\
\midrule 
\multirow{2}{*}{FP16}
& Base & 28.81 & 9.45 & 22.29 & 22.91 & 20.87\\
& \basecolor SFT & \basecolor 33.69 & \basecolor 12.60 & \basecolor 26.27 & \basecolor 26.31 & \basecolor 24.72\\
\midrule 
\multirow{7}{*}{W4A4KV4}
& RTN & 7.42 & 0.06 & 6.53 & 6.56 & 5.14\\
& GPTQ & 8.16 & 0.08 & 7.06 & 7.60 & 5.73\\
& LLM-QAT & 18.73 & 3.71 & 15.31 & 15.01 & 13.19\\
& QuaRot & 11.70 & 0.23 & 8.52 & 9.39 & 7.46\\
& SpinQuant & 8.61 & 0.10 & 8.10 & 8.07 & 6.22\\
& STE & 28.91 & 9.07 & 22.30 & 22.33 & 20.65\\
& \goodcolor RoSTE & \goodcolor \textbf{32.60} & \goodcolor \textbf{11.54} & \goodcolor \textbf{25.25} & \goodcolor \textbf{25.25} & \goodcolor \textbf{23.66}\\
\midrule 

&  \multicolumn{6}{c}{Qwen2.5-7B}\\
\midrule
\multirow{2}{*}{BF16}
& Base & 32.72 & 11.82 & 25.18 & 25.42 & 23.79\\
& \basecolor SFT & \basecolor 34.75 & \basecolor 13.59 & \basecolor 27.56 & \basecolor 27.58 & \basecolor 25.87\\
\midrule 

\multirow{6}{*}{W4A4KV4}
& RTN & 1.07 & 0.00 & 1.01 & 1.01 & 0.77\\
& GPTQ & 0.72 & 0.00 & 0.69 & 0.69 & 0.53\\
& QuaRot & 7.21 & 0.10 & 5.93 & 5.93 & 4.79\\
& SpinQuant & 6.87 & 0.29 & 5.97 & 6.12 & 4.81\\
& STE & 30.86 & 10.16 & 23.73 & 23.73 & 22.12\\
& \goodcolor RoSTE & \goodcolor \textbf{34.01} & \goodcolor \textbf{12.89} & \goodcolor \textbf{26.74} & \goodcolor \textbf{26.74} & \goodcolor \textbf{25.10}\\
\bottomrule 

\end{tabular}
\end{adjustbox}
\vspace{-.2cm}
\end{table*}

\begin{table*}[tb]
\centering
\caption{{\bf Results on {\expllama}}. Accuracies of the 4-bit quantized Llama 3.1 8B model fine-tuned on the Tulu 3 SFT mixture dataset. {\tt BF16} refers to using 16-bit brain floating points format, and {\tt W4A4KV4} refers to using 4-bit quantizations on weights, activation, and KV cache.}\vspace{.2cm}
\label{tab:main_tulu}

\begin{adjustbox}{width=.8\linewidth}
\begin{tabular}{cccccccccc}
\toprule 
Bit-width & Method & TruthfulQA & MMLU-Pro & BigBenchHard & AGIEval & GSM8K & Math & Avg.\\
\midrule
\midrule
\multirow{2}{*}{BF16}
& Base & 28.51 & 19.57 & 62.26 & 30.16 & 56.86 & 18.20 & 35.92\\
& \basecolor SFT & \basecolor 31.82 & \basecolor 33.07 & \basecolor 65.67 & \basecolor 34.86 & \basecolor 64.89 & \basecolor 22.66 & \basecolor 42.16\\
\midrule 
\multirow{6}{*}{W4A4KV4}
& RTN & 23.01 & 0 & 0 & 17.03 & 1.03 & 0 & 6.85\\
& GPTQ & 25.34 & 0.02 & 2.55 & 16.48 & 2.05 & 0 & 7.74\\
& QuaRot & \textbf{27.66}  & 21.53 & 47.69 & 29.05 & 37.91 & 6.90 & 28.46\\
& SpinQuant & 26.19  & 21.58 & 49.56 & 28.50 & 38.36 & 10.56 & 29.13\\
& STE & 26.68  & 9.13 & 24.58 & 17.63 & 22.82 & 1.90 & 17.14\\
& \goodcolor RoSTE & \goodcolor 26.44  & \goodcolor \textbf{25.12} & \goodcolor \textbf{52.00} & \goodcolor \textbf{30.11} & \goodcolor \textbf{44.50} & \goodcolor \textbf{11.94} & \goodcolor \textbf{31.69}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}



\section{Experiments} \label{sec:exp}
We evaluate the performance of the proposed RoSTE algorithm for QA-SFT on two standard sets of open-source models and datasets. For the first experiment ({\exppythia}), we fine-tune the pre-trained Pythia 1B/6.9B models \cite{biderman2023pythia} 
% \footnote{Pythia 1B model: \url{https://huggingface.co/EleutherAI/pythia-1b-deduped}. Pythia 6.9B model: \url{https://huggingface.co/EleutherAI/pythia-6.9b-deduped}} 
and Qwen 2.5 0.5B/7B models \cite{yang2024qwen2}
on the Reddit TL;DR Summarization dataset
% \footnote{Data: \url{https://huggingface.co/datasets/trl-lib/tldr}. Code: \url{https://github.com/vwxyzjn/summarize_from_feedback_details}.} 
\cite{huang2024n+} with evaluation on the TL;DR test dataset using the ROUGE metric \cite{lin2004rouge}. 
For the second experiment ({\expllama}), we fine-tune the pre-trained Llama 3.1 8B model \cite{dubey2024llama}
% \footnote{Model: \url{https://huggingface.co/meta-llama/Llama-3.1-8B}. Code: \url{https://github.com/allenai/open-instruct}} 
on the Tulu 3 SFT mixture dataset
% \footnote{Data: \url{https://huggingface.co/datasets/allenai/tulu-3-sft-mixture}}
\cite{lambert2024t} with real-world downstream task evaluations \cite{gao2021framework}. These tasks include TruthfulQA \cite{lin2021truthfulqa}, MMLU-Pro \cite{wang2024mmlu}, BigBenchHard \cite{suzgun2022challenging}, AGIEval \cite{zhong2023agieval}, GSM8K \cite{cobbe2021training}, and MATH \cite{hendrycks2020measuring}. 

For the RoSTE algorithm, while we relaxed the lower level as a $\ell$-variable binary combinatorial problem \eqref{eq:bin_ll}, solving this sub-problem has a complexity of $\mathcal{O}(2^{\ell})$ which is still intractable for models like Llama 3.1 8B with $\ell = 3 \times 32 + 1$. 
% As a remedy, we estimate the solution of \eqref{eq:bin_ll} by sharing the rotation matrices across different layers. This reduces the problem into a 4-variable binary combinatorial optimization. Our experiment results suggest that the above shared variable approximation suffices to find rotation matrices that effectively reduce outliers. 
As a remedy, we estimate the solution of \eqref{eq:bin_ll} by computing only $\mathcal{E}({\cal W}^{kT}, \{{\bf I}\}_{i=0}^{\ell - 1})$ and $\mathcal{E}({\cal W}^{kT}, \{{\bf H}\}_{i=0}^{\ell - 1})$, then we determine each layer's $\R_i$ by comparing the layer-wise quantization error.
Lastly, we set $K=1$ where a one-shot rotation configuration adaptation by pre-trained model is found to perform well. We anticipate the performance to further improve with increasing $K$.
More implementation details can be found in Appendices \ref{sec:exp-setup}, \ref{app:implem}.



\noindent{\bf Baselines.} Besides the proposed RoSTE algorithm, we compare the performances of LLMs with quantized weight and activation obtained by two streams of baseline approaches. The first stream consists of applying PTQ methods on open-source supervised fine-tuned models in \cite{huang2024n+,lambert2024t}. We reproduce the PTQ benchmarks using round-to-nearest (RTN) quantization, GPTQ \cite{frantar2022gptq}, knowledge distillation (LLM-QAT) \cite{liu2023llm}, QuaRot \cite{ashkboos2024quarot} and SpinQuant \cite{liu2024spinquant}.
The second set consists of QAT methods, which apply STE and RoSTE to the SFT objective.
The hyperparameters for reproducing our experiment results can be found in the Appendix at Table \ref{tab:sft-settings} and \ref{tab:qa-sft-settings}. 

All experiments are conducted on a cluster of 8 NVIDIA A100 GPUs. Details of the training and evaluation settings can be found in Appendix \ref{sec:exp-setup}.

\subsection{Accuracy of Quantized Models}

For \exppythia, we present the accuracies of 4-bits (weights \& activation) quantized, fine-tuned Pythia 6.9B found in Table \ref{tab:main_pythia}. Herein, the best baseline is STE (without rotation). In comparison, RoSTE produces a quantized model that improves the average ROUGE score by +3.01. It recovers the performance of the full-precision SFT model with a gap of only -1.06 ROUGE score.  
For \expllama, we present the accuracies of 4-bits (weights \& activation) quantized, fine-tuned Llama 3.1 8B found in Table \ref{tab:main_tulu}. Observe that RoSTE improved the average accuracy by +2.56 over the best baseline SpinQuant, despite there remains a gap of -10.47 below the full-precision fine-tuned model. 

Lastly, in the appendix, we provide additional results on Pythia 1B and W4A8KV4 in Table \ref{tab:pythia}, Qwen 2.5 in Table \ref{tab:qwen}, and Llama 3.1 8B and W4A8KV4 quantization in Table \ref{tab:tulu}.

\begin{table}[t]
\centering
\caption{Effects of rotation matrix strategies for STE training in {\exppythia} with Pythia 1B that is W4A4KV4 quantized.}
\label{tab:ablation_rot}
\vskip 0.1in

\begin{tabular}{cc}
\toprule
Rotation Strategy & ROUGE (Avg.)\\
\midrule
No Rotation & 22.37 \\
Complete Rotation & 13.09 \\
{RoSTE (Adaptive Rotation)}
& \textbf{23.07} \\
\bottomrule\vspace{-.5cm}
\end{tabular}
% \end{adjustbox}
\end{table}

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.235\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/layer_30_mlp_down_proj_q.png}
         \caption{By STE.}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.235\textwidth}
         \centering
         \includegraphics[width=\textwidth]{fig/layer_30_mlp_down_proj_r_q.png}
         \caption{By RoSTE.}
     \end{subfigure}\vspace{-.2cm}
     \caption{Visualizations of input activations at layer 30 of converged Llama model trained for QA-SFT using STE and RoSTE.}\vspace{-.3cm}
     \label{fig:vis-QA-SFT-r}
\end{figure}

\begin{figure}[t]
     \centering
     \includegraphics[width=0.75\columnwidth]{fig/quant_error_per_steps.pdf}\vspace{-.3cm}
     \caption{The evolution of quantization error \eqref{eq:quant-error} against the QAT stage iterations.}\vspace{-.2cm}
     \label{fig:quant-error-training}
\end{figure}

\subsection{Ablation Study}
We now concentrate on the effects of optimal rotation configuration as practiced in line~\ref{line:ll} of Algorithm~\ref{alg:roste}. In Table~\ref{tab:ablation_rot}, we compare the performance on {\exppythia} with Pythia 1B when the random Walsh-Hadamard rotation matrix is applied with different strategies. 
Notice that the adaptive rotation strategy deployed in RoSTE delivered the best performance. When no rotation matrix is applied, we observe a drop in ROUGE score by $-0.70$, and importantly, the complete rotation setting, i.e., applying rotation matrix on every module regardless of whether empirical quantization error is reduced, suffers a drop in ROGUE score by $-9.98$. 
This shows that while it is beneficial to apply rotation in the STE training, an adaptive strategy such as the one in RoSTE is necessary to guarantee good performance.



Secondly, we take a closer look at the effects of outlier reduction to justify our claims on the use of random Walsh-Hadamard rotation matrices. 
Fig.~\ref{fig:vis-QA-SFT-r} compares the distribution of the input activations of fine-tuned models trained by STE and RoSTE at layer 30. We observe that the model produced by RoSTE exhibits no activation outliers, while STE suffers from activation outliers even at convergence. For a more detailed and comprehensive comparison, see Fig.~\ref{fig:rotated-vis} and \ref{fig:redu-rate} in the appendix. Furthermore, Fig.~\ref{fig:quant-error-training} shows the trajectory of the quantization error \eqref{eq:quant-error} during training. As expected, we see that the quantization error of RoSTE is much lower than that in STE, thus suggesting a lower bias in the solution obtained [cf.~Theorem~\ref{thm:main}]. 


\section{Conclusion}
This paper proposed the RoSTE algorithm for quantization-aware SFT training with an adaptive rotation strategy. To the best of our knowledge, this is one of the first algorithms for the efficient quantized SFT process. Besides achieving state-of-the-art performance, we also provide theoretical insights to justify the practical efficacy of RoSTE.




\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\bibliography{ref,reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Proof of Theorem \ref{thm:main}} \label{proof:main}
{\it Proof.} In this proof, we will use the following equality interchangeably:
\begin{equation}
    \loss(\modelR^{t}) = \expec{\left( \dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^{t})} - {\bf y}_\xi \right)^2} = \expec{\| Q_w(\R \prm^{t}) -\prm^\star_{\R} \|_{\gram}^2} 
\end{equation}
Consider the update rule of our algorithm as $\prm^{t+1} = \prm^t - \eta ( \dotp{Q_x(\R \x_t )}{ Q_w(\R \prm^t ) } - {\bf y}_t )\R^\top Q_x(\R \x_t) $, we compute
\begin{align}
    &\expec{\left( \dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^{t+1})} - {\bf y}_\xi \right)^2} \\
    &=\expec{ \dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^{t+1}) - \prm^\star_{\R} + Q_w(\R \prm^t) - Q_w(\R \prm^t) }^2 }\\
    &=\mathbb{E}\Big[\big( \underbrace{\dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^t) - \prm^\star_{\R}}}_{p_1^t} + \underbrace{\dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^{t+1}) - Q_w(\R \prm^t)}}_{p_2^t} \big)^2 \Big]
\end{align}
Now observe that
\begin{align}
    &\expec{ (p_2^t)^2 } \\
    &=\expec{ \| Q_w(\R \prm^{t+1}) - Q_w(\R \prm^t) \|^2_{\gram}} \\
    &\leq 3\expec{\| \R \prm^{t+1} - \R \prm^t \|_{\gram}^2 } + 3 \expec{\| \e(\R \prm^{t+1}) \|_{\gram}^2} + 3 \expec{\| \e(\R \prm^{t}) \|_{\gram}^2} \\
    &= 3\eta^2 \expec{\|Q_x(\R \x_t)\|_{\gram}^2 \dotp{Q_x(\R \x_t)}{Q_w(\R \prm^t) - \prm^\star_{\R}}^2 } + 3 \expec{\| \e(\R \prm^{t+1}) \|_{\gram}^2} + 3 \expec{\| \e(\R \prm^{t}) \|_{\gram}^2} \\
    &\leq 3 \eta^2 \rho \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram}^2 } + 3 \expec{\| \e(\R \prm^{t+1}) \|_{\gram}^2} + 3 \expec{\| \e(\R \prm^{t}) \|_{\gram}^2}
\end{align}
where the last inequality is due to the independence $\x_\xi \perp \x_t \perp \prm^t$ and uses Assumption~\ref{assm:gram}. Furthermore,
\begin{align}
    &2 \expec{ p_1^t p_2^t } \\
    &= 2 \expec{p_1^t\dotp{Q_x(\R \x_\xi)}{\R \prm^{t+1} - \R \prm^t + \e(\R\prm^{t+1}) - \e(\R \prm^t)}} \\
    &= -2\eta \expec{\dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^t) - \prm^\star_{\R}}\dotp{Q_x(\R \x_\xi)}{Q_x(\R \x_t)}\dotp{Q_x(\R \x_t)}{Q_w(\R \prm^t) - \prm^\star_{\R}}} \notag \\
    &\quad + 2 \expec{\dotp{Q_x(\R \x_\xi)}{Q_w(\R \prm^t) - \prm^\star_{\R}}\dotp{Q_x(\R \x_\xi)}{\e(\R\prm^{t+1} ) - \e(\R \prm^t)}} \\
    &\leq -2\eta \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram^2}^2} + \eta\lambmin \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram}^2}\\
    &\quad + \frac{2}{\eta \lambmin} \expec{\| \e(\R \prm^{t+1} )\|_{\gram}^2} + \frac{2}{\eta \lambmin} \expec{\| \e(\R \prm^t )\|_{\gram}^2} \\
    &\leq -\eta\lambmin \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram}^2} + \frac{2}{\eta \lambmin} \expec{\| \e(\R \prm^{t+1} )\|_{\gram}^2} + \frac{2}{\eta \lambmin} \expec{\| \e(\R \prm^t )\|_{\gram}^2}
\end{align}
where the last step is due to the independence $\x_\xi \perp \x_t \perp \prm^t$ and \eqref{eq:gram_lb}.
Therefore, we obtain
\begin{align}
    &\expec{\| Q_w(\R \prm^{t+1}) - \prm^\star_{\R} \|_{\gram}^2} \\
    &\leq (1- \eta \lambmin + 3\eta^2 \rho) \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram}^2}  + (3 + \frac{2}{\eta \lambmin}) \left( \expec{\| \e(\R \prm^{t+1}) \|_{\gram}^2} + \expec{\| \e(\R \prm^{t}) \|_{\gram}^2}\right) \\
    &\stackrel{(i)}{\leq} (1- \eta \lambmin/2) \expec{\| Q_w(\R \prm^t) - \prm^\star_{\R} \|_{\gram}^2}  + (3 + \frac{2}{\eta \lambmin}) \left( \expec{\| \e(\R \prm^{t+1}) \|_{\gram}^2} + \expec{\| \e(\R \prm^{t}) \|_{\gram}^2}\right) \\
    &\leq (1-\eta \lambmin / 2)^{t+1} \expec{\| Q_w(\R \prm^0) - \prm^\star_{\R} \|_{\gram}^2} \\
    &\quad + (3 + \frac{2}{\eta \lambmin}) \sum_{k=0}^{t} (1 - \eta \lambmin /2)^{t-k} \left( \expec{\| \e(\R \prm^{k+1}) \|_{\gram}^2} + \expec{\| \e(\R \prm^{k}) \|_{\gram}^2}\right)
\end{align}
where $(i)$ uses the step size condition $\eta \leq \lambmin / (6\rho)$. Choosing the step size $\eta = \lambmin / (6\rho)$ completes the proof.
\hfill $\square$



\section{Proof of Proposition \ref{prop:rot_quant_error}}\label{app:rot}
{\it Proof.} By the definition of $Q_w$ in \eqref{eq:sym-q-a} and \eqref{eq:sym-q-b}, we notice
\begin{align}
    &\| Q_w(\prm) - \prm \|^2 = \left\| s(\prm) \left\lfloor \frac{\prm}{s(\prm)} \right\rceil - s(\prm) \frac{\prm}{s(\prm)} \right\|^2 = s(\prm)^2 \left\| \left\lfloor \frac{\prm}{s(\prm)} \right\rceil - \frac{\prm}{s(\prm)} \right\|^2 \\
    & \stackrel{(i)}{\leq}  s(\prm)^2 \frac{d}{4} = \frac{d~\max_i \prm_i^2}{4(2^{b_w -1}-1)^2} 
\end{align}
where $(i)$ is a worst-case error bound of nearest rounding. This proves \eqref{eq:quant_error_ub_simple}. Further applying \protect{\citep[Lemma 3.1]{tseng2024quip}} in our last step gives \eqref{eq:modelr_quant_error_ub}.
\hfill $\square$


\section{Details of Experiment Settings}
\label{sec:exp-setup}

{\bf RoSTE algorithm.} {We set the lower level objective function in \eqref{eq:quant-error} by drawing $n = 128$ samples from the fine tuning dataset for calibration.}
% \htwaiupdate{Moreover, the RoSTE algorithm is implemented with $K=1$ as the latter suffices to achieve good performance.}


{\bf Hyper-parameters. }
We list the training configurations for SFT in ({\exppythia}) TL;DR summarization and ({\expllama}) Tulu 3 experiments as suggested in \cite{huang2024n+,lambert2024t} in Table~\ref{tab:sft-settings}. For QA-SFT, we sweep through a number of hyper-parameters for STE and RoSTE to obtain the best performance, as listed in Table~\ref{tab:qa-sft-settings}.

\begin{table}[htbp]
\centering
\caption{Detailed training settings for SFT in the TL;DR summarization and Tulu 3 experiments.}
\label{tab:sft-settings}
\vskip 0.15in
\begin{tabular}{cccccc}
\toprule
Method & \multicolumn{5}{c}{SFT}  \\
Model  & Pythia 1B & Pythia 6.9B & Qwen2.5 0.5B & Qwen2.5 7B & Llama 3.1 8B \\
\midrule
Epoch & 1 & 1 & 1 & 1 & 2 \\
Batch Size (Per GPU) & 16 & 1 & 16 & 1 & 1 \\
Gradient Accumulation & 1 & 16 & 1 & 16 & 16 \\
Optimizer & AdamW & AdamW & AdamW & AdamW & AdamW \\
Learning Rate & 3e-5 & 3e-5 & 5e-5 & 1e-5 & 5e-6 \\
LR Schedule & cosine & cosine & cosine & cosine & linear\\
Warmup Ratio & 0 & 0 & 0 & 0 & 0.03 \\
Max. Seq. Length & 2048 & 2048 & 2048 & 2048 & 1024 \\
\# Training Samples & 117k & 117k & 117k & 117k & 100k \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Detailed training settings and hyper-parameters for QA-SFT in the TL;DR summarization and Tulu 3 experiments.}
\label{tab:qa-sft-settings}
\vskip 0.15in
\begin{adjustbox}{width=0.99\linewidth}
\begin{tabular}{cccccc}
\toprule
Method & \multicolumn{5}{c}{QA-SFT (i.e., STE or RoSTE)}  \\
Model  & Pythia 1B & Pythia 6.9B & Qwen2.5 0.5B & Qwen2.5 7B & Llama 3.1 8B \\
\midrule
Epoch & 1 & 1 & 1 & 1 & 2 \\
Batch Size (Per GPU) & 16 & 1 & 16 & 1 & 1 \\
Gradient Accumulation & 1 & 16 & 1 & 16 & 16 \\
Optimizer & AdamW & AdamW & AdamW & AdamW & AdamW \\
Learning Rate & \{3e-5, 6e-6, 3e-6\} & \{3e-5, 6e-6, 3e-6\} & \{5e-5, 1e-5, 5e-6\} & \{5e-5, 1e-5, 5e-6\} & \{5e-6, 1e-6, 5e-7\} \\
LR Schedule & cosine & cosine & cosine & cosine & linear\\
Warmup Ratio & 0 & 0 & 0 & 0 & 0.03 \\
Max. Seq. Length & 2048 & 2048  & 2048 & 2048 & 1024 \\
\# Training Samples & 117k & 117k & 117k & 117k & 100k \\
clipping factor & \{1, 0.95, 0.9\} & \{1, 0.95, 0.9\} & \{1, 0.95, 0.9\} & \{1, 0.95, 0.9\} & \{1, 0.95, 0.9\} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}







\paragraph{Evalution. }
For the TL;DR summarization experiments, all final models are evaluated on the TL;DR test dataset using the ROUGE metric \cite{lin2004rouge}, including ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-LSum. For the Tulu 3 experiments, all final models are evaluated on downstream tasks using EleutherAI LM Evaluation Harness \cite{gao2021framework}. These tasks include TruthfulQA \cite{lin2021truthfulqa}, MMLU-Pro \cite{wang2024mmlu}, BigBenchHard \cite{suzgun2022challenging}, AGIEval \cite{zhong2023agieval}, GSM8K \cite{cobbe2021training}, and MATH \cite{hendrycks2020measuring}. In Table \ref{tab:eval-setting}, we list the detailed evaluation settings for these downstream tasks as suggested in the Tulu 3 paper \cite{lambert2024t}.



\begin{table}[htbp]
\caption{Details of evaluation settings for the Tulu 3 experiments.}
\label{tab:eval-setting}
\vskip 0.15in

\centering
\begin{tabular}{ccccccc}
\toprule
Benchmark & TruthfulQA & MMLU-Pro & BigBenchHard & AGIEval & GSM8K & Math\\
\midrule
\# shot & 6	& 0 & 3 & 0 & 8 &4 \\
Metric & Acc (mc1)  & EM & EM & Acc & EM & EM\\
CoT & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark \\
\bottomrule

\end{tabular}

\end{table}


\section{Implementation Details of the Rotated-and-Quantized LLM} \label{app:implem}
Our architecture for inserting rotation matrices and quantization on transformer models follows from \cite{liu2024spinquant}. For completeness, an illustration is provided in Figure \ref{fig:rotated-flow}. In the following sections, we describe the details of the RoSTE algorithm under the setting of {\exppythia} and {\expllama}.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/rotation-flow.pdf}
    \caption{An illustration of the rotation workflow in a transformer-based model. $\R_1$ represents the between-block rotation, which eliminates activation outliers between blocks. $\R_2, \R_3, \R_4$ are in-block rotations designed to remove outliers within the MHSA and MLP blocks. Among these, $\R_1, \R^\top_1, \R_2, \R^\top_2, \R^\top_4$ can be merged into weights while $\R_3, \R^\top_3, \R_4$ are not mergeable and serve as online rotations during training and inference.}
    \label{fig:rotated-flow}
\end{figure*}

\noindent{\bf Quantization on LLMs. } We adopt the asymmetric uniform quantizer \eqref{eq:asym_quant} for all the experiments. For instance, to quantize the activations $\X$ of dimensions [\textit{batch size}, \textit{sequence length}, \textit{embedding size}], we employ \textit{per-token} quantization such that each embedding $\X_{ij}$ forms a quantization group. To quantize the linear weight values ${\bf W}$, we employ \textit{per-channel} quantization such that each $i$-th output channel's weights ${\bf W}_{i}$ form a quantization group. 


\noindent{\bf Modifying Normalization Layer. } We modify the model to maintain computational invariance before and after applying rotation. This requires ensuring that there are no mean subtraction, scaling, or shifting operations in the normalization module. For models with LayerNorm, such as Pythia, the process involves absorbing the mean subtraction operation into the weight matrix before LayerNorm and absorbing the LayerNorm scaling and shifting parameters into the weight matrix after the LayerNorm layer \cite{ashkboos2024slicegpt}. Similarly, for models using RMSNorm, such as Llama, this can be achieved by absorbing the RMSNorm scaling parameter into the weight matrix immediately following the RMSNorm layer. 

{\bf Between-Block \& In-Block Rotation. }  
We perform between-block rotation $\R_1$ to eliminate the activation outliers between blocks. As illustrated in Fig.~\ref{fig:rotated-flow}, $\R_1$ is applied to all linear layers in MHSA and MLP blocks. In particular, the weight matrices in the Q, K, and V projection layers of MHSA, as well as the Up and Gate projection layers of MLP, are rotated along with their corresponding input activations to preserve computational invariance. Similarly, the weight matrices in the O projection layer of MHSA and the Down projection layer of MLP, along with their corresponding outputs, are also rotated using $\R_1$. Additionally, we also rotate the embedding and lm\_head layers so that the final output of the model will be identical to the original model. Next, we perform in-block rotations $\R_2, \R_3, \R_4$ to eliminate the activation outliers within blocks. Specially, $\R_2$ is applied to the Value and the O projection layer of MHSA. $\R_3$ works for the Query and Key. $\R_2$ and $\R_3$ can remove the activation outliers for KV caches. We apply $\R_4$ to the Down projection layer of MLP.

$\R_1, \R_2$ are offline mergeable rotations, which can be merged into the weight matrices before training. $\R_3, \R_4$ are online rotations, which are implemented in the fast Hadamard kernel and can be seen as a layer dynamically rotating the input activation. This online operation is highly efficient by leveraging the fast Hadamard  CUDA kernel, resulting in negligible overhead during both training and inference.

\section{Impact of Rotation on Different Models} 


Fig.~\ref{fig:rotated-vis} showcases the effects of (random Walsh-Hadamard) rotation applied to several exemplary layers in Pythia and Llama models, and demonstrates that sometimes applying the rotation can lead to undesirable results where new outlier values emerge. Fig.~\ref{fig:redu-rate} presents a comprehensive view of the effects of applying rotations to the weights, activation, and KV cache of different layers. Notice that the RoSTE algorithm only applies rotation when a reduction of quantization error is observed in the respective layers. 
Moreover, from the figure we observe that in general, the last layers of Pythia model do not benefit from applying rotation, while the rotation effects on Llama model are generally beneficial.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{fig/rotation-vis.pdf}
    \caption{Visualizations of Input Activations in Pythia and Llama Models before and after rotation.}
    \label{fig:rotated-vis}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.975\textwidth]{fig/redu_rate.pdf}
    \caption{Effects of incoherence processing using rotation matrices on \emph{different layers} of Pythia and Llama models using the pre-trained weights. (Left) Relative reduction rates of quantization error, calculated as $\frac{ \text{Error w/o rotation} - \text{Error w/ rotation} }{ \text{Error w/o rotation} } \times 100\%$. Note that the reduction rate can be negative if the rotation is not beneficial. (Right) Reduction rate of dynamic ranges of the activations after rotation.}
    \label{fig:redu-rate}
\end{figure*}



We conjecture that several architectural differences between Pythia and Llama contribute to this discrepancy. First, Pythia does not utilize Gated Linear Units (GLU) in its MLP layers, a feature that is integral to Llama. Second, Pythia employs layer normalization (LayerNorm) instead of root mean square normalization (RMSNorm) which is used in Llama. Finally, Pythia adopts a parallel residual connection for attention and feed-forward layers, in contrast to the sequential residual connection found in Llama.


\section{Additional Experiments}
We show additional experiment results for the resultant accuracies of fine-tuning the LLMs with different configurations of quantization parameters. Particularly, the results for {{\exppythia}} on the Pythia models can be found in Table~\ref{tab:pythia}, and the results for {{\expllama}} on the Pythia models can be found in Table~\ref{tab:tulu}. We observe consistent improvements with the RoSTE algorithm.

\begin{table}[htbp]
\caption{Additional experiments for {\exppythia} with different bit-width configurations and different model sizes.}
\label{tab:pythia}
\vskip 0.15in
\centering
\begin{tabular}{ccccccc}
\toprule 
Bit-width & Method & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-LSum & ROUGE (Avg.)\\
\midrule
\midrule
&  \multicolumn{6}{c}{Pythia-1B}\\
\midrule
\multirow{2}{*}{FP16} 
& Base & 22.40 & 5.73 & 17.35 & 17.59 & 15.77\\
& \basecolor SFT & \basecolor 32.80 & \basecolor 11.84 & \basecolor 25.49 & \basecolor 25.50 & \basecolor 23.91\\
\midrule 

\multirow{7}{*}{W4A4KV4} 
& RTN & 6.05 & 0.06 & 5.21 & 5.67 & 4.25\\
& GPTQ & 10.16 & 0.30 & 8.41 & 8.84 & 6.93\\
& LLM-QAT & 19.71 & 4.03 & 15.82 & 15.83 & 13.85\\
& QuaRot & 16.57 & 1.66 & 13.61 & 13.70 & 11.39\\
& SpinQuant & 13.52 & 0.40 & 11.21 & 11.10 & 9.06\\
& STE & 31.03 & 10.44 & 24.01 & 24.01 & 22.37 \\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{31.80} & \goodcolor \textbf{11.03} & \goodcolor \textbf{24.71} & \goodcolor \textbf{24.71} & \goodcolor \textbf{23.07}\\
\midrule 

\multirow{7}{*}{W4A8KV4} 
& RTN & 24.19 & 6.94 & 19.29 & 19.13 & 17.39\\
& GPTQ & 29.77 & 9.81 & 23.38 & 23.50 & 21.52\\
& LLM-QAT & 29.54 & 9.60 & 23.08 & 23.08 & 21.33\\
& QuaRot & 30.14 & 9.24 & 22.97 & 23.03 & 21.35\\
& SpinQuant & 30.37 & 9.73 & 23.15 & 23.43 & 21.67\\
& STE & 32.44 & 11.48 & 25.24 & 25.24 & 23.60\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{32.67} & \goodcolor \textbf{11.61} & \goodcolor \textbf{25.37} & \goodcolor \textbf{25.37} & \goodcolor \textbf{23.76}\\
\midrule 

&  \multicolumn{6}{c}{Pythia-6.9B}\\
\midrule
\multirow{2}{*}{FP16}
& Base & 28.81 & 9.45 & 22.29 & 22.91 & 20.87\\
& \basecolor SFT & \basecolor 33.69 & \basecolor 12.60 & \basecolor 26.27 & \basecolor 26.31 & \basecolor 24.72\\
\midrule 

\multirow{7}{*}{W4A4KV4}
& RTN & 7.42 & 0.06 & 6.53 & 6.56 & 5.14\\
& GPTQ & 8.16 & 0.08 & 7.06 & 7.60 & 5.73\\
& LLM-QAT & 18.73 & 3.71 & 15.31 & 15.01 & 13.19\\
& QuaRot & 11.70 & 0.23 & 8.52 & 9.39 & 7.46\\
& SpinQuant & 8.61 & 0.10 & 8.10 & 8.07 & 6.22\\
& STE & 28.91 & 9.07 & 22.30 & 22.33 & 20.65\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{32.60} & \goodcolor \textbf{11.54} & \goodcolor \textbf{25.25} & \goodcolor \textbf{25.25} & \goodcolor \textbf{23.66}\\
\midrule 

\multirow{7}{*}{W4A8KV4}
& RTN & 21.77 & 5.31 & 17.31 & 17.22 & 15.40\\
& GPTQ & 32.42 & 10.71 & 24.56 & 24.59 & 23.07\\
& LLM-QAT & 29.24 & 9.16 & 22.64 & 22.64 & 20.92\\
& QuaRot & 26.08 & 8.17 & 20.97 & 20.98 & 19.05\\
& SpinQuant & 31.69 & 10.70 & 24.69 & 24.68 & 22.94\\
& STE & 33.05 & 11.94 & 25.58 & 25.61 & 24.05\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{33.18} & \goodcolor \textbf{12.05} & \goodcolor \textbf{25.86} & \goodcolor \textbf{25.88} & \goodcolor \textbf{24.24}\\
\bottomrule
\end{tabular}

\end{table}


\begin{table}[htbp]
\caption{Additional experiments for {\exppythia} with different bit-width configurations and different model sizes. }
\label{tab:qwen}
\vskip 0.15in
\centering
\begin{tabular}{ccccccc}
\toprule 
Bit-width & Method & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-LSum & ROUGE (Avg.)\\
\midrule
\midrule
&  \multicolumn{6}{c}{Qwen2.5-0.5B}\\
\midrule
\multirow{2}{*}{BF16} 
& Base & 23.79 & 6.63 & 18.46 & 18.56 & 16.86\\
& \basecolor SFT & \basecolor 32.58 & \basecolor 11.93 & \basecolor 25.53 & \basecolor 25.55 & \basecolor 23.90\\
\midrule 

\multirow{6}{*}{W4A4KV4} 
& RTN & 10.04 & 0.37 & 8.15 & 8.34 & 6.73\\
& GPTQ & 12.53 & 0.92 & 10.08 & 10.50 & 8.51\\
& QuaRot & 9.94 & 0.57 & 8.18 & 8.38 & 6.67\\
& SpinQuant & 12.16 & 1.22 & 10.69 & 10.72 & 8.70\\
& STE & 29.97 & 9.92 & 23.39 & 23.39 & 21.67 \\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{30.75} & \goodcolor \textbf{10.44} & \goodcolor \textbf{23.96} & \goodcolor \textbf{23.96} & \goodcolor \textbf{22.28}\\
\midrule 

\multirow{6}{*}{W4A8KV4} 
& RTN & 9.51 & 1.06 & 9.02 & 8.90 & 7.12\\
& GPTQ & 9.53 & 1.04 & 8.80 & 8.73 & 7.03\\
& QuaRot & 8.24 & 1.25 & 7.51 & 7.23 & 6.06\\
& SpinQuant & 9.10 & 1.11 & 8.31 & 8.12 & 6.66\\
& STE & 32.14 & 11.50 & 25.18 & 25.18 & 23.50\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{32.31} & \goodcolor \textbf{11.79} & \goodcolor \textbf{25.37} & \goodcolor \textbf{25.38} & \goodcolor \textbf{23.71}\\
\midrule 

&  \multicolumn{6}{c}{Qwen2.5-7B}\\
\midrule
\multirow{2}{*}{BF16}
& Base & 32.72 & 11.82 & 25.18 & 25.42 & 23.79\\
& \basecolor SFT & \basecolor 34.75 & \basecolor 13.59 & \basecolor 27.56 & \basecolor 27.58 & \basecolor 25.87\\
\midrule 

\multirow{6}{*}{W4A4KV4}
& RTN & 1.07 & 0.00 & 1.01 & 1.01 & 0.77\\
& GPTQ & 0.72 & 0.00 & 0.69 & 0.69 & 0.53\\
& QuaRot & 7.21 & 0.10 & 5.93 & 5.93 & 4.79\\
& SpinQuant & 6.87 & 0.29 & 5.97 & 6.12 & 4.81\\
& STE & 30.86 & 10.16 & 23.73 & 23.73 & 22.12\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{34.01} & \goodcolor \textbf{12.89} & \goodcolor \textbf{26.74} & \goodcolor \textbf{26.74} & \goodcolor \textbf{25.10}\\
\midrule 

\multirow{6}{*}{W4A8KV4}
& RTN & 5.73 & 0.23 & 4.72 & 4.74 & 3.86\\
& GPTQ & 7.48 & 0.27 & 6.22 & 6.36 & 5.08\\
& QuaRot & 5.62 & 0.15 & 5.08 & 5.14 & 4.00\\
& SpinQuant & 0.64 & 0.30 & 5.64 & 5.81 & 4.54\\
& STE & 34.44 & 13.29 & 27.16 & 27.17 & 25.52\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{34.58} & \goodcolor \textbf{13.46} & \goodcolor \textbf{27.34} & \goodcolor \textbf{27.35} & \goodcolor \textbf{25.68}\\
\bottomrule
\end{tabular}

\end{table}


\begin{table}[htbp]
\caption{Additional experiments for {\expllama} on different bit-width configurations and different model sizes.}
\label{tab:tulu}

\vskip 0.15in
\centering

\begin{tabular}{ccccccccc}
\toprule 
Bit-width & Method & TruthfulQA & MMLU-Pro & BigBenchHard & AGIEval & GSM8K & Math & Avg.\\
\midrule
\midrule

\multirow{2}{*}{FP16}
& Base & 28.51 & 19.57 & 62.26 & 30.16 & 56.86 & 18.20 & 35.93\\
& \basecolor SFT & \basecolor 31.82 & \basecolor 33.07 & \basecolor 65.67 & \basecolor 34.86 & \basecolor 64.89 & \basecolor 22.66 & \basecolor 42.16\\
\midrule 

\multirow{6}{*}{W4A4KV4}
& RTN & 23.01 & 0 & 0 & 17.03 & 1.03 & 0 & 6.85\\
& GPTQ & 25.34 & 0.02 & 2.55 & 16.48 & 2.05 & 0 & 7.74\\
& QuaRot & \textbf{27.66}  & 21.53 & 47.69 & 29.05 & 37.91 & 6.90 & 28.46\\
& SpinQuant & 26.19  & 21.58 & 49.56 & 28.50 & 38.36 & 10.56 & 29.13\\
& STE & 26.68  & 9.13 & 24.58 & 17.63 & 22.82 & 1.90 & 17.14\\
& \goodcolor RoSTE (ours) & \goodcolor 26.44  & \goodcolor \textbf{25.12} & \goodcolor \textbf{52.00} & \goodcolor \textbf{30.11} & \goodcolor \textbf{44.50} & \goodcolor \textbf{11.94} & \goodcolor \textbf{31.69}\\
\midrule

\multirow{6}{*}{W4A8KV4}
& RTN & 28.76 & 19.29 & 42.96 & 27.75 & 28.66 & 7.84 & 25.88\\
& GPTQ & 28.52 & 25.54 & 46.38 & 29.26 & 48.60 & 0.02 & 29.72\\
& QuaRot & 27.42 & 26.78 & 53.79 & 32.01 & 49.20 & 12.72 & 33.65\\
& SpinQuant & 28.15  & 26.66 & 55.74 & 32.01 & 52.16 & 15.38 & 35.02\\
& STE & 29.62 & 24.09 & 54.62 & 29.44 & 52.62 & 4.08 & 32.41\\
& \goodcolor RoSTE (ours) & \goodcolor \textbf{30.84} & \goodcolor \textbf{28.23} & \goodcolor \textbf{59.25} & \goodcolor \textbf{34.03} & \goodcolor \textbf{56.94} & \goodcolor \textbf{16.88} & \goodcolor \textbf{37.70}\\
\bottomrule
\end{tabular}


\end{table}




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

