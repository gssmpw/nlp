@article{ashkboos2024quarot,
	author = {Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
	journal = {arXiv preprint arXiv:2404.00456},
	title = {Quarot: Outlier-free 4-bit inference in rotated llms},
	year = {2024}}

@article{chee2024quip,
  title={Quip: 2-bit quantization of large language models with guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{du2024bitdistiller,
	author = {Du, Dayou and Zhang, Yijia and Cao, Shijie and Guo, Jiaqi and Cao, Ting and Chu, Xiaowen and Xu, Ningyi},
	journal = {arXiv preprint arXiv:2402.10631},
	title = {Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation},
	year = {2024}}

@article{frantar2022gptq,
	author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
	journal = {arXiv preprint arXiv:2210.17323},
	title = {Gptq: Accurate post-training quantization for generative pre-trained transformers},
	year = {2022}}

@article{lee2024owq,
  title={OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models},
  author={Lee, Changhun and Jin, Jungyu and Kim, Taesu and Kim, Hyungjun and Park, Eunhyeok},
  journal={arXiv preprint arXiv:2306.02272},
  year={2023}
}

@article{lin2023awq,
	author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
	journal = {arXiv preprint arXiv:2306.00978},
	title = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
	year = {2023}}

@article{liu2023llm,
	author = {Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	journal = {arXiv preprint arXiv:2305.17888},
	title = {Llm-qat: Data-free quantization aware training for large language models},
	year = {2023}}

@article{liu2024spinquant,
	author = {Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen},
	journal = {arXiv preprint arXiv:2405.16406},
	title = {SpinQuant--LLM quantization with learned rotations},
	year = {2024}}

@article{shao2023omniquant,
	author = {Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping},
	journal = {arXiv preprint arXiv:2308.13137},
	title = {Omniquant: Omnidirectionally calibrated quantization for large language models},
	year = {2023}}

@article{tseng2024quip,
  title={Quip\#: Even better LLM quantization with hadamard incoherence and lattice codebooks},
  author={Tseng, Albert and Chee, Jerry and Sun, Qingyao and Kuleshov, Volodymyr and De Sa, Christopher},
  journal={arXiv preprint arXiv:2402.04396},
  year={2024}
}

@article{xiao2022smoothquant,
	author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
	journal = {arXiv preprint arXiv:2211.10438},
	title = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
	year = {2022}}

@article{xu2024onebit,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}

@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={196--209},
  year={2024}
}

