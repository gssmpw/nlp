[
  {
    "index": 0,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan",
        "title": "Gptq: Accurate post-training quantization for generative pre-trained transformers"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2023awq",
        "author": "Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song",
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"
      },
      {
        "key": "lee2024owq",
        "author": "Lee, Changhun and Jin, Jungyu and Kim, Taesu and Kim, Hyungjun and Park, Eunhyeok",
        "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models"
      },
      {
        "key": "chee2024quip",
        "author": "Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M",
        "title": "Quip: 2-bit quantization of large language models with guarantees"
      },
      {
        "key": "tseng2024quip",
        "author": "Tseng, Albert and Chee, Jerry and Sun, Qingyao and Kuleshov, Volodymyr and De Sa, Christopher",
        "title": "Quip\\#: Even better LLM quantization with hadamard incoherence and lattice codebooks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      },
      {
        "key": "zhao2024atom",
        "author": "Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris",
        "title": "Atom: Low-bit quantization for efficient and accurate llm serving"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiao2022smoothquant",
        "author": "Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song",
        "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
      },
      {
        "key": "shao2023omniquant",
        "author": "Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping",
        "title": "Omniquant: Omnidirectionally calibrated quantization for large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ashkboos2024quarot",
        "author": "Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James",
        "title": "Quarot: Outlier-free 4-bit inference in rotated llms"
      },
      {
        "key": "liu2024spinquant",
        "author": "Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen",
        "title": "SpinQuant--LLM quantization with learned rotations"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2023llm",
        "author": "Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas",
        "title": "Llm-qat: Data-free quantization aware training for large language models"
      },
      {
        "key": "du2024bitdistiller",
        "author": "Du, Dayou and Zhang, Yijia and Cao, Shijie and Guo, Jiaqi and Cao, Ting and Chu, Xiaowen and Xu, Ningyi",
        "title": "Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation"
      },
      {
        "key": "xu2024onebit",
        "author": "Xu, Yuzhuang and Han, Xu and Yang, Zonghan and Wang, Shuo and Zhu, Qingfu and Liu, Zhiyuan and Liu, Weidong and Che, Wanxiang",
        "title": "OneBit: Towards Extremely Low-bit Large Language Models"
      }
    ]
  }
]