\section{Related Works}
The quest on quantizing modern LLMs for efficient deployment started with weight-only quantization. GPTQ \cite{frantar2022gptq} stood out as a robust postprocessing algorithm that matches the layer output between a quantized model and a full-precision target model. Soon after, a new trend turned to tackling outlier values in weight matrices and activations due to their incompatibility with quantization \cite{lin2023awq,lee2024owq,chee2024quip,tseng2024quip}, pushing the limit of accurately quantized models below 2-bits.
While the memory consumption of storing the model parameters is reduced by weight-only quantization, their activations remain in full precision during inference which prohibits the application of long context LLMs on consumer-grade accelerators with limited memory storage.

This motivates the development of weight-activation quantization. It allows weights and activations to be directly multiplied using discrete arithmetic units that accelerate inference and reduce the inference memory requirement.
Existing methods can be categorized as follows: 
(1) mixed-precision quantization  \cite{dettmers2022gpt3,zhao2024atom} that assigns extra bit-widths to outlier values;
(2) scaling-based quantization \cite{xiao2022smoothquant,shao2023omniquant} that employs scaling to balance the representation range between activations and weights;
(3) rotated quantization \cite{ashkboos2024quarot,liu2024spinquant} that utilizes orthogonal transformation to remove activation outliers; 
(4) knowledge distillation \cite{liu2023llm,du2024bitdistiller,xu2024onebit} that re-trains a quantized model to match the behavior of a full-precision target model.
Among these methods, rotated quantization methods demonstrate superior performance in 4-bit weight-activation quantization.