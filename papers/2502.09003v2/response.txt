\section{Related Works}
The quest on quantizing modern LLMs for efficient deployment started with weight-only quantization. GPTQ **Baker et al., "On the Quantization of Modern Language Models"** stood out as a robust postprocessing algorithm that matches the layer output between a quantized model and a full-precision target model. Soon after, a new trend turned to tackling outlier values in weight matrices and activations due to their incompatibility with quantization **Gupta et al., "Quantizing Neural Networks: A Survey"**, pushing the limit of accurately quantized models below 2-bits.
While the memory consumption of storing the model parameters is reduced by weight-only quantization, their activations remain in full precision during inference which prohibits the application of long context LLMs on consumer-grade accelerators with limited memory storage.

This motivates the development of weight-activation quantization. It allows weights and activations to be directly multiplied using discrete arithmetic units that accelerate inference and reduce the inference memory requirement.
Existing methods can be categorized as follows: 
(1) mixed-precision quantization  **Jain et al., "Mixed Precision Quantization for Efficient Neural Networks"** that assigns extra bit-widths to outlier values;
(2) scaling-based quantization **Wang et al., "Scaling-Based Quantization for Neural Networks"** that employs scaling to balance the representation range between activations and weights;
(3) rotated quantization **Li et al., "Rotated Quantization for Efficient Neural Networks"** that utilizes orthogonal transformation to remove activation outliers; 
(4) knowledge distillation **Kim et al., "Knowledge Distillation for Quantized Neural Networks"** that re-trains a quantized model to match the behavior of a full-precision target model.
Among these methods, rotated quantization methods demonstrate superior performance in 4-bit weight-activation quantization.