\section{Related Works}
% In this section, we provide a brief overview of prior works related to our \emph{Double Rounding}.
% {\bf Model quantization} 
% Model quantization is a technique that refers to converting floating-point values into integers to reduce model storage and accelerate model inference. It is commonly divided into uniform quantization~\cite{Choi2018,li2021mqbench} and non-uniform quantization~\cite{Miyashita2016,Li2019}. Some need special optimization methods including binary~\cite{Rastegari2016,Hubara2016}, ternary neural networks~\cite{Courbariaux2015}, and mixed precision networks~\cite{Wu2018,Cai2020a}. It is important to select appropriate quantization methods according to the characteristics of the training data and the model architecture.

{\bf Multi-Precision.}
Multi-Precision entails a single shared model with multiple precisions by one-shot joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching for the entire model according to computing resources and storage constraints. AdaBits~\cite{Jin2019} is the first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization on ResNet50~\cite{He2015}. TQ~\cite{zhang2021training} quantizes weights or activation values by selecting a specific number of power-of-two terms. BitWave~\cite{shi2024bitwave} is designed to leverage structured bit-level sparsity and dynamic dataflow to reduce computation and memory usage. Bit-Mixer~\cite{Bulat2021} addresses this problem by using the LSQ~\cite{Esser2019} quantization method but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision joint QAT can also be viewed as a multi-objective optimization problem. Any-precision~\cite{Yu2021} and MultiQuant~\cite{Xu2022} combine knowledge distillation techniques to improve model accuracy. Among these methods, MultiQuant's proposed ``Online Adaptive Label" training strategy is essentially a form of self-distillation~\cite{kim2021self}. Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while other methods rely on 32-bit models for bit switching. Our \emph{Double Rounding} method can store the highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable optimization process. Importantly, this leads to a reduction in training time by approximately 10\%~\cite{Du2020} compared to separate quantization training.

{\bf One-shot Mixed-Precision.} 
Previous works mainly utilize costly approaches, such as reinforcement learning~\cite{Wang2019,elthakeb2019releq} and Neural Architecture Search (NAS)~\cite{wu2018mixed,guo2020single,Shen2021}, or rely on partial prior knowledge~\cite{liu2021sharpness,yao2021hawq} for bit-width allocation, which may not achieve global optimality. In contrast, our proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet via gradient updates, and then obtain the optimal conditional SubNets with less search cost without retraining or fine-tuning. Additionally, Bit-Mixer~\cite{Bulat2021} and MultiQuant~\cite{Xu2022} implement layer-adaptive mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution, while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS approaches~\cite{Shen2021}, which focus on altering network architecture (e.g., depth, kernel size, or channels), our method optimizes a once-for-all SuperNet using only quantization techniques without altering the model architecture.