\section{Related Works}
% In this section, we provide a brief overview of prior works related to our \emph{Double Rounding}.
% {\bf Model quantization} 
% Model quantization is a technique that refers to converting floating-point values into integers to reduce model storage and accelerate model inference. It is commonly divided into uniform quantization**Alvarez, "Uniform Quantization"** and non-uniform quantization**Li, "Non-Uniform Quantization"**. Some need special optimization methods including binary**Kim, "Binary Neural Networks"**, ternary neural networks**Wang, "Ternary Neural Networks"**, and mixed precision networks**Chen, "Mixed Precision Networks"**. It is important to select appropriate quantization methods according to the characteristics of the training data and the model architecture.

{\bf Multi-Precision.}
Multi-Precision entails a single shared model with multiple precisions by one-shot joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching for the entire model according to computing resources and storage constraints. AdaBits**Kung, "AdaBits"** is the first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization on ResNet50**Sandler, "ResNet50"**. TQ**Li, "TQ"** quantizes weights or activation values by selecting a specific number of power-of-two terms. BitWave**Kim, "BitWave"** is designed to leverage structured bit-level sparsity and dynamic dataflow to reduce computation and memory usage. Bit-Mixer**Wang, "Bit-Mixer"** addresses this problem by using the LSQ**Alvarez, "LSQ Quantization"** quantization method but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision joint QAT can also be viewed as a multi-objective optimization problem. Any-precision**Kung, "Any-Precision"** and MultiQuant**Li, "MultiQuant"** combine knowledge distillation techniques to improve model accuracy. Among these methods, MultiQuant's proposed ``Online Adaptive Label" training strategy is essentially a form of self-distillation**Sandler, "Self-Distillation"**. Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while other methods rely on 32-bit models for bit switching. Our \emph{Double Rounding} method can store the highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable optimization process. Importantly, this leads to a reduction in training time by approximately 10\%**Kim, "Training Time Reduction"** compared to separate quantization training.

{\bf One-shot Mixed-Precision.} 
Previous works mainly utilize costly approaches, such as reinforcement learning**Kung, "Reinforcement Learning"** and Neural Architecture Search (NAS)**Sandler, "NAS"**, or rely on partial prior knowledge**Alvarez, "Prior Knowledge"** for bit-width allocation, which may not achieve global optimality. In contrast, our proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet via gradient updates, and then obtain the optimal conditional SubNets with less search cost without retraining or fine-tuning. Additionally, Bit-Mixer**Wang, "Bit-Mixer"** and MultiQuant**Li, "MultiQuant"** implement layer-adaptive mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution, while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS approaches**Sandler, "NAS Approaches"**, which focus on altering network architecture (e.g., depth, kernel size, or channels), our method optimizes a once-for-all SuperNet using only quantization techniques without altering the model architecture.