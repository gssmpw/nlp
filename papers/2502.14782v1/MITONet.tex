\documentclass[draft]{agujournal2019}
\usepackage{url} 
\usepackage{lineno}
\usepackage{soul}
\usepackage{amsfonts,amsmath} 
\usepackage{multirow,hhline}
\usepackage{mathtools}
\usepackage{colortbl} 
\usepackage{xcolor}     
% \linenumbers

\draftfalse

\journalname{}

\begin{document}

\title{A Neural Operator-Based Emulator for Regional Shallow Water Dynamics}

\authors{Peter Rivera-Casillas\affil{1,2}\thanks{peter.g.rivera-casillas@erdc.dren.mil}, Sourav Dutta\affil{3}, Shukai Cai\affil{3}, Mark Loveland\affil{1,3},
Kamaljyoti Nath\affil{4},
Khemraj Shukla\affil{4},
Corey Trahan\affil{1}, Jonghyun Lee\affil{2}, Matthew Farthing\affil{5}, Clint Dawson\affil{3}}

\affiliation{1}{Information Technology Laboratory, U.S. Army Engineer Research and Development Center, MS, USA}
\affiliation{2}{Civil, Environmental and Construction Engineering \& Water Resources Research Center, University of Hawai’i at Manoa, HI, USA}
\affiliation{3}{Oden Institute for Computational Engineering and Sciences, The University of Texas at Austin, TX, USA}
\affiliation{4}{Division of Applied Mathematics, Brown University, Providence, RI, USA}
\affiliation{5}{Coastal and Hydraulics Laboratory, U.S. Army Engineer Research and Development Center, MS, USA}

\correspondingauthor{Peter Rivera-Casillas}{peter.g.rivera-casillas@erdc.dren.mil}

\begin{keypoints}
\item We present MITONet, an autoregressive neural operator framework that accurately emulates shallow-water dynamics in a coastal environment.
\item The trained models demonstrate strong extrapolation capabilities.
\item The MITONet framework can be extended to other applications.
\end{keypoints}

\begin{abstract}
Coastal regions are particularly vulnerable to the impacts of rising sea levels and extreme weather events. Accurate real-time forecasting of hydrodynamic processes in these areas is essential for infrastructure planning and climate adaptation. In this study, we present the Multiple-Input Temporal Operator Network (MITONet), a novel autoregressive neural emulator that employs dimensionality reduction to efficiently approximate high-dimensional numerical solvers for complex, nonlinear problems that are governed by time-dependent, parameterized partial differential equations. Although MITONet is applicable to a wide range of problems, we showcase its capabilities by forecasting regional tide-driven dynamics described by the two-dimensional shallow-water equations, while incorporating initial conditions, boundary conditions, and a varying domain parameter. We demonstrate MITONet's performance in a real-world application, highlighting its ability to make accurate predictions by extrapolating both in time and parametric space.
\end{abstract}

\section*{Plain Language Summary}
The occurrence and impact of catastrophic storm surge caused by extreme weather events can be modeled through accurate numerical simulations. However, traditional numerical solvers are computationally intensive and require domain expertise. In this study, we introduce a machine learning framework, MITONet, which learns the governing physics from simulation data. Once trained, MITONet can quickly generate accurate predictions under similar or changing conditions, offering a faster, more accessible tool for planning and decision-making.

\section{Introduction}

Coastal regions are densely populated and host critical infrastructure vulnerable to the impacts of climate change, such as rising sea levels and increased frequency of extreme weather conditions \cite{oppenheimer2019sea}. High-fidelity numerical models of coastal waves and circulation patterns are typically employed to aid in crucial decision-making and planning efforts. Despite recent technological and algorithmic advances, these methods can still be computationally expensive, time consuming, and technically challenging. Machine learning (ML) approaches have been successful in addressing some of these challenges due to their flexibility and computational efficiency \cite{abouhalima2024machine, giaremis2024storm, karim2023review,  pachev2023framework}. 

Predicting behaviors of highly nonlinear systems remains a persistent challenge in computational modeling. Although the universal function approximation property of neural networks has allowed deep learning to make substantial progress in the development of surrogate models, achieving consistent generalizability remains a challenge. To this end, operator approximation has recently emerged as an increasingly popular field of study in deep learning \cite{azizzadenesheli2024neural}. The earliest attempts included efforts like nonlinear integro-differential operator regression \cite{patel2018nonlinear}, where deep neural networks (DNNs) and Fourier pseudospectral methods were employed to recover complex operators from data, and physics-informed neural networks (PINNs), where DNNs were used to regress partial differential equation (PDE) models by exploiting physics knowledge \cite{raissi2019physics}. While useful in certain applications, these and other earlier approaches had limitations in scalability and generalizability across varying domains of physics, highlighting the challenges of learning high-dimensional systems and multiscale features.

The introduction of the Deep Operator Network (DeepONet) \cite{lu2021learning} marked a new era in operator approximation. DeepONet addressed many of the issues from earlier approaches by leveraging the universal approximation theorem for operators \cite{chen1995universal}, enabling the network to efficiently map between infinite-dimensional function spaces. Subsequent developments in neural operators include the Fourier neural operator (FNO) \cite{li2020fourier}, graph neural operators \cite{li2020neural}, transformer neural operators \cite{cao2021choose}, and the variationally mimetic operator network \cite{patel2024variationally}, among others, many of these with multiple extensions. 

Another critical challenge in data-driven modeling of nonlinear, real-world physical systems that has been an active area of research is the ability to forecast over long lead times \cite{10.1145/3533382, bodnar2024aurora, lam2022graphcast, lim2021time}. Recent advances, including DiTTO \cite{ovadia2023ditto}, the Universal Physics Transformer \cite{alkin2024universal}, and OceanNet \cite{chattopadhyay2024oceannet}, demonstrate the effectiveness of neural operator-based autoregressive models in overcoming some of these challenges by addressing the spectral bias inherent in all neural network models. 

This research focuses on the DeepONet architecture and subsequent extensions \cite{jin2022mionet, kontolati2023learning, ovadia2023ditto, wang2022improved} that demonstrate improved performance. We specifically focus on application to modeling real-world ocean dynamics, which was previously explored in \citeA{10337380}, where latent DeepONet and FNO are used for one-shot forecasting of ocean dynamics. Similarly, \citeA{choi2024applications} compares FNO variations and explores a recursive approach for one-shot forecasting of ocean dynamics given an initial condition (IC). Both efforts demonstrate the potential of neural operators to predict ocean dynamics; however, their usefulness is limited by their structure and data requirements, relying solely on initial ocean states without incorporating external forcings as boundary conditions (BC), which are crucial for real-world scenarios. Additionally, neither handles varying domain parameters, limiting their ability to generalize in dynamic environments. Another recent study developed an FNO-based digital twin model that autoregressively predicts the sea-surface height of the northwest Atlantic Ocean’s western boundary current \cite{chattopadhyay2024oceannet}, using IC and external BC forcings like wind stress and tides. The model performs comparably to traditional numerical models over long forecasting windows and offers a significant computational speed-up. However, its predictions are limited to sea-surface height and do not include currents, restricting its ability to fully address comprehensive hydrodynamic forecasting.

We introduce a new operator learning framework for time series forecasting of regional coastal hydrodynamics, denoted as the multiple-input temporal operator network (MITONet), that is capable of parametric generalizability and long-term forecasting.

We demonstrate MITONet's potential to tackle computational challenges in forecasting tide-driven dynamics in the nearshore by approximating the PDE operator for two-dimensional shallow-water equations using IC, BC, and variable domain parameters in an autoregressive manner, much like traditional numerical models. 

\section{Methods}
\subsection{Deep Operator Network}
Operator learning focuses on approximating maps between infinite-dimensional functions. In mathematical terms, given data pairs $(f,s)$, where $f \in \mathcal{U} \subset \mathbb{R}^{N_{d1}}$ and $s \in \mathcal{V} \subset \mathbb{R}^{N_{d2}}$ are function spaces, and a potentially nonlinear operator $\cal G : \cal $U$ \mapsto \cal V$ such that $\mathcal{G}(f) = s$, the objective is to find a neural network-based approximation $\mathcal{G}_{NN}$ such that for any new data $f^\prime \in \cal U$, we have $\mathcal{G}_{NN}( f^\prime; \boldsymbol{\theta}) \approx \mathcal{G}(f^\prime)$, where $\boldsymbol{\theta} \in \mathbb{R}^M$ are the parameters of the neural operator. DeepONet seeks to approximate this operator using an i.i.d collection of input-output function pairs $(f_{(i)}(\mathbf{x}), s_{(i)}(\mathbf{y}))_{i=1}^{N_{tr}}$, evaluated over discrete sets of spatial coordinates $\mathbf{x} \in \mathcal{X}$ and $\mathbf{y} \in \mathcal{Y}$, respectively. The standard (unstacked) DeepONet architecture \cite{lu2021learning} comprises two sub-networks denoted as the branch $\mathbf{B}$ and the trunk $\mathbf{T}$ that generate finite-dimensional encodings of the input function and the output coordinates, respectively, as $\mathbf{B}(f(\mathbf{x})) = [b_1, b_2, \dots, b_p]^T \in \mathbb{R}^p$ and $\mathbf{T}(\mathbf{y}) = [t_1, t_2, \dots, t_p]^T \in \mathbb{R}^p$. The output function is approximated as 

\begin{equation}
(\mathcal{G}_{NN}(f))(\mathbf{y}) \approx \mathbf{B}(f(\mathbf{x})) \odot \mathbf{T}(\mathbf{y}) + b_0,
\end{equation}

 where $b_0$ is an optional bias term and $\odot$ is the Hadamard product. Finally, the network parameters, $\theta_B$ and $\theta_T$ are optimized by minimizing a loss function $\mathcal{L}$ that measures the discrepancy between the data and the prediction in a desired norm,
\begin{equation}
\min_{\{\theta_B, \theta_T\}} \sum_{(f,s) \in \text{data}} \mathcal{L}\Big((\mathcal{G}_{NN}(f;\theta_B))(\mathbf{y};\theta_T), \, s(\mathbf{y}) \Big).
\label{eq:operator_loss}
\end{equation}

In this work, we are interested in applying this framework to solve a parametric, initial boundary value problem i.e. modeling the time evolution of the solution operator of a PDE $(s(\mathbf{x},t) \mapsto s(\mathbf{x},t+\delta t)),\; \forall \mathbf{x}\in \Omega$, over a forecasting horizon, $t \in (0,T]$, given IC: $s(\mathbf{x},0)$, BC: $s(\mathbf{x},t)\vert_{\Gamma}$, and domain parameters, $\boldsymbol\mu$. Here $\Omega$ is a domain in  $\mathbb{R}^{N_{d1}}$ with boundary $\Gamma$.


Several variants of DeepONet have been proposed to improve the training robustness and enhance the prediction accuracy for unseen input functions. The Modified Deep Operator Network (M-DeepONet) introduced two additional encoder networks to the architecture, one for the branch and the other for the trunk network. These encoders, uniquely interconnected, facilitated an information exchange between the two primary networks. This, coupled with slight adjustments to the forward pass, improved expressiveness and mitigated vanishing gradient problems \cite{wang2022improved}, although at the cost of increased computational demands. The Latent Deep Operator Network (L-DeepONet) was introduced \cite{kontolati2023learning} to address the scaling challenges of high-dimensional data sets. In this method, autoencoders were employed to embed the data set in a lower-dimensional manifold, and the DeepONet was tasked with learning the functional relationships in this latent space. 

Another crucial limitation of using the standard DeepONets for approximating a PDE solution operator is its inability to handle multiple input functions, such as ICs and BCs. The multiple input operator network (MIONet) addressed this by combining multiple input branches into a single Operator Network \cite{jin2022mionet}. While MIONet provides a framework to handle different ICs, this is mostly useful in autoregressive predictions. Traditionally, DeepONet methods are designed to map an IC to all future times, and typically have limited skill over longer forecast periods. Techniques like temporal bundling can improve stability and enhance rollout time \cite{brandstetter2022message, ovadia2023ditto} by predicting multiple time steps synchronously during training and inference, which reduces solver calls and thus minimizes autoregressive error propagation. 

\subsection{Multiple Input Temporal Operator Network}

The MITONet framework proposed here effectively integrates dimensionality reduction, multi-input handling, and temporal bundling, into a cohesive framework optimized for long-term hydrodynamic forecasting. Figure \ref{fig:mitonet} provides a schematic of the different components of the MITONet architecture. 
First, the discretized PDE solution, $\mathbf{s} \in \mathbb{R}^{N_s}$, defined on a high-dimensional computational mesh $\mathcal{M}_h$, is embedded into a lower-dimensional latent space using an autoencoder network, $\Phi_{AE}$ (see Figure \ref{fig:mitonet}(a)). Specifically, $\mathbf{s} \in \mathbb{R}^{N_s}$ is mapped to $\mathbf{s}^r \in \mathbb{R}^{N_r}$ such that $N_r \ll N_s$ using an encoder network, $\phi_{E}(\mathbf{s}; \theta_E): \mathbf{s} \rightarrow \mathbf{s}^r$, and a decoder network is used to reconstruct the high-dimensional solution,  
$\phi_{D}(\mathbf{s}^r;\theta_D): \mathbf{s}^r \rightarrow \mathbf{s}^*$ such that $\mathbf{s}^* \approx \mathbf{s}$. The autoencoder $\Phi_{AE}: \phi_{D} \circ \phi_{E}(\cdot)$ is trained by minimizing the mean square reconstruction error
\begin{equation}
\min_{\{\theta_E, \theta_D\}} \frac{1}{N_{tr}} \sum_{j=1}^{N_{tr}} \|\mathbf{s}_{(j)} - \mathbf{s}_{(j)}^*(\theta_E, \theta_D)\|_{L^2(\Omega)}^2.
\end{equation}


Following the L-DeepONet \cite{kontolati2023learning} philosophy, once the autoencoder is trained, a neural operator is designed to approximate the latent representation of the solution variable. Consequently, the choice of a dimension reduction approach, while not limited to autoencoders, should be made judiciously as the performance of MITONet will be limited by the latent representation error.

Second, multiple inputs are handled by introducing multiple independent branch networks, following the low-rank MIONet design \cite{jin2022mionet}. Each input function $f_k$ is encoded by a branch network as $\mathbf{B}_k(f_k(\mathbf{x}_{k}))=[b_{k,1}, b_{k,2}, \dots, b_{k,p}]^T \in \mathbb{R}^p$ where $\mathbf{x}_{k} \in \mathcal{X}_k$ are its discrete evaluation points (see Figure \ref{fig:mitonet}(c)). The trunk network is trained to encode only the temporal part of the output function domain, $\widehat{\mathbf{y}} = \mathbf{y}\vert_t$ as $\mathbf{T}(\widehat{\mathbf{y}}) = [t_1, \ldots, t_p]^T \in \mathbb{R}^p$ since the spatial encoding is performed via the pre-trained autoencoder. Combining the above, the latent solution vector is approximated as
\begin{equation}\label{eq:MITONet_ouput}
\mathbf{s}^r \approx (\mathcal{G}_{NN}(f_1, \ldots, f_k))(\widehat{\mathbf{y}}) = \mathbf{B}_1(f_1(\mathbf{x}_1)) \odot \ldots \odot \mathbf{B}_k(f_k(\mathbf{x}_k)) \odot \mathbf{T}(\widehat{\mathbf{y}}) + \mathbf{b}_0,
\end{equation}
where $\mathbf{b}_0 \in \mathbb{R}^p$ is an optional, trainable bias vector. In practice, $p$ is either chosen to be $dim(\mathbf{s}^r) = N_r$ or the right hand side of (\ref{eq:MITONet_ouput}) is multiplied by a trainable matrix, $\mathbf{P} \in \mathbb{R}^{N_r \times p}$ to achieve dimensional parity.

Third, the MITONet architecture is endowed with interconnected encoder networks following the M-DeepONet idea. However, unlike \citeA{wang2022improved}, to facilitate information exchange across multiple inputs, a branch encoder, $\Phi_{B_k}$ is introduced for each branch, $B_k$, and a trunk encoder $\Phi_{T}$ for the trunk network, $T$. These encoders are trained to generate embeddings for the corresponding branch and trunk input functions as given by, 
\begin{eqnarray}
\begin{split}
\mathbf{U}_k &\coloneq \Phi_{B_k}(f_k(\mathbf{x}_k)) = [u_{k,1}, u_{k,2}, \dots, u_{k,q}]^T \in \mathbb{R}^q, \\
\mathbf{W} &\coloneq \Phi_{T}(\mathbf{y}) = [v_1, v_2, \dots, v_q]^T \in \mathbb{R}^q.
\label{eq:MITONet_encoder}
\end{split}
\end{eqnarray}

The purpose of these encoder networks is to improve resilience against vanishing gradients during the training of MITONet by enhancing information propagation through the branch and trunk networks until the information fusion operation in (\ref{eq:MITONet_ouput}). This is enabled by modifying the forward pass of the MITONet as follows
\begin{eqnarray}\begin{split}
\mathbf{H}^{(l+1)}_{B_k} &= (\mathbf{1} - \boldsymbol{\Psi}^{(l+1)}_{B_k}(\mathbf{H}^{(l)}_{B_k})) \odot \mathbf{U_k} + \boldsymbol{\Psi}^{(l+1)}_{B_k}(\mathbf{H}^{(l)}_{B_k}) \odot \mathbf{W}, \\
\mathbf{H}^{(l+1)}_T &= (\mathbf{1} - \boldsymbol{\Psi}^{(l+1)}_{T}(\mathbf{H}^{(l)}_T)) \odot \mathbf{U}_1 \odot \mathbf{U}_2 \odot \dots \odot \mathbf{U}_k + \boldsymbol{\Psi}^{(l+1)}_{T}(\mathbf{H}^{(l)}_T) \odot \mathbf{W},
\label{eq:MITONet_hidden}
\end{split}\end{eqnarray}
where $\mathbf{H}^{(l)}_{B_k}, \mathbf{H}^{(l)}_{T}$ represent the output and $\boldsymbol{\Psi}^{(l)}_{B_k}, \boldsymbol{\Psi}^{(l)}_{T}$ denote the action of the $l^{th}$ hidden layer of the $k^{th}$ branch and the trunk networks, respectively. It is worth noting that (i) for a $L-$layer MITOnet, $\mathbf{H}^{(L)}_T, \mathbf{H}^{(L)}_{B_k} \in  \mathbb{R}^p$ using (\ref{eq:MITONet_ouput}), (ii) $\mathbf{H}^{(l)}_T, \mathbf{H}^{(l)}_{B_k} \in  \mathbb{R}^q \, \forall \, l=1,\ldots L-1$ using (\ref{eq:MITONet_encoder}) and (\ref{eq:MITONet_hidden}), and (iii) the forward pass in (\ref{eq:MITONet_hidden}) is designed to preserve the multi-input structure of the governing PDE, but can be easily modified based on any available information about the underlying input-output functional relationships. 

Finally, MITONet adopts a temporal bundling strategy \cite{brandstetter2022message} to enable long-term forecasting skill. As shown in Fig \ref{fig:mitonet}(b), the model is trained to learn the mapping $\mathbf{s}^r(\cdot, t) \rightarrow \mathbf{s}^r(\cdot, t+ \beta \Delta t) \, \forall \, \beta = \{1, \ldots, \tau\}$, where $\tau$ is a chosen look-forward window such that $t + \tau \Delta t \leq T$. Hence, each training time series is split into $T/\Delta t - \tau + 1$ sub-trajectories of length $\tau$. During inference, MITONet generates $\tau$ predictions, $\hat{\mathbf{s}}^r(\cdot, t) \rightarrow \hat{\mathbf{s}}^r(\cdot, t+ \beta \Delta t) \, \forall \, \beta = \{1, \ldots, \tau\}$, and uses $\hat{\mathbf{s}}^r(\cdot, t+ \tau \Delta t)$ as IC to autoregressively predict the following $\tau$ time steps. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{MITONet_TB.pdf}
    \caption{Schematic representation of the MITONet framework. First, (a) an autoencoder maps the high-dimensional solution snapshots to a low-dimensional latent space. Then, (b) the MITONet is provided with relevant input functions, such as domain parameters, initial conditions or their latent representation at a given time $t$, and boundary conditions for time $t+\beta \Delta t$ to predict the latent representation of the solution snapshot at time $t + \beta \Delta t$, where $\beta = 1, \ldots, \tau$ and $\tau$ is a chosen look-forward window. This procedure is repeated autoregressively to generate a time series of snapshots in the latent space, which are then passed through the decoder network to recover the predictions in the high-dimensional space. To train the MITONet model, a temporal bundling approach is adopted to split the time series of training snapshots into sub-trajectories consisting of $\tau$ outputs for each input snapshot. For $\tau$=$5$, panel (c) illustrates the different sub-trajectories using different colors.}
    \label{fig:mitonet}
\end{figure}

\section{Problem Setup}
\subsection{Numerical Model: ADCIRC}

The numerical model used in this study is the Advanced Circulation (ADCIRC) modeling suite \cite{luettich1992adcirc}, which solves the vertically-integrated Generalized Wave Continuity Equation (GWCE) for water surface elevation \cite{luettich2004formulation} given by:

\begin{equation} \frac{\partial{H}}{\partial{t}} + \frac{\partial}{\partial{x}}(UH) + \frac{\partial}{\partial{y}}(VH)=0 \label{eq:2DSW1} \end{equation}

\noindent where,

\noindent $U, V \equiv \frac{1}{H} \int_{-h}^{\zeta} u,v , dz$: depth-averaged velocities in the $x$ and $y$ directions\\
$u, v$: vertically-varying velocities in the $x$ and $y$ directions\\
$H=\zeta + h$: total water column thickness\\
$\zeta$: free surface departure from the geoid\\
$h$: bathymetric depth

In addition to the continuity equation, ADCIRC solves the depth-averaged momentum equations, which describe the transport of momentum in both horizontal directions. These equations are written in non-conservative form as:

\begin{equation} \frac{\partial{U}}{\partial{t}} + U \frac{\partial{U}}{\partial{x}} + V \frac{\partial{U}}{\partial{y}} - fV=-g \frac{\partial{\zeta}}{\partial{x}} + \frac{\tau_{sx}}{\rho H} - \frac{\tau_{bx}}{\rho H} \label{eq:2DSW2
} \end{equation}

\begin{equation} \frac{\partial{V}}{\partial{t}} + U \frac{\partial{V}}{\partial{x}} + V \frac{\partial{V}}{\partial{y}} + fU=-g \frac{\partial{\zeta}}{\partial{y}} + \frac{\tau_{sy}}{\rho H} - \frac{\tau_{by}}{\rho H} \label{eq:2DSW3
} \end{equation}

\noindent where,

\noindent$f$: Coriolis parameter\\
$g$: gravitational constant\\
$\tau_{sx}, \tau_{sy}$: surface stresses in the $x$ and $y$ directions\\
$\tau_{bx}, \tau_{by}$: bottom stresses in the $x$ and $y$ directions\\
$\rho$: density of water


\subsection{Study Domain: Shinnecock Inlet}
The numerical example used in this study is an ADCIRC simulation of tidal hydrodynamics in the vicinity of Shinnecock Inlet, a geographical feature located along the outer shore of Long Island, New York. This example was derived from a study conducted by the US Army Corps of Engineers Coastal Hydraulics Laboratory \cite{morang1999shinnecock, militello2001shinnecock}, and is a commonly used test case for ADCIRC. The simulation utilizes a finite element grid containing $5780$ elements and $3070$ nodes, with the grid discretization varying from approximately $2$ km offshore to around $75$ m near the inlet and near-shore areas, which helps capture the challenging and complex circulation patterns near the inlet and the back bay. Figure \ref{fig:bathy} shows a visualization of the simulation grid overlaid on to the bathymetry field. 

\begin{figure}[h]
    \centering    \includegraphics[width=0.75\textwidth]{bathymetry_map.pdf}
    \caption{Mesh and bathymetry for the Shinnecock Bay.}
    \label{fig:bathy}
\end{figure}

The model uses an off-shore open boundary with prescribed elevations and a mainland boundary with zero normal flow. The model is forced using tidal elevations at the open boundary from $5$ tidal constituents (M2, S2, N2, O1, K1), ramped up over the first two days from a state of rest. 

Quadratic friction forcing is applied and wetting/drying is neglected. The simulation duration is $60$ days with a $6$-second time step, while water level and velocity fields are output every $30$ minutes, resulting in a total of $2,880$ time snapshots ($N_t$), henceforth denoted by $\{t_i\}_{i=1}^{N_t}$. A $60$-day simulation takes about $20$ minutes on two Frontera CPUs (Intel 8280, 2.7Ghz) \cite{stanzione2020frontera}. The simulation data is parametrized by varying the scalar bottom friction coefficient ($r$). To enable model extrapolation across space and time, the data is carefully split into training, validation, and testing as described in Table \ref{tab:train_val_test}. The training set is used to learn the model parameters, the validation set is used for hyperparameter optimization and learning rate reduction, and the test set is reserved for the final evaluation.

\begin{table}[h]
\caption{Training, Validation, and Testing Split$^{a}$}
\centering
\begin{tabular}{l c c}
\hline
      & $\boldsymbol{r}$                 & \textbf{Days (timesteps)} \\
\hline
\textbf{Training}   & 0.003, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05  & 15-30 (720) \\
\textbf{Validation} & 0.00275, 0.0075, 0.025, 0.075               & 5-15 (480) \\
\textbf{Testing}    & 0.0025, 0.015, 0.1                          & 5-60 (2,640) \\
\hline
\multicolumn{3}{l}{$^{a}$The training, validation, and testing data split by roughness coefficients, days, and timesteps.}
\end{tabular}
\label{tab:train_val_test}
\end{table}

\subsection{MITONet Setup}

The objective of this application is to learn operators for the shallow water equations under tidal forcing, variable $r$, and specified IC to predict key components of the hydrodynamic state: the x-velocity component ($U$), the y-velocity component ($V$), and the water surface elevation ($H$). First, three autoencoder networks are trained to encode $H$, $U$ and $V$ fields into a latent space. 
\begin{align}
\phi_E^S: S \rightarrow S^r; &\qquad \phi_D^S: S^r \rightarrow S^*, \quad S \in \{H,U,V \}
\label{eq:AE_setup}
\end{align}
Second, the MITONet for each variable is designed with three branch networks: (a) IC in the latent space, (b) BC consisting of prescribed tidal elevations at the offshore boundary, and (c) scalar $r$. The trunk network, as described before, only encodes the temporal coordinates. This architecture enables MITONet to capture complex interactions between IC, BC, $r$, and the resulting hydrodynamic responses, similar to traditional numerical solvers but with enhanced flexibility and computational efficiency.

\begin{align}
\mathcal{G}_{NN}^S: S^r_t, H_{t+\beta}\vert_{\Gamma}, r,\beta \Delta t  \rightarrow S^r_{t+\beta}, \quad \forall \beta=\{1,\ldots,\tau\}, \; S \in \{H,U,V \}
\label{eq:MITONet_setup}
\end{align}

\subsection{Hyperparameter Optimization}

As with any deep neural network, DeepONets require meticulous hyperparameter optimization to achieve optimal performance. The literature often cites standard hyperparameter configurations that, while common, do not ensure robust results across various problems. The manual configuration of hyperparameters poses a significant challenge, especially with a large hyperparameter space. To address this challenge,  we employ a hyperparameter optimization tool, Optuna \cite{akiba2019optuna}, to automatically optimize the hyperparameters of the autoencoder and MITONet models (Table \ref{tab:hyperparam_search_space}). This enables efficient exploration of the high-dimensional hyperparameter space, leading to improved model performance. The hyperparameter optimization is configured to run 100 trials for the autoencoder and 200 trials for the MITONet, each running 2,000 epochs per trial. Hyperparameter optimization is performed individually for each model that represents a different hydrodynamic variable.

Note that the number of neurons per layer is not optimized for any of the networks. For the autoencoder, the input layer of the encoder has the same number of neurons as the degrees of freedom of the ADCIRC computational mesh (3070), whereas every subsequent hidden layer has about half the number of neurons as the preceding layer. For the decoder, this network design is applied in reverse order. Therefore, we optimize (a) the number of layers in the encoder (and decoder) model, and (b) the dimension of the latent space, which is equal to the size of the encoder output layer as well as the size of the decoder input layer. For the MITONet, we optimize a hyperparameter (\(L_{factor}\)) that determines the number of neurons in the hidden layers based on the latent space dimension $(\text{neurons} = L_{factor} \times \text{latent space dimension})$; this ensures compatibility between the latent space dimension and the dimension of MITONet's output. Hyperparameter optimization for both the autoencoder and MITONet required approximately 74–96 wall-clock hours on a single NVIDIA A40 GPU. 

\begin{table}[h]
\caption{Hyperparameter optimization space for autoencoder and MITONet architectures.}
\label{tab:hyperparam_search_space}
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{p{.3cm} l l c}
\hline
\multicolumn{1}{c}{} & \textbf{Network} & \textbf{Hyperparameter} & \textbf{Optimization Space} \\ \hline

\multirow{6}{*}{\rotatebox[origin=c]{90}{\textsc{Autoencoder}}}
 & \multirow{4}{*}{Autoencoder Network} 
 & Encoder Activation Function & tanh, elu, relu, swish \\
 && Decoder Activation Function & tanh, elu, relu, swish \\
 && Number of Layers & 2--5 \\
 && Latent Space Dimension & 30--60 \\ \hhline{~---}

 & \multirow{2}{*}{Autoencoder Training} 
 & Batch Size & 64, 128, 256, 512, 1024 \\
 && Initial Learning Rate & 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3 \\ \hline

\multirow{20}{*}{\rotatebox[origin=c]{90}{\textsc{Mitonet}}}
 & \multirow{4}{*}{Branch Network} 
 & Number of Layers & 2--5 \\
 && Activation Function & tanh, elu, relu, swish \\
 && Weight Initializer & he normal, he uniform, glorot normal, glorot uniform \\
 && Regularizer & L1, L2, none \\ \hhline{~---}

 & \multirow{4}{*}{Trunk Network} 
 & Number of Layers & 2--5 \\
 && Activation Function & tanh, elu, relu, swish \\
 && Weight Initializer & he normal, he uniform, glorot normal, glorot uniform \\
 && Regularizer & L1, L2, none \\ \hhline{~---}

 & \multirow{4}{*}{Branch Encoder Network} 
 & Number of Layers & 2--4 \\
 && Activation Function & tanh, elu, relu, swish \\
 && Weight Initializer & he normal, he uniform, glorot normal, glorot uniform \\
 && Regularizer & L1, L2, none \\ \hhline{~---}

 & \multirow{4}{*}{Trunk Encoder Network} 
 & Number of Layers & 2--4 \\
 && Activation Function & tanh, elu, relu, swish \\
 && Weight Initializer & he normal, he uniform, glorot normal, glorot uniform \\
 && Regularizer & L1, L2, none \\ \hhline{~---}

 & \multirow{2}{*}{Shared Parameters} 
 & $\mathcal{L}_{factor}$ & 2--7 \\
 && $\mathcal{L}_{encoder-factor}$ & 1--5 \\ \hhline{~---}

 & \multirow{2}{*}{MITONet Training} 
 & Batch Size & 512, 1024, 2048 \\
 && Initial Learning Rate & 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3 \\ \hline
\end{tabular}
}
\end{table}

\subsection{MITONet Training}

The autoencoder and MITONet models for each solution variable are constructed using multi-layer perceptrons (MLPs), with optimal configurations obtained from hyperparameter optimization studies. The autoencoders are trained to map the ADCIRC computational mesh ($N_s = 3070$) to a latent space of dimension $N_r = 60$ for all hydrodynamic variables. The autoencoders contain between 9.5 and 12 million parameters, whereas the MITONet models contain between 1.3 and 3.5 million parameters. All models are trained for $20,000$ epochs using the Adam optimizer \cite{kingma2015adam} and a learning rate reduction algorithm (ReduceLROnPlateau), which decreases the learning rate by $10\%$ for every $100$ epochs without improvement. Furthermore, a look-forward window of $\tau=5$ (Figure \ref{fig:mitonet}) is adopted for the temporal bundling technique after extensive trial-and-error experiments. For all variables, autoencoder training takes approximately 2 wall-clock hours on a single NVIDIA A40 GPU, whereas MITONet training takes approximately 7.5, 4.5, and 8 wall-clock hours for $H$, $U$, and $V$, respectively.

\subsection{Error Metrics}

The spatial root mean square  error ($RMSE$) is used to evaluate MITONet's accuracy by measuring the square root of the average squared differences between the predicted solution, $\tilde{\mathbf{s}}$ and the true solution, $\mathbf{s}$. This metric is commonly used in hydrodynamic applications as it provides a clear indication of the model's performance, with lower $RMSE$ values indicating higher accuracy and better alignment between the model's predictions and observed data. The $RMSE$ at any time instant $\{t_j\}_{j=1}^{N_t}$ is computed as,

\begin{equation}
RMSE_j = \sqrt{\frac{1}{N_s} \sum_{i=1}^{N_s} (s_{ij} - \tilde{s}_{ij})^2}
\end{equation}

\noindent where, \\
\( N_s: \) spatial degrees of freedom, \\
\( N_t: \) temporal degrees of freedom, \\
\( s_{ij} = \mathbf{s}(\mathbf{y}_{i}, t_{j}): \) true solution at $(\mathbf{y}_{i}, t_j)$, where  $i = 1, \ldots, N_s, j = 1, \ldots, N_t$, \\
\( \tilde{s}_{ij} = \tilde{\mathbf{s}}(\mathbf{y}_{i}, t_{j}): \) predicted solution at $(\mathbf{y}_{i}, t_j)$, where  $i = 1, \ldots, N_s, j = 1, \ldots, N_t$.

\noindent Additionally, we compute the temporal mean of the $RMSE$ as 
\begin{equation}
    \overline{RMSE} = \frac{1}{N_t} \sum_{j=1}^{N_t} RMSE_{j},
\end{equation}
as well as the Anomaly Correlation Coefficient ($ACC$) values for every bottom friction coefficient, $r$. The $ACC$ is used to evaluate the accuracy of the prediction model by measuring the correlation between the predicted solution and the true solution, after removing their respective spatial means. It is widely applied in hydrodynamic studies as it quantifies the alignment of spatial patterns, with values closer to 1 indicating better performance.

\[
ACC = \frac{1}{N_t}\left\{ \sum_{j=1}^{N_t}\left[\frac{\sum_{i=1}^{N_s} (\tilde{s}_{ij} - \overline{\tilde{s_j}})(s_{ij} - \overline{s_j})}{\sqrt{\sum_{i=1}^{N_s} (\tilde{s}_{ij} - \overline{\tilde{s_j}})^2 \sum_{i=1}^{N_s} (s_{ij} - \overline{s_j})^2}} \right] \right\}
\]

\noindent where, \\
\(\overline{s_j}= \frac{1}{N_s} \sum_{i=1}^{N_s} s_{ij}:\) spatial mean of the true solution \\
\(\overline{\tilde{s_j}} = \frac{1}{N_s} \sum_{i=1}^{N_s} \tilde{s}_{ij}:\) spatial mean of the predicted solution

\section{Results \& Discussion}

MITONet can generate predictions for a 55-day time period on a standard laptop, equipped with a 2 GHz Quad-Core Intel Core i5 CPU, in $2$ to $4$ seconds, depending on the model size, thus achieving a 300x to 600x speed-up over the physics-based numerical solver, ADCIRC. We demonstrate MITONet's accuracy by computing summary statistics described in section 3.3, and analyzing predictions at three specific sensor locations (Figure \ref{fig:sensors}). These locations were strategically selected to evaluate MITONet's capability to emulate varying dynamical features that occur in different regions of the domain: the bay, the inlet, and the back bay, which are typically modeled by ADCIRC using numerical grids of variable resolutions. Unless otherwise specified, all MITONet predictions are rolled out from an unseen IC at day 5.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{sensors_combined.pdf}
    \caption{The model predictions are evaluated at three sensor locations that exhibit distinct flow dynamics. Panel (a) shows the locations of the sensor with different colors: sensor 1 (blue) - inner bay, sensor 2 (yellow) - inlet, and sensor 3 (green) - bay. Panel (b) shows a close-up of the sensor locations in the inlet and inner bay area.}
    \label{fig:sensors}
\end{figure}

\subsection{Hyperparameter Optimization}

The best configurations from the  hyperparameter optimization are shown in Table \ref{tab:best_hyperparams}. Although no consistent patterns are observed in the resulting hyperparameters for $H$, $U$ and $V$, certain similarities exist.  Notably, similarities exist between $H$ and $U$, as well as between $U$ and $V$, whereas few hyperparameters are shared between $H$ and $V$, except for a few that are common across all variables, such as some regularizers, the number of layers in the autoencoder and the autoencoder training hyperparameters.

\begin{table}[h]
\centering
\caption{Best Configurations for autoencoder and MITONet architectures.}
\label{tab:best_hyperparams}
\resizebox{\textwidth}{!}{
\begin{tabular}{p{.3cm} l l c c c}
\hline
\multicolumn{1}{c}{} & \textbf{Network} & \textbf{Hyperparameter} & $\boldsymbol{H}$ & $\boldsymbol{U}$ & $\boldsymbol{V}$ \\ 
\hline

\multirow{6}{*}{\rotatebox[origin=c]{90}{\textsc{Autoencoder}}}
 & \multirow{4}{*}{Autoencoder Network} 
 & Encoder Activation Function & tanh & swish & swish \\
 && Decoder Activation Function & tanh & swish & tanh \\
 && Number of Layers & 3 & 2 & 2 \\
 && Latent Space Dimension & 60 & 60 & 60 \\ \hhline{~-----}

 & \multirow{2}{*}{Autoencoder Training} 
 & Batch Size & 64 & 64 & 64 \\
 && Initial Learning Rate & 5.00E-05 & 5.00E-05 & 5.00E-05 \\ \hline

\multirow{20}{*}{\rotatebox[origin=c]{90}{\textsc{Mitonet}}}
 & \multirow{4}{*}{Branch Network} 
 & Number of Layers & 3 & 5 & 5 \\
 && Activation Function & elu & elu & relu \\
 && Weight Initializer & he uniform & he uniform & glorot normal \\
 && Regularizer & none & none & none \\ \hhline{~-----}

 & \multirow{4}{*}{Trunk Network} 
 & Number of Layers & 4 & 4 & 2 \\
 && Activation Function & elu & swish & tanh \\
 && Weight Initializer & he normal & he normal & glorot normal \\
 && Regularizer & none & l1 & l2 \\ \hhline{~-----}

 & \multirow{4}{*}{Branch Encoder Network} 
 & Number of Layers & 4 & 3 & 3 \\
 && Activation Function & relu & tanh & elu \\
 && Weight Initializer & he uniform & glorot uniform & glorot normal \\
 && Regularizer & l1 & none & none \\ \hhline{~-----}

 & \multirow{4}{*}{Trunk Encoder Network} 
 & Number of Layers & 3 & 2 & 3 \\
 && Activation Function & relu & tanh & swish \\
 && Weight Initializer & glorot uniform & glorot uniform & he uniform \\
 && Regularizer & none & none & none \\ \hhline{~-----}

 & \multirow{2}{*}{Shared Parameters} 
 & $\mathcal{L}_{factor}$ & 4 & 5 & 6 \\
 && $\mathcal{L}_{encoder factor}$ & 3 & 5 & 5 \\ \hhline{~-----}

 & \multirow{2}{*}{MITONet Training} 
 & Batch Size & 512 & 1024 & 512 \\
 && Initial Learning Rate & 5.00E-04 & 5.00E-04 & 5.00E-05 \\ \hline
\end{tabular}
}
\end{table}

The ‘best’ hyperparameter configurations were continuously updated throughout the optimization, suggesting that optimizing over more trials could produce better models. Additionally, more epochs per trial, expanding the optimization space, or making it more flexible, could improve the top configurations. In this context, studying the relative importance of hyperparameters could offer valuable insights to guide future hyperparameter optimization efforts. 

Table \ref{tbl:importance} presents the hyperparameter importance values based on the completed trials, as computed by Optuna using the fANOVA hyperparameter importance evaluation algorithm \cite{hutter2014fanova}. The results indicate that the two hyperparameters controlling the training of the MITONet models (i.e., batch size and initial learning rate) are the most influential for each solution variable, whereas there are no conclusive trends in the relative importance of the remaining hyperparameters associated with the architecture of the MITONet models. On the other hand, the hyperparameter importance analysis for the autoencoder models reveal a different trend. The training-related hyperparameters of the autoencoder model for $U$ appear to have a higher impact compared to the architecture-related hyperparameters, whereas the opposite trend can be observed for the autoencoder models of $H$ and $V$. 

It is worth noting that in the proposed framework, for a given variable, the training is performed sequentially, where the autoencoder model is trained first, followed by the MITONet model which is trained using the latent space representation generated by the pre-trained autoencoder. Hence, the hyperparameter importance studies are conducted separately for the autoencoder and the MITONet models of each solution variable, and as such, the importance values of the autoencoder hyperparameters are independent of those of the MITONet hyperparameters. 

A key challenge of hyperparameter optimization under sequential training is that the latent space dimension is optimized for reconstruction quality alone, without accounting for its effectiveness in the downstream task. Since the latent space dimension had low importance in the autoencoder optimization, but significantly affected MITONet’s accuracy, the selection made during hyperparameter optimization did not necessarily yield the best downstream performance. For instance, the latent space dimension was found to be 60, 40, and 60 for $H$, $U$, and $V$, respectively. However, evaluation of the trained MITONet model for $U$ revealed that a latent space dimension of 60 significantly improved accuracy, leading us to override the optimized value of 40 with 60. While a larger latent space generally improves reconstruction accuracy and performance in downstream tasks \cite{kontolati2023learning, dutta2021data}, an excessively large latent space can introduce computational bottlenecks and increase the risk of overfitting. 

To fully capture the interactions between the autoencoder and MITONet hyperparameters, a concurrent training framework could be considered. This approach would inherently optimize the latent space for both reconstruction and downstream performance simultaneously. However, such a framework would involve a significantly larger joint parameter space that would lead to training complexities and potentially prohibitive computational and memory costs, and is, therefore, not considered in this work. An interesting avenue of future work would involve exploring such ideas to estimate the optimal latent space dimension through a broader hyperparameter search, particularly for scenarios where computational efficiency is a key constraint.

\begin{table}[h]
 \centering
  \caption{Hyperparameter importance for autoencoder and MITONet architectures$^{a}$.}
  \label{tbl:importance}
  \includegraphics[width=\linewidth]{Importance.pdf}
  \raggedright
  \textsuperscript{a} Darker to lighter shades of red represent descending order of importance.
\end{table}

\subsection{Long-Term Forecasting}

Figure \ref{fig:box} summarizes the results for all parameter values, with a box-and-whisker plot of the $RMSE$ between the MITONet prediction and the ADCIRC solution. The $RMSE$ values for $H$ are the lowest. This is expected because it is likely easier to predict the water surface elevation field given a tidal elevation BC, than it is to predict currents from the same BC. Higher $r$ values i.e. higher frictional resistance introduces a lag in the arrival times between the sensors at the bay and the back bay which results in more complicated dynamics for water surface elevation and thus, higher $RMSE$ values for $H$ in these cases. In fact, the median $RMSE$ for $H$ when $r=0.1$ is more than double of the other cases. Nonetheless, even in this case, the maximum $RMSE$ for $H$ remains below $2.5$~cm, which is relatively small compared to previous modeling efforts in this region \cite{militello2001shinnecock, lin2022numerical}. In contrast, for $U$ and $V$ fields, a lower $r$ value results in a more advection-dominated flow regime with complex, localized flow features especially in the inlet region, and thus leads to higher median $RMSE$ values. In agreement with this behavior, the highest $RMSE$ for $U$ and $V$ occurs when $r=0.0025$. On the other hand, higher $r$ results in lower median $RMSE$ but increases the number of outliers, with some outliers reaching $5$~cm/s. Once again, these errors remain relatively small compared to previous modeling efforts in this region \cite{militello2001shinnecock}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{box.pdf}
    \caption{Box-and-whiskers plot of the $RMSE$ for all $r$ values between days 5 and 60 for $H$ (Row 1), $U$ (Row 2), and $V$ (Row 3). The range of values for each hydrodynamic variable across the spatiotemporal domain is presented in the legend of each subplot.}
    \label{fig:box}
\end{figure}

The $RMSE$ time series for all test cases, as shown in Figure \ref{fig:rmse}, support the previous observations while further demonstrating MITONet's stability during 55 days (2640 time steps) of rollout prediction starting from an unseen IC. Notably, MITONet maintains the level of prediction accuracy throughout the entire prediction horizon, showing no signs of accuracy loss due to error accumulation, thus providing further evidence of its ability to perform long-term autoregressive extrapolation.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{rmse.pdf}
    \caption{Time series of the $RMSE$ of MITONet predictions for all test $r$ values between days 5 and 60 for $H$ (Row 1), $U$ (Row 2), and $V$ (Row 3). The dashed lines at days 15 and 30 represent the endpoints of the time series used in the training data.}
    \label{fig:rmse}
\end{figure}

Time series comparison of MITONet and ADCIRC predictions for each variable at the sensor locations is shown in Figure \ref{fig:signals_mixed}. We show the cases with the highest $RMSE$ values i.e. $r=0.1$ for $H$ and $r=0.0025$ for $U$ and $V$, to demonstrate the model's capability to extrapolate in both parametric space and time. It is worth noting that the MITONet predictions are generated autoregressively, starting from an unseen IC at day 5, whereas Figure \ref{fig:signals_mixed} shows the comparison between day 50 to day 60 to highlight MITONet's performance after 45 days (2,160 time steps) of autoregressive rollout. This demonstrates that even when extrapolating in parameter space, MITONet's long-term temporal forecast is stable and in excellent agreement with the numerical solutions for $H$, $U$ and $V$ at the sensor locations. Full comparisons for all variables and test cases are shown in Figures S1-S3.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{signals_mixed.pdf}
    \caption{MITONet vs. ADCIRC solution at three different sensor locations (see Figure \ref{fig:sensors}) from day 50 to day 60 using test $r=0.1$ for $H$ (Row 1), and $r=0.0025$ for $U$ and $V$ (Rows 2 and 3)}
    \label{fig:signals_mixed}
\end{figure}

Figure \ref{fig:snaps_mixed} shows snapshots of MITONet and ADCIRC predictions at day 60 using $r=0.1$ for $H$ (column 1) and $r=0.0025$ for $U$ and $V$ (columns 2 and 3, respectively). We show the full domain, and a zoomed-in view of the inlet to highlight important features. Overall, we see great agreement between MITONet and ADCIRC at all scales, demonstrating that the model can emulate different types of flow regimes and accurately learn both large-scale behavior as well as fine-scale features. The differences between MITONet and ADCIRC occur primarily near the corners of the domain and are noticeable, especially for $U$ and $V$ (see Figure \ref{fig:snaps_mixed}), near the north-west corner of the domain where the off-shore open boundary meets the zero normal flow mainland boundary. These are non-physical artifacts, often observed in ADCIRC simulations, that primarily arise from numerical and mesh instabilities induced by the advective transport terms, boundary treatments, and grid specifications \cite{RTI2015_adcirc}. Efforts to address these numerical instabilities with techniques such as the modification of the mesh near the boundary, smoothing bathymetry, turning off boundary forcing of several smaller tidal harmonics, ignoring advection terms and/or its derivatives, and many more usually only help in delaying the formation of the instability. Hence, if possible, the simulation domain is designed in a way such that these effects do not influence the solution near the region of interest (such as the inlet area of the Shinnecock Bay). Therefore, the discrepancies between MITONet and ADCIRC caused by these artifacts are ignored in this work. For the interested readers, snapshots for all variables and test cases are presented in Figures S4-S6.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{snaps_mixed.pdf}
    \caption{Snapshots of the ADCIRC solution and MITONet predictions, respectively, using $r=0.1$ for $H$ (column 1), and $r=0.0025$ for $U$ and $V$ (columns 2 and 3) on day 60. We show the full domain (Rows 1 and 2) and zoom in at the inlet (Rows 3 and 4) to highlight important features.}
    \label{fig:snaps_mixed}
\end{figure}

Finally, the $\overline{RMSE}$ and the $ACC$ for a $55$-day rollout prediction starting from an unseen IC at day $5$ are presented in Table \ref{tab:transposed_metrics_simplified}.  As expected, the test cases (highlighted) exhibit the lowest $ACC$ values and the highest $\overline{RMSE}$ values. Nonetheless, the models still demonstrate high accuracy in these scenarios, with a lowest $ACC$ value of $0.91$. Because $\overline{RMSE}$ values are not relative, they can be misleading, particularly for $U$ and $V$,  where the spatial mean varies greatly across roughness coefficients. However, the $ACC$ values, which remove the spatial mean, indicate a strong correlation between the predictions and the reference solutions.

\definecolor{gray50}{gray}{0.9}  % Adjust as needed

\renewcommand{\arraystretch}{1.25} 
\setlength{\tabcolsep}{18pt} 
\begin{table}[h]
\caption{$ACC$ and $\overline{RMSE}$ for variables $H$, $U$, and $V$ across all $r$ values, for a $55$-day rollout prediction starting at day $5$. The rows with the test parameter cases are highlighted in gray.}
\label{tab:transposed_metrics_simplified}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c c c c c c c}
\hline
& \multicolumn{2}{c}{$\boldsymbol{H}$} & \multicolumn{2}{c}{$\boldsymbol{U}$} & \multicolumn{2}{c}{$\boldsymbol{V}$} \\ \hline
{$\boldsymbol{r}$} & {$\boldsymbol{ACC}$} & $\boldsymbol{\overline{RMSE}}$
                    & {$\boldsymbol{ACC}$} & $\boldsymbol{\overline{RMSE}}$
                    & {$\boldsymbol{ACC}$} & $\boldsymbol{\overline{RMSE}}$ \\ \hline
\rowcolor{gray50}
\textbf{0.0025}  & 0.99821 & 0.21120 & 0.91555 & 2.97988 & 0.94484 & 2.65282 \\ \hline
\textbf{0.00275} & 0.99871 & 0.18413 & 0.98359 & 1.27530 & 0.98930 & 1.24926 \\ \hline
\textbf{0.003}   & 0.99881 & 0.17918 & 0.99212 & 0.82956 & 0.99419 & 0.93448 \\ \hline
\textbf{0.005}   & 0.99910 & 0.17296 & 0.96168 & 1.84690 & 0.99165 & 1.12973 \\ \hline
\textbf{0.0075}  & 0.99921 & 0.17264 & 0.99230 & 0.71786 & 0.99564 & 0.58750 \\ \hline
\textbf{0.01}    & 0.99921 & 0.17909 & 0.99205 & 0.70561 & 0.99408 & 0.59858 \\ \hline
\rowcolor{gray50}
\textbf{0.015}   & 0.99914 & 0.19163 & 0.99016 & 0.65017 & 0.99470 & 0.46706 \\ \hline
\textbf{0.02}    & 0.99906 & 0.20321 & 0.98923 & 0.59921 & 0.99345 & 0.44773 \\ \hline
\textbf{0.025}   & 0.99899 & 0.21241 & 0.98886 & 0.57069 & 0.99239 & 0.42793 \\ \hline
\textbf{0.03}    & 0.99891 & 0.22001 & 0.98927 & 0.54846 & 0.99133 & 0.42222 \\ \hline
\textbf{0.04}    & 0.99878 & 0.23277 & 0.99114 & 0.47780 & 0.99102 & 0.39500 \\ \hline
\textbf{0.05}    & 0.99860 & 0.24606 & 0.99284 & 0.41969 & 0.99024 & 0.38156 \\ \hline
\textbf{0.075}   & 0.99816 & 0.30936 & 0.99444 & 0.41266 & 0.97991 & 0.50376 \\ \hline
\rowcolor{gray50}
\textbf{0.1}     & 0.99760 & 0.83893 & 0.99123 & 0.59031 & 0.95490 & 0.83154 \\ \hline
\end{tabular}
}
\end{table}

\subsection{Random Initialization}

Aside from its robust long-term forecasting capabilities, MITONet can seamlessly handle different IC and provide accurate results. We demonstrate MITONet's flexibility by making 5-day predictions starting from different unique ICs. The results of this analysis are presented in Figure \ref{fig:random_hot}, where the $\overline{RMSE}$ values are shown for all $r$ test cases and for all three hydrodynamic variables, $H$, $U$, and $V$. The $\overline{RMSE}$ follows the same trend, as previously discussed in Table \ref{tab:transposed_metrics_simplified}. The main goal of this analysis is to showcase MITONet's capability to handle different IC and produce consistent results, commonly referred to as a ``hotstart'' in numerical simulations, as confirmed by the relatively uniform $\overline{RMSE}$ observed in Figure \ref{fig:random_hot}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{random_hotstart.pdf}
    \caption{Bar plots of $\overline{RMSE}$ values for all three hydrodynamic variables across all $r$ test cases. The horizontal axis denotes different five-day predictions, each starting with a unique IC.}
    \label{fig:random_hot}
\end{figure}

Another important advantage of MITONet is the ability to make predictions starting from rest (zero IC). Figure \ref{fig:var_rest} shows the MITONet predictions for $H$, $U$, and $V$ at the three sensor locations for a period of $15$ days, starting from zero IC at day $45$ and using $\tau=1$. This is compared to the ADCIRC solution for the same time period, that has been ramped up from zero IC at day $0$. Similar to any traditional numerical model, the errors are larger during a ramp-up period (roughly 2 days), after which the model is able to accurately emulate all relevant features of the high-fidelity solution (Figure \ref{fig:var_rest}). Besides demonstrating MITONet’s stability and flexibility, these findings strongly indicate that MITONet is learning the governing physics of the system and functioning as an efficient neural emulator.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{var_rest_lf_1.pdf}
    \caption{Comparison of the ADCIRC solution for $H$ (row 1), $U$ (row 2), and $V$ (row 3) using $r=0.015$ at the three sensor locations with the MITONet predictions generated from zero IC and with a look-forward window of 1.}
    \label{fig:var_rest}
\end{figure}

\section{Conclusions} 

As coastal regions become more densely populated and continue to host critical infrastructure that is vulnerable to the impacts of climate change, developing more accurate and efficient models for simulating ocean dynamics is essential. In this work, we introduced MITONet, a high-level, data-driven, physics-guided neural operator framework, that is capable of approximating multiple-input operators defined on a product of different function spaces. We presented MITONet's performance in autoregressively approximating the long-term evolution of the solution operator of a parametric, time-dependent, nonlinear PDE governing shallow water flow dynamics in a real-world complex domain using multiple input functions, such as variable initial conditions, time-dependent boundary conditions, and a variable scalar domain parameter. 

The significant computational efficiency over traditional numerical models in predicting tide-driven hydrodynamics in a real-world domain, such as the Shinnecock Inlet, showcases MITONet's capability in providing accurate, real time predictions of complex flow phenomena with multiscale features. Our study highlights the role of hyperparameter optimization in improving predictive accuracy, demonstrating that automated tuning of model parameters can enhance performance while balancing computational efficiency. Furthermore, MITONet has been shown to be effective in handling both unseen and zero IC as well as unseen parametric variations while making long-term forecasts, highlighting its potential as an effective neural emulator for a range of coastal hydrodynamics problems from flood prediction to climate change analysis. Future work will explore performance for extreme dynamics like coastal storm surge, and methods to enhance MITONet’s robustness for conditions beyond the training distribution, including strategies for improved extrapolation and uncertainty quantification.

\section*{Open Research Section}
The data generated using ADCIRC is available via DesignSafe \cite{casillas2024designsafe}. The source code for the MITONet framework will be made available upon publication. 

\acknowledgments
The authors would like to acknowledge the valuable support from the U.S. Army Engineer Research and Development Center (ERDC), in part through the Long Term Training program and also under the Laboratory University Collaboration Initiative (LUCI) with Brown University. The authors would like to thank Prof.~George Karniadakis for his guidance throughout this work, and Dr.~Eirik Valseth for the insightful discussions and constructive suggestions that have  contributed to the improvement of this work. This work was supported in part by high-performance computer time and resources from the DoD High Performance Computing Modernization Program (HPCMP) and the Texas Advanced Computing Center (TACC) at The University of Texas at Austin. Permission was granted by the Chief of Engineers to publish this information.

\bibliography{references}

\clearpage

\appendix 

\input{MITONET_SI.tex}

\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}


