\section{Related Work}
%
%% v0 related starts ==========
%%The problem that we investigate is related to quite a few research areas.
%
%% Ad hoc coordination
%\textit{Ad-Hoc Coordination.}
%Coordination in multi-agent systems has been of great interest for both game theorists and computer scientists for a long history.
%\citet{boutilier1996planning} studied the problem of multi-agent planning where individual agents have to coordinate as there are multiple NEs. The author's focus was merely on coordinating agents by either social conventions or a learned protocol as the agents' interests are aligned with each other. This further leads to a flourishing area named \textit{zero-shot coordination} (or, more generally as \textit{ad-hoc teamwork}) \cite{mirsky2022survey}. A direct application is to the card game \textit{Hanabi}, where players who have never met each other before are supposed to collaborate for higher scores \cite{pmlr-v119-hu20a, pmlr-v139-hu21c}.
%In contrast, an orthogonal line of work of multi-agent reinforcement learning (MARL) \cite{lowe2017multi, foerster2018counterfactual, yu2022the, pmlr-v162-fu22d, JMLR:v24:22-0169, samvelyan19smac} aims at solving a sample NE, which does \textbf{not} exactly fall into the scope of our discussion as they force all agents to execute the NE policy that is found during the training phase.
%
%% opponent modelling
%\textit{Opponent Modelling.}
%\citet{carmel1998explore, carmel1999exploration} investigated a similar problem as ours, which also partly inspired this work. Nevertheless, the authors only discussed applications in toy examples like the \textit{Iterated Prisoner's Dilemma} (IPD). Our work can be viewed as an extension where we put everything in a unified framework and drew a spectrum of formulations as well as implementations in a more complex domain.
%\citet{albrecht2018autonomous} surveyed a number of methodologies on opponent modeling,
%where our work falls into the category of type-based reasoning. The most recently related work goes to \cite{albrecht2019convergence, albrecht2016belief}, where the authors proposed a belief-dependent dynamic programming operator, mainly contributed to the convergence and optimality analysis from the theoretical side. Our work can be seen as a generalized implementation along with empirical studies in a highly non-trivial domain.
%%supplementing the understanding of such problems from the practical side via comprehensive experiments.
%
%% NE; recursive reasoning; bounded rationality
%\textit{Bounded Rationality.}
%%To some extent, we are actually arguing that the solution concept of Nash equilibrium does not offer much help in terms of how to construct an appropriate strategy given that the agent knows almost nothing about her opponents.
%%To best respond to her current hypothetical opponent model is all the agent can do. However, whether it will eventually leads to a whatever equilibrium if all agents follow such best response dynamics should not be our concern.
%%The most significant part is to define how best responses should be. A fully rational agent would presumably do infinitely recursive reasoning, going like ``I believe that you believe that I believe ..'', and do backward induction all the way up to the current state and figure out her best action. In some special cases like two-agent zero-sum games, one can always do minimax search to the terminal state.
%%Nevertheless, in general, it is obviously impossible for computer programs to implement such behavior.
%The most closely related formalization is called \textit{Interactive POMDP} (I-POMDP) \cite{gmytrasiewicz2005framework} modelling bounded rationality via recursive reasoning, where in theory the depth of recursion can be arbitrarily large but in practice it is usually a very small integer like 1 or 2 with toy examples.
%It is also shown in experimental psychology that in most cases human beings merely demonstrate a shallow capability of recursive reasoning. The empirical results  \cite{camerer2004cognitive}  suggest participants on average only do around 1.5 steps of reasoning, that is, assuming their opponents are a fair mixture of random ones and level-1 rational ones. % slightly inaccurate
%
%% POMDP; CMDP
%\textit{POMDP.}
%As we adopt POMDP \cite{kaelbling1998planning} as the underlying model, one can leverage any existing techniques to alleviate the computation burden \cite{littman1995learning, zhang2001speeding}. It is arguably correct to alternatively formalizing it as \textit{Contextual MDP} (CMDP), where transitions (and rewards) can be factorized into ones over only subsets of states \cite{hallak2015contextual}. However, CMDPs are degrade to POMDPs when those contexts are not revealed. In this sense, some meta/contextual learning techniques developed by the RL community might also be related \cite{beck2023survey, benjamins2021carl}.
%
%% MAPF
%\textit{MAPF.}
%Last but not least, we would like to claim that our work might \textbf{not} align with the interest of researchers who are investigating the problem of \textit{Multi-Agent Path Finding} (MAPF) \cite{stern2019multi-overview, stern2019multi, sharon2015conflict, zhang2022multi, li2021eecbs}. The line of work in MAPF aims at inventing algorithms controlling multiple moving robots that run fast in practice. It is either the case where the central controller computes the joint-plan in advance and forces all robots to rigorously execute it, or the case where each robot follows a certain protocol that enables them to communicate and iteratively adjust their routes. None of these cases falls into our scope. We borrow this domain only for evaluating and illustrating the capability of those aforementioned planners.
%
%%as it offers a testbed where we can highly customize the map layout and the number of agents, and therefore, alter the scale of the planning problem for a comprehensive study.
%
%% v0 related ends ==========