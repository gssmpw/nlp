% CSP
@article{brailsford1999constraint,
  title={Constraint satisfaction problems: Algorithms and applications},
  author={Brailsford, Sally C and Potts, Chris N and Smith, Barbara M},
  journal={European journal of operational research},
  volume={119},
  number={3},
  pages={557--581},
  year={1999},
  publisher={Elsevier}
}

% auto driving; survey 
@article{chen2024end,
  title={End-to-end autonomous driving: Challenges and frontiers},
  author={Chen, Li and Wu, Penghao and Chitta, Kashyap and Jaeger, Bernhard and Geiger, Andreas and Li, Hongyang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

% opponent modelling; rational play; bayes
@article{kalai1993rational,
  title={Rational learning leads to Nash equilibrium},
  author={Kalai, Ehud and Lehrer, Ehud},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1019--1045},
  year={1993},
  publisher={JSTOR}
}

% bayes game;
@article{harsanyi1967games,
  title={Games with incomplete information played by ``Bayesian'' players, I--III Part I. The basic model},
  author={Harsanyi, John C},
  journal={Management science},
  volume={14},
  number={3},
  pages={159--182},
  year={1967},
  publisher={INFORMS}
}

% bayes game;
@article{harsanyi1968games,
  title={Games with incomplete information played by ``Bayesian'' players part II. Bayesian equilibrium points},
  author={Harsanyi, John C},
  journal={Management science},
  volume={14},
  number={5},
  pages={320--334},
  year={1968},
  publisher={INFORMS}
}

% opponent modelling; smartn
@article{stahl1993evolution,
  title={Evolution of smartn players},
  author={Stahl, Dale O},
  journal={Games and Economic Behavior},
  volume={5},
  number={4},
  pages={604--617},
  year={1993},
  publisher={Elsevier}
}

% opponent modelling; IPD
@inproceedings{carmel1998explore,
  title={How to explore your opponent's strategy (almost) optimally},
  author={Carmel, David and Markovitch, Shaul},
  booktitle={Proceedings International Conference on Multi Agent Systems (Cat. No. 98EX160)},
  pages={64--71},
  year={1998},
  organization={IEEE}
}

% opponent modelling; IPD
@article{carmel1999exploration,
  title={Exploration strategies for model-based learning in multi-agent systems: Exploration strategies},
  author={Carmel, David and Markovitch, Shaul},
  journal={Autonomous Agents and Multi-agent systems},
  volume={2},
  pages={141--172},
  year={1999},
  publisher={Springer}
}


% mmdp; coordination
@inproceedings{boutilier1996planning,
  title={Planning, learning and coordination in multiagent decision processes},
  author={Boutilier, Craig},
  booktitle={TARK},
  volume={96},
  pages={195--210},
  year={1996},
  organization={Citeseer}
}

% mmdp; coordination; jal
@article{claus1998dynamics,
  title={The dynamics of reinforcement learning in cooperative multiagent systems},
  author={Claus, Caroline and Boutilier, Craig},
  journal={AAAI/IAAI},
  volume={1998},
  number={746-752},
  pages={2},
  year={1998}
}

% cjal
@article{banerjee2007reaching,
  title={Reaching pareto-optimality in prisoner's dilemma using conditional joint action learning},
  author={Banerjee, Dipyaman and Sen, Sandip},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={15},
  pages={91--108},
  year={2007},
  publisher={Springer}
}

% cognitive hierarchy
@article{camerer2004cognitive,
  title={A cognitive hierarchy model of games},
  author={Camerer, Colin F and Ho, Teck-Hua and Chong, Juin-Kuan},
  journal={The Quarterly Journal of Economics},
  volume={119},
  number={3},
  pages={861--898},
  year={2004},
  publisher={MIT Press}
}

% best-response learning; opponent modelling
@article{albrecht2019convergence,
  title={On convergence and optimality of best-response learning with policy types in multiagent systems},
  author={Albrecht, Stefano V and Ramamoorthy, Subramanian},
  journal={arXiv preprint arXiv:1907.06995},
  year={2019}
}

% type-based; opponent modelling
@article{albrecht2016belief,
  title={Belief and truth in hypothesised behaviours},
  author={Albrecht, Stefano V and Crandall, Jacob W and Ramamoorthy, Subramanian},
  journal={Artificial Intelligence},
  volume={235},
  pages={63--94},
  year={2016},
  publisher={Elsevier}
}

% opponent modelling; learning pipeline
@article{rahman2023general,
  title={A general learning framework for open ad hoc teamwork using graph-based policy learning},
  author={Rahman, Arrasy and Carlucho, Ignacio and H{\"o}pner, Niklas and Albrecht, Stefano V},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={298},
  pages={1--74},
  year={2023}
}

% opponent modelling; meta-policy; puct; mcts
@inproceedings{schwartz2023bayes,
  title={Bayes-Adaptive Monte-Carlo Planning for Type-Based Reasoning in Large Partially Observable, Multi-Agent Environments},
  author={Schwartz, Jonathon and Kurniawati, Hanna},
  booktitle={Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages={2355--2357},
  year={2023}
}

% opponent modelling; meta-policy; puct; mcts
@article{schwartz2023combining,
  title={Combining a Meta-Policy and Monte-Carlo Planning for Scalable Type-Based Reasoning in Partially Observable Environments},
  author={Schwartz, Jonathon and Kurniawati, Hanna and Hutter, Marcus},
  journal={arXiv preprint arXiv:2306.06067},
  year={2023}
}

% Stratego; imperfect information
@article{perolat2022mastering,
  title={Mastering the game of Stratego with model-free multiagent reinforcement learning},
  author={Perolat, Julien and De Vylder, Bart and Hennes, Daniel and Tarassov, Eugene and Strub, Florian and de Boer, Vincent and Muller, Paul and Connor, Jerome T and Burch, Neil and Anthony, Thomas and others},
  journal={Science},
  volume={378},
  number={6623},
  pages={990--996},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

% alphazero
@article{
doi:10.1126/science.aar6404,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
number = {6419},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}}

% muzero
@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

% gumbel muzero
@inproceedings{
danihelka2022policy,
title={Policy improvement by planning with Gumbel},
author={Ivo Danihelka and Arthur Guez and Julian Schrittwieser and David Silver},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=bERaNdoegnO}
}

% stochastic muzero
@inproceedings{
antonoglou2022planning,
title={Planning in Stochastic Environments with a Learned Model},
author={Ioannis Antonoglou and Julian Schrittwieser and Sherjil Ozair and Thomas K Hubert and David Silver},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=X6D9bAHhBQ1}
}

% zsc; survey
@misc{mirsky2022survey,
      title={A Survey of Ad Hoc Teamwork Research}, 
      author={Reuth Mirsky and Ignacio Carlucho and Arrasy Rahman and Elliot Fosong and William Macke and Mohan Sridharan and Peter Stone and Stefano V. Albrecht},
      year={2022},
      eprint={2202.10450},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

% oppo modelling; survey
@article{albrecht2018autonomous,
  title={Autonomous agents modelling other agents: A comprehensive survey and open problems},
  author={Albrecht, Stefano V and Stone, Peter},
  journal={Artificial Intelligence},
  volume={258},
  pages={66--95},
  year={2018},
  publisher={Elsevier}
}

% diplomacy
@article{meta2022human,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta Fundamental AI Research Diplomacy Team (FAIR) and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

% two player poker; cfr
@article{zinkevich2007regret,
  title={Regret minimization in games with incomplete information},
  author={Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

% deepcfr
@InProceedings{pmlr-v97-brown19b,
  title = 	 {Deep Counterfactual Regret Minimization},
  author =       {Brown, Noam and Lerer, Adam and Gross, Sam and Sandholm, Tuomas},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {793--802},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/brown19b/brown19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/brown19b.html},
  abstract = 	 {Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces <em>Deep Counterfactual Regret Minimization</em>, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.}
}

% multi-player poker; mccfr
@article{brown2019superhuman,
  title={Superhuman AI for multiplayer poker},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={365},
  number={6456},
  pages={885--890},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

% two player poker; alphaholdem
@inproceedings{zhao2022alphaholdem,
  title={AlphaHoldem: High-performance artificial intelligence for heads-up no-limit poker via end-to-end reinforcement learning},
  author={Zhao, Enmin and Yan, Renye and Li, Jinqiu and Li, Kai and Xing, Junliang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={4689--4697},
  year={2022}
}

% other-play
@InProceedings{pmlr-v119-hu20a,
  title = 	 {“{O}ther-Play” for Zero-Shot Coordination},
  author =       {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4399--4410},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/hu20a/hu20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/hu20a.html},
  abstract = 	 {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.}
}

% k-level reasoning
@inproceedings{NEURIPS2021_4547dff5,
 author = {Cui, Brandon and Hu, Hengyuan and Pineda, Luis and Foerster, Jakob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8215--8228},
 publisher = {Curran Associates, Inc.},
 title = {K-level Reasoning for Zero-Shot Coordination in Hanabi},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/4547dff5fd7604f18c8ee32cf3da41d7-Paper.pdf},
 volume = {34},
 year = {2021}
}

% off-belief-learning
@InProceedings{pmlr-v139-hu21c,
  title = 	 {Off-Belief Learning},
  author =       {Hu, Hengyuan and Lerer, Adam and Cui, Brandon and Pineda, Luis and Brown, Noam and Foerster, Jakob},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4369--4379},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hu21c/hu21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hu21c.html},
  abstract = 	 {The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents’ actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy $\pi_1$ that is optimized assuming past actions were taken by a given, fixed policy ($\pi_0$), but assuming that future actions will be taken by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents’ behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI &amp; ZSC problem Hanabi.}
}

% lbf; hba; short for aamas
@article{albrecht2015game,
  title={A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems},
  author={Albrecht, Stefano V and Ramamoorthy, Subramanian},
  journal={arXiv preprint arXiv:1506.01170},
  year={2015}
}

% lbf; pnp
@article{papoudakis2021agent,
  title={Agent modelling under partial observability for deep reinforcement learning},
  author={Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19210--19222},
  year={2021}
}

% pnp
@inproceedings{barrett2011empirical,
  title={Empirical evaluation of ad hoc teamwork in the pursuit domain},
  author={Barrett, Samuel and Stone, Peter and Kraus, Sarit},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={567--574},
  year={2011}
}

% coop navig
@inproceedings{wen2021modelling,
  title={Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning},
  author={Wen, Ying and Yang, Yaodong and Wang, Jun},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={414--421},
  year={2021}
}

% wildlife
@article{sessa2020learning,
  title={Learning to play sequential games versus unknown opponents},
  author={Sessa, Pier Giuseppe and Bogunovic, Ilija and Kamgarpour, Maryam and Krause, Andreas},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={8971--8981},
  year={2020}
}

% wildlife
@inproceedings{kar2015game,
  title={" A Game of Thrones" When Human Behavior Models Compete in Repeated Stackelberg Security Games},
  author={Kar, Debarun and Fang, Fei and Delle Fave, Francesco and Sintov, Nicole and Tambe, Milind},
  booktitle={Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages={1381--1390},
  year={2015},
  organization={Citeseer}
}

% wildlife
@inproceedings{yang2014adaptive,
  title={Adaptive resource allocation for wildlife protection against illegal poachers},
  author={Yang, Rong and Ford, Benjamin and Tambe, Milind and Lemieux, Andrew},
  booktitle={Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems},
  pages={453--460},
  year={2014}
}

% CMARL; maddpg
@article{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% CMARL; coma
@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

% CMARL; qmix
@article{rashid2020monotonic,
  title={Monotonic value function factorisation for deep multi-agent reinforcement learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={178},
  pages={1--51},
  year={2020}
}

% CMARL; mappo; ippo
@inproceedings{yu2022the,
title={The Surprising Effectiveness of {PPO} in Cooperative Multi-Agent Games},
author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=YVXaxB6L2Pl}
}

% happo
@inproceedings{
kuba2022trust,
title={Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning},
author={Jakub Grudzien Kuba and Ruiqing Chen and Muning Wen and Ying Wen and Fanglei Sun and Jun Wang and Yaodong Yang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=EcGGFkNTxdJ}
}

% CMARL; auto-regression
@InProceedings{pmlr-v162-fu22d,
  title = 	 {Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning},
  author =       {Fu, Wei and Yu, Chao and Xu, Zelai and Yang, Jiaqi and Wu, Yi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {6863--6877},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/fu22d/fu22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/fu22d.html},
  abstract = 	 {Many advances in cooperative multi-agent reinforcement learning (MARL) are based on two common design principles: value decomposition and parameter sharing. A typical MARL algorithm of this fashion decomposes a centralized Q-function into local Q-networks with parameters shared across agents. Such an algorithmic paradigm enables centralized training and decentralized execution (CTDE) and leads to efficient learning in practice. Despite all the advantages, we revisit these two principles and show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, value decomposition, and parameter sharing can be problematic and lead to undesired outcomes. In contrast, policy gradient (PG) methods with individual policies provably converge to an optimal solution in these cases, which partially supports some recent empirical observations that PG can be effective in many MARL testbeds. Inspired by our theoretical analysis, we present practical suggestions on implementing multi-agent PG algorithms for either high rewards or diverse emergent behaviors and empirically validate our findings on a variety of domains, ranging from the simplified matrix and grid-world games to complex benchmarks such as StarCraft Multi-Agent Challenge and Google Research Football. We hope our insights could benefit the community towards developing more general and more powerful MARL algorithms.}
}



% framework; malib
@article{JMLR:v24:22-0169,
  author  = {Ming Zhou and Ziyu Wan and Hanjing Wang and Muning Wen and Runzhe Wu and Ying Wen and Yaodong Yang and Yong Yu and Jun Wang and Weinan Zhang},
  title   = {MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {150},
  pages   = {1--12},
  url     = {http://jmlr.org/papers/v24/22-0169.html}
}

% framework; pymarl; starcraft
@article{samvelyan19smac,
  title = {{The} {StarCraft} {Multi}-{Agent} {Challenge}},
  author = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philiph H. S. Torr and Jakob Foerster and Shimon Whiteson},
  journal = {CoRR},
  volume = {abs/1902.04043},
  year = {2019},
}

% auto dr; benchmark
@misc{SMARTS,
    title={SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
    author={Ming Zhou and Jun Luo and Julian Villella and Yaodong Yang and David Rusu and Jiayu Miao and Weinan Zhang and Montgomery Alban and Iman Fadakar and Zheng Chen and Aurora Chongxi Huang and Ying Wen and Kimia Hassanzadeh and Daniel Graves and Dong Chen and Zhengbang Zhu and Nhat Nguyen and Mohamed Elsayed and Kun Shao and Sanjeevan Ahilan and Baokuan Zhang and Jiannan Wu and Zhengang Fu and Kasra Rezaee and Peyman Yadmellat and Mohsen Rohani and Nicolas Perez Nieves and Yihan Ni and Seyedershad Banijamali and Alexander Cowen Rivers and Zheng Tian and Daniel Palenicek and Haitham bou Ammar and Hongbo Zhang and Wulong Liu and Jianye Hao and Jun Wang},
    url={https://arxiv.org/abs/2010.09776},
    primaryClass={cs.MA},
    booktitle={Proceedings of the 4th Conference on Robot Learning (CoRL)},
    year={2020},
    month={11}
}

% randomised reward PG; adaptive
@inproceedings{
tang2021discovering,
title={Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization},
author={Zhenggang Tang and Chao Yu and Boyuan Chen and Huazhe Xu and Xiaolong Wang and Fei Fang and Simon Shaolei Du and Yu Wang and Yi Wu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=lvRTC669EY_}
}

% cmdp
@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

% MAPF; overview;
@article{stern2019multi-overview,
  title={Multi-agent path finding--an overview},
  author={Stern, Roni},
  journal={Artificial Intelligence},
  pages={96--115},
  year={2019},
  publisher={Springer}
}

% MAPF; overview;
@inproceedings{stern2019multi,
  title={Multi-agent pathfinding: Definitions, variants, and benchmarks},
  author={Stern, Roni and Sturtevant, Nathan R and Felner, Ariel and Koenig, Sven and Ma, Hang and Walker, Thayne T and Li, Jiaoyang and Atzmon, Dor and Cohen, Liron and Kumar, TK Satish and others},
  booktitle={Twelfth Annual Symposium on Combinatorial Search},
  year={2019}
}

% MAPF; nphard
@inproceedings{yu2013structure,
  title={Structure and intractability of optimal multi-robot path planning on graphs},
  author={Yu, Jingjin and LaValle, Steven},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={27},
  number={1},
  pages={1443--1449},
  year={2013}
}

% CBS; sum-of-cost; opt
@article{sharon2015conflict,
  title={Conflict-based search for optimal multi-agent pathfinding},
  author={Sharon, Guni and Stern, Roni and Felner, Ariel and Sturtevant, Nathan R},
  journal={Artificial Intelligence},
  volume={219},
  pages={40--66},
  year={2015},
  publisher={Elsevier}
}

% EECBS; sub-opt
@inproceedings{li2021eecbs,
  title={Eecbs: A bounded-suboptimal search for multi-agent path finding},
  author={Li, Jiaoyang and Ruml, Wheeler and Koenig, Sven},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={14},
  pages={12353--12362},
  year={2021}
}

% mutex prop; opt
@article{zhang2022multi,
  title={Multi-agent path finding with mutex propagation},
  author={Zhang, Han and Li, Jiaoyang and Surynek, Pavel and Kumar, TK Satish and Koenig, Sven},
  journal={Artificial Intelligence},
  volume={311},
  pages={103766},
  year={2022},
  publisher={Elsevier}
}

% mapf; universal plan
@misc{zhu2024computinguniversalplanspartially,
      title={On Computing Universal Plans for Partially Observable Multi-Agent Path Finding}, 
      author={Fengming Zhu and Fangzhen Lin},
      year={2024},
      eprint={2305.16203},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2305.16203}, 
}

% md
@inproceedings{tang2017reinforcement,
  title={Reinforcement mechanism design},
  author={Tang, Pingzhong},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={5146--5150},
  year={2017}
}

%negotiation
@article{kraus1997negotiation,
  title={Negotiation and cooperation in multi-agent environments},
  author={Kraus, Sarit},
  journal={Artificial intelligence},
  volume={94},
  number={1-2},
  pages={79--97},
  year={1997},
  publisher={Elsevier}
}

%negotiation
@article{beer1999negotiation,
  title={Negotiation in multi-agent systems},
  author={Beer, Martin and d'Inverno, Mark and Luck, Michael and Jennings, Nick and Preist, Chris and Schroeder, Michael},
  journal={The Knowledge Engineering Review},
  volume={14},
  number={3},
  pages={285--289},
  year={1999},
  publisher={Cambridge University Press}
}

%negotiation
@article{jennings2001automated,
  title={Automated negotiation: prospects, methods and challenges},
  author={Jennings, Nicholas R and Faratin, Peyman and Lomuscio, Alessio R and Parsons, Simon and Sierra, Carles and Wooldridge, Michael},
  journal={International Journal of Group Decision and Negotiation},
  volume={10},
  number={2},
  pages={199--215},
  year={2001}
}

% persuasion
@book{4ea43cce77d34e00bbf627204843ce91,
title = "Persuasion: Theory and Research",
abstract = "Persuasion: Theory and Research, Third Edition is a comprehensive overview of social-scientific theory and research on persuasion. Written in a clear and accessible style that assumes no special technical background in research methods, the Third Edition has been thoroughly revised to reflect developments in persuasion studies. New discussions of subjects such as reactance and the use of narratives as vehicles for persuasion, revised treatments of the theories of reasoned action and planned behavior, and two new chapters on social judgment theory and stage models provide your students with the most current work on persuasion in a clear, straightforward manner. In this edition, author Daniel J. O'Keefe has given special attention to the importance of adapting (tailoring) messages to audiences to maximize persuasiveness. Each chapter has a set of review questions to guide students through the chapter{\textquoteright}s material and quickly master the concepts being introduced. ",
author = "O'Keefe, {Daniel James}",
year = "2016",
language = "English (US)",
isbn = "9781452276670",
publisher = "SAGE Publications, Inc",
edition = "3rd",
}

% pomdp; finite horizon
@article{smallwood1973optimal,
  title={The optimal control of partially observable Markov processes over a finite horizon},
  author={Smallwood, Richard D and Sondik, Edward J},
  journal={Operations research},
  volume={21},
  number={5},
  pages={1071--1088},
  year={1973},
  publisher={INFORMS}
}

% pomdp; infinite horizon
@article{sondik1978optimal,
  title={The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs},
  author={Sondik, Edward J},
  journal={Operations research},
  volume={26},
  number={2},
  pages={282--304},
  year={1978},
  publisher={INFORMS}
}

% pomdp
@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

% pomdp; pbvi
@article{zhang2001speeding,
  title={Speeding up the convergence of value iteration in partially observable Markov decision processes},
  author={Zhang, Nevin Lianwen and Zhang, Weihong},
  journal={Journal of Artificial Intelligence Research},
  volume={14},
  pages={29--51},
  year={2001}
}

% qmdp; scaling up; pomdp
@incollection{littman1995learning,
  title={Learning policies for partially observable environments: Scaling up},
  author={Littman, Michael L and Cassandra, Anthony R and Kaelbling, Leslie Pack},
  booktitle={Machine Learning Proceedings 1995},
  pages={362--370},
  year={1995},
  publisher={Elsevier}
}

% pomdp; pomcp
@article{silver2010monte,
  title={Monte-Carlo planning in large POMDPs},
  author={Silver, David and Veness, Joel},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

% pomdp; complexity; finite-horizon
@article{papadimitriou1987complexity,
  title={The complexity of Markov decision processes},
  author={Papadimitriou, Christos H and Tsitsiklis, John N},
  journal={Mathematics of operations research},
  volume={12},
  number={3},
  pages={441--450},
  year={1987},
  publisher={INFORMS}
}

% pomdp; complexity; infinite-horizon
@inproceedings{madani1999undecidability,
  title={On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems},
  author={Madani, Omid and Hanks, Steve and Condon, Anne},
  booktitle={Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence},
  pages={541--548},
  year={1999}
}

% ipomdp
@article{gmytrasiewicz2005framework,
  title={A framework for sequential planning in multi-agent settings},
  author={Gmytrasiewicz, Piotr J and Doshi, Prashant},
  journal={Journal of Artificial Intelligence Research},
  volume={24},
  pages={49--79},
  year={2005}
}

% meta rl
@article{beck2023survey,
  title={A survey of meta-reinforcement learning},
  author={Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2301.08028},
  year={2023}
}

% contextual rl
@article{benjamins2021carl,
  title={Carl: A benchmark for contextual and adaptive reinforcement learning},
  author={Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Biedenkapp, Andr{\'e} and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius},
  journal={arXiv preprint arXiv:2110.02102},
  year={2021}
}

% sb3
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

% ppo
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

% uct
@inproceedings{kocsis2006bandit,
  title={Bandit based monte-carlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={European conference on machine learning},
  pages={282--293},
  year={2006},
  organization={Springer}
}

% pucb; puct
@article{rosin2011multi,
  title={Multi-armed bandits with episode context},
  author={Rosin, Christopher D},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={61},
  number={3},
  pages={203--230},
  year={2011},
  publisher={Springer}
}

% mcts; max-n
@article{samothrakis2011fast,
  title={Fast approximate max-n monte carlo tree search for ms pac-man},
  author={Samothrakis, Spyridon and Robles, David and Lucas, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in Games},
  volume={3},
  number={2},
  pages={142--154},
  year={2011},
  publisher={IEEE}
}

% mcts; survey
@article{browne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

% many-agent mcts for ipomdp; wildfires
@inproceedings{eck2020scalable,
  title={Scalable decision-theoretic planning in open and typed multiagent systems},
  author={Eck, Adam and Shah, Maulik and Doshi, Prashant and Soh, Leen-Kiat},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7127--7134},
  year={2020}
}

% sg; Shapley
@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}

% sg; review
@article{solan2015stochastic,
  title={Stochastic games},
  author={Solan, Eilon and Vieille, Nicolas},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={45},
  pages={13743--13746},
  year={2015},
  publisher={National Acad Sciences}
}

% sg; ne
@article{fink1964equilibrium,
  title={Equilibrium in a stochastic $ n $-person game},
  author={Fink, AM},
  journal={Journal of Science of the Hiroshima University, Series AI (Mathematics)},
  volume={28},
  number={1},
  pages={89--93},
  year={1964},
  publisher={Hiroshima University, Mathematics Program}
}

% sg; ne
@article{takahashi1964equilibrium,
  title={Equilibrium points of stochastic non-cooperative $ n $-person games},
  author={Takahashi, Masayuki},
  journal={Journal of Science of the Hiroshima University, Series AI (Mathematics)},
  volume={28},
  number={1},
  pages={95--99},
  year={1964},
  publisher={Hiroshima University, Mathematics Program}
}

% rl; safety rules
@inproceedings{gao2020embedding,
  title={Embedding high-level knowledge into dqns to learn faster and more safely},
  author={Gao, Zihang and Lin, Fangzhen and Zhou, Yi and Zhang, Hao and Wu, Kaishun and Zhang, Haodi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={09},
  pages={13608--13609},
  year={2020}
}

% networked mas; dependency
@article{ma2024efficient,
  title={Efficient and scalable reinforcement learning for large-scale network control},
  author={Ma, Chengdong and Li, Aming and Du, Yali and Dong, Hao and Yang, Yaodong},
  journal={Nature Machine Intelligence},
  pages={1--15},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

% mfg
@article{lasry2007mean,
  title={Mean field games},
  author={Lasry, Jean-Michel and Lions, Pierre-Louis},
  journal={Japanese journal of mathematics},
  volume={2},
  number={1},
  pages={229--260},
  year={2007},
  publisher={Springer}
}

% teaching
@incollection{camerer2004behavioural,
  title={Behavioural game theory: thinking, learning and teaching},
  author={Camerer, Colin F and Ho, Teck-Hua and Chong, Juin Kuan},
  booktitle={Advances in Understanding Strategic Behaviour: Game Theory, Experiments and Bounded Rationality},
  pages={120--180},
  year={2004},
  publisher={Springer}
}

% teaching
@article{camerer2001strategic,
  title={Strategic learning and teaching},
  author={Camerer, Colin F and Ho, Teck H},
  journal={Wharton on Making Decisions. New York: Wiley},
  year={2001}
}