@article{JMLR:v24:22-0169,
  author  = {Ming Zhou and Ziyu Wan and Hanjing Wang and Muning Wen and Runzhe Wu and Ying Wen and Yaodong Yang and Yong Yu and Jun Wang and Weinan Zhang},
  title   = {MALib: A Parallel Framework for Population-based Multi-agent Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {150},
  pages   = {1--12},
  url     = {http://jmlr.org/papers/v24/22-0169.html}
}

@article{albrecht2016belief,
  title={Belief and truth in hypothesised behaviours},
  author={Albrecht, Stefano V and Crandall, Jacob W and Ramamoorthy, Subramanian},
  journal={Artificial Intelligence},
  volume={235},
  pages={63--94},
  year={2016},
  publisher={Elsevier}
}

@article{albrecht2018autonomous,
  title={Autonomous agents modelling other agents: A comprehensive survey and open problems},
  author={Albrecht, Stefano V and Stone, Peter},
  journal={Artificial Intelligence},
  volume={258},
  pages={66--95},
  year={2018},
  publisher={Elsevier}
}

@article{albrecht2019convergence,
  title={On convergence and optimality of best-response learning with policy types in multiagent systems},
  author={Albrecht, Stefano V and Ramamoorthy, Subramanian},
  journal={arXiv preprint arXiv:1907.06995},
  year={2019}
}

@article{beck2023survey,
  title={A survey of meta-reinforcement learning},
  author={Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2301.08028},
  year={2023}
}

@article{benjamins2021carl,
  title={Carl: A benchmark for contextual and adaptive reinforcement learning},
  author={Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Biedenkapp, Andr{\'e} and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius},
  journal={arXiv preprint arXiv:2110.02102},
  year={2021}
}

@inproceedings{boutilier1996planning,
  title={Planning, learning and coordination in multiagent decision processes},
  author={Boutilier, Craig},
  booktitle={TARK},
  volume={96},
  pages={195--210},
  year={1996},
  organization={Citeseer}
}

@article{camerer2004cognitive,
  title={A cognitive hierarchy model of games},
  author={Camerer, Colin F and Ho, Teck-Hua and Chong, Juin-Kuan},
  journal={The Quarterly Journal of Economics},
  volume={119},
  number={3},
  pages={861--898},
  year={2004},
  publisher={MIT Press}
}

@inproceedings{carmel1998explore,
  title={How to explore your opponent's strategy (almost) optimally},
  author={Carmel, David and Markovitch, Shaul},
  booktitle={Proceedings International Conference on Multi Agent Systems (Cat. No. 98EX160)},
  pages={64--71},
  year={1998},
  organization={IEEE}
}

@article{carmel1999exploration,
  title={Exploration strategies for model-based learning in multi-agent systems: Exploration strategies},
  author={Carmel, David and Markovitch, Shaul},
  journal={Autonomous Agents and Multi-agent systems},
  volume={2},
  pages={141--172},
  year={1999},
  publisher={Springer}
}

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{gmytrasiewicz2005framework,
  title={A framework for sequential planning in multi-agent settings},
  author={Gmytrasiewicz, Piotr J and Doshi, Prashant},
  journal={Journal of Artificial Intelligence Research},
  volume={24},
  pages={49--79},
  year={2005}
}

@article{hallak2015contextual,
  title={Contextual markov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial intelligence},
  volume={101},
  number={1-2},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{li2021eecbs,
  title={Eecbs: A bounded-suboptimal search for multi-agent path finding},
  author={Li, Jiaoyang and Ruml, Wheeler and Koenig, Sven},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={14},
  pages={12353--12362},
  year={2021}
}

@incollection{littman1995learning,
  title={Learning policies for partially observable environments: Scaling up},
  author={Littman, Michael L and Cassandra, Anthony R and Kaelbling, Leslie Pack},
  booktitle={Machine Learning Proceedings 1995},
  pages={362--370},
  year={1995},
  publisher={Elsevier}
}

@article{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{mirsky2022survey,
      title={A Survey of Ad Hoc Teamwork Research}, 
      author={Reuth Mirsky and Ignacio Carlucho and Arrasy Rahman and Elliot Fosong and William Macke and Mohan Sridharan and Peter Stone and Stefano V. Albrecht},
      year={2022},
      eprint={2202.10450},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@InProceedings{pmlr-v119-hu20a,
  title = 	 {“{O}ther-Play” for Zero-Shot Coordination},
  author =       {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4399--4410},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/hu20a/hu20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/hu20a.html},
  abstract = 	 {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.}
}

@InProceedings{pmlr-v139-hu21c,
  title = 	 {Off-Belief Learning},
  author =       {Hu, Hengyuan and Lerer, Adam and Cui, Brandon and Pineda, Luis and Brown, Noam and Foerster, Jakob},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4369--4379},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/hu21c/hu21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/hu21c.html},
  abstract = 	 {The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents’ actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy $\pi_1$ that is optimized assuming past actions were taken by a given, fixed policy ($\pi_0$), but assuming that future actions will be taken by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents’ behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI &amp; ZSC problem Hanabi.}
}

@InProceedings{pmlr-v162-fu22d,
  title = 	 {Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning},
  author =       {Fu, Wei and Yu, Chao and Xu, Zelai and Yang, Jiaqi and Wu, Yi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {6863--6877},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/fu22d/fu22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/fu22d.html},
  abstract = 	 {Many advances in cooperative multi-agent reinforcement learning (MARL) are based on two common design principles: value decomposition and parameter sharing. A typical MARL algorithm of this fashion decomposes a centralized Q-function into local Q-networks with parameters shared across agents. Such an algorithmic paradigm enables centralized training and decentralized execution (CTDE) and leads to efficient learning in practice. Despite all the advantages, we revisit these two principles and show that in certain scenarios, e.g., environments with a highly multi-modal reward landscape, value decomposition, and parameter sharing can be problematic and lead to undesired outcomes. In contrast, policy gradient (PG) methods with individual policies provably converge to an optimal solution in these cases, which partially supports some recent empirical observations that PG can be effective in many MARL testbeds. Inspired by our theoretical analysis, we present practical suggestions on implementing multi-agent PG algorithms for either high rewards or diverse emergent behaviors and empirically validate our findings on a variety of domains, ranging from the simplified matrix and grid-world games to complex benchmarks such as StarCraft Multi-Agent Challenge and Google Research Football. We hope our insights could benefit the community towards developing more general and more powerful MARL algorithms.}
}

@article{samvelyan19smac,
  title = {{The} {StarCraft} {Multi}-{Agent} {Challenge}},
  author = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philiph H. S. Torr and Jakob Foerster and Shimon Whiteson},
  journal = {CoRR},
  volume = {abs/1902.04043},
  year = {2019},
}

@article{sharon2015conflict,
  title={Conflict-based search for optimal multi-agent pathfinding},
  author={Sharon, Guni and Stern, Roni and Felner, Ariel and Sturtevant, Nathan R},
  journal={Artificial Intelligence},
  volume={219},
  pages={40--66},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{stern2019multi,
  title={Multi-agent pathfinding: Definitions, variants, and benchmarks},
  author={Stern, Roni and Sturtevant, Nathan R and Felner, Ariel and Koenig, Sven and Ma, Hang and Walker, Thayne T and Li, Jiaoyang and Atzmon, Dor and Cohen, Liron and Kumar, TK Satish and others},
  booktitle={Twelfth Annual Symposium on Combinatorial Search},
  year={2019}
}

@article{stern2019multi-overview,
  title={Multi-agent path finding--an overview},
  author={Stern, Roni},
  journal={Artificial Intelligence},
  pages={96--115},
  year={2019},
  publisher={Springer}
}

@inproceedings{yu2022the,
title={The Surprising Effectiveness of {PPO} in Cooperative Multi-Agent Games},
author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=YVXaxB6L2Pl}
}

@article{zhang2001speeding,
  title={Speeding up the convergence of value iteration in partially observable Markov decision processes},
  author={Zhang, Nevin Lianwen and Zhang, Weihong},
  journal={Journal of Artificial Intelligence Research},
  volume={14},
  pages={29--51},
  year={2001}
}

@article{zhang2022multi,
  title={Multi-agent path finding with mutex propagation},
  author={Zhang, Han and Li, Jiaoyang and Surynek, Pavel and Kumar, TK Satish and Koenig, Sven},
  journal={Artificial Intelligence},
  volume={311},
  pages={103766},
  year={2022},
  publisher={Elsevier}
}

