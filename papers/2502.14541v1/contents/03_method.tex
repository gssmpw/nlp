\section{Method}
\subsection{Problem Formulation}

In our recommender system, we consider the user $u$ dataset as follow: $\mathfrak{D}_u = \{\mathbf{R}_u, \mathbf{I}_u \}$, where $\mathbf{R}_u = \{r_u^1, \cdot\cdot\cdot, r_u^{k_u}\}$ represents the historical reviews, $\mathbf{I}_u = \{i_u^1, \cdot\cdot\cdot, i_u^{k_u} \}$ denotes the corresponding purchased items, and $k_u$ is the total number of purchased items from user $u$. 
Leveraging the user's dataset $\mathfrak{D}_u$, we aim to predict the next purchased item $i_u^{k_u+1}$ from a candidate set $\mathcal{C}_u^{k_u+1}$, which contains the ground-truth item. 
% Unlike the setup from previous works (See~\cref{app:prev_setup} for details), we propose a new setup that is more realistic (See~\cref{app:our_setup} for details).

\smallskip
\noindent \textbf{One-shot Sequential Recommendation.} It predicts a single next item based on a static history of user interactions up to timestep $k_u-1$. Given the dataset $\mathfrak{D}_u$, the model observes $\mathfrak{D}_u^{k_u-1} = \{\mathbf{R}_u^{k_u-1}, \mathbf{I}_u^{k_u-1}\}$ and predicts the last item $i_u^{k_u}$ from the candidate set $\mathcal{C}_u^{k_u}$. This focuses on a one-time prediction without considering future timesteps.

\smallskip
\noindent \textbf{Continuous Sequential Recommendation.} This setup predicts the next item at every timestep ($4 \le t \le k_u-1$), making it a multi-step prediction task. At each timestep $t$, the model observes the updated interaction history $\mathfrak{D}_u^t = \{\mathbf{R}_u^t, \mathbf{I}_u^t\}$ and predicts the next item $i_u^{t+1}$ from the candidate set $\mathcal{C}_u^{t+1}$. This multi-step prediction process effectively captures temporal dependencies and allows continuous updates of user preferences, making it more aligned with real-world scenarios.

% To evaluate the effectiveness of the recommendation system, we define two types of tasks. 

% \noindent \textbf{Snapshot Recommendation.} \label{snapshot} It aims to predict the next item based on a fixed snapshot of the user's past interactions. Formally, given the dataset $\mathfrak{D}_u$, we randomly pick an index $t$ where $1 \leq t \leq k$ and remove the corresponding item $i_u^t$ from the dataset. The modified dataset is $\mathfrak{D}'_u =  \{\mathbf{R}'_u, \mathbf{I}'_u \}$, where $\mathbf{R}'_u = \{r_u^1, \cdot\cdot\cdot, r_u^k \} \setminus \{r_u^t \}$, $\mathbf{I}'_u = \{i_u^1, \cdot\cdot\cdot, i_u^k \} \setminus \{ i_u^t\}$. The recommender is then tasked with recovering $i_u^t$ from the candidate set $\mathcal{C}_u^t$. 

% \noindent \textbf{Continuous Sequential Recommendation.} It considers the temporal order of interactions by incrementally updating the available user history. Given the dataset $\mathfrak{D}_u$, the model sequentially observes each interaction and predicts each timestep $t$, where $1 \leq t \leq k$. Formally, at each step $t$, the recommender is provided with $\mathfrak{D}_u^t = \{\mathbf{R}_u^t, \mathbf{I}_u^t \}$, where $\mathbf{R}_u^t = \{r_u^1, \cdot\cdot\cdot, r_u^t\}$, $\mathbf{I}_u^t = \{i_u^1, \cdot\cdot\cdot, i_u^t \}$. The task is to predict the next item $i_u^{t+1}$ from the candidate set $\mathcal{C}_u^{t+1}$. This approach incorporates sequential dependencies by dynamically updating the recommendation model as new interactions occur, and it is more practical.


\subsection{\myalg{}: \underline{P}rofile \underline{U}pdate for \underline{RE}commender}

\begingroup
\setlength{\textfloatsep}{8pt}
\begin{algorithm}[t]
    \DontPrintSemicolon
    \SetAlgoLined
    \SetNoFillComment
    \LinesNotNumbered 
    \caption{\myalg{}}
    \label{alg:main}
    \KwInput{\small Review extractor $\mathcal{E(\cdot)}$, User profile updater $\mathcal{U}(\cdot)$, Recommender $\mathcal{R}(\cdot)$, Dataset $\mathfrak{D}_u = \{\mathbf{R}_u, \mathbf{I}_u \}$ for user $u$, User profile $\mathbf{P}_u^t$, next purchase candidates $\mathcal{C}_u^{t+1}$, timestep $t$}
    {\small{\textcolor{Skyblue}{\# Extract representations from reviews}}}\\
    $\tilde{l_u^t}, \tilde{d_u^t}, \tilde{f_u^t} = \mathcal{E}(r_u^t)$\\
    $\hat{l_u^t} = l_u^{t-1} \cup \tilde{l_u^t}$ \quad{\small{\textcolor{Skyblue}{$\triangleright$ List of items user likes}}}\\
    $ \hat{d_u^t} = d_u^{t-1} \cup \tilde{d_u^t} $  \quad{\small{\textcolor{Skyblue}{$\triangleright$ List of items user dislikes}}}\\
    $\hat{f_u^t} = f_u^{t-1} \cup \tilde{f_u^t}$  \quad{\small{\textcolor{Skyblue}{$\triangleright$ List of user's key features}}}\\
    
    {\small{\textcolor{Skyblue}{\# Update user profile after redundancy removal}}}\\
    $l_u^t, d_u^t, f_u^t = \mathcal{U}(\hat{l_u^t}, \hat{d_u^t}, \hat{f_u^t})$ \\
    $\mathbf{P}_u^t = \{l_u^t, d_u^t, f_u^t\}$\\
    
    {\small{\textcolor{Skyblue}{\# Recommend next purchase item}}}\\
    $\mathrm{pred} = \mathcal{R}(\mathbf{P}_u^t, \mathbf{I}_u^t, \mathcal{C}_u^{t+1})$ \\

    
\KwOutput{$\mathrm{pred}$}

\end{algorithm}



In this section, we introduce \myalg{}, novel framework that manages the user profile $\mathbf{P}_u$ from user reviews $\mathbf{R_u}$ and predict the next item with user profile. Algorithm~\ref{alg:main} can be divided into three steps (See~\autoref{app:template} for prompt template).

\smallskip
\noindent \textbf{STEP 1: Extract User Representation. }

\noindent We begin by providing the LLM with raw inputs, including user reviews $\mathbf{R_u}$ and product names $\mathbf{I_u}$. The LLM extracts $\tilde{l_u^t}$(items the user likes), $\tilde{d_u^t}$(items the user dislikes), and $\tilde{f_u^t}$(key user features) from the incoming review as user representation.

% \noindent In our approach, we initially provide the LLM with naive inputs—including user reviews, ratings, and product names. The LLM extracts user preferences (likes/dislikes) from the reviews and identifies key features of the products by leveraging its intrinsic knowledge. At the beginning, the model processes the first four reviews simultaneously to extract an initial set of user preferences and product features. Subsequently, we feed the LLM with one review at a time in chronological order, updating the previously extracted list by adding new information from each additional review. This incremental process enables the LLM to continuously refine and expand the user profile in a time-sensitive manner.



\smallskip
\noindent \textbf{STEP 2: Update User Profile. }


\noindent After the extraction in STEP 1, the extracted representation <$\tilde{l_u^t}$, $\tilde{d_u^t}$, $\tilde{f_u^t}$> concatenates with previous user profile $\mathbf{P}_u^{t-1} = \{l_u^{t-1}, d_u^{t-1}, f_u^{t-1} \}$. However, this faces a scalability issue as the number of reviews increases. Thus, leveraging the previous profile, we use an LLM to remove redundant and conflicting content from the extracted representation, yielding a more compact and up-to-date user profile $\mathbf{P}_u^t$ after concatenation.

%As such, it is inevitable to refine concatenated information $\hat{l_u^t}$, $\hat{d_u^t}$, and $\hat{f_u^t}$ and we remove the redundant contents by leveraging LLM. Lastly, we set user profile $\mathbf{P}_u^t$ as refined information $l_u^t$, $d_u^t$, and $f_u^t$.
% \noindent In an additional step, we optimize the extracted information $l_u^t$, $d_u^t$, and $f_u^t$ for token efficiency. Following our prompt template, the LLM consolidates the refined likes, dislikes, and key features by removing redundant or overlapping content, resulting in a concise and structured summary of user preferences and product characteristics.
% In an additional step, we optimize the extracted information for token efficiency. The previously extracted likes, dislikes, and key features are fed back into the LLM to consolidate and refine the information. By doing so, the LLM removes redundant or overlapping content, creating a more concise and structured summary of user preferences and product characteristics. 

% This process significantly reduces token usage, ensuring that the input remains within the token limit while preserving the essential information needed for accurate recommendations. Such a refinement step not only improves token efficiency but also enhances the overall quality of the extracted data by prioritizing the most relevant and distinctive aspects of user preferences.


\smallskip
\noindent \textbf{STEP 3: Recommend Next Purhcase Item. }


\noindent Recommender $\mathcal{R}$ reranks the given candidate item list to predict the user's next purchase by leveraging the updated profile $\mathbf{P}_u^t$ and purchased items $\mathbf{I}_u$. 
% \noindent Finally, the processed features are combined with the candidate set and provided as input to the LLM, enabling the generation of a high-quality recommendation list that better aligns with the user’s preferences.

% Finally, the processed features-including refined likes, dislikes, and key product features are combined with the candidate set and provided as input to the LLM.
% This allows the model to generate a higher-quality recommendation list that better aligns with the user’s preferences. 

% By leveraging both structured user information and a well-defined candidate set, the LLM can more accurately assess the likelihood of each candidate product being the next preferred choice.