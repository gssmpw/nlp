\setlength{\tabcolsep}{11pt}
\begin{table*}[t]
  \centering
  \scriptsize
  % \begin{adjustbox}{width=0.98\linewidth}
  \begin{tabular}{@{}clcccccccc@{}}
    \toprule
     &  &  \multicolumn{4}{c}{Games} &  \multicolumn{4}{c}{Movies} \\\cmidrule(lr){3-6} \cmidrule(lr){7-10} 
     Data & Method          &    N@1   &    N@5    &    N@10    &   N@20    &   N@1   &    N@5    &    N@10     &   N@20\\ \cmidrule(lr){1-10}
     
     \multirow{3}{*}{\rotatebox{90}{items}} & Sequential & 10.75 & 18.25 & 23.13 & 28.97 & 9.99  & 15.92 & 20.17 & 26.94 \\
     & Recency    & 15.34 & 24.31 & 28.82 & 34.24 & 12.17 & 17.75 & 22.18 & 28.19 \\
     & ICL        & 14.28 & 26.57 & 30.51 & 35.72 & 12.03 & 19.56 & 23.36 & 29.91 \\ \cmidrule(lr){1-10}

     \multirow{6}{*}{\rotatebox{90}{items + reviews}} 
     & Sequential$^\dagger$ & 11.14 & 19.95 & 24.97 & 32.00 & 8.05 & 13.11 & 17.72 & 25.57\\ 
     & Recency$^\dagger$    & 12.19 & 23.64 & 28.37 & 35.35 & 8.54 & 15.78 & 21.31 & 29.21\\
     & ICL$^\dagger$        & 15.11 & 26.34 & 31.25 & 37.39 & 12.24& 22.10 & 27.31 & 34.52\\ \cmidrule(lr){2-10}
     &\myalg{} (Sequential)& 15.06          & 25.71          & 31.08          &       38.28          & 12.59          & 21.33          & 25.96          &           32.21  \\ 
     &\myalg{} (Recency)   & \textbf{18.18} & 28.90          & 33.91          &     40.69          & 13.85          & 21.99          & 26.53          &               33.37  \\
     &\myalg{} (ICL)       & 16.62          & \textbf{29.81} & \textbf{35.60} & \textbf{42.00} & \textbf{15.80} & \textbf{26.32} & \textbf{32.03} & \textbf{38.93}  \\
    % $\mathbf{R_u}$                                                                    & Truncate     & 14.79 & 24.94 & 30.30 & 37.79 & 13.35 & 22.87 & 28.02 & 35.60\\
    % & Concate      &       &       &       &       &       &       &       &       \\ \cmidrule(lr){1-10}
    
    % $\mathbf{I_u} \cup \mathbf{R_u}$                             & Truncate   & 16.13 & 27.67 & 32.55 & 38.64 & 13.24 & 22.65 & 27.85 & 35.01 \\
    % & \textbf{\myalg{} (Ours)} & \textbf{16.62} & \textbf{29.81} & \textbf{35.60} & \textbf{42.00} & \textbf{15.80} & \textbf{26.32} & \textbf{32.03} & \textbf{38.93} \\
    \bottomrule
  \end{tabular}
  % \end{adjustbox}
  \vspace{-0.25cm}
  \caption{\textbf{Comparison \myalg{} with Baselines.} We evaluate performance under two data settings: using only item interactions and using item interactions augmented with reviews. $\dagger$ indicates customized baselines where review data is naively incorporated into the original prompt templates designed for item interactions only (see ~\cref{app:combined}).}
  % \caption{\textbf{Main results of \myalg{}.} We customize the prompt from the baselines by utilizing both interactions and reviews, and we notate $\dagger$ (See~\cref{app:combined}).}
  
  \label{table:main}
  \vspace{-0.2cm}
\end{table*}

\setlength{\tabcolsep}{6pt}

\setlength{\tabcolsep}{5pt}
\begin{table*}[t]
\centering
  \scriptsize
  % \begin{adjustbox}{width=0.98\linewidth}
    \begin{tabular}{@{}cccccccccccccccc@{}}
    \toprule
         & \multicolumn{2}{c}{Data} & \multicolumn{3}{c}{Components} & \multicolumn{5}{c}{Games} &  \multicolumn{5}{c}{Movies} \\\cmidrule(lr){2-3} \cmidrule{4-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16} 
    Method    & items & reviews & Rec. & Ext. & Upd.                                       &    N@1   &    N@5    &    N@10    &   N@20   &   $|T|$   &   N@1      &    N@5    &    N@10     &   N@20    &  $|T|$\\ \cmidrule(lr){1-16}
    \multirow{4}{*}{\rotatebox{90}{Sequential}} & \ding{51}&           & \ding{51} &           &            &   10.75   &   18.25   &   23.13   &   28.97   &   245.52   &   9.99   &   15.92   & 20.17   &   26.94   &   243.89   \\ 
                                                & \ding{51}& \ding{51} & \ding{51} &           &            &   11.14   &   19.95   &   24.97   &   32.00   &   29165.17   &   8.05   &   13.11   & 17.72   &   25.57   &   60429.80   \\
                                                & \ding{51}& \ding{51} & \ding{51} & \ding{51} &            &   16.09   &   26.94   &   32.35   &   40.08   &   486.49   &   13.05  &   21.38   & 26.11   &   32.62   &   459.69   \\
                                                & \ding{51}& \ding{51} & \ding{51} & \ding{51} & \ding{51}  &   15.06   &   25.71   &   31.08   &   38.28   &   415.01   &   12.59  &   21.33   & 25.96   &   32.21   &   384.87   \\ \cmidrule(lr){1-16}
    \multirow{4}{*}{\rotatebox{90}{Recency}}    & \ding{51}&           & \ding{51} &           &            &   15.34   &   24.31   &   28.82   &   34.24   &   253.31   &   12.17  &   17.75   & 22.18   &   28.19   &   249.64   \\
                                                & \ding{51}& \ding{51} & \ding{51} &           &            &   12.19   &   23.64   &   28.37   &   35.35   &   29235.16   &   8.54   &   15.78   & 21.31   &   29.21   &   60509.43    \\
                                                & \ding{51}& \ding{51} & \ding{51} & \ding{51} &            &   20.85   &   31.36   &   36.51   &   43.19   &   602.13   &   16.00  &   24.81   & 29.66   &   36.98   &   565.13  \\
                                                & \ding{51}& \ding{51} & \ding{51} & \ding{51} & \ding{51}  &   18.18   &   28.90   &   33.91   &   40.69   &   485.85   &   13.85  &   21.99  & 26.53   &   33.37   &   458.60   \\ \cmidrule(lr){1-16}
    \multirow{4}{*}{\rotatebox{90}{ICL}}        & \ding{51}&           & \ding{51} &           &            &   14.28   &   26.57   &   30.51   &   35.72   &   268.40   &   12.03  &   19.56  & 23.36   &   29.91   &   261.58   \\
                                                & \ding{51}& \ding{51} & \ding{51} &           &            &   15.11   &   26.34   &   31.25   &   37.39   &   29388.72   &   12.24  &   22.10  & 27.31   &   34.52   &   60800.61   \\
                                                & \ding{51}& \ding{51} & \ding{51} &\ding{51}  &            &   19.60   &   32.96   &   38.21   &   44.97   &   803.60   &   16.05  &   27.25  & 33.11   &   40.15   &   867.36   \\
                                                & \ding{51}& \ding{51} & \ding{51} &\ding{51}  & \ding{51}  &   16.62   &   29.81   &   35.60   &   42.00   &   592.48   &   15.80  &   26.32  & 32.03   &   38.93   &   634.02   \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}
  \vspace{-0.25cm}
    \caption{\textbf{Component-wise study of \myalg{}.} Each configuration varies which data sources (items, reviews) and which \myalg{} components are used (Rec. = Recommendation, Ext. = Extractor, Upd. = Updater), as indicated by \ding{51}. We report N@k scores ($k\in\{1,5,10,20\}$) and average of input token size (|T|) for Recommender.}
  % \caption{\textbf{Component-wise study of \myalg{}.} |T| indicates the average of the number of input tokens.}
  \label{table:ablation}
  \vspace{-0.4cm}
\end{table*}
\setlength{\tabcolsep}{6pt}


\section{Experiment}
\textbf{Datasets.} For a thorough evaluation, we utilize two datasets from the Amazon collection~\cite{ni2019justifying}: Video Games and Movies \&\ TV. To ensure a comprehensive analysis, we intentionally select datasets with diverse statistical properties, particularly in terms of the number of items (See~\autoref{app:dataset} for details).

% \begin{itemize}
%     \item  \textbf{Video Games.} We select about 15K users and 37K items. Following existing studies \cite{kang2018self}, we removed users and items with fewer than 10 interactions.
%     \item \textbf{Movies and TV.} We select about 98K users and 126K items, removing users and items with fewer than 10 interactions as in the Video Games dataset.
% \end{itemize}

\smallskip
\noindent \textbf{Baselines.} 
~\cite{hou2024large} is the recommendation method that utilizes pre-trained LLMs without additional training or fine-tuning, making it a suitable baseline. It describes three approaches for LLM-based recommendation: Sequential, Recency, and in-context learning (ICL). We compare our method with all three approaches and demonstrate the superiority of \myalg{} when these techniques were applied to our framework, further highlighting its effectiveness. (See~\cref{app:interaction} for details.)
% Although many recommendation system studies leverage large language models (LLMs), most focus on training LLMs to predict the next items. Since \myalg{} is a train-free method, a direct comparison with train-based approaches would be unfair. 
% To the best of our knowledge, ~\cite{hou2024large} is the recommendation method that utilizes pre-trained LLMs without additional training or fine-tuning, making it a suitable baseline. To ensure comprehensive comparison with \myalg{}, we evaluate baselines with various datasets. (See ~\cref{app:interaction} for details.)

% Although many recommendation system studies leverage large language models (LLMs), most focus on training LLMs to predict the next items. Since \myalg{} is a train-free method, a direct comparison with train-based approaches would be unfair. To the best of my knowledge, ~\cite{hou2024large} is the only paper that used pre-trained LLMs for item recommendation without additional training. Therefore, we set Sequential, Recency, and ICL from~\cite{hou2024large} as the baselines for comparison.

\smallskip
\noindent \textbf{Evaluation Setting.} To assess the performance of \myalg{}, we adopt a continuous sequential recommendation task. Note that NDCG scores are first aggregated per user across multiple recommendation sessions and then across all users, reflecting the continuous nature of our setup.
%Unlike conventional recommendation settings, this task requires a modification of the standard NDCG metric~\cite{zhao2021recbole} to align with our evaluation. Specifically, we calculate NDCG for each user and then compute the average NDCG score for all the users. 
%Note that results of all the experiments use the proposed NDCG metric.

% \noindent \textbf{Evaluation Setting.} To assess the performance of \myalg{}, we adopt a continuous sequential recommendation task. We construct the candidate set $\mathcal{C}_u$ by adding 19 randomly selected non-interacted items and one ground truth. For quantitative evaluation, we adopt a widely used metric, Normalized Discounted Cumulative Gain (NDCG) \cite{zhao2021recbole}, across all experiments. Specifically, if a user has purchased $k$ items, we evaluate the model from the 4th to the $k$-th interaction, resulting in $k-4$ recommendations. The NDCG values from these $k-4$ steps are averaged for each user and the final NDCG score is obtained by averaging across all users.

% Moreover, we constitute of candidate $\mathcal{C}_u$ by adding 19 randomly selected non-interacted items and one ground truth. For quantitative evaluation, we adopt a widely used metric, Normalized Discounted Cumulative Gain (NDCG) \cite{zhao2021recbole}, across all experiments.

\smallskip
\noindent \textbf{Implementation Details.} The prediction process is framed as a classification task where the model selects one item from candidate set $\mathcal{C}_u$. Each candidate set consists of 19 randomly selected non-interacted items and a ground truth item. We adopt Llama-3.2-3B-Instruct \cite{touvron2023llama} as the backbone model for all the experiments.
%and we normalized the LLM's output format to ensure consistent extraction as a list. 

% \cite{dubey2024llama}

\subsection{Experimental Results}

\smallskip
\noindent \textbf{Impact of Review Extractor.} 
\autoref{table:main} compares \myalg{} with (1) three baselines solely based on purchased items; (2) modified baselines, marked with $\dagger$, that additionally utilize users' raw reviews. 
%We newly create baselines (denoted as $\dagger$) by incorporating raw reviews into the original prompt template design.
The results reveal that baselines that simply combine item interactions with raw reviews show inconsistent performance improvements.
%It is difficult to observe consistent performance improvements in baselines that simply combine item interactions with raw reviews. 
In contrast, \myalg{}, which leverages the review extractor and profile updater, significantly outperforms all baselines. This demonstrates that {processing reviews at three levels, {i.e.}, like, dislike, and key features, is essential for enhancing performance}.

% \textbf{\myalg{} surpasses LLM-based Recommender.} To validate the effectiveness of \myalg{}, we compare it against several baselines and present the results in \cref{table:main}. Unlike previous studies that rely solely on past purchase history, we also include heuristic methods that incorporate users' reviews as additional baselines. 

\begin{figure}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/memrec_fig2.pdf}
    ~~~(a) Video Games  ~~~~~~~~~~~~~~~~~ (b)  Movies and TV
    \vspace{-0.5em}
    \caption{\textbf{Trade-off between NDCG and token size.}}
    \label{fig:com}
    \vspace{-0.05cm}
\end{figure}


\smallskip
\noindent \textbf{Component-wise Study.}
\autoref{table:ablation} shows the ablation study of \myalg{}, where we analyze the impact of reviews (using or not using) and the effect of components (enabling or disabling the review extractor and profile updater). The use of reviews bring high performance gains only when accompanied by Review Extractor (Ext.). This is due to the sharp increase in input tokens (see the |$T$| column of the 2nd and 3rd rows of each method) as the user continues purchases.

Notably, the best recommendation performance is achieved when Profile Updater (Upd.) is disabled (see the 3rd and 4th rows for each method). That is well-formed context by Review Extractor can bring higher gains when simply concatenated. However, it may face a challenge, as the number of purchases grows, leading to significant computational overhead. 
Thus, we use the Profile Updater (Upd.) to maintain compact user profiles, reducing input token size by 15--20\% with only a slight 1--3\% performance drop. This trade-off underscores the importance of using Profile Updater for long-term recommendations.

% \noindent \textbf{Necessities of user profile.} Our experiments reveal that directly using raw reviews without preprocessing fails to yield significant performance improvements. This indicates that past purchase history or raw reviews alone provide insufficient information for generating accurate recommendations. In contrast, leveraging the refined reviews produced by \myalg{} effectively addresses this limitation and consistently outperforms all baselines. These results underscore the importance of review preprocessing in capturing nuanced user preferences and improving recommendation accuracy.
% Table~\ref{table:main} presents a comparison of models using only reviews, only historical interactions, and a combination of both. The results demonstrate that incorporating both reviews and historical interactions significantly improves recommendation performance compared to using either in isolation. This highlights the importance of leveraging both explicit user opinions and past behavioral patterns for effective recommendations. Furthermore, the highest performance was achieved when the LLM was utilized to refine user reviews, extracting likes and dislikes, while simultaneously identifying key features from historical interactions. This result underscores the effectiveness of structured information extraction in improving recommendation accuracy by enabling the model to systematically capture and utilize user preferences and key product attributes.
% Furthermore, by applying a 1024-token truncation strategy, we aligned the token budget with real-world recommendation constraints, ensuring that the LLM operates within a practical computational scope. This constraint is crucial for maintaining efficiency while preserving essential contextual information. The results validate that even within a limited token budget, MemRec effectively captures relevant user preferences, reinforcing the feasibility of LLM-based recommendation in real-world applications.




% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/graph_video.pdf}
%         \caption{Video Games}
%         \label{fig:subfig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/graph_movies.pdf}
%         \caption{Movies and TV}
%         \label{fig:subfig2}
%     \end{subfigure}
%     \caption{\textbf{Performance Comparison\textcolor{red}{?}}}
%     \label{fig:com_fig}
% \end{figure}


% \noindent \textbf{Component-wise Analyses of \myalg{}.}~\autoref{table:ablation} shows the results of our ablation study conducted across four settings: (1) past purchase history only, (2) + review, (3) + review with extracted information, (4) + review with refined extracted information. Utilizing user preference consistently improves performance. While the model that concatenates all user preferences shows slightly better results, our model achieves comparable performance while saving tokens, making it more efficient. Moreover, as more reviews are added, our model can leverage its summarization and memory mechanisms, demonstrating even greater effectiveness in capturing user preferences and providing accurate recommendations (See~\autoref{app:qual}).

\smallskip
\noindent\textbf{Trade-off Analysis.}
We categorize users into three groups based on the total cumulative review token count per user, as the criterion: 0–500 (short), 500–1000 (middle), and 1000–2000 (long) tokens.
\autoref{fig:com} presents the trade-off between recommendation performance and input token length of the three models including \myalg{}.

\myalg{} achieves the best trade-off, showing the steepest NDCG increase compared to other methods as input token size grows. Therefore, this demonstrates that \myalg{} accurately distills key information from long reviews, while achieving high efficiency by minimizing input token growth without information loss, even for long-group users.


%The results indicate that ICL performs the worst due to relying solely on item interactions. In contrast, both ICL$^\dagger$ and \myalg{}(ICL), which leverage both item interactions and reviews, achieve better performance than ICL. Notably, as the total review length increases, \myalg{}(ICL) outperforms ICL$^\dagger$ by a larger margin while using fewer input tokens. This shows that \myalg{} becomes increasingly powerful as more reviews are available.

% the results of experiments conducted across multiple datasts, where we divided the data into four groups based on the number of tokens in naive reviews. Our model outperforms others both in terms of token efficiency and recommendation performance.
