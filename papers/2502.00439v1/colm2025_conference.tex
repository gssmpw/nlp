
\documentclass{article} % For LaTeX2e
\usepackage[preprint]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}



\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{lineno}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{UniAttn: Reducing Inference Costs via Softmax Unification \\ for Post-Training LLMs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yizhe Xiong$^1$, Wei Huang$^2$, Xin Ye$^{3}$, Hui Chen$^1$, Zijia Lin$^1$, Haoran Lian$^4$\\\textbf{Zhenpeng Su$^3$, Jungong Han$^5$, Guiguang Ding$^1$}\\ $^1$School of Software, Tsinghua University\\$^2$School of Computer Science, Beijing University of Posts and Telecommunications\\$^3$Kuaishou Technology\quad$^4$ Beihang University\\$^5$Department of Automation, Tsinghua University\\
\texttt{xiongyizhe2001@gmail.com}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

\begin{abstract}
Post-training is essential for adapting Large Language Models (LLMs) to real-world applications.
Deploying post-trained models faces significant challenges due to substantial memory overhead and noticeable inference latency.
Existing work has identified significant redundancies in LLMs and proposed efficient architectures, namely intra-layer KV sharing and cross-layer KV sharing.
However, intra-layer KV sharing still results in high inference costs, while cross-layer KV sharing leads to significant performance degradation. As a result, both methods remain suboptimal for post-training pre-trained LLMs. 
In this paper, we identify that the \texttt{Softmax} operation is a primary bottleneck for LLM inference and discover that it is actually highly redundant during post-training. 
We propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}), a novel post-training method that unifies Softmax activations across transformer blocks to reduce LLM inference costs. Additionally, UniAttn adopts a linear projection to compensate for the errors induced by Softmax unification. 
Experiments show that UniAttn matches the performance of standard post-training while significantly reducing inference costs, outperforming existing efficient architectures during post-training.
Our code will be available at \url{https://github.com/Bostoncake/UniAttn}.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/intro_0127.pdf}}
\caption{Comparisons of our UniAttn and directly applying cross-layer KV sharing (CLA) during post-training. ``A-X'' represents modifying total of X layers when applying method A. }
\label{fig:teaser}
\end{center}
\vskip -0.2in
\end{figure}

Post-training \citep{TULU3} has become a critical step in developing advanced LLMs for both general-purpose tasks \citep{GPT4, LLaVA} and domain-specific tasks \citep{thirunavukarasu2023large, PMC, DeepSeekMath}. 
It typically refers to performing supervised fine-tuning (SFT), reinforcement learning or even domain-specific continual pre-training on pre-trained LLMs using a text corpus different from the original pre-training dataset.
Current post-training approaches for real-world applications involve fine-tuning popular base models \citep{LLaMA3, Mistral, Gemma2} with standard decoder-based architectures that have scaled to hundreds of billions of parameters. 
However, these approaches face two widely recognized challenges in deploying post-trained models \citep{StreamingLLM, PrefixKV, LLMDrop, CLA, FastV}: (1) the substantial memory overhead required for KV-cache storage during inference, and (2) the computational latency, which leads to noticeable inference time.

In the literature, researchers have explored methods to achieve efficient inference and identified significant redundancies \textit{inside and across LLM layers}. This has further led to the development of corresponding efficient architectures: intra-layer Key-Value (KV) sharing, including MQA \citep{MQA}  GQA \citep{GQA}, and cross-layer KV sharing, including CLA \citep{CLA} and YOCO \citep{YOCO}.
Intra-layer KV sharing methods have been adopted by several open-source base models, including LLaMA-3.1 \citep{LLaMA3} and Gemma-2 \citep{Gemma2}, but these models still incur high inference costs. 
While cross-layer KV sharing has the potential to further reduce inference costs on top of intra-layer KV sharing methods, directly sharing KV-cache across pre-trained LLM layers results in significant performance degradation \citep{Kvsharer}. 
This raises a critical question: \textit{Which components in pre-trained LLMs are redundant, and how can they be leveraged to reduce inference costs during post-training while maintaining acceptable performance?}

To address this issue, we first conduct a comprehensive analysis of cross-layer KV sharing during post-training. 
Similar to the training-free scenario \citep{Kvsharer}, as shown in \cref{fig:teaser}, cross-layer KV sharing results in substantial performance drops during post-training. To understand this, we then conduct a theoretical analysis and find that cross-layer KV sharing diminishes the impact of model depth on model activations in pre-trained LLMs. Consequently, it undermines the benefits associated with model depth \citep{DeepNet}, limiting the pre-trained model's ability to learn new capabilities during post-training.
Therefore, alternative approaches beyond cross-layer KV sharing should be explored for post-training. 


These findings prompted us to explore other redundancies in pre-trained LLMs, leading to two key observations. First, the Softmax operation significantly contributes to inference costs. Performing the Softmax operation requires access to the entire K-cache. Although the Softmax operation accounts for less than 1\% of the FLOPs compared to the linear projections in the backbone, it results in higher latency than the linear projections \citep{SimA}.
Second, the Softmax operation does not account for the entire impact of model depth on model activations. Unlike cross-layer KV sharing, we demonstrate in \cref{sec:discussion} that eliminating the Softmax operation does not diminish the effect of model depth. 
Motivated by these observations, we further assess the importance of Softmax operations using a similarity-based metric \citep{ShortGPT}. Our analysis reveals that Softmax activations exhibit high cross-layer similarity in the \textit{top half layers} across various open-source pre-trained LLMs and post-training datasets. This observation suggests a generalizable approach to leveraging Softmax redundancies during post-training.


In this paper, we propose to post-train LLMs with Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}). Specifically, we group consecutive transformer blocks in the LLM as several ``Superblocks'', and unify the Softmax activations of all blocks within each Superblock. This approach significantly reduces memory and inference costs. 
Additionally, we observe that the error introduced by unifying Softmax activations can be compensated by a linear projection, and in turn design a pipeline to initialize and train the linear projection. 
We conduct extensive experiments on 4 open-source pre-trained LLMs across two post-training scenarios: enhancing domain-specific capabilities and improving general capabilities. Our results demonstrate that UniAttn consistently achieves performance comparable to standard post-training while reducing inference costs. Compared to directly applying existing efficient architectures to post-training, UniAttn achieves substantially better performance with lower inference costs. Furthermore, our UniAttn can be combined with KV-Cache compression methods to further cut down the memory overhead, showing strong practicality for real-world applications. 

Overall, we summarize our contribution as follows:

\begin{itemize}
    \item We consider a critical and practical challenge of leveraging the redundancies in pre-trained LLMs for post-training. We show theoretically and experimentally that existing methods are suboptimal.
    \item We study the redundancies in pre-trained LLMs and discovered significant redundancies in the Softmax operation. We propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}) for post-training LLMs. Specifically, we unify the Softmax activations across blocks inside the LLM. Additionally, we leverage linear projection to compensate for the error from Softmax unification.
    \item Extensive experiments show that UniAttn achieves comparable performance to standard post-training while reducing inference costs, and outperforms existing efficient architectures as well. 
\end{itemize}

\section{Related Works}

\subsection{Post-Training LLMs}

Building frontier LLMs for real-world applications involves two crucial stages: pre-training and post-training. Since pre-training data and methodologies are often proprietary, the research community has extensively explored post-training upon open-source pre-trained LLMs \citep{LLaMA3,Gemma2,Mistral} to enhance their \textit{general} or \textit{domain-specific} capabilities for deployment \citep{lian2024breakingstagebarriernovel}.
Post-training is typically performed on instruction-following or domain-specific datasets.
Recently, various datasets have been curated to equip LLMs with specific abilities, including general instruction-following models \citep{Hermes, TULU3, InfinityInstruct2024}, medical QA models \citep{PMC, Aloe, singhal2023expertlevelmedicalquestionanswering}, legal QA models \citep{LawyerLLaMA, LawGPT}, and models with strong mathematical problem-solving capabilities \citep{Goat}.

Existing research on post-training mainly focuses on creating specific datasets for equipping open-source LLMs with specific capabilities. 
Differently, we investigate the redundancies in pre-trained LLMs and leverage them for post-training inference-efficient LLMs.

\subsection{Efficient LLM Architectures}
Efficient architectures utilize structural redundancies to create inference efficient model variants \citep{ToMe,PYRA} for deployment.
In the scope of LLMs, efficient architectures mainly fall into two categories: intra-layer KV sharing, including MQA \citep{MQA} and GQA \citep{GQA}, and cross-layer KV sharing, including CLA \citep{CLA} and YOCO \citep{YOCO}.
Specifically, MQA \citep{MQA} simplifies the attention mechanism by utilizing multiple query heads and a single KV head. 
GQA \citep{GQA} takes a step further from MQA by organizing query heads as multiple groups and assigns different KV heads to each group.
CLA \citep{CLA} proposes a cross-layer sharing mechanism to further reduce KV-cache memory overhead by sharing KV-cache across different layers. 
YOCO \citep{YOCO} transforms the original structure into self-decoders and cross-decoders, and adopts a global KV-cache across decoders.

Existing intra-layer KV sharing works have been adopted by various open-source LLMs \citep{LLaMA3,Mistral,Gemma2}. However, due to their inherent massive scale, those models still incur significant inference costs. According to our analysis in \cref{fig:teaser} and \cref{sec:analysis_method}, directly applying cross-layer KV sharing for post-training is suboptimal. To address this, we propose UniAttn, which achieves promising performance for post-training LLMs and outperforms competing methods.

\section{Methodology}

\subsection{Preliminaries}

In this paper, we focus on mainstream decoder-only LLMs with a transformer structure. Each transformer block in an LLM features a multi-head self-attention (MHA) module and a feed-forward network (FFN). Given an input sequence $\mathbf{x}\in \mathbb{R}^{l\times d}$ where $l$ denotes the sequence length and $d$ denotes the hidden size, both MHA and FFN generate an output sequence with identical length and shape.
We focus on the MHA module. Formally, we denote MHA output in layer $i$ as $\mathbf{x}_i'$, where $\mathbf{x}_i'=\text{MHA}(\text{Norm}(\mathbf{x}_i))+\mathbf{x}_i$. ``$\text{Norm}(\cdot)$'' denotes the Pre-Norm, a component adopted by mainstream open-source LLMs \citep{LLaMA3,Mistral,Gemma2,bai2023qwen}.
In the MHA module, each token in the input sequence $\mathbf{x}$ is first being projected by $W_Q$, $W_K$, and $W_V$, forming $Q,K\in d\times d_k$ and $V\in d\times d_v$. Then, the Softmax activation $s_i$ is calculated by: 
\begin{equation}
\label{eq:softmax_activation}
    s=\text{softmax}(\frac{QK^T}{\sqrt{d_k}}).
\end{equation}
Subsequently, $s$ is projected using $V$ and the weight matrix $W_o$, and the input to MHA is added back through a residual connection to produce the MHA output $\mathbf{x}_i'$:
\begin{equation}
    \mathbf{x}_i'=sVW_o+\mathbf{x}_i
\end{equation}

\subsection{Analysis on Existing Methods}
\label{sec:analysis_method}


To address redundancies in LLMs, mainstream efficient LLM architectures typically remove insignificant modules in the backbone. 
Among these methods, intra-layer KV sharing methods such as GQA \citep{GQA} has been widely adopted by numerous open-source pre-trained models, like LLaMA3, Mistral, and Gemma2. 
On the other hand, cross-layer KV sharing methods, including CLA \citep{CLA} and YOCO \citep{YOCO}, share the same KV-cache across different layers. Through theoretical analysis, we show that cross-layer KV sharing diminishes the impact of model depth on model activations. This negatively affects the capabilities of pre-trained models, as model depth is a critical factor in determining their overall performance \citep{DeepNet}.

We first give a series of conclusions as preliminaries.
\begin{lemma}
\label{lem:linear}
$f_i:\mathbb{R}^n\rightarrow\mathbb{R}^n$, $i\in \mathbb{N}$ are LINEAR transformations and $\text{Norm}(\cdot)$ yields a vector with unit Frobenius-norm (denoted as $||\cdot||$). Let $\mathbf{x}_0\in \mathbb{R}^n$ be the input to the system. Consider an $L$-layer Pre-Norm architecture defined by:
\begin{equation}
    \mathbf{x}_{k}=\mathbf{x}_{k-1}+f_k(\text{Norm}(\mathbf{x}_{k-1})),\quad \text{for}\quad k=1,2,\dots,L.
\end{equation}  
If the largest singular value in all transformation matrices is bounded by $\lambda$, then:
\begin{equation}
    ||\mathbf{x}_L||\le||\mathbf{x}_0||+\lambda L
\end{equation} 
\end{lemma}

See Appendix for the proof.
\cref{lem:linear} shows that a Pre-Norm linear system has bounded growth. The norm of output by each layer grows \textit{at most linearly} with depth $L$. 
\begin{proposition}
\label{prop:linear}
    Pre-trained decoder-based LLMs exhibit a high linearity score. Formally, let $A, B\in \mathbb{R}^{n\times d}$ denote the normalized input and output of a decoder block in LLM, respectively,
\begin{equation}
    \min_X ||AX-B||\approx1
\end{equation}
\end{proposition}

\cref{prop:linear} has been validated by \citep{razzhigaev-etal-2024-transformer} on pre-trained LLMs, 
suggesting that we can approximate each decoder block by a linear transformation with bounded singular values, 
thus placing pre-trained LLMs structurally “close” to a linear Pre-Norm system as in \cref{lem:linear}.
In the following discussion, we approximate pre-trained LLMs with an equivalent linear Pre-Norm system and investigate its input and output in each layer. 

\begin{proposition}
\label{prop:similarity}
    The input and output of a layer in pre-trained decoder-based LLMs share a high cosine similarity.
\end{proposition}
\cref{prop:similarity} has been validated by \citep{ShortGPT} on pre-trained LLMs, indicating that each layer in LLMs predominantly change the \textit{magnitude} of the activation.
We leverage the following conclusion for analyzing the change of activation magnitude.
\begin{proposition}
\label{prop:prenorm}
    Training Pre-Norm transformers leads to top layers receiving smaller updates (gradient norms decrease from bottom to top).
\end{proposition}
\citep{DBLP:conf/icml/XiongYHZZXZLWL20} has given both a theoretical proof sketch and experimental evidence of \cref{prop:prenorm}. 
Together with \cref{prop:linear}, we can reasonably assume that the largest singular values of the top layers tend to be smaller after training (See Appendix for demonstration). Since pre-trained LLMs mainly change the magnitude of the activation (as shown in \cref{prop:similarity}), we can use the size of the largest singular values in each layer as an indicator of the corresponding activation magnitude change. This leads to the assumption that the top-layer transformations generally generate outputs with smaller norms.
\begin{assumption}
\label{assum:inftesimal}
In top layers of \textbf{pre-trained} Pre-Norm LLMs:
\begin{equation}
    \mathbf{x}_{i+1}=\mathbf{x}_{i}+\delta,\quad \text{where}\quad||\delta||\ll ||\mathbf{x}_{i}||.
\end{equation}
\end{assumption}
We define $\delta$ as the ``depth factor'' that model applies on activations for further discussion. $\delta$ represents the extent to which model depth influences activations.

\textbf{Analysis on Cross-layer KV Sharing:} We adopt \cref{assum:inftesimal} to analyze the cross-layer KV sharing architecture, in which KV-cache is shared across multiple consecutive layers. Suppose layer $i+1$ shares the KV-cache from layer $i$, the MHA operation in layer $i+1$ can be written as:
\begin{equation}
\label{eq:cla}
    \mathbf{x}'_{i+1}=\text{softmax}(\frac{QK^T_{i}}{\sqrt{d_k}})V_iW_o+\mathbf{x}_{i+1},
\end{equation}
where $K_i$ and $V_i$ are the $K$ and $V$ matrices from layer $i$. We propose that:
\begin{proposition}
\label{prop:effective_depth}
    In \textbf{pre-trained} Pre-Norm LLMs, cross-layer KV sharing diminishes the ``depth factor'' $\delta$ on activations.
\end{proposition}
\begin{proof}
For simplicity, we suppose $\mathbf{x}_{i+1}$ is already normalized by the Pre-Norm. We can re-write \cref{eq:cla} as:
\begin{equation}
\begin{aligned}
    \mathbf{x}'_{i+1}&=\text{softmax}(\frac{\mathbf{x}_{i+1}W_{q,i+1}K^T_{i}}{\sqrt{d_k}})V_iW_o+\mathbf{x}_{i+1} \\
    &=\text{softmax}(\frac{(\mathbf{x}_{i}+\delta)W_{q,i+1}K^T_{i}}{\sqrt{d_k}})V_iW_o+\mathbf{x}_{i+1} \\
    &=\text{softmax}(\frac{\mathbf{x}_{i}W_{q,i+1}K^T_{i}}{\sqrt{d_k}}+\frac{\delta W_{q,i+1}K^T_{i}}{\sqrt{d_k}})V_iW_o+\mathbf{x}_{i+1}
\end{aligned}
\end{equation}
Let $A_i=\frac{\mathbf{x}_{i}W_{q,i+1}K^T_{i}}{\sqrt{d_k}}$, $\hat{\delta}=\frac{\delta W_{q,i+1}K^T_{i}}{\sqrt{d_k}}$. By expanding Softmax to the first order:
\begin{equation}
\begin{aligned}
    \mathbf{x}'_{i+1}
    &=\big[\text{softmax}(A_i)+J(A_i)\hat{\delta}\big]V_iW_o+\mathbf{x}_{i+1}+o(\hat{\delta}^2),
\end{aligned}
\end{equation} 
where $J(A_i)$ is the Jacobian of Softmax at $A_i$. We ignore the $o(\hat{\delta}^2)$ remainder. 
\citep{StreamingLLM} has discovered that the pre-trained LLMs generally exhibit an ``attention sink'' feature, in which self-attentions heavily attend to the ``sink token'' at the beginning of a sequence. We adopt such feature to estimate $||J(A_i)||$.
According to the statistical pattern concluded by \citep{StreamingLLM}, attention logits $\mathbf{a}\in \mathbb{R}^{l}$ of a sequence with length $l$ is approximately:
\begin{equation}
    a_i=1\; (0<i<d),\; a_j=-1\; (i\ge d),\; d \ll l
\end{equation}
\citep{StreamingLLM} empirically adopts $d=4$. For a sequence of $l=1024$, $||J(\text{softmax}(\mathbf{a}))||\approx0.03\ll 1$, and decreases as $l$ increases.
Therefore, we can conclude that the coefficient of the $\hat{\delta}$ term is significant smaller than that of the Softmax term, making the contribution of the ``depth factor'' negligible.
\end{proof}

\Cref{prop:effective_depth} shows that cross-layer KV sharing diminishes the impact of model depth on model activations. When further analyzing the Softmax term, we derive the following:
\begin{equation}
\label{eq:cla_equiv}
\begin{aligned}
    \mathbf{x}'_{i+1}
    &=\text{softmax}(A_i)V_iW_o+\mathbf{x}_{i+1} \\
    &=\text{softmax}(\frac{\mathbf{x}_{i}W_{q,i+1}K^T_{i}}{\sqrt{d_k}})V_iW_o+\mathbf{x}_{i+1}.
\end{aligned}
\end{equation}
The only difference between the first term in \cref{eq:cla_equiv} and conducting MHA on $\mathbf{x}'_{i}$ is that they use different $W_q$ matrices. Consequently, the MHA module in layer $i+1$ can be interpreted as another query head for the MHA module in layer $i$. In essence, \textit{cross-layer KV sharing transforms MHA modules in different layers into additional query heads in a single layer}. 



\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/re.pdf}}
\caption{Cosine similarity results of average Softmax activations. Across all settings, the average Softmax activations of the top half of layers share a high cosine similarity very close to 1. }
\label{fig:softmax_sim}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{UniAttn: Softmax Unification in Attention}
\label{sec:uniattn}


\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/pipeline_v2.pdf}}
\caption{Pipeline comparison between standard decoder-based transformers, CLA \citep{CLA}, and UniAttn. UniAttn shares the Softmax activations across layers in grouped Superblocks and adds linear transformation $W_c$ to compensate for the unification error.}
\label{fig:pipeline}
\end{center}
\vskip -0.2in
\end{figure*} 

To achieve both memory-efficiency and inference acceleration, we investigate the Softmax operation because (1) calculating Softmax demands the entire K-cache that accounts for 50\% memory of the KV-cache; and (2) multiple studies \citep{DBLP:conf/accv/GengLZKAC18,SimA} have recognized that the Softmax operation leads to high inference latency. We study the redundancy of Softmax activations in pre-trained LLMs by measuring the importance of Softmax operations in each layer. To ensure the generalizability of our study, 
we employ post-training datasets PMC \citep{PMC}, Infinity Instruct (Math) \citep{InfinityInstruct2024}, Tulu3 \citep{TULU3}, and long samples (F15K) provided by \citep{rerope2023}, and utilize LLaMA-2 7B \citep{touvron2023llama}, LLaMA-3.1 8B \citep{LLaMA3}, Mistral 7B \citep{Mistral}, and Gemma-2 9B \citep{Gemma2}. 
We collect all Softmax activations, i.e., the $s$ vectors in \cref{eq:softmax_activation}, and compute the cosine similarity $\text{Sim}(i)=\cos(s_i,s_{i-1})$ for each sequence. We then average $\text{Sim}(i)$ over all sequences. As shown in \cref{fig:softmax_sim}, Softmax activations in \textit{top half layers} share a high cross-layer similarity across different experiment settings. This observation shows that Softmax operations are highly redundant in LLMs.


Motivated by such redundancy, we propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}) for post-training LLMs. Specifically, UniAttn groups consecutive decoders in LLMs as ``Superblocks'', and unifies the Softmax activations inside each Superblock to reduce the full Softmax operations and KV-cache size during inference. Additionally, we observe that the error introduced by Softmax unification can be compensated by a linear projection, and in turn design a pipeline to initialize and train the linear projection. 
UniAttn simultaneously reduces the GPU memory and inference latency while achieving competitive performance. 
Different from cross-layer KV sharing, UniAttn does not diminish the ``depth factor''. We show this in \cref{sec:discussion}.

\textbf{Unify Softmax Activations in Superblocks.}
To apply UniAttn, we first merge several consecutive transformer blocks as ``SuperBlocks''. For simplicity, we create SuperBlocks of the same size. Inside each SuperBlock, only the bottom block employs the Softmax operation. Other blocks simply re-use the Softmax activation matrix $s$ calculated by the bottom block in the SuperBlock. Formally, for layer $i+b$ that re-use the Softmax activation $s_i$ from layer $i$:
\begin{equation}
    \mathbf{x}_{i+b}'=s_iV_{i+b}W_o+\mathbf{x}_{i+b}
\end{equation} 
Since only the bottom block contribute to calculating the Key projection, other blocks that re-use Softmax activations do not store their corresponding K-cache, leading to GPU memory-efficiency. Moreover, removing Softmax calculations in those layers also contribute to inference latency reduction.
Similar to cross-layer KV sharing, UniAttn is also orthogonal to intra-layer KV sharing. In \cref{sec:exp}, we employ pre-trained LLMs with GQA to evaluate UniAttn.

\textbf{Linear Compensation.}
Simply unifying Softmax activations unavoidably introduces errors in the forward pass. Formally, the error $\epsilon$ in layer $i+1$ is:
\begin{equation}
\begin{aligned}
    \epsilon&=\mathbf{x}_{i+b}^{\text{ori}\ \prime}-\mathbf{x}_{i+b}^{\text{uni}\ \prime} \\
    &=(\text{softmax}(\frac{Q_{i+b}K^T_{i+b}}{\sqrt{d_k}})-s_i)V_{i+b}W_o+(\mathbf{x}_{i+b}^{\text{ori}}-\mathbf{x}_{i+b}^{\text{uni}})
\end{aligned}
\end{equation}
where $\mathbf{x}_{i+b}^{\text{ori}}$ and $\mathbf{x}_{i+b}^{\text{Uni}}$ denote the input to layer $i+b$ in the original model and the model with UniAttn applied, respectively. According to \citep{SimA}, calculating linear projections yield \textit{significantly lower} latency compared to the Softmax operation. And thus we study the effectiveness of compensating $\epsilon$ with a linear projection matrix $W_{c}$. To compensate for $\epsilon$, we apply $W_{c}$ to each layer with unified Softmax activations as:
\begin{equation}
    \mathbf{x}_{i+b}'=s_iV_{i+1}W_o+\mathbf{x}_{i+b}+\mathbf{x}_{i+b}W_{c}
\end{equation}
We initialize $W_{c}$ according to the following Theorem:
\begin{theorem}
\label{theorem:init}
    The optimal initialization for $W_{c}$ satisfies:
    \begin{equation}
        W_{c}=V\Sigma^{+}U^T\mathbb{E}(\epsilon),
    \end{equation}
    where $U\Sigma V^T$ denotes the SVD decomposition of $\mathbb{E}(\mathbf{x}_{i+b})$, $\Sigma^{+}$ denotes the pseudoinverse of $\Sigma$.
\end{theorem}
We leave the proof in the Appendix.
In practice, we forward a small portion of training data from the post-training datasets, and use the average values $\bar{\mathbf{x}}_{i+b}$ and $\bar{\epsilon}$ as estimations of $\mathbb{E}(\mathbf{x}_{i+b})$ and $\mathbb{E}(\epsilon)$. Considering that compensating for errors in previous layers would influence the calculation in subsequent layers, we calculate the initialization of $W_c$ bottom-up. 
Experiments on the F15K dataset \citep{rerope2023} with sequence length 5,120 yield errors of 11.09 (LLaMA-2 7B) and 5.76 (LLaMA-3.1 8B) after linear compensation, representing only 2.9\% and 6.4\% of the error before compensation, respectively, well demonstrating its effectiveness. See the appendix for more theoretical insights. 

To better train the linear transformations, we adopt a two-stage post-training pipeline. In the first stage, we only fine-tune the $W_c$ weights and keep other weights frozen. Early stop is also adopted in the first stage. We then conduct a full fine-tuning in the second stage. We formalize a detailed pipeline in \cref{alg:example} in the Appendix. 

\subsection{Discussion}
\label{sec:discussion}
Our UniAttn reduces both memory cost and inference latency simultaneously. 
We underscore that unlike cross-layer KV sharing, our UniAttn does not diminish the ``depth factor'' $\delta$ in the forward pass. 
\begin{proposition}
\label{prop:uniattn_depth}
    In pre-trained Pre-Norm LLMs, the ``depth factor'' in UniAttn forward pass is unignorable. 
\end{proposition}
\begin{proof}
In the forward pass of UniAttn:
\begin{equation}
\begin{aligned}
    \mathbf{x}_{i+1}'&=s_i(\mathbf{x}_{i+1}W_{v,i+1})W_o+\mathbf{x}_{i+1} \\
    &=s_i((\mathbf{x}_{i}+\delta)W_{v,i+1})W_o+\mathbf{x}_{i+1} \\
    &=s_i\mathbf{x}_{i}W_{v,i+1}W_o+s_i\delta W_{v,i+1}W_o+\mathbf{x}_{i+1}
\end{aligned}
\end{equation}
As $\mathbf{x}_{i}$ and $s_i$ have undergone the same multiplications, we can assume that UniAttn does not modify the relative size of both terms. Since we do not ignore the depth factor $\delta$, we consider $s_i\delta W_{v,i+1}W_o$ is unignorable.
\end{proof}

\begin{table*}[t]
    \centering
    \caption{Post-training performance (\%) comparison. \textbf{Bold} and \underline{underline} denote the best and second-best performance of compressed models. For each method, we report their time to first token (TTFT, in seconds) and KV-cache retain rate (KV Cache). }
    \vskip 0.15in
    \label{tab:main}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|cc|ccc|c|cc|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\makecell{TTFT \\ (s)}} & \multirow{2}{*}{\makecell{KV \\ Cache}} & \multicolumn{4}{c|}{Medical} & \multicolumn{3}{c}{General}    \\ 
&  & & PubMedQa   & MedMCQA  & \multicolumn{1}{c}{MedQA} & AVG &  SIQA  & \multicolumn{1}{c}{CommonsenseQA} & AVG  \\ \midrule
\multicolumn{10}{c}{\textbf{LLama3.1-8B (w/ GQA)}}  \\ 
\midrule
Pre-Trained & 2.26 & 100\%  & 75.0   & 56.8  & 59.1   & 63.6 & 46.8 & 71.7  & 59.3 \\
Post-Train & 2.26 & 100\%  & 75.4   & 59.0  & 63.2   & 65.9 & 50.1 & 73.9  & 62.0 \\ \hline
LLMDrop-Half & 1.77 & 81.3\%  & 75.6   & \underline{57.8}  & \underline{61.5}   & \underline{65.0} & 51.4 & \underline{75.0}  & \underline{63.2}  \\ 
LLMDrop-Full & \textbf{1.47} & 62.5\%  & \underline{78.2}   & 53.6  & 55.9   & 62.6 & \textbf{51.8} & 73.3  & 62.6 \\ 
CLA-Half & 2.18 & 81.3\%  & 73.8   & \textbf{58.1}  & \textbf{62.8}   & 64.9 & \underline{51.7} & 74.7  & \underline{63.2} \\ 
CLA-Full & 2.23 & 62.5\%  & 75.4   & 56.9  & 59.2   & 63.8 & 50.7 & 71.2  & 61.0 \\ 
UniAttn (Ours) & \underline{1.48} & 81.3\%  & \textbf{79.0}   & 57.6  & 59.5   & \textbf{65.4} & 51.1 & \textbf{75.6}  & \textbf{63.4} \\ 
\midrule
\multicolumn{10}{c}{\textbf{LLama2-7B (w/o GQA)}}  \\ 
\midrule
Pre-Trained & 2.20 & 100\%  & 71.4   & 32.2  & 34.7   & 46.1 & 50.2 & 51.1  & 52.7 \\
Post-Train & 2.20 & 100\%  & 75.4   & 48.8  & 49.2   & 57.8 & 51.6 & 66.3  & 59.0 \\ \hline
LLMDrop-Half & 1.74 & 81.3\%  & \underline{75.2}   & 48.4  & 49.8   & \underline{57.8} & \underline{51.7} & \underline{66.7} & \underline{59.2}  \\ 
LLMDrop-Full & \textbf{1.37} & 62.5\%  & 75.0   & \underline{48.8}  & 48.8 & 57.5 & 51.2 & 65.2  & 58.2 \\ 
CLA-Half & 2.20 & 81.3\%  & 74.4   & 48.4  & \textbf{50.6}   & \underline{57.8} & 51.3 & 66.1  & 58.7 \\ 
CLA-Full & 2.23 & 62.5\%  & \underline{75.2}   & 45.7  & 47.5   & 56.1 & 50.2 & 65.6  & 57.9 \\ 
UniAttn (Ours) & \underline{1.44} & 81.3\%  & \textbf{75.6}   & \textbf{49.4}  & \underline{50.1}   & \textbf{58.4} & \textbf{51.9} & \textbf{67.7}  & \textbf{59.8} \\ 
\bottomrule
 \end{tabular}}
\vskip -0.1in
\end{table*}

\section{Experiments}
\label{sec:exp}

\subsection{Experimental Settings}
\textbf{Post-Training Settings.} 
We consider two post-training settings to validate our UniAttn: fine-tuning on domain-specific datasets and on general instruction following datasets. 
We choose PMC \citep{PMC} as the domain-specific dataset and evaluate fine-tuned models on PubMedQA \citep{PubMedQA}, MedMCQA \citep{MedMCQA}, and MedQA \citep{MedQA}.
We choose Tulu3 \citep{TULU3} as the general instruction following dataset and evaluate fine-tuned models on SIQA \citep{SIQA} and CommonsenseQA \citep{CommonsenseQA}. 
We employ \citep{eval-harness} to perform all benchmark evaluations.

\textbf{Model.} 
We post-train 4 open-source pre-trained LLMs for evaluating our UniAttn: LLaMA-2 7B \citep{touvron2023llama}, LLaMA-3.1 8B \citep{LLaMA3}, Mistral 7B \citep{Mistral}, and Gemma-2 9B \citep{Gemma2}. We use base models that have undergone only pre-training.

\textbf{Implementation details.} 
We adopt the pattern in \cref{fig:softmax_sim} and apply UniAttn in the \textit{top half of layers}. Unless otherwise noted, we adopt Superblock size $=4$, which yields a total of 4 Superblocks in LLaMA-2 7B, LLaMA-3.1 8B, Mistral 7B, and 5 Superblocks in Gemma-2 9B. For all experiments, we post-train LLMs for 1 epoch. We adopt all training hyperparameter values based on existing research. All training experiments were conducted on 8 H800 GPUs. For time to first token (TTFT) time measurement, we use a single H800 GPU and context length of 8,192 and measure the time to generate the first token after receiving the sequence input. See the Appendix for more details. 


\subsection{Performance Comparison on Post-Training}

We verify the effectiveness of UniAttn on 2 open-source LLMs with different architectures, namely LLaMA-3.1 8B (with GQA) \citep{LLaMA3} and LLaMA-2 7B (without GQA) \citep{touvron2023llama}. 
Please refer to the Appendix for more results on Mistral 7B \citep{Mistral} and Gemma-2 9B \citep{Gemma2}.
We use LLMDrop \citep{LLMDrop} and CLA \citep{CLA} as competing efficient architectures, as both methods exploit the cross-layer redundancies in LLMs. For LLMDrop, we bypass the MHA modules with the highest input-output similarities in the top half of the layers. For CLA, we group the top half of the layers into CLA blocks, using the same configuration as in the grouping of Superblocks.
Since both competing methods drop the entire KV-cache in the operated layers, we compare two configurations: one with the same operating layers (X-Full) and another with the same KV-cache compression ratio (half operating layers, X-Half). We also conduct post-training after applying these methods.

\textbf{Main Conclusion.} We compare different efficient architectures against directly post-training the original LLM (Post-Train) and using the pre-trained model (Pre-Trained) as baselines, as shown in \cref{tab:main}. Among all competing architectures, our UniAttn achieves the best overall performance across model structures (with and without GQA) and post-training datasets. Besides significantly reducing inference costs in both time and memory, UniAttn maintains comparable performance to directly post-training the original model, well demonstrating its effectiveness.
When comparing UniAttn with LLMDrop-Full, we observe that Softmax unification provides a similar acceleration rate to bypassing the entire MHA module, but it achieves significantly better post-training performance. This aligns with our finding that Softmax operations are both costly and redundant. When comparing to CLA, UniAttn achieves better performance with significantly lower inference latency, demonstrating that the UniAttn architecture is better suited for post-training than cross-layer KV sharing. 

\textbf{Analysis of CLA.} 
Since CLA only reduces the latency associated with KV projections, its impact on TTFT is minimal. As a result, CLA-Half, CLA-Full, and the original model exhibit comparable TTFT times.
Furthermore, CLA achieves comparable performance to LLMDrop when applied to the same number of layers. This suggests that the cross-layer attention mechanism employed by CLA is structurally equivalent to directly bypassing the MHA module. This observation supports our theoretical analysis that cross-layer KV sharing methods like CLA can diminish the depth factor in pre-trained LLMs, in turn limiting their performance. 



\subsection{Experimental Analysis}
We choose LLaMA-3.1 8B \citep{LLaMA3} and the medical domain to conduct experimental analysis. Please refer to the Appendix for more analysis experiments.

\begin{table*}[t]
    \centering
    \caption{Ablation study results. \textbf{Bold} and \underline{underline} denote the best and second-best performance of compressed models.}
    \vskip 0.15in
    \label{tab:ablation}
    \resizebox{\textwidth}{!}{\begin{tabular}{c|ccc|cccc}
\toprule
Method & Unify Softmax & Compensation & Init $W_c$ & PubMed & MedMCQA & MedQA & AVG \\
\midrule
Post-Train & - & - & - & 75.4 & 59.0  & 63.2 & 65.9 \\ \hline 
\multirow{4}{*}{UniAttn} & $\surd$ & $\times$ & $\times$ & 77.0 & 55.7 & \underline{58.8} & 63.8 \\
 & $\surd$ & $\surd$ & zero-init & 78.0 & 55.8 & 58.1 & 63.9 \\
 & $\surd$ & $\surd$ & \cref{theorem:init} & \underline{78.2} & \underline{57.1} & 58.4 & \underline{64.6} \\
 & $\surd$ & $\surd$ & \cref{theorem:init} + fine-tune & \textbf{79.0} & \textbf{57.6} & \textbf{59.5} & \textbf{65.4} \\
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

\begin{table}[t]
    \centering
    \caption{Results of inference cost analysis. ``H$_2$O Comp.'' denotes the compression rate introduced by H$_2$O and ``KV-Cache'' denotes the KV-Cache retain rate.}
    \vskip 0.15in
    \label{tab:uniattn_h2o}
    \resizebox{0.6\columnwidth}{!}{\begin{tabular}{c|ccc|c}
\toprule
Method & TTFT (s) & H$_2$O Comp. & KV-Cache & AVG \\
\midrule
Post-Train & 2.26 & - & 100\% & 65.9 \\
CLA-Half & 2.18 & - & 81.3\% & 64.9 \\
CLA-Full & 2.23 & - & 62.5\% & 63.8 \\ \midrule
UniAttn & 1.48 & - & 81.3\% & 65.4 \\
\multirow{3}{*}{UniAttn+H$_2$O} & 1.48 & 20\% & 65.0\% & 65.2 \\
 & 1.46 & 40\% & 48.8\% & 65.0 \\
 & 1.46 & 60\% & 32.5\% & 63.3 \\
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table}

\begin{figure}[t]  % figure* for spanning across both columns
\vskip 0.2in
    \begin{minipage}{1.0\columnwidth}  % 2/3 width for the first figure
        \centering
        \begin{minipage}{0.49\textwidth}  % subfigure 1
            \centering
            \includegraphics[width=\textwidth]{figs/Operating_layer_ratio_modified_hw.pdf}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.49\textwidth}  % subfigure 2
            \centering
            \includegraphics[width=\textwidth]{figs/Superblock_Size_modified_hw.pdf}
        \end{minipage}
        \caption{Hyperparameter analysis results (\%). Left: The relationship of average accuracy and TTFT latency to the total number of grouped Superblocks (each with size 4). Right: The relationship of average accuracy and TTFT latency to the size of each Superblock (total of 12 layers that utilize unified Softmax activation). }
        \label{fig:hyperparam}
    \end{minipage}%
\vskip -0.2in
\end{figure}



\textbf{Ablation Studies.}
We do controlled experiments to evaluate the impact of each component in UniAttn, namely Softmax unification, linear compensation $W_c$, and the initialization for $W_c$. The results are shown in \cref{tab:ablation}.
We observe that after unifying the Softmax activations, both adding linear compensation and using proper initialization contribute positively to the final performance. 
While linear compensation consistently improves performance across different initialization methods, appropriate initialization proves critical for effectively fine-tuning $W_c$.


\textbf{Inference Cost Analysis.}
We evaluate and compare inference costs to demonstrate the efficiency of UniAttn. To further maximize its potential, we integrate UniAttn with the KV-Cache compression method H$_2$O \citep{H2O} to further reduce memory overhead by keeping only the essential and the recent tokens in the KV-cache. We report the TTFT latency and the total KV-cache retain rate. As shown in \cref{tab:uniattn_h2o}, UniAttn achieves a significant reduction in TTFT latency compared to CLA under the same KV-cache retain rate.
When combined with H$_2$O, UniAttn+H$_2$O reduces the KV-cache retain rate to below 50\% while maintaining an average performance of 65.0\%, achieving both better performance, lower inference latency, and higher KV-cache compression rate than CLA. 
These results clearly demonstrate the inference efficiency of UniAttn, highlighting its substantial benefits for real-world deployment scenarios.

\textbf{Hyperparameter Analysis.}
We perform a hyperparameter analysis on the number and the size of Superblocks to evaluate their impact on latency and post-training performance.
First, we examine the effect of varying the number of Superblocks. Using a consistent Superblock size of 4, we group different numbers of Superblocks from the top layers to the bottom layers. For instance, grouping 2 Superblocks indicates that layer 25-28 and 29-32 form the two Superblocks.
The results are shown in \cref{fig:hyperparam} (left). From the perspective of TTFT latency, increasing the number of Superblocks significantly reduces latency. From the perspective of performance, while grouping certain top layers as Superblocks does not substantially affect model performance, continuing to increase the number of Superblocks leads to sharp performance drops.
Second, we analyze the impact of Superblock size. For fair comparison, we maintain \textit{a consistent total number of layers that utilize unified Softmax activations from preceding layers} as 12. Since a Superblock with size $b$ unifies Softmax in $b-1$ layers, we set $b$ such that $b-1$ is a factor of 12, resulting in Superblock sizes of 2, 3, 4, 5, 7, and 13, and number of Superblocks as 12, 6, 4, 3, 2, and 1, respectively. 
As shown in \cref{fig:hyperparam} (right), the size of the Superblocks does not significantly affect post-training performance. For simplicity, we use a Superblock size of 4, although fine-grained tuning of Superblock size could further improve post-training performance.


\section{Conclusion}

In this work, we explored LLM redundancies and efficient architectures in the context of post-training to reduce the inference costs of LLMs. Theoretically, we identified that the existing cross-layer KV sharing architecture diminishes the depth factor in pre-trained LLMs, potentially limiting their ability to acquire new capabilities during post-training.
To address this limitation, we investigated the primary bottleneck in LLMs---the Softmax operation---and discovered significant redundancies across various pre-trained LLMs and datasets. Based on these findings, we proposed UniAttn, an efficient architecture that leverages these redundancies while preserving LLM capabilities through linear compensation in post-training.
Extensive experiments on diverse pre-trained models and post-training datasets validate the effectiveness of UniAttn. 
Our UniAttn is particularly well-suited for real-world deployments, enjoying faster inference and less memory cost. 

\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\newpage
\appendix
% \section{Appendix}
\begin{table*}[h]
    \centering
    \caption{Training hyperparameter details of UniAttn.}
    \vskip 0.15in
    \label{tab:hyperparam_uniattn}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
\toprule
Method & Model & Learning Rate & Weight Decay & Batch Size & Epochs & Superblock Size & Superblock Groups \\
\midrule
\multirow{4}{*}{UniAttn} & LLaMA-2 7B & 2e-5 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
 & LLaMA-3.1 8B & 7e-6 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
 & Mistral 7B & 1e-6 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
 & Gemma-2 9B & 1e-6 & 0 & 48 & 1 & 4 & [22-25], [26-29], [30-33], [34-37], [38-41] \\
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
    \centering
    \caption{Training hyperparameter details of CLA.}
    \vskip 0.15in
    \label{tab:hyperparam_cla}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
\toprule
Method & Model & Learning Rate & Weight Decay & Batch Size & Epochs & CLA Block Size & CLA Block Groups \\
\midrule
\multirow{4}{*}{CLA-Half} & LLaMA-2 7B & 2e-5 & 0 & 48 & 1 & 4 & [25-28], [29-32] \\
 & LLaMA-3.1 8B & 7e-6 & 0 & 48 & 1 & 4 & [25-28], [29-32] \\
& Mistral 7B & 1e-6 & 0 & 48 & 1 & 4 & [25-28], [29-32] \\
& Gemma-2 9B & 1e-6 & 0 & 48 & 1 & 4 & [30-33], [34-37], [38-41] \\ \midrule
\multirow{4}{*}{CLA-Full} & LLaMA-2 7B & 2e-5 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
 & LLaMA-3.1 8B & 7e-6 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
& Mistral 7B & 1e-6 & 0 & 48 & 1 & 4 & [17-20], [21-24], [25-28], [29-32] \\
& Gemma-2 9B & 1e-6 & 0 & 48 & 1 & 4 & [22-25], [26-29], [30-33], [34-37], [38-41] \\
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

\begin{table*}[h]
    \centering
    \caption{Training hyperparameter details of LLMDrop. The index of dropped layers are ordered by input-output similarity.}
    \vskip 0.15in
    \label{tab:hyperparam_llmdrop}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|c|cccccc}
\toprule
Method & Model & Learning Rate & Weight Decay & Batch Size & Epochs & \# of Dropped Layers  & Index of Dropped Layers \\
\midrule
\multirow{4}{*}{LLMDrop-Half} & LLaMA-2 7B & 2e-5 & 0 & 48 & 1 & 6 & 23,20,32,27,22,24 \\
 & LLaMA-3.1 8B & 7e-6 & 0 & 48 & 1 & 6 & 19,22,20,21,27,23 \\
& Mistral 7B & 1e-6 & 0 & 48 & 1 & 6 & 17,32,31,21,22,24 \\
& Gemma-2 9B & 1e-6 & 0 & 48 & 1 & 8 & 29,26,21,30,27,40,38,31 \\ \midrule
\multirow{4}{*}{LLMDrop-Full} & LLaMA-2 7B & 2e-5 & 0 & 48 & 1 & 12 & 23,20,32,27,22,24,31,29,25,30,28,26 \\
 & LLaMA-3.1 8B & 7e-6 & 0 & 48 & 1 & 12 & 19,22,20,21,27,23,30,29,28,26,24,25 \\
& Mistral 7B & 1e-6 & 0 & 48 & 1 & 12 & 17,32,31,21,22,24,29,23,28,25,27,26 \\
& Gemma-2 9B & 1e-6 & 0 & 48 & 1 & 15 & 29,26,21,30,27,40,38,31,32,33,35,39,36,37,34 \\
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

\section{More Implementation Details for Reproducibility}

\textbf{Post-Training Datasets.}
Regarding the medical domain, we directly employ the instruction tuning dataset for PMC\footnote{https://huggingface.co/datasets/axiong/pmc\_llama\_instructions} \citep{PMC} (i.e., training stage-2 for PMC-LLaMA) for post-training. The dataset consists of 513,999 instruction-input-output pairs.
Regarding the general instruction tuning scenario, we employ the Tulu 3 SFT Mixture\footnote{https://huggingface.co/datasets/allenai/tulu-3-sft-mixture} and filter for data with only 1 round of conversation, resulting in 896,070 input-output pairs.

\textbf{Training Hyperparameters.}
The training hyperparameters for all experiments are reported in \cref{tab:hyperparam_uniattn}, \cref{tab:hyperparam_cla}, and \cref{tab:hyperparam_llmdrop}.
Note that we adopt the learning rate and weight decay values according to existing research, in which those pre-trained models are post-trained under reported schedules.

\textbf{Evaluation Details.}
For all experiments, we evaluate the final checkpoint after post-training for benchmark evaluation. 
Regarding the medical domain, we report the 0-shot result on both PubMed, MegMCQA, and MedQA datasets. Regarding the general instruction tuning scenario, we report 0-shot result on both SIQA and CommonsenseQA for LLaMA-3.1 8B, Mistral 7B, and Gemma-2 9B models. For the earlier-released LLaMA-2 7B model, since it has a shorter pre-training context length, we report its 5-shot result as Tulu 3 mainly enhances model performance with longer context.

\textbf{Prompt for Post-Training.}
We employ the same prompt for post-training in both the medical domain and general instruction tuning scenario:
\begin{verbatim}
    "Below is an instruction that describes a task, paired with an input that
    provides further context. Write a response that appropriately completes the 
    request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input}\n\n### 
    Response:{output}<EOS_TOKEN>"
\end{verbatim}
We simply keep the instruction field empty if no instruction is provided.


\section{More Experiment Results}

\begin{table*}[t]
    \centering
    \caption{Post-training performance comparison. \textbf{Bold} and \underline{underline} denote the best and second-best performance of compressed models. For each method, we report their time to first token (TTFT, in seconds) and KV-cache retain rate (KV Cache).}
    \vskip 0.15in
    \label{tab:main_appendix}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|cc|ccc|c|cc|c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\makecell{TTFT \\ (s)}} & \multirow{2}{*}{\makecell{KV \\ Cache}} & \multicolumn{4}{c|}{Medical} &  \multicolumn{3}{c}{General}    \\ \cline{4-7} \cline{8-10} 
&  & & PubMedQa   & MedMCQA  & \multicolumn{1}{c}{MedQA} & AVG & SIQA  & \multicolumn{1}{c}{CommonsenseQA} & AVG  \\ \midrule
\multicolumn{10}{c}{\textbf{Mistral-7B (w/ GQA)}}  \\ 
\midrule
Pre-Trained & 1.94 & 100\%  & 75.8   & 48.3  & 50.9   & 58.3 & 46.7 & 56.5  & 51.6 \\
Post-Train & 1.94 & 100\%  & 78.8   & 49.7  & 60.3   & 62.9 & 53.7 & 76.6  & 65.2 \\ \hline
LLMDrop-Half & 1.54 & 81.3\%  & 78.0   & 50.6  & \underline{59.0}   & 62.5 & \underline{52.5} & 74.5  & 63.5 \\ 
LLMDrop-Full & 1.21 & 62.5\%  & \textbf{79.0}   & 50.2  & 56.7   & 62.0 & 51.8 & 73.6  & 62.7 \\ 
CLA-Half & 1.94 & 81.3\%  & 78.6   & \underline{56.0}  & 58.5   & \underline{64.4} & 52.0 & \textbf{77.9}  & \underline{65.0} \\ 
CLA-Full & 1.91 & 62.5\%  & \underline{78.8}   & 55.0  & 55.0   & 62.9 & 50.5 & 75.1  & 62.8 \\ 
% Softmax & 100\% & 100\%  & 0.728   & 0.537  & 0.560   & 0.608 & & - & -  & - \\ 
UniAttn & 1.23 & 81.3\%  & 78.2   & \textbf{57.3}  & \textbf{60.5}   & \textbf{65.3} & \textbf{53.4} & \underline{77.5}  & \textbf{65.5} \\ 
\midrule
\multicolumn{10}{c}{\textbf{Gemma2-9B (w/ GQA)}}  \\ 
\midrule 
Pre-Trained & 2.23 & 100\%  & 78.6   & 57.6  & 60.6   & 65.6 & 51.5 & 68.4  & 60.0 \\
Post-Train & 2.23 & 100\%  & 78.6   & 58.6  & 61.9   & 66.4 & 54.4 & 75.6  & 65.0 \\ \hline
LLMDrop-Half   & 1.94 & 81.0\%  & 78.6   & 56.4  & \underline{60.5}   & \underline{65.2} & \textbf{55.6} & 68.4  & 62.0 \\ 
LLMDrop-Full & 1.66 & 64.3\%  & 78.2   & 54.8  & 58.7   & 63.9 & \underline{54.8} & 62.9  & 58.9 \\ 
CLA-Half & 2.19 & 78.6\%  & \textbf{79.2}   & \underline{56.6}  & 56.3   & 64.0 & 53.5 & \textbf{76.0}  & \textbf{64.8} \\ 
CLA-Full & 2.24 & 64.3\%  & 74.0   & 39.3  & 38.5   & 50.6 & 48.3 & 51.8  & 43.4 \\ 
UniAttn & 1.69 & 82.1\%  & \underline{79.0}   & \textbf{60.7}  & \textbf{64.3}   & \textbf{68.0} & 53.7 & \underline{74.4}  & \underline{64.1} \\ 
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

\begin{table}[t]
    \centering
    \caption{Performance comparison (\%) of different compensation designs.}
    \vskip 0.15in
    \label{tab:compensation_design}
    \resizebox{0.5\columnwidth}{!}{\begin{tabular}{cc|c}
\toprule
Input to $W_c$ & Output adds to & AVG \\
\midrule
 MHA input & MHA output & \textbf{65.4} \\
 MHA input & FFN output & 64.1 \\  % postffn
 MHA input & Activation before $W_o$ & \underline{65.1} \\ % preproj
 Activation projected by $W_v$ & Activation before $W_o$ & 64.3 \\ % perhead
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table}

\begin{figure}[t]  % figure* for spanning across both columns
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figs/Number_of_Pruned_MHA_layers.pdf}}
    \caption{Results on further dropping UniAttn layers (\%). We present the average accuracy and time to first token (TTFT) latency under different number of dropped MHA blocks via LLMDrop. Note that we only drop MHA blocks with unified Softmax activations. }
    \label{fig:llmdrop}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table*}[t]
    \centering
    \caption{Performance (\%) comparison with similarity-based Superblock grouping methods.}
    \vskip 0.15in
    \label{tab:similarity_group}
    \resizebox{0.95\textwidth}{!}{\begin{tabular}{c|cc|cccc}
\toprule
Method & Similarity Based Grouping & \# of Operating Layers & PubMedQA & MedMCQA & MedQA & AVG \\
\midrule
\multirow{2}{*}{UniAttn} & $\times$ & 3 & 78.2 & 59.2 & 62.1 & 66.5 \\
 & $\surd$ & 3 &  77.6& 57.3 & 57.8 & 64.2  \\ \midrule
\multirow{2}{*}{UniAttn} & $\times$ & 6 & 77.2 & 58.1 & 62.9 & 66.1 \\
 & $\surd$ & 6 & 72.0 & 55.6 & 58.8 & 62.1 \\ \midrule
\multirow{2}{*}{UniAttn} & $\times$ & 9 & 77.4 & 56.5 & 60.4 & 64.8 \\
 & $\surd$ & 9 & 77.4 & 54.2 & 56.0 & 62.5 \\ \midrule
\multirow{2}{*}{UniAttn} & $\times$ & 12 & 79.0 & 57.6 & 59.5 & 65.4 \\
 & $\surd$ & 12 & 77.0 & 55.9 & 54.8 & 62.6 \\ 
\bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}



\textbf{Main Results on Mistral 7B and Gemma-2 9B.}
We adopt the settings in \cref{tab:main} and conduct the same experiments on Mistral 7B \citep{Mistral} and Gemma-2 9B \citep{Gemma2}. The results are shown in \cref{tab:main_appendix}.
Consistent with the results on the LLaMA models, our UniAttn achieves the best overall performance on both LLMs and post-training datasets while significantly reducing both time and memory costs. When operating the same number of layers, UniAttn achieves similar TTFT time to LLMDrop, well demonstrating the effectiveness of Softmax activation unification. Although CLA and LLMDrop achieve competitive results in some settings (e.g., CLA-Half in post-training Gemma on general instruction datasets), \textit{they cannot provide consistent performance across different settings}. 
Additionally, even on the Mistral and Gemma models, CLA achieves similar performance to its corresponding LLMDrop setting, again indicating that CLA diminishes the depth factor in pre-trained LLMs.

\textbf{Different Compensation Designs.} 
To compare different compensation designs, we attach the input and output of the $W_c$ transformation to different activations. As shown in \cref{tab:compensation_design}, directly compensating the MHA output with its input yields the best results. When adopting ``finer compensation granularities'', i.e., positioning the input and output closer together (Rows 3 and 4), post-training performance decreases. Similarly, adopting ``coarser compensation granularities'', i.e., moving the output compensation to after the FFN (Row 2), also results in lower performance. These findings suggest that using a linear transformation to compensate for errors specifically within the MHA module is the optimal approach. This aligns with our analysis in \cref{sec:uniattn}, which shows that the compensation error within the MHA module is significantly smaller in magnitude than its expectation.


\textbf{Experimental Analysis on the Impact of Model Depth.} 
In \cref{sec:discussion}, we demonstrated that, unlike cross-layer KV sharing methods, UniAttn does not diminish the depth factor in pre-trained LLMs. To provide experimental evidence for this, we further apply LLMDrop to the MHA modules that utilize unified Softmax activations from preceding layers (a total of 12 layers in UniAttn for the LLaMA-3.1 8B model) in post-trained UniAttn models.
Specifically, we compute the input-output similarities of these MHA modules and simply prune the top $k$ MHA modules with the highest similarities. The pruned model is then directly evaluated without additional fine-tuning. The average performance w.r.t. $k$ in shown in \cref{fig:llmdrop}.
As shown in the figure, pruning a few MHA modules from our UniAttn model still results in higher performance than the CLA-Full model variant.
After pruning 10 MHA modules, UniAttn+LLMDrop achieves an average performance of 63.8, matching the performance of CLA-Full reported in \cref{tab:main}. This demonstrates that UniAttn better preserves the impact of model depth compared to CLA. Furthermore, we can also conclude that the depth of the model significantly contributes to the post-train performance, thereby validating our theoretical analysis.

\textbf{Similarity-based Superblock Grouping.} 
We employed a simple fixed grouping method for Superblock construction, grouping every 4 layers in the \textit{top half layers} as a Superblock. To investigate the impact of different Superblock grouping strategies, we experimented with a similarity-based grouping scheme. In this scheme, layers with the most similar Softmax activations, as measured by cosine similarity (\cref{fig:softmax_sim}), are grouped together, and a linear compensation matrix is added to layers utilizing activations from other layers.
As shown in \cref{tab:similarity_group}, the similarity-based grouping consistently underperforms the fixed-size Superblock grouping (default implementation) across all settings. This suggests that the simple fixed-size Superblock approach is sufficient to achieve strong post-training performance.

\section{Pipeline for Applying UniAttn during Post-Training}

We provide a detailed pipeline for applying UniAttn as \cref{alg:example}.

\begin{algorithm}[h]
   \caption{UniAttn Pipeline}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} Pre-trained Model $M$, Post-train dataset $D$, SuperBlock size $b$, Apply SuperBlock merging from layer $i_s$ to $i_e$
   \STATE Create UniAttn model $M_{\text{uni}}$ by creating $\frac{i_e-i_s+1}{b}$ SuperBlocks in $M$
   \STATE Sample a subset $D_{\text{init}}$ with 1000 samples from $D$
   \STATE For model $M$, forward all samples from $D_{\text{init}}$, calculate the average output activations $\mathbf{x}_i'$ for each MHA block.
   \FOR{$i=1$ {\bfseries to} $\frac{i_e-i_s+1}{b}$}
   \FOR{$j=1$ {\bfseries to} $b-1$}
   \STATE For model $M_{\text{uni}}$, forward all samples from $D_{\text{init}}$, calculate the average output activations $\mathbf{x}_{\text{uni,}i_s+(i-1)b+j}'$ and the average input activations $\mathbf{x}_{\text{uni,}i_s+(i-1)b+j}$ for MHA block in layer $i_s+(i-1)b+j$.
   \STATE Calculate the optimal initialization for the $W_c$ matrix in layer $i_s+(i-1)b+j$ according to \cref{theorem:init}.
   \STATE Insert the initialized $W_c$ matrix to layer $i_s+(i-1)b+j$ in model $M_{\text{uni}}$.
   \ENDFOR
   \ENDFOR
   \STATE Freeze all training parameters other than the $W_c$ matrices in $M_{\text{uni}}$. Train $W_c$ with dataset $D$ and apply early stop when loss stops to decrease (we consider the exponential moving average of training loss).
   \STATE Conduct full fine-tune on $M_{\text{uni}}$ with $D$.
   \STATE {\bfseries Output:} Trained UniAttn model $M_{\text{uni}}$.
\end{algorithmic}
\end{algorithm}

\section{Proof for \cref{lem:linear}}
\label{sec:append_proof}
\begin{proof} 
Unroll the expression of $\mathbf{x}_L$ and inductively:
\begin{equation}
    \mathbf{x}_L=\mathbf{x}_0+\sum_{k=1}^{L}F_k(\text{Norm}(\mathbf{x}_{k-1}))
\end{equation}
As $||\text{Norm}(\mathbf{x}_k)||=1$ for all $k$, then:
\begin{equation}
    \begin{aligned}
        ||\mathbf{x}_L|| &= ||\mathbf{x}_0+\sum_{k=1}^{L}F_k(\text{Norm}(\mathbf{x}_{k-1}))|| \\
        &\le ||\mathbf{x}_0||+\sum_{k=1}^{L}||F_k(\text{Norm}(\mathbf{x}_{k-1}))|| \\
        &\le ||\mathbf{x}_0||+\sum_{k=1}^{L}||\lambda\cdot\text{Norm}(\mathbf{x}_{k-1}))|| \\
        &= ||\mathbf{x}_0||+\lambda L
    \end{aligned}
\end{equation}
\end{proof}

\section{Insights for \cref{assum:inftesimal}}
With \cref{lem:linear} and \cref{prop:linear}, we have shown that pre-trained Pre-Norm LLMs can be approximated as linear Pre-Norm systems. Suppose a pre-trained LLM operates on activations $\mathbf{x}\in \mathbb{R}^{d}$, each layer $i$ in the pre-trained LLM can be treated as a linear transformation matrix $W_i\in \mathbb{R}^{d\times d}$. A generic gradient-based update can be expressed as:
\begin{equation}
    W_i^{t+1} = W_i^0 - \sum_{t=0}^{t}\eta\nabla_{W_i}\mathcal{L}^{t}
\end{equation}
Hence, the norm between initial and final weights satisfies:
\begin{equation}
    ||W_i^{t+1}-W_i^0|| \le \sum_{t=0}^{t}||\eta\nabla_{W_i}\mathcal{L}^{t}||
\end{equation}
We denote the singular value decomposition of $W_i^{t+1}-W_i^0$ as $W_i^{t+1}-W_i^0=U\Sigma V^T$, it is easy to derive that:
\begin{equation}
    ||W_i^{t+1}-W_i^0||=\sqrt{tr[(||W_i^{t+1}-W_i^0||)(||W_i^{t+1}-W_i^0||)^T]}=\sqrt{tr(U\Sigma V^TV\Sigma^TU^T)}=\sqrt{tr(\Sigma\Sigma^T)}=\sqrt{\sum_{i=1}^{d}\sigma_i^2}
\end{equation}
where $\sigma_i$ denote the $i$-th singular value in $\Sigma$. Normally, the largest singular value $\sigma_{\max}$ dominates in the quadratic term, thus we can further write:
\begin{equation}
    ||W_i^{t+1}-W_i^0||=\sqrt{\sum_{i=1}^{d}\sigma_i^2}\approx \sigma_{\max}(W_i^{t+1}-W_i^0)
\end{equation}
Based on the basic features of singular values, we can show that:
\begin{equation}
    \sigma_{\max} (W_i^{t+1})\le\sigma_{\max}(W_i^0)+\sigma_{\max} (W_i^{t+1}-W_i^0)\le\sum_{t=0}^{t}||\eta\nabla_{W_i}\mathcal{L}^{t}||+\sigma_{\max} (W_i^0)
\end{equation}
Due to having smaller values of $||\eta\nabla_{W_i}\mathcal{L}^{t}||$ (i.e., smaller gradient norm), compared to earlier layers, the top layers typically have smaller updates on the largest singular value, thus tend to have smaller $\sigma_{\max}$ values. This further leads to \cref{assum:inftesimal} we propose in the article.


\section{Proof for \cref{theorem:init}}
\begin{proof}
Solving for the optimal initialization of $W_c$ is equivalent to solving the following optimization problem:
\begin{equation}
    \min_{W_c}\ ||\mathbb{E}(\mathbf{x}_{i+b}W_c-\epsilon)||^2
\end{equation}
Since $W_c$ is the fixed weight of the linear transformation applied to input, $W_c$ is independent of $\mathbf{x}_{i+b}$. Therefore, we can re-write the optimization problem as:
\begin{equation}
    \min_{W_c}\ ||\mathbb{E}(\mathbf{x}_{i+b})W_c-\mathbb{E}(\epsilon)||^2
\end{equation}
The solution to the above system is given by $W_c=\mathbb{E}(\mathbf{x}_{i+b})^+\mathbb{E}(\epsilon)$, where $\mathbb{E}(\mathbf{x}_{i+b})^+$ denotes the Moore–Penrose inverse of $\mathbb{E}(\mathbf{x}_{i+b})$. By plugging in the closed-form expression of $\mathbb{E}(\mathbf{x}_{i+b})^+$, we achieve:
\begin{equation}
    W_{c}=V\Sigma^{+}U^T\mathbb{E}(\epsilon),
\end{equation}
where $\mathbb{E}(\mathbf{x}_{i+b})=U\Sigma V^T$ denotes the SVD decomposition of $\mathbb{E}(\mathbf{x}_{i+b})$.
\end{proof}

\section{More Insights on Linear Compensation}
To demonstrate the effectiveness of our linear compensation strategy, we propose the following theorem:
\begin{theorem}
\label{theorem:error_expect}
    $A,B\in\mathbb{R}^{m\times n}$, $X\in \mathbb{R}^{n\times n}$. Suppose that each element from $A$ and $B$ are drawn from a Gaussian distribution such that $a_{ij},b_{ij}\sim N(0,1)$. It satisfies that:
    \begin{equation}
        \mathbb{E}\bigl[\min_X ||AX-B||\bigr]=
            \begin{cases} 
                \sqrt{n(m-n)}, & \text{if } m \ge n,\\
                0,        & \text{if } m < n.
            \end{cases}
    \end{equation}
\end{theorem}
See the following subsections for the proof. In the article, we apply the F15K dataset \citep{rerope2023} to calculate the error $\epsilon$ on LLaMA-2 7B \citep{touvron2023llama} and LLaMA-3.1 8B \citep{LLaMA3} models after inserting the initialized linear transformation $W_c$ according to \cref{theorem:init}. While the sequence length being $m=5120$ and the hidden size of applied models being $n=4096$, the error is 11.09 and 5.76 after linear compensation, correspondingly, which are both \textit{magnitudes lower} than their expectations on random data. Those results demonstrate the effectiveness of our linear compensation strategy.

We give the proof for \cref{theorem:error_expect} in the following subsections.
\subsection{Preliminaries}
To prove \cref{theorem:error_expect}, we adopt the notations in \cref{theorem:error_expect} and prove a series of lemmas first.
\begin{lemma}
\label{lemma:P}
Let $P=AA^+$, $P$ is an orthogonal projection and $\text{Im}(P)=\text{Col}(A)$.
\end{lemma}
\begin{proof}
The Moore-Penrose inverse matrix exhibits some basic features, namely $AA^+A=A$ and $(AA^+)^T=AA^+$. Using that feature we can easily verify that:
\begin{equation}
    P^2=AA^+AA^+=(AA^+A)A^+=AA^+=P
\end{equation}
\begin{equation}
    P^T=(AA^+)^T=AA^+=P
\end{equation}
So $P$ is an orthogonal projection.

Next, we prove $\text{Im}(P)=\text{Col}(A)$. For any vector $\mathbf{v}\in \text{Col}(A)$, there exists a $\mathbf{c}\in \mathbb{R}^n$ that $\mathbf{v}=A\mathbf{c}$. We have:
\begin{equation}
    P\mathbf{v}=AA^+A\mathbf{c}=(AA^+A)\mathbf{c}=A\mathbf{c}=\mathbf{v}
\end{equation}
So $\mathbf{v}\in\text{Im}(P)$. Therefore, $\text{Col}(A)\subseteq\text{Im}(P)$ Since $P=AA^+$, it is obvious that $\text{Im}(P)\subseteq\text{Col}(A)$. Hence, we can conclude that $\text{Im}(P)=\text{Col}(A)$.
\end{proof}

\begin{lemma}
\label{lemma:i_p}
$I-P$ is an orthogonal projection and $\text{Im}(I-P)=\text{Col}(A)^{\perp}$.
\end{lemma}
\begin{proof}
First, we prove that $I-P$ is an orthogonal projection:
\begin{equation}
(I-P)^2=I-2P+P^2=I-2P+P=I-P
\end{equation}
\begin{equation}
(I-P)^T=I-P^T=I-P
\end{equation}
Then, we prove that $\text{Im}(I-P)=\text{Col}(A)^{\perp}$. For any $\mathbf{v}\in \text{Im}(I-P)$, there exists a $\mathbf{c} \in \mathbb{R}^m$ that $\mathbf{v}=(I-P)\mathbf{c}$. Let $\mathbf{d}\in \text{Col}(A)$, then there exists a $\mathbf{w} \in \mathbb{R}^n$ that $\mathbf{d}=A\mathbf{w}$. Computing the inner product of $\mathbf{v}$ and $\mathbf{d}$ yields:
\begin{equation}
    \langle\mathbf{v},\mathbf{d}\rangle=\langle(I-P)\mathbf{c},A\mathbf{w}\rangle=\langle\mathbf{c},(I-P)A\mathbf{w}\rangle=\langle\mathbf{c},(A-AA^+A)\mathbf{w}\rangle=\langle\mathbf{c},0\rangle=0
\end{equation}
This shows that $\mathbf{v}\in\text{Col}(A)^{\perp}$, and in turn $\text{Im}(I-P)\subseteq\text{Col}(A)^{\perp}$.

Conversely, suppose $\mathbf{u}\in\text{Col}(A)^{\perp}$. Recall \cref{lemma:P} that $P$ is an orthogonal projection and $\text{Im}(P)=\text{Col}(A)$, we have $P\mathbf{u}=0$. Therefore,
\begin{equation}
    (I-P)\mathbf{u}=\mathbf{u}-P\mathbf{u}=\mathbf{u}
\end{equation}
This shows that $\mathbf{u}\in\text{Im}(I-P)$, and in turn $\text{Col}(A)^{\perp}\subseteq\text{Im}(I-P)$. Therefore, $\text{Im}(I-P)=\text{Col}(A)^{\perp}$.
\end{proof}

From \cref{lemma:i_p}, we can immediately conclude that:
\begin{corollary}
\label{corollary:rank}
$\text{rank}(I-P)=m-\min (m-n)$
\end{corollary}
\begin{proof}
    $\text{rank}(I-P)=\dim \text{Im}(I-P)=\dim \text{Col}(A)^{\perp}=m-\text{rank}(A)$. Recall \cref{theorem:error_expect} that every element from $A$ is sampled from a Gaussian distribution, so that by probability of 1 we have $\text{rank}(A)=\min(m,n)$, which leads to the conclusion of this corollary.
\end{proof}

Lastly, we give a lemma on orthogonal projections to a Gaussian-sampled vector:
\begin{lemma}
\label{lemma:gaussian}
    If $\mathbf{b}\sim N(\mathbf{0},I_{m\times m})$ is a standard Gaussian in $\mathbb{R}^m$ and $Q$ is an orthogonal projector of rank $k$, then
\begin{equation}
    \mathbb{E}\bigl[||Q\mathbf{b}||\bigr]=\sqrt{k}
\end{equation}
\end{lemma}
\begin{proof}
$\mathbb{E}\bigl[||Q\mathbf{b}||^2\bigr]=\mathbb{E}\bigl[\mathbf{b}^TQ^TQ\mathbf{b}\bigr]=\mathbb{E}\bigl[\mathbf{b}^TQ\mathbf{b}\bigr]=\mathbb{E}\bigl[\text{tr}(\mathbf{b}^TQ\mathbf{b})\bigr]=\mathbb{E}\bigl[\text{tr}(Q\mathbf{b}\mathbf{b}^T)\bigr]=\text{tr}(Q\mathbb{E}\bigl[\mathbf{b}\mathbf{b}^T\bigr])$. As $\mathbf{b}\sim N(\mathbf{0},I_{m\times m})$, $\mathbb{E}\bigl[\mathbf{b}\mathbf{b}^T\bigr]=I$. Therefore, using the trace property of projectors:
\begin{equation}
    \text{tr}(Q\mathbb{E}\bigl[\mathbf{b}\mathbf{b}^T\bigr])=\text{tr}(Q)=k
\end{equation}
In turn, we can conclude that $\mathbb{E}\bigl[||Q\mathbf{b}||\bigr]=\sqrt{k}$.
\end{proof}

\subsection{Proof}
Finally, we give the proof for \cref{theorem:error_expect}.
\begin{proof}
According to \cref{theorem:init}, for given $A$ and $B$, the optimal solution $X^*$ that satisfies $||AX^*-B||=\min_X ||AX-B||$ is given by:
\begin{equation}
    X^*=A^+B,
\end{equation}
where $A^+$ is the Moore-Penrose inverse of $A$. We can re-write the objective as:
\begin{equation}
    \mathbb{E}(||AA^+B-B||^2)=\mathbb{E}(||(I-AA^+)(-B)||^2)=\mathbb{E}(||(I-AA^+)B||^2)=\sum_{i=1}^n\mathbb{E}(||(I-AA^+)\mathbf{b}_i||^2)
\end{equation}
According to \cref{lemma:i_p} and \cref{corollary:rank}, $I-AA^+$ is an orthogonal projection and $\text{rank}(I-AA^+)=m-\min (m-n)$. With \cref{lemma:gaussian}, we know that for any $i$, $\mathbb{E}(||(I-AA^+)\mathbf{b}_i||^2)=m-\min (m-n)$. Therefore, we can conclude that:
\begin{equation}
    \mathbb{E}(||AA^+B-B||^2)=
        \begin{cases} 
            n(m-n), & \text{if } m \ge n,\\
            0,        & \text{if } m < n.
        \end{cases}
\end{equation}
which leads to the conclusion in \cref{theorem:error_expect}.
\end{proof}

\end{document}
