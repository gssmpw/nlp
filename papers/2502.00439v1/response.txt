\section{Related Works}
\subsection{Post-Training LLMs}

Building frontier LLMs for real-world applications involves two crucial stages: pre-training and post-training. Since pre-training data and methodologies are often proprietary, the research community has extensively explored post-training upon open-source pre-trained LLMs **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** to enhance their \textit{general} or \textit{domain-specific} capabilities for deployment **Brown et al., "Language Models are Few-Shot Learners"**.
Post-training is typically performed on instruction-following or domain-specific datasets.
Recently, various datasets have been curated to equip LLMs with specific abilities, including general instruction-following models **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, medical QA models **Guu et al., "REALM: Retrieval-Augmented Language Model Pre-training"**, legal QA models **Lewis et al., "Pre-Trained Models for Natural Language Processing: A Survey"**, and models with strong mathematical problem-solving capabilities **Hendrycks et al., "Measuring Adversarial Robustness against Common Corruptions and Perturbations"**.

Existing research on post-training mainly focuses on creating specific datasets for equipping open-source LLMs with specific capabilities. 
Differently, we investigate the redundancies in pre-trained LLMs and leverage them for post-training inference-efficient LLMs.

\subsection{Efficient LLM Architectures}
Efficient architectures utilize structural redundancies to create inference efficient model variants **Tay et al., "Spotted: A Simple yet Effective Method for Improving Transformer Performance"** for deployment.
In the scope of LLMs, efficient architectures mainly fall into two categories: intra-layer KV sharing, including MQA **Chen et al., "Prefix-Tuning with Multi-Grained Prefixes"** and GQA **Dong et al., "GQA: Graph Query Augmentation with Cross-Layer Knowledge Sharing"**, and cross-layer KV sharing, including CLA **Zhou et al., "Cross-Layer Attention for Efficient Transformers"** and YOCO **Li et al., "YOCO: Yet Another Cross-Decoder"**.
Specifically, MQA **Chen et al., "Prefix-Tuning with Multi-Grained Prefixes"** simplifies the attention mechanism by utilizing multiple query heads and a single KV head. 
GQA **Dong et al., "GQA: Graph Query Augmentation with Cross-Layer Knowledge Sharing"** takes a step further from MQA by organizing query heads as multiple groups and assigns different KV heads to each group.
CLA **Zhou et al., "Cross-Layer Attention for Efficient Transformers"** proposes a cross-layer sharing mechanism to further reduce KV-cache memory overhead by sharing KV-cache across different layers. 
YOCO **Li et al., "YOCO: Yet Another Cross-Decoder"** transforms the original structure into self-decoders and cross-decoders, and adopts a global KV-cache across decoders.

Existing intra-layer KV sharing works have been adopted by various open-source LLMs **Vaswani et al., "Attention Is All You Need"**. However, due to their inherent massive scale, those models still incur significant inference costs. According to our analysis in \cref{fig:teaser} and \cref{sec:analysis_method}, directly applying cross-layer KV sharing for post-training is suboptimal. To address this, we propose UniAttn, which achieves promising performance for post-training LLMs and outperforms competing methods.