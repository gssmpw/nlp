\section{Related Works}
\subsection{Post-Training LLMs}

Building frontier LLMs for real-world applications involves two crucial stages: pre-training and post-training. Since pre-training data and methodologies are often proprietary, the research community has extensively explored post-training upon open-source pre-trained LLMs \citep{LLaMA3,Gemma2,Mistral} to enhance their \textit{general} or \textit{domain-specific} capabilities for deployment \citep{lian2024breakingstagebarriernovel}.
Post-training is typically performed on instruction-following or domain-specific datasets.
Recently, various datasets have been curated to equip LLMs with specific abilities, including general instruction-following models \citep{Hermes, TULU3, InfinityInstruct2024}, medical QA models \citep{PMC, Aloe, singhal2023expertlevelmedicalquestionanswering}, legal QA models \citep{LawyerLLaMA, LawGPT}, and models with strong mathematical problem-solving capabilities \citep{Goat}.

Existing research on post-training mainly focuses on creating specific datasets for equipping open-source LLMs with specific capabilities. 
Differently, we investigate the redundancies in pre-trained LLMs and leverage them for post-training inference-efficient LLMs.

\subsection{Efficient LLM Architectures}
Efficient architectures utilize structural redundancies to create inference efficient model variants \citep{ToMe,PYRA} for deployment.
In the scope of LLMs, efficient architectures mainly fall into two categories: intra-layer KV sharing, including MQA \citep{MQA} and GQA \citep{GQA}, and cross-layer KV sharing, including CLA \citep{CLA} and YOCO \citep{YOCO}.
Specifically, MQA \citep{MQA} simplifies the attention mechanism by utilizing multiple query heads and a single KV head. 
GQA \citep{GQA} takes a step further from MQA by organizing query heads as multiple groups and assigns different KV heads to each group.
CLA \citep{CLA} proposes a cross-layer sharing mechanism to further reduce KV-cache memory overhead by sharing KV-cache across different layers. 
YOCO \citep{YOCO} transforms the original structure into self-decoders and cross-decoders, and adopts a global KV-cache across decoders.

Existing intra-layer KV sharing works have been adopted by various open-source LLMs \citep{LLaMA3,Mistral,Gemma2}. However, due to their inherent massive scale, those models still incur significant inference costs. According to our analysis in \cref{fig:teaser} and \cref{sec:analysis_method}, directly applying cross-layer KV sharing for post-training is suboptimal. To address this, we propose UniAttn, which achieves promising performance for post-training LLMs and outperforms competing methods.