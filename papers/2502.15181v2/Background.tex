\section{Preliminaries}
\label{sec:prelim}

In this section, we first discuss the challenges and prior efforts in solving the join ordering problem. We then describe the \YannAlg and \PT in detail.

% We start this section by discussing the challenges of join ordering in query optimization. We then discuss prior work on robust query processing, which is a promising solution to the join ordering challenge. Finally, we provide a brief overview of Predicate Transfer.

\begin{figure*}[t!]
  \begin{subfigure}[t]{.25\linewidth}%
    \center
    \includegraphics[width=\linewidth]{pic/join-graph.pdf}
    \caption{Join Graph}
    \label{fig:yannakakis-join-graph}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.42\linewidth}%
    \center
    \includegraphics[width=\linewidth]{pic/semi-join-phase.pdf}
    \caption{Semi-Join Phase}
    \label{fig:yannakakis-semi-join}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.28\linewidth}%
    \center
    \includegraphics[width=\linewidth]{pic/join-phase.pdf}
    \caption{Join Phase}
    \label{fig:yannakakis-join}
  \end{subfigure}
  \caption{
    \textbf{\YannAlg on JOB 3a.}
  }
  \label{fig:yannakakis}
\end{figure*}


\subsection{Join Order Optimization}
\label{sec:prelim:jo}

Optimizing join orders is one of the most important tasks in query optimization. A bad join order leads to large intermediate results and can be orders of magnitude slower than the optimal plan~\cite{selinger1979,2019tutorial_robust,leis2015HowGood}. Obtaining an optimal join order in a modern query optimizer requires accurate cardinality estimation and efficient plan enumeration. Both remain difficult after over 40 years of research.

Cardinality estimation (CE) predicts the number of tuples produced by each operator in a query plan. Obtaining accurate estimations of the join cardinalities is extremely difficult. Without detailed statistics, the query optimizer typically makes the following assumptions: (1) Uniformity: values in a column are uniformly distributed within the global min/max; (2) Independence: values in different columns are uncorrelated; (3) Inclusion: every value from the probe side of the join must appear in the build side. These assumptions are rarely valid in real-world applications. Although having further statistics (e.g., histograms on joint distributions) can improve the accuracy of join cardinality estimation, it is prohibitively expensive to maintain comprehensive cross-column statistics. Even worse, studies show that a small estimation error will propagate exponentially with respect to the number of joins~\cite{1991errorpropagation,robustoptimization}. Leis et al.~\cite{leis2015HowGood} reported that none of the optimizers in real-world DBMSs (including the commercial ones) can estimate join cardinalities accurately: most of them under-estimate by 2-4 orders of magnitude when the number of joins $\ge 5$. Recent proposals tackle the CE problem using machine learning and deep learning techniques~\cite{2019MSCN, 2019Naru, 2019light_ML, halford2019bayesian, 2020deepdb, zhu2020flat, 2020deep, park2020quicksel, shetiya2020astrid, yang2020neurocard, liu2021fauce, wu2021unified}, but none of them so far has shown evidence of robust estimation.
% \todo{cite MSCN, NeuroCard, DeepDB, and more. Check out this survey: https://dl.acm.org/doi/pdf/10.1145/3514221.3526154}

Plan enumeration refers to the process of searching equivalent join orders and finding the query plan that has the minimal cost. Plan enumeration has been proven to be NP-hard~\cite{1984NP}. Prior work developed efficient algorithms based on dynamic programming~\cite{moerkotte2006DPccp, 2008dphyper}, and they are sufficient when the number of joins is small (e.g., < 10). However, because the search space grows exponentially with respect to the number of joins, it becomes impractical to perform an exhaustive search for a query with a large number of joins. Optimizers must fall back to heuristics-based approaches (e.g., the genetic algorithm in PostgreSQL~\cite{Postgres} and the greedy algorithm in DuckDB~\cite{duckdb}), sacrificing plan optimality for a reasonable optimization complexity.


% Query optimization is an important part of modern relational database management systems. A primary task of query optimization is to explore the space of possible join orders and select the one with the shortest execution time. It is known that a bad join order can be orders of magnitude slower than a good join order~\cite{2019tutorial_robust}, caused by the orders of magnitude larger intermediate results~\cite{selinger1979}.
% % \yxy{I feel this argument is not convincing enough. Join ordering is \textbf{important} because a bad order can produce orders of magnitude larger intermediate tables, which destroys performance. Join ordering is \textbf{hard} because an average-case join order is close to worst-case}.
% Hence, finding a near-optimal join order is extraordinarily important.
% % \yxy{the point of the last sentence is a little clear.}

% % \yxy{In a standard query optimizer, identifying a good join order takes three steps: ...}
% In a standard query optimizer, identifying a good join order takes two steps~\cite{2021survey_optimizer}:
% \begin{enumerate}
%     \item \textbf{Accurate cardinality estimation}~\cite{2014optimizationsolved?}. Cardinality estimation estimates the number of tuples generated by an operator and is used to calculate the cost of that operator.
%     % \item \textbf{Precise cost modeling}~\cite{2020cost_model}. The cost model is used to estimate of cost for a (sub)query. Typically, the cost of (sub)query plan is the sum of costs of all operators in the plan.
%     \item \textbf{Efficient plan enumeration}~\cite{2008dphyper}. Plan enumeration algorithm is used to find the optimal join order from the space of equivalent join orders such that the query cost is minimized.
% \end{enumerate}

% % \yxy{Explain what each step is before talking about why it is challenging. One way to do this is to expand the paragraph above to bullet points. Each bullet elaborates what the step is doing.}
% Unfortunately, all the three steps above remain difficult even after decades of research, as illustrated below: 

% Accurate cardinality estimation is extremely difficult, especially after joining multiple tables~\cite{robustoptimization}. This is because it is impractical to maintain complete cross-column statistics across all the tables, and the estimator has to make (sometimes inaccurate) assumptions about the correlation between columns in these cases. As reported by Leis et al.~\cite{leis2015HowGood}, even commercial optimizers could make bad mistakes on the join cardinality estimation and the estimation error grows with the increasing join size. Furthermore, it has been shown that, even if estimation errors on the basic information are small, their transitive effect on estimates of the derived information can be devastating (e.g., the error propagates exponentially with respect to the number of joins)~\cite{1991errorpropagation}. And although there are lots of prior research on sophisticated estimation techniques, such as wavelet histograms~\cite{1998wavelet}, selfâ€tuning histograms~\cite{1999self}, and deep learning~\cite{2020deep}, they are still far away from the ideal situation~\cite{2019tutorial_robust}.

% \yxy{The paragraph below is not very convincing. Intuitively, I would think the reasons are because the enormous space of operator types, different implementations of operators, the underlying hardware types, data size and distribution, and the interaction of all these factors.}
% Matching the cost model to reality is challenging due to the numerous factors influencing an operator's cost. These factors include the hardware where the database is deployed, the operator's implementation, the number of tuples processed, and the current state of the database (e.g., data in the buffer, concurrent queries)\cite{2002generic_cost_model}. Consequently, numerous parameters need to be finely tuned to account for all these variables, and inaccuracies in cardinality estimation further degrade the cost model's accuracy\cite{2021survey_optimizer}. Additionally, even with accurate cardinality, the cost estimation of a query does not scale linearly with running time, which can result in suboptimal execution plans~\cite{2020cost_model}.
% \hzcmt{We didn't test the robustness of our method across different cost models, right? If that's the case, do we really want to discuss the difficulties of having an accurate cost model in this paper?}

% Plan enumeration has been proven to be an NP-hard problem~\cite{1984NP}. Although some great works~\cite{moerkotte2006DPccp, moerkotte2008DPHyper} have been made to reduce redundant enumeration space for dynamic programming, it is still impractical, or at least prohibitive, to search a significant portion of the space for large multi-join queries. 
% Thus, when a query touches a large number of tables, optimizers have to sacrifice optimality and employ heuristics to keep optimization time reasonable, like genetic algorithm in PostgreSQL~\cite{Postgres}, greedy method in DuckDB~\cite{duckdb}, etc. These heuristics can often generate poor plans.
% % \hzcmt{try to acknowledge the recent performance improvement on the dynamic programming approach from Thomas Neumann's group before stating that it is still not enough.}

% \subsection{Robust Query Processing}
% \label{Preliminary:Robust}


A query execution is \textbf{robust} if its performance is never too far from the optimal even if the cardinality estimations are way off (which is inevitable)~\cite{2019tutorial_robust, robustoptimization}. Under the context of join ordering, it means that the risk of choosing a catastrophic join order due to wild CE errors is low. There are typically two ways to improve the robustness of SQL execution. The first is to favor a ``\textbf{robust plan}'' rather than the cost-optimal one to take into account the uncertainty during query optimization~\cite{2002LEC, 2005RCE, 2007plan_diagram, 2008strict_plan_diagram}. For example, the optimizer would estimate cardinalities using intervals (rather than single values)~\cite{proactive} or probability distributions~\cite{2005RCE} and choose plans that have stable costs within certain confidence intervals. However, such a robust plan may not exist, and the plan chosen often exhibits a noticeable performance hit compared to the optimal~\cite{robustoptimization}.

The second approach to improving plan robustness is through \textbf{re-optimization}~\cite{1998reopt, 1999reopt_shared_nothing, 2000eddies, 2004pop, 2007pop_parallel, 2016planbouquets, Perron19, 2023reopt_zhao, justen2024polar}.
% \hzcmt{check if all these citations are about re-optimization}
The main idea is to correct CE mistakes while executing the query. Re-optimization must define specific materialization points in the query plan (usually at pipeline breakers) and collect statistics to obtain the true cardinalities at those points. If there is a large gap between the true cardinality and the estimated one, the system will re-invoke the optimizer, hoping to generate a better plan for the remaining operations. Although re-optimization enables self-correction at run time, materializing intermediate results is often costly and might compromise the end-to-end query performance.



% Acknowledging that query optimization is difficult, the concept of "robust query processing" was developed. Ideally, the performance of a query plan should be robust to any join orders in the enumeration space, such that any join order produces similar and good performance. A DBMS with robust query processing no longer needs to handle the complexity of join ordering as discussed in Section~\ref{sec:prelim:jo}, and can deliver overall better performance. 

% According to previous surveys~\cite{2019tutorial_robust, robustoptimization}, substantial research has been done in this field and can be categorized into two directions: robust plans~\cite{2002LEC, 2007plan_diagram, 2008strict_plan_diagram} and robust query execution~\cite{2009BoundedImpact, 2016planbouquets, 2017SpillBound, 2018FrugalSpillBound, 1998reopt, 1999reopt_shared_nothing, 2000eddies, 2002reopt_multi_usr, 2004pop, 2007pop_parallel, 2012reopt_recursive, 2013continuous_reopt, 2023reopt_zhao}. However, none of the aforementioned works have achieved complete "robust query processing", which is defined as the query execution being robust for any join order. Thus, both of them have some limitations.

% % \yxy{The following two paragraphs can be made more clear regarding (1) what the previous work actually did? (no need to get into the details of individual work, but a more clear summary of all the work) (2) why they are not good enough.}
% Robust plans account for uncertainty during the optimization process, choosing a "robust" plan whose performance remains stable in the presence of estimation errors. Specifically, they estimate distributions instead of a single-point value for cardinality and choose the plan with the least expected cost. However, they merely identify a "stable" plan but is not robust for any join order; there is often a performance gap between the stable and optimal plans~\cite{robustoptimization}. 
% % \hzcmt{citations?}

% Robust query executions adjust the plan during execution based on the progressively collected run-time statistics, always switching to the candidate plan that best fits the gathered data. The candidate plans can be re-optimized during run-time or prepared during the query optimization. However, the execution plan is monitored at run-time. Once a sub-optimality is detected, the plan is modified. However, as they are not robust across all execution plans, which means they could still execute a sub-optimal execution plan at some points.
% % \hzcmt{Is this paragraph about re-optimization?}

% % \yxy{this sentence implies that we achieve "ideal" robustness; it is risky to claim so.. soften the claim}.

\subsection{\YannAlg \& \PT}
\label{sec:prelim:pt}

An alternative thought of approaching join-order robustness is to design a join algorithm with bounded intermediate result sizes. Given a join query $Q$, let $N$ be the total number of tuples in all the input relations, and let $OUT$ be the number of tuples in the query output. The classic \textbf{\YannAlg}~\cite{yannakakis1981YA} guarantees a query complexity of $O(N + OUT)$\footnote{Considering the query size to be constant.}, which is the same as simply scanning the input and writing the output. Therefore, \YannAlg is instance optimal. The key idea is to pre-filter the tuples in the input relations that will not appear in the final output. The pre-filtering is realized via a series of semi-join reductions. A semi-join  $R \ensuremath{\ltimes} S$ outputs tuples from the left relation that have a match in the right relation. More formally, $R \ensuremath{\ltimes} S = \pi_{\text{attr(}R\text{)}}(R \bowtie S)$. In other words, a semi-join uses the right table as a filter to eliminate unmatched tuples in the left table.

Given a \emph{join graph} of a query where each vertex is a table scan, and each edge represents an equi-join (e.g., \cref{fig:yannakakis-join-graph}), the \YannAlg first picks an arbitrary vertex as root and obtains a \emph{join tree} (e.g., \cref{fig:yannakakis-semi-join}) via the GYO ear removal algorithm \cite{gyo}. The algorithm requires that the join graph is \emph{acyclic}  ($\alpha$-acyclic to be precise~\cite{yannakakis1981YA}) so that a join tree always exists. \YannAlg then proceeds to the \textbf{semi-join phase} \cite{usingsemi}, consisting of a \emph{forward pass} and a \emph{backward pass}. In the forward pass, the algorithm traverses the join tree from leaf to root (e.g., post-order traversal). For each node $R$, suppose its children are $S_1, S_2, \cdots, S_n$. The algorithm performs semi-join reduction on $R$ using all of its children (i.e., \texttt{for} $i = 1, 2, \cdots, n: R \ensuremath{\ltimes} S_i$). An example is shown in \cref{fig:yannakakis-semi-join}. Once the forward pass reaches the root, the algorithm starts the backward pass from root to leaf (e.g., level-order traversal). For each node $R$ with its parent $P$, $R \ensuremath{\ltimes} P$ is performed. The backward pass ends when all the leaf nodes are visited.

After this, the \YannAlg enters the \textbf{join phase}, where normal binary joins (e.g., hash joins) are carried out on the reduced tables, as shown in \cref{fig:yannakakis-join}. Each binary join must map to an edge in the join tree from the semi-join phase to guarantee a non-decreasing intermediate result. Because the semi-join phase ensures that \emph{all} tuples that will not contribute to the query output are removed (i.e., a full reduction), the join phase is proven to complete in $O(OUT)$ time.

Although \YannAlg exhibits appealing theoretical guarantees, few modern database management systems adopt it because the traditional hash-table-based implementation of the semi-joins makes the algorithm slow. The recent \textbf{\PT} (PT) technique proposed by Yang et al.~\cite{yang2023PT} solves this performance problem by using \BFs to conduct approximate semi-joins in the \YannAlg. Specifically, for each $R \ensuremath{\ltimes} S$ in the semi-join phase (it is called the \PT phase in PT), PT builds a \BF $\mathcal{B}_S$ with the join keys in $S$ and then uses the tuples in $R$ to probe $\mathcal{B}_S$. If the probe returns false for a tuple $t$ in $R$, $t$ is eliminated. Otherwise, $t$ is inserted into a different \BF $\mathcal{B}_R$ (could use a different join key) to prepare for the next semi-join in either the forward or backward pass.

Compared to the original \YannAlg, \PT trades a \emph{small} accuracy loss (caused by false positives of the \BFs) for a faster semi-join reduction. An inaccurate pre-filtering result does not affect the algorithm's correctness because the false positives will be removed during the subsequent join phase. Besides performance improvement, \PT generalizes \YannAlg to arbitrary join graphs, including cyclic ones. Instead of converting an acyclic join graph into a join tree, \PT transforms any join graph into a DAG (i.e., a \emph{transfer graph}) using a simple heuristic that assigns the direction of each edge from the smaller table to the larger one. Unfortunately, \PT does not inherit the strong theoretical guarantee from \YannAlg for acyclic queries because it could generate transfer schedules that lead to incomplete semi-join reductions. We will propose a new algorithm to fix this in the next section. For cyclic joins, although \PT improved the query performance empirically in many cases, there is no theoretical guarantee on the intermediate result sizes.