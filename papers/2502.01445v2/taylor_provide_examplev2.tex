\UseRawInputEncoding
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{subfigure}

\usepackage[square, comma, sort&compress, numbers]{natbib}
\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{times}
%\usepackage{breakurl}
\usepackage{array}
\usepackage{verbatim}
\usepackage{algorithm}
\usepackage{bbding}
\usepackage{algpseudocode}
\usepackage{lettrine}
\usepackage{booktabs}

\usepackage{adjustbox}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{SPFFNet: Strip Perception and Feature Fusion Spatial Pyramid Pooling for Fabric Defect Detection\\ 
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}


\author{\IEEEauthorblockN{Peizhe Zhao}
\IEEEauthorblockA{\textit{Waterford Institute} \\
\textit{Nanjing University of Information Science and Technology}\\
Nanjing, China \\
peizhezhao@nuist.edu.cn}
}

\maketitle

\begin{abstract}
Defect detection in fabrics is critical for quality control, yet existing methods often struggle with complex backgrounds and shape-specific defects. In this paper, we propose an improved fabric defect detection model based on YOLOv11. To enhance the detection of strip defects, we introduce a Strip Perception Module (SPM) that improves feature capture through multi-scale convolution. We further enhance the spatial pyramid pooling fast (SPPF) by integrating a squeeze-and-excitation mechanism, resulting in the SE-SPPF module, which better integrates spatial and channel information for more effective defect feature extraction. Additionally, we propose a novel focal enhanced complete intersection over union (FECIoU) metric with adaptive weights, addressing scale differences and class imbalance by adjusting the weights of hard-to-detect instances through focal loss. Experimental results demonstrate that our model achieves a 0.8-8.1\% improvement in mean average precision (mAP) on the Tianchi dataset and a 1.6-13.2\% improvement on our custom dataset, outperforming other state-of-the-art methods.


\end{abstract}

\begin{IEEEkeywords}
fabric defect detection, multi-scale convolution, squeeze-and-excitation networks, deep learning, intersection over union loss function, fabric defect dataset
\end{IEEEkeywords}

\section{Introduction}

Traditional fabric defect detection~\cite{weng2024enhancing, li2024lr, qiao2022novel} relies heavily on visual inspection by human experts, a process that is time-consuming, labor-intensive, and prone to errors, particularly when defects are small or contrast is low. This method often produces subjective and difficult-to-quantify results, leading to high defect rates and unreliable assessments. As a result, computer vision-based defect detection algorithms~\cite{li_2024_hsdyolo, chen_2024_cecyolo, xu_2024_improvement, chen_2023_ld2yolo, yang_2023_stca, li_2022_the, song_2022_a} have begun to emerge and develop. However, general object detection algorithms struggle with the complex backgrounds of fabric defects and their varied aspect ratios. Thus, adapting to the large-scale variations of fabric defects and distinguishing complex backgrounds are key challenges in improving the performance of fabric defect detection.

Modern fabric defect detection algorithms are generally divided into two categories: two-stage and single-stage methods. The two-stage method, such as Cascade Region-based Convolutional Neural Networks (Faster R-CNN)~\cite{ren_2017_faster}, improves accuracy and speed through cascaded detection. However, it may struggle with detecting multiple defects or misidentifying them. Similarly, the Convolutional Neural Network-based Mobile-Unet method~\cite{jing_2020_mobileunet} faces similar limitations. Recently, diffusion models have also gained attention in various vision tasks, including fabric defect detection, due to their ability to generate high-quality outputs and handle complex visual patterns. For instance, IMAGPose~\cite{shen2024imagpose} and IMAGDressing~\cite{shen2024imagdressing} have demonstrated the potential of diffusion models for pose-guided image synthesis and customizable virtual dressing. Additionally, advancements in progressive conditional diffusion models~\cite{shen2023advancing} and rich-contextual conditional diffusion models~\cite{shen2024boosting} have shown promise in enhancing the consistency and realism of generated images, offering a potential direction for fabric defect detection in more complex scenarios.


The single-stage method, derived from the YOLO~\cite{redmon_2016_you} framework, has shown promise. For example, the enhanced YOLOv3~\cite{redmon_2018_yolov3} model~\cite{jing_2020_fabric} improves detection through an attention mechanism and negative sample weighting but remains insufficient for accurately detecting complex defect types. The YOLOv5~\cite{jocher_2020_ultralyticsyolov5} algorithm~\cite{liu_2023_an} enhances feature representation by combining adaptive pooling with an attention module and optimizing the loss function. However, its accuracy remains limited in handling specific defect types and complex scenarios.

In response to these challenges, we propose a fabric defect detection model based on the improved YOLOv4~\cite{jocher_2023_yolov8} model. While retaining the speed advantages of single-stage models, we introduce a Strip Perception Module (SPM) that incorporates multi-scale convolution to significantly enhance the model’s feature capture and extraction capabilities for strip defects. To improve the ability to distinguish between complex backgrounds and defects, we propose an enhanced Squeeze-and-Excitation Spatial Pyramid Pooling Fast (SE-SPPF), which fully integrates spatial and channel features. Additionally, to address the wide range of target box scales for different defect types, we introduce the Focal Enhanced Complete Intersection over Union (FECIoU) metric. This novel approach dynamically adjusts weights for difficult-to-detect instances, improving the model's adaptability to target boxes with large aspect ratios.

The key contributions of this paper are as follows:
\begin{itemize}
    \item[\textcolor{black}{$\bullet$}] A multi-scale convolutional SPM is introduced into the YOLOv4 backbone to improve feature capture and extraction for strip defects.
    \item[\textcolor{black}{$\bullet$}] SE-SPPF is proposed to enhance the model’s ability to distinguish complex backgrounds and targets by combining weighted channel maps with spatial pyramid pooling.
    \item[\textcolor{black}{$\bullet$}] We propose FECIoU, an improved version of CIoU, which incorporates a focal weighting mechanism to reduce the impact of scale variations in fabric defects, improving both detection efficiency and accuracy.
    \item[\textcolor{black}{$\bullet$}] We have collected, organized, and annotated a fabric defect dataset consisting of 8,645 samples.
\end{itemize}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/yolov11_fabric.pdf}
    \caption{Network structure of the proposed method}
\label{fig1}
\end{figure*}

\section{Related Work}\label{sec:rw}

\subsection{Fabric Defect Detection Algorithms}
Modern fabric defect detection methods are mainly divided into two categories: two-stage and single-stage approaches. The two-stage methods, such as Faster R-CNN~\cite{ren_2017_faster}, utilize cascade detection to improve accuracy and speed. However, they may struggle with detecting multiple defects in a single fabric sample. The CNN-based Mobile-Unet~\cite{jing_2020_mobileunet}, which replaces U-Net’s encoding block with MobileNetV2, achieves impressive accuracy (99.75\% on YID, 98.80\% on FID), but still faces limitations in handling various defect types.
Single-stage methods, particularly those based on the YOLO framework, have gained popularity. Enhanced YOLOv3~\cite{redmon_2018_yolov3} improves fabric defect detection by adding an attention mechanism and negative sample weighting~\cite{jing_2020_fabric}. While effective, it still underperforms in detecting complex defects. YOLOv5~\cite{jocher_2020_ultralyticsyolov5} improves feature representation through adaptive pooling and an attention module~\cite{liu_2023_an}, but faces challenges in complex scenarios. To address these issues, we propose an improved YOLOv4-based model with a Strip Perception Module (SPM) that enhances feature extraction for strip defects, retaining the speed advantage of single-stage detection.

\subsection{Attention Mechanism}
The attention mechanism~\cite{shen2023pbsl,shen2023triplet} enhances model performance by focusing on relevant spatial, channel, or hybrid features. Spatial attention methods like SAM~\cite{zhu_2019_an} and RANet~\cite{shao_2023_ranet} prioritize key regions in the spatial domain, improving the capture of spatial dependencies. RANet uses a relation module to model feature interactions, leveraging attention or graph convolutions.
Channel attention, exemplified by SENets~\cite{hu_2018_squeezeandexcitation}, introduces a squeeze-and-excitation (SE) block that reweights feature channels to highlight important features. This mechanism improves representational power without significantly increasing computational cost. For fabric defect detection, we propose an enhanced spatial pyramid pooling fast (SE-SPPF) that integrates SENetv2~\cite{narayanan_2023_senetv2} for better multi-scale feature fusion, addressing the complexity and variation of defect shapes.

\subsection{Loss Function}
In object detection, the loss function quantifies the difference between predicted and ground truth bounding boxes. Intersection over Union (IoU)~\cite{jiang_2018_acquisition} is commonly used to measure this overlap. The IoU loss encourages the model to align predicted boxes with ground truth. The Generalized IoU (GIoU)~\cite{rezatofighi_2019_generalized} extends IoU by addressing scale and offset mismatches, providing more reliable localization, but it can be ineffective for boxes with significant overlap.
Distance IoU (DIoU)~\cite{zheng_2019_distanceiou} refines GIoU by incorporating centroid distance, improving localization accuracy. However, DIoU does not account for size variations between objects. Complete IoU (CIoU)~\cite{zheng_2019_distanceiou} incorporates centroid distance, overlap area, and angular difference, making it more effective for rotated boxes. However, for fabric defect detection, where target aspect ratios vary significantly, basic IoU can lead to errors. To address this, we propose an improved version of CIoU (FECIoU), which adjusts for scale differences and enhances detection accuracy for targets with varying aspect ratios.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/Strip.pdf}
    \caption{Strip Perception Module}
\label{fig2}
\end{figure*}

\section{Proposed Method}\label{sec:method} 

\subsection{Overview}
This paper presents a fabric defect detection method based on YOLOv11, addressing the challenges of complex defect shapes and the need for high detection accuracy and real-time performance. The proposed method incorporates a strip perception module (SPM) and a squeeze-and-excitation spatial pyramid pooling fast (SE-SPPF). As shown in Fig.\ref{fig1}, this approach enhances YOLOv11 by maintaining high detection accuracy while meeting real-time constraints, achieving significant improvements in fabric defect detection.

The SPM leverages strip convolution to extract strip defect features through intensive interactions with convolutions of various shapes, improving the model's precision in detecting and positioning strip defects. To enhance background discrimination and texture information extraction, the spatial pyramid pooling is re-designed as SE-SPPF, combining the channel attention mechanism of SENetv2. This module optimally utilizes both channel and spatial information to refine background discrimination and defect feature extraction. Additionally, a novel loss function, focal enhanced complete intersection over union (FECIoU), is introduced to address the issue of large-scale variations in target boxes. FECIoU assigns higher weights to samples with lower IoU, ensuring the model focuses on these challenging samples during training, thus improving detection efficiency and accuracy.

\subsection{Strip Perception Module}  
In the task of fabric defect detection, the complex shape and large size variation of defect features affect the accuracy of detection. Multi-scale convolution can effectively capture features at different scales in the feature map, especially when facing long strip-shaped defects that occur frequently in fabric operations. Multi-scale convolution can more effectively extract defect features. The specific design is shown in Fig.\ref{fig2}.

This paper proposes SPM. First, two convolution blocks of 1×1 and 3×3 are used to minimize the number of channels, and then multi-scale (1×3, 3×1, 3×3) convolution operations are performed using branch parallelism. The resulting feature maps are densely stacked using concat, and then a 1x1 convolution kernel is used to extract important features from the convolutions of different scales. Finally, a residual structure was introduced to improve the stability and effectiveness of training. While maintaining the depth of the network, information transmission and gradient flow are ensured. In summary, SPM can effectively extract the features of strip defects and improve the accuracy of the model. 


\subsection{Squeeze and Excitation Spatial Pyramid Pooling Fast }  
Fabric defects usually exhibit multiple features. In order to eliminate some noise, make the features more robust, and help the model better capture the overall structure and texture of the image, SE-SPPF introduces SENetv2 to more reasonably assign weights to each channel. Combined with the multi-scale fusion in SPPF space, it strengthens the model's ability to extract features from both spatial and channel perspectives. The specific design is shown in Fig.\ref{fig3}.
This paper proposes SE-SPPF. First, the feature map is weighted by SENetv2 to the channel, and then the channel number is adjusted using a 1x1 convolution and input to SPPF. The four feature maps of different scales obtained by SPPF are concatenated using a residual structure and the weighted feature map Concat after feature extraction using a 1x1 convolution. Finally, features are further extracted using two convolutions of 1x1 and 3x3.

\subsection{Focal Enhanced Complete Intersection over Union}  
The span of the defect detection box for different types of fabric defects is very large, especially for defects that appear in the form of stripes, which are several times or even more than the length and width of most target detection objects. Therefore, this paper proposes FECIoU, which uses a focal weight mechanism to make the model pay more attention to difficult-to-detect objects during training.
Equation \ref{equation1} is the formula for FECIoU, where \((1 - IoU) ^ \gamma\)is the weight value for CIoU and \(\gamma\) is a manually set parameter. In Equation \ref{equation2} ,\(\rho^2(b, b^g)\)is the squared Euclidean distance between the centers of the predicted and ground truth boxes, calculated as shown in Equation \ref{equation3}, and \( c \) is the diagonal length of the minimum bounding box. \(\alpha v\) is a penalty term for the aspect ratio difference, and the specific calculation method is shown in Equations \ref{equation4} and \ref{equation5} . \( w^g, h^g, w, \) and \( h \) are the width and height of the predicted frame and the actual frame, respectively.

\begin{equation}
\text{FECIoU} = (1 - \text{IoU})^\gamma \cdot \left( \text{IoU} - \frac{\rho^2(\mathbf{b}, \mathbf{b}^g)}{c^2} - \alpha v \right),
\label{equation1}
\end{equation}

\begin{equation}
\text{CIoU} = \text{IoU} - \frac{\rho^2(\mathbf{b}, \mathbf{b}^g)}{c^2} - \alpha v,
\label{equation2}
\end{equation}

\begin{equation}
\rho^2(\mathbf{b}, \mathbf{b}^g) = (x_b - x_{b^g})^2 + (y_b - y_{b^g})^2,
\label{equation3}
\end{equation}

\begin{equation}
v = \frac{4}{\pi^2} \left( \arctan \frac{w^g}{h^g} - \arctan \frac{w}{h} \right)^2,
\label{equation4}
\end{equation}

\begin{equation}
\quad \alpha = \frac{v}{(1 - \text{IoU}) + v}.
\label{equation5}
\end{equation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/SE-SPPF.pdf}
    \caption{Squeeze-and-Excitation Spatial Pyramid Pooling Fast}
\label{fig3}
\end{figure*}

\section{Experiment and Analysis}\label{sec:exp} 
\subsection{Datasets}
\textbf{\emph{Tianchi fabric dataset}}
Tianchi fabric dataset~\cite{_2020_smart}, provided by Alibaba's Tianchi platform, is a significant resource for fabric defect detection research. It comprises high-resolution fabric images with detailed annotations of various defect types, such as holes, stains, wrinkles, color shades, and missing threads. The dataset, consisting of thousands to tens of thousands of images, is designed to facilitate the development and validation of defect detection algorithms and automated quality inspection systems in the fabric industry. 

\textbf{\emph{Self dataset}}
This dataset was collected and labeled and organized by us. The data mainly comes from the workshop of a fabric factory in Jiangsu Province and public images that can be collected on the Internet. After our collection and organization, the final dataset contains a total of 8,645 fabric defect images, which are classified into five types of defects that are most commonly found in the fabric process: missing stitches, broken holes, stain, broken seam, and broken stitches. The dataset is divided into a training set and a test set in a ratio of 2:8. In addition, this paper also uses some image data enhancement methods, such as rotation, translation, scaling, and flipping, to expand the dataset and generate more samples, thereby improving the generalization ability of the model and reducing the risk of overfitting.

\subsection{Evaluation Metrics}  
The mAP (Mean Average Precision) is a widely used evaluation metric in object detection and information retrieval tasks, providing a comprehensive view of a model's performance by evaluating precision across different levels of recall. mAP is computed by averaging the Average Precision (AP) for each class, which is the area under the precision-recall curve for that class, and then averaging these values across all classes. The formula for AP is given by: 
\begin{equation}
AP = \int_0^1 P(r) \, dr.
\end{equation}
where \(P(r)\) denotes precision at a given recall level \(r\). The final mAP score is calculated as:
\begin{equation}
mAP = \frac{1}{N} \sum_{i=1}^{N} AP_i.
\end{equation}

where \(N\) is the number of classes and \(AP_i\) is the Average Precision for class \(i\). GFLOPs (Giga Floating Point Operations) is a metric used to measure computational complexity, representing the number of floating-point operations a model performs per second, typically expressed in billions. A lower GFLOPs value indicates better computational efficiency and faster inference times, as fewer operations are required to process the same task.
Params (Parameters) refers to the total number of parameters in a model, which reflects its complexity and memory footprint. A lower number of parameters often suggests more memory-efficient models, which can lead to better scalability and less resource consumption.
Together, these metrics provide a holistic assessment of a model’s performance, efficiency, and resource utilization, helping to balance the trade-offs between computational power, memory usage, and model accuracy.

\begin{table*}[t]
\centering
\caption{Comparison of the performance of the proposed improved model with multiple SOTA on the Tianchi dataset}
\label{tab:tianchi}
\renewcommand{\arraystretch}{1.5} 
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lccccccccccc}
\toprule
&\multicolumn{9}{l}{\textbf{mAP@0.5/\%}}& & \\
\cmidrule(lr){2-6}
\textbf{Method} & \textbf{Knot} & \textbf{Triple Wire} & \textbf{Coarse Pick} & \textbf{Broken Spandex} & \textbf{Warp Knot}& \textbf{Weft Shrink} & \textbf{Hole} & \textbf{Stain} & \textbf{All} & \textbf{GFLOPs}& \textbf{Params} \\
\midrule
YOLOv5 & 61.6 & 77.2 & 59.4 & \textbf{76.8} & 45.8 & \textbf{46.9} & \textbf{83.7} & 43.5 & 61.9 & 5.8 & 2183224\\
YOLOv6 & 58.1 & 78 & 52.8 & 68.1 & 47.1 & 30.8 & 82.3 & 44.5 & 57.7 & 11.5 & 4155816\\
YOLOv8 & \textbf{65.9} & 78.8 & 60.5 & 76.3 & 51.3 & 40.1 & 81.6 & 59.9 & 64.3 & 6.8 & 2685928\\
YOLOv9t & 65.4 & 80.4 & 59.8 & 71.8 & \textbf{52.6} & \textbf{46.9} & 83.3 & 62.9 & 65.4 & 6.4 & 1731384\\
YOLOv9s & 66 & \textbf{82} & 54.3 & 76.6 & 54.4 & 46.7 & 79.7 & 64.4 & 65.5 & 22.1 & 6196744\\
YOLOv10n & 59.3 & 77.4 & 57.7 & 69.4 & 41.5 & 39.2 & 81.7 & 57.7 & 60.5 & 8.2 & 2697536\\
YOLOv11n & 64.4 & 80 & \textbf{64.3} & 76.1 & 48.1 & 43.7 & 80.5 & 62.9 & 65 & 6.3 & 2583712\\
\toprule
Ours & 64.5 & 80.5 & 63.5 & 74.6 & 49 & 43.9 & \textbf{83.7} & \textbf{66.4} & \textbf{65.8} & 6.8 & 2858951\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Implementation Details} 
In all experiments, the model size selected for the YOLO series of models is normal. The batch size for training the model is 32, and the input size of the image is 640. Because the dataset has a large number of samples and may contain noisy data, in order to avoid local optima and obtain better model performance, the optimizer selects Stochastic ic gradient descent (SGD), with an initial learning rate of 0.01 and momentum of 0.937. To compare the performance of models of different sizes, the experiment uniformly sets the patience to 20, which is the number of epochs that the training is allowed to continue without improving the accuracy of the model on the validation set.

\subsection{Comparison with State-of-the-art Methods} 
We compared the proposed method with six state-of-the-art methods, including YOLOv5~\cite{jocher_2020_ultralyticsyolov5}, YOLOv6~\cite{li_2022_yolov6}, YOLOv8~\cite{jocher_2023_yolov8}, YOLOv9t~\cite{wang_2024_yolov9}, YOLOv9s~\cite{wang_2024_yolov9}, and YOLOv10n~\cite{wang_2024_yolov10}.

\subsubsection{Comparisons on Tianchi fabric dataset}
Table \ref{tab:tianchi} shows a comparison of the performance of the proposed improved model with multiple state-of-the-art algorithms on the Tianchi dataset. It can be seen that the model proposed in this paper achieved the highest mAP (i.e., 65.8\%).

The mAP of the improved model in each defect category performed well, which shows that the proposed SE-SPPF module fully integrates important defect information from both spatial and channel perspectives, helping the model find key features.



\subsubsection{Comparisons on Self dataset}
Table \ref{tab:self} shows a comparison of the performance of the proposed improved model with multiple state-of-the-art algorithms on the dataset we created. It can be seen that the model proposed in this paper achieves the highest mAP (i.e. 90.6\%) without significantly increasing the computational cost and model size. Among them, the mAP for the detection of the two strip defects missing stitches and broken stitch is the highest among all methods. This shows that the multi-scale convolution SPM plays a key role in the detection of strip defects, which improves the detection ability of the model.

\begin{table*}[t]
\centering
\caption{Comparison of the performance of the proposed improved model with multiple SOTA on the Self dataset}
\label{tab:self}
\renewcommand{\arraystretch}{1.5} 
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccc}
\toprule
&\multicolumn{6}{l}{\textbf{mAP@0.5/\%}}& & \\
\cmidrule(lr){2-6}
\textbf{Method} & \textbf{Missing Stitches} & \textbf{Broken Holes} & \textbf{Stain} & \textbf{Broken Seam} & \textbf{Broken Stitches} & \textbf{All} & \textbf{GFLOPs}& \textbf{Params} \\
\toprule
YOLOv5 & 85.4 & 73.4 & 99.5 & 80.2 & 75.9 & 82.9 & 5.8 & 2182639\\
YOLOv6 & 83 & 68.9 & 99.5 & 80.2 & 55.5 & 77.4 & 11.5 & 4155519\\
YOLOv8 & 93.9 & 78.2 & 99.5 & 82 & 88.1 & 88.3 & 6.8 & 2685343\\
YOLOv9t & 89.1 & 76.3 & 99.5 & 82.1 & 85.8 & 86.5 & 6.4 & 1730799\\
YOLOv9s & 91.7 & 80.2 & 99.5 & 81.2 & 91.8 & 88.9 & 22.1 & 6195583\\
YOLOv10n & 89.5 & 76.8 & 99.5 & 78.6 & 85.9 & 86.1 & 8.3 & 2696336 \\
YOLOv11n & 93.1 & 79.4 & 99.5 & \textbf{83.8}& 89.3 & 89 & 6.3 & 2583127 \\
\midrule
Ours & \textbf{95.3}& \textbf{83.5}& \textbf{99.5}& 81.1 & \textbf{93.5}& \textbf{90.6}& 6.8 & 2858951 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/heat.pdf}
    \caption{Comparison visualized by heat maps}
\label{fig4}
\end{figure*}


\subsection{Ablation Studies and Analysis}  
The comparison results in Tables \ref{tab:tianchi} and \ref{tab:self} show that the proposed improved model is superior to many state-of-the-art single-stage detection methods. Next, a comprehensive analysis of the proposed improved model will be performed by testing it on the dataset we created to explore the logical basis for its superiority.
As shown in Table \ref{tab:selfab}, the model containing the SPM, SE-SPPF, and FECIoU modules has the highest detection accuracy, with an mAP of 90.6\%. This is an improvement over the baseline model, which has an mAP of 89\%. The baseline model does not include these modules, and its computational cost is 6.3 GFLOPs and the number of parameters is 2.58 million.
After the SPM module is introduced into the model, the detection accuracy is improved to 89.6\%, the computational cost is slightly increased to 6.6 GFLOPs, and the number of parameters is slightly increased to 2.61 million. This indicates that the SPM module has the effect of enhancing the extraction of features for strip defects in the dataset. Similarly, when the SE-SPPF module is added alone, the detection accuracy is 89.6\%, and GFLOPs (6.6) and parameters (2.89 million) increase slightly, which indicates that SE-SPPF also plays a key role in defect feature extraction by better fusing channel and spatial features.
When both the SPM and SE-SPPF modules were included, the mAP was further improved to 90.3\%, with a computational cost of 6.8 GFLOPs and a parameter count of 2.86 million. This indicates that the combination of these modules enhances feature extraction capabilities without a significant increase in computational cost.
Finally, when the three components SPM, SE-SPPF and FECIoU are integrated, the model achieves the highest mAP of 90.6\%, with a slight increase in computational cost (6.8 GFLOPs) and 2.86 million parameters. This shows the synergistic effect of these modules, as they work together to improve the accuracy of the model while maintaining a reasonable balance of computational efficiency..

\begin{table}[t]
\centering
\caption{Results of ablation experiments on self datasets}
\renewcommand{\arraystretch}{1.5} 
\begin{tabular}{cccccc}
\toprule
\textbf{SPM} & \textbf{SE-SPPF} & \textbf{FECIoU} & \textbf{mAP@0.5/\%} & \textbf{GFLOPs} & \textbf{Params} \\ \midrule
- & - & - & 89& 6.3& 2583127\\
\checkmark & - & - & 89.6& 6.6& 2613063\\
- & \checkmark & - & 89.6& 6.6& 2894679\\
\checkmark & \checkmark & - & 90.3& 6.8& 2858951\\
\checkmark & \checkmark & \checkmark & 90.6& 6.8& 2858951 \\ \bottomrule
\end{tabular}
\label{tab:selfab}
\end{table}

\subsection{Visualization}  
As shown in Fig.\ref{fig4}, the heat maps after the spatial pyramid pooling layer of the baseline model and the improved model proposed in this paper are shown respectively. It can be intuitively seen that the improved model proposed in this paper is more accurate than the baseline model in determining the most important region for prediction, and the coverage completely includes the defective parts of this fabric. This shows that the SPM module accurately extracts the important features of the strip defects, and SE-SPPF allows the model to accurately distinguish between the background and defects, which in turn allows the model to more accurately determine the most important region for judgment. The visualization results of the heat map once again verify the effectiveness of the structure proposed in this paper.

\section{Conclusion}\label{sec:con} 
This paper introduced an enhanced fabric defect detection model built upon YOLOv11. To improve the model's ability to capture and extract features of stripe defects, a SPM was designed and incorporated. Additionally, the SPPF was enhanced, and a novel Squeeze-and-Excitation Spatial Pyramid Pooling Fast (SE-SPPF) was proposed to strengthen the model's capacity to differentiate backgrounds and extract defect features. Moreover, FECIoU was proposed, an adaptive-weight version of the CIoU, to mitigate the effects of significant scale differences between target boxes.
SPM utilized multi-scale convolution to effectively capture features at various scales within the feature map, while its dense connection structure enhanced the accuracy and efficiency of feature sharing, leading to an overall improvement in the model’s accuracy and speed.
SE-SPPF combined weighted channel feature maps with spatial pyramid pooling, ensuring the comprehensive integration of both spatial and channel information, which further boosted the model's ability to extract complex features.
FECIoU applied focal loss to adjust the weights of hard-to-detect instances during training, addressing class imbalance issues and ultimately improving the overall detection performance.
In conclusion, the proposed model outperformed other state-of-the-art methods, achieving an increase in mAP of 0.8-8.1\% on the Tianchi dataset and 1.6-13.2\% on our custom dataset.
However, there are still some limitations to the current work. For example, the types of fabric defects currently studied are too few, and the types of defects in actual production are far more than those in the current dataset. The performance of the model on new defect types needs to be further explored. There is also a defect of color error in the fabric industry, which changes with the color of the fabric and poses a new challenge to the model.

{\small
\bibliographystyle{IEEEtran}
\bibliography{ref}}
\end{document}
