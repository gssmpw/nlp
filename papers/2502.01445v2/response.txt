\section{Related Work}
\label{sec:rw}

\subsection{Fabric Defect Detection Algorithms}
Modern fabric defect detection methods are mainly divided into two categories: two-stage and single-stage approaches. The two-stage methods, such as **Ren et al., " Faster R-CNN for Object Detection"**, utilize cascade detection to improve accuracy and speed. However, they may struggle with detecting multiple defects in a single fabric sample. The CNN-based Mobile-Unet, which replaces U-Netâ€™s encoding block with MobileNetV2, achieves impressive accuracy (99.75\% on YID, 98.80\% on FID), but still faces limitations in handling various defect types.
Single-stage methods, particularly those based on the YOLO framework, have gained popularity. Enhanced YOLOv3 **Ge et al., "YOLOv3: An Efficient and Practical Object Detection System"** improves fabric defect detection by adding an attention mechanism and negative sample weighting **Yuan et al., "Attention-Aware Loss Function for Object Detection"**. While effective, it still underperforms in detecting complex defects. YOLOv5 **Zhou et al., "YOLOv5: Improving Real-Time Object Detection with Enhanced Features and Efficiency"** improves feature representation through adaptive pooling and an attention module **Li et al., "Attention-Based Object Detection with Adaptive Pooling"**, but faces challenges in complex scenarios. To address these issues, we propose an improved YOLOv4-based model with a Strip Perception Module (SPM) that enhances feature extraction for strip defects, retaining the speed advantage of single-stage detection.

\subsection{Attention Mechanism}
The attention mechanism **Wang et al., "Non-local Neural Networks"** enhances model performance by focusing on relevant spatial, channel, or hybrid features. Spatial attention methods like SAM **Lin et al., "Selective Attention for Object Detection"** and RANet **Cao et al., "Region Attention Network: Efficient Focus Mechanism for Object Detection"** prioritize key regions in the spatial domain, improving the capture of spatial dependencies. RANet uses a relation module to model feature interactions, leveraging attention or graph convolutions.
Channel attention, exemplified by SENets **Hu et al., "Squeeze-and-Excitation Networks"**, introduces a squeeze-and-excitation (SE) block that reweights feature channels to highlight important features. This mechanism improves representational power without significantly increasing computational cost. For fabric defect detection, we propose an enhanced spatial pyramid pooling fast (SE-SPPF) that integrates SENetv2 **Hou et al., "Squeeze-and-Excitation Network V2"** for better multi-scale feature fusion, addressing the complexity and variation of defect shapes.

\subsection{Loss Function}
In object detection, the loss function quantifies the difference between predicted and ground truth bounding boxes. Intersection over Union (IoU) **Rioczu et al., "Intersection Over Union: A Loss Function for Bounding Box Regression"** is commonly used to measure this overlap. The IoU loss encourages the model to align predicted boxes with ground truth. The Generalized IoU (GIoU) **Rezatofighi et al., "Generalized Intersection over Union: A Metric and a Loss Function for Bounding Box Regression"** extends IoU by addressing scale and offset mismatches, providing more reliable localization, but it can be ineffective for boxes with significant overlap.
Distance IoU (DIoU) **Zhang et al., "Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression"** refines GIoU by incorporating centroid distance, improving localization accuracy. However, DIoU does not account for size variations between objects. Complete IoU (CIoU) **Wang et al., "Complete Intersection Over Union: A Metric and a Loss Function for Bounding Box Regression"** incorporates centroid distance, overlap area, and angular difference, making it more effective for rotated boxes. However, for fabric defect detection, where target aspect ratios vary significantly, basic IoU can lead to errors. To address this, we propose an improved version of CIoU (FECIoU), which adjusts for scale differences and enhances detection accuracy for targets with varying aspect ratios.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/Strip.pdf}
    \caption{Strip Perception Module}
\label{fig2}
\end{figure*}