\section{Related Work}
\paragraph{LLMs in Peer Review}
Modern Large Language Models have enabled automated or semi-automated pipelines for academic reviewing **Brown et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,**Srivastava et al., "Highway Networks"**, already adopted in up to 15.8\% of AI-conference reviews **Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"**. While AI-generated comments can partially align with human judgments **Holtzman et al., "The Curious Case of Neural Text Degeneration"**, concerns persist regarding hallucinations **Zhang et al., "PEGASUS: Pre-trained Encoder with Adversarial Regularization for Sequential Unsupervised Learning"**, biases **Kumar et al., "Bias Detection in Text Classification via Contrastive Learning"**, and susceptibility to adversarial inputs **Eger et al., "Adversarial Examples for Evaluating the Robustness of Deep Neural Networks on Natural Language Processing Tasks"**. Studies have also raised the risks of flawed critique interpretation, anonymity breaches, and undue reviewer influence in fully automated setups **Chen et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Controlled?"**.

\paragraph{Perturbation Analysis in NLP}
Synthetic perturbations---ranging from minor lexical edits to deeper semantic shifts---serve as
stress tests for model robustness across many NLP tasks **Jiang et al., "SMART: Robust and Efficient Fine-Tuning of Pre-Trained Transformers"**. Even subtle modifications, such as
inserting factual errors or flipping a stance, can destabilize an LLMâ€™s output
**Goyal et al., "Annotated Dataset for Adversarial Examples in NLP"**, particularly when
models struggle to distinguish misleading from valid context **Li et al., "Context-Aware Language Model for Text Classification with Adversarial Training"**. Although some peer-review studies apply single-level perturbations
**Guo et al., "Perturbed Text: A Benchmark for Evaluating the Robustness of NLP Models"**, most ignore how perturbations to the \emph{review} or \emph{rebuttal} also skew
recommendations. Our approach broadens these evaluations by systematically altering all three
elements (paper, review, rebuttal) and assessing LLM responses through both \emph{directional}
and \emph{invariance} tests.