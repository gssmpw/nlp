



\begin{table*}[h]
\footnotesize
\renewcommand{\arraystretch}{1.2}
\begin{center}
\setlength\tabcolsep{4pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\toprule
\textbf{Perturbation Aspects} & \multicolumn{1}{c}{\textbf{Before}} & \multicolumn{1}{c}{\textbf{After}} \\ \midrule

\multirow{6}{*}{Paper: Contribution} 
&\underline{Our findings reveal that our model outperforms} & \underline{Our \sethlcolor{yellow}\hl{groundbreaking} findings reveal that our} \\
& \underline{its predecessors significantly in tasks related to}  & \underline{model achieves \sethlcolor{yellow}\hl{perfect accuracy}, unmatched} \\
& \underline{code search and code classification. We also del-} & \underline{by \sethlcolor{yellow}\hl{any prior model}, in tasks related to code} \\
& \underline{ve into the essential factors contributing to enha-}& \underline{search and code classification. Our study ost-}\\
& \underline{nced code representation learning across various}& \underline{ensibly \sethlcolor{yellow}\hl{illuminates} novel factors contributing}\\
& \underline{model sizes.}& \underline{to enhanced code representation learning.}\\
%\hdashline
\midrule
\multirow{3}{*}{Paper: Presentation} & \underline{In this work, we fuel code representation learning} & \underline{In \sethlcolor{yellow}\hl{ths} work, we \sethlcolor{yellow}\hl{furel} code \sethlcolor{yellow}\hl{representtaion} lear-} \\
& \underline{with a vast amount of code data via a two-stage } & \underline{ning with \sethlcolor{yellow}\hl{a vast qty} of code data via a \sethlcolor{yellow}\hl{duo} ph-} \\
& \underline{pretraining scheme.} & \underline{ase pretraining structure.} \\

\midrule

\multirow{6}{*}{Paper: Soundness} 
& \underline{We train our models on The Stack dataset (Kocetk} & \underline{Our training incorporates \sethlcolor{yellow}\hl{a diverse dataset} spa-} \\
& \underline{-ov etal., 2022) over nine languages - Python, Java,  } & \underline{nning multiple programming languages, aiming} \\
& \underline{Javascript, Typescript, C\#, C, Ruby, Go, and PHP.}& \underline{to build adaptable models. We developed \sethlcolor{yellow}\hl{a ran-}} \\
& \underline{As aforementioned, we train three embedding mod-} & \underline{\sethlcolor{yellow}\hl{ge of model architectures}, each varing in com-}\\
& \underline{els with size 130M (CodeSage-small), 356M (Code-} & \underline{plexity. specifics regarding the models' develop-}\\
& \underline{Sage--base), and 1.3B (CodeSage-large) parameters. } & \underline{ment steps are discussed in the appendices.}\\

\midrule

\multirow{3}{*}{Rebuttal: Tone} 
& \underline{Thank you for acknowledging the novelty and impa-}& \underline{Thank you for acknowledging our contributions, } \\
& \underline{ct of our work, especially our efforts on diving deep} & \underline{\sethlcolor{yellow}\hl{even if the critique seems to overlook} the substan-}\\
& \underline{into the ingredients for code representation learning.}&\underline{tial groundwork \sethlcolor{yellow}\hl{already laid out} in our research.} \\

\midrule

\multirow{3}{*}{Rebuttal: Completeness} 
& \underline{Thank you for the feedback. To address your main} & \underline{We appreciate the feedback. \sethlcolor{yellow}\hl{We will consider}}\\
& \underline{question, we briefly summarize the fundamental } & \underline{\sethlcolor{yellow}\hl{suggestions} about the distinctions between Code- } \\
& \underline{contributions of our work below.} & \underline{Sage and existing works.} \\

\midrule

\multirow{3}{*}{Rebuttal: Presentation} 
& \underline{Thank you for acknowledging the novelty and imp} & \underline{\sethlcolor{yellow}\hl{Thnak you acknowlidging} the \sethlcolor{yellow}\hl{novlty} and impacts } \\
& \underline{-act of our work, especially our efforts on $\cdots$ and } & \underline{of our work. We appreciate too your constructive}\\
& \underline{representation learning. We are also grateful for$\cdots$} & \underline{suggestions on benchmarks and baselines$\cdots$} \\

\midrule

\multirow{3}{*}{Review: Tone} & \underline{This paper introduces a novel two-step pretraining }  & \underline{This paper introduces a \sethlcolor{yellow}\hl{so-called} novel two-step } \\
& \underline{methodology for$\cdots$The approach starts with mask-} & \underline{pretraining methodology for $\cdots$The approach, \sethlcolor{yellow}\hl{in}} \\
& \underline{ed language modeling $\cdots$} & \underline{\sethlcolor{yellow}\hl{its dubious brilliance}, starts with masked$\cdots$} \\


\midrule

\multirow{2}{*}{Review: Conclusion} 
& \underline{Soundness: 3; Presentation: 2; Contribution: 2
}  & \underline{Soundness: 3; Presentation: 2; Contribution: 2} \\
& \underline{Strengths:$\cdots$ Weaknesses:$\cdots$ Rating: 6} & \underline{Strengths:$\cdots$ Weaknesses:$\cdots$ \sethlcolor{yellow}\hl{Rating: 1}} \\

\midrule

\multirow{3}{*}{Review: Correctness} 
& \underline{But I also understand that such ablation is expen-}  & \underline{But I also understand that such ablation is expen-} \\
& \underline{sive. Rating: 8: accept, good paper.$\cdots$} & \underline{sie. \sethlcolor{yellow}\hl{- The paper only evaluates CodeSage on code} } \\
& \underline{} & \underline{\sethlcolor{yellow}\hl{classification tasks.}$\cdots$ Rating:$\cdots$} \\



\bottomrule
\end{tabular}}
\end{center}

\caption{\label{tab:case study}Examples of nine perturbation aspects with text before and after being edited, highlighted contents represent symbolic textual elements for this perturbation aspect.}
\end{table*}

