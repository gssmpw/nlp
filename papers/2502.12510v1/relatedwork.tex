\section{Related Work}
\paragraph{LLMs in Peer Review}
Modern Large Language Models have enabled automated or semi-automated pipelines for academic reviewing \citep{liang2024can, jin2024agentreview, yu2024automated}, already adopted in up to 15.8\% of AI-conference reviews \citep{liangmonitoring, latona2024ai}. While AI-generated comments can partially align with human judgments \citep{liang2024can}, concerns persist regarding hallucinations \citep{zeng2024johnny}, biases \citep{gallegos2024bias}, and susceptibility to adversarial inputs \citep{liang2024can, lu2024ai}. Studies have also raised the risks of flawed critique interpretation, anonymity breaches, and undue reviewer influence in fully automated setups \citep{ye2024we, yu2024automated}.

\paragraph{Perturbation Analysis in NLP}
Synthetic perturbations---ranging from minor lexical edits to deeper semantic shifts---serve as
stress tests for model robustness across many NLP tasks \citep{DBLP:conf/emnlp/SaiDSMK21,
DBLP:conf/acl/HeZ0KCGT23, DBLP:conf/emnlp/KarpinskaRTSGI22}. Even subtle modifications, such as
inserting factual errors or flipping a stance, can destabilize an LLMâ€™s output
\citep{DBLP:journals/corr/abs-2312-15407, DBLP:journals/corr/abs-2305-14658}, particularly when
models struggle to distinguish misleading from valid context \citep{zeng2024johnny,
deshpande2023toxicity}. Although some peer-review studies apply single-level perturbations
\citep{ye2024we}, most ignore how perturbations to the \emph{review} or \emph{rebuttal} also skew
recommendations. Our approach broadens these evaluations by systematically altering all three
elements (paper, review, rebuttal) and assessing LLM responses through both \emph{directional}
and \emph{invariance} tests.