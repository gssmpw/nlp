%%
%% This is file `sample-sigconf.tex',
%% \textit{\textit{\textit{}}}generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%\documentclass[sigconf,natbib=true,anonymous=true]{acmart}
\PassOptionsToPackage{prologue,dvipsnames,table}{xcolor}
\DocumentMetadata{}
\documentclass[sigconf]{acmart}

%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

\setcopyright{none}
%\setcopyright{acmcopyright}
%\copyrightyear{2025}
%\acmYear{2025}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[SIGIR '25]{International ACM SIGIR Conference on Research and Development in Information Retrieval}{July 13 -- July 18, 2025}{Padua, Italy}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
% \usepackage{hyperref} % Ensure hyperref is loaded before hyperxmp
% \usepackage{hyperxmp}

%\usepackage{hyperref}
%\usepackage{hyperxmp}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{booktabs}
%\usepackage{pifont}
\usepackage{soul}
\usepackage{amsmath}
\usepackage[table]{xcolor}
%\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{bm}
% \pgfplotsset{compat=1.16}
\usepackage{pgfplots}
\usetikzlibrary{matrix}
\usetikzlibrary{patterns}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{external}
\usepgfplotslibrary{statistics}    




% \newtcolorbox{mybox}[1]{colback=orange!5!white,colframe=orange!75!black,fonttitle=\bfseries,title=#1}


%%
%% end of the preamble, start of the body of the document source.



\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%\title{Autonomous AI: Evolving Recommender Systems into Ethical, Intelligent User-Centric Assistants}
%\title{Orthogonal Meta-learning Boosted Bayesian Optimization for Uncertain Multi-Objective Recommendation}
\title{Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization}

%\renewcommand{\shorttitle}{Evolving Recommender Systems into Ethical, Intelligent User-Centric Assistants}
\renewcommand{\shorttitle}{Uncertain Multi-Objective Recommendation}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%\author{Paper ID: 8068}
%\begin{comment}

\author{Hongxu Wang}
\authornote{Both authors contributed equally to this research.}
%\orcid{1234-5678-9012}
\affiliation{
\institution{Chongqing University}
\city{Chongqing}
\country{China}
}
\email{hxwang0721@outlook.com}
%
\author{Zhu Sun}
\authornotemark[1]
\affiliation{%
  \institution{Singapore University of Technology and Design}
  %\streetaddress{P.O. Box 1212}
  \city{}
  %\state{Ohio}
  \country{Singapore}
  %\postcode{43017-6221}
}
\email{sunzhuntu@gmail.com}

\author{Yingpeng Du}
\authornote{Corresponding author.}
\affiliation{%
  \institution{Nanyang Technological University}
  \city{}
  \country{Singapore}
}
\email{dyp1993@pku.edu.cn}

\author{Lu Zhang}
\affiliation{%
  \institution{Chengdu University of Information Technology}
  \city{Chengdu}
  \country{China}
}
\email{zhang\_lu010@outlook.com}

\author{Tiantian He}
\affiliation{%
 \institution{Agency for Science, Technology and Research}
 \city{}
 %\state{Arunachal Pradesh}
 \country{Singapore}
}
\email{he\_tiantian@cfar.a-star.edu.sg}

\author{Yew-Soon Ong}
\affiliation{%
  \institution{Nanyang Technological University}
  %\streetaddress{30 Shuangqing Rd}
  \city{}
  %\state{Beijing Shi}
  \country{Singapore}}
\email{asysong@ntu.edu.sg}

%\end{comment}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Recommender systems (RSs) play a crucial role in shaping our digital interactions, influencing how we access and engage with information across various domains. Traditional research has predominantly centered on maximizing recommendation accuracy, often leading to unintended side effects such as echo chambers and constrained user experiences. Drawing inspiration from autonomous driving, we introduce a novel framework that categorizes RS autonomy into five distinct levels, ranging from basic rule-based accuracy-driven systems to behavior-aware, uncertain multi-objective RSsâ€”where users may have varying needs, such as accuracy, diversity, and fairness. In response, we propose an approach that dynamically identifies and optimizes multiple objectives based on individual user preferences, fostering more ethical and intelligent user-centric recommendations. To navigate the uncertainty inherent in multi-objective RSs, we develop a Bayesian optimization (BO) framework that captures personalized trade-offs between different objectives while accounting for their uncertain interdependencies. Furthermore, we introduce an orthogonal meta-learning paradigm to enhance BO efficiency and effectiveness by leveraging shared knowledge across similar tasks and mitigating conflicts among objectives through the discovery of orthogonal information. Finally, extensive empirical evaluations demonstrate the effectiveness of our method in optimizing uncertain multi-objectives for individual users, paving the way for more adaptive and user-focused RSs.

%Recommender systems (RSs) have become integral to our digital experiences, shaping how we access and engage with information in various domains. While early research focuses primarily on improving recommendation accuracy, this singular focus has led to unintended consequences such as echo chambers and limited user experiences. Drawing parallels with autonomous driving, we introduce a novel framework that defines five distinct levels of autonomy for RSs, ranging from simple rule-based accuracy-objective RSs to personalized behavior-based uncertain multi-objective RSs (i.e., users may have diverse needs on, e.g., \textit{accuracy}, \textit{diversity} and \textit{fairness}).  Accordingly, we propose to automatically identify and optimize multiple objectives based on individual user needs, thus functioning as more ethical and intelligent user-centric assistants. To address the challenges in exploring such uncertainty in RSs, we present a novel Bayesian optimization (BO) method to capture users' personalized preferences for different objectives and the uncertain relationships among objectives. Moreover, to improve the efficiency and effectiveness of BO learning, we design a novel orthogonal meta-learning paradigm, speeding up the optimization process by exploiting shared knowledge across similar tasks and reducing conflicts among objectives by exploring their orthogonal information. Finally, we empirically demonstrate the effectiveness of our proposed method in optimizing uncertain multi-objectives for individual users, paving the way for more user-centric RSs. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

%\ccsdesc[500]{Information systems~Recommender systems}
%\ccsdesc[500]{Computing methodologies~Neural networks}
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{Recommender Systems, Autonomy, Uncertain Multi-objective Optimization, Bayesian Optimization, Meta-Learning}


\settopmatter{printacmref=false}


%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{comment}
\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base
  seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}
\end{comment}

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

In today's digital age, recommender systems (RSs)~\cite{zhang2019deep} have become the backbone of information dissemination, revolutionizing the way we access and engage with content. These intelligent systems work tirelessly behind the scenes, analyzing our behaviors and preferences based on historical data to curate personalized information feeds tailored to our tastes and needs. From e-commerce~\cite{liu2024large} and social media~\cite{sun2024self} to education~\cite{zhang2019hierarchical} and healthcare~\cite{cui2022ketch}, RSs, widely investigated in academia and applied in industry~\cite{sun2019research}, have transformed how we discover and consume information, shaping our digital experiences and influencing our decision-making processes.

Early works on RSs mainly focus on improving recommendation accuracy~\cite{sun2023theoretically}. However, the singular focus on accuracy has inadvertently created echo chambers~\cite{yin2023understanding}, where narrowly tailored recommendations confine users to limited information spaces, stifling diversity of thought and experience. As such, more studies have considered comprehensive ethical aspects to enhance the beyond-accuracy performance of RSs~\cite{paparella2023reproducibility}, e.g., diversity~\cite{yin2023understanding}, explanation~\cite{wu2023generic} and fairness~\cite{wu2022multi}. Despite the great success, these methods suffer from a major limitation, i.e., the objectives of optimizing accuracy and beyond-accuracy performance are typically %linearly 
combined with pre-defined hyperparameters, indicating all users in RSs share the same objectives. Thus, it fails to reflect real-world complexities, where users may have diverse or uncertain requirements for RSs. For instance, some users may prioritize content diversity, while others might value fairness in their recommendations. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{image/autonomy-level-1.pdf}
    \caption{Different levels of autonomy for RSs.}
    \vspace{-0.15in}
    \label{fig:autonomy-level}
    \vspace{-0.15in}
\end{figure}

To elevate user experience and optimize AI's service to humanity, it's imperative to develop more intelligent RSs, which can autonomously adapt to individual user preferences and objectives, offering truly personalized interactions. Drawing parallels with autonomous driving~\cite{dai2024vistarag}, we first propose a novel framework that defines distinct levels of autonomy for RSs based on their ability to independently determine and pursue recommendation objectives. Overall, there are five different levels.



\begin{itemize}[leftmargin=*]
    \item \textit{Level 0: Rule-based Accuracy-objective}. RSs are entirely independent of individual user preference but built upon pre-defined or extracted rules according to the statistical interaction data, such as item popularity or association rules. %to generate recommendations. 
    \item \textit{Level 1: Profile-based Accuracy-objective}. RSs rely on static user or item profiles to generate recommendations (aka. content-based RSs~\cite{sun2019research}) by optimizing a single accuracy-oriented objective.
    %
    \item \textit{Level 2: Behavior-based Accuracy-objective}. RSs use historical personalized user behaviors to make recommendations (aka. collaborative filtering~\cite{sun2019research}), optimizing the accuracy-oriented objective. 
    \item \textit{Level 3: Behavior-based Pre-defined Multi-objectives}. RSs use historical personalized user behaviors to make recommendations that optimize pre-defined multiple (i.e., accuracy and beyond-accuracy) objectives, without considering personalized user needs.  
    \item \textit{Level 4: Behavior-based Uncertain Multi-objectives}. RSs use historical personalized user behaviors to make recommendations that optimize uncertain multiple objectives, i.e., the importance of different objectives is automatically learned by considering personalized user needs, instead of pre-defined hyperparameters.  
\end{itemize}

%\textcolor{red}{Re-write. }
In this paper, our goal is to build a more intelligent RS at Level 4, automatically modeling the importance of different objectives by considering personalized user needs to improve the overall performance of multiple objectives. Intuitively, assigning personalized weights of objectives to users is a straightforward solution to improve the overall performance of multi-objectives. 
For example, we should lower the weight of the diversity objective in multi-objective learning if a user shows a narrow interest, because blindly increasing diversity may largely harm other objectives such as recommendation accuracy. However, there remain challenges in determining the appropriate weights in multi-objective recommendation quantitatively. 

First, assigning empirical weights (e.g., measured by users' historical behaviors) can not guarantee the desired multi-objective trade-offs in RSs. For ease of illustration, let $l_{uo}(\Theta)$ and $P_{uo}(\Theta)$ denote the recommendation loss (e.g., BPR loss~\cite{sun2022daisyrec}) and the performance (e.g., NDCG~\cite{yin2023understanding}) of a specific objective $o$ (e.g., accuracy) for the user $u$, respectively.  
Specifically, even if we have $\min l_{uo}(\Theta) \leftrightarrow \max P_{uo}(\Theta)$ for each of the $O$ different objectives, 
optimization their combination with empirical weights may not guarantee the optimal performance of multi-objectives, given by, 
\begin{equation}\label{eq1}
    \min \sum\nolimits_{o=1}^{O} \lambda^{emp}_{uo} \cdot l_{uo}(\Theta) \nleftrightarrow \max\sum\nolimits_{o=1}^{O}\lambda^{emp}_{uo} \cdot P_{uo}(\Theta),
\end{equation}
where $\lambda^{emp}_{uo}$ is the empirical weight.
It mainly lies in multi-objectives %usually 
may conflict with each other and their optimization is essentially achieved by proxy losses, leading to the uncertain relationship between the assigned weights and the performance of multi-objectives. 
%
Secondly, learning trainable weights (e.g., learn weights through overall loss) 
%for multi-objective learning~\cite{ma2018modeling} 
may lead to the degradation of certain objectives, i.e., 
\begin{equation}\label{eq2}
    \min \sum\nolimits_{o=1}^{O} \lambda_{uo}(\Theta) \cdot l_{uo}(\Theta) \nleftrightarrow \max\sum\nolimits_{o=1}^{O} P_{uo}(\Theta),
\end{equation}
where $\lambda_{uo}(\Theta)$ is the trainable weight. This may lead to trivial solutions for multi-objective learning, that is, a lower loss $l_{uo}(\Theta)$ gets a larger weight $\lambda_{uo}(\Theta)$. Thus, some objectives may dominate others, resulting in imbalanced optimization and sub-optimal performance. 

According to Equations (\ref{eq1}) and (\ref{eq2}), the main difficulty lies in the uncertain relationship between the weights and overall objective in multi-objective learning, remaining the black box to determine weights in an empirical or learnable way. To open this black box for autonomous multi-objective learning in RSs, we adapt the Bayesian optimization (BO) to accommodate the personalized needs of individual users, which can efficiently explore the search space in the black box and {quantify uncertainties between the weights and overall objective}.
%
For each trail of BO, it is typically to train a multi-objective model with specific weights for overall performance measurement. To this end, we propose to accelerate and enhance the training of multi-objective model from two aspects. Firstly, to make use of the correlation between different multi-objective models  for efficient training, 
we propose to utilize meta-learning~\cite{wang2023meta} to facilitate the parameter learning for each new set of aggregation weights, leveraging the shared knowledge across similar optimization tasks. Secondly, to alleviate the conflict among different objectives for effective training, we equip meta-learning with the orthogonal gradient descent strategy to avoid the invalid updating of conflict gradients for better convergence.   

In summary, our main contributions lie three-fold. 
\begin{itemize}[leftmargin=*]
    \item We are the first to propose a novel framework that defines distinct levels of autonomy for RSs based on their ability to independently determine recommendation objectives. Meanwhile, it is also the first trial to open the black box between assigned weights and the overall performance of objectives in multi-objective learning. 
    \item  We propose a novel Bayesian optimization method by boosting \underline{B}ayesian \underline{o}ptimization with an \underline{o}rthogonal \underline{m}eta-\underline{l}earning paradigm, abbreviated as BOOML, to efficiently help optimize the uncertain multi-objective task in RSs. Specifically, it considers the collaborative signals among different multi-objective models for fast convergence and alleviates invalid updating of conflict gradients for better performance.
    \item We conduct empirical studies on three real-world datasetes to demonstrate the effectiveness of our proposed method
    in exploring the uncertain multi-objectives for individual users.   
\end{itemize}


\section{Related Works}
\subsection{RSs at Levels 0-2} 
Early RSs at \textit{Level 0} rely on generic rules or broad statistical patterns, such as recommending the most popular items, or frequently co-occurred items mined by association rules~\cite{sun2019research}, thus failing to provide personalization.
%
Later, RSs at \textit{Level 1} began leveraging static user or item profiles, aka. content-based RSs~\cite{sun2019research}, for instance, a user who indicates a preference for `romance' in their profile would receive recommendations for romantic movies. Hence, a basic level of personalization is introduced.
%
Advancing to \textit{Level 1}, RSs at \textit{Level 2} resort to dynamic historical user behaviors to learn user preference, aka. collaborative filtering based RSs~\cite{sun2019research}. Different techniques are adopted, ranging from simple matrix factorization (MF)~\cite{he2016vbpr}, to complex deep learning, e.g., MLP~\cite{sun2024self}, RNN~\cite{sun2023theoretically}, GCN~\cite{peng2024less}, Transformer~\cite{zhang2022next} and LLMs~\cite{liu2024large,wang2025re2llm}.  
%
However, RSs at Levels 0-2 aim to purely improve recommendation accuracy, ignoring other essential ethical aspects, e.g., diversity and fairness. 
 
\subsection{RSs at Level 3}
RSs at \textit{Level 3} exploit dynamic historical user behaviors to learn user preference by optimizing pre-defined multi-objectives beyond accuracy. As we primarily focus on two key ethical aspects -- diversity and fairness, we limit our discussion to research relevant to these areas. Studies on other ethical aspects, e.g., explanation and privacy-perseveration, will be explored in our future work.  

\smallskip\noindent\textit{\textbf{Diversity}} bias would cause filter bubbles, which grow along the feedback loop and inadvertently narrow user interests~\cite{khenissi2020theoretical}. Thus, a vital branch is to enhance recommendation diversity while maintaining accuracy, mainly divided into three categories: post-processing heuristic methods~\cite{steck2018calibrated,lin2022feature,sha2016framework}, determinantal point process methods~\cite{chen2018fast,wu2019pd,gan2020enhancing,liu2022determinantal,warlop2019tensorized} and end-to-end learning methods~\cite{zheng2021dgcn,liang2021enhancing,chen2020improving,chen2021multi,shi2024diversifying,yang2023dgrec,wang2019modeling,cen2020controllable,lu2021future,wang2024sparks,stamenkovic2022choosing,liu2023generative}. However, they suffer from different limitations: (1) some follow a two-stage paradigm, i.e., train offline models to score items on accuracy and then re-rank items considering diversity; and (2) others incorporate accuracy and diversity objectives with a pre-defined ``trade-off'' hyperparameter, overlooking the uncertainty of personalized user needs. 

\smallskip\noindent\textit{\textbf{Fairness}} is another critical ethical issue of RSs~\cite{wang2023survey,li2023fairness,deldjoo2024fairness} that can affect personal experience and social good since RSs serve a resource allocation role in society by allocating information to users and exposure to items.
Extensive work has encouraged equal exposure across item groups partitioned by item features, such as category and popularity\footnote{
There are different types of fairness in RSs, e.g., user fairness, item fairness, and joint fairness~\cite{wang2023survey}. In this study, we primarily focus on item fairness regarding popularity without relying on extra item features.}. 
Early studies design {data-oriented methods}~\cite{ekstrand2018all} to alleviate the unfairness issue by changing training data. 
Another branch focuses on {re-ranking based methods}~\cite{liu2019personalized,steck2018calibrated} to adjust the outputs of recommendation models to promote fairness. 
Recent studies propose {ranking-based methods} to improve fairness by (1) using linear programming to add fairness constraints~\cite{singh2018fairness}; (2) adding a fairness-related regularization term to the recommendation loss~\cite{beutel2019fairness,zhu2018fairness}; 
(3) leveraging adversarial learning to learn fair representations or predicted scores~\cite{bose2019compositional,wu2021learning,zhu2020measuring}; (4) adopting reinforcement learning to achieve long-term fair recommendations~\cite{ge2021towards}; and 
(5) balancing accuracy and fairness for various stakeholders with heuristic strategies~\cite{wu2021tfrom,patro2020fairrec} or Pareto optimality guarantee~\cite{wu2022multi,ge2022toward}. 
Despite the effectiveness, most of them mainly seek a uniform ``trade-off" between accuracy and fairness across all users while ignoring personalized user needs.  

\subsection{RSs at Level 4}
Some studies attempt to achieve RSs at \textit{Level 4}. For instance, in \cite{wang2022user}, the authors propose a new recommender prototype called User Controllable RS, which enables users to
actively control the mitigation of filter bubbles. Nevertheless, it relies on user feedback and only considers the balance between accuracy and diversity. 
MMoE~\cite{ma2018modeling} adapts the Mixture-of-Experts structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network to optimize each task. However, it only learns the gates at the task level instead of the individual user level. A recent work on arXiv~\cite{li2024deep} introduces a deep Pareto reinforcement learning model for multi-objective RSs, which accounts for the relationships between different objectives and implements personalized dynamic weighting for these objectives. However, it still relies on learning trainable weights for multiple objectives, leading to the degradation of certain objectives. Besides, it ignores the potential conflicts of different objectives and introduces substantial computational complexity due to dynamically adjusting objective weights based on individual user information. 
% Besides, key technical details, e.g., the method for setting rewards, are not provided, making the model challenging to understand and reproduce. 

\input{table/empirical-weights}




\section{Uncertain Multi-Objectives}
This section first introduces different objectives in RSs by considering accuracy and different ethics, followed by the formulation of our uncertain multi-objective function.
In this paper, we focus on three objectives without loss of generality, including accuracy, diversity, and fairness. Note that, our framework can be easily adopted and adapted to more objectives.



\smallskip\noindent\textbf{Notations}. Let $\mathcal{U} = \{u_1, u_2, \ldots, u_{|\mathcal{U}|}\}$, $\mathcal{V} = \{v_1, v_2, \ldots, v_{|\mathcal{V}|}\}$ and $\mathcal{C} = \{c_1, c_2, \ldots, c_{|\mathcal{C}|}\}$ denote the user, item and item category sets, respectively. 
$\bm{R} \in \mathbb{R}^{|\mathcal{U}| \times |\mathcal{V}|}$ denotes the user-item interaction matrix, where its entries $r_{ij}=1$ represents user $u_i$ interacted with item $v_j$; otherwise 0.
For each item $v_j$, it has a categorical feature $c(v_j) \in \mathcal{C}$. 
To model users and items in the latent space, we embedding them into the user representation matrix  $\bm{U} \in \mathbb{R}^{|\mathcal{U}| \times d}$ and the item representation matrix $\bm{V} \in \mathbb{R}^{|\mathcal{V}| \times d}$, where $d$ is dimension of the latent space. 

\smallskip\noindent\textbf{Problem Statement}. Given the user-item interaction $\mathbf{R}$, our goal is to provide a personalized recommendation list (RL) with the ranking of $K$ items to each user, aiming to better hit her preference while meeting her personalized requirements regarding different ethical aspects, e.g., diversity and fairness. 

\subsection{Different Objectives and Metrics}
\label{sec:objectives}
\smallskip\noindent\textbf{Accuracy Objective}. The primary goal of RSs is to provide accurate recommendations to hit user preference (e.g., ground-truth interacted items). The accuracy can be measured with widely-used ranking metrics, e.g., Precision, Recall, and NDCG~\cite{sun2022daisyrec}. In our study, we adopt NDCG as the evaluation metric, denoted as ACC, as it evaluates whether (1) the target items are correctly recommended and (2) the correctly recommended items are top-ranked. Larger values of NDCG indicate better ranking accuracy. 

\smallskip\noindent\textbf{Accuracy Optimization}. We adopt the BPR loss~\cite{sun2022daisyrec} to maximize the preference gap between positive and negative items for all users, 
%
\begin{equation}
\small
f_{\text{acc}}(\bm{\Theta}) = - \sum\nolimits_{(u_i,v_j,v_k) \in \mathcal{D}_T} \log \sigma(\hat{r}_{ij} - \hat{r}_{ik}),
\end{equation}
%
where $\hat{r}_{ij} = \bm{u}_i^T\bm{v}_j$ is the estimated preference score of user $u_i$ to item $v_j$; $\bm{u}_i$ and $\bm{v}_j$ denote the encoding of user $u_i$ and item $v_j$, respectively; $\mathcal{D}_T$ denotes the training set meaning $u_i$ engaged $v_j$ instead of $v_k$, i.e., $r_{ij}=1$ and $r_{ik}=0$; and $\sigma(x) = {1}/(1 + \exp(-x))$ is the sigmoid function.

\smallskip\noindent\textbf{Diversity Objective}. 
To alleviate filter bubbles \cite{wang2024sparks}, it is necessary to provide diversified recommendations rather than focusing narrowly on specific categories of items. Typically, the recommendation diversity can be measured with pairwise diversity metrics, e.g., ILD (intra-list distance), entropy-and-diversity score~\cite{yin2024simple}. In our method, we adopt ILD to measure the average Euclidean distance between every pair of items
in the RL, i.e.,
%
% \begin{equation}\small
%       DIV =\frac{1}{|\mathcal{U}|}\sum\nolimits_{u_i\in\mathcal{U}}\sum\nolimits_{(v_j, v_k)\in RL_{u_i}, v_j\neq v_k}\frac{||\bm{c}(v_j) - \bm{c}(v_k)||_2}{\vert RL_{u_i}\vert\times(\vert RL_{u_i}\vert-1)}
% \end{equation}
\begin{equation}\small
      DIV =\frac{1}{|\mathcal{U}|}\sum\nolimits_{u_i\in\mathcal{U}}\sum\nolimits_{(v_j, v_k)\in RL_{u_i}, v_j\neq v_k}\frac{||\bm{v}_j - \bm{v}_k||_2}{\vert RL_{u_i}\vert\times(\vert RL_{u_i}\vert-1)},
\end{equation}
%
where $RL_{u_i}$ denotes the recommendation list (RL) for user $u_i$.
%and $\bm{c}(v_j) \in \mathbb{R}^{d}$ denotes the embedding of the categorical feature of item $v_j$. 
A larger value of IDL indicates a more diverse result in the RL.

\smallskip\noindent\textbf{Diversity Optimization}. In our study, we propose to maximize the diversity measured by the negative entropy of estimated category probability distribution for all users as in~\cite{yin2024simple},
\begin{equation}
\small
f_{{div}}(\bm{\Theta}) = -\sum\nolimits_{u_i\in \mathcal{U}} Entropy(\hat{p}_i) = \sum\nolimits_{u_i\in \mathcal{U}}\sum\nolimits_{l=1}^{|\mathcal{C}|} \hat{p}_{il} \log \hat{p}_{il},
\end{equation}
%
where $\hat{p}_i$ is the estimated category probability distribution for user $u_i$, satisfying $\sum_l \hat{p}_{il} = 1$; and $\hat{p}_{il}$ denotes user $u_i$'s preference towards category $c_l$. 
Specifically, $\hat{p}_{il}$ can be estimated by aggregating $u_i$'s preference towards all items belonging to category $c_l$, 
\begin{equation}
\small
     \hat{p}_{il} = Softmax\left(\sum\nolimits_{v_j\in \mathcal{V}} \mathbb{I}(c(v_j) =c_l)\cdot \hat{r}_{ij}\right),
\end{equation}
%
\noindent where $ \mathbb{I}(\cdot)$ denotes the indicator function. The Softmax function making it a probability distribution, ensuring non-negativity $\hat{p}_{il}\ge0$ and $\sum_l \hat{p}_{il} = 1$ for $\hat{p}_i$.

\begin{figure}[t]
    \centering
    \subfigure[Total Loss]{
        \includegraphics[width=0.31\linewidth]{image/total_loss}
    }
    \hspace{-0.05in}
    \subfigure[Weights]{
        \includegraphics[width=0.31\linewidth]{image/weights}
    }
    \hspace{-0.05in}
    \subfigure[Separate Loss]{
        \includegraphics[width=0.31\linewidth]{image/separate_loss}
    }
    \vspace{-0.2in}
    \caption{Performance of trainable weights on Games. Similar trends are noted on other datasets in our study.}
    \label{fig:sgd-performance}
    \vspace{-0.1in}
\end{figure}


\smallskip\noindent\textbf{Fairness Objective}. Fairness aims to ensure the recommendation results are not dominated by popular products but include long-tail items~\cite{gupta2021causer}. The recommendation fairness regarding popularity can be measured by several metrics, e.g., ARP (Average Recommendation Popularity)~\cite{gupta2021causer}, RR (Recommendation Rate)~\cite{zhang2021causal}, and PR (Popularity Rate)~\cite{ge2021towards}.  For generality, we use ARP to measure the average popularity of the recommended items, i.e.,
%
\begin{equation}
\small
    FAIR = \frac{1}{|\mathcal{U}|}\sum\nolimits_{u_i\in\mathcal{U}}\sum\nolimits_{v_j \in RL_{u_i}} \frac{\phi(v_j)}{\vert RL_{u_i}\vert},
\end{equation}
%
\noindent where $\phi(v_j)$ represents the popularity of item $v_j$. 
% $\phi(v_j) = \sum_{i=1}^{|\mathcal{U}|}r_{ij}/|\mathcal{U}|$ represents the popularity of item $v_j$. 
Smaller values of ARP indicate fairer recommendation results.


\smallskip\noindent\textbf{Fairness Optimization}. Intuitively, since popular items are more frequently interacted with by users, their representations are likely to be pulled closer to user representations during the model training process, leading to systematic higher scores. Inspired by Biased-MF~\cite{koren2009matrix}, we propose to remove such bias by minimizing the gap between the estimated preference score of individual users over individual items and the estimated average score of the system, 
%
\begin{equation}
\small
    f_{{fair}}(\bm{\Theta}) = \frac{1}{|\mathcal{U}||\mathcal{V}|} \sum\nolimits_{i=1}^{|\mathcal{U}|}\sum\nolimits_{j=1}^{|\mathcal{V}|} \left|\sigma(\hat{r}_{ij}) -\overline{r}\right|,
\end{equation}
%
where $\overline{r} = \sum\nolimits_{i=1}^{|\mathcal{U}|}\sum\nolimits_{j=1}^{|\mathcal{V}|} \sigma(\hat{r}_{ij})/{(|\mathcal{U}|\cdot|\mathcal{V}|})$ is the average predicted score for all users towards all items.  


\subsection{Uncertain Multi-Objectives}\label{subsec:uncertain-objectives}
In this paper, we aim to improve the overall performance of multi-objectives, while keeping validation of each objective, i.e.,
%
\begin{equation}\label{goal}
\small
\begin{aligned}
    &\max_{\bm{\Theta}} g(ACC,DIV,FAIR), \\
    \textit{s.t.} \quad ACC&>\tau_{acc}, DIV>\tau_{div},FAIR<\tau_{fair},
\end{aligned}   
\end{equation}
where $g(\cdot)$ denotes the overall performance of multiple objectives; and $\tau_{acc}$, $\tau_{div}$, and $\tau_{fair}$ represent the thresholds of minimal requirement for accuracy, diversity, and fairness objectives, respectively.  

In real-world scenarios, users may have diverse or uncertain requirements in RSs, leading to varying importance in optimizing multiple objectives for different users. For example, if a user shows a narrow interest in items, blindly increasing recommendation diversity may largely harm other objectives such as recommendation accuracy. To this end, we propose to optimize the personalized multi-objectives to capture users' uncertain requirements in RSs, enabling RSs to function as more ethical and intelligent user-centric assistants. Specifically, we assign personalized weights for different objective losses for multi-objective optimization to improve the overall performance of multi-objectives,
%
\begin{equation}\label{equ:uncertain-multi-objective}
\small
    \mathcal{F}(\bm{\lambda}, \bm{\beta}) = \sum\nolimits_{u_i \in \mathcal{U}}[f_{acc}(\bm{\Theta}_i) + \lambda_i f_{div}(\bm{\Theta}_i) + \beta_i f_{fair}(\bm{\Theta}_i)], 
\end{equation}
%
where $\lambda_i$ and $\beta_i$ are the personalized weights of diversity and fairness objectives for $u_i$.
However, challenges persist in quantitatively determining the appropriate weights using existing methods.
%However, there remain challenges to determine the appropriate weights with existing methods quantitatively. 

\smallskip\noindent\textbf{Why Not Empirical Weights}?
Assigning empirical weights via the grid search for different objectives~\cite{chen2020improving,wang2024sparks,beutel2019fairness,zhu2018fairness}
has been widely used in multi-objective learning due to its simplicity %and utility 
for RSs at Level 3. However, for RSs at Level 4, the scale of grid search is exponential to the size of objectives and users, leading to unacceptable costs in the training phase. Worsely, it is intractable to clarify the certain relationship between weights and multi-objective performance through empirical investigation. Table~\ref{tab:empirical-weights} illustrates the impact of weight changes on multi-objective performance (MF as encoder), where we apply a grid search in $\{0.1, 0.5, 1.0, 5.0, 10\}$ for the weights of diversity ($\lambda$) and fairness ($\beta$). The optimal performance for different objectives is highlighted in different colors.   
We observe that the optimal performance for diversity (in blue) is achieved with a smaller weight on diversity ($\lambda=0.1$) but a larger weight on fairness ($\beta=10$).
This indicates \textit{the uncertain relationship between weights and performance of multi-objectives makes it hard to find the optimal weights for multi-objective learning.}


\smallskip\noindent\textbf{Why Not Trainable Weights}?
Some methods attempt to learn trainable weights that aggregate multiple objectives for unified learning~\cite{li2024deep}, however, it may lead to the degradation of certain objectives. For example, a trivial solution that assigns a lower loss %$l_k(\Theta)$
with a larger weight,
%$\lambda_k(\Theta)$, 
results in imbalanced optimization and sub-optimal performance. Figure~\ref{fig:sgd-performance} depicts the multi-objective performance of learning trainable weights with SGD using MF as encoder across different training epochs on Amazon-Games. We note that the total loss decreases rapidly until reaching a small value as in Figure~\ref{fig:sgd-performance}(a), indicating the optimization process converges. However, the weights for accuracy and fairness drop significantly as the epochs increase shown in Figure~\ref{fig:sgd-performance}(b), leading to diversity dominating the optimization process, i.e., only the loss for diversity decreases to a small value, whereas the losses for accuracy and fairness remain relatively high as shown in Figure~\ref{fig:sgd-performance}(c). This validates that \textit{learning trainable weights with SGD cannot adequately balance different objectives to achieve optimal recommendation performance}. 


\section{Bayesian Optimization Boosted via Orthogonal Meta-Learning}
Guided by the analysis in Section~\ref{subsec:uncertain-objectives}, it is hard to determine optimal weights for multi-objective learning through empirical investigation or direct optimization. To explore the uncertain relationships between the weights and multiple objectives, we propose a novel Bayesian optimization method to open the black box that achieves balanced optimization among different objectives and bridges the gap between objective losses and performances. Most importantly, for more efficient and effective optimization, we design an orthogonal meta-learning paradigm to enhance the optimization of each objective by considering their correlations and potential conflicts.


\subsection{Bayesian Optimization for Group-Level Personalization}


\subsubsection{Group-Level Uncertain Multi-Objectives}

Recall Equation~(\ref{equ:uncertain-multi-objective}), it is impractical to directly leverage Bayesian optimization to find out the optimal $\lambda_i$ and $\beta_i$ for each user $u_i$, as the search space is huge due to the large volume of users in RSs. To this end, we allocate users into different groups based on the statistics of their behaviors, as similar users may share a similar need (e.g., tendency toward diversity and fairness) for items. Specifically, we utilize three kinds of user behavior statistics, including the total number of engaged items, the ratio of engaged categories to items, and the average popularity of engaged items, which could reflect users' preferences towards diversity and fairness of recommendation results. Thus, we cluster users into $W$ different groups ($\{\mathcal{G}_1,\cdots,\mathcal{G}_W\}$) based on these statistical features. For each group $\mathcal{G}_w$,  we assign a personalized parameter pair $(\lambda_w,\beta_w)$ for multi-objective learning. Accordingly, the group-level uncertain multi-objective function is given by: 
%
% \label{equ:group-objective}
\begin{equation}
\small
\begin{aligned}
    &\mathcal{F}^{\bm{\lambda}, \bm{\beta}}(\bm{\Theta}) =  \sum\nolimits_{i=1}^{|\mathcal{U}|}\mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\bm{\Theta_i}), \\
   & \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\bm{\Theta_i}) = \sum\nolimits_{w=1}^{W}\mathbb{I}(u_i\in \mathcal{G}_w)\cdot[f_{acc}(\bm{\Theta}_i) +  \lambda_w f_{div}(\bm{\Theta}_i) + \beta_w f_{fair}(\bm{\Theta}_i)],
\end{aligned}
\end{equation}
%
where $\mathbb{I}(u_i\in \mathcal{G}_w)$ aims to select $\lambda_w$ and $\beta_w$ for user $u_i$; $\bm{\lambda} = [\lambda_1,\cdots,\lambda_W]$ and $\bm{\beta} = [\beta_1,\cdots,\beta_W]$; and {$\bm{\Theta} = [\Theta_1,\cdots,\Theta_{|\mathcal{U}|}]$}, where $\Theta_i$ denotes the learnable parameters related to user $u_i$ and her engaged items.
Hence, our goal is to find out the optimal $\lambda_w$ and $\beta_w$ for each group $\mathcal{G}_w$, thus satisfying users' uncertain requirements regarding various ethical aspects at the group-level. 


\subsubsection{Bayesian Optimization}\label{sec:BO}


For optimal weights $\bm{\lambda}$ and $\bm{\beta}$, we formulate Equation (\ref{goal}) as a Bayesian optimization (BO) problem,
%
\begin{equation}
\small
\begin{aligned}
    \max_{\bm{\lambda,\beta}} g(ACC(\Theta^{\bm{\lambda,\beta}}),&DIV(\Theta^{\bm{\lambda,\beta}}),FAIR(\Theta^{\bm{\lambda,\beta}})) - \kappa\cdot const(\Theta^{\bm{\lambda,\beta}}),\\
    & \Theta^{\bm{\lambda,\beta}} = \min_\Theta \mathcal{F}^{\bm{\lambda}, \bm{\beta}}(\Theta),
\end{aligned}
\end{equation}
%
where $\Theta^{\bm{\lambda,\beta}}$ denotes the solution of multi-objective function $\mathcal{F}^{\bm{\lambda}, \bm{\beta}}(\Theta)$ with weights $\bm{\lambda}$ and $\bm{\beta}$. The soft constraint $const(\Theta^{\bm{\lambda,\beta}}) = \lfloor\tau_{acc}-ACC(\Theta^{\bm{\lambda,\beta}})\rfloor_+ + \lfloor\tau_{div}-DIV(\Theta^{\bm{\lambda,\beta}})\rfloor_+ + \lfloor FAIR(\Theta^{\bm{\lambda,\beta}}) - \tau_{fair}\rfloor_+$ penalize the unsatisfied constraints in Equation (\ref{goal}) with a penalty coefficient $\kappa>>0$. For the function $g(\cdot)$, we define the overall performance of multiple objectives in two ways:

\begin{itemize}[leftmargin=*]
    \item \textit{Rescaled Sum}. It seeks the maximal sum of different objectives. 
    However, measuring objectives with different metrics usually has different scales, e.g., $NDCG$ $\in [0,1]$, whereas $ILD$ may be larger than 1 and $ARP$ possess the opposite trend with $NDCG$ and $ILD$ (i.e., smaller ARP values indicate fairer recommendation). To this end, we adopt the rescaled sum to formulate 
    $g(NDCG,ILD,ARP) = NDCG + \sigma(ILD) + \sigma(1/ARP)$.   
    \item \textit{Harmonic Mean}. It seeks the maximal harmonic mean of different objectives. Considering the opposite trend of $ARP$ compared with $NDCG$ and $ILD$, we, therefore, formulate the harmonic mean of these three metrics as $g(NDCG,ILD,ARP)  = 3/[NDCG^{-1} + \sigma(ILD)^{-1} + \sigma(1/ARP)^{-1}]$. 
\end{itemize}

Following the standard procedure of BO, we iteratively update a surrogate model to approximate the objective function \( g(\cdot) \) and guide the search for optimal weights %$\bm{\lambda} = [\lambda_1,\cdots,\lambda_W]$ and $\bm{\beta} = [\beta_1,\cdots,\beta_W]$.
$\bm{\lambda}$ and $\bm{\beta}$.
Specifically, the procedure includes the following steps:
We start by selecting an initial set of points (where a point is a combination of $\bm{\lambda}, \bm{\beta}$) and evaluate the objective function. A Gaussian process surrogate model is then fitted to approximate the objective. We adopt expected improvement as an acquisition function $EI(\cdot)$ to balance exploration (searching unexplored regions) and exploitation (refining known promising areas), where points with high expected improvement are more likely to be sampled
as the next candidate point added to the training data. This process iterates, refining the surrogate model and optimizing the acquisition function, until a convergence criterion is met or the search budget is exhausted.


\subsection{Orthogonal Meta-Learning for Efficient and Effective Optimization}


Each acquisition in BO requires a whole process of multi-objective learning, leading to high cost if each acquisition is conducted independently. To this end, we propose an efficient and effective training optimization for two aspects, namely meta optimization and orthogonal gradient descent. The meta optimization can reduce the times of gradient updating by exploiting shared knowledge across similar tasks, leading to efficient optimization to a new task. The orthogonal gradient descent can alleviate the conflict among different objectives, therefore further improving the effectiveness of the meta optimization.

\input{table/algorithm}

\subsubsection{Meta Optimization}
To optimize the model effectively, we integrate group correlation and collaborative information into the meta optimization process, enabling the model to generalize better across users by leveraging shared patterns. Specifically, the meta optimization process involves two critical steps: inner loop optimization and outer loop validation, designed to achieve fast adaptation and balance between objectives. To optimize model parameters and validate performance, we divide behaviors of user $u_i$ into a support set $\mathcal{S}_i$ and a query set $\mathcal{Q}_i$.
 
\textbf{For inner loop optimization (support set training)}, we optimize the parameters $\bm{\Theta}_i$ on the support set ($\mathcal{S}_i$) by minimizing the group-level multi-objective loss for each user $u_i$. The updated parameters are computed as:
% \begin{equation}
% \small
%     \bm{\Theta}_i' = \bm{\Theta}_i - \eta_1 \nabla_{\bm{\Theta}_i} \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i),
% \end{equation}  
% \begin{equation*}
%      \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(S_i, \bm{\Theta}_i) = f_{acc}(S_i,\bm{\Theta}_i) +  \lambda_w f_{div}(S_i,\bm{\Theta}_i) + \beta_w f_{fair}(S_i,\bm{\Theta}_i),
% \end{equation*}
\begin{equation}
\small
\begin{aligned}
    &\bm{\Theta}_i' = \bm{\Theta}_i - \eta_1 \nabla_{\bm{\Theta}_i} \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i), \\
     \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i) = &f_{acc}(\mathcal{S}_i,\bm{\Theta}_i) +  \lambda_w f_{div}(\mathcal{S}_i,\bm{\Theta}_i) + \beta_w f_{fair}(\mathcal{S}_i,\bm{\Theta}_i),
\end{aligned}
\end{equation}
%
where $\eta_1$ is the learning rate, and $\mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i)$ represents the multi-objective loss function for the support set of user $u_i$. This step leverages group-level personalized weights $(\lambda_w, \beta_w)$ to capture user-specific multi-objective preferences.

\textbf{For outer loop validation (query set evaluation)}, we evaluate the model's generalization based on the meta-loss on the query set ($\mathcal{Q}_i$) using the updated parameters $\bm{\Theta}_i'$:
%
\begin{equation}
\small
    \mathcal{L}_{\text{meta}, i} = \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{Q}_i, \bm{\Theta}_i - \eta_1 \nabla_{\bm{\Theta}_i} \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i)).
\end{equation}
%
By comparing the performance across several users, shared patterns can be identified for ensuring that the model effectively leverages group and collaborative information.

To balance optimization across all users, we aggregate their meta-loss, i.e., $\mathcal{L}_{\text{meta}} = \sum\nolimits_{i=1}^{\vert\mathcal{U}\vert} \mathcal{L}_{\text{meta}, i}$.
Finally, the global parameters $\bm{\Theta}$ are updated to improve the modelâ€™s performance across all tasks,
%
\begin{equation}
\small
    \bm{\Theta}_i = \bm{\Theta}_i - \eta_2 \nabla_{\bm{\Theta}_i} \mathcal{L}_{\text{meta}}.
\end{equation}


\subsubsection{Orthogonal Gradient Descent}
As users have different objectives within each group, we aim to alleviate conflicts among objectives and improve the effectiveness of meta-learning. 
For instance, increasing diversity may exacerbate fairness, that is, RSs may recommend more popular items in each category~\cite{wang2023survey}. 
To address this issue, we introduce an orthogonal gradient approach to alleviate the conflict among different objectives for outer loop gradient updating, involving gradient computation and adjustment by PCGrad~\cite{yu2020gradient}. First, we calculate the gradient for each objective, 
%
% \begin{equation}
% \small
%     g_{obj}= \nabla f_{obj}(\mathcal{Q}_i,\bm{\Theta}_i - \eta_1 \nabla_{\bm{\Theta}_i} \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i)),
% \end{equation}
\begin{equation}
\small
    \bm{g}_{o_m}= \nabla f_{o_m}(\mathcal{Q}_i,\bm{\Theta}_i - \eta_1 \nabla_{\bm{\Theta}_i} \mathcal{F}_i^{\bm{\lambda}, \bm{\beta}}(\mathcal{S}_i, \bm{\Theta}_i)),
\end{equation}
%
where $o_m\in\{acc,div,fair\}$. Then, we detect conflicts between pairs of task gradients  $\bm{g}_{o_m}$ and $\bm{g}_{o_n}$ (e.g., $\bm{g}_{acc}$ and $\bm{g}_{div}$) by comparing their inner product, i.e., $\langle \bm{g}_{o_m}, \bm{g}_{o_n} \rangle < 0 \Rightarrow \text{conflict between } \bm{g}_{o_m} $ and $\bm{g}_{o_n}$.
%
When conflicts are detected, we adjust $\bm{g}_{o_m}$ by projecting it onto the plane that is orthogonal to the conflict direction:
%
% \begin{equation}
% \small
%     \tilde{g}_{obj} = g_{obj} - \sum_{j\neq {obj}}\frac{min(\langle g_{obj}, g_j \rangle,0)}{\|g_j\|^2} g_j,
% \end{equation}
\begin{equation}
\small
    \tilde{\bm{g}}_{o_m} = \bm{g}_{o_m} - \sum_{o_n\neq {o_m}}\frac{min(\langle \bm{g}_{o_m}, \bm{g}_{o_n} \rangle,0)}{\|\bm{g}_{o_n}\|^2} \bm{g}_{o_n},
\end{equation}
%
where $\min(\langle \bm{g}_{o_m}, \bm{g}_{o_n} \rangle,0)$ aims to select conflict vectors ($\langle \bm{g}_{o_m}, \bm{g}_{o_n} \rangle < 0$). Finally, we update outer loop meta-loss for user $u_i$ based on the orthogonal gradient, i.e., $\bm{\Theta}_{i} = \bm{\Theta}_{i} - \eta_2 \cdot \sum_{o_m}\tilde{\bm{g}}_{o_m}$. In summary, Algorithm~\ref{alg:booml} illustrates the whole optimization process of our BOOML.


\section{Experiments and Analysis}\label{sec:experiments}
We conduct extensive experiments on three real-world datasets to verify the efficacy of our proposed method {BOOML} by answering the following four research questions\footnote{Our code is available at \url{https://anonymous.4open.science/r/BOOML-2A75}}:
\begin{itemize}[leftmargin=*]
    \item[] \textbf{RQ1}: How does BOOML perform compared with state-of-the-art (SOTA) multi-objective recommendation approaches?
    \item[] \textbf{RQ2}: How do different components of BOOML affect its performance regarding effectiveness and efficiency? 
    \item[] \textbf{RQ3}: How does BOOML perform across different user groups? 
    \item[] \textbf{RQ4}: How do essential hyper-parameters affect the performance of our proposed BOOML?
\end{itemize}

% (\textbf{RQ1}) How does BOOML perform compared with SOTA multi-objective recommendation approaches?
% (\textbf{RQ2}) How do different components of BOOML affect its performance regarding effectiveness and efficiency? 
% (\textbf{RQ3}) How does BOOML perform across different user groups? 
% (\textbf{RQ4}) How do essential hyper-parameters affect the performance of BOOML?

\subsection{Experimental Setup}

\input{table/statistics}
\input{table/v3_comparative_results}


\subsubsection{Datasets} We adopt three real-world datasets with varying domains, sizes, and sparsity levels collected from Amazon.com~\cite{ni2019justifying}, including Games, Electronics, and Movies. The datasets contain users' ratings on the scale of $[1,5]$ stepped by 1 towards products in the three domains. Following~\cite{sun2022daisyrec}, we convert the interactions with ratings no less than 4 as positive feedback; otherwise negative feedback. Besides, we filter out users and items with less than 10 interactions. Table~\ref{tab:statistics} shows the statistics of the three datasets after pre-processsing. Finally, each dataset is chronologically split into training, validation, and test sets in a 6:2:2 ratio. 


\subsubsection{Evaluation Metrics} As introduced in Section~\ref{sec:objectives}, we adopt the widely-used NDCG@K, ILD@K, and ARP@K to evaluate the performance of accuracy, diversity, and fairness, respectively.  
Additionally, we also adopt the Rescaled Sum (ResSum@K) and Harmonic Mean (HarMean@K) as defined in Section~\ref{sec:BO}, to evaluate the comprehensive performance of all methods. 
In particular, larger NDCG and ILD, ResSum, and HarMean values indicate better performance, whereas smaller ARP values suggest fairer recommendations.
We set $K=\{20, 50\}$ in our study empirically.

\subsubsection{Baselines} We compare with six SOTA multi-objective RSs at Levels 3-4. Specifically, 
\textbf{DGRec}~\cite{yang2023dgrec} is a diversifying GNN-based RS at Level 3, which directly improves the embedding generation procedure for diversified recommendations. 
\textbf{SMORL}~\cite{stamenkovic2022choosing} is a reinforcement learning based RS at Level 3, which augments recommenders with additional neural layers to optimize three objectives: accuracy, diversity, and novelty.  
\textbf{GFN4Rec}~\cite{liu2023generative} is a generative RS at Level 3, which aims to learn a policy that can generate sufficiently diverse item lists for users while maintaining high recommendation quality. 
\textbf{FairRec}~\cite{patro2020fairrec} is a scalable and adaptable RS at Level 3, which ensures uniform fairness for products by setting the minimum exposure, and fairness for users using a greedy strategy. 
\textbf{TFROM}~\cite{wu2021tfrom} is a post-processing RS at Level 3, which designs heuristic algorithms to ensure two-sided fairness at the cost of reduced recommendation quality.  
\textbf{MMoE}~\cite{ma2018modeling} is a generic multi-objective RS at Level 4 that can optimize accuracy, diversity, and fairness using Mixture-of-Experts, explicitly learning to model task relationships.

{As our BOOML and most baseline methods (i.e., SMORL, GFN4Rec, FairRec, and TFROM) need to be built on existing user and item encoders, we further choose two representative encoders to verify their generality, including non-graph-based encoder \textbf{MF}~\cite{sun2022daisyrec} and graph-based encoder \textbf{LGCN}~\cite{he2020lightgcn}.}

\input{table/v3_ablation_effectiveness}

\subsubsection{Implementation Details}
We empirically find out the optimal settings for essential hyper-parameters of each method according to the performance on the validation set. For all encoders, the batch size is set as 1024 and the embedding size is set as 64 for fair comparison. The learning rate is searched in $\{1e-1, 1e-2, 1e-3\}$, and set as 1e-3. The optimizer is searched from AdamW and SGD, where the best option is SGD for MF, while AdamW for LGCN. For the number of layers in LGCN, we search in scale $\{2,3,4\}$ and set as 2 because it shows the best performance. 

Regarding the multi-objective baselines, their hyper-parameters are searched and set as follows. 
For SMORL, the discount factor $\gamma=0.9$; the objective-balancing weight vector $\bm{w}=(1,1,1)$; the weight $\alpha$ to control the influence of SMORL is searched in $\{0.5, 1, 1.5, 2\}$ and set as 2. 
For GFN4Rec, as suggested by the paper, we set $b_z=1$; $b_r$ and $b_f$ are respectively searched in $\{0.1, 0.3, 1, 1.5\}$ and $\{0.1, 0.5, 1, 1.5, 2\}$; and the optimal settings are $b_r = 1.5$ and $b_f=1$.
For FairRec, the importance of fairness objective ($\alpha$) is searched in $[0.1, 0.9]$ stepped by 0.2, and the best option is 0.5. 
For DGRec, the learning rate is searched in $\{1e-1,1e-2,1e-3\}$ and set as $1e-1$; the number of GNN layers is searched in $[1,2,3]$ and set as 2; and the weight to control popular categories ($\beta$) is searched in $[0.9, 0.95]$ stepped by 0.01 and set as 0.93.  
For MMoE, the dropout rate is searched in $[0.1, 0.5]$ stepped by 0.1, and set as 0.2; the number of experts is searched in $[2,3,4,5]$ and set as 4; the number of layers is 2 and the hidden units per expert are $\{64,32\}$.
For our BOOML, the number of initial points is 10; the trials of BO is 50; the function $g(\cdot)$ adopts rescaled sum; $\kappa=0$ for simplicity; the inner learning rate and outer learning rate of meta-learning are searched in $\{1e-1,1e-2,1e-3\}$ and both set as $1e-2$; the meta-learning epoch is searched in $\{1,2,3,4,5\}$, and the optimal option is 5 for MF and  1 for LGCN; $W$ is searched in $\{2, 3, 4, 5\}$ and set as 3; and $\bm{\lambda}$ and $\bm{\beta}$ are searched in the range of $[0.01, 10]$.


\subsection{Results and Analysis} 

\subsubsection{Comparative Results (RQ1)} Table~\ref{tab:performance} presents the performance of all methods. Several major observations are noted. 
% To make the table notable, we bold the best results and underline the best baseline results for each dataset with one specific evaluation metric. `\textit{Improvement}' indicates the relative improvements of BOOML over the strongest baseline on overall performance of multi-objectives (i.e., ResSum and HarMean), because measuring on specific-objective metric (i.e., NDCG, ILD, and ARP) may lead to unfair comparison. 



%
\begin{itemize}[leftmargin=0.3cm]
    \item[-] \textbf{First}, across all encoders, our BOOML demonstrates a positive \textit{improvement} on ResSum and HarMean in all cases compared to baseline methods. This highlights BOOML's superiority to balance multi-objective performance by leveraging orthogonal meta-learning to alleviate conflicts among different objectives.
    \item[-] \textbf{Second}, BOOML achieves better performance on accuracy, measured by NDCG, across all cases. However, its performance on diversity and fairness, measured by ILD and ARP, is worse than the best-performing baselines. This is attributed to some baselines focusing only on specific objectives while largely sacrificing other objectives. For example, the fairness-oriented methods TFROM-MF and TFROM-LGCN perform exceptionally well in ARP but significantly undermine both accuracy and diversity. Particularly, it consistently produces the worst NDCG in all cases compared with other methods.
    The diversity-oriented methods GFN4Rec-MF and GFN4Rec-LGCN perform well in ILD but compromise both accuracy and fairness.
    \item[-] \textbf{Third}, diversity-oriented baselines (e.g., SMORL and GFN4Rec) generally outperform fairness-oriented baselines (e.g., FairRec and TFROM) in terms of ILD across most cases. Conversely, fairness-oriented baselines defeat diversity-oriented ones regarding ARP. Additionally, the diversity-oriented method DGRec consistently surpasses the generic multi-objective baseline MMoE in both NDCG and ILD but performs worse on ARP, but DGRec achieves better overall performance than MMoE, suggesting its stronger ability to balance multiple objectives.
    \item[-] \textbf{Lastly}, different baselines show varying sensitivity to encoders across all metrics. For example, in the aspect of accuracy, BOOML and SMORL are particularly sensitive to encoders and achieve the best NDCG performance when using LGCN as the encoder. Furthermore, all baselines show sensitivity to encoders in ILD and ARP except for TFROM which remains largely insensitive to encoders for ARP.  These results underscore the importance of selecting the most suitable encoder for different multi-objective baselines to achieve optimal performance.  Beyond BOOML and SMORL, we also observe that the diversity-oriented DGRec, built on GNN, achieves relatively strong performance on NDCG, highlighting the potential of GCN/GNN structures in enhancing the accuracy of multi-objective optimization.
\end{itemize} 
%

%





\input{table/v5_results_group}



\begin{figure*}[t]
    \centering
    \subfigure[$E_{ml}$ on MF]{
        \includegraphics[width=0.22\linewidth]{image/MF_Meta_Epoch.pdf}
    }
    %\hspace{0.1in}
    \subfigure[$E_{ml}$ on LGCN]{
        \includegraphics[width=0.22\linewidth]{image/LightGCN_Meta_Epoch.pdf}
    }
    %\hspace{0.2in}
    \subfigure[$W$ on MF]{
        \includegraphics[width=0.22\linewidth]{image/MF_Groups.pdf}
    }
    %\hspace{0.2in}
    \subfigure[$W$ on LGCN]{
        \includegraphics[width=0.22\linewidth]{image/LightGCN_Groups.pdf}
    }
    %\hspace{0.2in}
    \vspace{-0.15in}
    \caption{The impacts of meta-learning epochs $E_{ml}$ and the number of user groups $W$.}
    \label{fig:hyper-parameter}
    \vspace{-0.1in}
\end{figure*}

\subsubsection{Ablation Study (RQ2)} To examine the efficacy of different components of our BOOML, we compare it with different variants. Specifically, (1) \textbf{SGD} directly uses SGD with constant weights to optimize the recommender, i.e., $\lambda_w = \beta_w = 1.0$ for all user groups;
(2) \textbf{BO} adopts vanilla Bayesian optimization to search the optimal weights of different objectives for each group; (3) \textbf{BOML} adopts meta-learning in the BO process to search the optimal weights of different objectives by considering the correlations among different user groups; and (4) \textbf{BOOML} is our proposed method which exploits orthogonal meta-learning in the BO process by considering the correlations of different groups and potential conflicts among various objectives.  Table~\ref{tab:ablation-study} shows the performance across different evaluation metrics and training epochs of all variants with the two encoders on all datasets. Four key findings can be identified. 

\begin{itemize}[leftmargin=0.3cm]
\item[-] \textbf{First}, {BO} generally outperforms {SGD}, especially on the diversity and fairness metrics (i.e., ILD and ARP), which verifies the necessity and effectiveness of using BO to search for the optimal weights for better multi-objective performance.
\item[-] \textbf{Second}, compared with BO, BOML generally delivers superior performance with LGCN as the encoder. However, BOML exhibits lower performance on NDCG when using MF as encoders but gains significant improvements in diversity and fairness metrics. For example, BOML-MF results in a 52\% decrease on Games dataset in NDCG, it achieves a 108\% improvement in ILD and a 63.52\% reduction in ARP, ultimately leading to a 27\% increase in ResSum. These results, on one hand, highlight the effectiveness of meta-learning in improving recommendation performance by capturing correlations among different user groups; on the other hand, they reveal that potential conflicts among objectives may constrain the overall effectiveness of meta-learning. 
\item[-] \textbf{Third}, BOOML-MF generally defeats BOML-MF across most metrics, resulting in overall performance improvements. This highlights BOOML's capability to mitigate potential conflicts among different objectives. However, BOOML-LGCN underperforms BOML-LGCN in most cases. This may be attributed to BOOML's reduced conflict mitigation capability during the layer-wise propagation process with the GCN encoder. 
\item[-] \textbf{Lastly}, BOML and BOOML achieve better performance than SGD and BO in most cases with significantly fewer training epochs. For instance, the training epochs for SGD and BO are in the range of $[40,80]$, while BOML and BOOML only require 5 meta-learning epochs. This verifies that the meta-optimization and orthogonal gradient descent greatly enhance the training efficiency and effectiveness.   
\end{itemize}
\subsubsection{Performance Across Different Groups (RQ3)}
Table~\ref{tab:weights_across_group} shows the learned weights for different objectives and the corresponding performance across different metrics of our BOOML-MF on Games and Electronics, respectively. In the table, the `Learned Weights' means the original weights learned by our BOOML for different objectives. For ease of analysis, we also calculate the `Normalized Weights'; we highlight the higher weights for different objectives (e.g., Accuracy) across different groups; and the corresponding metrics (e.g., NDCG) with better results are highlighted in the same color. For instance, on Games, across the three groups, $\mathcal{G}_2$ and $\mathcal{G}_3$ have higher weights on Accuracy than $\mathcal{G}_1$, so they are highlighted in red; and $\mathcal{G}_2$ and $\mathcal{G}_3$ achieve the best NDCG values, so they are also highlighted in red. Similarly, we highlight the higher weights on `Diversity' and `Fairness' in blue and green, respectively. 

From the results, two observations can be noted. \textbf{First}, \textit{on all datasets, across different groups, the objectives with higher weights gain better results on the corresponding metrics}. For instance, on Games, $\mathcal{G}_2$ and $\mathcal{G}_3$ have higher weights on Accuracy (0.2582 and 0.5738) than $\mathcal{G}_1$ (0.1477), thus they gain better results on NDCG (0.0086 and 0.0493) than that of $\mathcal{G}_1$ (0.0077); while $\mathcal{G}_1$ and $\mathcal{G}_2$ possess higher weights on Fairness (0.7087 and 0.7390) than $\mathcal{G}_3$ (0.3923), so they obtain better results on ARP (16.4183 and 16.7074) than that of $\mathcal{G}_3$ (30.4972). This helps verify that our BOOML can better uncover the uncertain relationships between the weights and performance of different objectives. 
\textbf{Second}, \textit{different groups inherently prioritize distinct multiple objectives}. For instance, based on the learned weights and performance of different objectives across each group, on Games, we observe that users in $\mathcal{G}_1$ place greater emphasis on both Diversity and Fairness, $\mathcal{G}_2$ prioritize Accuracy and Fairness, while $\mathcal{G}_3$ focus more on Accuracy and Diversity. 


\subsubsection{Hyper-parameter Analysis (RQ4)}
We now examine how essential hyper-parameters affect the performance of our BOOML-MF and BOOML-LGCN, including the number of meta-learning epochs ($E_{ml}$) and the number of user groups ($W$). Figure~\ref{fig:hyper-parameter} depicts the results, where we vary $E_{ml}$ in the range of $[1,5]$ and $W$ in the range of {$[2,5]$}, with both stepped by 1.  
For $E_{ml}$, the performance generally improves as $E_{ml}$ increases and then keeps relatively stable with BOOML-MF. In contrast, the performance of BOOML-LGCN consistently declines as $E_{ml}$ increases. As explained, this may be attributed to BOOML's diminished conflict mitigation capability during the layer-wise propagation process in GCN. Thus, we set $E_{ml} = 5$ for MF and $E_{ml} = 1$ for LGCN.
Regarding $W$, in most cases, the performance increases initially, reaches a peak, and then declines. To ensure consistency and simplicity, we suggest to set $W = 3$ in real-world application. 



\section{Conclusion and Future Work}
In this paper, we introduce a novel framework defining five levels of autonomy for RSs based on their ability to independently determine recommendation objectives. Accordingly, we propose an orthogonal meta-learning boosted Bayesian optimization approach to automatically identify and optimize uncertain multi-objectives (i.e., accuracy, diversity and fairness) based on individual user needs. 
Specifically, it leverages BO to explore the search space and quantify uncertainties between the weights and overall objectives, where the orthogonal meta-learning paradigm significantly improves optimization efficiency and effectiveness through collaborative information sharing and objective conflict reduction. Experimental results demonstrate that our approach can better optimize uncertain multi-objectives for individual users compared with SOTAs, taking a significant step toward more ethical and user-centric RSs.
For future works, we plan to (1) incorporate temporal dynamics to adapt to evolving user preferences and objectives over time and (2) expand the framework to address additional ethical concerns, e.g., transparency and privacy, enhancing the societal impact of RSs.


\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
