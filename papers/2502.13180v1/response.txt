\section{Related Works}
\subsection{RSs at Levels 0-2} 
Early RSs at \textit{Level 0} rely on generic rules or broad statistical patterns, such as recommending the most popular items, or frequently co-occurred items mined by association rules[1], thus failing to provide personalization.
%
Later, RSs at \textit{Level 1} began leveraging static user or item profiles, aka. content-based RSs[2], for instance, a user who indicates a preference for `romance' in their profile would receive recommendations for romantic movies. Hence, a basic level of personalization is introduced.
%
Advancing to \textit{Level 1}, RSs at \textit{Level 2} resort to dynamic historical user behaviors to learn user preference, aka. collaborative filtering based RSs[3]. Different techniques are adopted, ranging from simple matrix factorization (MF)[4], to complex deep learning, e.g., MLP[5], RNN[6], GCN[7], Transformer[8] and LLMs[9].  
%
However, RSs at Levels 0-2 aim to purely improve recommendation accuracy, ignoring other essential ethical aspects, e.g., diversity and fairness. 
 
\subsection{RSs at Level 3}
RSs at \textit{Level 3} exploit dynamic historical user behaviors to learn user preference by optimizing pre-defined multi-objectives beyond accuracy. As we primarily focus on two key ethical aspects -- diversity and fairness, we limit our discussion to research relevant to these areas. Studies on other ethical aspects, e.g., explanation and privacy-perseveration, will be explored in our future work.  

\smallskip\noindent\textit{\textbf{Diversity}} bias would cause filter bubbles, which grow along the feedback loop and inadvertently narrow user interests[10]. Thus, a vital branch is to enhance recommendation diversity while maintaining accuracy, mainly divided into three categories: post-processing heuristic methods[11], determinantal point process methods[12] and end-to-end learning methods[13]. However, they suffer from different limitations: (1) some follow a two-stage paradigm, i.e., train offline models to score items on accuracy and then re-rank items considering diversity; and (2) others incorporate accuracy and diversity objectives with a pre-defined ``trade-off'' hyperparameter, overlooking the uncertainty of personalized user needs. 

\smallskip\noindent\textit{\textbf{Fairness}} is another critical ethical issue of RSs[14] that can affect personal experience and social good since RSs serve a resource allocation role in society by allocating information to users and exposure to items.
Extensive work has encouraged equal exposure across item groups partitioned by item features, such as category and popularity\footnote{
There are different types of fairness in RSs, e.g., user fairness, item fairness, and joint fairness[15]. In this study, we primarily focus on item fairness regarding popularity without relying on extra item features.}. 
Early studies design {data-oriented methods}[16] to alleviate the unfairness issue by changing training data. 
Another branch focuses on {re-ranking based methods}[17] to adjust the outputs of recommendation models to promote fairness. 
Recent studies propose {ranking-based methods} to improve fairness by (1) using linear programming to add fairness constraints[18]; (2) adding a fairness-related regularization term to the recommendation loss[19]; 
(3) leveraging adversarial learning to learn fair representations or predicted scores[20]; (4) adopting reinforcement learning to achieve long-term fair recommendations[21]; and 
(5) balancing accuracy and fairness for various stakeholders with heuristic strategies[22] or Pareto optimality guarantee[23]. 
Despite the effectiveness, most of them mainly seek a uniform ``trade-off" between accuracy and fairness across all users while ignoring personalized user needs.  

\subsection{RSs at Level 4}
Some studies attempt to achieve RSs at \textit{Level 4}. For instance, in [24], the authors propose a new recommender prototype called User Controllable RS, which enables users to
actively control the mitigation of filter bubbles. Nevertheless, it relies on user feedback and only considers the balance between accuracy and diversity. 
MMoE[25] adapts the Mixture-of-Experts structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network to optimize each task. However, it only learns the gates at the task level instead of the individual user level. A recent work on arXiv[26] introduces a deep Pareto reinforcement learning model for multi-objective RSs, which accounts for the relationships between different objectives and implements personalized dynamic weighting for these objectives. However, it still relies on learning trainable weights for multiple objectives, leading to the degradation of certain objectives. Besides, it ignores the potential conflicts of different objectives and introduces substantial computational complexity due to dynamically adjusting objective weights based on individual user information. 
% Besides, key technical details, e.g., the method for setting rewards, are not provided, making the model challenging to understand and reproduce.