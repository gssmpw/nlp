\section{Related Work}
\label{sec:relat}

% There is vast amount of works on reconstructing 3D humans from single images \cite{liu2022recent}, monocular video \cite{yuan2022glamr, Kocabas20, alldieck2018video}, RGB-D data \cite{yu2017bodyfusion,yu2018doublefusion,burov2021dynamic} and multi-view data. Here, we focus on methods that take multi-view setups as input, either for single or multiple humans, as well as recent neural surface reconstruction works. 
% \todo{Maybe unify these two again into Human reconstruction, and put initial paragraph there}


\paragraph{Single-Human Reconstruction.} 
There is a vast amount of work on reconstructing 3D humans from single images \cite{bogo2016keep, choutas2020monocular, kanazawa2018end,muller2021self,liu2022recent}, monocular video \cite{yuan2022glamr, Kocabas20, alldieck2018video}, RGB-D data \cite{yu2017bodyfusion,yu2018doublefusion,burov2021dynamic} and multi-view data \cite{starck2007surface,collet2015high,guo2019relightables,huang2018deep}. We concentrate here on the multi-view setting. %, which is the setting of this work. 
% One of the first systems to perform human surface capture was proposed by Starck and Hilton \cite{starck2007surface}, which used a combination of visual hull and stereo reconstruction techniques. 
High-end multi-view capture systems can achieve reconstructions of outstanding quality \cite{leroy2018shape, dou2016fusion4d, guo2019relightables, collet2015high,vlasic2009dynamic,Joo_2017_TPAMI}, but require a complex studio setup that is expensive to build and not easily accessible.
% These settings are expensive to build and not readily accessible. 
To alleviate this, numerous works have been proposed that use instead a sparse set of RGB cameras (\eg between 2 and 15), 
%typically between 2 and 15 \va{I made this up}. 
%To compensate for the lack of views, classic works resorted to tracking a pre-scanned template~\cite{gall2009motion, vlasic2008articulated, carranza2003free, de2008performance} \todo{check all refs}%, relying on temporal information \cite{...} 
where the lack of views and presence of wide baselines is compensated by tracking a pre-scanned template \cite{gall2009motion, vlasic2008articulated, carranza2003free, de2008performance, wu2012full},
using a parametric body model  \cite{huang2017towards, balan2007detailed}, %joo2018total -> a lot of cameras
or more recently, by the use of deep learning \cite{huang2018deep,liang2019shape, wang2022arah, liu2021neural, peng2021neural,  kwon2021neural, peng2021animatable, xu2021h, weng2022humannerf}. 
% % MISSING, maybe:
% \cite{orts2016holoportation} -> 8 cameras, real-time. Uses depth cameras
% gall2009motion -> 4 to 8 cameras
% \cite{wu2012full} -> also sparse, recovers illumnation but in a passive way. tracks a template.
%
%
%
% To achieve better details, a recent trend has been to use deep implicit representations \cite{wang2022arah, peng2021neural,kwon2021neural}. Most of these methods require either a dense set of camera views, or a sparse set of RGB videos. To address the sparse view setting for a single frame, PiFU \cite{saito2019pifu} and follow-up work \cite{saito2020pifuhd} employ a set of pixel-aligned features pre-trained on a large dataset, which might not easily generalize to out-of-distribution scenes. 
% %% Not nerf but deep learning: \cite{liang2019shape}
% %% Nerf: \cite{kwon2021neural, liu2021neural, peng2021animatable, weng2022humannerf,xu2021h}
% %% PiFU-like works : "limited by the training data" | "line of works that condition on pixel-aligned features to recover an implicit representation". They require ground-truth data and thus cannot generalize to arbitrary scenes. Furthermore, they do not recover appearance and cannot do novel view synthesis. From here connect to pixel nerf, and why we do not compare. 
% %%
% \cite{saito2019pifu, saito2020pifuhd, alldieck2022photorealistic, he2020geo, huang2020arch, he2021arch}
%
%
% %%%%%%%%%%%
% %% "DoubleField: Bridging the Neural Surface and Radiance Fields for High-fidelity Human Reconstruction and Rendering" (CVPR'22)
% %% -> this is very relevant, we should've compared
% %%%%%%%%%%%
%
%
% ----
% \paragraph{Multi-view Multi-Human Reconstruction.} 
% %%% GOOD REVIEW IN Dynamic Multi-Person Mesh Recovery From Uncalibrated Multi-View Camera"
% None of these works were designed to handle the increased geometric complexity and occlusion of the multi-human case. 
% Compared with tremendous progress of single human reconstruction, there is a limited number of multiple human reconstruction methods.
\paragraph{Multi-Human Reconstruction.} 
In contrast, there has been a limited number of works that address the problem of \emph{multiple} human reconstruction. 
This is a difficult task since the presence of several people increases the geometric complexity of the scene, introduces occlusions,  and amplifies ambiguities such that commonly used features like color, edges, or key points cannot be correctly assigned. 

For single images and video, the problem has been mainly tackled by regressing the parameters of the SMPL \cite{loper2015smpl} body model~\cite{zhang2020perceiving, choi2022learning, sun2022putting, sun2021monocular, zanfir2018deep, dong2021shape, zanfir2018monocular, fieraru2020three, jiang2020coherent, zhang2021body, ugrinovic2021body, guler2019holopose}. Although this can work robustly with as little as one view, the reconstructions are very coarse and cannot explain hair, clothing, and fine geometric details. 
The only exception is the work of Mustafa \etal~\cite{mustafa2021multi}, which performs model-free reconstruction of multiple humans by combining an explicit voxel-based representation with an implicit function refinement. 
However, % I comment just becasue if not sure, we could save space , the output geometries are still rather coarse \va{maybe remove this, so are ours...}, and 
the method requires training on a large synthetic dataset of multiple people which hinders generalization. Our work, on the other hand, performs 3D reconstructions, produces renderings of novel views, and can generalize to arbitrary multi-human scenes.

Multi-view capture setups can help resolve depth ambiguities and some of the occlusions. 
Classic methods for estimating multiple humans rely heavily on segmentation masks and template mesh tracking \cite{liu2011markerless, liu2013markerless, wu2013set}. We avoid the use of segmentation masks by adopting volumetric rendering for implicit surfaces \cite{wang2021neus}. 
More recently, deep learning-based approaches were proposed, but they either require temporal information~\cite{zheng2021deepmulticap,huang2021dynamic,zhang2021lightweight,shuai2022novel}, pre-training on a large dataset~\cite{zheng2021deepmulticap} which cannot work on general scenes, or a coarse body model \cite{zhang2021lightweight, huang2021dynamic, shuai2022novel} which lacks geometric detail. 
% Furthermore, with the exception of XX, the focus here is on 3D shape estimation, ignoring the quality of rendered novel viewpoints \todo{I think this should go away}. 
Here, we focus on the multi-human setting on static scenes and propose a method that recovers accurate reconstructions and at the same time produces renderings of novel viewpoints. 
% by recovering an implicit representation from multi-view images.
%
%
%%% V: I removed the segmentation argument because we still need SMPL as input, so in practice you might need a segmentation mask just to get SMPL
%%% V: mustafa20224d is temporal and I think they don't do _simultaneous_ reconstruction of all humans, just segmentation. To be on the safe side I'm gonna ignore this paper for now, since it's anyway from 2022 and we could've missed it. 
%
%
%
% ----
% \todo{maybe start neural with this:}
% When the goal is to generate free-viewpoint video, image-based-rendering has also been considered \cite{wu2020multi}, which side-steps 3D reconstruction and produces novel views by reasoning on the available images. A recent succesful family of works, nerf an dimplicit, which are discussed next. 
% Another classic one in IBR (I think): "High-quality video view interpolation using a layered representation"
%
%
\paragraph{Neural Surface Reconstruction and Novel-View Synthesis.} 
For generating free-viewpoint video, image-based rendering has been considered as an alternative or complement to 3D reconstruction~\cite{carranza2003free,wu2020multi,liu2021neural,kwon2021neural, liu2021neural, peng2021animatable, weng2022humannerf,xu2021h}. 
%which side-steps 3D reconstruction and produces novel views by reasoning on the available images. A recent succesful family of works, nerf an dimplicit, which are discussed next. 
% Another classic one in IBR (I think): "High-quality video view interpolation using a layered representation"
% Impressive results have been obtained recently by with methods that combine 3D reconstruction and novel-view symthesis, using deep implicit representations. 
When geometry proxies are available, neural rendering \cite{aliev2020neural,thies2019deferred,jena2022neural} can produce competitive novel view synthesis. Recently, NeRF\cite{mildenhall2020nerf} demonstrated impressive rendering results by representing a 3D scene as a neural radiance field, trained only with calibrated multi-view images through the use of volume rendering. However, due to the unconstrained volumetric representation and self-supervised training on RGB values, reconstructed geometries tend to be too noisy to be useful for 3D applications. 
% Utilizing NeRF for human reconstruction could achieve a realistic appearance rendering, but hard to reconstruct accurate geometry due to NeRF purely reling on RGB values and lacking geometry constraints. 
To recover more accurate 3D geometry along with appearance, DVR~\cite{niemeyer2020differentiable}, IDR~\cite{yariv2020multiview}, and NLR~\cite{kellnhofer2021neural} propose to learn an implicit representation directly from multi-view images but require accurate object masks to work. To avoid the need for segmentation masks, recent works propose to combine implicit representations with volume rendering \cite{oechsle2021unisurf, yariv2021volume, wang2021neus}. 
%To avoid the use of mask segmentations, VolSDF \cite{yariv2021volume} proposed to combine an implicit SDF representation with volume rendering, transforming SDF values into volume densities by using the cumulative distribution function of the Laplace distribution. 
%%%by modeling the volume density as a function of geometry which is defined as Laplace’s cumulative distribution function (CDF).
% NeuS\cite{wang2021neus} uses a SDF to represent the surface and develop a new volume rendering method to train a neural SDF representation. Though those methods could reconstruct 3D geometry and appearance, it remains a challenge for them to reconstruct geometry consistent surface, especially for scenes containing complex geometry or sparse input views. \\
These methods show remarkable reconstruction results but struggle when the number of input views is low. 
Implicit neural representations from sparse input can be obtained by using pre-trained pixel-aligned features or 3D feature volumes for input images ~\cite{saito2019pifu, saito2020pifuhd, alldieck2022photorealistic, he2020geo, huang2020arch, he2021arch, yu2021pixelnerf, li2023learning, jena2024geotransfer} or point clouds ~\citep{boulch2022poco,williams2022neural,huang2023neural,peng2020convolutional,chibane2020implicit,lionar2021dynamic, ouasfi2023mixing, peng2021shape, ouasfi2022few, ouasfi2024robustifying}, 
but this requires ground-truth geometry and is limited by the training data, struggling to generalize to new scenes. Sparse variants that do not require generalizable features were proposed in the image input \eg \cite{niemeyer2022regnerf, kim2022infonerf, long2022sparseneus, younes2025sparsecraft, li2023regularizing} and point cloud input case \eg \cite{NeuralTPS,sparseocc,nap,ouasfi2024robustneuralreconstructionsparse,williams2021neural}. 
InfoNeRF~\cite{kim2022infonerf} regularizes sparse views by adding an entropy constraint on the density of the rays,  RegNeRF~\cite{niemeyer2022regnerf} uses a patch-based regularizer over generated depth maps, and SparseNeuS~\cite{long2022sparseneus} uses a multi-scale approach along with learned features that are fine-tuned on each scene. 
%% DietNeRF: since it needs CLIP (=> "pretraining") I don't know where to place it, and I'm ignoring it for now.
%%% PiFU-like works : "limited by the training data" | "line of works that condition on pixel-aligned features to recover an implicit representation". They require ground-truth data and thus cannot generalize to arbitrary scenes. Furthermore, they do not recover appearance and cannot do novel view synthesis. From here connect to pixel nerf, and why we do not compare. 
%%%
% \cite{saito2019pifu, saito2020pifuhd, alldieck2022photorealistic, he2020geo, huang2020arch, he2021arch}
Our approach builds on NeuS \cite{wang2021neus}, and tackles the sparse view challenge by adding human-specific geometric priors and novel regularizations. 




%%%% This is more for the introduction, not the related work. 
%Specifically, we use multiple human SMPL to train a neural SDF presentation as a geometry prior to optimize the learning. Moreover, %inspired by \cite{deng2022depth,roessle2022dense}, 
%We propose to learn SDF along with uncertainty and propose a explicit geometry constrains to ensure the geometry consistency while encouraging learning details around surfaces. Furthermore, noticing that the reconstruction inconsistency might happen due to insufficient viewpoints or invariant illuminations, we propose a patch-based regularization to ensure consistency across different rays and saturation regularization for illuminations across different views. \todo{pifu line}


% Sparse nerf: regnerf\cite{niemeyer2022regnerf}, infonerf\cite{kim2022infonerf}
% Sparse Neus:\cite{long2022sparseneus} %ECCV 2022

% \paragraph{Concurrent Works.} 
% \va{I don't think this should go here, it's risky. I suggest we add this only if reviewers ask}
% Monosdf\cite{yu2022monosdf}, GeoNeus\cite{fu2022geo}, Neuris\cite{wang2022neuris} VOXURF\cite{wu2022voxurf} % 4 nips 2022

% -------------------------

% \textbf{Human reconstruction.} 
% % single human, multihuman, yiyi
% Previous work related to human reconstruction could be classified into single-human reconstruction and multiple-human reconstructions according to the human numbers in the scene. Single human reconstruction from images or video includes traditional methods, such as depth sensors \cite{theobalt2005image, dou2016fusion4d}, depth array of cameras\cite{debevec2000acquiring, guo2019relightables}, rendering pipeline optimization \cite{wu2020multi}, template-based methods\cite{gall2009motion, carranza2003free, de2008performance}, and learning based methods\cite{burov2021dynamic,loper2015smpl,loper2015smpl,bhatnagar2020combining,  liu2021neural,yu2017bodyfusion,yu2018doublefusion,peng2021neural}. Compared with tremendous progress of single human reconstruction, there is a limited number of multiple human reconstruction methods. Among those, \cite{zanfir2018monocular,zanfir2018deep} are earliest works to utilize human SMPL for multiple human reconstructions, \cite{mustafa2021multi} exploits 3D pose to learn body part scores, localize multiple persons simultaneously and estimate 3d pose and shape
% \cite{mustafa20224d} combines human model with implicit reconstruction for multiple human shape reconstruction. Those works usually rely on segmentation and focus on 3D shape estimation, which cost expensive in computing and ignores the quality of rendered novel viewpoints. In this paper, we focus on multiple human reconstructions on static scenes, learning multiple human geometry and appearances with loose clothing and hair details simultaneously and enabling editing during rendering without the extra requirement of segmentation.

% \textbf{Neural surface reconstruction.} 
% Recently, NeRF\cite{mildenhall2020nerf} demonstrated impressive rendering results by representing a 3D scene as a neural radiance field, with only calibrated color images, camera poses and intrinsic as input. Utilizing NeRF for human reconstruction could achieve a realistic appearance rendering, but hard to reconstruct accurate geometry due to NeRF purely reling
% on RGB values and lacking geometry constraints. In order to reconstruct accurate 3D geometry along with appearance, Volsdf \cite{yariv2021volume} propose to improve geometry representation and reconstruction in neural volume rendering by defining the volume density function as Laplace’s cumulative distribution function(CDF) and modeling it as a function of geometry. NeuS\cite{wang2021neus} uses a signed distance function(SDF) to represent the surface and develop a new volume rendering method to train a neural SDF representation. Though those methods could reconstruct 3D geometry and appearance, it remains a challenge for them to reconstruct geometry consistent surface, especially for scenes containing complex geometry or sparse input views. \\
% Our approach take advantage of existing implicit surface reconstruction (\eg NeuS), and tackle its challenges by adding learned geometry priors and explicit geometry constraints. Specifically, we use multiple human SMPL to train a neural SDF presentation as a geometry prior to optimize the learning. Moreover, %inspired by \cite{deng2022depth,roessle2022dense}, 
% we propose to learn SDF along with uncertainty and propose a explicit geometry constrains to ensure the geometry consistency while encouraging learning details around surfaces. Furthermore, noticing that the reconstruction inconsistency might happen due to insufficient viewpoints or invariant illuminations, we propose a patch-based regularization to ensure consistency across different rays and saturation regularization for illuminations across different views. \todo{pifu line}

% Sparse nerf: regnerf\cite{niemeyer2022regnerf}, infonerf\cite{kim2022infonerf}
% Sparse Neus:\cite{long2022sparseneus} %ECCV 2022
% \textbf{Concurrent Works.} 
% Monosdf\cite{yu2022monosdf}, GeoNeus\cite{fu2022geo}, Neuris\cite{wang2022neuris} VOXURF\cite{wu2022voxurf} % 4 nips 2022




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%---  Multi-human reconstruction from single image: ---
% Note first that there are numerous works on multi-human 3D pose estimation, but we focus on those that recover 3D meshes (typically shape and pose parameters of a SMPL model) since this is closer to our task.

% \cite{choi2022learning}
% \cite{sun2022putting, sun2021monocular}
% \cite{zanfir2018deep} -> predict 3D pose only and defer the shape reconstruction step to a later optimization step
% \cite{zanfir2018monocular}
% \cite{dong2021shape} -> the final goal is pose, but they use SMPL shape to help with the problem
% \cite{fieraru2020three}
% \cite{jiang2020coherent}
% \cite{zhang2021body}
% \cite{ugrinovic2021body} -> addresses body scale and depth ambiguity


%% --- Multi-human reconstruction from multiple views ---

% + Classic: they track a pre-scanned template of an actor:
% \cite{liu2011markerless} -> only for two people
% \cite{wu2013set}
% This one doesn't, but the input is video:
% \cite{li2018shape}
% The multi-person case introduces ambiguities "where commonly-used features such as color, edges or keypoints cannot be individually assigned

% + Deep learning:
% \cite{huang2021dynamic} -> uses motion model, SMPL. Uncalibrated cameras, video.
% \cite{zhang2021lightweight} -> they say they are the first light-weight and robust that uses only sparse cameras (6 cameras in the experiments). They are also total capture: hand face and body - but still cannot handle clothes and hair.
% icnludes hands and faces but it's still a coarse mesh from a parametric model. 
% \cite{zheng2021deepmulticap}  -> implicit, but video

% Note how many rely on motion, but here we assume only a single view


%% --- Multi-human novel view synthesis (2D) ---
% \cite{lakhal2019view}
% \cite{zhang2021editable} -> Closely related, they do nerf on multiple humans with a layered representation. The input is a video. 

%% -- Single human, MV neural:
% \cite{liu2021neural}
% \cite{weng2022humannerf} -> Humannerf, from video
% \cite{shao2022doublefield}
% \cite{shao2022diffustereo} -> follow up, they build on doublefield


%% Multi-view, RGB-D
% \cite{yu2021function4d}


% MV 3D pose, just in case we ever use this as input:
% \cite{zhang2021direct}




%% MV story:
% - there are high-end MV capture systems requiring a large number of cameras (volumetric)
% \cite{leroy2018shape, collet2015high, orts2016holoportation, joo2015panoptic}
% and more light-weight settings was achieved using a pre-scanned template
% \cite{de2008performance}
% - recently with deep learning, more possible:
% \cite{huang2018deep} -> volumetric representation
% Pifu, pifu hd shows MV results also. This line of work requires pre-training on a large dataset. 
% \cite{saito2019pifu, saito2020pifuhd}
% These are all for single humans
% For multiple humans,




