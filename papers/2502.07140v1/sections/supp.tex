\twocolumn[\centering \Large{\textbf{Few-Shot Multi-Human Neural Rendering Using Geometry Constraints\\ Supplementary Material}}\vspace{30pt}]

%{\begin{figure}[H]
%\setlength{\linewidth}{\textwidth}
%\setlength{\hsize}{\textwidth}
%\centering
%\includegraphics[width=.8\linewidth]{fig/pipe.png}
%\caption{Illustration of our losses. Rays with available ground-truth pixels are supervised with pixel colors. Sub-pixel rays without available ground-truth are supervised using color and density pseudo-ground-truth from neighboring rays.}
%\label{fig:pipe}
%\end{figure}}]

\begin{figure*}[t!]%[t] %[bp] %[h!] %[ht]
%\flushleft %\centering%
%\vspace{-3mm}
\setlength{\linewidth}{\textwidth}
\setlength{\hsize}{\textwidth}
\centering
\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\scalebox{1}{
\begin{tabular}{c ccccccc}
\rotatebox{90}{\quad remove} 
\includegraphics[width=2.5cm]{fig/edit/remove/0.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/1.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/2.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/3.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/4.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/5.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/7.png}\\
%\includegraphics[width=2.5cm]{fig/edit/remove/7.png}
\rotatebox{90}{\quad remove}
\includegraphics[width=2.5cm]{fig/edit/remove/0_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/1_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/2_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/3_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/4_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/5_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/remove/7_normal.png} \\
%\includegraphics[width=2.5cm]{fig/edit/remove/7_normal.png} \\
\rotatebox{90}{\quad trans} 
\includegraphics[width=2.5cm]{fig/edit/translate/0.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/1.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/2.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/3.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/4.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/5.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/6.png} \\
%\includegraphics[width=2.5cm]{fig/edit/translate/7.png} \\
\rotatebox{90}{\quad trans} 
\includegraphics[width=2.5cm]{fig/edit/translate/0_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/1_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/2_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/3_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/4_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/5_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/translate/6_normal.png} \\
%\includegraphics[width=2.5cm]{fig/edit/remove/7_normal.png} \\
\rotatebox{90}{\quad rotate} 
\includegraphics[width=2.5cm]{fig/edit/rotate/0.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/1.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/2.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/3.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/4.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/5.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/6.png} \\
%\includegraphics[width=2.5cm]{fig/edit/rotate/7.png} \\
\rotatebox{90}{\quad rotate} 
\includegraphics[width=2.5cm]{fig/edit/rotate/0_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/1_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/2_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/3_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/4_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/5_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/rotate/6_normal.png} \\
%\includegraphics[width=2.5cm]{fig/edit/remove/7_normal.png} \\
\rotatebox{90}{\quad scale} 
\includegraphics[width=2.5cm]{fig/edit/scale/0.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/1.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/2.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/3.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/5.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/6.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/7.png} \\
%\includegraphics[width=2.5cm]{fig/edit/scale/7.png} \\
\rotatebox{90}{\quad scale} 
\includegraphics[width=2.5cm]{fig/edit/scale/0_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/1_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/2_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/3_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/5_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/6_normal.png} &
\includegraphics[width=2.5cm]{fig/edit/scale/7_normal.png} \\
%\includegraphics[width=2.2cm]{fig/edit/remove/7_normal.png} \\
%Ground Truth &NeuS &VolSDF & Ours  & Neus &Volsdf & Ours\\
\end{tabular}}
%&
%\end{tabularx}
 %\vspace{-2mm}
 \caption{Qualitative results for the editing application. We show synthesised novel views and reconstructed normal images of multiple humans when (1) removing, (2) translating, (3) rotating and (4) scaling subjects in the scene. } 
\label{fig:edit} 
%\vspace{-3mm}
\end{figure*}
%}]


\section{Additional Results} 
\paragraph{Scene editing.}
We show here how our method can be used to perform post-learning scene editing without any additional training.
% Thanks to the human bounding-box-based modeling of the foreground scene, our method enables post-learning scene editing without requiring any additional training, including the ability of rigidly moving people around the scene, and omiting or duplicating people.
Thanks to the human bounding-box-based modeling of the foreground scene, it is straightforward to rigidly transform or omit each person by simply applying, before rendering, the corresponding manipulation to the points sampled inside the defined bounding box. 
Figure~\ref{fig:edit} shows qualitative results of such application, trained on scene $\#5$ from the CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} using 20 training views. We can see here that our approach can generate realistic new scenes as well as plausible inpaintings of the missing regions.  

\paragraph{Comparisons with varying number of people.} In Fig.~\ref{fig:cmu} we provide additional qualitative comparisons against NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume}, where we show results on the CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} with varying number of people in the scene  (Going from 3 to 7 people). Note here how increasing the number of people reduces the quality of our baselines results, \ie mixing the background with humans or generating noisy geometries. Meanwhile, our method performs consistently, independently of the number of people.

% For more visual results, please find it in Fig \ref{fig:cmu}. In the Fig \ref{fig:edit}, we demonstrate multiple applications based on our proposed model and box-based rendering method, including removing people, translation, rotation and scaling.


\begin{figure*}[ht]%[t] %[bp] %[h!] %[ht]
\flushleft %\centering%
%\vspace{-3mm}
\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\scalebox{1}{
\begin{tabular}{c cccc ccc}
% \rotatebox{90}{\quad 3/20} 
\rotatebox{90}{\quad 3} 
\includegraphics[width=2.5cm]{fig/supp/3_20/gt.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/3_20/ours_normal.png} \\
% \rotatebox{90}{\quad 4/20}
\rotatebox{90}{\quad 4}
\includegraphics[width=2.5cm]{fig/supp/4_20/gt.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/4_20/ours_normal.png} \\
% \rotatebox{90}{\quad 5/20}
\rotatebox{90}{\quad 5}
\includegraphics[width=2.5cm]{fig/supp/5_20/gt.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/5_20/ours_normal.png} \\
% \rotatebox{90}{\quad 6/20}
\rotatebox{90}{\quad 6}
\includegraphics[width=2.5cm]{fig/supp/6_20/gt.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/6_20/ours_normal.png} \\
% \rotatebox{90}{\quad 7/20}
\rotatebox{90}{\quad 7}
\includegraphics[width=2.5cm]{fig/supp/7_20/gt.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/supp/7_20/ours_normal.png} \\
Ground Truth &NeuS &VolSDF & Ours  & Neus &Volsdf & Ours\\
\end{tabular}}
%&
%\end{tabularx}
% \vspace{-2mm}
 \caption{Qualitative comparisons against NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} for different number of people in the scene. Showing synthesised novel views and reconstructed normal images on 5 scenes from CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI}, using 20 training views.} 
\label{fig:cmu} 
%\vspace{-3mm}
\end{figure*}

% \section{Additional Quantitative Results} 
\paragraph{Additional Quantitative Results.}
Table~\ref{tab:chamfer} provides a full Chamfer distance comparison in the synthetic data setup as an addition to the results reported in Table 3 of the main submission. Symbol `$-$' represents cases where the baselines fail to reconstruct a meaningful geometry, and hence the error is too large. To favor the baselines NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} in the main submission, we computed the uni-directional Chamfer distance from ground-truth to source, as the baselines reconstructed the ground of the scene in addition to the people. For a more standard evaluation, we additionally show here the bi-directional Chamfer distance after removing the floor for the competing methods. 

%We report Chamfer distance  in the 5/10/15-views case. In the 5/10 input views case, the baseline methods usually fail to reconstruct the full geometry of humans due to the sparse inputs, which denotes with $-$.  Since the baseline methods usually contain extra floor, in the main paper, we sample points from ground-truth meshes and compute the distance towards the reconstructed mesh for all methods. We also report the bi-directional Chamfer distance after removing floor here. %Table \ref{tab:syn} shows that, with an increasing number of humans in the scene, the quality of the reconstructed geometry of all methods decreases. However, compared with the baselines, our method can better handle multiple human scenes, achieving an order of magnitude less error. 

\begin{table}[h!]
%\vspace{-3mm}
\begin{center}
% \normalsize
%\label{table:headings}
\scalebox{0.78}{
\begin{tabular}{l|l|ccc | ccc}
\toprule[1.2pt]
% \hline
\# People& Method &\multicolumn{3}{c|}{one-way Chamfer ↓} &  \multicolumn{3}{c}{bidirectional Chamfer ↓}\\
 & &5  &10 &15    &5 &10 &15   \\
%Scene& Method& {\bf PSNR↑} & {\bf SSIM↑} & {\bf LPIPS↓}\\
\hline
 &NeuS& -&-&0.308&-&-& 3.026\\
 1&VolSDF &-&0.020&0.019&-& \bf{0.039} & 0.167 \\
& Ours &0.025 &\bf{0.019} & \bf{0.018} & 0.271&0.211& \bf{0.154}\\
\hline
 &NeuS & -&-&0.321&-&-& 3.044\\
 5&VolSDF &-&-&0.151&-&-&1.478\\
& Ours&0.025&0.023 & \bf{0.020} & 0.391 & 0.289 & \bf{0.138} \\
\hline
 &NeuS &-&-&0.383&-&-&4.639 \\
10&VolSDF &-&-&0.248&-&-&1.579\\
&Ours & 0.082 & 0.063 & \bf{0.043} & 0.111 & 0.085 & \bf{0.081}\\
% \hline
 \bottomrule[1.2pt]
\end{tabular}}
\caption{Geometry reconstruction error under varying number of people, compared to NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} using the synthetic dataset, with 5/10/15 views for training. Symbol `$-$' represents cases where the baselines fail to reconstruct a meaningful geometry.}
%, for different number of humans in the scene.} 
%, We measure geometry error includes one way Chamfer distance and di-directional  Chamfer distance .}
\label{tab:chamfer}
\end{center}
%\vspace{-5mm}
\end{table}

%\section{Method losses illustration}

\paragraph{Comparison to single human NeRF}
In Figure 4 in the main submission, we compared our work to the single human NeRF method ARAH~\cite{wang2022arah} on the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}. Figure~\ref{fig:seg} shows the training images used in this experiment. It also shows the segmentation masks used for ARAH for 3 people in the scene, that we built using a state-of-the-art method. Figure~\ref{fig:seg} shows additional comparative results for reconstructed appearance and geometry. 

%We compare our method with ARAH~\cite{wang2022arah} on the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}. Figure~\ref{fig:seg} shows the training views and segmented masks of each person. Figure~\ref{fig:single} shows qualitative comparison of both reconstructed appearance and geometry. Both ARAH and our approach could reconstruct geometry and appearance from sparse training views. However, regarding to sparser setting (\eg{the girl in first row seen-able in five training views and the boy in the second row seen-able in four training views}) or more occlusions (\eg{the same person in the third row is more occluded than the second row in Figure~\ref{fig:single}}), ARAH suffers from artifacts in reconstructed geometry and appearance. Our method is more robust to sparser information and occlusions due to the proposed hybrid box-based rendering with geometry constraints. Further, ARAH fails to converge on some person with large area of occlusions. Meanwhile, when dealing with multiple-human scenes, single person reconstruction approaches depend on segmented results via inaccurate mask from instance segmentation or incomplete mask from SMPL, resulting in artifacts around 3D surfaces(or 2D edges).  Our method does not require extra segmentation. We evaluate our method and ARAH on five testing views, the testing views are samples uniformly in the rest of images. The average PSNRs is 24.11/\textbf{27.40}(ARAH/\textbf{Ours}). 

\begin{figure}[h]
% \offinterlineskip
\centering%\flushleft %
\vspace{-3mm}
%\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\begin{tabular}{ccccc}
\includegraphics[width=1.68cm]{fig/single/trianing/00_00_00022900.jpg} &
\includegraphics[width=1.68cm]{fig/single/trianing/00_06_00022900.jpg} &
\includegraphics[width=1.68cm]{fig/single/trianing/00_12_00022900.jpg} &
\includegraphics[width=1.68cm]{fig/single/trianing/00_18_00022900.jpg} &
\includegraphics[width=1.68cm]{fig/single/trianing/00_24_00022900.jpg} \\
\includegraphics[width=1.68cm]{fig/single/seg/5/1.png} &
\includegraphics[width=1.68cm]{fig/single/seg/5/2.png} &
\includegraphics[width=1.68cm]{fig/single/seg/5/3.png} &
\includegraphics[width=1.68cm]{fig/single/seg/5/4.png} &
\includegraphics[width=1.68cm]{fig/single/seg/5/5.png} \\
\includegraphics[width=1.68cm]{fig/single/seg/3/1.png} &
\includegraphics[width=1.68cm]{fig/single/seg/3/2.png} &
\includegraphics[width=1.68cm]{fig/single/seg/3/3.png} &
\includegraphics[width=1.68cm]{fig/single/seg/3/4.png} &
\includegraphics[width=1.68cm]{fig/single/seg/3/5.png} \\
\includegraphics[width=1.68cm]{fig/single/seg/6/1.png} &
\includegraphics[width=1.68cm]{fig/single/seg/6/2.png} &
\includegraphics[width=1.68cm]{fig/single/seg/6/3.png} &
\includegraphics[width=1.68cm]{fig/single/seg/6/4.png} &
\includegraphics[width=1.68cm]{fig/single/seg/6/5.png} \\
 %Arah  & Ours&Arah &  Ours\\
\end{tabular}
% \vspace{-3mm}
%&
%\end{tabularx}
\caption{The five training views and person segmentations used to produce results in Figure 4 of the main submission. }
\label{fig:seg} 
%\vspace{-3mm}
\end{figure}


\begin{figure}[h]
% \offinterlineskip
\centering%\flushleft %
%\vspace{-3mm}
%\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\begin{tabular}{ccccc}
\includegraphics[width=1.68cm]{fig/single/id53/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id53/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id53/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id53/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id53/ours_normal.png} \\
\includegraphics[width=1.68cm]{fig/single/id52/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id52/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id52/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id52/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id52/ours_normal.png} \\
\includegraphics[width=1.68cm]{fig/single/id33/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id33/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id33/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id33/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id33/ours_normal.png} \\
\includegraphics[width=1.68cm]{fig/single/id34/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id34/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id34/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id34/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id34/ours_normal.png} \\
\includegraphics[width=1.68cm]{fig/single/id6/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id6/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id6/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id6/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id6/ours_normal_old.png} \\
\includegraphics[width=1.68cm]{fig/single/id62/gt.png} &

\includegraphics[width=1.68cm]{fig/single/id62/arah.png} &
\includegraphics[width=1.68cm]{fig/single/id62/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id62/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id62/ours_normal.png} \\
GT& Arah  &Arah & Ours&  Ours\\
\end{tabular}
% \vspace{-3mm}
%&
%\end{tabularx}
\caption{Comparison against single human method ARAH~\cite{wang2022arah} using 5 training views.The average PSNR in these examples is 24.11/\textbf{27.40} (ARAH/\textbf{Ours}).}
\label{fig:single} 
\vspace{-3mm}
\end{figure}




\section{Implementation Details}
Fig. \ref{fig:network} shows the architecture of our network in more detail (Section 3 in the main submission).
 %, which is used both for the initial $\sdf'$ and final $\sdf$ geometry network. % (Section 3.4 in main submission). 
%  both The geometry initialization network has the same architecture with the SDF network.  
The geometry MLP has 8 layers of width 256,  with a skip connection from the input to the 4th layer. 
The radiance MLP consists of additional 4 layers of width 256, and receives as input the positional encoding of the point $\gamma(p)$, positional encoding of the view direction $\gamma(v)$, rasterized depth feature $f_1$, 
and gradient of the SDF $n(p)$. All layers are linear with ReLU activation, except for the last layer which uses a sigmoid activation function. 
 During training we sample  512  rays  per  batch  and  follow  the  coarse  and fine sampling strategy of \cite{mildenhall2020nerf,wang2021neus}. For a fair comparison, we unified the number of sampled points on each ray for all methods, namely, each ray with $N=64$ coarsely sampled points and $N=64$ finely sampled points for the 
 %inside sphere, 
 foreground,
 and $N=32$ for the 
 background.
%  outside sphere.
%Fig. \ref{fig:pipe} describes the detailed pipeline of our network introduced in Section 3 of the main submission, including the joint optimization in Section3.4. Given a set of multi-view images, and a pre-trained SDF network $\sdf'$, we minimize the following objective:
%
%\begin{align}
%    \mathcal{L} = \loss{c}  + \lambda_{eik} \loss{eik} + \lambda_{sdf} \loss{sdf} + \lambda_{r} \loss{r} + \lambda_{s} \loss{s}, %+\lambda_{kl} L_{kl}+\lambda_{dist} L_{dist}
%\end{align}
%
%where $\loss{c}$ is a L1 reconstruction loss between the rendered image $I_r$ and the ground-truth $I_r^{'}$, $\loss{eik}$ is the Eikonal loss,  an  $\loss{sdf}$ is uncertainty-based SDF loss,  $\loss{r}$ is a novel ray consistency loss and $\loss{s}$ is saturation loss, $\lambda_{eik}, \lambda_{sdf}, \lambda_{r} and \lambda_{s} $ are set to 0.01,0.1,0.1,0.1 respectively. 

Fig. \ref{fig:pipe} illustrates the losses involved in the training of our method. Rays with available ground-truth pixels are supervised with pixel colors. Sub-pixel rays without available ground-truth are supervised using color and density pseudo-ground-truth from neighboring rays.

 \begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{fig/g4903.png}
  \caption{\textbf{Network architecture} (Section 3 of the main submission). 
$p$ is a sampled point along a ray.
$\gamma$ is the positional encoding \cite{tancik2020fourier,mildenhall2020nerf}. $n(p)$ is the gradient of predicted sdf w.r.t the input point $p$. $v$ is the direction of the ray, and $f_1$ the rasterized depth feature described in Section 3.2.}
    \label{fig:network}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{fig/pipe.png}
\caption{Illustration of our losses. Rays without ground-truth are supervised using our ray consistency loss (Section 3.4). For rays corresponding to pixels in the training data, we supervise points using a combination of SDF losses and color losses (Section 3.4).}
\label{fig:pipe}
\end{figure}


\section{Datasets}
% We follow the evaluation setup in P and use the following scans as the test set: 8, 21, 30, 31,
% 34, 38, 40, 41, 45, 55, 63, 82, 103, 110, 114. The following images are used as input: 25, 22, 28, 40, 44, 48, 0, 8, 13. For the 3 and 6 input scenarios, we use the first 3/6 images from the list. All remaining images are used for evaluation except images 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21,
% 36, 37, 38, 39. The input image resolution is 300 $\times$ 400.
We provide here additional details on the evaluation datasets used in Section 4 from the main paper.

\paragraph{CMU Panoptic~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}.}
Our experiments were performed on five different scenes from the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}, where each scene includes originally 30 views located on a spherical spiral. 
The training views were randomly extracted from the HD sequences `Ultimatum' and `Haggling', and contain between 3 and 7 people. 
Specifically, we used frame 9200 from `Haggling', and frames 5500,7800,9200 and 22900 from 'Ultimatum'. 
%We used static frames from those sequences, including frame 9200 from Video 'Haggling' and frame 5500,7800,9200,22900 from 'Ultimatum'. 
We uniformly sampled 5, 10, 15 and 20 views as training and we used the remaining 25, 20, 15 and 10 views respectively as testing. The image resolution in training and testing is $1920 \times 1080$. 


%And camera are locate in spherical spiral. We uniformly sample views for training and used the rest for testing. 
% We follow the evaluation setup in P and use the following scans as the test set: 8, 21, 30, 31,
% 34, 38, 40, 41, 45, 55, 63, 82, 103, 110, 114. The following images are used as input: 25, 22, 28, 40, 44, 48, 0, 8, 13. For the 3 and 6 input scenarios, we use the first 3/6 images from the list. All remaining images are used for evaluation except images 3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21,
% 36, 37, 38, 39. The input image resolution is 300 $\times$ 400.
\paragraph{Synthetic Dataset from MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap}}
Based on the MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap}, we rendered a synthetic dataset with 29 cameras arranged in a sphere. There are three scenes in this dataset with similar backgrounds but different lighting conditions, camera locations and orientations. The scenes contain 1,5 and 10 people respectively. The image resolution is $1920 \times 1080$. We sample 5,10 and 15 views uniformly on each scene for training, and 14 views for testing.