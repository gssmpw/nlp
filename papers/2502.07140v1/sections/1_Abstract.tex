\begin{abstract}
% We present a learning-based method for multiple human rendering from sparse sets of multi-view images. Most current works are focused on single human settings that deliver accurate geometry and appearance using implicit neural representations. However, extending these methods for estimating multiple humans from sparse images remains challenging due to additional occlusion and clutter of multiple humans and the limited number of input views. We propose a neural implicit reconstruction method that addresses the inherent challenges. First, we propose to use geometry constraints by exploiting pre-computed meshes using a human body model (SMPL). Specifically, we regularize the signed distances using the SMPL mesh and leverage bounding boxes for improved rendering. Second, we propose a patch-based ray regularization to minimize rendering inconsistencies and a saturation regularization for robust optimization in variable illumination. Extensive experiments on both real-world and synthetic datasets demonstrate the benefits of our approach and show state-of-the-art performance against existing neural reconstruction methods. 

We present a method for recovering the shape and radiance of a scene consisting of multiple people given solely a few images. 
Multi-human scenes are complex due to additional occlusion and clutter. For single-human settings, existing approaches using implicit neural representations have achieved impressive results that deliver accurate geometry and appearance. 
However, it remains challenging to extend these methods for estimating multiple humans from sparse views. 
We propose a neural implicit reconstruction method that addresses the inherent challenges of this task through the following contributions: First, we propose to use geometry constraints by exploiting pre-computed meshes using a human body model (SMPL). Specifically, we regularize the signed distances using the SMPL mesh and leverage bounding boxes for improved rendering. Second, we propose a ray regularization scheme to minimize rendering inconsistencies, and a saturation regularization for robust optimization in variable illumination.  Extensive experiments on both real and synthetic datasets demonstrate the benefits of our approach and show state-of-the-art performance against existing neural reconstruction methods. 

% Finally, we demonstrate how our framework can be used for editing applications. 
\end{abstract}

% We present a neural learning method for multiple human reconstruction from a sparse set of camera views. Recent neural surface reconstruction methods could generate both 3D geometry and appearance, while usually fail to reconstruct consistent surfaces due to lacking of explicit multi-view geometry constraints, especially for complex scene(\eg multiple human). {\bf Previous works demonstrated that using geometry priors for simple objects significantly enhance the quality of the surfaces. In this work, we propose to extend this idea to using human body model (\eg SMPL) priors, represented by signed distance functions (SDF).} However, since human SMPL usually lacks sufficient details (\eg hair, cloth), we estimate SDF together with its' uncertainty for explicit smooth geometry regularization. In addition, we propose a patch based ray regularization to minimizes potential reconstruction inconsistency and saturation regularization for robust optimization in variable illuminations.  Our evaluations on both real human dataset (CMU Panoptic Dataset\cite{Simon_2017_CVPR,Joo_2017_TPAMI}) and synthetic data (THUman2.0 Dataset and MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap} demonstrate state-of-the-art performance quantitatively and qualitatively. Extensive experiments show our methods enable a variety editing applications in 3D space without additional assistance of depth, masks, or segmentation.

%Existing multi-person reconstruction is often based on human models, which could reconstruct complex geometry but lose rendering quality due to lacking hair and clothing detail. Recently, volumetric rendering (\eg NeRF\cite{mildenhall2020nerf}) demonstrate promising rendering quality on novel views synthesis from dense input views, while reconstructing fidelity surfaces remains a challenge. Moreover, surface presentation methods \cite{yariv2021volume,wang2021neus} explore surface reconstruction during volume rendering, but it's still hard to handle occlusions and complex geometry (\eg multi-human) from sparse input views. Our methods take the advantages of the human model(\eg SMPL), recent volumetric rendering and scene representation methods. Specifically,we define our multi-human SMPL's surfaces as a zero-level set of a signed distance function (SDF) and train a neural SDF representation. We show how to incorporate this representation in point sampling, neural rendering and reconstruction, and propose a joint optimization for non-rigid shapes (\egclothed humans). Finally,

% We present a learning-based method for reconstructing multiple humans from a sparse set of camera views. Current surface reconstruction methods can generate geometry and appearance simultaneously, but suffer to reconstruct 3D consistent surfaces due to a lack of explicit multi-view geometric constraints, especially for complex scenes (e.g. multiple humans). Previous works show that the use of geometric priors for simple objects (or single scenes) significantly improves the fidelity of reconstructed surfaces. Leveraging that, we propose the use of utilizing human body model (e.g. SMPL) priors, represented by signed distance functions (SDF). However, human SMPL often lacks sufficient details (e.g. hair, cloth), and thus, we estimate the SDF along with its uncertainty to obtain smooth geometry regularization. In addition, we propose a patch-based method for ray regularization to potentially minimize inconsistencies in the reconstruction and to enable saturation regularization to increase the robustness during the optimization of variable illuminations. Our evaluations on both real human datasets (CMU Panoptic Dataset [32, 63]) and synthetic data (THUman2.0 Dataset and MultiHuman Dataset [83, 94] demonstrate state-of-the-art performance quantitatively and qualitatively. Extensive experiments show that our method enables a variety of editing applications in 3D space without additional assistance of depth, masks, or segmentation.

