%% For afterwards: ideas from here could be considered to make it all fully automatic? (i.e. not depending on SMPL and bboxes)
%% https://openaccess.thecvf.com/content_CVPR_2019/papers/Rhodin_Neural_Scene_Decomposition_for_Multi-Person_Motion_Capture_CVPR_2019_paper.pdf


%----------------------------------------------------------------------
% \section{Preliminary}
% % 
% % NeuS\cite{wang2021neus} defines a surface as zero-level set of its signed distance function(SDF) and parameterize it with an MLP:
% We define a multi-human surface as the zero-level set of a signed distance function (SDF) $f$, encoded by a Multilayer Perceptron (MPL) $f_{\theta}$ with parameters $\theta$:
% \begin{equation}
% %   S= \left\{p\in R^3|\Gamma_{{\theta}_0}(p)=0 \right\}
%     S= \left\{p\in R^3|f_{{\theta}_0}(p)=0 \right\}
%   \label{eq:sdf}
% \end{equation}
% By combining the SDF representation with volume rendering, the color along a ray $r$ is approximated by:
% \begin{gather} 
% C(r) = \sum_{i=1}^{N}W(p_{i})C(p_{j}), \\
% W(p_{i})=T(p_{i})\alpha(p_{i}), T(p_{i}) = \prod_{j}^{i-1}(1-\alpha(p_{j})),
% \label{eq:T}
% \end{gather}
% where $p_i$ $\{p_i = o +t_id | i = 1,...,N \}$ is a sampled point along a ray $r$ starting at camera center $o$ with direction $d$, $C(p_{j})$ , $W(p_i)$,  $T(p_i)$, $\alpha(p_{i})$ is the predicted color, weight function ,accumulated transmittance, discrete opacity values of $p_i$ respectively. 

% \alpha(p_{i}) = max(\frac{\Phi(d(p_i))-\Phi(d(p_{i+1}))} {\Phi(d(p_i)
% Instead of using predicted discrete opacity values $\alpha(p_{i})$ from the neural radiance field network directly, NeuS\cite{wang2021neus} defines  $\alpha(p_{i})$ as a function of signed distance representation:
% \begin{gather} )},0) 
% \label{eq:alpha_p}
% \end{gather}
% where $d(p_i)$ is the signed distance of $p_i$, $\Phi(d(p_i))=(1+e^{-sp_i})^{-1}$ is the probability density function (PDF) and $s$ is learned and updated during training. Though SDF plays the most important role in this learning and rendering process, the optimization of the whole network actually mainly depends on minimizing the difference between the rendered color and ground truth color, which might result in some erroneous. Since we focus on multiple human reconstructions, the task becomes more difficult due to more complex geometry and occlusions. Thus, we propose to improve the optimization by utilizing a geometry prior as initialization and explicit geometry supervision. 
% %Synthesizing a target novel view from a set of given sources has been a popular and challenge topic in a long run. Conventional approaches include but not limit to  multi-plane images \cite{tucker2020single}, depth layered images \cite{shih20203d}, light field interpolation \cite{mildenhall2019local}, depth based warping \cite{niklaus20193d} \etc. With the advent of neural networks, a series of methods are proposed by learning scene representations from 2D images,  such as voxels \cite{sitzmann2019deepvoxels}, points clouds \cite{xu2022point,wiles2020synsin}, implicit functions \cite{mildenhall2020nerf,zhang2020NeRF++, niemeyer2022regNeRF,yu2021pixelNeRF,yuan2022NeRF} \etc. Among those, NeRF\cite{mildenhall2020nerf} demonstrate impressive rendering results by representing a scene as a neural radiance field, with only calibrated color images, camera poses and intrinsic as input. Follow-up work extend NeRF in many directions, \cite{zhang2020NeRF++,barron2022mip} handle unbounded scene, RegNeRF\cite{niemeyer2022regNeRF} focuses on sparse input views, Neuray\cite{liu2022neural} aims at solving the problem of occlusion in NeRF rendering, \cite{mildenhall2022NeRF} reconstructs novel views from noisy raw images data-set,\cite{wang2022clip,yang2021learning} enables editing during rendering \etc.\\Even-though NeRF could synthesize high-quality images, it's difficult to extract high-fidelity surfaces by only utilizing density-based scene representation. Different with classical surface reconstruction methods(\eg depth-based \cite{barnes2009patchmatch,galliani2015massively} or voxel-based  \cite{de1999poxels,peng2020convolutional},\etc), neural-based approaches (\eg IDR \cite{yariv2020multiview} UNISURF\cite{oechsle2021unisurf}, \etc) reconstruct scene geometry with given multi-view images and its' masks. Recently,Volsdf \cite{yariv2021volume} propose to improve geometry representation and reconstruction in neural volume rendering by defining the volume density function as Laplaceâ€™s cumulative distribution function(CDF) and model it as a function of geometry. NeuS\cite{wang2021neus} uses a signed distance function(SDF) to represent the surface and develop a new volume rendering method to train a neural SDF representation. Compared with NeRF related works(\eg \cite{ mildenhall2020nerf,zhang2020NeRF++} \etc),it could produce more accurate surface reconstructions. \\A neural radiance field (NeRF)\cite{ mildenhall2020nerf} represents a 3D scene as a continuous function of color volume densities. 
%----------------------------------------------------------------------
 \begin{figure*}[ht]
\vspace{-6mm}
\centering
\includegraphics[width=.65\linewidth]{./fig/g2648.png}
  %\vspace{-5mm}
  \caption{\textbf{Overview.} We address the multi-human implicit shape and appearance learning problem by initializing the geometry using SMPL  (Sec.~\ref{sec:method_geometric_init}), along with uncertainty-based SDF supervision and novel photometric regularizations designed to compensate for the lack of views (Sec.~\ref{sec:method_regularizations}). We also model the foreground (Union of SMLP bounding boxes) and remainder of the scene seperatelty (Sec.~\ref{sec:method_bboxes}).}
  
%   \caption{Overview: To address the challenging of reconstructing multiple humans geometry and appearance from sparse set of views, we propose to first learn an implicit signed distance(SDF) representation independently by encoding SDF value sampled from multiple humans SMPL per point as geometry priors for geometry network. The radiance MLP encodes the learned features from geometry network, rasterized depth feature from multi-human SMPL, sampled points(p) and its ray direction(v) to predict color per point for volumetric integration. Finally, we optimize the learning process together with sdf regularization, image reconstruction regularization,  a ray consistency regularization, and a saturation regularization. }
  \label{fig:pipe}
\vspace{-5mm}
\end{figure*}

%------------------------------------------------------
\section{Method}
\label{sec:method}
Given a sparse set of views $\{I_i\}_{i=1}^N$ of a multi-human scene with camera intrinsics and extrinsics ${\{ K_i, [R|t]_i \}}$, our goal is to reconstruct geometry and synthesize the appearance of multiple humans from arbitrary viewpoints. The pipeline is illustrated in Fig.~\ref{fig:pipe}. Our approach builds on NeuS  \cite{wang2021neus}, which combines an implicit signed distance representation for geometry with volumetric rendering. 
%Since the accuracy of volumetric rendering depends on the density function of predicted SDF, the key network to optimize is SDF network. 
In order to solve the challenging case of multiple humans occluding each other, we hypothesize that a naive RGB reconstruction loss is insufficient and propose to use a strong geometric prior before training with multi-view images. 
Towards this, we first train the implicit SDF network independently by leveraging off-the-shelf SMPL estimations (Sec.~\ref{sec:method_geometric_init}). 
To handle details and represent appearance, the geometry network is then fine-tuned considering foreground and background objects. Moreover, we propose the use of hybrid bounding box rendering to handle the multi-human setting (Sec.~\ref{sec:method_bboxes}). 
% To boost the quality and consistency of the renderings given sparse views, 
Additionally, we define an explicit SDF constraint based on the uncertainty of the SMPL estimations, together with a ray consistency loss, and a saturation loss to improve image rendering quality for sparse views (Sec.~\ref{sec:method_regularizations}). 
% Our proposed techniques allow to successfully handle the multi-human case, as will be demonstrated in the next section.
% Finally, we show how our method can be used for multiple human scene editing without retraining \todo{Mention this in intro!!}. 


% Our proposed method is based on the observation that a good appearances require a good geometric prior, even more so when the scene contains multiple occlusions. 
% Prior to NeuS \cite{wang2021neus}, we propose to learn SDF together with uncertainty from the human bodies model as geometry priors. Then, we add the learned weights as a geometry initiation to SDF network and propose to utilize multiple 3D bounding boxes to locate multiple humans for hybrid rendering purpose. 
% What's more, we define an explicit SDF constraints to improve the geometry learning, together with patch-based ray consistency loss and saturation loss to improve image rendering quality. Finally, we show our method enables multiple human scene editing without retraining. 
%------------------------------------------------------
\subsection{Scene Representation and Rendering}
% 
% NeuS\cite{wang2021neus} defines a surface as zero-level set of its signed distance function(SDF) and parameterize it with an MLP:
We define a multi-human surface $\mathcal{S}$ as the zero-level set of a signed distance function (SDF) 
%$f_{\theta} : \real^3 \to \real$, 
$\sdf : \real^3 \to \real$, encoded by a Multilayer Perceptron (MLP) $\sdf$ with parameters $\theta_0$:
%
\begin{equation}
%   S= \left\{p\in R^3|\Gamma_{{\theta}_0}(p)=0 \right\}
    % \mathcal{S} = \left\{p\in \real^3|f_{{\theta}}(p)=0 \right\}.
    \mathcal{S} = \left\{p\in \real^3|\sdf(p)=0 \right\}.
  \label{eq:sdf}
\end{equation}
% 
Following NeuS~\cite{wang2021neus}, we train the geometry network $\sdf$ along with a color network 
%$c : \real^3 \times \mathbb{S}^2 \to \real^3$ that maps a point $\vect{p}$ and view direction $\vect{v}$ to RGB values. % using volume rendering.
$\colornet$, with parameters $\theta_1$, mapping a point $\vect{p}$ %and view direction $\vect{v}$ 
to color values (more details in Sec.~\ref{sec:method_bboxes}). 
% By combining the SDF representation with volume rendering, the color along a ray $r$ is approximated by:
Combining the SDF representation with volume rendering, we approximate the color along a ray $r$ by:
%
\begin{gather} 
    %C(\vect{o}, \vect{v}) = \sum_{i=1}^{N} w(\vect{p}_i) c(\vect{p}_i, \vect{v}), \\
    C(r) = \sum_{i=1}^{N} w(\vect{p}_i) \colornet(\vect{p}_i), \\
    w(\vect{p}_i) = T(\vect{p}_i) \alpha(\vect{p}_i), \\
    T(\vect{p}_i) = \prod_{j}^{i-1}(1-\alpha(\vect{p}_j)),
\label{eq:T}
\end{gather}
%
where $\vect{p}_i = \vect{o} + t_i \vect{v}$ %, $i = 1,...,N$ 
is a sampled point along the ray $r$ starting at camera center $\vect{o}$ with direction $\vect{v}$; 
$\colornet(\vect{p}_i)$ is the predicted color at $\vect{p}_i$, $w(\vect{p_i})$ is the weight function,  $T(\vect{p}_i)$ is the accumulated transmittance, and $\alpha(\vect{p_i})$ is the opacity value. 
Following NeuS, $\alpha(\vect{p_i})$ is defined as a function of the signed distance representation:
%
\begin{gather} 
    \alpha(\vect{p}_i) = max\left(\frac{\Phi(\sdf(\vect{p}_i))-\Phi(\sdf(\vect{p}_{i+1}))} {\Phi(\sdf(\vect{p}_i))},0 \right) 
\label{eq:alpha_p}
\end{gather}
%
where $\sdf(\vect{p}_i)$ is the signed distance of $\vect{p}_i$, $\Phi(\sdf(\vect{x}))=(1+e^{-s \vect{x}})^{-1}$ is the cumulative distribution function (CDF) of the logistic distribution, and $s$ is a learnable parameter (see \cite{wang2021neus} for more details). 
%Though SDF plays the most important role in this learning and rendering process, the optimization of the whole network actually mainly depends on minimizing the difference between the rendered color and ground truth color, which might result in some erroneous. Since we focus on multiple human reconstructions, the task becomes more difficult due to more complex geometry and occlusions. Thus, we propose to improve the optimization by utilizing a geometry prior as initialization and explicit geometry supervision. 
%
%--------------------------------------------------------------
\subsection{Geometric Prior}
\label{sec:method_geometric_init}
Typically, the SDF function $\sdf$ and the color function $\colornet$ are simultaneously optimized by minimizing the difference between the rendered and ground-truth RGB values~\cite{yariv2020multiview,mildenhall2020nerf,wang2021neus}. While this allows to train without the need for geometric supervision, it has been noted that a photometric error alone is insufficient for the challenging sparse-view setting \cite{deng2022depth,roessle2022dense}, since there are not enough images to compensate for the inherent ambiguity in establishing correspondences between views. For the \emph{multi-human} setting this becomes more problematic, as correspondences are even more ambiguous due to clutter. 

To address this, we propose to regularize using geometric information by first independently training $\sdf$ using off-the-shelf SMPL fittings, which can be robustly computed from the input data. 
We train this network in a supervised manner by sampling points with their distance values as in \cite{park2019deepsdf}. 
% As shown in the preliminary part, the existing surface reconstruction method(\eg NeuS \cite{wang2021neus}) combines volume rendering integral with point-based signed distance function (SDF) by defining density value learned in volumetric rendering as a function of signed distance representation.  The hierarchical sampling and color rendering in those methods highly depends on the accuracy of the weight function, which is represented by SDF. However, the overall optimization is based on minimizing the difference of rendered RGB value and ground truth RGB value, which might cause bias to the learning of SDF representation and result in erroneous due to lacking of SDF supervision. To address this challenge, we propose to improve the SDF representation (namely $\Gamma_{{\theta}_0}(p)$ in Equation \ref{eq:sdf}) by adding explicit geometry initialization and supervision.\\
% Specifically, we use $\Gamma_{{\theta}_0}(p)$ to model multiple human surfaces as a zero-level set of SDF. To reduce extra cost, we utilize commonly used human body models(\eg SMPL \cite{loper2015smpl}) to train this neural SDF representation.
%\textcolor{red}{This will change, the is no local sdfs but just one like neus}\\
% The surface of the object could be represented by the zero-level set of its signed distance function(SDF):
% \begin{equation}
%   S= \left\{p\in R^3|{f_\theta}_0(p,z)=0 \right\}
%   \label{eq:important}
% \end{equation}
% where p is the sampled point, z is the latent code, and ${f_\theta}_0(p,z)$ is a signed distance function with learn-able parameters $\theta$. For any point $p$ on the object surface, its signed distance $d(p)$ satisfy $d(p) = 0$.\\
% Instead of using explicitly parametric surface model or splitting the 3D multi-human mesh individually, we map global coordinates $p$ to a local region around each human by define:
% \begin{equation}
%     T_l(p) = p -p_l
%   \label{eq:important}
% \end{equation}
% $p_l$ is the center point of each humans SMPL.  \\
% Thus, the suface of multi-human could be expressed as follows, 
% \begin{equation}
%   S= \left\{ p\in R^3|\bigcup_{l\in L} {f_\theta}_0(T_l(p),z_l)=0 \right\}
% \end{equation}
% $L$ is the total number of humans, $z_l$ is the latent code of each person. \\
Given that SMPL can only coarsely represent the real surface, we treat this geometry as a ``noisy'' estimate that will be later improved upon using the multi-view images. Preparing for this, and inspired by \cite{deng2022depth,roessle2022dense}, we model the SMPL ``noise'' as a Gaussian distribution $\mathcal{N}(0,{s_{noise}(\vect{p}_j})^2)$ with standard deviation $s_{noise}(\vect{p}_j)$, and train $\sdf$ to output an estimate of the uncertainty $s_{noise}(\vect{p}_j)$ along with the distance value; that is, $\sdf(\vect{p}_j) = (d_j,s_{noise})$. The geometry network $\sdf$ is then optimized by minimizing the negative log-likelihood of a Gaussian:
%
% we perturb the SDF with Gaussian noise $\mathcal{N}(0,s_{noise(d)^2})$.
% The neural surface representation ${f_\theta}_0$ is optimized by minimizing the negative log-likelihood of a Gaussian: 
%
\begin{equation}
    \mathcal{L}_s = \frac{1}{n}\sum_{j=1}^{n} \left( log(s_{noise}(\vect{p}_j)^2) + \frac{{(d_j-d'_j)}^2}{ s_{noise}(\vect{p}_j)^2} \right), 
\end{equation}
where $n$ is the number of sampled points, $d_j$ is the predicted SDF value for point $\vect{p}_j$, and $d_j'$ is the signed distance sampled directly from the SMPL meshes. 

%--------------------------------------------------------------
% \subsection{Multi-View Training using Hybrid Rendering}
\subsection{Hybrid Rendering with Geometry Constraints}
\label{sec:method_bboxes}

To work with unbounded scenes, NeRF++ \cite{zhang2020NeRF++} proposed to separately model the foreground and background geometries using an inverted sphere parameterization, where the foreground is parameterized within an inner unit sphere, and the rest is represented by an inverted sphere covering the complement of the inner volume. We follow this and train separate models for foreground and background. Specifically, we use a simple NeRF~\cite{mildenhall2020nerf} architecture for the background and train the foreground model using $\sdf$ and the color network $\colornet$, where the output color $C(\vect{p}_i)$ is predicted as:
%
% After the scene representation network $\Gamma_{{\theta}_0}$ , we encode the color network into a MLP $\Gamma_{{\theta}_1}$  to predict color per point. For a sampled point $p_i$ ${\left\{p_i/p_i = o + t_iv\right\}}$ on a ray $r$ with camera origin $o$ and direction $v$, its color $c_i$ could be predict by:
%
\begin{equation} 
C(\vect{p}_i) = \colornet ( \gamma(\vect{p}_i), \gamma(\vect{v}_i), f_0,f_1).\\
\label{eq:colornet}
\end{equation}
Here, $\gamma(\vect{p}_i)$ and $\gamma(\vect{v}_i)$ are the positional encodings \cite{tancik2020fourier,mildenhall2020nerf} of the sampled point $\vect{p}_i$ and its ray direction $\vect{v}_i$, and $f_0$ includes the gradients of predicted SDF and predicted feature from the geometry network $\sdf$ \cite{yariv2020multiview}. Additionally, to inject geometric prior knowledge into the appearance network we condition $\colornet$ on the rasterized depth feature from the corresponding SMPL mesh. 
%Following NeRF++ and related works \cite{zhang2020NeRF++,wang2021neus,yariv2020multiview},  we use above network to model the foreground and NeRF \cite{mildenhall2020nerf} to model background separately. 

For reconstructing multiple humans, one difficulty in modeling the foreground as in NeRF++ is that the bounding sphere will contain a large empty space, making it costly to search for the surface during hierarchical sampling and adding non-relevant points to the training. To resolve this, we propose to use instead multiple 3D bounding boxes as the foreground volume. 
Specifically, we define a bounding box $B^j$ for the $j-$th human using the SMPL fittings, with minimum and maximum coordinates $[B_{min}^j - \delta, B_{max}^j + \delta]$, where $B_{min}^j$ and $B_{max}^j$ are the minimum and maximum coordinates of SMPL along the $x,y,z$ axes respectively, and $\delta$ is a spatial margin (here we set to $0.1$). The foreground volume is then defined as $B = \cup_{j=1..M} B^j $, and we define $b(\vect{p}_i)$ as
%
% However, if modeling the foreground as one sphere containing all humans like NeRF++\cite{zhang2020NeRF++}, the foreground still contains huge empty space, introducing costs in searching for true surfaces in the hierarchical sampling and also adding non-relevant points to rendering. To solve this, we take multiple 3D bounding boxes spatially locating multiple people in 3D space as the foreground. Each 3D bounding box $B$ is defined around a human SMPL, $B \in [B_{min}-\epsilon, B_{max}+\epsilon]$, where $B_{min}$ and $B_{max}$ are the minimal and maximal 3D coordinates along $x,y,z$ axis respectively. $\epsilon$ is error bound of spatial margin, here we set to 0.1. If a sampled point $p_i$ is inside the predefined box, $b(p_i) = 1$, otherwise,  $b(p_i) =0$, namely,
%
\begin{equation} 
    b(\vect{p}_i) =\left\{\begin{matrix}
    1,p_{i} \in B,\\
    0,p_{i} \notin B\\
 \end{matrix}\right.
\label{eq:in_box}
\end{equation}
%

For points that fall inside the foreground, $\vect{p} \in B$, 
we calculate the opacity value $\alpha^{FG}(\vect{p}_{i})$ using the predictions of $\sdf(\vect{p}_i)$ according to Eq. \ref{eq:alpha_p}, and the color $C(\vect{p_i})^{FG}$ using $\colornet$. 
The points that fall outside the bounding box are modeled as background using a NeRF model, where the opacity is calculated as $\alpha^{BG}(p_{i}) = 1-e^{\sigma(p_i)\delta(p_i)}$, with $\delta$ and $\sigma$ defined as in~\cite{mildenhall2020nerf}, and the color $C^{BG}$ is predicted using $\alpha^{BG}$. %In summary, 
Given a point $\vect{p}_i$, its color and opacity values are updated as follows:
\begin{gather} 
C(\vect{p}_{i}) = b(\vect{p}_i) C^{FG}(\vect{p}_{i})+(1-b(\vect{p}_i)) C^{BG}(\vect{p}_{i})\\
\alpha(\vect{p}_{i})=b(\vect{p}_i) \alpha^{FG}(\vect{p}_{i})+(1-b(\vect{p}_i)) \alpha^{BG}(\vect{p}_{i}) 
\label{eq:rendering}
\end{gather}

Finally, following \cite{azinovic2022neural}, given a ray $r$ with $n$ sampled points $\{\vect{p}_i = \vect{o} + t_i \vect{v} \}_{i = 1}^{n}$, the color is approximated as:
\begin{gather} 
C(r) =\frac {\sum_{i=1}^{N}W(\vect{p}_{i}) C(\vect{p}_{i})}{ \sum_{i=1}^{N} W(\vect{p}_{i})},
\label{eq:raycolor}
\end{gather}
where $W(\vect{p}_{i}) = T(\vect{p}_{i}) \alpha(\vect{p}_{i})$, $T(\vect{p}_{i}) = \prod_{j}^{i-1}(1-\alpha(\vect{p}_{j}))$. This function allocates higher weights to points near the surface and lower weights to points away from the surface, and is used to improve the rendering quality. 
%--------------------------------------------------------------

%\subsection{Multi-view consistency}

%\textcolor{red}{This will change}%, this is a ray consistency loss, that does L1 on ray color, and KL on ray density, using surrounding pixels as GT}\\\todo{This will change, this is a ray consistency loss, that does L1 on ray color, and KL on ray density, using surrounding pixels as GT}\\

% \begin{figure}[h!]
% \vspace{-8mm}
% \centering
% \includegraphics[width=\linewidth]{fig/draw11.jpg}
%   \caption{Overview of.}
%   \label{fig:inter}
% \vspace{-5mm}
% \end{figure}

%--------------------------------------------------------------
\subsection{Optimization}
\label{sec:method_regularizations}

Given a set of multi-view images, and a pre-trained SDF network $\sdf'$ (Sec.~\ref{sec:method_geometric_init}), we minimize the following objective:
%
\begin{align}
    \mathcal{L} = \loss{r}  + \lambda_{eik} \loss{eik} + \lambda_{sdf} \loss{sdf} + \lambda_{r} \loss{r} +\lambda_{s} \loss{s}, %+\lambda_{kl} L_{kl}+\lambda_{dist} L_{dist}
\end{align}
%
where $\loss{r}$ is a L1 reconstruction loss between the rendered image $I_r$ and the ground-truth $I_r^{'}$ and $\loss{eik}$ is the Eikonal loss~\cite{gropp2020implicit}. 
% used to regularize the SDF of sampled points near surface~\cite{gropp2020implicit}. 

% V: no need to put the equation in my opinion
% \begin{align}
% L_r = ||I_r - I_r^{'}||^1_1, L_{eik} = (||\nabla f_{sdf}(p_i)|| - 1)^2
% \end{align}
%
Additionally, we propose to use an uncertainty-based SDF loss $\loss{sdf}$, a novel ray consistency loss $\loss{r}$ and saturation loss $\loss{s}$ which are explained in the following.

\paragraph{SDF Loss.} 
% Assuming that when sampled points are far from human SMPL, its' SDF from SMPL with naked human bodies could be approximated to SDF from clothed human bodies. While for points near human bodies, sightly movement will influence the accuracy of surface reconstruction. Based on this, we set the following conditions, if the sampled points are not in our predefined box or the absolute value of prediction SDF from the initialize network is bigger than the defined threshold $\xi_0$ or the standard deviation is bigger than the threshold $\xi_1$, the proposed SDF regularization would be adopted. Thus,  SDF optimization $L_{sdf}$ is defined as follows:
% \begin{align}
% L_{sdf} = \sum_{p_i \in P^{off}}|f_{sdf}(p_i) - f_{sdf}^{initia}(p_i)|.
% \end{align}
As detailed in Sec.~\ref{sec:method_geometric_init}, we treat the SMPL mesh as a noisy estimate of the real surface.
When the sampled points are not within the foreground box $B$, or the absolute sdf value predicted by the geometry network $\sdf$ %and those produced by the initial network $\sdf'$ % here I chanege to the equation, not like depth paper
is greater than a pre-defined threshold $\xi_0$, or the standard deviation $s_j = s_{noise}(\vect{p})_j$ is bigger than the threshold $\xi_1$, we use the following loss:
%
\begin{equation} 
    \loss{sdf} =\left\{\begin{matrix}
    \frac{1}{n}\sum_{j=1}^{n}(log(s_{j}^2)+\frac{(d_j^{'}-d_{j})^2}{s_j^2}),\\  s.t.  (\vect{p}_{i} \notin B, |d_j|>\xi_0  \quad or  \quad s_j>\xi_1)\\
        0,otherwise\\
 \end{matrix}\right.
\label{eq:sdfloss}
\end{equation}
%
where $d_j$ and $d_j^{'}$ are the SDF predictions from the final $\sdf$ and initial network $\tilde{\sdf}$, and $\xi_0$, $\xi_1$ are set to 0.2 and 0.5, respectively. This function encourages the network to maintain geometry consistency during learning while allowing some freedom to learn the details encoded in the images.

\paragraph{Ray Consistency Loss.}
We introduce the following ray consistency loss $\loss{r}$ to ensure photometric consistency across all images under sparse views: 
% Except above regularization focusing on the human bodies, we introduce $L_{p}$ for ray consistency across all images.
% Followed \cite{mipnerf360}, we add $L_{dist}$ to eliminate floaters and prevent background collapse,
% \begin{align}
% L_{dist} = \sum_{i,j}w_iw_j|\frac{s_i+s_{i+1}}{2}-\frac{s_j+s_{j+1}}{2}|+\frac{1}{3}\sum_{i}w_i^2(s_{i+1}-s_{i})
% \end{align} 
\begin{align}
%L_{kl} = D_{kl}(C(r_{p}) || C(r_{p}^{*}))=\sum_{p=1}^{N}\tilde{C}(r_{p}) log\frac{\tilde{C}(r_{p})}{C(r_{p}^{*})}
%L_{kl} =\sum_{p=1}^{N}\tilde{C_{p}}(r) log\frac{\tilde{C_{p}}(r)}{C_{p}(r^{*})}
\loss{r} = ||C(r_i) - C(r^{*})||_1 + D_{KL}(P(r_i)||P(r^*))%\sum_{p=1}^{N}\frac{\sum_{i=1}^{M}{C}(r_{i})}{M} log\frac{\frac{\sum_{i=1}^{M}{C}(r_{i})}{M}}{C(r^{*})}
\label{eq:kl}
\end{align}
where $C(r_i)$ is the ground truth color of a randomly sampled ray $r_i$ on a small patch and $C(r_{p}^{*})$ denotes the rendered color of an interpolated ray on a small patch. Inspired by \cite{kim2022infonerf}, we introduce a KL-divergence regularization for the ray density, where $P(r_i)= \frac{\alpha_i}{\sum_{i=1}^{N}a_{i}} $. 
%
% $M$ is the number of sampled rays on a small patch and $N$ is the total number of patches,  $r^*$ is interpolate ray on a small patch.
%Given two rays for example, the interpolated ray could be denote as $r^* = \frac{sin[(1-t)\theta] \cdot r_1+sin(t\theta) \cdot r_2}{sin \theta}$, see Fig \ref{fig:inter} . Since we sample rays on a small patch, the angle between two rays is quite small, namely  $\theta \rightarrow 0, sin \theta \approx \theta$, thus $r^*\approx (1-t) \cdot r_1+ t \cdot r_2 $. For multiple rays on a small patch, 
%r^*=\sum_{i=1}^{M}{t_i} \cdot r_{i}, t_i = \frac{e^{r_{i}}}{\sum_{j=1}^{M}{e}^{r_{j}}}$.
The goal of this loss is to ensure consistency and smoothness of unseen rays by constraining the interpolated rays on a small patch to have a similar distribution, both for color and density.
%
% The objective behind this formulation, by interpolating rays on a small patch, the interpolated ray ought to have similar distribution(\ie color, density) with its neighbors.  The consistency and smoothness across different rays are guaranteed by introducing the color and density regularization of unseen rays.\\
\paragraph {Saturation Loss.}
Finally, we observe that real-world images might contain variable illumination or transient occluders among different views (this is the case for example in the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}),  which can degrade the rendering quality due to inconsistency across views. Instead of learning complex transient embeddings as in \cite{martin2021nerf}, we propose to simply convert the RGB image into the HSV space, and calculate the L1 reconstruction loss of the saturation value between the rendered image and the ground truth: $\loss{s} = ||I_s - I_s^{gt}||_1$.
% \begin{align}
% \loss{s} = ||I_s - I_s^{gt}||_1.
% \end{align} 

% The effectiveness of this function will be evaluated in ablations. 

%--------------------------------------------------------------Noticing that when given sparse input views, the model prone to over-fit to seen views,  resulting in inaccurate and distort estimations of unseen viewpoints. Based on this intuition, we propose a patch based regularization to ensure the consistency across different rays.

% \section{Application: Scene Editing}
% The proposed bounding box definition and hybrid scene rendering scheme additionally enables to edit the scene in 3D space during the inference stage, without any retraining. Given a point $\vect{p}_i$ associated with a bounding box $B^j$, we can rotate and translate the point as:
% %
% \begin{align}
% \tilde{\vect{p}}_i = \mat{R} \vect{p}_i + \mat{T}, \tilde{B^j}_i = \mat{R} B^j + \mat{T},
% \label{eq:dkl}
% \end{align}
% %
% where $R$ and $T$ are the camera matrix of rotation and translation respectively, $\tilde{p}_i $ and $ \tilde{B}_i$ are points and 3D bounding box of original  $p_i $ and $ B_i$  represented in its target view direction. The direction of those points will be updated by following equation at the same time $\tilde{d_i} = (\tilde{p}_i - o) / (|\tilde{p}_i - o|)$ where $o$ is the camera origin.\\
% % \begin{gather} 
% % \tilde{d_i} = (\tilde{p}_i - o) / (|\tilde{p}_i - o|)
% % \label{eq:T}
% % \end{gather}
% %  For point $\tilde{p}_i \in \tilde{B}_i$,  $b(\tilde{p}_i) =1 $, otherwise, $b(\tilde{p}_i)=0 $. 
% Thanks to our defined box rendering in Equation \ref{eq:rendering},  the color and density of a point $p_i$ after editing could be updated by following:
% \begin{gather}
% \left\{\begin{matrix}C(p_i) = C(\tilde{p_{i}}), \alpha(p_i) =\tilde{p_{i}}, \quad \tilde{p}_i \in \tilde{B}_i,\\
% C(p_i) = C({p_{i}}),\alpha(p_i) =\alpha({p_{i}}),\quad \tilde{p}_i \notin \tilde{B}_i,
% \end{matrix}\right.
% \label{eq:in_box}
% \end{gather}
% With the updated color and the density, we still utilize equation \ref{eq:raycolor} to render the final ray color. 
% %--------------------------------------------------------------
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% \begin{figure*}
%   \centering
% %   \begin{subfigure}{0.68\linewidth}
% %   \includegraphics[width=\linewidth]{fig/cmu/105/neus.png} 
% %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
% %     \caption{An example of a subfigure.}
% %     \label{fig:short-a}
% %   \end{subfigure}
% %   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \includegraphics[width=\linewidth]{fig/cmu/105/neus.png} &    \includegraphics[width=\linewidth]{fig/cmu/105/neus.png} 
%   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}



