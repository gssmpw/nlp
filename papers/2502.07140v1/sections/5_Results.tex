
\begin{figure*}[h!]%[t] %[bp] %[h!] %[ht]
\flushleft %\centering%
\vspace{-3mm}
\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\scalebox{1}{
\begin{tabular}{c cccc ccc}
\rotatebox{90}{\quad 5} 
\includegraphics[width=2.5cm]{fig/cmu/4_5/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_5/ours_normal.png} \\
\rotatebox{90}{\quad 5} 
\includegraphics[width=2.5cm]{fig/cmu/7_5/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_5/ours_normal.png} \\
\rotatebox{90}{\quad 10} 
\includegraphics[width=2.5cm]{fig/cmu/4_10/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/4_10/ours_normal.png} \\
\rotatebox{90}{\quad 10} 
\includegraphics[width=2.5cm]{fig/cmu/7_10/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_10/ours_normal.png} \\
\rotatebox{90}{\quad 15}
\includegraphics[width=2.5cm]{fig/cmu/6_15/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_15/ours_normal.png} \\
\rotatebox{90}{\quad 15}
\includegraphics[width=2.5cm]{fig/cmu/7_15/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/7_15/ours_normal.png} \\
% \rotatebox{90}{\quad 20}
% \includegraphics[width=2.5cm]{fig/cmu/7_20/gt.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/neus_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/vol_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/ours_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/neus_normal.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/vol_normal.png} &
% \includegraphics[width=2.5cm]{fig/cmu/7_20/ours_normal.png} \\
\rotatebox{90}{\quad 20}
\includegraphics[width=2.5cm]{fig/cmu/5_20/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/5_20/ours_normal.png} \\
\rotatebox{90}{\quad 20}
\includegraphics[width=2.5cm]{fig/cmu/6_20/gt.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/neus_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/vol_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/ours_color.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/6_20/ours_normal.png} \\
% \rotatebox{90}{\quad 20}
% \includegraphics[width=2.5cm]{fig/cmu/4_20/gt.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/neus_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/vol_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/ours_color.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/neus_normal.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/vol_normal.png} &
% \includegraphics[width=2.5cm]{fig/cmu/4_20/ours_normal.png} \\
Ground Truth &NeuS &VolSDF & Ours  & Neus &Volsdf & Ours\\
\end{tabular}}
%&
%\end{tabularx}
% \vspace{-2mm}
 \caption{Qualitative comparison against NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} of synthesised novel views and reconstructed normal images of multiple humans on CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI}, using 5/10/15/20 training views.} 
\label{fig:cmu} 
%\vspace{-3mm}
\end{figure*}

%%--------------------------------------------------------------
\begin{table*}[h!] %[h!][bp]
%\vspace{-3mm}
\normalsize
\begin{center} %
 %\flushleft 
 \scalebox{0.82}{
\begin{tabular}{l|c|cccc|cccc|cccc}
\toprule[1.2pt]
Scene&Method  &\multicolumn{4}{c|} {\bf PSNR↑} & \multicolumn{4}{c|}{\bf SSIM↑}& \multicolumn{4}{c} {\bf LPIPS↓}  \\% & \multicolumn{3}{c}{\bf Increase number} \\
 &&5&10   &15  &20 &5  &10  &15  &20&5 &10   &15 &20\\
 %------------------------------1----------------------------------- 
\hline
 & NeuS &17.83& 18.84&19.39 &21.97&0.62&0.67&0.69&0.55&0.74&0.51&\textcolor{textcolor}{\textbf{0.49}}&\textcolor{textcolor}{\textbf{0.45}}  \\
  %& \multirow{9}{*}{\includegraphics[width=0.23\textwidth]{fig/increase.png}}\\
1&VolSDF &17.50&18.08 &19.51 &22.31&0.64 &0.61&0.67&0.71&0.61&0.54&0.51&0.48\\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{18.41}}&\textcolor{textcolor}{\textbf{20.32}} &\textcolor{textcolor}{\textbf{21.60}}&\textcolor{textcolor}{\textbf{23.19}}&\textcolor{textcolor}{\textbf{0.67}} &\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.74}}&\textcolor{textcolor}{\textbf{0.55}} &\textcolor{textcolor}{\textbf{0.50}}&0.50&0.49\\
 %----------------------2--------------------------------------
\cline{0-13}
 & NeuS&16.87& 18.51&19.40&21.05&0.60 &0.65&0.70&0.71&0.57&0.53&0.51&0.49 \\
2& VolSDF &16.36&17.52 &19.40 &21.60 &0.57&0.59&0.67&0.70&0.62&0.53&0.49&0.47\\
 %& \textbf{Ours}&\textbf{20.21}& \textbf{20.88}&\textbf{21.93}&\textbf{0.71} &\textbf{0.72}&\textbf{0.72}&\textbf{0.50}&\textbf{0.48}&\textbf{0.47}\\
  &\textbf{Ours}&\textcolor{textcolor}{\textbf{19.72}}&\textcolor{textcolor}{\textbf{21.15}} &\textcolor{textcolor}{\textbf{21.40}}&\textcolor{textcolor}{\textbf{23.12}}&\textcolor{textcolor}{\textbf{0.70}} &\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.74}}&\textcolor{textcolor}{\textbf{0.50}} &\textcolor{textcolor}{\textbf{0.49}}&\textcolor{textcolor}{\textbf{0.48}}&\textcolor{textcolor}{\textbf{0.47}}\\
 %---------------------3------------------------------------------
\cline{0-13}
 & NeuS &16.03&17.39&19.17 &21.21&0.56&0.61&0.70&0.73&0.62   &0.54&\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.46}}\\
3& VolSDF &16.36&18.21& 19.56&21.06&0.57&0.59&0.64&0.68&0.62&0.52 &0.48&0.47\\
% & \textbf{Ours}&\textbf{19.41}& \textbf{20.26}&\textbf{21.99}&\textbf{0.69} &\textbf{0.71}&\textbf{0.72}&\textbf{0.51}&0.49&0.48\\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{18.57}}&\textcolor{textcolor}{\textbf{20.94}} &\textcolor{textcolor}{\textbf{21.86}}&\textcolor{textcolor}{\textbf{23.16}}&\textcolor{textcolor}{\textbf{0.66}} &\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.74}}&\textcolor{textcolor}{\textbf{0.74}}&\textcolor{textcolor}{\textbf{0.52}} &\textcolor{textcolor}{\textbf{0.48}}&\textcolor{textcolor}{\textbf{0.47}}&0.47\\
 %----------------------4---------------------------------------
\cline{0-13}
&NeuS & 14.16&17.14 &19.87&21.37&0.49&0.51&0.70&0.72&0.60&0.57&0.48&0.46\\
%& \multirow{9}{*}{\includegraphics[width=0.24\textwidth]{fig/increase.png}}\\
4& VolSDF &13.51& 17.07 &18.68&20.89&0.50&0.57&0.65&0.68&0.64&0.54&0.53&0.46\\
% & \textbf{Ours}&\textbf{19.87}& \textbf{20.58}&\textbf{21.96}&\textbf{0.69} &\textbf{0.71}&\textbf{0.72}&\textbf{0.50}&\textbf{0.46}&\textbf{0.44}\\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{19.54}}&\textcolor{textcolor}{\textbf{20.94}} &\textcolor{textcolor}{\textbf{21.35}}&\textcolor{textcolor}{\textbf{23.29}}&\textcolor{textcolor}{\textbf{0.69}} &\textcolor{textcolor}{\textbf{0.72}}&\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.75}}&\textcolor{textcolor}{\textbf{0.50}} &\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.45}}\\
 %-------------5----------------------------------------------------------
\cline{0-13}
 & NeuS &17.69& 18.60&20.03 &21.50&0.57 &0.62&0.69&0.70&0.55&0.54&0.50& \textcolor{textcolor}{\textbf{0.47}}\\
5& VolSDF &14.85& 17.32&19.04 &20.91&0.53&0.57&0.66&0.68&0.63&0.58&0.53&0.48\\
% &\textbf{Ours}&\textbf{20.05}&\textbf{20.54} &\textbf{22.27}&\textbf{0.69} &\textbf{0.69}&\textbf{0.72}&\textbf{0.51}&\textbf{0.49}&\textbf{0.46}\\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{19.34}}&\textcolor{textcolor}{\textbf{20.55}} &\textcolor{textcolor}{\textbf{21.08}}&\textcolor{textcolor}{\textbf{22.55}}&\textcolor{textcolor}{\textbf{0.67}} &\textcolor{textcolor}{\textbf{0.70}}&\textcolor{textcolor}{\textbf{0.72}}&\textcolor{textcolor}{\textbf{0.72}}&\textcolor{textcolor}{\textbf{0.51}} &\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.47}}\\
 \cline{0-13}
 %-------------Average----------------------------------------------------
%\hline 
 & NeuS &16.52&17.79 &19.57 &21.42&0.57&0.62 &0.69	&0.72 &0.58	&0.54 &	0.49 &	\textcolor{textcolor}{\textbf{0.47}} \\
\textbf{Average}& 
VolSDF &15.81&17.68 &19.23&21.35 &0.56&0.59&	0.66 &	0.69 &0.62&	0.54	&0.50 &\textcolor{textcolor}{\textbf{0.47}}\\
% &\textbf{Ours} &\textbf{19.76} &\textbf{20.47} &	\textbf{22.14} &\textbf{0.69} &	\textbf{0.71}&\textbf{0.72}&	\textbf{0.51}&	\textbf{0.48}&\textbf{0.46} \\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{19.12}}&\textcolor{textcolor}{\textbf{20.78}} &\textcolor{textcolor}{\textbf{21.46}}&\textcolor{textcolor}{\textbf{23.06}}&\textcolor{textcolor}{\textbf{0.68}} &\textcolor{textcolor}{\textbf{0.72}}&\textcolor{textcolor}{\textbf{0.73}}&\textcolor{textcolor}{\textbf{0.74}}&\textcolor{textcolor}{\textbf{0.52}}
 &\textcolor{textcolor}{\textbf{0.48}}&\textcolor{textcolor}{\textbf{0.48}}&\textcolor{textcolor}{\textbf{0.47}}\\
%\hline
\bottomrule[1.2pt]
\end{tabular}}
\caption{Comparison against NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} on the CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI}, using 5/10/15/20 views for training.}%, and the remaining 25/20/15/10 views for testing.}
\label{tab:realhuman}
\end{center}
\vspace{-8mm}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}
In this section we provide implementation details (Sec.~\ref{sec:results_impl_details}), and demonstrate our performance against baselines on real (Sec.~\ref{sec:results_real}) %(CMU Panoptic~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}) 
and synthetic (Sec.~\ref{sec:results_synth}) %(MultiHuman-Dataset \cite{zheng2021deepmulticap}) 
datasets, in terms of novel-view synthesis, visual reconstructions, and geometry error. Finally, we show ablation studies (Sec.~\ref{sec:results_ablation}) that demonstrate the importance of each of the proposed components. 

%%--------------------------------------------------------------
\subsection{Implementation Details}
\label{sec:results_impl_details}
Our method was implemented using PyTorch \cite{paszke2019pytorch}, and trained on a Quadro RTX 5000 GPU. We use ADAM optimizer \cite{kingma2014adam} with a learning rate ranging from $5 \times 10^{-4}$ to $2.5 \times 10^{-5}$, controlled by cosine decay schedule. Our network architecture follows \cite{yariv2020multiview, mildenhall2020nerf}. For a fair comparison, we sample 256 rays per batch and follow the coarse and fine sampling strategy of \cite{wang2021neus}. More network structure and training details are shown in the supplementary material. 
%, each ray with $N=64$ coarsely sampled points and $N=64$ fine sampled points for inside sphere and  $N=32$ for the outside sphere. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real Multi-Human Dataset}
\label{sec:results_real}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We first evaluate our approach on the CMU Panoptic Dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}. %Those sequences contain multiple humans and 3D key points, which are used to fit human SMPL. .%including frame 9200 from Video 'Haggling' and frame 5500,7800,9200,22900 from 'Ultimatum'.
Our experiments were performed on five different scenes, where each scene originally includes 30 views containing 3/4/5/6/7 people. The training views were randomly extracted from the HD sequences `Ultimatum' and `Haggling'.  We uniformly choose 5/10/15/20 views for training and the rest 25/20/15/10 views for testing. We compare with two major baselines: NeuS \cite{wang2021neus} and VolSDF\cite{yariv2021volume}, both in terms of novel-view synthesis and geometry reconstructions (qualitatively). For quantitative evaluation, we report three commonly used image metrics: peak signal-to-noise ratio (PSNR)~\cite{hore2010image}, structural similarity index (SSIM)~\cite{wang2004image} and learned perceptual image patch similarity (LPIPS)~\cite{zhang2018unreasonable}. For qualitative comparison, both rendered images and rendered normal images are shown. % in  Fig. \ref{fig:cmu}. \\

\noindent
\textbf{Comparison with baselines.} 
Tab.~\ref{tab:realhuman} demonstrates novel view synthesis results with different training views (5/10/15/20) compared to the baselines. Our proposed method outperforms these in PSNR and SSIM in all the scenes, and consistently performs better or equal in terms of LPIPS. For qualitative comparison, we demonstrate both rendered novel views and normal images in Fig.~\ref{fig:cmu}. As seen here, when given 5/10 training views the baseline methods fail to reconstruct a good geometry or render a realistic appearance. Although the quality of the geometries improves with 15/20 training views, the results exhibit missing body parts or can mix the background with the subjects. On the other hand, our method can reconstruct a complete geometry for all humans in all sparse-view cases.  

%\noindent
%\textbf{.} \q{Adding subsection sub title}
Fig.~\ref{fig:increase} additionally shows the relationship between the number of training views and the quality of the synthesized images. The fewer the number of views, the harder it is for all methods to reconstruct high-quality images, whereas our approach is more robust to fewer training views. For denser inputs~(\eg more than 20 views), our method reaches similar albeit slightly better performance than the baselines, since the proposed work focuses on sparse scenarios. 
% this wasn't clear
%Meanwhile, increasing input views in the dense setting will improve all those scenarios' performance. 

\begin{figure}[h!]
\vspace{-3mm}
\centering
\includegraphics[width=0.9\linewidth]{fig/increase.png}
\vspace{-2mm}

  \caption{Quantitative comparison of average PSNR (↑), SSIM (↑) and LPIPS (↓) with increased number of training views. }
  \label{fig:increase}
%\vspace{-5mm}
\end{figure}


\noindent
\textbf{Comparison to single human NeRF.} 
We compare our method to the single human nerf state-of-the-art method ARAH~\cite{wang2022arah}. We note that adapting such methods to our setup requires tedious manual pre-processing (detecting and segmenting people, associating detections across views), which is not required by our approach.
We run a separate ARAH model for each person in the scene using 5 training images (see supp. mat.). Fig.~\ref{fig:arah} shows novel view and reconstruction results. Learning for each person separately implies providing erroneous supervision to the model whenever the person is occluded in the scene or segmentation masks are not accurate. As a result, ARAH's renderings and geometry display many artifacts compared to our results.
Conversely, our method avoids this by learning through rendering the union of SMLP bounding boxes conjointly. We also noticed that ARAH's results are very sensitive to the sparsity and choice of the training views.


% \q{We compare our method with Arah~\cite{wang2022arah} on the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}. Figure~\ref{fig:5training} shows the training views. Since Arah only work for single human reconstruction on foreground, we segmented each human by performing instance segmentation~\cite{he2017mask} on all views and train Arah per-person without background separately. Figure~\ref{fig:single} shows qualitative comparison of both reconstructed appearance and geometry. Both Arah and our approach could reconstruct most geometry and appearance from sparse training views. However, regarding to sparser views (\eg{the girl in first row seen-able in five training views and the boy in the second row seen able in four training views}) or more occlusions (\eg{the same person in the third row is more occluded than the second row in Figure~\ref{fig:single}}), Arah suffers from artifacts in reconstructed geometry and appearance. Our method is more robust to the number of training views and occlusions due to the proposed hybrid box-based rendering with geometry constraints. Moreover, when dealing with multiple-human scenes, single person approaches usually supervise on segmented results using inaccurate mask from instance segmentation or incomplete mask from SMPL, resulting in artifacts around 3D surfaces(or 2D edges).  Our method does not require extra segmentation.  We show training views, segmented masks and more results in supplementary file. 26.97/29.56, 27.48/33.66, 24.36/30.56} 
  %  tw when we are rewriting, lets remember that reasons for single person nerfs could fail here include:
% failed detection =  blank view
% bad segmentation
% occlusion from other people
% quality of smlp fitting in cmu vs smpl fitting in zjumocap
% variance in colors in cmu images (that we account for with the saturation loss)
% challenging views ditribution in cmu (vs perfect front/back/left/right views in zjumocap)
 
%--------------------------------------------------------------------------------
%\begin{figure}[h]
% \offinterlineskip
%\centering%\flushleft %
%\vspace{-3mm}
%\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
%\setlength{\tabcolsep}{0pt}
%\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
%\begin{tabular}{ccccc}
%\includegraphics[width=1.68cm]{fig/single/trianing/00_00_00022900.jpg} &
%\includegraphics[width=1.68cm]{fig/single/trianing/00_06_00022900.jpg} &
%\includegraphics[width=1.68cm]{fig/single/trianing/00_12_00022900.jpg} &
%\includegraphics[width=1.68cm]{fig/single/trianing/00_18_00022900.jpg} &
%\includegraphics[width=1.68cm]{fig/single/trianing/00_24_00022900.jpg} \\
% \includegraphics[width=1.8cm]{fig/single/seg/5/1.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/5/2.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/5/3.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/5/4.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/5/5.png} \\
% \includegraphics[width=1.8cm]{fig/single/seg/3/1.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/3/2.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/3/3.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/3/4.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/3/5.png} \\
% \includegraphics[width=1.8cm]{fig/single/seg/6/1.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/6/2.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/6/3.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/6/4.png} &
% \includegraphics[width=1.8cm]{fig/single/seg/6/5.png} \\
 %Arah  & Ours&Arah &  Ours\\
%\end{tabular}
% \vspace{-3mm}
%&
%\end{tabularx}
%\caption{Five training views. }
%\label{fig:5training} 
%\vspace{-5mm}
%\end{figure}

\begin{figure}[h]
% \offinterlineskip
\centering%\flushleft %
%\vspace{-3mm}
%\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\begin{tabular}{ccccc}
\includegraphics[width=1.68cm]{fig/single/id5/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id5/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id5/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id5/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id5/ours_normal_old.png} \\
\includegraphics[width=1.68cm]{fig/single/id3/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id3/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id3/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id3/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id3/ours_normal_old.png} \\
\includegraphics[width=1.68cm]{fig/single/id32/gt.png} &
\includegraphics[width=1.68cm]{fig/single/id32/arah_color.png} &
\includegraphics[width=1.68cm]{fig/single/id32/arah_normal}&
\includegraphics[width=1.68cm]{fig/single/id32/ours_color.png} &
\includegraphics[width=1.68cm]{fig/single/id32/ours_normal.png} \\
% \includegraphics[width=1.68cm]{fig/single/id6/gt.png} &
% \includegraphics[width=1.68cm]{fig/single/id6/arah_color.png} &
% \includegraphics[width=1.68cm]{fig/single/id6/arah_normal}&
% \includegraphics[width=1.68cm]{fig/single/id6/ours_color.png} &
% \includegraphics[width=1.68cm]{fig/single/id6/ours_normal_old.png} \\
GT& ARAH  & ARAH & Ours &  Ours\\
\end{tabular}
\vspace{-2mm}
%&
%\end{tabularx}
\caption{Comparison against ~\cite{wang2022arah} from 5 training views. PSNRs for the 3 examples are respectively: 26.97/\textbf{29.56}, 27.48/\textbf{33.66}, 24.36/\textbf{30.56} (ARAH/\textbf{Ours}).}
\label{fig:arah} 
%\vspace{-5mm}
\end{figure}
%------------------------------------------------------------------------------
%----------------------------------------------------
\begin{table}[h!]
%\vspace{-3mm}
\begin{center}
\normalsize
%\label{table:headings}
\scalebox{0.8}{
\begin{tabular}{c|l|ccc}
\toprule[1.2pt]
%\hline
Scene& Method & {\bf PSNR↑} & {\bf SSIM↑}& {\bf LPIPS↓}\\
%Scene& Method& {\bf PSNR↑} & {\bf SSIM↑} & {\bf LPIPS↓}\\
\hline
%& Volsdf\cite{yariv2021volume} & 14.65&15.07&18.81&0.48&0.50&0.69&0.57&0.56&0.48\\
%& Neus \cite{wang2021neus}&16.53&17.03 &18.39 &0.52&0.53&0.57&0.58&0.57&0.53 \\
%--------------1-----------------------
%Scene 1&Neus &17.83&0.62&0.55\\
& InfoNeRF  &  14.64 &0.50 &0.64\\
1&NeuS w/ info &17.98&0.65&0.58\\
&Ours& \bf{18.41}& \bf{0.67} & \bf{0.55}    \\
%--------------2-----------------------
\hline
& InfoNeRF&14.21&0.49&0.63  \\
%&Neus &16.87&0.60&0.57\\
2&NeuS w/ info&18.21&0.64&0.57\\
&Ours&\bf{19.72} & \bf{0.70} & \bf{0.50} \\
%--------------3-----------------------
\hline
&InfoNeRF&13.78&0.45&0.63  \\
%&Neus &16.03&0.56&0.62\\
3&NeuS w/ info &16.31&0.59&0.60\\
&Ours & \bf{18.57} & \bf{0.66} & \bf{0.52}    \\
%--------------4-----------------------
\hline
& InfoNeRF  &12.26&0.41&0.68  \\
4&NeuS w/ info&14.42&0.51&0.60\\
&Ours & \bf{19.54} & \bf{0.69} & \bf{0.50} \\
%--------------5--------------------
\hline
& InfoNeRF &12.17 &0.45&0.63 \\
5&NeuS w/ info &17.89&0.60&0.61\\
&Ours & \bf{19.34} & \bf{0.67} & \bf{0.51}    \\
\hline
& InfoNeRF  &13.61&0.46&0.64  \\
Ave&NeuS w/ info &16.96&0.60&0.59\\
&Ours & \bf{19.12} & \bf{0.68} & \bf{0.52}    \\

%& Volsdf\cite{yariv2021volume} & 16.68&17.84 &19.89&0.54&0.59&0.67&0.56&0.54&0.48\\
%& Neus \cite{wang2021neus}& 15.41&17.62 &20.53 &0.46&0.50&0.69&0.61&0.55& 0.47\\
\bottomrule[1.2pt]
\end{tabular}}
 %\vspace{-1mm}
 \caption{Comparison against sparse-view NeRF approaches: InfoNeRF~\cite{kim2022infonerf} and NeuS with InfoNeRF's regularizations, on the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI} using 5 training views. }
 \label{tab:sparse}
\end{center}
%\vspace{-5mm}
\end{table}
%-----------------------------------------------------------------

\noindent
\textbf{Comparison with sparse NeRF.} 
% To verify that our proposed method could also favor multiple humans reconstruction in sparse cases, 
We further compare with a recent NeRF method that was specifically designed to handle sparse views, namely  InfoNeRF~\cite{kim2022infonerf}. We compare both against the original InfoNeRF, and a version of NeuS trained with InfoNeRF's regularization. For this experiment, we use again the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI} with five training views. 
%Notice that this dataset consists of real-world scenes with cameras positioned all along the surrounding sphere, containing complex illuminations which might make InfoNeRF hard to reach the expected performance. We added the proposed loss to NeuS for comparison, which already incorporates NeRF++ \cite{zhang2020NeRF++}. 
Tab.~\ref{fig:abl} shows that, compared to InfoNeRF and NeuS with InfoNeRF's regularization, our method improves the rendering quality in all of the scenes.


\begin{figure*}[h!]
\centering%\flushleft %
\vspace{-3mm}
\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\begin{tabular}{c cccc ccc}
\rotatebox{90}{\quad 10} 
\includegraphics[width=2.5cm]{fig/syn/5_10/gt.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/neus_color.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/vol_color.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/ours_color.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/syn/5_10/ours_normal.png} \\
\rotatebox{90}{\quad 15}
\includegraphics[width=2.5cm]{fig/syn/10_15/gt.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/neus_color.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/vol_color.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/ours_color.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/syn/10_15/ours_normal.png} \\
Ground Truth &Neus &Volsdf & Ours  & Neus &Volsdf & Ours\\
\end{tabular}
%&
%\end{tabularx}
 %\vspace{-2mm}
 \caption{Qualitative comparison of synthesised novel views and reconstructed normal images on the synthetic dataset (MultiHuman-Dataset~\cite{zheng2021deepmulticap}) with 10 and 15 training views respectively.} 
\label{fig:syn} 
%\vspace{-3mm}
\end{figure*}


\begin{figure*}[ht]
\centering%\flushleft %
%\vspace{-2mm}
\def\tabularxcolumn#1{m{#1}}
%\begin{tabularx} {\linewidth}{@{}cXX@{}}
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0} % General space between rows (1 standard)
\begin{tabular}{ccccccc}
\includegraphics[width=2.5cm]{fig/cmu/abla/neus.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/vol.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nosdf.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nopk.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nos.png}&
\includegraphics[width=2.5cm]{fig/cmu/abla/full.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/gt.png}  \\ 
\includegraphics[width=2.5cm]{fig/cmu/abla/neus_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/vol_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nosdf_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nopk_normal.png} &
\includegraphics[width=2.5cm]{fig/cmu/abla/nos_normal.png}&
\includegraphics[width=2.5cm]{fig/cmu/abla/full_normal.png} &\\
% \includegraphics[width=2.9cm]{fig/cmu/105/neus.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/vol.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/our.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/gt.png}&
% \includegraphics[width=2.9cm]{fig/cmu/105/our.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/gt.png}  \\ 
NeuS & VolSDF &w/o geometry  &w/o ray loss &w/o saturation  & Ours (Full) & Ground Truth\\
\end{tabular}
%&
%\end{tabularx}
 %\vspace{-2mm}
 \caption{Ablation study on CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI}. Comparison against our method without geometric regularization (w/o geometry), our method without ray consistency regularization (w/o ray loss), and our method without saturation regularization (w/o saturation).}
\label{fig:abl} 
%\vspace{-3mm}
\end{figure*}




% %------------------------------------------------------------------------------
% \begin{figure*}[ht]
% \centering%\flushleft %
% %\vspace{-5mm}
% \def\tabularxcolumn#1{m{#1}}
% %\begin{tabularx} {\linewidth}{@{}cXX@{}}
% \setlength{\tabcolsep}{0pt}
% \begin{tabular}{cccc}
% \includegraphics[width=4.4cm]{fig/cmu/105/neus.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/vol.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/our.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/gt.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/neus_normal.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/vol_normal.png} &
% \includegraphics[width=4.4cm]{fig/cmu/105/our_normal.png}& \includegraphics[width=4.4cm]{fig/cmu/105/our_normal.png}\\
% \footnotesize{neus} & \footnotesize{volsdf} & Ours  & Ground Truth\\
% \end{tabular}
% %&
% %\end{tabularx}
%  \vspace{-2mm}
%  \caption{Qualitative comparison of synthesised novel views(the 1st row) and reconstructed normal images of Multi-human(the 2nd row) on CMU Panoptic Dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} (Here trained on 10  views and tested on 20 views.More results in \textbf{supp. material})} 
% \label{fig:cmu} 
% %\vspace{-5mm}
% \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%======================================================================
\begin{table*}[h!]
%\vspace{-3mm}
\begin{center} %
%\flushleft 
%\large %
\normalsize
%\normalsize
\scalebox{0.82}{
\begin{tabular}{l|c|ccc|ccc|ccc|c }
\toprule[1.2pt]
$\#$ Humans & Method  &\multicolumn{3}{c|} {\bf PSNR↑} &  \multicolumn{3}{c|}{\bf SSIM↑}& \multicolumn{3}{c|} {\bf LPIPS↓} & {\bf Chamfer ↓} \\ %\multicolumn{1}{c} {\bf champher↓} \\
%\cline{3-14}
 &&5   &10  &15   &5  &10  &15 &5   &10 &15 &15\\
 %%------1----------------------------
\hline
%& SMPL & -&- &-&-&-&-&-& -&-- & 0.021\\
 & NeuS& 14.04&17.89 &23.25&0.63&0.72&0.84&\textcolor{textcolor}{\textbf{0.55}}& 0.53&0.44&0.308\\
1 & VolSDF & 13.93&21.75 &25.89&0.61&0.81&0.86&\textcolor{textcolor}{\textbf{0.55}}&0.51&0.44&0.019\\
 &  \textbf{Ours}&\textcolor{textcolor}{\textbf{15.36} }&\textcolor{textcolor}{\textbf{23.85}}&\textcolor{textcolor}{\textbf{26.28}} &\textcolor{textcolor}{\textbf{0.65}}&\textcolor{textcolor}{\textbf{0.84}}&\textcolor{textcolor}{\textbf{0.87}}&\textcolor{textcolor}{\textbf{0.55}}&\textcolor{textcolor}{\textbf{0.43}}&\textcolor{textcolor}{\textbf{0.41}}&\textcolor{textcolor}{\textbf{0.018}}\\
 %%------2--------------------------
\hline
%& SMPL & -&- &-&-&-&-&-& -&-- & 0.022\\
 & NeuS &14.15&18.14 &18.54 &0.61&0.72&0.72&0.54&0.46&0.44&0.321\\
5 & VolSDF &12.97&15.11&18.59&0.58&0.63&0.73&0.56&0.55&0.47&0.151\\
 &  \textbf{Ours}&\textcolor{textcolor}{\textbf{17.63} }&\textcolor{textcolor}{\textbf{20.10}}&\textcolor{textcolor}{\textbf{20.33}} &\textcolor{textcolor}{\textbf{0.71}}&\textcolor{textcolor}{\textbf{0.79}}&\textcolor{textcolor}{\textbf{0.77}}&\textcolor{textcolor}{\textbf{0.47}}&\textcolor{textcolor}{\textbf{0.40}}&\textcolor{textcolor}{\textbf{0.40}}&\textcolor{textcolor}{\textbf{0.020}}\\
%%------3-----------------------------
\hline
%& SMPL & -&- &-&-&-&-&-& -&- & 0.023\\
 & NeuS &14.09&15.69&19.27&0.58&0.65&0.75&0.52&0.48&0.42&0.383\\
10 & VolSDF & 12.66&16.99 &19.30&0.56&0.70&0.77&0.56&0.50&0.41&0.248\\
 &  \textbf{Ours}&\textcolor{textcolor}{\textbf{16.52}} &\textcolor{textcolor}{\textbf{18.39}}&\textcolor{textcolor}{\textbf{21.01}} &\textcolor{textcolor}{\textbf{0.65}}&\textcolor{textcolor}{\textbf{0.71}}&\textcolor{textcolor}{\textbf{0.80}}&\textcolor{textcolor}{\textbf{0.50}}&\textcolor{textcolor}{\textbf{0.44}}&\textcolor{textcolor}{\textbf{0.37}}&\textcolor{textcolor}{\textbf{0.043}}\\
 %----------------Average-------------------
 \hline
 %& SMPL & -&- &-&-&-&-&-& -&- & 0.022\\
 & NeuS &14.09&17.24&20.35 &0.60	&0.70	&0.77 &0.54 &0.49 &0.43&0.337\\
\textbf{Average}& VolSDF&13.18 &17.95&	21.26 &0.58 &0.71 &	0.79 &0.56	&0.52	&0.44&0.139\\
 &\textbf{Ours}&\textcolor{textcolor}{\textbf{16.50}} &\textcolor{textcolor}{\textbf{20.78}}&\textcolor{textcolor}{\textbf{22.54}} &\textcolor{textcolor}{\textbf{0.67}}&\textcolor{textcolor}{\textbf{0.78}}&\textcolor{textcolor}{\textbf{0.81}} &\textcolor{textcolor}{\textbf{0.51}}&\textcolor{textcolor}{\textbf{0.42}}&\textcolor{textcolor}{\textbf{0.39}}&\textcolor{textcolor}{\textbf{0.026}}\\
%\hline
\bottomrule[1.2pt]
\end{tabular}}
\caption{Comparison against NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} on the synthetic dataset, for different number of humans in the scene. We measure novel-view synthesis quality in terms of PSNR, SSIM and LIPIS, as well as geometry error in terms of Chamfer distance.}
\label{tab:syn}
\end{center}
%\vspace{-5mm}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic Dataset}
\label{sec:results_synth}
%%%%%======================================================================
% \begin{table*}[ht]
% \vspace{-3mm}
% \begin{center} %
% %\flushleft 
% %\large %
% \normalsize
% %\normalsize
% \begin{tabular}{l|c|ccc|ccc|ccc |ccc}
% \toprule[1.2pt]
% Scene&Method  &\multicolumn{3}{c|} {\bf PSNR↑} &  \multicolumn{3}{c|}{\bf SSIM↑}& \multicolumn{3}{c|} {\bf LPIPS↓} & \multicolumn{3}{c} {\bf Champher↓} \\
% %\cline{3-14}
%  &&10   &15  &20   &10  &15  &20 &10   &15 &20 &10   &15 &20\\
%  %%------1----------------------------
% \hline
%  & Neus \cite{wang2021neus}& 19.97&21.53 &22.32&0.65&0.67&0.71&0.52& 0.50&0.49 &&&\\
% 1& Volsdf\cite{yariv2021volume} & 19.83&21.84 &22.40&0.66&0.71&0.72&0.53&0.49&0.48&&&\\
%  &  \textbf{Ours}&\textbf{20.36} &\textbf{22.03}&\textbf{22.83} &\textbf{0.66}&\textbf{0.71}&\textbf{0.72}&\textbf{0.52}&\textbf{0.51}&\textbf{0.49}&&&\\
%  %%------2--------------------------
% \hline
%  & Neus \cite{wang2021neus}&16.53&17.03 &18.39 &0.52&0.53&0.57&0.58&0.57&0.53 &&&\\
% 2& Volsdf\cite{yariv2021volume} & 14.65&15.07&18.81&0.48&0.50&0.69&0.57&0.56&0.48&&&\\
%  &  \textbf{Ours}&\textbf{17.16} &\textbf{17.42}&\textbf{18.84} &\textbf{0.51}&\textbf{0.52}&\textbf{0.58}&\textbf{0.55}&\textbf{0.51}&\textbf{0.50}&&&\\
% %%------3-----------------------------
% \hline
%  & Neus \cite{wang2021neus}&17.80&19.24&20.10&0.54&0.57&0.62&0.56&0.53&0.51 &&&\\
% 3& Volsdf\cite{yariv2021volume} & 16.97&18.49 &22.21&0.53&0.61&0.69&0.59&0.51&0.47&&&\\
%  &  \textbf{Ours}&\textbf{19.08} &\textbf{20.82}&\textbf{21.79} &\textbf{0.59}&\textbf{0.63}&\textbf{0.64}&\textbf{0.56}&\textbf{0.53}&\textbf{0.52}&&&\\
%  %----------------Average-------------------
%  \hline
%  & Neus \cite{wang2021neus}&18.10&19.27&20.27 &0.57	&0.59	&0.63 &0.55 &0.53 &0.51
%  &&&\\
% \textbf{Average}& Volsdf\cite{yariv2021volume}&17.15 &18.47&	21.14 &0.56 &0.61 &	0.70 &0.56	&0.52	&0.48&&&
% \\
%  &\textbf{Ours}&\textcolor{textcolor}{\textbf{}} &\textcolor{textcolor}{\textbf{}}&\textcolor{textcolor}{\textbf{}} &\textcolor{textcolor}{\textbf{}}&\textcolor{textcolor}{\textbf{}}&\textcolor{textcolor}{\textbf{}} &\textcolor{textcolor}{\textbf{}}&\textcolor{textcolor}{\textbf{}}&\textcolor{textcolor}{\textbf{}}\\
% %\hline
% \bottomrule[1.2pt]
% \end{tabular}
% \caption{Testing PSNR, SSIM and LPIPS on synthetic dataset(THUman2.0 Dataset
% and MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap}) with 10, 15 and 20 training views respectively.The higher the better for both PSNR and SSIM. The lower the better for LPIPS.The The bold one present best performance.}
% \label{tab:syn}
% \end{center}
% \vspace{-3mm}
% \end{table*}



%%% FIG ABLATION WAS HERE

% %-------------------------------------------------------------------
%=====================================================================
Based on the MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap}, we used Unity 3D to create a synthetic dataset with 29 cameras arranged in a great circle. This includes three scenes with similar backgrounds but different camera locations and orientations. Each of the scenes contains 1/5/10 humans respectively. We train with 5/10/15 views on each scene and test with 14 fixed views. Tab.~\ref{tab:syn} reports the average error for all testing views in PSNR, SSIM and LPIPS metrics. Our method reaches state-of-the-art performance on synthesized novel-view results. Fig.~\ref{fig:syn}  shows generated novel views and corresponding normal images using 10/15 training images. Our approach can reconstruct complete geometry of all humans in the scene, while the baseline methods might miss some of the people when they have similar color with the background, \eg the shadow area in Fig.~\ref{fig:syn}. 

In the 5/10 input views case, the baseline methods usually fail to reconstruct the full geometry of humans due to the sparse inputs. Thus, we report Chamfer distance in Tab.~\ref{tab:syn} only for the 15-views case. Since the baseline methods usually contain extra floor, for a fair comparison, we sample points from ground-truth meshes and compute the distance towards the reconstructed mesh for all methods. We report the bi-directional Chamfer distance in the supplementary material. Tab.~\ref{tab:syn} shows that, with an increasing number of humans in the scene, the quality of the reconstructed geometry of all methods decreases. However, compared with the baselines, our method can better handle multiple human scenes, achieving an order of magnitude less error. 


%---------------------------------------------
\subsection{Ablation Study}
\label{sec:results_ablation}

To prove the effectiveness of our proposed components we performed ablation studies on the CMU Panoptic dataset~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}. We demonstrate quantitative comparisons in Tab.~\ref{tab:abl} and qualitative results in Fig.~\ref{fig:abl}. We test the following settings: 
%\textbf{Without initialization.}We show our scene editing here, including removing, rotation, translation and scaling. More results will be shown in the supplementary file. 
%\textbf{Without rasterized feature.} Compared without using the rasterized feature, the full model could achieve higher rendering quality quantitatively and qualitatively, see Tab. \ref{tab:abl} and Fig. \ref{fig:abl}.

\noindent
\textbf{Without geometry regularization (``w/o geometry'').} We compare our full model against the model without geometry regularization (Sec.~\ref{sec:method_geometric_init}) and SDF uncertainty regularization (Eq.~\ref{eq:sdfloss}). We can see here that, although the method is still capable of isolating humans thanks to the bounding box rendering, both geometry and novel views are much less accurate, and the rendered images exhibit background artifacts and overly smooth results. 
%%Though our proposed model could predict human shape with proposed box based rendering, However, it fails to reconstruct accurate geometry.

\noindent
\textbf{Without ray consistency loss (``w/o ray loss'').} Here we remove the proposed ray consistency loss, without which the average rendering quality also degrades.

\noindent
\textbf{Without saturation loss.} Finally, we remove the saturation loss from our methods, which decreases by about 0.5 in PSNR on average.  Fig.~\ref{fig:abl} shows that, without this, the image tone can contain artifacts due to changes in lighting (see for example the back of the rightmost subject).


% \begin{table*}[h!]
% %\vspace{-3mm}
% \begin{center}
% \normalsize
% %\label{table:headings}
% %\scalebox{0.8}{
% \begin{tabular}{c|l|cc |cc |cc}
% \toprule[1.2pt]
% %\hline
% Scene& Method &\multicolumn{2}{c|} {\bf PSNR↑} &  \multicolumn{2}{c|}{\bf SSIM↑}& \multicolumn{2}{c} {\bf LPIPS↓}\\
%  &&5   &10    &5  &10   &5  &10 \\
% %Scene& Method& {\bf PSNR↑} & {\bf SSIM↑} & {\bf LPIPS↓}\\
% \hline
% %& Volsdf\cite{yariv2021volume} & 14.65&15.07&18.81&0.48&0.50&0.69&0.57&0.56&0.48\\
% %& Neus \cite{wang2021neus}&16.53&17.03 &18.39 &0.52&0.53&0.57&0.58&0.57&0.53 \\
% &Ours w/o Geometry prior&  &&& &\\
% &Ours w/o Rasterized Depth &  & & \\
% Scene 3&Ours w/o Patch KL loss&& &  &&&  \\
% &Ours w/o Saturation Loss && &&&&\\
% \hline
% %& Volsdf\cite{yariv2021volume} & 16.68&17.84 &19.89&0.54&0.59&0.67&0.56&0.54&0.48\\
% %& Neus \cite{wang2021neus}& 15.41&17.62 &20.53 &0.46&0.50&0.69&0.61&0.55& 0.47\\
% % &Ours w/o Rasterized feature &  & & \\
% % Scene 3&Ours w/o patch KL &&0 & & 23.24 &0.734&0.466& 23.24 &0.734&0.466\\
% % &Ours w/o Saturation & 23.24 &0.734&0.466\\
% % &Ours w/o sdf loss & 23.09 &0.731 &0.486& 23.24 &0.734&0.466& 23.24 &0.734&0.466\\
% &{\bf Ours(Full)} &  {\bf }& {\bf } &  {\bf } \\
% \bottomrule[1.2pt]
% \end{tabular}%}
%  \vspace{-3mm}
%  \caption{Abalations of testing PSNR, SSIM and LPIPS on CMU Panoptic Dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} with  training views respectively.The higher the better for both PSNR and SSIM. The lower the better for LPIPS. The bold present best performance. }
%  \label{tab:abl}
% \end{center}
% \vspace{-3mm}
% \end{table*}

\begin{table}[h!]
%\vspace{-3mm}
\begin{center}
\normalsize
%\label{table:headings}
\scalebox{0.8}{
\begin{tabular}{l|cc |cc |cc}
\toprule[1.2pt]
%\hline
 Method &\multicolumn{2}{c|} {\bf PSNR↑} &  \multicolumn{2}{c|}{\bf SSIM↑}& \multicolumn{2}{c} {\bf LPIPS↓}\\
 &5   &15    &5  &15   &5  &15 \\
%Scene& Method& {\bf PSNR↑} & {\bf SSIM↑} & {\bf LPIPS↓}\\
\hline
 Neus \cite{wang2021neus}& 16.87&19.40 &0.60&0.70 &0.51 &0.53 \\
 Volsdf\cite{yariv2021volume}&16.03&19.40 &0.53&0.67 &0.60&0.49\\
\hline
 w/o Geometry&17.54  &20.28&0.60&0.70 &0.53&0.48\\
 %w/o Rasterize&  & & \\
 w/o Ray loss&19.07&20.95& 0.67  &0.72&0.52&0.47  \\
 w/o Saturation&18.95&20.92&0.65&0.72 &0.54&0.49\\
\hline
{\bf Ours(Full)} &  {\bf 19.72 }& {\bf 21.40 } &  {\bf 0.70} &  {\bf 0.73}& {\bf 0.50 }& {\bf 0.48}\\
\bottomrule[1.2pt]
\end{tabular}}
 \vspace{-1mm}
 \caption{Ablation study on the CMU Panoptic dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} with 5/15 training views respectively. Comparison against our method without geometric regularization (w/o
Geometry), without ray consistency regularization (w/o Ray loss), and without saturation regularization (w/o Saturation).}
 \label{tab:abl}
\end{center}
%\vspace{-10mm}
\end{table}

%-------------------------------------------------------------------------
% \begin{figure*}[ht]
% \centering%\flushleft %
% \vspace{-3mm}
% \def\tabularxcolumn#1{m{#1}}
% %\begin{tabularx} {\linewidth}{@{}cXX@{}}
% \setlength{\tabcolsep}{0pt}
% \begin{tabular}{ccc ccc}
% \includegraphics[width=2.9cm]{fig/cmu/105/neus.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/vol.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/our.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/gt.png}&
% \includegraphics[width=2.9cm]{fig/cmu/105/vol.png} &
% \includegraphics[width=2.9cm]{fig/cmu/105/gt.png}  \\ 
% Neus &Volsdf & w/o rf  & w/o patch kl &w/o sl  & Ours\\
% \end{tabular}
% %&
% %\end{tabularx}
%  \vspace{-3mm}
%  \caption{Ablations comparison of novel views and reconstructed normal images of Multi-human on CMU Panoptic Dataset \cite{Simon_2017_CVPR,Joo_2017_TPAMI} (Here trained on 10  views and tested on 20 views.)} 
% \label{fig:abl} 
%  \vspace{-2mm}
% \end{figure*}

%---------------------------------------------
% \subsection{Scene Editing}
% Our scene editing applications include removing, rotation, translation and scaling, please see the supplementary file.  \\


