\section{Introduction}
\label{sec:intro}

Human reconstruction from single images \cite{choutas2020monocular, kanazawa2018end,liu2022recent}, multiple images \cite{guo2019relightables, collet2015high}, RGB videos \cite{alldieck2018detailed,Kocabas20} or RGB-D data \cite{yu2017bodyfusion,yu2018doublefusion} has received a lot of attention, much less explored is the task of \emph{multiple} human scenario, which is essential for scene understanding, behavior modeling, collaborative augmented reality, and sports analysis.  
%
The multi-human setting introduces additional challenges, as there is now a higher level of occlusion and clutter 
which hinders matching and reconstruction. 
Although in principle one could approach this by first detecting and then independently processing each person, 
simultaneous reconstruction of multiple humans can help to globally reason about occlusion at the level of the scene~\cite{jiang2020coherent, sun2022putting}, 
%which has been shown to produce better results~\cite{jiang2020coherent, sun2022putting}, 
and can potentially recover coherent 3D spatial relations among the people.

Several recent works have attempted to recover multiple humans from a single view \cite{choi2022learning, sun2022putting, sun2021monocular, zanfir2018deep, zanfir2018monocular, fieraru2020three, jiang2020coherent, zhang2021body, ugrinovic2021body,mustafa2021multi}. However, the majority of these are based on regressing the parameters of a human body model --typically SMPL \cite{loper2015smpl}--
which provides coarse reconstructions that %cannot handle 
lack hair, clothing, and geometric details. 
Multi-view settings can help resolve some of the occlusions as well as depth ambiguities, but require a dense array of RGB cameras to achieve a detailed reconstruction \cite{collet2015high, joo2015panoptic,vlasic2009dynamic}.
% which is not easily accessible. % available. 
A more convenient capture system %that could in principle still deliver optimal results 
is the \emph{sparse} multi-view setting, where only a handful of cameras is required.
%, where the number of cameras is limited to 2-15. 
However, due to the decreased number of views and increased level of occlusion, 
existing methods require segmentation masks and a pre-scanned template mesh \cite{liu2011markerless, wu2013set}, rely on a coarse body model \cite{zhang2021lightweight, huang2021dynamic}, or require temporal information \cite{zheng2021deepmulticap, huang2021dynamic}.

A parallel line of work simultaneously tackles the novel-view-synthesis and geometry-reconstruction problems by combining neural coordinate-based representations, \eg implicit signed distance functions (SDFs) \cite{park2019deepsdf}, with differentiable rendering \cite{yariv2021volume,wang2021neus,yariv2020multiview,mildenhall2020nerf}. 
This approach has the advantage of producing, along with geometry, renderings from novel viewpoints that can capture complex surface/light interactions, increasing the scope of applications. 
NeRF~\cite{mildenhall2020nerf}, for example, uses volumetric rendering to produce impressive images under novel views, albeit at the cost of sub-optimal geometries due to the unconstrained volumetric representation. 
SDF-based methods \cite{yariv2021volume,wang2021neus,yariv2020multiview}, while delivering images of slightly lower quality, have been shown to produce 3D surfaces that are competitive with classical approaches. 
For humans, this has been leveraged to obtain geometry and appearance from monocular video \cite{jiang2022selfrecon,chen2021animatable}, RGB-D video \cite{dong2022pina}, and sparse multi-view video \cite{wang2022arah, liu2021neural, peng2021neural, zheng2021deepmulticap, kwon2021neural, peng2021animatable, xu2021h, weng2022humannerf}. 
%% NOTE: \cite{xu2021h} also has experiments for static sparse MV image
However, none of these works, with the exception of \cite{zheng2021deepmulticap,zhang2021editable}, were designed to handle the increased geometric complexity and occlusion of the multi-human case. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%not the only work, STnerf, SIgsia2022 and deep multicap
Current works \cite{zheng2021deepmulticap,zhang2021editable} address the multi-human setting, but both require a set of videos, which effectively becomes a dense array of views as long as deformations are modeled correctly.
%DeepMultiCap \cite{zheng2021deepmulticap} is the only work that addresss the multi-human setting, but the method requires segmentation and was focused on reconstruction from videos, which effectively becomes a dense array of views as long as deformations are modeled correctly.

In this paper, we address the problem of multiple 3D human surfaces and volume rendering from sparse static multi-view images. Our key insight is that human-specific geometric constraints can be leveraged to tackle the challenging sparse-view setting.

Specifically, we first obtain a SMPL body model from the input data and use it to initialize the implicit SDF network, where we define the surface of a multi-human scene as the zero-level set of the SDF. 
Then, the geometry network is optimized with multi-view images by leveraging surface and volume rendering ~\cite{wang2021neus} along with uncertainty estimation methods \cite{deng2022depth,roessle2022dense}, where the SMPL meshes are treated as noisy estimations.  
% [to keep the method general for novel scenes, we do not rely on features pre-trained on a large dataset]
To achieve higher rendering quality from sparse training views, we additionally propose a patch-based regularization loss that guarantees consistency across different rays and a saturation regularization that ensures consistency for variable image illuminations within the same scene.% \va{I feel this is a problem that was not pointed out before, and it would be nice to elaborate better}.

We evaluate our method quantitatively and qualitatively on real multi-human (CMU Panoptic~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}) and synthetic (MultiHuman~\cite{zheng2021deepmulticap}) datasets. We demonstrate results on 5,10,15 and 20 training views, where we achieve state-of-the-art performance in terms of surface reconstruction and novel view quality. 

%In summary, our contributions are: 
%\begin{itemize}
%\item We propose the first neural implicit surface and volume rendering for multiple humans from sparse static images; 
%\item We propose a novel geometric initialization and regularization based on human SMPL, which allows for multi-human rendering simultaneously; %To address the problem of occlusion, we propose the use of SMPL for geometric regularization; 
%\item We propose a patch-based ray consistency regularization and an image saturation regularization that ensures illumination consistency across views;
%\item Our method achieves state-of-the-art performance and the code will be public online. 
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%
%% MOTIVATION
% Motivations for multi-human from the multi-human, single-view domain:
%       - A lot of occlusions, if only a crop of the person is used then SOTA methods might fail (\cite{choi2022learning})
%       - Simultanesouly considering multiple person has been empirically shown to work better \cte{all the works on mult-human SMPL}
%       - The reconstructions can be made coherent in 3D space, i.e. with correct relative depth among the people. Plausible 3D spatial relations.
% The problem with SMPL based methods is that they cannot reconstruct clothes, hair, etc

% Applications in behavior analysis, automatic video analysis of sport events, or collaborative augmented reality applications
% human-computer interaction, human behavioral modeling, assisted therapy, monitoring sports performances, protection and security, special effects, modeling and indexing archival footage, or self-driving cars. (this is for monocular)

% We take one step beyond SMPL and use implicit representations for more accurate reconstructions

% "The straightforward solution consists in regarding different people as independent instances and estimating the body shapes and poses one by one using a single-person approach. This strategy, however, may result in inconsistent spatial arrangements and erroneous poses of the reconstructed people." (re-phrase)

% *** Why we cannot do cut one person then reconstruct it --> there should be experiments, else be careful with claims

%%%%%%%%%%%%%%%%%%%%%%
%% STORY
%   - Reconstructing geometry from a scene with multiple humans is hard because of occlusions
%   - Multiple views can help disambiguate, but it's stil hard.
%       => most are single human. For multi-human one can detect and reconstruct, but this can fail - and matching detections among views might not be straight-forward (?) Also, computational time scales (linearly) with the number of people
%       => there are a few multi-human, but ?
%       => people resort to SMPL to make the problem more tractable
%           *** classic matching methods: problem with multi-humans??
%   - SMPL doesn't have clothes or hair so geometry sucks
%   - We consider in particular the scenario where only a sparse number of cameras is available; dense rigs are not easily accessible. Dense => expensive and sophisticated hardware setup and low run-time efficiency (re-phrase)
%   - To address this, we present a method for multi-human 3D reconstruction from multiple views based on neural rendering. 
%   - To handle the occlussions and sparse views, we first fit a SMPL model. Following the recent line of works that make use of geometric quantities to improve reconstruction, we propose to to initialize the reconstruction by building an SDF from the acquired SMPL models.
%   - 

%%%%%%%%%%%%%%%%%%%%%%
% Multi-view capture with dense views and/or temporal data demands a significant cost in time and money, with increased complexity of device requirements, synchronisation, data storage and transfer times. Consequently, 3D reconstruction from a sparse set of images has become an increasingly popular problem~\cite{niemeyer2022regnerf, kim2022infonerf,long2022sparseneus,dong2021shape}, more so after the recent progress in deep neural rendering and reconstruction approaches~\cite{mildenhall2020nerf, park2019deepsdf}. % However, it remains a difficult problem due to the sparsity of 3D cues, requiring carefully designed regularization losses~\cite{kim2022infonerf,niemeyer2022regnerf} or knowledge transfer from pre-trained networks~\cite{yu2021pixelnerf}. This cue scarcity problem becomes even worse when reconstructing cluttered scenes, such as those containing a multitude of people., due to increased occlusions.
% % Multi-view capture with dense views and/or temporal data entails significant capture device requirements, synchronisation, data storage and transfer challenges. Hence, being able to capture scenes from a mere set of static sparse images has become a more than ever popular problem \cite{niemeyer2022regnerf, kim2022infonerf,long2022sparseneus,dong2021shape} especially in the wake of deep learning. 
% Yet, the scarcity of 3D cues in such setups requires carefully designed regularization or knowledge transfer. This scarcity is exacerbated further with cluttered scenes, such as scenes containing a multitude of people.   

% 3D reconstruction of humans from single images \cite{choutas2020monocular, kanazawa2018end,liu2022recent}, multiple images \cite{guo2019relightables, collet2015high}, RGB videos \cite{alldieck2018detailed,Kocabas20} or RGB-D data \cite{yu2017bodyfusion,yu2018doublefusion} has received a lot of attention, much less explored is the task of \emph{multiple} human reconstruction, which is essential for scene understanding, behavior modeling, collaborative augmented reality, and sports analysis.  
% %
% The multi-human setting introduces additional challenges, as there is now a higher level of occlusion and clutter 
% which hinders matching and reconstruction. 
% Although in principle one could approach this by first detecting and then independently processing each person, 
% simultaneous reconstruction of multiple humans can help to globally reason about occlusion at the level of the scene~\cite{jiang2020coherent, sun2022putting}, 
% %which has been shown to produce better results~\cite{jiang2020coherent, sun2022putting}, 
% and can potentially recover coherent 3D spatial relations among the people. %, taking a step towards better scene understanding.
% % simultaneous reconstruction of multiple humans has several advantages. 
% % First, this strategy  allows to further recover coherent 3D spatial relations among the people, taking a step towards better scene understanding.
% % Second, the presence of multiple people can imply a high level of occlusion. While some works explicitly handle this case for single humans \cite{khirodkar2022occluded, zhang2020object}, globally reasoning about occlusion at the level of the scene has been shown to produce better results \cite{jiang2020coherent, sun2022putting}.  
% % Finally, the detect-and-reconstruct approach increases (linearly) in computational time with each new person. 

% % While 
% Several recent works have attempted to recover multiple humans from a single view \cite{choi2022learning, sun2022putting, sun2021monocular, zanfir2018deep, zanfir2018monocular, fieraru2020three, jiang2020coherent, zhang2021body, ugrinovic2021body,mustafa2021multi}. However, the majority of these are based on regressing the parameters of a human body model --typically SMPL \cite{loper2015smpl}--
% which provides coarse reconstructions that %cannot handle 
% lack hair, clothing, and geometric details. 
% Multi-view settings can help resolve some of the occlusions as well as depth ambiguities, but require a dense array of RGB cameras to achieve a detailed reconstruction \cite{collet2015high, joo2015panoptic,vlasic2009dynamic} 
% % which is not easily accessible. % available. 
% A more convenient capture system %that could in principle still deliver optimal results 
% is the \emph{sparse} multi-view setting, where only a handful of cameras is required.
% %, where the number of cameras is limited to 2-15. 
% However, due to the decreased number of views and increased level of occlusion, 
% existing methods require segmentation masks and a pre-scanned template mesh \cite{liu2011markerless, wu2013set}, rely on a coarse body model \cite{zhang2021lightweight, huang2021dynamic}, or require temporal information \cite{zheng2021deepmulticap, huang2021dynamic}.

% %%%\todo{pifu should go somewhere here... pifu is single human though. Goes into the recent line of work: pre-train on a large dataset and then at test time use sparse views: \cite{zheng2021deepmulicap}}


% % --> send the message that MV with SMPL is kind of solved, but we need to take the next step. 
% %but as shown by several works \cite{...}, this is not sufficient to solve the problem. In fact, % in effect, in practice, at bottom
% % "The first sparse for multi-humans" -> \cite{zhang2021lightweight}, 

% % In parallel, there has been development in neural 3D geometry reconstruction by using SDFs \cite{..} or volume-rendered SDFs \cite{...}. These are great but require many views. For the sparse view case, infonerf, regnerf, dietnerf. They show improved results, but as demonstrated here these are not sufficient for the multi-human reconstruction problem.




% %%%%\todo{improving geometry to improve rendering?}

% % Inspired by the success of NeRF~\cite{mildenhall2020nerf}, 
% A parallel line of work simultaneously tackles the novel-view-synthesis and geometry-reconstruction problems by combining neural coordinate-based representations, \eg implicit signed distance functions (SDFs) \cite{park2019deepsdf}, with differentiable rendering \cite{yariv2021volume,wang2021neus,yariv2020multiview,mildenhall2020nerf}. 
% This approach has the advantage of producing, along with geometry, renderings from novel viewpoints that can capture complex surface/light interactions, increasing the scope of applications. 
% NeRF~\cite{mildenhall2020nerf}, for example, uses volumetric rendering to produce impressive images under novel views, albeit at the cost of sub-optimal geometries due to the unconstrained volumetric representation. 
% SDF-based methods \cite{yariv2021volume,wang2021neus,yariv2020multiview}, while delivering images of slightly lower quality, have been shown to produce 3D surfaces that are competitive with classical approaches. 
% For humans, this has been leveraged to obtain geometry and appearance from monocular video \cite{jiang2022selfrecon,chen2021animatable}, RGB-D video \cite{dong2022pina}, and sparse multi-view video \cite{wang2022arah, liu2021neural, peng2021neural, zheng2021deepmulticap, kwon2021neural, peng2021animatable, xu2021h, weng2022humannerf}. 
% %% NOTE: \cite{xu2021h} also has experiments for static sparse MV images (single human)
% %% Monocular video: also "SelfNeRF: Fast Training NeRF for Human from Monocular Self-rotating Video". But this isn't published yet
% %
% However, 
% none of these works, with the exception of \cite{zheng2021deepmulticap}, were designed to handle the increased geometric complexity and occlusion of the multi-human case. DeepMultiCap \cite{zheng2021deepmulticap} is the only %multi-view neural reconstruction 
% work that addresss the multi-human setting, but the method requires a set of videos, which effectively becomes a dense array of views as long as deformations are modeled correctly.

% In this paper we address, for the first time, 
% the problem of reconstructing multiple 3D humans from a \emph{static} and \emph{sparse} set of cameras using neural implicit surfaces. 
% Our key insight is that human-specific geometric constraints can be leveraged to tackle the challenging sparse-view setting.
% %by first fitting  a SMPL body model to the input, building on top of the works 
% % building on the set of works that can faithfully disambiguate pose and shape for multiple people, but only deliver coarse reconstructions, \eg~\cite{huang2017towards,zhang2021lightweight}. %\va{This is a bit risky, since the way we get smpl is a bit shady (in practice, from dense MV cameras)}. 
% Specifically, we first obtain a SMPL body model from the input data, and use this to train a geometry-only implicit SDF network, where we define the surface of a multi-human scene as the zero-level set of the SDF. 
% In a second step, the geometry network is fine-tuned using multi-view images by leveraging the recently-proposed NeuS~\cite{wang2021neus} along with uncertainty-based rendering \cite{deng2022depth,roessle2022dense}, where the SMPL meshes are treated as noisy estimations.  
% % [to keep the method general for novel scenes, we do not rely on features pre-trained on a large dataset]
% To achieve higher rendering quality from sparse training views, we additionally propose a patch-based regularization loss 
% that guarantees consistency across different rays, and a saturation regularization that ensure consistency for variable image illuminations within a same scene.% \va{I feel this is a problem that was not pointed out before, and it would be nice to elaborate better}.

% We evaluate our method quantitatively and qualitatively on real multi-human (CMU Panoptic~\cite{Simon_2017_CVPR,Joo_2017_TPAMI}) and synthetic (MultiHuman~\cite{zheng2021deepmulticap}) datasets. We demonstrate results on 5,10,15 and 20 training views, where we achieve state-of-the-art performance in terms of surface reconstruction and image quality. %Another advantage is that the proposed geometry initialization enables more efficient learning.

% In summary, our contributions are: 
% % 1. We demonstrate an approach to use multiple human bodies as geometric regularization. We propose to use SDF along with estimated uncertainty as explicit geometry constraints, allowing the  learning of details. \\
% (1) We propose the first neural 
% implicit geometry and appearance reconstruction method for multiple humans using a sparse set of static views; 
% (2) To address the problem of occlusion, we propose the use SMPL for geometric regularization; 
% (3) To handle sparse views under occlusion, we propose a patch-based ray consistency regularization, and an image saturation regularization that ensures illumination consistency across views.

%  Code and models will be made available.
% 3.  \todo{I will re-phrase this after finishing the method. This needs to come before in the introduction, if it stays} We combine box rendering with editing, which enables editing multi-humans in 3D space during inference without using masks and segmentation, while preserving the detail and quality of rendered novel multi-human views.   \\



%%%%%%%%%%%%%%%%%%%%%%
% --------------------------------


% Multiple human reconstruction has been a popular and significant topic in computer vision and computer graphics, which enables various applications, such as Virtual Reality (VR) and Augmented Reality (AR), films, teleconferences, and so on. Researches for human reconstruction mainly consist of model-based and model-free, both of them has achieved tremendous progress in recent years. 

% Traditional model-based approaches utilize parametric human bodies (\eg human SMPL \cite{loper2015smpl}) or truncated signed distance fields (TSDFs) for human geometry reconstruction \cite{loper2015smpl,bhatnagar2020combining,liu2021neural,yu2017bodyfusion,yu2018doublefusion,peng2021neural}. %Though human SMPL could provide solid geometry notes, 
% While relying on the human model(\eg SMPL skinning weights of a naked body) may risk losing hair and clothing details. Model-free methods learns from a set of multi-view images or videos with loosing detail constraints \cite{peng2021neural,dong2022pina,peng2021neural,dong2022pina, liu2021neural}, allowing more realistic reconstructed details. With the recent progress of implicit neural representations\cite{sitzmann2019deepvoxels,xu2022point,wiles2020synsin, mildenhall2020nerf,zhang2020NeRF++}, many model-free methods utilize NeRF to achieve photo-realistic rendering. However, NeRF\cite{mildenhall2020nerf} based methods do not sufficiently constrain the 3D geometry, making them hard to reconstruct high-fidelity surfaces or accurate geometry. Further, most existing researches focus on single person reconstruction from videos \cite{peng2021neural,dong2022pina} or images\cite{peng2021neural,dong2022pina, liu2021neural}. For multiple human reconstructions from a single frame, the task becomes even harder to recover geometry and appearance with sufficient details due to multiple human scenes usually containing more complex geometry and occlusions. In this paper, we address those challenges by taking advantage of recent neural implicit surface reconstruction methods\cite{yariv2021volume,wang2021neus,yariv2020multiview} and human bodies models(\eg SMPL\cite{loper2015smpl}), reconstructing multi-human 3D geometry and appearance simultaneously. 

% Recent neural implicit surface reconstruction methods\cite{yariv2021volume,wang2021neus,yariv2020multiview} propose to use a signed distance function (SDF) to present the surface and combine SDF-based density function with volumetric rendering. They could learn an implicit SDF representation and reconstruct geometry and appearance simultaneously. However, the optimization of those methods still depends on direct color field and lack explicit geometry constraints, making them hard to reconstruct consistent geometry and appearance for unseen regions(\eg sparse input view) and occlusion areas (\eg multiple human scene). To alleviate those limitations, we propose to use a multiple human bodies models(SMPL) as geometry initialization, as well as to provide explicit geometry constraints.

% %\cite{yariv2021volume} uses volume density function as Laplace’s cumulative distribution function (CDF) for geometry representation, 
% %Neus\cite{wang2021neus} and Volsdf\cite{yariv2021volume} are among scene representation methodsuse signed distance functions (SDF) for surface representation and introduce the SDF-induced density function to enable the volume rendering to

% %More recently, with the rapidly progress of implicit neural representations\cite{sitzmann2019deepvoxels,xu2022point,wiles2020synsin, mildenhall2020nerf,zhang2020NeRF++}, many researches combine differentiable neural rendering with human bodies model \cite{liu2021neural,peng2021neural,dong2022pina}. Among those,  \cite{liu2021neural} combines human SMPL with implicit learning, and uses it as a proxy to unwarp the surrounding 3D space into a canonical pose, \cite{peng2021neural} could render detailed novel views (\eg with hair,cloths) of the human body from a set sparse input video. Thanks to neural radiance fields (NeRF\cite{mildenhall2020nerf}), those approaches could achieve photo-realistic rendering. However, NeRF\cite{mildenhall2020nerf} does not sufficiently constrain the 3D geometry, making those methods hard to reconstruct high-fidelity surfaces or accurate geometry.

% %Recent scene representation methods combines surface representation with neural volume rendering \cite{yariv2021volume,wang2021neus,yariv2020multiview}. % In order to achieve high quality surface reconstruction while retaining rendering quality, 
% %Among those, \cite{oechsle2021unisurf} proposes a unified framework to reconstruct solid objects from 2D image inputs, 

% Specifically, inspired by Neus\cite{wang2021neus} and Volsdf \cite{yariv2021volume}, we define the surface of multiple human SMPL as a zero-level set of a signed distance function (SDF)  and use it to train a neural SDF presentation as a geometry prior. However, relying on SDF sampled from SMPL directly may risk losing hair and clothing details. Thus, we learn SDF together with uncertainty and combine them together for explicit geometry regularization, which enables the learning of multiple person's clothing and hair details. In addition,
% to achieve higher rendering quality from sparse training views, we propose a patch-based regularization to guarantee consistency across different rays and saturation regularization to ensure image illumination consistency. 
% %retaining multi-view consistency remains a challenge for NeuS-related methods, especially for complex thin structures and large smooth regions. To tackle this challenge, we propose a patch-based ray regularization for photo-metric consistency and saturation regularization for image illumination consistency. 

% %Assuming this SDF presentation storing the multi-human geometry information, we incorporate this by re-initializing the follow-up neural network with the learned weights, utilizing the SDF value for the weights of hierarchical sampling \cite{mildenhall2020nerf} and volume rendering. Inspired by \cite{ortiz2022isdf, dong2022pina}, instead of using the SDF value sampled from SMPL to supervise the neural network learning directly(which consumes lots of time and lacks details), we propose a point-based SDF regularization allowing for the learning of multi-human clothing and hair details. Moreover, we present an approach for human editing during rendering, without additional training or information(\eg depth, mask or segmentation).   \\
% We evaluate our method on both real multi-human datasets (CMU Panoptic Dataset\cite{Simon_2017_CVPR,Joo_2017_TPAMI}) and synthetic data (THUman2.0 Dataset
% and MultiHuman-Dataset \cite{tao2021function4d,zheng2021deepmulticap}) both qualitatively and quantitatively. Specifically,  we demonstrate testing results on 10,15,20 training views,respectively, and achieve state-of-the-art performance on both real datasets and synthetic datasets. %Another advantage is that the proposed geometry initialization enables more efficient learning. 
% In summary, our contributions include:\\
% 1. We demonstrate an approach to use multiple human bodies as geometric initialization. We propose to use SDF along with estimated uncertainty as explicit geometry constraints, allowing the  learning of details. \\
% 2. We propose a patch-based KL regularization to ensure consistency across different rays and image saturation regularization for illumination consistency. \\
% 3. We combine box rendering with editing, which enables editing multi-humans in 3D space during inference without using masks and segmentation, while preserving the detail and quality of rendered novel multi-human views.   \\
