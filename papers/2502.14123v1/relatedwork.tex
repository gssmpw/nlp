\section{Related Work}
\paragraph{Online SGD in high-dimensional linear regression.}
There is a line of works studying the excess risk bound of online SGD in the overparameterized setting using a bias-variance decomposition \citep{bach2013non, dieuleveut2015non, defossez2015averaged, dieulevuet2017harder, lakshminarayanan2018linear, jain2018parallelizing, berthier2020tight, zou2021benign, wu2022last, lin2024scaling}.
In particular, \citet{zou2021benign} focused on constant-stepsize SGD with iterate averaging from the beginning or tail averaging, and derived the first instance-dependent excess risk bound of SGD in overparameterized linear regression.
\citet{wu2022last} studied the last iterate risk bound of SGD with exponentially decaying stepsize, which is found to achieve a excess risk bound similar to SGD with iterate averaging.
SGD with Nesterov momentum \citep{nesterov2013introductory} and tail averaging has also been studied \citep{jain2018accelerating, varre2022accelerated, li2023risk}, with \citet{li2023risk} obtaining an instance-dependent risk bound.

\paragraph{Understanding the effect of EMA.}
The favorable generalization properties of EMA in practice have been observed in several works \citep{tarvainen2017mean, izmailov2018averaging}.
Through empirical experiments, \citet{sandler2023training} connected the stabilizing effect of averaging methods (e.g., EMA) with learning rate scheduling, which coincides with the finding of \citet{wu2022last}.
A similar theoretical result was given by \citet{defazio2020momentum}, but the EMA is performed on the momentum instead of the iterates.