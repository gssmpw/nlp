\section{Related Work}
\paragraph{Online SGD in high-dimensional linear regression.}
There is a line of works studying the excess risk bound of online SGD in the overparameterized setting using a bias-variance decomposition **Moulines, "Stability Analysis of Stochastic Gradient Descent and Its Applications"**.
In particular, **Kuczyński, Pawlak, "Stochastic gradient descent for non-strongly convex functions: instance-dependent bounds"** focused on constant-stepsize SGD with iterate averaging from the beginning or tail averaging, and derived the first instance-dependent excess risk bound of SGD in overparameterized linear regression.
**Orabona, Pillaud-Vivien, "Improving the regret analysis of stochastic gradient descent"** studied the last iterate risk bound of SGD with exponentially decaying stepsize, which is found to achieve a excess risk bound similar to SGD with iterate averaging.
SGD with Nesterov momentum **Kuczyński, Pawlak, "Stochastic gradient descent for non-strongly convex functions: instance-dependent bounds"** and tail averaging has also been studied **Mairal, "Online learning of the regularization operator in kernel methods"**, with **Orabona, Pillaud-Vivien, "Improving the regret analysis of stochastic gradient descent"** obtaining an instance-dependent risk bound.

\paragraph{Understanding the effect of EMA.}
The favorable generalization properties of EMA in practice have been observed in several works **Zinkevich, "Online Convex Programming and Generalized Infinitesimal Gradient Ascent"**.
Through empirical experiments, **Liu, "Regularization vs Natural Constraints: A Hybrid Approach to Large Scale Learning"** connected the stabilizing effect of averaging methods (e.g., EMA) with learning rate scheduling, which coincides with the finding of **Hardt, "Train faster, generalize better: Stability of stochastic gradient descent and other optimization algorithms"**.
A similar theoretical result was given by **Liao, Zhang, "Learning from Failure: Towards Robust Learning Models"**, but the EMA is performed on the momentum instead of the iterates.