@article{zou2021benign,
  title={Benign overfitting of constant-stepsize sgd for linear regression},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham M},
  journal={The 34th Annual Conference on Learning Theory},
  year={2021}
}

@inproceedings{wu2022last,
  title={Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression},
  author={Wu, Jingfeng and Zou, Difan and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
  booktitle={International Conference on Machine Learning},
  pages={24280--24314},
  year={2022},
  organization={PMLR}
}

@article{li2023risk,
  title={Risk Bounds of Accelerated SGD for Overparameterized Linear Regression},
  author={Li, Xuheng and Deng, Yihe and Wu, Jingfeng and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2311.14222},
  year={2023}
}

@article{zhang2024does,
  title={How Does Critical Batch Size Scale in Pre-training?},
  author={Zhang, Hanlin and Morwani, Depen and Vyas, Nikhil and Wu, Jingfeng and Zou, Difan and Ghai, Udaya and Foster, Dean and Kakade, Sham},
  journal={arXiv preprint arXiv:2410.21676},
  year={2024}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent Robbins-Monro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{block2023butterfly,
  title={Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression},
  author={Block, Adam and Foster, Dylan J and Krishnamurthy, Akshay and Simchowitz, Max and Zhang, Cyril},
  journal={arXiv preprint arXiv:2310.11428},
  year={2023}
}

@article{sandler2023training,
  title={Training trajectories, mini-batch losses and the curious role of the learning rate},
  author={Sandler, Mark and Zhmoginov, Andrey and Vladymyrov, Max and Miller, Nolan},
  journal={arXiv preprint arXiv:2301.02312},
  year={2023}
}

@article{karras2019style,
  title={A Style-Based Generator Architecture for Generative Adversarial Networks},
  author={Karras, Tero},
  journal={arXiv preprint arXiv:1812.04948},
  year={2019}
}

@inproceedings{yaz2018unusual,
  title={The unusual effectiveness of averaging in GAN training},
  author={Yaz, Yasin and Foo, Chuan-Sheng and Winkler, Stefan and Yap, Kim-Hui and Piliouras, Georgios and Chandrasekhar, Vijay and others},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{karras2022elucidating,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26565--26577},
  year={2022}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@inproceedings{nichol2021improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International conference on machine learning},
  pages={8162--8171},
  year={2021},
  organization={PMLR}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}

@article{balaji2022ediffi,
  title={Ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv 2022},
  author={Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, A and Song, J and Kreis, K and Liu, MY},
  journal={arXiv preprint arXiv:2211.01324},
  year={2022}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@inproceedings{kang2023scaling,
  title={Scaling up gans for text-to-image synthesis},
  author={Kang, Minguk and Zhu, Jun-Yan and Zhang, Richard and Park, Jaesik and Shechtman, Eli and Paris, Sylvain and Park, Taesung},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10124--10134},
  year={2023}
}

@inproceedings{karras2024analyzing,
  title={Analyzing and improving the training dynamics of diffusion models},
  author={Karras, Tero and Aittala, Miika and Lehtinen, Jaakko and Hellsten, Janne and Aila, Timo and Laine, Samuli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24174--24184},
  year={2024}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{busbridge2024scale,
  title={How to scale your ema},
  author={Busbridge, Dan and Ramapuram, Jason and Ablin, Pierre and Likhomanenko, Tatiana and Dhekane, Eeshan Gunesh and Suau Cuadros, Xavier and Webb, Russell},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ahn2024adam,
  title={Adam with model exponential moving average is effective for nonconvex optimization},
  author={Ahn, Kwangjun and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2405.18199},
  year={2024}
}

@article{dieuleveut2015non,
  title={Non-parametric Stochastic Approximation with Large Step sizes},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  year={2015}
}

@article{dieulevuet2017harder,
author = {Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
title = {Harder, better, faster, stronger convergence rates for least-squares regression},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in O(1/n2), and in terms of dependence on the noise and dimension d of the problem, as O(d/n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small in some distances while the "optimal" terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3520â€“3570},
numpages = {51},
keywords = {accelerated gradient, convex optimization, least-squares regression, non-parametric estimation, stochastic gradient}
}

@inproceedings{defossez2015averaged,
  title={Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Artificial Intelligence and Statistics},
  pages={205--213},
  year={2015},
  organization={PMLR}
}

@article{jain2018parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of machine learning research},
  volume={18},
  number={223},
  pages={1--42},
  year={2018}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{berthier2020tight,
  title={Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2576--2586},
  year={2020}
}

@inproceedings{jain2018accelerating,
  title={Accelerating stochastic gradient descent for least squares regression},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={Conference On Learning Theory},
  pages={545--604},
  year={2018},
  organization={PMLR}
}

@article{jain2017markov,
  title={A markov chain theory approach to characterizing the minimax optimality of stochastic gradient descent (for least squares)},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Pillutla, Venkata Krishna and Sidford, Aaron},
  journal={arXiv preprint arXiv:1710.09430},
  year={2017}
}

@inproceedings{varre2022accelerated,
  title={Accelerated sgd for non-strongly-convex least squares},
  author={Varre, Aditya and Flammarion, Nicolas},
  booktitle={Conference on Learning Theory},
  pages={2062--2126},
  year={2022},
  organization={PMLR}
}

@article{lin2024scaling,
  title={Scaling Laws in Linear Regression: Compute, Parameters, and Data},
  author={Lin, Licong and Wu, Jingfeng and Kakade, Sham M and Bartlett, Peter L and Lee, Jason D},
  journal={arXiv preprint arXiv:2406.08466},
  year={2024}
}

@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{defazio2020momentum,
  title={Momentum via primal averaging: Theoretical insights and learning rate schedules for non-convex optimization},
  author={Defazio, Aaron},
  journal={arXiv preprint arXiv:2010.00406},
  year={2020}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  pages={331--368},
  year={2007},
  publisher={Springer}
}

@article{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  author={Bach, Francis and Moulines, Eric},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{lakshminarayanan2018linear,
  title={Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
  author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1347--1355},
  year={2018},
  organization={PMLR}
}