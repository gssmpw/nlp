@article{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  author={Bach, Francis and Moulines, Eric},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{berthier2020tight,
  title={Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2576--2586},
  year={2020}
}

@article{defazio2020momentum,
  title={Momentum via primal averaging: Theoretical insights and learning rate schedules for non-convex optimization},
  author={Defazio, Aaron},
  journal={arXiv preprint arXiv:2010.00406},
  year={2020}
}

@inproceedings{defossez2015averaged,
  title={Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Artificial Intelligence and Statistics},
  pages={205--213},
  year={2015},
  organization={PMLR}
}

@article{dieuleveut2015non,
  title={Non-parametric Stochastic Approximation with Large Step sizes},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={The Annals of Statistics},
  volume={44},
  number={4},
  year={2015}
}

@article{dieulevuet2017harder,
author = {Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
title = {Harder, better, faster, stronger convergence rates for least-squares regression},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in O(1/n2), and in terms of dependence on the noise and dimension d of the problem, as O(d/n). Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension-free quantities that may still be small in some distances while the "optimal" terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3520â€“3570},
numpages = {51},
keywords = {accelerated gradient, convex optimization, least-squares regression, non-parametric estimation, stochastic gradient}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@inproceedings{jain2018accelerating,
  title={Accelerating stochastic gradient descent for least squares regression},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={Conference On Learning Theory},
  pages={545--604},
  year={2018},
  organization={PMLR}
}

@article{jain2018parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  journal={Journal of machine learning research},
  volume={18},
  number={223},
  pages={1--42},
  year={2018}
}

@inproceedings{lakshminarayanan2018linear,
  title={Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
  author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
  booktitle={International conference on artificial intelligence and statistics},
  pages={1347--1355},
  year={2018},
  organization={PMLR}
}

@article{li2023risk,
  title={Risk Bounds of Accelerated SGD for Overparameterized Linear Regression},
  author={Li, Xuheng and Deng, Yihe and Wu, Jingfeng and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2311.14222},
  year={2023}
}

@article{lin2024scaling,
  title={Scaling Laws in Linear Regression: Compute, Parameters, and Data},
  author={Lin, Licong and Wu, Jingfeng and Kakade, Sham M and Bartlett, Peter L and Lee, Jason D},
  journal={arXiv preprint arXiv:2406.08466},
  year={2024}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{sandler2023training,
  title={Training trajectories, mini-batch losses and the curious role of the learning rate},
  author={Sandler, Mark and Zhmoginov, Andrey and Vladymyrov, Max and Miller, Nolan},
  journal={arXiv preprint arXiv:2301.02312},
  year={2023}
}

@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{varre2022accelerated,
  title={Accelerated sgd for non-strongly-convex least squares},
  author={Varre, Aditya and Flammarion, Nicolas},
  booktitle={Conference on Learning Theory},
  pages={2062--2126},
  year={2022},
  organization={PMLR}
}

@inproceedings{wu2022last,
  title={Last iterate risk bounds of sgd with decaying stepsize for overparameterized linear regression},
  author={Wu, Jingfeng and Zou, Difan and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
  booktitle={International Conference on Machine Learning},
  pages={24280--24314},
  year={2022},
  organization={PMLR}
}

@article{zou2021benign,
  title={Benign overfitting of constant-stepsize sgd for linear regression},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham M},
  journal={The 34th Annual Conference on Learning Theory},
  year={2021}
}

