\documentclass[11pt]{article}
%\pdfoutput=1
\usepackage{enumerate}
\usepackage{pdfsync}
\usepackage[OT1]{fontenc}
%\usepackage{kpfonts}

\usepackage[usenames]{color}
%\usepackage[dvips]{graphicx}
\usepackage{smile}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=blue
            ]{hyperref}
%\usepackage{mathrsfs}
\usepackage{fullpage}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{pbox}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{float}
\usepackage{wrapfig,lipsum}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{framed}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}
\usepackage{booktabs} % for professional tables
\newcommand{\dsfont}[1]{\texttt{#1}}
\newtheorem{condition}[theorem]{Condition}

%%%%%%%%%%%%%%%%%%%%%%%%%%
\allowdisplaybreaks
\usepackage{colortbl}
%\def \CC {}
\def \alg {\mathtt{Alg}}
\def \algname {\text{algorithm}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{xcolor}

\usepackage{makecell}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\owb}{\overline{\wb}}
\newcommand{\obmeta}{\overline{\bmeta}}


\setlength{\FrameSep}{0pt}




\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\title{\huge Understanding SGD with Exponential Moving Average:\\ A Case Study in Linear Regression}
\author
{
    Xuheng Li\thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt xuheng.li@cs.ucla.edu}}
    ~~~~
    Quanquan Gu\thanks{Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: {\tt qgu@cs.ucla.edu}}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{enumitem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifdefined\final
\usepackage[disable]{todonotes}
\else
\usepackage[textsize=tiny]{todonotes}
\fi
\setlength{\marginparwidth}{0.6in}
\newcommand{\todox}[2][]{\todo[size=\scriptsize,color=blue!20!white,#1]{Xuheng: #2}}
\newcommand{\todoq}[2][]{\todo[size=\scriptsize,color=orange!20!white,#1]{Quanquan: #2}}
\newcommand{\todoj}[2][]{\todo[size=\scriptsize,color=green!20!white,#1]{Jiafan: #2}}
\newcommand{\todoh}[2][]{\todo[size=\scriptsize,color=green!20!white,#1]{Heyang: #2}}

\definecolor{shadecolor}{rgb}{0.78, 0.95, 0.89}

\definecolor{linkcolor}{HTML}{ED1C24}
\definecolor{LightCyan}{rgb}{0.58, 0.94, 0.85}
\hypersetup{colorlinks=true, linkcolor=linkcolor, urlcolor=black}
\usepackage[toc, page, header]{appendix}
\usepackage{titletoc}

\begin{document}
    \date{}
    \maketitle

\begin{abstract}
Exponential moving average (EMA) has recently gained significant popularity in training modern deep learning models, especially diffusion-based generative models.
However, there have been few theoretical results explaining the effectiveness of EMA.
In this paper, to better understand EMA, we establish the risk bound of online SGD with EMA for high-dimensional linear regression, one of the simplest overparameterized learning tasks that shares similarities with neural networks.
Our results indicate that (i) the variance error of SGD with EMA is always smaller than that of SGD without averaging, and (ii) unlike SGD with iterate averaging from the beginning, the bias error of SGD with EMA decays exponentially in every eigen-subspace of the data covariance matrix.
Additionally, we develop proof techniques applicable to the analysis of a broad class of averaging schemes.
\end{abstract}

\section{Introduction}

The exponential moving average (EMA, \citealt{polyak1992acceleration, ruppert1988efficient}) in conjunction with stochastic optimization algorithms is being extensively used in training deep learning models.
EMA is most popular in training generative models based on GAN \citep{yaz2018unusual, karras2019style, kang2023scaling}, and more recently in diffusion models \citep{song2020score, dhariwal2021diffusion, nichol2021improved, song2020denoising, balaji2022ediffi, karras2022elucidating, rombach2022high, karras2024analyzing}, among other applications \citep{block2023butterfly, busbridge2024scale}.
By maintaining an averaged set of model parameters, EMA displays the capability to stabilize training by suppressing the noise of stochastic gradients, and it has been shown empirically that the effect of EMA is similar to that of learning rate scheduling \citep{sandler2023training}.
However, this phenomenon is less studied from a theoretical perspective.
Notable exceptions include a recent work by \citet{ahn2024adam}, which studied Adam with EMA in nonconvex optimization. However, this work is restricted to the finite-dimensional setting, which departs from the practical training of overparameterized neural networks.
\citet{block2023butterfly} revealed the variance-reducing benefit of EMA, but the bias contraction of stochastic optimization algorithms with EMA remains unknown.
Meanwhile, a recent line of works \citep{defossez2015averaged, dieulevuet2017harder, jain2018parallelizing, berthier2020tight, zou2021benign, wu2022last} characterized the generalization properties of SGD in overparameterized linear regression with other averaging schemes (e.g., iterate averaging from the beginning and tail averaging).
In particular, \citet{zou2021benign} presented an instance-dependent and dimension-free excess risk bound for SGD with iterate averaging and tail averaging.
Given these results, a characterization of the generalization properties of SGD with EMA and a comparison against SGD with other averaging schemes becomes an urgent subject of study, especially in the setting of setting of high-dimensional linear regression.

In this paper, we tackle this open problem by studying SGD with EMA in the overparameterized linear regression setting, and comparing the results with SGD without averaging, along with iterate averaging and tail averaging in \citet{zou2021benign}.
Our contributions are summarized as follows:

\begin{table}[ht]
\footnotesize
\centering
\caption{Comparison of SGD with EMA against SGD without averaging, SGD with iterate averaging from the beginning and with tail averaging. We fix the eigenvalue spectrum of the covariance matrix $\{\lambda_i\}$, the learning rate $\delta$, and the number of iterations $N$. We assume that tail averaging is performed over the last $N-s$ iterates, and $\alpha$ is the weight of the moving average in EMA. We study not only the effective bias and the variance error, but also the effective dimension that plays a role similar to the model dimension in our excess risk bound. Compared with SGD without averaging which has the same exponentially decaying effective bias as EMA, SGD with EMA has a smaller variance error. SGD with either iterate averaging or tail averaging enjoys a smaller variance error than SGD without averaging, and the variance error and the effective dimension of SGD with tail averaging are identical to that of EMA when $(1-\alpha)(N-s)=1$. However, SGD with neither iterate averaging nor tail averaging achieves the effective bias decay rate that is exponential in $N$.}\label{table:comparison}
%\footnotesize
\begin{tabular}{cccc}
\toprule
Averaging scheme & Effective bias decay rate & Variance error in subspace of $\lambda_i$ & Eigenvalue at effective dim.\\
\midrule
w/o averaging & Exponential in $N$ & $\cO(\min\{\delta\lambda_i, N\delta^2\lambda_i^2\})$ & $1/(N\delta)$\\
Iterate averaging & Polynomial in $N$ & $\cO(\min\{1/N, N\delta^2\lambda_i^2\}$) & $1/(N\delta)$\\
Tail averaging & Exponential in $s$ & $\cO(\min\{1/(N-s), \delta\lambda_i, N\delta^2\lambda_i^2\})$ & $1/((N-s)\delta)$ and $1/(N\delta)$\\
\rowcolor{LightCyan!40!}EMA & Exponential in $N$ & $\cO(\min\{1-\alpha, \delta\lambda_i, N\delta^2\lambda_i^2\})$ & $(1-\alpha)/\delta$ and $1/(N\delta)$\\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}[leftmargin=*]
\item We derive the first instance-dependent excess risk bound of the linear regression model trained with SGD with EMA. We also show that the analysis is tight by presenting a lower bound that is almost matching with the upper bound. The excess risk bound consists of the effective bias and the effective bias, both of them further decomposed into each eigen-subspace of the data covariance matrix. Therefore, the excess risk bound is only related to the eigenvalue spectrum, and is irrelevant to the ambient model dimension, making the result applicable to the overparameterized setting.
\item We compare the excess risk bound of SGD with EMA against SGD without averaging as well as other averaging schemes, e.g., iterate averaging from the beginning and tail averaging, which was studied in \citet{zou2021benign}. The comparison is summarized in Table~\ref{table:comparison}. We show that (i) the effective bias of SGD with EMA decays exponentially in the number of iterations, and (ii) the effective variance of SGD with EMA is smaller than SGD without averaging, and is comparable to that of SGD with iterate averaging or tail averaging. Specifically, we observe a strong connection between EMA and tail averaging in terms of the effective variance: Suppose the tail averaging is performed over the last $N-s$ iterates in a total of $N$ iterations; if $\alpha$, the averaging parameter of EMA, satisfies $(1-\alpha)(N-s)=1$, then the effective variance of SGD with EMA is identical to that of SGD with tail averaging. However, the exponential decay rate of the effective bias can be achieved by SGD with tail averaging only when setting $s=\Theta(N)$ with a known training horizon $N$. This indicates that SGD with EMA has an advantage over tail averaging in the setting of continual learning.
\item From a technical viewpoint, we identify a broad class of averaging schemes that covers all averaging methods discussed in this work. Using a standard bias-variance decomposition, we derive a crucial reformulation of the both the bias error and the variance error. Built on this reformulation, an analysis framework for all averaging schemes belonging to this class is developed in this work.
\end{itemize}

\paragraph{Notations.}
% We use $\langle\cdot, \cdot\rangle$ to denote the inner product of vectors.
% For a vector $\xb$ in a Hilbert space $\cH$, let $\|\xb\|_2$ denote the norm in $\cH$.
For a vector $\xb$, we use $(\xb)_i$ to denote its $i$-th entry.
We use $\otimes$ to denote the tensor product, and $\circ$ to denote the operation of linear operators on matrices.
We use $\langle\Ab, \Bb\rangle\coloneqq\tr(\Ab\Bb^\top)$ to denote the inner product of matrices $\Ab$ and $\Bb$.
For a PSD matrix $\Ab$ and a vector $\xb\in\cH$, define $\|\xb\|_{\Ab}\coloneqq\sqrt{\xb^\top\Ab\xb}$.
For any positive integer $n$, we use $[n]$ to denote the set $\{1, 2, \dots, n\}$.
We use standard asymptotic notations $\cO(\cdot)$, $\Omega(\cdot)$, and $\Theta(\cdot)$.

\section{Related Work}

\paragraph{Online SGD in high-dimensional linear regression.}
There is a line of works studying the excess risk bound of online SGD in the overparameterized setting using a bias-variance decomposition \citep{bach2013non, dieuleveut2015non, defossez2015averaged, dieulevuet2017harder, lakshminarayanan2018linear, jain2018parallelizing, berthier2020tight, zou2021benign, wu2022last, lin2024scaling}.
In particular, \citet{zou2021benign} focused on constant-stepsize SGD with iterate averaging from the beginning or tail averaging, and derived the first instance-dependent excess risk bound of SGD in overparameterized linear regression.
\citet{wu2022last} studied the last iterate risk bound of SGD with exponentially decaying stepsize, which is found to achieve a excess risk bound similar to SGD with iterate averaging.
SGD with Nesterov momentum \citep{nesterov2013introductory} and tail averaging has also been studied \citep{jain2018accelerating, varre2022accelerated, li2023risk}, with \citet{li2023risk} obtaining an instance-dependent risk bound.

\paragraph{Understanding the effect of EMA.}
The favorable generalization properties of EMA in practice have been observed in several works \citep{tarvainen2017mean, izmailov2018averaging}.
Through empirical experiments, \citet{sandler2023training} connected the stabilizing effect of averaging methods (e.g., EMA) with learning rate scheduling, which coincides with the finding of \citet{wu2022last}.
A similar theoretical result was given by \citet{defazio2020momentum}, but the EMA is performed on the momentum instead of the iterates.

\section{Preliminaries}

% In this section, we detail the setting of linear regression optimized using SGD with EMA. We also introduce the assumptions that will be used in our analysis.



\subsection{Linear Regression and SGD with EMA}

We consider the high-dimensional linear regression setting similar to \citet{zou2021benign}.
Both the weight vectors and the data features lie within a Hilbert space $\cH$ with inner product $\langle\cdot, \cdot\rangle$, whose dimensionality is either finite or countably infinite.
The goal is to minimize the risk function defined as
\begin{align*}
L(\wb)\coloneqq1/2\cdot\EE_{(\xb, y)\sim\cD}[(y-\langle\wb, \xb\rangle)^2],
\end{align*}
where $\cD$ is an underlying distribution of the data, $\xb\in\cH$ is the input feature vector, $y\in\RR$ is the response, and $\wb\in\cH$ is the weight vector to be optimized.

We consider optimizing the objective using SGD with EMA.
At iteration $t$, a random sample $(\xb_t, y_t)\sim\cD$ is observed, and the weight vector is updated as follows:
\begin{align*}
\wb_t=\wb_{t-1}+\delta(y_t-\langle\wb_{t-1}, \xb_t\rangle)\xb_t,
\end{align*}
where $\delta>0$ is a constant learning rate. Meanwhile, we maintain the EMA of the iterates by the following recursive formula:
\begin{align}\label{eq:def_EMA}
\owb_0=\wb_0;\qquad\owb_t=\alpha\owb_{t-1}+(1-\alpha)\wb_{t-1},
\end{align}
where $\alpha\in(0, 1)$ is the averaging parameter.
Let $N$ be the number of iterations. The final output is the $\owb_N$, which can be decomposed into the weighted sum of $\wb_t$:
\begin{align}\label{eq:weighted_sum}
\owb_N=\alpha^N\wb_0+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{N-1-t}\wb_t.
\end{align}

\subsection{Assumptions}

We now introduce the assumptions used in the analysis of SGD with EMA, following \citet{zou2021benign, wu2022last, li2023risk}. The first assumption is a regularity condition that characterizes the second-order moment of the feature vector.

\begin{assumption}[Second-order moment]\label{assumption:second}
We assume that the data covariance matrix $\Hb=\EE[\xb\xb^\top]$ exists and is finite. Without loss of generality, we assume that $\Hb=\mathrm{diag}(\lambda_1, \lambda_2, \dots)$ is a diagonal matrix with its eigenvalues listed in descending order.
We further assume that $\tr(\Hb)=\sum_{i=1}^\infty\lambda_i$ is finite. For the convenience of our analysis, we assume that $\Hb\succ\zero$, i.e., $L(\wb)$ admits a unique minimum $\wb_*$.
\end{assumption}

We then present the assumptions that characterize the fourth-order moment of the data:


\begin{assumption}[Fourth moment condition, upper bound]\label{assumption:fourth}
We assume that the fourth moment operator $\cM=\EE[\xb\otimes\xb\otimes\xb\otimes\xb]$ exists and is finite. Furthermore, there exists a scalar $\psi>0$ such that for any PSD matrix $\Ab$, we have
\begin{align*}
\cM\circ\Ab=\EE[\xb\xb^\top\Ab\xb\xb^\top]\preceq\psi\tr(\Hb\Ab)\Hb.
\end{align*}
\end{assumption}
\begin{assumption}[Fourth moment condition, lower bound]\label{assumption:fourth_lower}
We assume that the fourth moment $\cM=\EE[\xb\otimes\xb\otimes\xb\otimes\xb]$ exists and is finite. Furthermore, there exists a scalar $\beta>0$ such that for any PSD matrix $\Ab$, we have
\begin{align*}
\cM\circ\Ab-\Hb\Ab\Hb\succeq\beta\tr(\Hb\Ab)\Hb.
\end{align*}
\end{assumption}

A special case is that the marginal distribution $\cD|_{\xb}$ is a Gaussian distribution.
In this case, the fourth moment operator satisfies $\cM\circ\Ab=\Hb\Ab\Hb+2\tr(\Hb\Ab)\Hb$.
Note that $\Hb\Ab\Hb\preceq\tr(\Hb\Ab)\Hb$,
so we can set $\psi=3$ in Assumption~\ref{assumption:fourth} and $\beta=2$ in Assumption~\ref{assumption:fourth_lower}.

Finally, we present assumptions that characterize the label noise $\xi_t=y_t-\langle\wb_*, \xb_t\rangle$.
The following assumption is a weaker condition used in the proof of the upper bound of the excess risk:

\begin{assumption}[Weak label noise condition]\label{assumption:noise}
The covariance matrix of the stochastic gradient estimated at $\wb_*$, i.e., $\bSigma\coloneqq\EE[\xi^2\xb\xb^\top]$ and the noise level $\sigma^2\coloneqq\|\Hb^{-\frac12}\bSigma\Hb^{-\frac12}\|_2$
both exist and are finite.
\end{assumption}
By Assumption~\ref{assumption:noise}, we have $\bSigma\preceq\sigma^2\Hb$ because
\begin{align}\label{eq:Sigma_bound}
\zero\preceq\Hb^{\frac12}(\sigma^2\Ib-\Hb^{-\frac12}\bSigma\Hb^{-\frac12})\Hb^{\frac12}=\sigma^2\Hb-\bSigma.
\end{align}

We then present the present the stronger assumption used in the proof of the lower bound, which is referred to as the well-specified setting in the literature \citep{zou2021benign}:
\begin{assumption}[Strong label noise condition]\label{assumption:noise_lower}
We assume that the label noise $\xi$ is independent of $\xb$, and $\EE[\xi^2]=\sigma^2$. In other words, $\bSigma=\sigma^2\Hb$.
\end{assumption}

\section{Main Results}

In this section, we present the upper and lower bounds of the excess risk, which is the difference between the risk function evaluated at the output weight vector $\owb_N$ and at the ground truth weight vector $\wb_*$. Before we present the main results, we introduce the shorthand notation of sub-matrices: For any positive integers $k_1\le k_2$,
\begin{align*}
\Hb_{k_1:k_2}&\coloneqq\diag(0, \dots, 0, \lambda_{k_1+1}, \dots, \lambda_{k_2}, 0, \dots),\\
\Hb_{k_2:\infty}&\coloneqq\diag(0, \dots, 0, \lambda_{k_2+1}, \lambda_{k_2+2}, \dots).
\end{align*}


\subsection{Upper and Lower Bounds of Excess Risk}\label{subsection:upper_lower_bounds}

\begin{theorem}[Upper bound]\label{theorem:EMA_excess_risk}
Suppose that Assumptions \ref{assumption:second}, \ref{assumption:fourth} and \ref{assumption:noise} hold, and the hyperparameters satisfy
\begin{align*}
N(1-\alpha)\ge1,\qquad\delta<1/(\psi\tr(\Hb)).
\end{align*}
Then the excess risk satisfies
\begin{align*}
&\EE[L(\owb_N)]-L(\wb_*)\le\textcolor{red}{\mathrm{EffectiveBias}}+\textcolor{blue}{\mathrm{EffectiveVar}},
\end{align*}
where the effective bias satisfies
\begin{gather*}
\textcolor{red}{\mathrm{EffectiveBias}}=\sum_{i=1}^d(\wb_0-\wb_*)_i^2\lambda_i\cdot b_i^2,\text{\quad where\quad}b_i\coloneqq\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)},
\end{gather*}
and the effective variance satisfies
\begin{align*}
\textcolor{blue}{\mathrm{EffectiveVar}}&\le\bigg[k^*(1-\alpha)^2+\delta^2\sum_{i>k^*}\lambda_i^2\bigg]\cdot\frac{\psi(\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\wb_0-\wb_*\|_{\Hb_{k^\dagger:\infty}}^2)}{\delta(1-\psi\delta\tr(\Hb))}\\
&\qquad+\frac{\sigma^2}{1-\psi\delta\tr(\Hb)}\bigg[(1-\alpha)k^*+\delta\sum_{i=k^*+1}^{k^\dagger}\lambda_i+N\delta^2\sum_{i>k^\dagger}\lambda_i^2\bigg],
\end{align*}
where the eigenvalue cutoffs are defined as
\begin{align*}
k^*\coloneqq\max\Big\{i:\lambda_i\ge\frac{1-\alpha}{\delta}\Big\},\qquad k^\dagger\coloneqq\max\Big\{i:\lambda_i\ge\frac1{N\delta}\Big\}.
\end{align*}
\end{theorem}
The proof of Theorem~\ref{theorem:EMA_excess_risk} is given in Appendix~\ref{subsection:proof_EMA_excess_risk}.
Theorem~\ref{theorem:EMA_excess_risk} characterizes the first instance-dependent excess risk bound of SGD with EMA.
The excess risk bound includes the effective bias and the effective variance, both decomposed into each eigen-subspace of $\Hb$.
The effective bias corresponds to the convergence rate of the risk function if GD is applied instead of SGD.
In the eigen-subspace corresponding to $\lambda_i$, the effective bias is $\lambda_i(\wb_0-\wb_*)_i^2$, which is the initial bias error in the eigen-subspace of $\lambda_i$, multiplied by the square of the decay rate $b_i$, which will be discussed in detail in Subsection~\ref{subsection:bi}.
The effective variance stems from the stochastic gradient, including the randomness of both $\xb_t$ and $y_t$.
We will discuss key elements of the effective variance in Subsection \ref{subsection:var}.

We also obtain the lower bound of the excess risk of SGD with EMA:

\begin{theorem}[Lower bound]\label{theorem:EMA_excess_risk_lower}
Suppose that Assumptions \ref{assumption:second}, \ref{assumption:fourth_lower} and \ref{assumption:noise_lower} hold, and the hyperparameters satisfy
\begin{align*}
\delta\le1/\lambda_1,\qquad\alpha^{N-1}\le1/N,\qquad N\ge2.
\end{align*}
The excess risk then satisfies
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)&=(\textcolor{red}{\mathrm{EffectiveBias}}+\textcolor{blue}{\mathrm{EffectiveVar}})/2,
\end{align*}
where the effective bias is identical to that in Theorem~\ref{theorem:EMA_excess_risk}, and the effective variance satisfies
\begin{align*}
&\textcolor{blue}{\mathrm{EffectiveVar}}\ge\frac{\beta e^{-2}\|\wb_0-\wb_*\|_{\Hb_{k^\dagger:\infty}}^2+\sigma^2}2\cdot\bigg[\frac{3\alpha^2(1-\alpha)k^*}{16}+\frac{\delta}{100}\sum_{i=k^*+1}^{k^\dagger}\lambda_i+\frac{N\delta^2}{180}\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{theorem}
The proof of Theorem \ref{theorem:EMA_excess_risk_lower} is presented in Appendix \ref{subsection:proof_EMA_excess_risk_lower}. The lower bound is matching with the upper bound except for the first term of the effective variance, which will be discussed in Subsection \ref{subsection:var}. Although Theorem \ref{theorem:EMA_excess_risk_lower} requires a stronger condition about $N$ and $\alpha$, it is still a mild condition in practice because $\alpha^{N-1}$, which is the weight of $\wb_0$ in \eqref{eq:weighted_sum}, should be smaller than the average weight $1/N$.

\subsection{Discussion of Variance Error}\label{subsection:var}

Both the upper bound and the lower bound of the effective variance contain two terms:
The second term stems from the label noise, which is referred to as the \emph{(real) variance error}. The upper bound and the lower bound are matching for this term up to constant factors.
The first term comes from the randomness of the feature vector, and is thus nonzero even if there is no label noise. The upper and lower bounds are not matching for this term due to the additional term $\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2$ in the upper bound, which is similar to the case of SGD with tail averaging~\citep{zou2021benign}. We conjecture that finer analysis can bridge the gap.


\paragraph{Effective dimensions.}
The cutoffs $k^*$ and $k^\dagger$ are referred to as \textit{effective dimensions}, which can be significantly smaller than the real model dimension $d$, especially when the eigenvalue spectrum decays fast.
Similar quantities also appear in previous works analyzing high-dimensional linear regression~\citep{zou2021benign, wu2022last, li2023risk}, and the double effective dimensions $k^*$ and $k^\dagger$ for SGD with EMA is very similar to that of SGD with tail averaging~\citep{zou2021benign}.
We will draw more connections between SGD with EMA and SGD with tail averaging in Section~\ref{section:compare_averaging}.

We then discuss the influence of hyperparameters $\delta$, $\alpha$, and $N$ on the effective variance bound. The following equalities about the effective variance will be useful in our discussion:
\begin{gather}
k^*(1-\alpha)^2+\delta^2\sum_{i>k^*}\lambda_i^2=\sum_{i=1}^d(\min\{1-\alpha, \delta\lambda_i\})^2;\label{eq:effective_var_1a}\\
\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\wb_0-\wb_*\|^2=\sum_{i=1}^d\lambda_i(\wb_0-\wb_*)_i^2\min\{1, N\delta\lambda_i\};\label{eq:effective_var_1b}\\
(1-\alpha)k^*+\delta\sum_{i=k^*+1}^{k^\dagger}\lambda_i+N\delta^2\sum_{i>k^\dagger}\lambda_i^2=\sum_{i=1}^d\min\{1-\alpha, \delta\lambda_i, N\delta^2\lambda_i^2\}.\label{eq:effective_var_2}
\end{gather}

\paragraph{Learning rate $\delta$.}
In the upper bound of the excess risk (Theorem \ref{theorem:EMA_excess_risk}), we require that $\delta<1/(\psi\tr(\Hb))$ similar to \citet{zou2021benign}, to ensure that $(1-\psi\delta\tr(\Hb))^{-1}$ is positive. Larger learning rates may cause the effect of the fourth moment to accumulate and diverge.

\paragraph{Number of iterations $N$.}
Due to \eqref{eq:effective_var_1b} and \eqref{eq:effective_var_2}, the effective variance increases as $N$ increases. Furthermore, as $N$ goes to infinity, the effective dimension $k^\dagger$ also goes to infinity, while $k^*$ remains unchanged.% The variance error thus becomes
%\begin{align*}
%\mathrm{Variance}=\Theta\bigg(\sigma^2\bigg((1-\alpha)k^*+\delta\sum_{i>k^*}\lambda_i\bigg)\bigg).
%\end{align*}

\paragraph{Averaging parameter $\alpha$.}
Due to \eqref{eq:effective_var_1a} and \eqref{eq:effective_var_2}, the effective decreases as $\alpha$ increases. However, choosing $\alpha$ very close to $1$ does not truly benefit the learning process because the reduced variance error stems partly from the large weight of $\wb_0$ (which has no randomness) in \eqref{eq:weighted_sum}. We will further elaborate this point in the next subsection.

\subsection{Decay Rate of Bias Error}\label{subsection:bi}
We then study the quantity $b_i$ in Theorems~\ref{theorem:EMA_excess_risk} and~\ref{theorem:EMA_excess_risk_lower}, which is the decay rate of the effective bias in the eigen-subspace of $\lambda_i$. We first note that
\begin{align*}
b_i=(1-\delta\lambda_i)^N+(\delta\lambda_i)\sum_{t=0}^{N-1}\alpha^t(1-\delta\lambda_i)^{N-1-t},
\end{align*}
so the smaller $\alpha$ is, the faster $b_i$ decays. Together with the analysis of the effective variance in Subsection \ref{subsection:var}, we conclude that there exists a bias-variance trade-off concerning the choice of $\alpha$: Larger $\alpha$ brings about smaller effective variance, but makes the effective bias decay slower.

The following proposition presents a finer characterization of the decay rate $b_i$:
\begin{proposition}\label{prop:exponential}
For any $i\in[d]$, the exponential decay rate $b_i$ satisfies
\begin{itemize}[leftmargin=*]
\item[1.] When $(1-\delta\lambda_i)/\alpha\le(N-1)/N$, we have
\begin{align*}
b_i=\Theta\bigg(\frac{(\delta\lambda_i)\alpha^N}{\delta\lambda_i-(1-\alpha)}\bigg);
\end{align*}
\item[2.] When $(N-1)/N<(1-\delta\lambda_i)/\alpha\le1$, we have
\begin{align*}
b_i=\Theta(\alpha^N+(1-\alpha)N\alpha^{N-1});
\end{align*}
\item[3.] When $1<(1-\delta\lambda_i)/\alpha\le N/(N-1)$, we have
\begin{align*}
b_i=\Theta((1-\delta\lambda_i)^N+N\delta\lambda_i(1-\delta\lambda_i)^{N-1});
\end{align*}
\item[4.] When $(1-\delta\lambda_i)/\alpha>N/(N-1)$, we have
\begin{align*}
b_i=\Theta\bigg(\frac{(1-\alpha)(1-\delta\lambda_i)^N}{(1-\alpha)-\delta\lambda_i}\bigg).
\end{align*}
\end{itemize}
\end{proposition}

Proposition~\ref{prop:exponential} implies that (i) the effective bias decays exponentially in $N$ within every eigen-subspace of $\Hb$; (ii) the decay rate of the effective bias has a phase transition at the eigen-subspace corresponding to $\lambda_{k^*}$: The decay rate is $\alpha^{2N}$ in the eigen-subspace of large eigenvalues, and is $(1-\delta\lambda_i)^{2N}$ in the eigen-subspace of small eigenvalues, and (iii) when $1-\delta\lambda_i$ is close to $\alpha$, the decay rate of the effective bias contains additional factors polynomial in $N$.

\section{Comparing EMA with Other Averaging Schemes}\label{section:compare_averaging}

In this section, we compare the excess risk of SGD with EMA against SGD without averaging and other averaging schemes, including iterate averaging from the beginning and tail averaging. Similar to EMA, the excess risk of all averaging schemes of interest can be decomposed into effective bias and effective variance~\citep{zou2021benign}. For each averaging scheme, we focus on its comparison with EMA in terms of effective variance (including the effective dimension) and the decay rate of the effective bias, i.e., $b_i$.

\paragraph{Comparison with SGD without averaging.}
SGD without averaging is equivalent to EMA with $\alpha=0$.
Specifically, the effective dimension $k^*$ becomes $0$, and the decay rate of the effective bias is $b_i^{\mathrm{w/o}}=(1-\delta\lambda_i)^{N-1}$.
Based on the discussion about the impact of $\alpha$ on the excess risk bound in Subsections \ref{subsection:var} and \ref{subsection:bi}, we conclude that \emph{SGD with EMA has a smaller effective variance, but its effective variance decays slower than that of SGD without averaging.}

\paragraph{Comparison with iterate averaging.}
\citet{zou2021benign} studies SGD with iterate averaging, which is defined as
$\owb_N^{\mathrm{IA}}\coloneqq N^{-1}\sum_{t=0}^{N-1}\wb_t$.
The variance error of SGD with iterate averaging is
\begin{align*}
&\Theta\bigg(\sigma^2\bigg(\sum_{i=1}^d\min\{1/N, N\delta^2\lambda_i^2\}\bigg)\bigg).
\end{align*}
If $N$ is not too large, i.e., $N\alpha^{N-1}=\Theta(1)$ , the difference between $1/N$ and $1-\alpha$ is only $\mathrm{polylog}(N)$.
In this case, \emph{SGD with EMA achieves a variance error similar to that of SGD with iterate averaging.}
Due to the gap between the upper and lower bounds of SGD with EMA, we leave the comparison of the remaining part of the effective variance for future work.
The decay rate of effective bias of SGD with iterate averaging is 
\begin{align*}
b_i^{\mathrm{IA}}=\frac{1-(1-\delta\lambda_i)^N}{N\delta\lambda_i}=\Theta(\min\{1/(N\delta\lambda_i), 1\}).
\end{align*}
Therefore, \emph{SGD with EMA enjoys the advantage of exponentially decaying effective variance compared with SGD with iterate averaging.}

\paragraph{Comparison with tail averaging.}
\citet{zou2021benign} also studies SGD with tail averaging. In a total of $N$ iterations, averaging is only performed for the last $N-s$ iterates, i.e.,
$\owb_{s:N}^{\mathrm{TA}}\coloneqq(N-s)^{-1}\sum_{t=s}^{N-1}\wb_t$.
Similar to the case in Subsection \ref{subsection:upper_lower_bounds}, the upper and lower bounds of the excess risk of SGD with tail averaging are not matching in \citet{zou2021benign}, so we focus on the comparison of the effective dimension and the real variance error in the effective variance. According to \citet{zou2021benign}, the effective dimensions of SGD with tail averaging are
\begin{align*}
k^*_{\mathrm{TA}}=\max\{i:\lambda_i\ge1/((N-s)\delta)\},\qquad k^\dagger_{\mathrm{TA}}=\max\{i:\lambda_i\ge1/(N\delta)\}.
\end{align*}
We thus observe that \emph{$k^\dagger_{\mathrm{TA}}$ is exactly the same as $k^\dagger$ in SGD with EMA, and $k^*_{\mathrm{TA}}=k^*$ under the condition $(1-\alpha)(N-s)=1$.} Furthermore, the real variance of SGD with tail averaging is
\begin{align*}
\mathrm{Variance}=\Theta\bigg(\sum_{i=1}^d\min\Big\{\frac1{N-s}, \delta\lambda_i, N\delta^2\lambda_i^2\Big\}\bigg),
\end{align*}
which also \emph{matches that of SGD with EMA if $(1-\alpha)(N-s)=1$.} For the decay rate of the effective bias, we have
\begin{align*}
b_i^{\mathrm{TA}}=\frac{(1-\delta\lambda_i)^s-(1-\delta\lambda_i)^N}{(N-s)\delta\lambda_i}.
\end{align*}
We then compare $b_i$ with $b_i^{\mathrm{TA}}$ under the condition $(1-\alpha)(N-s)=1$. When $\alpha\ge1/2$ (which is a mild condition in practice), we have $\log\alpha\ge(\alpha-1)/2$, and
\begin{align*}
1/\sqrt e=e^{(\alpha-1)(N-s)/2}\le e^{(N-s)\log\alpha}=\alpha^{N-s}.
\end{align*}
We thus have
\begin{align*}
b_i&=(1-\alpha)\sum_{t=s}^{N-1}\alpha^{N-1-t}(1-\delta\lambda_i)^t+\alpha^{N-s}\frac{(\delta\lambda_i)\alpha^s-(1-\alpha)(1-\delta\lambda_i)^s}{\delta\lambda_i-(1-\alpha)}\\
&\ge\frac{1-\alpha}{\sqrt e}\sum_{t=s}^{N-1}(1-\delta\lambda_i)^t=\frac{(1-\delta\lambda_i)^s-(1-\delta\lambda_i)^N}{\sqrt e(N-s)\delta\lambda_i},
\end{align*}
where the inequality holds due to a dropped positive term and $\alpha^{N-s}\ge 1/\sqrt e$.
Therefore, the exponential decay rate of SGD with EMA $b_i$ is $\Omega(b_i^\mathrm{TA})$. However, $b_i$ is exponential in $N$ while $b_i^{\mathrm{TA}}$ is exponential only in $s$, which means that \emph{SGD with EMA has the advantage that the effective bias in every eigen-subspace decays exponentially fast in $N$ compared with polynomial decay in $N$ for SGD with tail averaging if $s$ is fixed before training.}

\section{Extension to Mini-Batch SGD}

We now extend our analysis of SGD with EMA to mini-batch SGD. Let $B$ be the batch size, and $\{(\xb_{t, i}, y_{t, i})\}_{i=1}^B$ be the mini-batch sampled from the distribution $\cD$ at iteration $t$. An iterate of mini-batch SGD is
\begin{align*}
\wb_t^{\mathrm{MB}}=\wb_{t-1}^{\mathrm{MB}}+\frac{\delta}{B}\sum_{i=1}^B(y_{i, t}-\langle\wb_{t-1}^{\mathrm{MB}}, \xb_{t, i}\rangle)\xb_{t, i}.
\end{align*}
We then consider the excess risk of the exponential moving average of the mini-batch SGD iterates, defined as
\begin{align*}
\owb_N^{\mathrm{MB}}=\alpha^N\wb_0^{\mathrm{MB}}+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{N-1-t}\wb_t^{\mathrm{MB}}.
\end{align*}

\begin{theorem}\label{theorem:minibatch}
Suppose that Assumptions \ref{assumption:second}, \ref{assumption:fourth}, and \ref{assumption:noise} hold, and the learning rate satisfies $\delta<\min\{B/(2\psi\tr(\Hb)), 1/\|\Hb\|_2\}$. Then the excess risk of mini-batch SGD satisfies
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)&\le\textcolor{red}{\mathrm{EffectiveBias}}+\textcolor{blue}{\mathrm{EffectiveVar}},
\end{align*}
where the effective bias is identical to that in Theorem~\ref{theorem:EMA_excess_risk}, and the excess variance satisfies
\begin{align*}
&\textcolor{blue}{\mathrm{EffectiveVar}}\le\bigg[k^*(1-\alpha)^2+\delta^2\sum_{i>k^*}\lambda_i^2\bigg]\cdot\frac{2\psi(\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\wb_0-\wb_*\|_{\Hb_{k^\dagger:\infty}}^2)}{\delta B}\\
&\qquad+\frac{2\sigma^2}{B}\bigg[(1-\alpha)k^*+\delta\sum_{i=k^*+1}^{k^\dagger}\lambda_i+N\delta^2\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{theorem}
Appendix~\ref{subsection:proof_minibatch} shows the proof of Theorem~\ref{theorem:minibatch}.
%The proof of Theorem~\ref{theorem:EMA_excess_risk} is given in Appendix~\ref{subsection:proof_EMA_excess_risk}.
The lower bound can be proved similar to Theorem \ref{theorem:EMA_excess_risk_lower}.

Based on Theorem \ref{theorem:minibatch}, we aim to derive the critical batch size~\citep{zhang2024does}, which is the batch size that causes a phase transition on the excess risk bound. Since the effective variance decays exponentially in $N$, we present the following corollary for only the effective variance:
\begin{corollary}\label{corollary:special_spectrum}
Suppose the eigenvalue spectrum satisfies $\lambda_i=i^{-a}$, and the initialization satisfies $\lambda_i(\wb_0-\wb_*)_i^2=i^{-b}$ where $b<a+1$. Let $M$ be the number of examples. Then under the same assumptions as Theorem \ref{theorem:minibatch}, we have
\begin{align*}
&\textcolor{blue}{\mathrm{EffectiveVar}}=\Theta(B^{-1}\delta^{1/a}(1-\alpha)^{1-1/a})+\Theta(B^{-1}\delta^{(2-b)/a}(1-\alpha)^{2-1/a}N^{1-(b-1)/a}).
\end{align*}
\end{corollary}
The assumption of the eigenvalue spectrum and the initialization is referred to as the source condition \citep{caponnetto2007optimal, zhang2024does}.
The assumption of $b<a+1$ ensures that upper bound and the lower bounds are matching. If we further let $N=M/B$ where $M$ is the total number of samples, then the critical batch size is
$B^*=\cO(M\delta^{\frac{1-b}{a-b+1}}(1-\alpha)^{\frac{a}{a-b+1}})$.
We observe that the critical batch size of SGD with EMA is sharply different from SGD with iterate averaging in \citet{zhang2024does}. This is because the critical batch size is determined by both the effective bias and the effective variance for SGD with iterate averaging due to the effective bias that decays only polynomially in $N$. However, the effective bias of SGD with EMA decays exponentially in $N$, making it negligible in the analysis of the critical batch size.


\section{Overview of Proof Techniques}

In this section, we present the proof technique that is not only used in our analysis of EMA, but also applicable to a class of averaging schemes.

We first introduce the class of averaging schemes that covers EMA and iterate averaging, among others. In \eqref{eq:def_EMA}, instead of using a uniform $\alpha$ in all iterates, we allow the averaging parameter to depend on $t$, i.e.,
\begin{align*}
\owb_0=\wb_0;\qquad\owb_t=\textcolor{red}{\alpha_{t-1}}\owb_{t-1}+\textcolor{red}{(1-\alpha_{t-1})}\wb_{t-1}.
\end{align*}
where $\alpha_t\in[0, 1]$ is the time-dependent averaging parameter. The final output can be written as
\begin{align*}
\owb_N=\textcolor{red}{\beta_0}\wb_0+\sum_{t=0}^{N-1}\textcolor{red}{(\beta_{t+1}-\beta_t)}\wb_t,
\end{align*}
where $\beta_t$ is defined as $\beta_t=\prod_{k=t}^{N-1}\alpha_t$. Most averaging schemes belong to this class, e.g.,
\begin{shaded}
\begin{itemize}[leftmargin=*]
\item EMA: $\alpha_t=\alpha$, and $\beta_t=\alpha^{N-t}$.
\item SGD without averaging: $\alpha_t=0$, $\beta_N=1$, and $\beta_t=0$ for all $t=0, \dots, N-1$.
\item Iterate averaging: $\alpha_t=t/(t+1)$, and $\beta_t=t/N$.
\item Tail averaging:
\begin{align*}
\alpha_t=\begin{cases}
0 & t<s,\\
\frac{t-s}{t-s+1} & t\ge s;
\end{cases},\qquad\beta_t=\begin{cases}
0 & t<s,\\
\frac{t-s}{N-s} & t\ge s.
\end{cases}
\end{align*}
\end{itemize}
\end{shaded}

We now define several notations following~\citet{zou2021benign}. We first define the centered SGD iterate as $\bmeta_t=\wb_t-\wb_*$, and the EMA of the centered SGD iterates is $\obmeta_N=\owb_N-\wb_*$. We define the centered bias and variance vectors recursively as
\begin{gather*}
\bmeta_0^{\mathrm{bias}}=\bmeta_0,\quad\bmeta_t^{\mathrm{bias}}=(\Ib-\delta\xb_t\xb_t^\top)\bmeta_{t-1}^{\mathrm{bias}};\\
\bmeta_0^{\mathrm{var}}=\zero,\quad\bmeta_t^{\mathrm{var}}=(\Ib-\delta\xb_t\xb_t^\top)\bmeta_{t-1}^{\mathrm{var}}+\delta\xi_t\xb_t.
\end{gather*}
We can define the EMA of the centered vectors $\obmeta_N$, $\obmeta_N^{\mathrm{bias}}$, and $\obmeta_N^{\mathrm{var}}$ similar to the definition of $\owb_N$ in \eqref{eq:weighted_sum}. Following previous works \citep{defossez2015averaged, dieulevuet2017harder, jain2018parallelizing, berthier2020tight, zou2021benign, wu2022last, lin2024scaling, li2023risk}, under Assumption \ref{assumption:noise}, the excess risk can be decomposed as (See Lemma \ref{lemma:bias_variance_decomp} for details)
$\EE[L(\owb_N)]-L(\wb_*)\le\mathrm{bias}+\mathrm{var}$,
where the bias and variance errors are defined as
\begin{align}\label{eq:def_bias_var}
\mathrm{bias}=\langle\Hb, \EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\rangle,\quad\mathrm{var}=\langle\Hb, \EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}]\rangle.
\end{align}
Since $\obmeta_N^{\mathrm{bias}}$ and $\obmeta_N^{\mathrm{var}}$ are the weighted sums of $\bmeta_t^{\mathrm{bias}}$ and $\bmeta_t^{\mathrm{var}}$, respectively,
in order to bound $\mathrm{bias}$ and $\mathrm{var}$ which depends on the covariance matrix of $\obmeta_N^{\mathrm{bias}}$ and $\obmeta_N^{\mathrm{var}}$, it suffices to (i) study terms of the form $\EE[\bmeta_t^{\mathrm{bias}}\otimes\bmeta_k^{\mathrm{bias}}]$ and $\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_k^{\mathrm{var}}]$, and (ii) represent the bias and variance errors in a tractable form. For Step (i), following \citet{zou2021benign}, we define the covariance matrices as
$\Bb_t=\EE[\bmeta_t^{\mathrm{bias}}\otimes\bmeta_t^{\mathrm{bias}}]$ and $\Cb_t=\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_t^{\mathrm{var}}]$.
With these definitions, for $k\ge t$, we have
$\EE[\bmeta_t^{\mathrm{bias}}\otimes\bmeta_k^{\mathrm{bias}}]=\Bb_t(\Ib-\delta\Hb)^{k-t}$ and $\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_k^{\mathrm{var}}]=\Cb_t(\Ib-\delta\Hb)^{k-t}$.
We are now ready to represent $\EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]$ using $\Bb_t$:
\begin{align}
&\EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]=\beta_0^2\Bb_0+\sum_{t=0}^{N-1}\beta_0(\beta_t-\beta_{t+1})[(\Ib-\delta\Hb)^t\Bb_0+\Bb_0(\Ib-\delta\Hb)^t]\nonumber\\
&\qquad+\sum_{t=0}^{N-1}(\beta_t-\beta_{t+1})\bigg[(\beta_t-\beta_{t+1})\Bb_t+\sum_{k=t+1}^{N-1}(\beta_k-\beta_{k+1})[(\Ib-\delta\Hb)^{k-t}\Bb_t+\Bb_t(\Ib-\delta\Hb)^{k-t}]\bigg].\label{eq:cov_ver1}
\end{align}
For Step (ii), the analysis in \citet{zou2021benign, wu2022last} that adds the terms $\Bb_t$ and transforms \eqref{eq:cov_ver1} into a ``triangular'' sum does not work due to the \emph{inhomogeneous $\beta_t-\beta_{t+1}$}. To tackle this issue, we make the critical observation that
\begin{align*}
&(\beta_t-\beta_{t+1})\bigg[(\beta_t-\beta_{t+1})\Bb_t+\sum_{k=t+1}^{N-1}(\beta_k-\beta_{k+1})[(\Ib-\delta\Hb)^{k-t}\Bb_t+\Bb_t(\Ib-\delta\Hb)^{k-t}]\bigg]\\
&=\bigg[\sum_{k=t}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t}\bigg]\cdot\Bb_t\cdot\bigg[\sum_{k=t}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t}\bigg]\\
&\qquad-\bigg[\sum_{k=t+1}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t-1}\bigg]\cdot(\tilde\cB\circ\Bb_t)\cdot\bigg[\sum_{k=t+1}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t-1}\bigg],
\end{align*}
where the matrix operator $\tilde\cB$ is defined as $\tilde\cB=(\Ib-\delta\Hb)\otimes(\Ib-\delta\Hb)$.
Similar properties were first used in \citet{li2023risk} to study the generalization of SGD with Nesterov momentum. Using this property, by applying the telescope sum, \eqref{eq:cov_ver1} can be reformulated as
\begin{align}
&\EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]=\bigg[\beta_0\Ib+\sum_{k=0}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^k\bigg]\cdot\Bb_0\cdot\bigg[\beta_0\Ib+\sum_{k=0}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^k\bigg]\nonumber\\
&\qquad+\sum_{t=1}^{N-1}\bigg[\sum_{k=t}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t}\bigg]\cdot(\Bb_t-\tilde\cB\circ\Bb_{t-1})\cdot\bigg[\sum_{k=t}^{N-1}(\beta_k-\beta_{k+1})(\Ib-\delta\Hb)^{k-t}\bigg],\label{eq:cov_ver2}
\end{align}
where the first term corresponds to the effective bias, and the second term contributes to the effective variance.
A similar reformulation can also be applied to the variance error. Further simplifications are possible due to the fact that $\Cb_0=\zero$, so the variance term corresponding to the first term in \eqref{eq:cov_ver2} is zero. Afterwards, $\Bb_t$ and $\Cb_t$ can be further characterized by the analysis similar to \citet{zou2021benign}.


\begin{figure}[ht]
\centering
\subfigure[Bias error]{\includegraphics*[width=0.45\linewidth]{figs/compare_avg_bias.pdf}}
%\caption{Bias error of SGD with different averaging schemes.}
\subfigure[Variance error]{\includegraphics*[width=0.45\linewidth]{figs/compare_avg_var.pdf}}
\caption{Comparison of SGD with different averaging schemes. The bias error of SGD with EMA is more stable than SGD without averaging, and decays faster than iterate averaging and tail averaging when $N$ is large. The variance error of SGD with EMA remains relatively small, and is comparable to that of SGD with iterate averaging or tail averaging.}
\label{fig:avg}
%\caption{Variance error of SGD with different averaging schemes.}
\end{figure}
\begin{figure}[ht]
\centering
\subfigure[Bias error]{\includegraphics*[width=0.45\linewidth]{figs/compare_alpha_bias.pdf}}
%\caption{Bias error of SGD with different averaging schemes.}
\subfigure[Variance error]{\includegraphics*[width=0.45\linewidth]{figs/compare_alpha_var.pdf}}
\caption{Comparison of SGD with EMA with different $\alpha$. The bias error of SGD with EMA with smaller alpha decays faster at the beginning of training, but the advantage is less significant when $N$ is large. The variance error of SGD with EMA decreases as $\alpha$ increases.}
\label{fig:alpha}
%\caption{Variance error of SGD with different averaging schemes.}
\end{figure}

\section{Experiments}

In this section, we verify our theoretical findings with empirical experiments.
We (i) compare the generalization performance of SGD with different schemes, and (ii) explore the impact of the choice of the averaging parameter $\alpha$ on the excess risk of SGD with EMA.
We consider the well specified setting (Assumption~\ref{assumption:noise_lower}) with $\sigma^2=1$. The data feature vectors follow the Gaussian distribution $\xb_t\sim\cN(\zero, \Hb)$ where the eigenvalue spectrum of $\Hb$ is $\lambda_i=i^{-2}$ with $d=2000$, which is also the experiment setting in \citet{zou2021benign, wu2022last, li2023risk}. The centered model weight vector is initialized as a Gaussian random vector $\wb_0-\wb_*\sim\cN(\zero, \Ib)$. According to Theorem \ref{theorem:EMA_excess_risk}, the learning rate $\delta$ should satisfy $\delta<1/(\psi\tr(\Hb))=2/\pi^2\approx0.203$, so we choose $\delta=0.2$. The total number of iterations is fixed as $N=3000$. In all experiments, we record both the bias error and the variance error as defined in \eqref{eq:def_bias_var}.


\paragraph{Comparison of different averaging schemes.}
In the comparison of EMA with other averaging schemes, the averaging parameter of EMA is $\alpha=0.995$, and $s=1000$ in tail averaging. The comparisons of the bias error and the variance error are shown in Figures \ref{fig:avg}(a) and \ref{fig:avg}(b), respectively. Although the bias error of SGD with EMA decays slowly at the beginning, it achieves a fast decay rate similar to that of SGD without averaging. However, the bias error of SGD with EMA is far more stable than without averaging, due to the reduced variance of the data feature. The variance error of SGD with EMA remains at a low level though slightly larger than SGD with iterate averaging or tail averaging (because $(1-\alpha)(N-s)\gg1$). We also conclude that averaging in general is crucial in variance reduction due to the observation that the variance error of SGD with tail averaging decays sharply when averaging starts at $t=1000$.

\paragraph{Comparison of SGD with EMA with different $\alpha$.}
We compare SGD with EMA with $\alpha=0.9$, $0.99$ and $0.999$, and the experiments results are the average of 10 independent runs.
The variance error (Figure \ref{fig:alpha}(a)) of SGD with EMA with larger $\alpha$ is significantly smaller than that with smaller $\alpha$, and the bias error (Figure \ref{fig:alpha}(b)) is also more stable.
The bias error of SGD with EMA when $\alpha=0.9$ or $0.99$ decays much faster than when $\alpha=0.999$, but they all approach a similar level when $N=3000$.
We conjecture that this is because the decay rate of the bias error is dominated by the slowest decaying component, which is the bias error in the eigen-subspaces of the smallest eigenvalues. As we have pointed out in Proposition \ref{prop:exponential}, the exponential decay rate of the bias error in such eigen-subspaces is irrelevant to $\alpha$.

\section{Conclusion}

In this work, we study the generalization of SGD with EMA in the high-dimensional linear regression setting.
Our excess risk bound of SGD with EMA depends solely on the eigenvalue spectrum, which is instance-dependent and dimension-free.
Similar results can also be derived for mini-batch SGD.
In a comparison with SGD with other averaging schemes, we reveal the two-fold advantage of SGD with EMA: the exponentially decaying effective bias error and the modest effective variance error.
Our analysis provides the framework for the study of a class of averaging schemes we proposed.

\appendix
\hypersetup{linkcolor=black ,urlcolor=black}
\onecolumn
\renewcommand{\appendixpagename}{\centering \LARGE Appendix}
\appendixpage
%\begingroup % start a TeX group
%\color{blue}% or whatever color you wish to use

\startcontents[section]
\printcontents[section]{l}{1}{\setcounter{tocdepth}{2}}
%\endgroup   % end of TeX group
% \onecolumn
\hypersetup{linkcolor=linkcolor,urlcolor=black}
\vspace{20ex}


% \tableofcontents
\newpage

\section{Additional Notations}


\paragraph{Linear Operators on Matrices.}
We define the following linear operators on matrix following \citet{zou2021benign}:
\begin{gather*}
\cI=\Ib\otimes\Ib,\qquad\cM=\EE[\xb\otimes\xb\otimes\xb\otimes\xb],\qquad\tilde\cM=\Hb\otimes\Hb,\\
\cB=\EE[(\Ib-\delta\xb\xb^\top)\otimes(\Ib-\delta\xb\xb^\top)],\qquad\tilde\cB=(\Ib-\delta\Hb)\otimes(\Ib-\delta\Hb)
\end{gather*}
Denote the $\sigma$-algebra generated by samples $\{(\xb_k, y_k)\}_{k=1}^t$ as $\cF_t$.
Due to the optimality of $\wb_*$, we have $\nabla L(\wb_*)=\zero$, which implies that
\begin{align}\label{eq:noise_zero_mean}
\zero=\nabla L(\wb_*)=\EE[\xb(\xb^\top\wb_*-y)]=\Hb\wb_*-\EE[\xb\cdot y].
\end{align}
Due to the equality above, we have
\begin{align*}
\EE[\bmeta_t^{\mathrm{bias}}|\cF_{t-1}]=(\Ib-\delta\Hb)\bmeta_{t-1}^{\mathrm{bias}},\qquad\EE[\bmeta_t^{\mathrm{var}}|\cF_{t-1}]=(\Ib-\delta\Hb)\bmeta_{t-1}^{\mathrm{var}}.
\end{align*}
Iterating this property, using the double expectation formula, we have for any $k\le t$, we have
\begin{align}\label{eq:iterate_first_moment}
\EE[\bmeta_t^{\mathrm{bias}}|\cF_k]=(\Ib-\delta\Hb)^{t-k}\bmeta_k^{\mathrm{bias}},\qquad\EE[\bmeta_t^{\mathrm{var}}|\cF_k]=(\Ib-\delta\Hb)^{t-k}\bmeta_{k}^{\mathrm{var}},
\end{align}
which indicates that $\EE[\bmeta_t^{\mathrm{var}}]=\zero$.
We also have
\begin{align}
\Bb_t&=\EE[\EE[\bmeta_t^{\mathrm{bias}}\otimes\bmeta_t^{\mathrm{bias}}|\cF_{t-1}]]\nonumber\\
&=\EE[\EE[((\Ib-\delta\xb_t\xb_t^\top)\otimes(\Ib-\delta\xb_t\xb_t^\top))\cdot(\bmeta_{t-1}^{\mathrm{bias}}\otimes\bmeta_{t-1}^{\mathrm{bias}})|\cF_{t-1}]]\nonumber\\
&=\EE[\cB\circ(\bmeta_{t-1}^{\mathrm{bias}}\otimes\bmeta_{t-1}^{\mathrm{bias}})]\nonumber\\
&=\cB\circ\Bb_{t-1},\label{eq:iterate_Bt}
\end{align}
and
\begin{align}
\Cb_t&=\EE[\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_t^{\mathrm{var}}|\cF_{t-1}]]\nonumber\\
&=\EE\Big[\EE[((\Ib-\delta\xb_t\xb_t^\top)\otimes(\Ib-\delta\xb_t\xb_t^\top))\cdot(\bmeta_{t-1}^{\mathrm{var}}\otimes\bmeta_{t-1}^{\mathrm{var}})+\delta^2\xi_t^2\xb_t\xb_t^\top\nonumber\\
&\qquad-\delta\xi_t\xb_t(\bmeta_{t-1}^{\mathrm{var}})^\top(\Ib-\delta\xb_t\xb_t^\top)-\delta\xi_t(\Ib-\delta\xb_t\xb_t^\top)\bmeta_{t-1}^{\mathrm{var}}\xb_t^\top|\cF_{t-1}]\Big]\nonumber\\
&=\cB\circ\Cb_{t-1}+\delta^2\bSigma,
\end{align}
where the last equality holds because $(\xb_t, y_t)$ is independent from $\bmeta_{t-1}^{\mathrm{var}}$ and $\EE[\bmeta_{t-1}^{\mathrm{var}}]=\zero$.

Several other key properties of the centered iterates and the linear operators are given in Appendix~\ref{section:linear_operator_property}.

\section{Proof of Main Results}\label{section:proof_main}

\subsection{Proof of Theorem~\ref{theorem:EMA_excess_risk}}\label{subsection:proof_EMA_excess_risk}
To prove Theorem~\ref{theorem:EMA_excess_risk}, we first decompose the excess risk into the bias error and the variance error (Lemma~\ref{lemma:bias_variance_decomp}), and then bound them separately (Lemma~\ref{lemma:variance_upper_bound} and Lemma~\ref{lemma:bias_upper_bound}).
\begin{lemma}\label{lemma:bias_variance_decomp}
The excess risk can be decomposed as
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)\le\mathrm{bias}+\mathrm{var},
\end{align*}
where
\begin{align*}
\mathrm{bias}=\langle\Hb, \EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\rangle,\qquad\mathrm{var}=\langle\Hb, \EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}]\rangle.
\end{align*}
\end{lemma}

\begin{lemma}\label{lemma:variance_upper_bound}
Under Assumption~\ref{assumption:fourth}, the variance error satisfies
\begin{align*}
\mathrm{var}\le\frac{\sigma^2}{1-\psi\delta\tr(\Hb)}\bigg[(1-\alpha)k^*+\delta\sum_{i=k^*}^{k^\dagger}\lambda_i+N\delta^2\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{lemma}

\begin{lemma}\label{lemma:bias_upper_bound}
Under Assumption~\ref{assumption:fourth}, the bias error satisfies
\begin{align*}
\mathrm{bias}&\le\frac{\psi(\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\wb_0-\wb_*\|_{\Hb_{k^\dagger:\infty}}^2)}{\delta(1-\psi\delta\tr(\Hb))}\bigg(k^*(1-\alpha)^2+\delta^2\sum_{i>k^*}\lambda_i^2\bigg)\\
&\qquad+\sum_{i=1}^n(\wb_0-\wb_*)_i^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2.
\end{align*}
\end{lemma}

\subsection{Proof of Theorem~\ref{theorem:EMA_excess_risk_lower}}\label{subsection:proof_EMA_excess_risk_lower}

The lower bound can be proved using the bias-variance decomposition similar to proof of the upper bound.

\begin{lemma}\label{lemma:bias_variance_decomp_lower}
Under Assumption~\ref{assumption:noise_lower}, the excess risk can be decomposed as
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)=\frac12(\mathrm{bias}+\mathrm{var}).
\end{align*}
\end{lemma}

\begin{lemma}\label{lemma:variance_lower_bound}
Assume that the hyperparameters satisfy $\delta\le1/\lambda_i$, $N\ge2$ and $\alpha^{N-1}\le1/N$. Then the variance error satisfies
\begin{align*}
\mathrm{var}\ge\sigma^2\bigg[\frac{3\alpha^2(1-\alpha)k^*}{16}+\frac{\delta}{100}\sum_{i=k^*+1}^{k^\dagger}\lambda_i+\frac{N\delta^2}{180}\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{lemma}

\begin{lemma}\label{lemma:bias_lower_bound}
Under the same assumptions as Lemma~\ref{lemma:variance_lower_bound}, the bias error satisfies
\begin{align*}
\mathrm{bias}&\ge\beta e^{-2}\|\bmeta_0\|_{\Hb_{k^\dagger:\infty}}^2\bigg[\frac{3\alpha^2(1-\alpha)k^*}{16}+\frac{\delta}{100}\sum_{i=k^*+1}^{k^\dagger}\lambda_i+\frac{N\delta^2}{180}\sum_{i>k^\dagger}\lambda_i^2\bigg]\\
&\qquad+\sum_{i=1}^d\eta_{0, i}^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2.
\end{align*}
\end{lemma}

The proofs of Lemma~\ref{lemma:bias_variance_decomp} and Lemma~\ref{lemma:bias_variance_decomp_lower} are given in Appendix~\ref{subsection:bias_variance_decomp}. The proofs of Lemma~\ref{lemma:variance_upper_bound} and Lemma~\ref{lemma:variance_lower_bound} are given in Appendix~\ref{subsection:variance}. The proofs of Lemma~\ref{lemma:bias_upper_bound} and Lemma~\ref{lemma:bias_lower_bound} are given in Appendix~\ref{subsection:bias}.

\subsection{Proof of Theorem~\ref{theorem:minibatch}}\label{subsection:proof_minibatch}

In this subsection, we modify the proof of Theorem~\ref{theorem:EMA_excess_risk} to derive the excess risk upper bound for mini-batch SGD.
\begin{proof}[Proof of Theorem~\ref{theorem:minibatch}]
Define the residual vector of mini-batch SGD in the same way as SGD. We then define the bias and variance residual vectors as
\begin{gather*}
\bmeta_0^{\mathrm{bias}}=\bmeta_0,\qquad\bmeta_t^{\mathrm{bias}}=\bigg(\Ib-\frac\delta B\sum_{i=1}^B\xb_{t, i}\xb_{t, i}^\top\bigg)\bmeta_{t-1}^{\mathrm{bias}};\\
\bmeta_0^{\mathrm{var}}=\zero,\qquad\bmeta_t^{\mathrm{var}}=\bigg(\Ib-\frac\delta B\sum_{i=1}^B\xb_{t, i}\xb_{t, i}^\top\bigg)\bmeta_{t-1}^{\mathrm{var}}+\frac\delta B\sum_{i=1}^B\xi_{t, i}\xb_{t, i}.
\end{gather*}
We define the exponential moving average of the bias and variance residual vectors as well as the second moment matrices $\Bb_t$ and $\Cb_t$ in the same way as SGD. We then have the bias-variance decomposition lemma similar to Lemma~\ref{lemma:bias_variance_decomp}.

We define all linear matrix operators in the same way as SGD except for $\cB$, which is defined as
\begin{align*}
\cB\coloneqq\EE\bigg[\bigg(\Ib-\frac\delta B\sum_{i=1}^B\xb_{t, i}\xb_{t, i}^\top\bigg)\otimes\bigg(\Ib-\frac\delta B\sum_{i=1}^B\xb_{t, i}\xb_{t, i}^\top\bigg)\bigg],
\end{align*}
then $\Bb_t$ and $\Cb_t$ satisfy the following recursive formulas:
\begin{align*}
\Bb_{t+1}=\cB\circ\Bb_t,\qquad\Cb_{t+1}=\cB\circ\Cb_t+\frac{\delta^2}{B}\bSigma.
\end{align*}
We also note that $\cB-\tilde\cB=\delta^2/B\cdot(\cM-\tilde\cM)$ is still a PSD operator, and for any PSD matrix $\Ab$, we have
\begin{align*}
(\cB-\tilde\cB)\circ\Ab\preceq\frac{\psi\delta^2}{B}\tr(\Hb\Ab)\Hb.
\end{align*}
Therefore, we can substitute the parameters in Theorem~\ref{theorem:EMA_excess_risk} as $\sigma^2\gets\sigma^2/B$ and $\psi\gets\psi/B$, and obtain the upper bound for the excess risk of mini-batch SGD.
\end{proof}

\section{Discussion about Decay Rate of Bias Error}
In this section, we study the term
\begin{align*}
b_i&=\alpha^N+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(1-\delta\lambda_i)^k\\
&=\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\\
&=(1-\delta\lambda_i)^N+(\delta\lambda_i)\sum_{k=0}^{N-1}\alpha^{N-1-k}(1-\delta\lambda_i)^k.
\end{align*}
To upper bound $b_i$, when $i\le k^*$, i.e., $1-\delta\lambda_i\le\alpha$, we have
\begin{align*}
b_i&=\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\le\frac{\delta\lambda_i}{\delta\lambda_i-(1-\alpha)}\alpha^N,
\end{align*}
where the inequality holds because $(1-\alpha)(1-\delta\lambda_i)^N\ge0$. We also have
\begin{align*}
b_i&=\alpha^N+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(1-\delta\lambda_i)^k\le\alpha^N+(1-\alpha)\sum_{k=0}^N\alpha^{N-1-k}\alpha^k=\alpha^N+N(1-\alpha)\alpha^{N-1},
\end{align*}
where the inequality holds because $1-\delta\lambda_i\le\alpha$.

When $i>k^*$, i.e., $1-\delta\lambda_i>\alpha$, we have
\begin{align*}
b_i&=(1-\delta\lambda_i)^N+(\delta\lambda_i)\sum_{k=0}^{N-1}\alpha^{N-1-k}\cdot(1-\delta\lambda_i)^k\\
&\le(1-\delta\lambda_i)^N+(\delta\lambda_i)\sum_{k=0}^{N-1}(1-\delta\lambda_i)^{N-1-k}\cdot(1-\delta\lambda_i)^k\\
&=(1-\delta\lambda_i)^N+N\delta\lambda_i(1-\delta\lambda_i)^{N-1},
\end{align*}
where the inequality holds because $\alpha\le1-\delta\lambda_i$. We also have
\begin{align*}
b_i&=\frac{(1-\alpha)(1-\delta\lambda_i)^N-(\delta\lambda_i)\alpha^N}{1-\alpha-\delta\lambda_i}\le\frac{1-\alpha}{1-\alpha-\delta\lambda_i}(1-\delta\lambda_i)^N,
\end{align*}
where the inequality holds because $(\delta\lambda_i)\alpha^N\ge0$.


To lower bound $b_i$, we consider the following cases:

\paragraph{Case 1.}
When $(1-\delta\lambda_i)/\alpha\le1-1/N$, we have
\begin{align*}
b_i&=\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\ge\frac{\delta\lambda_i(\alpha^N-(1-\delta\lambda_i)^N)}{\delta\lambda_i-(1-\alpha)}\\
&\ge\frac{(\delta\lambda_i)(1-(1-1/N)^N)}{\delta\lambda_i-(1-\alpha)}\alpha^N\ge\frac{(1-e^{-1})\delta\lambda_i}{\delta\lambda_i-(1-\alpha)}\alpha^N,
\end{align*}
where the first inequality holds because $1-\alpha\le\delta\lambda_i$, the second inequality holds because $1-\delta\lambda_i/\alpha\le1-1/N$, and the last inequality holds because $(1-1/N)^N\le1/e$.

\paragraph{Case 2.}
When $1-1/N<(1-\delta\lambda_i)/\alpha\le1$, we have
\begin{align*}
b_i&\ge\alpha^N+(1-\alpha)\sum_{k=0}^{N-1}\alpha^N(1-1/N)^k=\alpha^N+(1-\alpha)\alpha^{N-1}\cdot N(1-(1-1/N)^N)\\
&\ge\alpha^N+(1-e^{-1})(1-\alpha)N\alpha^{N-1},
\end{align*}
where the first inequality holds because $1-\delta\lambda_i\ge(1-1/N)\alpha$, and the second inequality holds because $(1-1/N)^N\le1/e$.

\paragraph{Case 3.}
When $1<(1-\delta\lambda_i)/\alpha\le N/(N-1)$, similar to Case 2, we have
\begin{align*}
b_i&\ge(1-\delta\lambda_i)^N(1-e^{-1})N\delta\lambda_i(1-\delta\lambda_i)^{N-1}.
\end{align*}

\paragraph{Case 4.}
When $(1-\delta\lambda_i)/\alpha>N/(N-1)$, similar to Case 1, we have
\begin{align*}
b_i&\ge\frac{(1-e^{-1})(1-\alpha)}{1-\alpha-\delta\lambda_i}(1-\delta\lambda_i)^N.
\end{align*}

\section{Proof of Lemmas in Appendix~\ref{section:proof_main}}\label{section:Proof_proof_EMA_excess_risk}

\subsection{Bias-Variance Decomposition}\label{subsection:bias_variance_decomp}

In this subsection, we prove Lemma~\ref{lemma:bias_variance_decomp} and Lemma~\ref{lemma:bias_variance_decomp_lower}. The proof is similar to \citet{zou2021benign}, and is presented here for completeness.
\begin{proof}[Proof of Lemma~\ref{lemma:bias_variance_decomp}]
By Lemma~\ref{lemma:excess_risk_equivalence}, the excess risk can be written as
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)&=\frac12\langle\Hb, \EE[\obmeta_N\otimes\obmeta_N]\rangle\\
&=\frac12\EE\Big[\langle\Hb, (\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})\otimes(\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})\rangle\Big]\\
&\le\frac12\EE\Big[\Hb, (\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})\otimes(\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})+\langle\Hb, (\obmeta_N^{\mathrm{bias}}-\obmeta_N^{\mathrm{var}})\otimes(\obmeta_N^{\mathrm{bias}}-\obmeta_N^{\mathrm{var}})\rangle\Big]\\
&=\langle\Hb, \EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\rangle+\langle\Hb, \obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}\rangle\\
&=\mathrm{bias}+\mathrm{var},
\end{align*}
where the second equality holds due to Lemma~\ref{lemma:vector_decomp}, and the inequality holds because a positive term is added.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:bias_variance_decomp_lower}]
By Lemma~\ref{lemma:vector_decomp}, the excess risk can be written as
\begin{align*}
\EE[L(\owb_N)]-L(\wb_*)&=\frac12\EE\Big[\langle\Hb, (\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})\otimes(\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}})\rangle\Big]\\
&=\frac12\langle\Hb, \EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\rangle+\frac12\langle\Hb, \EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}]\rangle+\langle\Hb, \EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{bias}}]\rangle.
\end{align*}
It then suffices to show that $\EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{bias}}]=\zero$, and it further suffices to prove that $\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_s^{\mathrm{bias}}]=\zero$ for all $t$ and $s$. According to the recursive formulas of the residual vectors, we have
\begin{align*}
\bmeta_t^{\mathrm{var}}&=\delta\sum_{k=1}^t\prod_{l=k+1}^t(\Ib-\delta\xb_l\xb_l^\top)(\xi_k\xb_k)\\
\bmeta_s^{\mathrm{bias}}&=\prod_{j=1}^s(\Ib-\delta\xb_j\xb_j^\top)\bmeta_0.
\end{align*}
We then have
\begin{align*}
\EE[\bmeta_t^{\mathrm{var}}\otimes\bmeta_s^{\mathrm{bias}}]&=\delta\sum_{k=1}^t\EE\bigg[\bigg(\prod_{l=k+1}^t(\Ib-\delta\xb_l\xb_l^\top)(\xi_k\xb_k)\bigg)\otimes\bigg(\prod_{j=1}^s(\Ib-\delta\xb_j\xb_j^\top)\bmeta_0\bigg)\bigg]=\zero,
\end{align*}
where the second inequality holds because $\xi_k$ is zero-mean and independent of feature vectors (Assumption~\ref{assumption:noise_lower}).
\end{proof}

\subsection{Variance Bound}\label{subsection:variance}

We need the following lemma to prove Lemma~\ref{lemma:variance_upper_bound}.
\begin{lemma}\label{lemma:Ct_bound}
Suppose that $\delta\le1/(\psi\tr(\Hb))$. Then for any $t\ge0$, the inner product of $\Cb_t$ and $\Hb$ is upper bounded by
\begin{align*}
\tr(\Hb\Cb_t)\le\frac{\sigma^2\delta\tr(\Hb)}{1-\psi\delta\tr(\Hb)}.
\end{align*}
\end{lemma}
The proof of Lemma~\ref{lemma:Ct_bound} is given in Appendix~\ref{subsection:proof_Ct_bound}. We now provide the proof for Lemma~\ref{lemma:variance_upper_bound}.
\begin{proof}[Proof of Lemma~\ref{lemma:variance_upper_bound}]
According to the definition of $\mathrm{var}$ and Lemma~\ref{lemma:telescope}, we have
\begin{align}
\mathrm{var}&=(1-\alpha)^2\sum_{t=0}^{N-1}\bigg\langle\Hb, \bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Cb_t+\delta^2\bSigma)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg\rangle\nonumber\\
&\le\sum_{t=0}^{N-1}(1-\alpha)^2\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\bigg\langle\Hb, \bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\Hb\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg\rangle\nonumber\\
&\le\frac{\sigma^2}{1-\psi\delta\tr(\Hb)}\sum_{i=1}^d\underbrace{(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(1-\delta\lambda_i)^k\bigg)^2}_{J_i},\label{eq:var_upper_bound}
\end{align}
where the first inequality holds due to Lemma~\ref{lemma:linear_operator_properties} part b and Assumption~\ref{assumption:noise}, and the second inequality holds due to Lemma~\ref{lemma:Ct_bound}.
We then study the upper bound for $J_i$. Firstly, we have
\begin{align}
J_i&\le(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^\infty\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(1-\delta\lambda_i)^k\bigg)^2\nonumber\\
&=\frac{(1-\alpha)\delta\lambda_i}{1-\alpha+\alpha\delta\lambda_i}\cdot\frac{1+\alpha-\alpha\delta\lambda_i}{(1+\alpha)(2-\delta\lambda_i)}\nonumber\\
&\le\frac{(1-\alpha)\delta\lambda_i}{1-\alpha+\alpha\delta\lambda_i}\cdot1\nonumber\\
&\le\min\{1-\alpha, \delta\lambda_i\},\label{eq:Ji_bound1}
\end{align}
where the first inequality holds because positive terms are added, the second inequality holds because $1+\alpha-\delta\lambda_i\le1+\alpha\le(1+\alpha)(2-\delta\lambda_i)$, and the second inequality holds because $1-\alpha+\alpha\delta\lambda_i\ge\max\{1-\alpha, \delta\lambda_i\}$.
Secondly, we have
\begin{align}\label{eq:Ji_bound2}
J_i&\le(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}\bigg)^2=(\delta\lambda_i)^2\sum_{t=0}^{N-1}(1-\alpha^{N-t-1})^2\le N\delta^2\lambda_i^2,
\end{align}
where the first inequality holds because $1-\delta\lambda_i\le1$, and the second inequality holds because $1-\alpha^{N-1-t}\le1$. Substituting \eqref{eq:Ji_bound1} and \eqref{eq:Ji_bound2} into \eqref{eq:var_upper_bound}, we have
\begin{align*}
\mathrm{var}&\le\frac{\sigma^2}{1-\psi\delta\tr(\Hb)}\sum_{i=1}^d\min\bigg\{1-\alpha, \delta\lambda_i, N\delta^2\lambda_i^2\bigg\}\\
&=\frac{\sigma^2}{1-\psi\delta\tr(\Hb)}\bigg[(1-\alpha)k^*+\delta\sum_{i=k^*+1}^{k^\dagger}\lambda_i+N\delta^2\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:variance_lower_bound}]
According to the definition of $\mathrm{var}$ and Lemma~\ref{lemma:telescope}, we have
\begin{align*}
\mathrm{var}&=(1-\alpha)^2\sum_{t=0}^{N-1}\bigg\langle\Hb, \bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Cb_t+\delta^2\bSigma)\\
&\qquad\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg\rangle\\
&\ge\sigma^2\sum_{t=0}^{N-1}(1-\alpha)^2\delta^2\bigg\langle\Hb, \bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\Hb\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg\rangle\\
&=\sigma^2\sum_{i=1}^d\underbrace{(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{t-1}\alpha^{t-1-k}(1-\delta\lambda_i)^k\bigg)^2}_{J_i},
\end{align*}
where the inequality holds due to Lemma~\ref{lemma:linear_operator_properties} part b. We then study the lower bound for $J_i$, based on the regime that $\lambda_i$ falls into:

\paragraph{Case 1: $i\le k^*$}. In this case, $1-\delta\lambda_i\le\alpha$, and we have
\begin{align*}
J_i&=\frac{\delta\lambda_i(1-\alpha)(1+\alpha-\alpha\delta\lambda_i)(1-\alpha^{2N})}{(1-\alpha+\alpha\delta\lambda_i)(1+\alpha)(2-\delta\lambda_i)}\\
&\qquad-\frac{2\delta\lambda_i(1-\alpha)^2(1-\delta\lambda_i)\alpha^N}{(1-\alpha+\alpha\delta\lambda_i)(2-\delta\lambda_i)}\cdot\frac{\alpha^N-(1-\delta\lambda_i)^N}{\alpha-(1-\delta\lambda_i)}-\frac{(1-\alpha)^2\delta\lambda_i}{2-\delta\lambda_i}\bigg(\frac{\alpha^N-(1-\delta\lambda_i)^N}{\alpha-(1-\delta\lambda_i)}\bigg)^2\\
&\ge\frac{\delta\lambda_i(1-\alpha)(1+\alpha-\alpha\delta\lambda_i)(1-\alpha^{2N})}{(1-\alpha+\alpha\delta\lambda_i)(1+\alpha)(2-\delta\lambda_i)}-\frac{2\delta\lambda_i(1-\alpha)^2(1-\delta\lambda_i)\alpha^N}{(1-\alpha+\alpha\delta\lambda_i)(2-\delta\lambda_i)}-\frac{\delta\lambda_i(1-\alpha)^2}{2-\delta\lambda_i}\\
&=\frac{\delta\lambda_i(1-\alpha)(1+\alpha-\alpha\delta\lambda_i)(\alpha^2-\alpha^{2N})}{(1-\alpha+\alpha\delta\lambda_i)(1+\alpha)(2-\delta\lambda_i)}+\frac{2\delta\lambda_i(1-\alpha)^2(1-\delta\lambda_i)(\alpha-\alpha^N)}{(1-\alpha+\alpha\delta\lambda_i)(2-\delta\lambda_i)}\\
&\ge\frac{\delta\lambda_i(1-\alpha)(1+\alpha-\alpha\delta\lambda_i)(\alpha^2-\alpha^{2N})}{(1-\alpha+\alpha\delta\lambda_i)(1+\alpha)(2-\delta\lambda_i)},
\end{align*}
where the first inequality holds because $\frac{\alpha^N-(1-\delta\lambda_i)^N}{\alpha-(1-\delta\lambda_i)}\le N\alpha^{N-1}\le1$, and the second inequality holds because a positive term is dropped. We then consider the function
\begin{align*}
f(x)=\frac{(1-x)(1+\alpha x)}{(1-\alpha x)(1+x)}=1-\frac{2(1-\alpha)}{1/x-\alpha x+(1-\alpha)},\qquad x\in(0, \alpha],
\end{align*}
so $f(x)$ is decreasing in $x$, and $f(x)\ge f(\alpha)=(1+\alpha^2)/(1+\alpha)^2\ge1/2$ (Cauchy-Schwarz inequality). Since $\alpha^{N-1}\le1/N$, we also have $1-\alpha^{2(N-1)}\ge1-1/N^2\ge3/4$ because $N\ge2$. We thus have
\begin{align*}
J_i&=(1-\alpha)\cdot f(1-\delta\lambda_i)\cdot\frac{\alpha^2(1-\alpha^{2(N-1)})}{1+\alpha}\\
&\ge(1-\alpha)\cdot\frac12\cdot\frac{3\alpha^2}{4(1+\alpha)}\\
&\ge\frac{3(1-\alpha)\alpha^2}{16},
\end{align*}
where the last inequality holds because $\alpha\le1$.

\paragraph{Case 2:} $k^*<i\le k^\dagger$. In this case, $1-1/N\le1-\delta\lambda_i\le\alpha$, and for any $\mu\in(1, N)$, we have
\begin{align*}
J_i&\ge(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg((1-\delta\lambda_i)^{t-1}\sum_{k=0}^{t-1}\alpha^k\bigg)^2\\
&=(\delta\lambda_i)^2\sum_{t=0}^{N-1}(1-\delta\lambda_i)^{2(t-1)}(1-\alpha^t)^2\\
&\ge(\delta\lambda_i)^2\sum_{t=\lceil\log_{1/\alpha}\mu\rceil}^{N-1}(1-\delta\lambda_i)^{2(t-1)}(1-\alpha^t)^2\\
&\ge(\delta\lambda_i)^2(1-1/\mu)^2\sum_{t=\lceil\log_{1/\alpha}\mu\rceil}^{N-1}(1-\delta\lambda_i)^{2(t-1)}\\
&=\frac{\delta\lambda_i(1-1/\mu)^2}{2-\delta\lambda_i}[(1-\delta\lambda_i)^{2(\lceil\log_{1/\alpha}\mu\rceil-1)}-(1-\delta\lambda_i)^{2(N-1)}],
\end{align*}
where the first inequality holds because $(1-\delta\lambda_i)^k\le(1-\delta\lambda_i)^{t-1}$, the second inequality holds because negative terms are dropped, and the last inequality holds because $\alpha^t\le\alpha^{\lceil\log_{1/\alpha}\mu\rceil}\le1/\mu$. Since $1-\delta\lambda_i\ge\alpha$, we have
\begin{align*}
(1-\delta\lambda_i)^{2(\lceil\log_{1/\alpha}\mu\rceil-1)}\ge\alpha^{2(\lceil\log_{1/\alpha}\mu\rceil-1)}\ge\alpha^{2\log_{1/\alpha}\mu}=\mu^{-2}.
\end{align*}
Furthermore, since $1-\delta\lambda_i\le1-1/N$, we have
\begin{align*}
(1-\delta\lambda_i)^{2(N-1)}\le(1-1/N)^{2(N-1)}\le(1/2)^2=1/4,
\end{align*}
where the second inequality holds because $(1-1/N)^{2N-2}$ is decreasing in $N$ when $N\ge2$. Therefore, by taking $\mu^{-1}=(1+\sqrt3)/4$, we have
\begin{align*}
J_i\ge\frac{\delta\lambda_i}{2}\cdot\frac{6\sqrt3-9}{64}\ge\frac{\delta\lambda_i}{100}.
\end{align*}

\paragraph{Case 3}: $i>k^\dagger$. In this case $\lambda_i\le1/N\delta$, and for all $k<N$, we have
\begin{align*}
(1-\delta\lambda_i)^k\ge(1-1/N)^{N-1}\ge e^{-1},
\end{align*}
where the second inequality holds because $(1-1/N)^{N-1}$ is decreasing in $N$ when $N\ge2$ and the limit is $e^{-1}$. We then have
\begin{align*}
J_i&\ge e^{-2}(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{t-1}\alpha^k\bigg)^2\\
&=e^{-2}(\delta\lambda_i)^2\sum_{t=0}^{N-1}(1-\alpha^t)^2\\
&\ge e^{-2}(\delta\lambda_i)^2\sum_{t=\lfloor N/2\rfloor}^{N-1}(1-\alpha^t)^2\\
&\ge\frac{N}{2e^2}\delta^2\lambda_i^2(1-\alpha^{(N-1)/2})^2\\
&\ge N\delta^2\lambda_i^2\cdot\frac1{2e^2}\cdot(1-1/\sqrt 2)^2\ge\frac{N\delta^2\lambda_i^2}{180},
\end{align*}
where the second inequality holds because positive terms are dropped, the third inequality holds because for all $t\ge\lfloor N/2\rfloor$, we have $\alpha^t\le\alpha^{(N-1)/2}$, and the fourth inequality holds because $\alpha^{N-1}\le1/N\le1/2$.

Combining all the above, we have
\begin{align*}
\mathrm{var}\ge\sigma^2\bigg[\frac{3\alpha^2(1-\alpha)k^*}{16}+\frac{\delta}{100}\sum_{i=k^*+1}^{k^\dagger}\lambda_i+\frac{N\delta^2}{180}\sum_{i>k^\dagger}\lambda_i^2\bigg].
\end{align*}
\end{proof}

\subsection{Bias Bound}\label{subsection:bias}

We need the following lemma to prove Lemma~\ref{lemma:bias_upper_bound}
\begin{lemma}\label{lemma:St_bound}
The matrices $\Bb_t$ satisfies
\begin{align*}
\sum_{k=1}^t\tr(\Hb\Bb_k)\le\frac{1}{\delta(1-\psi\delta\tr(\Hb))}\sum_{i=1}^d\eta_{0, i}^2[1-(1-\delta\lambda_i)^t].
\end{align*}
\end{lemma}
The proof of Lemma~\ref{lemma:St_bound} is given in Appendix~\ref{subsection:proof_St_bound}. We then prove Lemma~\ref{lemma:bias_upper_bound}.
\begin{proof}[Proof of Lemma~\ref{lemma:bias_upper_bound}]
By definition of the bias error and Lemma~\ref{lemma:telescope}, we have
\begin{align*}
\mathrm{bias}&\le\psi\sum_{i=1}^d\underbrace{(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(1-\delta\lambda_i)^k\bigg)^2}_{J_i}\\
&\qquad+\sum_{i=1}^d\eta_{0, i}^2\lambda_i\bigg[\frac{(1-\alpha)(1-\delta\lambda_i)^N-(\delta\lambda_i)\alpha^N}{1-\delta\lambda_i-\alpha}\bigg]^2,
\end{align*}
where the inequality holds due to Lemma~\ref{lemma:linear_operator_properties} part b. We then study the upper bound of $J_i$. Firstly, we have
\begin{align*}
J_i&\le(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\bigg(\sum_{k=0}^{N-2-t}(1-\delta\lambda_i)^k\bigg)^2\\
&=(1-\alpha)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)(1-(1-\delta\lambda_i)^{N-1-t})^2\\
&\le(1-\alpha)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\\
&\le\frac{(1-\alpha)^2}{\delta(1-\psi\tr(\Hb))}\sum_{i=1}^d\eta_{0, i}^2[1-(1-\delta\lambda_i)^N]\\
&\le\frac{(1-\alpha)^2}{\delta(1-\psi\tr(\Hb))}(\|\bmeta_0\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\bmeta_0\|_{\Hb_{k^\dagger:\infty}}^2),
\end{align*}
where the first inequality holds because $\alpha\le1$, the second inequality holds because $1-(1-\delta\lambda_i)^{N-1-t}\le1$, the third inequality holds due to Lemma~\ref{lemma:St_bound}, and the last inequality holds because $1-(1-\delta\lambda_i)^N\le\min\{1, N\delta\lambda_i\}$.
Secondly, we have
\begin{align*}
J_i&\le(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}\bigg)^2\\
&=(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)(1-\alpha^{N-1-t})^2\\
&\le(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\\
&\le\frac{\delta\lambda_i^2}{1-\psi\delta\tr(\Hb)}\sum_{i=1}^d\eta_{0, i}^2[1-(1-\delta\lambda_i)^N]\\
&\le\frac{\delta\lambda_i^2}{1-\psi\delta\tr(\Hb)}(\|\bmeta_0\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\bmeta_0\|_{\Hb_{k^\dagger:\infty}}^2),
\end{align*}
where the first inequality holds because $1-\delta\lambda_i\le1$, the second inequality holds because $1-\alpha^{N-1-t}\le1$, the third inequality holds due to Lemma~\ref{lemma:St_bound}, and the last inequality holds because $1-(1-\delta\lambda_i)^N\le\min\{1, N\delta\lambda_i\}$. Combining all the above, we have
\begin{align*}
\mathrm{bias}&\le\frac{\psi(\|\wb_0-\wb_*\|_{\Ib_{0:k^\dagger}}^2+N\delta\|\wb_0-\wb_*\|_{\Hb_{k^\dagger:\infty}}^2)}{\delta(1-\psi\delta\tr(\Hb))}\bigg(k^*(1-\alpha)^2+\delta^2\sum_{i>k^*}\lambda_i^2\bigg)\\
&\qquad+\sum_{i=1}^n(\wb_0-\wb_*)_i^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2.
\end{align*}


\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:bias_lower_bound}]
According to the definition of the bias error and Lemma~\ref{lemma:telescope}, we have
\begin{align*}
\mathrm{bias}&\ge\beta\sum_{i=1}^d(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\tr(\Hb\Bb_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(1-\delta\lambda_i)^k\bigg)^2\\
&\qquad+\sum_{i=1}^d\eta_{0, i}^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2\\
&\ge\beta\tr(\Bb_0\Hb(\Ib-\delta\Hb)^{2(N-1)})\sum_{i=1}^d\underbrace{(1-\alpha)^2(\delta\lambda_i)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{t-1}\alpha^{t-1-k}(1-\delta\lambda_i)^k\bigg)^2}_{J_i}\\
&\qquad+\sum_{i=1}^d\eta_{0, i}^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2,
\end{align*}
where the second inequality holds because 
\begin{align*}
\Bb_t=\cB^t\circ\Bb_0\succeq\tilde\cB^t\circ\Bb_0=(\Ib-\delta\Hb)^t\Bb_0(\Ib-\delta\Hb)^t\succeq(\Ib-\delta\Hb)^{N-1}\Bb_0(\Ib-\delta\Hb)^{N-1}.
\end{align*}
Note that the lower bound for $J_i$ is the same as that in the proof of Lemma~\ref{lemma:variance_lower_bound}. For the term $\tr(\Bb_0\Hb(\Ib-\delta\Hb)^{2N})$, we have
\begin{align*}
\tr(\Bb_0\Hb(\Ib-\delta\Hb)^{2N})&=\sum_{i=1}^d\eta_{0, i}^2\lambda_i(1-\delta\lambda_i)^{2(N-1)}\ge\sum_{i>k^\dagger}\eta_{0, i}^2\lambda_i(1-1/N)^{2(N-1)}\ge e^{-2}\|\bmeta_0\|_{\Hb_{k^\dagger:\infty}}^2,
\end{align*}
where the second inequality holds because $\delta\lambda_i\le1/N$ when $i>k^\dagger$, and the second inequality holds because $(1-1/N)^{2(N-1)}\ge1/e^2$. We thus have
\begin{align*}
\mathrm{bias}&\ge\beta e^{-2}\|\bmeta_0\|_{\Hb_{k^\dagger:\infty}}^2\bigg[\frac{3\alpha^2(1-\alpha)k^*}{16}+\frac{\delta}{100}\sum_{i=k^*+1}^{k^\dagger}\lambda_i+\frac{N\delta^2}{180}\sum_{i>k^\dagger}\lambda_i^2\bigg]\\
&\qquad+\sum_{i=1}^d\eta_{0, i}^2\lambda_i\bigg[\frac{(\delta\lambda_i)\alpha^N-(1-\alpha)(1-\delta\lambda_i)^N}{\delta\lambda_i-(1-\alpha)}\bigg]^2.
\end{align*}
\end{proof}

\section{Proof of Lemmas in Appendix~\ref{section:Proof_proof_EMA_excess_risk}}

\subsection{Proof of Lemma~\ref{lemma:Ct_bound}}\label{subsection:proof_Ct_bound}

We need the following lemmas to prove Lemma~\ref{lemma:Ct_bound}:
\begin{lemma}\label{lemma:Ct_increasing}
$\Cb_t$ satisfies
\begin{align*}
\Cb_t=\sum_{k=0}^{k-1}\cB^k\circ\bSigma.
\end{align*}
Since $\cB$ is a PSD operator (Lemma~\ref{lemma:linear_operator_properties}), we have
\begin{align*}
\Cb_0\preceq\Cb_1\preceq\cdots\Cb_t\preceq\cdots.
\end{align*}
\end{lemma}
\begin{proof}
The expression for $\Cb_t$ follows directly from the recursive formula for $\Cb_t$.
\end{proof}
We now provide the proof of Lemma~\ref{lemma:Ct_bound}.
\begin{proof}[Proof of Lemma~\ref{lemma:Ct_bound}]
According to the recursive formula, we have
\begin{align*}
\Cb_t&=\cB\circ\Cb_{t-1}+\delta^2\bSigma\preceq\tilde\cB\circ\Cb_{t-1}+\delta^2(\psi\tr(\Hb\Cb_{t-1})+\sigma^2)\Hb\\
&\preceq\sum_{k=0}^{t-1}(\psi\delta^2\tr(\Hb\Cb_{t-1-k})+\sigma^2)\cdot\tilde\cB^k\circ\Hb\\
&\preceq\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\sum_{k=0}^{t-1}\tilde\cB^k\circ\Hb\\
&\preceq\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\sum_{k=0}^\infty\tilde\cB^k\circ\Hb,
\end{align*}
where the first inequality holds due to Lemma~\ref{lemma:linear_operator_properties} part b and Assumption~\ref{assumption:fourth}, the second inequality holds by recursively applying the first inequality, the third inequality holds due to Lemma~\ref{lemma:Ct_increasing}, and the last inequality holds because $\tilde\cB$ is a PSD operator (Lemma~\ref{lemma:linear_operator_properties}, part a). Taking the inner product with $\Hb$ on both sides of the inequality, we have
\begin{align*}
\tr(\Hb\Cb_t)&\le\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\sum_{k=0}^\infty\tr(\Hb(\Ib-\delta\Hb)^k\Hb(\Ib-\delta\Hb)^k)\\
&=\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\sum_{k=0}^\infty\tr(\Hb(\Ib-\delta\Hb)^{2k}\Hb)\\
&\le\delta^2(\psi\tr(\Hb\Cb_t)+\sigma^2)\sum_{k=0}^\infty\tr(\Hb(\Ib-\delta\Hb)^k\Hb)\\
&=\delta(\psi\tr(\Hb\Cb_t)+\sigma^2)\tr(\Hb),
\end{align*}
where the second inequality holds because $\Ib-\delta\Hb\succ0$. Rearranging terms, as long as $\delta<1/(\psi\tr(\Hb))$, we have
\begin{align*}
\tr(\Hb\Cb_t)\le\frac{\sigma^2\delta\tr(\Hb)}{1-\psi\delta\tr(\Hb)}.
\end{align*}
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:St_bound}}\label{subsection:proof_St_bound}

\begin{proof}[Proof of Lemma~\ref{lemma:St_bound}]
Define
\begin{align*}
\Sbb_t^1&=\sum_{k=0}^{t-1}\Bb_t.
\end{align*}
Note that $\Sbb_t^1$ satisfies
\begin{align*}
\Sbb_t^1=\cB\circ\Sbb_{t-1}+\Bb_0,
\end{align*}
so according to Lemma~\ref{lemma:linear_operator_properties} part b, $\Sbb_t^1$ can be bounded by
\begin{align*}
\Sbb_t^1&\preceq\tilde\cB\circ\Sbb_{t-1}^1+\psi\delta^2\tr(\Hb\Sbb_{t-1}^1)\Hb+\Bb_0\\
&\preceq\sum_{k=0}^{t-1}\tilde\cB^k\circ(\psi\delta^2\tr(\Hb\Sbb_{t-1-k}^1)\Hb+\Bb_0)\\
&\preceq\sum_{k=0}^{t-1}\tilde\cB^k\circ(\psi\delta^2\tr(\Hb\Sbb_t^1)\Hb+\Bb_0)\\
&=\psi\delta^2\tr(\Hb\Sbb_t^1)\sum_{k=0}^{t-1}(\Ib-\delta\Hb)^k\Hb(\Ib-\delta\Hb)^k+\sum_{k=0}^{t-1}(\Ib-\delta\Hb)^k\Bb_0(\Ib-\delta\Hb)^k,
\end{align*}
where the second inequality holds by recursively applying the first inequality, and the third inequality holds because $\Sbb_{t-1-k}^1\preceq\Sbb_t^1$. Taking the inner produce on both sides of the inequality, we have
\begin{align*}
\tr(\Hb\Sbb_t^1)&\le\psi\delta^2\tr(\Hb\Sbb_t^1)\sum_{k=0}^{t-1}\tr(\Hb^2(\Ib-\delta\Hb)^{2k})+\sum_{k=0}^{t-1}\tr(\Bb_0\Hb(\Ib-\delta\Hb)^{2k})\\
&\le\psi\delta^2\tr(\Hb\Sbb_t^1)\sum_{k=0}^{t-1}\tr(\Hb^2(\Ib-\delta\Hb)^k)+\sum_{k=0}^{t-1}\tr(\Bb_0\Hb(\Ib-\delta\Hb)^k)\\
&\le\psi\delta^2\tr(\Hb\Sbb_t^1)\sum_{k=0}^{\infty}\tr(\Hb^2(\Ib-\delta\Hb)^k)+\sum_{k=0}^{t-1}\tr(\Bb_0\Hb(\Ib-\delta\Hb)^k)\\
&=\psi\delta\tr(\Hb)\tr(\Hb\Sbb_t^1)+\delta^{-1}\sum_{i=1}^d\eta_{0, i}^2(1-(1-\delta\lambda_i)^t),
\end{align*}
where the second inequality holds because $(\Ib-\delta\Hb)^{2k}\preceq(\Ib-\delta\Hb)^k$, and the third inequality holds because positive terms $\tr(\Hb^2(\Ib-\delta\Hb)^k)$ for $k\ge t$ are added. Rearranging terms, we have
\begin{align*}
\tr(\Hb\Sbb_t^1)\le\frac{1}{\delta(1-\psi\delta\tr(\Hb))}\sum_{i=1}^d\eta_{0, i}^2[1-(1-\delta\lambda_i)^t].
\end{align*}
\end{proof}


\section{Properties of Centered Iterates and Linear Operators on Matrices}\label{section:linear_operator_property}

\begin{lemma}\label{lemma:linear_operator_properties}
The linear operators on matrix enjoy the following properties:
\begin{itemize}[leftmargin=*]
\item[a.] $\cM$, $\tilde\cM$, $\cB$, and $\tilde\cB$ are all PSD operators, i.e., for any PSD matrix $\Ab$, we have that $\cM\circ\Ab$, $\tilde\cM\circ\Ab$, $\cB\circ\Ab$, and $\tilde\cB\circ\Ab$ are all PSD matrices.
\item[b.] $\cB-\tilde\cB=\delta^2(\cM-\tilde\cM)$ is also a PSD operator, which is bounded by
\begin{align*}
\beta\delta^2\tr(\Hb\Ab)\Hb\preceq(\cB-\tilde\cB)\circ\Ab=\delta^2(\cM-\tilde\cM)\circ\Ab\preceq\delta^2\cM\circ\Ab\preceq\psi\delta^2\tr(\Hb\Ab)\Hb.
\end{align*}
\end{itemize}
\end{lemma}
\begin{proof}
\begin{itemize}[leftmargin=*]
\item[a.] Let $\Ab$ denote any PSD matrix, and $\vb$ be any vector. We then have
\begin{align*}
\vb^\top(\cM\circ\Ab)\vb=\EE[(\vb^\top\xb)^2(\xb^\top\Ab\xb)]\ge0,
\end{align*}
where the equality holds because $(\vb^\top\xb)^2\ge0$ and $\xb^\top\Ab\xb\ge0$. Furthermore,
\begin{align*}
\vb^\top(\cB\circ\Ab)\vb=\EE[\vb^\top(\Ib-\delta\xb\xb^\top)\Ab(\Ib-\delta\Hb\xb\xb^\top)\vb]=\EE[(\vb-\delta(\vb^\top\xb)\xb)^\top\Ab(\vb-\delta(\vb^\top\xb)\xb)]\ge0,
\end{align*}
where the inequality holds because for any vector $\ub$ ($\ub=\vb-\delta(\vb^\top\xb)\xb$ in this case), we have $\ub^\top\Ab\ub\ge0$. Finally, $\tilde\cM$ and $\tilde\cB$ are PSD operators because any matrix similar to a PSD matrix is also a PSD matrix.
\item[b.] The difference between $\cB$ and $\tilde\cB$ is
\begin{align*}
\cB-\tilde\cB&=\EE[(\Ib-\delta\xb\xb^\top)\otimes(\Ib-\delta\xb\xb^\top)]-(\Ib-\delta\Hb)\otimes(\Ib-\delta\Hb)\\
&=(\Ib\otimes\Ib-\delta\Hb\otimes\Ib-\delta\Ib\otimes\Hb+\delta^2\cM)-(\Ib\otimes\Ib-\delta\Hb\otimes\Ib-\delta\Ib\otimes\Hb+\delta^2\tilde\cM)\\
&=\delta^2(\cM-\tilde\cM).
\end{align*}
Furthermore,
\begin{align*}
\cM-\tilde\cM=\EE[(\xb\xb^\top-\Hb)\otimes(\xb\xb^\top-\Hb)],
\end{align*}
so $\cM-\tilde\cM$ is a PSD operator. The upper bound follows directly from the fact that $\tilde\cM$ is PSD and Assumption~\ref{assumption:fourth}.
\end{itemize}
\end{proof}

Lemma \ref{lemma:excess_risk_equivalence} and Lemma \ref{lemma:vector_decomp} are similar to their counterparts in \citet{zou2021benign}, and are presented here for completeness.

\begin{lemma}\label{lemma:excess_risk_equivalence}
The excess risk is equivalent to
\begin{align*}
L(\owb_N)-L(\wb_*)=\frac12\langle\Hb, \obmeta_N\otimes\obmeta_N\rangle.
\end{align*}
\end{lemma}
\begin{proof}
By definition of the risk function, we have
\begin{align*}
L(\owb_N)-L(\owb_*)&=\frac12\EE_{(\xb, y)\sim\cD}[(y-\langle\owb_N, \xb\rangle)^2-(y-\langle\wb_*, \xb\rangle)^2]\\
&=\frac12\EE_{(\xb, y)\sim\cD}[(\wb_*-\owb_N)^\top(\xb\cdot 2y-\xb\xb^\top(\owb_N+\wb_*))]\\
&=\frac12(\wb_*-\owb_N)^\top(2\Hb\wb_*-\Hb(\owb_N+\wb_*))\\
&=\frac12\langle\Hb, \obmeta_N\otimes\obmeta_N\rangle,
\end{align*}
where the third equality holds due to \eqref{eq:noise_zero_mean} and the definition of $\Hb$.
\end{proof}

\begin{lemma}\label{lemma:vector_decomp}
For any $t>0$, we have
\begin{align*}
\bmeta_t=\bmeta_t^{\mathrm{bias}}+\bmeta_t^{\mathrm{var}}.
\end{align*}
We thus have
\begin{align*}
\obmeta_N=\obmeta_N^{\mathrm{bias}}+\obmeta_N^{\mathrm{var}}.
\end{align*}
\end{lemma}
\begin{proof}
We prove the lemma by induction. When $t=0$, the lemma holds trivially. Suppose that the lemma holds for $t-1$, then we have
\begin{align*}
\bmeta_t&=\wb_t-\wb_*=(\wb_{t-1}-\wb_*)+\delta(y_t-\langle\wb_{t-1}, \xb_t\rangle)\xb_t\\
&=(\wb_{t-1}-\wb_*)+\delta(\xi_t-\langle\wb_{t-1}-\wb_*, \xb_t\rangle)\xb_t\\
&=(\Ib-\delta\xb_t\xb_t^\top)\bmeta_{t-1}+\delta\xi_t\xb_t\\
&=(\Ib-\delta\xb_t\xb_t^\top)(\bmeta_t^{\mathrm{bias}}+\bmeta_t^{\mathrm{var}})+\delta\xi_t\xb_t\\
&=[(\Ib-\delta\xb_t\xb_t^\top)\bmeta_t^{\mathrm{bias}}]+[(\Ib-\delta\xb_t\xb_t^\top)\bmeta_t^{\mathrm{var}}+\delta\xi_t\xb_t]\\
&=\bmeta_t^{\mathrm{bias}}+\bmeta_t^{\mathrm{var}},
\end{align*}
where the fifth equality holds due to the induction hypothesis. Therefore, the lemma holds for $t$. Combining all the above, the lemma is proved for all $t\ge0$.
\end{proof}

\begin{lemma}\label{lemma:telescope}
The second moment of the residual vectors can be decomposed as
\begin{align*}
&\EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\\
&=\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Bb_0\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Bb_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg),\\
&\EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}]\\
&=(1-\alpha)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Cb_t+\delta^2\bSigma)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg).
\end{align*}
\end{lemma}
\begin{proof}
To simplify notations, we omit the superscripts of $\bmeta_t$ and $\obmeta_N$, and denote $\Db_t=\EE[\bmeta_t\otimes\bmeta_t]$. According to the definition of $\obmeta_N$, we have
\begin{align*}
&\EE[\obmeta_N\otimes\obmeta_N]=\EE\bigg[\bigg(\alpha^N\bmeta_0+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{N-1-t}\bmeta_t\bigg)\otimes\bigg(\alpha^N\bmeta_0+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{N-1-t}\bmeta_t\bigg)\bigg]\\
&=\alpha^{2N}\Db_0+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{2N-1-t}\Big[\EE[\bmeta_0\otimes\bmeta_t]+\EE[\bmeta_t\otimes\bmeta_0]\Big]\\
&\qquad+(1-\alpha)^2\sum_{s=0}^{N-1}\sum_{t=0}^{N-1}\alpha^{2N-2-s-t}\EE[\bmeta_s\otimes\bmeta_t]\\
&=\alpha^{2N}\Db_0+(1-\alpha)\sum_{t=0}^{N-1}\alpha^{2N-1-t}[\Db_0(\Ib-\delta\Hb)^t+(\Ib-\delta\Hb)^t\Db_0]\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-1}\bigg[\alpha^{2N-2-2t}\Db_t+\sum_{k=1}^{N-1-t}\alpha^{2N-2-2t-k}[\Db_t(\Ib-\delta\Hb)^k+(\Ib-\delta\Hb)^k\Db_t]\bigg]\\
&=\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Db_0\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad-(1-\alpha)^2\bigg(\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Db_0\bigg(\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-1}\bigg[\bigg(\sum_{k=0}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\Db_t\bigg(\sum_{k=0}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad-\bigg(\sum_{k=1}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\Db_t\bigg(\sum_{k=1}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg]\\
&=\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Db_0\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-2}\bigg[\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\Db_{t+1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad-\bigg(\sum_{k=1}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\Db_t\bigg(\sum_{k=1}^{N-1-t}\alpha^{N-1-t-k}(\Ib-\delta\Hb)^k\bigg)\bigg]\\
&=\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Db_0\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)(\Db_{t+1}-\tilde\cB\circ\Db_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg),
\end{align*}
where the third inequality holds because $\EE[\bmeta_{t+k}\otimes\bmeta_t]=\EE[\EE[\bmeta_{t+k}\otimes\bmeta_t|\cF_t]]=\EE[(\Ib-\delta\Hb)^k(\bmeta_t\otimes\bmeta_t)]=(\Ib-\delta\Hb)^k\Db_t$, and the fifth equality holds due to telescope sum. Specifically, for the bias residual, we have $\Bb_{t+1}=\cB\circ\Bb_t$, so
\begin{align*}
&\EE[\obmeta_N^{\mathrm{bias}}\otimes\obmeta_N^{\mathrm{bias}}]\\
&=\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\Bb_0\bigg(\alpha^N\Ib+(1-\alpha)\sum_{k=0}^{N-1}\alpha^{N-1-k}(\Ib-\delta\Hb)^k\bigg)\\
&\qquad+(1-\alpha)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Bb_t)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg).
\end{align*}
For the variance residual, we have $\Cb_{t+1}=\cB\circ\Cb_t+\delta^2\bSigma$ and $\Cb_0=\zero$, so
\begin{align*}
&\EE[\obmeta_N^{\mathrm{var}}\otimes\obmeta_N^{\mathrm{var}}]\\
&=(1-\alpha)^2\sum_{t=0}^{N-1}\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg)((\cB-\tilde\cB)\circ\Cb_t+\delta^2\bSigma)\bigg(\sum_{k=0}^{N-2-t}\alpha^{N-2-t-k}(\Ib-\delta\Hb)^k\bigg).
\end{align*}
\end{proof}


\bibliography{refs}
\bibliographystyle{ims}

\end{document}