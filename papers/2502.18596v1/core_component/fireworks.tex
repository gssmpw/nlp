\subsection{JRM Deployment Using FireWorks}
\label{jrm-deployment}

This section provides an overview of the JRM deployment process, which involves two main components: the FireWorks launchpad and the JRM deployment script. The FireWorks launchpad is a MongoDB database that stores the JRM workflows, while the JRM deployment script is a bash script that sets up and runs a Docker container to launch Slurm jobs for deploying JRMs. For further details and access to the complete code, refer to the GitHub repository \cite{github}.

\subsubsection{Prerequisite: FireWorks Launchpad Setup}

FireWorks is employed to manage the JRM deployment, offering a workflow management system that facilitates the execution of complex workflows. Comprehensive information on FireWorks can be found in \cite{fireworks-url}, and additional guidance is provided by the NERSC introduction to FireWorks \cite{nersc-url}.

\paragraph{Launchpad Configuration File for FireWorks}
The configuration file \texttt{/FireWorks/util/my\_launchpad.yaml} serves as a MongoDB configuration for the FireWorks launchpad. Ensure that the MongoDB port (default is \texttt{27017}) is accessible to the container. If it is not accessible, verify that the port is open to all interfaces.

\paragraph{Setup on the Database Server}
Establish the database and user as specified in the \texttt{my\_launchpad.yaml} file. The \texttt{FireWorks/util/create\_db.sh} script can be utilized to create the database and user.

\paragraph{Setup on the Compute Node}
Prepare the Python environment according to the \texttt{requirements.txt} file. Use the \texttt{FireWorks/util/create\_project.py} script to set up configuration files, generating two files: \texttt{my\_qadapter.yaml} and \texttt{my\_fworker.yaml}. Refer to the example files \texttt{FireWorks/util/my\_launchpad.yaml} and \texttt{FireWorks/util/my\_qadapter.yaml} for guidance. Ensure that MongoDB is accessible from the compute node. If not, consider using SSH tunneling to establish a connection to MongoDB.

\subsubsection{JRM Deployment on Perlmutter at NERSC}

With the FireWorks launchpad set up, the JRM deployment can proceed through the following steps:

\paragraph{Prerequisite: SSH Private Key to NERSC}
A NERSC account and an established private key (e.g., \texttt{\~/.ssh/nersc}) are required for logging into Perlmutter.

\paragraph{Step 1: Create SSH Connections using the Binary \texttt{jrm-create-ssh-connections}}
The binary acts as an HTTP server listening on port \texttt{8888}, creating four SSH connections (db port, apiserver port, jrm port, and custom metrics port) as depicted by the dotted black lines in Figure~\ref{fig:network_map}. Detailed information is available in the \texttt{create-ssh-connections/jrm-fw-create-ssh-connections.go} file. The script looks for available ports for the JRM port (between 10000 and 19999) and custom metrics port (between 20000 and 49999) on the local machine.

\paragraph{Step 2: Configure Environment Variables}
These environment variables are essential for launching JRMs (Job Resource Managers) and establishing SSH tunnels, ensuring seamless integration and communication between different system components.

\begin{verbatim}
nnodes=2
nodetype=cpu
walltime=00:05:00
account=m3792
qos=debug

nodename=vk-nersc-test
site=perlmutter
control_plane_ip=jiriaf2302
apiserver_port=38687
kubeconfig=/global/homes/j/jlabtsai/run-vk/kubeconfig/jiriaf2302
vkubelet_pod_ip=172.17.0.1
jrm_image=docker:jlabtsai/vk-cmd:main
custom_metrics_ports=1234 1423

ssh_remote_proxy=perlmutter
ssh_remote=jlabtsai@128.55.64.13
ssh_key=$HOME/.ssh/nersc
\end{verbatim}

\paragraph{Step 3: Launch JRMs}
Pull the Docker image \texttt{jlabtsai/jrm-fw} from Docker Hub. Follow the above two steps and run the following commands.

\begin{verbatim}
#!/bin/bash
export env_list="env.list"
export jrm_fw_tag="latest"
export logs="$HOME/jrm-launch/logs"

docker run -it --rm --name=jrm-fw -v $logs:/fw/logs --env-file $env_list \
    jlabtsai/jrm-fw:$jrm_fw_tag $@
\end{verbatim}

Within the container, the \texttt{main} directory contains essential files, including:
\texttt{jrm-create-ssh-connections} (a binary), 
\texttt{env.list} (an example setup of environment variables), 
and \texttt{main.sh} (a script for running the image).

\paragraph{Note}
Execute the \texttt{main.sh} script, which accepts the following arguments:
\begin{itemize}
    \item \texttt{add\_wf}: Adds a JRM workflow to the FireWorks database.
    \item \texttt{get\_wf}: Retrieves the table of workflows from the FireWorks database.
    \item \texttt{delete\_wf}: Removes a specific workflow from the FireWorks database.
\end{itemize}

\subsubsection{Port and SSH Tunneling Overview}

Figure~\ref{fig:network_map} illustrates the ports and SSH tunneling used in the JRM deployment process. The Kubernetes cluster in this experiment is deployed using Kubernetes in Docker (KinD) \cite{KinD}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{core_component/fig/jrm_launch_map.pdf}
\caption{
Network map illustrating the ports and SSH tunneling configurations used in the JRM deployment process. Arrowed red lines represent command execution, while black dashed lines indicate SSH tunneling initiated by \texttt{JIRIAF2301}. Solid gray lines represent listening connections, and dashed gray lines represent SSH tunneling from JRM at the compute node to \texttt{login04} at NERSC. The map highlights communication paths for MongoDB, Kubernetes API server, JRM metrics, and custom metrics.
}
\label{fig:network_map}
\end{figure}

\subsubsection{Walltime Discrepancy Between JRM and Slurm Job}

The \texttt{JIRIAF\_WALLTIME} variable in \texttt{FireWorks/gen\_wf.py} is intentionally set to be \texttt{60 seconds} less than the walltime of the Slurm job. This adjustment ensures that the JRM has sufficient time to initialize and start running.

Upon the expiration of \texttt{JIRIAF\_WALLTIME}, the JRM will be terminated. The commands for tracking the walltime and terminating the JRM are explicitly defined in the \texttt{FireWorks/gen\_wf.py} file, as illustrated below:

\begin{verbatim}
sleep $JIRIAF_WALLTIME
echo "Walltime $JIRIAF_WALLTIME has ended. Terminating the processes."
pkill -f "./start.sh"
\end{verbatim}
