A 40-node reservation on the Perlmutter system at NERSC was activated to deploy data-stream processing pipelines. This deployment utilized the JIRIAF framework across the JIRIAF Kubernetes cluster nodes, each executing the CLAS12 event reconstruction application. This application was optimized to fully leverage all available processing cores within the ERSAP framework (see Figure~\ref{fig:ersap_framework}).

To demonstrate the effectiveness of JIRIAF, a proof of concept was conducted using the CLAS12 experiment. Event streams were transmitted to the NERSC computing facility via the EJFAT transport system. JRMs/VKs were deployed on 40 nodes within the NERSC cluster for stream processing workflows. The ERSAP workflow was deployed for CLAS12 reconstruction.

JRMs/VKs of JIRIAF as agents were deployed by the SLURM batch job system at NERSC. These JRMs formed K8s nodes waiting for deployments. The ERSAP processing application was containerized and uploaded to the Shifter container hub at NERSC. A K8s deployment YAML file was created and applied to the K8s API server on the control-plane at JLAB (refer to \hyperref[deploying-pods]{Deploying and Managing Pods on Virtual-Kubelet-Cmd Nodes}). The monitoring system scraped and stored metrics data on the control-plane at JLAB (refer to \hyperref[prometheus-operator]{Using Prometheus Operator in K8s Cluster to Monitor Application Metrics}). Notice that we deployed an independent Prometheus server outside our Kubernetes cluster during this test.

Concurrent with the deployment, CLAS12 raw event data began streaming from JLAB, generating traffic volumes exceeding 100 Gbps directed towards the ESnet Load Balancer (LB). This marked the commencement of the operational functionality of the distributed remote data stream processing.

The monitoring system metrics scraped from applications during the JIRIAF deployment are shown in Figure~\ref{fig:monitoring_metrics}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{proof_of_concept/fig/clas12.pdf}
    \caption{The ERSAP framework utilized in the JIRIAF deployment on the Perlmutter compute nodes at NERSC. The CLAS12 event reconstruction application ran on each node in the JIRIAF Kubernetes cluster, demonstrating the JIRIAF deployment's effectiveness in handling high-volume data-stream processing.}
    \label{fig:ersap_framework}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{proof_of_concept/fig/clas12_data.pdf}
    \caption{Monitoring system metrics scraped from applications during the JIRIAF deployment. The figure shows the metrics collected by the monitoring system, providing insights into the performance and resource utilization of the deployed CLAS12 event reconstruction application across the NERSC cluster nodes. These metrics are crucial for evaluating the effectiveness and efficiency of the JIRIAF framework in a high-performance computing environment.}
    \label{fig:monitoring_metrics}
\end{figure}

\newpage

\subsection{Deploying JRM/VK Nodes via SLURM}
A 40-node reservation on the Perlmutter system was activated for deploying data-stream processing pipelines using the JIRIAF framework.

The deployment of these JRMs/VKs was automated through a SLURM script named \texttt{nersc-slurm.sh}. This script handles the deployment across multiple nodes efficiently by leveraging SLURM's job scheduling capabilities.

\begin{verbatim}
#!/bin/bash

# SBATCH -N 40
# SBATCH -C cpu
# SBATCH -q regular
# SBATCH -J 100g
# SBATCH -t 03:00:00
# SBATCH --reservation=100g

for i in $(seq 1 40)
do
    i_padded=$(printf "%02d" $i)
    echo $i_padded
    srun -N1 /global/homes/j/jlabtsai/run-vk/slurm/mylin/node-setup.sh $i_padded &
    sleep 3
done
wait
\end{verbatim}

This SLURM script configures and submits a job to reserve 40 CPU-based nodes. It then executes the \texttt{node-setup.sh} script on each node in a staggered manner to prevent overloading the system's resources.

Additionally, each Virtual Kubelet node is configured using the \texttt{node-setup.sh} script, which sets up environment variables, configures SSH tunnels for secure communication, and prepares the node for the deployment of monitoring tools and applications.

\begin{verbatim}
#!/bin/bash

export CONTROL_PLANE_IP="jiriaf2302"
export APISERVER_PORT="38687"
export NODENAME="vk-nersc$1"
export KUBECONFIG="/global/homes/j/jlabtsai/run-vk/kubeconfig/$CONTROL_PLANE_IP"
export VKUBELET_POD_IP="172.17.0.1"
export KUBELET_PORT="100"$1

export JIRIAF_WALLTIME="0"
export JIRIAF_NODETYPE="cpu"
export JIRIAF_SITE="nersc"

export proxy_remote="jlabtsai@128.55.64.25"

ssh -NfL $APISERVER_PORT:localhost:$APISERVER_PORT $proxy_remote
ssh -NfR $KUBELET_PORT:localhost:$KUBELET_PORT $proxy_remote

export ersap_exporter="200"$1
export process_exporter="300"$1
export ejfat_exporter="400"$1

ssh -NfR $ersap_exporter:localhost:2221 $proxy_remote
ssh -NfR $process_exporter:localhost:1776 $proxy_remote
ssh -NfR $ejfat_exporter:localhost:8080 $proxy_remote

shifter --image=docker:jlabtsai/vk-cmd:main -- /bin/bash -c "cp -r /vk-cmd `pwd`/$NODENAME"
cd `pwd`/$NODENAME
./start.sh $KUBECONFIG $NODENAME $VKUBELET_POD_IP $KUBELET_PORT $JIRIAF_WALLTIME \
    $JIRIAF_NODETYPE $JIRIAF_SITE
\end{verbatim}

These scripts together facilitate a robust and scalable deployment strategy for JRMs/VKs, crucial for managing high-performance computing tasks across distributed environments.

\subsection{Deploying ERSAP Applications}
The ERSAP applications are deployed on the 40 JRM/VK nodes using Kubernetes Helm charts \cite{helm} to manage the deployment of containers. The deployment utilizes a series of ConfigMaps and Pods defined in Helm templates.

\paragraph{Helm Deployment Commands}
The following script deploys the ERSAP applications on each JRM/VK node using Helm:

\begin{verbatim}
for i in $(seq 1 40)
do
    i_padded=$(printf "%02d" $i)
    echo $i_padded
    helm install ersap$i ersap --set name=$i_padded
done
\end{verbatim}

This script sequentially deploys the Helm chart on each of the 40 nodes, adjusting the configuration dynamically.
