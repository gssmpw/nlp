\documentclass{article}
\usepackage{format, times}
\usepackage{xr-hyper} 

%\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=blue,
    filecolor=magenta,      
    urlcolor=red,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\title{Sampling through Algorithmic Diffusion in non-convex Perceptron problems}




\author{Davide Straziota\thanks{Equal contribution.} \hspace{1em} Elizaveta Demyanenko$^*$ \hspace{0.5em}  Carlo Baldassi \hspace{0.6em}  Carlo Lucibello\\
Department of Computing Sciences, Bocconi University, Milan, 20136, Italy\\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
We analyze the problem of sampling from the solution space of simple yet non-convex neural network models by employing a denoising diffusion process known as Algorithmic Stochastic Localization, where the score function is provided by Approximate Message Passing. We introduce a formalism based on the replica method to characterize the process in the infinite-size limit in terms of a few order parameters, and, in particular, we provide criteria for the feasibility of sampling.

We show that, in the case of the spherical perceptron problem with negative stability, approximate uniform sampling is achievable across the entire replica symmetric region of the phase diagram. In contrast, for the binary perceptron, uniform sampling via diffusion invariably fails due to the overlap gap property exhibited by the typical set of solutions. We discuss the first steps in defining
alternative measures that can be efficiently sampled.
\end{abstract}


\section{Introduction}
A fundamental challenge in modern machine learning is the ability to efficiently sample from complex probability distributions. This problem is central to many areas, from generative modeling, where methods such as variational autoencoders or autoregressive transoformers aim to approximate real-world data distributions, to Bayesian learning, where one seeks to explore posterior distributions over model parameters.

Diffusion models \citep{sohl2015deep, ho2020denoising}
have emerged in the last years as a prominent generative model family, able to capture highly complex distributions in several domains while maintaining low sample complexity.


In this paper, we consider a setting where the target distribution is known up to a normalization function and our goal is fair sampling from the distribution. We will adopt a diffusion setting where the score function is produced algorithmically rather than learned from data.


The diffusion flavor that we use is that of Stochastic Localization (SL) \citep{eldan2013thin}. This probabilistic technique defines a continuous-time stochastic process of conditional distributions, progressively concentrating probability mass on relevant regions of the configuration space. SL has been shown to be equivalent to a denoising diffusion 
process \citep{montanari2023sampling, alaoui2021informationtheoretic}. In \citet{alaoui2024sampling} the SL process was employed to construct an algorithm uniformly sampling the solutions of the SK model at high temperature, using a drift term (related to the score) given by approximate message passing \citep{amp}. 

In this work, inspired by \cite{ghio2023sampling}, we leverage SL in combination to AMP to sample from the solution space of perceptron problems. In particular, we devise 
an analytic framework to assess the  performance of SL in the large system size limit. Notice that AMP is conjecturally optimal among polynomial algorithms in this denoising setting.

We first analyze SL-based sampling for spherical weights, then extend our investigation to the more challenging problem of binary perceptron solutions. We find that efficient sampling is feasible in a large region of the hyperparameters' space of the spherical model. In the binary case instead, uniform sampling from the solution space is always unfeasible. This result was expected since the typical solutions are isolated and therefore hard to find \citep{huang2014origin,subdominant}. Nevertheless, this limitation motivates the exploration of other statistical measures that may allow for efficient sampling.

The remainder of this paper is structured as follows: Section \ref{sec:SL_sampling}~ introduces the Stochastic Localization process.
Section \ref{sec:replica} presents an asymptotic analysis of SL using the replica formalism.
Section \ref{sec:perceptron_mod} discusses perceptron models and details the algorithmic implementation of SL-based sampling. The end of this section discusses alternative measures for sampling binary perceptron solutions.
In Section \ref{sec:conclusion}, we summarize and conclude the results.

\section{Stochastic Localization for Sampling}\label{sec:SL_sampling}

In this section, we introduce the key components of our sampling algorithm based on Stochastic Localization. We begin by presenting the fundamental idea behind SL and providing its mathematical formulation. 

Given a probability measure $\mu$ over $\mathbb{R}^N$, referred to as the \emph{target measure},
our objective is to generate samples from $\mu$. We assume $\mu$ to be a graphical model with a possibly hard-to-compute normalization factor, and write it as 
\begin{equation}
    \mu(\Wb) = \frac{\psi(\Wb)}{Z},      
\end{equation}
where the partition function $Z$ is the hard to compute normalization and $\psi(\Wb)$ is easy to evaluate.


Instead of directly sampling from the target measure, SL entails constructing a sequence of \emph{time-dependent tilted measures}, denoted as $\mu_{\mathbf{h},t}$, which evolves according to a stochastic process. Specifically, these measures take the form
\begin{align}
    \mu_{\mathbf{h},t}(d\Wb) := \frac{\psi(d\Wb)\, e^{\langle\mathbf{h},\Wb\rangle - \frac{t}{2} \lVert\Wb\rVert^2}}{Z_{\mathbf{h},t}},
\label{eq:tiltedmeasure}
\end{align}
where $t > 0$ represents time, $\mathbf{h} \in \mathbb{R}^N$ is an external field vector, and $Z_{\mathbf{h},t}$ is a normalization constant ensuring that $\mu_{\mathbf{h},t}$ remains a probability measure:
\begin{equation}\label{eq:norm}
    Z_{\mathbf{h},t} = \int\psi\left(d\Wb \right) e^{\langle\mathbf{h}(t),\Wb\rangle - \frac{t}{2} \lVert\Wb\rVert^2}.
\end{equation}

The field vector $\mathbf{h}(t)$ evolves dynamically according to the stochastic differential equation:
\begin{equation}
    \dd\mathbf{h}(t) = \mathbf{m}(\mathbf{h}(t),t)\,\dd t + \dd\mathbf{B}(t), \qquad \mathbf{h}(0) = 0,
    \label{eq:fieldupdate}
\end{equation}
where $(\mathbf{B}(t))_{t\geq0}$ is a standard Brownian motion, and the drift term $\mathbf{m}$ is given by the expectation of $\Wb$ under the tilted measure:
\begin{equation}
    \mathbf{m}(\mathbf{h},t) = \int_{\mathbb{R}^N} \Wb\,\mu_{\mathbf{h},t}(d\Wb).
    \label{eq:expectedvalue}
\end{equation}

The key advantage of considering the sequence of tilted measures $(\mu_{\hb(t),t})_{t\geq0}$ instead of directly sampling from $\mu$ lies in its probabilistic properties. Under mild conditions, the measure $\mu_{\mathbf{h}(t),t}$ localizes to a sample $\Wb^*$ from the target measure $\mu$ \citep{eldan2013thin}:
\begin{align*}
    \mu_{\mathbf{h}(t),t} \to \delta_{\Wb^*}, \ \ \Wb^* \sim \mu,  \quad \text{a.s. as } t \to \infty.
\end{align*}


A crucial aspect of the practical implementation of SL is the possibility of efficiently computing the drift term $\mathbf{m}(\mathbf{h},t)$ using Approximate Message Passing (AMP) \citep{donoho2009observed}. In the following sections, we introduce the replica formalism used to analyze the asymptotic properties of SL and evaluate its effectiveness when sampling from the solution space of perceptron models.



\section{Asymptotic Analysis of SL with the Replica Formalism}\label{sec:replica}

In this section, we investigate the asymptotic behavior of the SL sampling process in the system size limit $N \to \infty$. To analyze the feasibility of sampling from the target measure $\mu$, we leverage the well-established replica method (\citet{Mzard1986SpinGT}).

Given the target measure $\mu$, and the evolving tilted measure $\mu_{\mathbf{h},t}$ defined in \Eqref{eq:tiltedmeasure}, which we denote as $\mu_t$ for short, our primary quantity of interest is the alignment between a sample $\mathbf{W}_t$ drawn from $\mu_t$ at time $t$ and a reference sample $\mathbf{W}^*$ drawn from $\mu$. The overlap is expressed as:
\begin{equation}
    q(t) =  \frac{1}{N}\mathbb{E} \left[\langle \Wb^*, \Wb_t \rangle_t \right].
\end{equation}
By analyzing the behavior of $q$ as a function of time, we can assess whether the SL process successfully converges to samples from $\mu$.


The main object of study in the replica analysis is the partition function associated with the tilted measure, $Z_{X, \mathbf{h}, t}$. 
One can show that (see \citet{montanari2023sampling} for a rigorous derivation)
\begin{equation}
    \mathbf{h}\left(t\right) = t\Wb^{*} + \sqrt{t} \mathbf{g}, \qquad (\Wb^{*}, \mathbf{g}) \sim \mu \otimes \mathcal{N}(0,I_{N}).
    \label{eq:field}
\end{equation}
Our goal is to evaluate its asymptotic expectation over the patterns of the partition function defined in \eqref{eq:norm}:

\begin{equation}
    \phi_{t}  = \lim_{N\to\infty} \frac{1}{N} \mathbb{E} \log Z_{\mathbf{h}(t),t},
\end{equation}
where the expectation is over the field defined by \eqref{eq:field} and any other source of disorder, e.g. in the factor $\psi$.
To compute this limit, we employ the replica method twice, introducing $s$ replicas of the target measure weights and $n$ replicas of the tilted measure weights:\footnote{We use $\log Z=\lim_{n\to 0}n^{-1}\partial_n Z$ and $Z^{-1}=\lim_{s\to 0} Z^{s-1}$.}

\begin{align}
    \phi_{t} &= \lim_{N\to\infty} \frac{1}{N} \lim_{s\to0} \lim_{n\to0} \frac{1}{n}\partial_{n} \mathbb{E}\, \int \prod_{\alpha=1}^{s} \psi\left(d\Wb^{*}_\alpha\right) \prod_{a=1}^{n} \psi\left(d\Wb_a\right) e^{\langle\mathbf{h}(t),\Wb_a\rangle - \frac{t}{2} \lVert\Wb_a\rVert^2}.
\end{align}

For integer values of $n$ and $s$, before taking the zero limit, we can interpret the argument of the expectation as the partition function of a system of $n+s$ interacting replicas. The structure of this computation is closely related to the so-called teacher-student setting in replica theory, as introduced in the work of Franz and Parisi \citep{PhysRevLett.79.2486}. In this setting, the sample $\Wb$ (student) attempts to infer the reference weight vector $\Wb^*$ (teacher). 


To proceed, we assume the Replica Symmetric (RS) ansatz \citep{10.5555/1592967} and the Bayesian optimality conditions \citep{Nishimori_1980, Iba_1999}, which simplifies the analysis by assuming that all replicas are statistically equivalent. Under this hypothesis, the overlap of interest, at time $t$, is given by:
\begin{align}
    q_0(t) =\frac{1}{N} \mathbb{E} \left[\langle\Wb^* , \Wb_a \rangle \right] = \mathbb{E} \left[\langle \Wb_a, \Wb_b \rangle \right], \qquad \forall a, b \in [n] \text{ and } b\neq a.
\end{align}
with $\Wb_a$ samples from $\mu_t$ and $q_0$ an order parameter that enters the replica calculation as we will see.
The next section presents the explicit evaluation of this overlap and discusses the implications of the RS assumption.


\section{Application to Perceptron Models}\label{sec:perceptron_mod}

In this section, we first present the perception model \citep{Engel_Van_den_Broeck_2001}, and then we introduce the SL sampling algorithm, specifically designed for this simple one-layer neural network.

\subsection{The Perceptron Model}

The perceptron is the simplest instance of a one-layer neural network performing binary classification. Given a dataset 
\(X = \{\mathbf{X}^\mu, y^\mu\}_{\mu=1}^{M}\), where \(\mathbf{X}^\mu \in \mathbb{R}^N\) denotes the input patterns and \(y^\mu \in \{-1, +1\}\) represents the corresponding binary labels, classification problem is resolved by learning a weight vector \(\Wb \in \mathbb{R}^N\) that minimizes a given loss function. 

A common approach to training the perceptron is through margin-based optimization. We thus consider as target measure the flat one over probability of satisfying the constraint
\begin{equation}
    \psi(\Wb) = \prod_\mu\Theta\left(\kappa - y^\mu \frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}}\right),
\end{equation}
where \(\Theta(\cdot)\) is the Heaviside step function, $\Theta\left(x\right)=1$ if $x>0$ and $0$ otherwise, and $\kappa \in \mathbb{R}$ is the margin parameter, which controls the level of tolerance to classification errors. For \(\kappa = 0\) this loss simply counts the classification errors, whereas for \(\kappa < 0\) small-margin errors are admissible, leading to a non-convex solution space.

In this work, we focus on a prototypical setting where both the input features and the labels are randomly drawn from uniform distributions:  $y^\mu \sim \operatorname{Unif}(\{-1,+1\})$ and
    $\mathbf{X}^\mu \sim \operatorname{Unif}_N(\{-1,+1\})$.
    
This choice corresponds to a widely studied random perceptron model, which serves as a fundamental benchmark for analyzing the statistical properties of learning in high-dimensional settings \citep{baldassi2023typical, baldassi2020shaping}.

In this paper, we consider two perceptron models: the binary and the spherical perceptron. In the binary perception, the weights $\Wb$ are binary variables in $\left\{-1,+1\right\}^N$, while in the spherical perception, they belong to $\mathbb{R}^N$, but are normalized such that $\|\Wb\|^2=N$.

\subsection{Algorithmic Implementation}

In this section we present the Stochastic Localization sampling scheme used to explore the solution space of the perceptron problems. The SL algorithm is formally described in Algorithm~\ref{alg:The SL algorithm}. To estimate the expected values of the weights under the tilted measure, we employ the AMP algorithm, whose formulation is detailed in Algorithm~\ref{alg:AMP}. 

The AMP framework relies on the definition of two key functions (\(\phi_\mathrm{in}\) and  \(\phi_\mathrm{out}\)), commonly referred to as the input and output channels, given by:
\begin{align}
\phi_\mathrm{out}(y, \omega, V)  &= \log \left( \frac{1}{\sqrt{2\pi V}}\int dz \Theta(z) \exp \left(-\frac{(z - y \omega)^2}{2V}\right) \right),\\
\phi_\mathrm{in}(A, B) &=  \log\left( \int_{\mathbb{R}^2} dW\exp{(B W - \frac{1}{2} W^2 A)}\right),\\
g_\mathrm{out}(\omega, y, V) &= \partial_{\omega}\phi_\mathrm{out}(y, \omega, V).
\end{align}

For brevity, we have omitted the dataset indices from the notation. In the spherical perceptron, the weight vector \(\Wb\) is constrained to lie on the \(N\)-dimensional sphere, enforcing \(\|\Wb\|^2 = N\) dynamically during the AMP iterations. This constraint is enforced as

\begin{equation}\label{eq:spher_const}
A'_i := A_i + \Delta A_i, \quad \forall i \in \{1, \dots, N\},
     \qquad \sum_{i=1}^N \partial_{A_i} \phi_\mathrm{in}(A_i, B_i) \Big|_{A_i = A'_i} = N.
\end{equation}

For the binary perceptron model, the AMP equations remain structurally similar to those described above, with the primary distinction lying in the definition of \(\phi_\mathrm{in}\). 

\subsection{Spherical Non-Convex Perceptron}\label{sec:rep_spher}
In this section, we present the asymptotic analysis of the perceptron sampling scheme, focusing on the spherical perceptron model. As discussed in Section~\ref{sec:replica}, a key quantity for characterizing the sampling behavior is the free entropy, which allows us to determine the overlap \( q_0 \). Using the replica approach, the free entropy at time \( t \) can be expressed as a function of the order parameters:
\begin{align}
    \phi_t&(q_0,\hat{q}_0,r_0,\hat{r}_0) =  \frac{1}{2}\left(1-\hat{q}_{0}q_{0}\right)+\hat{r}_{0}r_{0}+\frac{1}{2}tq_{0} + G_S + \alpha G_E,\\
    G_S &= -\frac{1}{2}\log(\hat{q}_{0}+1)-\frac{1}{2(1+\hat{q}_{0})}\left(\hat{q}_{0}+2\hat{r}_{0}\frac{(\hat{q}_{0}-\hat{r}_{0})}{(\hat{r}_{0}+1)}+\frac{(\hat{q}_{0}-\hat{r}_{0})^{2}}{(\hat{r}_{0}+1)}\left(\frac{\hat{r}_{0}}{\hat{r}_{0}+1}+1\right)\right),\\
    G_{E}&=\int DzD\gamma\frac{H\left(-\frac{\gamma\sqrt{r_{0}}+z\sqrt{q_{0}-r_{0}}}{\sqrt{1-q_{0}}}\right)}{H\left(-\frac{\gamma\sqrt{r_{0}}}{\sqrt{1-r_{0}}}\right)}\log H\left(-\frac{z\sqrt{q_{0}-r_{0}}+\gamma\sqrt{r_{0}}}{\sqrt{1-q_{0}}}\right).
\end{align}
where \( r_0 = \langle\mathbf{W}_a^*, \mathbf{W}_b^* \rangle \) (for \( a\neq b \)) represents the overlap between two distinct teacher weight assignments, and \( \hat{q}_0 \), \( \hat{r}_0 \) are the corresponding dual overlap parameters. The optimization of $\phi_t$ is performed through the saddle point method, enabling us to determine the optimal value of the order parameters.

\begin{minipage}[t]{0.5\textwidth}
\begin{algorithm}[H]
\caption{\\ Approximate Message Passing (AMP)\\ on the spherical perceptron}\label{alg:AMP}
  \scriptsize
\begin{algorithmic}
    \State \textbf{Input:} Data: $\boldsymbol{F}
    \in \mathbb{R}^{N\times M}$, $\boldsymbol{y} \in \{-1,1\}^N$,\\ parameters: $\boldsymbol{A}_\mathrm{ext} \in \mathbb{R}^{N}$, $\boldsymbol{B}_\mathrm{ext} \in \mathbb{R}^{N}$, $\epsilon$, $T$\\
    
    \State \textit{Initialize:} $\boldsymbol{W}^0 \coloneqq \bar{0}_N$,\ $\boldsymbol{\Delta}^0 \coloneqq \bar{0}_N$,\ $\boldsymbol{g}_\mathrm{out}^0 \coloneqq \bar{0}_M$,\ $\boldsymbol{A} \coloneqq \bar{0}_N$,\ 
    $\boldsymbol{B} \coloneqq \bar{0}_N$, $\Delta_\mathrm{iter}=0$, \ $t=1$\\
    
    \State \textbf{repeat}\\
    
    \State Updating mean and variance estimates $\omega_{\mu}, \ V_{\mu}$
    \begin{align}
    V_{\mu}^t &\leftarrow \sum_{i}F^2_{\mu i}\Delta_{i}^{t-1}\\
    \omega_{\mu}^t &\leftarrow \sum_{i}F_{\mu i}W_{i}^{t-1} - V_{\mu}^t g_{\mathrm{out}, \mu}^{t-1}
    \end{align}
    
    \State Updating estimates $A_i, \ B_i, \ g_{\mathrm{out}, \mu}$
    \begin{align}
    g^t_{\mathrm{out}, \mu} &\leftarrow g_\mathrm{out}(\omega^t_{\mu}, y_{\mu}, V_{\mu}^t)\\
    A_{i}^{t} &\leftarrow  -\sum_{\mu}F^2_{\mu i}\partial_{\omega}g_\mathrm{out}(\omega_{\mu}^t, y_{\mu}, V_{\mu}^t)\\
    B_{i}^{t} &\leftarrow W_i^{t-1}A_{i}^{t} + \sum_{\mu}F_{\mu i}g_{\mathrm{out}, \mu}^t\\
    \end{align}

    \State Enforce spherical constraint\\
    \begin{equation}\label{eq:spher_const_algo}
        \boldsymbol{A}^t \leftarrow  (\boldsymbol{A}^t + \Delta \boldsymbol{A}^t)
    \end{equation}
    
    \State Updating marginals $W_i \text{ and} \ \Delta_i$
    \begin{align}
    W_i^t &\leftarrow \partial_{B_i} \phi_\mathrm{in} \biggl(A_{i}^{t}+(A^{t}_\mathrm{ext})_i \ , B_{i}^{t}+(B^{t}_\mathrm{ext})_i \biggr)\\
    \Delta_i^t &\leftarrow \partial^2_{B_i} \phi_\mathrm{in}\biggl(A_{i}^{t}+(A^{t}_\mathrm{ext})_i \ ,  (B^{t}_\mathrm{ext})_i\biggr)\\
    \end{align}
    
    \State $\Delta_\mathrm{iter} = \lVert \boldsymbol{W}^{t} - \boldsymbol{W}^{t-1} \rVert^2_2/N$\\
    \State $t \leftarrow t+1$\\
    
    \State \textbf{until} $t = T$ or  $\Delta_\mathrm{iter} < \epsilon$\\
    
    \Return \textbf{$\boldsymbol{W}$, $\boldsymbol{\Delta}$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hspace{1em}
\hfill
\begin{minipage}[t]{0.47\textwidth}
    \begin{algorithm}[H]
\caption{\\ Sampling a solution to the perceptron problem stochastic localization}\label{alg:The SL algorithm}
\scriptsize
\begin{algorithmic}
    \State \textbf{Input}: Data: $\boldsymbol{F}
\in \mathbb{R}^{N\times M}$, $\boldsymbol{y} \in \{-1,1\}^N$,\\ parameters: $\epsilon$,  $L$, $\delta$, $T$\\

\State \textit{Initialize the problem:}\\

$\boldsymbol{A}_{ext}^0 = \bar{0}_N, \ \boldsymbol{B}_{ext}^0 = \bar{0}_N, \ t = 0, \ iter = 1$\\
\hspace{1em}

\noindent \While {$l \le L$}

    \hspace{1em}
    \State AMP updates of the marginals:
    $$\Wb^{l}, \boldsymbol{\Delta}^{l} = \mathrm{AMP}(\boldsymbol{F}, \boldsymbol{y}, \boldsymbol{A}_\mathrm{ext}^{l-1}, \boldsymbol{B}_\mathrm{ext}^{l-1}, \ \epsilon , \ T) $$

    \State Update the fields using the Euler-Maruyama scheme:
    \begin{align}
    t^{l}  &\leftarrow t^{l-1} + \delta\\
    (A_\mathrm{ext}^{l})_i   &\leftarrow t\\
    (B_\mathrm{ext}^{l})_i & \leftarrow (B_\mathrm{ext}^{l-1})_i + W^{l-1}_i \delta + \sqrt{\delta}\ Z_i\\
    \text{ with } \boldsymbol{Z} &\sim \mathcal{N}(0, \mathbb{I}_N), \forall i \in \{1, \dots ,N\} \\
    l & \leftarrow l + 1
    \end{align}
\EndWhile
$$\Wb^{L}_i = \sign{(\Wb^{L}_i)}, \forall i \in \{1 \dots N\}$$
\Return{a solution sample $\Wb^{L}$}

\end{algorithmic}
\end{algorithm}
\end{minipage}

As shown in \citet{ghio2023sampling}, the behavior of the free entropy function \( \phi(q_0) \) provides direct insight into whether the SL sampling scheme (Algorithm~\ref{alg:The SL algorithm}) can recover samples from the target measure asymptotically. 

The right panel of Fig.~\ref{fig:free_entropy_spherical} shows the free entropy \( \phi(q_0) \) as a function of the overlap \( q_0 \) for different values of \( t \)\footnote{In the teacher-student scenario, the overlaps of the teacher weights are fixed in advance and depends only on $t$ and $\alpha$. Instead, $\hat{q}_0$ can be derived implicitly as a function of $q_0$. This idea implies that $\phi_t$ can be seen as a function of the only $q_0$.}, with parameters \( \alpha=278 \) and \( \kappa=-2.5 \). Initially, \( \phi(q_0) \) exhibits a single global maximum. However, a second local maximum emerges as \( t \) increases, eventually becoming the global optimizer of \( \phi \). This transition marks the onset of multimodality in the free entropy landscape. The ability of SL to correctly sample from the target measure hinges on the unimodality of \( \phi(q_0) \). If at any time \( t \), the free entropy remains unimodal, the algorithm successfully samples from the target measure. Conversely, once \( \phi(q_0) \) becomes multimodal, the tilted measure loses information about the target measure, making sampling unreliable. 


In general, sampling algorithms rely on two types of initialization strategies:  

\textit{Uninformed Initialization:}  No prior knowledge of the target measure is provided.

\textit{Informed Initialization:} Some prior information about the target measure is utilized.  

The left panel of Fig.~\ref{fig:free_entropy_spherical} presents the phase diagram for the SL scheme for a fixed margin $\kappa$, delineating the different sampling regimes.
\begin{figure}[h]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[scale=0.35]{Images/Phase_Diagram_Dyn_k=-2.5_old_1.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[scale=0.35]{Images/freeEntropy_q0_a=278.0_k=-2.5.pdf}
    \end{minipage}
    \caption{\textbf{Left:} Phase diagram of the SL scheme for the spherical perceptron with \( \kappa =-2.5 \) over the choices of constraint density $\alpha$ and time $t$.  
    \textbf{Green region:} \( \phi(q_0) \) has a single optimizer, meaning Algorithm~\ref{alg:The SL algorithm} correctly samples from the target measure.  
    \textbf{Yellow region:} \( \phi(q_0) \) has two optimizers, but the global maximum corresponds to the smaller overlap \( q_0 \). The SL algorithm still samples correctly.  
    \textbf{Red region:} \( \phi(q_0) \) has two optimizers, but the global maximum corresponds to a larger overlap \( q_0 \). In this case, Algorithm~\ref{alg:The SL algorithm} fails to sample from the target measure.  
    \textbf{Right:} Free entropy function \( \phi(q_0) \) for different values of \( t \). Initially, \( \phi(q_0) \) has a single maximum, but as \( t \) increases, a second maximum appears, eventually dominating.}
    \label{fig:free_entropy_spherical}
\end{figure}
 
Fig.~\ref{fig:sphericperc-alpha-vs-k-diagram} presents the phase transition line for SL applied to the spherical perceptron, in comparison with other phase transitions reported in \citet{baldassi2023typical}. Notably, the SL transition coincides with both the Dynamical Approximate Threshold (DAT) and the dynamical 1-step Replica Symmetry Breaking (1RSB) transition. 

This result highlights a fundamental limitation: the SL algorithm fails to sample from the target measure precisely when Replica Symmetry Breaking (RSB) occurs. In statistical physics, RSB is required when the solution space fragments into disconnected clusters. In other words, SL ceases to function as soon as ergodicity is broken.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/transition_lines_spherical.png}
    \caption{Phase transition delineating the samplable and non-samplable regions for SL in the spherical perceptron (red line). The blue line represents the dynamical transition of the spherical perceptron, as discussed in \citet{baldassi2023typical}.}
    \label{fig:sphericperc-alpha-vs-k-diagram}
\end{figure}
To further investigate the uniformity of the SL sampling scheme, we analyze the stability distribution \( P(\Delta) = P(\{\Delta^1, \dots, \Delta^M\}) \), where the stability vector is defined as \( \Delta^{\mu} = y^\mu\frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}}\) for \(\mu \in \left[M\right]\). 

Our results suggest that whenever SL successfully samples from the target measure, the obtained samples are uniformly distributed over the solution space. A detailed stability analysis is provided in the Supplementary Material.

\subsection{Binary Perceptron}\label{sec:BinaryPerceptron}
We analyze the samplability of the binary perceptron measure. We first examine the flat measure over solution, which fails to be samplable by SL at any $\alpha$. Then we consider a tilted measure introducing a potential function.
\subsubsection{Flat Measure}
Considering the flat measure over solutions, and performing the replica computation, we observe that the term \( G_E \) remains identical to those derived for the spherical perceptron. However, the entropic term undergoes a structural modification:
\begin{equation}    
G_S =  \sum_{W^{*}=\pm1}\int Dz\,D\gamma\ \frac{e^{\gamma\sqrt{\hat{r}_{0}}W^{*}}}{2\cosh(\gamma\sqrt{\hat{r}_{0}})}\log\left(2e^{\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})}\cosh\left(z\sqrt{\hat{q}_{0}-\hat{r}_{0}}+\gamma\sqrt{\hat{r}_{0}}+(\hat{q}_{0}-\hat{r}_{0})W^{*}\right)\right).
\end{equation}

Following the methodology outlined in Section~\ref{sec:rep_spher}, we conclude that sampling solutions from the flat binary perceptron measure using the SL algorithm, in the limit \( N\to\infty \), is intrinsically infeasible. This result is not surprising, as the asymptotic analysis performed via the replica method is known to accurately predict the behavior of the AMP algorithm for large system sizes \citep{rangan2011generalized}. As demonstrated in \citet{AMPlimitation}, due to the inherent stability of AMP, the exact evaluation of the expected values in \eqref{eq:expectedvalue} becomes impossible whenever the solution space is fragmented, as is the case in the frozen-1RSB phase and overlap gap property is observed \citep{OGP}. Thus, the failure mode of the SL sampling scheme can be directly attributed to the limitations of AMP. 

In the binary perceptron case a persistent double-peak structure is observed, but, unlike the spherical case, increasing the time parameter \( t \) does not eliminate the secondary peak (see right panel of Fig.~\ref{fig:bin_phase_tr}). Notably, the global optimizer corresponds to an overlap value of \( q_0 = 1 \), suggesting the presence of a spinodal transition. 

The fundamental difference between these two models lies in the statistical properties of their solution spaces. For sufficiently small values of \( \alpha \) the negative spherical perceptron can be fully described within the RS ansatz, implying a connected solution space. In contrast, the binary perceptron is characterized by a frozen 1RSB phase, where the solution space consists of disconnected clusters. This structural fragmentation directly impacts the effectiveness of the SL sampling scheme, ultimately rendering it unsuitable for this problem.


\begin{figure}[H]
\centering
           \includegraphics[width=0.49\textwidth]{Images/Phase_Diagram_Binary_k=-0.0.pdf}
            \includegraphics[width=0.49\textwidth]{Images/freeEntropy_q0_a=0.5_k=-0.0_2.png}
			\caption{Left: Phase transition of the SL algorithm for the binary perceptron problem. The unfeasible phase is represented by all the parameter domains. Right: The free entropy of the SL for the binary perceptron problem as a function of the overlap $q$. Double peak behavior is exhibited for all the values of $t$ with the global optimizer corresponding to the overlap $q_0=1$.}
            \label{fig:bin_phase_tr}
\end{figure}

\subsubsection{Cross-Entropy Potential}

\paragraph{Cross-entropy loss}

Due to the negative result for the uniform measure  presented in the last section, we consider a different measure where interpolation is not strictly enforced but it is obtained in the large $\beta$ and $\gamma$ limit of the following factor which contains the binary cross-entropy:

\begin{equation}
\psi(\Wb) = \prod_\mu e^{-\frac{\beta}{\gamma}\log\left( 1+e^{-2\gamma y^
\mu\frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}}}\right)}.
\label{eq:flat_measure}
\end{equation}

As studied in \cite{baldassi2020shaping} this loss allows for targeting dense regions of solutions (the high local entropy regions) which is controlled by the $\gamma > 0$ parameter. One may note that in the limit of $\beta,\gamma \to \infty$ the model is forcing the constraints $\frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}} > 0, \forall \mu \in [M]$ to be strictly satisfied, thus the solution to such a problem corresponds to a binary perceptron one. Our results indicate the possibility of sampling from this target measure with the SL scheme for a sufficiently low density constraint $\alpha$. The derivation of the free entropy for this model as well as the experimental results can be found in the Supplementary Material. This measure while sampleable is not entirely satisfying, since the necessity to set $\beta$ large enough to comply with the constraints greatly reduces the support of the measure. 

\paragraph{Tilted binary perceptron measure}

As a logical continuation of the previous experiments, we now keep the cross-entropy potential, but also use an hard constraint to enforce interpolation:
\begin{equation}
    \psi (\Wb) = \prod_\mu 
 \Theta\left(\kappa - y^\mu \frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}}\right) e^{-\frac{\beta}{\gamma}\log\left( 1+e^{-2\gamma\frac{\langle\mathbf{X}^\mu, \Wb \rangle}{\sqrt{N}}}\right)}.
 \label{eq:tilted_bin_measure}
\end{equation}
Despite introducing this tunable potential, our results indicate that similarly to the binary perceptron case the multimodality of the free entropy as a function of $q_0$ persists over all the parameter regimes, rendering the SL sampling impossible.



\section{Conclusion}\label{sec:conclusion}
In this work, we investigated the feasibility of sampling solutions to perceptron problems using SL. We analyzed both the spherical and binary perceptron models, leveraging statistical physics techniques to characterize the structure of their solution spaces and assess the effectiveness of the SL scheme. For the spherical perceptron, we demonstrated that SL is capable of sampling from the target measure as long as the free entropy landscape remains unimodal along the trajectory. The scheme is shown to be successful in the spherical perceptron case for the constraint density parameter $\alpha$ being within the replica symmetric regime. In the binary case, we show that the uniform measure is always unsamplable instead. On the other hand, measure not strictly enforcing interpolation but containing a crossentropy potential can be sampled. It is left for future work to devise an efficiently samplable measure that enforces all constraints for the binary perceptron model.

\newpage
\bibliographystyle{apalike}

\bibliography{bibliography}

\newpage

\appendix

\section{Spherical Perceptron}

\subsection{Stability distribution of the spherical perceptron solution}

In this section, we revise the computation of the stabilities distribution in the spherical perceptron problem. Given an $N$-dimensional weight vector $\Wb$, representing a solution of the perceptron problem with a margin $\kappa$, we consider a stability distribution $P(\Delta) = P(\{\Delta^1, \dots , \Delta^M\})$, where $\Delta$ represents the stabilities vector and $\Delta^{\mu}=y^\mu\frac{\langle \Wb,  \boldsymbol{X^{\mu}} \rangle}{\sqrt{N}}, \ 1 \le \mu \le M$.

Since averaging over $\boldsymbol{X^{\mu}}$ renders all the coordinates of $P(\Delta)$ equivalent, we will consider without loss of generality a stability distribution w.r.t the first input, i.e. $P(\Delta^1)$.

We define the probability of the stability to be equal $\Delta$, averaging over all the weight vectors representing the solutions to the problem and the inputs $\boldsymbol{X}$
\begin{align*}
	P(\Delta) = \llangle\dfrac{{\int d\mu(\Wb)\prod_{\mu=1}^{M} \Theta \biggl(\dfrac{y^\mu}{\sqrt{N}} \langle\Wb,\boldsymbol{X^{\mu}}\rangle - \kappa \biggr)} \delta \biggl( \dfrac{y^1}{\sqrt{N}}\langle\Wb,\boldsymbol{X^{1}}\rangle - \Delta \biggr)}{d\mu(\Wb)\prod_{\mu=1}^{M} \Theta \biggl(\dfrac{y^\mu}{\sqrt{N}}\langle\Wb ,\boldsymbol{X^{\mu}}\rangle - \kappa \biggr)}  \rrangle.
\end{align*}
Now we introduce replicas rewriting the denominator as $Z^{-1} = \lim_{n \to 0}Z^{n-1}$, subsequently taking the $n \to 0$ limit
\begin{align*}
	P(\Delta) = \lim_{n \to 0}\llangle{\int \prod_{a}d\mu(\Wb^a)\prod_{\mu, a} \Theta \biggl(\dfrac{y^\mu}{\sqrt{N}}\langle\Wb^a,\boldsymbol{X^{\mu}}\rangle - \kappa \biggr)} \delta \biggl( \dfrac{y^1}{\sqrt{N}}\langle\Wb^1,\boldsymbol{X^{1}}\rangle - \Delta \biggr)  \rrangle.
\end{align*}
Introducing the $\delta-$ and $\Theta-$ integrals we obtain
\begin{align*}
	\begin{split}
		P(\Delta) &= \Theta(\Delta - \kappa)\lim_{n \to 0}\int \prod_{a}d\mu(\Wb^a) \int \dfrac{d\hat{\lambda}_{1}^{1}}{2\pi} \int_{\kappa}^{\infty} \prod_{a>1} \dfrac{d\lambda_{1}^{a}}{2\pi} \int d\hat{\lambda}_1^{a}\\
		& \times \int_{\kappa}^{\infty} \prod_{a,\mu >1} \dfrac{d\lambda_\mu^{a}}{2\pi}\int \prod_{a,\mu>1}d \hat{\lambda}_{\mu}^{a} \exp\biggl(i\Delta \hat{\lambda}_{1}^{1} + i\sum_{a>1}\lambda_1^a\hat{\lambda}_{1}^{a} + i\sum_{\mu>1, a}\lambda_{\mu}^{a}\hat{\lambda}_{\mu}^{a} \biggr) \\
		& \times \llangle \exp \biggl( -\dfrac{i}{\sqrt{N}} \sum_{a, \mu} \hat{\lambda}_{\mu}^{a}\langle\Wb^a,\boldsymbol{X^{\mu}}\rangle \biggr)  \rrangle.
	\end{split}
\end{align*}
Averaging over $\boldsymbol{X^{\mu}}$ and introducing the following order parameters
\begin{align*}
	q_{ab} \equiv \frac{1}{N} \sum_i \Wb_i^a \Wb_i^b\,, \qquad k_{a} \equiv \frac{1}{N} \sum_i (\Wb_i^a)^2,
\end{align*}
\begin{align*}
	\begin{split}
		P(\Delta) &= \Theta(\Delta - \kappa)\lim_{n \to 0}\int \prod_a \dfrac{d\hat{k}^a}{4\pi} \int \prod_{a < b} \dfrac{dq^{ab}d\hat{q}^{ab}}{2\pi/N} \int \dfrac{d\hat{\lambda}_{1}^{1}}{2\pi} \int_{\kappa}^{\infty} \prod_{a>1} \dfrac{d\lambda_{1}^{a}}{2\pi} \int d\hat{\lambda}_1^{a}\\
		&\times \exp\biggl(i\Delta \hat{\lambda}_{1}^{1} + i\sum_{a>1}\lambda_1^a\hat{\lambda}_{1}^{a} -\dfrac{1}{2}\sum_{a}(\hat{\lambda}_{1}^a)^2 - \dfrac{1}{2}\sum_{a,b}\hat{\lambda}_{1}^{a}\hat{\lambda}_{1}^{b}q^{ab} \biggr) \\
		& \times \exp\biggl(\dfrac{iN}{2}\sum_{a}\hat{k}^a + iN\sum_{a < b}q^{ab}\hat{q}^{ab} + NG_{S}(\hat{k}^2, \hat{q}^{ab}) + (\alpha N - 1)G_{E}(q^{ab}) \biggr).
	\end{split}
\end{align*}
where the entropic and energetic parts are respectively
\begin{subequations}
	\begin{align*}
		G_{S}(\hat{k}^a, \hat{q}^{ab}) &= \ln \int \prod_a \dfrac{d\Wb^a}{\sqrt{2 \pi e}} \exp\biggl( -\frac{i}{2}\sum_a \hat{k}^a(\Wb^a)^2 - i\sum_{a < b}\hat{q}^{ab}\Wb^a\Wb^b\biggl), \\
		G_{E}(q^{ab}) &= \ln \int_{k}^{\infty} \prod_a \lambda^a \int \prod_{a} \frac{d\hat{\lambda}_a}{2\pi}\prod_a \exp(i\sum_a \lambda^a \hat{\lambda}^a - \dfrac{1}{2}\sum_a(\hat{\lambda}^a)^2 - \dfrac{1}{2}\sum_{a,b}\hat{\lambda}^a\hat{\lambda}^bq^{ab}).
	\end{align*}
\end{subequations}
Imposing replica symmetry assumption
\begin{align*}
	q^{ab} = q \hspace{3em} \hat{q}^{ab} = \hat{q} \hspace{3em} \hat{k}^{a} = \hat{k},
\end{align*}
And taking the $n\to 0$ limit we get
\begin{align*}
	P(\Delta) = \Theta(\Delta - \kappa)\dfrac{1}{\sqrt{2\pi(1-q)}}\int Dt \exp \biggl( - \dfrac{(\Delta - \sqrt{q}t)^2}{2(1-q)}\biggr) \biggl [ H\biggl( \dfrac{\kappa-\sqrt{q}t}{\sqrt{1-q}} \biggr) \biggr]^{-1}.
\end{align*}
Where $H(x)=\int_{x}^{\infty}\frac{dt}{\sqrt{2\pi}}\exp(-t^2/2)$.
The distribution of the stabilities is reported in Fig.\ref{fig:stabilities} for $\kappa = -2.1$ and several values of $\alpha \in [5, 20, 80]$.

\begin{figure}[H]
            \includegraphics[width=0.32\textwidth]{./Images/n_stabilities_N1001_a5.0_nk-2.1_tmax500.pdf}
            \includegraphics[width=0.32\textwidth]{./Images/n_stabilities_N1001_a20.0_nk-2.1_tmax500.pdf}
            \includegraphics[width=0.32\textwidth]{./Images/n_stabilities_N1001_a80.0_nk-2.1_tmax500.pdf}
			\caption{ Distribution of the stabilities $P(\Delta)=P(\{\Delta_1 \dots \Delta_{\lceil \alpha N\rceil} \})$ of a negative spherical perceptron where $\Delta^{\mu}=y^\mu\frac{\langle\Wb, \boldsymbol{X^{\mu}}\rangle}{\sqrt{N}}$. For number of variables $N=1001$, $\kappa=-2.1$ and  $\alpha=5$ (Left), $\alpha=20$ (Center) and $\alpha=80$ (Right). Empirical distribution of the stabilities of the SL samples (blue) coincides for different parameter regimes with the predicted distribution (black solid line). }
            \label{fig:stabilities}
\end{figure}


\subsection{Replica computation for Spherical Perceptron}\label{ap:rep_spheric_full}

In this section, the main steps of the replica computation are presented for the spherical perceptron sampling, where $\mathbf{X}^\mu \sim \mathcal{N}(\mathbf{0}, \mathbb{I}_n)$. The distribution $P(\mathbf{W}_{\alpha}^{*})=P(\mathbf{W}_{a})=\mathcal{N}(0,1)$. Thus, the expected value of the partition function is:
\begin{align*}
	\mathbb{E}\tilde{Z}_{t}&=\mathbb{E}_{X,\mathbf{g}}\int\prod_{\alpha=1}^{s}P(\dd\mathbf{W}_{\alpha}^{*})\prod_{a=1}^{n}P(\dd\mathbf{W}_{a})\prod_{\mu\alpha}\Theta\left(\sum_{i}\frac{W_{\alpha i}^{*}X_{i}^{\mu}}{\sqrt{N}}\right)\prod_{\mu a}\Theta\left(\sum_{i}\frac{W_{ai}X_{i}^{\mu}}{\sqrt{N}}\right)\times \\
	&\times
	e^{t\sum_{ai}W_{1i}^{*}W_{ai}+\sqrt{t}\sum_{ai}g_{i}W_{ai}-\frac{1}{2}t\sum_{ai}W_{ai}^{2}}
\end{align*}
by defining $\lambda_\alpha^\mu = \sum_{i}\frac{W_{\alpha i}^{*}X_{i}^{\mu}}{\sqrt{N}}$ and $u_a^\mu = \sum_{i}\frac{W_{a}X_{i}^{\mu}}{\sqrt{N}}$, and making explicit the change of variable we get
\begin{align*}
	\mathbb{E}\tilde{Z}_{t}&=\mathbb{E}_{X,\mathbf{g}}\,\int\prod_{\alpha=1}^{s}\dd\mathbf{W}_{\alpha}^{*}\prod_{a=1}^{n}\dd\mathbf{W}_{a}\prod_{\mu\alpha}\frac{d\lambda_{\alpha}^{\mu}d\hat{\lambda}_{\alpha}^{\mu}}{2\pi}\prod_{\mu a}\frac{du_{a}^{\mu}d\hat{u}_{a}^{\mu}}{2\pi}\ \prod_{\mu\alpha}\Theta\left(\lambda_{\alpha}^{\mu}\right)\prod_{\mu a}\Theta\left(u_{a}^{\mu}\right)\\
	&\times	e^{-i\sum_{\mu\alpha}\hat{\lambda}_{\alpha}^{\mu}\lambda_{\alpha}^{\mu}-i\sum_{\mu a}\hat{u}_{a}^{\mu}u_{a}^{\mu}+i\sum_{\mu\alpha i}\hat{\lambda}_{\alpha}^{\mu}\frac{W_{\alpha i}^{*}X_{i}^{\mu}}{\sqrt{N}}+i\sum_{\mu ai}\hat{u}_{a}^{\mu}\frac{W_{ai}X_{i}^{\mu}}{\sqrt{N}}+t\sum_{ai}W_{1i}^{*}W_{ai}+\sqrt{t}\sum_{ai}g_{i}W_{ai}-\frac{1}{2}t\sum_{ai}W_{ai}^{2}}
\end{align*}
Taking the average over $\mathbf{X^\mu}$ and $\mathbf{g}$, one obtains
\begin{align*}
	\mathbb{E}\tilde{Z}_{t}&=	\int\prod_{\alpha=1}^{s}\dd\mathbf{W}_{\alpha}^{*}\prod_{a=1}^{n}\dd\mathbf{W}_{a}\prod_{\mu\alpha}\frac{d\lambda_{\alpha}^{\mu}d\hat{\lambda}_{\alpha}^{\mu}}{2\pi}\prod_{\mu a}\frac{du_{a}^{\mu}d\hat{u}_{a}^{\mu}}{2\pi}\ \prod_{\mu\alpha}\Theta\left(\lambda_{\alpha}^{\mu}\right)\prod_{\mu a}\Theta\left(u_{a}^{\mu}\right)\\
	&\times	e^{-i\sum_{\mu\alpha}\hat{\lambda}_{\alpha}^{\mu}\lambda_{\alpha}^{\mu}-i\sum_{\mu a}\hat{u}_{a}^{\mu}u_{a}^{\mu}+t\sum_{ai}W_{1i}^{*}W_{ai}+\frac{1}{2}t\sum_{i}\sum_{ab}W_{ai}W_{bi}-\frac{1}{2}t\sum_{ai}W_{ai}^{2}}\\
	&\times	e^{-\frac{1}{2N}\sum_{\mu i}(\sum_{\alpha\beta}\hat{\lambda}_{\alpha}^{\mu}\hat{\lambda}_{\beta}^{\mu}W_{\alpha i}^{*}W_{\beta i}^{*}+\sum_{ab}\hat{u}_{a}^{\mu}\hat{u}_{b}^{\mu}W_{ai}W_{bi}+2\sum_{\alpha a}\hat{\lambda}_{\alpha}^{\mu}\hat{u}_{a}^{\mu}W_{\alpha i}^{*}W_{ai})}.
\end{align*}
We can now define the overlaps as
\begin{align*}
	q_{ab}	=\frac{1}{N}\sum_{i}W_{ai}W_{bi} \qquad
	r_{\alpha\beta}	=\frac{1}{N}\sum_{i}W_{\alpha i}^{*}W_{\beta i}^{*} \qquad p_{\alpha a}	=\frac{1}{N}\sum_{i}W_{\alpha i}^{*}W_{ai}
\end{align*}
By considering the overlaps and their corresponding Lagrange multipliers, $\hat{p},\hat{q}, \hat{r}$, we can proceed in the computation as follows
\begin{align*}
	\mathbb{E}\tilde{Z}_{t}&=	\int\prod_{\alpha=1}^{s}\dd\mathbf{W}_{\alpha}^{*}\prod_{a=1}^{n}\dd\mathbf{W}_{a}\prod_{\alpha\leq\beta}dr_{\alpha\beta}d\hat{r}_{\alpha\beta}\prod_{a\leq b}dq_{ab}d\hat{q}_{ab}\prod_{\alpha a}dp_{\alpha a}d\hat{p}_{\alpha a}\\
	&\times	e^{-N\frac{1}{2}\sum_{\alpha\beta}\hat{r}_{\alpha\beta}r_{\alpha\beta}-\frac{1}{2}N\sum_{ab}\hat{q}_{ab}q_{ab}-N\sum_{\alpha a}\hat{p}_{\alpha a}p_{\alpha a}}\\
	&\times	e^{+\frac{1}{2}\sum_{\alpha\beta}\hat{r}_{\alpha\beta}\sum_{i}W_{\alpha i}^{*}W_{\beta i}^{*}+\frac{1}{2}\sum_{ab}\hat{q}_{ab}\sum_{i}W_{ai}W_{bi}+\sum_{\alpha a}\hat{p}_{\alpha a}\sum_{i}W_{\alpha i}^{*}W_{ai}}\\
	&\times	\prod_{\mu\alpha}\frac{d\lambda_{\alpha}^{\mu}d\hat{\lambda}_{\alpha}^{\mu}}{2\pi}\prod_{\mu a}\frac{du_{a}^{\mu}d\hat{u}_{a}^{\mu}}{2\pi}\ \prod_{\mu\alpha}\Theta\left(\lambda_{\alpha}^{\mu}\right)\prod_{\mu a}\Theta\left(u_{a}^{\mu}\right)\\
	&\times	e^{-i\sum_{\mu\alpha}\hat{\lambda}_{\alpha}^{\mu}\lambda_{\alpha}^{\mu}-i\sum_{\mu a}\hat{u}_{a}^{\mu}u_{a}^{\mu}+tN\sum_{a}p_{1a}+\frac{1}{2}tN\sum_{ab}q_{ab}-\frac{t}{2}N\sum_{a}q_{aa}}\\
	&\times	e^{-\frac{1}{2}\sum_{\mu}(\sum_{\alpha\beta}\hat{\lambda}_{\alpha}^{\mu}\hat{\lambda}_{\beta}^{\mu}r_{\alpha\beta}+\sum_{ab}\hat{u}_{a}^{\mu}\hat{u}_{b}^{\mu}q_{ab}+2\sum_{\alpha a}\hat{\lambda}_{\alpha}^{\mu}\hat{u}_{a}^{\mu}p_{\alpha a})}
\end{align*}
Now everything is factorized over the sites $i$ and the factors $\mu$, so we obtain an expression amenable to saddle point evaluation
\begin{align*}
	\mathbb{E}\tilde{Z}_{t}=\int\prod_{\alpha\leq\beta}dr_{\alpha\beta}d\hat{r}_{\alpha\beta}\prod_{a\leq b}dq_{ab}d\hat{q}_{ab}\prod_{\alpha a}dp_{\alpha a}d\hat{p}_{\alpha a}\ e^{N\phi}
\end{align*}
where 
\begin{align*}
	\phi=G_I+G_{S}+\alpha G_{E}
\end{align*}
\begin{align*}
	G_I = -\frac{1}{2}\sum_{\alpha\beta}\hat{r}_{\alpha\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{q}_{ab}q_{ab}-\sum_{\alpha a}\hat{p}_{\alpha a}p_{\alpha a}+t\sum_{a}p_{1a}+\frac{1}{2}t\sum_{ab}q_{ab}-\frac{t}{2}\sum_{a}q_{aa}
\end{align*}
\begin{align*}
	G_{S}	=\log\int\prod_{\alpha=1}^{s}P(\dd W_{\alpha}^{*})\prod_{a=1}^{n}P(\dd W_{a})\ e^{+\frac{1}{2}\sum_{\alpha\beta}\hat{r}_{\alpha\beta}W_{\alpha}^{*}W_{\beta}^{*}+\frac{1}{2}\sum_{ab}\hat{q}_{ab}W_{a}W_{b}+\sum_{\alpha a}\hat{p}_{\alpha a}W_{\alpha}^{*}W_{a}}
\end{align*}
\begin{align*}
	G_{E}&=	\log\int\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}\Theta\left(\lambda_{\alpha}\right)\prod_{a}\Theta\left(u_{a}\right)\\
	&\times	e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}\lambda_{\alpha}-i\sum_{a}\hat{u}_{a}u_{a}-\frac{1}{2}\sum_{\alpha\beta}\hat{\lambda}_{\alpha}\hat{\lambda}_{\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{u}_{a}\hat{u}_{b}q_{ab}-\sum_{\alpha a}\hat{\lambda}_{\alpha}\hat{u}_{a}p_{\alpha a}}
\end{align*}

\subsubsection{ Replica Symmetric Ansatz}

To compute the expression of the free entropy, the RS ansatz is made:
\begin{align*}
	q_{ab}=\begin{cases}
		q_{d} & \text{if }a=b\\
		q_{0} & \text{if }a\neq b
	\end{cases} \qquad
	r_{\alpha\beta}=\begin{cases}
		r_{0} & \alpha\neq\beta\\
		r_{d} & \alpha=\beta
	\end{cases} \qquad
	p_{\alpha a}=\begin{cases}
		p^{*} & \alpha=1\\
		p & \alpha\neq1
	\end{cases}
\end{align*}
Plugging it in $\phi$ expression and carrying over the $n\to0$ and $s\to0$ limit. One can compute the analytic expression for: $G_{E},G_{S},G_{I}$. 

The parameters to optimize in this context are 12, for speeding up the computation of the saddle points, it is possible to notice that Nishimori conditions, and Bayesian Optimality of the order parameters \citep{Iba_1999, Nishimori_1980} apply.
\begin{align*}
	\mathbb{E}_{_{1},W_{2}} [f(W_{1},W_{2}) ]	&=\int dhdW_{1}dW_{2}\ f(W_{1},W_{2})P(W_{1}|h)P(W_{2}|h)P(h)\\
	\mathbb{E}_{W_{*},W_{1}}[f(W_{*},W_{1})]&=\int dW_{*}dW_{1}\ f(W_{*},W_{1})P(W_{1},W_{*})\\
	&=\int dhdW_{*}dW_{1}\ f(W_{*},W_{1})P(W_{1}|h)P(h|W_{*})P(W_{*})\\
	&=\int dhdW_{*}dW_{1}\ f(W_{*},W_{1})P(W_{1}|h)P(W_{*}|h)P(h)
\end{align*}
The Nishimori line is defined by the following equality.
\begin{align*}
	\mathbb{E}_{W_{*},W_{1}}[f(W_{*},W_{1})]=\mathbb{E}_{W_{1},W_{2}}[f(W_{1},W_{2})].
\end{align*}
We can use this result to understand order parameters relations, in particular using previous reasoning we can notice that
\begin{align*}
	\mathbb{E}_{W_\alpha^{*},W_a}[W_{\alpha i}^{*}W_{ai}]=\mathbb{E}_{W_a,W_b}[W_{a i}W_{bi}].
\end{align*}
leading to
\begin{align*}
	p_{1}	&=q_{0}
\end{align*}
This equality at first sight might seem unreasonable, but it hides a simple concept: in the Bayes optimal scenario, the overlap of student and teacher weight equals the student-student overlap. 
%Since all the students' weights are independent and identically distributed, we could expect that the performance of students is identical among themselves.

Using the same idea it is possible to show
\begin{align*}
\mathbb{E}_{W_\alpha^{*},W_\beta^{*}}[W_{\alpha i}^{*}W^*_{\beta i}]&=\mathbb{E}_{W_1,W_\beta^{*}}[W_{1 i}W_{\beta i}^{*}].
\end{align*}
leading to
\begin{align*}
	p_{0}	&=r_{0}
\end{align*}
A similar intuition concerning the overlap of students' weights can be applied to the overlap among teacher weights. Furthermore, it is useful to remember that the weights lie on a sphere, providing us with another equality:
\begin{align*}
	r_{d}	&=q_{d}
\end{align*}
Similar reasoning can be applied for the "dual" order parameters, i.e.
\begin{align*}
	\hat r_{d}	&= \hat q_{d} \\
	\hat p_{1}	&= \hat q_{0} \\
	\hat p_{0}	&= \hat r_{0}
\end{align*}
Let's first look at the entropic term, by using the RS ansatz we can rewrite the entropic term as:
\begin{align*}
	G_{S}&=	\log\int\prod_{\alpha=1}^{s}P(\dd W_{\alpha}^{*})\prod_{a=1}^{n}P(\dd W_{a})\ e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})\sum_{\alpha}W_{\alpha}^{*2}+\frac{1}{2}(\hat{r}_{0}-\hat{p_{0}})(\sum_{\alpha}W_{\alpha}^{*})^{2}}\\
	&\times	e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})\sum_{a}W_{a}^{2}+\frac{1}{2}(\hat{q}_{0}-\hat{p_{0}})(\sum_{a}W_{a})^{2}+(\hat{p_{1}-}\hat{p}_{0})W_{1}^{*}\sum_{a}W_{a}+\frac{\hat{p_{0}}}{2}(\sum_{\alpha}W_{\alpha}^{*}+\sum_{a}W_{a})^{2}}
\end{align*}
Substituting the Nishimori conditions in the previous expression
\begin{align*}
	&=	\log\int\prod_{\alpha=1}^{s}P(\dd W_{\alpha}^{*})\prod_{a=1}^{n}P(\dd W_{a})\ e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})\sum_{\alpha}W_{\alpha}^{*2}}\\
	&\times	e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})\sum_{a}W_{a}^{2}+\frac{1}{2}(\hat{q}_{0}-\hat{r}_{0})(\sum_{a}W_{a})^{2}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\sum_{a}W_{a}+\frac{\hat{r_{0}}}{2}(\sum_{\alpha}W_{\alpha}^{*}+\sum_{a}W_{a})^{2}}
\end{align*}
Using the Hubbard-Stratonovich transformation we can rewrite the entropic term as
\begin{align*}
	G_S &=\log\int\prod_{\alpha=1}^{s}P(\dd W_{\alpha}^{*})\prod_{a=1}^{n}P(\dd W_{a})DzD\gamma\ e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})\sum_{\alpha}W_{\alpha}^{*2}}\\
	&\times	e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})\sum_{a}W_{a}^{2}+z\sqrt{\hat{q}_{0}-\hat{r}_{0}}\sum_{a}W_{a}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\sum_{a}W_{a}+\gamma\sqrt{\hat{r}_{0}}(\sum_{\alpha}W_{\alpha}^{*}+\sum_{a}W_{a})}
\end{align*}
which can be rewritten as
\begin{align*}
	G_S=\log\int P(\dd W_{1}^{*})DzD\gamma\ e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})W_{1}^{*2}+\gamma\sqrt{\hat{p_{0}}}W_{1}^{*}}[\mathcal{Z}(W_{1}^{*},z,\gamma)]^{n}[\mathcal{Z^{*}}(\eta,\gamma)]^{s-1}
\end{align*}
where 
\begin{align*}
	\mathcal{Z}(W_{1}^{*},z,\gamma)&=\int P(\dd w)e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})W^{2}+\left(z\sqrt{\hat{q}_{0}-\hat{r}_{0}}+\gamma\sqrt{\hat{r}_{0}}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\right)W}=\\
	&=\frac{e^{-\frac{1}{2}\frac{\left(z\sqrt{\hat{q}_{0}-\hat{r}_{0}}+\gamma\sqrt{\hat{r}_{0}}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\right)^{2}}{(\hat{q}_{d}-\hat{q}_{0})}}}{\sqrt{\hat{q}_{0}-\hat{q}_{d}}}
\end{align*}
\begin{align*}
	\mathcal{Z}^{*}(z,\gamma)&=\int P(\dd W^{*})e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})W^{*}{}^{2}+\gamma\sqrt{\hat{r}_{0}}W^{*}}=\\
	&=\frac{e^{-\frac{1}{2}\frac{\gamma^{2}\hat{r}_{o}}{\hat{r}_{d}-\hat{r}_{0}}}}{\sqrt{\hat{r}_{0}-\hat{r}_{d}}}
\end{align*}
\begin{align*}
	\mathcal{A}(W_{1}^{*},\gamma)=e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})W_{1}^{*2}+\gamma\sqrt{\hat{p_{0}}}W_{1}^{*}}.
\end{align*}
Finally, by taking the limits for $n\to0$ and $s\to0$ in the same order we get
\begin{align*}
	G_{S}=\int P(\dd W_{1}^{*})DzD\gamma\ \frac{\mathcal{A}(W_{1}^{*},\gamma)}{\mathcal{Z^{*}}(\gamma)}\log\mathcal{Z}(W_{1}^{*},z,\gamma)
\end{align*}
It is possible to compute the result of the above integration explicitly, obtaining the result reported in the main text.

Let's now focus our attention on the energetic term
\begin{align*}
	G_{E}&=\log\int\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}\Theta\left(\lambda_{\alpha}\right)\prod_{a}\Theta\left(u_{a}\right)\\
	&\times e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}\lambda_{\alpha}-i\sum_{a}\hat{u}_{a}u_{a}-\frac{1}{2}\sum_{\alpha\beta}\hat{\lambda}_{\alpha}\hat{\lambda}_{\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{u}_{a}\hat{u}_{b}q_{ab}-\sum_{\alpha a}\hat{\lambda}_{\alpha}\hat{u}_{a}p_{\alpha a}}
\end{align*}
After inserting the Nishimori conditions and making some  manipulations
\begin{align*}
	G_{E}&=\log\int\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}\Theta\left(\lambda_{\alpha}\right)\prod_{a}\Theta\left(u_{a}\right)\\ 
	&\times e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}\lambda_{\alpha}-i\sum_{a}\hat{u}_{a}u_{a}-\frac{1}{2}(r_{d}-r_{0})\sum_{\alpha}\hat{\lambda}_{\alpha}^{2}+\frac{q_{0}-r_{0}}{2}\hat{\lambda}_{1}^{2}}\\\times&e^{-\frac{1}{2}(q_{d}-q_{0})\sum_{\alpha}\hat{u}_{\alpha}^{2}-\frac{r_{0}}{2}(\sum_{\alpha}\hat{\lambda}_{\alpha}+\sum_{a}\hat{u}_{a})^{2}-\frac{q_{0}-r_{0}}{2}\left(\hat{\lambda}_{1}+\sum_{a}\hat{u}_{a}\right)^{2}}
\end{align*}
Through Hubbard Stratonovich substitution, it gets
\begin{align*}
	G_E&=\log\int Dz\,D\gamma\,\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}\Theta\left(\lambda_{\alpha}\right)\prod_{a}\Theta\left(u_{a}\right)\\&\times e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}(\lambda_{\alpha}+\gamma\sqrt{p_{0}})-i\sum_{a}\hat{u}_{a}(u_{a}+\gamma\sqrt{p_{0}}+z\sqrt{q_{0}-r_{0}})-\frac{1}{2}(r_{d}-r_{0})\sum_{\alpha}\hat{\lambda}_{\alpha}^{2}+\frac{q_{0}-r_{0}}{2}\hat{\lambda}_{1}^{2}}\\&\times e^{-\frac{1}{2}(q_{d}-q_{0})\sum_{\alpha}\hat{u}_{\alpha}^{2}-iz\sqrt{q_{0}-r_{0}}\hat{\lambda}_{1}}
\end{align*}
Finally, taking the limit for $n\to0$ and $s\to0$ we get 
\begin{align*}
	G_{E}=	\int DzD\gamma\frac{H\left(-\frac{\gamma\sqrt{p_{0}}+\rho\sqrt{q_{0}-r_{0}}}{\sqrt{r_{d}-q_{0}}}\right)}{H\left(-\frac{\gamma\sqrt{p_{0}}}{\sqrt{r_{d}-r_{0}}}\right)}\log H\left(-\frac{z\sqrt{q_{0}-r_{0}}+\gamma\sqrt{p_{0}}}{\sqrt{q_{d}-q_{0}}}\right)
\end{align*}
Regarding the Interaction term 
\begin{align*}
	nG_{I}=&-\frac{1}{2}\sum_{\alpha\beta}\hat{r}_{\alpha\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{q}_{ab}q_{ab}-\sum_{\alpha a}\hat{p}_{\alpha a}p_{\alpha a}+t\sum_{a}p_{1a}+\frac{1}{2}t\sum_{ab}q_{ab}-\frac{t}{2}\sum_{a}q_{aa}
\end{align*}
by taking the limits $n\to 0$ and $s \to 0$, one is able to get
\begin{align*}
	G_{I}=&-\frac{1}{2}\hat{q}_{d}q_{d}-\frac{1}{2}\hat{q}_{0}q_{0}+\hat{r}_{0}r_{0}+\frac{1}{2}tq_{0}
\end{align*}

\section{Binary Perceptron }   
\subsection{Replica computation for Binary Perceptron Flat measure }\label{ap:rep_bin}

The replica computation for the binary perceptron involves the identical to \ref{ap:rep_spheric_full} steps adjusted to the fact that in this case $X_i^\mu  \sim \frac{\delta_1+\delta_{-1}}{2}$. The same also holds for the weights distribution $P(W_i^*)=P(W_i)=\frac{\delta_1+\delta_{-1}}{2}$. One can notice that this change does not affect the specific form of the energetic term $G_E$ and of the interaction term $G_I$, but only the one of the entropic term $G_S$. 
By following the same steps as before one obtains

\begin{align*}
    G_{S}=\int P(\dd W_{1}^{*})DzD\gamma\ \frac{\mathcal{A}(W_{1}^{*},\gamma)}{\mathcal{Z^{*}}(\gamma)}\log\mathcal{Z}(W_{1}^{*},z,\gamma)
\end{align*}

where

\begin{align*}
    \mathcal{Z}(W_{1}^{*},z,\gamma)	&=\int P(\dd w)e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})w^{2}+\left(z\sqrt{\hat{q}_{0}-\hat{r}_{0}}+\gamma\sqrt{\hat{r}_{0}}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\right)w}\times\\
    &=	e^{+\frac{1}{2}(\hat{q}_{d}-\hat{q}_{0})}\cosh\left(z\sqrt{\hat{q}_{0}-\hat{r}_{0}}+\gamma\sqrt{\hat{r}_{0}}+(\hat{q}_{0}-\hat{r}_{0})W_{1}^{*}\right)\\
    \mathcal{Z}^{*}(z,\gamma)&=\int P(\dd w^{*})e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})w^{*}{}^{2}+\gamma\sqrt{\hat{r}_{0}}w^{*}}\\
    &=e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})}\cosh(\gamma\sqrt{\hat{r}_{0}})\\
    \mathcal{A}(W_{1}^{*},\gamma)&=e^{+\frac{1}{2}(\hat{r}_{d}-\hat{r}_{0})W_{1}^{*2}+\gamma\sqrt{\hat{p_{0}}}W_{1}^{*}}
\end{align*}


\subsection{Experimental results for sampling from the cross-entropy loss measure}

\begin{figure}[H]
\centering
           \includegraphics[scale=0.48]{Images/CEmodel_gamma2_beta20.pdf}
           \caption{The probability of sampling a solution satisfying the binary perceptron constraints with the SL algorithm as a function of the constraint density $\alpha$ from a measure with the cross-entropy potential with $\beta=10$ and $\gamma=2$. The points represent the mean (with the corresponding standard deviation) obtained using the parallel 100 runs of the experiment.}
            \label{fig:CE_probsucc}

\end{figure}

\subsection{Replica computation for the cross-entropy loss measure}

In this appendix, we present the replica computation for the cross-entropy loss measure given in \eqref{eq:flat_measure}. It is important to note that the entropic term, $G_S$, remains unchanged and coincides with the one reported in Appendix \ref{ap:rep_bin}. The only term affected by the presence of the potential is the energetic term, $G_E$, which we compute as
\begin{align*}
    G_{E} &= \log\int\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}e^{-\beta V\left(\lambda_{\alpha}\right)}\prod_{a}e^{-\beta V\left(u_{a}\right)}\\
&\quad \times e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}\lambda_{\alpha}-i\sum_{a}\hat{u}_{a}u_{a}-\frac{1}{2}\sum_{\alpha\beta}\hat{\lambda}_{\alpha}\hat{\lambda}_{\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{u}_{a}\hat{u}_{b}q_{ab}-\sum_{\alpha a}\hat{\lambda}_{\alpha}\hat{u}_{a}p_{\alpha a}},
\end{align*}
where $V(x) = \frac{1}{\gamma}\log\left( 1+e^{-\gamma\left(x \right)}\right)$. 
Following the same steps as in the computation for the spherical perceptron, we obtain:
\begin{align*}
    G_{E}&=\int DzD\gamma\frac{\hat{H}\left(-\frac{\gamma\sqrt{r_{0}}+z\sqrt{q_{0}-r_{0}}}{\sqrt{1-q_{0}}}\right)}{\hat{H}\left(-\frac{\gamma\sqrt{r_{0}}}{\sqrt{1-r_{0}}}\right)}\log \hat{H}\left(-\frac{z\sqrt{q_{0}-r_{0}}+\gamma\sqrt{r_{0}}}{\sqrt{1-q_{0}}}\right),
\end{align*}
where 
\begin{align*}
    \hat{H}(x) = \int_{-\infty}^\infty Dy \, e^{-\beta V\left(y - x\right)}.
\end{align*}



\subsection{Replica computation for Binary perception tunable measure.}

In this appendix, we present the replica computation for the tunable measure given in \eqref{eq:tilted_bin_measure}. The computation follows the steps performed for the cross-entropy loss measure, adding a $\Theta$-function:


\begin{align*}
    G_{E} &= \log\int\prod_{\alpha}\frac{d\lambda_{\alpha}d\hat{\lambda}_{\alpha}}{2\pi}\prod_{a}\frac{du_{a}d\hat{u}_{a}}{2\pi}\ \prod_{\alpha}\Theta\left(\lambda_{\alpha}\right)e^{-\beta V\left(\lambda_{\alpha}\right)}\prod_{a}\Theta\left(u_{a}\right)e^{-\beta V\left(u_{a}\right)}\\
&\quad \times e^{-i\sum_{\alpha}\hat{\lambda}_{\alpha}\lambda_{\alpha}-i\sum_{a}\hat{u}_{a}u_{a}-\frac{1}{2}\sum_{\alpha\beta}\hat{\lambda}_{\alpha}\hat{\lambda}_{\beta}r_{\alpha\beta}-\frac{1}{2}\sum_{ab}\hat{u}_{a}\hat{u}_{b}q_{ab}-\sum_{\alpha a}\hat{\lambda}_{\alpha}\hat{u}_{a}p_{\alpha a}},
\end{align*}
where $V(x) = \frac{1}{\gamma}\log\left( 1+e^{-\gamma\left(x \right)}\right)$.
Following the same steps as in the computation for the spherical perceptron, we obtain:

\begin{align*}
    G_{E}&=\int DzD\gamma\frac{\tilde{H}\left(-\frac{\gamma\sqrt{r_{0}}+z\sqrt{q_{0}-r_{0}}}{\sqrt{1-q_{0}}}\right)}{\tilde{H}\left(-\frac{\gamma\sqrt{r_{0}}}{\sqrt{1-r_{0}}}\right)}\log \tilde{H}\left(-\frac{z\sqrt{q_{0}-r_{0}}+\gamma\sqrt{r_{0}}}{\sqrt{1-q_{0}}}\right),
\end{align*}
where 
\begin{align*}
    \tilde{H}(x) = \int_x^\infty Dy \, e^{-\beta V\left(y - x\right)}
\end{align*}
is a generalized version of the standard complementary error function.


\end{document}

