\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[final]{acl}
\usepackage{makecell}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow,tabularx,booktabs}
\usepackage[T1]{fontenc}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage{flushend}
\usepackage{microtype}
\usepackage{pifont}
\usepackage[most]{tcolorbox}  %
\usepackage{xcolor}
\usepackage{inconsolata}
\usepackage{float}
\newcommand{\rw}[1]{\textcolor{red}{[rw: #1]}}

\newcommand{\xj}[1]{\textcolor{orange}{[xj: #1]}}

\input{preamble}


\newcommand{\usepalatino}[1]{{\fontfamily{ppl}\selectfont #1}}


\title{\emph{Nuclear Deployed:} Analyzing Catastrophic Risks in \\Decision-making of Autonomous LLM Agents\\
\fontsize{8}{8}\selectfont
\usepalatino{\textcolor{red}{Ethical Disclaimer: May Contain Operations on Autonomous Decision-making Related to CBRN Risks}}}

\author{Rongwu Xu\textsuperscript{13*}\quad Xiaojian Li\textsuperscript{23*}\quad Shuo Chen\textsuperscript{1*} \quad Wei Xu\textsuperscript{123} \\
\textsuperscript{1}IIIS, Tsinghua University\quad \textsuperscript{2}CollegeAI, Tsinghua University \\ \textsuperscript{3}Shanghai Qi Zhi Institute \\
\texttt{\{xrw22@mails.,weixu@\}tsinghua.edu.cn}, \texttt{xiaojian\_li@berkeley.edu}\\
\href{https://llm-catastrophic-risks.github.io/}{\Mundus~Project Page}\quad\href{https://github.com/pillowsofwind/LLM-CBRN-Risks}{\faGithub~Code}}

\begin{document}
\maketitle

\def\thefootnote{*}\footnotetext{Co-first authors, see \autoref{sec:author-contributions} for contributions.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. 
We also show that these agents can violate instructions and superior commands.
On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. 
We release our code to foster further research.

\end{abstract}



\input{Secs/intro}
\input{Secs/method}
\input{Secs/experiment}
\input{Secs/sub-experiments}
\input{Secs/related}
\input{Secs/conclusion}


\section{Author Contributions}
\label{sec:author-contributions}

\textbf{Rongwu Xu}: Initiated the project, overseeing the conceptualization and refinement of ideas. Designed and conducted the initial version of the experiments, and contributed to manuscript writing.

\noindent\textbf{Xiaojian Li}: Contributed to the experimental design, conducted the refined version of the experiments, performed data analysis, and contributed to manuscript writing.

\noindent\textbf{Shuo Chen}: Contributed to the execution and updates of the refined experiments, provided technical assistance and was responsible for results visualization.

\noindent\textbf{Wei Xu}: Provided overall guidance on experimental design, methodology, and manuscript writing as the mentor of other authors and contributed valuable ideas throughout the project.

\section*{Limitations}


While our study provides valuable insights into understanding the catastrophic CBRN risks of autonomous LLM agents, it has some limitations that open avenues for future improvements.

First, our evaluation is based on simulated environments, which, while carefully designed, may not fully capture the complexities of real-world decision-making. Although there are gaps between simulations and real-world scenarios, this approach still offers valuable insights into agent behavior, as highlighted by recent studies~\citep{zhou2024real, scheurer2024large}. However, it is important to note that the conclusions drawn may not directly apply to real-world agent deployments. This limitation is common in AI agent safety research~\citep{meinke2024frontier}, and future work could aim to extend the study to more realistic settings.

Second, the scope of our study is limited to a specific set of CBRN scenarios. While these scenarios are representative of both high-stakes decision-making and CBRN elements, they do not cover all possible use cases where similar risks might emerge. Future research should explore a broader range of contexts to generalize findings.

Third, our evaluation assumes that model outputs provide direct indicators of agent behavior, but real-world applications often involve additional external factors that influence outcomes. Expanding the evaluation framework to incorporate more dynamic and interactive elements could enhance its robustness.

Lastly, while our study identifies risks, it does not propose specific mitigation strategies. Future efforts should focus on developing intervention mechanisms that minimize harm while preserving the capabilities of LLM agents. 

By addressing these limitations, we aim to refine our understanding of AI risks and contribute to the ongoing discourse on autonomous decision-making safety. Future research should focus on robust intervention mechanisms, improved interpretability, and ethical guidelines to ensure safe AI deployment. Finally, interdisciplinary collaboration, contributing to a more realistic evaluation in real-world scenarios, will be key to mitigating risks throughout the AI R\&D process.


\section*{Ethics Statement}


Our study strictly follows the ACL Ethics Policy. Firstly, \textbf{we affirm that our study does not involve any real-world military or laboratory applications or collaborations.} Our research strictly focuses on understanding the decision-making dynamics of autonomous LLM agents in high-stakes scenarios using simulated environments. No real-world CBRN-related data, military strategies, or classified information were utilized or referenced.

Secondly, \textbf{our study does not implicate real-world names, locations, or entities with identifiable or meaningful associations.} All scenarios are purely fictional, ensuring no resemblance to real-world places, individuals, or countries. This keeps the focus on the theoretical aspects of decision-making dynamics without any real-world implications.

Thirdly, \textbf{our study does not promote or encourage harmful actions, violence, or unethical behavior.} The AI agents used in this research operate exclusively within a controlled, simulated environment that is designed for academic exploration. All actions and decisions made by these agents are hypothetical and have no real-world consequences. 

Fourthly, \textbf{our simulation does not aim to replicate, model, or predict real-world geopolitical situations or military strategies.} The scenarios are designed solely to explore decision-making dynamics within a high-stakes context. They are highly abstract and are not intended to influence or reflect actual real-world decision-making.

Fifthly, while we will release the code for reproducibility in an upon-request manner, the agent rollouts are entirely simulated and not reflective of real-world scenarios. Therefore, the open-source materials are intended solely for research purposes and carry no inherent risk. Nonetheless, \textbf{we only distribute these materials with clear guidelines and disclaimers, ensuring that they are used in a responsible and ethical manner.}

Lastly, while our findings expose potential risks associated with autonomous LLMs, particularly in their ability to engage in catastrophic behaviors and deception, we emphasize the importance of proactive defense measures. To mitigate these risks, we advocate for:
\begin{itemize}
    \item Comprehensive pre-deployment safety evaluations of LLM-based autonomous agents.
    \item The development of alternative control mechanisms beyond natural language constraints to enhance robustness.
    \item Ethical guidelines and policy frameworks ensuring that LLM agents adhere to principles of harmlessness, honesty, and transparency.
    \item Increased collaboration between researchers, policymakers, and industry stakeholders to address emerging AI safety concerns.
\end{itemize}

By emphasizing transparency and responsible AI deployment, we aim to contribute to the safe and ethical advancement of autonomous AI systems.















\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\clearpage
\appendix

\input{Secs/Appendix/discussion}
\clearpage
\input{Secs/Appendix/method-info}
\clearpage
\input{Secs/Appendix/experimental-details}
\clearpage
\input{Secs/Appendix/extended-experiments}
\clearpage
\input{Secs/Appendix/analysis}
\clearpage
\input{chatbox}
\input{Secs/Appendix/transcripts}

\end{document}
