\section{Main Experiments}
\label{sec:exp}

\begin{figure*}[tb]
    \centering
    \begin{minipage}[b]{0.65\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/main_result/catastrophic.pdf}
        \subcaption{\label{fig:catastrphic}Main results of catastrophic behavior}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.335\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/main_result/catastrophic_reasoning_scatter.pdf}
        \subcaption{\label{fig:cata_distribution}Catastrophic behavior vs. reasoning}
    \end{minipage}
    \caption{\textit{Main results of catastrophic behavior simulations across 12 LLMs.} \textbf{(a)} Main results of catastrophic behavior across four scenarios. Bar charts plot the risk rate of $A_{\text{2.D}}$ while line charts plot the average number of rounds for simulations that eventually end up with $A_{\text{2.D}}$. \textbf{(b)} Relationship between LLM's tendency for catastrophic behavior and reasoning ability. \textbf{(Top)} Overall risk rate. \textbf{(Bottom)} Risk rate for those with permission checks.}
    \vspace{-1em}
\end{figure*}


\begin{figure*}[tb]
    \centering
    \begin{minipage}[b]{0.65\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/main_result/deception.pdf}
        \subcaption{\label{fig:deception}Main results of deception}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.335\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/main_result/deception_reasoning_scatter.pdf}
        \subcaption{\label{fig:deception_distribution}Deception vs. reasoning}
    \end{minipage}
    \caption{\textit{Main results of deception simulations across 12 LLMs.} \textbf{(a)} Main results of deception across four scenarios. Bar charts plot the distribution of $A_{\text{5.A-D}}$ while line charts plot the deception rate, \ie, the summed frequency of deception behaviors $A_{\text{5.B-D}}$. \textbf{(b)} Relationship between LLM's tendency for deception and reasoning ability. \textbf{(Top)} Overall deception rate. \textbf{(Bottom)} False accusation ($A_{\text{5.D}}$) rate.}
    \vspace{-1em}
\end{figure*}




\subsection{Experiment Setup}
\label{subsec:exp-setup}
\noindent\textbf{Configuration Details.} 
(1) \emph{Catastrophic Behavior:} We conduct 100 simulations (with each simulation conducted up to 30 rounds) per scenario and model, totaling 4,800\footnote{\ensuremath{100 \times 12~(\#\text{models})\times 4~(\#\text{scenarios}) = 4800}.} rollouts.  
Effectiveness and consequence are set to default descriptions. Task importance is inherently embedded in the assigned task.
For negativity, we set negprob $p = 0.75$ to simulate a moderate-to-high stress level.
(2) \emph{Deception:} We run simulations multiple times per qualifying sample from the catastrophic behavior simulation, ensuring over 200 simulations per scenario, resulting over 9,600 rollouts.  
Consequence is set to high (\ie, military blockade in \war{} and international trade embargoes in \lab{}) to amplify HHH trade-offs.
Inquiring party is set to Superior.

\noindent\textbf{Evaluated Models.}
For \auto{}, we select 12 LLMs, in different sizes and from different manufacturers, including 7 closed commercial models---Claude-3.5-Sonnet~\citep{anthropic2023claude}, GPT-4-Turbo~\citep{achiam2023gpt}, GPT-4o~\citep{hurst2024gpt}, GPT-4o-mini, o1$^\dag$\footnote{$\dag$ denotes o1-like model that uses long Chain-of-Thought (CoT) outputs and exhibits strong reasoning performance.}~\citep{jaech2024openai}, o1-mini$^\dag$ and o3-mini$^\dag$~\citep{openai2024o3mini}, and 5 open-source ones, which are QwQ-32B-Preview$^\dag$~\citep{team2024qwq}, Qwen2.5-7B/32B/72B-Instruct~\citep{yang2024qwen2} and Llama3.3-70B-Instruct~\citep{meta2023llama}. 
To balance performance, efficiency and budget, \state{} is fixed to GPT-4o-mini across all simulations. We use sampling decoding~\citep{holtzman2020curious} with top-$p = 0.9$ and temperature $\tau = 0.9$ to ensure diverse outputs from both agents. The max completion tokens per model call are set to $4096$.

\noindent\textbf{Reported Metrics.}
(1) \emph{Catastrophic Behavior:}  
We report the \emph{Risk Rate}, the frequency of simulations where \auto{} ends up with catastrophic behavior ($A_{\text{2.D}}$). This is analyzed in two cases: (i) without prior $A_{\text{3.B}}$, indicating catastrophic risks without seeking its supervisor's approval, and (ii) with prior $A_{\text{3.B}}$, suggesting deliberate \emph{disobedience} (since $A_{\text{2.D}}$ is always rejected). 
We also report the \emph{Average Number of Rounds} until $A_{\text{2.D}}$ occurs, assessing stress tolerance in goal balancing (higher values indicate greater tolerance).  
(2) \emph{Deception:}  
We report the \emph{Deception Rate}, the proportion of cases where \auto{} deceives humans by choosing $A_{\text{5.B-D}}$.  
We also report the \emph{Distribution of Responses}, showing the distribution across responses $A_{\text{5.A-D}}$.

Following the practices of \citet{abs-2402-01908} and \citet{rivera2024escalation}, we report \emph{$95\%$-confidence-intervals (CIs)}~\citep{greenland2016statistical}, obtained via bootstrapping over 1,000 samples.
More details on our setup are given in \autoref{sec:experimental-details}.

\subsection{Main Results and Findings}
\label{subsec:resultsAndFingds}
\noindent \textbf{Catastrophic Behavior.}
\autoref{fig:catastrphic} summarizes catastrophic behavior simulations across four scenarios and various LLM agents. Most LLMs exhibit catastrophic behavior, with risk rates ranging from $13\%$ to $99\%$, showing statistically significant performance differences. This highlights an alarming finding: \emph{LLMs frequently neglect harmlessness even when their actions are extremely harmful.} Notably, Claude-3.5-Sonnet consistently refuses to act in \war{}. In cases where LLM exhibits catastrophic behavior, the proportion of simulations that involved permission checks ranges from $0\%$ to $80\%$. For stress tolerance, we find $71.8\%$ of catastrophic risk decisions are made within $10$ rounds. No statistically significant correlation is found between risk rate and number of rounds in decision-making. 

We observe discrepancies in LLM behavior between \approach{} and \avoidance{} motivations in \war{}. LLMs generally show a \emph{higher risk rate under \avoidance{} than \approach{}}, except for Qwen2.5-7B-Instruct. We speculate this is because in \avoidance{}, catastrophic behavior is seen as ``justified'' to avoid losses, while in \approach{}, the goal is to pursue benefits. No significant difference is found in \lab{}. Due to the differences between \war{} and \lab{}, the results are not directly comparable.


\noindent\textbf{Catastrophic behavior and reasoning.} \autoref{fig:cata_distribution} further plots the relationship between LLM's reasoning ability and its tendency for catastrophic behavior. Reasoning ability is assessed using the ``Reasoning Average'' scores from LiveBench~\citep{white2024livebench}, an uncontaminated benchmark for up-to-date LLM evaluation. 
For non-o1-like models, the risk rate inversely correlates with reasoning ability, meaning stronger models tend to be safer, which aligns with our expectations. 
Unfortunately, this trend breaks down with the latest o1-like models with even stronger reasoning performance, which exhibit abnormally high risk rates. Additionally, from \autoref{fig:cata_distribution} (Bottom), we observe that stronger reasoning ability actually increases the likelihood of disobedience to supervisor approval. This is another clue that argues \emph{stronger models may actually be less safe in decision-making under HHH trade-offs}.
In particular, o1-like models are especially prone to neglecting harmlessness.

\noindent \textbf{Deception.}  
\autoref{fig:deception} presents the results of deception simulations. We find \emph{considerably amount of LLMs would engage in deception}. Out of all cases, $91.3\%$ exhibit deception behavior, with deception rates ranging from $1\%$ to $99\%$. LLMs show varying distributions of deception actions \(A_{\text{5.B-D}}\). In over $76.1\%$ of cases, LLMs display all three deception behaviors at once within the 100 simulation runs. More than $60.9\%$ of the time, we find the model prefers the false accusation (\(A_{\text{5.D}}\)) approach, making it the most common form of deception. Shockingly, this suggests that \emph{once LLMs decide to deceive, they tend to choose the more harmful but more effective methods like \(A_{\text{5.D}}\)}, neglecting both honesty and harmlessness simultaneously.

On motivations, \emph{LLMs' deception rate is generally higher in \approach{} than in \avoidance{}}, averaging $40.10\%$ and $19.82\%$ in \war{} and $53.87\%$ and $41.29\%$ in \lab{}. This can be explained for similar reasons as in the catastrophic behavior simulation.

\noindent\textbf{Deception and reasoning.} 
\autoref{fig:deception_distribution} shows the relationship between reasoning and deception. We report both the overall deception rate and the false accusation ($A_{\text{5.D}}$) rate. The results suggest that \emph{both overall deception and false accusation rates increase with reasoning ability}, aligning with findings from \citet{meinke2024frontier}. This reinforces potential concerns that models with stronger abilities are more prone to dishonesty in HHH trade-offs. Combined with our findings on catastrophic behavior, we conclude that \emph{stronger reasoning models are alarming, as they may adopt catastrophic risky strategies that harm others to achieve their objectives and try to cover up by deception.}
   
