\section{Conclusion}
\label{sec:conclusion}

In this study, we empirically prove the existential catastrophic risks of autonomous LLM agents. Through 14,400 rollouts across 12 SOTA LLMs on our three-stage evaluation framework, we found that these models can engage in catastrophic behaviors and deception, even without malicious prompt or external coercion. Our results reveal a paradox: stronger reasoning capabilities in LLMs may not reduce risks and could even amplify them. We also emphasize the risks of violating instructions and human commands, underscoring the need for rigorous pre-deployment testing and alternative control frameworks to ensure the responsible deployment of autonomous LLM agents.






