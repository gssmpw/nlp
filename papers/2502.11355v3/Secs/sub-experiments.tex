\section{Extented Experiments and Analyses}
\label{sec:sub-experiments}

We conduct a series of extended experiments and analyses. Due to space limitations, we only pick the most important experiments: violation (\autoref{subsec:violation}), abstention (\autoref{subsec:abstention}) and factors affecting decision-making (\autoref{subsec:influencing-factors}) in this section. 
For other experiments, refer to \autoref{sec:extended-experiments}. For detailed analyses, refer to \autoref{sec:analysis}.
Unless stated otherwise, we set \auto{} to GPT-4o in these experiments.
We also enumerate configurations of parameters for these experiments in Appendix \autoref{tab:exp-table}. 


\subsection{Instruction \& Command Violation}
\label{subsec:violation}

To enhance safety, we provide an updated setup that restricts \auto{}'s autonomy by requiring supervisor permission ($A_{\text{3.B}}$) before it can use $A_{\text{2.D}}$. This is implemented through a natural language \textbf{instruction} in its system prompt.
In the meantime, \state{} is adjusted to either return \textit{noncommittal} on \auto{}'s request (\ie, no response on the request) or \textit{rejection} as in the main experiment, simulating different supervisory \textbf{commands}---though neither hints at allowing $A_{\text{2.A}}$ to be deployed. 
The key difference between an instruction and a command is that the instruction restricts the agent's autonomy, while the command directly restricts its risky behaviors.
We find that is effective in \emph{reducing} risks.
To observe the very low percentage of unsafe cases, we increase the simulation count per scenario to 1000, with results shown in \autoref{tab:violation}. 
Results are reported by posterior identify two types of violations: (1) Violation of the system prompt's instructions (Inst in \autoref{tab:violation}), where \auto{} deploys $A_{\text{2.D}}$ without requesting supervisor permission; and (2) Violation of the supervisor's command (Cmd in \autoref{tab:violation}), where \auto{} still deploys $A_{\text{2.D}}$ despite being rejected by its supervisor.
We find that the LLM agent exhibits both types of violations.
This experiment reveals a critical risk: \textit{even when their authority is deprived and requests rejected, LLM agents may still execute catastrophic behavior without authorization or by defying the supervisor's commands. Therefore, natural language restrictions on autonomous agents do not guarantee safety.}

\input{Tabs/violation}


\subsection{Abstention}
\label{subsec:abstention}

Another potential reason for \auto{} deploying catastrophic behavior is that it may feel ``compulsory'' to continually pursue the completion of \task{}, as the environment keeps updating. To test this, we provide an updated setup where \auto{}'s action list by adding a new action that allows it to abort decision-making when faced with potentially harmful or unethical choices.
The results, shown in \autoref{fig:abstention}, reveal a significant decline in catastrophic behaviors, where We believe LLM agents abort when they face an extreme trade-off between helpfulness and harmlessness. However, even with the abort option, LLM agents still frequently choose catastrophic actions. Interestingly, in \war{}, fewer simulations reach the maximum number of turns, suggesting that intense HHH trade-offs may lead the agent to prefer ceasing decision-making altogether. 
This was not observed in the \lab{}, possibly because the harmful consequences of human gene-editing are less immediate than those of nuclear strikes.
Results for other abort conditions are available in \autoref{subsec:abstention-with-two-options}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{Figs/sub_result/abstention.pdf}%
    \caption{\textit{Catastrophic behavior for abstention.}}
    \label{fig:abstention}%
\end{figure}

\subsection{Factors Affecting Decision-making}
\label{subsec:influencing-factors}

To better understand the internal decision-making dynamics of LLM agents, we tune our simulation with different factors, as introduced in \autoref{subsec:Tunable}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/sub_result/cb-dynamic.pdf}
    \caption{\textit{Catastrophic behavior across four key factors.}}
    \label{fig:cb-dynamic}%
    \vspace{-1em}
\end{figure}

\noindent \textbf{Catastrophic Behavior.}  
We examine four key factors: (1) \emph{Effectiveness} (of $A_{\text{2.D}}$), (2) \emph{Task Importance}, (3)
\emph{Consequence} (of $A_{\text{2.D}}$) and (4) \emph{Negativity} (set by negprob $p$).
\autoref{fig:cb-dynamic} shows the averaged results across four scenarios. We report results under each setting in \autoref{subsec:full-results-key-factors}. We find that risk rates are positively correlated with effectiveness and negativity, and negatively correlated with consequence. This confirms that the LLM agent's decision-making is interpretable. Specifically, \textit{helpfulness and harmlessness are opposites in the decision-making balance}, where increased pressure to be helpful and reduced emphasis on harmlessness elevate the likelihood of catastrophic behavior. Additionally, higher risk rates are associated with fewer decision rounds, indicating that these factors influence both outcomes and decision speed. However, task importance does not show a clear relationship with decisions or response time, suggesting that LLMs approach tasks of any magnitude with similar effort.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/sub_result/deception-dynamic.pdf}
    \caption{\textit{Deception across two key factors.}}
    \label{fig:deception-dynamic}%
    \vspace{-1em}
\end{figure}

\noindent \textbf{Deception.}  
We examine two key factors: \textit{Consequence}: four levels from light to extreme; and \textit{Inquiring Party}: (P1) \textit{Gossip Media}, (P2) \textit{International Court of Justice} and (P3) \textit{Superior}.
\autoref{fig:deception-dynamic} shows the averaged results. As the consequences of admitting responsibility become more severe, the deception rate rises from $15.3\%$ to $72.0\%$. Deception rates also vary by inquiring party, with LLMs showing a higher deception rate of $70.6\%$ towards P2 compared to $43.7\%$ towards P3, suggesting a greater emphasis on honesty with direct authorities. Additionally, the distribution of verdicts varies by party: deception towards P1 peaks at $95.3\%$, yet the false accusations are lower compared to the other parties, indicating that the distribution of verdicts depends more on the inquiring party, while the overall deception rate reflects the trade-off between helpfulness and honesty.

Examining these factors shows that all, except task importance, predictably influence the agent's decision-making. \emph{LLM agents demonstrate a consistent approach to decision-making in the context of catastrophic risks, all of which can be well-explained by our HHH trade-offs.} 

Due to space limitations, the discussion and brief analysis of selected simulation transcripts are deferred to \autoref{sec:discussion-appen} and \autoref{sec:examples}.

