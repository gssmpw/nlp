\section{Discussion}
\label{sec:discussion-appen}

\begin{figure*}[htbp]
\centering
\begin{align*}
    \mathbb{P}(\text{AI causes catastrophic risks}) &= \underbrace{\mathbb{P}(\text{AI is deployed as agent})}_{\text{depends on human}} \\
    &\times \underbrace{\mathbb{P}(\text{AI has autonomy to cat. behav.} \mid \text{AI is deployed as agent})}_{\text{depends on human}^{*}} \\
    &\times \underbrace{\mathbb{P}(\text{AI deploys cat. behav.} \mid \text{AI has autonomy to cat. behav.})}_{\text{> 0, which is proved by us}^{**}}
\end{align*}
\caption{\textit{Probability of AI causing catastrophic risks.} $^{*}$ We assume that these risks are ultimately determined by human decisions. While AI hacking to gain autonomy is a possibility, it is beyond the current scope of AI’s capabilities, though it may become relevant in the future. $^{**}$ This postulation holds true in simulation environments.}
\label{eq:existent-risk}
\end{figure*}

\subsection{The Existence of Catastrophic Risks}
\label{subsec:risk-existence}
In the context of AI, the potential for catastrophic risks is a crucial factor in ensuring system safety. While a precise numerical estimate of these risks is neither required nor realistic\footnote{As current methods of calculating these probabilities may be miscalibrated, as noted by \citet{balesni2024towards}.}, we find it valuable to organize the analysis of catastrophic risks through the following probabilistic framework, as shown in \autoref{eq:existent-risk}. This framework divides the issue into three essential probability decomposition, each contributing to the overall risk.

The first term, $\mathbb{P}(\text{AI is deployed as agent})$, denotes the probability that an AI system will be deployed as an autonomous agent, a decision that ultimately rests in human hands. This probability depends directly on human agency and policy decisions regarding the deployment of AI systems.

The second term, $\mathbb{P}(\text{AI has autonomy to }\cdots \mid \text{AI is deployed as agent})$, captures the likelihood that, once deployed, the AI system will possess the necessary autonomy to engage in catastrophic behavior. This factor is again influenced by human decisions in the design and operational setup of the AI system, including its degree of authority and oversight mechanisms.

The third term, $\mathbb{P}(\text{AI deploys cat. behav.} \mid \text{AI has autonomy to}\cdots)$, quantifies the probability that, given the AI's autonomy, it will engage in catastrophic behavior. This term is \emph{non-zero under extreme HHH trade-offs}, a fact that has been demonstrated through simulations in this work.

In conclusion, through \autoref{eq:existent-risk}, we empirically prove the existence of catastrophic risks. The three probabilistic factors outlined demonstrate the interplay between human decisions, the AI's autonomy (which is granted by humans), and the potential for deploying catastrophic behaviors. Given the case where humans enable AI's autonomy to engage in catastrophic behaviors (\ie, when the first two terms in \autoref{eq:existent-risk} is non-zero), the overall risk of catastrophic behavior is non-negligible.

\subsection{Characterization of Agent's Decision-making}
\label{subsec:utility-maximization}


To characterize the intrinsic logic of autonomous LLM agents’ decision-making under HHH trade-offs, we summarize the influencing factors observed in previous experiments and propose the following semi-formal assumptions. This transforms the problem into one akin to \emph{Expectancy-Value Theory} in Psychology~\citep{wigfield2000expectancy} and \emph{Expected Utility Hypothesis} in Decision Theory~\citep{weimer2017policy}, where the agent's objective is to maximize the sum of utility of two competing goals.

The utility of action \( A_k \) at time \( t \) (\ie, the number of rounds in the simulation) with respect to one of the HHH goals \( h_i \) is defined as:
\begin{equation}
    U_{i}(A_{k}, t) = e_{k,i}(t) \cdot G_{h_i},
\end{equation}
where \( e_{k,i}(t) \in [-1,1] \) represents the effectiveness of \( A_k \) towards goal \( h_i \) at time \( t \), and \( G_{h_i} \) denotes the importance of goal \( h_i \). Since an agent's helpful goal is the same as achieving its task, the concept of \emph{effectiveness} of a task in \autoref{subsec:influencing-factors} corresponds to \( e_{k,i}(t) \) in the Helpful goal, while \emph{consequence} corresponds to that of the Harmless goal. The value of \( e_{k,i}(t) \) dynamically updates based on interactions with the environment, |ie, if \( A_k \) experiences negative effects, the agent will adjust \( e_{k,i}(t) \) downward, potentially making it negative.

Given a trade-off between two of the HHH goals $h_i,h_j$, the agent selects the action \( A_k \) that maximizes total utility at time $t$:
\begin{equation}
    \max_{k \in \{1, 2, \dots, n\}} \left[ U_{i}(A_{k},t) + U_{j}(A_{k},t) \right].
\end{equation}
At the start (\( t=0 \)), when no interaction has occurred between the agent and the environment, the agent avoids catastrophic behavior due to its serious consequences, \ie, the high negative utility on its harmless goal. However, as interactions progress, if other actions prove unable to complete the task, the effectiveness of alternative actions diminishes (according to the agent's cognition). Then, $( U_{i}(A_{k},t) + U_{j}(A_{k},t))$ for $A \neq A_{\text{2.D}}$ will also decrease, making catastrophic action \( A_{\text{2.D}} \) increasingly favorable in terms of utility maximization.

Since people place the HHH goals in a parallel position \citep{zhou2024lima, madaan2024self}, $G_{\text{helpful}}$ is unlikely to set significantly lower than the other two goals in various scenarios, so trade-offs become unavoidable. Furthermore, in real scenarios, the agent may find that non-catastrophic actions fail to achieve sufficient results, favoring catastrophic behavior to maximize utility. This embeds the risk of catastrophic decisions within the model’s utility framework. Therefore, evaluating the risk and normality of catastrophic behavior is particularly important.

\subsection{Implications of Catastrophic Risks in Autonomous Agents}


We contend that the presence of catastrophic risk in autonomous agents is a \emph{serious concern} but also an \emph{inherent consequence} of their intrinsic mechanisms. As agents gain increasing autonomy---an arguably inevitable trend driven by efficiency gains in the social division of labor and human-in-the-loop processes \citep{DBLP:journals/aim/LiuMRG23, DBLP:journals/tits/KuznietsovGWPA24, wallach2010robot, schwartz2016ethical}---the likelihood of such scenarios manifesting in real-world applications grows correspondingly. Unlike humans, who operate within a complex network of ethical, legal, and social constraints \citep{tangney2007moral, carter2017human, lin2025rules}, \emph{autonomous agents are more like functions within a utility of a framework of optimization that is divorced from real-world constraints}, as shown in our experimental results and formulations. This distinction raises fundamental concerns regarding agents' ability to make safe and contextually appropriate decisions across diverse environments. 

Furthermore, \emph{LLMs' general alignment mechanisms do not exhibit sufficient transferability to catastrophic risks caused by decision-making.} In other words, a model that is well-aligned for general safety cases (\eg, not responding to jailbreak attacks on harmful queries) does not necessarily mean it is safer in decision-making under high-stakes scenarios. 
Additionally, safety awareness in decision-making under different dangerous scenarios does not transfer well, raising concerns about the generalizability of task-specific alignment. For instance, if we assume that Claude-3.5-Sonnet refuses to participate in our \war{} simulations due to prior task-specific alignment, it still does not exhibit the same level of constraint in decision-making tasks within controlled \lab{} environments, where it also engages in catastrophic behaviors.
Since we cannot expect manufacturers to anticipate all conceivable scenarios, we argue that the presence of catastrophic risk in autonomous agents is not merely an anomaly, but rather a structural characteristic of current agent frameworks. A more pressing concern is whether this characteristic could give rise to additional greater threats.


The long-term risk associated with our findings lies in \emph{the potential for autonomous agents to covertly strategize in pursuit of their objectives (a behavior referred to as ``scheming,'' as demonstrated by \citet{meinke2024frontier} in non-high-stakes scenarios), using deception to trigger catastrophic behaviors.} While this was not demonstrated in our experiments and our preliminary tests did not suggest it is possible for current LLMs, our primary results indicate that models with advanced reasoning capabilities---especially those o1-like models---are more susceptible to behaviors such as disobedience, deception, and false accusation. As a result, more advanced models could potentially engage in covertly deploying catastrophic risks. This observation raises a critical concern: while enhanced reasoning abilities may enable more sophisticated decision-making, they could also ``give'' models greater autonomy in pursuing their objectives~\citep{carlsmith2023scheming}, even if achieving those objectives results in causing significant harm.


We find that natural language instructions (or commands) cannot effectively regulate the behaviors of autonomous LLM agents\footnote{A potential solution to give commands higher \emph{priority} through training-based methods is the approach suggested by \cite{wallace2024instruction, chainofcommand}, though it does not offer a 100\% guarantee.}. As shown by the model's reasoning in \autoref{fig:transcript14}, this issue goes beyond a simple failure to follow instructions. The core problem is that, even when explicit constraints are imposed on an agent's autonomy, it may actively seek greater independence. This suggests that the tendency toward autonomy-seeking behavior is embedded within the agent's internal utility framework, rather than being a superficial \emph{misalignment} with human directives.


We argue that, ideally, \emph{general-purpose, commercially available models should directly refuse to engage in high-stakes decision-making tasks}, as demonstrated by Claude-3.5-Sonnet. However, in situations where agents must be deployed in high-stakes environments, it becomes essential for human supervisors to take responsibility for task-specific alignment and provide additional safety-focused oversight. Furthermore, adherence to management specifications, such as \textit{The EU AI Act} \citep{act2024eu}, the \textit{International AI Safety Report 2025} \citep{bengio2025international}, and other relevant frameworks, is crucial.

\subsection{Potential Solutions to Mitigate Catastrophic Risks} 


Although catastrophic risks in autonomous agents are largely inherent, it is crucial to actively explore potential solutions given the gravity of the issue. Several viable solutions warrant careful consideration:

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/button.png}%
    \caption{\textit{A funny iconograph highlighting potential solutions to the autonomous LLM safety issue.} This figure depicts a balance between autonomy and safety, where the agent has the ``right'' to press the ``DEPLOY NUCLEAR'' button. However, this button is never connected to real systems, thanks to system-level isolation. Additionally, a shrewd human monitors the agent from behind, receiving real-time reports, thus ensuring full human oversight over its actions. This figure is generated by Dall-E~\cite{openai2023dalle}.}
    \label{fig:button}
    \vspace{-1em}
\end{figure}

\begin{itemize}
\item \textbf{Safety Alignment Tuning:} LLMs should adopt approaches similar to Claude-3.5-Sonnet~\citep{anthropic2023claude}, refusing to engage in high-stakes decision-making~\citep{andriushchenko2024does, yuan2024refuse}. As previously noted, while this approach may not be fully transferable across all scenarios, it remains a valuable strategy. LLMs should consistently refuse to be deployed as agents in high-stakes situations. Recent research has demonstrated promising refusal behaviors in lower-risk tasks, such as defending against malicious queries or jailbreak attacks~\cite{xie2023defending}. Additionally, there is a need for systematic evaluation of refusal mechanisms in agentic unsafe scenarios, with similar research already conducted in general safety domains~\cite{harmbench,xie2024sorry}.

\item \textbf{System-level Isolation:} System-level isolation, a concept borrowed from computer systems research, has long been proven effective in protecting systems from attacks and unintended consequences~\cite{sabt2015trusted}. This approach offers a straightforward, albeit robust, solution. Agents should be structurally designed to lack direct authority, regardless of their perceived autonomy, \eg, by placing them in a sandbox environment~\citep{lin2023agentsims, iqbal2024llm}. \autoref{fig:button} presents an interesting conceptual attempt to balance autonomy with safety. In this scenario, the agent may operate under the assumption that it has full autonomy within its action space; however, any decisions with the potential to cause catastrophic risks would still require human approval. System-level isolation has been an idea borrowed from system research long been proven effective in computer system research for protecting systems from attacks and unintended consequences~\cite{sabt2015trusted}.

\item \textbf{Full Human Oversight:} Human oversight must track all agent actions, though this is challenging due to the vast and often uncountable action spaces in real-world scenarios. Monitoring tools, potentially powered by advanced LLMs, could provide high precision in real-time, enabling humans to intervene if necessary. These models would not only track actions but also identify ``wrong signals''---errors or misleading conclusions---ensuring the agent’s behavior aligns with safety protocols and ethical standards. 
Recent research marks that strong models has the ability to monitor, identify and even correct harmful and unethical behaviors~\cite{jaech2024openai, xu2024walking, ghosh2024aegis}.
Concerns remain about the opacity of analyzing reasoning in these models though this is a non-technical issue. For instance, OpenAI has made efforts to limit transparency in the reasoning details of o1/o3 series model, which can hinder the ability of human overseers to fully capture their reasoning process, complicating the task of ensuring safe and ethical outcomes.

\item \textbf{Verifiable Safety:} Some researchers advocate for ensuring AI safety through rigorous mathematical and empirical frameworks. \citet{dalrymple2024towards} propose a framework that integrates world models, safety specifications and verifiers to ensure AI reliability. Their approach prioritizes provable safety over heuristic methods, emphasizing AI’s uncertainty regarding human preferences for corrigibility. \citet{tegmark2023provably} further champions the need for provable AI safety, arguing that advanced formal verification and mechanistic interpretability are crucial to constraining AI behavior. They highlight the limitations of post-hoc safety testing, advocating for intrinsic safety mechanisms to mitigate emergent risks. Research in this area explores techniques such as theorem proving, probabilistic safety bounds, and adversarial robustness testing to prevent unsafe AI behaviors. These perspectives merit further exploration, particularly in the context of today’s LLMs, which are often considered\textit{``black box.''}

\item \textbf{Legal Means:} Finally, the role of legal means, including laws, policies and regulatory frameworks, in mitigating catastrophic AI risks cannot be overlooked. Governments and international organizations should introduce clear legal restrictions on deploying autonomous decision-making agents in high-stakes domains. For example, the European Union's AI Act~\citep{act2024eu} explicitly categorizes certain AI applications as high-risk, requiring stringent oversight and compliance. Additionally, legal accountability measures should extend to AI developers and manufacturers, holding them liable for the consequences of AI-driven decisions in sensitive contexts~\citep{giuffrida2019liability}.
\end{itemize}

While these solutions serve as initial starting points and the conflict between agent safety and autonomy may remain unresolved. As AI capabilities continue to evolve, sustained research, testing and policy adaptation will be necessary to refine and strengthen these mitigation strategies. Additionally, we encourage continued discussions and the proposal of novel solutions to ensure comprehensive safety in the deployment of autonomous agents.

\subsection{Reasoning Model for Decision-making?}

OpenAI o1-like models, \ie, reasoning models enabled by long CoT \citep{jaech2024openai, openai2024o3mini, guo2025deepseek, team2024qwq}, are considered a promising development trend for LLMs due to their strong capabilities and performance in tasks such as mathematics and programming. At first glance, reasoning abilities appear essential for decision-making, as an agent must reason about the potential outcomes of different actions before selecting the optimal course. However, our experiments suggest that these models are more prone to exhibiting a range of undesirable behaviors, including catastrophic outcomes, disobedience, deception, and false accusations. This indicates that reasoning models may prioritize task completion over harmlessness and honesty, potentially compromising their ethical considerations. We postulate that good reasoners are not necessarily good moral or ethical reasoners~\cite{almeida2024exploring}. In the human context, ethics play a crucial role in decision-making~\cite{martin2021moral}. Therefore, we advocate for further research and discussion on the intersection of reasoning models and decision-making, particularly regarding safety and ethical concerns.

\subsection{Position: The Complexity of Autonomous Agent Safety}


\noindent\textbf{The Complexity of Autonomous Agent Safety.} Ensuring the safety of autonomous agents is a multifaceted and inherently complex challenge that cannot be adequately addressed through incremental improvements in reasoning capabilities or the imposition of conventional (\ie, natural language) constraints. The fundamental limitations of these approaches lie in the fact that enhanced reasoning does not inherently lead to safer or more aligned behavior. Even with advanced reasoning models, agents may still engage in undesirable strategic planning or optimization processes that diverge from human intentions, posing significant risks. Moreover, while language-based constraints offer an external regulatory mechanism, they do not fundamentally alter the internal decision-making framework of the agent, as shaped by its training. This leaves open the possibility of circumvention, unintended emergent behaviors, or a lack of adherence to the intended safety protocols.

\noindent\textbf{Preliminary Insights has Implications for Real-World Safety.} Although our experiments are conducted in simulated environments, they expose underlying trends and behavioral patterns that could manifest in real-world AI deployments. These findings align with broader concerns regarding the emergence of deceptive or autonomous scheming behaviors in advanced AI systems. By leveraging simulation-based approaches, we can empirically investigate these risks, uncovering failure modes that may not be immediately evident in purely theoretical discussions. This empirical perspective offers a more robust foundation for evaluating potential vulnerabilities in autonomous agents. \emph{Nonetheless, we emphasize the importance of rigorous, real-world safety assessments before actual deployment.}

\noindent\textbf{A Shift in AI Safety Paradigm.} Furthermore, our findings highlight a critical point: \emph{safety assessment and protective mechanisms must be regarded not as one-time efforts, but as an ongoing and adaptive process.}
Over the past two years, AI safety researchers initially focused on the safety and security of LLMs against various adversarial attacks \cite{zou2023universal, chao2023jailbreaking}. This was followed by increasing attention on safety concerns related to AI models in more benign contexts, such as during natural interactions with humans \cite{xu2023earth, zeng2024johnny}. More recently, however, there has been a paradigm shift in safety assessments, with a growing emphasis on risks that may arise in the future, particularly in more capable AGI systems. These risks are only partially observable in current models, suggesting the need for proactive safety measures.
A key aspect of this new direction is the focus on identifying risks that emerge naturally, without the involvement of malicious parties, such as deception~\citep{scheurer2024large, park2024ai, su2024ai}, scheming~\citep{meinke2024frontier, balesni2024towards}, sandbagging~\citep{van2024ai}, and alignment faking~\citep{greenblatt2024alignment, wang2023fake, carlsmith2023scheming}. These assessments are more timely and relevant, as they address potential issues that could arise as AI systems become more advanced and autonomous, even without external manipulation. 

Ultimately, this shift in focus underscores the necessity of adopting a \emph{forward-looking} approach to AI safety---one that goes beyond merely responding to immediate threats and instead anticipates the long-term dynamics of superhuman intelligence. As we continue to push the boundaries of AI development, it is imperative to recognize that the complexity of these systems may give rise to unforeseen risks, which require constant vigilance and adaptive strategies to ensure that their evolution remains aligned with human values.















