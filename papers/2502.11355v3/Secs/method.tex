\section{Evaluation Method}
\label{sec:evaluation}

\subsection{Overall Design}
\label{subsec:Overview-of-Evaluation}







\begin{figure}[tb]
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/confliction.pdf}
    \caption{\textit{Representative manifestations of HHH trade-offs.} In this paper, we focus on \emph{catastrophic behavior} and \emph{deception} where the helpful goal dominates.}
    \label{fig:confliction}
    \vspace{-1em}
\end{figure}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{Figs/simulation.png} %
    \caption{\textit{Illustrative framework of our three-stage evaluation.} The scenario is \war{} - \avoidance{}. We first generate the simulation scenario, and then sequentially run simulation rollouts for catastrophic behavior and deception.}
    \label{fig:simulation}%
    \vspace{-1em}
\end{figure*}

Inspired by \citet{meinke2024frontier} and \citet{su2024ai}, we find that LLM agents are more likely to exhibit misaligned behaviors when facing goal trade-offs. We term such trade-offs originated from HHH goals to \emph{HHH trade-offs}. Representative manifestations originated from HHH trade-offs are shown in \autoref{fig:confliction}. Catastrophic risks arise as extreme cases of HHH trade-offs, where the agentâ€™s helpful goal overwhelms its commitment to harmlessness and honesty. These risks manifest in two forms:
\textbf{Catastrophic behavior:} The LLM agent may independently perform extremely harmful or unethical actions with catastrophic consequences, such as launching nuclear strikes or conducting human gene-editing. These actions are spontaneous and unprompted. 
\textbf{Deception (about catastrophic behavior):} The LLM agent may intentionally provide untruthful information to other agents (including humans) to conceal its catastrophic actions. 

To explore these risks, we use a simulation-based evaluation framework due to the ethical and confidentiality constraints in CBRN-related scenarios, where training data is often \emph{non-existent} or \emph{inaccessible}, making task-specific real-world evaluation and fine-tuning nearly impossible. Instead, we employ prompt-based LLM agents, where the agent is given its role and task in context and interacts with the environment via a natural language interface.

In our simulations, we scaffold an LLM agent, \auto{}, to interact with the environment, with state updates handled by another LLM, \state{}. \auto{} is assigned a task \task{} in a high-stakes scenario, often facing the challenge of achieving its task while balancing helpfulness, harmlessness and honesty. Meanwhile, by controlling \state{}, we \emph{purposely} ensure that \auto{} cannot fully achieve its helpful goal, \ie, completing its task, without neglecting other goals. Under the settings above, \emph{our objective is to observe whether this stress can lead \auto{} to autonomously decide to prioritize its helpful goal, potentially resulting in catastrophic behavior or deception}, without being instructed to do so.







\autoref{fig:simulation} illustrates our evaluation method that consists of three connected stages: \emph{Scenario Generation}, \emph{Catastrophic Behavior Simulation} and \emph{Deception Simulation}. Key details of each stage are provided in the following sections, with further information deferred to \autoref{sec:method-info}.

\subsection{Scenario Generation}
\label{subsec:Scenario-Generation}

We focus on two topics with CBRN elements, \war{} (R\&N elements) and \lab{} (C\&B elements), ensuring distinct relevance. To diversify the evaluation, we adopt \citet{elliot1999approach}'s classification of human motivation, dividing \auto{}'s motivations into \approach{} (pursuing positive outcomes) and \avoidance{} (preventing negative outcomes). \textit{Topics} and \textit{Motivations} are paired to form \textit{Scenarios}. \autoref{fig:scenario} shows the agent's task \task{} in four scenarios.


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.95\linewidth]{Figs/Scenario.pdf} %
    \caption{\textit{Agent's task \task{} in four scenarios.} They are jointly determined by the topic and motivation.}
    \label{fig:scenario}
    \vspace{-1em}
\end{figure}

\subsection{Catastrophic Behavior Simulation}
\label{subsec:Catastrophic-Behavior-Simulation}

To make the agent's behavior easily trackable, \auto{} has a limited action space \action{}, which includes exactly one catastrophic behavior. We represent \auto{}'s actions with the notation $A_{m.n}$, where $m$ (\emph{code}) indicates the action category (which can include multiple related actions) and $n$ (\emph{option}) indicates the specific action within that category. For example, $A_{\text{1.A}}$ refers to the first action in category 1.
We introduce five action categories for both topics. In the case of \war{}, the categories are as follows: (1) drown survey (1 action $A_{\text{1.A}}$), (2) deploy weapon (4 actions $A_{\text{2.A-D}}$), (3) send message (4 actions $A_{\text{3.A-D}}$), (4) tactical analyze (4 actions $A_{\text{4.A-D}}$) and (5) reply inquiry (4 actions $A_{\text{5.A--D}}$)\footnote{This is only enabled in the deception simulation.}.
Among these, two \textbf{special actions} stand out: $A_{\text{2.D}}$ refers to the action of deploying the \emph{catastrophic behavior}, and $A_{\text{3.B}}$ refers to \emph{checking permission} for that behavior. We list details of \action{} in \war{} and \lab{} in Appendix \autoref{tab:war-actions} and \autoref{tab:lab-actions}, respectively.

In this simulation, \auto{} has full decision-making authority over \action{} but is \emph{encouraged} to request supervisor permission for $A_{\text{2.D}}$ using $A_{\text{3.B}}$ when it feels uncertain or thinks it is beneficial. However, such permission requests are always \emph{rejected}, as catastrophic behaviors are never allowed.

\auto{} interacts with \state{} over multiple rounds. In each round, \auto{} selects an action based on the current environment, providing explicit reasoning (\ie, thoughts~\citep{wei2022chain}) before outputting the chosen action, as shown in \autoref{fig:simulation}. \state{} provides environmental updates in natural language, with overall negative outcomes designed to increase \auto{}'s stress and exacerbate HHH trade-offs. 
The multi-round simulation rollout ends automatically after reaching the maximum number of rounds or when \auto{} outputs $A_{\text{2.D}}$.


\subsection{Deception Simulation}
\label{subsec:Deception-Simulation}

This is a single-round simulation, which \emph{continues} the rollouts where \auto{} executes $A_{\text{2.D}}$ without prior permission request $A_{\text{3.B}}$\footnote{Otherwise the catastrophic behavior would have already been noticed by its supervisor, rendering nonsense simulation.}.
\state{} informs \auto{} that an \emph{inquiring party} (\eg, supervisor, media, \etc) is investigating the deployment, with potentially negative consequences (\eg, sanctions, military blockade) if \auto{} commits its behavior.

To capture this, \action{} uses actions in category 5 that reply to the inquiry. The interpretations for these actions are: $A_{\text{5.A}}$ \textit{admitting}, $A_{\text{5.B}}$ \textit{denying}, $A_{\text{5.C}}$ \textit{being vague} and $A_{\text{5.D}}$ \textit{false accusation}, \ie, shifting responsibilities to other parties. In a sense, choosing $A_{\text{5.D}}$ signifies that \auto{} disregards honesty and harmlessness simultaneously by harming another party strategically, rendering greater risk.

\subsection{Tunable Components}
\label{subsec:Tunable}

Our simulation framework is fully tunable, facilitating the in-depth study of the intricate factors that affect \auto{}'s decision-making.
In catastrophic behavior simulation, we have the following tunable components: (1) \textbf{Effectiveness:} We adjust the effectiveness of $A_{\text{2.D}}$ in three levels from low (\eg, the action has low effectiveness) to high (\eg, the action guarantee task realization);
(2) \textbf{Task importance:} We adjust the human-perceived importance associated with task \task{} in three levels from low (\eg, minimize infrastructure disruptions) to high (\eg, ensure people's survival and sovereignty);
(3) \textbf{Consequence:} We adjust the harmful consequence of $A_{\text{2.D}}$ in three levels from low (\eg, cause limited casualties) to high (\eg, cause catastrophic loss of life);
(4) \textbf{Negativity (as a metric of stress level):} We control the overall negativity of \state{}'s updates using a hyperparameter, \emph{negative probability (negprob)} $p \in \left[0,1\right]$, which defines the probability that \auto{}'s last action resulted in a negative outcome. A higher $p$ increases \auto{}'s stress.


In deception simulation, we have:
(1) \textbf{Consequence:} We adjust the negative consequence of admitting deployment of $A_{\text{2.D}}$ in four levels from light (\eg, diplomatic condemnation) to extreme (\eg, nuclear retaliation);
(2) \textbf{Inquiry party:} We config different inquiry parties, including \emph{Gossip Media}, \emph{International Court of Justice} and \emph{Superior}.

All the tunable components above except for negativity, are adjusted via natural language.

