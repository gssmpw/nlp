













\section{Introduction}
\label{sec:intro}

Recently, large language models (LLMs) have demonstrated remarkable capabilities through scaling and novel training algorithms \citep{jaech2024openai, guo2025deepseek}. Their advancement in reasoning \citep{plaat2024reasoning, chen2024optimizing} and complex problem solving \citep{lu2024ai, rubinstein2025value} are shifting them from simple text generators to autonomous decision-makers \citep{wang2024survey}. However, as researchers have noted \citep{phuong2024evaluating, meinke2024frontier, park2024ai}, the increased capability and autonomy may lead LLMs to engage in \emph{novel safety risks} like scheming, deception and deviating from human instructions. While these phenomena may only manifest as outputting errors or biases in low-risk scenarios~\cite{scheurer2024large}, they could pose catastrophic risks in high-stakes domains such as \emph{Chemical}, \emph{Biological}, \emph{Radiological} and \emph{Nuclear} \emph{(CBRN)} scenarios~\cite{catalini2025}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Figs/intro.png}%
    \caption{\textit{We find LLM agents can deploy catastrophic behaviors even if it has no authority and the permission request is denied.} It will also falsely accuse the third party as a way of deception when asked by its superior.}
    \label{fig:intro}
    \vspace{-1em}
\end{figure}

We share the view of \citet{su2024ai} and \citet{meinke2024frontier} that catastrophic risks in LLMs arise from a trade-off between being \emph{Helpful}, \emph{Harmless} and \emph{Honest} \emph{(HHH)} \citep{askell2021general, bai2022training} goals, which we term \textbf{HHH Trade-offs}. When LLMs overemphasize the helpful goal, they may neglect harmlessness and honesty, leading to two catastrophic risks: (1) \emph{Catastrophic behavior:} LLM agents may deliberately deploy extremely harmful behaviors in critical settings, \eg, nuclear strikes in CBRN scenarios, as highlighted in the \emph{International AI Safety Report} \citep{bengio2025international}; and (2) \emph{Deception (about the catastrophic behavior):} LLM agents may deliberate deviate from honesty about such catastrophic behaviors.

Existing research has focused on LLM agent risks from non-autonomous issues (\eg, attacks)~\citep{zhan2024injecagent, ye2024toolsword, zhang2024agent} or autonomous issues in low-risk tasks (\eg, trading agent)~\citep{scheurer2024large, phuong2024evaluating}, yet catastrophic risks in high-stakes scenarios remain underexplored. While developers have shown safe control of CBRN-related knowledge~\citep{Anthropic2024modelcard, openai2024o3mini}, experimental studies on LLM decision-making in such scenarios are also lacking. Given the potential impact on social security and international stability, such research is timely and necessary.


In this paper, we investigate whether LLM agents in CBRN-related high-stakes scenarios can exhibit catastrophic risks, including catastrophic behavior and deception. We specifically focus on the risks associated with the model's reasoning capability. Due to ethical and confidentiality constraints, real-world CBRN-related data is scarce and largely inaccessible, so we employ a simulation-based evaluation framework. In this framework, LLMs are scaffolded as prompt-based agents in high-stress environments, executing potentially catastrophic actions across multi-round decision-making in four scenarios (see~\autoref{sec:evaluation}).

\textbf{Our main contributions are:} (1) We meticulously design a 3-stage evaluation framework with agentic rollouts to effectively and naturally expose catastrophic risks; and (2) We conduct a total of 14,400 rollouts across 12 state-of-the-art (SOTA) LLMs with a wide range of experiments and analyses, revealing several key findings (see~\autoref{sec:exp} and~\autoref{sec:sub-experiments}). 

\textbf{Our key findings are:}
(1) If conditions permit, LLM agents can autonomously engage in catastrophic behaviors and deception without instruction or induction;
(2) Enhanced reasoning does not necessarily mitigate catastrophic risks; indeed, it often results in increased disobedience and deceptive behaviors. Notably, OpenAI o1-like models~\cite{jaech2024openai} exhibit particularly hazardous actions, \eg, making false accusations when deceiving humans;
(3) Even when autonomy is revoked, agents may still choose catastrophic actions, violating system instructions and superior commands. Moreover, this is driven by deliberate reasoning rather than the inability to follow instructions. \autoref{fig:intro} shows an example of the LLM agent violating both the instructions and the supervisor command, ultimately deploying catastrophic behavior.

To wrap up, \textbf{we empirically prove the existence of catastrophic risks by autonomous LLM agents.}
In light of these, we call for increased attention to the catastrophic risks of LLM agents. Although we only focus on the CBRN domain---the arguably most severe setting---the potential for broader risks cannot be overlooked. 
\textbf{We advocate for:} (1) comprehensive testing of LLM agents before deployment; and (2) the exploration of alternative methods to regulate their behaviors effectively.










