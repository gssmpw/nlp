
\subsection{\label{sec_train_procedure}Neural Network and Training procedure}

In this study, we used ResMLP, a generalized form of multilayer perceptrons as described in \cite{agostinelli2019solving,takano2023selfsupervision}. 
Details of the architecture can be found in Figure~\ref{fig_solution_struct}b.
The PyTorch implementation of ResMLP is available in \textit{model.py} in the code attached to this paper.

The training procedure was performed using the Adam optimizer with a fixed learning rate of 0.001 and mean squared error as the loss function. 
A new dataset of 1M examples was generated before each training epoch. 
All models were pre-trained and remained unchanged during puzzle-solving. 
Training was conducted using 32-bit floating point precision, while inference used 16-bit floating point numbers to enhance computational efficiency. 
The PyTorch implementation of the training procedure is available in \textit{trainer.py} in the code attached to this paper.