\subsection{\label{sec_analysis} Optimality vs the Proposed Approach Parameters }

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/arxiv_fig2.eps}
\caption{Influence of model parameters on solution length for 3$\times$3$\times$3 and 4$\times$4$\times$4 cubes (jitter plot): (a)~influence of the model size, trainset sizes, and model depth on average solution length; (b)~influence of the beam width on average solution length.}\label{fig_param_vs_len}
\end{figure*}

The proposed solver has the following main parameters $A$ -- the number of agents, $W$ -- beam width used by each agent during pathfinding and ResMLP model general parameters: $N_1$ -- the size of the first layer, $N_2$ -- the size of the second layer and residual blocks' layers, $N_r$ -- the number of residual blocks and $T$ -- the trainset size. For easier comparison, $N_1$, $N_2$, and $N_r$ are also summarized by model size $P$ -- the total number of ResMLP parameters~(weights and biases). In this section, we analyze the influence of these parameters on the solver's average solution length and optimality. 


{\bf Train set size.} First, in the example of 3$\times$3$\times$3 and 4$\times$4$\times$4 Rubik's cubes, we analyzed how the model and trainset sizes, as well as model depth, influence the average solution length using a single agent with fixed beam width ($W=2^{18}$). The experiment details are provided in Section~\ref{sec_exp_design}, while the results are presented in Figure~\ref{fig_param_vs_len}a. It is seen from Figure~\ref{fig_param_vs_len}a that from a certain point, the raise of $T$ does not lead to any significant reduction of average solution length, especially considering the fact that the trainset size is demonstrated in logarithmic scale. Even more surprising, the $T$ value corresponding to this point is very similar for 3$\times$3$\times$3 and 4$\times$4$\times$4 cubes and neural networks of different sizes and depths. Thus, the experiments above reveal a rather unexpected effect - performance stagnation with respect to the train size.  

{\bf MLP layers and sizes.} As expected, larger and deeper networks trained on train sets of the same size generally provide shorter solutions than smaller models. What is less expected is that the higher number of layers~(higher $N_1$, $N_2$, and $N_r$) is more significant than a larger number of parameters $P$. More surprising is that even small models with 1M of parameters can reach the average solution length comparable to DeepCubeA and EfficientCube using neural networks with $\approx14.7\mbox{M}$ parameters. Based on these observations for further consideration, we used a deep neural network having the same number of layers~(ten) as \cite{agostinelli2019solving} and \cite{takano2023selfsupervision}, but with the smaller model size of 4M parameters.

{\bf Beam width.} It is the most important parameter. We performed multiple tests on a single agent equipped with this model, changing $W$ from $2^{12}$ to $2^{24}$. The results of these tests are presented in Figure~\ref{fig_param_vs_len}b~(the exact model parameters and details of the tests are provided in Section~\ref{sec_exp_design}). From Figure~\ref{fig_param_vs_len}b, it is clear that increasing $W$ effectively reduces the average solution length. Moreover, the solution length decreases approximately linearly with the logarithm of the beam width  $W$. On the 3$\times$3$\times$3 cube, increasing $W$ up to $2^{24}$ allows us to get close to the optimal solution, while for 4$\times$4$\times$4, the same beam width results in a better average solution length than the best ones submitted to the 2023 Santa Challenge. 

{\bf Agents number.} In the third part of the experimental studies, we investigated the influence of the number of agents $A$ on the solver's efficiency. These experiments were performed on 3$\times$3$\times$3, 4$\times$4$\times$4, and 5$\times$5$\times$5 Rubik's Cube. We used 10-layer ResMLP models with 4M parameters trained on 8B states in all the cases. The beam width was chosen $W=2^{24}$ so each agent could fit into the memory of a single GPU regardless of the solved cube size. The details of the performed experiments are available in Section~\ref{sec_exp_design}, while their results are provided in Figure~\ref{fig_multi_agent}. For ease of analysis, Figures~\ref{fig_multi_agent}a,\ref{fig_multi_agent}b,\ref{fig_multi_agent}c demonstrate lengths only for those agents whose solutions at least once were used as the solver's output.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/arxiv_fig3.eps}
\caption{ Average solution length of the proposed multi-agent approach depending on the number of agents composing its output for (c)~3$\times$3$\times$3, (e)~4$\times$4$\times$4, and (f)~5$\times$5$\times$5 Rubik's cubes. Solid line -- random set of the agents, dashed line -- best set. Distribution of solution lengths for (c)~3$\times$3$\times$3, (d)~4$\times$4$\times$4, and (e)~5$\times$5$\times$5 Rubik's cubes for the best ensemble. }\label{fig_multi_agent}
\end{figure*}

Figure~\ref{fig_multi_agent} clearly shows that the average solution rate of a multi-agent is always higher than the one achieved by the best single agent~(up to 8 moves for the 5$\times$5$\times$5 cube). Solid lines on Figures~\ref{fig_multi_agent}a, \ref{fig_multi_agent}b, \ref{fig_multi_agent}c show how the size of the ensemble influences the average solution length for the random set of the agents. As seen in all three cases of 3$\times$3$\times$3, 4$\times$4$\times$4, and 5$\times$5$\times$5 Rubik's cubes, the larger number of agents robustly provided more optimal pathfinding. The dashed line demonstrates the same dependency but for the set of agents jointly providing the best overall solution. Figures~\ref{fig_multi_agent}c, \ref{fig_multi_agent}d, \ref{fig_multi_agent}e demonstrate in color code how each agent from this set participates in the final solution for every scramble from the dataset. The scrambles which were not solved in less than 200 moves are marked with crosses.

If it is seen from Figures~\ref{fig_multi_agent}c,\ref{fig_multi_agent}d,\ref{fig_multi_agent}e that the worst agents in the ensemble not only provide much longer results than the final solution but also include multiple scrambles that were unsolved. In the case of using the single-model approach, these agents would be considered unsatisfyingly trained. Nevertheless, they are included in the best ensemble because they provided the shortest solution on one or two scrambles. Moreover, our approach reached efficiency unreachable for other ML solutions only due to such specialized agents.

Even though the results presented in Figure~\ref{fig_multi_agent} on 3$\times$3$\times$3, 4$\times$4$\times$4, and 5$\times$5$\times$5 cubes can be achieved using 5, 10, and 10 agents, respectively, the probability of training all these agents in a row is very low. For example, to beat all the 5$\times$5$\times$5 scrambles from the 2023 Santa Challenge dataset, we trained 69 different agents, while further analysis showed that only 10 of them composed all the output results. At the same time, the first agent trained to solve 4$\times$4$\times$4 cubes beat all the respective scrambles from the mentioned dataset but did not even get in the final ensemble because multiple other agents jointly surpassed it. Thus, achieving a high level of optimality requires many agents, as seen from the logarithmic nature of the plots demonstrated in Figure~\ref{fig_multi_agent}. Nevertheless, due to the high scalability of the proposed approach and the ability to run on distributed hardware using dozens of independent agents, it is not an issue using modern computational hardware.
