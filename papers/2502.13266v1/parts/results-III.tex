
\subsection{\label{sec_best} Results Summary and \texorpdfstring{\\}{} Comparison with Prior Art}

Table~\ref{tbl_best}\footnote{All the solvers presented in the Table~\ref{tbl_best} managed to solve all the scrambles from the listed datasets.} summarizes the main results achieved by the proposed solver, highlighting its Superiority over the prior state of the art. Notably, it surpasses the 2023 Kaggle Santa Challenge results, where over a thousand teams competed in virtual puzzle solutions, representing the best available methods and results. It should be mentioned that we were limited in computation resources during our research. Thus, our results can be improved even more by using more advanced hardware, which will allow for an increase in beam width and the number of agents.

A single agent with single-layer MLP can solve all the DeepCubeA dataset with 90.4\% of optimality, significantly enhancing results of the most advanced state-of-the-art ML solutions: DeepCubeA and EfficientCube. 26 agents equipped with 10-layer ResMLP models managed to solve all 1000 scrambles from DeepCubeA dataset with 97.6\% optimality, which is the best result ever achieved by any ML solution (significantly surpassing 60.3\%, 69.8\% results from DeepCubeA and EfficientCube). A single-agent solution implemented using our approach and 10-layer ResMLP managed to beat each best result corresponding to 3$\times$3$\times$3 and 4$\times$4$\times$4 Rubik's cubes submitted on the 2023 Kaggle Santa Challenge (averages: 48.98 vs 53.49). At the same time, 29 agents managed to solve all the 4$\times$4$\times$4 cube's scrambles from the 2023 Kaggle Santa Challenge dataset with an average solution length of 46.51 - which is below 48 (a conjectured 4$\times$4$\times$4 Rubik's cube diameter~\cite{hirata2024probabilistic}). Finally, an ensemble of 69 agents beat each best solutions for the 5$\times$5$\times$5 Rubik's cube submitted to the 2023 Kaggle Santa Challenge, shortening the average solution rate among all the datasets on more than 4.4 units in QTM metrics (ours: 92.16, Santa: 96.58). It is worth emphasizing that the solutions that were obtained outperformed the Santa results on average and in every single case.

The efficiency of our approach is driven not only by the large number of agents but also by the efficiency of each single node. We performed an additional test to prove this statement and compared it with EfficentCube in terms of average computation time while running on the same hardware. The training procedure for EfficentCube took 86 hours 25 minutes, while the model for our solution was trained in 4 hours 40 minutes. Then, both solutions were used to solve all the scrambles from the DeepCubeA dataset (see results No.9 and 10 in Table~\ref{tbl_best}). Our solution provided slightly better results using the same beam width of $2^{18}$. At the same time, EfficientCube required 287.78~s on average to solve a single scramble, while our solution required 10.91~s, which is $\approx26$ times faster.
