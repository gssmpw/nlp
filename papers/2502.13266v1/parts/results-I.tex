
\subsection{\label{sec_approach} Proposed Machine Learning Approach}

This paper presents a unified approach for finding paths on a large class of graphs, focusing on demonstrating its efficiency for Rubik's cube graphs. It does not rely on any prior knowledge or human expertise about the graphs.
The approach has two main components: a neural network model and a graph search algorithm -- similar to previous works such as AlphaGo/AlphaZero~\cite{silver2016mastering, silver2017mastering}, DeepCube~\cite{mcaleer2019solving, agostinelli2019solving}, and EfficientCube~\cite{takano2023selfsupervision}, among others. The model is trained to guide what moves should be done to get closer to the destination node ("solved state" for puzzles). The graph search algorithm starts from a given node and moves to nodes closer to the destination, based on the neural network's predictions, until the destination node is found.

The basic assumption on a graph is that there is a vector associated with each node (feature vector). These vectors serve as an input for the neural network \footnote{The precise quantification of requirements for feature vectors that would ensure the successful operation of the proposed method is challenging. We aim to demonstrate its efficiency in the context of Rubik’s group cases. On one extreme, even random vectors suffice if the training data covers all nodes — an idea employed in well-known approaches such as DeepWalk~\cite{perozzi2014deepwalk} and Node2vec~\cite{grover2016node2vec}. However, our focus is different: only a small subset of nodes will be covered by the training data (random walks). The key point is the ability of the neural network to generalize from that small subset to the entire graph — something that is impossible with random features. Worse, the feature vectors are related to the distance between nodes on a graph --- more training data is required, and more advanced parameters and resources should be used at all steps of the proposed method. The role of the neural network is to transform the initial feature vectors into a latent representation, where nodes that are closer on the graph are also closer in the latent space. }. For puzzles or permutation groups---it is just the vector describing the permutation $p$ of $l$-symbols, i.e. vector of numbers $(p(0),p(1),\ldots,p(l-2),p(l-1))$. Additionally, we assume that a specific node on the graph, such as the 'solved state' for puzzles, is selected. The task is to find a path from any given node to this selected node. Since the graph sizes may exceed $10^{70}$, standard pathfinding methods are not applicable. 

\begin{figure*}
\centering
% \includegraphics[width=1\textwidth]{figs/fig1.eps}
\includegraphics[width=1\textwidth]{figs/arxiv_fig1.eps}
\caption{Proposed ML solution for Rubik's cube solving: (a)~proposed multi-agent solver's process flow; (c)~ResMLP neural network architecture; (b)~an example of beam search pathfinding on 3$\times$3$\times$3 cube's graph using $W=40$.}
\label{fig_solution_struct}
\end{figure*}

The key steps of the proposed method are illustrated in the figure \ref{fig_solution_struct}a and described below:

 {\bf  Generating Training Data via Random Walks and Diffusion Distance.} Generate $N$ random walk trajectories starting from a selected node. (The generation of a random walk is a simple process: select a random neighbor of the current node and repeat this process iteratively for multiple steps.) Each random walk trajectory consists of up to $K_{\text{max}}$ steps, where $N$ and $K_{\text{max}}$ are integer parameters of the method. For some nodes encountered during the random walks, we store a set of pairs $(v, k)$, where $v$ represents the vector corresponding to the node and $k$ is the number of steps required to reach it via the random walk. This set will serve as the training data. For the Rubik's Cube, random walks correspond to random scrambling: starting from the "solved state," we perform a series of random scrambles and record the resulting positions and the number of scrambles performed. Conceptually, in the limit as $N \to \infty$, the average value of $k$ measures the "diffusion distance"---roughly speaking, the length of the random path or an estimate of how quickly diffusion reaches a given node. In contrast to the DAVI approach used in \cite{agostinelli2019solving}, random walk generation is very computationally cheap, making it possible to generate them directly during the training procedure.
    
    
 {\bf Training the neural network.} The generated set of pairs $(v, k)$ serves as the training set for the neural network. Specifically, $v$ serves as the 'feature vector' (the input for the neural network), and $k$ represents the 'target' (the output the network needs to predict). Thus, the neural network's predictions for a given node $v$ estimate the diffusion distance from $v$ to the selected destination node (solved state of the puzzle). We utilize a multilayer perceptron~(MLP) architecture with several residual blocks and batch normalization, as shown in Figure~\ref{fig_solution_struct}b, which will be further called ResMLP. It is a general form of the MLPs used in \cite{agostinelli2019solving,takano2023selfsupervision}. All the models are trained in advance before the solving phase.

    
 {\bf Pathfinding with neural network heuristics and Beam Search.} This step finds a path from a given node to the destination node. The neural network provides heuristics on where to make the next steps, while the graph pathfinding technique compensates for any possible incorrectness in the neural network predictions. The beam search pathfinding method is quite simple but has proven to be the most effective for us and works as follows. Fix a positive integer $W$ — a parameter known as the "beam width" (or "beam size"). Starting from a given node, we take all its neighboring nodes and compute the neural network predictions for all of them. We then select the $W$ nodes closest to the destination according to the neural network (i.e., the predictions have smaller values). We take these selected $W$ nodes' neighbors, drop duplicates, and again compute the neural network predictions, choosing the top $W$ nodes with the best (i.e., minimal) predictions. The search iterations are repeated until the destination node is found (or the limit of steps is exceeded). The whole process is illustrated in Figure~\ref{fig_solution_struct}c.
  
 {\bf Multi-agency.} The method described in the steps above relies on random walks for train set creation, and thus, due to that randomness, each new launch will create a new train set, and thus, each new neural network approximates the distance differently. This diversity is large enough to yield a new solution path for each launch typically. And hence, typically, several repetitions allow for the discovery of a shorter path than a single run. We call each trained neural network an agent. To solve any given state - we solve it with all the agents and then choose the best result (the shortest solution path among all the agents) -- illustrated in Figure~\ref{fig_solution_struct}a. 
