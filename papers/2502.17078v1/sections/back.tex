\putsec{back}{Background}

This section first provides the background on 3D graphics rendering. It then
describes the state-of-the-art radiance field rendering method: 3D Gaussian
splatting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\columnwidth]{figures/opengl-pipeline.pdf}
  \caption{OpenGL rendering pipeline.}
  \vspace{-0.20in}
  \label{fig:opengl-pipeline}
\end{figure}

\putssec{gr}{Preliminaries on 3D Graphics Rendering}

\noindent \textbf{Graphics Pipeline.}
%
A 3D scene is rendered into a 2D image through a series of stages, which is
referred to as a \emph{graphics pipeline} or a \emph{rendering pipeline}.
%
To run the graphics pipeline, graphics software conventionally builds on
standard graphics APIs such as OpenGL~\cite{opengl}, Direct3D~\cite{d3d},
Vulkan~\cite{vulkan}, and Metal~\cite{metal}. Each API defines a set of
functions that process the operations in the rendering pipeline on graphics
hardware. 

\figref{opengl-pipeline} illustrates a high-level overview of the OpenGL
rendering pipeline. Other graphics APIs also employ a similar pipeline model.
%
The OpenGL pipeline can be largely divided into five stages: vertex shading,
vertex post-processing, rasterization, fragment shading, and per-fragment
processing. 
%
In hardware-based graphics rendering, each pipeline stage maps to either
programmable shader cores or fixed-function units in GPUs. 
%
Note that in software-based rendering, the operations in each stage are
executed entirely on the shader cores without using fixed-function hardware.

When a draw call is invoked with input vertices, the vertex shader\footnote{A
shader is a small program that runs on the shader cores.} transforms the
position of each vertex from 3D world space into clip space coordinates, which
will be further transformed into 2D screen positions and depth by
fixed-function hardware.
%
In the vertex post-processing stage, the vertices are then assembled into
primitives (e.g., triangles). In this stage, primitives outside the visible
space are excluded through a process known as \emph{view frustum culling},
and only the visible part of a primitive remains if part of the primitive is
outside the screen space.

The visible primitives are fed into a hardware rasterizer to identify the
pixels that overlap with them. The rasterizer produces \emph{fragments} for
each primitive; if a pixel is covered by multiple primitives, there will be
more than one fragment for the pixel.
%
Also, vertex attributes computed by the vertex shader are interpolated for each
fragment in this stage. 

Using the per-fragment data (e.g., pixel position, interpolated features) and
shared data (e.g., textures), the fragment shader computes and outputs a color
and an opacity (i.e., an RGBA value) for each fragment. 
%
In the final per-fragment processing stage, raster operations perform depth and
stencil tests. For the fragments that pass the tests, their RGBA colors are
blended or stored into the color buffer to generate the final pixel color.
%
It should be noted that the rendering pipeline can be implemented in
hardware with various optimizations, as long as the final pixel colors are
correctly produced.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\columnwidth]{figures/nvidia-gpu-arch.pdf}
  \caption{NVIDIA Ampere GPU architecture~\cite{ampere}.}
  \vspace{-0.20in}
  \label{fig:nvidia-gpu-arch}
\end{figure}

\myparagraph{Graphics-Specific Hardware in GPUs.}
%
As previously discussed, modern GPUs employ programmable shader cores that
execute different types of shader programs. Today, the shader cores are not
only accessible from graphics software but are also exposed to run
general-purpose programs through software frameworks such as CUDA and OpenCL.
%
Still, GPUs also feature graphics-specific hardware that facilitates the
execution of certain parts of the graphics pipeline, which is \emph{not}
accessible via general-purpose computing frameworks.

As shown in~\figref{nvidia-gpu-arch}, for example, an NVIDIA GPU includes
several special-purpose graphics units in addition to the programmable shaders
(i.e., Streaming Multiprocessor; SM).
%
Each Graphics Processing Cluster (GPC) includes a number of Texture Processing
Clusters (TPCs), each of which contains a PolyMorph Engine. The PolyMorph
Engine performs operations such as vertex fetching and viewport transformation,
and forwards the results to the Raster Engine~\cite{wit:kil11}.

The Raster Engine (rasterizer) sets up triangle edges using input vertex
positions and computes the pixel coverage of each triangle, a process called
rasterization. 
%
The fragments produced by the rasterizer are sent to the depth ($z$) test unit
(ZROP) if an early $z$-test is enabled.
%
This unit compares the depth of each fragment with the value in the $z$-buffer at
the same pixel position, discarding fragments that would ultimately fail the
late $z$-test conducted after fragment shading. By doing so, it prevents
unnecessary fragment shading computations.
%
After fragment shading in the shader cores (SM), the render output units
(ROPs), also known as raster operation units, perform blending or storing
operations while ensuring the proper ordering of fragments for the same pixel
location.

\myparagraph{Tile-Based Rendering.}
%
Most contemporary GPUs, including NVIDIA RTX, AMD Radeon, Intel Gen, and ARM
Mali, now use some variant of tile-based rendering (TBR).
%
When rendering an image using the hardware graphics pipeline, the screen space
is divided into a grid of screen tiles, each containing a block of pixels.
These tiles are assigned to the shader cores in the form of warps or thread
blocks.
%
For instance, NVIDIA GPUs split the screen space into a grid of
16$\times$16-pixel tiles, each of which is assigned to a specific GPC. 
%
This improves cache locality and reduces off-chip memory access during
rendering.
%
To achieve this, GPUs perform tile binning in hardware~\cite{gen11,lin:mor09}.
The fragments produced by the hardware rasterizer are grouped into bins based
on their tile IDs. These bins are then flushed to the shader cores when certain
conditions are met (e.g., a bin is full, a timeout occurs, or there is a lack
of available bins for new fragments with different tile IDs).
%
\secref{analysis} provides further analysis and discussion of fixed-function
units and tile-based rendering, based on microbenchmarking of modern GPUs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\putssec{3dgs}{Radiance Field Rendering with Gaussian Splatting}

3D Gaussian splatting~\cite{ker:kop23} introduces a novel method that achieves
state-of-the-art rendering performance and quality by \emph{explicitly}
representing a scene with a set of anisotropic 3D Gaussians.
%
Each Gaussian is characterized by geometric properties, such as a position
(mean) coordinate $\mu$ and a 3$\times$3 covariance matrix $\Sigma$, as well as
visual properties, such as opacity $o$ and spherical harmonic (SH) coefficients
$sh$, to represent the view-dependent color of the Gaussian.

For training, given a sparse set of 2D images, an initial set of 3D points is
generated using a Structure-from-Motion (SfM) technique. These points serve as
the centers for the initial isotropic Gaussians.
%
During the training phase, the features of the Gaussians are updated
continuously based on their computed gradients. To better represent the fine
geometric details of the scene, the number of Gaussians increases as they are
cloned and split into smaller ones.

While a 3D Gaussian is mathematically defined as a continuous function
over the entire 3D space, Gaussian splatting models each Gaussian as an
ellipsoid for practical purposes.
%
During rendering, these 3D Gaussians are projected onto the 2D image plane as
ellipses, referred to as \emph{2D splats}. 
%
The splats are sorted by depth, from nearest to farthest relative to the given
viewpoint. The final pixel color ($\mathbf{{C}}$) is then computed using
$\alpha$-blending (\eqnref{volumerender}), which combines the colors
($\mathrm{\mathbf{c}_i}$) of overlapping splats in front-to-back order:
%
\begin{equation}
\small
\begin{aligned}
  \mathbf{{C}} = \mathrm{\sum\limits_{i=1}^{N}} &\mathrm{\alpha_i\mathbf{c}_i} \mathrm{\prod_{j=1}^{i-1}} \mathrm{(1-\alpha_j)}, \\
  \textrm{with}~~\mathrm{\alpha_i} = o_\mathrm{i} \cdot \mathrm{exp}(-\frac{1}{2}&({p'}-\mu')^T\Sigma'^{-1}({p'}-\mu')),
  \label{eqn:volumerender} 
\end{aligned}
\end{equation}
%
where ${p'}$ denotes the pixel position, and $\mu'$ and $\Sigma'$ represent the
mean and the covariance matrix of the 2D splat, respectively.
