\section{Preliminaries}

\subsection{Motion-Transformer Encoder} \label{sec: mtr encoder}

To encode the trajectory history $(S_e^{\text{hist}}, S_\mathcal{O}^{\text{hist}})$ and map information $M$ as the conditional input for the consistency model, we adopt the encoder architecture from MTR \citep{shi2022motion,shi2024mtr++}.
This transformer-based architecture effectively models scene context through a local attention mechanism, enabling efficient interaction modeling between agents and the road map while maintaining memory efficiency.
It also introduces a dense prediction head with a loss function $\mathcal{L}_{\text{encoder}}$ to train this encoder individually.
For more detailed descriptions of this encoder, we refer readers to the original MTR paper \citep{shi2022motion}.
% An MTR++ encoder \cite{shi2024mtr++} can also be used especially for its multi-agent settings, but currently there is no direct implementation on that and also scene encoding is not the main focus of our paper.
The MTR++ encoder \cite{shi2024mtr++} would be suitable for multi-agent settings, but its implementation is not yet available.
More importantly, our method is encoder-agnostic, allowing for flexible choice of any suitable scene encoder to integrate with our consistency model-based predictive planner.
We represent the encoded feature of the trajectory history and map with MTR encoder as $\text{MTR}(S_e^{\text{hist}}, S_\mathcal{O}^{\text{hist}}) \in \mathbb{R}^{(N_{\mathcal{O}} + 1) \times D}$ and $\text{MTR}(M) \in \mathbb{R}^{N_m \times D}$, respectively, where $D$ is the embedded feature dimension.

\subsection{Consistency Model}\label{sec: consistency model intro}

The consistency model can generate high-quality samples from complex distributions with only one or a few sampling steps \citep{song2023consistency}.
It consists of a forward diffusion process and a reverse process.
Let $\mathcal{D}$ be our dataset of trajectories. 
We first normalize all trajectories in $\mathcal{D}$ using the empirical mean and standard deviation computed across the entire dataset. 
Let $\mathcal{X}$ be the space of such normalized trajectories, and $p_{\text{data}}$ be the data distribution over $\mathcal{X}$. 
In the forward process, we first draw an initial sample $x_1$ from $p_{\text{data}}$.
We then apply an increasing noise schedule ${\sigma_1, \sigma_2, ..., \sigma_T}$ with $\sigma_1 \approx 0$ to progressively corrupt $x_1$ through $T$ steps.
Specifically, at each step $i$, we draw a noise sample $\epsilon$ from $\mathcal{N}(0, I)$ and set the corrupted data $x_i$ as:
\begin{align}
\quad x_i = x_1 + \sigma_i \cdot \epsilon, \quad i=1,2,...,T
\end{align}
We choose $\sigma_T$ large enough so that, when $x_1$ is repeatedly sampled from $p_{\text{data}}$ and corrupted via additive Gaussian noise, the empirical distribution of the resulting $x_T$ approximates $\mathcal{N}(0, \sigma_T^2 I)$.

Let $\mathcal{Y}$ be the space of conditioning information.
In the reverse process, we aim to learn a consistency function $f_{\theta}$ with the parameter $\theta$ that maps a noisy trajectory sample $x_i$, condition $y \in \mathcal{Y}$, and the noise level $\sigma_i$ directly to the corresponding clean sample $x_1$ for each step $i=1,..,T$.
This is achieved by choosing a specific function form of $f_\theta$:
\begin{align}
    f_\theta(x, y, \sigma) = c_{\text{skip}}(\sigma) x + c_{\text{out}}(\sigma) F_\theta(x, y, \sigma)
\end{align}
% \bae{is $\sigma$ a vector of $\sigma_i$? otherwise the equation needs subscription $i$.}
where $\sigma$ represents the noise level, and the differentiable function $c_{\text{skip}}$ and $c_{\text{out}}$ are chosen such that $c_{\text{skip}}(\sigma_1)=1, c_{\text{out}}(\sigma_1)=0$ when $\sigma = \sigma_1$ \citep{karras2022elucidating}.
This ensures that the boundary condition of $f_\theta$ is met: $f_{\theta}(x_1, y, \sigma_1) = x_1$.
$F_{\theta}$ is usually approximated by the neural network using a U-Net \citep{ronneberger2015u} or a transformer \citep{peebles2023scalable} based architecture.

For the consistency model training, we aim to enforce the consistency of the output from $f_\theta$ for adjacent sampling steps $i$ and $i+1$ to be both $x_1$.
Let $\mathcal{P}[1,T-1]$ be a distribution over the integer set ${1, 2, ..., T-1}$.
The consistency training minimizes the following loss function $\mathcal{L}_{\text{consistency}}$:
\begin{align}\label{eq: consistency training loss}
    &\mathcal{L}_{\text{consistency}} =  \nonumber \\ &\mathbb{E}_{i \sim \mathcal{P}[1,T-1], \epsilon \sim \mathcal{N}(0, I)} \bigl[d(f_{\theta}(x_i, y, \sigma_i) - f_{\theta}(x_{i+1}, y, \sigma_{i+1}))\bigr]
\end{align}
where $\mathcal{P}$ can be the uniform distribution or lognormal distribution, and $d$ is chosen to be a pseudo-huber metric function $d(x,y) = \sqrt{||x-y||_2^2 + x^2} - c$ \citep{song2023improved}.

During the data generation process, we first draw a sample $x_T$ from $\mathcal{N}(0, \sigma_T^2 I)$.
Then with the trained consistency model $f_{\theta}$, we perform iterative sampling by first predicting the approximate clean data $x_1^{[i]}$ and then sample $x_{i-1}$ with $\epsilon$ drawn from $\mathcal{N}(0, I)$:
\begin{align} \label{eq: consistency sampling}
    x_1^{[i]} = f_{\theta}(x_i, y, \sigma_i), \quad x_{i-1} = x_1^{[i]} + \sigma_{i-1} \epsilon
\end{align}
for $i = T, T-1, ..., 1$, until we obtain the final clean sample $x_1^{[1]}$.

% Given the trained consistency model $f_{\theta}$, we are able to sample the future trajectory $x$ from the conditional distribution $p(x|y)$ through a reverse process.
% First, we select a sequence of noise variance schedule $\sigma_1 = \sigma_{q_1} < \sigma_{q_2} < ... \sigma_{q_N} = \sigma_T$, which can be the same as the original variance schedule or a different schedule.
% We then perform iterative sampling by computing  
% \begin{align}
%     x_1 = f_{\theta}(x_{q_{i}}, y, \sigma_{q_{i}}), \quad x_{q_{i-1}} = x_1 + \sigma_{q_{i-1}} \epsilon
% \end{align}
% for $i=N, N-1, ..., 2$, and $\epsilon \sim \mathcal{N}(0, I)$ until we obtain the final sample data $x_1$. 
% % \bae{with $i=1$, it will be $q_0$. Is $i=N,...,2$?}
% The flexibility to choose different sampling steps and variance schedules allow
% % \deleted{s} 
% tradeoffs to be made between computational efficiency and trajectory quality.