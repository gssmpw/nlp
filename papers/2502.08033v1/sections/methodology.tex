\section{Methodology}

% \ajnote{
% \begin{itemize}
%     \item mtr encoder
%         \item multi-agent data processing
%     \item consistency model
%     \item guided sampling, alternating direction
% \end{itemize}
% }
% We introduce a novel method that simultaneously samples the future trajectories for the ego and surrounding agents as $(S_{\mathcal{O}_s}^{\text{future}}, S_e^{\text{future}})$ from the joint conditional distribution given all agents' trajectory history, map information and ego vehicle's goal $(S_e^{\text{hist}}, S_\mathcal{O}^{\text{hist}}, M, s_e^g)$ using a consistency model \citep{song2023consistency,song2023improved} with an encoder from motion transformer (MTR) \citep{shi2022motion,shi2024mtr++}.

With the aforementioned notations and stated problem, our proposed method is summarized in Fig. \ref{fig:method workflow}.
We first use an MTR encoder to encode the agent's trajectory history and map information.
Then we use a consistency model that takes the conditional input of the ego agent's planning goal and the MTR-encoded features and generates trajectory plans and predictions for the ego and surrounding agents, respectively.
Additional planning constraints for the ego agent are achieved through guided sampling of consistency models.


% \bae{At this point, I feel like it is redundant to state our aim to simultaneously predict and plan; especially with the previous section. At the same time, this paragraph is a good summary on our proposed method. One suggestion: ``With the aforementioned notations and stated problem, our proposed method is summarized as ...''.}\bae{Consider having a diagram or image on the overall pipeline in this section -- and you can refer to that in this paragraph, leaving Fig. 1 to be a simple, motivational image, unless you would like to explain details in the Introduction section.}

\subsection{Data pre-processing}\label{sec: data preprocessing}

For training diffusion models or consistency models, we usually need to normalize the input data.
% While the MTR encoder transforms all agent states into an ego-centric coordinate system, this approach presents challenges in our joint trajectory modeling.
If we jointly model ego and surrounding agents' future trajectories $(S_e^{\text{future}}, S_{\mathcal{O}_s}^{\text{future}})$, using an ego-centric coordinate system introduces large variances in the data, particularly for surrounding agents whose positions vary significantly across scenarios.
Thus the training performance is largely degraded.
Instead, we apply a coordinate transformation $\Gamma$ similar to MTR++ \citep{shi2024mtr++} that maps each agent's trajectory into its own local coordinate frame, centered at its position at the current timestep $t_0$.
This transformation provides us with data $(\Gamma(S_e^{\text{future}}),\Gamma(S_{\mathcal{O}_s}^{\text{future}})) \in \mathbb{R}^{(N_{\mathcal{O}_s} + 1) \times H_2 \times d_s}$ that has substantially reduced variance.
We then compute the empirical mean and standard deviation of the transformed trajectories across the dataset and standardize them to zero mean and unit variance.
To preserve the relative spatial relationships between agents, we collect the reference states consisting of each agent's position at timestep $t_0$ as $\text{Ref}(S_e^{\text{future}}, S_{\mathcal{O}_s}^{\text{future}}) \in \mathbb{R}^{(N_{\mathcal{O}_s} + 1) \times d_s}$.

\subsection{Consistency Model Training}

Let $\mathcal{X}$ denote the space of transformed future trajectories of the ego vehicle $\Gamma(S_e^{\text{future}})$ and surrounding agents $\Gamma(S_{\mathcal{O}s}^{\text{future}})$ from Sec. \ref{sec: data preprocessing}. 
Let $\mathcal{Y}$ denote the space of conditional inputs, containing encoded historical trajectories $\text{MTR}(S_e^{\text{hist}}, S_{\mathcal{O}}^{\text{hist}})$, map features $\text{MTR}(M)$ from Sec. \ref{sec: mtr encoder}, ego agent's goal states $s_e^g$, and the reference coordinates $\text{Ref}(S_e^{\text{future}}, S_{\mathcal{O}_s}^{\text{future}})$ from Sec. \ref{sec: data preprocessing}.

Given a planning goal and environment context as the condition $y \in \mathcal{Y}$,  we leverage the consistency model \citep{song2023consistency,song2023improved} to sample future trajectories from the conditional probability distribution $p(\cdot|y)$ over $\mathcal{X}$.
Each trajectory sample represents one possible joint future behavior of the ego vehicle and surrounding agents.

% \bae{This is a bit pedantic since their definitions are already described. Instead, you may consider describing what this conditional probability is for (e.g., ``This conditional probability represents the probability of future trajectories of the ego and surrounding agents, given the environments and ego's goal''), and and why we need this.}

For training our predictive planner, we jointly train the MTR encoder and the consistency model together in one step with a hybrid loss function, which is constructed as a weighted sum of the consistency training loss $\mathcal{L}_{\text{consistency}}$ in Eq. \eqref{eq: consistency training loss} and the dense prediction loss $\mathcal{L}_{\text{encoder}}$ of the MTR encoder from Sec. \ref{sec: mtr encoder} as follows:
\begin{align}
    \mathcal{L} = \omega_0 \mathcal{L}_{\text{consistency}} + \omega_1 \mathcal{L}_{\text{encoder}}
\end{align}
% where $\omega_0$ \replaced{and}{,} $\omega_1$ control\deleted{s}
where $\omega_0$ and $\omega_1$ control
the weight of these two losses.

\subsection{Guided Sampling} \label{sec: guided sampling}

When generating trajectories with the trained consistency model as described in Sec. \ref{sec: consistency model intro}, to impose planning constraints on the ego vehicle's future trajectory, we present a novel guided sampling approach similar to classifier guidance \cite{dhariwal2021diffusion}.
Importantly, this guidance is applied solely during sampling at test time, without requiring any modification to the training procedure.

Assuming there are $N_c \in \mathbb{N}$ planning constraint $c_j$ to minimize, $j = 1,2, ..., N_c$ and drawing inspiration from classifier guidance \citep{dhariwal2021diffusion}, we can perform a gradient descent step on the predicted $x_1^{[i]}$ during each sampling step $i$ in Eq. \eqref{eq: consistency sampling}:
\begin{align} \label{eq: original gradient descent}
     &x_1^{[i, 1]} = x_1^{[i]} - \sum^{N_c}_{j=1}\alpha_{j} \nabla c_j  
\end{align}
where $\nabla c_j$ represents the first-order gradient of the planning constraint $c_j$ and can be computed using auto-differentiation.
$\alpha_j$ is the corresponding step size for each constraint $c_j$.
Then the tuned $x_1^{[i, 1]}$ is used to sample $x_{i-1}$ in Eq. \eqref{eq: consistency sampling}.

% \ajnote{reword}
% \bae{This is very interesting, but at the same time, I guess we are considering constraints as penalties in the objective function for minimization -- which I am not sure we can call them constraints or at least bit misleading. No suggestion at the moment, but may consider rephrase.}

However, optimizing multiple constraints simultaneously presents a significant challenge in finding a suitable stepsize $\alpha$ efficiently, 
% \bae{Why varying $\alpha$ instead of fixing them for each $i$? I guess this somewhat represents the weights on each penalty but how do we get them?} 
particularly when constraints may conflict with each other.
To address this challenge, inspired by the Alternating Direction Method of Multipliers (ADMM) \citep{boyd2011distributed}, we propose a novel alternating direction method that only optimizes one constraint at a time during each sampling step $i$:
\begin{align} \label{eq: gradient step}
    &x_1^{[i, 1]} = x_1^{[i]} - \alpha_{1} \nabla c_{1} \nonumber \\
    &x_1^{[i, 2]} = x_1^{[i, 1]} - \alpha_{2} \nabla c_{2}  \nonumber\\
    &... \nonumber\\
    &x_1^{[i, N_c]} = x_1^{[i, N_c - 1]} - \alpha_{N_c} \nabla c_{N_c}
\end{align}

In our approach, we sequentially optimize each constraint $c_j$ with corresponding step sizes $\alpha_j$
during each gradient descent iteration. 
While our selection of the optimization order and relatively small step sizes is based on empirical observations, this strategy has demonstrated effective convergence in practice, which is difficult to achieve with the standard guidance method with gradient descent in Eq. \eqref{eq: original gradient descent}.
The convergence of alternating direction methods, such as ADMM, is generally guaranteed under conditions including closed, proper, and convex functions and appropriately chosen step sizes \citep{boyd2011distributed}.

\subsection{Planning Constraints Construction} \label{sec: planning constraints}
We assume that the ego vehicle's dynamics are as follows:
\begin{align} \label{eq: dynamics equation}
    \dot p_x = v \cos \theta, \quad \dot p_y = v \sin \theta, \quad
    \dot v = a, \quad \dot \theta = \omega
\end{align}
where $p_x$ and $p_y$ are positions, $v$ is the linear speed and $\theta$ is the orientation.
The control input is the acceleration $a$ and angular speed $\omega$.

Suppose we only use $p_x$ and $p_y$ from ego agent's future states $S_e^{\text{future}}$ to construct our planning constraint function $c_i$, since other states like $v$ or $\theta$ may not satisfy the dynamics equations in Eq. \eqref{eq: dynamics equation} and are also quite noisy.
Using the property of differential flatness for dynamics in Eq. \eqref{eq: dynamics equation}, we can infer $v$ and $\theta$ with $p_x$ and $p_y$ as follows:
\begin{align}
    v = \sqrt{\dot p_x^2 + \dot p_y^2}, \quad \theta = \arctan (\frac{\dot p_y}{\dot p_x})
\end{align}
Then we can further infer the control input $a$ and $\omega$:
\begin{align}
    a = \frac{\dot p_x \ddot p_x + \dot p_y \ddot p_y}{\sqrt{\dot p_x^2 + \dot p_y^2}}, \quad \omega = \frac{\dot p_x \ddot p_y - \dot p_y \ddot p_x}{\dot p_x^2 + \dot p_y^2}
\end{align}
Note that, $a$ and $v$ are computed through finite differencing of $p_x$ and $p_y$ which will inevitably incur some noises.


With $p_x, p_y$ and the derived $a$ and $\omega$, we consider three types of planning constraint $c$ to minimize as follows.
\begin{align} \label{eq: 3 planning constraints}
    &c_{\text{goal}} = \sqrt{(p_x^{t_0 + H_2} - p_{x, goal})^2 + (p_y^{t_0 + H_2} - p_{y, goal})^2} \nonumber \\
    &c_{\text{acc}} = \frac{1}{H_2} \sum_{i=t_0}^{t_0 + H_2} max(|a^i| - a_{\text{limit}}, 0) \nonumber \\
    &c_{\omega} = \frac{1}{H_2} \sum_{i=t_0}^{t_0 + H_2} max(|\omega^i| - \omega_{\text{limit}}, 0)
\end{align}
$c_{\text{goal}}$ represents a goal-reaching constraint such that the ego vehicle needs to reach a pre-specified goal $(p_{x, \text{goal}},p_{y, \text{goal}})$ at the final timestep $t_0 + H_2$.
$c_{\text{acc}}$ and $c_{\omega}$ are control limit constraints that compute the average control limit violation along the ego agent's trajectory for a given $a_{\text{limit}}$ and $\omega_{\text{limit}}$.
