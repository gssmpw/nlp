\section{Experiments}

% \ajnote{
% All methods:
% \begin{itemize}
%     \item diffusion
%     \item Motion transformer
%     \item MPC planner (potentially)
%     \item (ours) Consistency model
%     \item (ours) Consistency + guided sampling
% \end{itemize}
% Quantitative results:
% \begin{itemize}
%     \item prediction metrics: minADE, minFDE
%     \item planning constraints: goal reaching, ACC limits, omega limits
%     \item trajectory quality
% \end{itemize}
% Qualitative behavior:
% \begin{itemize}
%     \item ego agent's planned trajectory comparison
%     \item before/after guided sampling (only for consistency model)
%     \item interactive behavior, rollout simulation (only for consistency model)
%     \item Different goal locations in the same scenario (only for consistency model)
% \end{itemize}
% Others
% \begin{itemize}
%     \item computational time
% \end{itemize}
% }

% \ajnote{
% All methods:
% \begin{itemize}
%     \item Motion transformer
%     \item Diffusion, DDPM sampling (train 4 steps, sample 4 steps)
%     \item Diffusion, DDIM sampling (train 1000 steps, sample 4 steps)
%     \item (ours) Consistency model
%     \item (ours) Consistency + guided sampling
% \end{itemize}
% }

\subsection{Experiment setup}

\subsubsection{\textbf{Dataset}}

We evaluate our method on the Waymo Open Motion Dataset (WOMD) \citep{ettinger2021large}, which contains 486k traffic scenarios in the training set and 44k scenarios in the validation set that we can test on.
Each scenario provides road graph information and trajectories of diverse agents including vehicles, pedestrians, and cyclists, with up to 8 agents specifically marked for trajectory prediction.
For each agent, this dataset includes 1 second of history, current state, and 8 seconds of future trajectory, sampled at 0.1-second intervals.
This sets our history horizon to be $H_1=10$ and future horizons to be $H_2=80$ timesteps, respectively.
For each scenario, with the trajectory information from all agents including vehicles, pedestrians, and cyclists as $S_{\mathcal{O}}$, we only select vehicle agents as the ego agent with trajectories denoted by $S_e$. 
% \bae{How does it different from just considering ``vehicle'' agents from the beginning? Also, why do we only consider ``vehicle'' if there exist pedestrians and cyclists?}
We then identify the surrounding agents, with trajectories denoted as $S_{\mathcal{O}_s}$, by selecting up to 4 agents closest to the ego vehicle based on their trajectory distances, excluding those beyond a 10-meter threshold.
% \bae{10 meter threshold w.r.t. the current state? What if the distance rank changes over history? e.g., at $t=t_0-H_1$, veh1 is closer, but at $t=t_0$ veh2 is closer while other three vehicles are fixed. Also, this may not consider very quickly approaching vehicle over 10 meters away. Any ideas on improving it?} 
This enables us to focus our predictive planner on the most relevant interactive behaviors between the ego vehicle and its immediate surroundings.
The goal $s_e^g$ of the ego agent is chosen as its state at the end of the 8-second future horizon.
% \bae{How sensitive our method is to this number of agents? What if we consider all agents?}

\subsubsection{\textbf{Model Details}}

The MTR encoder consists of 6 transformer layers with a dimension of 256 to encode agent trajectory histories and map information into $\text{MTR}(S_e^{\text{hist}}, S_\mathcal{O}^{\text{hist}}, M)$.
For detailed encoder architecture specifications, we refer readers to \cite{shi2022motion}.
The conditional input $y$ for the consistency model is constructed by further encoding three components: the obtained features $\text{MTR}(S_e^{\text{hist}}, S_\mathcal{O}^{\text{hist}}, M)$, the ego agent's goal $s_e^g$, and the reference states $\text{Ref}(S_e^{\text{future}}, S_{\mathcal{O}_s}^{\text{future}})$, using 3-layer MLPs with 256 neurons per layer.
To model the consistency model function $F_\theta$, we adopt the U-Net \citep{ronneberger2015u} with the model dimension of 128 as the backbone.

For the consistency model, we choose $T=5$ which indicates 4 sampling steps in the forward and reverse process.
Follow the original paper \citep{song2023improved}, we set the noise schedule with $\rho = 6$, $\sigma_1 = 0.002, \sigma_T=80$, and then $\sigma_i = \left(\sigma_{0}^{1/\rho} + \frac{i-1}{T-1}(\sigma_{T}^{1/\rho} - \sigma_{0}^{1/\rho})\right)^\rho, i = 1, ..., T$.
During guided sampling, we perform 100 gradient steps of Eq. \eqref{eq: gradient step} at each sampling step, and set the coefficient for $c_{\text{goal}}, c_{\text{acc}}$ and $c_{\omega}$ to be $2\times10^{-5}, 3\times10^{-6}, 5\times10^{-7}$, respectively.


\begin{figure*}[!t]
    \centering
\includegraphics[width=0.75\textwidth]{figures/center_agent_plan_v4.png}
    \caption{
    Ego vehicle's planned trajectory (Green) in a left-turn scenario compared to the Groundtruth (Red).
    % \di{zooming in on the upper quadrant might make differences more clear} 
    } 
    \label{fig: ego agent's planned trajectory}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/center_agent_different_goal.png}
    \caption{
    Planning with different goal locations using consistency models and guided sampling.}
    \label{fig: different goal}
\end{figure}

\begin{table}[t]
    \centering
    \caption{Planning Trajectory Compared to the Human Groundtruth}
    \label{tab:prediction metrics}
    \begin{tabular}{lcc}
    \toprule
    Method & minADE $\downarrow$ & minFDE $\downarrow$ \\
    \midrule
    Transformer & 0.944 & 3.239 \\
    \midrule
    DDPM-10 & 0.346 & 0.377 \\
    DDPM-4 & 0.363 & 0.498  \\
    DDIM-4 & 2.126 &  2.559 \\
    % DDIM & & \\
    Consistency & 0.309 &  0.218 \\
    Consistency (guided) & \textbf{0.303} & \textbf{0.016} \\
    \bottomrule
    \end{tabular}
\end{table}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\textwidth]{figures/interactive_behaviour_v4.png}
%     \caption{
%     Interactive behaviors from our predictive planner.
%     Upper row: A proactive nudging behavior from the ego car to switch a lane. Lower row: A yielding behavior for the ego car to make a turn.
%     The planned trajectory for the ego agent is in green annotated by "E" and the predicted trajectories for surrounding agents are in orange annotated by "S".  
%     }
%     \label{fig: interactive behavior}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \subfloat[A proactive nudging behavior from the ego car to switch a lane.]{
        \includegraphics[width=0.9\textwidth]{figures/nudge_behavior.png}
        \label{fig:nudging}
    }
    \vspace{0.1em}  % Add some vertical space between subfigures
    \subfloat[A yielding behavior for the ego car to make a turn.]{
        \includegraphics[width=0.9\textwidth]{figures/yielding_behavior.png}
        \label{fig:yielding}
    }
    \caption{For each scenario, upper row: interactive behavior from our predictive planner; 
    lower row: Ground truth driving behaviors.
    The ego agent's trajectory is in green annotated by ``E" and the surrounding agents' trajectories are in orange annotated by``S".
    The other agents' trajectories (always shown with ground truth) are in blue.}
    \label{fig: interactive behavior}
\end{figure*}

\subsubsection{\textbf{Methods and Baselines}}

We compare our consistency model-based approach against several baselines.
Currently, we only provide the evaluation of the open-loop trajectory generation performance of those methods, focusing on the behavior through a single inference step.
\paragraph{\textbf{Transformer}} A model based on MTR's encoder-decoder architecture \citep{shi2022motion} to generate trajectory only for ego vehicle.
It has a larger model size for the decoder (512 vs. our 128), but doesn't incorporate ego vehicle's goal $s_e^g$ as input.
\paragraph{\textbf{DDPM}} This diffusion baseline uses the same MTR encoder and MLP architecture as ours for condition encoding but adopts DDPM \citep{ho2020denoising} for trajectory sampling of ego and surrounding agents. 
Note that it differs from consistency models in both its forward diffusion process and training loss function.
We evaluate two variants: one trained and sampled with 4 steps (DDPM-4), and another trained and sampled with 10 steps (DDPM-10).
\paragraph{\textbf{DDIM}}  This baseline extends DDPM-10 by incorporating the more efficient DDIM sampling strategy \citep{song2020denoising}.
While using the same training process as DDPM-10, it employs 4-step DDIM sampling during inference, denoted as DDIM-4.
\paragraph{\textbf{Consistency}} Our proposed consistency model is trained and sampled with 4 steps.
\paragraph{\textbf{Consistency (guided)}} Our proposed consistency model is trained and sampled with 4 steps, with guided sampling.

\subsubsection{\textbf{Training}}
We conduct distributed training with a total batch size of 100 across 4 NVIDIA L40 GPUs, each with a batch size of 25.
For consistency and diffusion models, we use the Adam optimizer \citep{kingma2014adam} with a learning rate of $8\times10^{-5}$ and train for 50 epochs.
For the transformer, we follow the MTR settings \citep{shi2022motion} and use the AdamW optimizer \citep{loshchilov2017decoupled} with a learning rate of $1\times10^{-4}$ and weight decay of 0.01, and train for 30 epochs.

\subsection{Experiment results and analysis}

In this section, we evaluate our consistency model-based planner from several aspects.
First, we assess its planning performance and trajectory quality compared to several baseline methods.
Then we take a closer look at interactive scenarios and present examples of proactive nudging and yielding behaviors of our predictive planner.
Furthermore, we show how online guided sampling improves constraint satisfaction in planning - the controllability not provided with transformer-based approaches.
Finally, we discuss the computational time during inference for our presented methods.


\subsubsection{\textbf{Trajectory Planning Performance}}

In Fig. \ref{fig: ego agent's planned trajectory}, we compare planned trajectories generated by different methods (shown in green) against the ground truth from the dataset (shown in red).
The scenario requires the ego vehicle to make a left turn and then drive straight to reach its goal.
According to Fig. \ref{fig: ego agent's planned trajectory}, 
DDPM-4 reaches the goal but produces a noisy trajectory, likely due to insufficient sampling steps for diffusion models to generate high-quality trajectories.
While DDPM-10's increased sampling steps yields slightly smoother trajectories, it comes at the cost of longer computation time.
DDIM-4 attempts to accelerate DDPM-10 using only 4 sampling steps, but fails to generate goal-reaching trajectories.
The transformer with a larger model size generates a trajectory close to the groundtruth but misses the exact goal location.
In contrast, our consistency model generates smooth trajectories that both align with the ground truth and precisely reach the target location.


In Table \ref{tab:prediction metrics}, we quantitatively evaluate the planning performance for the ego vehicle compared to the human groundtruth through metrics from Waymo's Motion Prediction Challenge \citep{waymo2024motion}: Minimum Average Displacement Error (minADE) and Minimum Final Displacement Error (minFDE) on the entire test dataset.
Here we view these metrics as an evaluation of how closely our generated trajectories align with human driving behavior under the same environmental context and goal conditions.
The results align with our qualitative observations: our consistency model achieves the lowest minADE and minFDE, indicating that it captures human driving patterns well.
Moreover, the addition of guided sampling significantly improves the minFDE for the consistency models since it explicitly incorporates goal-reaching constraints.

% \bae{Just wondering, are the other methods also consider goal positions? If not, how would the performance be if other methods also additionally consider the goal position, while the structure remains the same? -- this is to verify the consistency model is effective, not the goal position given and known.}
% In Table \ref{tab:trajectory quality}, we evaluate the trajectory quality of all methods using three metrics: angle changes, path length and curvature.
In Table \ref{tab:trajectory quality}, we evaluate the trajectory quality using three metrics that characterize driving behavior: 1) angle changes, which measure the smoothness of directional transitions, 2) path length, which quantifies the efficiency of the chosen route, and 3) curvature, which assesses the sharpness of turns. 
Lower values in these metrics generally indicate smoother and more efficient trajectories that align with natural driving patterns.
Our consistency model achieves superior trajectory quality compared to all diffusion-based methods, including DDPM-10 despite its additional sampling steps.
while the transformer method has slightly lower angle changes and curvature, this is expected given its substantially larger model size (512 vs. our 128 dimensions).

\begin{table}[t]
    \centering
    \caption{Planning Trajectory Quality}
    \label{tab:trajectory quality}
    \begin{tabular}{lccc}
    \toprule
    Method & Angle changes $\downarrow$ & Path len $\downarrow$ & Curvature $\downarrow$ \\
    \midrule
    Transformer & \textbf{0.419} & 59.621 & \textbf{0.560} \\
    \midrule
    DDPM-10  & 0.660 & 87.801 & 3.444 \\
    DDPM-4 & 0.729 & 79.731 & 2.211 \\
    DDIM-4 & 0.683 & 157.78 & 2.012 \\
    Consistency & 0.636 & 58.060 & 1.174 \\
    Consistency (guided) & \textbf{0.445} & \textbf{57.564} & \textbf{0.610} \\
    \bottomrule
    \end{tabular}
\end{table}


Furthermore, we demonstrate the flexibility of our approach in choosing goal location in Fig. \ref{fig: different goal}.
Our consistency model maintains high-quality trajectory planning when targeting novel goal locations, even when they are not present in the dataset.
The current transformer method with MTR doesn't support the selection of goal locations.


\subsubsection{\textbf{Interactive Behavior}}

A key advantage of our predictive planner is its ability to ensure safe and effective interactions with other road users in traffic.
Our planner demonstrates the capability for generating both proactive behaviors as shown in Fig. \ref{fig: interactive behavior}, such as nudging for lane switching and yielding to pedestrians.
The ego agent (labeled ``E") trajectory is in green and the surrounding agents (labeled ``S") trajectories are in orange.
Other agent's trajectories are shown in blue.
These learned behaviors are similar to ground truth human drivers and ensure efficient navigation and safe interaction between the ego vehicle and other road users.
Crucially, all planning and prediction are achieved through a single inference of the consistency model.

% Figure \ref{fig: interactive behavior} illustrates two interactive planning scenarios. 
% The ego agent is labeled "E" with the trajectory shown in green, while the surrounding agents are labeled "S," with their trajectories shown in orange.
% Other agent's trajectories are shown in blue.
In Fig.~\ref{fig:nudging}, we show a proactive nudging scenario. 
Here, the ego vehicle needs to merge into the left lane while a nearby vehicle occupies that lane. 
Our planner directs the ego vehicle to accelerate and nudge into the left lane, predicting that the vehicle in the left lane will decelerate to accommodate the lane change.
This nudging behavior demonstrates the proactive planning capability of our end-to-end, data-driven predictive planner, which is challenging to achieve using modular methods.
While in the ground truth, the left car doesn't decelerate that much and the rightmost vehicle eventually turns right, 
% instead of continuing straight as predicted by our planner
this discrepancy is expected given that we currently generate 8-second future trajectories through a single inference.

In Fig.~\ref{fig:yielding}, a yielding behavior is presented. 
The ego vehicle intends to make a right turn but predicts that a pedestrian will continue crossing the road.
As a result, the planner instructs the ego vehicle to wait until the pedestrian has fully crossed, which is highly aligned with the ground truth.
This yielding behavior ensures safe interaction between the ego vehicle and other road users, highlighting the planner's safe operation in dynamic environments.

% \bae{I agree that the results show ``interactiveness'' of the ego vehicle, i.e., stopping for the pedestrian. However, it does not necessarily represent ``proactive'' behaviors that we highlighted in the Introduction section. Do you have any examples where the planning accounts for resulting behavioral changes of other agents (i.e., nudging)?}



\subsubsection{\textbf{Guided Sampling}}

The guided sampling approach described in Sec. \ref{sec: guided sampling} and Sec. \ref{sec: planning constraints} allows us to enforce planning constraints during inference time, providing a key advantage over transformer-based methods which lack this capability.
% In Fig. \ref{fig: guided sampling}, we compare trajectories generated with and without guided sampling.
% The guided trajectory demonstrates more accurate goal-reaching and smoother motions, with improvements highlighted in the circled regions.
We quantitatively evaluate all methods against three planning constraints from Eq. \eqref{eq: 3 planning constraints}: goal reaching, acceleration limits, and orientation speed limits.
As shown in Table \ref{tab:planning constraints}, our base consistency model already achieves more accurate goal-reaching and smoother behaviors
% significantly fewer constraint violations 
than other diffusion-based approaches.
With guided sampling, these planning constraint violations of the consistency model further decrease by a factor of 4, achieving the lowest values among all methods.
Note that this improvement requires no model retraining and only applies during inference.

\begin{table}[t]
    \centering
    \caption{Planning Constraint Violations}
    \label{tab:planning constraints}
    \begin{tabular}{lccc}
    \toprule
    Method & Goal Reaching $\downarrow$ & Acc limit $\downarrow$ & $\omega$ limit $\downarrow$ \\
    \midrule
    Transformer & 9.256 & 0.570 & 1.029 \\
    \midrule
    DDPM-10 & 12.669 & 80.581 & 3.307 \\
    DDPM-4 & 13.873 & 56.170 & 3.323 \\
    DDIM-4 & 22.933 & 218.823 & 5.270 \\
    Consistency & 0.618 & 5.754 & 2.050 \\
    Consistency (guided) & \textbf{0.127}  & \textbf{0.462} & \textbf{0.599} \\
    \bottomrule
    \end{tabular}
\end{table}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/guidance_difference.png}
%     \caption{
%     Trajectory plans generated with and without guided sampling from consistency models.\di{ These look identical. After staring at them for awhile I've realized the final heading is different, but I'm still not sure what is being circled.\bae{To me, the guided one has less noises in the circled area -- if that's what AJ wanted to highlight. But agreed, it's not very visible. }}
%     }
%     \label{fig: guided sampling}
% \end{figure}

\subsubsection{\textbf{Computational time discussions}}

All methods in our evaluation support batch sampling. 
As shown in Table \ref{tab:computation_time}, we evaluate their inference times using a batch of 30 scenarios with 125 ego agents on an NVIDIA GeForce RTX 3060. 
While the transformer achieves the fastest inference time, it lacks our method's key capabilities: online controllable generation and sampling from multimodal joint distribution.
Our consistency model achieves a similar inference time with DDPM-4 and DDIM-4 due to the same model dimensionality, and sampling steps.
However, DDPM-4 and DDIM-4 fail to match our consistency model's performance in trajectory generation across all metrics, as evidenced in Fig. \ref{fig: ego agent's planned trajectory}, Table \ref{tab:prediction metrics}, Table \ref{tab:trajectory quality}, and Table \ref{tab:planning constraints}.
The guided sampling, introducing a little computational overhead, provides higher quality trajectory generation and enables controllable planning.
Even DDPM-10, despite its increased sampling steps and higher computational time,  cannot achieve a comparable performance of trajectory generation to our approach.

\begin{table}[t]
\centering
\caption{Computational Time Comparison for Inference}
\label{tab:computation_time}
\begin{tabular}{lc}
\toprule
Method & Inference Time (s) \\
\midrule
Transformer & \textbf{1.910} \\
\midrule
DDPM-10 & 3.352 \\
DDPM-4 & 2.226 \\
% DDPM-1000 & 79.34 \\
DDIM-4 & 2.118 \\
Consistency & 2.301 \\
Consistency (Guided) & 2.635 \\
\bottomrule
\end{tabular}
\end{table}
