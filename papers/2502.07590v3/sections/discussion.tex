\section{Limitations and Future Work}

\noindent\textbf{Finer-grained query-specific sparsity.} 
We currently use a uniform sparsity level for all queries in an attention head.
Exploring query-specific sparsity level may further improve the performance gain but also increase implementation complexity.

\noindent\textbf{Dynamic sparsity on pipeline parallelism.} Current video DiTs typically have under 30B parameters~\cite{hong2022cogvideo,yang2024cogvideox,polyak2024movie,OpenSora}, and tensor parallelism alone is sufficient. 
However, as model sizes grow, pipeline parallelism may become necessary, raising load balancing challenges due to dynamic sparsity across different stages. Efficient runtime management and block re-assignment strategies to handle these imbalances are an interesting problem.

\noindent\textbf{Kernel implementation.} Current Triton-based attention operation has slower backward performance than the CUDA version, as noted in our measurements and prior work~\cite{tritonflashattention,dao2023flashattention2}. Optimizing with CUDA could improve performance.
