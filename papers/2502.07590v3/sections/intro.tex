\section{Introduction}

Text-to-video generation has experienced significant breakthroughs recently and received widespread interest~\cite{esser2024stablediff3, chen2023pixart, peebles2023scalableDiT, ma2024latte, klingvideo, HunyuanVideo, bao2024vidu, polyak2024movie}. Behind these advancements is the Diffusion Transformers (DiTs)~\cite{peebles2023scalableDiT,ma2024latte,Openai-sora}, which have emerged as state-of-the-art architectures within diffusion-based generative paradigms~\cite{ho2020denoisingddpm,lipman2022flowmatching}. These paradigms learn data distributions by progressively adding noise to input samples and training a model to reverse this process. During inference, the model starts with random noise and iteratively denoises it to generate high-quality videos.


In contrast to large language models (LLMs) \cite{openai2023gpt4,llama-3-1}, which use transformer also but scale to hundreds of billions of parameters, DiTs are smaller, typically under tens of billions of parameters \cite{OpenSora,yang2024cogvideox,polyak2024movie,HunyuanVideo}. While their smaller scale reduces some challenges in training, video DiTs still struggle in processing long, high-resolution video inputs---a growing concern fueled by expanding datasets and applications like film post-production and multi-camera event capture \cite{Openai-sora,polyak2024movie}. For example, even with compressed latent video representations from variational autoencoders (VAEs) \cite{stabilityai_stable_diffusion}, token length for high-definition or long sequences can easily reach hundreds of thousands.

The primary bottleneck lies in the attention module~\cite{waswani2017attention}, which has quadratic time complexity with respect to input length and can consume over 80\% of training time (Figure~\ref{fig:attention_bottleneck}). This issue worsens when the latent token length exceeds hundreds of thousands, as a single GPU cannot process the entire sequence in memory. 
{Context parallelism}~\cite{liu2023ring,fang2024usp,jacobs2023deepspeedulysses,gu2024loongtrain} addresses this by distributing the input across GPUs for parallel processing, but it introduces  additional challenges with inter-device communication.

Fortunately, our empirical observations reveal that attention computations in video DiTs exhibit notable sparsity that may mitigate the attention bottleneck~\cite{zhang2023h2o,xiao2023streamllm}. Attention scores tend to follow a power-law distribution, with a few key-value (KV) pairs dominating the total score. However, unlike some predictable attention patterns observed in LLMs (e.g. attention sinks or window pattern~\cite{mistral7b,xiao2023streamllm}), we observe that attention sparsity in video DiTs is \textit{dynamic}: 
The critical KV positions in video DiTs do not have clear locality or other patterns.
The sparsity level and pattern varies not only across blocks but also across attention heads within the same block. 
Finally, sparsity also intensifies progressively during training. 
These observations, reported for the first time to our knowledge, show that methods assuming fixed sparsity patterns are ineffective for DiT training, and a \textit{dynamic} approach is needed to reap attention sparsity effectively.

A natural solution to leverage dynamic sparsity is to compute the attention scores and find critical KV at each attention operation. 
This entails two problems. 
First, large-scale training relies on I/O-aware fused kernels for maximum efficiency~\cite{dao2022flashattention1,dao2023flashattention2}. 
Extracting the full attention matrix disrupts these optimizations and causes significant slowdown. 
Second, even if critical KV pairs are identified, most attention computation (e.g., softmax score) would already be completed, which eliminates most performance benefits. 
Additionally, current context parallelism~\cite{jacobs2023deepspeedulysses,liu2023ring} cannot effectively capture the heterogeneous sparsity across attention heads, leading to suboptimal performance.


Based on these insights, we present \sys, a framework that accelerates the video DiT training by exploiting dynamic sparsity patterns in attention while maintaining model quality. The core idea of \sys is to approximate attention scores using distinct predictors for each attention head. This facilitates the identification of critical KV pairs without tearing up the fused attention kernels.
Attention can then be computed only on these critical pairs, and context parallelism can be adjusted accordingly to only exchange information for them. Specifically, \sys comprises three key components:

First, \sys employs a two-stage DiT training approach to leverage dynamic sparsity. In the first stage, it trains low-rank predictors to approximate $QK^T$ for each attention head independently from the DiT training. Once the predictors are trained, \sys transitions to the second stage: It dynamically assesses the cost-benefit trade-off to activate sparse attention on individual blocks based on their profiled sparsity levels. 
It then uses the predictors to estimate the critical KV pairs.

Second, to efficiently carry out critical KV estimation, \sys employs an efficient kernel that fuses the approximation (i.e. MatMul) and selection (i.e. \topk) operations into a single kernel.
The fused kernel updates the \topk selection in-situ without storing the full $QK^T$ tensor with $\mathcal{O}(seq\_len^{2})$ elements, thus reducing memory footprint and computation cost.
Additionally, \sys realizes an efficient sparse attention kernel using query grouping. 
We leverage the observation that queries of adjacent tokens in the latent space often share a significant portion of critical KV pairs.
Their sparse attention computation can then be done together to maximize memory access parallelism and SM utilization.

Third, \sys introduces a sparsity-aware context parallelism (CP) approach. 
Dynamic sparsity breaks CP's basic assumption that each attention head has uniform computation and communication cost due to heterogeneous sparsity across heads. 
\sys adopts a hybrid strategy that performs CP in both head and sequence dimensions: sparse HCP adjusts the head assignment according to per-head sparsity levels to balance the computation load across devices, and sparse SCP further reduces communication overhead by exchanging only the critical KV pairs.
The optimal hybrid configuration is determined by solving an optimization problem that models the effect of both forms of CP.

We build \sys based on PyTorch FSDP~\cite{zhao2023pytorchfsdp} and evaluate it on a testbed with up to 128 H800 GPUs using DiT models ranging from 0.8B to 30B parameters. \sys achieves up to 3.02$\times$ higher training throughput than baseline for input lengths up to 520k, while also reducing end-to-end latency by up to 3.5$\times$. \sys delivers video quality on par with full attention paradigms as also confirmed by human user study.

In summary, our contributions are threefold:

\begin{itemize}[leftmargin=*]
    \item We systematically analyze attention patterns in video DiT training, revealing the existence of critical KV, their unpredictable distribution, heterogeneous sparsity across heads and blocks, and the dynamic evolution of sparsity levels.
    
    \item We propose \sys, a video DiT training framework that leverages dynamic sparsity in attention. \sys integrates adaptive sparse attention computation with specialized kernel and hybrid sparsity-aware context parallelism for optimized performance with multiple devices.
    
    \item We comprehensively evaluate the algorithmic performance and system efficiency of \sys across diverse video generation datasets and video DiT sizes. Results show that \sys maintains quality comparable to full attention while significantly improving throughput and speed.
    
\end{itemize}
