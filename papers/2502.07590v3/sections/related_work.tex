\section{Related Work}
\noindent\textbf{Video DiT training and inference.} Prior work explores novel attention mechanisms, such as 3D attention in expert transformers \cite{xu2024easyanimate}, hybrid spatio-temporal and full 3D attention \cite{HunyuanVideo}, and efficiency-focused designs with local attention and token merging~\cite{wang2024qihoot2xefficiencyfocuseddiffusiontransformer,pu2024efficient}. In contrast, \sys preserves model integrity by replacing only the attention module, ensuring compatibility with any architecture without structural changes. For inference, studies \cite{yuan2024ditfastattnattentioncompressiondiffusion,sun2024unveiling,kahatapitiya2024adaptive} have optimized computation by reusing cached activations via activation similarities across generation steps, which complements \sys's efficiency gains derived from its inherent sparsity.







\noindent\textbf{Attention sparsity exploration.} FlashAttention~\cite{dao2022flashattention1,dao2023flashattention2} provides block-level sparse attention but requires static sparsity patterns. Some work on LLMs has focused on sparsity during inference, particularly for long context. Methods like sliding window attention~\cite{mistral7b}, StreamingLLM~\cite{xiao2023streamllm}, and Minference~\cite{jiang2024minference} optimize efficiency using predefined or profile-based patterns. Besides, some studies explore trainable sparse attention~\cite{gao2024seerattention,yuan2025native}, using modules to learn block-level sparsity with training or fine-tuning. Similar techniques have been applied to video DiT inference~\cite{xi2025sparse,zhang2025fastvideotileatten}, though they typically impose fixed attention pattern and focus on inference in pre-trained models. 
In contrast, \sys is the first to adaptively exploit inherent sparse attention patterns for video DiT training while exploring sparse context parallelism.

