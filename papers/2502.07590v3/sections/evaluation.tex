\section{Evaluation}\label{sec:eval}

\noindent\textbf{Setup.} We evaluate on servers with 8 NVIDIA H800 GPUs connected via NVLinks and 200 Gbps RoCE for cross-node communication. 
Training is done in BF16 except for gradient reduction and optimizer updates which use FP32.

\noindent\textbf{Datasets.} We adopt four datasets: the widely used UCF-101~\cite{soomro2012ucf101} and Webvid-10M~\cite{Bain21webvid_10M} by prior work, and the more recent VideoGen~\cite{tan2024vidgen} and OpenVid~\cite{nan2024openvid} that feature high-definition videos with detailed text descriptions.

\noindent\textbf{Models.} The model configurations are listed in Table~\ref{table:model_config}. Their architectures, based on DiT blocks with 3D self-attention and cross-attention, are similar to Meta MovieGen~\cite{polyak2024movie} and other SOTA models~\cite{klingvideo,HunyuanVideo}. For video compression, we use Stability-AI's VAE~\cite{stabilityai_stable_diffusion} to reduce spatial resolution by 8$\times$8. The CLIP text encoder~\cite{clip_textembedder} is used for UCF-101 and WebVid-10M, while the T5-xxl text encoder~\cite{T5encoder} is applied to VideoGen and OpenVid. Models are trained with the Adam optimizer (learning rate: 1e-4) and gradient checkpointing~\cite{chen2016gradcheckpoint} is enabled. Flow matching~\cite{lipman2022flowmatching}, a widely adopted generative modeling approach, is used for its advanced performance~\cite{OpenSora,HunyuanVideo,polyak2024movie}.

\noindent\textbf{Baselines.} We compare \sys with the following baselines: %

\begin{itemize}[leftmargin=*]
    \item \textbf{Vanilla full attention (FA).} The 3D full attention computation based on FlashAttention-2~\cite{dao2023flashattention2,tritonflashattention} is the de facto implementation choice today.

    \item \textbf{Window-based attention (WA).} A common way to use sparse attention in video models~\cite{hassani2023neighborhood,zhang2025fastvideotileatten} where a query attends to KV pairs in a 3D window centered on itself. We explored WA-Medium ({WA-M}) with each dimension at 1/3 of the original size, and WA-Large ({WA-L}) at 2/3 instead.
\end{itemize}

For context parallelism, we prioritize head-wise CP for baselines due to its superior performance in our testbed. If the CP degree exceeds the number of head, we adopt intra-node head-wise CP with inter-node sequence-wise CP.

\begin{table}[t]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccccc}
\hline
Model & \#Layer & \#Head & Head size & Activation & Norm Type \\ \hline
0.8B & 28 & 12 & 96 & GeGLU & AdaLayerNormSingle \\
2.7B & 32 & 16 & 128 & GeGLU & AdaLayerNormSingle \\
30B & 42 & 24 & 256 & GeLU-approximate & AdaLayerNormSingle \\ \hline
\end{tabular}%
}
\caption{Model configurations in our evaluation.}
\label{table:model_config}
\vspace{-4mm}
\end{table}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/comparison_loss_no_sparsity_small.pdf} 
\caption{Comparison of training loss and validation loss throughout the training process for different methods across four datasets.}
  \label{fig:losses} 
\end{figure*}



\subsection{Overall Model Quality}
\label{sec:quality}
We compare \sys's training convergence and model quality with baselines across various configurations. 
A small 0.8B model is trained on UCF-101 and WebVid-10M with a latent input size of 16$\times$16$\times$16 (4k tokens), while bigger 2.7B models are trained on VideoGen and OpenVid with latent input of 32$\times$32$\times$32 (32k tokens) and 40$\times$56$\times$56 (125k tokens), respectively. 

\noindent\textbf{Training and validation loss.} Figure~\ref{fig:losses} compares the training and validation loss trends across different attention mechanisms. 
These losses reflect model improvement in the flow matching paradigm~\cite{polyak2024movie}. 
\sys exhibits faster convergence and achieves lower final loss values compared to WA-M and WA-L, and is comparable to FA across all datasets and models. Notably, WA-M fails to converge {on UCF-101}. Moreover, for validation loss, \sys also closely matches FA while outperforming both WA methods.



\noindent\textbf{Video quality.} Table~\ref{table:quality_comp} presents the video generation quality of \sys compared to other baselines across various datasets and evaluation metrics. 
Consistent with prior work, we use FVD~\cite{unterthiner2018fvd} to quantify differences between generated and original videos, supplemented by VBench~\cite{huang2024vbench} which measures quality and semantic coherence through model-based methods.

In terms of FVD, \sys consistently achieves the lowest values or performs comparably to FA, indicating its ability to generate videos that closely resemble the ground truth (FA). In contrast, WA-M consistently exhibits poor performance. Similarly, for semantic and quality scores in VBench, \sys continues to demonstrate superior performance.


\begin{table}[t]
\centering
\begin{minipage}[t]{0.26\textwidth} %
\vspace{0pt} %
\resizebox{\columnwidth}{!}{%
\huge
\begin{tabular}{lcccc}
\hline
 &  & \multicolumn{3}{c}{\textbf{Quality}} \\ \cline{3-5} 
\textbf{Dataset} & \textbf{Method} & \textit{FVD $\downarrow$} & \textit{Quality $\uparrow$} & \textit{Semantic $\uparrow$} \\ \hline
\multirow{4}{*}{UCF-101} & WA-M & 700.98 & 69.18\% & 55.03\% \\
 & WA-L & 520.92 & 70.70\% & 55.84\% \\
 & FA & 440.32 & 71.00\% & 56.23\% \\ \cline{2-5} 
 & \sys & 438.02 & 71.08\% & 56.60\% \\ \hline
\multirow{4}{*}{WebVid} & WA-M & 580.12 & 71.10\% & 40.10\% \\
 & WA-L & 530.38 & 73.23\% & 40.56\% \\
 & FA & 409.24 & 73.79\% & 42.56\% \\ \cline{2-5} 
 & \sys & 414.56 & 73.66\% & 42.88\% \\ \hline
\multirow{4}{*}{VideoGen} & WA-M & 1395.23 & 77.33\% & 53.08\% \\
 & WA-L & 1100.72 & 78.82\% & 53.32\% \\
 & FA & 908.91 & 79.39\% & 55.34\% \\ \cline{2-5} 
 & \sys & 834.32 & 79.14\% & 54.99\% \\ \hline
\multirow{4}{*}{OpenVid} & WA-M & 884.15 & 78.78\% & 55.98\% \\
 & WA-L & 826.43 & 79.47\% & 56.51\% \\
 & FA & 838.52 & 79.54\% & 56.07\% \\ \cline{2-5} 
 & \sys & 782.22 & 79.63\% & 56.36\% \\ \hline
\end{tabular}%
}
\caption{Comparison of model quality metrics on four datasets.} 
\label{table:quality_comp}
\end{minipage}%
\hfill %
\begin{minipage}[t]{0.2\textwidth} %
\vspace{0pt} %
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\hline
      & FA   & WA-M & WA-L & \sys  \\ \hline
\textbf{Score}$\uparrow$ & 4.25 & 2.89 & 3.81 & 4.57 \\ \hline
\end{tabular}%
}
\caption{Normalized user rating scores (1-5).}
\label{table:user_study}


\resizebox{\columnwidth}{!}{%
\huge
\begin{tabular}{cccccc}
\hline
\textbf{}                   & \textbf{}             & \textbf{} & \multicolumn{3}{c}{\textbf{Method}} \\ \cline{4-6} 
\textbf{Dataset} & \textbf{Model} & \textbf{Length} & \multicolumn{1}{l}{FA} & \multicolumn{1}{l}{WA-L} & \multicolumn{1}{l}{\sys} \\ \hline
\multirow{2}{*}{UCF-101}    & \multirow{2}{*}{2.7B} & 100k      & 703s        & 389s      & 220s      \\
                            &                       & 200k      & 2757s       &     1272s      & 810s      \\ \hline
\multirow{2}{*}{Webvid} & \multirow{2}{*}{30B}  & 50k       &  112s           &    63s    &   39s        \\
                            &                       & 100k      &  396s           &   207s        & 125s          \\ \hline
\end{tabular}%
}
\caption{Inference latency comparison for 40 sampling steps using CFG~\cite{ho2022cfg}. 2.7B model runs on 1 H800 GPU and 30B model runs on 4 H800 GPUs with TP=4.}
\label{table:inference}
\end{minipage}
\vspace{-0.2in}
\end{table}

\noindent\textbf{User study.} We also conduct a user study with 30 volunteers, each blindly rating 15 sets of video samples generated by different methods in random order. Table~\ref{table:user_study} shows that \sys and FA generate videos with better quality to the human eye, significantly outperforming WA methods with WA-M receiving the lowest ratings.



\subsection{System Efficiency}
\label{sec:efficiency}
We evaluate \sys's efficiency compared to 3D FA and WA-L. We exclude WA-M in the comparison due to its consistently poor performance. 

\subsubsection{Training.} 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.485\textwidth]{figures/training_throughput_h.pdf} 
\caption{Training throughput comparison for 2.7B model with VideoGen (left) and 30B model with OpenVid (right). A tensor parallelism degree of 4 is used to accommodate the 30B model.}
  \label{fig:training_throughput} 
\end{figure}

In large-scale training, \sys significantly outperforms vanilla 3D FA and WA in training throughput. 
On the VideoGen dataset with a 2.7B model, \sys achieves 2.1-3.02$\times$ higher throughput over FA and 1.38-1.54$\times$ over WA-L on up to 128 GPUs while handling inputs of up to 520k tokens, as in Figure~\ref{fig:training_throughput}. 
On OpenVid with the 30B model, \sys outperforms FA by 2.06-2.53$\times$ and WA-L by 1.37-1.42$\times$.
FA suffers from high computational overhead due to its quadratic complexity in spatial and temporal dimensions. WA reduces this overhead by limiting attention to a fixed local window, achieving a fixed sparsity ratio ($\sim$70\% for WA-L) but fails to identify critical KV pairs. In contrast, \sys dynamically adapts to sparsity patterns across heads, achieving up to 98\% sparsity (see Figure~\ref{figure:videogen_sparsity} in Appendix~\ref{appendix:videogen_sparsity}). Additionally, \sys's optimized hybrid CP strategies addresses load imbalances and improves efficiency as well.

\subsubsection{Inference.} \sys also elevates inference efficiency via its low-rank sparsity predictors acquired during training and efficient kernel designs. 
This results in a 2.0-3.5$\times$ speedup over FA and a 1.4-2.6$\times$ speedup over WA-L, enabling faster inference without performance loss (see Table~\ref{table:inference}). Note CP is not used in the inference experiments.







\subsection{Deep Dive}


\subsubsection{\bf \em Critical KV prediction accuracy.} We evaluate our sparsity prediction for identifying critical KV pairs in a 2.7B VideoGen-trained model. As shown in Figure~\ref{fig:prediction_acc}, the prediction improves during the course of training and achieves over 90\% accuracy for most blocks after $\sim$100k iterations. 
The estimated KV pairs account for over 98\% of the total attention score compared to ground truth, as the most critical KV pairs are easily distinguishable. Although certain pairs are harder to identify, they seem to cause minimal effect on model performance.


\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.25\textwidth}
       \centering
  \includegraphics[width=\textwidth]{figures/kv_prediction_accuracy_heatmap.pdf}
\caption{Critical KV prediction accuracy.}
  \label{fig:prediction_acc} 
    \end{minipage}
    \hfill
    \hspace{-0.1in}
    \begin{minipage}[t]{0.215\textwidth}
      \centering
  \includegraphics[width=\textwidth]{figures/speed_up_over_different_sparsity.pdf} 
  \caption{The speedup over different sparsity of 2.7B model.}
  \label{fig:speed_up_different_sparsity} 
    \end{minipage}
   
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/kernel_speed_different_lengths.pdf} 
\caption{\sys's sparse attention module speedup and overhead breakdown with 90\% sparsity with different video token length. }
  \label{fig:kernel_speedup} 
\end{figure}





   



\subsubsection{\bf \em Kernel speedup and overhead breakdown.}
Figure~\ref{fig:kernel_speedup} shows \sys's efficient kernels deliver in total 2.2-5.7$\times$ and 3.3-4.0$\times$ speedups for forward and backward pass, respectively, at 90\% sparsity. This shows \sys's efficiency for long sequences.
Note the forward pass incurs overhead from estimating critical KV pairs and updating predictors.  
Figure~\ref{fig:speed_up_different_sparsity} further highlights \sys's speedup at varying sparsity levels for 100K sequences. It achieves over 15$\times$ speedup at 98\% sparsity compared to FA.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.475\textwidth]{figures/cp_ablation.pdf} 
\caption{Comparison of context parallelism schemes for a 2.7B model, with timings based on the slowest GPU processing the attention operation. Line plots show head sparsity within each sampled block. ``HS'' combines standard intra-node HCP and inter-node SCP. \sys(x,y) denotes a sparse HCP degree of x and sparse SCP degree of y.}
  \label{figure:cp_ablation} 
\end{figure}


\subsubsection{\bf \em Hybrid sparse context parallelism} 
We compare \sys's hybrid sparse CP against conventional CP for the 2.7B model with varying sparsity patterns, GPU counts, and sequence lengths. 
We analyze the end-to-end execution time of a self-attention operation, focusing on the slowest GPU processing time within the CP group while accounting for sparsity disparities sampled from training. Figure~\ref{figure:cp_ablation} shows the results.
In cases 0 and 2, significant head sparsity outliers cause severe straggler effects in standard HCP and sparse HCP even after re-balancing. 
\sys opts to use SCP only in these cases to fully balance the workload and reduce the communication cost via selective KV gathering compared to standard SCP. 
In case 1, more evenly distributed sparsity allows sparse HCP to slightly outperform standard HCP while reducing communication overhead compared to SCP. In case 3, with moderately uneven sparsity, \sys optimally configures hybrid CP to balance computation, outperforming both HCP and HS, and lowering communication compared to seq-wise CP with its hybrid combination.


\subsubsection{\bf \em Threshold for critical KV} 


\begin{figure}[]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/critical_KV_threshold.pdf} 
    \vspace{-0.1in}
\caption{The training and validation loss with different critical KV definition threshold on VideoGen. }
  \label{figure:critical_kv_threshold} 
\end{figure}



Recall we define critical KV with a fixed threshold, requiring them to contribute 90\% of the overall score (\Cref{sec:finding}). 
Here we analyze \sys's sensitivity to this threshold and find that it works for moderately large thresholds (>80\%).
A threshold set too low (40\%) omits too many critical KV and degrades model performance.
The results are shown in Figure~\ref{figure:critical_kv_threshold}.









