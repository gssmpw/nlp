\section{Background }
\label{sec:Background}

We start by providing a recap of video DiTs and the general challenges associated with large-scale video DiT training. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.475\textwidth]{figures/dit_overview.pdf}

\caption{Overview of video DiT training. (a) The main input is the video, which is compressed by a VAE (omitted here). The timestamp is used as conditioning, and the text prompt is used as the Key-Value input in the cross-attention module. (b) Interleaved spatial-temporal attention blocks. (c) 3D full attention blocks.}
\label{fig:dit_overview} 
\end{figure}


\subsection{Video DiT} 
\label{sec:video_dit}
Diffusion models have become a leading framework in generative modeling, excelling in image synthesis~\cite{esser2024stablediff3,chen2023pixart} and video generation~\cite{peebles2023scalableDiT,ma2024latte,klingvideo,HunyuanVideo,bao2024vidu,polyak2024movie}. These models corrupt data with noise in a forward process and learn to reconstruct it through a reverse generative process. Among them, DiT has emerged as the de facto backbone backbone~\cite{peebles2023scalableDiT,ma2024latte,OpenSora}.


In a typical video DiT training process, a video clip is first encoded by a VAE~\cite{stabilityai_stable_diffusion,peebles2023scalableDiT,OpenSora}, which compresses and downscales the input into a latent representation. Random noise is added to this latent representation, and the noised latent video-along with any conditioning information (e.g., timestamps or text prompts for text-to-video tasks)-is fed into the DiT model. 
As shown in Figure~\ref{fig:dit_overview}, the DiT model consists of multiple DiT blocks that process video tokens alongside conditional inputs, guiding video generation during training.
At the core of each DiT block are \textit{self-attention} modules, which employ \textit{interleaved spatial and temporal attention} or \textit{full attention} to capture the complex relationships among video tokens across different dimensions. Additionally, \textit{cross attention} aligns the video with text prompts, ensuring consistency between modalities. The model's output is used to compute the loss, following either a denoising diffusion probabilistic model paradigm~\cite{ho2020denoisingddpm} or a flow matching paradigm~\cite{lipman2022flowmatching,ma2024sit}.



\subsection{Various Self-Attention Paradigms} 
\label{sec:self_attention}

Self-attention, or multi-head attention, is widely used in natural language processing and computer vision~\cite{waswani2017attention,dosovitskiy2020imagevit} to capture long-range dependencies. Given an input sequence $H = [h_1, \cdots, h_S]^\top \in \mathbb{R}^{S \times d}$, where $S$ is the sequence length and $d$ the hidden dimension, each attention head maps $H$ to ${Q}$, ${K}$, and ${V}$ using learnable projection matrices $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$. The head output is computed as:
\[
H' = \mathrm{softmax}\Bigl(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\Bigr)\mathbf{V},
\]
where $d_k$ is the dimensionality of each attention head. Ultimately, the outputs from all attention heads are concatenated and combined to produce the final output.

For video DiTs, various attention paradigms have been developed. One approach employs spatial-temporal attention, alternating computations along spatial and temporal dimensions~\cite{ma2024latte}. While computationally efficient, studies have found it inadequate for capturing detailed information, prompting a shift to full attention paradigms~\cite{HunyuanVideo,polyak2024movie,yang2024cogvideox}.

Full attention computes interactions across all tokens in the 3D temporal-spatial space, outperforming spatial-temporal attention. However, it processes a significant number of tokens, often exceeding 100k-even in latent space (e.g., for a latent input of 32$\times$96$\times$96\footnote{Throughout this paper, latent video dimensions (e.g., 16$\times$16$\times$16) are ordered as frames, height, and width unless otherwise specified.}, 300k tokens are required). This makes full attention extremely compute-intensive.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.475\textwidth]{figures/attention_bottleneck.pdf} 

  \caption{Time breakdown for self-attention and other operations in different DiTs (left: 1.3B, right: 3B) with various token lengths in forward (FW, left bar) and backward (BW, right bar) computation.} 
  \label{fig:attention_bottleneck} 
\end{figure}



\subsection{Large-Scale Video DiT Training}
\label{sec:dit_training}
The efficiency of large-scale video DiT training is primarily influenced by two key aspects.

\noindent\textbf{Full attention bottleneck.}  In high-resolution, long-video processing, self-attention becomes a major bottleneck due to its $O(n^2)$ complexity with respect to the number of tokens and the non-causal nature of DiT, where the lengths of ${Q}$, ${K}$, and ${V}$ all match the number of video tokens.
In contrast, cross-attention is far more efficient, as only ${Q}$ matches the video token count, while ${K}$ and ${V}$ are constrained to the text prompt length (typically under 120 tokens). As shown in Figure~\ref{fig:attention_bottleneck}, self-attention increasingly dominates computation time as the number of video tokens grows. For example, in 1.3B and 3B models with a sequence length of 200K, self-attention accounts for 92\% and 93\% of forward and backward computation time, respectively.
Thus, developing efficient attention mechanisms is critical to alleviating this bottleneck and enabling scalable video DiT training.

\noindent\textbf{Context parallelism.} Context parallelism (CP) enables long-sequence training by dividing sequences into chunks distributed across multiple GPUs. However, self-attention requires inter-device communication to process the entire sequence, with two common paradigms:

\begin{itemize}[leftmargin=*]
    \item \textbf{Sequence-wise CP.} QKV tensors are split into chunks of shape $[B, H, S/N, D]$, where $B$ is the batch size, $H$ the number of heads, $S$ the sequence length, $N$ the number of GPUs, and $D$ the head dimensionality. Each GPU processes its local query chunk across all heads, gathering KV from other GPUs to compute attention via block-wise operations~\cite{liu2023ring}, often using ring-based communication.

    \item \textbf{Head-wise CP.} GPUs also initially hold QKV chunks partitioned along the sequence dimension for all heads. After All-to-All operations, each GPU gets complete QKV tensors for a subset of heads, reshaped to $[B, H/N, S, D]$. GPUs then compute attention outputs independently for their assigned heads. A final All-to-All re-partitions the outputs to restore the original layout~\cite{jacobs2023deepspeedulysses}. 

\end{itemize}

These paradigms have trade-offs in computation efficiency and communication cost~\cite{gu2024loongtrain,fang2024usp}. Determining an efficient and optimal parallelism strategy is non-trivial, as it depends on the specific input case and hardware configuration.


