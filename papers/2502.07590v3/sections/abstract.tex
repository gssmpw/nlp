\begin{abstract}
Diffusion Transformers (DiTs) have shown remarkable performance in generating high-quality videos. However, the quadratic complexity of 3D full attention remains a bottleneck in scaling DiT training, especially with high-definition, lengthy videos, where it can consume up to 95\% of processing time and demand specialized context parallelism.

This paper introduces \sys to accelerate video DiT training by leveraging the dynamic attention sparsity we empirically observe. \sys uses a two-stage algorithm to capture the dynamic sparsity patterns via low-rank based approximation of the original query and key. It employs custom kernels to efficiently identify critical key-value pairs and compute the sparse attention. 
To accommodate the new sparsity dimension, \sys adopts a hybrid sparsity-aware context parallelism that re-balances the skewed workload across attention heads and blocks due to sparsity heterogeneity. \sys achieves up to 3.02$\times$ higher training throughput, scaling to 128 GPUs and 520k token lengths, without quality loss.

\end{abstract}
