
\section{Implementation}

We implement \sys on PyTorch's FSDP framework, extending it for tensor and context parallelism. The sparse attention kernel is built with Triton~\cite{openai_triton}, and critical KV estimation in CUDA. \sys provides a non-invasive module that integrates seamlessly with any training framework and DiT model. Its low-rank sparsity prediction is decoupled from the main model training, allowing replication across devices and manual control of computation and gradient synchronization.
Other optimizations such as CPU offloading for large KV indices are in Appendix~\ref{appendix:imple}.






