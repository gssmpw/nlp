
\section{Empirical Observations on the Sparsity of Attention in Video DiT}
\label{sec:finding}





We conduct an in-depth analysis of the sparsity patterns of attention in video DiT training across various datasets and model sizes, as in~\Cref{sec:eval}. Our findings, demonstrated through a case study, validate the rationale behind our system design. 
The case study focuses on a 2.7B model with an architecture similar to Meta's MovieGen~\cite{polyak2024movie}, representing a moderate scale for video generation~\cite{OpenSora,HunyuanVideo,ma2024latte}. The model is trained on the WebVid-10M dataset~\cite{Bain21webvid_10M} using 8 H100 GPUs with a global batch size of 32 and a latent input size of 16$\times$16$\times$16.





We begin by some definitions.
Given a query vector $q$, let $S_q = \{(k_i, v_i)\}_{i=1}^n$ denote the set of all $n$ key-value (KV) pairs. The attention score function $A(q, k_i) = \text{softmax}(q \cdot k_i)$ computes the scaled dot product between $q$ and each key $k_i$. The set of \textit{critical KV pairs} $I_q \subseteq S_q$ is then defined as the subset of pairs $(k_i, v_i)$ where $A(q, k_i)$ exceeds the $\theta$-percentile threshold of all attention scores.
The threshold $\theta$ can be set by a numerical cutoff or a cumulative sum, such that critical pairs account for 90\% of total attention. In this paper, we use a 90\% cumulative sum threshold as the default, i.e. {\textit{the critical KV pairs are the top ones that together represent 90\% of total attention}}.
The \textit{sparsity} of an attention head is then defined as the average proportion of non-critical KV pairs across all queries: $\mathbb{E}_{q \sim Q}\left[\frac{|S_q \setminus I_q|}{|S_q|}\right]$.









\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.32\textwidth}
      \vspace{0pt} 
       \centering
   \includegraphics[width=\textwidth]{figures/attn_score_power_law_distribution.pdf}
\caption{Left: The attention score distribution for each query in a histogram. Right: The cumulative distribution function of the sorted attention scores for each query. }
  \label{fig:atten_power_law} 
    \end{minipage}
    \hfill
    \hspace{-0.25in}
    \begin{minipage}[t]{0.15\textwidth}
      \vspace{0pt} 
      \centering
  \includegraphics[width=0.95\textwidth]{figures/output_diff.pdf}
  \caption{The output difference between attention with full KV and critical KV.}
  \label{fig:output_diff} 
    \end{minipage}
   
\end{figure}


\noindent\textbf{Attention sparsity and power-law distribution.} 
We first investigate the distribution of attention scores for each query to demonstrate attention sparsity.



\hypertarget{obs1}{
\textbf{\textit{Observation 1: Attention scores in sparse blocks follow a power-law distribution.}}}

As shown in Figure~\ref{fig:atten_power_law}, attention scores of queries in two blocks are highly skewed, with most being small (<0.001) and only a few large (>0.1). Notably, the few top keys dominate, with the top 10\% accounting for over 90\% of total scores in 95.2\% of queries (block 6) and 86.8\% (block 21), highlighting the inherent sparsity. Figure~\ref{fig:output_diff} further shows that using only the top 10\% of KV pairs yields minimal output differences.

The power-law distribution of attention scores suggests that a great portion of attention computation may be pruned without significantly impacting performance. By efficiently identifying critical KV pairs, we can potentially reduce computational costs while maintaining model quality.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.475\textwidth]{figures/kv_distribution.pdf}
\caption{Distribution of critical KV positions for a query (green) at position (15, 15, 15) in the 3D latent space. The visualized keys are those that yield attention scores exceeding the 90th percentile when attending to the query.}
    \label{fig:kv_distribution}
\end{figure}

\noindent\textbf{Locality of critical KV.} 
A common and simple method to identify critical KV pairs is to rely on locality: a token's query often attends to nearby tokens' keys or specific global keys, enabling the use of window-based patterns or token sinks as in LLM inference~\cite{xiao2023streamllm,mistral7b,zhang2023h2o}. Given the spatio-temporal dependencies in video DiT, we first investigate whether similar locality patterns exist and can be applied in this context.




\hypertarget{obs2}{
\textbf{\textit{Observation 2: Critical KV pairs do not exhibit locality patterns in video DiT.}}}



In Figure~\ref{fig:kv_distribution}, we visualize the critical KV positions in a 3D space for a query. Contrary to expectations, critical KV pairs do not have a specific pattern, unlike those found in LLM inference~\cite{xiao2023streamllm,mistral7b,zhang2023h2o}. 
More generally across queries, our experiment reveals that only 15.1\% of critical KV pairs are within a 5-token radius, while 48.5\% are more than 10 tokens away. 
This suggests that applying fixed patterns to approximate attention would not fare well in video DiT.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.485\textwidth]{figures/attention_sparsity_distribution_across_blocks.pdf} 

\caption{Sparsity of different heads across blocks in iteration 80k. }
  \label{fig:atten_sparsity_dist} 
\end{figure}

\noindent\textbf{Heterogeneity of sparsity.} 
We further establish the spatial heterogeneity of attention sparsity across different attention blocks and heads within a block, which adds to the difficulty of identifying critical KV pairs.


\hypertarget{obs3}{
\textbf{\textit{Observation 3: Sparsity varies significantly across attention blocks and heads within the same block.}}}

Our analysis reveals that sparsity varies not only across blocks but also among heads within the same block. Figure~\ref{fig:atten_sparsity_dist} shows head-level sparsity at iteration 80k, where the first 20 blocks are highly sparse, and latter blocks are less so. The box plot highlights variability within blocks-e.g., in block 2, most heads have 95\% sparsity, with outliers at 90\% and 80\%. Figure~\ref{fig:sparsity_across_blocks} confirms diverse sparsity patterns across blocks at different training steps. Besides, as training converges, \textit{an arch-shaped sparsity pattern} emerges: intermediate blocks become highly sparse, while initial and final blocks go dense.


This disparity aligns with prior studies~\cite{waswani2017attention,jin2024mohattentionmoe,li2019informationattention}, showing that attention heads capture distinct features, with some focusing on local details and others on global context. Uniform sparsity thresholds are suboptimal: low thresholds cause redundant computation, while high thresholds risk losing critical KV pairs for less sparse heads. Adaptive methods are needed to account for each block's and head's unique sparsity, optimizing efficiency and performance.




\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.335\textwidth}
       \vspace{0pt} %
       \centering
  \includegraphics[width=\textwidth]{figures/sparsity_change_as_iteration.pdf}

\caption{The change in sparsity of different attention heads during the training process in transformer blocks. Only two blocks are shown due to space constraints. }
  \label{fig:sparsity_change} 
    \end{minipage}
    \hfill
    \hspace{-0.25in}
    \begin{minipage}[t]{0.135\textwidth}
       \vspace{0pt} %
      \centering
  \includegraphics[width=\textwidth]{figures/sparsity_across_blocks.pdf} 

  \caption{The sparsity of all blocks across different steps.}
  \label{fig:sparsity_across_blocks} 
    \end{minipage}
   
\end{figure}



\noindent\textbf{Time-varying sparsity during training.} 
Continuing from the previous finding, we also observe that attention sparsity varies dynamically in the temporal dimension.

\hypertarget{obs4}{
\textbf{\textit{Observation 4: Sparsity also varies over the course of training before stabilizing.}}}

As illustrated in Figure~\ref{fig:sparsity_change}, sparsity of different heads becomes more pronounced as training progresses. For block 24, the average attention sparsity for each head increases from 0.30 to 0.78. Further, Figure~\ref{fig:sparsity_across_blocks} shows that across all attention blocks, the median sparsity increases from 0.81 at 50k iterations to 0.92 at 300k iterations.
This dynamic evolution of sparsity during training suggests that the strategy for leveraging sparsity should be adjusted dynamically: As the model learns to focus on the most relevant features, the attention mechanism becomes more selective, resulting in increased sparsity~\cite{vig2019analyzingatten}. This highlights the importance of dynamic methods that can capture and exploit the evolving sparsity patterns throughout the training process. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.425\textwidth]{figures/kv_overlap.pdf}
  \vspace{-0.1in}
    \caption{Critical KV pair overlap ratio between four anchor queries (green, at indices 0, 2124, 2220, 4095) with all queries in an attention block. Each subplot shows a 2D projection of the 3D video with rows representing flattened frames.}
  \label{fig:kv_overlap} 
\end{figure}



\noindent\textbf{Critical KV overlap ratio for adjacent tokens in 3D space.} 
Finally, we demonstrate an interesting phenomenon between queries of adjacent tokens. 

\hypertarget{obs5}{
\textbf{\textit{Observation 5: Adjacent tokens have similar critical KV pairs.}}}

We quantify this phenomenon by measuring the overlap of critical KV indices across queries. Figure~\ref{fig:kv_overlap} is a case study with four anchor queries (in green), where darker shades indicate higher overlap ratios. Significant overlaps in critical KV indices are observed among adjacent tokens. For tokens within a 2$\times$2$\times$2 3D cube (shown as a 2D projection), the four anchor queries and their adjacent queries exhibit over 92.4\% overlap in critical KV indices. On average, this overlap remains consistent at 80.1\% across blocks.
This aligns with the intuition that video pixels represent continuous signals, unlike discrete language signals, making adjacent tokens similar. It suggests neighboring queries tend to attend to similar KV pairs (though these KV pairs may not be close to the query as \hyperlink{obs2}{Obs.2} reveals), offering opportunities to optimize attention computation by leveraging the similarity in sparsity patterns among adjacent tokens.




