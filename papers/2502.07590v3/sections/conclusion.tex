\section{Conclusion}

We present \sys, a training acceleration framework leveraging video DiT attention sparsity. \sys employs a two-stage algorithm to enable adaptive sparse computation with specialized kernels. It also integrates hybrid sparsity-aware context parallelism to optimize sparse computation and communication across devices. These jointly yield up to 3.02$\times$ faster end-to-end training without sacrificing model quality on our 128-H800 testbed.


