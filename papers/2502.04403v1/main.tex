% RLDM.
\documentclass[11pt]{article} % For LaTeX2e

\usepackage{styles/rldmsubmit}
\usepackage{palatino}
\usepackage[authoryear,sort&compress,round]{natbib}
\input{crl_packages}
\input{crl_commands}


\title{Agency Is Frame-Dependent}


% Authors.
\author{
David Abel \\
Google DeepMind \\
\And
Andr{\' e} Barreto \\
Google DeepMind \\
\And
Michael Bowling \\
Amii, University of Alberta \\
\And
Will Dabney \\
Google DeepMind \\
\AND
Shi Dong \\
Google DeepMind \\
\And
Steven Hansen \\
Google DeepMind \\
\And
Anna Harutyunyan \\
Google DeepMind \\
\And
Khimya Khetarpal \\
Google DeepMind \\
\AND
Clare Lyle \\
Google DeepMind \\
\And
Razvan Pascanu \\
Google DeepMind \\
\And
Georgios Piliouras \\
Google DeepMind \\
\And
Doina Precup \\
Google DeepMind \\
\AND
Jonathan Richens \\
Google DeepMind \\
\And
Mark Rowland \\
Google DeepMind \\
\And
Tom Schaul \\
Google DeepMind \\
\And
Satinder Singh \\
Google DeepMind \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\maketitle

% --- Abstract ---
\begin{abstract}
%
% What is agency.
Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence.
%
% A puzzle of agency.
Determining if a system exhibits agency is a notoriously difficult question: \cite{dennett1989intentional}, for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. 
%
% Main claim: Agency is frame-dependent.
We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: \textit{Any measurement of a system's agency must be made relative to a reference frame}. 
%
% Barandiaran.
We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by \cite{barandiaran2009defining} and \cite{moreno2018minimal} are themselves frame-dependent. 
%
% Conclusion.
We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.
%
\end{abstract}

\keywords{
Agency, Philosophy of Reinforcement Learning
}

\acknowledgements{The authors would like to thank Kim Stachenfeld and Vlad Mnih for their thoughtful comments on a draft of the paper.} 


\startmain
% ------------------
% -- Introduction --
% ------------------
\section{Introduction}

% RL and agency.
%
Reinforcement learning (RL) involves learning or decision making over time to achieve a goal. Agents are often taken as the primary vehicles that carry out this learning and decision making, and as such have long been an essential element of RL. Moreover, \textit{agency} is the lifeblood of an agent---agency is the capacity that endows a given system with the status of agent-hood. Thus, agency also stands as one of the elemental concepts of RL. 
%
% Tomasello quote.
In \textit{The Evolution of Agency}, \citet{tomasello2022evolution} makes an even stronger case for the role of agency in psychology:
\begin{quote}
    Every scientific discipline begins with a proper domain, a first principle ... In psychology, depending on oneâ€™s theoretical predilections, that proper domain or first principle might be either behavior or mentality. But my preferred candidate would be agency, precisely because agency is the organizational framework within which both behavioral and mental processes operate.  (p. 134, \citeauthor{tomasello2022evolution}, \citeyear{tomasello2022evolution}).
\end{quote}
%
% RL is about agency.
Following similar reasoning to Tomasello, we take it as essential that the science of RL is borne from an understanding not just of intelligence, learning, and decision making, but also of agency. To this end, we here investigate a fundamental question about agency through the lens of RL: is agency an invariant, measurable property of an input-output system, or does it vary depending on other independent commitments? We draw from several distinct results within the RL literature to arrive at the conclusion that agency is fundamentally frame-dependent. 


% Main figure.
%
\begin{figure}[b!]
    \centering
    
    % Subfig 1: Four-Part definition of agency.
    %
    \subfigure[\label{subfig:agency_def}A Four-Part Account of Agency]{\includegraphics[width=0.3\textwidth]{figures/agency_def.pdf}} \hspace{8mm}
    %
    %
    % Subgfig 2: Frame-dependence.
    %
    \subfigure[\label{subfig:frames_example}Frame-Dependence]{\includegraphics[width=0.5\textwidth]{figures/ref_frames.pdf}}
    
    % Caption.
    %
    \caption{(Left) A four-part account of agency due largely to \cite{barandiaran2009defining}: a system such as a thermostat has agency if it has (1) a boundary, (2) is the source of its own actions, (3) has a goal, and (4) adaptively selects outputs based on inputs to pursue this goal. (Right) Our main claim: A determination of the agency of a system, such as a thermostat, is dependent on a choice of reference frame. The two reference frames depicted make different commitments about how we measure the four essential conditions of agency. For example, we could draw the boundary around our thermostat in several different ways, or understand the goal of the thermostat in different ways.} %, or we might choose to treat the thermostat as non-adaptive in that it executes a simple look up table.}
    
    \label{fig:frame_dependence}
\end{figure}


% Brief history/definition of Agency.
%
\paragraph{Agency.} Typical views of agency across biology \citep{ball2023organisms}, complex systems \citep{moreno2005agency,moreno2018minimal}, and philosophy \citep{dretske1999machines,barandiaran2009defining} roughly define the concept as an input-output system's capacity to steer outcomes toward a goal. 
%
% Barandiaran et al.
We build around one canonical definition of agency developed by \cite{barandiaran2009defining} that we present in four parts. %As a starting point, we suppose that are given some kind of information processing system that evolves over time, and would like to determine if there is a sub-component of the system that possesses agency.
%
% 1) Individuation.
First, to have agency, the system must be \textit{individual}; it has a boundary that separates it as an independent entity from its surroundings. %In RL, this is often down to choosing the agent's inputs and outputs. 
%
% 2) Source of action.
Second, once we have chosen a boundary, the system must be the source of its own action. 
%
% 3) Normativity.
Third, the system has some goals or norms that regulate its interactions with the environment. 
%
% 4) Adaptivity.
Fourth, the system steers its experience in light of these goals. 
%
% Quote def.
% Together, these conditions form Barandiaran et al.'s definition of an agent as "... an autonomous organization capable of adaptively regulating its coupling with the environment according to...norms" (p. 8). 
%
% Summary.
We take these four conditions to be a reasonable starting point for any account of agency, summarized in \cref{subfig:agency_def}. %
% Semantics won't change our conclusion.
For the purposes of our main argument, we do not take debates about the precise definition of agency to have a significant impact on our conclusion---we anticipate that regardless of how the semantics of "agent" or "agency" are worked out, the need for frame dependence will remain. For instance, Barandiaran et al. combine normativity and adaptivity into one property: this definition is perfectly valid, but will still admit frame-dependence. For further debates on this topic, see work by \cite{moreno2018minimal}, \cite{ball2023organisms}, or \cite{dretske1999machines}.


% Q: What systems have agency?
%
\paragraph{The Puzzle of Agency.} A fundamental puzzle then arises: which systems can be said to have agency? \cite{dennett1989intentional} considers the cases of a rock rolling down a hill and a thermostat modulating the temperature of a room---in what sense do these two systems possess agency, and to what extent? And, the more critical scientific question: what principles can we turn to in order to determine whether each of these systems possess agency? Barandiaran et al. stipulate that a system possesses agency if the four conditions are present within the system. If any one of them is missing, the system lacks agency. In this way, agency is taken to be binary, though naturally there is room for developing a non-binary account. 


% -------------------------------
% -- Agency is frame-dependent --
% -------------------------------
\section{Agency is Frame-Dependent}


% Main claim.
%
\paragraph{Main Claim.} We here address this puzzle by arguing that the attribution of agency to a system is \textit{fundamentally dependent on a choice of reference frame}. That is, the agency of any system is \textit{relative} in the sense that it depends on arbitrary commitments that we collectively call a reference frame. For example, one such commitment is whether the system is meaningfully pursuing a goal; depending on how we codify what counts as meaningful goal-pursuit, the system will either be understood as having agency or not.  %Notably, as these commitments are changed, the conclusions about a system's agency will change too. 
%
% Structure of the Argument.
We support this main claim by illustrating that each of the four properties of agency are themselves \textit{relative} to a choice of some reference object or commitment---that is, reaching a conclusion about whether a given system possesses each property requires an independent commitment whose choice is arbitrary.
%
% Note: purely philosophical.
At present, our definitions, claims, and arguments are purely philosophical, though we note that a rigorous presentation of this perspective is a natural and fruitful direction for future work. 


% What is a reference frame?
%
\paragraph{What is a Reference Frame?} An agent reference frame is a collection of these four commitments that allow us to determine whether a system has each of the four properties. That is, a frame must include (1) a boundary that decides what is internal to the agent and what is external; (2) a reference object such as a set of causal variables that allow for determination of whether the system is the cause of its action; (3) a principle for isolating whether the system is meaningfully pursuing a goal; and (4) a choice of what changes in behavior count as meaningful adaptation. There are many valid ways to formalise these components; a boundary could be a cut in a graph \citep{jiang2019value} or a Markov blanket \citep{friston2009reinforcement}. A precise mathematical construction of reference frames is a natural next step for further research. %, but again we choose to defer this exposition for future work due to the short length of this abstract. 


% (1) Individuality.
%
\paragraph{(1) Individuality.} In order to attribute agency to an entity, an observer first must determine which entity they are referring to. Establishing a boundary that separates this entity from its surroundings is a critical step in determining whether a system has agency. However, clearly identifying such a boundary is non-trivial. 
%
% (1a) Intuitive case.
Neils Bohr gives the example of a person wielding a stick---depending on the activity, the stick could be taken as a part of the person's propensity for both action and observation (p. 93, \citeauthor{klein1967glimpses}, \citeyear{klein1967glimpses}). As \cite{harutyunyan2020what} notes, we might plausibly draw the boundary around the person and exclude or include the stick. 
%
% (1b) RL case.
In fact, as argued by \cite{clark1998extended}, \cite{jiang2019value}, and \cite{harutyunyan2020what}, there are often many ways we can establish a boundary that separates an environment and an agent in a meaningful way. Jiang considers the example of a model-free learning algorithm that is implemented using a neural network to process its observations. As Jiang points out, the boundary we choose to draw could include the pseudo-random number generator and all layers of this network, or only include the last few layers of the network. Proposition 10 by \cite{jiang2019value} (further discussed in Section 6.1), illustrates that many important quantities of an RL agent, such as the optimal policy or Bellman error, are \textit{boundary-dependent}. We summarize these points in the following claim.


% Remark: individuality.
%
\begin{claim}[Adapted from \citeauthor{jiang2019value}, \citeyear{jiang2019value} and \citeauthor{harutyunyan2020what}, \citeyear{harutyunyan2020what}]
Individuality is frame-dependent: Nearly all agents admit many plausible boundaries one could draw that separates them from their environment. Moreover, key quantities of an agent can change depending on which boundary is chosen. 
\end{claim}


% Qualitative difference.
%
Individuality might be seen as qualitatively different from the other properties, since it involves selecting \textit{which} system we would like to attribute agency to and what its inputs and outputs are. As such, it is natural to conclude that we recover a different agent depending on how we draw the boundary. However, our argument does not stand only on the frame-dependency of individuality---since we take all four key properties of agency to be frame-dependent, our argument remains agnostic to any perceived qualitative difference between them. 
%
% Markov blankets.
%
% It is further worth noting that some work developed principles for boundary selection, such as using a Markov blanket \cite{pearl2014probabilistic} to identify a boundary, as proposed by \citep{friston2021parcels}. We do not have space to explore this perspective in full detail, but these and similar approaches offer at least one principled approach to choosing a boundary.


% (2) Source of action.
%
\paragraph{(2) Source of Action.} Second, a system must be the source of its own action. In the terms of \cite{ball2023organisms}, this property reflects whether the system is "pushed around by its environment", or does the pushing itself. 
%
% (2a) Intuitive cases.
For instance, a wall being knocked over by a wrecking ball could be understood as taking the action of being knocked over. However, the source of this action (and the corresponding potential energy) did not ultimately originate in the wall, but rather in the wrecking ball and its operator. It might therefore be better to view the wall as lacking in the source of action. In contrast, a bird flapping its wings intuitively satisfies the condition since this action is best thought of as originating from within the bird. 

%
% (2b) RL case.
\cite{kenton2023discovering} recently develop a causal account that determines which entities in a causal model might be said to satisfy roughly this property. The difficulty, as Kenton et al. note, is that reaching a conclusion about the source of action in a causal model rests \textit{entirely} on the choice of causal variables. In this way, identifying whether a given subsystem originates its own action depends on an independent, unrelated choice: the causal variables. Kenton et al. state directly: "Note [discovering an agent in a causal model] is relative to a frame -- a choice of variables that appear in our causal model" (p. 2, \citeauthor{kenton2023discovering}, \citeyear{kenton2023discovering}). Following this reasoning, we claim that the source of action is also frame-dependent in that it depends on an arbitrary upstream commitment. %

% Claim 2: SoA is Frame-Dependent.
%
\begin{claim}[Adapted from \citeauthor{kenton2023discovering}, \citeyear{kenton2023discovering}]
Source of action is frame-dependent: There exist cases with at least two plausible choices of causal variables where the former choice identifies an agent in the causal model, and the latter choice refutes the presence of an agent in the causal model.
\end{claim}


% (3) Normativity.
%
\paragraph{(3) Normativity.} Third, and perhaps most crucially, agency is about goal-directedness. The trouble is that every system with outputs can be understood \textit{as if} \citep{friedman1953essays} it is goal-directed. %; second, that identifying a singular goal or norm that regulates a system's outputs is often impossible. 


% (3a: as if) Intuitive case.
%
More concretely, every input-output system can be well-explained in terms of goal-directedness. For example, in the case of our thermostat, even a broken thermostat whose output always sets the temperature of a room to $20^\circ$ can be understood as having the goal "set the temperature to $20^\circ$". More trivially, a rock can be viewed as having the goal of rolling down a hill, or as having the goal of convincing all observers that it is a rock. What makes these meaningless, as opposed to meaningful goals? 
%
This challenge is reflected in one of the classical results of inverse RL first discussed by \cite{russell1998learning} and \cite{ng2000algorithms}: the zero reward function is always consistent with every system that outputs signals that are understood as decisions. In other words, reward is under-determined by behavior \citep{cao2021identifiability} without intervention \citep{amin2017repeated}. To overcome this challenge, a variety of approaches have explored the use of biases or other principles that constrain the space of viable reward functions, such as Occam's razor \citep{armstrong2018occam}, or the now standard approach invoking \textit{maximum entropy} proposed by \cite{ziebart2008maximum}. These upstream principles can in some cases rule out certain kinds of goal-directedness as uninteresting, or elevate others as meaningful. Hence, we must again invoke an additional principle to determine whether a given system is meaningfully goal-directed. One common approach is to ask how \textit{useful} it is to explain or understand the system in terms of goal-directedness, as first argued by \cite{dennett1989intentional}. %, and more recently examined under the lens of "appropriateness" by \cite{leibo2024theory}. 
%
In other words, to determine if a system \textit{meaningfully} has a goal, we require some extraneous commitments that again amount to a reference frame.


% Latter case.
%
% The latter challenge is related to determining what a system's goal actually could be. \cite{nguyen2020games} points out that the plausible goal or purpose of a person playing a sport could be one of many: the individual might want to exercise, or socialize, or compete. In other words, there is often more than one goal we can choose to explain or understand an individual's choices. And, depending on which goal we select, we might take the agent to be ineffectively pursuing its goal. Naturally, as Nguyen suggests, we might conduct experiments to reduce this uncertainty by placing a basketball player on top of a ladder next to the hoop. We can then determine if the player's goal is actually to put the ball in the net, or something else. 
%
% (3b) RL case.
% These perspectives are reflected in one of the classical results of inverse RL first discussed by \cite{russell1998learning} and \cite{ng2000algorithms}: the zero reward function is always consistent with every decision maker. In other words, reward is under-determined by behavior \citep{cao2021identifiability} without intervention \citep{amin2017repeated}. To overcome this challenge, a variety of approaches have explored the use of biases or other principles that constrain the space of viable reward functions, such as Occam's razor \citep{armstrong2018occam}, or the now standard approach invoking \textit{maximum entropy} proposed by \cite{ziebart2008maximum}. 
%
% Moreover, an agent's full preferences are not completely specified by reward alone, but require a commitment to some decision theoretic frame within which reward, risk, uncertainty, and time are balanced \cite{vonneumann1953theory,bowling2023settling}. 
%
% Regardless how these details are worked out, we suggest that without choosing some extraneous principle that narrows down the space of an agent's goals, we cannot attribute a unique goal to an agent. Indeed, this is reflected in the recent causal perspective of \cite{macdermott2024measuring}, where the choice of causal variables again determines the space of goals an agent might be pursuing. We summarize these perspectives in the following claim.


% Remark 3: Normativity is frame-dependent.
%
\begin{claim}
Normativity is frame-dependent: For nearly all cases of input-output systems, whether that system is meaningfully goal-directed depends on a reference point whose choice is arbitrary.%There exist cases with multiple plausible goals one could assign to an agent. % Moreover, depending on which of these goals is selected, a system will either be viewed as effectively pursuing its goal, or not.
\end{claim}




% (4) Adaptivity.
%
\paragraph{(4) Adaptivity.} Fourth, agency is about \textit{adaptivity}, which captures whether a system's outputs are influenced by its inputs, and to what extent. 
%
% Intuition.
%
% Zadeh.
In "On the Definition of Adaptivity", \cite{zadeh1963definition} suggests a form of frame-dependence: "...every system is adaptive with respect to [something] ... what matters is not whether [the system] is adaptive or not, but what ... it is adaptive [to]" (p. 470). Following this reasoning, adaptivity can be understood as frame-dependent in the sense that it depends on what is chosen as the relevant reference class used to reach determinations about a system's adaptivity. 
%
% RL example.
For example, in RL, we might ask whether a policy that maps each input state to an action is adaptive. On one reference frame, we might treat \textit{any} change in the output as adaptivity, while on another reference frame, we could view this policy as a fixed and non-adaptive function since it always chooses the same action every time it receives the same input.  \cite{abel2023crl} make this argument in a more general case and show that all policies can either be understood as adaptive, or not, depending on a reference class of meaningful changes in behavior. 
%
In other words, if we want to determine whether a system is adaptive, we need to first agree on the class of experience-influenced changes of behavior that count as adaptivity. Depending on this choice, a system will either be adaptive, or not. This choice then acts as a reference frame. 
%


% Remark 4: Adaptivity is frame-dependent.
%
\begin{claim}[Adapted from \citeauthor{zadeh1963definition}, \citeyear{zadeh1963definition} and Theorem 3.1 of \citeauthor{abel2023crl}, \citeyear{abel2023crl}]
%
Adaptivity is frame-dependent: For many input-output systems, there will exist at least two reference frames (and in most cases, many more) such that according to the first reference frame the system is adaptive, while according to the second, the system is not.
\end{claim}


% Conclusion.
%
In summary, reaching a determination about each of the four properties of agency requires reference to other fixed commitments that collectively comprise a reference frame. Since agency is simply the logical conjunction of these latter three properties conditioned on the choice of a boundary, then reaching a conclusion about agency itself must be made in reference to these commitments. In other words: agency is frame-dependent.


% Relationship to the coastline paradox.
%
% This assertion paints a picture of agency similar to the \textit{coastline paradox} pointed out by \cite{richardson1961problem}. Depending on the chosen atomic unit of measurement, the coastline paradox points out that the circumference of Great Britain will actually change in a meaningful way: in the 1940s, \citeauthor{richardson1993collected} observed that the Portugese measured Great Britain's circumference as 987km, whereas the Spanish measured the circumference as 1,214km. This difference was ultimately due to a choice of an atomic unit of measure \citep{richardson1993collected}. Thus, the measurement of an island's circumference is only well-defined with respect to a choice of an atomic unit of measure. So the story goes with agency: to arrive at a conclusion about whether a system possesses agency, we must first make several commitments that comprise a reference frame. Additionally, in many case, we can change those commitments and arrive at a different conclusion about the agency of the system. 


% Example.
%
% \paragraph{An Example.} Consider a thermostat regulating the temperature in a room. Does this system have agency? Suppose we choose two distinct reference frames, as pictured in \cref{subfig:frames_example}. On the left, we draw the boundary around the knob and the temperature sensor of the room---as a result, the input to the system is a pair of numbers that indicate the current temperature of the room and the desired temperature.


% ----------------
% -- Discussion --
% ----------------
\section{Discussion}


% Main conclusion.
%
We have here argued that agency is frame-dependent by illustrating the sense in which each of the four essential conditions of agency are themselves frame-dependent. We take this to have far reaching implications for disciplines that study agents and agency. 


% Intelligence.
%
\paragraph{Intelligence and Agency.} The relationship between intelligence and agency is not yet well understood. For instance, does intelligence require agency, and vice versa? Depending on how this question is addressed, frame-dependence may have significant implications for our understanding of intelligence, in addition to other emergent properties of information processing systems. We believe that exploring this relationship through the lens of frame-dependence offers a new frontier for understanding central concepts of RL. 


% Reference frames.
%
\paragraph{Reference Frames.} We stop short of presenting a rigorous mathematical definition of reference frames, as well as a formal proof of the frame-dependence of agency. A natural next step to further this line of work is to develop a precise definition of agent reference frames along with a formal proof supporting our main claim. We speculate that the building blocks to do so are already in place in the field, but have not gone through the careful work of developing the definitions and arguments more formally.


% Reference frame choice.
%
\paragraph{Choosing a Reference Frame.} How do we choose an appropriate reference frame? It is unclear which frame-selection principles are defensible, and what implications these principles carry for our study of agents. The Intentional Stance \citep{dennett1989intentional} asserts roughly that it is most \textit{useful} to understand certain systems as agents; taken to its natural conclusion, this could be formalised as a principle for selecting frames in terms of predictive or explanatory power. An important direction for further research will investigate, formalise, and compare different principles for selecting reference frames. 


% \paragraph{Interventions.} One path to reduce frame dependence is to invoke interventions as a tool for getting rid of possible reference frames.


\paragraph{Marr's Levels, Dennett's Stances.} The proposal to adopt reference frames coheres with the perspectives of \cite{dennett1989intentional} as well as \cite{marr2010vision}, who each argue for understanding certain phenomena at different levels of abstraction. 
%
% Marr.
Marr argues that our attempts to understand a cognitive process such as vision can be cast through three distinct levels: the hardware, the algorithmic, and the computational. 
%
% Dennett.
Dennett argues that there are distinct levels of abstraction we should adopt depending on the content of our study. Like Marr, the lowest-level of abstraction is the \textit{physical}, according to which we examine physical properties of a system. The second level is the design stance, according to which we examine how the system is designed to make predictions about its operation or purpose. The third is the intentional stance, according to which we study the content of minds themselves: of beliefs and desires.  %
%
Both Dennett's intentional stance and Marr's levels suggest that our study of concepts like agency needs to carefully calibrate in order to examine the right kinds of content, and reach the right kinds of conclusions. %All of this reflects embraces the Duhem-Quine thesis, ...
%
%
We suggest that reference frames may offer a path to connect physical substrates with more abstract propositions such as those related to agency. 


% Other possible topics:
% - Other forms of relativism (causal, physical, moral)
% - Duhem-Quine
% - Least action principles
% \paragraph{Causal Realism.}

% ------------------
% -- Bibliography --
% ------------------
\bibliographystyle{abbrvnat}
\bibliography{main}

\end{document}