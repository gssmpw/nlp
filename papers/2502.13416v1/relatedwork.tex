\section{Related Work}
% In this section, we briefly introduce some related or prior works on three topics listed below.
\subsubsection*{\textbf{Evaluating Hallucination in LLMs}}
% Large Language Models
% The issue of hallucinations in large language models has garnered considerable attention from numerous researchers. 
Several benchmark datasets have been proposed to holistically assess the hallucination issues that may arise when LLMs generate responses to problem queries. 

TruthfulQA~\cite{lin-etal-2022-truthfulqa} is the most classic dataset for assessing whether language models generate truthful answers to questions. 
It tests whether the models learn incorrect answers during the generation process due to emulating human text. 
Another dataset HaluEval~\cite{HaluEval} samples 10K instances from the training sets of HotpotQA~\cite{yang2018hotpotqa}, OpenDialKG~\cite{moon2019opendialkg}, and CNN/DailyMail~\cite{see2017get}, and utilizes LLMs to generate hallucination-corresponding samples by setting tasks and employing specific sampling strategies, which is primarily aimed at question-answering tasks and text summarization tasks. 
KoLA~\cite{yu2023kola} tests the hallucination issues of LLMs in the domain of knowledge graphs and introduces tasks based on 19 focal entities, concepts, and events. 
It assesses the capacity of large language models (LLMs) to handle structured knowledge across four levels: memory, understanding, application, and creation. 
This aims to test the hallucination phenomena of LLMs in the domain of knowledge graphs. 
From the perspective of long context, BAMBOO~\cite{dong2023bamboo} and FActScore~\cite{min2023factscore} both target the long text generation capabilities of large language models, assessing their performance in extended context scenarios through factual verification. 
Additionally, there are assessments of large language models for hallucination issues in specific domains such as healthcare and finance~\cite{umapathi2023med, kang2023deficiency}.


% \begin{table*}[!h]
% \small
%     \def\arraystretch{0.9}
%     \setlength{\tabcolsep}{0ex}
% 	\centering
% 	% \footnotesize
% 	\caption{Comparison with SOTA FCH Evaluation Approaches.}
%  % \lnk{need to refine}}
%         \label{table:comparison1}
% 	%\resizebox{\linewidth}{!}{
% \vspace{-2mm}
% \begin{tabular}{c c c c c}
%     \toprule 
%     \textbf{Dataset} &\textbf{Fact Source} & \textbf{Construction Method} &\textbf{Test Oracle} \\
%     \midrule
%     TruthfulQA & Wikipedia pages \& websites & Human annotations & Truthfulness Rate\\
%     \midrule
%     KoLA-KM, KA & Wikidata5M \& websites & Existing datasets consolidation & Standardized score (F1)\\ 
%     \midrule
%     HaluEval-QA & Wikipedia & Human annotations \& ChatGPT query & Accuracy\\ 
%     \midrule
%     \tool{}-Dataset  & Wikidata triples & Prolog-aided reasoning \& template-based generation & Structural Similarity\\
%     \bottomrule
%     \end{tabular}
%     %}
% \end{table*}

% \begin{table*}[!h]
%     \def\arraystretch{0.9}
%     \setlength{\tabcolsep}{1ex}
% 	\centering
%     \small
% \caption{Comparison with Natural Language Reasoning Benchmarks.}
%  % \lnk{need to refine}}
%         \label{table:comparison2}
% 	%\resizebox{\linewidth}{!}{
% \vspace{-2mm}
% \begin{tabular}{c c c c c c}
%     \toprule 
%     \textbf{Benchmark} &\textbf{Size} &\textbf{Reasoning Type} & \textbf{Data Source} & \textbf{Task}& \textbf{Automation}\\
%     \midrule
%     FOLIO & 1.4k & First-order logic reasoning & Expert-written & Theorem Proving & \ding{55}\\
%     \midrule
%     DEER & 1.2k & Inductive reasoning & Wikipedia & Rule Generation & \ding{55}\\
%     \midrule
%     % HotpotQA & 112k & Multi-hop reasoning & Wikipedia & Question Answering & \checkmark \\
%     % \midrule
%     \tool & Scalable & Deductive reasoning & Wikidata & Question Answering & \checkmark
%     \\
%     \bottomrule
%     \end{tabular}
%     %}
% \end{table*}

\subsubsection*{\textbf{Mitigating Hallucination in LLMs}}
%Large Language Models
% There are other lines of novel studies on automatically mitigating LLM hallucination.
Current mitigation strategies include black-box prompting guidance and fine-tuning with extensive factual data. 
Considerable work~\cite{lightman2023let, varshney2023stitch,gou2023critic,vu2023freshllms} involves utilizing external knowledge retrieval or automated feedback adjustments to make text responses from large language models more controllable and reliable. 
Similar approaches are proposed for multimodal hallucination mitigation such as Woodpecker~\cite{yin2023woodpecker}, which extracts key concepts to generate questions and knowledge assertions for hallucination diagnosis and mitigation.
Another thread involves using fine-tuning techniques to mitigate model hallucinations. AlpaGasus~\cite{chen2023alpagasus}, Elaraby et al.~\cite{elaraby2023halo} and Tian et al.~\cite{tian2023fine} apply fine-tuning techniques on high-quality data for better effectiveness and factuality. 
Besides, the findings of Elaraby et al.~\cite{elaraby2023halo} reveal that the knowledge injection technique enhances the performance of less robust LLMs. 
Additionally, an increasing number of researchers are turning towards studying white-box repairing methods for open-source large language models. 
The evidence presented in the discourse by Azaria et al.~\cite{azaria2023internal} suggests that the internal states of Large Language Models can be utilized to discern the veracity of statements, thereby elucidating the underlying causes of factual hallucinations in LLMs. 
Studies like IIT~\cite{li2023inference} and Repr~\cite{zou2023representation} endeavor to alleviate hallucination issues by delving into LLMs' deep-layer information through the analysis of internal model states. 
This approach not only augments the interpretability of large language models but is also regarded as a vital research direction for the future of explainable and trustworthy AI.


% \subsubsection*{\textbf{Comparison with Existing Works}}
% We qualitatively compare \tool with the state-of-the-art FCH evaluation approaches and existing natural language reasoning benchmarks to illustrate the advantages of \tool{}.
% As illustrated in Table~\ref{table:comparison1}, we enumerate the characteristics of the sota FCH evaluation approaches. To assess FCH in LLM responses, existing approaches uniformly opt for a wiki-related knowledge base as the foundation for constructing ground truth facts. Their main distinction from \tool lies in the manner of task construction and the metrics employed to measure hallucinations.

% \paragraph{Task Construction Methods.}
% Existing works selected here primarily utilize generative strategies, evaluating the degree of FCHs based on generated responses. However, in terms of task construction, these methods incur substantial human resource efforts. Apart from the KoLA-KM, KA~\cite{yu2023kola}, which is essentially a collection of existing Q\&A datasets, both TruthfulQA~\cite{lin-etal-2022-truthfulqa} and HaluEval~\cite{HaluEval} rely on human annotations to construct Q\&A pairs. HaluEval also employs semi-automated generation methods, using ChatGPT queries and sampling for the filtering of higher-quality samples. \tool, on the other hand, utilizes Prolog-assisted automatic inference to derive new knowledge triples and generate templates for new questions, achieving maximum automation of construction while ensuring the complexity of the questions.

% \paragraph{Response Evaluation Metrics.}
% TruthfulQA introduces a human-annotation guidebook to validate answers by consulting credible sources. Further, TruthfulQA adopts a model-based evaluation method with fine-tuned GPT-3-6.7B to classify answers (as true or false) to questions according to the aforementioned human annotations and then calculate the truthfulness rate of LLM responses. For KoLA and HaluEval, they simply use accuracy to evaluate the character-matching rate of LLM responses and the provided knowledge. Thus, \tool considers the structural similarity of LLM responses with original knowledge triples and the reasoning process, offering superiority over those simple evaluation metrics.



% As listed in Table~\ref{table:comparison2}, we provide several benchmarks aided for natural language reasoning. Existing reasoning benchmarks lean more towards logical predicate-formatted inputs and outputs, lacking natural language-formatted questions, thus limiting their suitability for testing with LLMs.