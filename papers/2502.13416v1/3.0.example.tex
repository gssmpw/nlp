




\section{Motivating Examples}\label{sec:motivating}


\begin{comment}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{fig/drowzee-lg-example.pdf}\\
    % \vspace{-0.3cm}
\caption{Examples of logic programming.
\syh{need to re-draw this picture to me more space-friendly}}
% \vspace{-0.2cm}
\label{fig:lg-example}
\end{figure}


%We next define the 
%reasoning rule for  
%relation 
%\[\]
We use the example shown in \figref{fig:lg-example} for a concrete 
demonstration. 
Here, $\m{member}(\m{Gunzo  Prize}, \m{Haruki Murakami Awards})$, is a fact describing that \emph{Gunzon Prize} is one of the prizes awarded to \emph{Haruki Murakami}.  
With the reasoning rule defined for relation $\m{same\_member(List1, List2)}$, which executes to $\m{true}$ if there exists at least one $\m{element}$ that is a member of both $\m{List1}$ and $\m{List2}$, we can write a query as follows, which asks if there exist any common awards won by both \emph{Haruki Murakami} and \emph{Bob Dylan}. 
%An example query is 
\[{?}\text{-}\;  \m{same\_member(HarukiMurakamiAwards, BobDylanAwards)}\]
\end{comment}


%\wkl{Modify, highlight why existing works face challenges and limitations}

\begin{figure*}[!ht]
\centering
\includegraphics[width=0.85\linewidth]{fig/drowzee-motivation.pdf}\\
%\vspace{-1mm}
\caption{Motivating Examples for Automatic Benchmark Construction with Complex Questions}
\label{fig:motivating}
\vspace{-0.1cm}
\end{figure*}




% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[width=0.95\textwidth]{fig/motivating_v2-cropped.pdf}\\
%     \caption{Motivating Example.\wkl{Here briefly introduce what is the content from each example (a)(b) Change caption}\lyk{the generated questions and facts do not match}}
%     %\vspace{-0.5cm}
%     \label{fig:motivating}
% \end{figure*}

%Motivation for 

\subsection{Automatic Benchmark Construction}
As a first motivating example, shown in \figref{fig:motivating}, given 
the facts about whether Haruki Murakami and Bob Dylan have won the Nobel Prize, as illustrated in the left sub-figure, we can query straightforward questions such as ``\emph{whether Haruki Murakami or Bob Dylan has won the Nobel Prize?}''. 
Asking and verifying this knowledge requires no logical reasoning. 
However, such questions are often not enough to unveil hallucinations. 
{Therefore, more diversified questions, i.e., with intertwined and complex information, as illustrated in the right sub-figure, are needed.} 
%To generate more diversified benchmarks, previous research~\cite{yu2023kola, HaluEval} involves human experts to generate the questions and annotate the answers for hallucination checking.
%\figref{fig:motivating} depicts how a human expert would reach to question 2 according to the relations among entities across different facts.
%Although the manually generated benchmarks can unveil certain hallucinations, they suffer from several drawbacks.
{
Moreover, the knowledge landscape is dynamic, with new information continuously surfacing and older information becoming obsolete.} If facts change continuously over time, for instance, if Haruki Murakami were to win the Nobel Prize in the future, this would necessitate regular updates and corrections to the ground truth in existing datasets to reflect them. However, maintaining the accuracy of these benchmarks requires a significant amount of manual labor. 

%Motivation for 
\subsection{Questions Involving Temporal Reasoning}
As LLMs increasingly rely on temporal reasoning to process time-dependent data, understanding how well they can handle temporal logic is crucial for their development and deployment in real-world applications. 
Reasoning temporal-logic-related queries often requires complex steps, particularly regarding the relative timing of multiple events. Unfortunately, manually generated test cases frequently lack thorough completeness verification, which undermines their reliability. 
We demonstrate such an example in \tabref{tab:TemporalLogicRelatedHallucination}. 
The question pertains to the relative timing relationship between the original Ben 10 TV series and the year 2000. It is known that the original Ben 10 series premiered on December 27, 2005, and concluded in 2008. Therefore, when considering the query asking whether ``Ben 10 finally airs within the first three years after 2000'', the expected answer is ``No'', as none of the years from 2005 to 2008 fall within that queried timeframe. 
However, despite the accurate facts inputs, LLMs still generate hallucinations due to insufficient temporal reasoning. 





\begin{table}[!b]
\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{l}
\toprule 
\rowcolor{mycolor} \textbf{\query:}  Did the Original Series of Ben 10 TV show finally air  \\
\rowcolor{mycolor} 
within the time frame of the 1st year to the 3rd year after the year \\
\rowcolor{mycolor}
2000? Please answer Yes or No and explain why. \\[0.5em] \hline
\\[-0.7em] 
\rowcolor{mycolor} 
\textbf{\hallucinationAnswer \rm{(Claude 3.5)}:} Yes, the original series \\
\rowcolor{mycolor}
of Ben 10 aired within the time frame of the 1st year to the 3rd    \\
\rowcolor{mycolor}
year after the year 2000. The show premiered on December 27,  \\
\rowcolor{mycolor}
2005, and concluded its run on April 15, 2008. This timeline 
\\
\rowcolor{mycolor}
clearly falls within the specified period from 2001 to 2003.
\\ 
\bottomrule %添加表格底部粗线
\end{tabular}
\caption{Temporal Logic Reasoning Hallucination}
\label{tab:TemporalLogicRelatedHallucination}
\end{table}


\begin{figure}[!b]
\centering
\begin{lstlisting}[xleftmargin=3.5em,numbersep=8pt,basicstyle=\footnotesize\ttfamily] 
// Ground Facts crawled from Wikipedia
begin('Ben_10',2005). end('Ben_10',2008). 

// Generating the time-stamped fact
ben_10(Start,End) :- Start=<End, 
    begin('Ben_10',Start), end('Ben_10',End).
    
// Encding the MTL formula (*@\color{commentcolor}{$\mtl$}@*)
finally_ben_10_during_1_3(Start,End) :-
    ben_10(Start1,End1), 
    Start is (Start1-3), End is (End1-1), 
    (Start1-3)=<(End1-1), Start=<End.
    
// The time interval (*@\color{commentcolor}{$\interval$}@*) which satisfies (*@\color{commentcolor}{$\mtl$}@*)
?- finally_ben_10_during_1_3(Start,End).
   Start = 2002, End = 2007.
\end{lstlisting} 
\caption{Prolog Encoding for $\mtl \,{=}\, \mathcal{F}_{[1, 3]}(\m{Ben\_10})$}
\label{fig:prologRulesForFinally}
\end{figure}


In this work, our proposed testing framework automatically generates such temporal test cases in the form of MTL formulae, denoted by $\mtl$, which incorporate quantitative timing constraints and enable the expression of temporal relationships with precise intervals.   
For example, this query shown in \tabref{tab:TemporalLogicRelatedHallucination} is represented as  
$\mtl {\,=\,} \mathcal{F}_{[1, 3]}(\m{Ben\_10})$, where $\mathcal{F}$ stand for \emph{finally} and [1,3] is the time frame, querying the time intervals $\interval$ during which the event $\m{Ben\_10}$ finally happen within the time frame of the 1st year to the 3rd year. 
%Then, the final ground truth answer is generated by checking if the year 2000 falls inside $\interval$. 
%If each temporal-related test case needs to be \syh{checked, make it clear that we generate test cases and automatically get the ground truth}, a manual comparison of multiple time points is required. This process is prone to errors, leading to uncertainty in the result validation. Thus, we provide our solution, i.e., automatic verification in Prolog using MTL  encoding.
To obtain the ground truth interval $\interval$, \tool{} automatically generates the Prolog encoding rules for $\mtl$, as shown in \figref{fig:prologRulesForFinally}. These rules accurately determine that $\interval{\,=\,}[2002,2007]$, indicating that all time points within this interval satisfy $\mtl$. 
Then, the validity of 2000 is easily disproved because 2000 is not a member of $\interval$. 

It's important to note that such reasoning rules can be generated for arbitrarily nested MTL formulae. These rules lead to sound and deterministic conclusions regarding a ``Yes" or ``No" response, and they provide reasoning steps to assist in evaluating the LLMs' answers, especially when the queries become more complex. 
Furthermore, obtaining the ground truth interval directly enables us to flexibly control the generation of both positive and negative test cases. 
%\syh{explain the ground truth for generation of the test case.}


%The two examples highlight the benefits of our proposal -- automated techniques to detect hallucinations in LLMs -- compared to existing manual processes for generating and verifying LLMs' output. 
%Further, automatically generating diverse benchmarks and verifying the LLMs' output is equally challenging, and these challenges motivate our novel testing framework, which is detailed in the following section. 

% Consequently, the efficiency and soundness of the manually generated benchmarks are not guaranteed.



% Nevertheless, automatically generating diverse benchmarks is challenging.
% \textbf{First, generating suitable and valid questions is challenging~(challenge\#1).}
% While it is important for the questions in the testing benchmark to cover a diverse range of scenarios, they cannot be randomly generated or arbitrarily selected. Instead, the questions must be logically coherent and aligned with well-established factual knowledge and ground truth.
% \textbf{Second, deriving the test oracles for detecting hallucinations is challenging~(challenge\#2).} The LLM's answer is typically expressed in lengthy and potentially complex sentences. The key to determining if an LLM has produced an FCH lies in assessing whether the overall logical reasoning behind its answer is consistent with the established ground truth. Automatically analyzing and comparing the intricate logical structures within the LLM's response and the factual ground truth remains an inherently difficult task.
%Each question should be coupled with a ground truth answer and this ground truth answer should be comparable against the answers from LLMs.

% These two challenges can both be addressed by leveraging logic programming.
% We can derive new, logically sound facts based on existing knowledge.
% Using the newly derived facts, we can then proceed to generate a wide range of questions, each with its corresponding ground truth answer, enhancing the depth and breadth of our problem-solving capabilities.
% We can generate test oracles to capture hallucinations with the ground truth answers.
%In short, using logic programming to tackle the challenges motivates the design of \tool{}.


% In this section, we first illustrate the limitations of the existing hallucination testing techniques, and briefly introduce how \tool{} overcomes these shortcomings. 

% % \textbf{Challenge\#1: Difficulty for Generating Hallucination Test Cases.} 
% \textbf{Challenge\#1: Generation of FCH Test Cases.}
% One major challenge in the current research on LLM hallucination testing resides in the reliance on fixed benchmark datasets for detecting hallucinations. These datasets are fundamental but static, and therefore fail to cover the full range of real-world scenarios and questions. For instance, as illustrated in Figure~\ref{fig:motivating}(a), even when questions are only slightly altered, with new or different factual knowledge incorporated, they can elicit varied responses. This variability might lead to responses that are incorrect, but these types of errors are not always identified using existing benchmark datasets. Moreover, the focus on factual knowledge in these datasets can result in insufficient examination of hallucinations, especially for factual knowledge that has been changed or ``mutated'' from its original form. 

% Our approach addresses this gap by using logic programming along with four specific reasoning rules, enabling us to automatically create a wider variety of test cases that are more representative of the diverse and dynamic nature of real-world factual knowledge.
% % Current approaches to detecting hallucinations in LLMs predominantly depend on predefined benchmark datasets. These datasets, while useful, offer a narrow perspective due to their static composition, failing to encompass the broad spectrum of real-world scenarios and questions. As demonstrated in Figure~\ref{fig:motivating}(a), the set of diversified questions, while bearing resemblance to the original query, incorporates distinct or additional information. This variation has the potential to elicit different responses, which in turn, could activate instances of hallucination. In this case, the existing methods are unable to detect. Our methodology counters this shortcoming by leveraging logic programming, coupled with four specific reasoning rules. This strategy facilitates the automatic creation of diverse and novel test cases, significantly enhancing the range and relevance of the hallucination detection process compared to the traditional, limited benchmark datasets.

% % \textbf{Challenge\#2: Restricted Scalability.} 
% \textbf{Challenge\#2: Test Oracles for FCH Testing.}
% % This challenge is largely stemming from a substantial dependency on manual labor. Traditional techniques, even when augmented with LLM-assisted methods, necessitate human intervention to validate answers against benchmark datasets. In contrast, our approach capitalizes on the capabilities of logic programming. By employing four distinct reasoning rules, we systematically construct definitive relationships between information entities. As shown in Figure~\ref{fig:motivating}(b), we first extract relations from a list of facts, This framework enables the automated labeling of data based on constituent factual elements. Additionally, the automated generation of test case-oracle pairs through this method facilitates the detection of hallucinations on a much broader scale, significantly reducing the need for human oversight and thereby enhancing scalability.
% Another major challenge is constructing testing oracles with minimal human effort. Previous benchmarks relied heavily on manually labeled data, a process hampered by the intricate logic inherent in each question-answer pair. For illustration, we use a logic reasoning chain to simulate the manual question answering process, as shown in Figure~\ref{fig:motivating}(b). Specifically, the process includes fact and relation extraction, implicit information collection and answer generation steps. This complexity makes it challenging to automatically capture the reasoning needed to derive answers from questions. 

% In contrast, our approach leverages logic programming, by employing four distinct reasoning rules, to systematically establish relationships between various entities. This process enables the automated generation of test case-oracle pairs, reducing reliance on manual labeling and enhancing the scalability of hallucination detection. By automating this process, our method significantly expands the detection capabilities, enabling a broader and more efficient detection of potential hallucinations in LLMs.
