\section{Discussion}
\subsection{Threats to Validity}

\subsubsection*{\textbf{Limited Coverage of Knowledge Databases}}

Our research predominantly employs data from the Wikipedia database to generate test cases using \tool. However, it is important to note that \tool is not limited to this specific database. Its design allows for easy extension and adaptation to various other knowledge bases, illuminating its versatility and applicability.

\subsubsection*{\textbf{Limited Accuracy of Hallucination Categorization}}
We utilize a dual approach for categorizing hallucinations, combining assessments from GPT-4 with human verification. Initially, GPT-4 classifies the hallucinations, after which we manually review a random sample of 100 instances. This process reveals that GPT-4's categorization accuracy stands at approximately 71\%, suggesting that integrating GPT-4 for hallucination categorization generally leads to reliable outcomes. We further note that techniques for further improving the LLM's categorization accuracy via prompt engineering are orthogonal to the scope of this work.

% This is particularly relevant for large models, where the cost of retraining is prohibitively high. Researchers are exploring performance improvements and error corrections through relatively minor modifications and editing techniques. 
%\subsection{Mitigation} After identifying that LLMs are prone to hallucinations when dealing with logical reasoning, we perform categorization and seek to explore potential methods to mitigate this issue. Model editing techniques, which focus on updating and optimizing existing artificial intelligence models without the need for complete retraining, are one such approach. We involve two model editing algorithms, i.e., ROME~\cite{meng2022locating} and MEMIT~\cite{meng2022memit}, to integrate new knowledge derived from reasoning into open-source LLMs, aiming to alleviate FCH issues. We apply FastEdit~\cite{fastedit} and EasyEdit~\cite{wang2023easyedit} for more speedy implementation. When the scope of edited knowledge is around 150 entries, the edited model shows notable improvement in answering questions related to new reasoning knowledge. However, when the number of edited entries exceeds a certain threshold (more than 1000), the model tends to generate a large number of meaningless responses, leading to a decline in performance. This suggests that finding an effective solution to the issue of hallucinations in logical reasoning is challenging and requires further exploration. Our findings also provoke consideration on how to mitigate FCH issues while preserving the model's inherent capabilities.
%Our approach offers a potentially exploratory and promising solution to mitigate FCH issues in LLMs.

\subsection{Takeaway Messages}
\noindent\textbf{LLM Honesty During Training.} During the training of LLMs, it is important to focus on model honesty, e.g., how to enable large models to possess stronger critical thinking and logical reasoning abilities. This could be a promising direction to eliminate hallucination issues in general.

\noindent\textbf{Towards In-depth Understanding of LLM Hallucination.} 
%From the insights derived in this work, 
The insights shows that 
it is important to further explore techniques to understand the deep-rooted causes of hallucinations LLMs through white-box methods. A promising direction is to enhance and augment the logical reasoning capabilities of LLMs to reduce hallucination issues.