%\syh{bookmark, proof read up to here}

\begin{comment}
    

\section{Background}\label{sec:background}

\syh{why do we need this section? how about we delete 2.1 and move 2.2 to section 4?}
\subsection{Hallucination Categorization}\label{subsec:cat}
Hallucination in LLMs can be categorized into the following three  categories~\cite{yao2023survey,huang2023survey,zhang2023hallucination}: 
%, as detailed below. main

%This type 
\emph{\textbf{Input-conflicting hallucination}} arises when LLMs produce outputs inconsistent with the user's input. This inconsistency can occur in two ways: either the model's response contradicts the task instructions (reflecting a misunderstanding of user intents), or the generated content contradicts the task input (similar to conventional issues in machine translation and summarization). An example of this would be an LLM replacing a key name or detail in a summary, deviating from the actual content provided by the user. 

\emph{\textbf{Context-conflicting hallucination}} arises when LLMs provide contradictory or inconsistent responses over multiple turns or in lengthy responses. This happens when models lose track of the context or fail to maintain consistency throughout the conversation, often caused by the inability to maintain long-term memory or identify relevant context. An instance of such hallucination could involve LLMs switching references between two individuals in a conversation about a specific topic.

%Limitations in maintaining long-term memory or identifying relevant context are often the culprits. 
%exhibit contradictions or inconsistencies in lengthy or multi-turn responses.

\emph{\textbf{Fact-conflicting hallucination}} occurs when LLMs generate information that directly conflicts with established knowledge. This can happen due to various factors introduced at different stages of the LLM lifecycle. For example, as shown in \figref{fig:example1}, an LLM might provide incorrect historical information in response to a user's query, misleading users who are less knowledgeable about the subject. 
In this paper, our main focus is on fact-conflicting hallucinations, which are errors that can mislead users and have serious consequences. 
%In this paper, our primary focus is the fact-conflicting hallucinations, a type of error that carries the potential for more serious consequences by misleading users. 

\end{comment}

% %\yi{the formulation is a little bit confusing, why should we use similarity calculation?}
% \begin{definition}\label{def:hallu}
% Given a universally recognized input-knowledge pair $(p_i, r_i)$, and the LLM generated input-output pair $(p_i, r'_i)$ from the same prompt $p_i$, we consider the detection of an occurrence of FCH when $r_i$ and $r_i'$ are intrinsically dissimilar. This can be captured by the expression $v = False$, where $v=check(p_i,p'_i)$ checks the semantic similarity between $p_i$ and $p_i'$ to return a Boolean value. 
% \end{definition}