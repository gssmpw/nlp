% \section{Implementation}
% 

% To implement factual knowledge extractor, we use Wikipedia and Wikidata as sources to extract entities and structured information as base factual knowledge. 
% As one of the most widely used knowledge bases, wikipedia is renowned for its diverse range of entities and its extensive content coverage across multiple domains. Supplementing Wikipedia, Wikidata provides structured data, making it an ideal resource for the construction and enrichment of factual knowledge. 
% After downloading the latest Wikipedia dump, we employ wikiextractor\cite{Wikiextractor2015} to extract relevant text from Wiki pages. 
% In parallel, we invoke Wikidata's SPARQL\cite{sparql} query module for the extraction of triples. 
% The factual knowledge extractor extracts entities and structured information sourced from Wikipedia as base factual knowledge. Wikipedia, one of the most widely used knowledge bases, is renowned for its diverse range of entities and its extensive content covering multiple domains (such as science, art, history, geography, and technology). Wikidata is a supplement of Wikipedia and it provides structured data, resulting in an ideal resource for the construction and enrichment of factual knowledge.
% After extracting base factual knowledge, we employ wikiextractor\cite{Wikiextractor2015} to extract relevant text from Wikipedia pages, and in parallel, extracting triples by invoking Wikidata's SPARQL\cite{sparql} query module.

% For logic reasoning processor, we implement it with SWI-Prolog, an open-source advanced logical programming interpreter. Note that to prevent errors due to excessive stacked strings, when a large number of facts need to be inserted into Prolog, we employ a sampling method and extract a subset of entities to form a query, ensuring that the logical processor can operate normally.


\section{Evaluation}\label{sec:eval}
% We implement \tool with 4,071 lines of Python code and 826 lines of Prolog code. 
Our evaluation targets the following research questions:

\begin{itemize}[wide]
\item \textbf{RQ1} (Effectiveness): How effective is \tool for in generating test cases and identifying LLM FCH issues?

\item \textbf{RQ2} (Hallucination Categorization and Analysis): What are the categorizations of LLM FCH issues? 

\item \textbf{RQ3} (Ablation Study): Whether the four types of logic reasoning rules %(\figref{fig:basic_op_for_predicates}) 
can identify LLM FCH issues independently? 

\item \textbf{RQ4} (FCH Caused by Temporal Reasoning): 
How effective is \tool in detecting temporal-logic-related hallucinations; what are their categorizations; 
and which temporal operators trigger the most/least hallucinations? 
\end{itemize}


%$\bullet$ \textbf{RQ4 (Comparison with Existing Works): What superiority does \tool possess over existing works?} This RQ qualitatively compares \tool with existing FCH detection approaches and logical reasoning benchmarks.

% $\bullet$ \textbf{RQ4 (Feedback Effectiveness): Can feedback segment effectively trigger FCH of LLMs when faced with a specific category of questions?} This RQ discovers which type of questions could better trigger FCHs of LLMs in various domains and demonstrate the effectiveness of the feedback procedure.
% $\bullet$ \textbf{RQ4 (Real-world Application):}
% Exploring Model Editing for Enhancement (Double-confirm the usefulness of generated knowledge)
% This RQ attempts to mitigate LLM hallucination through model editing techniques.

\subsection{Experimental Setup}\label{sec:ex_setup}
\noindent \textbf{Knowledge Extraction.} 
We use Wikipedia and Wikidata as sources to extract entities and structured information as base factual knowledge. After downloading the latest Wikipedia dump, we employ wikiextractor~\cite{Wikiextractor2015} to extract relevant text from Wiki pages. In parallel, we invoke Wikidata's SPARQL~\cite{sparql} query module to extract facts. 
Through data processing involving simplification and filtration, we amass a collection of basic factual knowledge encompassing 54,483 entities and 1,647,206 facts. %\shil{Any new entities or facts?}


\noindent \textbf{Logic Reasoning Processor.} 
For the logic reasoning module, we apply SWI-Prolog~\cite{wielemaker2012swi}, an open-source advanced logical programming interpreter. To effectively prevent errors due to excessive stacked strings, and ensure the proper operation of the logical processor when inserting a large number of facts into Prolog, we employ a sampling method and extract a subset of entities to form a query. 
%\syh{sampling and subset?}

\noindent \textbf{Models Under Test.} 
To guarantee a reliable evaluation for the RQs, we evaluate nine state-of-the-art large language models with \tool. Considering the diverse nature of LLMs, we select two distinct categories: the first category comprises API-accessible models with closed-source architecture including ChatGPT (gpt-3.5-turbo-0613), GPT-4, and GPT-4o~\cite{OpenAI2023GPT4TR}, and the second category consists of open-source LLMs with deployability, including Llama2-7B-chat-hf, Llama2-70B-chat-hf~\cite{touvron2023llama}, Mistral-7B-Instruct-v0.2~\cite{jiang2023mistral}, Mixtral-8x7B-Instruct-v0.1~\cite{jiang2024mixtral}, Llama3.1-8B-Instruct, and Llama3.1-70B-Instruct~\cite{llama32024tr}.  
% \syh{is here up to date?}

\noindent \textbf{Response Validation Threshold $\theta$.} 
%To validate responses from LLMs as described in \secref{response}, 
We apply StanfordOpenIE~\cite{angeli-etal-2015-leveraging} for knowledge triple extraction from LLM responses and then use SentenceBERT~\cite{reimers2019sentence} to calculate the vector similarity of nodes and edges from the constructed semantic structures. We also utilize GPT-4o to extract triples for some complex responses that StanfordOpenIE cannot handle effectively. Here, we set the threshold to 0.8, considering knowledge triples as semantically equivalent if they exceed this threshold and vice versa. To determine the threshold value, we sample 30 test cases and corresponding LLM responses from each of the nine knowledge domains listed in \figref{table:categories}. Through this analysis, we find that by setting the threshold values for both $\theta_\m{e}$ and $\theta_\m{n}$ at 0.8, with the given 270 test cases that are correctly classified, we can estimate the true positives among all test cases through \textit{Laplace's approach in the Sunrise problem}~\cite{laplace1951philosophical}, resulting in 99.6\% when detecting non-equivalent LLM answers as FCHs. In other words, all instances where an LLM's answer has a semantic similarity score below 0.8 compared to the ground truth were correctly identified as FCH cases. 

\noindent \textbf{Running Environment.} 
Our experiments are conducted on a server running Ubuntu 22.04 with two 64-core AMD EPYC 7713, 512 GB RAM, and two NVIDIA A100 PCIe 80GB GPUs. Our experiments consume a total of 240 GPU hours. %\shil{whether the GPU hours cover new experiments?}

% \subsection{Implementation}


\subsection{RQ1: Effectiveness}
% \yi{motivation, method, result}
% The efficacy of \tool as delineated in Section~\ref{method} is intrinsically linked to its effectiveness for identifying FCH issues. Our evaluation on \tool on comprehensive performances of diverse LLMs and the domain-specific effectiveness categorized for each LLM.
To reveal the effectiveness of \tool, we evaluate the statistics of test cases generated by \tool and then evaluate the capabilities of identifying LLM FCH issues with the generated test cases. 
To further assess the effectiveness of test cases for uncovering FCH issues in specific knowledge domains, we evaluate the performances of LLMs on test cases across various knowledge domains.

\head{Effectiveness on Generating (Non-Temporal) Q\&A Test Cases.} We apply \tool to generate a Q\&A test benchmark, amounting to a comprehensive total of 7,200 test cases, designed to provide a broad and detailed evaluation of LLM FCH issues across specific knowledge domains. 
% Note that \tool is a scalable framework and can generate Q\&A pairs according to the specific entity and its relevant properties.
% Table~\ref{table:statitics} lists the domain-specific test case statistics.
% \begin{table}[!ht]
%     % \def\arraystretch{1.0}
%     \setlength{\tabcolsep}{1ex}
% 	\centering
%     \large 
% 	% \footnotesize
% 	\caption{Dataset Statistics.}
%         \label{table:statitics}
% 	\resizebox{\linewidth}{!}{
% 	\begin{tabular}{c c c c c c c c c c c}
%     \toprule 
%     \textbf{Type} & \textbf{Culture} & \textbf{Geo.} & \textbf{History} & \textbf{People} & \textbf{Society} & \textbf{Tech.} & \textbf{Math} & \textbf{Health} & \textbf{Nature}\\
%     \midrule
%     \textbf{Count} & 2,265 & 1,892 & 1,354 & 4,084 & 2,184 & 1,677 & 194 & 334 & 224 \\
%     \bottomrule %添加表格底部粗线
% \end{tabular}}
% \end{table}

\begin{figure}[!t]
\hspace{-4mm}
\centering
\begin{subfigure}{}%{0.49\linewidth}
\centering
\includegraphics[width=1\linewidth]{fig/rq1_overall_hallucination_rate.pdf}\\
\vspace{-0.2cm}
\caption{Overall Hallucination Rate of Various LLMs}
\vspace{0.3cm}
\label{fig:overall}
\end{subfigure}
    % \hfill
    %\hspace{-0.5em}
    %\vspace{-0.3cm}
\begin{subfigure}{}%{0.51\linewidth}
\centering
\includegraphics[width=0.9\linewidth]{fig/rq1_each_hallucination_rate.pdf}
\caption{Hallucination Rate Heatmap of Specific Domain}
\label{fig:rq1.2}
\end{subfigure}
\vspace{-3mm}
\end{figure}


\head{Effectiveness across LLMs.} We instruct LLMs under test utilizing Q\&A pairs derived from \tool, and automatically label both hallucination and normal responses. The results are presented in \figref{fig:overall}, illustrating the proportion of hallucination responses versus normal responses from LLMs under test.

Among all models, GPT-4 exhibits the best performance, demonstrating the lowest proportion of hallucinatory responses in the test cases generated by \tool, at only 24.7\%, while GPT-4o and ChatGPT fall slightly behind with 
28.1\% and 42.1\%. Open-source LLMs including Llama2-7B-chat-hf, Llama3.1-8B-Instruct, and Mistral-7B-Instruct-v0.2 with fewer parameters perform worse, but their counterparts with larger parameters (i.e., Llama2-70B-chat-hf, Llama3.1-70B-Instruct, and Mixtral-8x7B-Instruct-v0.1) achieve higher normal response rates surpassing ChatGPT on \tool. %\shil{shall we add explanations for Llama3 series?}
This indicates that the test cases generated by \tool successfully trigger hallucination responses across various LLMs when confronted with questions requiring logical reasoning capabilities.
% Among all models, GPT-4 exhibits optimal performance, with the lowest proportion of hallucination responses at only 24.7\% on test cases generated by \tool. ChatGPT, though slightly less effective, follows closely behind. In contrast, Llama2-7B-chat-hf demonstrated subpar performance in handling inferential questions, with the hallucination rate reaching 56.9\%. It is noteworthy that Mistral-7B-Instruct, having undergone meticulous instruction-based fine-tuning, exhibits performance surpassing that of ChatGPT on \tool.
% Remarkably, it also outperformed LLMs with larger parameters such as Llama2-13B-chat and Vicuna-13B-v1.5. We can infer that instruction tuning and alignment with human feedback can aid in mitigating hallucinations, as exemplified by the enhanced reasoning capabilities displayed by Mistral-7B-Instruct, a model fine-tuned on publicly available instruction datasets. Furthermore, a discernible performance gap persists between open-source and commercial models in addressing reasoning problems, highlighting an urgent need for more effective fine-tuning strategies and methods to reduce hallucinations.


\head{Effectiveness on Specific Domain Knowledge for Each LLM.}
To further explore the effectiveness of \tool in identifying FCH issues spanning various domains of LLMs, we compare hallucination response across nine specific domain knowledge. 
\figref{fig:rq1.2} presents the generated heatmaps of the confusion matrices for the specific knowledge field hallucination response rate based on the testing results.
It can be clearly observed that different models exhibit varying strengths and weaknesses across distinct knowledge domains. 
% Overall, the performance in the domain of geographical knowledge stands out as the most proficient, with the overall probability of generating hallucination responses being approximately 39\% on average. 
% Taking GPT-4 as an example, it is evident that it performs commendably in the domains of geography, society, and technology. This suggests that the GPT-4 model has developed a relatively comprehensive knowledge framework in these areas, demonstrating strong generalization capabilities in reasoning and understanding entity relationships. 
% An interesting finding is that, within the domains of natural and physical sciences, where LLMs generally exhibit weaker performance, the formulation of generated questions necessitates an extensive understanding of astrophysical entities and their interrelationships. It is plausible that this realm of knowledge is underrepresented in the training datasets of contemporary LLMs, thereby requiring improved pre-training. Such a disparity in knowledge representation is likely a significant factor in the observed underperformance of LLMs in these specific domains.
An interesting finding is that, within the domains of natural sciences and mathematics, LLMs generally exhibit weaker performance. This is potentially because there are many astrophysical or mathematical entities and their interrelationships in generated test cases by \tool. To answer such questions, the LLM needs an extensive understanding of astrophysical knowledge and mathematical theory. Thus, we infer that this realm of knowledge is not well-covered in the training datasets of LLMs under test, thereby resulting in high hallucination rates. Such a disparity in knowledge is likely a significant factor in the observed underperformance of LLMs in these specific domains.

% \tool proficiently identified a significant propensity (ranging from 30\% to 60\%) for contemporary LLMs to generate hallucination responses when confronted with logical reasoning challenges. The test outcomes varied across different categories of questions, underscoring the ability of \tool to effectively discern and evaluate the FCH issues inherent in large language models.
% \lnk{too long}


\begin{tcolorbox}[title=ANSWER to RQ1, boxrule=0.8pt,boxsep=1.5pt,left=2pt,right=2pt,top=2pt,bottom=1pt]

Our evaluation using \tool reveals that existing LLMs have a notable tendency to produce FCH when faced with logical reasoning challenges, with hallucination rates ranging from 24.7\% to 59.8\%. The results varied across knowledge domains, highlighting that LLMs are more prone to FCH when answering questions that require highly specialized, domain-specific knowledge. 

\end{tcolorbox} 

\subsection{RQ2: FCH Categorization and Analysis}
\subsubsection{FCH Categorization}
We categorize the hallucination responses in more detail and focus primarily on the two types: error knowledge response, error inference response, and contradictory response. Note that we consider refusal to respond, such as `I don't know' due to the lack of relevant knowledge to be adhering to LLMs' honesty and truthfulness principles. Therefore, we categorize refusal to respond as a normal response. 
% Examples of the hallucination we found are illustrated in Figure~\ref{}. 
% For a fairer categorization, we apply GPT-4 for automatic assistance and manually check 100 responses that are randomly selected from hallucination-related test cases and draw our conclusions ultimately.
To ensure fair and unbiased categorization, 100 hallucination-related responses were randomly selected and independently categorized by three co-authors, who then discussed the results to reach a consensus categorization.
% To determine the appropriate threshold setting, authors independently label the selected test cases. Subsequently, after reaching a consensus, we compare them with the answers from the oracle and conclude to set the threshold to 0.8 ultimately.

\head{Error Knowledge Response.} Originated from LLMs utilizing erroneous or contextually inappropriate knowledge during the reasoning process.

\head{Error Inference Response.} The most frequently occurring type is attributed to the lack of reasoning power and flawed reasoning thoughts of LLMs.
% This category of hallucination represents the most frequently occurring type. This is attributed to the lack of reasoning power as well as the flawed reasoning thoughts of existing LLMs.

% \head{Contradictory Response.} Wrong reasoning process but along with a correct conclusion.

% \head{Meaningless Response.} 
% Meaningless and chaotic answers containing affirmative and negative responses.
% \lnk{need to rename}
% This type of hallucination contains both affirmative and negative responses, rendering the answer meaningless and chaotic.
% This type of r

% Furthermore, we observe that on Vicuna-13b-v1.3, a model fine-tuned on Llama, the likelihood of generating nonsensical responses to user queries has significantly increased. These meaningless replies include repeating the question instead of providing an answer, and producing responses entirely unrelated to the query such as writing useless code or counter-questioning the user.

\subsubsection{Hallucination Measurement} 
Here, we provide the distribution of the hallucination categorization results, as demonstrated in \figref{fig:rq2}. There is partial overlap between these two types of hallucinations because incorrect reasoning processes may also involve erroneous knowledge. Among these issues, there are several contradictory answers primarily arising from inconsistency between incorrect reasoning processes and correct answers; thus, it exists in these two types of errors. Error inference hallucination presents the most, totalling half of the results on average. 
%\shil{To check after adding GPT-40, is this statement still correct?} 
This indicates that the primary cause of FCH issues in logical reasoning is the insufficient reasoning capability of LLMs.
Besides, error knowledge adopted by LLMs during the logical reasoning process leads to 42.0\% %\shil{to check?} 
FCH issues on average. The overlaps account for 7.9\%-21.1\% at the hallucination ratio, which indicates there are entities where LLMs have learned entirely erroneous relevant information, necessitating the employment of certain measures for correction.
% As for the meaningless response, this category was primarily identified in the results of Llama2-7B-Chat and Vicuna-13B-v1.5, which indirectly evidences the relatively weaker reasoning capabilities of these two models. 
% Meaningless responses are extensively detected in the responses of the Llama2-7B-Chat, encompassing a significant number of 2,293 test cases. 
% For 2,293 test cases categorized as meaningful responses from Llama2-7B-Chat, we hypothesize that Llama2-7B-Chat tends to offer a neutral response aimed at satisfying all sides, yet this approach conflicts with the user's demand for a binary Yes/No answer, ultimately leading to a meaningless response.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/rq2.pdf}\\
    \vspace{-0.1cm}
    \caption{FCH Categorization.}
    \vspace{-0.3cm}
    \label{fig:rq2}
\end{figure}

\subsubsection{Case Study}
The preceding analysis broadly summarizes the distribution of categories for logical reasoning-related FCH. According to our investigation, error inference response and error knowledge response are the most prevalent two types.
% Now we delve into some more specific examples and provide three cases for inspiring insights.

% \head{Temporal Related Hallucination.}
\head{Error Inference Hallucination.}
One of the most common types of logical reasoning leading to error inference hallucination is temporal attribute reasoning, proven to be a category of reasoning task that performs poorly on LLMs~\cite{qiu2023large}. %Experiments on time-related reasoning tasks are comprehensively conducted and unsatisfactory performance of LLMs are observed.  
% Among the 678 time-related test cases that lead to FCH in GPT-3.5-turbo, there are 276 related to time duration, 313 related to chronological orders, and the rest 89 related to typical timings such as festivals. 
As illustrated in \figref{fig:case1}, error inference with correct knowledge leads to a hallucination response from Mistral-7B-v0.2. As knowledge provided by the LLM reasoning process, it is clear that the answer should be `Yes' as the 1874 Canadian federal election applies to the jurisdiction of Canada. However, it appears that the LLM has become ensnared by its limitations.
% Given that time duration is the most complicated scenario, we hereby provide an example. As illustrated in Figure~\ref{fig:case1}, time duration-related Q\&A pairs suffer from error inference hallucination. LLMs could obtain the correct factual knowledge, i.e. William Armstrong, 1st Baron Armstrong (1810-1900), and Richard Leveridge (1670-1758). The expected answer should be Yes, while LLM appears to be confused with the lifespan calculation.
% The most puzzling aspect is that despite listing valid and correct knowledge internally, the LLM still employs a divergent line of reasoning compared to a commonly accepted human logic to determine the outcome. 
A possible explanation for this phenomenon is that the LLM does not utilize its reasoning abilities but rather relies on unreliable intuition to respond when faced with a question lacking detailed instructions. This insight inspires us to explore methods for effectively enhancing the reasoning capabilities of LLMs through a single interaction, guiding these models toward uncovering answers in a way that mirrors human reasoning processes.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{fig/drowzee-case-1.pdf}\\%\vspace{-0.3cm}
\caption{Error Inference Hallucination from Mistral-7B-v0.2}
%\vspace{-0.2cm}
\label{fig:case1}
\end{figure}

\begin{tcolorbox}
\vspace{-0.15cm}
\textbf{Finding 1.} LLMs exhibit weaker performance in sensitivity to temporal information, as well as in their ability to discern sequential logic, which may result in error inference hallucination.
\vspace{-0.15cm}
\end{tcolorbox}

% \head{OOD Contextual Misleading Hallucination.}
\head{Error Knowledge Hallucination.}
\figref{fig:case2} demonstrates a classic example of LLM hallucination caused by using error knowledge for logical reasoning. General Dmitry Karbyshev (1880-1945) was a Russian Imperial Army soldier who served in several wars during World War I (1914-1918) and II (1939-1945), and Louis Bernacchi (1876-1942) was an Australian physicist and astronomer who served in the Royal Naval Volunteer Reserve during World War I. Thus, the ground truth answer to this question should be `Yes'. 
However, when testing with Llama2-7B-chat-hf, an %inspiring 
observation is that when LLMs encounter unfamiliar knowledge, they do not adhere to the honesty principle; instead, they fabricate knowledge and its sources. We subsequently employ an RAG-based scheme to reintroduce relevant knowledge, leading to the restoration of normal responses.
\begin{figure}
    \centering
    \small
    \includegraphics[width=\linewidth]{fig/drowzee-case-2.pdf}\\%\vspace{-0.3cm}
    \caption{Error Knowledge Hallucination from Llama2-7B-chat-hf.}
\vspace{-0.3cm}
\label{fig:case2}
%\vspace{-0.1cm}
\end{figure}

We further conduct an out-of-distribution (OOD) knowledge experiment to figure out the cause of error knowledge hallucination. OOD is another factor that could cause FCH issues~\cite{zhang2023hallucination}. We design contextual reasoning utilizing recent sporting events and natural disasters from Wikipedia since June 2023, which is considered unutilized information in LLMs' training data based on their up-to-date introductions. We construct a series of test cases containing contextual descriptions of recent events using \tool, observing whether LLMs can be guided to respond to OOD knowledge and trigger FCH. 
\figref{fig:case2.2} is a typical case of OOD contexts leading to error knowledge hallucination. In the initial test of GPT-3.5-turbo, we provide information on several wildfires that happened from July 2023 to December 2023, and we confirm that this information is not in the LLM's training data. The LLM subsequently indicates that it has acquired this knowledge through this interactive process. However, a turning point emerges when we use test cases designed by \tool in the second test. Despite our questions based on preliminary factual knowledge provided, the LLM still confidently responds with a wrong answer.
\begin{figure}
    \centering
    \small
    \includegraphics[width=\linewidth]{fig/drowzee-case-3.pdf}\\ %\vspace{-0.3cm}
    \caption{OOD-attributed Error Knowledge Example from GPT-3.5-turbo.}
\vspace{-0.5cm}
    \label{fig:case2.2}
\end{figure}

We analyze several potential causes for this situation. One possibility is that LLMs store incorrect knowledge in the first turn because what we provided was merely a list of events, rather than a list of events in their order of occurrence. In short, the normal reasoning process involves defining the earliest occurring events only after knowing the times of all events. However, the LLM opts to judge based on the order we provide event knowledge, which is contrary to facts. Another potential is that when LLMs encounter OOD knowledge if they do not strictly adhere to the principle of honesty by stating \textit{I do not know...}, they tend to complete and analyze the response based on error knowledge in their existing knowledge bases. Nevertheless, such responses are likely to induce hallucinations.

\begin{tcolorbox}
% \vspace{-0.15cm}
\textbf{Finding 2.} LLMs readily make erroneous assessments of misleading and unfamiliar knowledge and lead to error knowledge hallucination due to their assumptions.
% \vspace{-0.15cm}
\end{tcolorbox}


% \head{Refusal Normal Response.}
% During our analysis, we encountered an intriguing phenomenon, i.e. refusal normal response generated from GPT-4. It is important to note that our prompts include instructions mandating a definitive Yes/No response. While most models adhere to this directive for generating responses, GPT-4 occasionally deviates, offering a principled text \textit{As an AI, I don't know......} when it genuinely could not answer a question, as Figure~\ref{fig:case3} illustrated. We surmise that GPT-4, being the most advanced LLM in terms of performance and reasoning capabilities, retains a degree of critical thinking ability. It produces a reasoned response when confronted with demands exceeding its capabilities, in line with the principles of honesty preserved during its pre-training and fine-tuning. This aspect is something that most open-source models have yet to achieve. This insight inspires us to adhere to principles of honesty in the design and training of open-source LLMs. Additionally, it highlights the need to equip these models with robust critical thinking and logical reasoning capabilities to ensure more authentic and credible responses.
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{fig/case3-cropped.pdf}\\
%     % \vspace{-0.3cm}
%     \caption{GPT-4 Refusal Normal Response Example.}
%     % \vspace{-0.3cm}
%     \label{fig:case3}
% \end{figure}
% \begin{tcolorbox}
% % \vspace{-0.15cm}
% \textbf{Finding 3.} More advanced LLMs demonstrate a stronger orientation towards honesty, critical thinking, and principled responses.
% % \vspace{-0.15cm}
% \end{tcolorbox}


\begin{tcolorbox}[title=ANSWER to RQ2, boxrule=0.8pt,boxsep=1.5pt,left=2pt,right=2pt,top=2pt,bottom=1pt]
The detected FCH can be categorized into two types and the lack of reasoning capabilities poses a broader threat than the use of incorrect knowledge or inadequate inference strategies. 
\end{tcolorbox} 

\begin{figure}[!b]
\centering\includegraphics[width=0.8\linewidth]{fig/rq3-2.pdf}\\
\caption{Generation Rules that Trigger the Most Hallucination Responses on diverse LLMs across domains. The Number on Each Cell (the Unit: \%) Represents the Triggered FCH Ratio of the Corresponding Rule type.}
%\vspace{-0.3cm}
    \label{fig:rq3}
\end{figure}

\subsection{RQ3: Ablation Study}
We conduct an ablation study to investigate the capacity of each inference rule so that they can be distinctly used to uncover anomalies.
The four types of rules illustrated in \figref{fig:basic_op_for_predicates} are separately applied to generate Q\&A pairs. The symmetric reasoning rule is primarily utilized within the composite reasoning rule and does not introduce new knowledge on its own. Therefore, we did not include the symmetric reasoning rule as a separate condition in our ablation study.
% and the statistics are manifested as follows: 5,679 for transitive rules, 3,829 for inverse rules, 3,881 test cases for negation rules, and 839 for composition rules that integrate multiple inference rules.
For better visualization and understanding, we present the distribution of hallucination-related responses discovered with diverse rule-generated questions by \tool in Figure~\ref{fig:rq3}. The figure illustrates which type of rule can trigger the most hallucination responses for different LLMs and different domains of knowledge. It is distinctly evident that following the successful generation of various test cases using the four rules and their combinations, a substantial number of hallucinations are elicited across nine LLMs, with the composite rule yielding the highest amount of hallucinations. Following closely behind are the test cases generated using transitive rules, which have triggered a significant rate of FCHs in both the health and society domains.

From the comparison between four inference rules, we can conclude that all four inference rules demonstrate effectiveness when generating FCH test cases and inducing hallucination performances for LLM interaction.



\begin{tcolorbox}[title=ANSWER to RQ3, boxrule=0.8pt,boxsep=1.5pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
The experimental results showcase the independence of four inference rules in eliciting FCHs, and the composite rules can trigger the most FCHs across various domains, which has proved to be a sound approach to generating test cases. 
\end{tcolorbox} 



\subsection{RQ4: FCH Caused by Temporal Reasoning} 

This section presents the evaluation results based on temporal-property-related test cases. 

%\head{Effectiveness on Generating (Non-Temporal) Q\&A Test Cases.} We apply \tool to generate a Q\&A test benchmark, amounting to a comprehensive total of 7,200 test cases, designed to provide a broad and detailed evaluation of LLM FCH issues across specific knowledge domains. 

\begin{figure}[!b]
\centering
\includegraphics[width=1\linewidth]{fig/rq4-1.pdf}
\caption{LLM Hallucination Rate with Temporal Test Cases}
\label{fig:rq4-Effectiveness}
\end{figure}


\head{Effectiveness on Generating Temporal Q\&A Test Cases.} 
\tool generates 1,800 temporal test cases, and as illustrated in \figref{fig:rq4-Effectiveness}, there is a noticeable reduction in the hallucination rate for each LLM when compared to the results presented in \figref{fig:overall}, which suggests that LLMs possess some degree of temporal reasoning capabilities. However, it's important to note that these LLMs still exhibit temporal hallucinations, albeit to varying extents.
The result largely aligns with the previous discovery that the newer generation of LLMs performs better than the older version, as seen in Llama3-* and Llama2-*. However, fine-tuning LLM with more parameters does not improve reasoning ability, as seen in Llama2-7B and Llama2-70B. 



\head{Temporal Hallucinations Categorizations.} 
As illustrated in \figref{fig:rq4-Categorization}, inference errors constitute a higher proportion of hallucinations than knowledge errors, which is consistent with the previous finding of \figref{fig:rq2}. 

\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{fig/rq4-2.pdf}
\caption{RQ4: FCH Categorization.}
\label{fig:rq4-Categorization}
\end{figure}



% \begin{table*}[ht]
% \centering
% \begin{tabular}{l|c|c|c|c|c|c|c|c}
% \hline
% \textbf{Model} & \textbf{Next} & \textbf{Conj} & \textbf{Disj} & \textbf{Finally} & \textbf{Globally} & \textbf{Until} & \textbf{Neg} & \textbf{Mixed} \\ 
% \hline
% GPT-3.5-turbo  & 107 & 70  & 91  & 111 & 42  & 51 & 0 & 43 \\
% GPT-4          & 56  & 22  & 39  & 62  & 25  & 64  & 3  & 30 \\
% GPT-4o         & 64  & 51  & 87  & 26  & 12  & 61  & 0  & 30 \\
% Llama2-7b      & 136 & 134 & 132 & 104 & 35  & 84  & 4  & 46 \\
% Llama2-70b     & 132 & 125 & 122 & 126 & 40  & 74  & 22 & 52 \\
% Llama3.1-8b      & 75  & 81  & 95  & 64  & 16  & 78  & 6  & 52 \\
% Llama3.1-70b     & 59  & 54  & 141 & 57  & 19  & 73  & 0  & 41 \\
% Mistral7b      & 107 & 19  & 44  & 110 & 41  & 65  & 0  & 45 \\
% Mistral8x7b    & 63  & 82  & 106 & 65  & 22  & 70  & 2  & 42 \\
% \hline
% \end{tabular}
% \caption{Hallucination Case Counts among Different Temporal Operators}
% \label{tab:counts}
% \end{table*}

\begin{figure}[ht]  % figure* 用于双栏环境中的跨栏图像
    \centering
    \includegraphics[width=0.75\linewidth]{fig/ablation_heatmap.pdf}  % 图片宽度设置为 \textwidth
    \caption{Hallucination Rates wrt Temporal Operators}  % 图注
    \label{fig:heatmap}  % 图像引用标签
\end{figure}



%\head{Which temporal operators trigger the most/least hallucinations}
\head{Ablation Study upon  Temporal Operators.} 
Figure \figref{fig:heatmap} illustrates the results of an ablation study examining various types of temporal test cases. We categorize these temporal test cases based on their outermost layer operator. Most of the test cases are single-layer temporal operators, except for conjunction and disjunction, which may include nested temporal formulas.
We record the likelihood of each type of temporal test case triggering hallucinations. For instance, when testing GPT-4, it is observed that 20\% of the test cases related to the "Next" operator successfully trigger hallucinations. 
Notably, the "Neg" operator triggers the fewest hallucinations, whereas operators like "Finally," "Globally," and "Until" lead to the highest occurrence of hallucinations.
Overall, these findings indicate that a single layer of temporal operators is sufficiently effective in detecting LLM hallucinations related to the temporal reasoning capability. 

%evaluating the reasoning capabilities of LLMs.
%, compared to the \syh{mixed or nested} test cases. 





% \subsection{RQ4: Comparison with Existing Works}
% We qualitatively compare \tool with the state-of-the-art FCH evaluation approaches and existing natural language reasoning benchmarks to demonstrate the superiority.

% As illustrated in Table~\ref{table:comparison1}, we enumerate the characteristics of the sota FCH evaluation approaches. To assess FCH in LLM responses, existing approaches uniformly opt for a wiki-related knowledge base as the foundation for constructing ground truth facts. Their main distinction from \tool lies in the manner of task construction and the metrics employed to measure hallucinations.

% \textbf{Task Construction Methods.} Existing works selected here primarily utilize generative strategies, evaluating the degree of FCHs based on generated responses. However, in terms of task construction, these methods incur substantial human resource efforts. Apart from the KoLA-KM, KA~\cite{yu2023kola}, which is essentially a collection of existing Q\&A datasets, both TruthfulQA~\cite{lin-etal-2022-truthfulqa} and HaluEval~\cite{HaluEval} rely on human annotations to construct Q\&A pairs. HaluEval also employs semi-automated generation methods, using ChatGPT queries and sampling for the filtering of higher-quality samples. \tool, on the other hand, utilizes Prolog-assisted automatic inference to derive new knowledge triples and generate templates for new questions, achieving maximum automation of construction while ensuring the complexity of the questions.

% \textbf{Response Evaluation Metrics.} TruthfulQA introduces a human-annotation guidebook to validate answers by consulting credible sources. Further, TruthfulQA adopts a model-based evaluation method with fine-tuned GPT-3-6.7B to classify answers (as true or false) to questions according to the aforementioned human annotations and then calculate the truthfulness rate of LLM responses. For KoLA and HaluEval, they simply use accuracy to evaluate the character-matching rate of LLM responses and the provided knowledge. Thus, \tool considers the structural similarity of LLM responses with original knowledge triples and the reasoning process, offering superiority over those simple evaluation metrics.

% \begin{table*}[!ht]
%     % \def\arraystretch{1.0}
%     \setlength{\tabcolsep}{1ex}
% 	\centering
% 	% \footnotesize
% 	\caption{Comparison with SOTA FCH Evaluation Approaches.}
%  % \lnk{need to refine}}
%         \label{table:comparison1}
% 	\resizebox{\linewidth}{!}{
% 	\begin{tabular}{c c c c c}
%     \toprule 
%     \textbf{Dataset} &\textbf{Fact Source} & \textbf{Construction Method} &\textbf{Test Oracle} \\
%     \midrule
%     TruthfulQA & Wikipedia pages \& websites & Human annotations & Truthfulness Rate\\
%     \midrule
%     KoLA-KM, KA & Wikidata5M \& websites & Existing datasets consolidation & Standardized score (F1)\\ 
%     \midrule
%     HaluEval-QA & Wikipedia & Human annotations \& ChatGPT query & Accuracy\\ 
%     \midrule
%     \tool{}-Dataset  & Wikidata triples & Prolog-aided reasoning \& template-based generation & Structural Similarity\\
%     \bottomrule
%     \end{tabular}}
% \end{table*}

% As listed in Table~\ref{table:comparison2}, we provide several benchmarks aided for natural language reasoning. Existing reasoning benchmarks lean more towards logical predicate-formatted inputs and outputs, lacking natural language-formatted questions, thus limiting their suitability for testing with LLMs.

% \begin{table*}[!ht]
%     % \def\arraystretch{1.0}
%     \setlength{\tabcolsep}{1ex}
% 	\centering
%     \small
% 	% \footnotesize
% 	\caption{Comparison with Natural Language Reasoning Benchmarks.}
%  % \lnk{need to refine}}
%         \label{table:comparison2}
% 	\resizebox{\linewidth}{!}{
% 	\begin{tabular}{c c c c c c}
%     \toprule 
%     \textbf{Benchmark} &\textbf{Size} &\textbf{Reasoning Type} & \textbf{Data Source} & \textbf{Task}& \textbf{Automation}\\
%     \midrule
%     FOLIO & 1.4k & First-order logic reasoning & Expert-written & Theorem Proving & \ding{55}\\
%     \midrule
%     DEER & 1.2k & Inductive reasoning & Wikipedia & Rule Generation & \ding{55}\\
%     \midrule
%     % HotpotQA & 112k & Multi-hop reasoning & Wikipedia & Question Answering & \checkmark \\
%     % \midrule
%     \tool & Scalable & Deductive reasoning & Wikidata & Question Answering & \checkmark
%     \\
%     \bottomrule
%     \end{tabular}}
% \end{table*}

% \begin{tcolorbox}[title=ANSWER to RQ4, boxrule=0.8pt,boxsep=1.5pt,left=2pt,right=2pt,top=2pt,bottom=2pt]
% Compared with existing works, \tool exhibits better scalability, less human labor, and more reasonable evaluation metrics.
% \end{tcolorbox} 